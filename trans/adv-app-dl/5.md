# 5.成本函数和风格转移

在这一章中，我们将更深入地研究成本函数在神经网络模型中的作用。特别是，我们将讨论 MSE(均方误差)和交叉熵，并讨论它们的来源和解释。我们将着眼于为什么我们可以使用它们来解决问题，MSE 如何在统计意义上解释，以及交叉熵如何与信息论相关。然后，给你一个更高级的特殊损失函数的例子，我们将学习如何进行神经风格转移，这里我们将讨论一个神经网络以著名画家的风格绘画。

## 神经网络模型的组件

至此，您已经看到并开发了几个试图解决不同类型问题的模型。你现在应该知道，在所有的神经网络模型中，(至少)有三个主要构件:

*   网络架构(层数、层类型、激活功能等。)

*   损失函数(MSE，交叉熵等。)

*   优化器

优化器通常不是特定于问题的。例如，为了解决回归或分类问题，您需要选择不同的体系结构和损失函数，但是在这两种情况下您可以使用相同的优化器。在回归中，您可以使用前馈网络和 MSE 作为损失函数。在分类中，你可以选择卷积神经网络和交叉熵损失函数。但是在这两种情况下，您都可以使用 Adam 优化器。在决定网络可以学习什么方面起最大作用的组件是损失函数。改变它，你就会改变你的网络能够预测和学习的东西。

### 培训被视为一个优化问题

让我们试着更详细地理解为什么会这样。从纯理论的角度来看，训练一个网络无非就是解决一个真正复杂的优化问题。连续优化问题的标准公式是寻找给定函数的最小值

![$$ \underset{x}{\min }f(x) $$](img/470317_1_En_5_Chapter_TeX_Equa.png)

受制于两种约束类型

![$$ {\displaystyle \begin{array}{c}{g}_i(x)\le 0,\kern1em i=1,\dots, m\\ {}{p}_j(x)=0,\kern1em j=1,\dots, n\end{array}} $$](img/470317_1_En_5_Chapter_TeX_Equb.png)

其中*f*:`ℝ`<sup>*n*</sup>→`ℝ`是我们要最小化的连续函数，*g*<sub>*I*</sub>(*x*)≤0 表示不等式约束，*p*<sub>*j*</sub>(*x*)= 0 表示等式约束， *m 当然，没有约束也有可能出现问题。但是这和神经网络有什么关系呢？可以得出以下相似之处:*

*   函数 *f* ( *x* )是我们在建立神经网络模型时选择的损失函数。

*   输入 *x* ∈ `ℝ` <sup>*n*</sup> 是我们网络的权值(可学习参数)。请记住，我们可能选择的任何损失函数总是网络输出的函数(我们用![$$ \hat{y} $$](img/470317_1_En_5_Chapter_TeX_IEq1.png)表示)，并且输出总是权重 *W* (网络的可学习参数)的函数。

当我们训练一个网络时，我们实际上是在解决一个优化问题，一个我们想要最小化关于权重的损失函数的问题。我们隐式地拥有约束，尽管我们通常不会显式地声明它们。例如，我们可能有这样的约束，即我们希望一次观察所需的推断时间少于 10ms。在这种情况下，我们将有 *n* = 0(没有等式约束)， *m* = 1(一个不等式约束)，其中 *g* <sub>1</sub> 是推理运行时间。引用维基百科 <sup>[1](#Fn1)</sup> :

> *损失函数或成本函数是将一个事件或一个或多个变量的值映射到一个实数上的函数，该实数直观地表示与事件*相关的一些“成本”

通常，损失函数衡量模型对数据的理解程度。让我们来看几个简单的例子，这样你就可以在一个具体的案例中理解网络训练的这个公式。

### 一个具体的例子:线性回归

如你所知，如果你选择身份函数 <sup>[2](#Fn2)</sup> 作为激活函数，你可以用一个只有一个神经元的网络进行线性回归。我们用*x*<sup>[*I*]</sup>∈`ℝ`<sup>*n*</sup>和 *i* = 1、…， *m* 来表示观察值集合，其中 *m* 是我们拥有的观察值的数量。神经元(以及网络)将会有输出

![$$ {\hat{y}}^{\left[i\right]}=\sum \limits_{k=1}^n{w}_k{x}_k^{\left[i\right]}+b $$](img/470317_1_En_5_Chapter_TeX_Equc.png)

这里我们用 *w* = ( *w* <sub>1</sub> ，… *w* <sub>*n*</sub> )来表示权重。我们可以选择损失函数作为均方误差(MSE):

![$$ J\left(w,b\right)=\frac{1}{m}\sum \limits_{k=1}^m{\left({\hat{y}}^{\left[i\right]}-{y}^{\left[i\right]}\right)}^2 $$](img/470317_1_En_5_Chapter_TeX_Equd.png)

其中 *y* <sup>[ *i* ]</sup> 是我们要为第 *i* <sup>*th*</sup> 观察预测的目标变量。很容易看出，我们定义的损失函数是权重和偏差的函数。事实上，我们有

![$$ J\left(w,b\right)=\frac{1}{m}\sum \limits_{i=1}^m{\left({\hat{y}}^{\left[i\right]}-{y}^{\left[i\right]}\right)}^2=\frac{1}{m}\sum \limits_{i=1}^m{\left(\sum \limits_{k=1}^n{w}_k{x}_k^{\left[i\right]}+b-{y}^{\left[i\right]}\right)}^2 $$](img/470317_1_En_5_Chapter_TeX_Eque.png)

像我们通常使用(例如)梯度下降算法那样训练该网络无非是解决一个无约束优化问题，其中我们有(使用我们在开始时使用的符号):

![$$ f:= J $$](img/470317_1_En_5_Chapter_TeX_Equf.png)

## 成本函数

### 数学符号

让我们定义一些我们将在下一节中使用的符号。我们将使用

![$$ {\hat{y}}^{\left[\mathrm{i}\right]}\in {\mathbb{R}}^k $$](img/470317_1_En_5_Chapter_TeX_IEq2.png)是对 *i* <sup>* th *</sup> 观察的网络输出。

![$$ \hat{Y}\in {\mathbb{R}}^{m\times k} $$](img/470317_1_En_5_Chapter_TeX_IEq3.png)是包含所有观测值的网络输出的张量。<sup>[3](#Fn3)T4】</sup>

![$$ {x}^{\left[i\right]}\in {\mathbb{R}}^{n_x\times {n}_y\times {n}_c} $$](img/470317_1_En_5_Chapter_TeX_IEq4.png)代表 *i* <sup>* th *</sup> 观察输入特性(一般来说，对于图像我们会有 *n* <sub>* c *</sub> 通道，以及分辨率为*n*<sub>*x*</sub>×*n*<sub>*y*</sub>)。

![$$ X\in {\mathbb{R}}^{m\times {n}_x\times {n}_y\times {n}_c} $$](img/470317_1_En_5_Chapter_TeX_IEq5.png)是包含所有输入观测值的张量。

w 是网络中使用的所有可学习参数的集合(包括偏差)。

*m* 是观察次数。

*n* <sub>*c*</sub> 是图像通道的数量(对于 RGB 图像为 3)。

*n* <sub>*x*</sub> 是输入图像的水平分辨率。

*n*<sub>T3】yT5】是输入图像的垂直分辨率。</sub>

*J* 是成本函数。

通常，我们将所谓的成本(或损失)函数 *J* 一般定义如下:

![$$ J\left(X,\hat{Y}\left(\mathrm{W}\right)\right) $$](img/470317_1_En_5_Chapter_TeX_Equg.png)

除了网络架构之外，该功能将定义我们的神经网络模型能够解决什么样的问题。注意这个函数是如何

*   取决于网络架构，因为它取决于网络输出![$$ \hat{Y} $$](img/470317_1_En_5_Chapter_TeX_IEq6.png)(并因此取决于可学习的参数 W)

*   取决于输入数据集，因为它取决于输入 *X*

这是寻找最佳权重时将使用的函数。在几乎所有的优化器中，权重都以某种形式使用![$$ {\nabla}_{\mathrm{W}}J\left(X,\hat{Y}\left(\mathrm{W}\right)\right) $$](img/470317_1_En_5_Chapter_TeX_IEq7.png)来更新。

### 典型成本函数

正如我们在前面章节中看到的，在训练神经网络时，可以使用几个成本函数。在接下来的章节中，我们将详细介绍两个最常用的词汇，并试图理解它们的含义和来源。

#### 均方误差

均方误差函数

![$$ J\left(w,b\right)=\frac{1}{m}\sum \limits_{k=1}^m{\left({\hat{y}}^{\left[i\right]}-{y}^{\left[i\right]}\right)}^2 $$](img/470317_1_En_5_Chapter_TeX_Equh.png)

可能是开发回归模型时最常用的成本函数。对于这个成本函数有几种解释，但是下面两种应该可以帮助你对它有一个直观和更正式的理解。

##### 直观的解释

*J* 无非是预测值和实测值的平方差的平均值。所以基本上，它衡量的是预测值与期望值的差距。一个能够完美预测数据的完美模型(![$$ {\hat{y}}^{\left[i\right]}={y}^{\left[i\right]} $$](img/470317_1_En_5_Chapter_TeX_IEq8.png)代表所有的 *i* = 1，…， *m* )应该是 *J* = 0。一般来说，它保持最小的 *J* 预测越好。

### 注意

一般来说，它认为 MSE 越小，预测越好(因此，模型越好)。

最小化 MSE 意味着找到参数，使我们的网络输出尽可能接近我们的训练数据。请注意，您可以通过使用以下公式给出的 MAE(平均绝对误差)来获得类似的结果

![$$ MAE=\frac{1}{m}\sum \limits_{k=1}^m\left|{\hat{y}}^{\left[i\right]}-{y}^{\left[i\right]}\right| $$](img/470317_1_En_5_Chapter_TeX_Equi.png)

尽管通常不这样做。

##### MSE 作为矩生成函数的二阶矩

有一种更正式的方法来解释 MSE。让我们定义数量

![$$ \Delta {Y}^{\left[i\right]}={\hat{y}}^{\left[i\right]}-{y}^{\left[i\right]} $$](img/470317_1_En_5_Chapter_TeX_Equj.png)

让我们定义力矩生成函数

![$$ {M}_{\Delta Y}(t):= E\left[{e}^{t\varDelta Y}\right] $$](img/470317_1_En_5_Chapter_TeX_Equk.png)

这里我们有 *t* ∈ `ℝ`，我们用 *E* [ ]表示变量在所有观测中的期望值。我们将跳过关于期望值存在与否的讨论，取决于δ*Y*的特性，因为这超出了本书的范围。我们可以用泰勒级数展开法来展开*e*<sup>*tδ*Y</sup>(我们假设我们可以这样做):

![$$ {e}^{t\varDelta \mathrm{Y}}=1+t\Delta Y+\frac{t^2\varDelta {Y}^2}{2!}+\dots $$](img/470317_1_En_5_Chapter_TeX_Equl.png)

因此

![$$ {M}_{\varDelta Y}(t):= E\left[{e}^{t\varDelta \mathrm{Y}}\right]=1+ tE\left[\Delta Y\right]+\frac{t^2E\left[\varDelta {Y}^2\right]}{2!}+\dots $$](img/470317_1_En_5_Chapter_TeX_Equm.png)

*E*【δ*Y*<sup>T5】n</sup>称为函数的 *n* <sup>*th*</sup> 矩*M*<sub>*δY*</sub>(*t*)。你可以看到，这些时刻很容易解释(至少第一次):

*   *E*[*δY*:*M*<sub>*δY*</sub>(*t*)-*δY 的一阶矩*

**   *E*[*δY*<sup>2</sup>:M*<sub>*δY*</sub>(*t*)-*就是我们定义的 MSE 函数**

    **   *E*[*δY*<sup>3</sup>:M*<sub>*δY*</sub>(*t*)-*偏斜度* <sup>[5](#Fn5)</sup>*

    **   *E*[*δY*<sup>4</sup>:M*<sub>*δY*</sub>(*t*)-*峰度* <sup>[6](#Fn6)</sup>**** 

 **我们可以简单地把二阶矩写成观测值的平均值

![$$ E\left[\varDelta {Y}^2\right]:= \frac{1}{m}\sum \limits_{k=1}^m\Delta {Y^{\left[i\right]}}^2=\frac{1}{m}\sum \limits_{k=1}^m{\left({\hat{y}}^{\left[i\right]}-{y}^{\left[i\right]}\right)}^2 $$](img/470317_1_En_5_Chapter_TeX_Equn.png)

如果我们假设我们的模型用*E*【δ*Y*】= 0 来预测数据，那么*E*【δ*Y*T8】2(因此 MSE)无非是我们的数据点分布的方差δ*Y*T12】[*I*。在这种情况下，它只是测量我们的点在平均值(即零)周围的分布范围:完美的预测。记住，如果对于一个观测，我们有δ*Y*T18】[*I*]= 0，这意味着我们有![$$ {\hat{y}}^{\left[i\right]}={y}^{\left[i\right]} $$](img/470317_1_En_5_Chapter_TeX_IEq9.png)，意味着预测是完美的。只是为了给出正确的术语，如果*E*[*δY*]不为零，那么这些矩有时被称为*非中心矩* *。如果你正在处理非中心矩，你不能再直接把它们解释为统计量(方差)。*

### 注意

如果你正在处理非中心矩，你不能再直接把它们解释为统计量(方差)。如果δ*Y*<sup>[*I*]</sup>的平均值为零，那么 MSE 就是我们预测的分布的方差。当然，值越小，预测就越准确。

#### 交叉熵

有几种理解交叉熵损失函数的方法，但我认为最迷人的方法是从信息论开始讨论。在这一节中，我们将在更直观的基础上讨论一些基本概念，以给你足够的信息和理解，从而对交叉熵有一个非常有力的理解。

##### 事件的自我信息或抑制

我们需要从自我信息的概念开始，或者说一个事件的极限。为了对它有一个直观的理解，请考虑以下几点:当一个事件发生一个不可能的结果时，我们把它与高层次的信息联系起来。当一个结果总是发生时，通常它没有太多的相关信息。换句话说，当不太可能的事件发生时，我们会更惊讶；因此，它也被称为一个结果的上限。我们如何用数学的形式来表达它呢？我们来考虑一个随机变量 *X* 带 *n* 可能结果 *x* <sub>1</sub> ， *x* <sub>2</sub> ，…， *x* <sub>*n*</sub> 和概率质量函数<sup>[7](#Fn7)</sup>*P*(*X*)。让我们用*<sub>*I*</sub>=*P*(*x*<sub>*I*</sub>)来表示事件*x<sub>*I*</sub>发生的概率。在 0 和 1 之间的任何单调递减函数*I*(*p*<sub>*I*</sub>)都可以用来表示随机变量 *X* 的上界(或自身信息)。但是这个函数必须有一个重要的性质:如果事件是独立的，那么 *I* 应该满足**

*![$$ I\left({p}_i{p}_j\right)=I\left({p}_i\right)+I\left({p}_j\right) $$](img/470317_1_En_5_Chapter_TeX_Equo.png)*

 *如果结果 *i* 和 *j* 是独立的。人们马上想到一个具有这种特性的函数:对数。事实上，这是事实

![$$ \ln \left({p}_i{p}_j\right)=\log {p}_i+\log {p}_j $$](img/470317_1_En_5_Chapter_TeX_Equp.png)

为了让它单调递减，我们可以选择以下公式:

![$$ I\left({p}_i\right)=-\log {p}_i $$](img/470317_1_En_5_Chapter_TeX_Equq.png)

##### 与事件 X 相关的 Suprisal

总的来说，我们有多少关于特定事件的信息？这是通过对 *X* 的所有可能结果的期望值来衡量的(我们将用 *P* 来表示这个集合)。数学上，我们可以把它写成

![$$ H(X)={E}_P\left[I(X)\right]=\sum \limits_{i=1}^nP\left({x}_i\right)I\left({x}_i\right)=-\sum \limits_{i=1}^nP\left({x}_i\right){\log}_bP\left({x}_i\right) $$](img/470317_1_En_5_Chapter_TeX_Equr.png)

*H* ( *X* )被称为*香农熵*，而 *b* 是算法的基础，通常被选为 2、10 或 *e* 。

##### 交叉熵

现在让我们假设我们想要比较事件 *X* 的两种概率分布。我们来分析一下，当我们训练一个神经网络进行分类时，我们做了什么。请考虑以下几点:

*   我们的例子给出了事件的“真实”或预期分布(真实标签)。他们的分布将是我们的 P。例如，我们的观测可能包含具有一定概率的猫类(假设这是类 1)*P*(*x*T6】1，其中*x*T10】1 是结果“这个图像中有一只猫”。我们有给定的概率质量函数， *P* 。

*   我们训练的网络将会给我们一个不同的概率质量函数， *Q* ，因为预测将不会与训练数据完全相同。结局*x*T5】1(“图像里有一只猫”)会以不同的概率发生， *Q* ( *x* <sub>1</sub> )。您应该记得，在构建分类网络时，我们对输出层使用了一个`softmax`激活函数来将输出解释为概率。你看到所有的事情突然变得更有意义了吗？

我们希望有一个尽可能反映给定标签的预测，这意味着我们希望有一个尽可能类似于 *P* 的概率质量函数 *Q* 。

为了比较两个概率质量函数(我们感兴趣的)，我们可以简单地用实例得到的分布来计算我们的网络得到的自我信息的期望值。以更数学的形式

![$$ H\left(Q,P\right)={E}_P\left[I(Q)\right]={E}_P\left[-{\log}_bQ\right]=-\sum \limits_{i=1}^nP\left({x}_i\right){\log}_bQ\left({x}_i\right) $$](img/470317_1_En_5_Chapter_TeX_Equs.png)

如果你有信息论方面的经验， *H* ( *Q* ， *P* )会给出两个概率质量函数 *Q* 和 *P* 相似性的度量。为了理解为什么，让我们考虑一个实际的例子。这将是一场公平的掷硬币游戏。 *X* 将有两种可能的结果:*X*T16】1 将是硬币的头部，而*X*T20】2 将是硬币的尾部。“真实的”概率质量函数当然是一个常数函数，其中*P*(*x*<sub>1</sub>)= 0.5，*P*(*x*<sub>2</sub>)= 0.5。现在让我们考虑另一种概率质量函数 *Q* <sub>*i*</sub> (为了说明的目的，我们将只考虑 9 个可能的值):

*   *I*= 1→T2】q1(*x*1= 0.1，*q*T12】

*   *I*= 2→T2】q*2(*x*t1= 0.2，*q*T12】*

*   *I*= 3→T2】【q】(*<sub>)= 0.3，*【q11】*</sub>*

**   *I*= 4→T2】q<sub>4</sub>(*x*<sub>1</sub>= 0.4，*q*T12】

    *   *I*= 5→T2】q<sub>5</sub>(*x*<sub>1</sub>= 0.5，*q*T12】

    *   *I*= 6→T2】q6(*x*t1= 0.6，*q*T12】

    *   *I*= 7→T2】q<sub>7</sub>(*x*<sub>1</sub>= 0.7，*q*T12】

    *   *I*= 8→T2】q8(*x*1= 0.8，*q*T12】

    *   *I*= 9→T2】q<sub>9</sub>(*x*<sub>1</sub>= 0.9，*q*T12】* 

 *我们来计算一下*H*(*Q*<sub>T5】I</sub>， *P* )对于 *i* = 1，…5。对于 *i* = 6，..我们不需要计算 *H* 。。，9 既然函数是对称的，意思就是比如说那个*H*(*Q*<sub>4</sub>，*P*)=*H*(*Q*<sub>6</sub>， *P* )。在图 [5-1](#Fig1) 中可以看到*H*(*Q*<sub>*I*</sub>， *P* )的剧情。你可以看到当两个概率质量函数相同时，当 *i* = 5 时达到最大值。

![img/470317_1_En_5_Fig1_HTML.jpg](img/470317_1_En_5_Fig1_HTML.jpg)

图 5-1

H(Q <sub>i</sub> ，P)为 i = 1，…5。当两个概率质量函数完全相同时，对于 i = 5 获得最小值。

### 注意

交叉熵 *H* ( *Q* ， *P* )是两个质量概率函数 *Q* 和 *P* 相似程度的度量。

##### 二元分类的交叉熵

现在让我们考虑一个二元分类问题，看看交叉熵是如何工作的。假设我们的事件 *X* 是给定图像的两类分类。可能的结果只有两个:1 类或 2 类。为了说明的目的，让我们假设我们的图像属于类别 1。我们对于图像的“真实”概率质量函数将具有*P*(*x*T6】1)= 1.0，*P*(*x*T12】2)= 0。换句话说，由于我们知道真实值，我们的概率质量函数 *P* 只能是 0 或 1。

你会记得，在一个二元分类问题中，我们使用了以下内容

![$$ \mathcal{L}\left({\hat{y}}^{(j)},{y}^{(j)}\right)=-\left({y}^{(j)}\ \log {\hat{y}}^{(j)}+\left(1-{y}^{(j)}\right)\log \left(1-{\hat{y}}^{(j)}\right)\right) $$](img/470317_1_En_5_Chapter_TeX_Equt.png)

其中 *y* <sup>( *j* )</sup> 表示真实标签(0 表示类 1，1 表示类 2)，而![$$ {\hat{y}}^{(j)} $$](img/470317_1_En_5_Chapter_TeX_IEq10.png)是图像 *j* 属于类 2 的概率，或者换句话说，是假设值为 1 的网络输出的概率。我们将最小化的成本函数由所有观察值(或例子)的总和给出

![$$ J\left(\boldsymbol{w},b\right)=\frac{1}{m}\sum \limits_{j=1}^m\mathcal{L}\left({\hat{y}}^{(j)},{y}^{(j)}\right) $$](img/470317_1_En_5_Chapter_TeX_Equu.png)

使用上一节的符号，我们可以为图像 *j* 编写

![$$ {p}_j\left({x}_1\right)=1-{y}^{(j)} $$](img/470317_1_En_5_Chapter_TeX_Equv.png)

![$$ {p}_j\left({x}_2\right)={y}^{(j)} $$](img/470317_1_En_5_Chapter_TeX_Equw.png)

记住*y*T2(*j*)只能是 0 或 1；所以我们只有两种可能:*p*<sub>*j*</sub>(*x*<sub>1</sub>)= 1，*p*<sub>*j*</sub>(*x*<sub>2</sub>)= 0 或者*p*<sub>*j*</sub>(*x 我们也可以写网络的预测*

![$$ {q}_j\left({x}_1\right)=1-{\hat{y}}^{(j)} $$](img/470317_1_En_5_Chapter_TeX_Equx.png)

![$$ {q}_j\left({x}_2\right)={\hat{y}}^{(j)} $$](img/470317_1_En_5_Chapter_TeX_Equy.png)

请记住:这个结果是由我们如何构建我们的网络(因为我们在输出层使用了`softmax`激活函数来获得概率)和我们如何编码我们的标签(0 和 1，以便它们可以被解释为概率)决定的。现在，让我们使用我们的神经网络符号来编写上一节中定义的交叉熵，但是对所有示例求和(请记住，我们希望获得所有事件的整个交叉熵，换句话说，所有图像的交叉熵):

![$$ H\left(Q,P\right)=-\sum \limits_{j=1}^m\sum \limits_{i=1}^2{p}_j\left({x}_i\right){\log}_b{q}_j\left({x}_i\right)=-\sum \limits_{j=1}^m\left({y}^{(j)}{\log}_b{\hat{y}}^{(j)}+\left(1-{y}^{(j)}\right){\log}_b\left(1-{\hat{y}}^{(j)}\right)\right) $$](img/470317_1_En_5_Chapter_TeX_Equz.png)

所以基本上![$$ \mathcal{L}\left({\hat{y}}^{(i)},{y}^{(i)}\right) $$](img/470317_1_En_5_Chapter_TeX_IEq11.png)只不过是在信息论中得到的交叉熵。

### 注意

直觉上，当我们最小化二元分类问题中的交叉熵时，我们最小化了当我们的预测与我们的期望不同时我们可能有的惊讶。

*H* ( *Q* ， *P* )衡量我们的预测概率密度函数( *Q* )与我们的训练样本概率密度函数( *P* )的匹配程度。

### 注意

当我们使用交叉熵设计用于分类的网络，并且我们在最终层使用`softmax`激活函数来将输出解释为概率时，我们简单地构建了基于信息论的复杂分类系统。我们应该感谢香农 <sup>[8](#Fn8)</sup> 用神经网络进行分类。

#### 成本函数:最后一句话

现在应该很清楚，成本函数决定了神经网络可以学习什么。改变它，网络就会学到完全不同的东西。毫不奇怪，要获得特殊的结果，比如艺术，只需要选择正确的架构和正确的成本函数。在本章的下一部分，我们将着眼于神经类型转移，选择正确的成本函数(在这个例子中，我们将看到多个)是实现非凡结果的关键，这一点将变得非常清楚。

## 神经类型转移

此时，您已经拥有了开始使用网络进行更高级技术的所有工具:使用预先训练的 CNN，从隐藏层提取信息，以及使用自定义成本函数。这开始成为高级材料，所以你需要很好地理解我们在前面章节中讨论的所有基础知识。如果有什么不清楚的地方，回头再研究一遍。

CNN 的一个有趣而好玩的应用是制作艺术品，神经风格转移(NST)指的是一种操纵数字图像的技术，采用另一幅图像的外观或风格 <sup>[9](#Fn9)</sup> 。一个有趣的应用程序是拍摄一幅图像，让网络操纵它，使其采用著名画家的风格，比如梵高。使用深度学习的 NST 最早出现在 Gatys 等人 2015 年的一篇论文中 <sup>[10](#Fn10)</sup> 。这是一种新技术。Gatys 开发的方法使用预先训练的深度 CNN 来将图像的内容与风格分开。

这个想法是将一幅图像输入预先训练好的 VGG-19 <sup>[11](#Fn11)</sup> CNN，在`imagenet`数据集上进行训练。作者假设图像的内容可以在网络中间层输出中找到(图像通过每层中的学习过滤器)，而风格在于不同层输出的相关性(编码在格拉米矩阵中)。预先训练的网络可以很好地识别图像的内容，因此每一层学习的特征必须与图像的内容紧密相关，而不是与风格相关。事实上，一个擅长识别图像的健壮的 CNN 并不太在乎风格。直观地说，风格包含在图像空间上不同的滤波器响应是如何相关的。画家可能会使用宽或窄的笔触，可能会使用许多彼此接近的颜色或仅使用几种颜色，等等。请记住，在 CNN 中，每一层都只是图像过滤器的集合；因此，给定层的输出只是输入图像的不同过滤版本的集合 10。

另一种方式是，当你从远处看一幅图像时(你不太关心细节)，内容被发现，而当你在更近的尺度上看图像时，风格被发现，这取决于图像的不同部分如何相互联系。Gatys 等人聪明地用数学方法简单地实现了这些想法。为了给你一个思路，请看图 [5-2](#Fig2) 。一个网络已经将原始图像(左上)处理成了右上角梵高画作的风格，以获得底部的图像。

![img/470317_1_En_5_Fig2_HTML.jpg](img/470317_1_En_5_Fig2_HTML.jpg)

图 5-2

NST 的一个例子。该方法将原始图像(左上)处理成右上的梵高绘画风格，以获得底部的图像。

### NST 背后的数学

原始论文使用的是 VGG19 网络，Keras 提供给我们下载和使用。我们在这里用 *x* 表示的输入图像(我将尽可能使用原始符号)被编码在 CNN 的每一层中。带有*N*<sub>T5】l</sub>过滤器(有时也称为内核)的图层将具有 *N* <sub>*l*</sub> 特征地图作为输出。在该算法中，这些输出将在尺寸为 *M* <sub>*l*</sub> 的一维向量中展平，其中 *M* <sub>*l*</sub> 是当应用于输入图像时每个滤波器的输出的高度乘以宽度。层 *l* 的响应可以被编码到张量![$$ {F}^l\in {\mathbb{R}}^{N_l\times {M}_l} $$](img/470317_1_En_5_Chapter_TeX_IEq12.png)中。让我们在这里暂停一下，试着用一个具体的例子来理解我们的意思。

假设我们使用彩色图像作为输入图像，每个图像的尺寸为 32 × 32。让我们考虑用代码创建的 CNN 中的第一个卷积层:

```py
Conv2D(32, (3, 3), padding="same", activation="relu", input_shape=input_shape))

```

当然是哪里`input_shape = (32,32,3)`。图层的输出将具有以下尺寸

```py
(None, 32, 32, 32)

```

其中当然`None`将假设所使用的观察值的数量。这是因为我们使用了参数`padding = 'same'`。在这种情况下，层 *l* = 1 的输出是 32 个特征图(或输入图像与 32 个过滤器卷积的结果)，每个尺寸为 32 × 32。在这种情况下，我们将有 *N* <sub>*l* = 1</sub> = 32 和*M*<sub>*l*= 1</sub>= 32×32 = 1024。在计算格拉米矩阵之前，将展平每个 32 × 32 的特征图。您将在后面的代码中清楚地看到这是如何实现的。

我们姑且称原图 *p* 。这就是我们想要改变的形象。作为输出生成的图像被称为 *x* 。我们将用 *P* <sup>*l*</sup> 和 *F* <sup>*l*</sup> 表示它们各自从图层 *l* 中得到的特征图。我们定义称为*内容损失函数*的平方误差损失如下:

![$$ {\mathcal{L}}_{content}\left(p,x,l\right)=\frac{1}{2}\sum \limits_{i,j}{\left({F}_{ij}^l-{P}_{ij}^l\right)}^2 $$](img/470317_1_En_5_Chapter_TeX_Equaa.png)

在 Keras 中，我们将使用以下代码实现这一点:

```py
content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2)
                             for name in content_outputs.keys()])

```

其中`content_outputs[]`和`content_targets[]`将分别包含应用于输入(`content_outputs`)和生成的图像(`content_targets`)时 VGG19 的特定层的输出(已经展平)。稍后我们将更详细地讨论它；如果你没有完全理解，暂时不要担心。你可能想知道为什么我们没有因子 1/2，但我们并不需要它，因为![$$ {\mathcal{L}}_{content}\left(p,x,l\right) $$](img/470317_1_En_5_Chapter_TeX_IEq13.png)将乘以另一个因子，这将使 1/2 无用。

我们需要计算损失函数相对于图像的梯度。这是相当重要的一点。这意味着我们想要学习的参数是我们想要改变的图像的像素值。网络的参数是固定的，我们不需要改变它们。对于 Keras，我们需要使用以下形式的`tape.gradient`函数:

```py
tape.gradient(loss, image)

```

我们需要将图像定义为一个张量流`Variable`(稍后会详细介绍)。如果你不熟悉`tape.gradient`的工作原理，我建议你去 [`https://www.tensorflow.org/tutorials/eager/automatic_differentiation`](https://www.tensorflow.org/tutorials/eager/automatic_differentiation) 查阅官方文档。

### 注意

我们要学习的参数是我们要改变的图像的像素值，而不是网络的权重。

现在我们需要注意风格。为此，我们需要为样式定义一个损失函数。为此，我们需要定义 Gramian 矩阵*G*<sup>T3】lT5】，它是图层 *l* 中展平后的特征图 *i* 和 *j* 之间的内积。换句话说</sup>

![$$ {G}_{ij}^l=\sum \limits_k{F}_{ik}^l{F}_{kj}^l $$](img/470317_1_En_5_Chapter_TeX_Equab.png)

有了这个新定义的量，我们将定义一个样式损失函数![$$ {\mathcal{L}}_{style}\left(a,x\right) $$](img/470317_1_En_5_Chapter_TeX_IEq14.png)，其中 *a* 是我们想要使用样式的图像

![$$ {\mathcal{L}}_{style}\left(a,x\right)=\sum \limits_{l=1}^5{w}_l{E}_l $$](img/470317_1_En_5_Chapter_TeX_Equac.png)

在哪里

![$$ {E}_l=\frac{1}{4{N}_l^2{M}_l^2}\sum \limits_{i,j}{\left({G}_{ij}^l-{A}_{ij}^l\right)}^2 $$](img/470317_1_En_5_Chapter_TeX_Equad.png)

其中 *w* <sub>*l*</sub> 为原试卷中选取的权重，等于 1/5。在 Keras 中，我们将通过代码实现这种丢失(我们将在后面查看细节):

```py
tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2)
                           for name in style_outputs.keys()])

```

`style_outputs`和`style_targets`变量将包含 VGG19 网络五层的输出。在原始论文中，使用了以下五层:

```py
l=1 - block1_conv1
l=2 - block2_conv1
l=3 - block3_conv1
l=4 - block4_conv1
l=5 - block5_conv1

```

这些是 VGG19 网络中每个模块的第一层。请记住，您可以通过以下代码从 VGG19 中获取图层名称:

```py
vgg = tf.keras.applications.VGG19(include_top=False, weights="imagenet")

print()
for layer in vgg.layers:
  print(layer.name)

```

会得到这样的结果:

```py
input_1
block1_conv1
block1_conv2
block1_pool
block2_conv1
block2_conv2
block2_pool
block3_conv1
block3_conv2
block3_conv3
block3_conv4
block3_pool
block4_conv1
block4_conv2
block4_conv3
block4_conv4
block4_pool
block5_conv1
block5_conv2
block5_conv3
block5_conv4
block5_pool

```

注意，我们没有密集层，因为我们使用了`include_top=False`。最后，我们将最小化下面的损失函数

![$$ {\mathcal{L}}_{total}\left(p,x,a\right)=\alpha {\mathcal{L}}_{style}\left(a,x\right)+\beta \sum \limits_{l=1}^5{\mathcal{L}}_{content}\left(p,x,l\right) $$](img/470317_1_En_5_Chapter_TeX_Equae.png)

使用梯度下降(例如)，相对于我们想要改变的图像。可以选择常数 *α* 和 *β* 来赋予样式或内容更大的权重。对于图 [5-1](#Fig1) 中的结果，我选择了 *α* = 1.0， *β* = 10 <sup>4</sup> 。其他典型值有*α*= 10<sup>—2</sup>， *β* = 10 <sup>4</sup> 。

### Keras 风格转移的一个例子

我们将在此讨论的代码取自最初的 TensorFlow NST 教程，并针对本次讨论进行了极大的简化。为了简化讨论，我们将只讨论部分代码，因为整个代码相对较长。你可以在这本书的 GitHub 资源库的 Chapter [5](5.html) 文件夹中找到完整的简化版。我建议你在启用 GPU 的情况下运行 Google Colab 中的代码，因为它的计算量相当大。给你一个概念，在我的笔记本电脑上，一个 epoch 大约需要 13 秒，而在谷歌 Colab 上，处理 512 × 512 像素的图像需要 0.5 秒。

为了确保您安装了最新的 TensorFlow 版本，您应该在笔记本的开头运行以下代码:

```py
from __future__ import absolute_import, division, print_function, unicode_literals
!pip install tensorflow-gpu==2.0.0-alpha0
import tensorflow as tf

```

如果您在 Google Colab 上运行代码，您需要将想要处理的图像保存在 Google drive 上并挂载它。为此，您需要在硬盘上上传两个图像:

*   一个风格形象:比如一幅名画。这是您想要从中获取样式的图像。

*   内容图像:例如，您拍摄的风景或照片。这是您要修改的图像。

我在这里假设你已经把你的图片上传到了 Google drive 根目录下的一个名为`data`的文件夹中。你现在需要做的是在 Google Colab 中安装你的 Google drive 来访问这些图片。为此，您需要以下代码:

```py
from google.colab import drive
drive.mount('/content/drive')

```

如果您运行这段代码，您需要转到一个特定的 URL(由 Google Colab 提供),在那里您将收到需要粘贴到笔记本中的代码。在 [`http://toe.lt/a`](http://toe.lt/a) 可以找到关于如何做到这一点的很好的概述。装载后，您将获得目录中的文件列表，如下所示:

```py
!ls "/content/drive/My Drive/data"

```

我们可以定义我们将使用的图像的文件名

```py
content_path = '/content/drive/My Drive/data/landscape.jpg'
style_path = '/content/drive/My Drive/data/vangogh_landscape.jpg'

```

当然，您需要将文件名改为您自己的文件名。但是如果您想尝试使用这些图片，您可以在 GitHub 存储库中找到我在这个例子中使用的图片。如果没有的话，您需要创建`data`目录，并将图像复制到那里。图像将通过`load_img()`功能加载。请注意，在开头的函数中，我们调整了图像的大小，使其最大尺寸等于 512(`load_img()`函数的完整代码可以在 GitHub 上找到)。这是一个可管理的大小，但是如果您想生成更好看的图像，您需要增加这个值。图 [5-1](#Fig1) 中的图像是用`max_dim = 1024`生成的。该函数开始于

```py
def load_img(path_to_img):
  max_dim = 512
  img = tf.io.read_file(path_to_img)

```

因此，您更改了`max_dim`变量的值来处理更大的图像。现在，我们只需要选择一些层的输出，正如我们在上一节中所描述的。为此，我们将想要使用的层的名称放在两个列表中:

```py
# Content layer where will pull our feature maps
content_layers = ['block5_conv2']

# Style layer we are interested in
style_layers = ['block1_conv1',
                'block2_conv1',
                'block3_conv1',
                'block4_conv1',
                'block5_conv1']

```

这样，我们可以使用名称选择正确的层。我们需要的是一个模型，从每一层获取输入并返回所有的特征地图。为此，我们使用以下代码

```py
def vgg_layers(layer_names):
  vgg = tf.keras.applications.VGG19(include_top=False, weights="imagenet")
  vgg.trainable = False

  outputs = [vgg.get_layer(name).output for name in layer_names]

  model = tf.keras.Model([vgg.input], outputs)
  return model

```

此函数获取一个带有图层名称的列表作为输入，并使用以下代码行选择给定图层的网络层输出:

```py
outputs = [vgg.get_layer(name).output for name in layer_names]

```

请注意，没有检查，所以如果你有一个错误的层名称，你不会得到你期望的结果。但是由于我们需要的层是固定的，所以不需要检查网络中是否存在这些名称。这条线

```py
model = tf.keras.Model([vgg.input], outputs)

```

根据`layer_names`输入列表中的层数，创建一个有一个输入(`vgg.input`)和一个或多个输出的模型。

为了计算![$$ {G}_{ij}^l $$](img/470317_1_En_5_Chapter_TeX_IEq15.png)(格拉米矩阵)，我们使用这个函数

```py
def gram_matrix(input_tensor):
  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)
  input_shape = tf.shape(input_tensor)
  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)
  return result/(num_locations)

```

其中变量`num_locations`简单来说就是 *M* <sub>*l*</sub> 。现在有趣的部分来了:损失函数的定义。我们需要定义一个名为`StyleContentModel`的类，它将接受我们的模型，并在每次迭代中返回不同层的输出。该类有一个`__init__`部分，我们将在这里跳过(您可以在 Jupyter 笔记本中找到代码)。有趣的部分是`call()`功能:

```py
def call(self, inputs):
    inputs = inputs*255.0
    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)
    outputs = self.vgg(preprocessed_input)
    style_outputs, content_outputs = (outputs[:self.num_style_layers],
                                      outputs[self.num_style_layers:])

    style_outputs = [gram_matrix(style_output)
                     for style_output in style_outputs]

    content_dict = {content_name:value
                    for content_name, value
                    in zip(self.content_layers, content_outputs)}

    style_dict = {style_name:value
                  for style_name, value
                  in zip(self.style_layers, style_outputs)}

    return {'content':content_dict, 'style':style_dict}

```

该函数将返回一个包含两个元素的字典— `content_dict`包含内容层及其输出，而`style_dict`包含样式层及其输出。您可以使用此功能:

```py
extractor = StyleContentModel(style_layers, content_layers)

```

然后:

```py
style_targets = extractor(style_image)['style']
content_targets = extractor(content_image)['content']

```

这样，当应用于不同的图像时，我们可以得到不同层的输出。请记住，当应用于我们的梵高画作时，我们需要样式层的输出，但当应用于风景(或您的图像)图像时，我们需要内容层的输出。让我们将内容图像(风景或您的图像)保存在一个变量中，并定义一个函数(它将在后面 9 中有用),该函数将在 0 和 1 之间裁剪数组的值:

```py
image = tf.Variable(content_image)
def clip_0_1(image):
  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)

```

那么我们可以将两个变量 *α* 、 *β* 定义如下:

```py
style_weight=1e-2
content_weight=1e4

```

现在我们有了定义损失函数所需的一切:

```py
def style_content_loss(outputs):
    style_outputs = outputs['style']
    content_outputs = outputs['content']
    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2)
                           for name in style_outputs.keys()])
    style_loss *= style_weight / num_style_layers

    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2)
                             for name in content_outputs.keys()])
    content_loss *= content_weight / num_content_layers
    loss = style_loss + content_loss
    return loss

```

这段代码是不言自明的，因为我们已经讨论过它的各个部分。这个函数期望我们使用`StyleContentModel`类获得的字典作为输入。

现在让我们创建一个更新权重的函数:

```py
@tf.function()
def train_step(image):
  with tf.GradientTape() as tape:
    outputs = extractor(image)
    loss = style_content_loss(outputs)

  grad = tape.gradient(loss, image)
  opt.apply_gradients([(grad, image)])
  image.assign(clip_0_1(image))

```

我们使用`tf.GradientTape`来更新图像。注意，当你用`@tf.function`注释一个函数时，你仍然可以像调用其他函数一样调用它。但是会被编译成图，这意味着你获得了更快执行的好处，在 GPU 或者 TPU 上运行，或者导出到 SavedModel(参见 [`https://www.tensorflow.org/alpha/guide/autograph`](https://www.tensorflow.org/alpha/guide/autograph) )。请记住，变量`extractor`是通过以下代码获得的:

```py
extractor = StyleContentModel(style_layers, content_layers)

```

并且是具有不同层的输出的字典。

现在，这段代码在开始时理解起来相当高级和复杂，所以不要着急，同时打开 Jupyter 笔记本阅读页面，以便能够理解代码和解释。如果一开始你不明白所有的事情，不要气馁。该行:

```py
grad = tape.gradient(loss, image)

```

将计算损失函数相对于我们已经定义的变量`image`的梯度。每个更新步骤都可以通过一行简单的代码来完成:

```py
train_step(image)

```

现在我们可以轻松地进行最后一个循环了:

```py
epochs = 20
steps_per_epoch = 100

step = 0
for n in range(epochs):
  for m in range(steps_per_epoch):
    step += 1
    train_step(image)
    print(".", end=")
  display.clear_output(wait=True)
  imshow(image.read_value())
  plt.title("Train step: {}".format(step))
  plt.show()

```

当它运行时，你会看到图像每一个时代都在变化，你可以见证它是如何变化的。

### 有剪影的 NST

你可以用 NST 做一个有趣的应用，它与剪影 <sup>[12](#Fn12)</sup> 有关。一个*剪影*是一个用单一颜色的固体形状表示的图像。在图 [5-3](#Fig3) 中，可以看到一个例子；如果你是*星球大战的粉丝，*你知道是谁(提示:达斯维达 <sup>[13](#Fn13)</sup> )。

![img/470317_1_En_5_Fig3_HTML.jpg](img/470317_1_En_5_Fig3_HTML.jpg)

图 5-3

《星球大战》角色达斯·维德的剪影

你应该在互联网上搜索类似马赛克或彩色玻璃的图像，如图 [5-4](#Fig4) 所示。

![img/470317_1_En_5_Fig4_HTML.jpg](img/470317_1_En_5_Fig4_HTML.jpg)

图 5-4

马赛克般的图像

目标是获得如图 [5-5](#Fig5) 所示的图像。

![img/470317_1_En_5_Fig5_HTML.jpg](img/470317_1_En_5_Fig5_HTML.jpg)

图 5-5

NST 在应用蒙版后在剪影上完成(稍后会详细介绍)

### 掩饰

屏蔽有几种含义，取决于您使用它的领域。这里我指的*蒙版*是根据剪影把图像的部分变成绝对白色的过程。这个想法在图 [5-6](#Fig6) 中进行了图示。你可以这样想:你在你的图像上放一个剪影(它们应该有相同的分辨率)，只保留剪影是黑色的部分。

![img/470317_1_En_5_Fig6_HTML.jpg](img/470317_1_En_5_Fig6_HTML.jpg)

图 5-6

应用于图 [5-4](#Fig4) 中镶嵌图像的遮蔽

这没问题，但是有点不满意，因为例如你在结果中没有边。马赛克形状简单地从中间切开。视觉上这不是很令人满意。但是我们可以使用 NST 来使最终图像更好。该过程如下:

*   您使用类似马赛克的图像作为样式图像。

*   您使用您的轮廓图像作为内容图像。

*   最后，使用剪影图像将遮罩应用到最终结果中。

你可以在图 [5-5](#Fig5) 中看到结果(使用相同的代码)。你可以看到你得到了很好的边缘，马赛克瓷砖没有被切成两半。

你可以在本书第五章[的 GitHub 知识库中找到完整的代码。但是作为参考，让我们假设您将图像保存为 numpy 数组。让我们假设剪影保存在一个名为`mask`的数组中，而你的图像保存在一个名为`result`的数组中。假设(您应该检查一下)掩码数组将只包含 0 或 255 个值(黑和白)。然后简单地用这个做屏蔽:](5.html)

```py
result[mask] = 255

```

这只是使白色的结果图像中有白色的轮廓，其余的保持不变。

<aside class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

[T2`https://en.wikipedia.org/wiki/Loss_function`](https://en.wikipedia.org/wiki/Loss_function)

  [2](#Fn2_source)

这个例子在 Michelucci，Umberto，2018 中有详细讨论。*应用深度学习:基于案例的理解深度神经网络的方法*。1.奥弗拉格。纽约:新闻。国际标准书号 978-1-4842-3789-2。可从: [`https://doi.org/10.1007/978-1-4842-3790-8`](https://doi.org/10.1007/978-1-4842-3790-8)

  [3](#Fn3_source)

记住维度的顺序取决于你如何构建你的网络，你可能需要改变它。此处的尺寸仅用于说明目的。

  [4](#Fn4_source)

[T2`https://en.wikipedia.org/wiki/Taylor_series`](https://en.wikipedia.org/wiki/Taylor_series)

  [5](#Fn5_source)

[`https://en.m.wikipedia.org/wiki/Skewness`](https://en.m.wikipedia.org/wiki/Skewness) `.`在*E*[*δY*= 0 的情况下。

  [6](#Fn6_source)

[`https://en.m.wikipedia.org/wiki/Kurtosis`](https://en.m.wikipedia.org/wiki/Kurtosis) 。在*E*[*δY*]= 0 的情况下。

  [7](#Fn7_source)

在[概率](https://en.wikipedia.org/wiki/Probability_theory)和[统计](https://en.wikipedia.org/wiki/Statistics)中，概率质量函数(PMF)是给出[离散随机变量](https://en.wikipedia.org/wiki/Discrete_random_variable)恰好等于某个值的概率的函数【Stewart，William J. (2011)。 [*概率、马尔可夫链、队列、模拟:性能建模的数学基础*](https://books.google.com/books%253Fid%253DZfRyBS1WbAQC%2526pg%253DPT105) *。*普林斯顿大学出版社。第 105 页。[ISBN](https://en.wikipedia.org/wiki/International_Standard_Book_Number)[978-1-4008-3281-1](https://en.wikipedia.org/wiki/Special:BookSources/978%252D1%252D4008%252D3281%252D1)。]

  [8](#Fn8_source)

[T2`https://en.wikipedia.org/wiki/Claude_Shannon`](https://en.wikipedia.org/wiki/Claude_Shannon)

  [9](#Fn9_source)

[T2`https://en.wikipedia.org/wiki/Neural_Style_Transfer`](https://en.wikipedia.org/wiki/Neural_Style_Transfer)

  [10](#Fn10_source)

莱昂·加蒂斯；亚历山大·埃克；马蒂亚斯·贝斯吉(2015 年 8 月 26 日)。《艺术风格的神经算法》。[T2`https://arxiv.org/abs/1508.06576`](https://arxiv.org/abs/1508.06576)

  [11](#Fn11_source)

“用于大规模视觉识别的非常深的 CNN”。`Robots.ox.ac.uk`。2014.检索到 2019 年 2 月 13 日， [`http://www.robots.ox.ac.uk/~vgg/research/very_deep/`](http://www.robots.ox.ac.uk/%257Evgg/research/very_deep/)

  [12](#Fn12_source)

本章的这一部分受到了中帖 [`https://becominghuman.ai/creating-intricate-art-with-neural-style-transfer-e5fee5f89481`](https://becominghuman.ai/creating-intricate-art-with-neural-style-transfer-e5fee5f89481) 的启发。

  [13](#Fn13_source)

[T2`https://en.wikipedia.org/wiki/Darth_Vader`](https://en.wikipedia.org/wiki/Darth_Vader)

  [14](#Fn14_source)

请注意，本章中使用的所有图像都是无版权和免费使用的图像。如果你在你的论文或作品中使用图像，确保你可以自由使用它们，否则你需要支付版税。

 </aside>****