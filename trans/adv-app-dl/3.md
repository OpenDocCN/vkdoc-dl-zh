# 3.卷积神经网络基础

在这一章，我们将看看卷积神经网络(CNN)的主要组成部分:内核和池层。然后我们将看看典型的网络是什么样子的。然后，我们将尝试用一个简单的卷积网络解决一个分类问题，并尝试将卷积运算可视化。这样做的目的是试图理解，至少是直观地理解，学习是如何进行的。

## 内核和过滤器

CNN 的主要组件之一是滤波器，它是具有维度*n*<sub>T3】KT5】×*n*<sub>*K*</sub>的方阵，其中 *n* <sub>*K*</sub> 是整数，并且通常是小数字，如 3 或 5。有时过滤器也被称为内核。使用内核来自经典的图像处理技术。如果你用过 Photoshop 或者类似的软件，你就习惯了做锐化、模糊、浮雕之类的操作。 <sup>[1](#Fn1)</sup> 所有这些操作都是用内核来完成的。在这一节我们将会看到内核到底是什么以及它们是如何工作的。请注意，在本书中，我们将互换使用这两个术语(内核和过滤器)。让我们定义四种不同的滤波器，并在本章后面检查它们在卷积运算中的效果。对于这些示例，我们将使用 3 × 3 滤波器。目前，只是把下面的定义作为参考，我们将在本章的后面看到如何使用它们。</sub>

*   The following kernel will allow the detection of horizontal edges

    ![$$ {\mathfrak{I}}_H=\left(\begin{array}{ccc}1&amp; 1&amp; 1\\ {}0&amp; 0&amp; 0\\ {}-1&amp; -1&amp; -1\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equa.png)

*   The following kernel will allow the detection of vertical edges

    ![$$ {\mathfrak{I}}_V=\left(\begin{array}{ccc}1&amp; 0&amp; -1\\ {}1&amp; 0&amp; -1\\ {}1&amp; 0&amp; -1\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equb.png)

*   The following kernel will allow the detection of edges when luminosity changes drastically

    ![$$ {\mathfrak{I}}_L=\left(\begin{array}{ccc}-1&amp; -1&amp; -1\\ {}-1&amp; 8&amp; -1\\ {}-1&amp; -1&amp; -1\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equc.png)

*   The following kernel will blur edges in an image

    ![$$ {\mathfrak{I}}_B=-\frac{1}{9}\left(\begin{array}{ccc}1&amp; 1&amp; 1\\ {}1&amp; 1&amp; 1\\ {}1&amp; 1&amp; 1\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equd.png)

在接下来的章节中，我们将使用滤镜对测试图像进行卷积，看看它们的效果如何。

## 盘旋

理解 CNN 的第一步是理解卷积。最简单的方法是通过几个简单的案例来看它的实际应用。首先，在神经网络的环境中，卷积是在张量之间进行的。该操作得到两个张量作为输入，并产生一个张量作为输出。操作通常用操作符*表示。

让我们看看它是如何工作的。考虑两个张量，维数都是 3 × 3。卷积运算通过应用以下公式来完成:

![$$ \left(\begin{array}{ccc}{a}_1&amp; {a}_2&amp; {a}_3\\ {}{a}_4&amp; {a}_5&amp; {a}_6\\ {}{a}_7&amp; {a}_8&amp; {a}_9\end{array}\right)\ast \left(\begin{array}{ccc}{k}_1&amp; {k}_2&amp; {k}_3\\ {}{k}_4&amp; {k}_5&amp; {k}_6\\ {}{k}_7&amp; {k}_8&amp; {k}_9\end{array}\right)=\sum \limits_{i=1}^9{a}_i{k}_i $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Eque.png)

在这种情况下，结果仅仅是每个元素的总和， *a* <sub>*i*</sub> ，乘以各自的元素， *k* <sub>*i*</sub> 。在更典型的矩阵形式中，这个公式可以用一个双和写成

![$$ \left(\begin{array}{ccc}{a}_{11}&amp; {a}_{12}&amp; {a}_{13}\\ {}{a}_{21}&amp; {a}_{22}&amp; {a}_{23}\\ {}{a}_{31}&amp; {a}_{32}&amp; {a}_{33}\end{array}\right)\ast \left(\begin{array}{ccc}{k}_{11}&amp; {k}_{12}&amp; {k}_{13}\\ {}{k}_{21}&amp; {k}_{22}&amp; {k}_{23}\\ {}{k}_{31}&amp; {k}_{32}&amp; {k}_{33}\end{array}\right)=\sum \limits_{i=1}^3\sum \limits_{j=1}^3{a}_{ij}{k}_{ij} $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equf.png)

然而，第一个版本的优点是使基本思想非常清楚:来自一个张量的每个元素乘以第二个张量的对应元素(相同位置的元素)，然后将所有值求和以获得结果。

在上一节中，我们谈到了核，原因是卷积通常是在张量和核之间进行的，我们可以在这里用 *A* 表示。典型地，核很小，3 × 3 或 5 × 5，而输入张量 *A* 通常更大。例如，在图像识别中，输入张量 *A* 是尺寸可能高达 1024 × 1024 × 3 的图像，其中 1024 × 1024 是分辨率，最后一个尺寸(3)是颜色通道的数量，即 RGB 值。

在高级应用中，图像甚至可能具有更高的分辨率。为了理解当我们有不同维数的矩阵时如何应用卷积，让我们考虑一个 4 × 4 的矩阵 *A*

![$$ A=\left(\begin{array}{cccc}{a}_1&amp; {a}_2&amp; {a}_3&amp; {a}_4\\ {}{a}_5&amp; {a}_6&amp; {a}_7&amp; {a}_8\\ {}{a}_9&amp; {a}_{10}&amp; {a}_{11}&amp; {a}_{12}\\ {}{a}_{13}&amp; {a}_{14}&amp; {a}_{15}&amp; {a}_{16}\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equg.png)

在这个例子中，我们将取核为 3 × 3

![$$ K=\left(\begin{array}{ccc}{k}_1&amp; {k}_2&amp; {k}_3\\ {}{k}_4&amp; {k}_5&amp; {k}_6\\ {}{k}_7&amp; {k}_8&amp; {k}_9\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equh.png)

想法是从矩阵的左上角 *A* 开始，选择一个 3 × 3 的区域。在这个例子中

![$$ {A}_1=\left(\begin{array}{ccc}{a}_1&amp; {a}_2&amp; {a}_3\\ {}{a}_5&amp; {a}_6&amp; {a}_7\\ {}{a}_9&amp; {a}_{10}&amp; {a}_{11}\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equi.png)

或者，这里用粗体标记的元素:

![$$ A=\left(\begin{array}{cccc}{\boldsymbol{a}}_{\mathbf{1}}&amp; {\boldsymbol{a}}_{\mathbf{2}}&amp; {\boldsymbol{a}}_{\mathbf{3}}&amp; {a}_4\\ {}{\boldsymbol{a}}_{\mathbf{5}}&amp; {\boldsymbol{a}}_{\mathbf{6}}&amp; {\boldsymbol{a}}_{\mathbf{7}}&amp; {a}_8\\ {}{\boldsymbol{a}}_{\mathbf{9}}&amp; {\boldsymbol{a}}_{\mathbf{1}\mathbf{0}}&amp; {\boldsymbol{a}}_{\mathbf{1}\mathbf{1}}&amp; {a}_{12}\\ {}{a}_{13}&amp; {a}_{14}&amp; {a}_{15}&amp; {a}_{16}\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equj.png)

然后，我们执行卷积，如开头所解释的，在这个更小的矩阵 *A* <sub>1</sub> 和 *K* 之间，得到(我们将用 *B* <sub>1</sub> 表示结果):

![$$ {B}_1={A}_1\ast K={a}_1{k}_1+{a}_2{k}_2+{a}_3{k}_3+{k}_4{a}_5+{k}_5{a}_5+{k}_6{a}_7+{k}_7{a}_9+{k}_8{a}_{10}+{k}_9{a}_{11} $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equk.png)

然后我们需要将一列的矩阵 *A* 中所选的 3 × 3 区域向右移动，并选择这里用粗体标记的元素:

![$$ A=\left(\begin{array}{cccc}{a}_1&amp; {\boldsymbol{a}}_{\mathbf{2}}&amp; {\boldsymbol{a}}_{\mathbf{3}}&amp; {\boldsymbol{a}}_{\mathbf{4}}\\ {}{a}_5&amp; {\boldsymbol{a}}_{\mathbf{6}}&amp; {\boldsymbol{a}}_{\mathbf{7}}&amp; {\boldsymbol{a}}_{\mathbf{8}}\\ {}{a}_9&amp; {\boldsymbol{a}}_{\mathbf{10}}&amp; {\boldsymbol{a}}_{\mathbf{11}}&amp; {\boldsymbol{a}}_{\mathbf{12}}\\ {}{a}_{13}&amp; {a}_{14}&amp; {a}_{15}&amp; {a}_{16}\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equl.png)

这将给我们第二子矩阵 *A* <sub>2</sub> :

![$$ {A}_2=\left(\begin{array}{ccc}{a}_2&amp; {a}_3&amp; {a}_4\\ {}{a}_6&amp; {a}_7&amp; {a}_8\\ {}{a}_{10}&amp; {a}_{11}&amp; {a}_{12}\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equm.png)

然后，我们再次执行这个更小的矩阵 *A* <sub>2</sub> 和 *K* 之间的卷积:

![$$ {B}_2={A}_2\ast K={a}_2{k}_1+{a}_3{k}_2+{a}_4{k}_3+{a}_6{k}_4+{a}_7{k}_5+{a}_8{k}_6+{a}_{10}{k}_7+{a}_{11}{k}_8+{a}_{12}{k}_9 $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equn.png)

我们不能再向右移动我们的 3 × 3 区域，因为我们已经到达了矩阵 *A* 的末尾，所以我们要做的是将它向下移动一行，并从左侧重新开始。下一个选择的区域将是

![$$ {A}_3=\left(\begin{array}{ccc}{a}_5&amp; {a}_6&amp; {a}_7\\ {}{a}_9&amp; {a}_{10}&amp; {a}_{11}\\ {}{a}_{13}&amp; {a}_{14}&amp; {a}_{15}\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equo.png)

同样，我们执行*A*T2 3 与 *K* 的卷积

![$$ {B}_3={A}_3\ast K={a}_5{k}_1+{a}_6{k}_2+{a}_7{k}_3+{a}_9{k}_4+{a}_{10}{k}_5+{a}_{11}{k}_6+{a}_{13}{k}_7+{a}_{14}{k}_8+{a}_{15}{k}_9 $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equp.png)

您可能已经猜到了这一点，最后一步是将我们的 3 × 3 选定区域向右移动一列，并再次执行卷积。我们选择的区域现在将是

![$$ {A}_4=\left(\begin{array}{ccc}{a}_6&amp; {a}_7&amp; {a}_8\\ {}{a}_{10}&amp; {a}_{11}&amp; {a}_{12}\\ {}{a}_{14}&amp; {a}_{15}&amp; {a}_{16}\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equq.png)

此外，卷积将给出以下结果:

![$$ {B}_4={A}_4\ast K={a}_6{k}_1+{a}_7{k}_2+{a}_8{k}_3+{a}_{10}{k}_4+{a}_{11}{k}_5+{a}_{12}{k}_6+{a}_{14}{k}_7+{a}_{15}{k}_8+{a}_{16}{k}_9 $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equr.png)

现在我们不能再移动我们的 3 × 3 区域了，无论是向右还是向下。我们计算了四个值: *B* <sub>1</sub> 、 *B* <sub>2</sub> 、 *B* <sub>3</sub> 、 *B* <sub>4</sub> 。这些元素将形成卷积运算的结果张量，给出张量 *B* :

![$$ B=\left(\begin{array}{cc}{B}_1&amp; {B}_2\\ {}{B}_3&amp; {B}_4\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equs.png)

当张量 *A* 较大时，可以应用相同的过程。你将简单地得到一个更大的结果 *B* 张量，但是得到元素 *B* <sub>*i*</sub> 的算法是相同的。在继续之前，我们还有一个小细节需要讨论，那就是 stride 的概念。在前面的过程中，我们总是将 3 × 3 区域向右移动一列，向下移动一行。在本例 1 中，行数和列数称为*步距*，通常用 *s* 表示。Stride *s* = 2 仅仅意味着我们在每一步将我们的 3 × 3 区域向右移动两列，向下移动两行。

我们需要讨论的另一件事是输入矩阵 *A* 中选定区域的大小。在此过程中，我们移动的选定区域的尺寸必须与所使用的内核的尺寸相同。如果你使用 5 × 5 的内核，你需要在 *A* 中选择一个 5 × 5 的区域。一般来说，给定一个*n*<sub>*K*</sub>×*n*<sub>*K*</sub>内核，你在 *A* 中选择一个*n*<sub>*K*×*n*<sub>*K*</sub>区域。</sub>

在更正式的定义中，在神经网络上下文中，与步幅 *s* 的卷积是这样一个过程，它取一个张量 *A* 的维数*n*<sub>*A*</sub>×*n*<sub>*A*</sub>和一个核*K*n*<sub>*K*</sub>×*n**

![$$ {n}_B=\left\lfloor \frac{n_A-{n}_K}{s}+1\right\rfloor $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equt.png)

这里我们用⌊ *x* ⌋表示 *x* 的整数部分(在编程界，这通常被称为 *x* 的底)。这个公式的证明需要花很长时间来讨论，但是很容易看出为什么它是正确的(试着推导它)。为了简单一点，假设 *n* <sub>*K*</sub> 是奇数。你很快就会明白为什么这很重要(虽然不是基本的)。让我们开始正式解释这个情况，步长为 1。该算法根据以下公式从输入张量 *A* 和核 *K* 生成新的张量 *B*

![$$ {B}_{ij}={\left(A\ast K\right)}_{ij}=\sum \limits_{f=0}^{n_K-1}\kern1em \sum \limits_{h=0}^{n_K-1}{A}_{i+f,j+h}{K}_{i+f,j+h} $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equu.png)

这个公式晦涩难懂。让我们再研究一些例子，以便更好地理解意思。在图 [3-1](#Fig1) 中，你可以看到卷积如何工作的直观解释。假设有一个 3 × 3 的滤镜。那么在图 [3-1](#Fig1) 中，你可以看到矩阵 *A* 的左上九个元素，用黑色实线画出的正方形标记，就是根据这个公式用来生成矩阵*B*T8】1 的第一个元素。用虚线画的正方形标记的元素是用于生成第二个元素*B*2 的元素，以此类推。

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig1_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig1_HTML.jpg)

图 3-1

卷积的直观解释

为了重申我们在开始的例子中讨论的内容，基本思想是将矩阵 *A* 的 3 × 3 平方的每个元素乘以核 *K* 的相应元素，并将所有数字求和。这个和就是新矩阵 *B* 的元素。计算出*B*T8】1 的值后，将原始矩阵中一列的区域向右移动(图 [3-1](#Fig1) 中用虚线表示的方块)并重复操作。您继续向右移动区域，直到到达边界，然后向下移动一个元素，并从左侧重新开始。你继续以这种方式，直到矩阵的右下角。相同的内核用于原始矩阵中的所有区域。

以内核![$$ {\mathfrak{I}}_H $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq1.png)为例，你可以在图 [3-2](#Fig2) 中看到 *A* 的哪些元素乘以![$$ {\mathfrak{I}}_H $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq2.png)中的哪些元素，元素*B*T8】1 的结果就是所有乘法的总和

![$$ {B}_{11}=1\times 1+2\times 1+3\times 1+1\times 0+2\times 0+3\times 0+4\times \left(-1\right)+3\times \left(-1\right)+2\times \left(-1\right)=-3 $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equv.png)

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig2_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig2_HTML.jpg)

图 3-2

与内核卷积的可视化![$$ {\mathfrak{I}}_H $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq3.png)

在图 [3-3](#Fig3) 中，可以看到步长 *s* = 2 的卷积示例。

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig3_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig3_HTML.jpg)

图 3-3

步幅为 *s* = 2 的卷积的直观解释

输出矩阵的维数只占的底(整数部分)

![$$ \frac{n_A-{n}_K}{s}+1 $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equw.png)

在图 [3-4](#Fig4) 中可以直观的看到。如果 *s* > 1，根据 *A* 的尺寸，可能发生的情况是，在某一点上你不能再在矩阵 *A* (例如你在图 [3-3](#Fig3) 中看到的黑色方块)上移动你的窗口，并且你不能完全覆盖矩阵 *A* 的全部。在图 [3-4](#Fig4) 中，您可以看到如何在矩阵 *A* (标有许多 X)的右侧需要一个额外的列来执行卷积运算。在图 [3-4](#Fig4) 中，我们选择了 *s* = 3，由于我们有*n*<sub>*A*</sub>= 5 和*n*<sub>*K*</sub>= 3，因此 *B* 将是一个标量。

![$$ {n}_B=\left\lfloor \frac{n_A-{n}_K}{s}+1\right\rfloor =\left\lfloor \frac{5-3}{3}+1\right\rfloor =\left\lfloor \frac{5}{3}\right\rfloor =1 $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equx.png)

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig4_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig4_HTML.jpg)

图 3-4

直观解释为什么在评估生成的矩阵 *B* 尺寸时需要 floor 函数

从图 [3-4](#Fig4) 中你可以很容易地看到，一个 3 × 3 的区域，只能覆盖 *A* 的左上区域，由于步长 *s* = 3，你会在 *A* 之外结束，因此可以只考虑一个区域进行卷积运算。因此，你最终得到了一个标量张量 *B* 。

现在让我们看几个额外的例子，让这个公式更加清晰。先说一个 3 × 3 的小矩阵

![$$ A=\left(\begin{array}{ccc}1&amp; 2&amp; 3\\ {}4&amp; 5&amp; 6\\ {}7&amp; 8&amp; 9\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equy.png)

此外，让我们考虑内核

![$$ K=\left(\begin{array}{ccc}{k}_1&amp; {k}_2&amp; {k}_3\\ {}{k}_4&amp; {k}_5&amp; {k}_6\\ {}{k}_7&amp; {k}_8&amp; {k}_9\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equz.png)

步幅 *s* = 1。卷积将由下式给出

![$$ B=A\ast K=1\cdotp {k}_1+2\cdotp {k}_2+3\cdotp {k}_3+4\cdotp {k}_4+5\cdotp {k}_5+6\cdotp {k}_6+7\cdotp {k}_7+8\cdotp {k}_8+9\cdotp {k}_9 $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equaa.png)

而且，结果 *B* 会是一个标量，因为*n*<sub>T5】A</sub>= 3，*n*<sub>*K*</sub>= 3。

![$$ {n}_B=\left\lfloor \frac{n_A-{n}_K}{s}+1\right\rfloor =\left\lfloor \frac{3-3}{1}+1\right\rfloor =1 $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equab.png)

如果你考虑一个维数为 4 × 4 的矩阵 *A* ，或者*n*<sub>T5】A</sub>= 4，*n*<sub>*K*</sub>= 3， *s* = 1，你将得到维数为 2 × 2 的矩阵 *B* ，因为

![$$ {n}_B=\left\lfloor \frac{n_A-{n}_K}{s}+1\right\rfloor =\left\lfloor \frac{4-3}{1}+1\right\rfloor =2 $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equac.png)

例如，您可以验证给定的

![$$ A=\left(\begin{array}{cccc}1&amp; 2&amp; 3&amp; 4\\ {}5&amp; 6&amp; 7&amp; 8\\ {}9&amp; 10&amp; 11&amp; 12\\ {}13&amp; 14&amp; 15&amp; 16\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equad.png)

和

![$$ K=\left(\begin{array}{ccc}1&amp; 2&amp; 3\\ {}4&amp; 5&amp; 6\\ {}7&amp; 8&amp; 9\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equae.png)

我们有步距为*s*= 1

![$$ B=A\ast K=\left(\begin{array}{cc}348&amp; 393\\ {}528&amp; 573\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equaf.png)

我们用我给你的公式来验证其中一个元素: *B* <sub>11</sub> 。我们有

![$$ {\displaystyle \begin{array}{l}{B}_{11}=\sum \limits_{f=0}^2\kern1em \sum \limits_{h=0}^2{A}_{1+f,1+h}{K}_{1+f,1+h}=\sum \limits_{f=0}^2\left({\mathrm{A}}_{1+\mathrm{f},\kern0.5em 1}{K}_{1+f,1}+{\mathrm{A}}_{1+\mathrm{f},\kern0.5em 2}{K}_{1+f,2}+{\mathrm{A}}_{1+\mathrm{f},\kern0.5em 3}{K}_{1+f,3}\right)=\\ {}\kern3em \left({\mathrm{A}}_{1,\kern0.5em 1}{K}_{1,1}+{\mathrm{A}}_{1,\kern0.5em 2}{K}_{1,2}+{\mathrm{A}}_{1,\kern0.5em 3}{K}_{1,3}\right)+\left({\mathrm{A}}_{2,\kern0.5em 1}{K}_{2,1}+{\mathrm{A}}_{2,\kern0.5em 2}{K}_{2,2}+{\mathrm{A}}_{2,\kern0.5em 3}{K}_{2,3}\right)+\\ {}\kern3em \left({\mathrm{A}}_{3,\kern0.5em 1}{K}_{3,1}+{\mathrm{A}}_{3,\kern0.5em 2}{K}_{3,2}+{\mathrm{A}}_{3,\kern0.5em 3}{K}_{3,3}\right)=\left(1\cdotp 1+2\cdotp 2+3\cdotp 3\right)+\left(5\cdotp 4+6\cdotp 5+7\cdotp 6\right)+\\ {}\kern3em \left(9\cdotp 7+10\cdotp 8+11\cdotp 9\right)=14+92+242=348\end{array}} $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equag.png)

请注意，我给你的卷积公式仅适用于步长 *s* = 1，但可以很容易地推广到其他值的 *s* 。

这个计算很容易用 Python 实现。对于 *s* = 1，下面的函数可以足够容易地计算两个矩阵的卷积(您可以在 Python 中使用现有的函数来完成，但我认为从头开始看如何做是有启发性的):

```
import numpy as np
def conv_2d(A, kernel):
    output = np.zeros([A.shape[0]-(kernel.shape[0]-1), A.shape[1]-(kernel.shape[0]-1)])

    for row in range(1,A.shape[0]-1):
        for column in range(1, A.shape[1]-1):
            output[row-1, column-1] = np.tensordot(A[row-1:row+2, column-1:column+2], kernel)

    return output

```

注意，输入矩阵 *A* 甚至不需要是平方矩阵，但是假设内核是并且它的维数*n*<sub>T5】K</sub>是奇数。可以用下面的代码评估前面的示例:

```
A = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])
K = np.array([[1,2,3],[4,5,6],[7,8,9]])
print(conv_2d(A,K))

```

这给出了结果:

```
[[ 348\. 393.]
[ 528\. 573.]]

```

## 卷积的例子

现在，让我们尝试将我们在开始时定义的内核应用到一个测试图像中，看看结果。作为测试图像，让我们用代码创建一个尺寸为 160 × 160 像素的棋盘:

```
chessboard = np.zeros([8*20, 8*20])
for row in range(0, 8):
    for column in range (0, 8):
        if ((column+8*row) % 2 == 1) and (row % 2 == 0):
            chessboard[row*20:row*20+20, column*20:column*20+20] = 1
        elif ((column+8*row) % 2 == 0) and (row % 2 == 1):
            chessboard[row*20:row*20+20, column*20:column*20+20] = 1

```

在图 [3-5](#Fig5) 中，可以看到棋盘的样子。

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig5_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig5_HTML.jpg)

图 3-5

用代码生成的棋盘图像

现在让我们用步长为 *s* = 1 的不同内核对该图像进行卷积。

使用内核，![$$ {\mathfrak{I}}_H $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq4.png)将检测水平边缘。这可以应用于代码

```
edgeh = np.matrix('1 1 1; 0 0 0; -1 -1 -1')
outputh = conv_2d (chessboard, edgeh)

```

在图 [3-6](#Fig6) 中，您可以看到输出的样子。使用以下代码可以很容易地生成图像:

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig6_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig6_HTML.jpg)

图 3-6

在内核![$$ {\mathfrak{I}}_H $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq5.png)和棋盘图像之间执行卷积的结果

```
Import matplotlib.pyplot as plt
plt.imshow(outputh)

```

现在你可以理解为什么这个内核检测水平边缘了。此外，这个内核检测你什么时候从亮到暗，反之亦然。注意，正如所料，这张图片只有 158 × 158 像素，因为

![$$ {n}_B=\left\lfloor \frac{n_A-{n}_K}{s}+1\right\rfloor =\left\lfloor \frac{160-3}{1}+1\right\rfloor =\left\lfloor \frac{157}{1}+1\right\rfloor =\left\lfloor 158\right\rfloor =158 $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equah.png)

现在让我们使用这段代码来应用![$$ {\mathfrak{I}}_V $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq6.png):

```
edgev = np.matrix('1 0 -1; 1 0 -1; 1 0 -1')
outputv = conv_2d (chessboard, edgev)

```

这给出了如图 [3-7](#Fig7) 所示的结果。

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig7_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig7_HTML.jpg)

图 3-7

在内核![$$ {\mathfrak{I}}_V $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq7.png)和棋盘图像之间执行卷积的结果

现在我们可以使用内核![$$ {\mathfrak{I}}_L $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq8.png):

```
edgel = np.matrix ('-1 -1 -1; -1 8 -1; -1 -1 -1')
outputl = conv_2d (chessboard, edgel)

```

这给出了如图 [3-8](#Fig8) 所示的结果。

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig8_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig8_HTML.jpg)

图 3-8

在内核![$$ {\mathfrak{I}}_L $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq9.png)和棋盘图像之间执行卷积的结果

此外，我们可以应用模糊内核![$$ {\mathfrak{I}}_B $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq10.png):

```
edge_blur = -1.0/9.0*np.matrix('1 1 1; 1 1 1; 1 1 1')
output_blur = conv_2d (chessboard, edge_blur)

```

在图 [3-9](#Fig9) 中，你可以看到两幅图——左边是模糊图像，右边是原始图像。这些图像只显示了原始棋盘的一小部分区域，以使模糊更加清晰。

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig9_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig9_HTML.jpg)

图 3-9

模糊内核![$$ {\mathfrak{I}}_B $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq11.png)的效果左边是模糊图像，右边是原始图像。

为了结束这一部分，让我们试着更好地理解如何检测边缘。考虑具有急剧垂直过渡的以下矩阵，因为左边部分全是 10，右边部分全是 0。

```
ex_mat = np.matrix('10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0')

```

这看起来像这样

```
matrix([[10, 10, 10, 10, 0, 0, 0, 0],
        [10, 10, 10, 10, 0, 0, 0, 0],
        [10, 10, 10, 10, 0, 0, 0, 0],
        [10, 10, 10, 10, 0, 0, 0, 0],
        [10, 10, 10, 10, 0, 0, 0, 0],
        [10, 10, 10, 10, 0, 0, 0, 0],
        [10, 10, 10, 10, 0, 0, 0, 0],
        [10, 10, 10, 10, 0, 0, 0, 0]])

```

我们来考虑一下内核![$$ {\mathfrak{I}}_V $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq12.png)。我们可以用这段代码执行卷积:

```
ex_out = conv_2d (ex_mat, edgev)

```

结果如下:

```
array([[ 0., 0., 30., 30., 0., 0.],
       [ 0., 0., 30., 30., 0., 0.],
       [ 0., 0., 30., 30., 0., 0.],
       [ 0., 0., 30., 30., 0., 0.],
       [ 0., 0., 30., 30., 0., 0.],
       [ 0., 0., 30., 30., 0., 0.]])

```

在图 [3-10](#Fig10) 中，可以看到原始矩阵(左边)和右边卷积的输出。与内核![$$ {\mathfrak{I}}_V $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq13.png)的卷积已经清楚地检测到原始矩阵中的急剧转变，在从黑到白的转变发生的地方用垂直黑线标记。例如，考虑*B*T5】11= 0

![$$ {\displaystyle \begin{array}{l}{B}_{11}=\left(\begin{array}{ccc}10&amp; 10&amp; 10\\ {}10&amp; 10&amp; 10\\ {}10&amp; 10&amp; 10\end{array}\right)\ast {\mathfrak{I}}_V=\left(\begin{array}{ccc}10&amp; 10&amp; 10\\ {}10&amp; 10&amp; 10\\ {}10&amp; 10&amp; 10\end{array}\right)\ast \left(\begin{array}{ccc}1&amp; 0&amp; -1\\ {}1&amp; 0&amp; -1\\ {}1&amp; 0&amp; -1\end{array}\right)\\ {}=10\times 1+10\times 0+10\times -1+10\times 1+10\times 0+10\times -1+10\times 1+10\times 0+10\times -1=0\end{array}} $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equai.png)

注意，在输入矩阵中

![$$ \left(\begin{array}{ccc}10&amp; 10&amp; 10\\ {}10&amp; 10&amp; 10\\ {}10&amp; 10&amp; 10\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equaj.png)

没有过渡，因为所有值都是相同的。相反，如果你考虑*B*T2】13 你需要考虑输入矩阵的这个区域

![$$ \left(\begin{array}{ccc}10&amp; 10&amp; 0\\ {}10&amp; 10&amp; 0\\ {}10&amp; 10&amp; 0\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equak.png)

其中有一个明显的过渡，因为最右列由 0 和其余的 10 组成。你现在得到一个不同的结果

![$$ {\displaystyle \begin{array}{l}{B}_{11}=\left(\begin{array}{ccc}10&amp; 10&amp; 0\\ {}10&amp; 10&amp; 0\\ {}10&amp; 10&amp; 0\end{array}\right)\ast {\mathfrak{I}}_V=\left(\begin{array}{ccc}10&amp; 10&amp; 0\\ {}10&amp; 10&amp; 0\\ {}10&amp; 10&amp; 0\end{array}\right)\ast \left(\begin{array}{ccc}1&amp; 0&amp; -1\\ {}1&amp; 0&amp; -1\\ {}1&amp; 0&amp; -1\end{array}\right)\\ {}=10\times 1+10\times 0+0\times -1+10\times 1+10\times 0+0\times -1+10\times 1+10\times 0+0\times -1=30\end{array}} $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equal.png)

此外，这正是一旦水平方向上的值有显著变化，卷积就返回高值的原因，因为在核中乘以列 1 的值将更有意义。当沿着水平轴存在从小到大的值的转变时，乘以-1 的元素将给出绝对值更大的结果。因此，最终结果将是负的，绝对值很大。这就是为什么这个内核也可以检测到你是否从一个浅色到一个深色，反之亦然。如果您考虑不同假设矩阵*中的相反转变(从 0 到 10)*，您将会

![$$ {\displaystyle \begin{array}{l}{B}_{11}=\left(\begin{array}{ccc}0&amp; 10&amp; 10\\ {}0&amp; 10&amp; 10\\ {}0&amp; 10&amp; 10\end{array}\right)\ast {\mathfrak{I}}_V=\left(\begin{array}{ccc}0&amp; 10&amp; 10\\ {}0&amp; 10&amp; 10\\ {}0&amp; 10&amp; 10\end{array}\right)\ast \left(\begin{array}{ccc}1&amp; 0&amp; -1\\ {}1&amp; 0&amp; -1\\ {}1&amp; 0&amp; -1\end{array}\right)\\ {}=0\times 1+10\times 0+10\times -1+0\times 1+10\times 0+10\times -1+0\times 1+10\times 0+10\times -1=-30\end{array}} $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equam.png)

我们沿着水平方向从 0 移动到 10。

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig10_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig10_HTML.jpg)

图 3-10

如文本中所述，矩阵`ex_mat`与核![$$ {\mathfrak{I}}_V $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq14.png)的卷积结果

请注意，正如所料，输出矩阵的维数是 5 × 5，因为原始矩阵的维数是 7 × 7，而核是 3 × 3。

## 联营

池化是 CNN 的第二个基本操作。这个运算比卷积容易理解得多。为了理解它，让我们看一个具体的例子，并考虑什么叫做 *max pooling。*再次考虑我们在卷积讨论中讨论过的 4 × 4 矩阵:

![$$ A=\left(\begin{array}{cccc}{a}_1&amp; {a}_2&amp; {a}_3&amp; {a}_4\\ {}{a}_5&amp; {a}_6&amp; {a}_7&amp; {a}_8\\ {}{a}_9&amp; {a}_{10}&amp; {a}_{11}&amp; {a}_{12}\\ {}{a}_{13}&amp; {a}_{14}&amp; {a}_{15}&amp; {a}_{16}\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equan.png)

为了执行最大池，我们需要定义一个大小为*n*<sub>*K*</sub>×*n*<sub>*K*</sub>的区域，类似于我们对卷积所做的。我们来考虑一下*n*<sub>*K*</sub>= 2。我们需要做的是从我们的矩阵左上角的 *A* 开始，选择一个*n*<sub>*K*</sub>×*n*<sub>*K*</sub>区域，在我们的例子中是从 *A* 的 2 × 2。在这里，我们将选择

![$$ \left(\begin{array}{cc}{a}_1&amp; {a}_2\\ {}{a}_5&amp; {a}_6\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equao.png)

或者，矩阵中以粗体标记的元素 *A* 在此:

![$$ A=\left(\begin{array}{cccc}{\boldsymbol{a}}_{\mathbf{1}}&amp; {\boldsymbol{a}}_{\mathbf{2}}&amp; {a}_3&amp; {a}_4\\ {}{\boldsymbol{a}}_{\mathbf{5}}&amp; {\boldsymbol{a}}_{\mathbf{6}}&amp; {a}_7&amp; {a}_8\\ {}{a}_9&amp; {a}_{10}&amp; {a}_{11}&amp; {a}_{12}\\ {}{a}_{13}&amp; {a}_{14}&amp; {a}_{15}&amp; {a}_{16}\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equap.png)

从选择的元素中， *a* <sub>1</sub> ， *a* <sub>2</sub> ， *a* <sub>5</sub> 和 *a* <sub>6</sub> ，最大汇集运算选择最大值。结果用*B*T18】1 表示

![$$ {B}_1=\underset{i=1,2,5,6}{\max }{a}_i $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equaq.png)

然后，我们需要将 2 × 2 窗口向右移动两列，通常与所选区域的列数相同，并选择以粗体标记的元素:

![$$ A=\left(\begin{array}{cccc}{a}_1&amp; {a}_2&amp; {\boldsymbol{a}}_{\mathbf{3}}&amp; {\boldsymbol{a}}_{\mathbf{4}}\\ {}{a}_5&amp; {a}_6&amp; {\boldsymbol{a}}_{\mathbf{7}}&amp; {\boldsymbol{a}}_{\mathbf{8}}\\ {}{a}_9&amp; {a}_{10}&amp; {a}_{11}&amp; {a}_{12}\\ {}{a}_{13}&amp; {a}_{14}&amp; {a}_{15}&amp; {a}_{16}\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equar.png)

或者换句话说，更小的矩阵

![$$ \left(\begin{array}{cc}{a}_3&amp; {a}_4\\ {}{a}_7&amp; {a}_8\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equas.png)

然后，最大池算法将选择这些值中的最大值，并给出一个用 *B* <sub>2</sub> 表示的结果

![$$ {B}_2=\underset{i=3,4,7,8}{\max }{a}_i $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equat.png)

此时，我们不能再将 2 × 2 区域向右移动，所以我们将其向下移动两行，并从 *A* 的左侧再次开始该过程，选择以粗体标记的元素并获得最大值，将其命名为 *B* <sub>3</sub> 。

![$$ A=\left(\begin{array}{cccc}{a}_1&amp; {a}_2&amp; {a}_3&amp; {a}_4\\ {}{a}_5&amp; {a}_6&amp; {a}_7&amp; {a}_8\\ {}{\boldsymbol{a}}_{\mathbf{9}}&amp; {\boldsymbol{a}}_{\mathbf{10}}&amp; {a}_{11}&amp; {a}_{12}\\ {}{\boldsymbol{a}}_{\mathbf{13}}&amp; {\boldsymbol{a}}_{\mathbf{14}}&amp; {a}_{15}&amp; {a}_{16}\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equau.png)

在这种情况下，步距 *s* 与我们在卷积中已经讨论过的意义相同。它只是在选择元素时移动区域的行数或列数。最后，我们选择 *A* 底部的最后一个区域 2 × 2，选择元素*A*T6】11、*A*T10】12、*A*T14】15 和 *a* <sub>16</sub> 。然后我们得到最大值，称之为*B*T22】4。利用我们在此过程中获得的值，在本例中是四个值 *B* <sub>1</sub> 、 *B* 、T30】 2 、 *B* 、T34】 3 和 *B* 、 <sub>4</sub> ，我们将构建一个输出张量:

![$$ B=\left(\begin{array}{cc}{B}_1&amp; {B}_2\\ {}{B}_3&amp; {B}_4\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equav.png)

在这个例子中，我们有 *s* = 2。基本上，该操作将矩阵 *A* 、步距 *s* 和内核大小 *n* <sub>*K*</sub> (我们在之前的示例中选择的区域的维度)作为输入，并返回新的矩阵 *B* ，其维度由我们针对卷积讨论的相同公式给出:

![$$ {n}_B=\left\lfloor \frac{n_A-{n}_K}{s}+1\right\rfloor $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equaw.png)

为了重申这个想法，从矩阵 *A* 的左上角开始，取一个维度为*n*<sub>T5】KT7】×*n*<sub>*K*</sub>的区域，对所选元素应用 max 函数，然后向右移动 *s* 元素的区域，再次选择一个维度为 *n* <sub>*K*</sub> 的新区域在图 [3-11](#Fig11) 中，您可以看到如何从步长为 *s* = 2 的矩阵 *A* 中选择元素。</sub>

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig11_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig11_HTML.jpg)

图 3-11

步长为 *s* = 2 的池的可视化

例如，对输入 *A* 应用最大池化

![$$ A=\left(\begin{array}{cccc}1&amp; 3&amp; 5&amp; 7\\ {}4&amp; 5&amp; 11&amp; 3\\ {}4&amp; 1&amp; 21&amp; 6\\ {}13&amp; 15&amp; 1&amp; 2\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equax.png)

会给你这个结果(很容易验证):

![$$ B=\left(\begin{array}{cc}4&amp; 11\\ {}15&amp; 21\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equay.png)

因为四是用粗体标记的值的最大值。

![$$ A=\left(\begin{array}{cccc}\mathbf{1}&amp; \mathbf{3}&amp; 5&amp; 7\\ {}\mathbf{4}&amp; \mathbf{5}&amp; 11&amp; 3\\ {}4&amp; 1&amp; 21&amp; 6\\ {}13&amp; 15&amp; 1&amp; 2\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equaz.png)

11 是这里用粗体标记的最大值:

![$$ A=\left(\begin{array}{cccc}1&amp; 3&amp; \mathbf{5}&amp; \mathbf{7}\\ {}4&amp; 5&amp; \mathbf{11}&amp; \mathbf{3}\\ {}4&amp; 1&amp; 21&amp; 6\\ {}13&amp; 15&amp; 1&amp; 2\end{array}\right) $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equba.png)

诸如此类。值得一提的是另一种池化方法，尽管它没有 max-pooling 使用得那么广泛: ***平均池化*** 。它不是返回所选值的最大值，而是返回平均值。

### 注意

最常用的池操作是*最大池*。平均池的使用并不广泛，但可以在特定的网络架构中找到。

### 填料

这里值得一提的是*填充*。有时，在处理图像时，从维度不同于原始图像的卷积运算中获得结果并不是最佳选择。这时需要填充。这个想法很简单:在最终图像的顶部和底部添加像素行，在右侧和左侧添加像素列，这样得到的矩阵与原始矩阵大小相同。一些策略用零填充添加的像素，用最接近的像素的值填充，等等。例如，在我们的例子中，带有零填充的`ex_out`矩阵如下所示

```
array([[ 0., 0., 0., 0., 0., 0., 0., 0.],
       [ 0., 0., 0., 30., 30., 0., 0., 0.],
       [ 0., 0., 0., 30., 30., 0., 0., 0.],
       [ 0., 0., 0., 30., 30., 0., 0., 0.],
       [ 0., 0., 0., 30., 30., 0., 0., 0.],
       [ 0., 0., 0., 30., 30., 0., 0., 0.],
       [ 0., 0., 0., 30., 30., 0., 0., 0.],
       [ 0., 0., 0., 0., 0., 0., 0., 0.]])

```

仅作为参考，在使用填充符 *p* (用作填充符的行和列的宽度)的情况下，在卷积和合并的情况下，矩阵 *B* 的最终尺寸由下式给出

![$$ {n}_B=\left\lfloor \frac{n_A+2p-{n}_K}{s}+1\right\rfloor $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equbb.png)

### 注意

当处理真实图像时，你总是有彩色图像，用三个通道编码:RGB。这意味着卷积和合并必须在三个维度上完成:宽度、高度和颜色通道。这将增加算法的复杂性。

## CNN 的构建模块

卷积和汇集操作用于构建 CNN 中使用的层。在 CNN 中，您通常可以找到以下层

*   卷积层

*   池层

*   完全连接的层

*全连接层*正是我们在前面所有章节中看到的:一个层，其中的神经元与前一层和后一层的所有神经元相连。你已经认识他们了。另外两个需要一些额外的解释。

### 卷积层

卷积层将张量(由于三个颜色通道，它可以是三维的)作为输入，例如图像，应用特定数量的核，通常是 10、16 或更多，添加偏差，应用 ReLu 激活函数(例如)以将非线性引入卷积的结果，并产生输出矩阵 *B* 。

在前面的章节中，我展示了一些用一个内核应用卷积的例子。如何同时应用几个内核？答案很简单。最终的张量(我现在使用张量这个词，因为它不再是一个简单的矩阵) *B* 将不是二维而是三维。让我们用*n*<sub>T5】c</sub>来表示您想要申请的内核数量(由于有时人们会谈到通道，所以会使用 *c* )。您只需将每个过滤器独立应用于输入，并将结果堆叠起来。因此，你得到的不是一个维数为 n*<sub>*B*</sub>×*n*<sub>*B*</sub>×*的单一矩阵*B*n*<sub>*B*</sub>×B*n*<sub>*B*</sub>B![$$ \overset{\sim }{B} $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq15.png)这意味着这*

*![$$ {\overset{\sim }{B}}_{i,j,1}\kern1em \forall i,j\in \left[1,{n}_B\right] $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equbc.png)*

 *将是输入图像与第一内核的卷积的输出，以及

![$$ {\overset{\sim }{B}}_{i,j,2}\kern1em \forall i,j\in \left[1,{n}_B\right] $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equbd.png)

将是与第二个内核卷积的输出，以此类推。卷积层只是将输入转换成输出张量。然而，这一层的权重是什么呢？网络在训练阶段学习的权重或参数是内核本身的元素。我们讨论过我们有 *n* <sub>*c*</sub> 个内核，每个*n*<sub>*K*</sub>×*n*<sub>*K*</sub>个维度。这意味着卷积层中有![$$ {n}_K^2{n}_c $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq16.png)参数。

### 注意

卷积层中的参数数量![$$ {n}_K^2{n}_c $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_IEq17.png)与输入图像大小无关。这个事实有助于减少过度拟合，尤其是在处理大输入图像时。

有时这一层用单词`POOL`表示，然后是一个数字。在我们的例子中，我们可以用`POOL1`来表示这个层。在图 [3-12](#Fig12) 中，你可以看到一个卷积层的示意图。通过应用与维度为*n*<sub>*A*</sub>×*n*<sub>*A*×*n*<sub>*c*</sub>的张量中的*n*<sub>*c*</sub>核的卷积来变换输入图像。</sub>

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig12_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig12_HTML.jpg)

图 3-12

卷积层的表示 <sup>[2](#Fn2)</sup>

当然，卷积层不一定紧接在输入之后。当然，卷积层可以将任何其他层的输出作为输入。请记住，输入图像通常会有尺寸*n*<sub>*A*×*n*<sub>*A*</sub>×3，因为彩色图像有三个通道:红色、绿色和蓝色。在考虑彩色图像时，对 CNN 中张量的完整分析超出了本书的范围。在图中，层通常被简单地表示为立方体或正方形。</sub>

### 池层

池层通常用`POOL`和一个数字表示:例如`POOL1`。它将一个张量作为输入，在将池应用于输入后，给出另一个张量作为输出。

### 注意

一个池层没有需要学习的参数，但是它引入了额外的超参数: *n* <sub>*K*</sub> 和 stride *v* 。通常，在池化层中，不使用任何填充，因为使用池化的原因之一通常是为了减少张量的维数。

### 将层堆叠在一起

在 CNN 中，你通常将卷积层和池层堆叠在一起。一个接一个。在图 [3-13](#Fig13) 中，您可以看到一个卷积层和一个池层堆栈。卷积层之后总是有一个池层。有时这两层合在一起被称为*层*。原因是池层没有可学习的权重，因此它仅仅被视为与卷积层相关联的简单操作。因此，当你阅读报纸或博客时，要注意并检查他们的意图。

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig13_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig13_HTML.jpg)

图 3-13

如何堆叠卷积层和池层的表示

在图 [3-14](#Fig14) 中总结 CNN 的这一部分，你可以看到一个 CNN 的例子。在图 [3-14](#Fig14) 中，你可以看到一个非常著名的 LeNet-5 网络的例子，你可以在这里阅读更多内容: [`https://goo.gl/hM1kAL`](https://goo.gl/hM1kAL) 。您有输入，然后两次卷积池层，然后三个完全连接的层，然后一个输出层，用一个`softmax`激活函数来执行多类分类。我在图中放了一些指示性的数字，让你对不同层的大小有个概念。

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig14_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig14_HTML.jpg)

图 3-14

类似于著名的 LeNet-5 网络的 CNN 的代表

## CNN 中的权重数

指出 CNN 中的权重在不同层中的位置是很重要的。

### 卷积层

在卷积层中，学习的参数是滤波器本身。例如，如果您有 32 个滤波器，每个尺寸为 5×5，您将获得 32×5×5 = 832 个可学习参数，因为对于每个滤波器，您还需要添加一个偏差项。请注意，这个数字不取决于输入图像的大小。在典型的前馈神经网络中，第一层中的权重数取决于输入大小，但在这里不是这样。

一般来说，卷积层中的权重数由下式给出:

![$$ {n}_C\cdotp {n}_K\cdotp {n}_K+{n}_C $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Eqube.png)

### 汇集层

池层没有可学习的参数，正如前面提到的，这是它通常与卷积层相关联的原因。在这一层(操作)中，没有可学习的权重。

### 致密层

在这一层中，权重是您从传统的前馈网络中知道的权重。所以数量取决于神经元的数量以及前一层和后一层的神经元数量。

### 注意

CNN 中唯一具有可学习参数的层是卷积层和致密层。

## CNN: MNIST 数据集的例子

让我们从一些编码开始。我们将开发一个非常简单的 CNN，并尝试在 MNIST 数据集上进行分类。从第[章到第](2.html)章，你现在应该对数据集非常了解了。

像往常一样，我们首先导入必要的包:

```
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.utils import np_utils

import numpy as np
import matplotlib.pyplot as plt

```

在开始加载数据之前，我们需要一个额外的步骤:

```
from keras import backend as K
K.set_image_dim_ordering('th')

```

原因如下。为模型加载图像时，您需要将它们转换为张量，每个张量都有三个维度:

*   沿 x 轴的像素数

*   沿 y 轴的像素数

*   颜色通道的数量(在灰色图像中，此数量为；如果您有彩色图像，这个数字是 3，每个 RGB 通道一个)

在做卷积时，Keras 必须知道它在哪个轴上找到信息。特别是，定义颜色通道维度的索引是第一个还是最后一个是相关的。为了实现这一点，我们可以用`keras.backend.set_image_dim_ordering()`来定义数据的排序。该函数接受一个字符串作为输入，该字符串可以有两个可能的值:

*   `'th'`(对于库 Theano 使用的约定):Theano 期望通道维度是第二个(第一个将是观察指标)。

*   `'tf'`(TensorFlow 使用的约定):tensor flow 期望通道维度是最后一个。

您可以使用这两种方法，但是在准备数据时要注意使用正确的约定。否则，你会得到关于张量维数的错误信息。在接下来的内容中，我们将转换张量中的图像，颜色通道维度作为第二个维度，稍后你会看到。

现在，我们准备用以下代码加载 MNIST 数据:

```
(X_train, y_train), (X_test, y_test) = mnist.load_data()

```

该代码将交付“扁平化”的图像，这意味着每个图像将是一个包含 784 个元素(28x28)的一维向量。我们需要将它们重塑为合适的图像，因为我们的卷积层需要图像作为输入。之后，我们需要标准化数据(记住图像是灰度的，每个像素可以有一个从 0 到 255 的值)。

```
X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')
X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')
X_train = X_train / 255.0
X_test = X_test / 255.0

```

请注意，既然我们已经将排序定义为`'th'`，那么通道的数量(在本例中为 1)就是`X`数组的第二个元素。下一步，我们需要对标签进行一次性热编码:

```
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)

```

我们知道我们有 10 个类，所以我们可以简单地定义它们:

```
num_classes = 10

```

现在让我们定义一个函数来创建和编译我们的 Keras 模型:

```
def baseline_model():
    # create model
    model = Sequential()
    model.add(Conv2D(32, (5, 5), input_shape=(1, 28, 28), activation="relu"))
    model.add(MaxPool2D(pool_size=(2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation="relu"))
    model.add(Dense(num_classes, activation="softmax"))
    # Compile model
    model.compile(loss='categorical_crossentropy', optimizer="adam", metrics=['accuracy'])
    return model

```

你可以在图 [3-15](#Fig15) 中看到这个 CNN 的示意图。

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig15_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig15_HTML.jpg)

图 3-15

描述我们在文中使用的 CNN 的图表。这些数字是每一层产生的张量的维数。

为了确定我们有哪种模型，我们简单地使用`model.summary()`调用。让我们首先创建一个模型，然后检查它:

```
model = baseline_model()
model.summary()

```

输出(查看图 [3-15](#Fig15) 中的图表)如下:

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 32, 24, 24)        832
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 12, 12)        0
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 12, 12)        0
_________________________________________________________________
flatten_1 (Flatten)          (None, 4608)              0
_________________________________________________________________
dense_1 (Dense)              (None, 128)               589952
_________________________________________________________________
dense_2 (Dense)              (None, 10)                1290
=================================================================
Total params: 592,074
Trainable params: 592,074
Non-trainable params: 0

```

如果您想知道为什么 max-pooling 层产生 12x12 尺寸的张量，原因是因为我们没有指定跨距，Keras 将把过滤器的尺寸作为标准值，在我们的例子中是 2x2。步长为 2 的输入张量为 24x24，您将得到 12x12 的张量。

这个网络相当简单。在模型中，我们只定义了一个卷积和池化层，我们添加了一点 dropout，然后添加了一个具有 128 个神经元的密集层，然后为具有 10 个神经元的`softmax`添加了一个输出层。现在我们可以简单地用`fit()`方法训练它:

```
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1, batch_size=200, verbose=1)

```

这将只训练一个时期的网络，并且应该给出如下输出(您的数字可能会稍有不同):

```
Train on 60000 samples, validate on 10000 samples
Epoch 1/1
60000/60000 [==============================] - 151s 3ms/step - loss: 0.0735 - acc: 0.9779 - val_loss: 0.0454 - val_acc: 0.9853

```

我们已经达到了良好的精度，没有任何过度拟合。

### 注意

当您将优化器参数传递给`compile()`方法时，Keras 将使用它的标准参数。如果您想要更改它们，您需要单独定义一个优化器。例如，要指定一个起始学习率为 0.001 的 Adam 优化器，可以使用`AdamOpt = adam(lr=0.001)`，然后用`model.compile(optimizer=AdamOpt, loss="categorical_crossentropy", metrics=['accuracy'])`将其传递给编译方法。

## CNN 学习的可视化

### 简单题外话:keras.backend.function()

有时从计算图中获得中间结果是有用的。例如，出于调试目的，您可能对特定层的输出感兴趣。在低级 TensorFlow 中，您可以简单地在会话中评估图中的相关节点，但要理解如何在 Keras 中执行并不那么容易。为了找到答案，我们需要考虑 Keras 后端是什么。最好的解释方式就是引用官方文献( [`https://keras.io/backend/`](https://keras.io/backend/) ):

> *Keras* *是一个模型级的库，为开发深度学习模型提供高级的构建模块。它本身不处理张量积、卷积等低级运算。相反，它依赖于一个专门的、优化良好的张量操作库来完成，充当 Keras 的“后端引擎”。*

为了完整起见，需要注意的是 Keras 使用了三个后端:TensorFlow 后端、T2 后端和 T4 后端。当您想要编写自己的特定函数时，您应该使用抽象的 Keras 后端 API，它可以用以下代码加载:

```
from keras import backend as K

```

理解如何使用 Keras 后端超出了本书的范围(记住本书的重点不是 Keras)，但是我建议你花一些时间去了解它。可能会很有用。例如，要在使用 Keras 时重置会话，可以使用以下命令:

```
keras.backend.clear_session()

```

这一章我们真正感兴趣的是在后面提供的一个具体方法:`function()`。其论点如下:

*   输入:占位符张量列表

*   输出:输出张量列表

*   更新:更新操作列表

*   `**kwargs`:传递到`tf.Session.run`

在本章中，我们将只使用前两个。为了理解如何使用它们，让我们以前面几节中创建的模型为例:

```
model = Sequential()
model.add(Conv2D(32, (5, 5), input_shape=(1, 28, 28), activation="relu"))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(128, activation="relu"))
model.add(Dense(num_classes, activation="softmax"))

```

例如，我们如何获得第一个卷积层的输出？我们可以通过创建一个函数轻松做到这一点:

```
get_1st_layer_output = K.function([model.layers[0].input],[model.layers[0].output])

```

这将使用以下参数

*   输入:`model.layers[0].input`，这是我们网络的输入

*   outputs: `model.layers[0].output`，第一层的输出(索引为 0)

给定一组特定的输入，您只需让 Keras 评估您的计算图中的特定节点。注意到目前为止我们只定义了一个函数。现在我们需要将它应用到特定的数据集。例如，如果我们想将它应用到一个单一的图像，我们可以这样做:

```
layer_conv_output = get_1st_layer_output([np.expand_dims(X_test[21], axis=0)])[0]

```

这个多维数组的维数`(1, 32, 24, 24)`和预期的一样:一个图像，32 个过滤器，24x24 输出。在下一节中，我们将使用该函数来查看网络中学习过的滤波器的效果。

### 内核效应

有趣的是，可以看到学习后的核对输入图像的影响。为此，让我们从测试数据集中取一个图像(如果您打乱了数据集，您可能会在索引 21 处得到一个不同的数字)。

```
tst = X_test[21]

```

注意这个数组是如何拥有维度`(1,28,28)`的。这是一个六，如图 [3-16](#Fig16) 所示。

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig16_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig16_HTML.jpg)

图 3-16

测试数据集中的第一个图像

为了获得第一层(卷积层)的效果，我们可以使用下面的代码(在前一节中解释过)

```
get_1st_layer_output = K.function([model.layers[0].input],[model.layers[0].output])
layer_conv_output = get_1st_layer_output([tst])[0]

```

请注意`layer_conv_output`是一个多维数组，它将包含输入图像与每个滤波器的卷积，相互堆叠。它的维数是(1，32，24，24)。第一个数字是 1，因为我们仅将该层应用于一个单个图像，第二个数字是 32，是我们拥有的过滤器的数量，第二个数字是 24，因为如我们所讨论的，`conv`层的输出张量维数由下式给出

![$$ {n}_B=\left\lfloor \frac{n_A+2p-{n}_K}{s}+1\right\rfloor $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equbf.png)

此外，在我们的情况下

![$$ {n}_B=\left\lfloor \frac{28-5}{1}+1\right\rfloor =24 $$](../images/470317_1_En_3_Chapter/470317_1_En_3_Chapter_TeX_Equbg.png)

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig17_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig17_HTML.jpg)

图 3-17

测试图像(a 6)与网络学习的前 12 个过滤器进行了卷积

既然在我们的网络中，我们有 *n* <sub>*A*</sub> = 28， *p* = 0，*n*<sub>*K*</sub>= 5，步距 *s* = 1。在图 [3-17](#Fig17) 中，你可以看到我们的测试图像与前 12 个滤波器(32 个对于一个图来说太多了)进行了卷积。

从图 [3-17](#Fig17) 中，您可以看到不同的过滤器如何学习检测不同的特征。例如，第三个过滤器(如图 [3-18](#Fig18) 所示)学会了检测对角线。

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig18_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig18_HTML.jpg)

图 3-18

测试图像与第三滤波器卷积。它学会了检测对角线。

其他过滤器学习检测水平线或其他特征。

### 最大池效应

下一步是将最大池应用于卷积层的输出。正如我们所讨论的，这将减少张量的维数，并试图(直观地)浓缩相关信息。

在图 [3-19](#Fig19) 中，您可以看到来自前 12 个滤波器的张量输出。

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig19_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig19_HTML.jpg)

图 3-19

当应用于来自卷积层的前 12 个张量时，池层的输出

让我们看看我们的测试图像是如何通过一个过滤器从一个卷积层和池层进行转换的(考虑第三个，只是为了说明)。在图 [3-20](#Fig20) 中可以很容易地看到效果。

![../images/470317_1_En_3_Chapter/470317_1_En_3_Fig20_HTML.jpg](../images/470317_1_En_3_Chapter/470317_1_En_3_Fig20_HTML.jpg)

图 3-20

数据集(图 a)中的原始测试图像；与第三学习滤波器卷积的图像(b 图)；在最大池图层之后，使用第三个滤镜进行卷积的图像(面板 c)

请注意图像的分辨率是如何变化的，因为我们没有使用任何填充。在下一章中，我们将看看更复杂的架构，称为*盗梦网络* *，*，它们在处理图像时比传统的 CNN(我们在本章中已经描述过)工作得更好。事实上，简单地增加越来越多的卷积层不会轻易提高预测的准确性，而更复杂的架构会更有效。

既然我们已经看到了 CNN 最基本的组成部分，我们准备进入一些更高级的话题。在下一章中，我们将探讨许多令人兴奋的话题，如初始网络、多重损失函数、自定义损失函数和迁移学习。

<aside class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

你可以在维基百科的 [`https://en.wikipedia.org/wiki/Kernel_(image_processing)`](https://en.wikipedia.org/wiki/Kernel_%2528image_processing%2529) 找到一个很好的概述。

  [2](#Fn2_source)

猫图片来源: [`https://www.shutterstock.com/`](https://www.shutterstock.com/)

 </aside>*