# 6.物体分类:简介

在这一章中，我们将研究用神经网络可以实现的更高级的图像处理任务。我们将着眼于语义分割、定位、检测和实例分割。本章的目标不是让你成为专家，因为你可以很容易地阅读许多关于这个主题的书籍，而是给你足够的信息，以便能够理解算法和阅读原始论文。我希望，在这一章结束的时候，你会明白这些方法之间的区别，你会对这些方法的构建模块有一个直观的理解。

这些算法需要许多先进的技术，如多损失函数和多任务学习，我们在前面的章节中已经讨论过了。我们将在这一章中再看几个。请记住，在某些情况下，关于方法的原始论文只有几年的历史，所以要掌握这个主题，您需要亲自动手阅读原始论文。

训练和使用论文中描述的网络在简单的笔记本电脑上是不可行的，因此你会在本章(和下一章)中发现更少的代码和例子。我试图为您指出正确的方向，并告诉您在撰写本文时有哪些经过预先培训的库和网络可用，以防您想在自己的项目中使用这些技术。那将是下一章的主题。在相关的地方，我试图指出不同方法的区别、优点和缺点。我们将以非常肤浅的方式来看待最先进的方法，因为细节非常复杂，只有研究原始论文才能给你自己实现那些算法所需的所有信息。

## 什么是对象本地化？

让我们从直观的理解什么是对象定位开始。我们已经看到了许多形式的图像分类:它告诉我们图像的内容是什么。这听起来很容易，但在很多情况下这很难，而且不是因为算法。例如，考虑当你在一个图像中同时有一只狗*和一只猫*的情况。图像的类别是什么:猫还是狗？图像的内容是什么:一只猫还是一只狗？当然，两者都在那里，但是分类算法只给你一个类别，所以它们不能告诉你你在图像中有两种动物。如果你有很多猫和狗呢？如果你有几个对象呢？你明白了。

知道猫和狗在图像中的位置可能很有趣。考虑一下自动驾驶汽车的问题:知道一个人在哪里很重要，因为这可能意味着一个死去的路人和一个活着的人之间的区别。正如我们在前面章节中所看到的，分类通常不能单独用于解决图像的实际问题。通常，识别图像中的对象有许多实例需要找到它们在图像中的位置，并能够区分它们。为此，我们需要能够找到图像中每个实例的位置及其边界。这是图像识别技术中最有趣(也是最困难)的任务之一，可以用 CNN 来解决。

通常，对于对象定位，我们希望确定一个对象(例如，一个人或一辆车)在图像中的位置，并在其周围绘制一个矩形边界框。

### 注意

对于对象定位，我们希望确定一个或多个对象(例如，人或汽车)在图像中的位置，并在其周围绘制一个矩形边界框。

有时在文献中，当图像只包含一个对象实例(例如，只有一个人或一辆车)时，研究人员使用术语*定位*，当图像包含多个对象实例时，使用术语*检测*。

### 注意

*定位*通常是指当一幅图像只包含一个对象的实例时，而*检测*是指当一幅图像中有多个对象的实例时。

为了对术语进行总结和澄清，以下是所有使用的词语和术语的概述(图 [6-1](#Fig1) 中显示了直观的解释):

![../images/470317_1_En_6_Chapter/470317_1_En_6_Fig1_HTML.jpg](../images/470317_1_En_6_Chapter/470317_1_En_6_Fig1_HTML.jpg)

图 6-1

描述在图像中定位一个或多个对象的一般任务的不同术语的直观解释

*   **分类**:给一张图片贴上标签，换句话说，就是“理解”图片里的东西。例如，一只猫的图像可能有“猫”的标签(在前面的章节中我们已经看到了几个这样的例子)。

*   **分类和定位**:给一幅图像加一个标签，确定其中包含的对象的边界(通常在对象周围画一个矩形)。

*   **对象检测**:当一幅图像中有一个对象的多个实例时，使用这个术语。在对象检测中，您想要确定几个对象(例如，人、汽车、标志等)的所有实例。)并在它们周围绘制边界框。

*   **实例分割**:你要为每一个单独的实例用特定的类来标记图像的每一个像素，以便能够找到对象实例的确切界限。

*   **语义分割**:你要给图像的每一个像素点贴上特定的类别标签。实例分段的不同之处在于，您不在乎是否有几个汽车实例作为实例。属于汽车的所有像素将被标记为“汽车”。在实例分割中，您仍然能够知道一辆汽车有多少个实例，以及它们的确切位置。为了理解其中的区别，参见图 [6-1](#Fig1) 。

分割通常是所有任务中最困难的，并且在特定情况下分割尤其困难。许多先进的技术结合起来解决这些问题。需要记住的一点是，获得足够的训练数据并不容易。请记住，这比简单的分类要困难得多，因为有人需要标记物体的位置。对于分割，需要有人对图像中的每个像素进行分类，这意味着训练数据非常昂贵且难以收集。

### 最重要的可用数据集

可以用来解决这些问题的一个众所周知的数据集是位于 [`http://cocodataset.org`](http://cocodataset.org) 的微软 COCO 数据集。该数据集包含 91 种对象类型，在 328，000 幅图像中总共有 250 万个标记实例。 <sup>[1](#Fn1)</sup> 为了让您对所使用的标注类型有个概念，图 [6-2](#Fig2) 显示了数据集中的一些例子。您可以看到对象的特定实例(如人和猫)是如何在像素级别分类的。

![../images/470317_1_En_6_Chapter/470317_1_En_6_Fig2_HTML.jpg](../images/470317_1_En_6_Chapter/470317_1_En_6_Fig2_HTML.jpg)

图 6-2

COCO 数据集中的图像示例

关于大小的快速说明:2017 年的训练图像大约为 118，000 个，需要 18GB <sup>[2](#Fn2)</sup> 的硬盘空间，所以请记住这一点。用如此大量的数据训练一个网络并不简单，需要时间和大量的计算能力。有一个 API 可以下载 COCO 图像，您可以使用它，Python 中也有这个 API。更多信息可以在主网页或 API GitHub 库 [`https://github.com/cocodataset/cocoapi`](https://github.com/cocodataset/cocoapi) 找到。图像有五种注释类型:对象检测、关键点检测、填充分割、全景分割和图像字幕。更多信息请访问 [`http://cocodataset.org/#format-data`](http://cocodataset.org/%2523format%252Ddata) 。

你可能遇到的另一个数据集是 Pascal VOC 数据集。不幸的是，该网站并不稳定，因此镜像存在于你可以找到文件的地方。一面镜子是 [`https://pjreddie.com/projects/pascal-voc-dataset-mirror/`](https://pjreddie.com/projects/pascal-voc-dataset-mirror/) 。请注意，这是一个比 COCO 数据集小得多的数据集。

在这一章和下一章，我们将主要集中在物体分类和定位。我们将假设在图像中我们只有一个特定对象的实例，任务是确定它是什么类型的对象，并围绕它绘制一个边界框(矩形)。这些现在已经足够挑战了！我们将简要地看一下分段是如何工作的，但我们不会深入研究它的许多细节，因为它的问题极难解决。我会提供参考资料，你可以自己检查和研究。

### 并集交集(IoU)

让我们考虑对图像进行分类，然后在其中的对象周围绘制一个边界框的任务。在图 [6-3](#Fig3) 中，你可以看到一个我们期望的输出示例(这里的类是`cat`)。

![../images/470317_1_En_6_Chapter/470317_1_En_6_Fig3_HTML.jpg](../images/470317_1_En_6_Chapter/470317_1_En_6_Fig3_HTML.jpg)

图 6-3

物体分类和定位的例子 <sup>[3](#Fn3)</sup>

这是一项完全监督的任务。这意味着我们需要知道边界框在哪里，并将它们与一些给定的事实进行比较。我们需要一个度量来量化预测的边界框和实际情况之间的重叠程度。这通常是通过 Union 上的交集)完成的。在图 [6-4](#Fig4) 中，你可以看到它的直观解释。作为一个公式，我们可以写成

![$$ IOU=\frac{Area\ of\ overlap}{Area\ of\ union} $$](../images/470317_1_En_6_Chapter/470317_1_En_6_Chapter_TeX_Equa.png)

![../images/470317_1_En_6_Chapter/470317_1_En_6_Fig4_HTML.jpg](../images/470317_1_En_6_Chapter/470317_1_En_6_Fig4_HTML.jpg)

图 6-4

IOU 指标的直观解释

在完美重叠的理想情况下，我们有 *IOU* = 1，而如果完全没有重叠，我们有 *IOU* = 0。你会在博客和书籍中找到这个术语，所以知道如何使用基本事实测量边界框是个好主意。

## 一种解决目标定位的简单方法(滑动窗口方法)

解决本地化问题的一个简单方法如下(剧透:这不是一个好主意，但是看看为什么会有启发性):

1.  您从左上角开始剪切输入图像的一小部分。假设你的图像有维度 *x* 、 *y* ，你的那部分有维度 *w* <sub>*x*</sub> 、 *w* <sub>*y*</sub> ，有*w*<sub>*x*</sub><*x*和*w*<sub>*y*</sub>

**   你使用一个预先训练好的网络(你如何训练它或者你如何得到它在这里是不相关的)，你让它对你剪切的图像部分进行分类。

     *   你移动这个窗口一个我们称之为*步距*的量，然后用 *s* 向右下方指示。你用网络来分类这第二部分。

     *   一旦滑动窗口覆盖了整个图像，你就选择给你最高分类概率的窗口位置。这个位置会给你对象的边界框(记住你的窗口有尺寸 *w* <sub>*x*</sub> ， *w* <sub>*y*</sub> )。

     *

 *在图 [6-5](#Fig5) 中，可以看到算法的图解说明(我们假设*w*<sub>T5】x</sub>=*w*<sub>*y*</sub>=*s*)。

![../images/470317_1_En_6_Chapter/470317_1_En_6_Fig5_HTML.jpg](../images/470317_1_En_6_Chapter/470317_1_En_6_Fig5_HTML.jpg)

图 6-5

解决目标定位问题的滑动窗口方法的图解说明

如图 [6-5](#Fig5) 所示，我们从左上角开始，向右滑动窗口。一旦我们到达图像的右边界，并且我们没有任何空间将窗口进一步向右移动，我们回到左边界，但是我们将它向下移动*的*像素。我们继续以这种方式，直到我们到达图像的右下角。

您可能会立即发现这种方法的一些问题:

*   根据*w*<sub>T3】xT5、 *w* <sub>*y*</sub> 、 *s* 的选择，我们可能无法覆盖整个图像。(您是否看到图 [6-5](#Fig5) 中窗口 4 右侧的一小部分图像未被分析？)</sub>

*   如何选择*w*<sub>T3】xT5、 *w* <sub>*y*</sub> 、 *s* ？这是一个相当棘手的问题，因为我们的对象的边界框将正好具有尺寸 *w* <sub>*x*</sub> ， *w* <sub>*y*</sub> 。如果物体更大或更小呢？我们通常不知道它的尺寸，如果我们想要精确的边界框，这是一个很大的问题。</sub>

*   如果我们的对象流过两个窗口呢？在图 [6-5](#Fig5) 中，你可以想象物体一半在窗口 2，一半在窗口 3。那么你的边界框将不会是正确的，如果你按照所描述的算法。

我们可以通过使用 *s* = 1 来解决第三个问题，以确保我们涵盖了所有可能的情况，但是前两个问题并不那么容易解决。为了解决窗口大小的问题，我们应该尝试所有可能的大小和所有可能的比例。你觉得这里有什么问题吗？你需要对你的网络进行的进化的数量正在失去控制，并且很快在计算上变得不可行。

### 滑动窗口方法的问题和局限性

在本书的 GitHub 资源库中，在 Chapter [6](6.html) 文件夹中，您可以找到滑动窗口算法的实现。为了使事情变得更简单，我决定使用 MNIST 数据集，因为在这一点上你应该非常了解它，并且它是一个易于使用的数据集。作为第一步，我建立了一个在 MNIST 数据集上训练的 CNN，准确率达到了 99.3%。然后，我开始将模型和权重保存在磁盘上。我使用的 CNN 具有以下结构:

```py
_______________________________________________________________
Layer (type)                 Output Shape              Param #
===============================================================
conv2d_1 (Conv2D)            (None, 26, 26, 32)        320
_______________________________________________________________
conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496
_______________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0
_______________________________________________________________
dropout_1 (Dropout)          (None, 12, 12, 64)        0
_______________________________________________________________
flatten_1 (Flatten)          (None, 9216)              0
_______________________________________________________________
dense_1 (Dense)              (None, 128)               1179776
_______________________________________________________________
dropout_2 (Dropout)          (None, 128)               0
_______________________________________________________________
dense_2 (Dense)              (None, 10)                1290
===============================================================
Total params: 1,199,882
Trainable params: 1,199,882
Non-trainable params: 0
_______________________________________________________________

```

然后，我使用下面的代码保存了模型和权重(我们已经讨论过如何做到这一点):

```py
model_json = model.to_json()
with open("model_mnist.json", "w") as json_file:
    json_file.write(model_json)
model.save_weights("model_mnist.h5")

```

您可以在图 [6-6](#Fig6) 中看到网络训练和准确度如何随着历元数的变化而变化。

![../images/470317_1_En_6_Chapter/470317_1_En_6_Fig6_HTML.jpg](../images/470317_1_En_6_Chapter/470317_1_En_6_Fig6_HTML.jpg)

图 6-6

训练(实线)和验证(虚线)数据集的损失函数值和准确度与历元数的关系

权重和模型可以在 GitHub 存储库中找到。我这样做是为了避免每次都要重新训练 CNN。我每次都可以通过重新加载来重用模型。你可以用这段代码来做这件事(如果你想像我一样在 Google Colab 中运行这段代码，就在你安装了 Google drive 之后):

```py
model_path = '/content/drive/My Drive/pretrained-models/model_mnist.json'
weights_path = '/content/drive/My Drive/pretrained-models/model_mnist.h5'

json_file = open(model_path, 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
loaded_model.load_weights(weights_path)

```

让事情变得简单。我决定创建一个中间有一个数字的更大的图像，看看如何有效地在它周围放置一个边界框。为了创建图像，我使用了以下代码:

```py
from PIL import Image, ImageOps
src_img = Image.fromarray(x_test[5].reshape(28,28))
newimg = ImageOps.expand(src_img,border=56,fill='black')

```

生成的图像为 140x140 像素。在图 [6-7](#Fig7) 中可以看到。

![../images/470317_1_En_6_Chapter/470317_1_En_6_Fig7_HTML.jpg](../images/470317_1_En_6_Chapter/470317_1_En_6_Fig7_HTML.jpg)

图 6-7

通过在 MNIST 数据集中的一个数字周围添加 56 像素的白色边框创建的新图像

现在让我们从一个 28x28 像素的滑动窗口开始。我们可以编写一个函数来尝试定位数字，并获取图像作为输入，步幅 *s* ，以及值*w*<sub>T5】x</sub>和 *w* <sub>*y*</sub> :

```py
def localize_digit(bigimg, stride, wx, wy):
  slidx, slidy = wx, wy

  digit_found = -1
  max_prob = -1
  bbx = -1 # Bounding box x upper left
  bby = -1 # Bounding box y upper left
  max_prob_ = 0.0
  bbx_ = -1
  bby_ = -1
  most_prob_digit = -1

  maxloopx = (bigimg.shape[0] -wx) // stride
  maxloopy = (bigimg.shape[1] -wy) // stride
  print((maxloopx, maxloopy))

  for slicey in range (0, maxloopx*stride, stride):
    for slicex in range (0, maxloopy*stride, stride):
      slice_ = bigimg[slicex:slicex+wx, slicey:slicey+wx]
      img_ = Image. fromarray(slice_).resize((28, 28), Image.NEAREST)
      probs = loaded_model.predict(np.array(img_).reshape(1,28,28,1))
      if (np.max(probs > 0.2)):
        most_prob_digit = np.argmax(probs)
        max_prob_ = np.max(probs)
        bbx_ = slicex
        bby_ = slicey

      if (max_prob_ > max_prob):
        max_prob = max_prob_
        bbx = bbx_
        bby = bby_
        digit_found = most_prob_digit

  print("Digit "+str(digit_found)+ " found, with probability "+str(max_prob)+" at coordinates "+str(bbx)+" "+str(bby))

  return (max_prob, bbx, bby, digit_found)

```

我们的形象是这样的:

```py
localize_digit(np.array(newimg), 28, 28, 28)

```

返回此代码:

```py
Digit 1 found, with probability 1.0 at coordinates 56 56
(1.0, 56, 56, 1)

```

由此产生的边界框如图 [6-8](#Fig8) 所示。

![../images/470317_1_En_6_Chapter/470317_1_En_6_Fig8_HTML.jpg](../images/470317_1_En_6_Chapter/470317_1_En_6_Fig8_HTML.jpg)

图 6-8

滑动窗口法找到的边界框 *w* <sub>*x*</sub> = 28， *w* <sub>*y*</sub> = 28，步距 *s* = 28

所以这很有效。但是您可能已经注意到，我们使用了值为 28 的 *w* <sub>*x*</sub> ， *w* <sub>*y*</sub> 和 *s* ，这是我们图像的大小。如果我们改变了会发生什么？例如，考虑图 [6-9](#Fig9) 中描述的情况。您可以清楚地看到，一旦窗口的大小和比例变为不同于 28 的值，这种方法就会停止工作。

![../images/470317_1_En_6_Chapter/470317_1_En_6_Fig9_HTML.jpg](../images/470317_1_En_6_Chapter/470317_1_En_6_Fig9_HTML.jpg)

图 6-9

*w* <sub>*x*</sub> ， *w* <sub>*y*</sub> ， *s* 不同值的滑动窗口算法结果

检查左下框中图 [6-9](#Fig9) 中分类的置信度。挺低的。例如，对于 40x40 的窗口和 10 的跨距，数字的分类是正确的(a 1 ),但是以 21%的概率完成。那就是低价值！在右下角的方框中，分类完全错误。请记住，您需要调整从图像中剪切的小部分的大小，因此它看起来可能与您使用的训练数据不同。

在这种情况下，选择正确的窗口大小和比例似乎很容易，因为您知道图像看起来像什么，但通常您不知道什么值会起作用。你必须测试不同的比例和大小，得到几个可能的边界框和分类，然后决定哪一个是最好的。您可以很容易地看到，对于可能包含几个具有不同尺寸和比例的对象的真实图像，这在计算上是不可行的。

## 分类和本地化

我们已经看到滑动窗口方法是一个坏主意。更好的方法是使用多任务学习。这个想法是，我们可以建立一个网络，同时学习类和边界框的位置。我们可以通过在 CNN 的最后一层之后增加两层致密层来实现。一个具有(例如) *N* <sub>*c*</sub> 神经元(用于分类 *N* <sub>*c*</sub> 类)将预测具有交叉熵损失函数的类(我们将用 *J* <sub>*分类*</sub> 来表示)，一个具有四个神经元，将学习具有 *ℓ* <sub>2</sub> 损失函数的边界框(即你可以在图 [6-10](#Fig10) 中看到网络图。

![../images/470317_1_En_6_Chapter/470317_1_En_6_Fig10_HTML.jpg](../images/470317_1_En_6_Chapter/470317_1_En_6_Fig10_HTML.jpg)

图 6-10

描述可以同时预测类和边界框位置的网络的图

由于这将是一个多任务学习问题，我们将需要最小化两个损失函数的线性组合:

![$$ {J}_{classification}+\alpha {J}_{BB} $$](../images/470317_1_En_6_Chapter/470317_1_En_6_Chapter_TeX_Equb.png)

当然， *α* 是一个需要调优的附加超参数。正如参考 a*ℓ*T4】2 损失与 MSE 成正比

![$$ {\ell}_2\ Loss\ Function=\sum \limits_{i=1}^m{\left({y}_{true}^{(i)}-{y}_{predicted}^{(i)}\right)}^2 $$](../images/470317_1_En_6_Chapter/470317_1_En_6_Chapter_TeX_Equc.png)

像往常一样，我们用 m 表示我们所拥有的观察数据的数量。同样的想法也非常成功地用于人体姿态估计，这可以找到人体的特定点(例如关节)，如图 [6-11](#Fig11) 所示。

![../images/470317_1_En_6_Chapter/470317_1_En_6_Fig11_HTML.jpg](../images/470317_1_En_6_Chapter/470317_1_En_6_Fig11_HTML.jpg)

图 6-11

人体姿态估计的一个例子。CNN 可以被训练来寻找人体的重要点，例如关节。

在这个领域有很多研究正在进行，在接下来的章节中，我们将看看这些方法是如何工作的。实现变得非常复杂和耗时。如果你想和这些算法打交道，最好的办法就是看看原始论文，研究一下。不幸的是，没有即插即用的库可以用来完成这些任务，尽管您可以找到一个 GitHub 库来帮助您。在这一章中，我们将看看最常见的 CNN 变体来进行对象定位——R-CNN、快速 R-CNN 和更快 R-CNN。在下一章，我们将研究 YOLO(你只看一次)算法。接下来的部分应该只作为相关论文的指针，并且会让你对网络的构建有一个基本的了解。这绝不是对这些实现的详尽分析，因为这需要大量的空间。

## 基于区域的 CNN (R-CNN)

基于区域的 CNN(也称为 R-CNN)的基本思想非常简单(但实现起来却不简单)。正如我们所讨论的，简单方法的主要问题是你需要测试大量的窗口来找到最佳匹配的边界框。搜索每个可能的位置在计算上是不可行的，因为它测试所有可能的纵横比和窗口大小。

因此，Girshick 等人 <sup>[4](#Fn4)</sup> 提出了一种方法，他们使用一种称为选择性搜索 <sup>[5](#Fn5)</sup> 的算法，首先从图像中提出 2000 个区域(称为区域建议)，然后，他们不是对大量区域进行分类，而是仅对这 2000 个区域进行分类。

选择性搜索与机器学习无关，使用经典方法来确定哪些区域可能包含对象。该算法的第一步是使用像素强度和基于图形的方法分割图像(例如，Felzenszwalb 和 Huttenlocher <sup>[6](#Fn6)</sup> 的方法)。你可以在图 [6-12](#Fig12) 中看到这种分割的结果。

![../images/470317_1_En_6_Chapter/470317_1_En_6_Fig12_HTML.jpg](../images/470317_1_En_6_Chapter/470317_1_En_6_Fig12_HTML.jpg)

图 6-12

应用于图像的分割示例(图像来源: [`http://cs.brown.edu/people/pfelzens/segment/`](http://cs.brown.edu/people/pfelzens/segment/) )

在此步骤之后，基于以下特征的相似性将相邻区域分组在一起:

*   颜色相似性

*   纹理相似性

*   尺寸相似性

*   形状兼容性

如何做到这一点的具体细节超出了本书的范围，因为这些技术通常用在图像处理算法中。

在 OpenCV <sup>[7](#Fn7)</sup> 库中，有算法的实现，可以试试。在图 [6-13](#Fig13) 中，你可以看到一个例子。我将该算法应用于我拍摄的一张照片，并要求该算法提出 40 个区域。

![../images/470317_1_En_6_Chapter/470317_1_En_6_Fig13_HTML.jpg](../images/470317_1_En_6_Chapter/470317_1_En_6_Fig13_HTML.jpg)

图 6-13

OpenCV 库中实现的选择性搜索算法的输出示例

我用的 Python 代码可以在以下网站找到: [`https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/`](https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/) 。R-CNN 的主要思想是使用 CNN 来标记该算法提出的区域，然后使用支持向量机进行最终分类。

例如，在图 [6-9](#Fig9) 中，您可以看到笔记本电脑未被识别为物体。但这就是为什么在 R-CNN 中使用 2000 个区域，以确保有足够的区域被提出。人工检查许多区域不能由人用肉眼完成。区域的数量及其重叠如此之大，以至于该任务不再可行。如果您尝试该算法的 OpenCV 实现，您会注意到它相当慢。这是开发其他方法的主要原因之一。例如，手动方法不适合实时对象检测(例如，在自动驾驶汽车中)。

R-CNN 可以概括为以下几个步骤(这些步骤已从 [`http://toe.lt/d`](http://toe.lt/d) )开始:

1.  拿一个预先训练好的`imagenet` CNN(比如 Alexnet)。

2.  用需要检测的对象和“无对象”类重新训练最后一个完全连接的层。

3.  从选择性搜索中获得所有建议(每张图片大约 2000 个地区建议),并调整它们的大小以匹配 CNN 的输入。

4.  训练 SVM 对物体和背景之间的每个区域进行分类(每个类别一个二元 SVM)。

5.  使用边界框回归。训练一个线性回归分类器，它将为边界框输出一些校正因子。

## 快速 R-CNN

Girshick 改进了它的算法，创造了所谓的“快速 R-CNN”。 <sup>[8](#Fn8)</sup> 这个算法背后的主要思想如下

1.  图像通过 CNN 并提取特征图(卷积层的输出)。

2.  提出区域，不是基于初始图像，而是基于特征图。

3.  然后，相同的特征图和建议的区域被用于传递到分类器，该分类器决定哪个对象在哪个区域中。

解释这些步骤的图表如图 [6-14](#Fig14) 所示。

![../images/470317_1_En_6_Chapter/470317_1_En_6_Fig14_HTML.jpg](../images/470317_1_En_6_Chapter/470317_1_En_6_Fig14_HTML.jpg)

图 6-14

描述快速 R-CNN 算法的主要步骤的图

这种算法比 R-CNN 更快的原因是，你不必每次 <sup>[9](#Fn9)</sup> 都向卷积神经网络馈送 2000 个区域提议——你只需要做一次。

## 更快的 R-CNN

注意，R-CNN 和快速 R-CNN 都使用选择性搜索来提议区域，因此相对较慢。即使快速的 R-CNN 对于每个图像也需要大约两秒钟，使得这种变化不适合实时对象检测。R-CNN 需要 50 秒左右，快速 R-CNN 需要两秒左右。但事实证明，我们可以做得更好，通过消除使用选择性搜索的需要，因为这是两种算法的瓶颈。

任等人 <sup>[10](#Fn10)</sup> 提出了一个新的想法:使用神经网络从标记数据中学习区域，完全去除了缓慢的选择性搜索算法。更快的 R-CNN 需要大约 0.2 秒，使它们成为对象检测的快速算法。在 [`http://toe.lt/e`](http://toe.lt/e) 有一个非常好的图表，描述了更快的 R-CNN 的主要步骤。 <sup>[11](#Fn11)</sup> 我们在图 [6-15](#Fig15) 中为您报告了它，因为我认为它确实有助于直观地理解更快的 R-CNN 的主要构件。细节往往相当复杂，因此直观和肤浅的描述不会为你服务。要理解这些步骤和微妙之处，你需要更多的时间和经验。

![../images/470317_1_En_6_Chapter/470317_1_En_6_Fig15_HTML.jpg](../images/470317_1_En_6_Chapter/470317_1_En_6_Fig15_HTML.jpg)

图 6-15

描述快速 R-CNN 主要部分的图表。图片来源: [`http://toe.lt/e`](http://toe.lt/e) 。

在下一章，我们将看另一个算法(YOLO ),看看你如何在你自己的项目中使用这些技术。

<aside class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

描述数据集的原始论文是:宗-林逸、迈克尔·梅尔、塞尔日·贝隆吉、卢博米尔·布尔德夫、罗斯·吉尔希克、詹姆斯·海斯、皮埃特罗·佩罗娜、德瓦·拉曼南、c·劳伦斯·齐特尼克、皮奥特·多拉尔、*微软可可:上下文中的常见对象、* [`https://arxiv.org/abs/1405.0312`](https://arxiv.org/abs/1405.0312)

  [2](#Fn2_source)

[T2`http://cocodataset.org/#download`](http://cocodataset.org/%2523download)

  [3](#Fn3_source)

图片来源: [`http://www.cbsr.ia.ac.cn/users/ynyu/detection.html`](http://www.cbsr.ia.ac.cn/users/ynyu/detection.html)

  [4](#Fn4_source)

[T2`https://arxiv.org/pdf/1311.2524.pdf`](https://arxiv.org/pdf/1311.2524.pdf)

  [5](#Fn5_source)

Jasper R. R. Uijlings，Koen E. A. van de Sande，Theo Gevers，Arnold w . m . smulders*《国际计算机视觉杂志》，*第 104 卷(2)，第 154-171 页，2013 年[ [`http://toe.lt/b`](http://toe.lt/b) ]

  [6](#Fn6_source)

页（page 的缩写）Felzenszwalb，D. Huttenlocher *，有效的基于图的图像分割，国际计算机视觉杂志，*第 59 卷，第 2 期，2004 年 9 月

  [7](#Fn7_source)

[T2`https://opencv.org`](https://opencv.org)

  [8](#Fn8_source)

[T2`https://arxiv.org/pdf/1504.08083.pdf`](https://arxiv.org/pdf/1504.08083.pdf)

  [9](#Fn9_source)

[T2`http://toe.lt/c`](http://toe.lt/c)

  [10](#Fn10_source)

[T2`https://arxiv.org/pdf/1506.01497.pdf`](https://arxiv.org/pdf/1506.01497.pdf)

  [11](#Fn11_source)

图像的一部分出现在 Ren 的原始论文中，但是额外的标签和信息由 Leonardo Araujo dos Santos([`https://legacy.gitbook.com/@leonardoaraujosantos`](https://legacy.gitbook.com/%2540leonardoaraujosantos))添加。

 </aside>*