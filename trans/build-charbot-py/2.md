# 2.聊天机器人的自然语言处理

本章旨在让你开始使用 Python 进行自然语言处理(NLP)来构建聊天机器人。您将使用一个名为 spaCy 的令人惊叹的开源库来学习 NLP 的基本方法和技术。如果您是 Python 生态系统的初学者或中级用户，那么不要担心，因为您将开始学习聊天机器人 NLP 所需的每一步。这一章不仅教你 NLP 中的方法，还用实际例子和编码例子来演示它们。我们还将讨论为什么聊天机器人可能需要特定的 NLP 方法。注意 NLP 本身就是一种技能。

我们将密切关注词性标注、词干、实体检测、停用词、依存句法分析和名词组块，并找出单词之间的相似性。当你构建你的用例的聊天机器人时，所有这些方法都会对你很有帮助。

除了本章介绍的方法之外，还有很多其他的 NLP 方法。基于你对正在构建的聊天机器人的需求，你可以尝试学习它们。在本章结束时，我们将要学习使用的 SpaCy 库将会给你足够的关于如何扩展你的 NLP 知识库及其理解的想法。所以，让我们开始，在下一节中首先尝试理解聊天机器人的 NLP。

## 为什么我需要知道自然语言处理来构建聊天机器人？

要了解这个问题的答案，我们先来了解一下自然语言处理(NLP)。

**自然语言处理(NLP)** 是人工智能的一个领域，使计算机能够分析和理解人类语言。

现在，为了执行 NLP，或者说，自然语言理解(NLU)，我们有很多方法，接下来我们将讨论这些方法。你听到了一个新术语**自然语言理解(NLU)**——那是什么？

简单来说，NLU 是 NLP 更大图景的子集，就像机器学习、深度学习、NLP 和数据挖掘是人工智能(AI)更大图景的子集，人工智能是任何做一些智能事情的计算机程序的总称。

> 一个很好的经验法则是使用术语 NLU 来表达机器理解人类提供的自然语言的能力。

现在，关于你是否真的需要了解 NLP 来构建一个聊天机器人的问题——答案是肯定的和否定的。困惑了吗？你没听错，不是说你不知道 NLP 的方法和技术就根本造不出聊天机器人，只是你的范围会有些局限。您无法在扩展应用程序的同时保持代码整洁。当聊天机器人不能行走和奔跑时，NLP 给了它飞翔的翅膀。

对于普通人来说，聊天机器人只不过是与另一端的智能机器进行交流的一种方式。这种机器既可以是基于语音的，也可以是基于文本的，用户将用他们自己的语言输入，这在计算机科学中通常被称为自然语言。

我们知道不存在有魔力的黑盒，一切都很好。一要知道 AI 里没有什么人工的东西；它实际上是由伟大的人编写的机器学习和深度学习算法，在引擎盖下运行。机器还没有达到可以像人类一样思考拥有自己智能的阶段。今天的人工智能系统——它们做什么和它们的行为方式——是我们如何训练它们的结果。

因此，要理解用户的自然语言，不管它是什么语言，也不管它是什么输入形式(文本、语音、图像等)。)，我们必须编写算法并使用 NLP 的技术。NLP 被认为是聊天机器人的大脑，它处理原始数据，执行 munging，清理数据，然后准备采取适当的行动。

NLP 本身是一个巨大的主题，需要时间和毅力来完全学习，但是对于一个聊天机器人开发者来说，有一些方法是必须知道的，这是我们在本章将要学习的。

## spaCy 是什么？

spaCy 是一个用于高级 NLP 的开源软件库，用 Python 和 Cython 编写，由 Matthew Honnibal 构建。它提供了直观的 API 来访问其由深度学习模型训练的方法。

spaCy 提供了世界上最快的语法分析器。直接取自 spaCy 的文档，他们有一些惊人的基准测试结果，如下所示。

### spaCy 的基准结果

2015 年的两篇同行评议论文证实，spaCy 提供了世界上最快的语法分析器，其准确性在现有最佳水平的 1%以内。少数更精确的系统要慢 20 倍甚至更多。让我们试着看一下图 [2-1](#Fig1) ，它显示了基于速度和准确性的 spaCy 基准测试结果，与其他库进行了比较。

![img/461879_1_En_2_Fig1_HTML.jpg](img/461879_1_En_2_Fig1_HTML.jpg)

图 2-1

空间基准测试结果

spaCy 还提供多种语言的统计神经网络模型，如英语、德语、西班牙语、葡萄牙语、法语、意大利语、荷兰语和多语言 NER。它还为各种其他语言提供了标记化。此表显示了 Choi 等人的速度基准，因此在不同硬件上比较 spaCy v2.x 基准是不公平的。这就是您看不到 spaCy v2.x 的速度列值的原因。

### spaCy 提供了什么？

spaCy 声称提供了三个主要的东西，并且非常有帮助。让我们来看看这些，并理解为什么人们应该知道并使用 spaCy 作为进行 NLP 的首选模块。

#### 世界上最快的图书馆

spaCy 在提取大规模信息方面做得非常好。在 Cython 库的帮助下，它是从零开始编写的，对内存有着极大的关注。

#### 把事情做完

spaCy 的设计理念是“把事情做好”。它帮助我们完成真实世界的 NLP 场景。干净的文档为开发人员和计算语言学爱好者节省了大量时间，并使他们更有效率。它很容易安装，就像任何其他 Python 包一样。

#### 深度学习

spaCy 是开源社区中处理深度学习算法文本的最佳库之一。它与 TensorFlow、PyTorch、scikit-learn、Gensim 以及 Python 的其他相关技术无缝协作。深度学习开发者可以很容易地为一系列 NLP/NLU 问题构建语言复杂的统计模型。

## 空间特征

没有其他 NLP 库提供了范围极其广泛的 API 来做几乎所有的事情，这正是 spaCy 所做的。这个库最大的优点是它在不断发展，变得越来越好。让我们先睹为快，看看他们官网[ [`https://spacy.io/`](https://spacy.io/) ]上提到的 spaCy 的特性。

*   非破坏性标记化

*   命名实体识别

*   支持 28 种以上的语言

*   8 种语言的 13 种统计模型

*   预先训练的单词向量

*   轻松的深度学习集成

*   词性标注

*   标记依存句法分析

*   句法驱动的句子分割

*   语法和 NER 的内置可视化工具

*   方便的字符串到哈希映射

*   导出到 numpy 数据数组

*   高效的二进制序列化

*   简单的模型打包和部署

*   最先进的速度

*   稳健、经过严格评估的精确度

现在，让我们深入研究 Python 中这个令人敬畏的 NLP 模块:spaCy。

### 安装和先决条件

在我们真正深入空间和代码片段之前，请确保您的操作系统上安装了 Python。如果没有，请参考[1]。

你可以使用你喜欢的任何版本的 Python。今天，大多数系统都预装了默认的 Python 2.7 . x 版本。在本章中，我们将使用 Python 3。所以，如果你想使用 Python 3，请从 [`https://www.python.org/downloads/`](https://www.python.org/downloads/) 下载 Python 3 安装在你的操作系统上。如果已经安装了 Python 2，也可以使用它；它可能需要也可能不需要微小的代码更改。

我们将通过 pip [2]安装 spaCy。

我们将使用虚拟环境[3]并将 spaCy 安装到一个用户目录中。

如果您使用的是 macOS/OSX/Linux，请遵循以下步骤:

```py
Step 1:
python3 -m pip install -U virtualenv

Step 2:
virtualenv venv -p /usr/local/bin/python3 #Make sure you use your own OS path for python 3 executable.

Step 3:
source venv/bin/activate

Step 4:
pip3 install -U spacy # We'll be using spaCy version 2.0.11.

```

最后一步可能需要时间，所以耐心等待。

如果您使用的是 Windows，只需将步骤 3 更改为

```py
venv\Scripts\activate

```

现在，我们将在我们的虚拟环境中安装 *Jupyter Notebook* ，这是我们在步骤 3 中激活的。使用 *Jupyter Notebook* 比标准的 Python 解释器要容易得多，效率也更高。在接下来的章节中，我们将执行 Jupyter 笔记本中的所有片段。

要安装 Jupyter Notebook，请运行以下 pip 命令:

```py
pip3 install jupyter

```

此命令将在您的系统中安装 Jupyter 笔记本。

此时，您应该已经在您的 virtualenv 中安装了 spaCy 和 Jupyter Notebook。让我们验证一下是否所有的东西都安装成功了。

1.  转到您的命令行界面，键入以下内容，您应该会看到一个服务器正在启动，并在您的默认浏览器中打开一个 url。

    ```py
    $ jupyter notebook

    ```

    默认的网址是`http://localhost:8888/tree`。它应该看起来像图 [2-2](#Fig2) 。

1.  Click on New as shown in Figure [2-2](#Fig2), and choose Python 3\. It will open a new tab in your current browser and create a new notebook for you, where you can play with the Python code. You can execute any Python code, import libraries, plot charts, and markdown cells.

    ![img/461879_1_En_2_Fig2_HTML.jpg](img/461879_1_En_2_Fig2_HTML.jpg)

    图 2-2

    木星笔记本第一眼

2.  Type import spaCy and run the cell by clicking on “Run” button or by pressing Shift + Enter. It should look something like Figure [2-3](#Fig3).

    ![img/461879_1_En_2_Fig3_HTML.jpg](img/461879_1_En_2_Fig3_HTML.jpg)

    图 2-3

    验证空间安装

如果步骤 3 没有抛出任何错误消息，那么您已经成功地在您的系统上安装了 spaCy 模块。你应该在你的笔记本上看到你安装的空间版本。如果您想安装相同版本的 spaCy，那么您可以在通过 pip 安装 spaCy 时指定版本。

```py
pip3 install –U spacy==2.0.11

```

### 什么是 SpaCy 模型？

SpaCy 模型就像任何其他机器学习或深度学习模型一样。模型是算法的产物，或者说，是在使用机器学习算法训练数据之后创建的对象。spaCy 有很多这样的模型，可以像下载其他 Python 包一样直接放在我们的程序中。

现在，我们将把 spaCy 模型作为 Python 包安装。

为此，我们将利用 notebook 的 magic 命令在 notebook 中运行以下命令。通过前缀！在 shell 命令之前，我们也可以从 Jupyter 笔记本中运行 shell 命令。让我们看看它看起来怎么样。

```py
!python3 -m spacy download en

```

使用 Jupyter Notebook 下载`Python 3`的空间模型时，您可能会遇到权限问题。转到您的终端，运行以下命令:

```py
sudo python3 –m download en

```

参见图 [2-4](#Fig4) 进行参考。

![img/461879_1_En_2_Fig4_HTML.jpg](img/461879_1_En_2_Fig4_HTML.jpg)

图 2-4

下载空间模型

如图 [2-4](#Fig4) 所示，spaCy 试图下载一些核心文件，并作为 Python 包安装。

### 注意

`!`【感叹号运算符】只在 Jupyter 笔记本中起作用。要直接从终端安装 spaCy 型号，您需要删除！【感叹号运算符】；否则会导致错误。

1.  [T2`https://www.python.org/downloads/`](https://www.python.org/downloads/)

2.  [T2`https://packaging.python.org/tutorials/installing-packages/#installing-from-pypi`](https://packaging.python.org/tutorials/installing-packages/#installing-from-pypi)

3.  [T2`http://docs.python-guide.org/en/latest/dev/virtualenvs/`](http://docs.python-guide.org/en/latest/dev/virtualenvs/)

## 构建聊天机器人的自然语言处理基本方法

擅长基础知识，成为某方面的专家，并高效地完成它，这真的很重要。要构建聊天机器人，我们需要了解自然语言处理的基本方法。这些方法有助于我们将输入分解成块，并使其有意义。在下一节，我们将学习一些最常用的自然语言处理方法，这些方法不仅能帮助你擅长自然语言处理，还能帮助你构建很酷的聊天机器人。我们越能更好、更有效地处理输入文本，我们就能更好地响应用户。

### 词性标注

词性标注是一个过程，在这个过程中，你阅读一些文本，并为每个单词或标记分配词性，如名词、动词、形容词等。

当你想识别给定句子中的某个实体时，词性标注变得极其重要。第一步是做词性标注，看看我们的文本包含什么。

让我们用一些真正的 POS 标记的例子来弄脏我们的手。

**例 1:**

```py
nlp = spacy.load('en') #Loads the spacy en model into a python object
doc = nlp(u'I am learning how to build chatbots') #Creates a doc object
for token in doc:
    print(token.text, token.pos_) #prints the text and POS

```

**输出:**

```py
('I', 'PRON')
('am', 'VERB')
('learning', 'VERB')
('how', 'ADV')
('to', 'PART')
('build', 'VERB')
('chatbots', 'NOUN')

```

**例 2:**

```py
doc = nlp(u'I am going to London next week for a meeting.')
for token in doc:
    print(token.text, token.pos_)

```

**输出:**

```py
('I', 'PRON')
('am', 'VERB')
('going', 'VERB')
('to', 'ADP')
('London', 'PROPN')
('next', 'ADJ')
('week', 'NOUN')
('for', 'ADP')
('a', 'DET')
('meeting', 'NOUN')
('.', 'PUNCT')

```

正如我们所看到的，当我们打印从方法`nlp,`返回的`Doc`对象的标记时，方法`nlp,`是一个用于访问注释的容器，我们得到了用句子中的每个单词标记的 POS。

这些标签是属于单词的属性，它们决定了单词在语法正确的句子中的使用。我们可以利用这些标签作为信息过滤等领域的词汇特征。

让我们试着举另一个例子，我们试着探索来自`Doc`对象的令牌的不同属性。

**例 3:**

```py
doc = nlp(u'Google release "Move Mirror" AI experiment that matches your pose from 80,000 images')

     for token in doc:
          print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
                token.shape_, token.is_alpha, token.is_stop)

```

**输出:**

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"> <col class="tcol7 align-left"> <col class="tcol8 align-left"></colgroup> 
| 

**正文**

 | 

**引理**

 | 

**位置**

 | 

**标签**

 | 

dep

 | 

**形状**

 | 

alpha

 | 

**停止**

 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 谷歌 | 谷歌 | -好的 | NNP | 复合的 | 五 x 综合征 | 真实的 | 错误的 |
| **发布** | 释放；排放；发布 | 名词 | 神经网络 | 模式 | 电影站 | 真实的 | 错误的 |
|  | " | 点点 | `` | 点点 | " | 错误的 | 错误的 |
| **移动** | 移动 | -好的 | NNP | 模式 | 电影站 | 真实的 | 错误的 |
| **镜子** | 镜子 | -好的 | NNP | 模式 | 五 x 综合征 | 真实的 | 错误的 |
|  | " | 点点 | " | 点点 | " | 错误的 | 错误的 |
| **艾** | 人工智能 | -好的 | NNP | 复合的 | xx | 真实的 | 错误的 |
| **实验** | 实验 | 名词 | 神经网络 | 根 | 电影站 | 真实的 | 错误的 |
| **那个** | 那 | 形容词 | 禁水试验 | nsubj | 电影站 | 真实的 | 真实的 |
| **匹配** | 比赛 | 动词 | 核黄素 | relcl | 电影站 | 真实的 | 错误的 |
| **你的** | PRON | 形容词 | PRP 元 | 可能的 | 电影站 | 真实的 | 真实的 |
| 姿势 | 姿态 | 名词 | 神经网络 | 扔过来 | 电影站 | 真实的 | 错误的 |
| **来自** | 从 | 腺苷二磷酸 | 在…里 | 准备 | 电影站 | 真实的 | 真实的 |
| **8 万** | Eighty thousand | 全国矿工联合会 | 激光唱片 | nummod | dd，ddd | 错误的 | 错误的 |
| **图像** | 图像 | 名词 | 非营养性吸吮 | 比吉 | 电影站 | 真实的 | 错误的 |

**例 4:**

```py
doc = nlp(u'I am learning how to build chatbots')
for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
        token.shape_, token.is_alpha, token.is_stop)

```

**输出** **:**

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"> <col class="tcol7 align-left"> <col class="tcol8 align-left"></colgroup> 
| 

**正文**

 | 

**引理**

 | 

**位置**

 | 

**标签**

 | 

dep

 | 

**形状**

 | 

alpha

 | 

**停止**

 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| **我** | PRON | 代词 | 富含血小板血浆 | nsubj | X | 真实的 | 错误的 |
| **上午** | 是 | 动词 | 文件 | 去吧 | xx | 真实的 | 真实的 |
| **学习** | 学习 | 动词 | 静脉全血葡萄糖 | 根 | 电影站 | 真实的 | 错误的 |
| **如何** | 怎么 | 副词 | war refugee board 战时难民事务委员会 | advmod | xxx | 真实的 | 真实的 |
| **至** | 到 | 部分 | 到 | 去吧 | xx | 真实的 | 真实的 |
| **建造** | 建设 | 动词 | 动词 | 断续器 | 电影站 | 真实的 | 错误的 |
| **chatbots** | 聊天机器人 | 名词 | 非营养性吸吮 | 扔过来 | 电影站 | 真实的 | 错误的 |

请参考下表，了解我们在代码中打印的每个属性的含义。

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

文本

 | 

正在处理的实际文本或单词

 |
| --- | --- |
| 引理 | 正在处理的单词的词根形式 |
| 刷卡机 | 词的词性 |
| 标签 | 它们表达词性(如动词)和一定量的形态信息(如动词是过去式)。 |
| 资料执行防止 | 句法依赖性(即，标记之间的关系) |
| 形状 | 单词的形状(例如，大写、标点、数字格式) |
| 希腊字母的第一个字母 | 令牌是字母字符吗？ |
| 停止 | 这个单词是停用词还是停用列表的一部分？ |

您可以参考下表来了解 token 对象的每个 POS 属性值的含义。这个列表给出了 spaCy 模型分配的词性标签的详细想法。

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

**位置**

 | 

**描述**

 | 

**例题**

 |
| --- | --- | --- |
| **ADJ** | 形容词 | *大，老，绿，不可理解，第一* |
| **ADP** | ADHD 位置 | *在，到，在*期间 |
| **ADV** | 副词 | *很，明天，下来，哪里，那里* |
| au | 辅助的 | *是，已经(做了)，将要(做了)，应该(做了)* |
| **CONJ** | 结合 | *与，或，但* |
| cconj | 并列连词 | *与，或，但* |
| **它** | 限定词 | *一个，一个，* |
| intj | 感叹词 | 嘶，哎哟，好极了，你好 |
| **名词** | 名词 | 女孩，猫，树，空气，美女 |
| 中的 | 数字 | *1，2017，一，七十七，四，MMXIV* |
| **部分** | 颗粒 | *的，不是的，* |
| pron | 代词 | 我，你，他，她，我自己，他们自己，某人 |
| 提议 | 专有名词 | 玛丽、约翰、伦敦、北约、HBO |
| **点** | 标点 | *。, (, ), ?* |
| **scoj** | 从属连词 | *如果，虽然，那* |
| **SYM** | 标志 | *$、%，、、、+、-、×、:、* |
| **动词** | 动词 | *跑，跑，跑，吃，吃，吃* |
| **X** | 其他的 | *sfpksdpsxma* |
| **空间** | 空间 |   |

那么为什么聊天机器人需要词性标注呢？

回答:降低理解一篇无法训练或者训练的信心不足的文本的复杂度。通过使用词性标注，我们可以识别文本输入的各个部分，并仅对这些部分进行字符串匹配。例如，如果您要查找一个位置是否存在于一个句子中，那么词性标记会将位置词标记为名词，因此您可以从标记列表中获取所有名词，并查看它是否是预设列表中的位置之一。

### 词干化和词汇化

词干化是将屈折词还原为词干(基本形式)的过程。

词干算法将单词“say”简化为词根单词“say”，而“proposely”变成了 propose。如你所见，这可能是也可能不是 100%正确的。

**词汇化**与**词干化**密切相关，但词汇化是根据单词的预期含义确定其词汇的算法过程。

例如，在英语中，动词“行走”可能显示为“行走”、“散步”、“散步”或“散步”人们可能会在字典中查找的基本形式“walk”被称为该词的*词条*。spaCy 没有任何内置的词干分析器，因为词汇化被认为更加正确和有效。

词干化和词汇化的区别

*   词干提取以一种粗糙的、启发式的方式来完成这项工作，即砍掉单词的末尾，假设剩下的单词就是我们真正要找的，但它通常包括去除派生词缀。

*   **词汇化**试图通过使用词汇和词形分析来更优雅地完成这项工作。它尽最大努力只删除屈折词尾，并返回单词的字典形式，称为词条。

尽管很少有库提供词干化和词汇化的方法，但使用词汇化来正确获取词根始终是最佳实践。

让我们通过一些例子来探讨词汇化:

**例 1:**

```py
from spacy.lemmatizer import Lemmatizer
from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES
lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)
lemmatizer('chuckles', 'NOUN') # 2nd param is token's part-of-speech tag

```

**输出:**

```py
[u'chuckle']

```

**例 2:**

```py
lemmatizer('blazing', 'VERB')

```

**输出:**

```py
[u'blaze']

```

**例 3:**

```py
lemmatizer('fastest', 'ADJ')

```

**输出:**

```py
[u'fast']

```

如果您想比较词干分析器和 lemmatizer，那么您需要安装 Python 最流行的库之一:自然语言工具包(NLTK)。spaCy 最近很受欢迎，但正是 NLTK 让每个 NLP 爱好者投身到 NLP 及其技术的海洋中。

查看下面的例子，我们尝试使用 NLTK 提供的两种词干技术。首先，我们尝试使用 PorterStemmer 获得单词“First”的词干，然后使用 SnowBallStemmer。两者给出的结果是一样的——也就是“最快”——但是当我们用 spaCy 的方法做引理化时，它给我们的是“fast”作为“fast”的词干，这更有意义，也更正确。

```py
from nltk.stem.porter import *
from nltk.stem.snowball import SnowballStemmer
porter_stemmer = PorterStemmer()
snowball_stemmer = SnowballStemmer("english")
print(porter_stemmer.stem("fastest"))
print(snowball_stemmer.stem("fastest"))

fastest
fastest

```

### 注意

在尝试运行这段代码之前，确保使用 pip3 安装`nltk`包。

既然你很清楚在 NLP 中词干化或词元化是做什么的，你应该能够理解每当你遇到需要单词的词根形式的情况时，你需要在那里做词元化。例如，它经常被用于构建搜索引擎。你一定想知道谷歌是如何在搜索结果中给你你想要的文章的，即使搜索文本没有被恰当地表达。

这就是使用词汇化的地方。想象一下，你用文字“*搜索，《权力的游戏》下一季什么时候上映？*

现在，假设搜索引擎做简单的文档词频匹配，给你搜索结果。在这种情况下，前面提到的查询可能不会匹配标题为“*Game of Thrones next season release date”的文章*

如果我们在将输入与文档进行匹配之前对原始问题进行词汇化，那么我们可能会得到更好的结果。

在接下来的章节中，我们将尝试测试这一理论。

### 命名实体识别

**命名实体识别**(**)，又名**实体识别**或**实体提取**，是在给定文本中寻找[命名实体](https://en.wikipedia.org/wiki/Named_entity)并将其分类到预定义类别的过程。**

 **NER 任务非常依赖于用来训练 NE 提取算法的知识库，因此它可能工作也可能不工作，这取决于它被训练的所提供的数据集。

spaCy 带有一个非常快速的实体识别模型，能够从给定的文档中识别实体短语。实体可以是不同的类型，例如人、位置、组织、日期、数字等。这些实体可以通过`doc`对象的`.ents`属性来访问。

让我们借助 spaCy 强大的 NER 标记功能，通过一些例子来尝试找到命名实体。

**例 1:**

```py
my_string = u"Google has its headquarters in Mountain View, California having revenue amounted to 109.65 billion US dollars"
doc = nlp(my_string)

for ent in doc.ents:
    print(ent.text, ent.label_)

```

**输出:**

```py
('Google', 'ORG')
('Mountain View', 'GPE')
('California', 'GPE')
('109.65 billion US dollars', 'MONEY')

```

我们可以看到 spaCy 模型是如何漂亮和自动地识别出单词 **Google** 是一个**组织**， **California** 是一个**地缘政治实体**，在给定的句子中，我们谈论的是**1096.5 亿美元，**实际上是关于钱的。

让我们试着探索更多的例子。

**例 2:**

```py
my_string = u"Mark Zuckerberg born May 14, 1984 in New York is an American technology entrepreneur and philanthropist best known for co-founding and leading Facebook as its chairman and CEO."
doc = nlp(my_string)

for ent in doc.ents:
    print(ent.text, ent.label_)

```

**输出:**

```py
('Mark Zuckerberg', 'PERSON')
('May 14, 1984', 'DATE')
('New York', 'GPE')
('American', 'NORP')
('Facebook', 'ORG')

```

**例 3:**

```py
my_string = u"I usually wake up at 9:00 AM. 90% of my daytime goes in learning new things."
doc = nlp(my_string)
for ent in doc.ents:
    print(ent.text, ent.label_)

```

**输出:**

```py
('9:00 AM', 'TIME')
('90%', 'PERCENT')

```

如您所见，实体提取器可以很容易地从给定的字符串中提取时间信息。此外，正如你所看到的，实体提取器不仅试图识别数字，而且准确的百分比值。

根据 spaCy 的文档，在[onto notes 5](https://catalog.ldc.upenn.edu/ldc2013t19)<sup>[1](#Fn1)</sup>语料库上训练的模型支持以下实体类型。

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

**类型**

 | 

**描述**

 |
| --- | --- |
| **人** | 人，包括虚构的 |
| **NORP** | 国籍、宗教或政治团体 |
| **FAC** | 建筑物、机场、高速公路、桥梁等。 |
| **组织** | 公司、代理处、机构等。 |
| gpe | 国家、城市、州 |
| **LOC** | 非 GPE 地区、山脉、水体 |
| **产品** | 物体、交通工具、食物等。(非服务) |
| **事件** | 命名飓风、战役、战争、体育赛事等。 |
| **艺术品** | 书籍、歌曲等的名称。 |
| **定律** | 被命名为法律的文件 |
| **语言** | 任何命名语言 |
| **日期** | 绝对或相对的日期或时期 |
| **时间** | 小于一天的时间 |
| **百分比** | 百分比，包括“%” |
| **钱** | 货币价值，包括单位 |
| **数量** | 测量，如重量或距离 |
| **序数** | “第一”、“第二”等等。 |
| 基数 | 不属于另一种类型的数字 |

每当我们打算用简单的术语构建一个对话代理或聊天机器人时，我们总是在头脑中有一个域。例如，我们希望聊天机器人预约医生、点餐、支付账单、填写银行申请、电子商务等。聊天机器人也可以解决这些问题。通过找出问题中的实体，人们可以对提出问题的背景有一个公平的想法。

让我们以两个单词相似但意思不同的句子为例来试着理解这一点。

```py
my_string1 = u"Imagine Dragons are the best band."
my_string2 = u"Imagine dragons come and take over the city."

doc1 = nlp(my_string1)
doc2 = nlp(my_string2)

for ent in doc1.ents:
    print(ent.text, ent.label_)

```

上面的`for`循环遍历`doc1`对象*给出一个输出:*

```py
('Imagine Dragons', 'ORG')

```

太棒了，不是吗？当您意识到实体识别器不能识别第二个<sup>和</sup>字符串中的任何实体时，这将变得更加有趣。运行下面的代码，`doc2`不会产生任何输出。

```py
for ent in doc2.ents:
    print(ent.text, ent.label_)

```

现在，假设您要在真实环境中提取上述两个字符串的上下文。你会怎么做？在*实体提取器*的帮助下，人们可以很容易地找出语句的上下文，并智能地进一步展开对话。

### 停止言语

停用词是像 *a、*、*、*、*到*和*还有*这样的高频词，我们有时想在进一步处理之前将其从文档中过滤掉。停用词通常没有什么词汇内容，也没有多少意义。

下面是在 Reuters-RCV1 中常见的 25 个语义非选择性停用词的列表。

```py
a   an    and    are    as    at    be    by    for
from    has    he    in    is    it    its    of    on
that    the    to    was    were    will    with

```

让我们进入一些代码，试着理解事物是如何工作的。

要查看 spaCy 中定义为停用词的所有词，可以运行以下代码行:

```py
from spacy.lang.en.stop_words import STOP_WORDS
print(STOP_WORDS)

```

您应该会看到类似这样的内容:

```py
set(['all', 'six', 'just', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'fifty', 'four', 'not', 'own', 'through', 'using', 'go', 'only', 'its', 'before', 'one', 'whose', 'how',
......................................................................................................................................................................................................................................................................................................
'whereby', 'third', 'i', 'whole', 'noone', 'sometimes', 'well', 'together', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'otherwise', 'make', 'once'])

```

spaCy 的停用词表中定义了大约 305 个停用词。如果需要，您可以随时定义自己的停用字词，并覆盖现有列表。

要查看一个单词是否是停用词，可以使用 spaCy 的`nlp`对象。我们可以使用`nlp`对象的`is_stop`属性。

**例 1:**

```py
nlp.vocab[u'is'].is_stop

```

**输出:**

```py
True

```

**例 2:**

```py
nlp.vocab[u'hello'].is_stop

```

**输出:**

```py
False

```

**例 3:**

```py
nlp.vocab[u'with'].is_stop

```

**输出:**

```py
True

```

停用词是文本清理的一个非常重要的部分。它有助于在我们尝试进行实际处理以理解文本之前删除无意义的数据。

假设你处于这样一种情况，你正在建造一个机器人，通过评估人们的情绪来让他们开心。现在，需要分析用户输入的文本中的情感，以便可以制定正确的响应。这里，在乞求做基本的情感分析之前，我们要去除数据中以停用词形式存在的噪音。

### 依存句法分析

依赖解析是 spaCy 更漂亮、更强大的特性之一，它既快又准确。解析器还可以用于句子边界检测，并让您遍历基本名词短语或“组块”

spaCy 的这个特性为您提供了一个解析树，它解释了单词或短语之间的父子关系，并且与单词出现的顺序无关。

让我们举一个例子，你必须分析下面的句子:

*帮我订一张从班加罗尔到果阿的机票*

**例 1:**

```py
doc = nlp(u'Book me a flight from Bangalore to Goa')
blr, goa = doc[5], doc[7]
list(blr.ancestors)

```

**输出:**

```py
[from, flight, Book]

```

上面的输出可以告诉我们，用户希望预订从班加罗尔出发的航班。

让我们试着列出`goa.ancestors`物体的祖先:

```py
list(goa.ancestors)

```

**输出:**

```py
[to, flight, Book]

```

这个输出可以告诉我们，用户希望预订飞往果阿的航班。

#### 依存解析中的祖先是什么？

祖先是这个标记的语法后代的最右边的标记。就像上例中对于对象`blr`的祖先分别是*从、*和*书。*

记住，你总是可以使用`ancestors`属性列出一个`doc`对象项目的祖先。

```py
list(doc[4].ancestors) #doc[4]==flight

```

上面的代码将输出:

```py
[flight, Book]

```

为了以编程方式检查一个`doc`对象项是否是另一个 *doc* 对象项的祖先，我们可以做以下事情:

```py
doc[3].is_ancestor(doc[5])

```

以上返回`True`，因为`doc[3]`(即 flight)是`doc[5]`(即 Bangalore)的祖先。您可以尝试更多这样的例子，以便更好地理解依赖解析和祖先概念。

如果我们试着想象一个真实世界的场景，我们可能会在尝试构建聊天机器人时遇到类似这样的句子

我想预订一辆去酒店的出租车和一张餐馆的桌子。

在这句话中，重要的是要知道请求了什么*任务*以及它们的目标是哪里(即*用户是想预订出租车去酒店还是餐馆* ) *。*

*让我们试着用下面的代码来做这件事:*

**例 1:**

```py
doc = nlp(u'Book a table at the restaurant and the taxi to the hotel')
tasks = doc[2], doc[8] #(table, taxi)
tasks_target = doc[5], doc[11] #(restaurant, hotel)

for task in tasks_target:
          for tok in task.ancestors:
              if tok in tasks:
                  print("Booking of {} belongs to {}".format(tok, task))
      break

```

**输出:**

```py
Booking of table belongs to restaurant
Booking of taxi belongs to hotel

```

#### 依存解析中的子元素是什么？

子代是该标记的直接语法依赖项。我们可以像使用`ancestors`一样使用`children`属性来查看单词的子单词。

```py
list(doc[3].children)

```

将输出

```py
[a, from, to]

```

#### 用于依存解析的交互式可视化

第一次理解完整的依存解析概念是非常困难的。spaCy 提供了一种非常简单的交互式方法来理解它的依赖解析。spaCy v2.0+有一个可视化模块，在这里我们可以传递一个`Doc`或一列`Doc`对象给 displaCy，并调用 displaCy 的`serve`方法来运行 web 服务器。

图 [2-5](#Fig5) 显示了交互式可视化将如何寻找依赖解析。

![img/461879_1_En_2_Fig5_HTML.jpg](img/461879_1_En_2_Fig5_HTML.jpg)

图 2-5

用于依存解析的交互式可视化

您也可以生成图 [2-5](#Fig5) 中的依赖解析可视化。要创建这样的可视化效果，运行下面的代码，然后在浏览器中转到`http://localhost:5000`。

让我们试着将我们的任务示例和任务目标可视化。

```py
from spacy import displacy
doc = nlp(u'Book a table at the restaurant and the taxi to the hotel')
displacy.serve(doc, style="dep")

```

运行这段代码将得到如图 [2-6](#Fig6) 所示的输出。如果你得到类似的东西，那么转到浏览器的另一个标签，输入`http://localhost:5000`。

![img/461879_1_En_2_Fig6_HTML.jpg](img/461879_1_En_2_Fig6_HTML.jpg)

图 2-6

正在本地主机上启动依赖关系解析服务器

我们在代码中得到这个字符串的依赖解析的可视化(如图 [2-7](#Fig7) 所示)。

![img/461879_1_En_2_Fig7_HTML.jpg](img/461879_1_En_2_Fig7_HTML.jpg)

图 2-7

依存解析示例

让我们再举一个依赖解析的例子，我们假设用户在问下面的句子:

*柏林有哪些值得参观的地方，在吕贝克逗留的时间有哪些？*

我们将首先创建 d `oc`对象，如下所示:

```py
 doc = nlp(u"What are some places to visit in Berlin and stay in Lubeck")

```

现在，我们得到了正在谈论的地点和用户想要的动作:

```py
places = [doc[7], doc[11]] #[Berlin, Lubeck]
actions = [doc[5], doc[9]] #[visit, stay]

```

因为您已经知道了词性标注和实体提取，所以您可以很容易地自动获得位置和动作。

现在我们有了位置，让我们遍历它的每个祖先，并检查是否在 actions 中找到了任何祖先。在动作列表中找到的 place 的第一个父代应该是有问题的 place 的动作。

```py
for place in places:
    for tok in place.ancestors:
        if tok in actions:
            print("User is referring {} to {}").format(place, tok)
            break

```

**输出:**

```py
User is referring: Berlin to visit
User is referring: Lubeck to stay

```

正如我们在这些例子中看到的，依赖解析使得理解用户所指的内容变得非常容易。我们看到，在两个不同任务的情况下，我们可以很好地计算出期望，并在此基础上，制定下一个响应。

#### 聊天机器人中的依赖解析有什么用？

从零开始构建聊天机器人时，依赖解析是最重要的部分之一。当你想弄清楚用户向聊天机器人输入的文本的含义时，这就变得更加重要了。可能会有这样的情况，你没有训练你的聊天机器人，但你仍然不想失去你的客户或像一个哑机器一样回复。在这些情况下，依赖解析确实有助于找到关系，并解释更多关于用户可能要求的内容。

如果我们要列出依赖关系解析有所帮助的事情，有些可能是:

*   它有助于发现语法正确的句子的单词之间的关系。

*   它可以用于句子边界检测。

*   发现用户是否同时谈论一个以上的上下文是非常有用的。

你一定想知道，如果你的机器人用户说了任何语法不正确的句子，或者在输入一些东西时使用了任何短信，那该怎么办？正如在第 [1](1.html) 章中所讨论的，你必须对这些情况保持谨慎，并相应地使用 NLP 技术来处理它们。

你需要编写自己的自定义 NLP 来理解用户或聊天机器人的上下文，并在此基础上识别用户可能犯的语法错误。

总而言之，您必须准备好应对用户输入垃圾值或语法错误句子的情况。您不能一次处理所有这样的场景，但是您可以通过添加自定义 NLP 代码或通过设计限制用户输入来不断改进您的聊天机器人。

### 名词块

名词组块或 NP 组块基本上是“基本名词短语”我们可以说它们是以名词为中心的扁平短语。你可以把名词块想象成一个名词和描述这个名词的词。

让我们试着举个例子，更好地理解它。

**例 1:**

```py
doc = nlp(u"Boston Dynamics is gearing up to produce thousands of robot dogs")
list(doc.noun_chunks)

```

**输出:**

```py
[Boston Dynamics, thousands, robot dogs]

```

尽管从给定的句子中获取名词块很有帮助，spaCy 还提供了其他有用的属性。让我们试着探索其中的一些。

**例 2:**

```py
doc = nlp(u"Deep learning cracks the code of messenger RNAs and protein-coding potential")
for chunk in doc.noun_chunks:
    print(chunk.text, chunk.root.text, chunk.root.dep_,
          chunk.root.head.text)

```

**输出:**

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
| 

**正文**

 | 

**根。正文**

 | 

**根。DEP_**

 | 

**根。头.正文**

 |
| --- | --- | --- | --- |
| **深度学习** | 学问 | nsubj | 裂缝 |
| **代码** | 密码 | 扔过来 | 裂缝 |
| **信使核糖核酸** | royal naval air station 皇家海军航空兵基地 | 比吉 | 关于 |
| **蛋白质编码潜能** | 潜在的 | 连词 | royal naval air station 皇家海军航空兵基地 |

从这个表中我们可以看到，我们得到了名词块和它们的属性。下表将帮助您理解每一列。

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

**栏**

 | 

**意为**

 |
| --- | --- |
| **正文** | 原始名词块的文本 |
| **根文本** | 连接名词块和剩余语法分析的原始单词的文本 |
| 根 dep | 连接根和头的依赖关系 |
| **根头文字** | 根令牌头的文本 |

### 寻找相似性

寻找两个词之间的相似性是一个用例，你会发现大部分时间都在使用 NLP。有时发现两个词是否相似变得相当重要。在构建聊天机器人时，你会经常遇到这样的情况，你不仅要找到看起来相似的单词，还要找到两个单词在逻辑上有多接近。

spaCy 使用高质量的单词向量，使用*手套算法*(单词表示的全局向量)来查找两个单词之间的相似性。

GloVe 是一种无监督学习算法，用于获取单词的矢量表示。GloVe 算法使用来自语料库的聚合的全局单词-单词共现统计来训练模型。

让我们尝试使用令牌的 *vector* 属性来查看 spaCy 的向量内部的实际值。

```py
doc = nlp(u'How are you doing today?')
for token in doc:
    print(token.text, token.vector[:5])

```

**输出:**

```py
(u'How', array([-0.29742685,  0.73939574, -0.04001453,  0.44034013,  2.8967502 ],      dtype=float32))(u'are', array([-0.23435134, -1.6145049 ,  1.0197453 ,  0.9928169 ,  0.28227055],      dtype=float32))(u'you', array([ 0.10252178, -3.564711  ,  2.4822793 ,  4.2824993 ,  3.590245  ],      dtype=float32))(u'doing', array([-0.6240922 , -2.0210216 , -0.91014993,  2.7051923 ,  4.189252  ],      dtype=float32))(u'today', array([ 3.5409122 , -0.62185854,  2.6274266 ,  2.0504875 ,  0.20191991],      dtype=float32))(u'?', array([ 2.8914998 , -0.25079122,  3.3764176 ,  1.6942682 ,  1.9849057 ],      dtype=float32))

```

看到这个输出，没有太大的意义和意义。从应用程序的角度来看，最重要的是不同单词的向量有多相似——也就是单词本身的意思。

为了找到空间中两个词之间的相似性，我们可以做以下工作。

**例 1:**

```py
hello_doc = nlp(u"hello")
hi_doc = nlp(u"hi")
hella_doc = nlp(u"hella")
print(hello_doc.similarity(hi_doc))
print(hello_doc.similarity(hella_doc))

```

**输出:**

```py
0.7879069442766685
0.4193425861242359

```

如果你看到单词 *hello，*它与单词 *hi* 更相关和相似，尽管单词 *hello* 和 *hella 之间只有一个字符的区别。*

> 让我们再举一个句子的例子，了解 spaCy 是如何进行相似性比较的。还记得我们前面章节中的《权力的游戏》的例子吗？我们将尝试这样做，看看相似之处。

**代码:**

```py
GoT_str1 = nlp(u"When will next season of Game of Thrones be releasing?")
GoT_str2 = nlp(u"Game of Thrones next season release date?")
GoT_str1.similarity(GoT_str2)

```

**输出:**

```py
0.785019122782813

```

正如我们在这个例子中看到的，两个句子之间的相似度约为 79%，这足以说两个句子非常相似，这是真的。这可以帮助我们在构建聊天机器人*时节省大量编写自定义代码的时间。*因此，我们得出一个事实，spaCy 使用单词向量给我们两个单词之间有意义的相似性，而不仅仅是看它们的拼写或字母。

我们将举一个非常简单的例子，试图找出单词之间的相似性。

```py
example_doc = nlp(u"car truck google")

for t1 in example_doc:
    for t2 in example_doc:
        similarity_perc = int(t1.similarity(t2) * 100)
        print "Word {} is {}% similar to word {}".format(t1.text, similarity_perc,  t2.text)

```

**输出:**

```py
Word car is 100% similar to word car
Word car is 71% similar to word truck
Word car is 24% similar to word google
Word truck is 71% similar to word car
Word truck is 100% similar to word truck
Word truck is 36% similar to word google
Word google is 24% similar to word car
Word google is 36% similar to word truck
Word google is 100% similar to word google

```

当我们打算构建任何非常依赖于 NLP 实现的应用程序时，寻找单词或句子之间的相似性变得非常重要。如果你曾经使用过 StackOverFlow，每当我们试图提出一个新的问题时，它都会试图列出平台上已经问过的类似问题。这是最好的例子之一，在这种情况下，寻找两组句子之间的相似之处可能会有所帮助。spaCy 基于一个已经训练好的模型找到两个词之间相似性的信心纯粹取决于所采取的一般假设的种类。

在构建聊天机器人时，查找相似性对于以下情况非常方便:

*   当构建聊天机器人进行推荐时

*   删除重复项

*   构建拼写检查器

我们学到的这些东西在构建聊天机器人时非常重要，这样我们就知道如何解析用户输入，以便在代码中编写业务逻辑时它们有意义。

## 聊天机器人了解自然语言处理方面的知识很好

在这一节中，我们将了解几个有趣的主题，当您计划编写自己的自定义 NLP 方法来处理某些场景时，这些主题经常会派上用场。确保你看完它们，因为当你最不期待的时候，它是最需要的。我们将简要讨论在聊天机器人场景中正则表达式的标记化和使用。

### 标记化

标记化是 NLP 的一个简单而基本的概念，我们将文本分割成有意义的片段。spaCy 首先对文本进行标记(即，将其分割成单词，然后是标点符号和其他东西)。您可能会想到一个问题:为什么我不能使用 Python 语言内置的 *split* 方法来进行标记化呢？Python 的 split 方法只是一个原始的方法，在给定分隔符的情况下将句子分割成标记。它不考虑任何意义，而标记化也试图保留意义。

让我们尝试一些代码，看看标记化是如何工作的。

**例 1:**

```py
doc = nlp(u'Brexit is the impending withdrawal of the U.K. from the European Union.')
for token in doc:
    print(token.text)

```

**输出:**

```py
Brexit
is
the
impending
withdrawal
of
the
U.K.
from
the
EuropeanUnion

```

如果您在上面的输出中看到，在标记化过程之后，U.K .作为一个单词出现，这是有意义的，因为 U.K .是一个国家名称，拆分它是错误的。即使在这之后，如果您对 spaCy 的标记化不满意，那么在完全依赖 spaCy 的标记化方法之前，您可以使用它的`add_special_case` case 方法来添加您自己的规则。

### 正则表达式

您必须已经了解正则表达式及其用法。这本书假设你必须熟悉一般的正则表达式。在本节中，我们将浏览一些例子，看看正则表达式在构建聊天机器人时是如何有益和有用的。

文本分析和处理本身就是一个大课题。有时候，单词以一种让机器极难理解和训练的方式组合在一起。

正则表达式可以方便地用于机器学习模型的一些回退。它具有模式匹配的能力，可以确保我们正在处理的数据是正确的还是不正确的。在“聊天机器人的历史”一节的第 [1](1.html) 章中讨论的大多数早期聊天机器人都非常依赖模式匹配。

让我们举两个简单易懂的例子。我们将尝试使用正则表达式从两个句子中提取信息。

帮我订一辆从机场站到香港站的地铁。

帮我订一辆从香港机场到亚洲国际博览馆的出租车。

代码如下:

```py
sentence1 = "Book me a metro from Airport Station to Hong Kong Station."
sentence2 = "Book me a cab to Hong Kong Airport from AsiaWorld-Expo."

import re
from_to = re.compile('.* from (.*) to (.*)')
to_from = re.compile('.* to (.*) from (.*)')

from_to_match = from_to.match(sentence2)
to_from_match = to_from.match(sentence2)

if from_to_match and from_to_match.groups():
    _from = from_to_match.groups()[0]
    _to = from_to_match.groups()[1]
    print("from_to pattern matched correctly. Printing values\n")
    print("From: {}, To: {}".format(_from, _to))

elif to_from_match and to_from_match.groups():
    _to = to_from_match.groups()[0]
    _from = to_from_match.groups()[1]
    print("to_from pattern matched correctly. Printing values\n")
    print("From: {}, To: {}".format(_from, _to))

```

**输出:**

```py
to_from pattern matched correctly. Printing values
From: AsiaWorld-Expo., To: Hong Kong Airport

```

尝试将`sentence2`改为`sentence1`,看看代码是否能很好地识别模式。鉴于目前机器学习的能力，正则表达式和模式匹配已经后退了一步，但请确保您对它有所了解，因为它可能随时需要从单词、句子或文本文档中解析特定细节。

## 摘要

在这一点上，你必须有公平的想法，为什么我们需要知道 NLP 之前，开始建立聊天机器人。在本章中，我们学习了 Python 中的 spaCy 模块，它的特性，以及如何安装它。我们深入研究了 NLP 的各种方法，这些方法在构建聊天机器人时被广泛使用。我们学习了词性标注、词干化和词条化之间的区别、实体识别、名词组块以及寻找单词集之间的相似性。

我们执行了所有这些概念的代码，并通过实践而不仅仅是阅读来学习所有这些，这正是本书所强调的。我们还复习了记号化和正则表达式的基础知识。我们将在下一章使用一个免费的工具 Dialogflow 来构建我们的聊天机器人。我们将在下一章学习如何训练我们的聊天机器人理解并提取用户给出的信息。

<aside class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

[T2`https://catalog.ldc.upenn.edu/ldc2013t19`](https://catalog.ldc.upenn.edu/ldc2013t19)

 </aside>**