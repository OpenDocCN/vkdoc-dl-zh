# 7.结论和下一代自然语言处理

本章总结了各种过去、现在和未来的自然语言处理方法和技术。本章以关于 NLP 和深度学习的优秀研究论文的信息结束。

到目前为止，您已经了解了以下内容。

*   如何收集、读取、清理和处理文本数据

*   如何将文本转换为特征

*   如何使用 NLP 技术结合机器学习和深度学习来构建应用程序

现在让我们看看 NLP 的最新进展和未来。

我们讨论以下食谱。

*   食谱 1。文本到要素或分布式表示的最新进展

*   食谱 2。面向自然语言处理的高级深度学习

*   配方 3:强化学习在 NLP 中的应用

*   食谱 4。转移学习和预培训模型

*   食谱 5。自然语言处理中的元学习

*   食谱 6。面向自然语言处理的胶囊网络

但是在开始食谱之前，让我们快速回顾一下。

到目前为止，你已经在人工智能和 NLP 的保护伞下解决了一些最有趣的项目。

您还看到了 NLP 在与机器学习和深度学习相结合时，如何帮助解决跨行业和领域的复杂业务问题。

大约 50 年前，人类首次开始使用计算方法来分析人类语言，尽管这些技术中的大多数最近都取得了成功。

NLP 是 Siri 和 Alexa 背后的声音。类似地，客户服务聊天机器人利用 NLP 的力量在电子商务、医疗保健和公用事业部门生成个性化的响应。一些最流行的 NLP 应用程序是虚拟助手、情感分析、客户服务和翻译。

随着技术的发展，NLP 的未来变得更加以用户为中心。例如，虚拟助理可以回答更复杂的问题，并评估问题的含义和字面意义。(问:今天天气怎么样？甲:下雨了。你需要一把伞。)未来，公司将能够提供各种专业的客户服务，接听电话，向真人传递问题。

NLP 的应用不仅限于解决客户问题或提供个性化建议；这主要是技术援助。目前，如果你用 NLP 问，“我的网络怎么了？”，您可以训练它提供一个错误列表。未来，NLP 将能够理解用户的真实意图。NLP 的未来令人兴奋，因为 NLP 的进步将使人类从问题转向结果。当 NLP 理解用户评论并为他们的真实意图提供更复杂的解决方案时，这将是一个巨大的飞跃。

NLP 将能够理解人类的情感。随着 NLP 技术的发展，计算机将把它们目前的处理能力扩展到对人类语言的整体理解。到目前为止，NLP 仅限于解释有限的人类情感，包括喜悦或愤怒。最终，NLP 将被编程为理解更复杂的人类语言元素，如幽默、讽刺等等。

在一个激动人心的时刻，NLP 将结合人脸和手势识别等其他技术为企业创造收入，使其更加灵活和高效。

有了 Alexa、Siri 和 Google Duplex，下一代 NLP 才刚刚开始。

NLP 对于教导机器执行复杂的自然语言任务也是有用的，例如机器翻译和对话生成。

现在，让我们通过菜谱来揭示一些最先进的下一代算法将如何在未来的 NLP 时代发挥重要作用。

## 配方 7-1。文本到要素或分布式表示的最新进展

本食谱讨论了文本到特征或分布式表示的最新进展。

### 问题

在文本到特征或分布式表示方面，有哪些超出你所学的新进展(例如，单词嵌入、GloVe、fastText 等)。)?

### 解决办法

分布式表示在过去已经被广泛用于研究各种 NLP 任务，但是 CBOW 和 skip-gram 模型的流行度已经增加。第 [3](3.html) 章讨论了大多数最新的嵌入技术。但是请保持警惕，因为这个领域正在快速发展。

## 配方 7-2。面向自然语言处理的高级深度学习

这个食谱着眼于 NLP 的一些高级深度学习技术。

### 问题

你想了解 NLP 深度学习技术的最新进展。

### 解决办法

让我们讨论循环神经网络和深度生成模型。

#### 循环神经网络

循环神经网络的基本形式，网络函数将组件向上组合，以计算更高级句子的表示。循环神经网络用于各种应用中，如下所示。

*   从语法上分析

*   使用短语级表示的情感分析

*   语义关系的分类(例如，主题消息)

#### 深度生成模型

变分自动编码器(VAE)和生成对抗网络(GAN)等深度生成模型被应用于自然语言处理，以发现丰富的自然语言结构。众所周知，由于不受限制的潜在空间，标准句子自动编码器不能生成真实的句子。VAE 执行隐藏空间的先验分布，允许模型生成适当的样本。VAE 由编码到潜在空间的编码器和生成器网络组成，然后从该空间生成样本。训练旨在最大化在生成的模型中观察到的数据的对数概率的变化下限。

## 配方 7-3。强化学习在自然语言处理中的应用

这个食谱讨论了强化学习在 NLP 中的应用。

### 问题

你想了解强化学习在 NLP 领域的作用。

### 解决办法

强化学习利用行为心理学，软件代理在环境中执行操作，增加代理的累积回报。该系统试图通过在模拟器环境中反复试验来理解行为随时间的变化。

我们来讨论一下强化学习方法。

#### 勘探与开发的权衡

在这里，代理必须对文本进行分类。智能体通过不同的神经网络层来决定下一步的动作状态。探索和利用之间的权衡涉及到代理必须探索可能的行为状态的困境，这将有助于合法地对文本进行分类，并利用当前的行为状态来获得最佳结果。在 NLP 中，我们应该通过使用 softmax 函数计算置信限来解决这个问题。即使存在未知的不确定性，softmax 和更高的置信限也有助于获得最高的回报。

#### 时间差异

时差概念涉及一种无模型的强化学习方法。它是基于时间 *t* + 1 的下一个行动状态可能比时间 t 的结果更好的概念，这就像蒙特卡罗方法。您可以根据结果调整权重。但是在这里，使用称为启动的概念，在结果已知之前，您根据当前输入调整结果以获得最佳结果。例如，您希望对所有未知数据进行排名。在时间 *t* + 1，你应该可以预测标签，同理，在 *t* + 2，你应该可以确定下一个动作状态。下一步是您应该能够对属于该标签的所有数据进行分类，并通过行为克隆找到所有其他类似的标签。

## 配方 7-4。转移学习和预培训模型

这份食谱讨论了迁移学习和预训练模型是如何改变 NLP 前景的。

### 问题

您希望深入了解迁移学习和 NLP 预训练模型的最新进展。

### 解决办法

简单来说，迁移学习就是在大规模数据集上形成一个模型，然后用这个预先训练好的模型去学习另一个下游任务(如目标任务)的过程。通过 ImageNet 数据集，迁移学习在计算机视觉领域获得了广泛的应用。在这里，我们不讨论计算机视觉。相反，让我们关注这些概念如何应用于自然语言处理领域。

迁移学习旨在使用源领域中有价值的知识来帮助模拟目标领域中的表现。

#### 为什么需要迁移学习 NLP？

在 NLP 应用中，特别是当我们没有足够的数据集来解决任务(称为 *T-target tasks* )时，我们希望从其他任务中转移知识，以避免过拟合，提高 T 的性能。

将知识转移到语义相似/相同但数据集不同的任务。

NLP 中神经传递的学习很大程度上取决于源数据集和目标数据集的语义相似度。

对于 NLP 和 ML 的研究人员和爱好者来说，这些都是激动人心的时刻。最近，预训练的语言模型在广泛的 NLP 任务中获得了最新的结果，例如序列标签和句子分类。最近使用预训练语言模型的工作包括 ULMFit、ELMo、GLoMo 和 OpenAI 转换。这些语言建模系统使用分层表示或图形表示来执行整个模型的大量预训练。这个概念改变了多年来用于处理许多 NLP 任务的一元嵌套单词的使用，支持更复杂和抽象的表示。

NLP 中的前期训练是如何进行的？为什么前期培训有用？它允许模型从大规模语料库中捕捉和学习各种语言现象，如长期依赖和否定。然后使用(转移)这些知识来初始化，然后形成另一个模型来执行特定的 NLP 任务，例如情感分类。

这在 NLP 中是可行的；例如，否定是从文本信息中检测情感极性的重要属性。此外，否定也可能是有用的，例如，检测情绪或讽刺，这是最复杂和未解决的 NLP 任务之一。否定在许多 NLP 任务中是有用的，因此预训练模型具有共同的属性。

在 NLP 研究中缺少带注释的数据集或语言资源时，具有通用属性的语言模型可能是有用的。这个想法令人兴奋，因为我们正试图建立一个通用模型，并解决 NLP 研究的一些困难挑战:数据和语言资源的可用性。

到目前为止，我们知道从预先形成的语言模型(如嵌入单词的形式)中获得的知识适用于许多 NLP 任务。这里的问题是，这种潜在的知识形式的知识不够广泛或不足以正确地执行目标任务或下游任务。对此有许多解释——有些事情我们知道，有些我们不知道——但现在，我们简要介绍一种解决这些限制的最新方法。

ELMo 是一种最近流行的方法，它被描述为“使用跨神经层堆栈的深度上下文表示来预训练整个模型”，而不是简单地使用嵌套单词(一键编码特征表示)作为初始化。

**伯特、埃尔莫、乌尔姆菲特。**(NLP 破解迁移学习的方式)

2018 年是基于文本的机器学习模型(更准确地说，是自然语言处理或 NLP)的转折点。我们对如何最好地表达单词和短语以更好地理解潜在含义和关系的概念理解正在迅速发展。此外，NLP 社区强调，你可以免费下载所有像 BERT 这样的预训练模型，并在你自己的模板和管道(例如 ImageNet)中使用它们，这表明了类似的开发如何加速了具有令人难以置信的强大组件的机器学习的发展。

这一发展的最新里程碑之一是 BERT 的发布，它被描述为标志着 NLP 新时代的开始。BERT 是一个模型，它打破了一些记录，并确定了模型如何处理基于语言的任务。在模型文档发布后不久，该团队还开放了模型代码，将预先训练好的模型版本下载到大型数据集。这是一个突破，任何建立涉及语言处理的机器学习模型的人都可以将这个强大的工具作为现成的组件使用——节省时间和可以形成语言处理模型的精力、知识和资源。

BERT 基于 NLP 社区中出现的一些最新发展，包括半监督学习序列(Andrew Wells Dale)、ELMo (Matthew Peters 和研究人员 AI2 和 UW CSE)、ULMFiT(由创始人 Fast.ai、杰瑞米·霍华德和塞巴斯蒂安·罗德斯开发)、OpenAI transformer(研究人员 OpenAI Redford、Nara Singham 和 Salimans Sutskever)和 transformers (Vaswani 等)。).

你需要了解一些概念才能理解 BERT 是什么。因此，在查看模型本身所涉及的概念之前，让我们先看看使用 BERT 的不同方式。

BERT 是一个转换器编码器堆栈，它有两种型号。

*   BERT-Base: OpenAI 变压器

*   BERT-Large:一个非常大的模型，具有最先进的结果

两种 BERT 模型都有许多编码器层(也称为变换块):12 的基本版本和 24 的大型版本。它们还具有更高的预测网络和更多的注意力头(分别为 12 和 16)，而不是基础纸上的 transformer 实现的默认配置(分别为 768 和 1024 个隐藏单元)(六层编码器、512 个隐藏单元和八个注意力头)。

#### 嵌入的新时代

这些新的发展导致了单词编码方式的新变化。到目前为止，主流 NLP 模型如何对待语言，嵌入词一直是主力。word2vec 和 Glove 等方法已广泛用于此类任务。在指出变化之前，让我们回顾一下它们的用法。

该领域很快意识到，使用大量文本数据比形成与这种通常很小的数据集模型并行的预集成要好。因此，word2vec 或 GloVe 可以下载预训练期间生成的单词列表及其组合。

#### ULMFiT:自然语言处理中的迁移学习

ULMFiT 引入了一种有效利用模型在预训练期间学习的大部分内容的方法——这不仅仅是情境化的组合。ULMFiT 使用语言模型和过程来有效地使语言模型适应各种任务。

NLP 最终会找到让迁移学习成为可能的方法，就像计算机视觉一样。

#### 变形金刚:超越 LSTM

《变形金刚》文档、代码和机器翻译的发布让一些人相信他们正在取代 LSTM。此外,《变形金刚》在管理长期依赖关系方面比 LSTM 更胜一筹。

#### 天资

flair 框架在解决 NER (POS)、词义消歧和文本分类等自然语言处理问题方面提供了一流的性能。这个 NLP 框架直接建立在 PyTorch 之上。

当今大多数先进的方法都是基于一种叫做文本嵌入的技术。它将文本转换成大空间的数字表示。它允许文档、句子、单词、字符在这个大空间中作为向量来表达它们自己。

flair 是 NLP 的一个令人兴奋的新成员，因为 Zalando Research 最近的文章“用于序列标记的上下文字符串嵌入”( [`http://alanakbik.github.io/papers/coling2018.pdf`](http://alanakbik.github.io/papers/coling2018.pdf) )描述了一种总是比尖端解决方案更好的方法。它在 flair 中得到实现和完全支持，可以创建文本分类器。

#### 为什么是伯特？

随着网络的大规模增长，我们有了大量的数据。并且只有一些文本数据被注释。对于像自然语言处理这样的任务，我们需要大量带注释的数据来学习有监督的或无注释的数据来进行无监督学习。各种研究人员更喜欢无监督学习。他们在网上使用大量无注释的文本(称为预训练)来突出一些常见的语言表示模型训练技术。

BERT 是谷歌开发的这些预训练模型之一，可以适应新的数据，并创建 NLP 系统，如回答问题，生成文本，排序文本，文本合成和情感分析。由于 BERT 是在大量数据的基础上形成的，它简化了语言建模过程。使用预先形成的 BERT 模型的主要优点是，与这些数据集的训练相比，精确度显著提高。

BERT 是基于在训练之前在上下文表示中的最近工作。这是先前使用纯文本语料库形成的第一个深度双向无监督语言表示。BERT 表示具有左右上下文的上下文表示。概念简单，经验丰富。BERT 优于其他方法，因为它是第一个具有域自适应特征的用于 NLP 预训练的无监督深度双向系统。从 BERT 文档中可以确定，通过适当的语言模型训练方法，基于转换器(自关注)的编码器可以潜在地用作语言模型的替代。

#### 伯特和 RNN

RNN(理论上)给出了左边的无限上下文(目标词左边的词)。但是我们可能想要的是使用每一个左右的上下文来看看这个单词是否适合这个句子。

RNN 是一个用于翻译和顺序语言处理的网络架构。顺序性使得并行处理单元(例如 TPU)的全部功能变得困难。RNN 遇到了梯度问题，消失并爆炸。RNN 有短期记忆，因为长时间保存条目不好。

#### BERT 诉 LSM 案

LSTM 模型的使用限制了短期预测的能力。BERT 使用屏蔽语言建模(MLM)。MLM 目标允许左右上下文的表示，这允许预先形成深度双向变换器。

#### 伯特 vs .OpenAI GPT(全球定位系统)

当您将精确调优方法应用于令牌级任务时(比如回答 SQuAD 的问题)，将上下文集成到两个方向是很重要的。当使用 OpenAI GPT 时，它使用内置架构。从左到右，不能访问每个令牌。前一个令牌在变形金刚的焦点层。

*   GPT 使用短语分隔符([SEP])和分类标记([CLS])，它们只在特定时间输入。

*   伯特在整个预训练过程中学习[SEP](特殊标记)、[CLS](类别标记)句子和句子的整合。

*   GPT 对所有的拟合实验都使用类似的 5 到 5 的学习率。BERT 在开发集上选择精确的、特定任务的和最有效的开发学习率。

以下是一些挑战。

*   因为我们有大量的训练数据，即使用一个 GPU 也很难训练；因此，可以使用谷歌 TPU。

*   推理需要的时间长。因此，我们修改了超参数，以使系统准确并尽快得到结果。它对每个超参数都有一个记录，并选择了超参数的优化组合。

## 配方 7-5。自然语言处理中的元学习

这个食谱讨论了自然语言处理中的元学习。

### 问题

你想了解自然语言处理中的元学习。

### 解决办法

在自然语言处理(NLP)中有一个有趣的东西，即循环神经网络(RNN)，用于元学习和神经网络模型。

优化神经网络模型的元学习者的行为与循环神经网络的行为相同。作为一个 RNN，训练时模型的一系列参数和梯度作为输入序列。序列是来自计算串行输出(更新模型参数集)的输入。

我们发现元学习语言模型可以形成记忆最近条目的文章，并且是预测文章下一部分的有用起点。

## 配方 7-6。面向自然语言处理的胶囊网络

这个食谱着眼于 NLP 的胶囊网络。

### 问题

你想了解一下 NLP 的胶囊网络。

### 解决办法

让我们看看研究人员在将胶囊网络应用于 NLP 任务时发现了什么。

首先，你需要理解下面的层和算法。

*   **n-gram 卷积**层是标准的卷积层，通过各种卷积滤波器提取句子中不同位置的 n-gram 特征。

*   **主胶囊层**为第一胶囊层。该胶囊用矢量输出胶囊代替 CNN 标量输出特征检测器，以保留诸如单词的局部顺序和单词的语义表示等参数。

*   在**卷积胶囊**层中，每个胶囊仅连接到下层中的局部区域。区域中的这些胶囊乘以一个变换矩阵来学习父子关系。然后执行弦路由以在该层中创建父胶囊。胶囊层完全连接。层下面的胶囊在胶囊列表中被展平，并被引入完全连接的胶囊层。该封装通过一个转换矩阵，然后通过一个路由协议产生一个倍增的最终封装及其类别。

*   动态路由的基本思想是设计非线性映射。通过确保每个胶囊的输出被发送到下一层中适当的父节点，非线性映射被迭代地构造。对于每个潜在的父代，胶囊网络可以增加或减少与动态路由的连接，这比原始路由策略更有效，例如 CNN 中的最大累积，它检测文本中是否存在某个特征。但是，关于实体的空间信息会丢失。研究人员探索了三种策略，通过减少一些嘈杂胶囊的不便来提高路由过程的准确性。不与特定类别相关联的词有助于胶囊网络更有效地建立亲子关系模型。

研究人员已经证明了胶囊网络在文本分类中的有效性。更重要的是，胶囊网络还显示了从多类到多标签标签的文本分类的显著改进。

#### NLP 中的多任务处理

多任务可以在相关任务之间共享知识，并隐式增加任务中的训练数据。研究人员已经探索了文本胶囊网络的性能，并提供了一种统一、简单、高效的多任务胶囊式学习架构。

多任务处理(MTL)在自然语言处理领域取得了巨大成功。多任务和深度神经网络(DNN)通过标准化对 DNN 产生另一种协同效应。

胶囊网络可以在 MTL 中用来区分任务的特征。

要深入了解 NLP 中的多任务处理，请参考 [`www.aclweb.org/anthology/D18-1486`](http://www.aclweb.org/anthology/D18-1486) 的“MCapsNet:多任务学习的文本胶囊网络”。

感谢您的阅读。我们相信你有一个伟大的学习之旅。这是我们目前所有的。下一期再见。