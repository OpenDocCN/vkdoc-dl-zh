# 1.提取数据

本章涵盖了文本数据的各种来源以及提取文本数据的方法。文本数据可以为企业提供信息或见解。涵盖以下食谱。

*   食谱 1。使用 API 收集文本数据

*   食谱 2。用 Python 读取 PDF 文件

*   食谱三。阅读 Word 文档

*   食谱 4。读取 JSON 对象

*   食谱 5。读取 HTML 页面和 HTML 解析

*   食谱 6。正则表达式

*   食谱 7。字符串处理

*   食谱 8。网页抓取

## 介绍

在进入这本书的细节之前，让我们看看普遍可用的数据来源。我们需要确定能够帮助解决数据科学用例的潜在数据源。

## 客户数据

对于任何问题陈述，来源之一是已经存在的数据。企业决定要将数据存储在哪里。数据存储取决于业务类型、数据量以及与数据源相关的成本。以下是一些例子。

*   SQL 数据库

*   分布式文件系统

*   云存储

*   平面文件

## 免费资源

互联网上有大量的免费数据。你只需要简化问题，并开始探索多个免费的数据源。

*   像 Twitter 这样的免费 API

*   维基百科(一个基于 wiki 技术的多语言的百科全书协作计划ˌ也是一部用不同语言写成的网络百科全书ˌ 其目标及宗旨是为全人类提供自由的百科全书)ˌ开放性的百科全书

*   政府数据(如 [`http://data.gov`](http://data.gov) )

*   人口普查数据(如 [`www.census.gov/data.html`](http://www.census.gov/data.html) )

*   医疗保健索赔数据(如 [`www.healthdata.gov`](http://www.healthdata.gov) )

*   数据科学社区网站(如 [`www.kaggle.com`](http://www.kaggle.com) )

*   谷歌数据集搜索(如 [`https://datasetsearch.research.google.com`](https://datasetsearch.research.google.com) )

## 网页抓取

使用 Python 中的网络抓取包，从网站、博客、论坛和零售网站中提取内容/数据，以便在获得相应来源的许可后进行审查。

还有很多其他来源，如新闻数据和经济数据，可以用来进行分析。

## 配方 1-1。收集数据

有很多免费的 API，你可以通过它们收集数据，并用它来解决问题。让我们讨论一下 Twitter API。

### 问题

您希望使用 Twitter APIs 收集文本数据。

### 解决办法

Twitter 拥有海量数据，其中蕴含着巨大的价值。社交媒体营销人员以此为生。每天都有大量的推文，每条推文都有故事要讲。当所有这些数据被收集和分析时，它给企业提供了关于他们的公司、产品、服务等等的巨大洞察力。

现在让我们看看如何提取数据，然后在接下来的章节中探讨如何利用它。

### 它是如何工作的

#### 步骤 1-1。登录 Twitter 开发者门户

在 [`https://developer.twitter.com`](https://developer.twitter.com) 登录 Twitter 开发者门户。

在 Twitter 开发者门户中创建自己的应用，并获得以下密钥。一旦有了这些凭证，就可以开始提取数据了。

*   消费者密钥:与应用程序(Twitter、脸书等)相关联的密钥。)

*   消费者秘密:用于向认证服务器(Twitter、脸书等)认证的密码。)

*   访问令牌:成功认证密钥后给予客户端的密钥

*   访问令牌密码:访问密钥的密码

#### 步骤 1-2。在 Python 中执行查询

一旦所有的凭证都准备好了，就使用下面的代码来获取数据。

```py
# Install tweepy
!pip install tweepy
# Import the libraries
import numpy as np
import tweepy
import json
import pandas as pd
from tweepy import OAuthHandler
# credentials

consumer_key = "adjbiejfaaoeh"
consumer_secret = "had73haf78af"
access_token = "jnsfby5u4yuawhafjeh"
access_token_secret = "jhdfgay768476r"
# calling API

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth)
# Provide the query you want to pull the data. For example, pulling data for the mobile phone ABC
query ="ABC"
# Fetching tweets

Tweets = api.search(query, count = 10,lang='en',exclude='retweets',tweet_mode='extended')

```

当搜索产品 ABC 时，该查询提取前十条推文。API 提取英语推文，因为给定的语言是`'en'`。它不包括转发。

## 配方 1-2。从 pdf 收集数据

您的大部分数据存储在 PDF 文件中。您需要从这些文件中提取文本，并存储它以供进一步分析。

### 问题

你想阅读 PDF 文件。

### 解决办法

阅读 PDF 文件最简单的方法是使用 PyPDF2 库。

### 它是如何工作的

按照本节中的步骤从 PDF 文件中提取数据。

#### 步骤 2-1。安装并导入所有必需的库

这是第一行代码。

```py
!pip install PyPDF2
import PyPDF2
from PyPDF2 import PdfFileReader

```

Note

您可以从网上下载任何 PDF 文件，并将其放在运行这个 Jupyter 笔记本或 Python 脚本的位置。

#### 第 2-2 步。从 PDF 文件中提取文本

现在我们来摘抄课文。

```py
#Creating a pdf file object
pdf = open("file.pdf","rb")
#creating pdf reader object
pdf_reader = PyPDF2.PdfFileReader(pdf)
#checking number of pages in a pdf file
print(pdf_reader.numPages)
#creating a page object
page = pdf_reader.getPage(0)
#finally extracting text from the page
print(page.extractText())
#closing the pdf file
pdf.close()

```

请注意，该功能不适用于扫描的 pdf。

## 配方 1-3。从 Word 文件中收集数据

接下来我们来看另一个用 Python 读取 Word 文件的小菜谱。

### 问题

你想读 Word 文件。

### 解决办法

最简单的方法是使用 docx 库。

### 它是如何工作的

按照本节中的步骤从 Word 文件中提取数据。

#### 步骤 3-1。安装并导入所有必需的库

下面是安装和导入 docx 库的代码。

```py
#Install docx
!pip install docx
#Import library
from docx import Document

```

Note

您可以从网上下载任何 Word 文件，并将其放在运行 Jupyter 笔记本或 Python 脚本的位置。

#### 第 3-2 步。从 Word 文件中提取文本

现在让我们来读课文。

```py
#Creating a word file object
doc = open("file.docx","rb")
#creating word reader object
document = docx.Document(doc)
#create an empty string and call this document. #This document variable stores each paragraph in the Word document.
#We then create a "for" loop that goes through each paragraph in the Word document and appends the paragraph.
docu=""
for para in document.paragraphs.
       docu += para.text
#to see the output call docu
print(docu)

```

## 配方 1-4。从 JSON 收集数据

JSON 是一种开放的标准文件格式，代表 *JavaScript 对象符号。*当数据从服务器发送到网页时经常使用。这个方法解释了如何读取 JSON 文件/对象。

### 问题

你想读取一个 JSON 文件/对象。

### 解决办法

最简单的方法是使用请求和 JSON 库。

### 它是如何工作的

按照本节中的步骤从 JSON 中提取数据。

#### 步骤 4-1。安装并导入所有必需的库

下面是导入库的代码。

```py
import requests
import json

```

#### 第 4-2 步。从 JSON 文件中提取文本

现在我们来摘抄课文。

```py
#extracting the text from "https://quotes.rest/qod.json"
r = requests.get("https://quotes.rest/qod.json")
res = r.json()
print(json.dumps(res, indent = 4))
#output
{
    "success": {
        "total": 1
    },
    "contents": {
        "quotes": [
            {
                "quote": "Where there is ruin, there is hope for a treasure.",
                "length": "50",
                "author": "Rumi",
                "tags": [
                    "failure",
                    "inspire",
                    "learning-from-failure"
                ],
                "category": "inspire",
                "date": "2018-09-29",
                "permalink": "https://theysaidso.com/quote/dPKsui4sQnQqgMnXHLKtfweF/rumi-where-there-is-ruin-there-is-hope-for-a-treasure",
                "title": "Inspiring Quote of the day",
                "background": "https://theysaidso.com/img/man_on_the_mountain.jpg",
                "id": "dPKsui4sQnQqgMnXHLKtfweF"
            }
        ],
        "copyright": "2017-19 theysaidso.com"
    }

}
#extract contents
q = res['contents']['quotes'][0]
q
#output
{'author': 'Rumi',
 'background': 'https://theysaidso.com/img/man_on_the_mountain.jpg',
 'category': 'inspire',
 'date': '2018-09-29',
 'id': 'dPKsui4sQnQqgMnXHLKtfweF',
 'length': '50',
 'permalink': 'https://theysaidso.com/quote/dPKsui4sQnQqgMnXHLKtfweF/rumi-where-there-is-ruin-there-is-hope-for-a-treasure',
 'quote': 'Where there is ruin, there is hope for a treasure.',
 'tags': ['failure', 'inspire', 'learning-from-failure'],
 'title': 'Inspiring Quote of the day'}
#extract only quote

print(q['quote'], '\n--', q['author'])
#output
It wasn't raining when Noah built the ark....
-- Howard Ruff

```

## 配方 1-5。从 HTML 收集数据

HTML 是超文本标记语言的简称。它构建网页并在浏览器中显示它们。有各种 HTML 标签来构建内容。这个食谱着眼于阅读 HTML 页面。

### 问题

你想读解析/读 HTML 页面。

### 解决办法

最简单的方法是使用 bs4 库。

### 它是如何工作的

按照本节中的步骤从 web 中提取数据。

#### 步骤 5-1。安装并导入所有必需的库

首先，导入库。

```py
!pip install bs4
import urllib.request as urllib2
from bs4 import BeautifulSoup

```

#### 第 5-2 步。获取 HTML 文件

你可以选择任何你想提取的网站。让我们在这个例子中使用维基百科。

```py
response = urllib2.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')
html_doc = response.read()

```

#### 第 5-3 步。解析 HTML 文件

现在我们来获取数据。

```py
#Parsing
soup = BeautifulSoup(html_doc, 'html.parser')
# Formating the parsed html file
strhtm = soup.prettify()
# Print few lines
print (strhtm[:1000])
#output
<!DOCTYPE html>
<html class="client-nojs" dir="ltr" lang="en">
 <head>
  <meta charset="utf-8"/>
  <title>
   Natural language processing - Wikipedia
  </title>
  <script>
   document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );
  </script>
  <script>

   (window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Natural_language_processing","wgTitle":"Natural language processing","wgCurRevisionId":860741853,"wgRevisionId":860741853,"wgArticleId":21652,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Webarchive template wayback links","All accuracy disputes","Articles with disputed statements from June 2018","Wikipedia articles with NDL identifiers","Natural language processing","Computational linguistics","Speech recognition","Computational fields of stud

```

#### 第 5-4 步。提取标签值

您可以使用以下代码从标记的第一个实例中提取标记的值。

```py
print(soup.title)
print(soup.title.string)
print(soup.a.string)
print(soup.b.string)
#output
 <title>Natural language processing - Wikipedia</title>
Natural language processing - Wikipedia
None
Natural language processing

```

#### 第 5-5 步。提取特定标签的所有实例

这里我们得到了我们感兴趣的标签的所有实例。

```py
for x in soup.find_all('a'): print(x.string)
#sample output
 None
Jump to navigation
Jump to search
Language processing in the brain
None
None
automated online assistant
customer service
[1]
computer science
artificial intelligence
natural language
speech recognition
natural language understanding
natural language generation

```

#### 第 5-6 步。从特定标签中提取所有文本

最后，我们得到文本。

```py
for x in soup.find_all('p'): print(x.text)
#sample output
Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.
Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.

The history of natural language processing generally started in the 1950s, although work can be found from earlier periods.
In 1950, Alan Turing published an article titled "Intelligence" which proposed what is now called the Turing test as a criterion of intelligence.

```

注意，`p`标签提取了页面上的大部分文本。

## 配方 1-6。使用正则表达式解析文本

这个菜谱讨论了正则表达式在处理文本数据时是如何有用的。当处理来自 web 的包含 HTML 标签、长文本和重复文本的原始数据时，正则表达式是必需的。在开发应用程序的过程中，以及在输出中，您不需要这样的数据。

您可以使用正则表达式进行各种基本和高级的数据清理。

### 问题

您希望使用正则表达式来解析文本数据。

### 解决办法

最好的方法是使用 Python 中的`re`库。

### 它是如何工作的

让我们看看在我们的任务中使用正则表达式的一些方法。

基本标志是 I，L，M，S，U，x。

*   `re.I`忽略大小写。

*   `re.L`找一个当地的家属。

*   `re.M`在多行中查找模式。

*   `re.S`查找点匹配。

*   `re.U`适用于 Unicode 数据。

*   以更易读的格式编写正则表达式。

下面描述正则表达式的功能。

*   查找出现一次的字符 a 和 b: `[ab]`

*   查找除 a 和 b 以外的字符:`[^ab]`

*   求 a 到 z 的字符范围:`[a-z]`

*   查找除 a 到 z `:` `[^a-z]`以外的字符范围

*   查找从 A 到 Z 和 A 到 Z 的所有字符:`[a-zA-Z]`

*   查找任意单个字符:`[]`

*   查找任何空白字符:`\s`

*   查找任何非空白字符:`\S`

*   查找任意数字:`\d`

*   查找任何非数字:`\D`

*   查找任何非单词:`\W`

*   查找任何单词:`\w`

*   查找 a 或 b: `(a|b)`

*   的出现次数要么为零，要么为一
    *   匹配零个或不超过一个事件`: a? ; ?`

    *   a 的出现次数为零或更多次`: a* ; * matches zero or more than that`

    *   a 出现一次或多次`: a+ ; + matches occurrences one or more than one time`

*   匹配三个同时出现的`:` `a{3` }

*   匹配三个或更多同时出现的 a: `a{3,}`

*   匹配三到六个同时出现的`:` `a{3,6}`

*   字符串的开头:`^`

*   字符串结尾:`$`

*   匹配单词边界:`\b`

*   非单词边界:`\B`

`re.match()`和`re.search()`函数查找模式，然后根据应用程序的要求进行处理。

我们来看看`re.match()`和`re.search()`的区别。

*   `re.match()`仅在字符串开头检查匹配。因此，如果它在输入字符串的开头找到一个模式，它将返回匹配的模式；否则，它返回一个名词。

*   `re.search()`检查字符串中任何地方的匹配。它在给定的输入字符串或数据中查找该模式的所有匹配项。

现在让我们看几个使用这些正则表达式的例子。

#### 符号化

记号化就是把一个句子拆分成单词。一种方法是使用`re.split`。

```py
# Import library
import re
#run the split query
re.split('\s+','I like this book.')
['I', 'like', 'this', 'book.']

```

关于正则表达式的解释，请参考主食谱。

#### 提取电子邮件 id

提取电子邮件 id 最简单的方法是使用`re.findall.`

1.  阅读/创建文档或句子。

1.  执行`re.findall`功能。

```py
doc = "For more details please mail us at: xyz@abc.com, pqr@mno.com"

```

```py
addresses = re.findall(r'[\w\.-]+@[\w\.-]+', doc)
for address in addresses.
    print(address)
#Output
xyz@abc.com
pqr@mno.com

```

#### 替换电子邮件 id

让我们将句子或文档中的电子邮件 id 替换为其他电子邮件 id。最简单的方法是使用`re.sub`。

1.  阅读/创建文档或句子。

1.  执行`re.sub`功能。

    ```py
    new_email_address = re.sub(r'([\w\.-]+)@([\w\.-]+)', r'pqr@mno.com', doc)
    print(new_email_address)
    #Output
    For more details please mail us at pqr@mno.com

    ```

```py
doc = "For more details please mail us at xyz@abc.com"

```

关于正则表达式的解释，请参考配方 1-6。

如果您在使用 regex 处理电子邮件时观察到了这两种情况，那么我们已经实现了一个非常基本的实例。我们声明由`@`分隔的单词有助于捕获电子邮件 id。然而，可能有许多边缘情况；例如，点(`.`)包含域名并处理数字、+(加号)等，因为它们可以是电子邮件 ID 的一部分。

以下是提取/查找/替换电子邮件 id 的高级正则表达式。

```py
([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\.[a-zA-Z0-9_-]+)

```

还有更复杂的方法来处理所有的边缘情况(例如，“电子邮件 id”中的“. co.in”)。请试一试。

#### 从电子书中提取数据并执行正则表达式

让我们通过使用您到目前为止所学的技术来解决一个从电子书中提取数据的案例研究。

1.  从书中摘录内容。

    ```py
    # Import library
    import re
    import requests
    #url you want to extract
    url = 'https://www.gutenberg.org/files/2638/2638-0.txt'
    #function to extract
    def get_book(url).
     # Sends a http request to get the text from project Gutenberg
     raw = requests.get(url).text
     # Discards the metadata from the beginning of the book
     start = re.search(r"\*\*\* START OF THIS PROJECT GUTENBERG EBOOK .* \*\*\*",raw ).end()
     # Discards the metadata from the end of the book
     stop = re.search(r"II", raw).start()
     # Keeps the relevant text
     text = raw[start:stop]
     return text
    # processing

    def preprocess(sentence).
     return re.sub('[^A-Za-z0-9.]+' , ' ', sentence).lower()
    #calling the above function
    book = get_book(url)
    processed_book = preprocess(book)
    print(processed_book)
    # Output
     produced by martin adamson david widger with corrections by andrew sly the idiot by fyodor dostoyevsky translated by eva martin part i i. towards the end of november during a thaw at nine o clock one morning a train on the warsaw and petersburg railway was approaching the latter city at full speed. the morning was so damp and misty that it was only with great difficulty that the day succeeded in breaking and it was impossible to distinguish anything more than a few yards away from the carriage windows. some of the passengers by this particular train were returning from abroad but the third class carriages were the best filled chiefly with insignificant persons of various occupations and degrees

    picked up at the different stations nearer town. all of them seemed weary and most of them had sleepy eyes and a shivering expression while their complexions generally appeared to have taken on the colour of the fog outside. when da

    ```

2.  使用 regex 对此数据执行探索性数据分析。

    ```py
    # Count number of times "the" is appeared in the book
    len(re.findall(r'the', processed_book))
    #Output
    302
    #Replace "i" with "I"
    processed_book = re.sub(r'\si\s', " I ", processed_book)
    print(processed_book)
    #output
     produced by martin adamson david widger

    with corrections by andrew sly the idiot by fyodor dostoyevsky translated by eva martin part I i. towards the end of november during a thaw at nine o clock one morning a train on the warsaw and petersburg railway was approaching the latter city at full speed. the morning was so damp and misty that it was only with great difficulty that the day succeeded in breaking and it was impossible to distinguish anything more than a few yards away from the carriage windows. some of the passengers by this particular train were returning from abroad but the third class carriages were the best filled chiefly with insignificant persons of various occupations and degrees picked up at the different stations nearer town. all of them seemed weary and most of them had sleepy eyes and a shivering expression while their complexions generally appeared to have taken on the colour of the fog outside. when da

    #find all occurance of text in the format "abc--xyz"
    re.findall(r'[a-zA-Z0-9]*--[a-zA-Z0-9]*', book)
    #output
     ['ironical--it',
     'malicious--smile',
     'fur--or',
     'astrachan--overcoat',
     'it--the',
     'Italy--was',
     'malady--a',
     'money--and',
     'little--to',
     'No--Mr',
     'is--where',
     'I--I',

     'I--',
     '--though',
     'crime--we',
     'or--judge',
     'gaiters--still',
     '--if',
     'through--well',
     'say--through',
     'however--and',
     'Epanchin--oh',
     'too--at',
     'was--and',
     'Andreevitch--that',
     'everyone--that',
     'reduce--or',
     'raise--to',
     'listen--and',
     'history--but',
     'individual--one',
     'yes--I',
     'but--',

     't--not',
     'me--then',
     'perhaps--',
     'Yes--those',
     'me--is',
     'servility--if',
     'Rogojin--hereditary',
     'citizen--who',
     'least--goodness',
     'memory--but',
     'latter--since',
     'Rogojin--hung',
     'him--I',
     'anything--she',
     'old--and',

     'you--scarecrow',
     'certainly--certainly',
     'father--I',
     'Barashkoff--I',
     'see--and',
     'everything--Lebedeff',
     'about--he',
     'now--I',
     'Lihachof--',
     'Zaleshoff--looking',
     'old--fifty',
     'so--and',
     'this--do',
     'day--not',
     'that--',

     'do--by',
     'know--my',
     'illness--I',
     'well--here',
     'fellow--you']

    ```

## 配方 1-7。处理字符串

这个菜谱讨论了如何处理字符串和文本数据。您可以使用字符串操作进行各种基本的文本探索。

### 问题

你想探索处理字符串。

### 解决办法

最简单的方法是使用下面的字符串功能。

*   `s.find(t)`是 s 中字符串 t 的第一个实例的索引(如果找不到，则为–1)

*   `s.rfind(t)`是 s 中字符串 t 的最后一个实例的索引(如果找不到，则为–1)

*   `s.index(t)`类似于`s.find(t)`,除了它在未找到时引发 ValueError

*   `s.rindex(t)`类似于`s.rfind(t)`,除了它在未找到时引发 ValueError

*   使用 s 作为粘合剂将文本的单词组合成一个字符串

*   在找到 t 的地方将 s 分割成一个列表(默认为空白)

*   将 s 拆分成一个字符串列表，每行一个

*   `s.lower()`是字符串 s 的小写版本

*   `s.upper()`是字符串 s 的大写版本

*   `s.title()`是字符串 s 的带标题的大小写版本

*   `s.strip()`是没有前导或尾随空白的 s 的副本

*   `s.replace(t, u)`用 s 中的 u 替换 t 的实例

### 它是如何工作的

现在让我们来看几个例子。

#### 替换内容

创建一个字符串并替换内容。创建字符串很容易。这是通过用单引号或双引号将字符括起来实现的。而要替换，可以使用`replace`功能。

1.  创建一个字符串。

    ```py
    String_v1 = "I am exploring NLP"
    #To extract particular character or range of characters from string
    print(String_v1[0])
    #output
    "I"
    #To extract the word “exploring”
    print(String_v1[5:14])
    #output
    exploring

    ```

2.  将前面字符串中的`"exploring"`替换为`"learning"`。

    ```py
    String_v2 = String_v1.replace("exploring", "learning")
    print(String_v2)
    #Output
    I am learning NLP

    ```

#### 连接两个字符串

下面是简单的代码。

```py
s1 = "nlp"
s2 = "machine learning"
s3 = s1+s2
print(s3)

#output
'nlpmachine learning'

```

#### 在字符串中搜索子字符串

使用`find`函数获取整个字符串中子串的起始索引值。

```py
var="I am learning NLP"
f= "learn"
var.find(f)
#output
5

```

## 配方 1-8。从网上抓取文本

这个食谱讨论了如何从网上抓取数据。

Caution

在抓取任何网站、博客或电子商务网站之前，请确保您阅读了该网站是否允许数据抓取的条款和条件。通常，robots.txt 包含条款和条件(例如，参见 [`www.alixpartners.com/robots.txt`](http://www.alixpartners.com/robots.txt) )，而站点地图包含 URL 的地图(例如，参见 [`www.alixpartners.com/sitemap.xml`](http://www.alixpartners.com/sitemap.xml) )。

Web 抓取也称为 web 采集和 web 数据提取。它是一种从网站中提取大量数据并保存在数据库或本地的技术。您可以使用这些数据来提取与您的客户、用户或产品相关的信息，从而使企业受益。

对 HTML 有基本的了解是先决条件。

### 问题

你想通过抓取从网上提取数据。让我们以 IMDB.com 为例来搜集顶级电影。

### 解决办法

最简单的方法是使用 Python 漂亮的 Soup 或 Scrapy 库。让我们在这个食谱中使用美丽的汤。

### 它是如何工作的

按照本节中的步骤从 web 中提取数据。

#### 步骤 8-1。安装所有必需的库

```py
!pip install bs4

!pip install requests

```

#### 步骤 8-2。导入库

```py
from bs4 import BeautifulSoup

import requests
import pandas as pd
from pandas import Series, DataFrame
from ipywidgets import FloatProgress
from time import sleep
from IPython.display import display
import re
import pickle

```

#### 步骤 8-3。识别提取数据的 URL

```py
url = 'http://www.imdb.com/chart/top?ref_=nv_mv_250_6'

```

#### 步骤 8-4。使用 Beautiful Soup 请求 URL 并下载内容

```py
result = requests.get(url)
c = result.content

soup = BeautifulSoup(c,"lxml")

```

#### 步骤 8-5。理解网站的结构以提取所需的信息

转到网站，右键单击页面内容，检查网站的 HTML 结构。

确定要提取的数据和字段。例如，您想要电影名称和 IMDB 分级。

检查 HTML 中的哪个 div 或类包含电影名称，并相应地解析这个美丽的 Soup。在这个例子中，你可以通过`<table class ="chart full-width">`和`<td class="titleColumn">`解析这个汤来提取电影名。

同样，您可以获取其他数据；请参考步骤 8-6 中的代码。

![img/475440_2_En_1_Figa_HTML.jpg](img/475440_2_En_1_Figa_HTML.jpg)

#### 步骤 8-6。使用 Beautiful Soup 从 HTML 标签中提取和解析数据

```py
summary = soup.find('div',{'class':'article'})
# Create empty lists to append the extracted data

.
moviename = []
cast = []
description = []
rating = []
ratingoutof = []
year = []
genre = []
movielength = []
rot_audscore = []
rot_avgrating = []
rot_users = []
# Extracting the required data from the html soup.

rgx = re.compile('[%s]' % '()')
f = FloatProgress(min=0, max=250)
display(f)
for row,i in zip(summary.find('table').findAll('tr'),range(len(summary.find('table').findAll('tr')))):

    for sitem in row.findAll('span',{'class':'secondaryInfo'}).
        s = sitem.find(text=True)
        year.append(rgx.sub(", s))
    for ritem in row.findAll('td',{'class':'ratingColumn imdbRating'}).
        for iget in ritem.findAll('strong').
            rating.append(iget.find(text=True))
            ratingoutof.append(iget.get('title').split(' ', 4)[3])
    for item in row.findAll('td',{'class':'titleColumn'}).
        for href in item.findAll('a',href=True).
            moviename.append(href.find(text=True))
            rurl = 'https://www.rottentomatoes.com/m/'+ href.find(text=True)
            try.
                rresult = requests.get(rurl)
            except requests.exceptions.ConnectionError.
                status_code = "Connection refused"
            rc = rresult.content
            rsoup = BeautifulSoup(rc)
            try:

                rot_audscore.append(rsoup.find('div',{'class':'meter-value'}).find('span',{'class':'superPageFontColor'}).text)
                rot_avgrating.append(rsoup.find('div',{'class':'audience-info hidden-xs superPageFontColor'}).find('div').contents[2].strip())
                rot_users.append(rsoup.find('div',{'class':'audience-info hidden-xs superPageFontColor'}).contents[3].contents[2].strip())
            except AttributeError.
                rot_audscore.append("")
                rot_avgrating.append("")
                rot_users.append("")
            cast.append(href.get('title'))
            imdb = "http://www.imdb.com" + href.get('href')
            try.
                iresult = requests.get(imdb)
                ic = iresult.content

                isoup = BeautifulSoup(ic)
                description.append(isoup.find('div',{'class':'summary_text'}).find(text=True).strip())
                genre.append(isoup.find('span',{'class':'itemprop'}).find(text=True))
                movielength.append(isoup.find('time',{'itemprop':'duration'}).find(text=True).strip())

            except requests.exceptions.ConnectionError.
                description.append("")
                genre.append("")
                movielength.append("")
    sleep(.1)
    f.value = i

```

请注意，由于以下原因，您很有可能在执行该脚本时遇到错误。

*   您对 URL 的请求失败。如果是这样，请过一段时间再试一次。这在网络抓取中很常见。

*   网页是动态的，这意味着 HTML 标签不断变化。研究一下标签，并根据 HTML 对代码做一些小的修改，你就可以开始了。

#### 步骤 8-7。将列表转换为数据框架，并执行符合业务要求的分析

```py
# List to pandas series

moviename = Series(moviename)
cast = Series(cast)
description = Series(description)
rating = Series(rating)
ratingoutof = Series(ratingoutof)
year = Series(year)
genre = Series(genre)
movielength = Series(movielength)
rot_audscore = Series(rot_audscore)
rot_avgrating = Series(rot_avgrating)
rot_users = Series(rot_users)
# creating dataframe and doing analysis
imdb_df = pd.concat([moviename,year,description,genre,movielength,cast,rating,ratingoutof,rot_audscore,rot_avgrating,rot_users],axis=1)
imdb_df.columns = ['moviename','year','description','genre','movielength','cast','imdb_rating','imdb_ratingbasedon','tomatoes_audscore','tomatoes_rating','tomatoes_ratingbasedon']
imdb_df['rank'] = imdb_df.index + 1
imdb_df.head(1)
#output

```

![img/475440_2_En_1_Figb_HTML.jpg](img/475440_2_En_1_Figb_HTML.jpg)

#### 第八步。下载数据框

```py
# Saving the file as CSV.
imdb_df.to_csv("imdbdataexport.csv")

```

本章实现了从数据源中提取文本数据的大部分技术。在接下来的章节中，您将了解如何探索、处理和清理数据。您还将学习功能工程和构建 NLP 应用程序。