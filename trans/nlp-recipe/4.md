# 4.高级自然语言处理

本章涵盖了各种高级 NLP 技术，并利用机器学习算法从文本数据中提取信息，以及高级 NLP 应用程序的解决方案和实现。

*   食谱 1。名词短语抽取

*   食谱 2。文本相似度

*   食谱三。词性标注

*   食谱 4。信息提取-NER-实体识别

*   食谱 5。主题建模

*   食谱 6。文本分类

*   食谱 7。情感分析

*   食谱 8。词义消歧

*   食谱 9。语音识别和语音转文本

*   食谱 10。文本到语音

*   食谱 11。语言检测和翻译

在进入食谱之前，让我们先了解 NLP 管道和生命周期。这本书实现了许多概念，你可能会被它的内容淹没。为了使它更简单、更流畅，让我们看看 NLP 解决方案需要遵循的流程。

例如，让我们考虑对一个产品、一个品牌或一项服务的客户情感分析和预测。

*   **定义问题**。了解所有产品的客户情绪。

*   **了解问题的深度和广度**。了解整个产品的客户/用户情绪。我们为什么要这么做？业务影响是什么？

*   **做** **数据需求头脑风暴**。进行头脑风暴活动，列出所有可能的数据点。
    *   顾客在亚马逊、Flipkart 等电子商务平台上的所有评论

    *   客户发送的电子邮件

    *   保修索赔表

    *   调查数据

    *   使用语音转文本的呼叫中心对话

    *   反馈表格

    *   Twitter、脸书和 LinkedIn 等社交媒体数据

*   **数据收集**:你在第 [1](1.html) 章中学习了不同的数据收集技术。根据数据和问题，您可能需要整合不同的数据收集方法。在这种情况下，您可以使用 web 抓取和 Twitter APIs。

*   **文本预处理**:你知道数据不会总是干净的。你需要花大量的时间来处理它，并使用第 [2](2.html) 章中讨论的方法提取洞察力。

*   文本是由字符组成的，机器很难理解它们。使用您在前面章节中学习的任何方法，将它们转换为机器和算法可以理解的功能。

*   **机器学习/深度学习**:机器学习和深度学习是人工智能保护伞的一部分，它使系统在没有被编程的情况下自动学习数据中的模式。大多数 NLP 解决方案都基于此。由于文本被转换为特征，因此您可以利用机器学习或深度学习算法来实现文本分类和自然语言生成等目标。

*   **洞察力和部署**:如果没有正确的洞察力与业务沟通，构建 NLP 解决方案是没有用的。总是花时间将模型/分析输出和业务之间的点连接起来，从而产生最大的影响。

## 配方 4-1。提取名词短语

这个菜谱从文本数据(一个句子或文档)中提取一个名词短语。

### 问题

你想提取一个名词短语。

### 解决办法

当你想分析一个句子中的*谁*时，名词短语提取是很重要的。让我们看一个使用`TextBlob`的例子。

### 它是如何工作的

执行下面的代码来提取名词短语。

```py
#Import libraries
import nltk
from textblob import TextBlob
#Extract noun
blob = TextBlob("John is learning natural language processing")
for np in blob.noun_phrases:
    print(np)

```

这是输出。

```py
john
natural language processing

```

## 食谱 4-2。寻找文本之间的相似性

这个菜谱讨论了如何找到两个文档或文本之间的相似性。有许多类似的度量标准，如欧几里德、余弦和雅克卡。文本相似性的应用可以在拼写校正、重复数据删除、简历筛选、跨各种领域的搜索应用以及基于内容的推荐系统中找到。

下面是一些相似性度量。

*   **余弦相似度**:计算两个向量之间角度的余弦值。

*   **Jaccard 相似度**:使用单词的交集或并集计算得分。

*   **Jaccard Index** :(两组中的数字)/(任一组中的数字)* 100。

*   **Levenshtein 距离**:将字符串 *a* 转换成字符串 *b* 需要最少的插入、删除和替换。

*   **汉明距离**:两个字符串中符号相同的位置个数。只能为长度相等的字符串定义它。

您希望找到文本和文档之间的相似之处。

### 解决办法

最简单的方法是使用 sklearn 库中的余弦相似度。

### 它是如何工作的

按照本节中的步骤计算文本文档之间的相似性得分。

#### 步骤 2-1。创建/读取文本数据

这是数据。

```py
documents = (
"I like NLP",
"I am exploring NLP",
"I am a beginner in NLP",
"I want to learn NLP",
"I like advanced NLP"
)

```

#### 第 2-2 步。寻找相似之处

执行下面的代码来找出相似之处。

```py
#Import libraries
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
#Compute tfidf : feature engineering(refer previous chapter – Recipe 3-4)
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(documents)
tfidf_matrix.shape
#output
(5, 10)
#compute similarity for first sentence with rest of the sentences
cosine_similarity(tfidf_matrix[0:1],tfidf_matrix)
#output
array([[ 1\.       ,  0.17682765,  0.14284054,  0.13489366,  0.68374784]])

```

与其余句子相比，第一句和最后一句具有更高的相似性。

#### 语音匹配

相似性检查的下一个版本是语音匹配，它粗略地匹配两个单词或句子，并创建一个字母数字字符串作为文本或单词的编码版本。这对于搜索大型文本语料库、纠正拼写错误和匹配相关名称非常有用。Soundex 和 Metaphone 是用于此目的的两种主要语音算法。最简单的方法是使用模糊库。

1.  安装并导入库。

1.  运行 Soundex 函数。

```py
!pip install fuzzy
import fuzzy

```

1.  生成语音形式。

    ```py
    soundex('natural')
    #output
    'N364'
    soundex('natuaral')
    #output
    'N364'
    soundex('language')
    #output
    'L52'
    soundex('processing')
    #output
    'P625'

    ```

```py
soundex = fuzzy.Soundex(4)

```

Soundex 对自然和天然一视同仁。两个字符串的语音代码都是 N364。而对于语言和处理，分别是 L52 和 P625。

## 食谱 4-3。词性标注

词性标注是自然语言处理的另一个重要部分，它涉及用词性(如名词、动词、形容词等)来标记单词。词性是命名实体解析、问答和词义消歧的基础。

### 问题

你想标记句子中的词类。

### 解决办法

有两种方法可以创建标记器。

*   **基于规则的**:人工创建的规则，标记属于特定词性的单词。

*   **基于随机的**:这些算法捕捉单词的序列，并使用隐马尔可夫模型标记序列的概率。

### 它是如何工作的

同样，NLTK 拥有最好的 POS 标记模块。`nltk.pos_tag(word)`是为任何给定单词生成词性标注的函数。使用 for 循环并为文档中出现的所有单词生成 POS。

#### 步骤 3-1。将文本存储在变量中

这里是变量。

```py
Text  =  "I love NLP and I will learn NLP in 2 month"

```

#### 第 3-2 步。为 POS 导入 NLTK

这是代码。

```py
# Importing necessary packages and

stopwords
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
stop_words = set(stopwords.words('english'))
# Tokenize the text
tokens = sent_tokenize(text)
#Generate tagging for all the tokens using loop
for i in tokens:
    words = nltk.word_tokenize(i)
    words = [w for w in words if not w in stop_words]
    #  POS-tagger.
    tags = nltk.pos_tag(words)
tags

```

这些是结果。

```py
[('I', 'PRP'),
 ('love', 'VBP'),
 ('NLP', 'NNP'),
 ('I', 'PRP'),
 ('learn', 'VBP'),
 ('NLP', 'RB'),
 ('2month', 'CD')]

```

以下是词性标注的简写形式和解释。爱这个词是 VBP，意思是唱。当前，非 3D 拍摄。

*   CC 并列连词

*   CD 基数

*   DT 限定词

*   EX 存在那里(如*有*...想象一下*存在*

*   FW 外来词

*   在介词/从属连词中

*   JJ 形容词例如:*大*

*   JJR 形容词，比较级例如:*更大的*

*   JJS 形容词，最高级例如:*最大的*

*   LS 列表标记 1)

*   MD modal 可能会

*   NN 名词，单数*书桌*

*   NNS 名词复数*书桌*

*   NNP 专有名词，单数*哈里森*

*   NNPS 专有名词，复数*美国人*

*   PDT 预定器*所有孩子*

*   词尾所有格父母

*   PRP 人称代词我，他，她

*   PRP$所有格代词我的，他的，她的

*   非常，无声地

*   RBR 副词，比较级更好

*   RBS 副词，最高级 best

*   RP 粒子放弃

*   去商店

*   呃感叹词

*   VB 动词，基本形式 take

*   VBD 动词，过去式*带了*

*   VBG 动词，动名词/现在分词*取*

*   VBN 动词，过去分词*取*

*   VBP 动词，唱，现在。非 3D *拍摄*

*   VBZ 动词，第三人称演唱。呈现*带走*

*   WDT 疑问词 which

*   代词谁，什么

*   WP$所有格 wh-代词谁的

*   wh-副词 where，when

## 食谱 4-4。从文本中提取实体

这个菜谱讨论了如何从文本中识别和提取实体，称为*命名实体识别*。多个库执行这项任务，如 NLTK chunker、斯坦福 NER、spaCy、OpenNLP 和 NeuroNER。还有很多 API，像沃森 NLU，阿尔切米亚皮，呆子，谷歌云自然语言 API，等等。

### 问题

您希望从文本中识别和提取实体。

### 解决办法

最简单的方法是使用 NLTK 或 spaCy 中的`ne_chunk`。

### 它是如何工作的

按照本节中的步骤执行 NER。

#### 步骤 4-1。读取/创建文本数据

这是正文。

```py
sent = "John is studying at Stanford University in California"

```

#### 第 4-2 步。提取实体

执行下面的代码。

#### 使用 NLTK

```py
#import libraries

import nltk
from nltk import ne_chunk
from nltk import word_tokenize
#NER
ne_chunk(nltk.pos_tag(word_tokenize(sent)), binary=False)
#output
Tree('S', [Tree('PERSON', [('John', 'NNP')]), ('is', 'VBZ'), ('studying', 'VBG'), ('at', 'IN'), Tree('ORGANIZATION', [('Stanford', 'NNP'), ('University', 'NNP')]), ('in', 'IN'), Tree('GPE', [('California', 'NNP')])])
Here "John" is tagged as "PERSON"
"Stanford" as "ORGANIZATION"
"California" as "GPE". Geopolitical entity, i.e. countries, cities, states.

```

#### 使用空间

```py
import spacy

nlp = spacy.load('en')
# Read/create a sentence
doc = nlp(u'Apple is ready to launch new phone worth $10000 in New york time square ')
for ent in doc.ents:
   print(ent.text, ent.start_char, ent.end_char, ent.label_)
#output

Apple 0 5 ORG
10000 42 47 MONEY
New york 51 59 GPE

```

按产量来说，苹果是个组织，10000 是钱，纽约是个地方。结果是准确的，可用于任何自然语言处理应用。

## 配方 4-5。从文本中提取主题

这个菜谱讨论了如何从文档中识别主题。例如，有一个在线图书馆，根据图书的种类/流派设有多个部门。你查看独特的关键词/主题来决定这本书可能属于哪个部门，并相应地放置它。在这种情况下，主题建模会派上用场。它被称为*文档标记和聚类*。

### 问题

您希望从文档中提取或识别主题。

### 解决办法

最简单的方法是使用 gensim 库。

### 它是如何工作的

按照本节中的步骤使用 genism 识别文档中的主题。

#### 步骤 5-1。创建文本数据

这是正文。

```py
doc1 = "I am learning NLP, it is very interesting and exciting. it includes machine learning and deep learning"
doc2 = "My father is a data scientist and he is nlp expert"
doc3 = "My sister has good exposure into android development"
doc_complete = [doc1, doc2, doc3]
doc_complete
#output
['I am learning NLP, it is very interesting and exciting. it includes machine learning and deep learning',
 'My father is a data scientist and he is nlp expert',
 'My sister has good exposure into android development']

```

#### 第 5-2 步。清理和预处理数据

接下来，我们来清理一下。

```py
# Install and import libraries
!pip install gensim
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
import string
# Text preprocessing as discussed in chapter 2
stop = set(stopwords.words('english'))
exclude = set(string.punctuation)
lemma = WordNetLemmatizer()
def clean(doc):
    stop_free = " ".join([i for i in doc.lower().split() if i not in stop])
    punc_free = ".join(ch for ch in stop_free if ch not in exclude)
    normalized = " ".join(lemma.lemmatize(word) for word in punc_free.split())
    return normalized
doc_clean = [clean(doc).split() for doc in doc_complete]
doc_clean
#output
[['learning',
  'nlp',
  'interesting',
  'exciting',

  'includes',
  'machine',
  'learning',
  'deep',
  'learning'],
 ['father', 'data', 'scientist', 'nlp', 'expert'],
 ['sister', 'good', 'exposure', 'android', 'development']]

```

#### 第 5-3 步。准备文档术语矩阵

下面是代码。

```py
# Importing gensim
import gensim
from gensim import corpora
# Creating the term dictionary of our corpus, where every unique term is assigned an index.
dictionary = corpora.Dictionary(doc_clean)
# Converting a list of documents (corpus) into Document-Term Matrix using dictionary prepared above.
doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]
doc_term_matrix
#output
[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 3), (5, 1), (6, 1)],
 [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],
 [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]]

```

#### 第 5-4 步。创建 LDA 模型

最后一部分创建 LDA 模型。

```py
# Creating the object for LDA model using gensim library
Lda = gensim.models.ldamodel.LdaModel
# Running and Training LDA model on the document term matrix for 3 topics.
ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)
# Results
print(ldamodel.print_topics())
#output
[(0, '0.063*"nlp" + 0.063*"father" + 0.063*"data" + 0.063*"scientist" + 0.063*"expert" + 0.063*"good" + 0.063*"exposure" + 0.063*"development" + 0.063*"android" + 0.063*"sister"'), (1, '0.232*"learning" + 0.093*"nlp" + 0.093*"deep" + 0.093*"includes" + 0.093*"interesting" + 0.093*"machine" + 0.093*"exciting" + 0.023*"scientist" + 0.023*"data" + 0.023*"father"'), (2, '0.087*"sister" + 0.087*"good" + 0.087*"exposure" + 0.087*"development" + 0.087*"android" + 0.087*"father" + 0.087*"scientist" + 0.087*"data" + 0.087*"expert" + 0.087*"nlp"')]

```

所有与句子主题相关的权重看起来都差不多。您可以对大型文档执行此操作，以提取重要的主题。在样本数据上实现这一点的整个想法是让您熟悉它，并且您可以使用相同的代码片段在大量数据上执行，以获得重要的结果和见解。

## 配方 4-6。文本分类

文本分类根据预先训练的类别自动对文本文档进行分类。它有以下应用。

*   情感分析

*   文件分类

*   垃圾邮件/业余邮件分类

*   投诉分类

*   产品分类

*   假新闻检测

### 问题

使用机器学习的垃圾邮件/火腿分类。

### 解决办法

Gmail 有一个叫垃圾邮件的文件夹。它将你的电子邮件分类为垃圾邮件和火腿，这样你就不必阅读不必要的电子邮件。

### 它是如何工作的

按照循序渐进的方法构建分类器。

#### 步骤 6-1。收集和理解数据

请从`www.kaggle.com/uciml/sms-spam-collection` `-dataset#spam.csv`下载数据并保存在您的工作目录中。

```py
#Read the data
Email_Data = pd.read_csv("spam.csv",encoding ='latin1')
#Data undestanding
Email_Data.columns
#output
Index(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype="object")
Email_Data = Email_Data[['v1', 'v2']]
Email_Data = Email_Data.rename(columns={"v1":"Target", "v2":"Email"})
Email_Data.head()
#output
    Target   Email
0      ham   Go until jurong point, crazy.. Available only ...
1      ham   Ok lar... Joking wif u oni...
2      spam  Free entry in 2 a wkly comp to win FA Cup fina...
3      ham   U dun say so early hor... U c already then say...
4      ham   Nah I don't think he goes to usf, he lives aro...

```

#### 第 6-2 步。文本处理和特征工程

下面是代码。

```py
#import
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import string
from nltk.stem import SnowballStemmer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import os
from textblob import TextBlob
from nltk.stem import PorterStemmer
from textblob import Word
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
import sklearn.feature_extraction.text as text
from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm
#pre processing steps like lower case, stemming and lemmatization
Email_Data['Email'] = Email_Data['Email'].apply(lambda x: " ".join(x.lower() for x in x.split()))
stop = stopwords.words('english')
Email_Data['Email'] = Email_Data['Email'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))
st = PorterStemmer()
Email_Data['Email'] = Email_Data['Email'].apply(lambda x: " ".join([st.stem(word) for word in x.split()]))
Email_Data['Email'] = Email_Data['Email'].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))
Email_Data.head()

#output
  Target                                              Email
0    ham  go jurong point, crazy.. avail bugi n great wo...
1    ham                        ok lar... joke wif u oni...
2    spam free entri 2 wkli comp win fa cup final tkt 21...
3    ham          u dun say earli hor... u c alreadi say...
4    ham              nah think goe usf, live around though
#Splitting data into train and validation

train_x, valid_x, train_y, valid_y = model_selection.train_test_split(Email_Data['Email'], Email_Data['Target'])
# TFIDF feature generation for a maximum of 5000 features
encoder = preprocessing.LabelEncoder()
train_y = encoder.fit_transform(train_y)
valid_y = encoder.fit_transform(valid_y)
tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\w{1,}', max_features=5000)

tfidf_vect.fit(Email_Data['Email'])
xtrain_tfidf =  tfidf_vect.transform(train_x)
xvalid_tfidf =  tfidf_vect.transform(valid_x)
xtrain_tfidf.data
#output
array([0.39933971, 0.36719906, 0.60411187, ..., 0.36682939, 0.30602539, 0.38290119])

```

#### 第 6-3 步。模特培训

这是用于训练任何给定模型的通用函数。

```py
def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):
    # fit the training dataset on the classifier
    classifier.fit(feature_vector_train, label)
    # predict the labels on validation dataset
    predictions = classifier.predict(feature_vector_valid)
    return metrics.accuracy_score(predictions, valid_y)
# Naive Bayes trainig
accuracy = train_model(naive_bayes.MultinomialNB(alpha=0.2), xtrain_tfidf, train_y, xvalid_tfidf)
print ("Accuracy: ", accuracy)
#output
Accuracy:  0.985642498205
# Linear Classifier on Word Level TF IDF Vectors
accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)
print ("Accuracy: ", accuracy)
#output

Accuracy:  0.970567121321

```

朴素贝叶斯分类器比线性分类器提供更好的结果。您应该尝试其他几个分类器，然后选择最好的一个。

## 配方 4-7。进行情感分析

这个食谱讨论了一个特定句子或语句的情感。情感分析是各行业广泛使用的技术之一，用于了解客户/用户对产品/服务的情感。情感分析给出倾向于正面或负面的句子/语句的情感分数。

### 问题

你想做一个情感分析。

### 解决办法

最简单的方法是使用 TextBlob 或 VADER。

### 它是如何工作的

按照本节中的步骤使用 TextBlob 进行情感分析。它有两个指标。

*   极性位于[–1，1]的范围内，其中 1 表示肯定的陈述，而–1 表示否定的陈述。

*   主观性[0，1]是一种观点，而不是事实信息。

#### 步骤 7-1。创建示例数据

这是样本数据。

```py
review = "I like this phone. screen quality and camera

clarity is really good."
review2 = "This tv is not good. Bad quality, no clarity, worst experience"

```

#### 第 7-2 步。清理和预处理数据

该步骤请参考第 [2](2.html) 章，配方 2-10。

#### 第 7-3 步。获取情感分数

使用预先训练的 TextBlob 来获得情感分数。

```py
#import libraries
from textblob import TextBlob
#TextBlob has a pre trained sentiment prediction model
blob = TextBlob(review)
blob.sentiment
#output
Sentiment(polarity=0.7, subjectivity=0.6000000000000001)

```

这似乎是一个非常积极的评论。

```py
#now lets look at the sentiment of review2
blob = TextBlob(review2)
blob.sentiment
#output

Sentiment(polarity=-0.6833333333333332, subjectivity=0.7555555555555555)

```

这是一个负面评价，因为极性为–0.68。

Note

在下一章的菜谱 5-2 中会介绍一个端到端实现的实时情感分析用例。

## 食谱 4-8。消除文本歧义

歧义的产生是由于单词在不同的上下文中有不同的含义。

举个例子，

```py
Text1 = 'I went to the bank to deposit my money'
Text2 = 'The river bank was full of dead fish'

```

在文本中，单词 bank 根据句子的上下文有不同的含义。

### 问题

你想要理解消除词义的歧义。

### 解决办法

Lesk 算法是词义消歧的最佳算法之一。让我们看看如何使用`pywsd`和`nltk`包来解决它。

### 它是如何工作的

以下是实现结果的步骤。

#### 步骤 8-1。导入库

首先，导入库。

```py
#Install pywsd
!pip install pywsd
#Import functions
from nltk.corpus import wordnet as wn
from nltk.stem import PorterStemmer
from itertools import chain
from pywsd.lesk import simple_lesk

```

#### 步骤 8-2。消除词义的歧义

这是代码。

```py
# Sentences
bank_sents = ['I went to the bank to deposit my money',
'The river bank was full of dead fishes']
# calling the lesk function and printing results for both the sentences
print ("Context-1:", bank_sents[0])
answer = simple_lesk(bank_sents[0],'bank')
print ("Sense:", answer)
print ("Definition : ", answer.definition())
print ("Context-2:", bank_sents[1])
answer = simple_lesk(bank_sents[1],'bank','n')
print ("Sense:", answer)
print ("Definition : ", answer.definition())
#Result:

Context-1: I went to the bank to deposit my money
Sense: Synset('depository_financial_institution.n.01')
Definition :  a financial institution that accepts deposits and channels the money into lending activities

Context-2: The river bank was full of dead fishes
Sense: Synset('bank.n.01')
Definition :  sloping land (especially the slope beside a body of water)

```

观察在上下文-1 中，“银行”是金融机构，而在上下文-2 中，“银行”是坡地。

## 食谱 4-9。将语音转换为文本

将语音转换成文本是一种非常有用的自然语言处理技术。

### 问题

你想把语音转换成文本。

### 解决办法

最简单的方法是使用语音识别和 PyAudio。

### 它是如何工作的

按照本节中的步骤实现语音到文本转换。

#### 步骤 9-1。定义业务问题

与机器的互动正趋向于语音，这是人类交流的常用方式。常见的例子有苹果 Siri、亚马逊 Alexa 和谷歌 Home。

#### 步骤 9-2。安装并导入必要的库

这里是图书馆。

```py
!pip install SpeechRecognition
!pip install PyAudio
import speech_recognition as sr

```

#### 第 9-3 步。运行代码

现在，在您运行下面的代码片段之后，您在麦克风上说的任何话(使用`recognize_google`函数)都将被转换成文本。

```py
r=sr.Recognizer()
with sr.Microphone() as source:
    print("Please say something")
    audio = r.listen(source)
    print("Time over, thanks")
try:
    print("I think you said: "+r.recognize_google(audio));
except:
    pass;
#output
Please say something
Time over, thanks
I think you said: I am learning natural language processing

```

此代码适用于默认的英语语言。如果您说的是另一种语言，例如印地语，则文本将以英语解释，如下所示。

```py
#code snippet
r=sr.Recognizer()
with sr.Microphone() as source:
    print("Please say something")
    audio = r.listen(source)
    print("Time over, thanks")
try:
    print("I think you said: "+r.recognize_google(audio));
except:
    pass;
#output
Please say something
Time over, thanks
I think you said: aapka naam kya hai

```

如果您希望文本出现在口语中，请运行下面的代码片段。对`recognize_google –language` ( `'hi-IN'`，意为印地语)做了一个小改动。

```py
#code snippet
r=sr.Recognizer()
with sr.Microphone() as source:
    print("Please say something")
    audio = r.listen(source)
    print("Time over, thanks")
try:

    print("I think you said: "+r.recognize_google(audio, language ='hi-IN'));
except sr.UnknownValueError:
    print("Google Speech Recognition could not understand audio")
except sr.RequestError as e:
    print("Could not request results from Google Speech Recognition service; {0}".format(e))
except:
    pass;

```

## 配方 4-10。将文本转换为语音

将文本转换成语音是另一种有用的 NLP 技术。

### 问题

你想把文本转换成语音。

### 解决办法

最简单的方法是使用 gTTs 库。

### 它是如何工作的

按照本节中的步骤将文本转换为语音。

#### 步骤 10-1。安装并导入必要的库

这里是图书馆。

```py
!pip install gTTS
from gtts import gTTS

```

#### 步骤 10-2。用 gTTs 函数运行代码

现在，在您运行下面的代码片段之后，您在 text 参数中输入的任何内容都会被转换成音频。

```py
#chooses the language, English('en')
convert = gTTS(text='I like this NLP book', lang="en", slow=False)
# Saving the converted audio in a mp3 file named
myobj.save("audio.mp3")
#output

Please play the audio.mp3 file saved in your local machine to hear the audio.

```

## 食谱 4-11。翻译演讲

语言检测和翻译。

### 问题

每当您试图分析来自全球各地的博客的数据时，尤其是来自像中国这样的中文占主导地位的国家的网站，分析这样的数据或对这样的数据执行 NLP 任务将是困难的。这就是语言翻译的用处。你想把一种语言翻译成另一种语言。

### 解决办法

最简单的方法是使用 goslate 库。

### 它是如何工作的

按照本节中的步骤用 Python 实现语言翻译。

#### 步骤 11-1。安装并导入必要的库

这里是图书馆。

```py
!pip install goslate
import goslate

```

#### 步骤 11-2。输入文本

输入以下简单的短语。

```py
text = "Bonjour le monde"

```

#### 步骤 11-3。运行 goslate 函数

运行翻译功能。

```py
gs = goslate.Goslate()
translatedText = gs.translate(text,'en')
print(translatedText)
#output

Hi world

```

Note

你也可以使用多语言库。它有各种多语言应用程序，并在 NLP 任务中支持 100 多种语言，如语言检测、标记化、NER、词性标注和情感分析。

嗯，感觉很圆满。您已经实现了许多高级的 NLP 应用程序和技术。但这还不是全部，伙计们。前面有更多有趣的章节，您将看到围绕 NLP 的工业应用、它们的解决方案和端到端的实现。