# 七、循环神经网络

随着深度学习的出现，自然语言处理(NLP)领域已经见证了显著的增长。这种运动很大程度上可以归功于循环神经网络(RNNs)及其变体。基于语音的 AI 助手、智能手机键盘中文本的自动完成以及基于情感分类的基于文本的评论都是 RNNs 有效解决的问题。

本章首先探讨与 RNNs 相关的基本概念。然后，我们探索更适合现代计算任务的香草 RNN 模型的几个变种。最后，我们将在从我们最喜欢的平台 Kaggle 借来的真实数据集上使用 PyTorch 研究 RNN 的实际实现。

让我们开始吧。

## RNNs 简介

循环神经网络(RNNs)本质上是采用递归的神经网络，其使用来自神经网络上的前向传递的信息。本质上，所有的 rnn 都可以描述为一个递归关系。rnn 适用于这样的问题，并且在应用于这些问题时取得了令人难以置信的成功，在这些问题中，要对其进行预测的输入数据是序列形式的(顺序很重要的一系列实体)。序列数据的例子包括时间序列、自然语言处理、语音分析等。

图 [7-1](#Fig1) 展示了一个规则的 RNN 是如何展开(随时间)形成一个循环神经网络的。在下一节中，我们将探讨 RNN 利用的基础。

![img/478491_2_En_7_Fig1_HTML.jpg](img/478491_2_En_7_Fig1_HTML.jpg)

图 7-1

一个正规的 RNN 展开了(来源——深度学习[www . Deep Learning book . org/contents/rnn . html](http://www.deeplearningbook.org/contents/rnn.html)

让我们从描述 RNN 的运动部件开始。首先，我们介绍一些符号。我们将假设输入由一系列实体组成*x*<sup>【1】</sup>， *x* <sup>(2)</sup> ，…，*x*<sup>(*)*</sup>*。对应于这个输入，我们需要产生一个序列*y*<sup>【1】</sup>，*y*<sup>【2】</sup>，…，*y*<sup>(*τ*)</sup>或者整个输入序列 *y* (或者一个不同长度的序列)。不同架构的 RNN 将为不同的用例提供解决方案。图 [7-2](#Fig2) 展示了基于输入输出长度的 RNN 类型。*

 *![img/478491_2_En_7_Fig2_HTML.jpg](img/478491_2_En_7_Fig2_HTML.jpg)

图 7-2

基于输入和输出长度的 RNN 类型

当我们有一个不利用来自先前状态的信息的 RNN 时，我们有一个传统的神经网络。然而，随着循环的出现，我们有了几种新的可能性。如今，NLP 中最常见的用例围绕着多对一和多对多模型。示例包括命名实体识别和机器翻译(例如，将文档从法语翻译成英语)。本章探索了几个简单的例子，但是深入讨论每个变体超出了本书的范围。强烈建议读者独立探索命名实体识别、机器翻译(以及可选的音乐生成)。

让我们从基础开始。

为了区分 RNN 生产的产品(即预测)和理想预期生产的产品(即实际)，我们用 RNN 生产的![$$ {\hat{y}}^{(1)},{\hat{y}}^{(2)},\dots, {\hat{y}}^{\left(\tau \right)} $$](img/478491_2_En_7_Chapter_TeX_IEq1.png)或![$$ \hat{y} $$](img/478491_2_En_7_Chapter_TeX_IEq2.png)来表示预测。

类似地，我们将表示基本事实，即 RNN 应该理想产生的实际值，表示为*y*<sup>【1】</sup>，*y*<sup>【2】</sup>，…， *y* <sup>( *τ* )</sup> 。图 [7-3](#Fig3) 显示了 RNN 产生的输出(预测)为![$$ {\hat{y}}^{(1)},{\hat{y}}^{(2)},\dots, {\hat{y}}^{\left(\tau \right)} $$](img/478491_2_En_7_Chapter_TeX_IEq3.png)。为了计算与实际值的差异，我们将比较这些生成的输出与实际值，表示为*y*<sup>【1】</sup>，*y*<sup>【2】</sup>，…，*y*<sup>(*)*</sup>*。*

 *rnn 或者为输入序列中的每个实体产生一个输出(多对多)，或者为整个序列产生一个输出(多对一)，如图 [7-2](#Fig2) 所示。让我们考虑一个 RNN，它为输入中的每个实体产生一个输出(本质上指的是图 [7-1](#Fig1) 中所示的展开的网络)。

![img/478491_2_En_7_Fig3_HTML.jpg](img/478491_2_En_7_Fig3_HTML.jpg)

图 7-3

展开的 RNN(多对多)，代表图形 [7-1](#Fig1) 的一部分

可以使用以下等式来描述 RNN:

![$$ {h}^{(t)}=\mathit{\tanh}\left(U{x}^{(t)}+W{h}^{\left(t-1\right)}+b\right) $$](img/478491_2_En_7_Chapter_TeX_Equa.png)

![$$ {\hat{y}}^{(t)}= softmax\left(V{h}^{(t)}+c\right) $$](img/478491_2_En_7_Chapter_TeX_Equb.png)

*U* 是网络输入的权重， *V* 是激活函数输出的权重， *W* 是当前隐藏状态的权重矩阵。

关于 RNN 方程，应注意以下几点:

1.  RNN 计算包括计算序列中实体的隐藏状态。这用 *h* <sup>( *t* )</sup> 来表示。

2.  *h* <sup>( *t* )</sup> 的计算使用实体 *x* <sup>( *t* )</sup> 处的相应输入和之前的隐藏状态*h*<sup>(*t*—1)</sup>。

3.  使用隐藏状态*h*T3】(*t*)计算输出![$$ {\hat{y}}^{(t)} $$](img/478491_2_En_7_Chapter_TeX_IEq4.png)。

4.  在计算当前隐藏状态时，一组权重与输入和前一隐藏状态相关联。这分别由 *U* 和 *W* 表示。还有一个偏差项，用 *b* 表示。

5.  类似地，在计算输出时，一组权重也与当前隐藏状态相关联。这由 *V* 表示。还有一个偏差项，用 *c* 表示。

6.  此外，在计算隐藏状态时，使用了`tanh`激活函数(在前面的章节中介绍过)。

7.  `softmax`激活功能用于输出的计算。

8.  如等式所述，RNN 可以处理任意大的输入序列。

9.  RNN 的参数— *U* 、 *W* 、 *V* 、 *b* 、 *c* 等。-在隐藏层和输出值的计算中共享(对于序列中的每个图元)。

图 [7-4](#Fig4) 显示了 RNN。注意隐藏状态下与自循环的递归关系。

![img/478491_2_En_7_Fig4_HTML.jpg](img/478491_2_En_7_Fig4_HTML.jpg)

图 7-4

RNN(使用以前的隐藏状态重复)

图 [7-4](#Fig4) 还描述了与每个输入相关的每个输出的损失函数。当讨论如何训练 rnn 时，我们将回头参考它。

将 RNN 与我们之前讨论的所有前馈神经网络(包括卷积网络)的不同内在化是至关重要的。关键的区别是隐藏状态，它表示过去看到的实体的汇总(对于同一序列)。

暂时忽略如何训练 RNN，应该清楚如何使用训练过的 RNN。对于给定的输入序列，RNN 将为输入中的每个实体生成一个输出。

现在让我们考虑 RNN 中的一种变化，其中我们使用前一状态产生的输出来代替使用隐藏状态的递归(图 [7-5](#Fig5) )。

![img/478491_2_En_7_Fig5_HTML.jpg](img/478491_2_En_7_Fig5_HTML.jpg)

图 7-5

RNN(使用先前输出的递归)

描述这样一个 RNN 的方程式如下:

![$$ {h}^{(t)}=\mathit{\tanh}\left(U{x}^{(t)}+W{\hat{y}}^{\left(t-1\right)}+b\right) $$](img/478491_2_En_7_Chapter_TeX_Equc.png)

![$$ {\hat{y}}^{(t)}= softmax\left(V{h}^{(t)}+c\right) $$](img/478491_2_En_7_Chapter_TeX_Equd.png)

应注意以下几点:

1.  RNN 计算包括计算序列中实体的隐藏状态。这用 *h* <sup>( *t* )</sup> 来表示。

2.  *h* <sup>( *t* )</sup> 的计算使用实体 *x* <sup>( *t* )</sup> 的相应输入和先前的输出![$$ {\hat{y}}^{\left(t-1\right)}. $$](img/478491_2_En_7_Chapter_TeX_IEq5.png)

3.  使用隐藏状态*h*T3】(*t*)计算输出![$$ {\hat{y}}^{(t)} $$](img/478491_2_En_7_Chapter_TeX_IEq6.png)。

4.  在计算当前隐藏状态时，一组权重与输入和先前输出相关联。这分别由 *U* 和 *W* 表示。还有一个偏差项，用 *c* 表示。

5.  计算输出时，权重与隐藏状态相关联。这由 *V* 表示。还有一个偏差项，用 *c* 表示。

6.  `tanh`激活函数用于隐藏状态的计算。

7.  softmax 激活函数用于计算输出。

现在让我们考虑 RNN 的一种变体，其中整个序列只产生一个输出(图 [7-6](#Fig6) )。这样的 RNN 是使用以下等式来描述的:

![$$ {h}^{(t)}=\mathit{\tanh}\left(U{x}^{(t)}+W{\hat{y}}^{\left(t-1\right)}+b\right) $$](img/478491_2_En_7_Chapter_TeX_Eque.png)

![$$ \hat{y}= softmax\left(V{h}^{\left(\tau \right)}\kern0.5em +c\right) $$](img/478491_2_En_7_Chapter_TeX_Equf.png)

![img/478491_2_En_7_Fig6_HTML.jpg](img/478491_2_En_7_Fig6_HTML.jpg)

图 7-6

RNN(为整个输入序列产生单个输出)

应注意以下几点:

1.  RNN 计算包括计算序列中实体的隐藏状态。这用 *h* <sup>( *t* )</sup> 来表示。

2.  *h* <sup>( *t* )</sup> 的计算使用实体 *x* <sup>( *t* )</sup> 处的相应输入和之前的隐藏状态*h*<sup>(*t*—1)</sup>。

3.  对输入序列*x*T8】(1)*x*<sup>(2)</sup>，…，*x*<sup>(*τ*)</sup>中的每个实体进行 *h* <sup>( *t* )</sup> 的计算。

4.  仅使用最后一个隐藏状态*h*T3(*τ*)来计算输出![$$ \hat{y} $$](img/478491_2_En_7_Chapter_TeX_IEq7.png)。

5.  在计算当前隐藏状态时，一组权重与输入和前一隐藏状态相关联。这分别由 *U* 和 *W* 表示。还有一个偏差项，用 *b* 表示。

6.  计算输出时，权重与隐藏状态相关联。这由 *V* 表示。还有一个偏差项，用 *c* 表示。

7.  `tanh`激活函数用于隐藏状态的计算。

8.  softmax 激活函数用于计算输出。

## 培训注册护士

本节描述了如何训练注册护士。我们首先需要看看当我们展开递归关系时，RNN 是什么样子的，递归关系是 RNN 的核心。展开对应于 RNN 的递归关系只是通过递归地替换定义递归关系的值来写出方程。

在图 [7-1](#Fig1) 中的 RNN 的情况下，这是*h*T4(*t*)。也就是说， *h* <sup>( *t* )</sup> 的值由*h*<sup>(*t*—1)</sup>定义，依次由*h*<sup>(*t*—2)</sup>定义，以此类推，直到 *h* <sup>(0) 【T29 我们将假设 *h* <sup>(0)</sup> 或者由用户预定义，设置为零，或者作为另一个参数/权重被学习(像 *W* 、 *V* 或 *b* 一样被学习)。*展开*简单来说就是写出用 *h* <sup>(0)</sup> 描述 RNN 的方程。当然，为了做到这一点，我们需要确定序列的长度，用 *τ* 表示。在这一节中，我们将探索展开我们上面探索的几个不同的 rnn。我们将从展开 RNN 开始，之前的隐藏状态用于递归(如图 [7-3](#Fig3) 所示)。稍后，我们也将探索同样的 RNN 使用以前的输出进行递归，并最终展开一个单输出的 RNN。</sup>

图 [7-7](#Fig7) 示出了与图 [7-4](#Fig4) 中的 RNN 相对应的展开的 RNN，假设输入序列的大小为 4。类似地，图 [7-8](#Fig8) 和图 [7-9](#Fig9) 分别示出了与图 [7-5](#Fig5) 和图 [7-6](#Fig6) 所示的 rnn 相对应的展开的 rnn。

![img/478491_2_En_7_Fig7_HTML.jpg](img/478491_2_En_7_Fig7_HTML.jpg)

图 7-7

展开图 [7-4](#Fig4) 对应的 RNN

图 [7-7](#Fig7) 展开图 [7-4](#Fig4) 所示的递归网络——即递归单元从之前的隐藏状态开始添加。我们可以通过将 h <sub>0</sub> 传递给 h <sub>1</sub> 来注意到这一点，这是 x <sup>(1)</sup> 的隐藏状态。类似地，隐藏状态 h <sub>3</sub> 被传递到 h <sub>4</sub> ，这是本图中的最后一步。权重 W 和偏差 b 在重复单元之间共享。

![img/478491_2_En_7_Fig8_HTML.jpg](img/478491_2_En_7_Fig8_HTML.jpg)

图 7-8

展开图 [7-5](#Fig5) 对应的 RNN

图 [7-8](#Fig8) 展开图 [7-5](#Fig5) 所示的递归网络——即从之前的输出状态增加递归单元。我们可以通过引用传递给 h <sub>1</sub> 的![$$ \hat{y} $$](img/478491_2_En_7_Chapter_TeX_IEq8.png) <sup>(0)</sup> 来注意到这一点，即 x <sup>(1)</sup> 的隐藏状态。类似地，输出状态![$$ \hat{y} $$](img/478491_2_En_7_Chapter_TeX_IEq9.png) <sup>(3)</sup> 被传递到 h <sub>4</sub> ，这是本图中的最后一步。权重 W 和偏差 b 在重复单元之间共享。

![img/478491_2_En_7_Fig9_HTML.jpg](img/478491_2_En_7_Fig9_HTML.jpg)

图 7-9

展开图 [7-6](#Fig6) 对应的 RNN(单输出)

展开过程基于输入序列的长度预先已知的假设进行操作，并基于此展开递归。一旦 RNN 展开，我们基本上就有了一个非循环神经网络。

需要学习的参数— *U* 、 *W* 、 *V* 、 *b* 、 *c* 等。(在图 [7-9](#Fig9) 中用黑色表示)-在隐藏层和输出值的计算中共享。我们之前在卷积神经网络的上下文中已经看到了这样的参数共享。

给定给定大小的输入和输出(例如， *τ* ，在图 [7-7](#Fig7) 到 [7-9](#Fig9) 中假设为 4)，我们可以展开 RNN，并计算要学习的参数相对于损失函数的梯度(如前面章节所述)。

因此，训练 RNN 简单地首先展开给定大小的输入和相应的期望输出的 RNN，然后通过计算梯度和使用随机梯度下降来训练展开的 RNN。

如前所述，RNNs 可以处理任意长的输入；相应地，它们需要在任意长的输入上被训练。图 [7-10](#Fig10) 至 [7-12](#Fig12) 展示了如何针对不同尺寸的输入展开 RNN。请注意，一旦 RNN 展开，训练 RNN 的过程与训练常规神经网络的过程相同，如前几章所述。在图[7-10](#Fig10)1-7-11 . 1 . 3 中，图 [7-4](#Fig4) 中描述的 RNN 对于输入尺寸 1、2、3 和 4 展开。

![img/478491_2_En_7_Fig10_HTML.jpg](img/478491_2_En_7_Fig10_HTML.jpg)

图 7-10

展开图 [7-4](#Fig4) 对应的 RNN(步骤 1 和步骤 2)

图 [7-10](#Fig10) 展示了步骤 1 和步骤 2——即依次展开输入序列 x <sup>(1)</sup> 和 x <sup>(2)</sup> 。在步骤 1 中，假设我们没有先前的隐藏状态，我们将 h <sup>(0)</sup> 传递给当前的隐藏状态。在图 [7-10](#Fig10) 中，我们将时间序列限制为展开，即*τ*= 4；因此，网络展开为 4 步。图 [7-11](#Fig11) 和图 [7-12](#Fig12) 依次演示了增量展开步骤。

![img/478491_2_En_7_Fig11_HTML.jpg](img/478491_2_En_7_Fig11_HTML.jpg)

图 7-11

展开图 [7-4](#Fig4) 对应的 RNN(步骤 3)

这里，我们将第三个输入序列连接到展开的网络。权重 U、W 和 V 在整个网络中共享。在下一个也是最后一个步骤中，我们可以看到展开的网络与图 [7-7](#Fig7) 中所示的网络相同(即，针对四个输入序列展开)。

![img/478491_2_En_7_Fig12_HTML.jpg](img/478491_2_En_7_Fig12_HTML.jpg)

图 7-12

展开图 [7-4](#Fig4) 对应的 RNN(步骤 4) |与图 [7-7](#Fig7) 相同

假设要训练的数据集由不同大小的序列组成，输入序列被分组，使得相同大小的序列归入一组。然后，对于一个组，我们可以展开序列长度的 RNN 并训练它。针对不同组的训练将需要针对不同的序列长度展开 RNN。因此，可以通过展开来训练不同大小输入的 RNN，并根据序列长度展开来训练它。

必须注意的是，训练图 [7-4](#Fig4) 中所示的展开的 RNN 本质上是一个连续的过程，因为隐藏状态是相互依赖的。在递归超过输出而不是隐藏状态的 RNNs 的情况下(图 [7-5](#Fig5) ，可以使用一种叫做*老师强制*的技术，如图 [5-9](#Fig9) 所示。这里的关键思想是训练时在*h*<sup>(*t*)</sup>的计算中用*y*<sup>(*t*—1)</sup>代替![$$ {\hat{y}}^{\left(t-1\right)} $$](img/478491_2_En_7_Chapter_TeX_IEq10.png)。然而，在进行预测时(当模型被部署使用时)，使用了![$$ {\hat{y}}^{\left(t-1\right)} $$](img/478491_2_En_7_Chapter_TeX_IEq11.png)。

## 双向 RNNs

现在让我们看看 RNNs 的另一种变体，双向 RNN。双向 RNN 背后的关键思想是使用序列中位于更远处的实体来对当前实体进行预测。对于我们到目前为止考虑的所有 rnn，我们一直使用序列中的先前实体(由隐藏状态捕获)和当前实体来进行预测。然而，我们并没有利用序列中更靠后的实体的信息来进行预测。双向 RNN 利用这些信息，在许多情况下可以提高预测的准确性(图 [7-13](#Fig13) )。

考虑下面这个简单的例子，它来自吴恩达的 Coursera 讲座:

*   他说，“泰迪熊是漂亮的玩具。”

*   他说，“泰迪·罗斯福，美国总统。”

在这些句子中，考虑到 NLP 的一个经典案例(预测下一个单词)，没有办法正确预测“Teddy”之后的单词(假设单向向前 RNN)。来自右侧的上下文本质上揭示了对下一个单词的准确预测。考虑一个情感分析任务，其中一个模型试图将句子分类为肯定或否定。随着网络中左右语境的建立，双向模型可以有效地在句子中“向前看”,以查看“未来”标记是否会影响当前决策。在情绪分类(多对一 RNN)的情况下，有一些讽刺性的评论，其中肯定词后面的词否定了肯定词的存在——例如，“我喜欢这部电影，有史以来最大的笑话！”在这里，右边的上下文否定了“爱”这个词的存在。

双向 RNN 可以使用以下等式来描述:

![$$ {h}_f^{(t)}=\mathit{\tanh}\left({U}_f{x}^{(t)}+{W}_f{h}^{\left(t+1\right)}+{b}_f\right) $$](img/478491_2_En_7_Chapter_TeX_Equg.png)

![$$ {h}_b^{(t)}=\mathit{\tanh}\left({U}_b{x}^{(t)}+{W}_b{h}^{\left(t-1\right)}+{b}_b\right) $$](img/478491_2_En_7_Chapter_TeX_Equh.png)

![$$ {\hat{y}}^{(t)}= softmax\left({V}_b{h}_b^{(t)}+{V}_f{h}_f^{(t)}+c\right) $$](img/478491_2_En_7_Chapter_TeX_Equi.png)

RNN 计算包括计算序列中实体的前向隐藏状态和后向隐藏状态。这分别由![$$ {h}_f^{(t)} $$](img/478491_2_En_7_Chapter_TeX_IEq12.png)和![$$ {h}_b^{(t)} $$](img/478491_2_En_7_Chapter_TeX_IEq13.png)表示。![$$ {h}_f^{(t)} $$](img/478491_2_En_7_Chapter_TeX_IEq14.png)的计算使用实体*x*T5(*t*)和先前隐藏状态![$$ {h}_f^{\left(t-1\right)}. $$](img/478491_2_En_7_Chapter_TeX_IEq15.png)的相应输入![$$ {h}_b^{(t)} $$](img/478491_2_En_7_Chapter_TeX_IEq16.png)的计算使用实体*x*<sup>(*t*)</sup>和先前隐藏状态![$$ {h}_b^{\left(t-1\right)}. $$](img/478491_2_En_7_Chapter_TeX_IEq17.png)的相应输入

使用隐藏状态![$$ {h}_f^{(t)} $$](img/478491_2_En_7_Chapter_TeX_IEq19.png)和![$$ {h}_b^{(t)} $$](img/478491_2_En_7_Chapter_TeX_IEq20.png)计算输出![$$ {\hat{y}}^{(t)} $$](img/478491_2_En_7_Chapter_TeX_IEq18.png)。在计算当前隐藏状态时，一组权重与输入和前一隐藏状态相关联。这分别用 *U* <sub>* f *</sub> ， *W* <sub>* f *</sub> ， *U* <sub>* b *</sub> ， *W* <sub>* b *</sub> 来表示。还有偏置项，分别用 *b* <sub>* f *</sub> 和 *b* <sub>* b *</sub> 表示。

类似地，在计算输出时，一组权重与计算输出时的隐藏状态相关联。这用 *V* <sub>*b*</sub> 和 *V* <sub>*f*</sub> 来表示。还有一个偏置项，用 *c* 表示。`tanh`激活函数用于隐藏状态的计算。softmax 激活函数用于计算输出。

如等式所述，RNN 可以处理任意大的输入序列。RNN 的参数—*U*<sub>T3】fT5】，*U*<sub>T9】b</sub>， *W* <sub>*f*</sub> ， *W* <sub>*b*</sub> ， *V* <sub>*b*</sub> ， *V* -在隐藏层和输出值的计算中共享(对于序列中的每个图元)。</sub>

![img/478491_2_En_7_Fig13_HTML.jpg](img/478491_2_En_7_Fig13_HTML.jpg)

图 7-13

双向 RNN

## 消失和爆炸渐变

由于消失和爆炸梯度，训练 rnn 可能具有挑战性(图 [7-14](#Fig14) )。消失梯度意味着当在展开的 rnn 上计算梯度时，梯度的值可以下降到非常小的数字(接近零)。类似地，梯度可以增加到非常高的值，这被称为*爆炸梯度问题*。在这两种情况下，训练 RNN 都是一个挑战。消失或爆炸梯度通常是为网络超参数和参数设置不适当或不需要的值的结果。因此，随着每次增量权重更新，网络需要花费异常长的时间来脱离斜率，并学习用例的最佳权重。

让我们再来看看描述 RNN 的方程。

![$$ {h}^{(t)}=\mathit{\tanh}\left(U{x}^{(t)}+W{h}^{\left(t-1\right)}+b\right) $$](img/478491_2_En_7_Chapter_TeX_Equj.png)

![$$ {\hat{y}}^{(t)}= softmax\left(V{h}^{(t)}+c\right) $$](img/478491_2_En_7_Chapter_TeX_Equk.png)

我们可以通过应用链式法则推导出![$$ \frac{\partial L}{\partial W} $$](img/478491_2_En_7_Chapter_TeX_IEq21.png)的表达式。如图 [7-10](#Fig10) 所示。

![$$ \frac{\partial L}{\partial W}={\sum}_{1\le t\le \tau}\frac{\partial {L}^{(t)}}{\partial {h}^{(t)}}\left[{\sum}_{1\le k\le t}\left[{\prod}_{k\le j\le t-1}\frac{\partial {h}^{\left(j+1\right)}}{\partial {h}^{(j)}}\right]\ \frac{\partial {h}^{(k)}\ }{\partial W}\right] $$](img/478491_2_En_7_Chapter_TeX_Equl.png)

现在让我们关注表达式![$$ {\prod}_{k\le j\le t-1}\frac{\partial {h}^{\left(j+1\right)}}{\partial {h}^{(j)}} $$](img/478491_2_En_7_Chapter_TeX_IEq22.png)的部分，它涉及 *W* 的重复矩阵乘法，这有助于消失和爆炸梯度问题。直觉上，这类似于一个实数值一次又一次地相乘，这可能导致乘积缩小到零或爆炸到无穷大。

## 渐变剪辑

处理爆炸梯度的一个简单技术是，每当梯度超过用户定义的阈值时，重新调整梯度的范数。具体来说，如果梯度用![$$ \hat{g}=\frac{\partial L}{\partial W} $$](img/478491_2_En_7_Chapter_TeX_IEq23.png)表示，如果![$$ \left\Vert \hat{g}\ \right\Vert &gt;c $$](img/478491_2_En_7_Chapter_TeX_IEq24.png)，那么我们设置![$$ \hat{g}=\frac{c}{\left\Vert \hat{g}\right\Vert }\ \hat{g} $$](img/478491_2_En_7_Chapter_TeX_IEq25.png)。这种技术既简单又计算高效，但它确实引入了一个额外的超参数。

如果没有梯度裁剪，参数会大幅下降并流出所需区域。通过限幅，下降步长被限制，并且参数保持在期望的区域内。渐变裁剪将“裁剪”渐变，或者将它们限制在一个阈值，以防止它们变得太大。在图 [7-14](#Fig14) 中，梯度因过冲而被剪切，成本函数遵循虚线值，而不是其在期望区域外的原始轨迹。

![img/478491_2_En_7_Fig14_HTML.jpg](img/478491_2_En_7_Fig14_HTML.jpg)

图 7-14

渐变剪辑

## 长短期记忆

让我们来看看 RNNs 的另一种变体，长短期记忆(LSTM)网络(见图 [7-15](#Fig15) )。香草 RNN 有几个权衡，导致网络在学习序列之间的长相关性时表现不佳。总的来说，RNN 更容易产生噪音，在训练时容易过度疲劳。训练它们在计算上也非常昂贵。

LSTMs 非常适合通过使用更直观的方法来解决这些问题。与 rnn 相比，它们通常对噪声更鲁棒，并且更准确地捕捉短期和长期相关性，同时易于调整和训练。LSTMs 还具有比 rnn 更快的计算速度。LSTMs 具有配备便利功能的门，这些功能帮助网络记住长期依赖关系以及忘记无关紧要的依赖关系。在 RNNs 中，先前的隐藏状态是网络记得的唯一先前的记忆。有了 LSTMs，除了之前的隐藏状态，小区状态也被网络记住。

LSTM 网络的核心概念是单元状态和门(输入、输出和遗忘门)。这些门和单元状态包括几个操作，例如 sigmoid 和 tanh 激活、逐点乘法和加法以及向量连接。这些操作帮助单元状态和门训练网络忘记或通过网络传播重要信息。细胞状态连接整个网络的信息，从而有助于在需要时传递序列之间的长依赖性。

LSTM 可以用下面的一组等式来描述。请注意，⨀符号表示两个向量的逐点相乘——也就是说，如果 *a* = [1，1，2]并且 *b* = [0.5，0.5，0.5]，那么 *a* ⨀ *b* = [0.5，0.5，1]。函数 *σ* 、 *g* 和 *h* 为非线性激活函数； *W* 和 *R* 为权重矩阵；并且 *b* 项是偏置项。

![$$ {z}^{(t)}=g\left({W}_z{x}^{(t)}+{R}_z\kern0.5em {\hat{y}}^{\left(t-1\right)}+{b}_z\right) $$](img/478491_2_En_7_Chapter_TeX_Equm.png)

![$$ {i}^{(t)}=\sigma \left({W}_i{x}^{(t)}+{R}_i\ {\hat{y}}^{\left(t-1\right)}+{p}_i\bigodot {c}^{\left(t-1\right)}+{b}_i\right) $$](img/478491_2_En_7_Chapter_TeX_Equn.png)

![$$ {f}^{(t)}=\sigma \left({W}_f{x}^t+{R}_f\ {\hat{y}}^{\left(t-1\right)}+{p}_f\bigodot {c}^{\left(t-1\right)}+{b}_f\right) $$](img/478491_2_En_7_Chapter_TeX_Equo.png)

![$$ {c}^{(t)}={i}^{(t)}\bigodot {z}^{(t)}+{f}^{(t)}\bigodot {c}^{\left(t-1\right)} $$](img/478491_2_En_7_Chapter_TeX_Equp.png)

![$$ {o}^{(t)}=\sigma \left({W}_o{x}^{(t)}+{R}_o\ {\hat{y}}^{\left(t-1\right)}+{p}_o\bigodot {c}^{(t)}+{b}_o\right) $$](img/478491_2_En_7_Chapter_TeX_Equq.png)

![$$ {\hat{y}}^{(t)}={o}^{(t)}\bigodot h\left({c}^{(t)}\right) $$](img/478491_2_En_7_Chapter_TeX_Equr.png)

应注意以下几点:

1.  LSTM 最重要的元素是细胞状态，用*c*T2】(*t*)=*I*<sup>(*t*)</sup>⨀*z*<sup>(*t*)</sup>+*f*<sup>(*t*)</sup>⨀*c*基于块输入*z*T32】(*t*)和先前单元状态*c*<sup>(*t*—1)</sup>更新单元状态。输入门*I*t<sup>(*t*)</sup>确定块输入的哪一部分进入单元状态(因此称为*门*)。遗忘门*f*<sup>(*t*)</sup>决定了要保留多少先前的单元格状态。**

2.  输出![$$ {\hat{y}}^{(t)} $$](img/478491_2_En_7_Chapter_TeX_IEq26.png)由单元状态*c*T3】(*t*)和输出门 *o* <sup>( * t * )</sup> 决定，决定了单元状态对输出的影响程度。

3.  *z* <sup>( *t* )</sup> 项，称为*块输入*，根据当前输入和先前输出产生一个值。

4.  *i* <sup>( *t* )</sup> 项，称为*输入门*，决定了在单元状态 *c* <sup>( *t* )</sup> 下保留多少输入。

5.  所有的 *p* 项都是窥视孔连接，允许单元状态的一部分考虑到所讨论的项的计算中。

6.  单元格状态 *c* <sup>( *i* )</sup> 的计算不会遇到渐变消失的问题。(这被称为*恒定误差旋转*。)然而，LSTMs 受到爆炸梯度的影响，并且在训练时使用梯度剪裁。

![img/478491_2_En_7_Fig15_HTML.png](img/478491_2_En_7_Fig15_HTML.png)

图 7-15

一个长短期记忆网络

## 实际实施

本节描述了一个用 PyTorch 实现 RNN 和 LSTM 的实例。我们将把练习分成两部分。首先，我们将只使用没有额外处理的普通 RNN 网络(来自 NLP 领域),并在情感分类数据集上训练网络。我们预计这种普通的网络性能会很差。第二，我们将对网络进行重大改进。我们将利用 LSTM 层，而不是 RNN 层，并使网络双向辍学正规化。这样的网络在我们的数据集上会表现得更好。

我们将使用 TorchText 包，它由数据处理工具和 NLP 的流行数据集组成。我们将利用位于 [`https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format`](https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format) 的 Kaggle 上托管的数据集。

我们建议利用 Kaggle 笔记本进行练习(打开互联网选项并启用 GPU 加速器)。

让我们从导入基本包开始(清单 [7-1](#PC1) )。

```py
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import torch
from torch import nn,optim
import torchtext
from torchtext import data

#Check if we have GPU enabled
if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"
print("Device =",device)

input_data_path = "/kaggle/input/imdb-dataset-sentiment-analysis-in-csv-format/"

Listing 7-1Importing the Packages for the RNN

```

首先，让我们使用 Pandas 在高层次上探索数据集。这里的目标是对数据集有一个粗略的了解。对于本练习的剩余部分，我们将使用基于 TorchText 的包装器来处理 NLP 领域内的训练和验证数据集。清单 [7-2](#PC2) 将我们用例的数据读入内存。

![img/478491_2_En_7_Figa_HTML.jpg](img/478491_2_En_7_Figa_HTML.jpg)

```py
#Read the csv dataset using pandas
df = pd.read_csv("/input/imdb-dataset-sentiment-analysis-in-csv-format/Train.csv")
print("DF.shape :\n",df.shape)
print("df.label = ",df.label.value_counts())
df.head()

Output[]
DF.shape :  (40000, 2)

df.label =  0    20019
            1    19981
Name: label, dtype: int64

Listing 7-2Reading Data into Memory

```

我们在数据集中只有两列:“文本”，包含实际的注释，“标签”，包含值 0(负)和 1(正)。正负之间的分布相当均匀。

接下来，我们将使用 TorchText 数据集包装器，它将帮助我们创建基于迭代器的数据集，简化我们需要的数据处理任务。如清单 [7-3](#PC3) 所示，我们从定义训练和验证数据集所需的原始数据类型开始。

```py
#Define a custom tokenizer
my_tokenizer  = lambda x:str(x).split()

#Define fields for our input dataset
TEXT = data.Field(sequential=True, lower= True,tokenize = my_tokenizer,use_vocab=True)
LABEL  = data.Field(sequential = False,use_vocab = False)

#Define inut fields as a list of tuples of fields
trainval_fields = [("text",TEXT),("label",LABEL)]

#Contruct dataset
train_data, val_data = data.TabularDataset.splits(path = input_data_path, train = "Train.csv", validation = "Valid.csv", format = "csv", skip_header = True, fields = trainval_fields)

#Build vocabulary
MAX_VOCAB_SIZE = 25000
TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)

#Define iterators for  train and validation
train_iterator  = data.BucketIterator(train_data, device = device
                             , batch_size = 32
                             , sort_key = lambda x:len(x.text)
                             ,sort_within_batch = False
                             ,repeat = False)

val_iterator = data.BucketIterator(val_data, device = device,
                             batch_size= 32
                             , sort_key = lambda x:len(x.text)
                             , sort_within_batch = False
                             , repeat = False)

print(TEXT.vocab.freqs.most_common()[:10])

Output[]
[('the', 511112), ('a', 253702), ('and', 251397), ('of', 229381), ('to', 211883)
, ('is', 164005), ('in', 143530), ('i', 113576), ('this', 110892), ('that', 104153)]

Listing 7-3Defining the Tokenizer, Fields, and Dataset for Training and Validation

```

在清单 [7-3](#PC3) 中，我们处理了一些我们的网络所必需的东西。对于 NLP 用例，在使用数据进行网络训练之前，作为文本处理的一部分，我们需要对数据进行标记化和数值化。你可能已经猜到了，神经网络只处理数字数据。上述两项操作都由 PyTorch 内部巧妙处理。我们可以提供一个现有的标记器——例如 SpaCy(一个开源的高级 NLP 库)PyTorch 会完成剩下的工作。在这个例子中，我们使用一个定制的简单的。接下来，我们为数据集定义必要的字段(原始数据)。`Field`类对可以用张量表示的常见文本处理数据类型进行建模。此外，它还保存了一个`Vocab`对象，该对象定义了承载字段中出现的所有单词的数字表示的向量。我们的数据集有两列，“文本”和“标签”，前者是简单的英文注释，后者是数字标签(0/1)。因此，我们将 TEXT 和 LABEL 定义为代表我们的列的两个单独的字段。我们添加了一个参数来定义这个字段所需的标记化函数，一个布尔标志来将文本转换为小写，一个布尔标志来指示这个字段中的数据是连续的。对于标签字段，我们没有顺序数据；因此，我们将其设置为 False。

接下来，我们定义创建数据集时需要的数据字段列表。该列表表示数据集中的每一列。如果我们计划不使用这个数据集中的某个特定列，那么在定义列的列表时，我们需要将列名指定为“None”。我们将这个列表分配给`trainval_fields`变量。然后，我们创建一个`TabularDataset`对象，其中包含对数据列进行必要操作的精简列表。请注意，`splits()`函数实际上并不分割现有的数据集。只有当路径中已经有单独分离的数据集时，才应该使用它。

接下来，我们需要构建词汇表(在我们的字段文本中出现的唯一单词的数字化表示)。这一步非常重要，有几种执行手段。我们可以使用预训练的单词嵌入来创建词汇，或者我们可以定制一个。使用预训练的很简单，所以我们将在下一个例子中使用它。我们将最大词汇量设置为 25，000。该函数还将创建两个额外的单词，总数为 25，002—一个用于所有未知的标记(例如，新单词)，另一个用于填充(用于生成等长的句子)。

最后，我们创建迭代器对象。`sort_within_batch`参数根据`sort_key`对每个小批量内的数据进行降序排序。当我们想将`pack_padded_sequence`用于填充的序列数据并将填充的序列张量转换为`PackedSequence`对象时，这是必要的。我们不会在第一个练习中利用这个特性，但是我们将在下一个练习中使用它，在下一个练习中我们将改进我们的模型。本质上，PyTorch 在序列中添加了填充符，这样所有序列的长度都相等。通过按关键字的降序对数据进行排序，该过程变得高效，并确保网络不会学习填充。最后一行打印 vocab 中最常用的单词，并返回与向量中每个单词相关的索引(嵌入)。

准备好要处理的数据后，我们将构建我们的 RNN 类，如清单 [7-4](#PC4) 所示。

```py
class RNNModel(nn.Module):

    def __init__(self,embedding_dim,input_dim,hidden_dim,output_dim):
        super().__init__()
        self.Embedding  = nn.Embedding(input_dim,embedding_dim)
        self.rnn  = nn.RNN(embedding_dim,hidden_dim)
        self.fc  = nn.Linear(hidden_dim,output_dim)

    def forward(self,text):
        embed = self.Embedding(text)
        output, hidden = self.rnn(embed)
        out  = self.fc(hidden.squeeze(0))
        return(out)

#Define model
INPUT_DIM = len(TEXT.vocab)
EMBEDDING_DIM = 100
HIDDEN_DIM = 256
OUTPUT_DIM = 1

#Create model instance
model = RNNModel(EMBEDDING_DIM, INPUT_DIM,HIDDEN_DIM, OUTPUT_DIM)

Listing 7-4Defining the RNN Class

```

这段代码的很大一部分与我们在第 [5](5.html) 和 [6](6.html) 章中的实验非常相似。这里新增加的是嵌入层和 RNN 层。RNN 层返回输出以及隐藏层计算(不像我们到目前为止探索的其他层)。输入维度是我们的 vocab 列表的长度。嵌入维数是我们决定在数字上最能代表一个单词的一个值。我们这里用 100，但也可能是 200，300，或者更高。更大的数字并不总是有价值的，而且会显著增加计算量。此外，我们为隐藏层选择 256 维，为输出层选择 1 维(因为结果是二进制的)。

接下来，在清单 [7-5](#PC5) 中，我们定义了两个函数，这两个函数将包装给定时期的训练步骤和评估步骤。随后，我们通过另一个函数为每个时期编排训练步骤和评估步骤。

```py
#Define training step
def train(model, data_iterator,optimizer,loss_function):
    epoch_loss,epoch_acc,epoch_denom = 0,0,0

    model.train()    #Explicitly set model to train mode

    for i, batch in enumerate(data_iterator):

        optimizer.zero_grad()
        predictions = model(batch.text)

        loss = loss_function(predictions.reshape(-1,1), batch.label.float().reshape(-1,1))
        acc = accuracy(predictions.reshape(-1,1), batch.label.reshape(-1,1))

        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        epoch_acc += acc.item()
        epoch_denom += len(batch)

    return epoch_loss/epoch_denom,epoch_acc, epoch_denom

#Define evaluation step
def evaluate(model, data_iterator,loss_function):
    epoch_loss,epoch_acc,epoch_denom = 0,0,0

    model.eval()     #Explcitly set model to eval mode

    for i, batch in enumerate(data_iterator):
        with torch.no_grad():
            predictions = model(batch.text)

            loss = loss_function(predictions.reshape(-1,1), batch.label.float().reshape(-1,1))
            acc = accuracy(predictions.reshape(-1,1), batch.label.reshape(-1,1))

            epoch_loss += loss.item()
            epoch_acc += acc.item()
            epoch_denom += len(batch)

    return epoch_loss/epoch_denom, epoch_acc, epoch_denom

Listing 7-5Defining the Training and Evaluation Step

```

在这里，内容与前面的实验相似。我们为训练循环创建必要的样板代码。注意，我们在 evaluate 函数中需要一个助手函数来计算精度(在我们的例子中是二进制结果)。这一部分不是强制性的，但它有助于在每个时期后准确地查看中间结果。清单 [7-6](#PC6) 为我们的网络定义了功能和必要的位。

```py
#Compute binary accuracy
def accuracy(preds, y):
    rounded_preds = torch.round(torch.sigmoid(preds))

    #Count the number of correctly predicted outcomes
    correct = (rounded_preds == y).float()
    acc = correct.sum()

    return acc

#Define optimizer, loss function
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.BCEWithLogitsLoss()

#Transfer components to GPU, if available.
Model = model.to(device)
criterion = criterion.to(device)

Listing 7-6Defining the Accuracy Function, Loss Function, and Optimizer, and Instantiating the Model

```

最后，在清单 [7-7](#PC7) 中，我们用定义损失函数和优化器在五个时期的循环中训练上面实例化的模型。我们在这里定义 5 只是为了说明的目的；对于实际的例子，我们建议根据数据的大小和网络的复杂性增加历元的数量。

```py
n_epochs = 5

for epoch in range(n_epochs):
    #Train and evaluate
    train_loss, train_acc,train_num = train(model, train_iterator, optimizer, criterion)
    valid_loss, valid_acc,val_num = evaluate(model, val_iterator,criterion)

    print("Epoch-",epoch)

    print(f'\tTrain  Loss: {train_loss: .3f} | Train Predicted Correct : {train_acc}
                                   | Train Denom: {train_num} |
                          PercAccuracy: {train_acc/train_num}')
    print(f'\tValid  Loss: {valid_loss: .3f} | Valid Predicted Correct: {valid_acc}
                                        | Val Denom: {val_num}|
                          PercAccuracy: {train_acc/train_num}')
Output[]
Epoch -0
Train  Loss:  0.022 | Train Predicted Correct : 20149.0 | Train Denom: 40000 | PercAccuracy: 0.503725
Valid  Loss:  0.022 | Valid Predicted Correct: 2537.0 | Val Denom: 5000| PercAccuracy: 0.503725

Epoch -1
Train  Loss:  0.022 | Train Predicted Correct : 20048.0 | Train Denom: 40000 | PercAccuracy: 0.5012
Valid  Loss:  0.022 | Valid Predicted Correct: 2497.0 | Val Denom: 5000| PercAccuracy: 0.5012

Epoch -2
Train  Loss:  0.022 | Train Predicted Correct : 20023.0 | Train Denom: 40000 | PercAccuracy: 0.500575
Valid  Loss:  0.022 | Valid Predicted Correct: 2507.0 | Val Denom: 5000| PercAccuracy: 0.500575

Epoch -3
Train  Loss:  0.022 | Train Predicted Correct : 20143.0 | Train Denom: 40000 | PercAccuracy: 0.503575
Valid  Loss:  0.022 | Valid Predicted Correct: 2556.0 | Val Denom: 5000| PercAccuracy: 0.503575

Epoch -4
Train  Loss:  0.022 | Train Predicted Correct : 19996.0 | Train Denom: 40000 | PercAccuracy: 0.4999
Valid  Loss:  0.022 | Valid Predicted Correct: 2492.0 | Val Denom: 5000| PercAccuracy: 0.4999

Listing 7-7Training the Model for Five Epochs

```

我们可以看到该模型的性能几乎没有提高。虽然五个纪元其实太少了，但我们应该已经看到了小的变化。整体准确性并没有真正增加模型的任何价值。性能差。为了改善我们的结果，我们将在第二次实验中采取更全面的方法。

在我们的第二个实验中，我们将利用 Spacy 的标记器(而不是使用我们的自定义标记器)和预训练的单词嵌入(而不是从头开始训练)，并添加双向 LSTM 层(而不是单向 RNN 层)。我们还将增加辍学，以减少过度拟合。

我们实际上需要从头开始，而不是继续使用相同的代码库(尽管变化很小)。

像往常一样，我们从导入所需的包开始，如清单 [7-8](#PC8) 所示。

```py
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import torch,torchtext
from torch import nn, optim
from torch.optim import Adam
from torchtext import data

if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"
print("Device =",device)

input_data_path = " /input/imdb-dataset-sentiment-analysis-in-csv-format/"

#Define fields for our input dataset
TEXT = data.Field(sequential=True, lower= True,tokenize = 'spacy', include_lengths = True)
LABEL  = data.Field(sequential = False,use_vocab = False)

#Define a list of tuples of fields
trainval_fields = [("text",TEXT),("label",LABEL)]

#Contruct dataset
train_data, val_data = data.TabularDataset.splits(path = input_data_path, train = "Train.csv", validation = "Valid.csv", format = "csv", skip_header = True, fields = trainval_fields)

#Build Vocab using pretrained
MAX_VOCAB_SIZE = 25000
TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE,   vectors = 'fasttext.simple.300d')
BATCH_SIZE = 64

train_iterator, val_iterator =  data.BucketIterator.splits(
                              (train_data, val_data),
                              batch_size = BATCH_SIZE,
                              sort_key  = lambda x:len(x.text),
                              sort_within_batch = True,
                              device = device)

Listing 7-8Importing the Required Packages

```

我们将只关注前面代码片段中的变化。在定义数据字段时，我们使用了 Spacy 的 tokenizer。使用字符串`spacy`作为 tokenize 参数就足够了；PyTorch 在后端管理必要的繁重工作。我们还添加了参数`include_length`作为`true`。这是必要的，因为我们稍后会添加填充并对一批中的样本进行排序。为了利用这一点，我们现在需要将样本的长度和文本一起传递给 RNN 模型的类定义中的 forward 函数。

在构建词汇表时，我们使用`vectors = 'fasttext.simple.300d'`告诉 PyTorch 下载预先训练好的 fasttext 向量，并为我们的文本字段中的单词创建一个嵌入向量。(如果使用的是 Kaggle 内核，应该在笔记本环境设置中开启互联网选项)。这个预训练的向量有 300 个维度。在创建网络实例时，我们需要注意这一变化。这一步可能需要一段时间，取决于你的网速。最后，我们还启用了排序并定义了排序键。PyTorch 下载已定义的预训练向量(通常为 300MN 或更多),并基于 25，000 个令牌为我们的用例创建一个子集。

现在让我们定义我们改进的序列模型，如清单 [7-9](#PC9) 所示。

```py
class ImprovedRNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):

        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)
        self.lstm = nn.LSTM(embedding_dim,
                           hidden_dim,
                           num_layers=n_layers,
                           bidirectional=bidirectional,
                           dropout=dropout)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, text, text_lengths):

        embedded = self.dropout(self.embedding(text))

        #pack sequence
        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)
        packed_output, (hidden, cell) = self.lstm(packed_embedded)

        #unpack sequence
        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)
        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))

        return self.fc(hidden)

Listing 7-9Defining the (Improved) RNN Class

```

请注意，我们在这里做了相当多的添加。我们现在有一个 LSTM 层，而不是香草 RNN。当`bidirectional`标志被设置为`True`时，它使我们能够捕捉前向和后向上下文。线性层的尺寸现在将是原始层的两倍，因为我们有一个串联的前向和后向网络。我们最初在定义最初的`FIELD;`时添加了`include_lengths=True`，因此，我们的转发函数现在将接受一个额外的参数。在从嵌入输出接收数据后，在将数据传递到线性层之前，打包和解包数据时，此信息是必需的。隐藏层现在将前向和后向网络的输出连接起来，然后再传递给下一层。清单 [7-10](#PC10) 定义了模型属性并复制了预训练的权重。

```py
#Define model input parameters
INPUT_DIM = len(TEXT.vocab)
EMBEDDING_DIM = 300
HIDDEN_DIM = 256
OUTPUT_DIM = 1
N_LAYERS = 2
BIDIRECTIONAL = True
DROPOUT = 0.5

#Create model instance
model = ImprovedRNN(INPUT_DIM,
            EMBEDDING_DIM,
            HIDDEN_DIM,
            OUTPUT_DIM,
            N_LAYERS,
            BIDIRECTIONAL,
            DROPOUT,
            PAD_IDX)

#Copy pretrained vector weights
model.embedding.weight.data.copy_(pretrained_embeddings)

#Initialize the embedding with 0 for pad as well as unknown tokens
UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]
model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)
PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]
model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)

print(model.embedding.weight.data)

Output []
torch.Size([25002, 300])

Listing 7-10Defining the Model Properties and Copying the Pretrained Weights

```

接下来，我们定义训练和评估函数，类似于我们之前的练习。唯一的区别是，我们需要将`text_lengths`作为模型中的一个附加参数来处理。我们还将定义计算二进制精度所需的精度函数，定义模型的损失函数、优化器，并在 GPU 上加载模型和损失函数(如果可用)。这些步骤与我们之前的练习相同。在清单 [7-11](#PC11) 中，我们训练我们改进的模型定义。

```py
#Define train step
def train(model, iterator, optimizer, criterion):

    epoch_loss,epoch_acc,epoch_denom = 0,0,0

    model.train()

    for batch in iterator:

        optimizer.zero_grad()
        text, text_lengths = batch.text
        predictions = model(text, text_lengths).squeeze(1)
        loss = criterion(predictions.reshape(-1,1), batch.label.float().reshape(-1,1))
        acc = accuracy(predictions, batch.label)

        loss.backward()

        optimizer.step()

        epoch_loss += loss.item()
        epoch_acc += acc.item()
        epoch_denom += len(batch)

    return epoch_loss/epoch_denom, epoch_acc, epoch_denom

#Define evaluate step
def evaluate(model, iterator, criterion):

    epoch_loss,epoch_acc,epoch_denom = 0,0,0
    model.eval()

    with torch.no_grad():
        for batch in iterator:
            text, text_lengths = batch.text
            predictions = model(text, text_lengths).squeeze(1)
            loss = criterion(predictions, batch.label.float())
            acc = accuracy(predictions, batch.label)
            epoch_loss += loss.item()
            epoch_acc += acc.item()
            epoch_denom += len(batch)

    return epoch_loss/epoch_denom, epoch_acc, epoch_denom

#Define optimizer, loss funciton and load to GPU
optimizer = optim.Adam(model.parameters())
criterion = nn.BCEWithLogitsLoss()

model = model.to(device)
criterion = criterion.to(device)

#similar to previous exercise, we deifne our accuracy function
def accuracy(preds, y):
    rounded_preds = torch.round(torch.sigmoid(preds))

    correct = (rounded_preds == y).float()
    acc = correct.sum()

    return acc

#Finally lets train our model for 5 epochs
N_EPOCHS = 5

for epoch in range(N_EPOCHS):

    train_loss, train_acc,train_num = train(model, train_iterator, optimizer, criterion)
    valid_loss, valid_acc,val_num = evaluate(model, val_iterator, criterion)
    print("Epoch-",epoch)
    print(f'\tTrain  Loss: {train_loss: .3f} | Train Predicted Correct : {train_acc}
                                   | Train Denom: {train_num} |
                          PercAccuracy: {train_acc/train_num}')
    print(f'\tValid  Loss: {valid_loss: .3f} | Valid Predicted Correct: {valid_acc}
                                       | Val Denom: {val_num}|
                          PercAccuracy: {train_acc/train_num}')

Output[]
     Train  Loss:  0.005 | Train Predicted Correct : 34911.0 | Train Denom: 40000 | PercAccuracy: 0.872775
     Valid  Loss:  0.003 | Valid Predicted Correct: 4558.0 | Val Denom: 5000| PercAccuracy: 0.872775
Epoch- 1
     Train  Loss:  0.003 | Train Predicted Correct : 37193.0 | Train Denom: 40000 | PercAccuracy: 0.929825
     Valid  Loss:  0.004 | Valid Predicted Correct: 4557.0 | Val Denom: 5000| PercAccuracy: 0.929825
Epoch- 2
     Train  Loss:  0.002 | Train Predicted Correct : 38079.0 | Train Denom: 40000 | PercAccuracy: 0.951975
     Valid  Loss:  0.003 | Valid Predicted Correct: 4591.0 | Val Denom: 5000| PercAccuracy: 0.951975
Epoch- 3
     Train  Loss:  0.002 | Train Predicted Correct : 38659.0 | Train Denom: 40000 | PercAccuracy: 0.966475
     Valid  Loss:  0.004 | Valid Predicted Correct: 4569.0 | Val Denom: 5000| PercAccuracy: 0.966475
Epoch- 4
     Train  Loss:  0.001 | Train Predicted Correct : 39030.0 | Train Denom: 40000 | PercAccuracy: 0.97575
     Valid  Loss:  0.004 | Valid Predicted Correct: 4564.0 | Val Denom: 5000| PercAccuracy: 0.97575

Listing 7-11Training the Improved Model

```

如你所见，性能提高了很多。我们只训练了五个纪元的网络，但结果令人印象深刻。建议读者通过对网络进行更改来进行试验。实验可以包括改变预先训练的向量(可能是 glove 而不是 fasttext)，在输入数据上处理更多 NLP 相关的动作，添加更多积极的退出，添加更多的纪元，等等。

我们的第二个练习到此结束，在这个练习中，我们试图提高序列模型的性能。我们使用了普通的 RNN 网络、LSTM 网络和双向网络。我们还利用预先训练的嵌入来实现单词的量化表示。(对于几乎所有与 NLP 相关的任务，强烈建议这样做。)还存在门控循环单元(gru ),其非常类似于 LSTMs，但是其计算速度稍快，因为它们具有较少的运算。然而，当谈到性能时，大多数研究人员发现 LSTMs 和 GRUs 非常相似。在 NLP 实验中，使用 LSTMs 和 GRUs 迭代，取其精华是很常见的。你可以在 [`https://arxiv.org/abs/1412.3555`](https://arxiv.org/abs/1412.3555) 阅读更多关于这项研究的内容。

讨论 GRUs 的细节超出了本章的范围。鼓励读者自己进一步探索这个话题。

## 摘要

在本章中，我们介绍了循环神经网络(RNNs)的基础知识。本章的要点是隐藏状态的概念，通过展开(通过时间的反向传播)训练 RNNs，消失和爆炸梯度的问题，以及长短期记忆(LSTM)网络。重要的是内在化 rnn 如何包含允许它们对一系列输入进行预测的内部/隐藏状态——这是一种超越传统神经网络的能力。**