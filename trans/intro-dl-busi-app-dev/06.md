# 六、强化学习和机器人技术

由于深度学习[GBC16]受益于大数据、强大计算和新算法技术的最新成就，我们一直在见证强化学习的复兴，特别是强化学习和深度神经网络的结合，即所谓的深度强化学习(deep RL)。深度 Q 网络(dqn)通过让机器在雅达利游戏和非常艰难的围棋棋盘游戏中实现超人的表现，点燃了深度 RL [MKS <sup>+</sup> 15]的领域。

众所周知，当用非线性函数(如神经网络)逼近动作值 Q 函数时，RL 是不稳定的。然而，DQNs 在提高学习的稳定性方面做出了一些贡献。

*   DQNs 使用带重放的 CNN 稳定了 Q-动作值函数近似的训练。
*   DQNs 使用端到端 RL 方法，仅将原始像素和游戏分数作为输入。
*   DQNs 使用具有相同算法、网络架构和超参数的灵活网络来玩不同的 Atari 游戏。

以下是 RL 的一些最新进展和架构:

*   深度 Q-networks [MBM <sup>+</sup> 16]帮助 AlphaGo [SHM <sup>+</sup> 16]击败了围棋世界冠军。
*   [深度强化学习的异步方法](https://arxiv.org/pdf/1602.01783.pdf)
*   [价值迭代网络](https://arxiv.org/abs/1602.02867)
*   指导性政策搜索[LFDA16]
*   [生成性对抗性模仿学习](https://arxiv.org/abs/1606.03476)
*   [无监督强化和辅助学习](https://arxiv.org/abs/1611.05397)
*   [神经架构设计](https://openreview.net/pdf?id=r1Ue8Hcxg)

## 6.1 什么是强化学习？

强化学习解决的是顺序决策问题，即在获得奖励之前需要几个步骤的问题，比如电子游戏。RL 代理通常会随着时间的推移与环境交互并改变它，因此它们在移动的背景下工作并追逐移动的目标。

在每个时间步 t，代理处于状态 s <sub>t</sub> 并遵循策略![$$ \pi \left({a}_t|{s}_t\right) $$](img/A454512_1_En_6_Chapter_IEq1.gif)从某个动作空间 A 中选择动作 a <sub>t</sub> ，这是代理的行为，换句话说，是从状态 s <sub>t</sub> 到动作 a <sub>t</sub> 的映射。它接收奖励 r <sub>t</sub> ，并且根据环境动态或模型，分别对于给定的奖励函数 R(s，a)和状态转移概率![$$ P\left({s}_{t+1}|{s}_t,{a}_t\right) $$](img/A454512_1_En_6_Chapter_IEq3.gif)，移动到下一个状态![$$ {s}_{t+1} $$](img/A454512_1_En_6_Chapter_IEq2.gif)。

价值函数是对预期的、累积的、贴现的、未来的回报的预测，衡量每个状态或状态-行动对有多好。这里，动作值是在状态 s 下选择动作 a，然后遵循策略π:

![$$ {Q}^{\pi}\left(s,a\right)=E\left[{R}_t|{s}_t=s,{a}_t=a\right] $$](A454512_1_En_6_Chapter_Equ1.gif)

(6.1)

最佳动作值函数 Q* (s，a)是状态 s 和动作 a 的任何策略可实现的最大动作值。您可以类似地定义状态值 V <sup>π</sup> (s)和最佳状态值 V*(s)。时间差异学习是强化学习的一个中心思想。它通过自举，以无模型、在线和完全增量的方式，直接从具有 TD 误差的经验中学习价值函数 V(s)。更新后的规则如下:

![$$ V\left({s}_t\right)\leftarrow V\left({s}_t\right)+\alpha \left[{r}_t+\gamma \left({s}_{t+1}\right)-V\left({s}_t\right)\right] $$](A454512_1_En_6_Chapter_Equ2.gif)

(6.2)

这里，α是学习率，γ是折现因子，![$$ {r}_t+\gamma \left({s}_{t+1}\right)-V\left({s}_t\right) $$](img/A454512_1_En_6_Chapter_IEq4.gif)是 TD 误差。

类似地，Q-learning 用更新规则学习动作值函数，这里显示:

![$$ Q\left({s}_t,{a}_t\right)\leftarrow Q\left({s}_t,{a}_t\right)+\alpha \left[r+\gamma \max {a}_{t+1}Q\left({s}_{t+1},{a}_{t+1}\right)-Q\left({s}_t,{a}_t\right)\right] $$](A454512_1_En_6_Chapter_Equ3.gif)

(6.3)

Q-learning 是一种与 SARSA 相反的非策略控制方法，SARSA 代表状态、行动、奖励、(下一个)状态、(下一个)行动。这是一种基于策略的控制方法，具有更新规则。

![$$ Q\left({s}_t,{a}_t\right)\leftarrow Q\left({s}_t,{a}_t\right)+\alpha \left[r+\gamma \left({s}_{t+1},{a}_{t+1}\right)-Q\left({s}_t,{a}_t\right)\right] $$](A454512_1_En_6_Chapter_Equ4.gif)

(6.4)

SARSA 在行动价值方面贪婪地提炼策略。

## 6.2 传统 RL

传统控制理论中的强化学习可以被构造如下:假设一个智能体位于一个复杂多变的环境中(例如，一个突围游戏)。在每个时间步，环境处于给定的状态(例如，球拍的位置、球的方向、砖块的位置等)。).代理能够实现环境中的许多动作并改变它们(例如，移动桨)。这些行为可能会导致奖励或惩罚，有些行为可能会改变环境并导致一种新的状态，在这种状态下，代理可以执行一组新的行为。选择这些操作的规则由策略指定。一般来说，环境是随机的，这意味着下一个状态将有一个小的随机成分(例如，如果你发射一个球，它可能会朝着随机的方向)。这种情况的特点是具有可观察或不可观察(隐藏)状态的马尔可夫决策过程(MDP)(见图 [6-1](#Fig1) )。

在这种情况下，RL 表示为一个迭代方程，称为贝尔曼方程。

![$$ V(s)=\max F\left(s,a\right)+\beta V\left(T\left(s,a\right)\right) $$](img/A454512_1_En_6_Chapter_Equa.gif)T2】

这里，s 是状态，a 是可能的行动，F 是当代理人改变到一个新的状态 t 时的收益，代理人试图找到一组行动，使收益随着时间的推移而最大化。

强化 DL 的最终目标是创建一个通用的表示学习框架，其中给定一个目标，用最少的领域知识直接从原始输入中学习实现该目标所需的表示。深度学习 RL 在玩游戏(如围棋和视频游戏)、探索世界(3D 世界和迷宫)、控制物理系统(操纵物体、行走、游泳)和进行用户交互(推荐算法、优化、个性化)方面取得了成功。

![A454512_1_En_6_Fig1_HTML.gif](img/A454512_1_En_6_Fig1_HTML.gif)

图 6-1

Markovian states in a traditional reinforcement learning problem

一个 RL 代理通常包括以下组件:策略(代理的行为函数)、价值函数(每个状态和/或动作有多好)和模型(代理对环境的表示)(见图 [6-1](#Fig1) )。一个策略，或者代理的行为，基本上是从内部状态到动作的映射。它可以是确定性的，如π(s)，也可以是随机性的，如![$$ \pi \left(a|s\right)=P\left[a|s\right] $$](A454512_1_En_6_Chapter_IEq5.gif)。

DNNs 可以用来表示世界的价值函数、政策、模型等所有组成部分，损失函数可以通过随机梯度下降得到。

价值函数是对状态 s 中行动 a 的未来回报的预测。Q 值函数给出了状态 s 和行动 a 在政策π下的预期总回报，贴现因子为γ。

![$$ Q\left(s,a\right)=E\left[ rt+1+\gamma rt+2+\gamma 2 rt+3+\dots |s,a\right]. $$](A454512_1_En_6_Chapter_Equb.gif)T2】

折扣系数只是一种传播延迟回报的方式(见图 [6-2](#Fig2) )。

![A454512_1_En_6_Fig2_HTML.gif](img/A454512_1_En_6_Fig2_HTML.gif)

图 6-2

Learning: adapting policy

## 6.3 DNN 用于强化学习

策略梯度算法通常用于具有连续动作空间的 RL 问题。这些算法通过将策略表示为概率分布![$$ {\pi}^{\theta}\left(a|s\right)=P\left[a|s;\theta \right] $$](A454512_1_En_6_Chapter_IEq6.gif)来工作，该概率分布根据表示模型参数的向量θ在状态空间 s 中随机选择一组动作 a。策略梯度算法通过对该策略进行采样并调整参数以最大化累积回报来进化。

2014 年，西尔弗推出了确定性政策梯度(DPG)，这是一种高效估计政策梯度的算法，后来扩展到深度神经网络( [`http://proceedings.mlr.press/v32/silver14.pdf`](http://proceedings.mlr.press/v32/silver14.pdf) )。DPG 被公式化为动作-值函数的期望梯度(它将动作和状态合并到单个表示中)。通过这种方式，可以比通常的随机政策梯度更有效地估计 DPG。

引导政策搜索(GPS)是由 Levine [LFDA16]提出的。GPS 利用以轨迹为中心的 RL 提供的训练数据，将策略搜索转换为监督学习。GPS 在以轨迹为中心的 RL 和监督学习之间交替，并利用预训练来减少训练视觉运动策略的经验数据量。在一系列需要定位、视觉跟踪和处理复杂接触动力学的现实世界操作任务中取得了良好的性能。作者声称“这是第一种可以通过直接扭矩控制训练复杂、高维操纵技能的深度视觉运动策略的方法”。

### 6.3.1 决定性政策梯度

Silver 为具有连续作用空间的 RL 问题引入了 DPG 算法。确定性策略梯度是动作值函数的期望梯度，其在状态空间上积分；而在随机情况下，政策梯度在状态空间和行动空间上整合。因此，可以比随机政策梯度更有效地估计确定性政策梯度。

作者引入了一种脱离策略的行动者-批评家算法，以从探索性行为策略中学习确定性目标策略，并使用确定性策略梯度的兼容函数近似来确保无偏的策略梯度。实证结果表明，它在几个问题上优于随机策略梯度，特别是在高维任务中:一个高维土匪；具有低维动作空间的山地车和钟摆以及 2D 水坑世界的标准基准 RL 任务；控制具有高维动作空间的章鱼手臂。实验是用瓦片编码和线性函数逼近器进行的。

### 6.3.2 深度确定性政策梯度

尽管 DQN 算法能够解决高维观察空间的问题，但它是为处理离散的低维动作空间而设计的。然而，大多数控制任务处理的是连续的高维空间。Lillicrap 等人提出了一种无模型、脱离策略的行动者-批评家算法，该算法使用函数逼近器，可以在高维、连续的动作空间中学习策略。他们在行动者-批评家方法中使用批量标准化，并依赖于 DQN 之前的两项创新:用重播样本训练网络偏离策略以最小化相关性，用目标 Q-网络训练网络以在时间差备份期间提供一致的目标。

在“深度强化学习的异步方法”( [`https://arxiv.org/abs/1602.01783`](https://arxiv.org/abs/1602.01783) )中，作者通过扩展 DQN 算法，提出了一种在连续动作空间中的行动者-批评者、无模型、深度确定性策略梯度(DDPG)算法。行动者-批评家避免在每个时间步优化行动，以获得如 Q-learning 中的贪婪策略，这将使其在具有大函数逼近器的复杂行动空间中不可行，如深度神经网络。

DDPG 算法通过将从噪声过程采样的噪声添加到行动者策略，基于来自探索策略的经验来学习行动者策略。在 MuJoCo 环境中，使用相同的学习算法、网络架构和超参数解决了 20 多个不同难度的模拟物理任务。DDPG 算法可以用比 DQN 少 20 倍的经验步骤来解决问题，尽管它仍然需要大量的训练集来找到解决方案，就像大多数无模型 RL 方法一样。它是端到端的，以原始像素作为输入。

### 深度 Q 学习

深度 Q-learning 是一种无模型的强化学习算法，用于在控制任务(如玩 Atari 游戏)上训练深度神经网络。q 学习算法与基于策略的算法略有不同。

与试图学习将观察直接映射到动作的函数的策略梯度方法不同，Q-学习试图学习处于给定状态 s 并在那里采取特定动作 a 的值。它将动作和状态组合成一个单一的表示。虽然这两种方法都引导代理人获得有效的奖励，但是他们如何获得最佳行动的过程是不同的(见图 [6-3](#Fig3) )。

在 Q-learning 中，训练一个深度网络来逼近最优行动值函数 Q(s，a)，这是在状态 s 中采取行动 a 然后最优选择未来行动的预期长期累积回报。这可能是一个相当复杂的映射，但只要你提供足够的训练数据，网络就会学习它。

请记住，无模型强化学习算法直接学习控制策略，而无需显式建立环境模型(奖励和状态转移分布)，而基于模型的算法学习环境模型，并使用它来通过规划选择行动。

![A454512_1_En_6_Fig3_HTML.gif](img/A454512_1_En_6_Fig3_HTML.gif)

图 6-3

Deep Q-learning algorithm

Q(s，a)表示在游戏或一组任务结束时来自一个状态 s 的最佳可能分数。Q 指的是在给定状态下某个动作的“质量”。

Q 学习的主要思想是你可以用贝尔曼方程迭代地逼近 Q 函数。在最简单的情况下，Q 函数被实现为一个表，状态作为行，动作作为列。图 [6-4](#Fig4) 显示了 DQN 的伪代码。

![A454512_1_En_6_Fig4_HTML.gif](img/A454512_1_En_6_Fig4_HTML.gif)

图 6-4

Deep Q-network, adapted from [`https://arxiv.org/pdf/1701.07274.pdf`](https://arxiv.org/pdf/1701.07274.pdf)

DeepMind 在一个由 3000 万对人类围棋棋手组成的数据集上使用了 Q 学习方法，然后用强化学习来改进这个神经网络，并与自己对弈。它通过使用监督学习数据来训练第二个网络，增加了蒙特卡罗树搜索(MCTS)，第二个网络的评估速度要快得多，称为部署网络。完整的策略网络只使用一次，以获得对一步棋有多好的初步估计，然后使用更快的推出策略来选择更多需要的移动，以在 MCTS 推出中结束游戏。这使得模拟中的移动选择比随机选择更好，但足够快以获得 MCTS 的好处。

第三个技巧是，DeepMind 训练了一个神经网络来预测什么是好棋，另一个神经网络来评估每个围棋位置。DeepMind 使用已经训练好的高质量政策网络来生成该游戏中的立场和最终结果的数据集，并训练一个价值网络，该网络根据从该立场赢得游戏的总体概率来评估一个立场。因此，政策网络建议评估有前途的举措，然后通过 MCTS 推广(使用推广网络)和价值网络预测的组合来完成，结果证明这比单独使用任何一种都好得多。AlphaGo 在 48 个 CPU 和 8 个 GPU 上运行，神经网络计算是并行进行的。

要了解更多关于 Deep RL 的内容，请参见关于 Q-learning 的有趣教程 [`https://www.nervanasys.com/demystifying-deep-reinforcement-learning/`](https://www.nervanasys.com/demystifying-deep-reinforcement-learning/) ，或者参见 TensorFlow 中的 [`https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0`](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0) 。

### 6.3.4 演员-评论家算法

演员-评论家算法(A3C)是由谷歌的 DeepMind group 在 2016 年发布的，它使 DQN 过时了。它更快、更简单、更健壮，并且能够在标准的深度强化学习任务上取得更好的成绩。最重要的是，它可以在连续和离散的动作空间中工作。鉴于此，它已经成为事实上的深度 RL 算法，用于解决具有复杂状态和动作空间的新的挑战性问题。OpenAI 刚刚发布了一个版本的 A3C 作为其“通用启动代理”，用于与新的(非常多样化的)宇宙环境一起工作(见图 [6-5](#Fig5) )。

![A454512_1_En_6_Fig5_HTML.jpg](img/A454512_1_En_6_Fig5_HTML.jpg)

图 6-5

Actor-critic architecture (source: [`https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2`](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2) )

不像 DQN，单个智能体由与单个环境交互的单个神经网络表示，A3C 使用多个智能体来更有效地学习。在 A3C 中，有一个全局网络和多个工作代理，每个工作代理都有自己的一组网络参数(见图 [6-6](#Fig6) )。在其他代理与其环境交互的同时，这些代理中的每一个都与其自己的环境副本进行交互。这比只有一个代理更有效的原因是每个代理的经验独立于其他代理的经验。通过这种方式，可用于培训的总体经验变得更加多样化。

接下来是每个 actor-learner 线程的异步优势 actor-critic 的伪代码。A3C 维护一个策略![$$ \pi \left({a}_t|{s}_t;\theta \right) $$](img/A454512_1_En_6_Chapter_IEq7.gif)和一个价值函数 V(s <sub>t</sub> 的估计；θ <sub>v</sub> ，在每次 t <sub>max</sub> 动作或达到终端状态后，在正向视图中用 n 步返回进行更新，类似于使用小批量。梯度更新可以被视为![$$ {\nabla}_{\theta \hbox{'}}\log \pi \left({a}_t|{s}_t;{\theta}^{\prime}\right)A\left({s}_t,{a}_t;\theta, {\theta}_v\right) $$](img/A454512_1_En_6_Chapter_IEq8.gif)，其中![$$ A\left({s}_t,{a}_t;\theta, {\theta}_v\right)=\sum \limits_{i=0}^{k-1}\ {\gamma}^i{r}_{t+i}+{\gamma}^kV\left({s}_{t+k};{\theta}_v\right)-V\left({s}_t;{\theta}_v\right) $$](img/A454512_1_En_6_Chapter_IEq9.gif)是优势函数的估计，k 由 t <sub>max</sub> 上界。

![A454512_1_En_6_Fig6_HTML.gif](img/A454512_1_En_6_Fig6_HTML.gif)

图 6-6

A3C, each actor-learner thread, based on [MBM<sup>+</sup>16]

Actor-critic 结合了价值迭代法(Q-learning)和策略迭代法(策略梯度)的优点。A3C 估计一个价值函数 V (s)(某个状态有多好)和一个策略π(s)，后者是网络顶部每个全连接层的一组动作概率输出。关键的是，代理使用价值评估(批评家)来更有效地更新策略(执行者)。

使用优势估计而不仅仅是贴现回报的观点是允许代理人不仅确定其行为有多好，而且确定它们比预期好多少。直观地说，这使得算法可以专注于网络预测缺失的地方。

![$$ Advantage:A=Q\left(s,a\right)-V(s) $$](img/A454512_1_En_6_Chapter_Equc.gif)T2】

由于您不会直接在 A3C 中确定 Q 值，因此您可以使用贴现收益(R)作为 Q(s，a)的估计值，以便生成优势的估计值。

## 6.4 机器人和控制

机器人可能仍然是人工智能应用程序最明显的选择。现实总是落后于好莱坞大肆宣扬的世界末日杀手机器人。然而，DL 带来了一套完整的新工具包，来帮助解决一些与机器人相关的复杂任务，如移动、抓取、对象操纵以及传感器数据处理。这一节回顾一些最近的突破和应用。

机器人学中最重要的任务之一是物体抓取。平托和古普塔提出了一种方法，在不依赖人类标记的数据集的情况下，在抓取物体的硬任务上对机器人(巴克斯特)进行自我训练；参见“超级自我监督:从 50K 次尝试和 700 个机器人小时中学习掌握”( [`https://arxiv.org/abs/1509.06825`](https://arxiv.org/abs/1509.06825) )。他们使用机器人自主收集了 5 万个数据点的庞大数据集，抓取尝试超过 700 小时。这允许训练一个深度卷积神经网络(CNN)来完成预测抓取位置的任务。在多阶段学习方法中，在一个阶段训练的 CNN 被用于在后续阶段收集正面/负面的例子，在新物体上实现了 66%的准确率，在已经看到的物体上实现了 73%的准确率。作者声称，与基于几何的方法相比，CNN 的优势在于它不会忽略物体的密度和质量分布。

在马里兰大学的一个项目中，Y. Yang 等人再次使用机器人 Baxter 学习操纵，并通过观看视频来构思行动计划( [`https://www.umiacs.umd.edu/yzyang/paper/YouCookMani_CameraReady.pdf`](https://www.umiacs.umd.edu/yzyang/paper/YouCookMani_CameraReady.pdf) )。使用了两个基于 CNN 的识别模块，以及一个用于动作预测的语言模型(带有 RNN)。他们使用基于概率操作动作语法的解析器(Viterbi)来生成命令。机器人通过观看由不受约束的演示视频组成的烹饪视频(一个烹饪数据集)进行学习。该系统能够稳健地识别和生成动作命令，从广泛指定的自然语言输入中准备新食谱的能力证明了这一点。

最近，Levine 等人[LFDA16]提出了一种机器人对象操作的手眼协调方法，需要最少的规划。人类非常依赖持续的视觉反馈来处理物体和进行复杂的协调。然而，将复杂的感觉输入直接整合到反馈控制器中是具有挑战性的。因此，作者提出了一种基于学习的手眼协调方法，使用直接来自图像像素的端到端训练。通过不断地重新计算最有希望的运动命令，这种方法不断地整合来自环境的感官线索，允许它调整运动，以最大化特定任务的成功概率。这意味着该模型不需要摄像机相对于末端执行器精确校准，而是依靠视觉线索来确定场景中抓取器和可抓取对象之间的空间关系。

伯克利机器人公司的研究人员使用消费级虚拟现实设备(Vive VR)，一个老化的 WIllow Garage PR2 机器人，以及为远程操作者构建的定制软件，创建了一个单一的系统来教会机器人执行任务。该系统使用能够将原始像素输入映射到动作的单一神经网络架构； [`https://arxiv.org/abs/1710.04615`见](https://arxiv.org/abs/1710.04615)。对于每个任务，不到 30 分钟的演示数据足以学习成功的策略，在所有任务中使用相同的超参数设置和神经网络架构。任务包括伸手，抓，推，把一个简单的模型飞机放在一起，用锤子去掉一个钉子，抓住一个物体并把它放在某个地方，抓住一个物体并把它放在一个碗里然后推碗，移动布，并连续拾取和放置两个物体。在许多任务的测试时间内，竞争结果达到了 90%的准确率，尽管注意到拿起和放置两个物体达到了 80%(因为现代人工智能技术在物理动作序列方面仍然有问题)，在类似的任务中，拿起一个物体并将其放入碗中，然后推动碗，达到了 83%。

Peng 等人[PBvdP16]使用经过强化学习训练的深度神经网络和基于物理的模拟，从基本原则出发，开发了一个包含状态、行动、奖励和控制策略的顺序决策问题，并取得了显著的成果。他们能够设计控制策略，直接对高维度角色状态描述(83 维)和环境状态进行操作，环境状态由使用 200 维的即将到来的地形的高度场图像组成。他们还将动作空间参数化为 29 个维度，这允许控制策略在跳跃、跳跃和台阶的水平上操作。新颖之处在于引入了演员-评论家专家(MACE)混合架构来加速学习。梅斯发展了 N 个单独的控制策略和它们相关的价值函数，然后每个策略专门研究整体运动的特定机制。在最终策略执行期间，以类似于具有离散动作的 Q 学习的方式，执行与最高值函数相关联的策略。结果很有趣；参见 [`https://www.youtube.com/watch?v=HqV9H2Qk-DM`](https://www.youtube.com/watch?v=HqV9H2Qk-DM) 处的视频和 [`https://www.cs.ubc.ca/van/papers/2016-TOG-deepRL`](https://www.cs.ubc.ca/van/papers/2016-TOG-deepRL) 处的论文。

最近，DeepMind 开发了一种算法( [`https://arxiv.org/abs/1707.02286`](https://arxiv.org/abs/1707.02286) )，用于训练代理(类人和类蜘蛛)学习如何在具有挑战性的虚拟景观中行走、奔跑和跳跃。它使用丰富的环境来促进智能体沉浸其中的复杂行为的学习。DeepMind 使用了一种策略梯度强化学习的变体，称为近似策略优化，在不使用任何显式基于奖励的指导的情况下，教代理跑、跳、蹲和转。可以在 [`https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/`](https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/) 了解更多。

## 6.5 自动驾驶汽车

深度学习通过分析一组不同的信号，在自动驾驶汽车技术中发挥了相当大的作用，其中视频最具挑战性。随着谷歌无人驾驶汽车最近的成功，几乎所有的汽车制造商都在考虑在未来的汽车版本中使用这一选项。接受测试的车型包括丰田普锐斯、奥迪 TT 和雷克萨斯 RX450h。特斯拉 S3 可能会是第一款默认包含自动驾驶功能的量产车。所有这些模型都依赖于深度学习技术来进行对象识别、规划、路由和对象回避。

谷歌开发了自己的定制汽车，由 Roush Enterprises 组装。它依赖于 64 束激光探测器，允许车辆生成其环境的详细 3D 地图。该算法使用这些地图，并将它们与高分辨率的世界地图相结合，为自我导航产生足够详细的模型。谷歌已经在自动模式下测试了其车队，总里程超过 150 万英里。谷歌的车辆已经证明有足够的能力在城市的繁忙交通中行驶，以及在具有挑战性的越野地形上行驶。

百度正在大力投资自动驾驶汽车，并使用阿波罗软件计划在 2018 年发布完全自动驾驶的公交车。三星也在韩国测试自动驾驶汽车，通用汽车和 Cruise 在 2017 年宣布推出首款量产自动驾驶汽车。奥托(Otto)等其他公司专注于自动驾驶卡车的软件。核心技术基于 DL 算法。百度和谷歌正在推动政府对无人驾驶汽车的监管，声称它们只需要对实际基础设施进行微小的改变。百度的目标是到 2018 年在中国城市运营班车服务；另一家初创公司 NuTonomy 正计划在新加坡推出自己的班车服务。

[开车。ai](https://www.drive.ai) 也在致力于将 DL 带入自动驾驶汽车技术。[开车，而不是给汽车编程。ai](https://www.drive.ai) 将允许算法自行学习，尽管它没有透露该公司在这项技术上的进展。

但是，自动驾驶需要直觉心理。自动驾驶汽车需要有一些常识性的理解，或者能够推断行人的行为和信念(他们认为过马路是安全的吗？他们注意到了吗？)以及欲望(他们想去哪里？他们很匆忙吗？).类似地，路上的其他司机也有类似的复杂心理状态隐藏在他们的行为中(他们是想变道还是想超过另一辆车？).这种类型的心理推理，以及其他类型的基于模型的因果推理和物理推理，在缺乏相关训练数据的挑战性和新奇的驾驶环境中可能特别有价值。最近，一辆特斯拉 Model S 汽车在自动驾驶模式下行驶，导致司机死亡，引发了人们对该技术安全性和可靠性的一些担忧。尽管特斯拉声称，它已经在超过 1 亿英里的里程上测试了自动驾驶技术，但似乎很明显，一些粗糙的边缘仍需要打磨(例如，如何在不太可能的事件中驾驶车辆，比如汽车在高速公路上以错误的方式行驶或与醉酒的司机在一起)。

加州大学伯克利分校推出了 DeepDrive 平台( [`https://deepdrive.berkeley.edu/`](https://deepdrive.berkeley.edu/) )。所谓的 BDD 工业联盟研究用于汽车应用的计算机视觉和机器学习方面的最新技术。这是一个由加州大学伯克利分校主持的多学科中心，由特雷弗·达雷尔教授领导。该中心致力于开发汽车行业中具有实际应用的新兴技术。

## 6.6 对话机器人(聊天机器人)

聊天机器人，也称为对话代理或对话系统，是一种旨在拥有人类级别对话能力的算法。几家公司正在使用这项技术，要么作为个人助理，要么作为语言理解的对话算法。机器人的目标是达到与人类无法区分的自然对话水平，从而能够通过图灵测试。有两种类型的机器人:基于检索的机器人使用预定义响应的存储库和一些启发式方法来根据输入和上下文选择适当的响应，而生成模型根据过去的经验和上下文生成自动响应。后者大多依赖于深度学习技术。

生成模型可能相当复杂，并从创建数据的潜在表示中学习。它们很灵活，但是需要大量的文本(训练数据)。像[SVL14]这样的序列对序列学习方法有很大的潜力，但是大多数产生式系统仍然基于对话检索的硬性规则。短文本对话更容易优化，尤其是在封闭领域知识中，可能的输入和输出空间在某种程度上受限于特定的上下文。

Vinyals 等人使用序列对序列框架创建了一个会话模型，该模型能够根据会话中的前一句话预测下一句话[VTBE14]。它是从头到尾设计的，需要一些精心制作的功能。他们在大型对话数据集上训练它，它有足够的能力使用 it 服务台数据集提供好的建议，同时在电影对话数据集中显示常识推理。然而，机器人在对话中未能保持一致(见图 [6-7](#Fig7) )。

Serban 等人[SSB <sup>+</sup> 15]最近提出了一种使用生成式分层神经网络模型的端到端对话系统。作者提出了一种分层递归自动编码器，并将其应用于包含约 500 部电影的评论和评论的数据集(名为 MovieTriples );从每部电影中提取了大约 200，000 个三元组。三联体 U1、U2、U3 是 A 和 B 之间的三个话轮对话，A 发出第一个话语 U1；b 用 U2 回应；并且 A 用最后一句话 U3 来响应。他们表明，该模型在建模话语和言语行为方面优于 n-gram 模型，实现了大约 26 的困惑度，大约是 n-gram 方法实现的一半。作者发现了提高性能的两个关键因素:使用大型外部独白语料库来初始化单词嵌入，以及使用大型相关但非对话的语料库来预训练递归网络。

姚等人[YZP15]提出了一个模型，叫做有意图注意。他们的模型由三个循环神经网络组成:编码器，这是一个表示句子的单词级模型；意向网络，它是一个递归网络，对意向过程的动态进行建模；以及解码器网络，它是一个递归网络，在给定输入的情况下产生响应。它是一种依赖于意图的语言模型，具有注意机制。该模型使用来自服务台呼叫中心的 10，000 次对话(涉及约 100，000 次呼叫)进行了端到端的训练，而没有标记数据。使用 200 的嵌入维度，作者获得了 22.1 的困惑度。

生成模型是强大的，但语法错误可能代价高昂，所以公司仍然依赖旧的检索技术。然而，随着公司获得更多数据，生成模型将成为常态，但可能会有一些人的监督，以防止他们出现“不当行为”，就像微软 Twitter 聊天机器人 Tay ( [`https://en.wikipedia.org/wiki/Tay_(bot)`](https://en.wikipedia.org/wiki/Tay_(bot)) )所发生的那样。

大多数大公司都在使用、测试或考虑在其服务和运营中实现聊天机器人。利用其个人助理 Cortana 的经验，微软最近为聊天机器人的实现开放了一个开发框架，并发布了 Luis。ai ，一个语言理解的 API。

脸书获得了智慧。ai ，一家致力于语音识别技术的公司。苹果正在改进 Siri 和谷歌 Cleverbot。IBM 提供了一个简单的 API 来将其强大的知识推理机 Watson 嵌入到对话机器人中。

这些服务中的大多数可以很容易地整合到 Twitter、Whatsapp、Skype、微信、Telegraf 或 Slack 等对话服务中。例如，Slack 允许基于硬性或软性规则的简单或复杂的对话自动化。它与 Howdy 集成，可自动执行重复性任务。Howdy 提出问题，收集回答，并提交报告。Chatfuel.com 是一个聊天工具的实现平台。

![A454512_1_En_6_Fig7_HTML.gif](img/A454512_1_En_6_Fig7_HTML.gif)

图 6-7

Conversational excerpt from Google Cleverbot (source: Google Research)

已经有许多使用 RNN 和深度学习技术的对话机器人。例如，Medwhat 是一个医学顾问，它探索生物医学数据的大型数据集，以回答与健康相关的问题并创建个人建议。由于提炼信息的迭代过程，对话机器人也是一种更自然的搜索方式；例如，参见 [`https://www.intellogo.com`](https://www.intellogo.com) ，它使用 DL 进行上下文搜索。在 [`chatbotsmagazine.com`](https://chatbotsmagazine.com) 有一些聊天机器人的好资源和新闻。

聊天机器人面临的主要挑战是上下文整合，特别是在长对话和与身份持久性相关的问题中。

以下是聊天机器人应用程序的简短列表:

*   石英:新闻聊天机器人
*   接线员:采购助理
*   第一意见:聊天机器人医生
*   卢卡:旧金山的餐馆推荐
*   Lark:健身教练
*   Hyper:航班和酒店
*   Pana:航班、酒店、推荐
*   Fin:股东大会
*   佩妮:个人理财教练
*   梅兹:购物助理
*   埃维娅:保险助理
*   Suto:专家产品推荐
*   HelloShopper:礼物创意
*   艾娃:专家发现者
*   X.ai:个人助理
*   爱丽丝:人工智能伙伴

最近脸书推出了 ParlAI ( [`https://code.facebook.com/posts/266433647155520/parlai-a-new-software-platform-for-dialog-research/`](https://code.facebook.com/posts/266433647155520/parlai-a-new-software-platform-for-dialog-research/) )。ParlAI 平台结合了人工智能的不同进步，使对话机器人更加高效。

该框架为研究人员提供了一种更简单的方法来建立对话式人工智能系统，并使开发人员更容易建立聊天机器人，不那么容易被意想不到的问题难倒。长期的希望是 ParlAI 将通过减少开发和测试不同方法所需的工作量来帮助推进自然语言研究的最新发展。它内置了 20 种不同的自然语言数据集，包括来自斯坦福、微软和脸书的问答示例，并提供了与流行的机器学习库的兼容性。

由斯坦福大学心理学家和人工智能专家团队创建的 Woebot 使用简短的日常聊天对话、情绪跟踪、精选视频和文字游戏来帮助人们管理心理健康。在花了去年的时间构建测试版和收集临床数据后，Woebot Labs 刚刚推出了完整的商业产品——一个厚脸皮的个性化聊天机器人，每天检查你一次，每月 39 美元。

## 6.7 新闻聊天机器人

零售银行和金融科技初创公司现在正在探索使用聊天机器人进行数字体验，以查看银行账户余额，找到附近的自动取款机，进行支付，甚至建议如何更明智地花钱。

在 Slack 等其他竞争对手开始自动化一些对话之后，Zendesk 推出了一款聊天机器人来自动回答客户的询问。

聊天机器人的深度学习( [`https://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/`](https://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/) )提供了一个关于如何使用 Ubuntu 论坛的数据从头开始构建机器人的优秀教程。

几家银行，如 Toshka 银行和苏格兰皇家银行，已经引入了用于客户服务的对话机器人，并有望成为完全的个人助理，能够提供全方位的银行功能[AV18]。

Stratumn 与 Deloitte 和 Lemonway 合作，使用 LenderBot 来管理小额保险。它将通过社交媒体实现定制保险。Digibank 将聊天机器人作为银行体验不可或缺的一部分，并声称拥有最集成的解决方案。星展银行正在使用聊天机器人帮助客户在脸书和 WhatsApp 上管理资金和支付。Olivia AI 使用对话代理来管理账户和交易，并提供省钱的见解，而丹麦的一家银行 LunarWay 也推出了自己的聊天机器人。

最后，最近一款名为 Fin ( [`https://www.fin.com`](https://www.fin.com) )的产品声称可以取代一名行政助理。它也是和 M 一样的混合人力聊天机器人，每月两小时 120 美元。大多数对话助手只会“理解”和执行基本命令，比如“在 Spotify 上播放一些音乐”或“启动 20 分钟的计时器”。但 Fin 声称其对话机器人可以理解复杂的语音命令。Fin 可以购买产品、寻找汽车租赁，并创建带有选择、价格和可用性列表的 Google 文档。它甚至可以在易贝上预订会议和购买商品。

Phocuswright 提出了一种新的夏波特酒来帮助旅游业。你不必进入在线旅行社进行搜索并看到 150 家酒店的列表，而是在你的个人资料中输入你要找的东西，聊天机器人会在信息界面上提供三到四家精选的列表。其他为旅游业生产 chabots 的公司包括 Pana 和 Mezi。

在美国，一个名为 AskMyUncleSam ( [`http://askmyunclesam.com/`](http://askmyunclesam.com/) )的机器人通过回答可能的税收减免问题来帮助纳税人填写表格；你可以把它看作一个 FAQ 数据库，就像一个真人一样，用户可以与之聊天。

旧金山初创公司 Digit 专注于帮助客户省钱，方法是使用其算法分析你的收入和消费习惯，并找到它可以为你留出的小额资金。

来自蒙特利尔学习算法研究所(MILA)的研究人员发表了一篇研究论文，概述了 MILABOT，他们进入亚马逊关于对话代理的 Alexa 竞赛( [`https://arxiv.org/abs/1709.02349`](https://arxiv.org/abs/1709.02349) )。他们不得不面对兴趣无限的人们的开放式对话互动。MILABOT 进入了半决赛，并在用户满意度方面获得了相当高的分数，同时进行了比赛中最长的一些对话。它依靠强化学习绑定的集合策略来决定如何在不同的模型之间进行选择，以改善对话。

## 6.8 应用

波士顿动力公司开发了 Atlas，设计用于室外和建筑物内部。它专门用于移动操作，由电力和液压驱动。它使用身体和腿部的传感器来保持平衡，使用头部的立体传感器来避免障碍，评估地形，帮助导航，并操纵物体。

BIG-I 是由天伦林设计的人形机器人，是一种服务机器人，旨在帮助房主完成各种家务。它可以跟踪各种家用电器的位置，并通过其爪状机械手将物品从一个地方运送到另一个地方。

中国推出了其首个机器人安全警卫 AnBot，这是一种智能巡逻机器，具有先进的基于紧急警报的导航和环境监控功能。据其开发者称，AnBot 在检测生化和爆炸相关威胁方面非常有用。

低成本自主导航和定位以及智能视频监控方面的突破促进了机器人的发展，除其他功能外，机器人还能够在紧急情况下做出反应。

Kuri 是一个家用机器人，能够识别宠物，并以高清方式观看和播放。梅菲尔德机器人公司的 Kuri ( [`https://www.heykuri.com/`](https://www.heykuri.com/) )可以识别面孔和家人、朋友以及宠物。Kuri 有一个 1080p 高清摄像头和虚拟眼睛，可以以最高质量进行直播，并捕捉静态图像和视频。

韩国制造业每 1 万名工人拥有约 400 台机器人。德国有近 300 个机器人，美国刚刚超过 150 个。牛津大学几年前发表的一项研究预测，美国近 50%的劳动力市场仍面临机械化的风险。它预计，近 700 种不同的人工工作可能会在几年内完全自动化。

## 6.9 展望和未来前景

深度学习自动化人工流程和提高生产率的能力将对机器人行业产生深远的影响。尽管机器人在制造业中得到广泛应用，但它们价格昂贵且难以编程。对于大多数企业来说，机器人还没有用处。2015 年，工业机器人的全球单位销量仅为 25 万台，大约是峰值时大型计算机数量的 10 倍。相比之下，2016 年服务器和 PC 的总销量分别为 1000 万台和 3 亿台。显然，机器人技术正处于萌芽阶段，需要在普及之前大幅提高成本和易用性。

成本改善进展顺利。ARK 估计，工业机器人的成本目前约为 10 万美元，在未来十年将下降一半。与此同时，一种为与人类合作使用而设计的新型机器人将耗资约 3 万美元。像软银的 Pepper 这样的零售助理机器人，包括服务费在内，价格约为 1 万美元。利用消费电子行业的组件，如相机、处理器和传感器，应该会使成本更接近这些消费产品。

更难克服的障碍是易用性。工业机器人不是从以用户为中心的角度来设计的。它们需要使用工业控制系统进行精确的编程，在工业控制系统中，每项任务都必须分解成一系列的六维运动。新任务必须被明确地编程；机器人没有能力从经验中学习并归纳出新的任务。

这些限制将机器人的市场局限于那些任务可预测且定义明确的工业应用。深度学习可以让机器人变成学习机器。机器人从数据和经验的结合中学习，而不是精确的编程，这使它们能够承担各种各样的任务。例如，仓库机器人能够从货架上挑选任何物品并将其放入箱子中，这是许多企业非常需要的。然而，直到最近，开发人员还不能让机器人识别和抓取形状和大小各异的物体。

## 6.10 关于自动驾驶汽车的新闻

以下是一些需要关注的新闻:

*   特斯拉最近宣布，其配备自动驾驶硬件的车辆已经累计行驶了 7.8 亿英里，其中 1 亿英里使用了自动驾驶。特斯拉现在每天捕获的数据(相机、GPS、雷达和超声波)比谷歌自 2009 年成立以来记录的数据还要多！
*   Europilot 是一个用于训练自动驾驶卡车的开源平台，它允许您重新利用复杂的技术专用游戏 Eurotruck 模拟器，作为训练代理通过强化学习驾驶的模拟环境。Europilot 提供了几个额外的功能来简化训练，并正在测试 AI，包括能够在训练时从屏幕输入中自动输出一个 Numpy 数组，并在测试时创建一个可见的虚拟屏幕操纵杆，网络可以用它来控制车辆。你可以在 [`https://github.com/marshq/europilo`](https://github.com/marshq/europilo) 找到代码。
*   波士顿动力公司发布了一段令人难以置信的视频( [`https://www.youtube.com/watch?v=tf7IEVTDjng`](https://www.youtube.com/watch?v=tf7IEVTDjng) )，展示了其最新创造的 SpotMini，这是一个全电动机器人，运行了 90 分钟。它可以自主地执行一些任务，能够爬楼梯，自己爬起来，并处理敏感的抓取任务。莫利机器人公司 [`https://www.youtube.com/watch?v=KdwfoBbEbBE`](https://www.youtube.com/watch?v=KdwfoBbEbBE) 的视频展示了机器人如何根据菜谱烹饪。
*   最近一份名为“brain 4 cars:Car That Before know via Sensory-Fusion Deep Learning Architecture”([`https://arxiv.org/abs/1601.00740`](https://arxiv.org/abs/1601.00740))的出版物解决了预测和评估汽车驾驶员下一步行动(例如，转向和撞上一辆看不见的自行车)的问题，时间长达 3.5 秒。该方法依赖于装备有 LSTM 单元的 rnn，该单元学习视频捕获、车辆动态、GPS 数据和街道地图。
*   一辆拉力赛车的 1/5 复制品配备了复杂的控制算法，可以在越野赛道上高速行驶。这款名为 [AutoRally](https://autorally.github.io/) 的汽车有一个惯性测量单元，两个前置摄像头，GPS，每个车轮上的旋转传感器，一个英特尔四核 i7 处理器，Nvidia GPU 和 32GB RAM 它不需要其他外部传感或计算资源。该算法由在跑道上驾驶的飞行员预先训练。然后，传感器测量用于结合控制和规划，以实现自动驾驶。每隔 16 毫秒，它平均评估 2560 条不同的未来可能轨迹，以选出最佳轨迹。
*   星舰科技公司( [`https://www.starship.xyz/starship-technologies-launches-testing-program-self-driving-delivery-robots-major-industry-partners`](https://www.starship.xyz/starship-technologies-launches-testing-program-self-driving-delivery-robots-major-industry-partners) )推出了一个很大程度上自主的送货机器人舰队，主要是食品和小件物品。
*   [逗号。ai](https://comma.ai) 发布了一组包含 7.5 小时相机图像、转向角度和其他车辆数据的高速公路驾驶数据集。它使用带有自动编码器和 RNNs 的对抗性生成网络来创建特定道路快照的下一个似乎合理的场景，以便网络预测汽车的下一个动作，给定模型想象的道路在几百毫秒前的样子。
*   最近，Craig Quiter ( [`https://hackerfall.com/story/integrating-gta-v-into-universe`](https://hackerfall.com/story/integrating-gta-v-into-universe) )基于 72 小时的训练和使用 OpenAI Gymn 平台，推出了一个基于侠盗猎车手(GTA)视频游戏的驾驶模拟器环境(DeepDrive)。这个想法是成为一个用强化学习训练自动驾驶汽车的试验台。该网络控制转向、油门、偏航和速度。
*   在 [`http://moralmachine.mit.edu/`](http://moralmachine.mit.edu/) ，研究人员表明，研究参与者希望成为车辆中的乘客，不惜一切代价保护他们的乘客，同时更愿意其他人购买由功利主义道德控制的车辆(即为了更大的利益牺牲乘客)。人类伦理中的矛盾比比皆是。
*   百度推出平台阿波罗( [`http://apollo.auto/`](http://apollo.auto/) )。百度自称是全球自动驾驶平台最大的合作伙伴生态系统之一。阿波罗自动驾驶项目有 50 个合作伙伴，包括中国主要汽车制造商之一一汽集团，该公司将与百度合作，实现该技术的商业化。其他合作伙伴包括中国汽车公司奇瑞、长安和长城汽车，以及博世、大陆、英伟达、微软云、威力登、TomTom、UCAR 和 Grab Taxi。
*   韩国推出了 K-City ( [`https://www.businesskorea.co.kr/english/news/sciencetech/18018-k-city-world's-largest-test-bed-self-driving-cars-be-opened-korea`](https://www.businesskorea.co.kr/english/news/sciencetech/18018-k-city-world's-largest-test-bed-self-driving-cars-be-opened-korea) )，号称世界上最大的自动驾驶汽车试验台。K-City 的开放是为了通过提供一个像城市一样大的试验场来为开发者提供更多的帮助。
*   美国众议院于 2017 年 8 月通过了《自动驾驶法案》( [`https://www.wired.com/story/congress-self-driving-car-law-bill/`](https://www.wired.com/story/congress-self-driving-car-law-bill/) )。该法案为国家公路交通安全管理局(NHTSA)提供了监管自动驾驶汽车设计、建造和性能的权力，就像它对普通车辆一样。在未来 24 个月内，NHTSA 将编写汽车制造商必须遵守的功能集和规则，以证明他们的车辆是安全的。该法案还提出了一项“隐私计划”，汽车制造商必须据此描述他们将如何收集、使用和存储乘客数据。NHTSA 也可以向正在测试无人驾驶汽车的公司授权数万张牌照。

最近，OpenAI 的一篇论文提出了一种通过人类交互来训练强化学习代理的方法( [`https://arxiv.org/pdf/1706.03741.pdf`](https://arxiv.org/pdf/1706.03741.pdf) )。这是一个重大突破，因为传统的强化学习不容易适应通过人类类型的交流来学习。作者探索了根据(非专家)人类偏好定义的目标，在不使用奖励功能的情况下解决复杂的 RL 任务，包括 Atari 游戏和模拟机器人运动。他们能够用大约一个小时的人类时间训练代理人复杂的新行为。