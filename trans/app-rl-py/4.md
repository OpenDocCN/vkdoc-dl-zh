# 四、通过强化学习做市

除了攻击强化学习中的一些标准问题，因为它们在许多书中都可以找到，作为一个例子，看看答案既不客观也没有完全解决的领域是很好的。在金融领域，尤其是强化学习领域，做市是最好的例子之一。我们将讨论学科本身，提出一些不基于机器学习的基线方法，然后测试几种基于强化学习的方法。

## 什么是做市？

在金融市场中，利用交易所的人们对流动性有着持续的需求。在任何给定的时刻，试图出售资产的每个人的订单都与想要购买的人完全匹配，这是不可能的。因此，做市商在促进通常希望在不同持续时间长度的金融工具(多头或空头)中持有头寸的人的订单的执行方面起着至关重要的作用。通常，做市被描述为金融市场中的人们能够在金融市场中持续赚钱的少数方式之一，这与押注风险更高但有条件获得更高回报的押注模式相反。现在，让我们试着理解我们正在处理的数据是什么，以及我们可以期待什么。图 [4-1](#Fig1) 是带有相关订单的订单簿的样本图像。

![img/480225_1_En_4_Fig1_HTML.jpg](img/480225_1_En_4_Fig1_HTML.jpg)

图 4-1

订单簿示例

图 [4-1](#Fig1) 是位于订单簿两边的订单示例，代表出价和要价。当有人向交易所发送订单并使用限价订单时，他们试图出售的数量将留在订单簿上，直到订单被执行。虽然交易所专用的履行算法可以从一个到另一个变化，但是它们通常寻求履行它们被接收的订单，使得最近的订单是最后被履行的订单。利用限价单的好处是，它们可以大大减少所谓的“市场影响”简单地说，每当有人想购买大量的东西，它就向市场的其他人发出信号，表明需求很大。这意味着我们可以影响自己获得最佳价格和完成订单的能力。正因为如此，交易者经常分散他们的限价单，以尽可能掩盖他们的意图。

为了给出一个更具体的做市商的例子，让我们假设我们是某个金融交易所，它有大量的客户，这些客户通常希望交换每一笔大额订单。然而，并非所有这些订单都是均匀分布的，每个想买的人都有另一个想卖的大客户。因此，我们决定通过提供非常优惠的费率来激励做市商，以提供流动性，从而促进这些大客户的订单。交易所越能吸引做市商，从而带来更多的流动性，通常交易所对想要交易的人就越有利，特别是机构买家。

做市的基本思想是，有人通常愿意以任何给定的价格购买和/或出售一种工具，这样随着时间的推移，他们的策略会为他们带来回报。做市的主要吸引力在于，一旦确定了可扩展的成功策略，它的有效期通常会比对冲基金和其他交易平台可能采用的传统方向模型长得多。除此之外，与做市相关的风险更低。话虽如此，做市在实际意义上的主要困难在于，根据市场的不同，做市可能需要大量资金。尽管如此，我们将利用强化学习来尝试开发一种更直观的策略开发方式，而不是尝试进行更传统的定量金融研究。

## 交易健身房

类似于 OpenAI gym，以及我们用来玩各种视频游戏如*超级马里奥兄弟*和*末日*的那个包的衍生产品，这里的读者将会使用 Trading Gym。这是一个开源项目，其目标是使在交易环境中应用强化学习算法变得容易。在图 [4-2](#Fig2) 中，你可以看到在环境渲染时通常应该显示的绘图。

![img/480225_1_En_4_Fig2_HTML.jpg](img/480225_1_En_4_Fig2_HTML.jpg)

图 4-2

交易馆可视化

在这种环境下，读者通常有三种选择:

1.  购买乐器

2.  出售乐器

3.  担任当前职务

Trading Gym 通常允许您处理一个(或多个)产品/金融工具，其中数据的格式为(bid_product1，ask_product1，bid_product2，ask_product2)。我们将*出价*定义为个人购买产品的最佳可能价格，将*要价*定义为销售产品的最佳可能价格。我们将向读者介绍如何将他们自己的订单簿数据导入到环境中，但在此之前，让我们首先讨论我们试图解决的问题，并看看解决该问题的更确定的方法。

## 为什么要针对这个问题进行强化学习？

尽管从交易体操中看不出这一点，但所有的做市都需要使用限价单才能有效。由于完成订单所需的流动性几乎总是有保证的(低于一定额度)，市场订单的不利之处在于，交易所通常会收取相当高的费用。因此，利用做市策略的唯一方法是在限价订单簿上下单，并允许它们被执行。如上所述，这会引入如下几个问题:

1.  应该买什么价位的？

2.  我应该卖什么价格？

3.  应该持什么价位？

在机器学习的背景下，所有这些问题都不容易回答。具体来说，我们活动的空间是连续的。如前所述，市场在不断变化，我们在市场上的行为本身会使我们的订单更难兑现或无法兑现。因此，普通的机器学习方法不会考虑这些环境因素，除非我们将它们作为特征包括在内。即便如此，除非我们事先对市场做了大量的研究，否则很难尝试对这些方面进行编码。其次，大多数机器学习方法可能是无效的，因为这是一个时间序列任务。唯一合适的方法是递归神经网络(RNN)，特别是因为这项任务的粒度，我们将不得不预测相当数量的序列。这将导致一个模型，在这个模型中，我们持有头寸的平均时间比我们在做市环境中希望的时间长得多。我们想要敏捷性和灵活性，而使用机器学习方法可能会迫使我们在预定的时间内持有头寸，而不是根据市场环境在对我们最有利的时候退出头寸。所有这些原因都证明了基于强化学习的方法是正确的。让我们继续描述代码，以及我们如何创建一个合理的例子。下面是将执行该函数的代码示例:

```py
memory = Memory(max_size=memory_size)

environment = SpreadTrading(spread_coefficients=[1],
                            data_generator=generator,
                            trading_fee=trading_fee,
                            time_fee=time_fee,
history_length=history_length)

state_size = len(environment.reset())

```

在我们进一步讨论之前，有几个重要的属性是我们为 SpreadTrading()类定义的，我们应该浏览一下。其中一些相当简单，因为金融市场上的所有交易都要花钱在普通交易所进行，所以我们必须设定一个费用。在我们使用的第一个示例中，将合成交易数据，第二个示例将使用真实的订单簿数据。我们将收取象征性的费用，不对应任何特定的交流。我们将 time_fee 设置为 0，因为应该没有成本。然而，最重要的是，我们应该讨论 DataGenerator 类及其作用。

## 利用交易平台综合订单数据

使用 trading gym 时，我们可以选择直接处理订单数据，也可以综合我们自己的数据。首先，我们将使用 WavySignal 函数，如下所示:

```py
class WavySignal(DataGenerator):
    def _generator(period_1, period_2, epsilon, ba_spread=0):
        i = 0
        while True:
            i += 1
            bid_price = (1 - epsilon) * np.sin(2 * i * np.pi / period_1) + \
                epsilon * np.sin(2 * i * np.pi / period_2)
            yield bid_price, bid_price + ba_spread

```

对于那些不熟悉生成器函数的人来说，它们通常用于我们需要遍历大量数据的情况，我们已经预先确定了应该从哪里读取这些数据；然而，考虑到我们正在寻找的应用程序的性质，在内存中存储这些数据会太大。相反，对象被存储。然而，向前看，这个生成器将根据前面的逻辑生成假数据。在生成器工作的情况下，我们使用以下命令运行该文件:

```py
"pythonw –m chapter4.market_making_example"

```

我们应该观察到类似于图 [4-3](#Fig3) 中的输出。

![img/480225_1_En_4_Fig3_HTML.jpg](img/480225_1_En_4_Fig3_HTML.jpg)

图 4-3

WavySignal 数据发生器的输出

在这个特殊的例子中，我们利用深度 Q 网络来解决这个问题。正如我们所见，DQN 更喜欢开发环境，而不是强调勘探。除了积累的知识之外，这也让我们获得了比之前更高的分数。因为这是合成数据，所以没有必要继续分析这个问题。这有助于我们将重点放在训练和选择算法上。然而，在现实环境中，我们显然希望解决问题，以便找出我们可以在现实生活场景中部署的解决方案。

## 使用 Trading Gym 生成订单簿数据

在这种环境下，我们有两个选择:(1)使用假数据或(2)使用真实的市场数据。除了让自己熟悉环境是如何运作的，我不认为假数据有多大效用。因此，我们将开始利用真实数据。这将我们带到了 CSVStreamer()类，如下所示:

```py
class CSVStreamer(DataGenerator):
def _generator(filename, header=False):
        with open(filename, "r") as csvfile:
            reader = csv.reader(csvfile)
            if header:
                next(reader, None)
            for row in reader:
                #assert len(row) % 2 == 0
                yield np.array(row, dtype=np.float)

(code redacted, please see tgym github!)

```

CSVStreamer 类本质上可以用 _generator()函数来概括，我们在前面的代码中已经展示过了。它只是浏览文件中的每一行，假设第一列是出价，第二列是要价。读者可以从 LOBSTER 下载数据，从而获得不同的订单数据，或者从彭博等供应商那里购买这些数据。可以通过以下 URL 访问这个库: [`https://lobsterdata.com/`](https://lobsterdata.com/) 。

这显然是相当昂贵的，所以它应该留给那些有大量研究预算和/或在已经有彭博终端可用的机构工作的人。我们将在这个例子中使用的“生成器”变量是加载这个存储库中包含的订单簿数据的 CSVStreamer。接下来，让我们从检查本例中执行大部分计算的函数开始:

```py
def train_model(model, environment):
(code redacted, please see github)
            while step < max_steps:

                step += 1; decay_step += 1

                action, explore_probability = exploit_explore(...)
                state, reward, done, info = environment.step(action)

```

类似于我们在前一章展示的*厄运*的例子，大部分代码最终都是同质和相似的。我们将以与前面相同的方式迭代环境，除了这里，我们将集中比较多种方法的性能，并评估我们应该使用哪一种。

## 试验设计

虽然做市商使用什么算法很少公开，但一般来说，我们希望使用一套简单的规则。下面的算法将形成我们的控制组，以及对为什么基于规则的系统优于随机生成的选择集的基本理解。与其他实验一样，控制组的目的是让我们将模型的结果与它进行比较，看看我们是否超过了控制组设定的基准。这套新的方法将组成实验组。我们将根据以下标准评估算法是否成功:

*   整体报酬

*   整个实验的平均回报

事不宜迟，让我们讨论一下我们是如何得出对照组/基线算法的。下面列出了我们两个策略的要求:

**策略 1(实验组)**

*   随机选择所有选项。

**策略二(对照组)**

*   随机选择买入，持有，卖出。

*   如果头寸很长，出售资产。

*   如果头寸不足，购买资产。

*   如果我们持有现金头寸，随机选择一个选项。

这两种策略的代码都将由 baseline_model()函数执行，如下所示:

```py
def baseline_model(n_actions, info, random=True):

    if random == True:
        action = np.random.choice(range(n_actions), p=np.repeat(1/float(n_actions), 3))
        action = possible_actions[action]

    else:

        if len(info) == 0:
            action = np.random.choice(range(n_actions), p=np.repeat(1/float(n_actions), 3))
            action = possible_actions[action]

        elif info['action'] == 'sell':
            action = buy

        else:
            action = sell

    return action

```

读者现在应该已经熟悉了“info”字典，它显示了来自相关环境的信息。在 Trading Gym 中，信息字典显示最近的动作。如果我们持有现金，字典将是空的。如果我们有一个未结头寸，它会在“操作”键下显示“买入”或“卖出”，有时会显示在我们没有持有现金的情况下最近一次操作的利润。在前面的实验中，我们将在 1000 次试验中重复 100 个单独的交易。最后，在我们训练完我们的模型后，我们将重复同样的方案并比较结果。利用这两种策略，我们从实验中获得了以下结果:

*   **策略 1 平均奖励**–30，890

*   **策略 2 平均奖励**–62，777

我们有以下与这些实验相关的分布和数据(图 [4-4](#Fig4) 和 [4-5](#Fig5) )。

![img/480225_1_En_4_Fig5_HTML.jpg](img/480225_1_En_4_Fig5_HTML.jpg)

图 4-5

通过算法选择答案的分数分布

![img/480225_1_En_4_Fig4_HTML.jpg](img/480225_1_En_4_Fig4_HTML.jpg)

图 4-4

随机选择行动的分数分布

正如之前的 1000 次试验所显示的，当我们的决策背后有一些合理的逻辑时，我们选择的奖励会比在所有这些试验中随机选择行动产生更好的结果。因此，按照这种逻辑，如果我们找到一个模型，能够最优地选择这些结果，而不是像我们之前那样只采用简单的启发式方法，那么我们应该能够进一步提高我们的产量。考虑到这种方法，让我们采取我们提出的解决方案。

### RL 方法 1:政策梯度

虽然普通的策略梯度确实有其缺点，但是允许我们轻松地迭代选择的决策数量相对有限。这个空间的负面影响是，我们可能没有捕捉到状态空间的连续元素。既然如此，我们有一个需要解决的紧迫问题，那就是损失函数。当我们第一次使用策略梯度时，我们只有两个类，并且在一个离散的样本空间中操作。因此，我们能够利用对数似然损失。然而，在这个例子中，我们有多个类，并且在一个连续的空间中操作。这些都是我们应该意识到的挑战，我们将在后面研究其结果。

对于这个例子，我们将使用分类交叉熵损失函数以及另一个自定义损失函数。前者源于 Keras，通常用于包含两个以上类的分类方案中。

当我们运行前面设计的实验时，这个实例中的结果都非常糟糕。在许多不同的参数和不同的风格中，利用政策梯度是很不明智的。考虑到这一点，让我们试试深度 Q 网络。

### RL 方法 2:深度 Q 网络

对于这个例子，就我们如何构建问题而言，Q 学习肯定是一个极好的选择，但深度 Q 学习最终应该是我们选择的方法。这背后的原因是，状态空间，特别是当考虑到众多的选项时，可能相当大。当我们运行这部分功能时，我们应该注意到类似于图 [4-6](#Fig6) 的输出。

![img/480225_1_En_4_Fig6_HTML.jpg](img/480225_1_En_4_Fig6_HTML.jpg)

图 4-6

培训 DQL 模型的示例截图

在几次迭代的训练中，考虑到我们拥有的数据量，我观察到超过一集的训练基本上是不可取的。尽管如此，有时取得的结果非常不一致。在一些迭代中，我观察到结果非常好，模型的一些结果根本不选择任何动作，而一些动作。有几次，我观察到，在这种情况下，做市算法在训练中表现相当好，但这些结果既不稳定也不一致。绝大多数情况下，我注意到我所建议的模型通常表现不佳，并且经常做出不受欢迎的决策。但是，接下来，让我们看看重复样本外实验时的结果(图 [4-7](#Fig7) )。

![img/480225_1_En_4_Fig7_HTML.jpg](img/480225_1_En_4_Fig7_HTML.jpg)

图 4-7

奖励分数分布

上述结果不仅大大优于基线，而且显著优于政策梯度模型，使其成为显而易见的选择。平均奖励为 34，286，348，这绝对是一个可行的解决方案。正如我们在图 [4-7](#Fig7) 中看到的，我们的分数是令人满意的，我们似乎有一个双峰分布。

## 结果和讨论

在回顾了所有的结果之后，有理由声明读者既不应该使用深度 Q 学习算法，也不应该使用策略梯度。总之，我们提出这一建议的原因如下:

*   **超过基线算法—**为了证明任何实验方法的合理性，我们必须超过基线。值得检查的是，我们是否适当地对数据进行了采样，或者是否有足够的数据用于这一特殊情况。

*   **有些算法赔钱了—**对我们在这里采用的第一种方法的最客观的批评是，它没有实现其业务目标，即制定一个有利可图的战略。在商业环境中使用这种算法是不可取的，并且最终会超出理论上可行的范围，我们必须选择实际可行的方法。

未来可能的解决方案是阅读这一领域的一些现有文献，尝试解决这个问题。也就是说，许多公开发表的论文同样遇到了算法要么暂时盈利，要么最终不盈利的问题。读者也应该在他们自己的时间内随意尝试和应用 ActorCritic 方法，但是也应该大胆尝试其他现有的解决方案，尝试不同的参数、费用结构和这里没有提到的对策略的不同约束。强化学习研究的困难在于，奖励函数的设计是一个抽象的过程，但也就是说，它是设计好实验的关键组成部分。

## 结论

下面的例子完成后，我们现在已经到了第一章的末尾，在这一章中，我们从头开始处理强化学习问题，并尝试改进现有的方法。本章的一些关键要点是试图创建一个可部署的解决方案的困难，但向读者提出一个框架，并展示我们如何在样本中连续达到更好的结果，每次都表明我们离答案越来越近。在此之前，我们处理的许多问题都是相对简单或经典的例子，它们的价值在于能够透明地展示算法的力量。现在我们终于到了这个话题的难点，那就是学习推动各种解决方案。对于那些直接从事研究或工业的人来说，这个过程应该很熟悉。如果不是，我强烈建议你开始实现它。也就是说，我们将进入最后一章，在这里我们将重复这一过程，但在一个全新的环境中，我们将引导读者从头开始创建自己的 OpenAI 健身房环境，以便他们可以开始自己进行研究！