# 三、强化学习算法：Q 学习及其变体

随着对政策梯度和行动者-批评家模型的初步讨论结束，我们现在可以讨论读者可能会发现有用的替代深度学习算法。具体来说，我们将讨论 Q 学习、深度 Q 学习以及深度确定性策略梯度。一旦我们涵盖了这些，我们将足够精通，开始处理更抽象的问题，更具体的领域，这将教用户如何处理不同任务的强化学习。

## q 学习

q 学习是无模型学习算法家族的一部分，它通过观察所有可能的动作并评估它们中的每一个来学习策略。在这个算法中，有两个矩阵我们会经常引用:Q 矩阵和 R 矩阵。前者代表与算法同名的算法，包含我们在其中实现策略的环境的累积知识。这个矩阵中的所有条目都被初始化为 0，目标是使产生的奖励最大化。在环境中的每一步，Q 矩阵被更新。R 矩阵是这样一个环境，每行代表一个州，每列代表调到另一个州的奖励。该矩阵的结构类似于相关矩阵，其中每行和每列索引相互镜像。我们在图 [3-1](#Fig1) 和 [3-2](#Fig2) 中有一个 Q 和 R 矩阵的可视化。

![img/480225_1_En_3_Fig2_HTML.jpg](img/480225_1_En_3_Fig2_HTML.jpg)

图 3-2

R 表的可视化

![img/480225_1_En_3_Fig1_HTML.jpg](img/480225_1_En_3_Fig1_HTML.jpg)

图 3-1

Q 表的可视化

代理可以看到 R 表中它可以采取的即时操作，但看不到其他任何内容。由于这个限制，这正是 Q 表变得重要的地方。前面提到的 Q 表包含了在给定时间段内它所填充的关于环境的所有累积信息。在某种意义上，我们可以把 Q 表想象成地图，把 R 表想象成世界。具体来说，Q 表是如何被更新的由下面给出:

![$$ Q\left({s}_t,{a}_t\right):= Q\left({s}_t,{a}_t\right)+\alpha \left[\left(r\left({s}_t,{a}_t\right)+\gamma .\max \left\{Q\left({s}_{t+1},{a}_{t+1}\right)\right\}\right)-Q\left({s}_t,{a}_t\right)\right] $$](img/480225_1_En_3_Chapter_TeX_Equa.png)

其中*Q*(*s*<sub>T5】t</sub>，*a*<sub>*t*</sub>)=单元格条目， *α* =学习率， *γ* =折扣因子，max {*Q*(*s*<sub>*t*+1</sub>， *a*

## 时间差异学习

在引言一章中，我们简要地谈到了马尔可夫决策过程的主题。更具体地说，MDP 指的是部分随机但也依赖于决策者或受决策者控制的事件。我们将 MDP 定义为以下 4 元组:

![$$ \left(S,A,{P}_a,{R}_a\right) $$](img/480225_1_En_3_Chapter_TeX_Equb.png)

其中 *S* =表示状态的集合， *A* =表示允许动作的集合， *P* <sub>*a*</sub> =在时间 *t* 处于状态 *s* 的动作 *a* 导致在时间 *t* + 1 处于状态 *s* ʹ，*r*<sub>*a*</sub>

 *提醒一下，图 [3-3](#Fig3) 是马尔可夫决策过程的一个例子。

![img/480225_1_En_3_Fig3_HTML.jpg](img/480225_1_En_3_Fig3_HTML.jpg)

图 3-3

马尔可夫决策过程

正如我们之前所说的，大多数强化学习都围绕着这样的状态，从这些状态中，我们可以执行产生回报的行动。我们试图达到的目标是为决策者选择最优的政策，使产生的回报最大化。我们在引言中简单地提到了时间差异学习，但是现在是详细讨论这个问题的合适时机。

TD 学习被广泛地描述为一种预测依赖于特定信号的未来值的量的方法。它指的是在不同时间步长上预测的“时间差”。TD 学习被设计成使得在当前时间步长的预测被更新为，以便下一个时间步长的后续预测是正确的。q 学习本身就是 TD 学习的一个例子。我们可以解决 TD 学习问题的一种方法是ε贪婪算法，特别是在这里。

## ε-贪婪算法

最终，在大量迭代之后，Q 表足够好，可以被代理直接利用。为了达到这一点，我们希望 Q 学习算法利用表中的信息少于它探索的信息。这就是通常所说的勘探-开采权衡，它由ε参数控制。这里的关键是，可能用来达成解决方案的第一条可能路径并不一定是最佳路径。有鉴于此，如果我们不断探索，就不可能总会找到比目前更好的解决办法，因此我们放弃解决问题。为了缓解这个问题，建议使用ε-贪婪算法。

ε-贪婪算法属于多臂土匪问题家族。这被描述为一个问题，我们必须在各种选项中做出选择，最终目标是最大化回报。说明这个问题的经典例子是想象一个赌场，我们有四台机器，每台机器都有不同的未知回报概率。我们将一个伯努利多臂土匪描述为一个集合动作和奖励分别表示在元组< *A* 、 *R* >中，其中有 *K* 个机器，奖励概率为{*θ*T8】1、…、*θ*<sub>*K*</sub>}。每个动作对应于与相应吃角子老虎机的互动，奖励是随机的，因为它们将以概率*Q*(*a*<sub>*t*</sub>)返回，否则为 0。预期回报由以下等式表示:

![$$ Q\left({a}_k\right)=\mathbbm{E}\left[{r}_k|{a}_k\right]={\theta}_k,k\in \left\{1,\dots, k\right\} $$](img/480225_1_En_3_Chapter_TeX_Equc.png)

我们的目标是通过选择最佳行动来最大化累积回报，其中最佳回报概率和损失函数分别由以下等式给出:

![$$ {\theta}^{\ast }=Q\left({a}^{\ast}\right)=\underset{a\in A}{\max }Q(a)=\underset{1\le i\le K}{\max }{\theta}_i $$](img/480225_1_En_3_Chapter_TeX_Equd.png)

![$$ {\mathcal{L}}_T=\mathbbm{E}\left[\sum \limits_{t=1}^T\left({\theta}^{\ast }-Q\left({a}_t\right)\right)\right] $$](img/480225_1_En_3_Chapter_TeX_Eque.png)

虽然有多种方法来解决多臂土匪问题，我们将在这里集中讨论策略。这是一种算法，通过下面的等式来估计动作的质量:![img/480225_1_En_3_Figa_HTML.jpg](img/480225_1_En_3_Figa_HTML.jpg)

其中*N*<sub>*t*</sub>(*a*)=动作 a 被执行的次数，![img/480225_1_En_3_Figb_HTML.jpg](img/480225_1_En_3_Figb_HTML.jpg) =二进制指示函数。

如果ϵ很小，那么我们将探索我们周围的环境。然而，除此之外，我们将采取目前已知的最佳行动。为了说明 Q 学习算法的整体，我们将学习玩一个叫做“冰封湖”的游戏

## 用 Q 学习解决的冰湖

冰封湖是健身房提供的游戏，玩家试图训练一个代理人从湖的起点走到另一个终点。然而，并不是所有的冰面都是冰冻的，踩上去会导致我们输掉比赛。除了达成目标，我们不接受任何奖励。读者可以想象环境看起来像下图(图 [3-4](#Fig4) )。

![img/480225_1_En_3_Fig4_HTML.jpg](img/480225_1_En_3_Fig4_HTML.jpg)

图 3-4

冰冻湖泊环境

与我们编写的大多数其他文件类似，我们从定义以后可以使用的参数以及环境开始。两个主要函数 populate_q_matrix()和 play_frozen_lake()包含了前面定义的许多辅助函数。让我们从遍历填充 Q 矩阵的函数开始。

```py
def populate_q_table(render=False, n_episodes=n_episodes):

(documentation redacted, please see github)

    for episode in range(n_episodes):
        prior_state = environment.reset()
        _ = 0
        while _ < max_steps:
            if render == True: environment.render()
            action = exploit_explore(prior_state)
            observation, reward, done, info = environment.step(action)

            update_q_matrix(prior_state=prior_state,
                            observation=observation,
                            reward=reward,
                            action=action)
      (CODE TO BE CONTINUED)

```

浏览代码直到第二个帮助函数 update_q_matrix()，我们看到我们定义了许多剧集，我们将在这些剧集上填充 Q 表。读者可以多加或少加几集，看看表现如何变化，但这里我们选了一万集。我们现在来看第一个助手函数 exploit_explore()。不言而喻，这是执行ε-贪婪探索算法的算法，以确定我们应该采取这两个动作中的哪一个。以下函数对此进行了详细描述。

```py
def exploit_explore(prior_state, epsilon=epsilon):
(documentation redacted, please read github)
    if np.random.uniform(0, 1) < epsilon:
        return environment.action_space.sample()
    else:
        return np.argmax(Q_matrix[prior_state, :])

```

正如读者可以看到的，我们只探索了一个随机动作，在这个例子中，我们从均匀分布中随机抽取的值是 0。否则，我们会选择给定状态下我们所知道的最佳可能行动。在更大的函数体中向前移动，继续我们在前面的例子中让代理在环境中执行一个动作。这就产生了差异；然而，现在我们必须更新 Q 矩阵。

```py
def update_q_matrix(prior_state, observation, reward, action):
prediction = Q_matrix[prior_state, action]
    actual_label = reward + gamma * np.max(Q_matrix[observation, :])
    Q_matrix[prior_state, action] = Q_matrix[prior_state, action] + learning_rate*(actual_label - prediction)

```

根据前面的等式，我们更新 Q 矩阵的条目，其中每一列表示要采取的动作，每一行表示不同的状态。我们在每一集里继续这个过程，直到我们达到允许的最大步数，或者掉进冰里。一旦我们达到了最大集数，我们就可以用 Q 表玩游戏了。读者应该观察游戏在终端运行时会出现如图 [3-5](#Fig5) 所示的画面。

![img/480225_1_En_3_Fig5_HTML.jpg](img/480225_1_En_3_Fig5_HTML.jpg)

图 3-5

冰湖游戏

当你在某一集赢或输时，终端会输出信息。我们通常使用参数进行多次实验观察，假设代理通常会在 10 集内赢得两到三次，并在大约 20-30 步内达成解决方案。

在某种程度上，Q 学习的主要优点是它不需要模型，并且算法相当透明。很容易解释为什么在给定的时间状态下代理人会选择一个动作。也就是说，这样做的主要缺点是，如果我们要用信息充分填充 Q 矩阵，当我们处理非常大的环境时，获得在给定状态下做什么的知识所需的经验在计算上是非常昂贵的。虽然这个冰冻湖的例子相当有限，但是像更复杂的视频游戏这样的环境可能需要特别长的时间才能得到一个好的 Q 表。为了克服这一限制，设计了深度 Q 学习。

## 深度学习

深度 Q 学习来自 Q 学习，相当简单，因为这两种方法之间唯一的真正区别是 DQL 近似 Q 表中的值，而不是试图手动填充它们。精确地说，这是如何实现的是ε-贪婪搜索(或替代算法)和动作结果之间的联系。epsilon-greedy 搜索算法解决了我们如何决定是利用还是探索的问题，并且我们反过来基于该状态下的动作值来更新 Q 矩阵。从这个意义上说，我们可以看到，我们希望将达到目标和采取行动之间的损失降至最低。在这个意义上，我们现在有了可以利用梯度下降的东西，它被表示为下面的等式:

![$$ {\mathcal{L}}_i\left({\theta}_i\right)={\mathbbm{E}}_{a\sim \mu}\left[{\left({y}_i-Q\left(s,a;{\theta}_i\right)\right)}^2\right], $$](img/480225_1_En_3_Chapter_TeX_Equg.png)

![$$ {y}_i:= {\mathbbm{E}}_{a^{\prime}\sim \mu}\Big[r+\gamma \underset{a^{\prime }}{\max }Q\left({s}^{\prime },{a}^{\prime };{\theta}_{i-1}\right)\left|\ {S}_t=s,{A}_t=a\right] $$](img/480225_1_En_3_Chapter_TeX_Equh.png)

其中 *μ* =行为策略， *θ* =神经网络参数。

目标标签和 Q 矩阵都由两个独立的神经网络预测。目标网络共享 Q 网络的权重和偏差，但是它们在 Q 网络之后被更新。然而，接下来，让我们讨论一下体验回放的重要性，以及我们在这里如何利用它。如果我们在强化学习的环境中引入全新的数据，神经网络将会覆盖权重。因此，这就是为什么经常有不同的模型被训练用于不同的目的。体验回放是我们如何通过存储观察到的体验来利用它们，然后这有助于减少我们可能观察到的体验之间的相关性。实际上，我们将本章开始时介绍的元组保存在内存中。在训练期间，我们将使用元组计算目标标签，然后应用梯度下降，以便我们具有将在整个环境上很好地推广的权重和偏差。然而，接下来，让我们现在尝试使用深度 Q 学习来解决一个问题，并看看我们的问题的复杂性如何发生了显著变化。

## 用深度 Q 学习玩毁灭战士

利用 DQL 的一个经典例子是原始的 Doom 视频游戏，如图 [3-6](#Fig6) 所示，它的环境也是测试各种机器学习算法的一个极好的环境。Doom 是一款第一人称射击游戏，玩家必须在一个三维环境中与敌人战斗。因为这是一个更老的 3D 游戏，玩家在环境中移动的方式与我们许多理论上的代理在 Q 矩阵中移动的方式相同。这将是我们应用强化学习的第一个连续控制问题。

![img/480225_1_En_3_Fig6_HTML.jpg](img/480225_1_En_3_Fig6_HTML.jpg)

图 3-6

厄运中的一个关卡的例子

简单地说，我们把连续控制系统和离散控制系统区分开来，前者的变量和参数是连续的，后者是离散的。强化学习环境下的连续过程的一个例子是驾驶汽车或教机器人走路。离散控制过程的一个例子是我们处理的第一个问题，车杆，以及“经典控制”中的其他问题，如摆动钟摆。尽管为了理解算法，有许多离散的任务值得分析，但许多用强化学习实现的任务是连续的。这与状态空间的巨大规模一起，使其成为深度 Q 学习的绝佳候选。我们将通过观察简单水平与更困难水平的差异以及算法性能的差异来解决这个问题。

具体到游戏本身，目标相当简单。我们必须在没有死亡的情况下完成关卡，这显然需要在关卡结束前杀死敌方战斗人员。大多数敌人会先发制人地报复，因此算法将主要集中在如何基于此做出反应的训练上。概括地说，我们将通过该算法执行的两个主要过程是:( 1)对环境进行采样并将经验存储在 MDP 元组中;( 2)选择其中的一些作为批量训练示例。让我们首先讨论除了我们将利用什么类型的模型架构之外，我们将如何为这个模型预处理我们的数据。

```py
class DeepQNetwork():

    def __init__(self, n_units, n_classes, n_filters, stride, kernel, state_size, action_size, learning_rate):
    (code redacted, please see github)
        self.input_matrix = tf.placeholder(tf.float32, [None, *state_size])
        self.actions = tf.placeholder(tf.float32, [None])
        self.target_Q = tf.placeholder(tf.float32, [None, *state_size])

        self.network1 = convolution_layer(inputs=self.input_matrix,
                                     filters=self.n_filters,
                                     kernel_size=self.kernel,
                                     strides=self.stride,
                                     activation='elu')

(code redacted please see github)

```

类似于之前我们定义为图的TensorFlow图，我们将从定义几个特定的属性开始。这些将在后面的 doom_example.py 中的“play_doom()”函数中使用，但是我们将在后面解决这些问题。接下来，我们可以看到，类似于我们在《超级马里奥兄弟》中使用的示例，我们将希望使用 LeNet 架构，只是在这种情况下，我们将利用一个接受四维的层，因为我们正在攻击帧。类似地，我们最终将特征地图展平成一个数组，然后通过一个完全连接的 softmax 层输出。从这个 softmax 层，我们将在训练中采样我们的动作。图 [3-7](#Fig7) 显示了我们将用于 Deep Q 网络的模型架构示例。

![img/480225_1_En_3_Fig7_HTML.jpg](img/480225_1_En_3_Fig7_HTML.jpg)

图 3-7

深度 Q 网络的示例架构

回到讨论输入数据，在前面的例子中，我们没有堆叠帧，而是原样传递当前和先前状态，作为输入数据的重新格式化矩阵。为什么这很重要，尤其是在三维环境中，背后的原因是因为它让深度 Q 网络了解代理正在诱导的运动。这个方法是 Deep Mind 提出来的。我们通过以下函数对帧进行预处理和堆叠:

```py
def preprocess_frame(frame):
    cropped_frame = frame[30:-10,30:-30]
    normalized_frame = cropped_frame/float(255)
    preprocessed_frame = transform.resize(normalized_frame, [84,84])
    return preprocessed_frame

```

我们首先利用灰度图像开始，感谢 vizdoom 库以这种形式提供给我们。如果这不是灰度级，用户应该利用像 **OpenCV** 这样的库来执行预处理。接下来，我们将像素值再次缩放 255 倍，就像我们在超级马里奥的例子中所做的一样，理由也是一样的。然而，一个小的不同是，在这个初始的例子中，我们将裁剪掉框架的顶部，因为《毁灭战士》中的天花板只是为了营造气氛，并不包含任何值得评估的东西。当我们堆叠帧时，我们在这里利用前面的函数:

```py
def stack_frames(stacked_frames, state, new_episode, stack_size=4):

    frame = preprocess_frame(state)

    if new_episode == True:

        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)
        for i in range(4):
            stacked_frames.append(frame)

        stacked_state = np.stack(stacked_frames, axis=2)

    else:

        stacked_frames.append(frame)
        stacked_state = np.stack(stacked_frames, axis=2)

    return stacked_state, stacked_frames

```

与将帧转换为四个堆栈的函数不同，这个函数的重要之处在于它是如何精确地发生的。当第一次调用这个函数时，我们取前四帧。接下来，我们添加最新的帧，同时删除最后一个帧，这样这个过程应该代表一个先进后出(FILO)过程。然而，要记住的是，这个过程不是很现实，因为人类不会看到多个交错的帧，而是一次看到所有帧。除此之外，由于用于存储这些堆叠图像的存储器，这使得训练明显更加困难。当我们在接下来的章节中学习不同的例子时，用户应该记住这一点。接下来，我们将利用稍微复杂一点的贪婪ε策略，其中我们还将利用衰减率，如以下函数所示:

```py
def exploit_explore(session, model, explore_start, explore_stop, decay_rate, decay_step, state, actions):
    exp_exp_tradeoff = np.random.rand()
    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)

    if (explore_probability > exp_exp_tradeoff):
        action = random.choice(possible_actions)
    else:
        Qs = session.run(model.output, feed_dict = {model.input_matrix: state.reshape((1, * state.shape))})
        choice = np.argmax(Qs)
        action = possible_actions[int(choice)]

```

这背后的想法是贪婪ε策略本质上与我们在原始 Q 学习示例中看到的相同，除了衰减是指数的，因为随着时间的推移，我们越来越有可能探索更少的内容，迫使算法利用其积累的知识。现在，在解释了助手函数之后，让我们来看一下实际用于训练模型的函数。事不宜迟，让我们来观察在这个级别上训练模型的结果。然后，我们将移动到一个不同的水平，看看模型如何执行。

### 简单毁灭级别

在这个场景中，玩家处于一个简单的环境中，他们可以向左、向右移动，和/或向敌方战斗人员射击。这个敌方战斗人员不会还击，只是偶尔向左或向右移动。运行代码时，读者应该会看到如图 [3-8](#Fig8) 和 [3-9](#Fig9) 所示的输出和脚本。

![img/480225_1_En_3_Fig9_HTML.jpg](img/480225_1_En_3_Fig9_HTML.jpg)

图 3-9

简单厄运环境示例

![img/480225_1_En_3_Fig8_HTML.jpg](img/480225_1_En_3_Fig8_HTML.jpg)

图 3-8

培训模式截图

## 培训和绩效

图 [3-10](#Fig10) 显示了在不同事件中训练 Q 矩阵的结果以及样本外结果。

![img/480225_1_En_3_Fig10_HTML.jpg](img/480225_1_En_3_Fig10_HTML.jpg)

图 3-10

训练时深 Q 网络分数

读者应该知道，像这样的任务，正如我们所说的，由于使用了预处理和计算，是相当大的内存密集型任务。除此之外，当神经网络陷入局部最优时，有时它不能恰当地学习采取正确的行动。虽然列出的参数通常产生可接受的样本外解决方案，但有时该神经网络**表现不佳。这是局限性之一。**

 **## 深度 Q 学习的局限性

正如我们之前所展示的，深度 Q 学习并不是没有缺点。但是，除了这个例子之外，这些低效的地方在哪里呢？1993 年，巴斯蒂安·特龙和安东·施瓦茨在他们的论文*使用函数逼近进行强化学习*中对此进行了更具体的研究。他们发现，深度 Q 网络经常因为高估而学习到非常高的动作值。根据设计，这是由于下面给出的目标标签公式:

![$$ {y}_i:= {\mathbbm{E}}_{a^{\prime}\sim \pi}\left[r+\gamma \underset{a^{\prime }}{\max }Q\left({s}^{\prime },{a}^{\prime };{\theta}_{i-1}\right)\right] $$](img/480225_1_En_3_Chapter_TeX_Equi.png)

在这个等式中，我们可以看到，我们总是选择当时的最大已知值，这可以优先选择我们的网络在这些值可能高得不切实际的阶段学习这些值。这就是函数逼近会导致高估的具体原因。高估，正如这里可能发生的那样，会导致糟糕的政策，并倾向于导致模型中的偏差。正如在毁灭战士的例子中所表现的那样，代理人经常感到被迫射击，而不管它相对于敌人的位置。这个问题能解决的有多精确？

## 双 Q 学习和双深度 Q 网络

如前面等式中所强调的，在给定环境状态的情况下，max 运算符使用相同的值来选择和评估动作。准确地说，当我们把它分成两个独立的过程(选择和评估)时，我们得到了双 Q 学习。双 Q 学习利用两个价值函数，每个价值函数有两个相应的权重集。其中一个权重集用于确定贪婪利用或探索权衡问题，另一个用于确定给定动作的价值。然后，我们将目标近似值改写如下:

![$$ {Y}_t^Q={R}_{t+1}+\gamma Q\left({S}_{t+1},\arg \underset{a}{\max }Q\left({S}_{t+1},a;{\theta}_t\right);{\theta}_t\right) $$](img/480225_1_En_3_Chapter_TeX_Equj.png)

至此，我们可以讨论双 Q 网络以及如何利用它们来克服深 Q 网络的缺点。我们不是添加额外的模型，而是利用目标网络来估计价值，同时利用在线网络来评估勘探-开发决策过程。双 Q 网络的目标函数如下:

![$$ {Y}_t^{DoubleQN}\equiv {R}_{t+1}+\gamma Q\left({S}_{t+1},\arg \underset{a}{\max }Q\left({S}_{t+1},a;{\theta}_t\right),{\theta}_t^{-}\right) $$](img/480225_1_En_3_Chapter_TeX_Equk.png)

## 结论

Q 学习和深度 Q 学习的例子都完成了，我们建议读者尝试在各种上下文中应用这些算法。在必要的地方，他们可以更改参数并派生/更改现有的代码和模型。无论如何，我建议读者在前进的道路上谨记以下几点:

*   **Q 学习简单明了且易于解释—**该算法的好处是易于理解为什么 Q 值是这样输入的。对于实现算法需要透明性的任务，考虑这样的事情并不是不明智的。

*   **Q 学习对大状态空间有限制！**–虽然前面的评论适用于简单的问题，但重要的是要认识到，在像 Doom 示例和更复杂的环境中，Vanilla Q Learning 将花费大量的时间来完成。

*   **深度 Q 学习还能陷入局部最优！–**像其他强化学习算法一样，DQN 仍然可以找到局部最优策略，但不能找到全局最优策略。从训练的角度来看，找到这种全局最优可能是彻底的。

*   **尝试实现双 Q 学习和双深度 Q 网络！**–Q 学习和 dqn 的局限性已经被越来越先进的技术快速克服。这个起点应该允许您从头开始尝试实现最先进的算法。

完成这些例子后，让我们继续讨论一些我们还没有涉及的其他强化学习算法，并深入讨论这些算法。***