# 11.深度学习和其他示例问题

既然我已经充分地介绍了如何使用和应用机器学习概念，我们应该最终使用 r 来应用和编码深度学习模型。这似乎是一项艰巨的任务，但不要被吓倒。如果您已经能够成功地编写本书中的所有代码，那么就只需要适应新的软件包了。我们将讨论各种深度学习示例，但将从处理更简单的模型开始，然后最终转向更复杂的模型。这些练习有两个目的:

*   展示如何构建这些模型或者从不同的包中访问它们
*   举例说明它们在实际概念中的应用

## 自动编码器

该书深度学习章节中描述的许多其他模型在谈到如何使用它们时都相对简单，但我发现自动编码器的使用不会自动变得清晰。因此，我想探索一个用例，在这个用例中，自动编码器的使用在实际环境中变得非常清晰。让我们考虑这样一种情况，我们希望使用自动编码器来提高第 [10](10.html) 章中分类算法的性能。具体来说，我指的是我们走过的分类问题，其中我们试图根据几个特征来确定一对个体是否会进行第二次约会。让我们从贝叶斯分类器开始:

```
#Bayes Classifier
#Bayes Classifier
AUC <- c()
for (i in 1:100){
  rows <- sample(1:nrow(processedData), 92)
  bayesClass <- naiveBayes(y = as.factor(second_date[rows]), x = processedData[rows, ], data = processedData)
  y_h <- predict(bayesClass, processedData[rows, ], type = c("class"))
  AUC <- append(roc(y_h, as.numeric(second_date[rows]))$auc, AUC)
}

summaryStatistics(AUC)
curve <- roc(y_h, as.numeric(second_date[rows]))
plot(curve, main = "Bayesian Classifier ROC")

```

当执行前面的代码时，会产生如图 [11-1](#Fig1) 所示的结果。

![A435493_1_En_11_Fig1_HTML.jpg](../Images/A435493_1_En_11_Fig1_HTML.jpg)

图 11-1。

ROC plot for Bayesian classifier

当收集样本统计数据时，我们观察该模型的 AUC 分数:

```
Mean       Std.Dev     Min        Max    Range
0.8210827  0.02375922  0.7571429  0.875  0.1178571

```

这些都是客观上的好成绩。然而，出于这个例子的目的，我们将使用一个自动编码器来帮助进一步提高这个模型的性能。这就是我介绍 h2o 的地方。h2o 为 R(以及其他语言)提供了一个深度学习框架，您会发现它对实现许多模型非常有用。我鼓励你搜索文档，因为深度学习模型的一些实现很难找到(更不用说找到健壮的实现了)。让我们初始化 h2o 并使用自动编码器:

```
#Autoencoder
h2o.init()
training_data <- as.h2o(processedData, destination_frame = "train_data")
autoencoder <- h2o.deeplearning(x = colnames(processedData),
 training_frame = training_data, autoencoder = TRUE, activation = "Tanh",
 hidden = c(6,5,6), epochs = 10)
autoencoder

```

h2o 类似于 TensorFlow，每个会话都必须初始化。初始化之后，无论什么数据通过所用的模型，都必须转换成 h2o 友好的格式。我们对训练数据进行转换。我们的 autoencoder 有三个隐藏层，每个层在给定的层中分别有六个、五个和六个神经元(由`h2o.deeplearning()`函数中的“hidden”参数表示)。我们用 tanh 作为我们的激活函数。执行以下代码后，我们会看到如图 [11-2](#Fig2) 所示的内容。

![A435493_1_En_11_Fig2_HTML.jpg](../Images/A435493_1_En_11_Fig2_HTML.jpg)

图 11-2。

Summary of autoencoder function

注意 MSE 值。因为我们试图重新创建一个函数的输入，这就变成了一个回归任务。因此，我们使用传统的回归统计(MSE 和 RSME)来评估该算法的有效性。让我们仔细看看此处得出的 MSE，并根据保存训练数据的数据帧的索引来查看 MSE:

```
#Reconstruct Original Data Set
syntheticData <- h2o.anomaly(autoencoder, training_data, per_feature = FALSE)
errorRate <- as.data.frame(syntheticData)

#Plotting Error Rate of Feature Reconstruction
plot(sort(errorRate$Reconstruction.MSE), main = "Reconstruction Error Rate")

```

`h2o.anomaly()`函数使用自动编码器来检测异常，我们在统计上将异常定义为在重建过程中 MSE 明显高于其他观测值的观测值。当执行前面的代码时，我们得到图 [11-3](#Fig3) 。

![A435493_1_En_11_Fig3_HTML.jpg](../Images/A435493_1_En_11_Fig3_HTML.jpg)

图 11-3。

Plot of reconstruction error

我们可以看到，从指数水平 225 到训练数据结束，MSE 稳定增加，但也急剧增加。我们可以合理地说，离群值通常是这些最后的输入。考虑到这一点，我们将使用由 MSE 确定的阈值来将离群值从非离群值中分离到它们各自的子集。我们试图通过将我们的模型拟合到这些子集来训练我们的贝叶斯分类器，并观察模型的性能相对于 AUC 分数如何提高(或不提高):

```
#Removing Anomolies from Data
train_data <- processedData[errorRate$Reconstruction.MSE < 0.01, ]

#Bayes Classifier
AUC <- c()
for (i in 1:100){
  rows <- sample(1:nrow(processedData), 92)
  bayesClass1 <- naiveBayes(y = as.factor(second_date[rows]), x = processedData[rows, ], data = processedData)
  y_h <- predict(bayesClass1, processedData[rows, ], type = c("class"))
  AUC <- append(roc(y_h, as.numeric(second_date[rows]))$auc, AUC)
}

#Summary Statistics
summaryStatistics(AUC)

```

我们遵循第 [10 章](10.html)中关于模型训练的相同一般步骤，收集 100 次试验的 AUC 统计样本。这里唯一的区别是，我们使用了低于 MSE 阈值的指数值的数据子集。查看汇总统计数据时，我们观察到以下情况:

```
Mean       Std.Dev     Min   Max        Range
0.8274664  0.03076285  0.75  0.9117647  0.1617647

```

当将我们的结果分布与原始模型进行比较时，我们观察到一个稍高的平均值，一个较高的最大值。然而，我们也观察到一个较低的最小值。因此，我们的结果的范围和标准偏差增加。让我们评估一下我们只看异常情况时的结果:

```
##########################################################################
#Using only Anomalies in Data Set
train_data <- processedData[errorRate$Reconstruction.MSE >= 0.01, ]

#Bayes Classifier
AUC <- c()
for (i in 1:100){
  rows <- sample(1:nrow(processedData), 92)
  bayesClass2 <- naiveBayes(y = as.factor(second_date[rows]), x = processedData[rows, ], data = processedData)
  y_h <- predict(bayesClass2, processedData[rows, ], type = c("class"))
  AUC <- append(roc(y_h, as.numeric(second_date[rows]))$auc, AUC)
}

#Summary Statistics
summaryStatistics(AUC)

```

执行上述代码时，我们会看到以下结果:

```
Mean       Std.Dev     Min        Max        Range
0.8323727  0.03168166  0.7692308  0.9107143  0.1414835

```

在这里，我们观察到，这种分布包含最高的平均值和最小值，与范围和标准偏差的中等结果。当在两个数据集之间进行选择时，我会主张在这种情况下使用第二个子集，因为平均而言 AUC 得分表现更好，并且考虑到至少我们仍然可以期待更高的得分。

这种技术的重要性在于，它是一种有效的方法，通过这种方法，您可以在数据子集上拟合高级模型。如果您发现您的数据集比您想要的要小，这将非常方便。尽管使用了适当的交叉验证技术、数据预处理技术和参数调整技术，但有时您会发现自己在尝试调整一个性能稍微不令人满意的模型时遇到困难。在由于缺乏数据而导致这种情况的情况下，在试图获取更多数据之前，我会首先尝试使用这种技术。至于我们实验的最后一步，让我们使用拟合的模型，看看它们在样本外的表现如何:

```
#Fitted Models and Out of Sample Performance
AUC1 <- AUC2 <- c()

for (i in 1:100){
  rows <- sample(1:nrow(processedData), 92)
  y_h1 <- predict(bayesClass1, processedData[-rows,], type = c("class"))
  y_h2 <- predict(bayesClass2, processedData[-rows,], type = c("class"))
  AUC1 <- append(roc(y_h1, as.numeric(second_date[-rows]))$auc, AUC1)
  AUC2 <- append(roc(y_h2, as.numeric(second_date[-rows]))$auc, AUC2)
}
summaryStatistics(AUC1)
summaryStatistics(AUC2)

```

当执行前面的代码时，我们看到模型与没有异常和只有异常的子集相匹配的结果，分别如图 [11-4](#Fig4) 和 [11-5](#Fig5) 所示:

![A435493_1_En_11_Fig5_HTML.jpg](../Images/A435493_1_En_11_Fig5_HTML.jpg)

图 11-5。

ROC curve for Bayes model w/o anomalies (AUC: 0.8188)

![A435493_1_En_11_Fig4_HTML.jpg](../Images/A435493_1_En_11_Fig4_HTML.jpg)

图 11-4。

ROC curve for Bayes model without anomalies (AUC : 0.7821)

```
Mean       Std.Dev     Min        Max        Range
0.7890102  0.01468805  0.75       0.8194444  0.06944444
Mean       Std.Dev     Min        Max        Range
0.8303613  0.01506222  0.7957983  0.8688836  0.07308532

```

当回顾我们实验的结果时，已经变得非常清楚的是，仅拟合异常的第二个模型比拟合没有异常的观察的模型产生明显更好的模型。但是，在我们完全确信应该使用第二个模型之前，让我们使用来自这两个模型的数据快速执行一个双边假设检验。

因为我们对结果进行了 100 次采样，所以我们可以安全地使用 Z 检验。因此，我们设置 Z 测试参数，如以下代码所示:

```
#Two Sided Hypothesis Test
require(BSDA)

z.test(x = AUC1, y = AUC2, alternative = "two.sided", mu = mean(AUC2) - mean(AUC1),
                 conf.level = 0.99, sigma.x = sd(AUC1), sigma.y = sd(AUC2))

```

当执行前面的函数时，它产生如图 [11-6](#Fig6) 所示的输出。

![A435493_1_En_11_Fig6_HTML.jpg](../Images/A435493_1_En_11_Fig6_HTML.jpg)

图 11-6。

Two-sided hypothesis test results

从统计学上看，在 99%的置信区间内，我们已经确定两个模型的结果在统计学上彼此不同，因此我们可以放心地选择拟合的第二个贝叶斯模型，因为我们知道它是最优模型。

## 卷积神经网络

当我在第 5 章[中讨论 CNN 时，我通过讨论 MNIST 数字识别用例展示了这个模型的威力。尽管这一度是 CNN 的主要用途，但现在它们正被用于越来越困难和复杂的任务。现在我想探索一个用例，在这个用例中，我们试图区分比手写数字复杂得多的不同对象。在本教程中，我们将使用加州理工学院 101 数据集，它包含 101 个对象类别，每个类别中有 60 到 800 张图像。我们将从每个类别中选取不同的图片，这样我们就可以得到不同的图片，而不会选取完全不同的图片。我们将在吉他和笔记本电脑的图像中进行选择。这些照片的样本如图](05.html) [11-7](#Fig7) 和 [11-8](#Fig8) 所示。

![A435493_1_En_11_Fig8_HTML.jpg](../Images/A435493_1_En_11_Fig8_HTML.jpg)

图 11-8。

Photo of laptop

![A435493_1_En_11_Fig7_HTML.jpg](../Images/A435493_1_En_11_Fig7_HTML.jpg)

图 11-7。

Photo of guitar

这些图像是技术的产物，但它们彼此之间有着明显的不同，我们希望人类能够区分它们。现在让我们讨论我们应该如何为 CNN 准备数据。

### 预处理

处理图像文件需要一种特殊类型的预处理，我们还没有详细讨论过，主要是因为图像识别和计算机视觉是计算机科学的一个非常特殊的子领域。明智的做法是寻找其他文本来建立你对计算机视觉的理解，但是这篇文章将会给你一个基本的概述。我们正在处理彩色图像，每个图像都有 x，y，z 维度，其中 x 和 y 是每张照片特有的，但是 z 总是 3。就计算机理解的图像文件而言，它们是相互堆叠的三层矩阵，每个像素是矩阵中的一个单独的条目。对于这个任务，我推荐你使用 EBImage 包，这样你就可以灰度化和调整图像大小。为了帮助神经网络的训练时间，我们将调整图像的大小，使它们更小，因此神经网络接受的数据更少。但是让我们一步一步地完成我们的预处理:

```
#Loading required packages
require(mxnet)
require(EBImage)
require(jpeg)
require(pROC)

#Downloading the strings of the image files in each directory
guitar_photos <- list.files("/file/path/to/image")
laptop_photos <- list.files("/file/path/to/image")

```

加州理工学院图书馆被组织成多个层次的目录，所以当试图以自动化的方式访问这些图像时要小心。每个类别的所有目录都具有相同的文件名格式:图像文件表示为 image_000，X，其中 X 是目录中图像的编号。但是每个目录都有不同数量的文件，所以我们应该使用`list.files()`函数来收集目录中所有图像文件的名称。我们在下面的代码中使用它们。使用`list.files()`功能时吉他照片目录的内容以截断的形式显示在图 [11-9](#Fig9) 中。

![A435493_1_En_11_Fig9_HTML.jpg](../Images/A435493_1_En_11_Fig9_HTML.jpg)

图 11-9。

List of files from image directory

现在我们有了各个文件的名称，我们可以使用以下过程将它们加载到`img_data`数据框中:

```
#Creating Empty Data Frame
img_data <- data.frame()

#Turning Photos into Bitmaps
#Guitar Bitmaps
for (i in 1:length(bass_photos)){
  img <- readJPEG(paste("/path/to/image/directory/", guitar_photos[i], sep = ""))

```

我们在这里使用`paste`函数将目录和带有字符串的图像结合起来，这样它就可以引导我们找到数据。使用 jpeg 包中的`readJPEG()`函数，我们可以将图像读入位图，就像前面描述的矩阵堆栈一样。每个维度代表构成每张彩色照片的三种颜色(红色、蓝色和绿色)。但是为了降低我们正在处理的图像的复杂性，我们将把这些图像转换成灰度(黑白)。当处理黑白图像时，我们给像素值分配一个 0 到 1 之间的数字，0 代表黑色，1 代表白色。中间的颜色决定了特定颜色向光谱任一侧的强度:

```
#Reshape to 64x64 pixel size and grayscale image
img <- Image(img, dim = c(64, 64), color = "grayscale")

#Resizing Image to 28x28 Pixel Size
img <- resize(img, w = 28, h = 28)
img <- img@.Data

```

我们使用 EBImage 中提供的`resize()`函数对各种图像进行整形和调整大小。如果你对查看图像灰度化后的样子感兴趣，可以随意使用`display()`和`Image()`函数。调整图像大小后，我们将位图转换成矢量，以便更好地存储。最后，在创建和训练模型时，我们必须向数据向量添加一个标签。这在计算我们模型的准确性时会很有用。具体来说，吉他将被标记为 1，笔记本电脑将被标记为 2:

```
  #Transforming to vector
  img <- as.vector(t(img))

  #Adding Label
  label <- 1

  img <- c(label, img)

  #Appending to List
 img_data <- rbind(img_data, img)

}

```

我们对笔记本电脑图像重复这一过程。如果您想使用这种预处理和模型评估的结构，请随意，或者尝试其他预处理方法。在创建 CNN 模型之前，我们必须确保模型的输入格式是正确的。MXNet 和许多神经网络模型都有您应该熟悉的特定格式。第一步是创建一个训练和测试集。在本例中，我们将拆分数据集，这样我们可以针对 75%的数据进行训练，针对剩余的 25%进行测试。我们现在将转换数据，使其成为一个矩阵，其中每一行都是不同的图像观察，标签作为第一列条目，位图值作为连续的列条目。然后，我们将从 X 矩阵中剥离标签，并将其用作 y 向量相应观察顺序中的值。然后，我们使用`sample()`函数执行交叉验证:

```
#Transforming data into matrix for input into CNN
training_set <- data.matrix(img_data)

#Cross Validating Results
rows <- sample(1:nrow(training_set), nrow(training_set)*.75)

#Training Set
x_train <- t(training_set[rows, -1])
y_train <- training_set[rows, 1]
dim(x_train) <- c(28,28, 1, ncol(x_train))

```

在前面的代码中，指出一个明显的细节是很重要的，如果忽略这个细节，您将无法执行代码。MXNet CNN 模型只取一个 4 维的 X 矩阵。请务必记住这一点，否则您将浪费时间来调试这个问题！我们还相应地改变了测试集的维度:

```
#Test Set
x_test <- t(training_set[-rows, -1])
y_test <- training_set[-rows, 1];
dim(x_test) <- c(28,28, 1, ncol(x_test))

```

既然我们已经完成了数据的预处理，我们终于可以开始构建和训练我们的模型了。

## 模型构建和培训

CNN 模型的构建方式是数据通过每一层，但实际输入到`FeedForward()`函数的唯一一层是最后一层。因此，我们在这里激活模型之前构建它。有些包可能更专有，需要更少的架构，但 MXNet 允许很大程度的定制，如果你想构建不同的 ConvNet 结构，这将是有用的，如第 [5 章](05.html)中详述的那些。如果你想提高这里的结果，这可能是一个很好的利用你的时间。

让我们来看看建筑。这里我们将使用通用的 LeNet 架构，这是图像识别任务的标准。因此，我们以同样的方式组织这些层:

```
data <- mx.symbol.Variable('data')

#Layer 1
convolution_l1 <- mx.symbol.Convolution(data = data, kernel = c(5,5), num_filter = 20)
tanh_l1 <- mx.symbol.Activation(data = convolution_l1, act_type = "tanh")
pooling_l1 <- mx.symbol.Pooling(data = tanh_l1, pool_type = "max", kernel = c(2,2), stride = c(2,2))

#Layer 2
convolution_l2 <- mx.symbol.Convolution(data = pooling_l1, kernel = c(5,5), num_filter = 20)
tanh_l2 <- mx.symbol.Activation(data = convolution_l2, act_type = "tanh")
pooling_l2 <- mx.symbol.Pooling(data = tanh_l2, pool_type = "max", kernel = c(2,2), stride = c(2,2))

```

我们首先创建一个虚拟的`data`变量，该变量将用于以对 ConvNet 友好的文件格式传递 x 矩阵值。`data`经过每一层，如第 [5](05.html) 章所述，模型从数据的较低抽象到较高抽象进行构建，以做出决定。在这里，我们将使用一般建议的步幅 2，20 个过滤器在第一 Conv 层，50 个过滤器在第二 Conv 层。作为激活函数，我们使用 tanh。该激活函数将在整个模型中保持不变，但输出函数除外:

```
#Fully Connected 1
fl <- mx.symbol.Flatten(data = pooling_l2)
full_conn1 <- mx.symbol.FullyConnected(data = fl, num_hidden = 500)
tanh_l3 <- mx.symbol.Activation(data = full_conn1, act_type = "tanh")

#Fully Connected 2
full_conn2 <- mx.symbol.FullyConnected(data = tanh_l3, num_hidden = 40)

#Softmax Classification Layer
CNN <- mx.symbol.SoftmaxOutput(data = full_conn2)

```

数据继续传递到完全连接的层。在完全连接的层中分别有 500 和 40 个隐藏神经元。最后，数据到达最后一层，在这里我们有一个 softmax 分类器来确定观察值的类别。

但是，在我们做出任何预测之前，我们必须使用上一节中建议的方法来训练我们的参数。如果可能，特别是在神经网络的情况下，强烈建议对支持这些功能的包使用本地搜索方法。具体来说，h2o 支持网格搜索功能来调整参数。虽然我们在这里使用 MXNet，但是让读者了解提供这些功能的包是很有用的。

让我们从训练参数开始:

```
#Learning Rate Parameter
AUC <- c()
learn_rate <- c(0.01, 0.02, 0.03, 0.04)
CPU <- mx.cpu()

for (i in 1:length(learn_rate)){
  cnn_model <- mx.model.FeedForward.create(CNN, X = x_train, y = y_train, ctx = CPU, num.round = 50, array.batch.size = 40,
learning.rate = learn_rate[i],
momentum = 0.9, eval.metric = mx.metric.accuracy,
epoch.end.callback = mx.callback.log.train.metric(100),
 optimizer = "sgd")
#Code redated partially, please check github!

```

与其他神经网络模型类似，学习率参数决定了更新连接各层的权重时梯度的大小。我们给出了一个数组，并根据图 [11-10](#Fig10) 中的调整参数绘制了 AUC。

![A435493_1_En_11_Fig10_HTML.jpg](../Images/A435493_1_En_11_Fig10_HTML.jpg)

图 11-10。

AUC score over learning rate

我们可以清楚地看到，这里 0.04 的学习率是最优的，因为它产生最高的 AUC 分数。

现在让我们训练动量参数:

```
AUC1 <- c()
mom <- c(0.5, 0.9, 1.5)
for (i in 1:length(mom)){
cnn_model <- mx.model.FeedForward.create(CNN, X = x_train, y = y_train, ctx = CPU, num.round = 50, array.batch.size = 40, learning.rate = 0.04,
momentum = mom[i], eval.metric = mx.metric.accuracy,
epoch.end.callback = mx.callback.log.train.metric(100), optimizer = "sgd")
#Code redacted partially, please check github!

```

当我们执行前面的代码时，我们会收到如图 [11-11](#Fig11) 所示的结果。

![A435493_1_En_11_Fig11_HTML.jpg](../Images/A435493_1_En_11_Fig11_HTML.jpg)

图 11-11。

AUC over momentum value

当评估不同参数的结果时，我们将动量值设置为 0.9。现在我们已经调优了这两个参数，我们可以在最后一部分开始训练调优的模型，并在测试和训练集上评估它的性能:

```
#Fitted Model Training
cnn_model <- mx.model.FeedForward.create(CNN, X = x_train, y = y_train, ctx = CPU, num.round = 150, array.batch.size = 40,
learning.rate = 0.04, momentum = 0.9, eval.metric = mx.metric.accuracy,
initializer = mx.init.normal(0.01) , optimizer = "sgd")

#Calculating Training Set Accuracy
y_h <- predict(cnn_model, x_train)
Labels <- max.col(t(y_h)) - 1
roc(as.factor(y_train), as.numeric(Labels))
curve <- roc(as.factor(y_train), as.numeric(Labels))
#Code partially redacted, please check github!

```

在执行代码之前，我想指出一个细节。这里，我们没有启用 GPU 训练。如果您想减少训练时间并提高计算性能，请查看 MXNet 文档中启用此功能的必要步骤。在这个例子中，我们将使用 CPU 培训。您还应该意识到，增加`num.round`参数的诱惑通常会很强烈，因为这将直接影响模型对训练集数据的准确性。请注意，将该参数设置得太高会导致过度拟合，特别是对于我们在本例中使用的数据集。当执行上述代码时，用户应该看到终端以如下格式打印出训练精度:

```
[184] Train-accuracy=0.708333333333333
[185] Train-accuracy=0.708333333333333
[186] Train-accuracy=0.708333333333333
[187] Train-accuracy=0.708333333333333
[188] Train-accuracy=0.708333333333333

```

单词`Train-accuracy`左侧的数字代表当前迭代，该迭代将运行到`num.round`参数中指示的数字。这里使用的`accuracy`参数相当于 AUC 分数，由`mx.metric.accuracy`对象给出。像往常一样，学习率很难近似，但我们可以通过使用随机梯度下降优化器调整神经网络中的权重来减轻准确性的损失。执行代码时，我们得到图 [11-12](#Fig12) 。

![A435493_1_En_11_Fig12_HTML.jpg](../Images/A435493_1_En_11_Fig12_HTML.jpg)

图 11-12。

ROC plot for CNN over training data

该 ROC 图的 AUC 为 0.7706。当评估测试数据的性能时，产生图 [11-13](#Fig13) 和结果。

![A435493_1_En_11_Fig13_HTML.jpg](../Images/A435493_1_En_11_Fig13_HTML.jpg)

图 11-13。

ROC plot for CNN over test data

当对照测试数据进行预测时，模型的 AUC 为 0.7063。粗略地说，这里的性能相当相似，尽管正如我们所料，我们确实注意到从训练集到测试集的性能有所下降。也就是说，即使在这种情况下，也不太可能有任何过度拟合的迹象。然而，如果您想将这样的东西投入生产，您可能仍然倾向于改进这些模型的性能。理想情况下，在对图像进行分类时，我们希望模型的准确率至少达到 90%。虽然这里的图像分类情况是相当良性的，但是存在不正确的分类会在每次观察中损失大量金钱或者导致不正确的诊断从而导致患者接受不适当的护理的情况。考虑到这一点，你将如何从这一点着手？

最合理的下一步是为这个模型的训练阶段获取更多的数据。这通常是我们认为建立足够的卷积神经网络的最大挑战:获得足够的训练数据。对于许多不同的商业产品，合法地获取这些数据可能是一项非常艰巨的任务，在最坏的情况下，需要团队自己在现实世界中获取数据。在为特定任务创建 CNN 时，读者应该注意这一点，因为有时任务的可行性纯粹是数据可访问性的问题。在这种情况下，我们使用的数据集大约只有总共 170 张照片，其中 75%以上是我们训练的。

另一个你可能想注意的建议是使用另一个网络架构，或者如果你有足够的野心，尝试创建自己的网络架构。然而，创建自己的网络架构可能是一项极其艰巨的任务。另一个可能的探索途径是创建几个卷积神经网络。从这些模型中，我们可以创建一个数据集，其中每个特征都是给定 CNN 的输出。这个数据集然后可以被输入到传统的机器学习模型中。但是，您应该意识到，这些方法本身可能需要对前面概述的方法进行重大调整。

### 协同过滤

对于我们的最后一个例子，我们将简要地处理推荐系统的问题，就像在前面的章节中简要地处理的那样。推荐系统是不断发展的，但是由于数据科学在其中的应用，提出这个概念是有用的。在这里，您将了解插补的实际应用，以及数据科学的一些软技能，如数据转换，这些都已简要介绍过，但从未介绍过。

推荐系统是 Amazon.com 等电子商务网站特有的，但也存在于网飞等基于内容的网站。动机相当简单，因为向客户推荐他们合理喜欢的产品是合理的。然而，这样做的任务比看起来更困难。大多数用户不会使用某个公司提供的所有产品。即使他们这样做了，也不意味着他们会对他们使用的每一种产品进行评级。这就给我们留下了一个矩阵中数值稀少的问题。然而，我们已经回顾了处理这个问题的技术，并将继续检查我们的数据集。

对于这个实验，我们将使用第三个 Jester 数据集( [`http://goldberg.berkeley.edu/jester-data/`](http://goldberg.berkeley.edu/jester-data/) )。特征都代表个人笑话，行代表用户。矩阵中的每个条目都是一个笑话的等级，其中下限是–10，上限是 10。然而，每当没有一个笑话的条目时，就用 99 来表示。当检查数据集的头部时，我们看到如图 [11-14](#Fig14) 所示的矩阵。

![A435493_1_En_11_Fig14_HTML.jpg](../Images/A435493_1_En_11_Fig14_HTML.jpg)

图 11-14。

Snapshot of the Jester data set

这里的目标是根据笑话本身的相似性来衡量不同用户口味的相似性。为此，我们将计算列向量之间的余弦相似度。简而言之，在讨论结合矩阵分解和 RBMs 来估算缺失值之前，让我们先讨论余弦相似性的概念。当处理试图比较向量的问题时，余弦相似性是一个经常被引用的概念。直观上，我们将余弦相似度定义为两个非零向量不同的程度。数学上，我们用下面的等式

![$$ \mathrm{similarity}= \cos \left(\theta \right)=\frac{A* B}{{\left|\left| A\right|\right|}_2{\left|\left| B\right|\right|}_2} $$](../Images/A435493_1_En_11_Chapter_Equa.gif)

定义余弦相似度，其中 A，B =两个不同的向量。

与相关系数类似，余弦相似值的范围从–1 到 1。余弦相似度为 1 表示值完全相同，而–1 表示值完全相反。零值表示向量之间完全没有关系。考虑到这一点，我们将比较某些音乐的消费模式，这样我们就可以比较哪些项目彼此最相似，因此应该推荐给其他人。

然而，对于那些密切关注的人来说，余弦相似性与两个非零向量一起使用，这意味着我们必须为我们的数据集生成缺失值。已经讨论了许多插补技术，但 Geoffrey Hinton 认为在这种情况下有用的一种技术是矩阵分解。具体来说，我建议你用奇异值分解(SVD)。

本书其他地方讨论的 SVD 和 PCA 是高度相关的技术。它们都是执行矩阵特征分解，但是 SVDs 应用不同于 PCA 的应用。特别地，奇异值分解可以用来逼近缺失值。因此，让我们使用`impute.svd()`函数估算我们的值:

```
require(lsa)
require(bcv)
require(gdata)
require(Matrix)

#Upload the data set
#Please be patient this may take a handful of seconds to load.
data <- read.xls("/path/to/data/.xls", sheet = 1)
colnames(data) <- seq(1, ncol(data), 1)

#Converting 99s to NA Values (1)
data[data == 99] <- NA

#Converting 99s to Mean Column Values (2)
for (i in 1:ncol(data)){
  data[is.na(data[,i]), i] <- mean(data[,i], na.rm = TRUE)
}

```

我们首先将 99(1)转换为 NA 值，然后将 NA 值更改为列 means (2)。在这一点上，我们可以前进，估算的价值:

```
#Imputing Data via SVD
newData <- impute.svd(data, k = qr(data)$rank, tol = 1e-4, maxiter = 200)
print(newData$rss)
head(data[, 2:10])

```

请注意，`impute.svd()`函数要求您估算缺失值的任一列平均值，或者如果一整列的观察值缺失，则使其为 0。如果你不遵循这些指示，你会收到不正确的结果。当执行前面的代码时，我们产生如图 [11-15](#Fig15) 所示的输出。

![A435493_1_En_11_Fig15_HTML.jpg](../Images/A435493_1_En_11_Fig15_HTML.jpg)

图 11-15。

Head of imputed data set

当执行 SVD 时，我们还计算了关于非缺失值和这些非缺失值的预测的平方和为 4.398197e-20。想要挑战自我的读者可以不使用 SVD 估算值，而是使用 RBM。但是，请注意，这项任务的计算量非常大，并且针对这项任务修改 RBM 并不容易。寻找 Geoffrey Hinton 给出的关于这个主题的高层次概述( [`http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf`](http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf) )。

我们现在可以计算列之间的余弦距离:

```
itemData <- matrix(NA, nrow = ncol(data), ncol = 11,
                   dimnames=list(colnames(data)))
#Getting Cosine Distances
for (i in 1:nrow(itemData)){
  for (j in 1:ncol(itemData)){
    itemData[i,j] <- cosine(data[,i], data[,j])
  }
}

```

当执行前面的代码时，我们产生如图 [11-16](#Fig16) 所示的数据集。

![A435493_1_En_11_Fig16_HTML.jpg](../Images/A435493_1_En_11_Fig16_HTML.jpg)

图 11-16。

Head of the cosine distance data set

从这个数据集，我们现在可以执行最终的数据转换，这样每行代表一个特定的笑话，每列代表从左到右降序排列的最相似的笑话。我们首先通过实例化一个具有适当维度(1)的空矩阵来做到这一点。实例化该矩阵后，我们可以通过对余弦值进行排序并获取包含前 11 个值的索引来填充数据——我们获取前 11 个值是因为数字 1 值本身总是相同的项目:

```
#Creating Matrix for ranking similarities (1)
similarMat <- matrix(NA, nrow = ncol(itemData), ncol = 11)

#Sorting Data Within Item Data Matrix (2)
for(i in 1:ncol(itemData)) {
  rows <- order(itemData[,i], decreasing = TRUE)
  similarMat[i,] <- (t(head(n=11, rownames(data[rows ,][i]))))
}

#Printing Result
similarMat

```

当执行前面的代码时，我们得到我们的最终答案，如图 [11-17](#Fig17) 所示。

![A435493_1_En_11_Fig17_HTML.jpg](../Images/A435493_1_En_11_Fig17_HTML.jpg)

图 11-17。

Top 10 recommendations for 11 separate jokes

我们将结果解释为对 11 个不同的笑话产生了前 10 个推荐。您可以在一个平台中实现这一点，这样，在一个网页上，用户可以收到不同页面、产品或类似实体的推荐。

## 摘要

我们现在已经到了本章的结尾，以及我们对深度学习和机器学习技术的全面回顾。第 [12](12.html) 章提供了所有数据科学家在他们的研究或专业努力中前进时应该知道的简要建议。