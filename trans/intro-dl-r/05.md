# 5.卷积神经网络

类似于第 [4](04.html) 章中关于多层感知器问题的概念，卷积神经网络(CNN)也有多层，用于计算给定数据集的输出。这个模型的发展可以追溯到 20 世纪 50 年代，当时研究人员 Hubel 和 Wiesel 对动物视觉皮层进行了建模。在 1968 年的一篇论文中，他们详细讨论了他们的发现，这些发现在他们研究的猴子和猫的大脑中识别出了简单细胞和复杂细胞。他们观察到，相对于观察到的直边，简单细胞具有最大化的输出。相反，观察到复杂细胞中的感受野相当大，并且它们的输出相对不受上述感受野内边缘位置的影响。除了图像识别之外，CNN 最初获得并仍然保持其恶名，CNN 还有相当多的其他应用，例如在自然语言处理和强化学习领域。

## CNN 的结构和性质

广义地说，CNN 是多层神经网络模型。与 Hubel 和 Weisel 描述的动物视觉皮层的结构一致，该模型可以被视觉地解释，如图 [5-1](#Fig1) 所示。

![A435493_1_En_5_Fig1_HTML.jpg](img/A435493_1_En_5_Fig1_HTML.jpg)

图 5-1。

Broad visual display of a CNN

每个区块代表 CNN 的一个不同层，我将在本章后面详细解释。从左到右是输入层、隐藏层(卷积层、汇集层和分离层)和全连接层。在最后一层之后，模型输出一个分类。现在考虑图 [5-2](#Fig2) 。

![A435493_1_En_5_Fig2_HTML.jpg](img/A435493_1_En_5_Fig2_HTML.jpg)

图 5-2。

CNN architecture diagram

完全连接的层加强了神经元和相邻层之间的局部连接，如图 [5-2](#Fig2) 所示。因此，隐藏层的输入是来自该隐藏层之前的层的神经元的子集。这确保了学习的子集神经元产生最佳可能的响应。此外，单元在特征/激活图中共享相同的权重和偏差，使得给定层中的所有神经元都在分析/检测完全相同的特征。

至于 CNN 上下文中的特征，我指的是图像中不同的部分。这就是我们的过滤器与它正在分析的图像部分进行比较的内容，因此它可以确定正在扫描的图像部分与正在分析的特征的相似程度。假设我们有足够的训练数据和足够多的图像类别，这些特征足够明显，有助于区分不同的类别。

想象我们正在看两幅图像，具体来说是 X 和 O，如图 [5-3](#Fig3) 所示。

![A435493_1_En_5_Fig3_HTML.jpg](img/A435493_1_En_5_Fig3_HTML.jpg)

图 5-3。

O and X example photo

如果我们把 X 和 O 都想象成不同的图像，那么对于人眼来说，我们可以确定它们是明显不同的字母。它们的区别因素包括 O 的中心是空的，而 X 的中心有两条相交的线。图 [5-4](#Fig4) 和 [5-5](#Fig5) 显示了这些数值的特征图示例。

![A435493_1_En_5_Fig5_HTML.jpg](img/A435493_1_En_5_Fig5_HTML.jpg)

图 5-5。

Feature map of “O”

![A435493_1_En_5_Fig4_HTML.jpg](img/A435493_1_En_5_Fig4_HTML.jpg)

图 5-4。

Feature map of “X”

这些值通常表示为矩阵中的一个条目，黑色和白色分别为–1 和 1。当处理彩色图像时，每个像素通常被表示为矩阵中的一个条目，对于黑色和白色分别具有值 1 或 256。但是，根据所使用的语言，零索引可能会影响 RGB 值的表示，从而使边界向后移动 1。

## CNN 架构的组件

本节涵盖 CNN 架构的组件。

### 卷积层

这一层是任何给定 CNN 中大多数计算发生的地方，因此是图像通过输入后的第一层。在卷积层中，我们用滤波器扫描图像的一部分。就高度和宽度而言，每个过滤器都不是特别大，但是它们都延伸通过该层的整个长度。

例如，假设我们试图将一幅图像分类为 1 或 0，而实际上这幅图像是 1。想象一下，图像有一个黑色背景，但数字是用白色像素勾勒出来的。图 [5-6](#Fig6) 显示了该图像的一个示例。

![A435493_1_En_5_Fig6_HTML.jpg](img/A435493_1_En_5_Fig6_HTML.jpg)

图 5-6。

Example image of “1”

计算机会将值为 1 的白色像素和值为-1 的黑色像素区分开来。当我们通过卷积层输入该图像时，该模型提取图像的独特特征，这些特征通常是最终定义特定图像的颜色、形状和边缘。一旦我们有了给定训练图像的特征，我们就对输入的图像进行所谓的过滤。过滤是获取图像特征的过程，在这种情况下，我们可以将其想象为一个 3 x 3 像素的正方形，并将其与输入图像的一部分匹配，该部分也是一个 3 x 3 像素的正方形。在图 [5-7](#Fig7) 中，我们可以看到过滤的过程是什么样子。

![A435493_1_En_5_Fig7_HTML.jpg](img/A435493_1_En_5_Fig7_HTML.jpg)

图 5-7。

Example of a filter

然后，我们将特征的像素数量乘以图像块的相应像素数量。在示例中，我们应该为每个操作获得 1 或–1 的输出。直观上，当像素完全匹配时，它们应该输出为 1，当不匹配时，它们应该输出为–1。最后，我们取像素乘积的平均值。如果图像完全匹配，则平均值应为 1。如果没有，那就比 1 低了相当大的度数。在这种情况下，想象图像补丁和选择的特征根本不匹配。因此，当我们取平均值时，它应该输出到-1。我们将该产品放在我们正在分析的图像片位置的中心，在所谓的特征图上具有给定的特征。这最终将是卷积层的输出，并将用于下一层。卷积层将通过不同的迭代产生多个特征图。在每个可能的位置将特征与给定的图像块进行匹配的过程称为图像卷积。我们将给定 CNN 的特征/激活图表示为

![$$ {h}_{i, j}^k= \tanh \left({\left({w}^k x\right)}_{i, j}+{b}_k\right) $$](img/A435493_1_En_5_Chapter_Equa.gif)

其中 w <sup>k</sup> 是权重，b <sub>k</sub> 是偏差，x 是特定像素的值，tanh 是数据中的非线性。下标 I、j 指的是代表特征/激活图的矩阵的条目。权重 w <sup>k</sup> 最终将特征图中的像素连接到前一层。最终，卷积图层是由前面描述的操作生成的要素地图的堆叠。然后，我们将要素地图放入池层。我们计算输出体积的空间大小为

![$$ Spatial\ S i z{e}_{Output}=\frac{W- F+2 P}{S+1} $$](img/A435493_1_En_5_Chapter_Equb.gif)

其中 W =输入音量大小，F =卷积层中感受野的大小，P =零填充量，S =步幅。

### 汇集层

在连续的卷积层之间，通常放置一个所谓的池层。简而言之，汇集层采用卷积层中生成的特征地图，并将它们“汇集”到一个图像中。汇集层有效地执行维度缩减，因此优先强调空间表示，从而降低模型的复杂性。这可以与决策树中的修剪过程相比较，同样有助于防止给定模型的过度拟合。在原型 CNN 模型中，池层有一个 2 x 2 的过滤器，步幅为 2，输入中的每个深度切片都被向下采样，因此我们相对于高度和宽度移动了 2 个像素。池层中的这些操作有助于丢弃 75%的功能/激活映射。这一层使用最大值运算，在前面提到的例子中，最大值超过 4 个数字，或任何给定特征/激活图中的 4 个像素。

与之前描述的示例保持一致，假设使用 2 x 2 过滤器，我们正在查看 9 x 9 特征地图，其中我们使用以下分数分析左上角:

<colgroup><col> <col></colgroup> 
| .88 | Zero |
| Zero | .95 |

使用最大值运算时，我们会选择 0.95，因为这是 2 x 2 窗口内的最大值。因为我们的步幅为 2，所以我们向右移动了 2 个像素，这意味着我们看到的是一个 2 x 2 的图像切片，其中该切片的左上角应该是特征图的第三列，直到我们得到一个最大合并图像，这大大减少了模型的不必要的复杂性。作为该层中使用的最大值运算的直接结果，在分析图像时，我们不需要像前一层那样精确，因此这有助于创建一个更健壮的模型，可以更容易地对输入进行分类。我这样说的具体意思是，连接每一层的权重值可以更一般化到它们所接触到的所有训练数据，而不是以 CNN 在样本外表现不佳的方式过度拟合。

决定输出空间大小的函数由

![$$ Spatial\ S i z{e}_{Output}={W}_2\ x\ {H}_2\ x\ {L}_2, $$](img/A435493_1_En_5_Chapter_Equc.gif)

给出

其中

![$$ {W}_2=\frac{w_1- F}{s+1}, \kern0.5em {H}_2=\frac{H_1- F}{S+1},\kern0.5em {L}_2={L}_1 $$](img/A435493_1_En_5_Chapter_Equd.gif)

### 校正线性单位(ReLU)图层

整流器是激活功能的另一个术语。通常，我们将以下函数应用于该层的输入

![$$ f(x)= \max \left(0, x\right) $$](img/A435493_1_En_5_Chapter_Eque.gif)

其中 x =神经元的输入。

当应用于特征图时，我们可以想象特征图中的任何负值现在都为零。具体来说，这有助于勾勒出更接近最相关图像的特征地图。我们对所有的特征地图都这样做，然后得到图像的“堆栈”。

### 全连接(FC)层

这一层中的任何神经元都连接到前一层中的所有激活图。这一层通常位于用户确定数量的卷积层、池层和 ReLU 层之后。由于先前操作中指定的图像缩小，输入到该层的图像将明显小于原始输入。在这一层，我们扫描简化的图像，这些图像应该对应于每个特征图，并将这里给出的每个值转换为值列表。这个列表对应于我们放入的 k 个图像中的一个。根据本章开头的例子，我们最初输入 1。在移动通过所有层之后，我们取对应于这个图像的分数的平均值，然后这是图像为 1 或 0 的概率。应当注意，该层与卷积层之间的唯一区别在于，卷积层仅连接到输入中的局部区域，并且卷积层体积中的许多神经元共享参数。考虑到这一点，在构建给定架构时，我们还可以在 FC 层和卷积层之间进行转换。

### 损失层

在这一层，我们将预测的标签与图像的实际标签进行比较。当试图从 k 个可能的特征级别中对对象进行分类时，我们将使用 softmax 损失分类器。为了对特定图像的标签进行回归，使用欧几里德函数也是常见的。它们的功能如下:

1.  Softmax loss function :

    ![$$ \sigma {(z)}_j=\frac{e^{z_j}}{{\displaystyle {\sum}_{k=1}^K}{e}^{z_k}\kern0.75em } $$](img/A435493_1_En_5_Chapter_Equf.gif)

2.  Euclidean loss function :

    ![$$ E=\frac{1}{2 N}{\displaystyle \sum_{i=1}^N}{\left|\left|{\hat{y}}_i-{y}_i\right|\right|}_2^2 $$](img/A435493_1_En_5_Chapter_Equg.gif)

3.  Softmax normalization :

    ![$$ {x}_i^{\hbox{'}}=\frac{1}{1+{e}^{-\left(\frac{x_i-{\mu}_i}{\sigma_I}\ \right)}} $$](img/A435493_1_En_5_Chapter_Equh.gif)

当使用反向传播算法时，我们制作一个混淆矩阵来比较 1 或 0，其中我们将答案的标签减去分配的概率。按照我们一直使用的例子，假设 1 = 1，0 = 0，但当我们输入 1 时，我们只收到 0 的概率为 0 . 85，1 的概率为 0 . 45。因此，我们会有–. 60 的累积误差。然后，我们通过在完全连接的层中显示的权重，使用如前所述的梯度方法，以指定的学习速率来调整每个特征映射像素。我们将权重初始化为 0，并在达到损失容限或达到最大迭代阈值的点停止 CNN。必须考虑前面章节中描述的关于收敛到最优解的相同考虑。

## 调谐参数

发送到输入层的图像应该多次被 2 整除。常见的图像尺寸为 32 x 32、64 x 62 等。卷积层的过滤器尺寸应该最多为 3 x 3 或 5 x 5，并且应该以不改变输入的空间尺寸的方式执行零填充。对于池层，它们的尺寸应该是 2 x 2，通常步长为 2。使用这些参数，75%的激活将被丢弃。大于 3 的池层会导致分类过程中的过多损失。当描述神经元及其排列时，超参数与这一对话最相关。具体来说，我将提到步幅、深度和零填充。在 CNN 最重要的参数中，步幅是一个固定参数，它决定了滑过过滤器的像素数量。例如，如果步幅为 2，则每次有 2 个像素滑过过滤器。通常，步幅不大于 2，也不小于 1。零填充是输入体积边界周围零的大小。通过控制零填充，我们可以从一层到另一层更仔细地控制激活图和其他输出的大小。最后，深度是指我们为给定实验选择的滤波器数量，每个滤波器都是最终在卷积层中搜索每个图像的内容。

## 著名的 CNN 架构

*   LeNet:由著名的深度学习研究人员 Yann LeCun 在 20 世纪 90 年代开发，LeNet 是一个相对简单的架构，从各方面考虑。该模型最初的目的是对数字进行分类，读取邮政编码，并执行一般的简单图像分类。这被认为类似于任何开发者首先用给定语言编写的“Hello World”程序，因为它被认为是第一个成功应用于实际任务的 CNN 程序。如图 [5-8](#Fig8) 所示，涉及的层次如下:
    *   input, conv layer, ReLU, pooling layer, conv layer, ReLU, pooling layer, fully connected, ReLu, fully connected, and softmax classifier.

        ![A435493_1_En_5_Fig8_HTML.jpg](img/A435493_1_En_5_Fig8_HTML.jpg)

        图 5-8。

        Visualization of LeNet
*   GoogLeNet (Inception):这种架构在 2014 年赢得了 ImageNet 大规模视觉识别挑战(ILSVRC)比赛，以此向 Yann LeCun 的 LeNet 致敬。它是由克里斯蒂安·塞格迪、、杨青·贾、皮埃尔·塞尔马内、斯科特·里德、德拉戈米尔·安盖洛夫、杜米特鲁·埃赫兰、钦文·万霍克和安德鲁·拉宾诺维奇开发的。GoogLeNet 的名字来源于这样一个事实，即该架构的相当多的开发人员在 Google Inc .工作。在他们的论文“用卷积加深”中，他们描述了一种允许“在保持计算预算约束的同时增加网络的深度和宽度”的架构。如图 [5-9](#Fig9) 所示，建议的结构如下:初始架构的重点是通过前面描述的层中的方向，CNN 模型允许“增加每个阶段的单元数量”,而不会使模型变得太复杂。总的来说，该模型旨在处理各种规模的视觉信息，然后将计算汇总到下一阶段，以便同时分析更高层次的抽象。
    *   Input, conv. layer, max pool, conv layer, pooling layer (with max function), inception (2 layers), max pool, inception, inception (5 layers), max pool, inception (2 layers), average pooling layer, dropout, ReLU, softmax classifier.

        ![A435493_1_En_5_Fig9_HTML.jpg](img/A435493_1_En_5_Fig9_HTML.jpg)

        图 5-9。

        GoogLeNet architectureThe focus of the inception architecture is that through the orientation in the layers as described earlier, the CNN model allows for “increasing the number of units at each stage” without doing so to the point where the model becomes too complex. Overall, the model seeks to process visual information at various scales and then aggregate the calculations to the next stage so higher levels of abstraction are analyzed simultaneously.
*   AlexNet: Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton , this won the ILSVRC in 2012\. Similar in architecture to LeNet, AlexNet uses “non-saturating neurons” and efficiently implements the GPU for the convolution layers. The neurons in fully connected layers are connected to all neurons in the previous layer, response-normalization layers follow the first and second convolutional layers, and the kernel of layers two, four, and five are connected only to the kernel maps in the previous layer, which would be on the same GPU. The architecture is as follows (and shown in Figure [5-10](#Fig10)):

    ![A435493_1_En_5_Fig10_HTML.jpg](img/A435493_1_En_5_Fig10_HTML.jpg)

    图 5-10。

    AlexNet architecture
    *   卷积(5 层)，全连接(3 层)，[输出为 1000 路 softmax 分类器]
*   VGGNet:它在 ILSVRC 2014 竞赛中获得了 AlexNet 的第二名。VGGNet 是由牛津大学的卡伦·西蒙扬和安德鲁·齐泽曼开发的。感受野是 3×3，有 1×1 个过滤器，步幅是 2，最大集合大小是 2×2。该架构是这样的，输入通过几个卷积层馈送到三个完全连接的层(第一层和第二层具有 4096 个通道，最后一层是执行 1000 路分类的 softmax 层)。
*   ResNet:2015 年 ILSVRC 的第一名得主，ResNet 拥有 152 层——远远超过了前面提到的网络的数量。它是由微软研究院的何开民、、任和开发的。这种架构的目的是形成一个学习参考层输入的剩余函数的网络，而不是学习未参考函数的网络。最终结果是，网络更容易学习，更容易优化，并通过增加深度来提高精度，而不是通过增加深度来降低精度。

## 正规化

当多层感知器有不止一层时，它们被认为具有逼近给定目标的能力，这将导致过度拟合。为了防止过度拟合，通常建议对输入数据进行正则化，然而，在 CNN 的情况下，这是一个稍微不同的过程，我们可以使用:1) DropOut，这是从人脑中观察到的现象的灵感中获得的。这是一个给定的隐藏层不被通过的概率，这个概率我们设置为一个超参数。2)随机池，其中随机选择激活。可以说，随机池不需要超参数，可以作为一种启发式方法，与其他正则化技术一起使用。3) DropConnect，它是 dropout 的推广，其中每个连接都可以以 1–p 的概率被丢弃。该层中的每个单元都从前一层中的随机单元输入数据，这些数据在每次迭代时都会发生变化。这有助于确保重量不会过重。4)权重衰减，其功能类似于 L1/L2 正则化，其中我们严重惩罚大的权重向量。

在这些方法中，CNN 对使用 DropOut 有相当大的热情，因为它被证明是一种有效和强大的技术。除了防止过度拟合之外，已经观察到丢弃提高了具有大量参数的网络的计算效率，因为这种形式的正则化实际上导致网络在给定迭代期间变得更小。在所有这些迭代之后，较小网络的性能可以被平均为完整网络的性能的一般预测。第二，可以观察到，丢弃层在网络中引入了随机化的性能，这使得数据内的噪声被平均，使得其对数据内的信号的屏蔽被减小。

使用 L1 正则化也并不少见，但要注意的是，这种情况下的权重向量通常会缩小到 0，有时缩小到足够小，这样我们就可以得到稀疏填充的权重矩阵。这种类型的正则化的负面影响是，由于层之间的“死”连接，包含重要信息的某些层的输入可能变得完全不被注意。相比之下，当你特别想要非常明确的特征选择时，L1 正则化可能会产生明显更好的性能。

传统上，L2 正则化被视为在 CNN 中执行正则化的标准方法，因为它倾向于惩罚异常大的权重，而支持那些相对于整个矩阵而言比例一般较低的权重。与 L1 正则化相比，您会得到一个更加丰富的权重矩阵，这将导致网络从一个给定的层向下一层提供更多的数据。因此，特征选择将比使用 L1 正则化时更不明显。

你应该知道的最后一种正则化是对 L1 或 L2 正则化的补充，通过对给定权重的标准大小施加限制。因此，这将允许参数更新具有硬限制，并因此限制给定网络能够产生的可能解决方案的数量。这将有助于通过限制可能的解决方案来更快地训练网络，并且在最佳解决方案中防止参数在不正确的方向上更新太多。

## 摘要

我们已经充分讨论了 CNN 的概念，并浏览了目前最新的所有架构。有关 CNN 的应用示例，请参见第 [11](11.html) 章，特别是关于图像数据的预处理，这是构建图像识别软件的一个非常重要的步骤。接下来，我们将讨论递归神经网络(RNs)以及在基于时间序列的数据中检测模式的复杂性。