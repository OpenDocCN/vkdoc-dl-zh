# 8.实验设计和启发式

在回顾了所有与你将遇到的问题解决相关的机器学习和深度学习模型之后，终于到了谈论构建你的研究的有用方法的时候了，包括正式和非正式的方法。

除了知道如何正确评估开发的解决方案，你还应该熟悉与实验设计领域相关的概念。罗纳德·费雪是 20 世纪杰出的英国统计学家，也是统计学领域最有影响力的人物之一。他的技术在进行实验时经常被引用，即使你没有明确地使用它们，回顾一下也是有用的。

## 方差分析(ANOVA)

方差分析是一组用于研究数据中各组观察值之间差异的方法。z 和 t 检验的扩展，类似于回归，我们观察响应和解释变量之间的相互作用。我们假设数据中的观察值是独立且同分布的(IID)正态随机变量，残差是正态分布的，方差是齐次的。在多个方差分析模型中，下面是本节其余部分讨论的模型。

### 单向方差分析

用于比较三个或更多样本空间的平均值。具体来说，它用于由具有两个或更多级别的一个变量/因子执行分类的情况。

### 双向(多向)方差分析

这类似于单因素方差分析，除了这种模型可用于有两个或更多解释变量的情况。

### 混合设计方差分析

与之前描述的模型相比，混合设计方差分析的区别在于其中一个因素变量是跨受试者分析的，而另一个因素是受试者内的变量。

### 多元方差分析

这种方法类似于单因素方差分析和双因素方差分析，只是它特别用于分析多变量样本均值，或者在给定数据集中有两个或更多解释变量时。

讨论了各种方差分析模型后，下一节将讨论评估结果的方法:F 统计量。

## f 统计量和 f 分布

以罗纳德·费雪命名的 F 统计量是两个统计方差的比值。f 统计基于 f 分布，一种连续的概率分布(见图 [8-1](#Fig1) )。我们把这种分布称为 f 检验的给定检验统计量的零分布。让我们假设我们有变量 A 和 B，使得它们都具有分别具有 n 和 d 自由度的卡方分布，使得

![$$ X=\frac{\frac{A}{n}}{\frac{V}{d}}, $$](../Images/A435493_1_En_8_Chapter_Equa.gif)

![$$ f(x)=\left(\frac{\varGamma \left(\frac{n}{2}+\frac{d}{2}\right)}{\varGamma \left(\frac{n}{2}\right)\varGamma \left(\frac{d}{2}\right)}\right)\left(\frac{n}{d}\right)\left(\frac{{\left[\left(\frac{n}{d}\right) x\right]}^{\frac{n}{2}-1}}{{\left[1+\left(\frac{n}{d}\right) x\right]}^{\frac{n}{2}+\frac{d}{2}}}\right),\kern0.5em x\in \left(0,\ \infty \right) $$](../Images/A435493_1_En_8_Chapter_Equb.gif)

![A435493_1_En_8_Fig1_HTML.jpg](../Images/A435493_1_En_8_Fig1_HTML.jpg)

图 8-1。

PDF for F-distribution

比方说，我们正在考虑单向方差分析，并假设一组人群的平均值相等且呈正态分布。我们将 F 统计量定义为

![$$ F=\frac{\frac{SSE}{k}}{\frac{SSR}{n- k-1}} = \left(\frac{\frac{\varSigma {\left({\widehat{Y}}_i - \overline{Y}\right)}^2}{k}}{\left(\frac{\varSigma {\left({Y}_i-\widehat{Y}\right)}^2}{n- k-1}\right)}\right), $$](../Images/A435493_1_En_8_Chapter_Equc.gif)

其中 k 是自由度，n 是 n 个响应变量的数量。零假设表示仅使用 x 截距创建的模型和由用户创建的模型产生不可区分的结果(在给定的置信区间内)。另一个假设是，读者创建的模型明显优于仅具有 x 截距的模型。就像测试任何其他统计显著性的度量一样，这是基于我们想要设置的阈值来确定的。(90%的置信度，95%的置信度，以此类推)。

现在，让我们用一个玩具例子来应用和解释我们刚刚提到的概念。对于本例，我们将使用虹膜数据集:

```py
#Loading Data
data("iris")

#Simple ANOVA
#Toy Example Using Iris Data as Y
y <- iris[, 1]
x <- seq(1, length(y), 1)
plot(y)

```

该数据集将用于在以下实验中创建响应和/或解释变量。在第一个玩具示例中，我们采用 iris 数据集的第一列(代表每次观察的萼片长度)并将其作为解释变量。然而，在我们进行单因素方差分析之前，让我们验证将数据拟合到线性模型所需的假设。我们将从目视检查我们的数据开始，如图 [8-2](#Fig2) 所示。

![A435493_1_En_8_Fig2_HTML.jpg](../Images/A435493_1_En_8_Fig2_HTML.jpg)

图 8-2。

Visualization of data

我们立即注意到，数据在方向上是相当线性的，具有正斜率。这是一个很好的第一个指标，但我们应该更深入地挖掘，以确保我们的其他假设得到满足。在这种情况下，我们将专注于绘制拟合模型的残差。所谓残差，我指的是实际值减去模型预测值的余数。在处理线性模型时，您应该大量使用残差分析，但一般情况下也是如此，因为残差分析提供了对特定模型工作情况以及数据方向的直观了解。在图 [8-3](#Fig3) 中，我们看到通过拟合 x 和 y 的线性模型创建的以下曲线图:

![A435493_1_En_8_Fig3_HTML.jpg](../Images/A435493_1_En_8_Fig3_HTML.jpg)

图 8-3。

Residual plot

```py
plot(glm(y∼x))

```

注意图 [8-3](#Fig3) 中显示的四个中右上角的图形。这是一个分位数图，它有效地显示了残差分布的正常程度。当仔细检查该图时，我们可以看到相当数量的数据位于虚线 45 度角线上，这是数据中正常的标记。然而——通常情况下——我们注意到尾部倾向于稍微高于这条线。值得注意的是，我们认为是正态分布的数据几乎总是表现出相似的模式。在现实世界中，当我们拥有足够多的数据时，大多数数据往往接近正态分布，但它不太可能完全呈正态分布。因此，我们在这里接受数据是正态分布的，并继续验证其余的假设。当数据呈正态分布时，它可以符合线性模型，因此我们可以合理地估计 x 变量范围内的值。

因为我们还要求误差呈现恒定方差，所以让我们将注意力转向左上角的图。请注意，此图中的 x 轴表示回归输出的值，而 y 轴详细说明了残差的值。穿过图中心的水平线表示拟合值等于实际值的区域，或者观测值的残差为零的区域。具体参考我们的数据时，我们可以看到，一般来说，从图的左侧到右侧，所绘制的残差的形状似乎是一致的。因此，我们可以说残差实际上表现出恒定的方差。如果不是，我们会注意到散点图的形状中会有明显的图案，从图的左侧到右侧会变得更夸张或不那么夸张。

最重要的是，注意图右下方的情节。它提出了一个重要的概念，有助于理解某些数据点如何改变回归模型的拟合线。杠杆被描述为特定观察值与数据集其余部分之间的相对差异。通过将索引放置在数据点附近，在 R 中表示特别具有高杠杆作用的观察值。我们用

![$$ {h}_i=\frac{X_i - \overline{X}}{{\displaystyle {\sum}_{j=1}^n}\left({X}_j - \overline{X}\right)\kern2.25em } + \frac{1}{n} $$](../Images/A435493_1_En_8_Chapter_Equd.gif)

来定义杠杆

其中 n =观察次数，X<sub>I</sub>= X 的第 I 次观察，![$$ \overline{X} $$](../Images/A435493_1_En_8_Chapter_IEq1.gif)= X 内所有观察的平均值，i = 1，2，…，n

与杠杆高度相关的是库克距离(Cook's distance)的概念，它直接估计一个特定的观察对这个回归模型的影响。我们把库克的距离定义为

![$$ {D}_i=\frac{e_i^2}{s^2 p}\left[\frac{h_i}{{\left(1-{h}_i\right)}^2}\right], $$](../Images/A435493_1_En_8_Chapter_Eque.gif)

其中 e <sub>i</sub> <sup>2</sup> =给定观测值的残差平方，s <sup>2</sup> =模型的均方误差，p =模型中参数的个数，H<sub>I</sub>= H 矩阵的第 I 条对角线其中![$$ X{\left({X}^T X\right)}^{-1}{X}^T y, $$](../Images/A435493_1_En_8_Chapter_IEq2.gif) i = 1，2，…，n，n =观测值个数。

通常，如果观察值的 Cook 距离值大于 1 或距离值大于 4/n，我们认为该观察值特别有影响。使用哪个阈值最终取决于您，但很明显，这将取决于具体情况，值得在实验的基础上检查是否提供了更多或更少异常值的数据集，以及这将如何影响您的最终目标。例如，如果实验的目的是异常检测，那么降低阈值使得数据集中更多的噪声被限定为信号可能是愚蠢的。当回头参考我们的特定图时，我们可以看到相当数量的数据点被标记为有影响。在我们选择型号时，我们将牢记这一点。

当评估数据集中的所有图时，我们可以自信地说，尽管存在异常值，并且我们的假设不完全符合，但 OLS 回归的稳健性允许克服这些轻微的偏差。因此，选择 OLS 回归作为这项任务的模型是合理的，因此方差分析将产生具有统计意义的结果。执行代码时，我们观察到以下情况:

```py
simpleAOV <- aov(y ∼ x)
summary(simpleAOV)

             Df Sum Sq Mean Sq F value Pr(>F)
x             1  52.48   52.48   156.3 <2e-16 ***
Residuals   148  49.69    0.34

```

正如当我们在 glm 对象上使用`summary()`函数时，我们得到了它的统计显著性的度量。然而，我们得到的不是 Z 分数，而是本例之前提出的概念的 F 分数及其相对 p 分数。在这种情况下，我们可以以大于 99%的显著性说，我们拒绝了零假设的结果。因此，该模型比仅截距模型更适合，因此我们对其结果更有信心。然而，假设我们想要比较多个合适的模型。因此，让我们来看看当我们包含不止一个变量时会发生什么，但是也要研究这两个变量之间的相互作用。

正如我们在下面的代码中看到的，我们在这个模型中使用第二列和第三列作为解释变量。在拟合我们的模型时，我们将两个解释变量相乘。执行代码时，我们会观察到以下结果:

```py
#Mixed Design Anova
x1 <- iris[,2]
x2 <- iris[,3]
mixedAOV <- aov(y ∼ x1*x2)
summary(mixedAOV)

             Df Sum Sq Mean Sq F value   Pr(>F)
x1            1   1.41    1.41    12.9 0.000447 ***
x2            1  84.43   84.43   771.4  < 2e-16 ***
x1:x2         1   0.35    0.35     3.2 0.075712 .
Residuals   146  15.98    0.11

```

我们的残差明显更小，并且所有变量在至少 90%的置信区间内具有统计显著性。让我们执行下面的代码，直观地比较图 [8-4](#Fig4) 中的两个模型:

![A435493_1_En_8_Fig4_HTML.jpg](../Images/A435493_1_En_8_Fig4_HTML.jpg)

图 8-4。

Mixed design ANOVA plot

```py
par(mfrow = c(2,2))
plot(glm(y ∼ x1*x2))
dev.off()

```

我们可以看到，我们需要满足的所有假设都做得非常好。实际上，所有的残差都是正态分布的，如正态 Q-Q 图所示，残差呈现恒定的方差，相当小的一部分具有杠杆作用。因此，当在我们定义的两个模型之间进行选择时，与第一个相比，我们选择第二个是合理的。

这是一个简单的例子，说明我们如何在模型选择过程中使用方差分析。在第 [10](10.html) 和 [11](11.html) 章中，你将学会有效地执行这些关于比较深度学习和机器学习算法的相同分析。

现在让我们在费希尔原理的指导下，更详细地讨论如何组织我们的实验。

### 费希尔原则

有史以来最杰出的统计学家之一罗纳德·费雪解释了实验设计的原则。以下是对他的原则的描述，以及关于如何实施这些原则的一般建议:

1.  实验陈述:你应该明确陈述激发实验的场景，非常明确地给出实验中将要发生的步骤的概要。一般认为，导言应该包括对主题的高层次概述，每一部分都应该更详细地描述不同的组成部分，从实验开始到结束有逻辑地进行。
2.  解释及其推理基础:从一开始，给出你可能期望的合理结果是合理的。你应该陈述你认为必须考虑的结果，但要意识到为你的下属提供一个无止境的结果列表可能不会很有帮助。此外，当讨论所有可能的结果时，要为阅读你的研究的人提供可操作的见解。不言而喻，确实能给出可操作见解的研究，会给误用留下更多空间。
3.  显著性测试:在评估机器学习和深度学习解决方案的背景下，一个简单的建议是引导用于评估给定模型的测试统计。有理由假设，如果你在足够长的时间内得到足够多的测试统计数据，那么数据将呈正态分布。从这一点出发，可以进行 Z 检验，以确定模型的合理统计置信度。
4.  零假设:这种假设应该声明显示的结果没有显著性，并且测试人群之间的任何偏差都是由于一些无关的误差，例如不适当的采样或与适当的实验实践的偏差。这必须是所有统计测试的一个组成部分。
5.  随机化:测试有效性的物理基础:当进行测试时，所得到的结果应该以一种结果没有偏差的方式进行。在某些情况下，这可能需要对数据进行随机观察，以消除实验建模中可能导致某些结果的任何固有偏差。
6.  统计复制:从测试中得出的结果应该而且必须是可复制的。鉴于数据集和我们预期观察这种情况的环境的固有限制，得出的不合理的结果不如可复制的结果有价值。
7.  分组:将不同的实验组划分开来，从而减少或完全防止不同的变异和偏差影响实验结果的过程。

## 普拉克特-伯尔曼设计公司

Plackett-Burman 设计是由 Robin Plackett 和 J. P. Burman 在 20 世纪 40 年代创建的，是一种寻找解释变量的可量化相关性的方法，在这种情况下我们称之为因子，其中每个因子有 L 个水平。总体目标是使用有限数量的实验来最小化相关性估计的方差。为了实现这个目标，选择一个实验设计，使得任何给定因素对的每个组合在每个实验“运行”中出现相同的次数

Plackett-Burman 设计需要少量实验，特别是 4 到 36 的倍数，并且该设计有 N 个样本，可以研究多达 k 个参数，其中 k = N–1。在 L = 2 的情况下，使用每个元素为–1 或 1 的正交矩阵。这个矩阵也称为哈达玛矩阵。这种方法对于识别不同因素对响应变量的主要影响是有用的，这样我们就可以消除那些似乎影响很小或没有影响的因素。Plackett 和 Burman 自己给出了 L 等于 3、4、5 和 7 的具体设计。

请看图 [8-5](#Fig5) 中的矩阵，它直观地描述了 Plackett-Burman 设计。当执行实验设计(DOE)时，您必须将适当的行作为设计表的第一行。在这种情况下，我们从+、-、+、-、+、+开始。这是每行中出现的序列的排列，代表一种治疗组合。您可以将治疗组合视为一个功能集的独特组合。然后，通过将前一行中的序列向右移动一列来创建第二行。对剩余的每一行重复这一过程。最后一行显示所有负元素。不过，重要的是要认识到，Plackett-Burman 设计不能描述对一个给定因素的影响是否会导致另一个因素的影响，同样，它也不能知道给定足够小的设计的影响本身。这种设计被认为是数据分析的准备步骤，除了随后采取的其他步骤之外，还建议将替代的准备步骤与其并列。

![A435493_1_En_8_Fig5_HTML.jpg](../Images/A435493_1_En_8_Fig5_HTML.jpg)

图 8-5。

Plackett-Burman matrix

## 填空白

这些方法不需要离散的参数，样本大小的选择与参数总数无关。对于读者想要创建响应面的情况，推荐使用这些方法，但是应该注意，很难分别确定给定或一组参数的主要影响和相互作用。

## 全因子

全析因是实验设计中最流行的方法之一，其中 N = 2^K，k 等于因子的个数。举个例子，让我们有 k 个因子，其中 L = 2。在这个模型中，在实验发生之前，我们不区分干扰因素和主要因素。假设 L = 2，我们将它们表示为高电平“h”或低电平“L”。高级因子的值为 1，低级因子的值为–1。我们将变量的交互作用确定为单个因素的乘积。从任何给定因子约束的可能实验来看，一次改变一个因子的样本仍然是样本空间的一部分。这考虑了每个因素对响应变量的影响。现在，让我们将 M 定义为变量 x 的主要交集。这是高水平样本的平均响应变量与低水平样本的平均响应之间的差异。如果我们有三个因子，每个因子有两个级别，那么 X_1 的 M 将被定义如下:

![$$ {M}_{X_1}=\frac{y_{h, h, h} + {y}_{h, h, l}+{y}_{h, l, h}+{y}_{h, l, l}}{4}-\frac{y_{l, l, l} + {y}_{l, l, h} + {y}_{l, h, l} + {y}_{l, h, h}}{4} $$](../Images/A435493_1_En_8_Chapter_Equf.gif)

如果我们想了解两个或更多因素之间的相互作用，等式将是相同的，除了变量的相互作用将由变量的乘积来表示，而不是一个因素在给定状态下拥有的单个值。主效应和交叉效应统计提供了一种有效的方法来确定单个因素或因素组合对响应变量的影响程度。全因子设计不会以任何这样的方式使数据复杂化，并且提供了一种检查可变效应的透明方法。如果有两个以上的级别，则必须进行调整，以获得所有级别对给定响应变量的平均影响，其中分母为 N，因此

![$$ \overline{y}=\frac{{\displaystyle {\sum}_i^{L_1}}{\displaystyle {\sum}_j^{L_2}}{\displaystyle {\sum}_l^{L_3}}{\displaystyle {\sum}_m^{L_4}}{y}_{i, j, l, m}}{N} $$](../Images/A435493_1_En_8_Chapter_Equg.gif)

## Halton、Faure 和 Sobol 序列

在空间填充技术的保护伞下，其中许多是由伪随机数发生器激发的。伪随机数是通过随机性测试的序列生成集。我们将伪随机数发生器表示为以下函数:

![$$ \phi :\left[0,1\right)\to \left[0,1\right),\kern0.5em {\upgamma}_{\mathrm{k}}=\phi \left({\gamma}_{k-1}\right),\kern0.75em k=1,2,\dots $$](../Images/A435493_1_En_8_Chapter_Equh.gif)

我们必须选择一个能给出γ <sub>k</sub> 均匀分布的ϕ值。实现这一点的一种流行方法是范德科尔普序列，其中我们有一个基数 b，≥ 2 和连续的整数 n 以它们的 b-adic 展开形式表示，使得下面是真实的

![$$ n={\displaystyle \sum_{j=1}^T}{a}_j{b}^{j-1}, $$](../Images/A435493_1_En_8_Chapter_Equi.gif)

![$$ {\varphi}_b:{\mathrm{N}}_0\to \left[0,1\right), $$](../Images/A435493_1_En_8_Chapter_Equj.gif)

![$$ {\varphi}_b(n)=\kern0.5em {\displaystyle \sum_{j=1}^T}\frac{a_j}{b^j} $$](../Images/A435493_1_En_8_Chapter_Equk.gif)

其中 a 表示展开系数。

Halton 序列分别在第一、第二和第三维中使用基数为 2、基数为 3 和基数为 5 的 Van der Corput 序列。这种模式继续下去，在每一个连续的维度中，质数被用作基数。也就是说，多维聚类导致维度之间的高度相关性，实际上违背了实验设计本身的目的。为了解决这个问题，Faure 和 Sobol 序列对所有维度只使用一个基，对每个维度使用不同的向量元素排列。

## A/B 测试

在设计应用程序、网站和/或仪表板应用程序时，确定某些功能的变化对产品的影响是很有用的。例如，我们可以想象，一个工程师正试图用某种统计确定性来确定一个新特性的实现是否对获得新用户有影响。在这种情况下，建议人们使用 A/B 测试。广义的 A/B 检验是指用来比较两个数据集的统计假设检验方法，一个是对照组，一个是测试组，分别是 A 和 B。我们也可以修改测试，这样我们可以测试一个和多个额外的控制测试。

A/B 测试的动机很简单，因为不同产品的开发，不管它们是否具有机器学习或深度学习能力，都允许我们用统计上的信心来确定我们是否从最初的迭代到下一次迭代做出了改进。也就是说，我们可以使用这些过程作为一系列实验，从一代软件迭代到下一代软件，以观察效率的提高。通常，beta-二项式层次模型是最流行的方法之一，通过它我们可以 A/B 测试一个控制组而不是多个测试组。因此，我们将回顾这一模式。然而，首先让我们回顾一个简单的双样本 A/B 测试。

### 简单双样本 A/B 检验

假设我们在这里将一个控制组与一个测试组进行比较，并且我们试图了解我们的新网站是否会因为功能变化而产生更多的点击。我们将坚定地表明，虽然这个测试对两个例子是稳定的，但您应该避免对两个以上的样本使用这个测试。假设我们有两个数据集代表不同网站的不同属性，我们希望在 95%的置信度下进行测试。为此，我们将使用 t 检验。现在，我们还假设在执行 t 检验后，我们观察到均值的差异显著不同，并且 x2 与之前的模型相比显著提高。现在让我们假设我们一直在制作不同版本的网页，并不断尝试使用这种模式。经过九次不同的测试，x2 仍然被证明是最优秀的车型。但是当我们运行 x2 时，我们实际上看不到 x2 对其他网站的点击有任何改善。双样本 A/B 测试的常见问题是由于假阳性。

接下来，我将通过二项式分布展示 10 个个体假设检验显示正确结果的概率。设事件 A = x2 假设在 90%置信区间优于其他 9 个同行，B = x2 在 95%置信区间优于其他 9 个同行，C = x2 在 99%置信区间优于其他 9 个同行:

![$$ P\left(\mathrm{A}\right) = {.90}^{10} = 34.87\%, \kern0.5em P(B) = {.95}^{10}=59.87\%,\kern0.75em P(C) = {.99}^{10}=90.44\% $$](../Images/A435493_1_En_8_Chapter_Equl.gif)

简单地说，在事件 A、B 和 C 下，我们可以预期我们的实验产生 6.5、4.013 和 0.95 的假阳性。虽然 99%的置信区间在这个例子中表现最好，但是我们可以看到在其他置信区间下为什么这个方法会成为一个问题。因此，为了测试多个组，建议我们使用 beta-二项式分布。

### 用于 A/B 测试的 beta-二项式分层模型

贝叶斯统计是关于概率概念的一个学派。这里，它成为该模型的理论基础，并且还可以用于提供关于分布的改进的分级模型。在贝叶斯统计中，我们经常提到先验分布和后验分布。先验分布是指某个参数(我们已经获得的数据)的概率分布，而后验分布是指某个参数(我们想要获得的数据)的概率分布。先验分布和后验分布形成共轭分布。为了便于分析，我们通常寻求使用同一家族内的分布来表示先验分布和后验分布，这就是为什么在这个层次模型中我们使用贝塔分布和二项式分布。

贝塔分布是区间[0，1]内的概率分布，参数α和β最终控制分布的形状。通常，我们使用贝塔分布来统计建模随机变量。如前所述，与贝塔分布属于同一家族的是二项分布。这通常用于模拟以独立二元结果为特征的概率分布，例如掷硬币。我们将贝塔分布和二项式分布(见图 [8-6](#Fig6) 和 [8-7](#Fig7) )的概率密度函数分别定义为

![$$ \frac{x^{\alpha -1}{\left(1- x\right)}^{\beta -1}}{\frac{\varGamma \left(\alpha \right)\varGamma \left(\beta \right)}{\varGamma \left( a+\beta \right)}}, $$](../Images/A435493_1_En_8_Chapter_Equm.gif)

![$$ \left(\begin{array}{c}\hfill n\hfill \\ {}\hfill k\hfill \end{array}\right){p}^k{\left(1- p\right)}^{n- k} $$](../Images/A435493_1_En_8_Chapter_Equn.gif)

其中 n =成功的次数，k =试验的总次数，p =成功的概率。

![A435493_1_En_8_Fig7_HTML.jpg](../Images/A435493_1_En_8_Fig7_HTML.jpg)

图 8-7。

Binomial distribution

![A435493_1_En_8_Fig6_HTML.jpg](../Images/A435493_1_En_8_Fig6_HTML.jpg)

图 8-6。

Beta distribution

然后，我们将 beta 分布和先验分布的后验期望值建模为二项式，然后我们比较先验分布和后验分布之间的均值差异，以比较网站性能。

## 特征/变量选择技术

现在我们已经讨论了几个实验设计模型，让我们来谈谈在你对给定数据集中的因素有了更多的了解之后，你想要采取的步骤。变量选择似乎与实验设计直接相关，但本节将讨论用于降低维度的更具体的算法，以及用于分析变量及其与响应变量的相互作用的探索性较少的方法。这一点很重要，原因有很多，但在部署机器学习算法时，这往往是优化它们的一个主要因素。特征选择是一个比参数调整简单得多的过程，尤其是在深度学习模型中。因此，这是一种快速创建模型的方法，可以更快地训练并产生更准确的输出。与前面描述的许多技术一样，必须小心谨慎，因为过多的特征选择会导致创建过度拟合的模型。

### 向后和向前选择

向后选择是最简单的变量选择方法之一，在使用简单或多元线性回归时尤其常见。首先，你应该获取包含所有解释变量的数据集，并根据响应变量对它们进行回归。在这一步之后，选择适合于给定情况的统计显著性水平(85%、90%、95%等等)。一次一个变量，我们从数据集中删除具有最低统计显著性的变量(例如在使用 glm()模型时由 summary()函数产生的统计显著性)。我们回归原始数据集的新子集，并继续下去，直到数据集中的所有变量都具有统计显著性。在正向选择中，过程与先前的方法相同，除了区别在于您从没有变量的模型开始，添加变量，并检查它们的统计显著性。如果它们处于或高于阈值，则应该添加它们。如果没有，就应该删除。使用这些方法时要记住的注意事项是显著减少统计噪声，但要避免模型过度拟合测试数据，特别是当样本外预测是所构建模型的最终目标时。您还应该注意不要删除过多的变量，以免降低性能。

对于深度学习，一些模型中嵌入了特征选择。具体来说，CNN 中的某些层可以说是为了消除噪声而存在的，以便留下的数据富含信息。具体来说，可以认为池层就是这样做的。通过减少输入大小，我们减轻了从输入到输出的计算负荷，同时还帮助算法更准确地调整这些层之间的权重，并最终对图像进行分类。

除了使用 P 值，您还可以选择其他统计标准来确定保留/删除哪些变量。其中最常见的是赤池信息准则(AIC)和贝叶斯信息准则(BIC):

![$$ A I C=2 k-2 \ln \left(\widehat{L}\right), $$](../Images/A435493_1_En_8_Chapter_Equo.gif)

![$$ B I C= \ln (n) k-2 \ln \left(\widehat{L}\right), $$](../Images/A435493_1_En_8_Chapter_Equp.gif)

![$$ \widehat{L}= p\left( x\Big|\widehat{\theta},\ M\right) $$](../Images/A435493_1_En_8_Chapter_Equq.gif)

其中 L 是模型的最大似然函数，![$$ \widehat{\theta} $$](../Images/A435493_1_En_8_Chapter_IEq3.gif)是参数，k 是参数的数量。

AIC 和 BIC 关系非常密切。AIC 基于信息论领域，目标是选择一个具有最小 AIC 值的模型。根据函数的定义，对数似然值越大，AIC 值越小。从今以后，更接近数据拟合的模型将最终具有较低的 AIC 值。BIC 最终是由贝叶斯统计推动的，与 AIC 相似。BIC 分数专门用于评估模型在训练集上的性能，其中我们选择产生最小 BIC 的模型。特别是，BIC 惩罚有更多而不是更少参数的模型。正因为如此，BIC 天生喜欢不会过度适应数据集的模型，因此制定了一个标准，鼓励你选择一个能概括你正在分析的数据的模型。请注意，BIC 无法处理复杂的模型集合，仅在 n 远大于 k 的情况下才有效。考虑到 AIC，计算的 AIC 值必须跨相同的数据。具体来说，它不是一个客观的衡量标准，如决定系数。

### 主成分分析

主成分分析(PCA)是最常用的变量选择技术之一，可以专门用于数值数据。前面在几个例子中提到，PCA 是一种用于降低数据集维数的统计方法。简而言之，我们将数据转换为新的变量，称为主成分，并消除主成分，这些主成分解释了数据集中可忽略的方差。这种技术的好处是，我们保留了数据集的方差，同时能够比转换之前更容易地执行可视化和探索性分析。

我们的目标是从 x 向量中找到随机变量的线性函数，从α向量中找到方差最大的常数向量。这个线性函数产生我们的主分量。尽管如此，每个主成分必须按照方差递减的顺序排列，并且每个主成分必须彼此不相关。我们的目标如下:

![$$ \mathrm{Maximize}\ V a r\left({\alpha}_k^{\hbox{'}} x\right)={\alpha}_k^i\varSigma alph{a}_k $$](../Images/A435493_1_En_8_Chapter_Equr.gif)

我们寻求使用约束优化，因为如果没有约束，a <sub>k</sub> 的值可能无限大。因此，我们将选择以下规范化约束，其中![$$ {\alpha}_k^{\hbox{'}}{a}_k=1 $$](../Images/A435493_1_En_8_Chapter_IEq4.gif)

拉格朗日乘子法是一种对可微函数进行约束优化的工具。特别是，它有助于在给定的约束条件下找到相应函数的局部最大值和最小值。在实验范围内，拉格朗日乘数应用如下

![$$ {\alpha}_k^{\hbox{'}}\varSigma {a}_k-\lambda \left({\alpha}_k^{\hbox{'}}{a}_k-1\right), $$](../Images/A435493_1_En_8_Chapter_Equs.gif)

![$$ \frac{d\left({\alpha}_k^{\hbox{'}}\varSigma {a}_k-\lambda \left({\alpha}_k^{\hbox{'}}{a}_k-1\right)\right)}{d{\alpha}_k} = 0, $$](../Images/A435493_1_En_8_Chapter_Equt.gif)

![$$ \varSigma {\alpha}_k-\lambda {\alpha}_k=0, $$](../Images/A435493_1_En_8_Chapter_Equu.gif)

![$$ \varSigma {\alpha}_k={\lambda}_k{\alpha}_k $$](../Images/A435493_1_En_8_Chapter_Equv.gif)

方程的最后一步产生特征向量α<sub>k</sub>及其相应的特征值λ<sub>k</sub>。我们的目标是最大化λ <sub>k</sub> ，并且特征向量以降序定义。如果λ <sub>1</sub> 是最大的特征向量，那么第一主成分定义为![$$ \varSigma {\alpha}_1=\lambda {\alpha}_1 $$](../Images/A435493_1_En_8_Chapter_IEq5.gif)一般来说，我们定义给定的特征向量为 x 的第 k 个主成分，并且给定特征向量的方差由其对应的特征值表示。我现在将演示当 k = 1 和 k > 2 时的这个过程。第二主成分在与第一主成分不相关的情况下最大化方差，非相关约束如下:

![$$ c o v\left({\alpha}_1^{\hbox{'}} x{\alpha}_2^{\hbox{'}} x\right)={\alpha}_1^{\hbox{'}}\varSigma {\alpha}_2={\alpha}_2^{\hbox{'}}\varSigma {\alpha}_1={\alpha}_2^{\hbox{'}}{\lambda}_1{\alpha}_1^{\hbox{'}}={\lambda}_1{\alpha}_2^{\hbox{'}}{\alpha}_2=0, $$](../Images/A435493_1_En_8_Chapter_Equw.gif)

![$$ {\alpha}_2^{\hbox{'}}\varSigma {\alpha}_2-{\lambda}_2\left({\alpha}_2^{\hbox{'}}{\alpha}_2-1\right)-\phi {\alpha}_2^{\hbox{'}}{\alpha}_1 $$](../Images/A435493_1_En_8_Chapter_Equx.gif)

![$$ \frac{d\left({\alpha}_2^{\hbox{'}}\varSigma {\alpha}_2-{\lambda}_2\left({\alpha}_2^{\hbox{'}}{\alpha}_2-1\right)-\phi {\alpha}_2^{\hbox{'}}{\alpha}_1\right)}{d{\alpha}_2}=\varSigma {\alpha}_2-{\lambda}_2{\alpha}_2-\phi {\alpha}_1=0, $$](../Images/A435493_1_En_8_Chapter_Equy.gif)

![$$ {\alpha}_1^{\hbox{'}}\varSigma {\alpha}_2-{\alpha}_1^{\hbox{'}}{\lambda}_2{\alpha}_2-{\alpha}_1^{\hbox{'}}\phi {\alpha}_1 = 0, $$](../Images/A435493_1_En_8_Chapter_Equz.gif)

![$$ 0 - 0-\phi 1=0, $$](../Images/A435493_1_En_8_Chapter_Equaa.gif)

![$$ \phi =0, $$](../Images/A435493_1_En_8_Chapter_Equab.gif)

![$$ \varSigma {\alpha}_2-{\lambda}_2{\alpha}_2=0 $$](../Images/A435493_1_En_8_Chapter_Equac.gif)

这个过程可以重复到 k = p，产生 p 个随机变量中每一个的主分量。但是，与 PCA 相关的限制是多方面的，对于问题类型必须加以考虑。首先，PCA 假设特征之间存在线性相关性。显然，在实际环境中不一定总是如此，因此使得 PCA 得出的结果值得怀疑。其次，PCA 只能用于数字数据集，数字编码分类数据的下降(在本章后面讨论)会增加隐含偏差，使这种技术的结果变得无用。此外，PCA 明确假设方差是分析数据集时最重要的统计量。尽管方差通常是一个重要的统计量，但在一些问题案例中，它不一定是。

PCA 如何应用于深度学习的一个例子是通过 PCA 白化的过程。当我们提到白化时，我们指的是使输入数据不那么同质的过程，以努力使数据从一个观察到另一个观察时不那么同质。在 CNN 的例子中，这对于图像分类非常有用。具体而言，在图像数据中，彼此相邻的许多像素在大区域内通常具有相似的值，如果不是相同的话。

这方面的一个例子是查看 MNIST 数据集，看看图像的哪些部分是黑色的，哪些是白色的。相反，PCA 白化产生矩阵的特征分解，从而消除了这种同质性。因此，每个个体的特征与其原始形式相比明显不太相似，但是数据内的方差被保留，这是在矩阵上执行特征分解时的好处。

### 要素分析

因素是不可观察的变量，彼此高度相关，并影响给定的解释变量。与 PCA 的最终目的降维不同，因子分析寻求定位自变量。此外，我们想确定这些因素对表面属性有什么影响。它建立在这样的假设上，即观察到的变量可以减少到一个表现出相似方差的子集。在因子分析中，我们要求数据必须是正态分布的，并且数据集中实际上没有异常值。我们还应该寻求分析大量的观察数据，虽然相关性不是近似线性的以避免多重共线性，但在整个数据集上必须是中等至高的。典型的因子分析模型由

![$$ {X}_j={a}_{j1}{F}_1+{a}_{j2}{F}_2+\dots +{a}_{j m}{F}_m+{e}_{j,},\ j=1, \dots,\ p $$](../Images/A435493_1_En_8_Chapter_Equad.gif)

给出

其中 e <sub>j</sub> =给定解释变量的唯一和特定因素，j =因素负荷，X <sub>j</sub> =解释变量，m =潜在因素

因素负荷可以被认为是权重，它们表示相对于单个变量，它们对给定因素的影响程度。表面属性被表示为单独的解释变量。典型地，因子分析模型将产生因子，使得各个变量之间没有相关性，因此我们有独立变量，类似于主成分。应该注意的是，因素不是被创建的，而是基于表面属性之间的相关性而被揭示的。看不见的因素可以是无形的，但却是可以想象的。例如，我们可以想象在一个给定的实验中，与一个人相比，一个人的阅读或写作能力。就我们如何衡量它们而言，这些属性是不客观的，但当评估一个标准化的考试时，例如，阅读和写作部分，显然会影响一个人的分数。

### 因子分析的局限性

因子分析可以找到一种方法来获得从随机数生成的数据中的模式。因此，人们应该记住，如果可以在随机数据中找到结构，那么他们在结构化数据中观察到的模式也可能是错误的。此外，在数据中发现的结构最终是输入到因子分析中的变量/数据集的衍生物。简而言之，数据集中没有显而易见的客观模式，最终数据集/变量的重组会导致因子分析产生的结果出现显著差异。因此，一个人如何解释因素分析的结果最终比它看起来更主观。也就是说，建议将因子分析与统计方法一起使用，并且/或者对数据进行结构化，使其符合所处理问题领域内已知为真的假设。

## 处理分类数据

在您可能遇到的所有困难中，最大的挑战之一是处理和分析分类数据，或数值数据。通常，我们经常遇到分类数据作为一个因素变量具有不同的水平。本节讨论将会遇到的一些常见问题以及可能的解决方案，同时要记住一些注意事项。

### 编码因子级别

例如，假设我们有一个数据集，其中我们正在分析一个变量，即给定街区的所有街道。这是一个特别有趣的例子，因为街道可能都是名称(如“枫树街”、“云杉街”、“红杉街”等等)，也可能都是数字(第一街、第二街、第三街等等)。如果街道是名字，我们可以采取这种方法，用数字对街道进行编码。这是给每个变量一个唯一标识符的简单方法，但是它有局限性。机器学习算法将把级别解释为值的指示，而不是唯一的标识符，这实质上没有给出关于观察的“质量”的描述性数据。具体来说，如果我们将“枫树街”标记为 1，将“云杉街”标记为 2，当没有证据确定这一点时，许多算法可能会将云杉街解释为比枫树街更重要。当考虑数字的情况时，同样的问题也存在，但它只是隐含的，而不是由标签编码引起的。这种技术的另一个局限性是，如果编码变量与其他变量高度相关，多重共线性可能会被引入数据集，否则它将不会存在。

### 分类标签问题:级别太多

为了与使用街道名称的示例保持一致，我们可以想象在许多城市中，这将导致我们拥有数百甚至数千条单独的街道。尽管有变化的变量比完全没有变化的变量产生更好的结果，但这也会在执行模型评估时造成困难。因此，在这些情况下，对变量进行编码并使用分类/回归树或随机森林模型可能是一个好主意。此外，一个建议的方法是对变量进行编码，并使用 K-means 聚类来获得聚类数，然后我们用该变量替换级别。尽管在许多方面，这仍然会使我们之前讨论的编码变量的偏差融入到聚类观察中，但这仍然是一种有效降低级别的方法，应该在必要时进行探索。

### 典型相关分析

与 PCA 密切相关的是典型相关分析(CCA ),这是一种寻找两个变量的线性组合的方法，使得它们彼此具有最大可能的协方差。通常，这是一种数据预处理技术，适用于使用多元线性回归的相同情况，但特别是当有两组多元数据集时，我们希望检查以下各项之间的关系:

给定两个矢量![$$ X,\ Y\in {\mathrm{\mathbb{R}}}^{m\ x\ n} $$](../Images/A435493_1_En_8_Chapter_IEq6.gif)和方向![$$ \alpha \in {\mathrm{\mathbb{R}}}^m\ \mathrm{and}\ \beta \in {\mathrm{\mathbb{R}}}^n $$](../Images/A435493_1_En_8_Chapter_IEq7.gif)

![$$ \alpha,\ \beta =\underset{{\left|\left| X\alpha \right|\right|}_2 = {\left|\left| Y\beta \right|\right|}_2=1}{\mathrm{argmax}} c o v\left( X\alpha,\ Y\beta \right) $$](../Images/A435493_1_En_8_Chapter_Equae.gif)

## 包装器、过滤器和嵌入式(WFE)算法

当评估一些更先进的变量选择技术时，我们采用 WFE 算法。通过在数据上运行每个可能的特征子集并评估模型性能来区分包装器算法，从而选择对于给定模型表现最佳的子集。嵌入式算法被明确地写入模型的过程中(使用 LASSO 的 L1 正则化)。过滤方法试图通过查看数据本身来评估特征的优点，而不是仅通过方法来评估其性能。

### 救济算法

由 Aha、Kibler 和 Albert 在 1991 年设计的 relief 算法是一种基于特征的权重算法，其灵感来自基于实例的学习。每个特征被分配一个表示其与目标的相关性的权重。该算法是随机化的，并且相关性值的更新取决于所选实例和两个最近实例之间的差异。

#### 算法

1.  给定![$$ {\left\{\left({x}_n,\ {y}_n\right)\right\}}_{n=1}^{\mathrm{N}},\ \mathrm{set}\ {w}^0=\frac{1}{I},\kern0.75em T=\mathrm{number}\ \mathrm{of}\ \mathrm{iterations},\kern0.5em \sigma =\mathrm{kernel}\ \mathrm{width},\ \uptheta =\mathrm{stopping}\ \mathrm{criterion} $$](../Images/A435493_1_En_8_Chapter_IEq8.gif) T =迭代次数，s =核宽度，q =停止准则。
2.  对于 t = 1 : T
    1.  计算成对距离 w.r.t. ![$$ {w}^{t-1} $$](../Images/A435493_1_En_8_Chapter_IEq9.gif)
    2.  计算 P <sub>m</sub> ，P <sub>h</sub> ，P <sub>o</sub> 。
    3.  更新权重。
    4.  如果![$$ \left|\left|{w}^t-{w}^{t-1}\right|\right|<\theta,\ \mathrm{break} $$](../Images/A435493_1_En_8_Chapter_IEq10.gif)。 

## 其他本地搜索方法

如果不是直接相关的话，本文后面部分提到的许多算法将从优化的这个子领域中得到启发，这个子领域通常用于计算密集型优化问题。我们认为所有可能的解决方案都在我们称为特征空间或搜索空间的集合中。目标是满足我们寻求解决的优化问题的全局最优。局部搜索算法从特征空间中的随机元素开始，并在每次迭代中基于从当前邻域产生的信息选择新的解决方案。在这个阶段之后，算法将移动到最近的邻域中的给定邻域，但是根据问题，搜索算法可以选择不止一个邻域。

### 爬山搜索方法

在 20 世纪 80 年代和 90 年代机器学习发展之前，爬山往往是更受欢迎的搜索方法之一。爬山形成了本章中描述的许多新的搜索方法的动机，并且对于参数调整仍然是一种有用的技术。与其他搜索方法一样，爬山法寻求在当前点的局部范围内优化目标函数。爬山法最适用于有一个最大值或一个最小值的函数，这样算法就可以相对容易地找到问题的解。然而，对于具有大量局部极小值的函数，它面临许多问题。为了解决这个问题，许多不同的启发式算法和方法，如随机重启以避免局部极小值和搜索轨迹的随机邻域选择，都被添加到基本的爬山算法中。

### 遗传算法

遗传算法被认为是人工智能领域的直接产物，因为它们直接模拟了进化过程。在该算法中，总特征空间的几个子集“进化”,使得下一个子集在统计上优于上一次迭代。当一个更好的子集不能被创建时，进化过程停止，并且最好的子集被选择作为答案。与其他算法相比，该算法的优势在于，遗传算法可以在多次迭代中积累关于给定特征空间的信息，该过程本质上是并行的，因此陷入局部最小值的可能性较小，并且该算法本身相对容易理解。遗传算法的局限性之一是，如果存在大量的局部最优解，遗传算法并不总是收敛于全局最优解。此外，该算法可能不是部署的最佳选择，因为它难以扩展，因为特征空间大小随着可能子集的数量呈指数增长。

#### 算法

*   选择一组初始随机解决方案供选择。
*   基于一些统计标准评估解决方案，例如 MSE。
*   选择最佳人选。
*   通过“变异”先前选择的解决方案来产生新的个体。
*   评估新解决方案的适用性。
*   当达到某个标准时停止，如损失容限。

### 模拟退火

在我们将要讨论的启发式技术中，SA 是少数被评估的概率模型之一。SA 的设计灵感来自于冶金学中的退火，它模拟了缓慢冷却的效果，即缓慢降低接受更差解决方案的可能性。我们将每个解视为一个状态，算法可以搜索的邻域逐渐变小。在特征空间已经被完全搜索之后，或者已经达到另一个停止标准之后，该算法收敛于一个解。一

#### 算法

*   T =温度=热，冻结=停止标准。
*   而(温度！=冻结)，移动到特征空间中的随机点并计算能量。
*   当系统在电流 t 下处于热平衡时，If 能量< 0 or loss tolerance, accept new state with probability ![$$ {e}^{-\frac{\varDelta E}{T}} $$](../Images/A435493_1_En_8_Chapter_IEq11.gif)。
*   If (E 在最后几次迭代中递减)，![$$ T= T\left( itertion+1\right) $$](../Images/A435493_1_En_8_Chapter_IEq12.gif)，否则 T =冻结。

SA 的最大困难是所需的参数调整量，随着特征量(和相应的特征空间)的增加，这变得很耗时。此外，对于这些参数中的任何一个都没有通用的基线或经验法则，这进一步增加了这种技术处理大量变化的数据集的难度。它应该更像是一种研究技术，而不是在算法中使用的技术。

### 蚁群优化算法

蚁群算法是 20 世纪 90 年代首次提出的一套优化算法。对组合数学问题最有用，ACO 已经被用于诸如车辆路径、计算机视觉、特征子集选择、定量金融和其他领域的任务。直觉是基于成群蚂蚁的活动，最终目标通常是从特征空间中找到给定随机选项集的最佳选项。在这种情况下，我们可以将一个蚁群想象成一个由边连接起来的图，其中每个节点代表数据集中 k 个特征中的一个。蚂蚁沿着边缘行进，“释放信息素”以吸引更多的蚂蚁进行后续迭代。根据设计，信息素会随着时间的推移而衰减，但是从 x 点到 y 点沿着尽可能短的边行进的蚂蚁会沿着给定的路径储存更多的信息素。因为蚂蚁被吸引到有更多信息素的路径上，这是寻找最优解的方法。每个“蚂蚁”从一个给定的状态移动，其概率由

![$$ {p}_{x, y}^k=\frac{\tau_{x y}^{\alpha}\left({\eta}_{x y}^{\beta}\right)}{{\displaystyle {\sum}_{j\in {J}_i^k}}{\tau}_{x j}^{\alpha}\left({\eta}_{x j}^{\beta}\right)} $$](../Images/A435493_1_En_8_Chapter_Equaf.gif)

给出

其中𝓇 ![$$ S\left({S}^{\prime}\in {N}_k(S)\right) $$](../Images/A435493_1_En_8_Chapter_IEq13.gif) =沉积在给定路径上的信息素，η =从 x : y 到所有路径距离之和的比例，β =调谐参数，j<sub>I</sub>T3】k=未被访问的邻居节点

用信息素更新为

![$$ {\tau}_{xy}\left( t+1\right)\to \left(1-\rho \right){\tau}_{xy} + {\displaystyle \sum_k}\varDelta {\tau}_{xy}^k $$](../Images/A435493_1_En_8_Chapter_Equag.gif)

其中τ <sub>xy</sub> =沉积的环己烯酮，ρ =环己烯酮的蒸发速率。

我们将δτ<sub>xy</sub>表示为由

![$$ \varDelta {\tau}_{xy}=\left\{\begin{array}{l} Q/{L}_k,\kern2.5em if\ kth\ ant\ travels\ along\ xy\\ {}\kern3.5em 0,\kern2.5em elsewhere\end{array}\right. $$](../Images/A435493_1_En_8_Chapter_Equah.gif)

给出的单个蚂蚁在给定路径上投放的信息素数量

其中 Q =某个常数，L <sub>k</sub> =用户定义的损失函数。

虽然 ACO 问题在没有大量特征的情况下是成功的，并且它通常比模拟退火和遗传算法执行得更好，但是随着更多节点的增加，问题变得更加难以解决。除此之外，虽然收敛是有保证的，但是不确定收敛实际何时发生。

#### 算法

*   通过创建完整的解决方案空间进行初始化。
*   当没有达到停止标准时，将每个蚂蚁定位在给定的开始节点。
*   对于每个蚂蚁，通过状态转换规则选择下一个节点。
*   应用信息素更新，直到每只蚂蚁都达到给定的解决方案。
*   根据选择标准评估每个解决方案。
*   更新最佳解决方案，并在此路径上应用信息素更新。
*   重复直到收敛到全局最优。

### 可变邻域搜索(VNS)

VNS 是一个特征子集选择算法家族，旨在处理组合学的挑战，并因此提供有保证的收敛性。发展于 20 世纪 90 年代末，VNS 的灵感来自于寻找离散和连续优化问题的解决方案的愿望(线性和非线性规划问题就是一个例子)。VNS 的假设是，关于给定邻域的局部最小值理论上可能不是另一个邻域中的局部最小值，局部最小值在一个或多个邻域之间彼此相对接近，全局最小值是解空间内所有邻域的局部最小值。在关于局部搜索方法的可用于 VNS 的算法中，有一些相关的扩展对于给定的任务更加具体。对于基于特征的选择，我们将研究基于过滤器的 VNS 算法。

#### 算法

*   找到一个初步的解决方案。
*   为 k = 1，…，j 选择邻域集合 N <sub>k</sub> ，其中 j = #个邻域和一个停止准则。
*   设 k = 1，从![$$ S\left({S}^{\prime}\in {N}_k(S)\right) $$](../Images/A435493_1_En_8_Chapter_IEq14.gif)的第 k <sup>个</sup>邻域生成一个随机点 S’。
*   如果基于目标函数，应用搜索方法使停止标准更接近达到。
*   如果此解决方案优于以前的解决方案，请将解决方案更新为当前解决方案。否则，设置 k = k + 1，保留当前解。
*   继续进行，直到收敛到全局最优或达到停止标准。

通常，我们选择信息商或线性相关性作为这些算法中的评估函数，但这最终是一个可以改变的参数。如果你觉得更先进，请随意实现自己的深度学习和/或机器学习算法，而不是传统的梯度下降，你可以使用上述搜索方法之一进行参数优化。尽管这可能很困难，但它将为您提供一个熟悉特定算法的绝佳练习，同时还能帮助您理解给定算法中的特定操作如何影响性能。这就把我们带到了一个关于改进现有机器学习算法的类似主题:反应式搜索优化。

## 反应式搜索优化

RSO 是优化领域中相对较新的创新。它产生了有趣的影响，值得更高级的读者一提。RSO 的目的是为那些打算创建机器学习平台和工具的人提供特别的帮助，这些平台和工具是为那些不像典型的机器学习工程师那样技术娴熟的用户设计的。智能优化指的是 RSO 内部更具体的研究领域，但仍然是相关的。在这个范例中，我们评估不同学习方案的有效性。大致有三种，我们称之为线上、线下以及两者不同比例的结合。这是在不同环境中实现算法的思想，使得它们具有不同的搜索历史，这最终影响当前会话中的时期的动作。

### 被动禁令

基于禁止的技术和智能方案，与基本的启发式搜索如局部搜索相反，是禁忌搜索的智力动机。禁忌搜索方法主要是在 20 世纪 80 年代获得了最初的牵引力，鉴于它所占据的肥沃土壤，它已被证明是一个大的研究领域。与局部搜索方法相比，禁忌搜索(TS)特别值得注意，因为它使用了从数据集中收集的先验信息，以及它如何影响新迭代的结果。假设我们有一个由长度为![$$ L:\ \mathcal{X}={\left\{0,1\right\}}^L $$](../Images/A435493_1_En_8_Chapter_IEq15.gif)的二进制字符串组成的可行搜索空间，X 是当前配置，N(X)是前一个邻域。以下等式与禁忌搜索相关，即基于禁止的

![$$ {X}^{t+1}=\mathrm{BestNeighbor}\left({N}_A\left({X}^t\right)\right), $$](../Images/A435493_1_En_8_Chapter_Equai.gif)

![$$ {N}_A\left({X}^{t+1}\right)=\mathrm{ALLOW}\left( N\left({X}^{t+1}\right),\ {X}^0, \dots,\ {X}^{t+1}\right) $$](../Images/A435493_1_En_8_Chapter_Equaj.gif)

其中允许功能选择![$$ \left( N\left({X}^{t+1}\right)\right) $$](../Images/A435493_1_En_8_Chapter_IEq16.gif)的子集，使得它依赖于整个搜索轨迹![$$ {X}^0, \dots,\ {X}^{t+1} $$](../Images/A435493_1_En_8_Chapter_IEq17.gif)。

禁忌搜索算法有多种分类方式，但我将阐述的最初区别因素是 TS 算法中确定性系统与随机系统。禁忌搜索的最基本形式被称为严格禁忌搜索。在该算法中，我们观察到 N(X)具有以下值:

![$$ {N}_A\left({X}^{t+1}\right)=\left\{ X\in N\left({X}^{t+1}\right)\kern0.5em s. t.\kern0.5em X\notin \left\{{X}^0, \dots,\ {X}^{t+1}\right\}\right\} $$](../Images/A435493_1_En_8_Chapter_Equak.gif)

当引入一个禁止参数 T 时，它决定了一个移动在执行其逆移动后将保持禁止多长时间，我们可以得到两个不同于严格禁忌搜索的算法。当且仅当通过将方向应用于搜索而从当前点获得邻居，使得其逆在最后 T 次迭代期间没有被使用，例如

![$$ {N}_A\left({X}^{t+1}\right)=\left\{ X=\mu \circ {X}^t\ s. t.\ \mathrm{LastUsed}\left({\mu}^{-1}\right)<\left( t- T\right)\right\}, $$](../Images/A435493_1_En_8_Chapter_Equal.gif)

时，才允许邻居

其中 LastUsed()是 move μ的最后一次使用时间。如果 T 随着迭代计数器而变化，生成搜索轨迹的一般动态系统包括 T 的附加演化方程，使得

![$$ {T}^t=\mathrm{React}\Big({T}^{t-1},\ {X}^0, \dots,\ {X}^t, $$](../Images/A435493_1_En_8_Chapter_Equam.gif)

![$$ \begin{array}{l}{N}_A\left({X}^{t+1}\right)=\left\{ X=\mu \circ {X}^t\ s. t.\ \mathrm{LastUsed}\left({\mu}^{-1}\right)<\left( t- T\right)\right\},\ {X}^{t+1}\\ {}\kern14em = \mathrm{Best}-\mathrm{Neighbor}\left({N}_A\left({X}^t\right)\right)\Big\}\ \end{array} $$](../Images/A435493_1_En_8_Chapter_Equan.gif)

对于作用于二进制字符串的基本移动，![$$ \mu ={\mu}^{-1} $$](../Images/A435493_1_En_8_Chapter_IEq18.gif)。

对于随机模型，我们可以用概率生成-接受规则代替禁止规则，大概率表示允许的移动，小概率表示禁止的移动。随机性可以增加 TS 算法的鲁棒性。随机性会限制或消除记忆诱发活动的益处，这是禁忌搜索的主要吸引力。鲁棒禁忌搜索的特点是禁止参数在搜索过程中在上限和下限之间随机变化。在固定禁忌搜索中，可以通过随机打破束缚来增加随机性，或者通过一个以上的最佳邻居()函数的候选来获得成本函数的降低。当在反应式禁忌搜索中实现随机性时，观察到相同的效果。

### 固定禁忌搜索

让我们假设我们有一个搜索空间 X，使得![$$ X=\left[{b}_1,\ {b}_2{b}_3\right] $$](../Images/A435493_1_En_8_Chapter_IEq19.gif)具有一个成本函数![$$ f\left(\left[{b}_1,{b}_2,{b}_3\right]\right) = {b}_1+2{b}_2+3{b}_3=7{b}_1{b}_2{b}_3 $$](../Images/A435493_1_En_8_Chapter_IEq20.gif)，其中 b 是一个 3 比特的字符串。可行点将是如图 [8-8](#Fig8) 所示的三维立方体的边缘。给定点的邻域是与边相连的点集。点 X^0 = [0，0，0]与 f(X^0) = 0 是一个局部极小值，因为其他移动产生了更高的成本。

![A435493_1_En_8_Fig8_HTML.jpg](../Images/A435493_1_En_8_Fig8_HTML.jpg)

图 8-8。

A feature space with error function, E, and f value = [x,y,z], using tabu search

我们将定义两个参数，用于测试给定禁忌搜索时期的效率，表示为汉明距离和最小重复间隔。汉明距离描述了沿着搜索轨迹的起始点和最成功点之间的距离，而最小重复间隔描述了沿着给定搜索轨迹访问类似移动的次数。这些参数的方程式如下:

![$$ H\left({X}^{t+1},\ {X}^t\right)=\tau, \kern1em \tau \le T+1, $$](../Images/A435493_1_En_8_Chapter_Equao.gif)

![$$ {X}^{t+ R} = {X}^t\Rightarrow R\ge 2\left( T+1\right) $$](../Images/A435493_1_En_8_Chapter_Equap.gif)

向前看，我们应该注意避免搜索轨迹的吸引子，这里我们把吸引子定义为由确定性局部搜索产生的局部极小值。如果代价函数是下界的，并且从任意点开始，它将终止于局部极小点。我们还定义了所谓的吸引盆地。吸引盆由所有点组成，使得从它们开始的确定性局部搜索轨迹终止于特定的局部极小点。确定性搜索轨迹经常遭受偏向于吸引盆地的痛苦，并且因此可能产生不是全局极小值的结果。为了解决这个问题，给定的搜索点保持接近在搜索轨迹开始时发现的局部极小值。在这之后，搜索轨迹可以搜索关于降低成本函数的更好的吸引流域。一如既往，我们必须意识到存在一些局限性。使用禁忌搜索时，最常遇到的困难是确定适当的禁止参数，并使该技术足够健壮，从而不需要从一个上下文到另一个上下文进行繁琐的调整。这就把我们带到了反应式禁忌搜索，它已经被提出作为解决这些问题的一种方法。

### 反应式禁忌搜索

反应式禁忌搜索(RTS)具有一个禁止参数，该参数通过搜索轨迹内的反应机制来确定。我们一开始用值 1 初始化它，但是我们给它的变化增加了一个确定性的方面。如果有证据表明搜索轨迹需要多样化，T 增加。一旦这个证据不明显，T 就会降低。当我们沿着搜索轨迹重复访问先前的点时，就获得了搜索路径多样化的充分证据，因为它们存储在算法的“存储器”中。此外，为了避免算法非常严格地陷入吸引域的情况，RTS 有一个逃逸机制。这是在给定周期内重复了太多搜索轨迹配置时启动的，其特征是当前搜索路径的随机重新配置。

目标函数 f 最终是搜索轨迹方向的信息来源。因此，下面的算法直接属于这个范例。

### WalkSAT 算法

WalkSAT 算法可以理解为 GSAT 算法的一个更一般化的版本，它是一种局部搜索算法。在该算法中，对于给定的迭代次数，允许有固定数量的机会来找到解决方案。在给定的迭代过程中，算法在两个标准之间选择一个变量。此后，变量被放入翻转函数![$$ \mathrm{FLIP}\left({\mathrm{x}}_{\mathrm{i}}\right)=\left(1-{x}_i\right) $$](../Images/A435493_1_En_8_Chapter_IEq21.gif)。WalkSAT 通过比 GSAT 更少的计算获得能量，因为它在给定时间考虑的参数更少。除此之外，通过确定变量选择的子句的乘积，它因此有机会解决可能阻止收敛到全局最优的问题变量。子句加权也可以合并到 WalkSAT 算法中，这为参数调整和产生的反馈循环提供了新的可能性。下面的算法建议将权重作为一种鼓励优先解决较难子句的方法。几个结构之后，困难的分句被认为是这样的。

### k-最近邻(KNN)

KNN 被认为是基于实例的学习，其特征在于函数的局部近似和分类后发生的所有计算。它也可以用于回归，但通常被描述为一种搜索方法。它的主要优点是相对容易理解，并且在数据模式不规则的情况下是有效的。在分类的情况下，这些模型被认为是基于记忆的，其中我们定义了我们想要考虑的 k 个相邻点。我们对标准化数据使用欧几里德范数来确定给定点与其 k 个邻居之间的距离。这个方程给出为

![$$ d\left( x, y\right) = {\displaystyle \sum_{i=1}^N}\sqrt{{\left({x}_i- y\right)}^2} $$](../Images/A435493_1_En_8_Chapter_Equaq.gif)

其中 I = 1，2，…，N，N =观察总数，x <sub>i</sub> =第 I 次观察，y =我们要分类的特定点。

随着 K 的增加，通常我们会注意到类之间的定义变得不那么严格，导致通常更健壮的模型。就涉及特征选择而言，KNN 可以用作数据预处理技术，通常与其他搜索技术一起用于更精细的特征选择。Tahir、Bouridane 和 Kurugollu 在 2007 年的一篇论文中给出了一个例子，他们使用禁忌搜索和 KNNs 的变体创建了一个混合算法。该算法执行特征加权和选择，产生更准确的分类结果。流水线发生使得特征通过禁忌搜索被选择和加权，并通过 KNN 被分类。如果我们不使用禁忌搜索来执行特征选择，或者根本不执行特征选择，那么更多的噪声将被结合到 KNN 算法的决策过程中。通常情况下，在这里执行特征选择有助于算法在对每个观察值进行分类时做出更精确的选择。

## 摘要

到目前为止，这一章是对所讨论的所有粒度细节的一种元启发。首先，实验设计、特性选择和 A/B 测试对任何数据科学家的职业都至关重要。正确构建实验的能力至关重要，通过这些实验，您可以进行建模，通过修改输入来提高它们的性能，然后定量验证模型的结果。第 9 章讨论了为那些对创建个人或专业使用的构建感兴趣的人提供的硬件解决方案。