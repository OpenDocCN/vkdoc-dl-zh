# 2.数学评论

在讨论机器学习之前，有必要简要概述一下统计学。广义而言，统计是对定量数据的分析和收集，最终目标是对这些数据进行可操作的洞察。也就是说，虽然机器学习和统计学不是同一个领域，但它们密切相关。本章简要概述了与本书后面的讨论相关的术语。

## 统计概念

如果不先讨论概率的概念，任何关于统计学或机器学习的讨论都是不合适的。

### 可能性

概率是对事件发生的可能性的度量。虽然许多机器学习模型倾向于确定性的(基于算法规则)而不是概率性的，但是概率的概念除了在更复杂的深度学习架构(如循环神经网络和卷积神经网络)中引用之外，还特别在诸如期望最大化算法的算法中引用。数学上，这个算法定义如下:

![$$ Probability\ of\ Event\ A = \frac{number\ of\ times\ event\ A\ occurs}{all\ possible\ event s} $$](img/A435493_1_En_2_Chapter_Equa.gif)

这种计算概率的方法代表了频率主义者对概率的观点，在这种观点中，概率基本上是由下面的公式推导出来的。然而，另一个概率学派，贝叶斯，采取了不同的方法。贝叶斯概率理论是基于概率是有条件的假设。换句话说，事件发生的可能性受到当前存在的条件或之前发生的事件的影响。我们在下面的等式中定义条件概率。假设事件 B 已经发生，事件 A 的概率等于:

![$$ P\left( A\Big| B\right)=\frac{P\left( A{\displaystyle \cap } B\right)}{P(B)}, $$](img/A435493_1_En_2_Chapter_Equb.gif)

![$$ Provided\ P(B) > 0\. $$](img/A435493_1_En_2_Chapter_Equc.gif)

在这个等式中，我们将![$$ P\left( A\Big| B\right) $$](img/A435493_1_En_2_Chapter_IEq1.gif)读作“给定 B 的概率”，![$$ P\left( A{\displaystyle \cap } B\right) $$](img/A435493_1_En_2_Chapter_IEq2.gif)读作“A 和 B 的概率。”

也就是说，计算概率并不像看起来那么简单，因为必须经常评估依赖性和独立性。举个简单的例子，假设我们正在评估两个事件 A 和 B 的概率，我们还假设事件 B 发生的概率依赖于 A 的发生。因此，如果 A 不发生，B 发生的概率为 0。数学上，我们将两个事件 A 和 B 的依赖性与独立性定义如下:

![$$ P\left( A\Big| B\right) = P(A) $$](img/A435493_1_En_2_Chapter_Equd.gif)

![$$ P\left( B\Big| A\right)= P(B) $$](img/A435493_1_En_2_Chapter_Eque.gif)

![$$ P\left( A{\displaystyle \cap } B\right) = P(A) P(B) $$](img/A435493_1_En_2_Chapter_Equf.gif)

在图 [2-1](#Fig1) 中，我们可以把事件 A 和 B 想象成两个集合，A 和 B 的并集是两个圆的交点:

![A435493_1_En_2_Fig1_HTML.jpg](img/A435493_1_En_2_Fig1_HTML.jpg)

图 2-1。

Representation of two events (A,B)

如果这个等式在给定的环境中不成立，事件 A 和事件 B 被称为是相关的。

### And vs. Or

通常，当谈到概率时，例如，当评估两个事件 A 和 B 时，通常在“A 和 B 的概率”或“A 或 B 的概率”的上下文中讨论概率。直观地说，我们将这些概率定义为两个不同的事件，因此它们的数学推导是不同的。简单地说，或表示事件概率的相加，而和意味着事件概率的相乘。以下是所需的方程式:

和(概率的乘法定律)是两个事件 A 和 B 相交的概率:

![$$ P\left( A{\displaystyle \cap } B\right) = P(A) P\left( B\Big| A\right) $$](img/A435493_1_En_2_Chapter_Equg.gif)

![$$ = P(B) P\left( A\Big| B\right) $$](img/A435493_1_En_2_Chapter_Equh.gif)

如果事件是独立的，那么

![$$ P\left( A{\displaystyle \cap } B\right)= P(A) P(B) $$](img/A435493_1_En_2_Chapter_Equi.gif)

或者(p 可加性法则)是两个事件 A 和 B 并的概率:

![$$ P\left( A{\displaystyle \cup } B\right) = P(A) + P(B)- P\left( A{\displaystyle \cap } B\right) $$](img/A435493_1_En_2_Chapter_Equj.gif)

符号![$$ P\left( A{\displaystyle \cup } B\right) $$](img/A435493_1_En_2_Chapter_IEq3.gif)的意思是“A 或 b 的概率”

图 [2-2](#Fig2) 说明了这一点。

![A435493_1_En_2_Fig2_HTML.jpg](img/A435493_1_En_2_Fig2_HTML.jpg)

图 2-2。

Representation of events A,B and set S

A 和 B 的概率仅仅是它们各自球面上不相交的部分，而 A 或 B 的概率是这两个部分加上交点的和。我们将 S 定义为我们在给定问题中考虑的所有集合加上这些集合之外的空间的总和。因此，S 的概率总是 1。

话虽如此，A 和 B 之外的空间代表了这些事件的反面。例如，假设 A 和 B 分别代表母亲下午 5 点回家和父亲下午 5 点回家的概率。空白代表他们俩都不会在下午 5 点回家的概率

### 贝叶斯定理

如上所述，贝叶斯统计在机器学习和深度学习领域不断获得赞赏。尽管这些技术通常需要大量的硬编码，但它们的强大之处在于相对简单的理论基础，同时功能强大，适用于各种环境。贝叶斯定理建立在条件概率的概念上，是一个事件 A 的概率与其他类似事件的概率相关的概念:

![$$ P\left({B}_j\Big| A\right)=\frac{P\left( A\Big|{B}_j\right) P\left({B}_j\right)}{\varSigma_i^k P\left( A\Big|{B}_i\right) P\left({B}_i\right)} $$](img/A435493_1_En_2_Chapter_Equk.gif)

在后面的章节中提到，贝叶斯分类器是建立在这个公式以及期望最大化算法的基础上的。

### 随机变量

通常，当分析事件的概率时，我们在一组随机变量内进行。我们将随机变量定义为一个量，其值取决于一组可能的随机事件，每个事件都有相关的概率。它的值在被绘制之前是已知的，但是它也可以被定义为从概率空间映射的函数。通常，我们通过一种叫做随机抽样的方法来抽取这些随机变量。从总体中随机抽样，当每个观察值都以这样的方式被选择，即它和总体中的其他观察值一样有可能被选择时，就说是随机的。

广义地说，读者可能会遇到两种类型的随机变量:离散随机变量和连续随机变量。前者指的是只能取有限个不同值的变量，而后者指的是有无限个可能变量的变量。一个例子是车库里的汽车数量与股票价格百分比变化的理论变化。当分析这些随机变量时，我们通常依赖于读者可能会经常看到的各种统计数据。但是这些统计数据通常直接用于算法中，或者在各个步骤中，或者在评估给定的机器学习或深度学习模型的过程中。

例如，算术平均值直接用于 K-means 聚类等算法，同时也是均方差等模型评估统计的理论基础(本章后面会提到)。直观地说，我们将算术平均值定义为一组离散数字的集中趋势，具体来说，就是这些值的总和除以这些值的个数。数学上，这个等式由下面给出:

![$$ \overline{x}=\frac{1}{N}{\displaystyle \sum_{i=1}^N}{x}_i $$](img/A435493_1_En_2_Chapter_Equl.gif)

广义地说，算术平均值代表随机变量中一组值的最可能值。然而，这不是我们可以用来理解随机变量的唯一一种均值。几何平均值也是一种描述数字序列集中趋势的统计量，但它是通过使用值的乘积而不是总和来获得的。这通常在比较一个序列中的不同项目时使用，尤其是当它们分别具有多个属性时。几何平均值的方程式如下:

![$$ {\left({\displaystyle \prod_{i=1}^n}{x}_i\right)}^{\frac{1}{n}}=\kern0.5em {\left({x}_1*\ {x}_2* \dots *\ {x}_n\right)}^{\frac{1}{n}} $$](img/A435493_1_En_2_Chapter_Equm.gif)

对于那些经常使用时间序列的领域来说，几何平均对于获取特定时间间隔(小时、月、年等)内的变化度量是有用的。也就是说，随机变量的集中趋势并不是描述数据的唯一有用的统计数据。通常，我们希望分析数据围绕最可能值的分散程度。从逻辑上讲，这将我们引向方差和标准差的讨论。这两种统计数据高度相关，但它们有一些关键的区别:方差是标准差的平方值，在各个领域中，标准差通常比方差更有参考价值。在处理后一个区别时，这是因为方差很难直观地描述，而且方差的单位是模糊的。标准偏差以被分析的随机变量为单位，易于可视化。

例如，当评估给定机器学习算法的效率时，我们可以从几个时期得出均方误差。收集这些变量的样本统计数据可能会有所帮助，这样我们就可以了解这些统计数据的离差。数学上，我们将方差和标准差定义如下

### 变化

![$$ {\sigma}^2=\frac{\varSigma {\left( X - \mu \right)}^2}{N} $$](img/A435493_1_En_2_Chapter_Equn.gif)

![$$ V a r(X) = E\left[\right( X- E{\left(\left[ X\right]\right)}^2\Big] $$](img/A435493_1_En_2_Chapter_Equo.gif)

![$$ = E\Big[{X}^2-2 X E\left[ X\right]+{\left( E\left[ X\right]\right)}^2 $$](img/A435493_1_En_2_Chapter_Equp.gif)

![$$ = E\left[{X}^2\right] - 2 E\left[ X\right]\kern0.5em E\left[ X\right]+{\left( E\left[ X\right]\right)}^2 $$](img/A435493_1_En_2_Chapter_Equq.gif)

![$$ = E\left[{X}^2\right]-2 E\left[ X\right] E\left[ X\right]+{\left( E\left[ X\right]\right)}^2 $$](img/A435493_1_En_2_Chapter_Equr.gif)

### 标准偏差

![$$ \sigma =\sqrt{\left(\frac{{\displaystyle {\sum}_i^n}{\left({x}_i-\overline{x}\right)}^2}{n-1}\right)} $$](img/A435493_1_En_2_Chapter_Equs.gif)T2】

此外，协方差可用于测量一个要素的变化对另一个要素的影响程度。数学上，我们定义协方差如下:

![$$ c o v\left( X, Y\right)=\frac{1}{n}{\displaystyle \sum_{i=1}^n}\left({x}_i-\overline{x}\right)\Big({y}_i-\overline{y\Big)} $$](img/A435493_1_En_2_Chapter_Equt.gif)

尽管深度学习在建模具有非线性相关性的变量之间的关系方面取得了重大进展，但一些用于更简单任务的估计器需要这一点作为初步假设。例如，线性回归要求这是一个假设，尽管许多机器学习算法可以对复杂数据进行建模，但有些算法比其他算法更擅长。因此，建议在选择估计量特征之前，使用这些先验统计量检查它们之间的关系。因此，这就引出了对相关系数的讨论，相关系数用来衡量变量之间线性相关的程度。数学上，我们这样定义:

![$$ correlation=\rho =\frac{1}{n}{\displaystyle \sum_{i=1}^n}\frac{\left({x}_i-\overline{x}\right)\left({y}_i-\overline{y\Big)}\right)}{\sqrt{{\left({x}_i-\overline{x}\right)}^2{\left({y}_i-\overline{y\Big)}\right)}^2}} $$](img/A435493_1_En_2_Chapter_Equu.gif)

相关系数的值可以低至–1，高至 1，下限代表相反的相关性，上限代表完全的相关性。从统计学上讲，相关系数为 0 表示完全没有相关性。在评估机器学习模型时，特别是那些执行回归的模型，我们通常会参考决定系数(R 平方)和均方误差(MSE)。我们认为 R 平方是模型的估计回归线与数据分布拟合程度的度量。因此，我们可以说，这个统计量最广为人知的是给定模型的适合度。MSE 测量从模型预测到观察数据的偏差的平方误差的平均值。我们将两者分别定义如下:

### 决定系数(R 的平方)

![$$ {R}^2=1-{\displaystyle \sum_i^n}\frac{{\left({\widehat{y}}_i- y\right)}^2}{{\left({\widehat{y}}_i-\overline{y}\right)}^2} $$](img/A435493_1_En_2_Chapter_Equv.gif)T2】

### 均方误差

![$$ M S E=\frac{1}{n}{\displaystyle \sum_{i=1}^n}{\left({y}_i-\overline{y}\right)}^2 $$](img/A435493_1_En_2_Chapter_Equw.gif)T2】

关于这些值应该是什么，我将在本文后面详细讨论。简而言之，我们通常寻求比其他估计量具有更高的 R 平方值和更低的 MSE 值的模型。

## 线性代数

线性代数的概念在机器学习、数据科学和计算机科学中被大量使用。虽然这不是一个详尽的综述，但所有读者至少应该熟悉以下概念。

### 标量和向量

标量是只有一个属性的值:量值。标量的集合，称为向量，可以有大小和方向。如果在一个给定的向量中有一个以上的标量，我们称之为向量空间的元素。向量空间的区别在于，它是标量序列，可以相加和相乘，并且可以对其执行其他数值运算。向量被定义为 n 个数字的列向量。当我们提到向量的索引时，我们将 I 描述为索引值。比如我们有一个向量 x，那么 x <sub>1</sub> 指的是向量 x 中的第一个值，直观的说，把向量想象成一个类似于文件柜内文件的物体。这个向量中的值是单张纸，向量本身是保存所有这些值的文件夹。

向量是本文中讨论的许多概念的主要构件之一(见图 [2-3](#Fig3) )。例如，在深度学习模型(如 Doc2Vec 和 Word2Vec)中，我们通常将单词和文本文档表示为向量。这种表示允许我们将大量数据压缩成一种易于输入神经网络进行计算的格式。从这种大规模降维中，我们可以确定一个文档与另一个文档的相似度或相异度，或者我们可以获得比简单贝叶斯推理更好的同义词理解。对于已经是数字的数据，向量提供了一种简单的方法来“存储”这些数据，并将其输入到用于相同目的的算法中。向量(和矩阵)的属性，特别是关于数学运算的属性，允许对大量数据进行相对快速的计算，也提供了对数据集中的每个单独值进行手动运算的计算优势。

![A435493_1_En_2_Fig3_HTML.jpg](img/A435493_1_En_2_Fig3_HTML.jpg)

图 2-3。

Representation of a vector

### 向量的性质

向量维数通常由ℝ <sup>n</sup> 或ℝ <sup>m</sup> 表示，其中 n 和 m 是给定向量中值的数量。例如，![$$ x\in {\mathrm{\mathbb{R}}}^5 $$](img/A435493_1_En_2_Chapter_IEq4.gif)表示具有实分量的 5 个向量的集合。虽然我到目前为止只讨论了一个列向量，但是我们也可以有一个行向量。也可以执行将列向量转换为行向量的转换，称为转置。转置是矩阵/向量 X 的变换，使得 X 的行被写成 X <sup>T</sup> 的列，X 的列被写成 X <sup>T</sup> 的行。

#### 添加

让我们定义两个向量![$$ d={\left[{d}_1,\ {d}_2, \dots,\ {d}_n\right]}^T $$](img/A435493_1_En_2_Chapter_IEq5.gif)和![$$ e={\left[{e}_1,{e}_2,\dots,\ {e}_n\right]}^T $$](img/A435493_1_En_2_Chapter_IEq6.gif)其中

![$$ {d}_n={e}_n,\ f o r\ i=1,\ 2, \dots,\ n $$](img/A435493_1_En_2_Chapter_Equx.gif)

因此，矢量之和如下:

![$$ d+ e={\left[\left({d}_1+{e}_1\right),\ \left({e}_2+{d}_2\right), \dots,\ \left({d}_n+{e}_n\right)\right]}^T $$](img/A435493_1_En_2_Chapter_Equy.gif)

#### 减法

假设前一个例子中的假设没有改变，向量 d 和 e 之间的差异如下:

![$$ d- e={\left[\left({d}_1 - {e}_1\right),\ \left({e}_2 - {d}_2\right), \dots,\ \left({d}_n - {e}_n\right)\right]}^T $$](img/A435493_1_En_2_Chapter_Equz.gif)

#### 逐元素乘法

假设前一个例子中的假设没有改变，向量 d 和 e 的乘积如下:

![$$ d* e={\left[\left({d}_1*{e}_1\right),\ \left({e}_2*{d}_2\right), \dots,\ \left({d}_n*{e}_n\right)\right]}^T $$](img/A435493_1_En_2_Chapter_Equaa.gif)

### 公理

设 A、b 和 x 是集合 A 中的一组向量，e 和 d 是 b 中的标量。如果某物是向量空间，则下列公理必须成立:

#### 关联属性

关联属性是指给定表达式中括号的重新排列不会改变最终值:

![$$ x+\left( a+ b\right)=\left( x+ a\right) + b $$](img/A435493_1_En_2_Chapter_Equab.gif)

#### 交换性质

交换性是指改变给定表达式中操作数的顺序不会改变最终值:

![$$ a + b= b+ a $$](img/A435493_1_En_2_Chapter_Equac.gif)

#### 加法的单位元

![$$ a + 0= a,\ f o r\ a ll\ a\in A $$](img/A435493_1_En_2_Chapter_Equad.gif)T2】

哪里![$$ 0\in A $$](img/A435493_1_En_2_Chapter_IEq7.gif)。在这种情况下，0 是零向量，或零向量。

#### 加法的逆元素

在这种情况下，对于每个 a := A，存在一个元素–A:= A，我们将其标记为 a:

![$$ a+\left(- a\right) = 0 $$](img/A435493_1_En_2_Chapter_Equae.gif)

的加法逆

#### 标量乘法的单位元

![$$ (1) a= a $$](img/A435493_1_En_2_Chapter_Equaf.gif)T2】

#### 标量乘法关于向量加法的分配性

![$$ e\left( a+ b\right) = e a+ e b $$](img/A435493_1_En_2_Chapter_Equag.gif)T2】

#### 标量乘法关于场加法的分配性

![$$ \left( a+ b\right) d= ad+ b d $$](img/A435493_1_En_2_Chapter_Equah.gif)T2】

### 子空间

向量空间的子空间是满足向量空间要求的非空子集，特别是线性组合留在子空间中。这个子集在加法和标量乘法下是“封闭的”。最值得注意的是，零向量将属于每个子空间。例如，由支持向量回归产生的超平面之间的空间就是子空间的一个例子，支持向量回归是一种机器学习算法，我将在后面介绍。在这个子空间中是响应变量的可接受值。

### 矩阵

矩阵是我们数学复习中线性代数的另一个基本概念。简单地说，矩阵是以行和列排列的数字、符号或表达式的矩形阵列。矩阵有多种用途，但特别是经常用于存储数字数据。例如，当使用卷积神经网络执行图像识别时，我们将照片中的像素表示为三维矩阵中的数字，该矩阵表示由彩色照片组成的红色、绿色和蓝色照片的矩阵。通常，我们将一个单独的像素取为 256 个单独的值，通过这种数学解释，一种难以理解的数据表示成为可能。关于向量和标量，矩阵包含每个单独值的标量，由行和列向量组成。当我们索引一个给定的矩阵 A 时，我们将使用符号 A <sub>ij</sub> 。我们也称之为![$$ A={a}_{ij,\kern0.5em }\ A\in {\mathrm{\mathbb{R}}}^{m\ x\ n} $$](img/A435493_1_En_2_Chapter_IEq8.gif)。

#### 矩阵属性

根据矩阵是向量的组合的定义，矩阵本身具有许多与向量相同的基本性质。但是，有一些重要的关键差异，特别是关于矩阵乘法。例如，矩阵乘法是理解普通最小二乘回归如何工作的关键要素，也是从根本上理解为什么我们在执行线性回归时会对使用梯度下降感兴趣。也就是说，矩阵的性质将在本节的其余部分讨论。

#### 添加

假设 A 和 B 都是 m×n 维矩阵:

![$$ A+ B=\left({A}_{ij}+{B}_{ij}\right),\ f o r\ i=1,2,\dots,\ n $$](img/A435493_1_En_2_Chapter_Equai.gif)

#### 纯量乘法

让我们假设 A 和 B 都是 m×n 维矩阵

![$$ AB=\left({A}_{ij}*{B}_{ij}\right),\ f o r\ i=1,2,\dots,\ n $$](img/A435493_1_En_2_Chapter_Equaj.gif)

#### 调换

![$$ {A}_{ij}^T={A}_{ji} $$](img/A435493_1_En_2_Chapter_Equak.gif)T2】

#### 矩阵的类型

矩阵有多种形式，通常用它们呈现的形状来表示。虽然矩阵可以有多个维度，但有许多维度通常会被引用。其中最简单的是方阵，它的区别在于它有相等数量的行和列:

![$$ \mathbf{A} = \left[\begin{array}{ccc}\hfill {a}_{1,1\ }\hfill & \hfill {a}_{1,2}\kern0.5em {a}_{1,3}\cdots \hfill & \hfill {a}_{1, n}\hfill \\ {}\hfill \vdots \hfill & \hfill \ddots \hfill & \hfill \vdots \hfill \\ {}\hfill {a}_{n,1}\hfill & \hfill {a}_{n,2}{a}_{n,3} \cdot s \hfill & \hfill {a}_{n, n}\hfill \end{array}\right] $$](img/A435493_1_En_2_Chapter_Equal.gif)

一般来说，读者不太可能碰到正方形矩阵，但是矩阵性质的含义使得讨论它是必要的。也就是说，这使我们讨论不同类型的矩阵，如对角矩阵和单位矩阵。对角矩阵是这样一种矩阵，其中不沿着矩阵的主对角线(从左上角到右下角)的所有条目都是零，由下面给出:

![$$ A=\kern0.75em \begin{array}{ccc}\hfill 5\hfill & \hfill 0\hfill & \hfill 0\hfill \\ {}\hfill 0\hfill & \hfill 4\hfill & \hfill 0\hfill \\ {}\hfill 0\hfill & \hfill 0\hfill & \hfill 3\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equam.gif)

类似于对角矩阵，单位矩阵也具有沿着除矩阵对角线之外的所有条目的值的零。然而，这里的关键区别在于对角矩阵中的所有元素都是 1。这个矩阵由下图给出:

![$$ {I}_n=\kern1.25em \begin{array}{ccc}\hfill 1\hfill & \hfill 0\hfill & \hfill 0\hfill \\ {}\hfill 0\hfill & \hfill 1\hfill & \hfill 0\hfill \\ {}\hfill 0\hfill & \hfill 0\hfill & \hfill 1\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equan.gif)

另一个你可能看不到，但从理论角度来看很重要的矩阵是对称矩阵，它的转置等于非变换矩阵。我将在本章中描述转置，但它可以简单地理解为将行转换成列，反之亦然。

我将定义的矩阵的最终类型，特别是牛顿法(第 [3](03.html) 章描述的一种优化方法)中提到的，是正定和半正定矩阵。如果所有元素都大于零，则称对称矩阵为正定矩阵。但是如果所有的值都是非负的，这个矩阵叫做正半定。尽管在下一章中会有更详细的描述，但这对于理解一个问题是否有全局最优解(以及牛顿法是否可以用来寻找这个全局最优解)是很重要的。

#### 矩阵乘法

与向量不同，矩阵乘法包含独特的规则，这将有助于计划应用这些知识的读者，尤其是那些使用编程语言的读者。例如，假设我们有两个矩阵，A 和 B，我们想把它们相乘。这些矩阵只有在 A 中的列数与 b 中的行数相同的条件下才能相乘。我们称这个矩阵乘积为矩阵 A 和 b 的点积。下面几节讨论矩阵乘法及其乘积的例子。

#### 纯量乘法

假设我们有一个矩阵 A，我们想用它乘以标量值σ。该操作的结果如下图所示:

![$$ \sigma A=\sigma \left[\begin{array}{ccc}\hfill {A}_{1,1}\hfill & \hfill {A}_{1,2\ }\kern0.5em \cdots \hfill & \hfill {A}_{1, m}\hfill \\ {}\hfill \vdots \hfill & \hfill \ddots \hfill & \hfill \vdots \hfill \\ {}\hfill {A}_{n,1}\hfill & \hfill {A}_{n,2}\kern0.75em \cdots \hfill & \hfill {A}_{n, m}\hfill \end{array}\right]=\left[\begin{array}{ccc}\hfill \sigma {A}_{1,1}\hfill & \hfill \sigma {A}_{1,2}\kern0.75em \cdots \hfill & \hfill \sigma {A}_{1, m}\hfill \\ {}\hfill \vdots \hfill & \hfill \ddots \hfill & \hfill \vdots \hfill \\ {}\hfill \sigma {A}_{n,1}\hfill & \hfill \sigma {A}_{n,2}\kern0.75em \cdots \hfill & \hfill \sigma {A}_{n, m}\hfill \end{array}\right] $$](img/A435493_1_En_2_Chapter_Equao.gif)

矩阵中的每个值乘以随后产生的新矩阵中的标量。具体来说，我们可以看到这种关系显示在以下与本征分解相关的方程中。

#### 矩阵乘矩阵乘法

矩阵乘法用于几种回归方法，特别是 OLS、岭回归和 LASSO。这是一种高效而简单的方式来表示独立数据集上的数学运算。在下面的例子中，设 D 是一个 n×m 矩阵，E 是一个 m×p 矩阵，这样当我们将它们相乘时，我们得到如下:

![$$ D = \left[\begin{array}{ccc}\hfill {D}_{1,1}\hfill & \hfill {D}_{1,2\ }\kern0.5em \cdots \hfill & \hfill {D}_{1, m}\hfill \\ {}\hfill \vdots \hfill & \hfill \ddots \hfill & \hfill \vdots \hfill \\ {}\hfill {D}_{n,1}\hfill & \hfill {D}_{n,2}\kern0.75em \cdots \hfill & \hfill {D}_{n, m}\hfill \end{array}\right],\ E = \left[\begin{array}{ccc}\hfill {E}_{1,1}\hfill & \hfill {E}_{1,2\ }\kern0.5em \cdots \hfill & \hfill {E}_{1, p}\hfill \\ {}\hfill \vdots \hfill & \hfill \ddots \hfill & \hfill \vdots \hfill \\ {}\hfill {E}_{m,1}\hfill & \hfill {E}_{n,2}\kern0.75em \cdots \hfill & \hfill {E}_{m, p}\hfill \end{array}\right] $$](img/A435493_1_En_2_Chapter_Equap.gif)

![$$ D E = \left[\begin{array}{ccc}\hfill D{E}_{1,1}\hfill & \hfill D{E}_{1,2}\kern0.5em \cdots \hfill & \hfill D{E}_{1, p}\hfill \\ {}\hfill \vdots \hfill & \hfill \ddots \hfill & \hfill \vdots \hfill \\ {}\hfill D{E}_{n,1}\hfill & \hfill D{E}_{n,2}\cdots \hfill & \hfill D{E}_{n, p}\hfill \end{array}\right] $$](img/A435493_1_En_2_Chapter_Equaq.gif)

假设维数相等，一个矩阵中的每个元素乘以另一个元素的相应元素，得到一个新矩阵。尽管浏览这些例子似乎毫无意义，但它实际上比看起来更重要——特别是因为所有的操作都将由计算机执行。如果只是为了调试代码中的错误，读者应该熟悉矩阵乘法的乘积。我们将看到不同的矩阵运算，它们也将在以后不同的上下文中出现。

#### 行列向量乘法

对于那些想知道矩阵乘法如何精确地产生单个标量值的人，下一节将对此进行进一步的阐述。如果

![$$ \boldsymbol{X}=\left(\begin{array}{ccc}\hfill \boldsymbol{x}\hfill & \hfill \boldsymbol{y}\hfill & \hfill \boldsymbol{z}\hfill \end{array}\right),\kern1em \boldsymbol{Y} = \begin{array}{c}\hfill \boldsymbol{d}\hfill \\ {}\hfill \boldsymbol{e}\hfill \\ {}\hfill \boldsymbol{f}\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equar.gif)

那么它们的矩阵乘积由下面给出:

![$$ \boldsymbol{X}\boldsymbol{Y}=\left(\begin{array}{ccc}\hfill \boldsymbol{x}\hfill & \hfill \boldsymbol{y}\hfill & \hfill \boldsymbol{z}\hfill \end{array}\right)\begin{array}{c}\hfill \boldsymbol{d}\hfill \\ {}\hfill \boldsymbol{e}\hfill \\ {}\hfill \boldsymbol{f}\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equas.gif)

![$$ \boldsymbol{X}\boldsymbol{Y}=\boldsymbol{x}\boldsymbol{d} + \boldsymbol{y}\boldsymbol{e}+\boldsymbol{z}\boldsymbol{f} $$](img/A435493_1_En_2_Chapter_Equat.gif)

对比:

![$$ \boldsymbol{Y}\boldsymbol{X} = \begin{array}{c}\hfill \boldsymbol{d}\hfill \\ {}\hfill \boldsymbol{e}\hfill \\ {}\hfill \boldsymbol{f}\hfill \end{array}\left(\begin{array}{ccc}\hfill \boldsymbol{x}\hfill & \hfill \boldsymbol{y}\hfill & \hfill \boldsymbol{z}\hfill \end{array}\right) $$](img/A435493_1_En_2_Chapter_Equau.gif)

![$$ \boldsymbol{Y}\boldsymbol{X} = \begin{array}{ccc}\hfill \boldsymbol{dx}\hfill & \hfill \boldsymbol{dy}\hfill & \hfill \boldsymbol{dz}\hfill \\ {}\hfill \boldsymbol{ex}\hfill & \hfill \boldsymbol{ey}\hfill & \hfill \boldsymbol{ez}\hfill \\ {}\hfill \boldsymbol{fx}\hfill & \hfill \boldsymbol{fy}\hfill & \hfill \boldsymbol{fz}\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equav.gif)

#### 列向量和方阵

在某些情况下，我们需要用整个矩阵乘以一个列向量。在这个实例中，以下成立:

![$$ \boldsymbol{B}=\begin{array}{ccc}\hfill 1\hfill & \hfill 2\hfill & \hfill 3\hfill \\ {}\hfill 4\hfill & \hfill 5\hfill & \hfill 6\hfill \\ {}\hfill 7\hfill & \hfill 8\hfill & \hfill 9\hfill \end{array},\kern1em \boldsymbol{C} = \begin{array}{c}\hfill \boldsymbol{d}\hfill \\ {}\hfill \boldsymbol{e}\hfill \\ {}\hfill \boldsymbol{f}\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equaw.gif)

B 和 C 的矩阵乘积由下式给出:

![$$ \boldsymbol{Y}\boldsymbol{X} = \begin{array}{ccc}\hfill 1\boldsymbol{d}\hfill & \hfill 2\boldsymbol{d}\hfill & \hfill 3\boldsymbol{d}\hfill \\ {}\hfill 4\boldsymbol{e}\hfill & \hfill 5\boldsymbol{e}\hfill & \hfill 6\boldsymbol{e}\hfill \\ {}\hfill 7\boldsymbol{f}\hfill & \hfill 8\boldsymbol{f}\hfill & \hfill 9\boldsymbol{f}\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equax.gif)

#### 正方形矩阵

其中最简单的矩阵运算是当我们处理两个方阵时，如下:

![$$ \boldsymbol{B}=\begin{array}{ccc}\hfill 1\hfill & \hfill 2\hfill & \hfill 3\hfill \\ {}\hfill 4\hfill & \hfill 5\hfill & \hfill 6\hfill \\ {}\hfill 7\hfill & \hfill 8\hfill & \hfill 9\hfill \end{array}, \kern0.5em \boldsymbol{D}=\begin{array}{ccc}\hfill 9\hfill & \hfill 8\hfill & \hfill 7\hfill \\ {}\hfill 6\hfill & \hfill 5\hfill & \hfill 4\hfill \\ {}\hfill 3\hfill & \hfill 2\hfill & \hfill 1\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equay.gif)

![$$ \boldsymbol{B}\boldsymbol{D}=\begin{array}{ccc}\hfill 1\hfill & \hfill 2\hfill & \hfill 3\hfill \\ {}\hfill 4\hfill & \hfill 5\hfill & \hfill 6\hfill \\ {}\hfill 7\hfill & \hfill 8\hfill & \hfill 9\hfill \end{array}\kern0.75em \boldsymbol{x}\kern0.75em \begin{array}{ccc}\hfill 9\hfill & \hfill 8\hfill & \hfill 7\hfill \\ {}\hfill 6\hfill & \hfill 5\hfill & \hfill 4\hfill \\ {}\hfill 3\hfill & \hfill 2\hfill & \hfill 1\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equaz.gif)

![$$ =\kern0.75em \begin{array}{ccc}\hfill \left(1*9\right)+\left(2*6\right)+\left(3*3\right)\hfill & \hfill \left(1*8\right)+\left(2*5\right)+\left(3*2\right)\hfill & \hfill \left(1*7\right)+\left(2*4\right)+\left(3*1\right)\hfill \\ {}\hfill \left(4*9\right)+\left(5*6\right)+\left(6*3\right)\hfill & \hfill \left(4*8\right)+\left(5*5\right)+6*2\Big)\hfill & \hfill \left(4*7\right)+\left(5*4\right)+\left(6*1\right)\hfill \\ {}\hfill \left(7*9\right)+\left(8*6\right)+\left(9*3\right)\hfill & \hfill \left(7*8\right)+\left(8*5\right)+\left(9*2\right)\hfill & \hfill \left(7*7\right)+\left(8*4\right)+\left(9*1\right)\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equba.gif)

![$$ \boldsymbol{B}\boldsymbol{D}=\begin{array}{ccc}\hfill 30\hfill & \hfill 24\hfill & \hfill 18\hfill \\ {}\hfill 84\hfill & \hfill 69\hfill & \hfill 54\hfill \\ {}\hfill 138\hfill & \hfill 114\hfill & \hfill 90\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equbb.gif)

按此逻辑:

![$$ \boldsymbol{D}\boldsymbol{B} = \begin{array}{ccc}\hfill 90\hfill & \hfill 114\hfill & \hfill 138\hfill \\ {}\hfill 54\hfill & \hfill 69\hfill & \hfill 84\hfill \\ {}\hfill 18\hfill & \hfill 24\hfill & \hfill 30\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equbc.gif)

#### 行向量、方阵和列向量

在其他情况下，我们将对每个具有不同形状的矩阵/向量执行操作:

![$$ \mathbf{A}\kern0.5em =\begin{array}{ccc}\hfill 9\hfill & \hfill 8\hfill & \hfill 7\hfill \\ {}\hfill 6\hfill & \hfill 5\hfill & \hfill 4\hfill \\ {}\hfill 3\hfill & \hfill 2\hfill & \hfill 1\hfill \end{array},\kern0.5em \mathbf{B} = \begin{array}{ccc}\hfill 1\hfill & \hfill 2\hfill & \hfill 3\hfill \end{array}, \kern0.75em \mathbf{C} = \begin{array}{c}\hfill 4\hfill \\ {}\hfill 5\hfill \\ {}\hfill 6\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equbd.gif)

![$$ \mathbf{A}\mathbf{B}\mathbf{C} = \begin{array}{ccc}\hfill 9\hfill & \hfill 8\hfill & \hfill 7\hfill \\ {}\hfill 6\hfill & \hfill 5\hfill & \hfill 4\hfill \\ {}\hfill 3\hfill & \hfill 2\hfill & \hfill 1\hfill \end{array} x\begin{array}{ccc}\hfill \kern0.5em 1\hfill & \hfill 2\hfill & \hfill 3\hfill \end{array} x\begin{array}{c}\hfill 4\hfill \\ {}\hfill 5\hfill \\ {}\hfill 6\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Eqube.gif)

![$$ \begin{array}{ccc}\hfill 9\hfill & \hfill 8\hfill & \hfill 7\hfill \\ {}\hfill 6\hfill & \hfill 5\hfill & \hfill 4\hfill \\ {}\hfill 3\hfill & \hfill 2\hfill & \hfill 1\hfill \end{array}\ \mathbf{X}\kern0.75em \begin{array}{ccc}\hfill \kern0.5em 4\hfill & \hfill 10\hfill & \hfill 18\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equbf.gif)

![$$ \boldsymbol{A}\boldsymbol{B}\boldsymbol{C} = \begin{array}{ccc}\hfill 36\hfill & \hfill 32\hfill & \hfill 28\hfill \\ {}\hfill 60\hfill & \hfill 50\hfill & \hfill 40\hfill \\ {}\hfill 54\hfill & \hfill 36\hfill & \hfill 18\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equbg.gif)

#### 矩形矩阵

我们最后的例子处理矩形矩阵。对于这个例子，我们有两个矩阵 Z 和 Y，例如:

![$$ \mathbf{Z}\kern0.75em =\begin{array}{ccc}\hfill 1\hfill & \hfill 2\hfill & \hfill 3\hfill \\ {}\hfill 4\hfill & \hfill 5\hfill & \hfill 6\hfill \end{array}\kern0.75em ,\kern1em \mathbf{Y}=\kern0.75em \begin{array}{cc}\hfill 9\hfill & \hfill 8\hfill \\ {}\hfill 7\hfill & \hfill 6\hfill \\ {}\hfill 5\hfill & \hfill 4\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equbh.gif)

![$$ \mathbf{Z}\mathbf{Y}=\begin{array}{ccc}\hfill 1\hfill & \hfill 2\hfill & \hfill 3\hfill \\ {}\hfill 4\hfill & \hfill 5\hfill & \hfill 6\hfill \end{array} x\begin{array}{cc}\hfill 9\hfill & \hfill 8\hfill \\ {}\hfill 7\hfill & \hfill 6\hfill \\ {}\hfill 5\hfill & \hfill 4\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equbi.gif)

![$$ =\kern0.75em \begin{array}{cc}\hfill 9\hfill & \hfill 40\hfill \\ {}\hfill 28\hfill & \hfill 18\hfill \\ {}\hfill 10\hfill & \hfill 240\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equbj.gif)

#### 矩阵乘法属性(两个矩阵)

#### 不可交换

一般情况下，给定两个矩阵 A 和 B，AB ≠ BA，AB 和 BA 可能不同时定义，即使同时定义，也仍然可能不相等。这与普通的数字乘法相反。例如，要口头指定矩阵乘法的顺序，A 与 B 前乘意味着 BA，而 A 与 C 后乘意味着 AC。只要矩阵的元素来自一个有单位元且 n > 1 的环，则该环上存在一对 n×n 非交换矩阵。一个显著的例外是单位矩阵，因为它与每个方阵互换。

#### 分配超矩阵加法

矩阵中的分配性遵循与向量中相同的逻辑。因此，以下公理成立:

左分配性:

![$$ A\left( B+ C\right) = AB+ B C $$](img/A435493_1_En_2_Chapter_Equbk.gif)

右分配性:

![$$ \left( A+ B\right) C= AC+ B C $$](img/A435493_1_En_2_Chapter_Equbl.gif)

这些操作的指标符号分别如下:

![$$ {\varSigma}_k{A}_{ik}\left({B}_{k j}+{C}_{k j}\right)={\varSigma}_k{A}_{ik}{B}_{k j}+{\varSigma}_k{A}_{ik}{C}_{k j} $$](img/A435493_1_En_2_Chapter_Equbm.gif)

![$$ {\varSigma}_k\left({A}_{ik}+{B}_{ik}\right){C}_{k j}={\varSigma}_k{A}_{ik}{C}_{k j}+{\varSigma}_k{B}_{ik}{C}_{k j} $$](img/A435493_1_En_2_Chapter_Equbn.gif)

#### 标量乘法与矩阵乘法兼容

根据我们前面关于矩阵的标量乘法的讨论，我们在这里看到矩阵的标量乘法的分配性也成立。例如，我们有下面的等式，它是这样证明的:

![$$ \lambda (AB)=\left(\lambda A\right) B $$](img/A435493_1_En_2_Chapter_Equbo.gif)

![$$ (AB)\lambda = A\left( B\lambda \right) $$](img/A435493_1_En_2_Chapter_Equbp.gif)

λ是标量。如果矩阵的元素是实数或复数，那么所有四个量都相等。更一般地，如果λ属于矩阵的元素环的中心，则所有四个都相等，因为在这种情况下![$$ \lambda X= X\lambda $$](img/A435493_1_En_2_Chapter_IEq9.gif)。

这是如下的索引符号:

![$$ \lambda {\varSigma}_K\left({A}_{ik}{B}_{k j}\right)={\varSigma}_k\left(\lambda {A}_{ik}\right){B}_{k j}=\varSigma {A}_{ik}\left(\lambda {B}_{k j}\right) $$](img/A435493_1_En_2_Chapter_Equbq.gif)

![$$ {\varSigma}_k\left({A}_{ik}{B}_{k j}\right)\lambda =\varSigma \left({A}_{ik}\lambda \right){B}_{k j}={\varSigma}_k{A}_{ik}\left({B}_{k j}\lambda \right) $$](img/A435493_1_En_2_Chapter_Equbr.gif)

#### 移项

如前所述，矩阵的转置是对矩阵的操作，其中该变换的乘积是新矩阵，其中新矩阵的行是原始矩阵的列，新矩阵的列是原始矩阵的行。给定两个矩阵 A 和 B

![$$ {(AB)}^T=\kern0.5em {B}^T{A}^T $$](img/A435493_1_En_2_Chapter_Equbs.gif)

，下面的等式显示了我们如何表示这种变换

其中 T 表示转置，即矩阵中第 I 行与第 I 列的互换。这个恒等式适用于交换环上的任何矩阵，但不适用于一般的所有环。注意 A 和 B 是颠倒的。

指数表示法:![$$ {\left[{(AB)}^T\right]}_{ij}={(AB)}_{ji} $$](img/A435493_1_En_2_Chapter_Equbt.gif)

![$$ ={\varSigma}_K{(A)}_{jk}{(B)}_{ki} $$](img/A435493_1_En_2_Chapter_Equbu.gif)

![$$ ={\left[\left({B}^T\right)\left({A}^T\right)\right]}_{ij} $$](img/A435493_1_En_2_Chapter_Equbx.gif)

#### 微量

乘积 AB 的迹与 A 和 b 的阶无关，迹也可以认为是一个矩阵的对角线:

![$$ t r(AB) = t r(BA) $$](img/A435493_1_En_2_Chapter_Equby.gif)

指数表示法:![$$ t r(AB)={\varSigma}_i{\varSigma}_k{A}_{i k}{B}_{i k} $$](img/A435493_1_En_2_Chapter_Equbz.gif)

![$$ ={\varSigma}_k{\varSigma}_i{B}_{k i}{A}_{i k} $$](img/A435493_1_En_2_Chapter_Equca.gif)

![$$ = t r(BA) $$](img/A435493_1_En_2_Chapter_Equcb.gif)

#### 规范

范数是为向量空间中的每个向量分配严格正的长度或大小的函数。在机器学习中，你会遇到许多不同的规范，除了增加分类模型的准确性之外，它们在减少回归模型的 MSE 方面也起着至关重要的作用。例如，岭回归使用 L2 范数在高度多重共线性期间缩小回归系数，而 LASSO 使用 L1 范数将某些回归系数缩小为零。我将在第 3 章详细回顾这两个回归模型。

在深度学习的背景下，在深度神经网络中添加不同层的实验(其中规范用于对数据执行维度缩减)在一些任务中被证明是成功的。例如，在卷积神经网络中使用 L2 范数层。但是这也可以用作多层感知器中的相异/损失度量，而不是传统的梯度函数。

#### 欧几里得范数

这描述了在ℝ <sup>n</sup> 的欧几里得空间中一个向量的距离。让我们假设![$$ x=\left({x}_1,{x}_2,\dots,\ {x}_n\right) $$](img/A435493_1_En_2_Chapter_IEq10.gif)。

#### L2 常模

这给出了向量内原点到 x 内最后一点的距离，通常称为 L2 范数:

![$$ {\left|\left| x\right|\right|}_2^2=\sqrt{x_1^2 + {x}_2^2+\dots +{x}_n^2} $$](img/A435493_1_En_2_Chapter_Equcc.gif)

#### L1 常模

这和 L2 范数是一样的，除了标量不是平方的:

![$$ \left|\left| x\right|\right| = \sqrt{x_1+{x}_2+\dots +{x}_n} $$](img/A435493_1_En_2_Chapter_Equcd.gif)

L1 和 L2 标准的形状如图 [2-4](#Fig4) 所示。

![A435493_1_En_2_Fig4_HTML.jpg](img/A435493_1_En_2_Fig4_HTML.jpg)

图 2-4。

L1 and L2 norm shapes

注意，在 L1 范数下，我们观察到的是正方形(或立方体)，而在 L2 范数下，我们观察到的是圆形(或球形)。在某些情况下，最好在执行回归分析的同时使用 L1 范数来执行变量选择，但是这个问题并不总是存在，我将在第 [8](08.html) 章中进一步详细讨论。

使用 L1 范数的优势是显而易见的，因为您可以在执行回归的同时执行特征选择。然而，应该注意，在数据集的缩减已经发生之后执行特征选择会鼓励过拟合。第 [8](08.html) 章更广泛地回顾了构建稳健模型的策略和一般实践，但通常建议读者在将数据拟合到模型之前，在很少或没有进行特征选择的情况下使用 L1 规范。

对于那些对车辆路线问题感兴趣的人来说，出租车(曼哈顿)规范与那些想要关注与运输和/或递送或包裹/人员相关的领域的人相关。出租车定额描述了出租车沿给定城市街区行驶的距离:

![$$ \left|\left| x\right|\right|={\varSigma}_i\left|{x}_i\right|,\ f o r\ i=1,2, \dots,\ n $$](img/A435493_1_En_2_Chapter_Equce.gif)

绝对值范数是由实数或复数形成的一维向量空间上的范数。绝对值范数已被用来代替其他损失函数或相异函数:

![$$ \left|\left| x\right|\right|=\left| x\right| $$](img/A435493_1_En_2_Chapter_Equcf.gif)

#### p-范数

设 p ≥ 1 为实数:

![$$ {\left|\left| x\right|\right|}_p=\varSigma {\left({\left|{x}_i\right|}^p\right)}^{\frac{1}{p}} $$](img/A435493_1_En_2_Chapter_Equcg.gif)

该定额的形状如图 [2-5](#Fig5) 所示。

![A435493_1_En_2_Fig5_HTML.jpg](img/A435493_1_En_2_Fig5_HTML.jpg)

图 2-5。

P-norm

对于![$$ p=1 $$](img/A435493_1_En_2_Chapter_IEq11.gif),我们得到出租车范数，对于![$$ p=2, $$](img/A435493_1_En_2_Chapter_IEq12.gif),我们得到欧几里德范数，对于![$$ p\to \infty $$](img/A435493_1_En_2_Chapter_IEq13.gif),我们得到无穷范数或最大范数。p-范数与广义均值或幂均值相关。然而当![$$ 0< p<1 $$](img/A435493_1_En_2_Chapter_IEq14.gif)时，我们没有得到离散定义的范数，因为它违反了三角形不等式。三角形不等式表明，三角形的任何一条边都必须小于或等于其他两条边之和。

#### 矩阵范数

矩阵范数是来自![$$ {\mathrm{\mathbb{R}}}^{nxn}\to \mathrm{\mathbb{R}} $$](img/A435493_1_En_2_Chapter_IEq15.gif)的函数，它满足给定数量的属性，用||A||表示，给定矩阵 A。

这些属性如下:

1.  ![$$ \left|\left| A\right|\right| > 0\kern0.5em f o r\ all\ M\in {\mathrm{\mathbb{R}}}^{nxn}\ and\ also\ \left|\left| A\right|\right| = 0\ i i f\ A=0 $$](img/A435493_1_En_2_Chapter_Equch.gif)

2.  ![$$ \left|\left| a M\right|\right|=\left| a\right|*\left|\left| M\right|\right|\kern0.5em f o r\ a ll\ a\in {\mathrm{\mathbb{R}}}^n $$](img/A435493_1_En_2_Chapter_Equci.gif)

3.  ![$$ \left|\left| M+ N\right|\right|\le \left|\left| M\right|\right|+\left|\left| N\right|\right| $$](img/A435493_1_En_2_Chapter_Equcj.gif)

4.  ![$$ \left|\left| M N\right|\right|\le \left|\left| M\right|\right|*\ \left|\left| N\right|\right| $$](img/A435493_1_En_2_Chapter_Equck.gif)

#### 内部产品

机器学习文献中经常提到的一种重要的向量空间是内积。向量空间的这个元素允许某人知道向量的长度或者两个向量之间的角度。此外，还可以从内积中确定赋范向量空间。具体地，内积是在支持向量机的内核中使用的函数，用于计算支持向量机从输入空间放入特征空间的数据的图像。的内积空间是一个函数〈.,.〉定义如下，其中 u 和 v 是矢量，![$$ u=\left[{u}_1,{u}_2, \dots,\ {u}_n\left],\ v=\right[{v}_1,{v}_2,\dots,\ {v}_n\right] $$](img/A435493_1_En_2_Chapter_IEq16.gif) :

![$$ \left\langle u, v\right\rangle ={u}_1{v}_1+{u}_2{v}_2+\dots +{u}_n{v}_n\kern0.5em f o r\ i=1,2, \dots,\ n $$](img/A435493_1_En_2_Chapter_Equcl.gif)

对于要成为内积的函数，它必须满足三个公理:

共轭对称:

![$$ \left\langle u, v\right\rangle = \left\langle v, u\right\rangle $$](img/A435493_1_En_2_Chapter_Equcm.gif)

第一个自变量中的线性:

![$$ \left\langle au+ b v, w\right\rangle = a\left\langle u, w\right\rangle + b\left\langle v, w\right\rangle $$](img/A435493_1_En_2_Chapter_Equcn.gif)

正定性:

![$$ F o r\ any\ u\in V,\ \left\langle u, u\right\rangle \ge 0; and\ \left\langle u, u\right\rangle =0\ o nly\ if\ u=0 $$](img/A435493_1_En_2_Chapter_Equco.gif)

#### 内积空间上的范数

内积空间自然具有定义的范数，该范数基于空间本身的范数，由以下给出:

![$$ \left|\left| x\right|\right|=\sqrt{x} $$](img/A435493_1_En_2_Chapter_Equcp.gif)

直接从公理出发，我们可以证明如下:柯西-施瓦茨不等式陈述对于一个内积空间的所有向量 u 和 v，以下为真:

![$$ {\left|\left|\left\langle u, v\right\rangle \right|\right|}^2\le\ \left\langle u, u\right\rangle *\left\langle v, v\right\rangle $$](img/A435493_1_En_2_Chapter_Equcq.gif)

![$$ \left|\left\langle u, v\right\rangle \right|\le \left|\left| u\right|\right|*\left|\left| v\right|\right| $$](img/A435493_1_En_2_Chapter_Equcr.gif)

只有当且仅当 u 和 v 线性相关时，两边才被视为相等，这意味着它们必须平行，其中一个向量的大小为零，或者一个向量是另一个向量的标量乘数。

#### 证明

第一个证明:展开括号并收集相同的项得到下面的等式:

![$$ {\varSigma}_i^n{\varSigma}_j^n{\left({a}_i{b}_j-{a}_j{b}_i\right)}^2=\left({\varSigma}_i^n{a}_i^2\right){\varSigma}_j^n{b}_j^2 + \left({\varSigma}_i^n{b}_i^2\right){\varSigma}_j^n{a}_j^2-2\left(\varSigma {a}_i{b}_i\right){\varSigma}_j^n{b}_j{a}_j $$](img/A435493_1_En_2_Chapter_Equcs.gif)

![$$ =2\left({\varSigma}_i^n{a}_i^2\right)\left({\varSigma}_i^n{b}_i^2\right)-2{\left({\varSigma}_i^n{a}_i{b}_i\right)}^2 $$](img/A435493_1_En_2_Chapter_Equct.gif)

因为等式的左边是实数的平方和，所以它大于或等于零。照此，以下必然成立:

![$$ \left({\varSigma}_i^n{a}_i^2\right)\left({\varSigma}_i^n{b}_i^2\right)\ge\ {\left({\varSigma}_i^n{a}_i{b}_i\right)}^2 $$](img/A435493_1_En_2_Chapter_Equcu.gif)

第二个证明:考虑以下二次多项式方程:

![$$ f(x)=\left({\varSigma}_i^n{a}_i^2\right){x}^2-2\left({\varSigma}_i^n{a}_i{b}_i\right) x+{\varSigma}_i^n{b}_i^2=\varSigma {\left({a}_i x-{b}_i\right)}^2 $$](img/A435493_1_En_2_Chapter_Equcv.gif)

因为![$$ f(x)\ge\ 0\ f o r\ \mathrm{any}\ x\in \mathrm{\mathbb{R}}, $$](img/A435493_1_En_2_Chapter_IEq17.gif)接下来 f(x)的判别式是负的，因此下面的情况一定是:

![$$ {\left({\varSigma}_i^n{a}_i{b}_i\right)}^2-\left({\varSigma}_i^n{a}_i^2\right)\left({\varSigma}_i^n{b}_i^2\right)\le\ 0 $$](img/A435493_1_En_2_Chapter_Equcw.gif)

第三个证明:考虑下面两个欧几里德范数 A 和 B:

![$$ Let\ A = \sqrt{a_1^2+{a}_2^2+\dots +{a}_n^2},\kern0.75em B = \sqrt{b_1^2+{b}_2^2+\dots +{b}_n^2} $$](img/A435493_1_En_2_Chapter_Equcx.gif)

由算术-几何平均不等式，我们得到

![$$ \frac{\varSigma_i^n\left({a}_i{b}_i\right)}{ A B}\le {\varSigma}_i^n\left(\frac{1}{2}\right)\left(\left(\frac{a_i^2}{A^2}\right)+\left(\frac{b_i^2}{B^2}\right)\right) = 1, $$](img/A435493_1_En_2_Chapter_Equcy.gif)

如

![$$ \varSigma {a}_i{b}_i\le AB = \sqrt{a_1^2+{a}_2^2+\dots +{a}_n^2}\sqrt{b_1^2+{b}_2^2+\dots +{b}_n^2} $$](img/A435493_1_En_2_Chapter_Equcz.gif)

于是，就产生了下面的:

![$$ {\left(\varSigma {a}_i{b}_i\right)}^2\le \left({\varSigma}_i^n{a}_i^2\right)\left({\varSigma}_i^n{b}_i^2\right) $$](img/A435493_1_En_2_Chapter_Equda.gif)

#### 正交性

正交性被描述为不相关性的度量或程度。例如，向量的正交变换产生一个向量，使得它与我们变换的向量无关。内积在角度和长度方面的几何解释激发了我们在这些空间中使用的许多术语。事实上，柯西-施瓦茨不等式的一个直接结果是，它证明了定义两个非零向量之间的角度是正确的:

![$$ \mathrm{Angle}\left(\mathrm{x},\mathrm{y}\right)= \arccos \frac{x, y}{\left|\left| x\right|\right|*\left|\left| y\right|\right|} $$](img/A435493_1_En_2_Chapter_Equdb.gif)

#### 外部产品

两个向量的张量积与前面定义的内积略有关系。张量积是一种创建类似于整数乘法的新向量空间的方式:

![$$ \mathrm{Let}\ \mathrm{u}\ \mathrm{and}\ \mathrm{v}\ \mathrm{equal}\ \mathrm{two}\ \mathrm{v}\mathrm{ectors}\ \mathrm{where}\ \mathrm{x}=\left[{x}_1,{x}_2,{x}_3\right],\kern0.5em y={\left[{y}_1,\ {y}_2,{y}_3\right]}^T $$](img/A435493_1_En_2_Chapter_Equdc.gif)

![$$ y \otimes x= y{x}^T = \begin{array}{c}\hfill {y}_1\hfill \\ {}\hfill {y}_2\hfill \\ {}\hfill {y}_3\hfill \end{array}*\kern0.5em \begin{array}{ccc}\hfill {x}_1\hfill & \hfill {x}_2\hfill & \hfill {x}_3\hfill \end{array}\kern0.5em = \begin{array}{ccc}\hfill {y}_1{x}_1\hfill & \hfill {y}_1{x}_2\hfill & \hfill {y}_1{x}_3\hfill \\ {}\hfill {y}_2{x}_1\hfill & \hfill {y}_2{x}_2\hfill & \hfill {y}_2{x}_3\hfill \\ {}\hfill {y}_3{x}_1\hfill & \hfill {y}_3{x}_2\hfill & \hfill {y}_3{x}_3\hfill \end{array} $$](img/A435493_1_En_2_Chapter_Equdd.gif)

#### 特征值和特征向量

特征值是从方阵中导出的数，对应于特定的特征向量，也与方阵相关联。它们一起“提供了矩阵的特征分解”简单地说，矩阵的特征分解仅仅以特征向量及其相应的特征值的形式提供矩阵。特征分解很重要，因为它是一种“我们可以找到包含矩阵的函数的最大值(或最小值)的方法。”

特征分解:

![$$ A u=\lambda u $$](img/A435493_1_En_2_Chapter_Equde.gif)

![$$ \left( A-\lambda I\right) u=0 $$](img/A435493_1_En_2_Chapter_Equdf.gif)

其中 A =方阵，u =矩阵 A 的特征向量(如果向量乘以 A 后长度改变):

![$$ \uplambda =\mathrm{eigenvalue}\ \mathrm{t}\mathrm{o}\ \mathrm{corresponding}\ \mathrm{eigenvecvtor}\ \mathrm{u} $$](img/A435493_1_En_2_Chapter_Equdg.gif)

假设以下也成立:

![$$ A = \left(\begin{array}{cc}\hfill 2\hfill & \hfill 3\hfill \\ {}\hfill 2\hfill & \hfill 1\hfill \end{array}\right) $$](img/A435493_1_En_2_Chapter_Equdh.gif)

因此:

![$$ {u}_1=\left(\frac{3}{2}\right),\kern0.5em {u}_2=\left(-\frac{1}{1}\right),\kern0.5em {\lambda}_1=4,\kern0.5em {\lambda}_2 = -1 $$](img/A435493_1_En_2_Chapter_Equdi.gif)

对于大多数应用，特征向量被归一化为单位向量，如下:

![$$ {u}^T u=1 $$](img/A435493_1_En_2_Chapter_Equdj.gif)

此外，a 的特征向量放在矩阵 u 中。u 的每一列是 a 的特征向量。特征值存储在对角矩阵^中，其中矩阵的迹或对角线给出特征值。由此我们相应地改写第一个方程:

![$$ AU= U A $$](img/A435493_1_En_2_Chapter_Equdk.gif)

![$$ A= U\wedge {U}^{-1} $$](img/A435493_1_En_2_Chapter_Equdl.gif)

![$$ =\left[\begin{array}{cc}\hfill 3\hfill & \hfill -1\hfill \\ {}\hfill 2\hfill & \hfill 1\hfill \end{array}\right]\left[\begin{array}{cc}\hfill 4\hfill & \hfill 0\hfill \\ {}\hfill 0\hfill & \hfill -1\hfill \end{array}\right]\left[\begin{array}{cc}\hfill 2\hfill & \hfill 2\hfill \\ {}\hfill -4\hfill & \hfill 6\hfill \end{array}\right] $$](img/A435493_1_En_2_Chapter_Equdm.gif)

![$$ = \left[\begin{array}{cc}\hfill 2\hfill & \hfill 3\hfill \\ {}\hfill 2\hfill & \hfill 1\hfill \end{array}\right] $$](img/A435493_1_En_2_Chapter_Equdn.gif)

图 [2-6](#Fig6) 给出了特征向量的图示。

![A435493_1_En_2_Fig6_HTML.jpg](img/A435493_1_En_2_Fig6_HTML.jpg)

图 2-6。

Visulaization of eigenvectors

本征向量和本征值成为理解一种技术不可或缺的一部分，这种技术将在我们后面关于称为主成分分析(PCA)的变量选择技术的讨论中讨论。对称半正定矩阵的特征分解产生特征向量的正交基，每个特征向量具有非负特征值。PCA 研究变量之间的线性关系，并对输入数据集的协方差矩阵或相关矩阵执行。对于协方差或相关矩阵，特征向量对应于主分量，特征值对应于由主分量解释的方差。相关矩阵的主分量分析为观测数据的空间提供了标准正交本征基:在该基中，最大的本征值对应于与包含观测数据集的最大协变性相关联的主分量。

#### 线性变换

线性变换是两个模块之间的映射![$$ \mathrm{V}\to \mathrm{W} $$](img/A435493_1_En_2_Chapter_IEq18.gif),它保留加法和标量乘法的操作。当 V = W 时，我们称之为 V 的线性算子，或自同态，线性变换总是将线性子空间映射到线性子空间上，有时这可以在一个较低的维数上。这些线性映射可以表示为矩阵，例如旋转和反射。使用线性变换的一个例子是 PCA。稍后将详细讨论，PCA 是数据集中的特征到不相关主分量的正交线性变换，因此对于 K 个特征，我们有 K 个主分量。我将在下面的小节中详细讨论正交性，但是现在我将重点放在 PCA 的更广泛的方面。每个主成分保留了原始数据集的方差，但给出了它的表示，这样我们可以根据方差对数据集的贡献来推断给定主成分的重要性。当将其转换为原始数据集时，我们可以从数据集中删除我们认为没有显著差异的特征。

如果以下条件成立，函数![$$ \mathrm{\mathcal{L}}:{\mathrm{\mathbb{R}}}^n\to {\mathrm{\mathbb{R}}}^m $$](img/A435493_1_En_2_Chapter_IEq19.gif)称为线性变换:

![$$ \mathrm{\mathcal{L}}(ax) = a\mathrm{\mathcal{L}}(x) f o r\ every\ x\in {\mathrm{\mathbb{R}}}^n a nd\ a\in \mathrm{\mathbb{R}} $$](img/A435493_1_En_2_Chapter_Equdo.gif)

![$$ \mathrm{\mathcal{L}}\left( x+ y\right) = \mathrm{\mathcal{L}}(x) + \mathrm{\mathcal{L}}(y) f o r\ every\ x,\ y,\ \in\ {\mathrm{\mathbb{R}}}^n $$](img/A435493_1_En_2_Chapter_Equdp.gif)

当我们固定ℝ <sup>n</sup> 和ℝ <sup>m</sup> 的基时，线性变换ℒ可以用矩阵 a 来表示。具体地，存在![$$ A\in {\mathrm{\mathbb{R}}}^{mxn} $$](img/A435493_1_En_2_Chapter_IEq20.gif)使得下面的表示成立。假设![$$ x\in {\mathrm{\mathbb{R}}}^n $$](img/A435493_1_En_2_Chapter_IEq21.gif)是给定的向量，x’是 x 相对于ℝ <sup>m</sup> 的给定基的代表。如果![$$ y=\mathrm{\mathcal{L}}(x) $$](img/A435493_1_En_2_Chapter_IEq22.gif)和 y’是 y 相对于ℝ <sup>m</sup> 给定基的代表，则

![$$ {y}^{\prime } = Ax\hbox{'} $$](img/A435493_1_En_2_Chapter_Equdq.gif)

我们称 a 为ℒ关于ℝ <sup>n</sup> 和ℝ <sup>m</sup> 的给定基的矩阵表示。

#### 二次型

二次型是多个变量的二次齐次多项式，在机器学习中有应用。具体来说，我们寻求优化的两次可微的函数可以使用牛顿法进行优化。这其中的力量在于，如果一个函数是两次可微的，我们知道我们可以达到一个客观的最小值。

二次型![$$ f:\ {\mathrm{\mathbb{R}}}^n\to {\mathrm{\mathbb{R}}}^m $$](img/A435493_1_En_2_Chapter_IEq23.gif)是一个函数，使得以下成立:

![$$ F(x) = {x}^T Q x $$](img/A435493_1_En_2_Chapter_Equdr.gif)

其中 Q 是一个 n×n 实矩阵。假设 Q 是对称的——即![$$ Q={Q}^T $$](img/A435493_1_En_2_Chapter_IEq24.gif),不失一般性

矩阵 Q 的子矩阵是通过连续地从 Q 中移除行和列而获得的矩阵的行列式。主要子矩阵是 detQ 本身以及通过移除第 I 行和第 I 列而获得的矩阵的行列式。

#### 西尔威斯特标准

Sylvester 准则是判定矩阵是否半正定的充要条件。简单地说，它说明了一个矩阵是半正定的，所有的主要子矩阵都必须是正的。

证明:如果实对称矩阵 A 有正的非负特征值，称为正定。当特征值恰好非负时，称 A 是正半定的。

一个实对称矩阵 A 有非负的特征值当且仅当 A 可以分解为![$$ A={B}^T B $$](img/A435493_1_En_2_Chapter_IEq25.gif)并且所有的特征值都是正的当且仅当 B 是非奇异的。

前向含义:如果![$$ A\in {R}^{nxn} $$](img/A435493_1_En_2_Chapter_IEq26.gif)是对称的，那么有一个正交矩阵 P 使得![$$ A= P D{P}^T $$](img/A435493_1_En_2_Chapter_IEq27.gif)其中![$$ \mathrm{D}=\mathrm{diag}\left({\uplambda}_1{\uplambda}_2,\dots,\ {\uplambda}_{\mathrm{n}}\right) $$](img/A435493_1_En_2_Chapter_IEq28.gif)是一个实对角矩阵，其元素使得它的列是 a 的特征向量。如果![$$ {\lambda}_i\ge 0 $$](img/A435493_1_En_2_Chapter_IEq29.gif)对于每个 I，![$$ {D}^{\frac{1}{2}} $$](img/A435493_1_En_2_Chapter_IEq30.gif)存在。

反向蕴涵:如果 a 可以因式分解为 A = B^TB，那么 a 的所有特征值都是非负的，因为对于任何特征对(x，λ)

![$$ \lambda =\left(\frac{x^T Ax}{x^{T x}}\right)=\left(\frac{x^T{B}^T Bx}{x^T x}\right)=\left(\frac{{\left|\left| Bx\right|\right|}^2}{{\left|\left| x\right|\right|}^2}\right)\ge 0 $$](img/A435493_1_En_2_Chapter_Equds.gif)

#### 正交投影

投影是从向量空间到其自身的线性变换 P，使得![$$ {P}^2= P $$](img/A435493_1_En_2_Chapter_IEq31.gif)直观地，这意味着每当 P 被应用于任何值两次时，它给出与它被应用一次时相同的结果。它的图像是不变的，而且这个定义推广了图形投影的概念。![$$ \mathcal{V} $$](img/A435493_1_En_2_Chapter_IEq32.gif)是![$$ {\mathrm{\mathbb{R}}}^n\ \mathrm{if}\ {\mathrm{x}}_1,{\mathrm{x}}_2\in \mathcal{V}\to \alpha {x}_1+\beta {x}_2\in \mathcal{V}\ \mathrm{f}\mathrm{o}\mathrm{r}\ \mathrm{all}\ \upalpha, \upbeta \in \mathrm{\mathbb{R}}. $$](img/A435493_1_En_2_Chapter_IEq33.gif)的子空间，这个子空间的维数也等于![$$ \mathcal{V} $$](img/A435493_1_En_2_Chapter_IEq34.gif)中线性无关向量的最大数目如果![$$ \mathcal{V} $$](img/A435493_1_En_2_Chapter_IEq35.gif)是ℝ <sup>n</sup> 的子空间，则![$$ \mathcal{V} $$](img/A435493_1_En_2_Chapter_IEq36.gif)降级的![$$ {\mathcal{V}}^{\perp } $$](img/A435493_1_En_2_Chapter_IEq37.gif)的正交补由与![$$ \mathcal{V} $$](img/A435493_1_En_2_Chapter_IEq38.gif)中的每个向量正交的所有向量组成，因此，以下为真:

![$$ {\mathcal{V}}^{\perp }=\left\{ x:{v}^T x=0\ for\ all\ v\in\ \mathcal{V}\right\} $$](img/A435493_1_En_2_Chapter_Equdt.gif)

![$$ \mathcal{V} $$](img/A435493_1_En_2_Chapter_IEq39.gif)的正交补也是子空间。总之，![$$ \mathcal{V}\ \mathrm{and}\ {\mathcal{V}}^{\perp } $$](img/A435493_1_En_2_Chapter_IEq40.gif)跨越ℝ <sup>n</sup> ，在这个意义上，每个向量![$$ x\in\ {\mathrm{\mathbb{R}}}^n $$](img/A435493_1_En_2_Chapter_IEq41.gif)都可以表示为

![$$ x={x}_1+{x}_2 $$](img/A435493_1_En_2_Chapter_Equdu.gif)

其中![$$ {x}_1\in \mathcal{V}\ \mathrm{and}\ {x}_2\in {\mathcal{V}}^{\perp }. $$](img/A435493_1_En_2_Chapter_IEq42.gif)我们称上面的表示为 x 关于![$$ \mathcal{V} $$](img/A435493_1_En_2_Chapter_IEq43.gif)的正交分解，我们说 x <sub>1</sub> 和 x <sub>2</sub> 分别是 x 到子空间![$$ \mathcal{V} $$](img/A435493_1_En_2_Chapter_IEq44.gif)和![$$ {\mathcal{V}}^{\perp } $$](img/A435493_1_En_2_Chapter_IEq45.gif)的正交投影。我们写![$$ {\mathrm{\mathbb{R}}}^n = \mathcal{V} \otimes {\mathcal{V}}^{\perp } $$](img/A435493_1_En_2_Chapter_IEq46.gif)并且说ℝ <sup>n</sup> 是![$$ \mathcal{V} $$](img/A435493_1_En_2_Chapter_IEq47.gif)和![$$ {\mathcal{V}}^{\perp }. $$](img/A435493_1_En_2_Chapter_IEq48.gif)的直和，我们说 p 的线性变换是到![$$ \mathcal{V} $$](img/A435493_1_En_2_Chapter_IEq49.gif)的正交投影，对于所有的![$$ x\in {\mathrm{\mathbb{R}}}^n $$](img/A435493_1_En_2_Chapter_IEq50.gif)我们有![$$ P x\in \mathcal{V}\ \mathrm{and}\ x- P x\in {\mathcal{V}}^{\perp } $$](img/A435493_1_En_2_Chapter_IEq51.gif)

#### 矩阵的值域

矩阵的值域定义了它包含的列向量的数量。

设 A 的值域或图像写为:

![$$ \mathrm{\mathcal{R}}(A)\triangleq\ \left\{ Ax\ : x\in {\mathrm{\mathbb{R}}}^n\right\} $$](img/A435493_1_En_2_Chapter_Equdv.gif)

#### 矩阵的零空间

两个向量空间之间的线性映射![$$ \mathrm{\mathcal{L}}:\mathcal{V}\to\ \mathcal{W} $$](img/A435493_1_En_2_Chapter_IEq53.gif)的零空间是![$$ v\kern0.5em \mathrm{of}\ \mathcal{V} $$](img/A435493_1_En_2_Chapter_IEq54.gif)的所有元素的集合，其中![$$ \mathrm{\mathcal{L}}(v) = 0 $$](img/A435493_1_En_2_Chapter_IEq55.gif)中的零表示![$$ \mathcal{W} $$](img/A435493_1_En_2_Chapter_IEq56.gif)中的零向量

A 的零空间，或者说内核，写作如下:

![$$ \mathcal{N}(A)\ \triangleq\ \left\{ x\in\ {\mathrm{\mathbb{R}}}^n\ : Ax=0\right\} $$](img/A435493_1_En_2_Chapter_Equdw.gif)

#### 超平面

前面我提到了支持向量机和超平面的重要性。在回归问题的上下文中，超平面内的观察值作为响应变量解是可接受的。在分类问题的背景下，超平面形成了不同观察类之间的边界(如图 [2-7](#Fig7) 所示)。

![A435493_1_En_2_Fig7_HTML.jpg](img/A435493_1_En_2_Fig7_HTML.jpg)

图 2-7。

Visualization of hyperplane

我们将超平面定义为比其周围空间小一个维度的子空间，或者称为围绕对象的特征空间。

设![$$ u=\left[{u}_1,{u}_2, \dots,\ {u}_n\right],\kern0.5em \mathrm{u}\in \mathrm{\mathbb{R}} $$](img/A435493_1_En_2_Chapter_IEq57.gif)，其中 u <sub>i</sub> 中至少有一个是非零的。满足线性方程

![$$ {u}_1{x}_1+{u}_2{x}_2+\dots +{u}_n{x}_n= v $$](img/A435493_1_En_2_Chapter_Equdx.gif)

的所有点![$$ x={\left[{x}_1,{x}_2, \dots,\ {x}_n\right]}^T $$](img/A435493_1_En_2_Chapter_IEq58.gif)的集合

称为空间ℝ <sup>n</sup> 的超平面。我们可以用下面的等式来描述超平面:

![$$ \left\{ x\in {\mathrm{\mathbb{R}}}^n\ :{u}^T x= v\right\} $$](img/A435493_1_En_2_Chapter_Equdy.gif)

超平面不一定是ℝ <sup>n</sup> 的子空间，因为一般来说，它不包含原点。对于 n = 2，超平面的方程具有形式![$$ {u}_1{x}_1+{u}_2{x}_2= v $$](img/A435493_1_En_2_Chapter_IEq59.gif)，它是一条直线的方程。因此，直线是ℝ <sup>2</sup> 中的超平面。在ℝ <sup>3</sup> 中，超平面就是普通平面。超平面 h 将ℝ <sup>n</sup> 分成两个半空间，表示如下:

![$$ {H}_{+} = \left\{ x\in\ {\mathrm{\mathbb{R}}}^n\ :{u}^T x\ge 0\right\}, $$](img/A435493_1_En_2_Chapter_Equdz.gif)

![$$ {H}_{-} = \left\{ x\in {\mathrm{\mathbb{R}}}^n\ :{u}^T x\le 0\right\}\ . $$](img/A435493_1_En_2_Chapter_Equea.gif)

这里![$$ {H}_{+} $$](img/A435493_1_En_2_Chapter_IEq60.gif)是正半空间，![$$ {H}_{-} $$](img/A435493_1_En_2_Chapter_IEq61.gif)是负半空间。超平面 H 本身由![$$ \left\langle u, x- a\right\rangle =0 $$](img/A435493_1_En_2_Chapter_IEq62.gif)的点组成，其中![$$ a={\left[{a}_1,{a}_2, \dots,\ {a}_n\right]}^T $$](img/A435493_1_En_2_Chapter_IEq63.gif)是超平面的任意点。简单地说，超平面 H 是向量 u 和 x–a 彼此正交的所有点 x。

#### 顺序

实数序列是一个函数，其定义域是自然数 1，2，…，k 的集合，值域包含在ℝ.内因此，一个实数序列可以看作一组数{x <sub>1</sub> ，x <sub>2</sub> ，…，x <sub>k</sub> }，通常也表示为{x <sub>k</sub> }。

#### 序列的性质

序列的长度被定义为其中元素的数量。有限长度 n 的序列也称为 n 元组。有限序列也包括空序列或没有元素的序列。无限序列是指在一个方向上无限的序列。因此，它被描述为具有第一个元素，但没有最后一个元素。既没有第一个元素也没有最后一个元素的序列称为双向无限序列或双无限序列。

此外，如果每一项都大于或等于前一项，则称一个序列是单调递增的。例如，序列![$$ an(n) = 1 $$](img/A435493_1_En_2_Chapter_IEq64.gif)是单调递增的，如果且仅如果对于所有的![$$ {a}_{n+1}\ \ge {a}_n $$](img/A435493_1_En_2_Chapter_IEq65.gif)，术语非递减和非递增经常用于代替递增和递减，以避免分别与严格递增和严格递减的任何可能的混淆。

如果实数序列的所有项都小于某个实数，则称该序列从上有界。这意味着存在 M 使得对于所有 n，a <sub>n</sub> ≤ M ![$$ {a}_n\le M $$](img/A435493_1_En_2_Chapter_IEq66.gif)。任何这样的 M 称为一个上界。同样，如果，对于某个实数 m，![$$ {a}_n\ge m $$](img/A435493_1_En_2_Chapter_IEq67.gif)对于所有大于某个 N 的 N，那么序列从下有界，任何这样的 m 称为下界。

#### 限制

极限是当输入或索引接近某个值时，函数或序列接近的值。一个数![$$ {x}^{*}\in \mathrm{\mathbb{R}} $$](img/A435493_1_En_2_Chapter_IEq68.gif)称为数列的极限，如果对于任何正ϵ都有一个数 k，使得对于所有的![$$ k> K,\ \left| xk-{x}^{*}\right|<\epsilon $$](img/A435493_1_En_2_Chapter_IEq69.gif) :

![$$ {x}^{*}=\underset{k\to \infty }{ \lim } x k $$](img/A435493_1_En_2_Chapter_Equeb.gif)

有极限的序列称为收敛序列。非正式地说，一个单无限序列有一个极限，当 n 变得很大时，如果它接近某个值 L，称为极限。如果它向某个极限收敛，那么它就是收敛的。否则就是发散。图 [2-8](#Fig8) 显示了一个收敛于极限的序列。

![A435493_1_En_2_Fig8_HTML.jpg](img/A435493_1_En_2_Fig8_HTML.jpg)

图 2-8。

A function converging upon 0 as x increases

我们通常在机器学习和深度学习的背景下谈论收敛，以达到最佳解决方案。这是我们所有算法的最终目标，但随着读者遇到更困难的用例，这变得更加模糊。不是每个解决方案都有单一的全局最优解，相反，它可能有局部最优解。避免这些局部最优的方法将在后面的章节中详细讨论。通常这需要机器学习和深度学习算法的参数调整，这是算法训练过程中最困难的部分。

#### 导数和可微性

可微性成为机器学习和深度学习的重要组成部分，特别是为了参数更新。这可以通过用于训练多层感知器的反向传播算法以及卷积神经网络和循环神经网络的参数更新来看出。函数的导数衡量一个量相对于另一个量的变化程度。衍生产品最常见的例子之一是斜率(y 随 x 的变化)，或股票回报(价格百分比随时间的变化)。这是微积分的基本工具，但也是我们将在本书后半部分学习的许多模型的基础。

如果存在一个线性函数![$$ \mathrm{\mathcal{L}}:\ {\mathrm{\mathbb{R}}}^n\to {\mathrm{\mathbb{R}}}^m $$](img/A435493_1_En_2_Chapter_IEq70.gif)和一个向量![$$ y\in\ {\mathrm{\mathbb{R}}}^m $$](img/A435493_1_En_2_Chapter_IEq71.gif)，使得

![$$ A(x) = \mathrm{\mathcal{L}}(x) + y $$](img/A435493_1_En_2_Chapter_Equec.gif)

，则认为该函数是仿射的

对于每一个![$$ x\in {\mathrm{\mathbb{R}}}^n $$](img/A435493_1_En_2_Chapter_IEq72.gif),考虑一个函数![$$ f:\ {\mathrm{\mathbb{R}}}^n\to\ {\mathrm{\mathbb{R}}}^m $$](img/A435493_1_En_2_Chapter_IEq73.gif)和一个点![$$ {x}_0\in\ {\mathrm{\mathbb{R}}}^n. $$](img/A435493_1_En_2_Chapter_IEq74.gif),我们想要找到一个仿射函数 A，它在点 x0 附近逼近 f。第一，很自然的强加这个条件:

![$$ A\left({x}_0\right) = f\left({x}_0\right) $$](img/A435493_1_En_2_Chapter_Equed.gif)

我们通过ℒ、

![$$ \mathrm{\mathcal{L}}+ y = \mathrm{\mathcal{L}}(x) - \mathrm{\mathcal{L}}\left({x}_0\right)+ f\left({x}_0\right) = \mathrm{\mathcal{L}}\left( x-{x}_0\right) + f\left({x}_0\right) $$](img/A435493_1_En_2_Chapter_Equee.gif)

![$$ A(x) = \mathrm{\mathcal{L}}\left( x-{x}_0\right) + f\left({x}_0\right) $$](img/A435493_1_En_2_Chapter_Equef.gif)

的线性度得到![$$ y= f\left({x}_0\right)-\mathrm{\mathcal{L}}\left({x}_0\right). $$](img/A435493_1_En_2_Chapter_IEq75.gif)

我们还要求 A(x)接近 f(x)的速度要比 x 接近 x <sub>0</sub> 的速度快。

#### 偏导数和梯度

偏导数也在各种机器学习推导中大量使用。它类似于导数，只是我们只对函数中的一个变量求导，而其他变量保持不变，而在全微分中，所有变量都要计算。梯度下降算法在第 [3](03.html) 章中讨论，但是我们现在可以讨论梯度本身的更广泛的概念。当应用于几个变量的函数时，梯度是导数概念的概括。梯度代表函数中增长率最大的点，其大小是图形在该方向的斜率。这是一个向量场，它在一个坐标系中的分量会在从一个坐标系到另一个坐标系时发生变换:

![$$ \nabla f(x)= grad\ f(x) = \frac{df(x)}{dx} $$](img/A435493_1_En_2_Chapter_Equeg.gif)

#### 海森矩阵

函数可以不止一次可微，这就引出了海森矩阵的概念。Hessian 是一个标量值函数的二阶偏导数的方阵，或者标量场:

![$$ \mathbf{H}=\left(\begin{array}{ccc}\hfill \frac{\partial^2 f}{\partial {x}_1^2}\hfill & \hfill \frac{\partial^2 f}{\partial {x}_1\partial {x}_2} \cdot s \hfill & \hfill \frac{\partial^2 f}{\partial {x}_1\partial {x}_n}\hfill \\ {}\hfill \vdots \hfill & \hfill \ddots \hfill & \hfill \vdots \hfill \\ {}\hfill \frac{\partial^2 f}{\partial {x}_n\partial {x}_1}\hfill & \hfill \frac{\partial^2 f}{\partial {x}_n\partial {x}_2} \cdot s \hfill & \hfill \frac{\partial^2 f}{\partial {x}_n^2}\hfill \end{array}\right) $$](img/A435493_1_En_2_Chapter_Equeh.gif)

如果函数的梯度在某点 x 为零，那么 f 在 x 处有一个临界点，那么在 x 处的 Hessian 行列式称为判别式。如果这个行列式为零，则称 x 为 f 的退化临界点，或者 f 的非 Morse 临界点，否则为非退化临界点。

雅可比矩阵是向量值函数的一阶偏导数的矩阵。当这是一个方阵时，这个矩阵及其行列式都称为雅可比矩阵:

![$$ \mathbf{J}=\frac{\mathrm{df}}{\mathrm{dx}}=\left[\frac{\partial f}{\partial {x}_1}\dots \frac{\partial f}{\partial {x}_n}\right] = \left[\begin{array}{ccc}\hfill \frac{\partial f1}{\partial {x}_1}\hfill & \hfill \cdots \hfill & \hfill \frac{\partial {f}_1}{\partial {x}_n}\hfill \\ {}\hfill \vdots \hfill & \hfill \ddots \hfill & \hfill \vdots \hfill \\ {}\hfill \frac{\partial {f}_m}{\partial {x}_1}\hfill & \hfill \cdots \hfill & \hfill \frac{\partial {f}_m}{\partial {x}_n}\hfill \end{array}\right] $$](img/A435493_1_En_2_Chapter_Equei.gif)

## 摘要

这使我们得出了基本统计学和数学概念的结论，这些将在后面的章节中引用。当读者对后面章节中的任何内容不确定时，应该鼓励他们回头再看这一章。接下来，我们将讨论为机器学习算法提供动力的更高级的优化技术，以及那些形成了我们随后将处理的深度学习方法的灵感的相同机器学习算法。