# 四、用于监督学习的深度神经网络：分类

在第 3 章中，我们探索了一个用于回归的 DL 用例。我们用业务推进战略探索了整个问题解决方法。我们利用我们从基础 DL 和 Keras 框架的第 [1](1.html) 和 [2](2.html) 章节中学到的知识来开发回归用例的 DNNs。在这一章中，我们将进一步学习并设计一个分类用例的网络。总体方法保持不变，但是在解决分类用例时，我们需要记住一些细微差别。此外，我们将把本章的学习向前推进一步，扩展 DNN 体系结构。让我们开始吧。

## 入门指南

类似于第 3 章[中的](3.html)，我们将考虑 Kaggle 作为我们用例的数据源。从可用选项中，我们将使用为“Red Hat Business Value”竞赛提供的数据集。这个比赛是几年前在 Kaggle 上举办的，数据集对于我们的研究来说是一个非常好的商业用例。存档比赛可在 [`www.kaggle.com/c/predicting-red-hat-business-value`](https://www.kaggle.com/c/predicting-red-hat-business-value) 获取。就像在前面的用例中一样，我们需要在为我们的实验下载数据集之前阅读并接受竞争规则。一旦你接受了比赛规则，你就可以从“数据”标签或 [`www.kaggle.com/c/predicting-red-hat-business-value/data`](http://www.kaggle.com/c/predicting-red-hat-business-value/data) 下载数据集。数据将以 zip 文件的形式下载。解压缩后，您将有四个不同的数据集。我们只需要其中的两个:act_train.csv 和 people.csv。

您可以复制这两个数据集，并将它们保存在一个新的文件夹中，用于当前章节的实验。我们将为用例使用相同的环境，但是在我们开始之前，让我们看一下问题陈述并定义 SCQ 和解决方案方法，就像我们在第 [3](3.html) 章中所做的一样。

## 问题陈述

高层次的问题陈述在比赛的描述页中被提及。它强调了基于运营交互数据来预测高价值客户的问题，从而帮助公司有效地优化资源以产生更多业务并更好地服务于客户。

让我们从一个更加以业务为中心的角度来看问题陈述。我们将从更好地了解客户开始。该组织是一家美国跨国软件公司，向企业社区提供开源软件产品。他们的主要产品是 Red Hat Enterprise Linux，这是 Linux OS 最流行的发行版，被各种大型企业所使用。在其服务中，它通过开放的业务模式和经济实惠、可预测的订阅模式提供企业级解决方案，从而帮助组织调整其 it 战略。这些来自大型企业客户的订阅为他们创造了很大一部分收入，因此对他们来说，了解他们有价值的客户并通过优化资源和策略来更好地服务他们以提高业务价值是至关重要的。

### 设计 SCQ

这个问题的最终涉众可能是销售团队或业务开发团队；这两个团队都处于公司运营的最前沿，为他们最有价值的客户提供最好的服务。为了更有效地实现这一目标，业务开发团队现在已经探索了一种数据驱动的解决方案。鉴于大量的运营交互数据和一些客户属性，他们希望开发数据驱动的技术来预测业务的潜在高价值客户。在这种背景下，现在让我们为业务问题起草 SCQ，就像我们在第 [3](3.html) 章中为回归用例所做的那样。

![img/475458_1_En_4_Figa_HTML.jpg](img/475458_1_En_4_Figa_HTML.jpg)

### 设计解决方案

上图展示的 SCQ 清楚地界定了当前局势和期望的未来局势，同时阐明了障碍和需要回答的问题，以便克服实现更大目标的障碍。为了设计解决方案，我们需要从关键问题开始，然后逆向工作。

#### 我们如何识别潜在客户？

红帽已经存在超过 25 年了。在长期的业务中，他们从客户互动及其描述性属性中积累并获取了大量数据。这种丰富的数据源可能是模式的金矿，可以通过研究交互数据中大量复杂的历史模式来帮助识别潜在客户。

随着 DL 的日益普及和强大，我们可以开发一种 DNN，它可以从历史客户属性和运营交互数据中学习，以了解深层模式并预测新客户是否有可能成为各种业务服务的高价值客户。

因此，我们将开发和训练一个 DNN，使用各种客户属性和运营互动属性来学习一个客户成为潜在高价值客户的机会。

## 探索数据

现在，我们已经清楚地草拟了业务问题，并准备好了高级解决方案，让我们开始研究数据。从 Kaggle 下载红帽商业价值竞赛数据的流程与之前在第 [3 章](3.html)中展示的流程相同。所需数据集可在此下载: [`www.kaggle.com/c/predicting-red-hat-business-value/data`](http://www.kaggle.com/c/predicting-red-hat-business-value/data) 。请按照上一章演示的五个步骤下载数据。

现在，让我们打开 Jupyter 笔记本，为当前的实验创建一个新的笔记本。

```py
#Import the necessary packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

#Import the 2 datasets provided in the Zip Folder
df = pd.read_csv("/Users/jojomoolayil/Book/Chapter4/Data/act_train.csv")
people = pd.read_csv("/Users/jojomoolayil/Book/Chapter4/Data/people.csv")

#Explore the shape of the datasets
print("Shape of DF:",df.shape)
print("Shape of People DF:",people.shape)

```

**输出**

```py
Shape of DF: (2197291, 15)
Shape of People DF: (189118, 41)

```

最后一行代码:

```py
#Explore the contents of the first dataset
df.head()

```

**输出**

![img/475458_1_En_4_Figb_HTML.jpg](img/475458_1_En_4_Figb_HTML.jpg)

我们可以看到，为比赛提供的训练数据超过 200 万行 15 列，而 people 数据集大约有 19 万行 41 列。探索训练数据集的内容，我们可以看到它大部分有客户交互数据，但完全匿名。考虑到客户及其属性的保密性，整个数据都是匿名的，这使得我们对其真实性质知之甚少。这是数据科学中常见的问题。开发 DL 模型的团队经常面临终端客户数据保密性的挑战，因此只能得到匿名的、有时是加密的数据。这仍然不应该是一个路障。拥有一个数据字典和对数据集的完全理解肯定是最好的，但是尽管如此，我们仍然可以用提供的信息开发模型。

act_train.csv(以下称为活动数据)有许多空数据点。在高层次上，数据集捕获客户活动并提供一些活动属性、一些客户属性(在前面的输出中显示为空)、另一个我们不太了解的名为“char_10”的分类特征，以及最后的结果变量。

让我们看看活动数据有多少个空值。

```py
#Calculating the % of Null values in each column for activity data
df.isnull().sum()/df.shape[0]

```

**输出**

```py
people_id            0.000000
activity_id          0.000000
date                 0.000000
activity_category    0.000000
char_1               0.928268
char_2               0.928268
char_3               0.928268
char_4               0.928268
char_5               0.928268
char_6               0.928268
char_7               0.928268
char_8               0.928268
char_9               0.928268
char_10              0.071732
outcome              0.000000
dtype: float64

```

大约九个要素的空值超过 90%。我们无法修复这些功能。让我们继续，看看人员数据集。

```py
#Explore the contents of People dataset
people.head()

```

**输出**

![img/475458_1_En_4_Figc_HTML.jpg](img/475458_1_En_4_Figc_HTML.jpg)

我们已经知道，人员数据集(此后称为客户数据)大约有 41 列；当我们进入内容时(由于大量的列，在前面的图中只显示了一部分)，我们看到提供给我们许多客户属性，尽管我们无法理解它们。此外，列名与活动数据中的列名相同。我们需要在加入之前更改它们，以避免名称冲突。

让我们检查一下客户数据集有多少缺失的数据点。由于客户数据集有大约 40 多个特性，我们可以将所有列的缺失值百分比与前面的代码结合起来，而不是单独查看每一列。

```py
#Calculate the % of null values in for the entire dataset
people.isnull().sum().sum()

```

**输出**

```py
0

```

我们看到客户数据集中没有任何列缺少值。

为了创建一个合并的数据集，我们需要在 people_id 键上连接活动和客户数据。但在此之前，我们需要注意一些事情。我们需要删除活动数据中有 90%缺失值的列，因为它们无法修复。其次,“date”和“char_10”列出现在两个数据集中。为了避免名称冲突，让我们将活动数据集中的“date”列重命名为“activity_date”，将活动数据中的“char_10”重命名为“activity_type”接下来，我们还需要修复“activity_type”列中缺失的值。一旦这两项任务完成，我们将连接这两个数据集并研究整合后的数据。

```py
#Create the list of columns to drop from activity data
columns_to_remove = ["char_"+str(x) for x in  np.arange(1,10)]
print("Columns to remove:",columns_to_remove)

#Remove the columns from the activity data
df = df[list(set(df.columns) - set(columns_to_remove))]

#Rename the 2 columns to avoid name clashes in merged data
df = df.rename(columns={"date":"activity_date","char_10":"activity_type"})

#Replace nulls in the activity_type column with the mode
df["activity_type"] = df["activity_type"].fillna(df["activity_type"].mode()[0])

#Print the shape of the final activity dataset
print("Shape of DF:",df.shape)

```

**输出**

```py
Columns to remove: ['char_1', 'char_2', 'char_3', 'char_4', 'char_5', 'char_6', 'char_7', 'char_8', 'char_9']

Shape of DF: (2197291, 6)

```

我们现在可以连接这两个数据集，以创建一个整合的活动和客户属性数据集。

```py
#Merge the 2 datasets on 'people_id' key
df_new = df.merge(people,on=["people_id"],how="inner")
print("Shape before merging:",df.shape)
print("Shape after merging :",df_new.shape)

```

**输出**

```py
Shape before merging: (2197291, 6)
Shape after merging : (2197291, 46)

```

一致的行数和增加的列数有助于我们验证连接操作是否按预期工作。现在让我们研究数据集中名为“结果”的目标(即我们想要预测的变量)。我们可以检查潜在客户和非潜在客户之间的分布。

```py
print("Unique values for outcome:",df_new["outcome"].unique())
print("\nPercentage of distribution for outcome-")
print(df_new["outcome"].value_counts()/df_new.shape[0])3

Outcome

Unique values for outcome: [0 1]

Percentage of distribution for outcome-
0    0.556046
1    0.443954
Name: outcome, dtype: float64

```

我们可以看到，潜在客户的分布情况很好，大约 45%是潜在客户。

## 数据工程

接下来，假设我们总共有 45 个专栏要探索和转换，让我们通过自动化一些事情来加速这个过程。让我们看看整合数据框架中的不同数据类型。

```py
#Checking the distinct datatypes in the dataset
print("Distinct DataTypes:",list(df_new.dtypes.unique()))

```

**输出**

```py
Distinct DataTypes: [dtype('int64'), dtype('O'), dtype('bool')]

```

数据集中有数字、分类(对象)和布尔特征。Python 中的 Boolean 表示真或假值；我们需要将其转换成数字(1 和 0 ),以便模型处理数据。以下代码片段将 dataframe 中的布尔列转换为基于数字(1 和 0)的值。

```py
#Create a temp dataset with the datatype of columns
temp = pd.DataFrame(df_new.dtypes)
temp.columns = ["DataType"]
#Create a list with names of all Boolean columns
boolean_columns = temp.index[temp["DataType"] == 'bool'].values

print("Boolean columns - \n",boolean_columns)

#Convert all boolean columns to Binary numeric values
for column in boolean_columns:
    df_new[column] = np.where(df_new[column] == True,1,0)

print("\nDistinct DataTypes after processing:",df.dtypes.unique())

```

**输出**

```py
Boolean columns -
['char_10"char_11"char_12"char_13"char_14"char_15"char_16'
'char_17"char_18"char_19"char_20"char_21"char_22"char_23'
'char_24"char_25"char_26"char_27"char_28"char_29"char_30'
'char_31"char_32"char_33"char_34"char_35"char_36"char_37']

Distinct DataTypes after processing: [dtype('int64') dtype('O')]

```

现在让我们来看看分类特征。我们将首先进行健全性检查，以了解每个分类特性中不同值的数量。如果分类特征中有非常多的不同值，我们必须决定是否真的可以将它们转换成一个独一无二的编码结构，以便进一步处理。

```py
#Extracting the object columns from the above dataframe
categorical_columns = temp.index[temp["DataType"] == 'O'].values

#Check the number of distinct values in each categorical column
for column in categorical_columns:
    print(column+" column has :",str(len(df_new[column].unique()))+" distinct values")

```

**输出**

```py
activity_category column has : 7 distinct values
activity_id column has : 2197291 distinct values
people_id column has : 151295 distinct values
activity_type column has : 6516 distinct values
activity_date column has : 411 distinct values
char_1 column has : 2 distinct values
group_1 column has : 29899 distinct values
char_2 column has : 3 distinct values
date column has : 1196 distinct values
char_3 column has : 43 distinct values
char_4 column has : 25 distinct values
char_5 column has : 9 distinct values
char_6 column has : 7 distinct values
char_7 column has : 25 distinct values
char_8 column has : 8 distinct values
char_9 column has : 9 distinct values

```

输出中显示的五个突出显示的列具有大量不同的值。很难将它们转换成一键编码的形式，因为它们在处理过程中会消耗太多的内存。如果您有多余的 RAM，请随意将它们转换为一键编码的数据形式。

现在，我们可以看看这些分类列中的内容，以了解将它们转换成数字的方法。另外，`date`和`activity_date`列是日期值；因此，我们可以像上一章那样将它们转换成与数据相关的特征。让我们首先修复与日期相关的列，然后再处理剩余的分类列。以下代码片段将日期值转换为新功能，然后删除实际的列。

```py
#Create date related features for 'date' in customer data
df_new["date"] = pd.to_datetime(df_new["date"])

df_new["Year"] = df_new["date"].dt.year
df_new["Month"] = df_new["date"].dt.month
df_new["Quarter"] = df_new["date"].dt.quarter
df_new["Week"] = df_new["date"].dt.week
df_new["WeekDay"] = df_new["date"].dt.weekday
df_new["Day"] = df_new["date"].dt.day

#Create date related features for 'date' in activity data
df_new["activity_date"] = pd.to_datetime(df_new["activity_date"])

df_new["Activity_Year"] = df_new["activity_date"].dt.year
df_new["Activity_Month"] = df_new["activity_date"].dt.month
df_new["Activity_Quarter"] = df_new["activity_date"].dt.quarter
df_new["Activity_Week"] = df_new["activity_date"].dt.week
df_new["Activity_WeekDay"] = df_new["activity_date"].dt.weekday
df_new["Activity_Day"] = df_new["activity_date"].dt.day

#Delete the original date columns
del(df_new["date"])
del(df_new["activity_date"])

print("Shape of data after create Date Features:",df_new.shape)

```

**输出**

```py
Shape of data after create Date Features: (2197291, 56)

```

现在让我们看看剩下的分类列，它们有非常多的不同值。

```py
print(df_new[["people_id","activity_type","activity_id","group_1"]].head())

```

**输出**

```py
people_id activity_type   activity_id      group_1
0   ppl_100       type 76  act2_1734928  group 17304
1   ppl_100        type 1  act2_2434093  group 17304
2   ppl_100        type 1  act2_3404049  group 17304
3   ppl_100        type 1  act2_3651215  group 17304
4   ppl_100        type 1  act2_4109017  group 17304

```

似乎我们可以通过从每一列中提取相关的数字 ID，将前面所有的分类列转换成数字，因为每一列都有形式为`someText_someNumber`的值。我们可以暂时将这些分类列用作数字特征，而不是将它们转换成臃肿的一次性编码数据集。然而，如果在几次实验后，模型的性能没有达到我们的期望，我们可能不得不重新审视这些特性，并尽最大努力以不同的方式融入它们。但是现在，我们可以把它们看作数字特征。

下面的代码片段提取列的数字部分，并将列从字符串转换为数字特征。

```py
#For people ID, we would need to extract values after '_'
df_new.people_id = df_new.people_id.apply(lambda x:x.split("_")[1])
df_new.people_id = pd.to_numeric(df_new.people_id)

#For activity ID also, we would need to extract values after '_'
df_new.activity_id = df_new.activity_id.apply(lambda x:x.split("_")[1])
df_new.activity_id = pd.to_numeric(df_new.activity_id)

#For group_1 , we would need to extract values after "
df_new.group_1 = df_new.group_1.apply(lambda x:x.split("")[1])
df_new.group_1 = pd.to_numeric(df_new.group_1)

#For activity_type , we would need to extract values after "
df_new.activity_type = df_new.activity_type.apply(lambda x:x.split("")[1])
df_new.activity_type = pd.to_numeric(df_new.activity_type)

#Double check the new values in the dataframe
print(df_new[["people_id","activity_type","activity_id","group_1"]].head())

```

**输出**

```py
people_id  activity_type  activity_id  group_1
0      100.0             76    1734928.0    17304
1      100.0              1    2434093.0    17304
2      100.0              1    3404049.0    17304
3      100.0              1    3651215.0    17304
4      100.0              1    4109017.0    17304

```

现在，我们将布尔列转换为数字列，并且将包含大量不同值的分类列也转换为数字列。(注意:这种分类到数字的转换并不总是可行的。)接下来，让我们将剩余的非重复值数量相对较少的分类列转换为一键编码形式，并呈现最终的合并数据集。

```py
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

#Define a function that will intake the raw dataframe and the column name and return a one hot encoded DF
def create_ohe(df, col):
    le = LabelEncoder()
    a=le.fit_transform(df_new[col]).reshape(-1,1)
    ohe = OneHotEncoder(sparse=False)
    column_names = [col+ "_"+ str(i) for i in le.classes_]
    return(pd.DataFrame(ohe.fit_transform(a),columns =column_names))

#Since the above function converts the column, one at a time
#We create a loop to create the final dataset with all features
target = ["outcome"]
numeric_columns = list(set(temp.index[(temp.DataType =="float64") |
              (temp.DataType =="int64")].values) - set(target))

temp = df_new[numeric_columns]
for column in categorical_columns:
    temp_df = create_ohe(df_new,column)
    temp = pd.concat([temp,temp_df],axis=1)

print("\nShape of final df after onehot encoding:",temp.shape)

```

**输出**

```py
Shape of final df after onehot encoding: (2197291, 183)

```

我们现在已经为模型开发准备好了数据集的最终形式。在本练习中，我们转换并保留了与日期相关的功能，因为它们是数字形式，而不是转换为一键编码形式。这个选项是可选的。我考虑了数据集的大小，大约有 180 列，开始时足够大了。我们将进行一些基本实验，如果我们没有看到良好的性能，我们将需要重新访问数据。在这种情况下，我们需要寻找改进的策略，以便以最节省内存和计算的方式从大量选择的特征中提取最佳信息。

最后，在我们开始模型开发之前，我们需要将我们的数据集分成训练、验证和测试，就像我们在第 [3](3.html) 章中对回归用例所做的那样。以下代码片段利用 Python 中 sklearn 包的“train_test_split”将前面创建的最终数据集拆分为 train 和 test，然后进一步将 train 划分为 train 和 validation。

```py
from sklearn.model_selection import train_test_split

#split the final dataset into train and test with 80:20
x_train, x_test, y_train, y_test = train_test_split(temp,df_new[target], test_size=0.2,random_state=2018)
#split the train dataset further into train and validation with 90:10
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=2018)

#Check the shape of each new dataset created
print("Shape of x_train:",x_train.shape)
print("Shape of x_test:",x_test.shape)
print("Shape of x_val:",x_val.shape)
print("Shape of y_train:",y_train.shape)
print("Shape of y_test:",y_test.shape)
print("Shape of y_val:",y_val.shape)

```

**输出**

```py
Shape of x_train: (1582048, 183)
Shape of x_test: (439459, 183)
Shape of x_val: (175784, 183)
Shape of y_train: (1582048, 1)
Shape of y_test: (439459, 1)
Shape of y_val: (175784, 1)

```

现在，我们有了构建分类的 DL 模型所需形式的训练数据。我们需要定义一个基线基准，它将帮助我们设置我们应该从模型中期望的阈值性能，以使它们被认为是有用的和可接受的。

## 定义模型基线准确性

在第 [3](3.html) 章中，当我们使用回归用例时，我们通过使用训练数据集的平均值作为测试数据集中所有值的最终预测来定义基线精度。然而，在分类用例中，我们需要一个稍微不同的方法。

对于所有监督分类用例，我们的目标变量将是二元或多类(两个以上的类)结果。在我们的用例中，我们的结果是 0 或 1。为了验证模型的有效性，我们应该将结果与如果我们没有模型会发生的情况进行比较。在这种情况下，我们将最大的类作为所有客户的预测，并检查其准确性。

如果您还记得，我们用例中的目标(即结果变量)具有 1 和 0 的良好分布。这里是结果变量在 1 和 0 之间的分布。

```py
#Checking the distribution of values in the target
df_new["outcome"].value_counts()/df_new.shape[0]

Output

0    0.556046
1    0.443954
Name: outcome, dtype: float64

```

因此，对于前面的分布，我们可以说，如果我们没有任何模型，所有预测都为 0(最大类)，也就是说，预测没有一个客户是潜在的高价值客户，那么无论如何，我们都至少有 55.6%的准确性。这是我们的基线精度。如果我们建立一个模型，提供给我们的整体精度低于我们的基准，那么它实际上是没有用的。

## 设计用于分类的 DNN

对于这个用例，我们有更大的数据集。训练过程可能比回归用例更耗时。为了节省我们的时间，并且能够快速得到一个功能良好的架构，我们将使用一个简单的策略。对于我们将要试验的每种网络，我们将从三个时期开始，一旦我们发现有希望的结果，我们将用期望数量的时期来重新训练最佳架构以获得改进的结果。

首先，让我们遵循我们在第 [3](3.html) 章中学到的相同的架构开发指南。也就是说，让我们遵循规则 1:从小处着手。

下面的代码片段构建了一个只有一层和 256 个神经元的 DNN。我们使用`binary_crossentropy`(因为这是一个二进制分类问题)作为损失函数，并使用精确度作为监控的度量。对于分类问题，我们可以使用 Keras 中可用的其他几个指标，但是准确性很简单，也很容易理解。我们将只训练三个时期的网络，并持续监控训练和验证数据集的损失和准确性。如果我们看不到有希望的结果，我们可能不得不尝试新的架构。

```py
from keras.models import Sequential
from keras.layers import Dense

#Design the deep neural network [Small + 1 layer]
model  = Sequential()
model.add(Dense(256,input_dim = x_train.shape[1],activation="relu"))
model.add(Dense(256,activation="relu"))
model.add(Dense(1,activation = "sigmoid")) #activation = sigmoid for binary classification

model.compile(optimizer = "Adam",loss="binary_crossentropy",metrics=["accuracy"])

model.fit(x_train,y_train, validation_data = (x_val,y_val),epochs=3, batch_size=64)

```

**输出**

```py
Using TensorFlow backend.
Train on 1582048 samples, validate on 175784 samples
Epoch 1/3
1582048/1582048 [==============================] - 112s 71us/step - loss: 8.8505 - acc: 0.4449 - val_loss: 8.8394 - val_acc: 0.4455
Epoch 2/3
1582048/1582048 [==============================] - 111s 70us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455

Epoch 3/3
1582048/1582048 [==============================] - 110s 69us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455

```

如果您仔细观察来自训练输出的结果，您将会看到训练和验证数据集的总体准确度约为 0.44 (44%)，这远远低于我们的基线准确度。因此，我们可以得出结论，进一步训练这个模型可能不是一个富有成效的想法。

让我们为相同数量的神经元尝试一个更深的网络。所以，我们保持一切不变，但增加了一层相同数量的神经元。

```py
#Design the deep neural network [Small + 2 layers]
model  = Sequential()
model.add(Dense(256,input_dim = x_train.shape[1],activation="relu"))
model.add(Dense(256,activation="relu"))
model.add(Dense(1,activation = "sigmoid"))

model.compile(optimizer = "Adam",loss="binary_crossentropy",metrics=["accuracy"])

model.fit(x_train,y_train, validation_data = (x_val,y_val),epochs=3, batch_size=64)

```

**输出**

```py
Train on 1582048 samples, validate on 175784 samples
Epoch 1/3
1582048/1582048 [==============================] - 124s 79us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455
Epoch 2/3
1582048/1582048 [==============================] - 125s 79us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455
Epoch 3/3
1582048/1582048 [==============================] - 124s 78us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455

```

同样，正如我们所看到的，最初的结果一点也不乐观。来自更深层次网络的训练和验证准确性与我们预期的相差甚远。让我们试着用一个更大(中等规模)的网络来训练，而不是尝试另一个更深层次的网络，比如三到五层。这次我们将使用一个只有一层但有 512 个神经元的新架构。让我们再次训练三个纪元，并查看度量标准，以检查它是否符合我们的预期。

```py
#Design the deep neural network [Medium + 1 layers]
model  = Sequential()
model.add(Dense(512,input_dim = x_train.shape[1],activation="relu"))
model.add(Dense(1,activation = "sigmoid"))

model.compile(optimizer = "Adam",loss="binary_crossentropy",metrics=["accuracy"])

model.fit(x_train,y_train, validation_data = (x_val,y_val),epochs=3, batch_size=64)

```

**输出**

```py
Train on 1582048 samples, validate on 175784 samples
Epoch 1/3
1582048/1582048 [==============================] - 113s 71us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455
Epoch 2/3
1582048/1582048 [==============================] - 112s 71us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455
Epoch 3/3
1582048/1582048 [==============================] - 112s 71us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455

```

中型网络也返回了令人失望的结果。中型网络的训练和验证准确性与我们的预期相差甚远。现在，让我们尝试增加中型网络的深度，看看结果是否有所改善。

```py
#Design the deep neural network [Medium + 2 layers]
model  = Sequential()
model.add(Dense(512,input_dim = x_train.shape[1],activation="relu"))
model.add(Dense(512,activation="relu"))
model.add(Dense(1,activation = "sigmoid"))

model.compile(optimizer = "Adam",loss="binary_crossentropy",metrics=["accuracy"])

model.fit(x_train,y_train, validation_data = (x_val,y_val),epochs=3, batch_size=64)

```

**输出**

```py
Train on 1582048 samples, validate on 175784 samples
Epoch 1/3
1582048/1582048 [==============================] - 135s 86us/step - loss: 7.1542 - acc: 0.5561 - val_loss: 7.1813 - val_acc: 0.5545
Epoch 2/3
1582048/1582048 [==============================] - 134s 85us/step - loss: 7.1534 - acc: 0.5562 - val_loss: 7.1813 - val_acc: 0.5545

Epoch 3/3
1582048/1582048 [==============================] - 135s 85us/step - loss: 7.1534 - acc: 0.5562 - val_loss: 7.1813 - val_acc: 0.5545

```

我们可以看到结果有所改善，但只是一点点。我们看到训练和验证数据集的准确率约为 55%,但这些结果也不是很好，尽管比我们之前的结果要好。

## 重新审视数据

最初试图建立一个具有良好结果的模型的努力已经失败。我们可以进一步增加网络的规模和深度，但这只会略微提高网络性能。如前所述，我们可能不得不考虑改进用于训练的数据。我们有两个主要选择。我们已经在第 [2](2.html) 章的“输入数据”部分和第 [3](3.html) 章的“探索数据”部分讨论了这两点。我们可以使用 Python 的 sklearn 包中的工具，使用“Standardscaler”或“Minmaxscaler”对输入数据进行标准化，或者我们可以探索各种选项，重新对我们编码为数值的分类要素进行一次性编码。从这两个选项中，最简单和最省时的是标准化或规范化数据。

### 标准化、规范化或缩放数据

如果您还记得，在第 [2](2.html) 章“Keras 中的 DL 入门”下的“输入数据”一节中，我们讨论过在提供数据作为 DL 模型的训练数据之前，将数据标准化或规范化是一个好的做法。我们没有在第 [3](3.html) 章的回归用例中使用这一选项，因为该模型在常规数据上表现良好。然而，在我们的分类用例中，我们可以看到在原始数据上的性能非常差。为了提高我们的模型性能，让我们尝试标准化我们的数据。(或者，您也可以标准化数据。)

在标准化中，我们将数据转换为均值为 0、标准差为 1 的形式。这种形式的数据分布是我们神经元激活功能的一个很好的输入候选，因此提高了更恰当地学习的能力。

最简单的形式是，标准化可以通过以下使用虚拟输入数据集的示例来解释。我们执行标准缩放；查看转换后的值、平均值及其标准差；最后将输出逆变换为其原始形式。

```py
#Create a dummy input
dummy_input = np.arange(1,10)
print("Dummy Input = ",dummy_input)

from sklearn.preprocessing  import StandardScaler

#Create a standardscaler instance and fit the data
scaler = StandardScaler()
output = scaler.fit_transform(dummy_input.reshape(-1,1))

print("Output =\n ",list(output))
print("Output's Mean = ",output.mean())
print("Output's Std Dev = ",output.std())
print("\nAfter Inverse Transforming = \n",list(scaler.inverse_transform(output)))

```

**输出**

```py
Dummy Input =  [1 2 3 4 5 6 7 8 9]
Output =
[array([-1.54919334]), array([-1.161895]), array([-0.77459667]),
array([-0.38729833]), array([0.]), array([0.38729833]),
array([0.77459667]), array([1.161895]), array([1.54919334])]

Output's Mean =  0.0

Output's Std Dev =  1.0

After Inverse Transforming =
[array([1.]), array([2.]), array([3.]), array([4.]), array([5.]),
array([6.]), array([7.]), array([8.]), array([9.])]

```

### 转换输入数据

要转换用于模型开发的输入数据，请注意，我们应该仅使用训练数据来拟合缩放器转换，并使用相同的拟合对象来转换验证和测试输入数据。以下代码片段使用`x_train`数据集来拟合和转换所有三个数据集的缩放值(即`x_train`和`x_val`以及`x_test`)。

```py
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(x_train)

x_train_scaled = scaler.transform(x_train)
x_val_scaled = scaler.transform(x_val)
x_test_scaled = scaler.transform(x_test)

```

既然我们已经有了标准的缩放数据集，我们就可以为训练提供这些新增加的数据。请注意，我们没有对标签或目标进行任何转换。

## 用于改进数据分类的 DNNs

现在让我们从一个中等规模的网络开始，看看我们是否能得到改进的结果。我们将从三个时代开始。

```py
from keras import Sequential
from keras.layers import Dense
model  = Sequential()
model.add(Dense(512,input_dim = x_train_scaled.shape[1],activation="relu"))
model.add(Dense(1,activation = "sigmoid"))

model.compile(optimizer = "Adam",loss="binary_crossentropy",metrics=["accuracy"])

model.fit(x_train_scaled,y_train, validation_data = (x_val_scaled,y_val), epochs=3, batch_size=64)

```

**输出**

```py
Train on 1582048 samples, validate on 175784 samples
Epoch 1/3
1582048/1582048 [==============================] - 109s 69us/step - loss: 0.2312 - acc: 0.8994 - val_loss: 0.1894 - val_acc: 0.9225
Epoch 2/3
1582048/1582048 [==============================] - 108s 68us/step - loss: 0.1710 - acc: 0.9320 - val_loss: 0.1558 - val_acc: 0.9387
Epoch 3/3
1582048/1582048 [==============================] - 108s 68us/step - loss: 0.1480 - acc: 0.9444 - val_loss: 0.1401 - val_acc: 0.9482

```

现在，我们走吧！

我们可以看到网络在提供标准化数据集方面的性能有了显著提高。我们在训练和验证数据集上有几乎 95%的准确率。让我们使用这个模型来评估我们之前创建的测试数据集上的模型性能。

```py
result = model.evaluate(x_test_scaled,y_test)
for i in range(len(model.metrics_names)):
    print("Metric ",model.metrics_names[i],":",str(round(result[i],2)))

```

**输出**

```py
439459/439459 [==============================] - 34s 76us/step
Metric  loss : 0.1
Metric  acc : 0.96

```

我们在测试数据集上看到了很好的结果。让我们试着改进一下架构，然后看看。我们可以建立一个中等规模的深度网络，看看结果是否比中等规模的网络更好。

```py
#Designing the Deep Neural Network [Medium – 2 Layers]
model  = Sequential()
model.add(Dense(512,input_dim = x_train_scaled.shape[1],activation="relu"))
model.add(Dense(512,activation="relu"))
model.add(Dense(1,activation = "sigmoid"))

model.compile(optimizer = "Adam",loss="binary_crossentropy",metrics=["accuracy"])

model.fit(x_train_scaled,y_train, validation_data = (x_val_scaled,y_val),epochs=3, batch_size=64)

```

**输出**

```py
Train on 1582048 samples, validate on 175784 samples
Epoch 1/3
1582048/1582048 [==============================] - 131s 83us/step - loss: 0.1953 - acc: 0.9141 - val_loss: 0.1381 - val_acc: 0.9421
Epoch 2/3
1582048/1582048 [==============================] - 130s 82us/step - loss: 0.1168 - acc: 0.9529 - val_loss: 0.1051 - val_acc: 0.9578
Epoch 3/3
1582048/1582048 [==============================] - 131s 83us/step - loss: 0.0911 - acc: 0.9646 - val_loss: 0.0869 - val_acc: 0.9667

```

训练和验证准确率进一步提高到 96%。这个只有 3 个纪元的小增长是令人敬畏的。我们现在可以对该架构模型的性能充满信心。我们肯定可以尝试更多的架构并检查结果，但让我们用一个更大更深的网络做最后一次尝试，看看 3 个时期的结果。如果我们只看到很小的改进，我们将在 15 个时期使用相同的架构，并使用该模型进行最终预测。

```py
#Designing the network Deep Neural Network – [Large + 2 Layers]
model  = Sequential()
model.add(Dense(1024,input_dim = x_train_scaled.shape[1],activation="relu"))
model.add(Dense(1024,activation = "relu"))
model.add(Dense(1,activation = "sigmoid"))

model.compile(optimizer = "Adam",loss="binary_crossentropy",metrics=["accuracy"])

model.fit(x_train_scaled,y_train, validation_data = (x_val_scaled,y_val),epochs=3, batch_size=64)

```

**输出**

```py
Train on 1582048 samples, validate on 175784 samples
Epoch 1/3
1582048/1582048 [==============================] - 465s 294us/step - loss: 0.2014 - acc: 0.9099 - val_loss: 0.1438 - val_acc: 0.9390
Epoch 2/3
1582048/1582048 [==============================] - 483s 305us/step - loss: 0.1272 - acc: 0.9469 - val_loss: 0.1184 - val_acc: 0.9524
Epoch 3/3
1582048/1582048 [==============================] - 487s 308us/step - loss: 0.1015 - acc: 0.9593 - val_loss: 0.1011 - val_acc: 0.9605

```

我们看到验证数据集的总体准确率为 96%,训练数据集也有类似的分数。因此，由于将规模从中等(512 个神经元)增加到更大(1024 个神经元)的架构，模型的性能实际上没有太大的改善。利用这些结果来验证我们的实验，让我们训练一个中等规模(512 个神经元)的两层深度网络 15 个时期，查看最终的训练和验证准确性，然后使用训练好的模型来评估测试数据集。

```py
#Designing the network Deep Neural Network – [Medium + 2 Layers]
model  = Sequential()
model.add(Dense(512,input_dim = x_train_scaled.shape[1],activation="relu"))
model.add(Dense(512,activation = "relu"))
model.add(Dense(1,activation = "sigmoid"))

model.compile(optimizer = "Adam",loss="binary_crossentropy",metrics=["accuracy"])

model.fit(x_train_scaled,y_train, validation_data = (x_val_scaled,y_val),epochs=15, batch_size=64)

```

**输出**

```py
Train on 1582048 samples, validate on 175784 samples
Epoch 1/15
1582048/1582048 [==============================] - 133s 84us/step - loss: 0.1949 - acc: 0.9142 - val_loss: 0.1375 - val_acc: 0.9426
Epoch 2/15
1582048/1582048 [==============================] - 133s 84us/step - loss: 0.1173 - acc: 0.9527 - val_loss: 0.1010 - val_acc: 0.9599
Epoch 3/15
1582048/1582048 [==============================] - 133s 84us/step - loss: 0.0911 - acc: 0.9643 - val_loss: 0.0887 - val_acc: 0.9660

    ----Skipping output from intermediate epochs -----

Epoch 14/15
1582048/1582048 [==============================] - 133s 84us/step - loss: 0.0402 - acc: 0.9863 - val_loss: 0.0614 - val_acc: 0.9821
Epoch 15/15
1582048/1582048 [==============================] - 133s 84us/step - loss: 0.0394 - acc: 0.9869 - val_loss: 0.0629 - val_acc: 0.9818

```

具有 512 个神经元和两层的中等规模架构的最终模型在训练和验证数据集上给出了很好的性能结果。我们对两个数据集的准确率都在 98%左右。现在让我们在测试数据集上验证模型性能。

```py
result = model.evaluate(x_test_scaled,y_test)

for i in range(len(model.metrics_names)):
    print("Metric ",model.metrics_names[i],":",str(round(result[i],2)))

```

**输出**

```py
439459/439459 [==============================] - 20s 45us/step
Metric  loss : 0.06
Metric  acc : 0.98

```

在看不见的测试数据集上的性能也很好，并且一致。我们的模型在测试数据集上表现得非常好。让我们看看模型的损失曲线，就像我们对回归用例所做的那样。我们将为训练和验证数据集绘制每个历元中的损失(对于该模式总共 15 个)。下面的代码片段利用了模型历史并绘制了这些指标。

```py
import matplotlib.pyplot as plt
%matplotlib inline
plt.plot(model.history.history['loss'])
plt.plot(model.history.history['val_loss'])
plt.title("Model's Training & Validation loss across epochs")
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()

```

![img/475458_1_En_4_Figd_HTML.jpg](img/475458_1_En_4_Figd_HTML.jpg)

我们可以看到两个数据集的损失都在减少。类似地，让我们看看模型训练期间的准确性度量。训练和验证数据集的准确性度量也存储在模型历史中。

```py
plt.plot(model.history.history['acc'])
plt.plot(model.history.history['val_acc'])
plt.title("Model's Training & Validation Accuracy across epochs")
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()

```

![img/475458_1_En_4_Fige_HTML.jpg](img/475458_1_En_4_Fige_HTML.jpg)

正如我们所看到的，精确度随着每个时期不断提高。在我们观察到任何指标的训练和验证数据之间存在巨大差距的情况下，这将是模型过度拟合的迹象。在这种情况下，模型在训练数据集上表现很好，但在看不见的数据(即验证和测试数据集)上表现很差。

## 摘要

在本章中，我们探索了一个业务用例，并通过利用 DNN 进行分类来解决它。我们从理解问题陈述的业务本质开始，探索所提供的数据，并扩充成适合 DNNs 的形式。我们试验了一些架构，牢记我们在第 [3](3.html) 章中学到的经验法则，我们看到了模型性能的一个主要缺点。然后，我们重新审视了这些数据，并使用标准化技术以一种更加 DL 友好的形式和架构为一些 dnn 表示这些数据，我们看到了惊人的结果。总的来说，我们加强了在数据工程、数据探索、DL 以及 Keras 和 Python 方面的学习。在下一章中，我们将探索通过超参数调整提高模型性能的其他策略，了解迁移学习，并探索大型软件的模型部署的高级流程。