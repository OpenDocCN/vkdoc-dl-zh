# 2\. 硬行动

在这一章中，我们将探索 Keras 框架，并从动手练习开始，学习 Keras 的基础知识以及一些 Python 和必要的 DL 主题。考虑到这是一个快速入门指南，需要注意的是:在 DL 中，我们没有足够的篇幅来详细讨论所有的主题。相反，我们将从一个简单的主题开始，探索其背后的基本思想，并添加参考资料，以便您可以更深入地了解该主题的更多基础知识。

## 设置环境

如前所述，我们将使用 TensorFlow 作为 Python 中的后端来开发带有 Keras 堆栈的 DL 模型。因此，为了开始，我们需要通过安装 Python、几个重要的 Python 包、TensorFlow 以及最后的 Keras 来设置我们的操场环境。

让我们开始吧。

### 选择 Python 版本

Python 目前有两个主要版本:2.7.x 和 3.x。尽管 Python 3.x 是最新版本，也是 Python 的未来，但由于开发人员社区在从 2.7 过渡到 3.x 方面的落后无能，已经出现了一系列冲突。不幸的是，许多开发人员仍然与 Python 2.7.x 版本联系在一起。然而，对于我们的用例，我强烈建议从 Python 3.x 开始，因为它是未来的趋势。有些人可能不愿意从 Python 3 开始，认为 3.x 版本中的许多包会有问题，但是对于几乎所有的实际用例，我们已经为 3.x 更新了所有主要的 DL、ML 和其他有用的包。

### 为 Windows、Linux 或 macOS 安装 Python

市场上有许多 Python 发行版。你可以从 python.org 官方网站下载并安装 Python，也可以选择任何流行的发行版。对于 ML 和 DL，最推荐的 Python 发行版是来自 Continuum Analytics 的 Anaconda 发行版。Anaconda 是 Python 的免费开源发行版，特别适合 ML 和 DL 大规模处理。它简化了整个包管理和部署过程，并附带一个非常易于使用的虚拟环境管理器和一些附加的编码工具，如 Jupyter 笔记本和 Spyder IDE。

要开始使用 Anaconda，您可以进入 [`www.anaconda.com/download/`](http://www.anaconda.com/download/) ，根据您选择的 OS (Mac/Windows/Linux)和架构(32 位/64 位)选择合适的版本。在写这本书的时候，Python 3 的最新版本是 3.6。当你读到这本书的时候，可能会有更新的版本。您应该轻松下载并安装 Anaconda Python 的最新版本。

下载安装程序后，请安装应用程序。

对于 Windows 用户，这将是一个简单的可执行文件安装。双击。exe 文件，并按照屏幕上的指导完成安装过程。

Linux 用户可以在导航到下载的文件夹后使用以下命令:

```py
bash Anaconda-latest-Linux-x86_64.sh

```

Mac 用户可以通过双击下载的来安装该软件。pkg 文件，然后按照屏幕上的说明进行操作。

Python 的 Anaconda 发行版通过安装 DL 所需的所有主要 Python 包，简化了 DL 和 ML 的过程。

### 安装 Keras 和 TensorFlow 后端

现在 Python 已经设置好了，我们需要安装 TensorFlow 和 Keras。使用 Python 的包管理器`pip`可以很容易地在 Python 中安装包。您可以在终端或命令提示符下使用命令`pip install package-name`安装任何 Python 包。

所以，让我们安装我们需要的包(即 TensorFlow 和 Keras)。

```py
pip install keras

```

然后

```py
pip install tensorflow

```

如果您在使用 TensorFlow 和 Keras 设置 Anaconda Python 时遇到任何问题，或者您希望仅在 Python 虚拟环境中进行实验，您可以在此浏览更详细的安装指南:

[T2`https://medium.com/@margaretmz/anaconda-jupyter-notebook-tensorflow-and-keras-b91f381405f8`](https://medium.com/@margaretmz/anaconda-jupyter-notebook-tensorflow-and-keras-b91f381405f8)

此外，如果您的系统有任何兼容 NVIDIA CUDA 的 GPU，您可能需要安装支持 GPU 的 TensorFlow。以下是在 Windows、Mac 和 Linux 上安装带 GPU 支持的 TensorFlow 的分步指南链接: [`www.tensorflow.org/install/`](http://www.tensorflow.org/install/)

要检查您的 GPU 是否与 CUDA 兼容，请浏览 NVIDIA 官方网站上的列表:

[T2`https://developer.nvidia.com/cuda-gpus`](https://developer.nvidia.com/cuda-gpus)

编写代码和开发模型，可以选择 Anaconda(即 Spyder)提供的 IDE，native terminal 或 command prompt，也可以选择基于 web 的笔记本 IDE，名为 Jupyter Notebooks。对于所有与数据科学相关的实验，我强烈推荐使用 Jupyter 笔记本电脑，因为它在探索性分析和再现性方面提供了便利。我们将在书中的所有实验中使用 Jupyter 笔记本。

Jupyter 笔记本预装了 Anaconda Python 如果您使用的是虚拟环境，您可能需要使用包管理器或命令来安装它

```py
conda install jupyter

```

要启动 Jupyter 笔记本，您可以使用 Anaconda Navigator 或输入命令

```py
jupyter notebook

```

在命令提示符或终端中；然后，Jupyter 应该在本地主机上的默认浏览器中启动。下面的截图显示了 Jupyter 在浏览器中运行的情况。

![img/475458_1_En_2_Figa_HTML.jpg](img/475458_1_En_2_Figa_HTML.jpg)

单击最右侧的“新建”按钮，并从下拉菜单中选择 Python。如果你已经安装了一个或多个虚拟环境，所有的虚拟环境都会显示在下拉列表中；请选择您所选择的 Python 环境。

选择后，您的 Jupyter 笔记本应该会打开，并准备好开始使用。下面的截图展示了一个 Jupyter 笔记本在浏览器中运行。

![img/475458_1_En_2_Figb_HTML.jpg](img/475458_1_En_2_Figb_HTML.jpg)

绿色突出显示的单元格是您编写代码的地方，Ctrl + Enter 将执行选定的单元格。您可以使用控制栏中的“+”图标添加更多单元格，或者从菜单栏中浏览其他选项。如果这是你第一次使用 Jupyter，我推荐导航菜单中的可用选项。

现在我们已经设置并运行了所有需要的工具，让我们从简单的带有 Keras 的 DL 构建块开始。

## Keras 中的 DL 入门

让我们从研究 DNN 及其逻辑组件开始，理解每个组件的用途以及这些构建块如何在 Keras 框架中映射。

如果您还记得第 [1](1.html) 章中的主题“分解 DL 模型”，我们已经将 DNN 中的逻辑组件定义为输入数据、神经元、激活函数、层(即神经元组)、神经元或边之间的连接、学习过程(即反向传播算法)和输出层。

让我们一个一个地看看这些逻辑组件。

### 输入数据

DL 算法的输入数据可以有多种类型。本质上，该模型将数据理解为“张量”。张量只不过是向量的一般形式，或者用计算机工程术语来说，是一个简单的 n 维矩阵。任何形式的数据最终都表示为一个齐次的数字矩阵。因此，如果数据是表格形式的，它将是一个二维张量，其中每一列代表一个训练样本，整个表/矩阵将是 m 个样本。为了更好地理解这一点，请看下图。

![img/475458_1_En_2_Figc_HTML.jpg](img/475458_1_En_2_Figc_HTML.jpg)

您还可以颠倒训练样本的表示(即，每一行可以是一个训练样本)，因此在测试示例中的学生通过/失败的上下文中，一行将指示一个学生的所有属性(他的分数、年龄等)。).对于 n 行，我们将有一个包含 n 个训练样本的数据集。但是在 DL 实验中，通常在一列中使用一个训练样本。因此，m 列将表示 m 个样本。

此外，DL 模型只能解释数字数据。如果数据集有任何分类数据，如值为“男性”和“女性”的“性别”，我们将需要将它们转换为一次性编码变量(即，简单地用值 0 或 1 表示列，其中 0 表示“男性”，1 表示“女性”，反之亦然)。

图像数据也需要转换成 n 维张量。我们不会在本书中讨论图像数据的 DL 模型，但是我想让你知道它作为输入数据的表示。图像作为三维张量存储在数据中，其中二维定义 2D 平面上的像素值，第三维定义 RGB 颜色通道的值。所以本质上，一个图像变成三维张量，n 个图像变成四维张量，其中第四维将堆叠一个三维张量图像作为训练样本。因此，如果我们有 100 张分辨率为 512 × 512 像素的图像，它们将被表示为形状为 512 × 512 × 3 × 100 的 4D 张量。

最后，在训练之前对输入值进行规范化、标准化或定标是一个很好的做法。对值进行归一化会将输入张量中的所有值带入 0–1 范围内，而标准化会将值带入平均值为 0 且标准差为 1 的范围内。这有助于减少计算，因为学习提高了很大的幅度，性能也提高了，因为激活函数(在下面讨论)表现得更合适。

### 神经元

在 DNN 的核心，我们有执行输出计算的神经元。一个神经元接收来自前一层神经元的一个或多个输入。如果神经元位于第一个隐藏层，它们将接收来自输入数据流的数据。在生物神经元中，当接收到具有较高影响的输入时，电信号作为输出给出。为了在数学神经元中映射该功能，我们需要一个函数，该函数对输入的和乘以相应的权重(在下面的视图中表示为 f(z ))进行操作，并根据输入以适当的值进行响应。如果接收到更高影响力的输入，则输出应该更高，反之亦然。它在某种程度上类似于激活信号(即，更高的影响->然后激活，否则去激活)。对计算出的输入数据起作用的函数称为激活函数。

![img/475458_1_En_2_Figd_HTML.jpg](img/475458_1_En_2_Figd_HTML.jpg)

### 激活功能

激活函数是这样一种函数，它采用上图所示的组合输入 z，对其应用函数，并传递输出值，从而试图模仿激活/停用函数。因此，激活函数通过计算组合输入的激活函数来确定神经元的状态。

一个快速的想法可能会出现在你的脑海中:当我们可以传递 z 的值作为最终输出时，为什么我们真的需要一个激活函数来计算组合输出 z？这里有几个问题。首先，输出值的范围将是-无穷大到+无穷大，在这种情况下，我们没有明确的方法来定义应该发生激活的阈值。其次，网络将在某种程度上变得无用，因为它不会真正学习。这就是微积分和导数的作用。为了简化故事，我们可以说，如果你的激活函数是线性函数(基本没有激活)，那么那个函数的导数就变成了 0；这成为一个大问题，因为用反向传播算法进行训练有助于向网络提供关于错误分类的反馈，从而有助于神经元通过使用函数的导数来调整其权重。如果这个值变成 0，网络就失去了这种学习能力。换句话说，我们可以说拥有 DNN 毫无意义，因为只有一层的输出与拥有 n 层的输出相似。为了简单起见，我们总是需要一个非线性激活函数(至少在所有隐藏层中)来让网络正确学习。

有多种选择可用作激活功能。最常见的是 sigmoid 函数和 ReLU(整流线性单元)。

### Sigmoid 激活函数

一个 sigmoid 函数被定义为![$$ \frac{1}{\left(1+{e}^{-z}\right)} $$](img/475458_1_En_2_Chapter_TeX_IEq1.png)，它呈现 0 和 1 之间的输出，如下图所示。非线性输出(如图所示的 s 形)很好地改善了学习过程，因为它非常类似于以下原则——*较低影响:低输出*和*较高影响:较高输出*——并且还将输出限制在 0 到 1 的范围内。

在 Keras 中，sigmoid 激活函数可作为 keras.activations.sigmoid(x)使用。

我们可以简单地用`import`命令将其导入 Python:

```py
import keras.activations.sigmoid

```

![img/475458_1_En_2_Fige_HTML.jpg](img/475458_1_En_2_Fige_HTML.jpg)

#### ReLU 激活功能

类似地，ReLU 使用函数 **f(z) = max(0，z)，**这意味着如果输出为正，它将输出相同的值，否则它将输出 0。该函数的输出范围如下图所示。

![img/475458_1_En_2_Figf_HTML.jpg](img/475458_1_En_2_Figf_HTML.jpg)

Keras 提供 ReLU as

```py
keras.activations.relu(x, alpha=0.0, max_value=None)

```

这个函数看起来可能是线性的，但事实并非如此。ReLU 是一个有效的非线性函数，事实上作为一个激活函数工作得非常好。这不仅提高了性能，而且大大有助于减少训练阶段的计算量。当 z 为负时，这是输出中 0 值的直接结果，从而使神经元失活。

但是由于输出为 0 的水平线，我们有时会面临严重的问题。例如，在上一节中，我们讨论了一条水平线，它是一个导数为 0 的常数，因此可能成为训练过程中的瓶颈，因为权重不容易更新。为了解决这个问题，提出了一种新的激活函数:Leaky ReLU，其中负值输出一条稍微倾斜的线而不是水平线，这有助于通过反向传播有效地更新权重。

泄漏 ReLU 定义为

*   f(z)= z；当 z >0 时

*   f(z)=∝z；当 z<0 且其中∝是定义为小常数的参数，比如 0.005 时

Keras 提供如下泄漏 ReLU:

```py
keras.layers.LeakyReLU(X, alpha=0.0, max_value=None).

```

我们可以通过设置一个小常数α的值来直接使用激活函数。

![img/475458_1_En_2_Figg_HTML.jpg](img/475458_1_En_2_Figg_HTML.jpg)

在 DNN 中可以使用的激活功能还有很多，在 Keras 中也可以使用。其他一些流行的是 tanh(双曲线 tan 激活)，swish 激活，elu(指数线性单位)，卢瑟(缩放 elu)，等等。

### 模型

DNN 的整体结构是使用 Keras 中的模型对象开发的。这提供了一种通过一个接一个地添加新层来创建层堆栈的简单方法。

定义模型最简单的方法是使用顺序模型，这样可以很容易地创建线性层堆栈。

下面的例子展示了一个简单的顺序模型的创建，该模型有一个层，后面有一个激活。该层将具有 10 个神经元，并且将接收具有 15 个神经元的输入，并且被 ReLU 激活功能激活。

```py
from keras.models import Sequential
from keras.layers import Dense, Activation

model = Sequential()
model.add(Dense(10, input_dim=15))
model.add(Activation('relu'))

```

### 层

DNN 中的层被定义为一组神经元或分层网络结构中逻辑上分离的组。随着 DL 变得越来越流行，人们对网络架构进行了多次实验，以提高各种用例的性能。用例围绕着常规的监督算法，如分类和回归、计算机视觉实验、扩展 DL 用于自然语言处理和理解、语音识别以及不同领域的组合。为了简化模型开发过程，Keras 为我们提供了几种类型的层和各种连接它们的方法。讨论所有这些问题超出了本书的范围。但是，我们将仔细查看几个层，并浏览一些重要的层，以了解其他高级用例，您可以在以后探索这些用例。

#### coreplayer

我们将在大多数用例中使用几个重要的层。

##### 致密层

密集层是一个常规的 DNN 层，它将定义层中的每个神经元与前一层中的每个神经元连接起来。例如，如果第 1 层有 5 个神经元，第 2 层(密集层)有 3 个神经元，则第 1 层和第 2 层之间的连接总数将是 15 (5 × 3)。因为它容纳了各层之间的所有可能的连接，所以它被称为“密集”层。

Keras 提供具有以下默认参数的密集层。

```py
keras.layers.Dense(units, activation=None, use_bias=True,
                   kernel_initializer='glorot_uniform',
                   bias_initializer='zeros',
                   kernel_regularizer=None,
                   bias_regularizer=None,
                   activity_regularizer=None,
                   kernel_constraint=None,
                   bias_constraint=None)

```

它为任何给定的层提供了许多定制。我们可以指定单元的数量(即该层的神经元)、激活类型、内核和偏差的类型初始化以及其他约束。大多数情况下，我们只是使用像单位和激活这样的参数。为简单起见，其余部分可以保留默认值。当我们在专门的用例中工作时，这些额外的参数变得很重要，在这些用例中，为给定的层使用特定类型的约束和初始化器是非常重要的。

我们还需要为 Keras 层定义输入形状。只需为第一层定义输入形状。后续层只需要定义的神经元数量。我们可以使用`input_dim`属性来定义输入有多少个维度。例如，如果我们有一个包含 10 个要素和 1000 个样本的表，我们需要将`input_dim`设置为 10，以便图层了解输入数据的形状。

示例:具有一个隐藏层和用于简单二进制分类的输出层的网络。

第 1 层有 5 个神经元，预期输入有 10 个特征；因此`, input_dim =10`。最后一层是输出，有一个神经元。

```py
model = Sequential()
model.add(Dense(5,input_dim=10,activation = "sigmoid"))
model.add(Dense(1,activation = "sigmoid"))

```

#### 脱落层

DL 中的 dropout 层通过在模型中引入正则化和泛化功能来帮助减少过拟合。从字面意义上来说，辍学层放弃了一些神经元或将它们设置为 0，并减少了训练过程中的计算。任意丢弃神经元的过程在减少过度拟合方面非常有效。我们将在第 [5](5.html) 章更深入地探讨这个主题，并理解过度拟合、模型概括背后的基本原理。

Keras 提供了具有以下默认参数的辍学层:

```py
keras.layers.Dropout(rate, noise_shape=None, seed=None)

```

我们在 DL 模型架构中的常规层之后添加了 dropout 层。以下代码显示了一个示例:

```py
model = Sequential()
model.add(Dense(5,input_dim=10,activation = "sigmoid"))
model.add(Dropout(rate = 0.1,seed=100))
model.add(Dense(1,activation = "sigmoid"))

```

#### 其他重要层

考虑到用例的多样性，Keras 内置了大多数已定义的层。在计算机视觉用例中，输入通常是图像。有专门的图层从图像中提取特征；它们被称为卷积层。同样，对于自然语言处理和类似的用例，有一个高级的 DNN，称为递归神经网络(RNN)。Keras 为其开发提供了几种不同类型的循环层。

这个列表相当长，我们现在不会涉及其他高级层。然而，为了让您及时了解最新情况，以下是 Keras 中的一些其他重要层，它们对您将来的高级用例很有用:

*   嵌入图层- [`https://keras.io/layers/embeddings/`](https://keras.io/layers/embeddings/)

*   [`https://keras.io/layers/convolutional/`](https://keras.io/layers/convolutional/) 卷积层数

*   汇集图层- [`https://keras.io/layers/pooling/`](https://keras.io/layers/pooling/)

*   合并图层- [`https://keras.io/layers/merge/`](https://keras.io/layers/merge/)

*   轮回层- [`https://keras.io/layers/recurrent/`](https://keras.io/layers/recurrent/)

*   归一化层层加多- [`https://keras.io/layers/normalization/`](https://keras.io/layers/normalization/)

您还可以在 Keras 中为不同类型的用例编写自己的层。更多详情可以在这里探讨: [`https://keras.io/layers/writing-your-own-keras-layers/`](https://keras.io/layers/writing-your-own-keras-layers/)

### 损失函数

损失函数是帮助网络理解它是否在正确的方向上学习的度量。用简单的话来描述损失函数，把它看作是你在一次考试中取得的分数。假设你参加了同一个主题的几次测试:你会用什么标准来理解你在每次测试中的表现？很明显，考试成绩。假设你在连续五次的语言测试中得了 56、60、78、90 和 96 分(满分为 100 分)。你会清楚地看到，考试成绩的提高表明你的表现有多好。如果考试分数一直在下降，那么结论就是你的表现在下降，你需要改变你的学习方法或材料来提高。

同样，网络如何理解它是否在每次迭代中改进它的学习过程？它使用损失函数，类似于测试分数。损失函数本质上测量目标的损失。假设你正在开发一个模型来预测一个学生是否会通过或失败，通过或失败的机会是由概率定义的。因此，1 表示他有 100%的把握通过，0 表示他肯定会失败。

该模型从数据中学习，并预测该学生的分数为 0.87，可以通过。因此，这里的实际损失是 1.00–0.87 = 0.13。如果它用一些参数更新重复该练习以便改进，并且现在实现了 0.40 的损失，它将理解它所做的改变没有帮助网络适当地学习。或者，0.05 的新损失将指示来自学习的更新或改变在正确的方向上。

基于数据结果的类型，我们在 ML 和 DL 中定义了几个标准损失函数。对于回归用例(即，最终预测将是一个连续的数字，如学生的分数、商店售出的产品数量、联络中心收到的客户来电数量等)。)，以下是一些常用的损失函数:

*   均方差-实际值和预测值之间的平均平方差。差值的平方使得更容易对差值越高的模型进行更多的惩罚。因此，差 3 将导致损失 9，但差 9 将返回损失 81。
    *   数学上的等价形式是![$$ \sum \limits_{n=1}^k\frac{{\left( Actual- Predicted\right)}^2}{k} $$](img/475458_1_En_2_Chapter_TeX_IEq2.png)

    *   喀拉斯当量

        `keras.losses.mean_squared_error(y_actual, y_pred)`

*   平均绝对误差–实际和预测之间的平均绝对误差。
    *   数学上的等价形式是![$$ \sum \limits_{n=1}^k\mid Actual- Predicted\mid $$](img/475458_1_En_2_Chapter_TeX_IEq3.png)

    *   喀拉斯当量

        `keras.losses.mean_absolute_error(y_actual, y_pred)`

*   类似地，很少有其他变体
    *   MAPE-平均绝对百分比误差

        `keras.losses.mean_absolute_percentage_error`

    *   男性的

        `keras.losses.mean_squared_logarithmic_error`

对于分类结果，您的预测将是针对一个类，如学生是否会通过(1)或失败(0)，客户是否会购买，客户是否会拖欠付款，等等。一些用例可能有多个类作为结果，比如对疾病类型进行分类(A 型、B 型或 C 型)，将图像分类为猫、狗、汽车、马、风景等等。

在这种情况下，由于显而易见的原因，前面定义的损失不能使用。我们需要将课堂的结果量化为概率，并根据概率估计值定义损失为预测。

Keras 中分类结果损失的几种常见选择如下:

*   **二元交叉熵:**定义分类结果为二元变量时的损失，即有两种可能的结果:(通过/失败)或(是/否)
    *   数学形式应该是

        损耗=[y * log(p)+(1y)* log(1p)]

    *   喀拉斯当量

        `keras.losses.binary_crossentropy(y_actual, y_predicted)`

*   **分类交叉熵:**定义分类结果为非二元时的损失，即> 2 种可能的结果:(是/否/可能)或(1 型/2 型/…n 型)
    *   数学形式应该是

        ![$$ \mathrm{Loss}=-\sum \limits_i^n\kern0.375em {y}_i^{`} lo{g}_2\ {y}_i $$](img/475458_1_En_2_Chapter_TeX_IEq4.png)

    *   喀拉斯当量

```py
keras.losses.categorical_crossentropy(y_actual, y_predicted)

```

### 优化者

模型训练最重要的部分是优化器。到目前为止，我们已经通过一种叫做反向传播的算法解决了向模型提供反馈的过程；这其实是一种优化算法。

为了添加更多的上下文，想象一下您定义的用于分类学生是否会通过或失败的模型结构。通过定义具有神经元数量、激活函数和输入输出形状的层序列而创建的结构在开始时用随机权重初始化。确定一个神经元对下一个神经元或最终输出的影响的权重由网络在学习过程中更新。

简而言之，具有随机权重和确定结构的网络是模型的起点。该模型可以在这一点上做出预测，但几乎总是没有价值。该网络采用一个训练样本，并使用其值作为第一层神经元的输入，然后产生具有定义的激活函数的输出。输出现在成为下一层的输入，依此类推。最终层的输出将是对训练样本的预测。这就是损失函数发挥作用的地方。损失函数有助于网络了解当前的一组权重在训练样本上的表现是好是坏。模型的下一步是减少损失。它如何知道应该对权重执行什么步骤或更新来减少损失呢？优化器功能有助于理解这一步。优化函数是一种数学算法，它使用微积分中的导数、偏导数和链规则来了解通过对神经元的权重进行微小的改变，网络会在损失函数中看到多少变化。损失函数的变化将是增加或减少，这有助于确定连接权重所需的变化方向。从输入层到输出层的一个训练样本的计算被称为一遍。通常，由于系统内存的限制，训练会分批进行。批处理是来自整个输入的训练样本的集合。网络在处理一批中的所有样本后更新其权重。这被称为一次迭代(即一批中所有样品的成功通过，随后在网络中进行重量更新)。利用逐批权重更新对输入数据中提供的所有训练样本的计算被称为一个时期。在每次迭代中，网络利用优化器函数对其权重参数(在开始时随机初始化)进行小的改变，以通过减少损失函数来改善最终预测。一步一步地，通过几次迭代和几次历元，网络更新其权重，并学习对给定的训练样本做出正确的预测。

优化器功能运行的数学解释以一种简单的方式进行了抽象，便于您理解和体会在培训过程中 DNN 中发生的后台操作。深入的数学方程和优化过程的推理超出了本书的范围。如果你对学习数学和优化算法的实际过程非常好奇，我会推荐阅读 Santanu Pattanayak(2017 年出版)的《用 *TensorFlow* 进行 *Pro 深度学习》一书中的一章。这本书用一种非常直观的方法解释了 DL 背后的数学原理，做了一件了不起的工作。我向所有探索 DL 的博士生强烈推荐这本书。*

鉴于您对整个优化过程有相当的了解，我想花一点时间来讨论 Keras 中可用的各种优化算法。

#### 随机梯度下降

SGD 对每个训练样本执行迭代(即，在每个训练样本通过后，它计算损失并更新权重)。由于权重更新过于频繁，总的损失曲线会非常嘈杂。但是，与其他优化相比，优化速度相对较快。

权重更新的公式可以简单地表示如下:

*   权重=权重-学习率*损失

*   其中学习速率是我们在网络架构中定义的参数。

*   比方说，学习率=0.01

Keras 为 SGD 提供

```py
keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)

```

对于每个训练样本的更新，我们需要在模型训练函数中使用 batch_size=1。

为了减少 SGD 优化中的高波动，更好的方法是通过提供一个微型批次来减少迭代次数，这样就可以对一个批次中的所有样本的损失求平均值，并在批次结束时更新权重。这种方法更加成功，培训过程更加顺畅。批量大小通常设置为 2 的幂(即 32、64、128 等)。).

#### 圣经》和《古兰经》传统中）亚当（人类第一人的名字

Adam 是 Adaptive Moment Estimation 的缩写，是 DL 中最流行、最广泛使用的优化器。在大多数情况下，您可以盲目地选择 Adam 优化器，而忘记其他优化方案。这种优化技术计算每个参数的自适应学习率。它定义损失梯度的动量和方差，并利用组合效应来更新权重参数。动量和方差一起帮助平滑学习曲线并有效地改进学习过程。

数学表示可以通过以下方式简化:

*   权重=权重-(动量和方差的组合)

Keras 提供了 Adam 优化器

```py
keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999,
epsilon=None, decay=0.0, amsgrad=False)

```

参数β_ 1 和β_ 2 分别用于计算动量和方差。默认值非常有效，在大多数用例中不需要改变。

#### 其他重要的优化器

还有许多其他流行的优化器也可以用于不同的 DL 模型。讨论所有这些问题超出了本书的范围。为了让您更好地了解可用的选项，我想列出几个在 Keras 中使用和可用的其他流行的优化选项:

*   阿达格拉德

*   阿达德尔塔

*   RMSProp

*   阿达玛斯

*   那达慕

每种优化技术都有自己的优缺点。我们在 DL 中经常遇到的一个主要问题是消失梯度和鞍点问题。您可以更详细地研究这些问题，同时为您的问题选择最佳的优化器。但是对于大多数用例，Adam 总是工作得很好。

### 韵律学

类似于损失函数，我们也在 Keras 中为模型定义度量。简单地说，指标可以理解为用于判断模型在不同的未知数据集(也称为验证数据集)上的性能的函数。度量和损失函数之间的唯一区别在于，度量的结果不用于训练关于优化的模型。它们仅用于在报告时验证测试结果。

Keras 中的一些可用指标选项如下:

*   二进制精度- keras.metrics.binary_accuracy

*   分类准确性-keras . metrics . caetogracal _ Accuracy

*   稀疏分类准确性-keras . metrics . sparse _ category _ Accuracy

您还可以为您的模型指标定义定制函数。Keras 为您提供了使用用户定义的指标轻松配置模型的能力。

### 模型配置

现在我们已经了解了 Keras 中 DNN 的最基本的构建模块，我们可以看看最终的模型配置步骤，它将前面的所有组件编排在一起。

一旦你设计了你的网络，Keras 为你提供了一个简单的“编译”命令一步到位的模型配置过程。为了编译一个模型，我们需要提供三个参数:一个优化函数、一个损失函数和一个度量模型在验证数据集上的性能的指标。

以下示例构建了一个具有两个隐藏层的 DNN，分别具有 32 个和 16 个神经元，并具有 ReLU 激活函数。最终输出是使用 sigmoid 激活的二进制分类数值输出。我们使用 Adam 优化器编译模型，并将二进制交叉熵定义为损失函数，将“准确性”定义为验证的度量。

```py
from keras.models import Sequential
from keras.layers import Dense, Activation

model = Sequential()
model.add(Dense(32, input_dim=10,activation = "relu"))
model.add(Dense(16,activation = "relu"))
model.add(Dense(1,activation = "sigmoid"))

model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])

```

### 模特培训

一旦我们配置了一个模型，我们就已经准备好了模型所需的所有部分。我们现在可以继续用数据训练模型。在训练时，为我们提供一个验证数据集来评估模型在每个时期后的表现是否符合预期，这始终是一个好的做法。该模型利用训练数据来训练自己并学习模式，在每个时期结束时，它将使用看不见的验证数据来进行预测和计算指标。验证数据集上的性能是整体性能的良好提示。

对于验证数据，通常的做法是将可用数据分成三部分，比例为 60:20:20。我们将 60%用于训练，20%用于验证，最后 20%用于测试。这个比例不是强制性的。您可以根据自己的选择灵活地改变比例。一般来说，当你有非常大的训练数据集时，比如说 n>1MN 个样本，可以把 95%用于训练，2%用于验证，3%用于测试。同样，该比率是您根据自己的判断和可用数据做出的选择。

Keras 为模型对象提供了一个拟合函数，以便使用提供的训练数据进行训练。

下面是一个调用其 fit 方法的示例模型。在这一点上，假设您已经按照前面的讨论定义并配置(编译)了模型架构。

```py
model.fit(x_train, y_train, batch_size=64, epochs=3, validation_data=(x_val, y_val))

```

我们在名为 x_train 的训练数据集上训练了一个模型，实际标签在 y_train 中。我们选择批量为 64。因此，如果有 500 个训练样本，模型将在更新模型权重之前一次成批接收和处理 64 个样本。如果不可用，最后一批可能具有少于 64 个训练样本。我们已经将纪元的数量设置为三个；因此，每批 64 个样本中训练 500 个样本的整个过程将重复三次。此外，我们还提供了 x_val 和 y_val 形式的验证数据集。在每个时期结束时，模型将使用验证数据进行预测，并计算模型配置的度量参数中定义的性能度量。

现在，我们已经拥有了设计、配置和训练模型所需的所有部分，让我们将所有部分放在一起，看看它是如何工作的。

```py
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation

# Generate dummy training dataset
np.random.seed(2018)
x_train = np.random.random((6000,10))
y_train = np.random.randint(2, size=(6000, 1))

# Generate dummy validation dataset
x_val = np.random.random((2000,10))
y_val = np.random.randint(2, size=(2000, 1))

# Generate dummy test dataset
x_test = np.random.random((2000,10))
y_test = np.random.randint(2, size=(2000, 1))

#Define the model architecture
model = Sequential()
model.add(Dense(64, input_dim=10,activation = "relu")) #Layer 1
model.add(Dense(32,activation = "relu"))               #Layer 2
model.add(Dense(16,activation = "relu"))               #Layer 3
model.add(Dense(8,activation = "relu"))                #Layer 4
model.add(Dense(4,activation = "relu"))                #Layer 5
model.add(Dense(1,activation = "sigmoid"))             #Output Layer

#Configure the model
model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])

#Train the model
model.fit(x_train, y_train, batch_size=64, epochs=3, validation_data=(x_val,y_val))

```

训练模型时的输出显示如下:

![img/475458_1_En_2_Figh_HTML.jpg](img/475458_1_En_2_Figh_HTML.jpg)

我们可以看到，在每个时期之后，模型打印平均训练损失和准确度以及验证损失和准确度。我们可以使用这些中间结果来判断模型的性能。在大多数大型 DL 用例中，我们会有几个时期用于训练。一个很好的实践是使用我们配置的度量标准来跟踪模型性能，以便在几个时期后看到结果。如果结果似乎对您不利，停止培训并重新访问模型架构和配置可能是个好主意。

### 模型评估

在所有前面的例子中，我们已经研究了模型开发步骤的特定部分，或者我们已经以模型训练结束。到目前为止，我们还没有讨论模型性能。理解你的模型在一个未知的测试数据集上的表现是非常重要的。

Keras 提供了模型对象，配备了内置的模型评估和另一个函数来预测测试数据集的结果。让我们使用前面示例中生成的训练模型和虚拟测试数据来看看这两种情况。

Keras 为时序模型提供的方法如下所示:

```py
evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)

```

我们在参数 x 和 y 中提供测试数据和测试标签。如果测试数据也很大，并且预计会消耗大量内存，您可以使用批处理大小来告诉 Keras 模型按批处理方式进行预测，然后合并所有结果。

```py
print(model.evaluate(x_test,y_test))
[0.6925005965232849, 0.521]

```

在 evaluate 方法中，模型返回损失值和模型配置中定义的所有指标。这些度量标签在模型属性 metrics_names 中可用。

```py
print(model.metrics_names)
['loss', 'acc']

```

因此，我们可以看到，该模型在测试数据集上的总体准确率为 52%。这肯定不是一个好的模型结果，但这是意料之中的，因为我们只使用了一个虚拟数据集。

或者，您可以使用模型的预测方法，并利用实际的概率预测(对于此用例，因为是二进制分类):

```py
#Make predictions on the test dataset and print the first 10 predictions
pred = model.predict(x_test)
pred[:10]

```

**输出**

![img/475458_1_En_2_Figi_HTML.jpg](img/475458_1_En_2_Figi_HTML.jpg)

该输出可用于做出更精确的最终预测。一个简单的例子是，模型将使用 0.5 作为预测的阈值。因此，任何高于 0.5 的预测值都被归类为 1(比如说，通过)，其他的被归类为 0(失败)。

根据您的使用情况，您可能希望稍微调整您的预测，以便更积极地正确预测 1(通过)，因此您可能选择阈值为 0.6 而不是 0.5，反之亦然。

## 把所有的积木放在一起

我希望你现在能理解我们在第 [1](1.html) 章最后一节看到的第一个 DNN 模型。在理解所有的基本构建模块之前，理解模型开发中使用的代码的推理是非常困难的。

既然我们已经准备好了所有基本的必要成分，在我们结束本章之前，让我们看看更具体的用例。要做到这一点，让我们拿一个更好的数据集，看看事情是什么样子的。Keras 还提供了一些数据集供您使用。这些都是真实的数据集，通常被大多数初学者在最初的 ML 和 DL 实验中使用。

在我们的实验中，让我们选择一个流行的 Keras 数据集来开发模型。我们可以从波士顿房价数据集开始。它取自卡内基梅隆大学的 StatLib 图书馆。数据存在于亚马逊 S2 桶中，我们可以通过使用专门为数据集提供的简单 Keras 命令来下载。

```py
#Download the data using Keras; this will need an active internet connection
from keras.datasets import boston_housing
(x_train, y_train), (x_test, y_test) = boston_housing.load_data()

```

数据集被直接下载到 Python 环境中，并且随时可以使用。我们来看看数据是什么样子的。我们将使用基本的 Python 命令来查看数据的类型、长度和宽度，以及内容预览。

```py
#Explore the data structure using basic python commands
print("Type of the Dataset:",type(y_train))
print("Shape of training data :",x_train.shape)
print("Shape of training labels :",y_train.shape)
print("Shape of testing data :",type(x_test))
print("Shape of testing labels :",y_test.shape)

```

**输出**

```py
Type of the Dataset: <class 'numpy.ndarray'>
Shape of training data : (404, 13)
Shape of training labels : (404,)
Shape of testing data : <class 'numpy.ndarray'>
Shape of testing labels : (102,)

```

我们可以看到，训练和测试数据集是 Python numpy 数组。Numpy 是一个 Python 库，用于处理大型多维数组。我们有 404 行数据，在训练数据集中有 13 个特征，在测试数据集中有 102 行数据有相同数量的特征。总的来说，培训和测试的比例大约是 80:20。我们可以将 402 行训练数据进一步分为 300 行用于训练，102 行用于验证。

好的，数据结构和它的形状看起来很棒。让我们快速浏览一下数据集的内容。前面的代码展示了我们有 13 列数据。为了理解实际的列名，我们需要参考 CMU 提供的数据字典。你可以在这里找到更多关于数据集的细节: [`http://lib.stat.cmu.edu/datasets/boston`](http://lib.stat.cmu.edu/datasets/boston) 。

下表展示了数据中的特征描述。列表中的最后一行是我们用例中的标签或实际房价。

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

**列名**

 | 

**描述**

 |
| --- | --- |
| 卷曲 | 按城镇分列的人均犯罪率 |
| 锌 | 划作 25，000 平方英尺以上地段的住宅用地比例。制成 |
| 印度西北部的河流 | 每个城镇的非零售商业用地比例 |
| 临床科研信息整合平台 | 查尔斯河虚拟变量(= 1，如果区域边界为河流；否则为 0) |
| 氮氧化合物 | 一氧化氮浓度(百万分之一) |
| 空间 | 每所住宅的平均房间数 |
| 年龄 | 1940 年以前建造的自有住房比例 |
| 阴间 | 到五个波士顿就业中心的加权距离 |
| 皇家舞蹈学院 | 放射状公路可达性指数 |
| 税 | 每 10，000 美元的全额财产税税率 |
| ptratio(ptratio) | 按城镇分列的师生比例 |
| B | 1000(Bk–0.63)^2，其中 bk 是按城镇划分的黑人比例 |
| 上帝啊 | %较低的人口地位 |
| 矢量 | 以千美元为单位的自有住房的中值 |

为了查看训练数据集的内容，我们可以使用 Python 的 numpy 库为 numpy n 维数组提供的索引切片选项。

```py
x_train[:3,:]

```

**输出**

```py
array([[1.23247e+00, 0.00000e+00, 8.14000e+00, 0.00000e+00, 5.38000e-01, 6.14200e+00, 9.17000e+01, 3.97690e+00,
        4.00000e+00, 3.07000e+02, 2.10000e+01, 3.96900e+02, 1.87200e+01],
       [2.17700e-02, 8.25000e+01, 2.03000e+00, 0.00000e+00, 4.15000e-01, 7.61000e+00, 1.57000e+01, 6.27000e+00, 2.00000e+00, 3.48000e+02, 1.47000e+01, 3.95380e+02,
        3.11000e+00],
       [4.89822e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 6.31000e-01, 4.97000e+00, 1.00000e+02, 1.33250e+00,
        2.40000e+01, 6.66000e+02, 2.02000e+01, 3.75520e+02,
        3.26000e+00]])

```

所有列都有数值，所以不需要数据转换。通常，一旦我们导入了数据集，我们将需要广泛地研究数据，并且在开始开发模型之前，几乎总是要清理、处理和扩充数据。

但是现在，我们将直接使用一个简单的模型，看看结果是什么样的。

```py
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation

#Extract the last 100 rows from the training data to create the validation datasets.
x_val = x_train[300:,]
y_val = y_train[300:,]

#Define the model architecture
model = Sequential()
model.add(Dense(13, input_dim=13, kernel_initializer="normal", activation="relu"))
model.add(Dense(6, kernel_initializer="normal", activation="relu"))
model.add(Dense(1, kernel_initializer="normal"))

# Compile model
model.compile(loss='mean_squared_error', optimizer="adam", metrics=['mean_absolute_percentage_error'])

#Train the model
model.fit(x_train, y_train, batch_size=32, epochs=3, validation_data=(x_val,y_val))

```

**输出**

```py
Train on 404 samples, validate on 104 samples
Epoch 1/3
404/404 [==============================] - 2s 4ms/step - loss: 598.8595 - mean_absolute_percentage_error: 101.7889 - val_loss: 681.4912 - val_mean_absolute_percentage_error: 100.0789
Epoch 2/3
404/404 [==============================] - 0s 81us/step - loss: 583.6991 - mean_absolute_percentage_error: 99.7594 - val_loss: 674.8345 - val_mean_absolute_percentage_error: 99.2616
Epoch 3/3
404/404 [==============================] - 0s 94us/step - loss: 573.6101 - mean_absolute_percentage_error: 98.3180 - val_loss: 654.3787 - val_mean_absolute_percentage_error: 96.9662

```

我们已经为回归用例创建了一个简单的双隐藏层模型。我们选择 MAPE 作为度量单位。一般来说，这不是研究模型性能的最佳选择，但是它的优点是理解结果简单。它给出了一个简单的误差百分比值，比如 10%的误差。所以，如果你知道你的预测的平均范围，你可以很容易地估计出预测的结果。

现在让我们训练模型，并使用评估函数来研究模型的结果。

```py
results = model.evaluate(x_test, y_test)

for i in range(len(model.metrics_names)):
    print(model.metrics_names[i]," : ", results[i])

```

**输出**

```py
102/102 [==============================] - 0s 87us/step
loss  :  589.7658882889093
mean_absolute_percentage_error  :  96.48218611174939

```

我们可以看到，MAPE 约为 96%，对于模型性能来说，这实际上不是一个很好的数字。这将转化为我们的模型预测约 96%的误差。所以，总的来说，如果一栋房子的价格是 10K，我们的模型会预测到 2 万英镑。

在 DL 中，模型在每次迭代后更新权重，并在每个时期后进行评估。由于更新非常小，一般模型通常需要相当多的历元才能正确学习。为了再次测试性能，让我们将时期的数量从 3 个增加到 30 个。这将显著增加计算量，并且可能需要一段时间才能执行。但由于这是一个相当小的数据集，用 30 个历元进行训练应该不成问题。它应该在您的系统上运行大约 1 分钟。

```py
#Train the model
model.fit(x_train, y_train, batch_size=32, epochs=30, validation_data=(x_val,y_val))

```

**输出**

```py
Train on 404 samples, validate on 104 samples
Epoch 1/1000
404/404 [==============================] - 0s 114us/step - loss: 536.6662 - mean_absolute_percentage_error: 93.4381 - val_loss: 580.3155 - val_mean_absolute_percentage_error: 88.6968
Epoch 2/1000
404/404 [==============================] - 0s 143us/step - loss: 431.7025 - mean_absolute_percentage_error: 79.0697 - val_loss: 413.4064 - val_mean_absolute_percentage_error: 67.0769

```

***跳过中间时期的输出。**T3】*

(仅添加最后三个时期的输出，即 28 至 30)

```py
Epoch 28/30
404/404 [==============================] - 0s 111us/step - loss: 6.0758 - mean_absolute_percentage_error: 9.5185 - val_loss: 5.2524 - val_mean_absolute_percentage_error: 8.3853
Epoch 29/30
404/404 [==============================] - 0s 100us/step - loss: 6.2895 - mean_absolute_percentage_error: 10.1037 - val_loss: 6.0818 - val_mean_absolute_percentage_error: 8.9386
Epoch 30/30
404/404 [==============================] - 0s 111us/step - loss: 6.0761 - mean_absolute_percentage_error: 9.8201 - val_loss: 7.3844 - val_mean_absolute_percentage_error: 8.9812

```

如果我们仔细观察验证数据集的损失和 MAPE，我们可以看到显著的改进。从上例的 96%降到了现在的 8.9%。

让我们看看测试结果。

```py
results = model.evaluate(x_test, y_test)

for i in range(len(model.metrics_names)):
    print(model.metrics_names[i]," : ", results[i])

```

**输出**

```py
102/102 [==============================] - 0s 92us/step
loss  :  22.09559840782016
mean_absolute_percentage_error  :  16.22196163850672

```

我们可以看到，结果有了显著的改善，但验证数据集和测试数据集的 MAPE 之间似乎仍有很大的差距。如前所述，这种差距表明模型过度拟合，或者简单地说，学习过程过于复杂。我们将在下一章中详细讨论减少 DNNs 中过拟合的步骤，以获得更大更好的用例。到目前为止，我们已经成功地在真实数据集(虽然很小)上探索了 Keras，并在 Keras 中的 DL 构建块上使用了我们的知识。

## 摘要

在本章中，我们通过动手练习以及主题的上下文深度深入探讨了 Keras。我们研究了 DL 的基本构件及其在 Keras 中的实现。在使用 Keras 开发 DNN 模型时，我们研究了如何将不同的构件组合在一起。在下一章中，我们将开始一步一步地探索一个真实的用例，通过探索、清理、提取和应用必要的转换来为开发 DL 模型准备好数据。