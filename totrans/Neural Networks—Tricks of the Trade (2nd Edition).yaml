- en: Preface To The Second Edition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二版前言
- en: There have been substantial changes in the field of neural networks since the
    first edition of this book in 1998. Some of them have been driven by external
    factors such as the increase of available data and computing power. The Internet
    made public massive amounts of labeled and unlabeled data. The ever-increasing
    raw mass of user-generated and sensed data is made easily accessible by databases
    and Web crawlers. Nowadays, anyone having an Internet connection can parse the
    4,000,000+ articles available on Wikipedia and construct a dataset out of them.
    Anyone can capture a Web TV stream and obtain days of video content to test their
    learning algorithm.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自1998年本书第一版以来，神经网络领域发生了显著变化。其中一些变化受到外部因素的推动，例如可用数据和计算能力的增加。互联网提供了大量标记和未标记的数据。用户生成和感知数据的不断增加通过数据库和网络爬虫变得易于获取。如今，任何有互联网连接的人都可以解析维基百科上超过400万个可用文章，并从中构建数据集。任何人都可以捕获网络电视流并获得数天的视频内容来测试他们的学习算法。
- en: Another development is the amount of available computing power that has continued
    to rise at steady rate owing to progress in hardware design and engineering. While
    the number of cycles per second of processors has thresholded due to physics limitations,
    the slow-down has been offset by the emergence of processing parallelism, best
    exemplified by the massively parallel graphics processing units (GPU). Nowadays,
    everybody can buy a GPU board (usually already available in consumer-grade laptops),
    install free GPU software, and run computation-intensive simulations at low cost.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个发展是可用计算能力的持续增长，这得益于硬件设计和工程的进步。尽管由于物理限制，处理器的每秒周期数已经达到了阈值，但这一减缓被处理并行性的出现所抵消，最典型的例子就是大规模并行图形处理单元（GPU）。如今，每个人都可以购买一块GPU板（通常在消费级笔记本电脑中已有），安装免费的GPU软件，以低成本运行计算密集型仿真。
- en: 'These developments have raised the following question: Can we make use of this
    large computing power to make sense of these increasingly complex datasets? Neural
    networks are a promising approach, as they have the intrinsic modeling capacity
    and flexibility to represent the solution. Their intrinsically distributed nature
    allows one to leverage the massively parallel computing resources.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发展引发了以下问题：我们能否利用这种巨大的计算能力来理解这些日益复杂的数据集？神经网络是一种有前途的方法，因为它们具有内在的建模能力和灵活性来表示解决方案。它们内在的分布特性允许利用大规模并行计算资源。
- en: During the last two decades, the focus of neural network research and the practice
    of training neural networks underwent important changes. Learning in deep (or
    "deep learning") has to a certain degree displaced the once more prevalent regularization
    issues, or more precisely, changed the practice of regularizing neural networks.
    Use of unlabeled data via unsupervised layer-wise pretraining or deep unsupervised
    embeddings is now often preferred over traditional regularization schemes such
    as weight decay or restricted connectivity. This new paradigm has started to spread
    over a large number of applications such as image recognition, speech recognition,
    natural language processing, complex systems, neuroscience, and computational
    physics.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的二十年中，神经网络研究的重点和神经网络训练的实践经历了重要变化。深度学习在某种程度上取代了曾经更为普遍的正则化问题，或者更确切地说，改变了正则化神经网络的实践。通过无监督的逐层预训练或深度无监督嵌入使用未标记数据，现在通常更受欢迎，而不是传统的正则化方案，如权重衰减或限制连接。这一新范式已经开始在图像识别、语音识别、自然语言处理、复杂系统、神经科学和计算物理等多个应用中传播。
- en: The second edition of the book *reloads* the first edition with more tricks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的第二版在第一版的基础上*重新加载*了更多技巧。
- en: These tricks arose from 14 years of theory and experimentation (from 1998 to
    2012) by some of the world's most prominent neural networks researchers.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技巧源于世界上一些最著名的神经网络研究者在1998年至2012年间的14年理论和实验。
- en: These tricks can make a substantial difference (in terms of speed, ease of implementation,
    and accuracy) when it comes to putting algorithms to work on real problems. Tricks
    may not necessarily have solid theoretical foundations or formal validation. As
    Yoshua Bengio states in Chap. 19, "the wisdom distilled here should be taken as
    a guideline, to be tried and challenged, not as a practice set in stone" [1].
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技巧在将算法应用于实际问题时，可以在速度、实现难度和准确性方面带来显著差异。技巧不一定有坚实的理论基础或正式验证。正如Yoshua Bengio在第19章所述：“这里提炼的智慧应被视为指导方针，应尝试和挑战，而不是固定不变的实践”[1]。
- en: The second part of the new edition starts with tricks to faster optimize neural
    networks and make more efficient use of the potentially infinite stream of data
    presented to them. Chapter 18 [2] shows that a simple stochastic gradient descent
    (learning one example at a time) is suited for training most neural networks.
    Chapter 19 [1] introduces a large number of tricks and recommendations for training
    feed-forward neural networks and choosing the multiple hyperparameters.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 新版的第二部分开始于更快地优化神经网络的技巧，并更有效地利用呈现给它们的潜在无限数据流。第18章[2]显示，简单的随机梯度下降（一次学习一个示例）适合训练大多数神经网络。第19章[1]介绍了大量技巧和建议，用于训练前馈神经网络并选择多个超参数。
- en: When the representation built by the neural network is highly sensitive to small
    parameter changes, for example, in recurrent neural networks, second-order methods
    based on mini-batches such as those presented in Chap. 20 [9] can be a better
    choice. The seemingly simple optimization procedures presented in these chapters
    require their fair share of tricks in order to work optimally. The software Torch7
    presented in Chap. 21 [5] provides a fast and modular implementation of these
    neural networks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络构建的表示对小参数变化非常敏感时，例如在递归神经网络中，基于小批量的二阶方法（如第20章中提出的方法）[9]可能是更好的选择。这些章节中提出的看似简单的优化过程需要相应的技巧以实现最佳效果。第21章介绍的软件Torch7[5]提供了这些神经网络的快速和模块化实现。
- en: The novel second part of this volume continues with tricks to incorporate invariance
    into the model. In the context of image recognition, Chap. 22 [4] shows that translation
    invariance can be achieved by learning a k-means representation of image patches
    and spatially pooling the k-means activations. Chapter 23 [3] shows that invariance
    can be injected directly in the input space in the form of elastic distortions.
    Unlabeled data are ubiquitous and using them to capture regularities in data is
    an important component of many learning algorithms. For example, we can learn
    an unsupervised model of data as a first step, as discussed in Chaps. 24 [7] and
    25 [10], and feed the unsupervised representation to a supervised classifier.
    Chapter 26 [12] shows that similar improvements can be obtained by learning an
    unsupervised embedding in the deep layers of a neural network, with added flexibility.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本卷新颖的第二部分继续介绍将不变性纳入模型的技巧。在图像识别的背景下，第22章[4]显示，通过学习图像块的k均值表示并对k均值激活进行空间池化，可以实现平移不变性。第23章[3]展示了不变性可以通过弹性扭曲直接注入输入空间。无标签数据无处不在，利用它们捕捉数据中的规律是许多学习算法的重要组成部分。例如，我们可以学习数据的无监督模型作为第一步，如第24章[7]和第25章[10]中讨论的那样，然后将无监督表示输入到监督分类器中。第26章[12]展示了通过在神经网络的深层学习无监督嵌入，可以获得类似的改进，并增加灵活性。
- en: The book concludes with the application of neural networks to modeling time
    series and optimal control systems. Modeling time series can be done using a very
    simple technique discussed in Chap. 27 [8] that consists of fitting a linear model
    on top of a "reservoir" that implements a rich set of time series primitives.
    Chapter 28
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书以神经网络在时间序列和最优控制系统建模的应用结束。时间序列建模可以使用第27章讨论的一种非常简单的技术[8]，该技术包括在“水库”上拟合一个线性模型，水库实现了一组丰富的时间序列原语。第28章。
- en: '[13] offers an alternative to the previous method by directly identifying the
    underlying dynamical system that generates the time series data. Chapter 29 [6]
    presents how these system identification techniques can be used to identify a
    Markov decision process from the observation of a control system (a sequence of
    states and actions in the reinforcement learning terminology). Chapter 30 [11]
    concludes by showing how the control system can be dynamically improved by fitting
    a neural network as the control system explores the space of states and actions.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] 通过直接识别生成时间序列数据的潜在动态系统，提供了一种替代于之前方法的方式。第29章[6]展示了如何利用这些系统识别技术从控制系统的观察中识别马尔可夫决策过程（在强化学习术语中为状态和动作的序列）。第30章[11]通过展示如何通过拟合神经网络动态改善控制系统，从而结束控制系统对状态和动作空间的探索。'
- en: The book intends to provide a timely snapshot of tricks, theory, and algorithms
    that are of use. Our hope is that some of the chapters of the new second edition
    will become our companions when doing experimental work—eventually becoming classics,
    as some of the papers of the first edition have become. Eventually in some years,
    there may be an urge to reload again...
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本书旨在提供关于实用技巧、理论和算法的及时快照。我们希望新第二版的一些章节在进行实验工作时能成为我们的伙伴——最终成为经典，正如第一版的一些论文所成为的那样。几年后，可能会再次产生重载的冲动...
- en: September 2012 Grégoire Klaus Acknowledgments. This work was supported by the
    World Class University Program through the National Research Foundation of Korea
    funded by the Ministry of Education, Science, and Technology, under Grant R31-10008.
    The editors also acknowledge partial support by DFG (MU 987/17-1).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年9月 Grégoire Klaus 致谢。本工作得到了韩国教育部、科学技术部资助的国家研究基金会通过世界一流大学项目的支持，资助编号为R31-10008。编辑们还感谢DFG（MU
    987/17-1）的部分支持。
- en: '[1] Bengio, Y.: Practical Recommendations for Gradient-based Training of Deep
    Architectures. In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of
    the Trade, 2nd edn. LNCS, vol. 7700, pp. 437–478. Springer, Heidelberg (2012)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Bengio, Y.: 基于梯度的深度架构训练的实用建议。在：Montavon, G., Orr, G.B., Müller, K.-R.（编）NN:
    交易技巧，第二版。LNCS，卷7700，第437–478页。施普林格，海德堡（2012）'
- en: '[2] Bottou, L.: Stochastic Gradient Descent Tricks. In: Montavon, G., Orr,
    G.B.,'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Bottou, L.: 随机梯度下降技巧。在：Montavon, G., Orr, G.B.,'
- en: 'Müller, K.-R. (eds.) NN: Tricks of the Trade, 2nd edn. LNCS, vol. 7700, pp.
    421–436. Springer, Heidelberg (2012)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 'Müller, K.-R.（编）NN: 交易技巧，第二版。LNCS，卷7700，第421–436页。施普林格，海德堡（2012）'
- en: '[3] Ciresan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Deep Big
    Multilayer Perceptrons for Digit Recognition. In: Montavon, G., Orr, G.B., Müller,
    K.-R. (eds.) NN: Tricks of the Trade, 2nd edn. LNCS, vol. 7700, pp. 581–598.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Ciresan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: 用于数字识别的深度大多层感知机。在：Montavon,
    G., Orr, G.B., Müller, K.-R.（编）NN: 交易技巧，第二版。LNCS，卷7700，第581–598页。'
- en: Springer, Heidelberg (2012)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 施普林格，海德堡（2012）
- en: '[4] Coates, A., Ng, A.Y.: Learning Feature Representations with k-means. In:
    Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the Trade, 2nd edn.
    LNCS,'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Coates, A., Ng, A.Y.: 使用k均值学习特征表示。在：Montavon, G., Orr, G.B., Müller, K.-R.（编）NN:
    交易技巧，第二版。LNCS，'
- en: vol. 7700, pp. 561–580. Springer, Heidelberg (2012)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 卷7700，第561–580页。施普林格，海德堡（2012）
- en: '[5] Collobert, R., Kavukcuoglu, K., Farabet, C.: Implementing Neural Networks
    Efficiently. In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the
    Trade, 2nd edn. LNCS, vol. 7700, pp. 537–557. Springer, Heidelberg (2012)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Collobert, R., Kavukcuoglu, K., Farabet, C.: 高效实现神经网络。在：Montavon, G., Orr,
    G.B., Müller, K.-R.（编）NN: 交易技巧，第二版。LNCS，卷7700，第537–557页。施普林格，海德堡（2012）'
- en: '[6] Duell, S., Udluft, S., Sterzing, V.: Solving Partially Observable Reinforcement
    Learning Problems with Recurrent Neural Networks. In: Montavon, G., Orr, G.B.,
    Müller, K.-R. (eds.) NN: Tricks of the Trade, 2nd edn. LNCS, vol. 7700, pp. 687–707.
    Springer, Heidelberg (2012)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Duell, S., Udluft, S., Sterzing, V.: 使用递归神经网络解决部分可观测的强化学习问题。在：Montavon,
    G., Orr, G.B., Müller, K.-R.（编）NN: 交易技巧，第二版。LNCS，卷7700，第687–707页。施普林格，海德堡（2012）'
- en: '[7] Hinton, G.E.: A Practical Guide to Training Restricted Boltzmann Machines.
    In:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Hinton, G.E.: 训练限制玻尔兹曼机的实用指南。在：'
- en: 'Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the Trade, 2nd
    edn. LNCS, vol. 7700, pp. 621–637. Springer, Heidelberg (2012)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'Montavon, G., Orr, G.B., Müller, K.-R.（编）NN: 交易技巧，第二版。LNCS，卷7700，第621–637页。施普林格，海德堡（2012）'
- en: '[8] Lukoševičius, M.: A Practical Guide to Applying Echo State Networks. In:
    Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the Trade, 2nd edn.
    LNCS,'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Lukoševičius, M.: 实用的回声状态网络应用指南。在：Montavon, G.，Orr, G.B.，Müller, K.-R.（编辑）NN：行业技巧，第2版。LNCS，'
- en: vol. 7700, pp. 659–686. Springer, Heidelberg (2012)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第7700卷，第659–686页。施普林格，海德堡（2012年）
- en: '[9] Martens, J., Sutskever, I.: Training Deep and Recurrent Networks with Hessianfree
    Optimization. In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of
    the Trade, 2nd edn. LNCS, vol. 7700, pp. 479–535. Springer, Heidelberg (2012)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Martens, J.，Sutskever, I.: 使用海森优化训练深度和递归网络。在：Montavon, G.，Orr, G.B.，Müller,
    K.-R.（编辑）NN：行业技巧，第2版。LNCS，第7700卷，第479–535页。施普林格，海德堡（2012年）'
- en: '[10] Montavon, G., Müller, K.-R.: Deep Boltzmann Machines and the Centering
    Trick.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Montavon, G.，Müller, K.-R.: 深度玻尔兹曼机与中心技巧。'
- en: 'In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the Trade,
    2nd edn. LNCS, vol. 7700, pp. 621–637. Springer, Heidelberg (2012)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在：Montavon, G.，Orr, G.B.，Müller, K.-R.（编辑）NN：行业技巧，第2版。LNCS，第7700卷，第621–637页。施普林格，海德堡（2012年）
- en: '[11] Riedmiller, M.: 10 Steps and Some Tricks to Set Up Neural Reinforcement
    Controllers. In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the
    Trade, 2nd edn. LNCS, vol. 7700, pp. 735–757. Springer, Heidelberg (2012)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Riedmiller, M.: 设置神经强化控制器的10个步骤和一些技巧。在：Montavon, G.，Orr, G.B.，Müller,
    K.-R.（编辑）NN：行业技巧，第2版。LNCS，第7700卷，第735–757页。施普林格，海德堡（2012年）'
- en: '[12] Weston, J., Ratle, F., Collobert, R.: Deep Learning Via Semi-supervised
    Embedding. In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the
    Trade, 2nd edn. LNCS, vol. 7700, pp. 639–655. Springer, Heidelberg (2012)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Weston, J.，Ratle, F.，Collobert, R.: 通过半监督嵌入进行深度学习。在：Montavon, G.，Orr,
    G.B.，Müller, K.-R.（编辑）NN：行业技巧，第2版。LNCS，第7700卷，第639–655页。施普林格，海德堡（2012年）'
- en: '[13] Zimmermann, H.-G., Tietz, C., Grothmann, R.: Forecasting with Recurrent
    Neural Networks: 12 Tricks. In: NN: Tricks of the Trade, 2nd edn. LNCS, vol. 7700,
    pp. 687–707. Springer, Heidelberg (2012)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Zimmermann, H.-G.，Tietz, C.，Grothmann, R.: 使用递归神经网络进行预测：12个技巧。在：NN：行业技巧，第2版。LNCS，第7700卷，第687–707页。施普林格，海德堡（2012年）'
- en: Table Of Contents
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: 'Introduction*......................................................* 1 Speeding
    Learning Preface *..........................................................*
    7 1. Efficient BackProp *..............................................* 9 Yann
    LeCun, Leon Bottou, Genevieve B. Orr, and Klaus-Robert Müller Regularization Techniques
    to Improve Generalization Preface *..........................................................*
    49 2. Early Stopping - But When? *....................................* 53 Lutz
    Prechelt 3. A Simple Trick for Estimating the Weight Decay Parameter *..........*
    69 Thorsteinn S. Rögnvaldsson 4. Controlling the Hyperparameter Search in MacKay''s
    Bayesian Neural Network Framework *.............................................*
    91 Tony Plate 5. Adaptive Regularization in Neural Network Modeling *...............*
    111 Jan Larsen, Claus Svarer, Lars Nonboe Andersen, and Lars Kai Hansen 6. Large
    Ensemble Averaging *.......................................* 131 David Horn, Ury
    Naftaly, and Nathan Intrator Improving Network Models and Algorithmic Tricks Preface
    *..........................................................* 139 7. Square Unit
    Augmented, Radially Extended, Multilayer Perceptrons . . 143 Gary William Flake
    8. A Dozen Tricks with Multitask Learning *...........................* 163 Rich
    Caruana 9. Solving the Ill-Conditioning in Neural Network Learning *.............*
    191 Patrick van der Smagt and Gerd Hirzinger 10. Centering Neural Network Gradient
    Factors *.......................* 205 Nicol N. Schraudolph 11. Avoiding Roundoff
    Error in Backpropagating Derivatives*............* 225 Tony Plate Representing
    and Incorporating Prior Knowledge in Neural Network Training Preface *..........................................................*
    231 12. Transformation Invariance in Pattern Recognition - Tangent Distance and
    Tangent Propagation *...............................* 235 Patrice Y. Simard, Yann
    A. LeCun, John S. Denker, and Bernard Victorri 13. Combining Neural Networks and
    Context-Driven Search for On-line, Printed Handwriting Recognition in the Newton
    *...........* 271 Larry S. Yaeger, Brandyn J. Webb, and Richard F. Lyon 14. Neural
    Network Classification and Prior Class Probabilities *.........* 295 Steve Lawrence,
    Ian Burns, Andrew Back, Ah Chung Tsoi, and C. Lee Giles 15. Applying Divide and
    Conquer to Large Scale Pattern Recognition Tasks *........................................................*
    311 Jürgen Fritsch and Michael Finke Tricks for Time Series Preface *..........................................................*
    339 16. Forecasting the Economy with Neural Nets: A Survey of Challenges and Solutions
    *.......................................* 343 John Moody 17. How to Train Neural
    Networks *..................................* 369 Ralph Neuneier and Hans Georg
    Zimmermann Big Learning in Deep Neural Networks 18. Stochastic Gradient Descent
    Tricks*...............................* 421 Léon Bottou 19. Practical Recommendations
    for Gradient-Based Training of Deep Architectures *..........................................*
    437 Yoshua Bengio 20. Training Deep and Recurrent Networks with Hessian-Free Optimization
    *..................................................* 479 James Martens and Ilya
    Sutskever 21. Implementing Neural Networks Efficiently*.........................*
    537 Ronan Collobert, Koray Kavukcuoglu, and Clément Farabet Preface *..........................................................*
    419 Better Representations: Invariant, Disentangled and Reusable Preface *..........................................................*
    559 22. Learning Feature Representations with K-Means *...................* 561
    Adam Coates and Andrew Y. Ng 23. Deep Big Multilayer Perceptrons for Digit Recognition
    *.............* 581 Dan Claudiu Cireşan, Ueli Meier, Luca Maria Gambardella, and
    Jürgen Schmidhuber 24. A Practical Guide to Training Restricted Boltzmann Machines
    *......* 599 Geoffrey E. Hinton 25. Deep Boltzmann Machines and the Centering
    Trick *................* 621 Grégoire Montavon and Klaus-Robert Müller 26. Deep
    Learning via Semi-supervised Embedding *....................* 639 Jason Weston,
    Frédéric Ratle, and Ronan Collobert Identifying Dynamical Systems for Forecasting
    and Control 27. A Practical Guide to Applying Echo State Networks *...............*
    659 Mantas Lukoševičius 28. Forecasting with Recurrent Neural Networks: 12 Tricks
    *.............* 687 Hans-Georg Zimmermann, Christoph Tietz, and Ralph Grothmann
    29. Solving Partially Observable Reinforcement Learning Problems with Recurrent
    Neural Networks *.................................* 709 Siegmund Duell, Steffen
    Udluft, and Volkmar Sterzing 30. 10 Steps and Some Tricks to Set up Neural Reinforcement
    Controllers*....................................................* 735 Martin Riedmiller
    Preface *..........................................................* 657 Author
    Index *................................................* 759 Subject Index *................................................*
    761'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 引言*......................................................* 1 加速学习 序言 *..........................................................*
    7 1. 高效反向传播 *..............................................* 9 Yann LeCun, Leon
    Bottou, Genevieve B. Orr, 和 Klaus-Robert Müller 正则化技术以改善泛化 序言 *..........................................................*
    49 2. 提前停止 - 但何时？ *....................................* 53 Lutz Prechelt 3. 估计权重衰减参数的简单技巧
    *..........* 69 Thorsteinn S. Rögnvaldsson 4. 在MacKay的贝叶斯神经网络框架中控制超参数搜索 *.............................................*
    91 Tony Plate 5. 神经网络建模中的自适应正则化 *...............* 111 Jan Larsen, Claus Svarer,
    Lars Nonboe Andersen, 和 Lars Kai Hansen 6. 大型集成平均 *.......................................*
    131 David Horn, Ury Naftaly, 和 Nathan Intrator 改进网络模型和算法技巧 序言 *..........................................................*
    139 7. 方形单元增强、径向扩展的多层感知器 . . 143 Gary William Flake 8. 多任务学习的十二个技巧 *...........................*
    163 Rich Caruana 9. 解决神经网络学习中的病态条件 *.............* 191 Patrick van der Smagt 和
    Gerd Hirzinger 10. 中心化神经网络梯度因子 *.......................* 205 Nicol N. Schraudolph
    11. 避免反向传播导数中的舍入误差*............* 225 Tony Plate 在神经网络训练中表示和纳入先验知识 序言 *..........................................................*
    231 12. 模式识别中的变换不变性 - 切线距离和切线传播 *...............................* 235 Patrice
    Y. Simard, Yann A. LeCun, John S. Denker, 和 Bernard Victorri 13. 结合神经网络和上下文驱动搜索实现在线印刷手写识别
    *...........* 271 Larry S. Yaeger, Brandyn J. Webb, 和 Richard F. Lyon 14. 神经网络分类与先验类别概率
    *.........* 295 Steve Lawrence, Ian Burns, Andrew Back, Ah Chung Tsoi, 和 C. Lee
    Giles 15. 将分而治之应用于大规模模式识别任务 *........................................................*
    311 Jürgen Fritsch 和 Michael Finke 时间序列技巧 序言 *..........................................................*
    339 16. 用神经网络预测经济：挑战与解决方案综述 *.......................................* 343 John
    Moody 17. 如何训练神经网络 *..................................* 369 Ralph Neuneier 和 Hans
    Georg Zimmermann 深度神经网络中的大规模学习 18. 随机梯度下降技巧*...............................* 421
    Léon Bottou 19. 深度架构基于梯度的训练实用建议 *..........................................* 437
    Yoshua Bengio 20. 使用海森优化训练深度和递归网络 *..................................................*
    479 James Martens 和 Ilya Sutskever 21. 高效实现神经网络*.........................* 537
    Ronan Collobert, Koray Kavukcuoglu, 和 Clément Farabet 序言 *..........................................................*
    419 更好的表示：不变、解耦和可重用 序言 *..........................................................*
    559 22. 使用K均值学习特征表示 *...................* 561 Adam Coates 和 Andrew Y. Ng 23. 深度大型多层感知器用于数字识别
    *.............* 581 Dan Claudiu Cireşan, Ueli Meier, Luca Maria Gambardella, 和
    Jürgen Schmidhuber 24. 限制玻尔兹曼机训练的实用指南 *......* 599 Geoffrey E. Hinton 25. 深度玻尔兹曼机与中心化技巧
    *................* 621 Grégoire Montavon 和 Klaus-Robert Müller 26. 通过半监督嵌入进行深度学习
    *....................* 639 Jason Weston, Frédéric Ratle, 和 Ronan Collobert 识别动态系统用于预测和控制
    27. 应用回声状态网络的实用指南 *...............* 659 Mantas Lukoševičius 28. 使用递归神经网络进行预测：12个技巧
    *.............* 687 Hans-Georg Zimmermann, Christoph Tietz, 和 Ralph Grothmann
    29. 用递归神经网络解决部分可观察的强化学习问题 *.................................* 709 Siegmund Duell,
    Steffen Udluft, 和 Volkmar Sterzing 30. 设置神经强化控制器的10个步骤和一些技巧*....................................................*
    735 Martin Riedmiller 序言 *..........................................................*
    657 作者索引 *................................................* 759 主题索引 *................................................*
    761
- en: Introduction-
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言-
- en: It is our belief that researchers and practitioners acquire, through experience
    and word-of-mouth, techniques and heuristics that help them successfully apply
    neural networks to difficult real world problems. Often these "tricks" are theoretically
    well motivated. Sometimes they are the result of trial and error. However, their
    most common link is that they are usually hidden in people's heads or in the back
    pages of space-constrained conference papers. As a result newcomers to the field
    waste much time wondering why their networks train so slowly and perform so poorly.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信，研究人员和实践者通过经验和口耳相传，获得了一些技巧和启发式方法，帮助他们成功地将神经网络应用于复杂的现实问题。这些“技巧”往往在理论上有很好的动机，有时则是试错的结果。然而，它们最常见的联系在于，通常隐藏在人们的脑海中或在空间有限的会议论文的后页。因此，初学者在这一领域常常浪费大量时间思考为什么他们的网络训练如此缓慢且表现不佳。
- en: This book is an outgrowth of a 1996 NIPS workshop called *Tricks of the Trade*
    whose goal was to begin the process of gathering and documenting these tricks.
    The interest that the workshop generated, motivated us to expand our collection
    and compile it into this book. Although we have no doubt that there are many tricks
    we have missed, we hope that what we have included will prove to be useful, particularly
    to those who are relatively new to the field. Each chapter contains one or more
    tricks presented by a given author (or authors). We have attempted to group related
    chapters into sections, though we recognize that the different sections are far
    from disjoint. Some of the chapters (e.g. 1,13,17) contain entire systems of tricks
    that are far more general than the category they have been placed in.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本书源于1996年名为*行业技巧*的NIPS研讨会，旨在开始收集和记录这些技巧的过程。研讨会引发的兴趣促使我们扩大我们的收集，并编纂成这本书。尽管我们毫不怀疑还有许多技巧被我们遗漏，但我们希望所包含的内容对那些相对新手的读者能有所帮助。每一章包含由特定作者（或作者们）呈现的一项或多项技巧。我们试图将相关章节分组为几个部分，尽管我们意识到不同部分之间并非完全不相干。有些章节（例如1、13、17）包含的技巧系统远比它们所在的类别要更为一般化。
- en: Before each section we provide the reader with a summary of the tricks contained
    within, to serve as a quick overview and reference. However, we do not recommend
    applying tricks before having read the accompanying chapter. Each trick may only
    work in a particular context that is not fully explained in the summary. This
    is particularly true for the chapters that present systems where combinations
    of tricks must be applied together for them to be effective.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个部分之前，我们为读者提供包含的技巧摘要，以便快速概览和参考。然而，我们不建议在阅读相关章节之前应用技巧。每个技巧可能只在特定的上下文中有效，而该上下文在摘要中并未完全解释。这一点在呈现需要组合应用的技巧系统的章节中特别明显。
- en: Below we give a coarse roadmap of the contents of the individual chapters.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是各个章节内容的粗略路线图。
- en: Speeding Learning
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速学习
- en: 'The book opens with a chapter based on Leon Bottou and Yann LeCun''s popular
    workshop on efficient backpropagation where they present a system of tricks for
    speeding the minimization process. Included are tricks that are very simple to
    implement as well as more complex ones, e.g. based on second-order methods. Though
    many of the readers may recognize some of these tricks, we believe that this chapter
    provides both: a thorough explanation of their theoretical basis as well as an
    understanding of the subtle interactions among them.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本书以基于Leon Bottou和Yann LeCun的高效反向传播热门研讨会的一章开篇，他们在其中提出了加速最小化过程的技巧系统。包括一些非常简单的实施技巧以及更复杂的技巧，例如基于二阶方法的技巧。尽管许多读者可能会认出其中的一些技巧，但我们相信本章提供了对其理论基础的透彻解释，以及对它们之间微妙相互作用的理解。
- en: This chapter provides an ideal introduction for the reader. It starts with discussing
    fundamental tricks addressing input representation, initialization, target
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为读者提供了理想的入门介绍。它首先讨论了与输入表示、初始化、目标等相关的基本技巧。
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '- 以前发表在：Orr, G.B. 和 Müller, K.-R.（编）：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0（1998年）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    1–5, 2012.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon 等（编）：NN: 行业技巧，第二版，LNCS 7700，第1–5页，2012年。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: values, choice of learning rates, choice of the nonlinearity, and so on. Subsequently,
    the authors introduce in great detail tricks for estimation and approximation
    of the Hessian in neural networks. This provides the basis for a discussion of
    second-order algorithms, fast training methods like the stochastic LevenbergMarquardt
    algorithm, and tricks for learning rate adaptation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 值、学习率选择、非线性选择等。随后，作者详细介绍了在神经网络中估计和近似 Hessian 的技巧。这为讨论二阶算法、快速训练方法（如随机 Levenberg-Marquardt
    算法）以及学习率适应技巧奠定了基础。
- en: Regularization Techniques To Improve Generalization
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改善泛化能力的正则化技术
- en: Fast minimization is important but only if we can also insure good generalization.
    We therefore next include a collection of chapters containing a range of approaches
    for improving generalization. As one might expect, there are no tricks that work
    well in all situations. However, many examples and discussions are included to
    help the reader to decide which will work best for their own problem.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 快速最小化很重要，但前提是我们也能确保良好的泛化能力。因此，接下来我们包括了一系列章节，包含提高泛化能力的各种方法。正如人们所期待的那样，并没有适用于所有情况的绝佳技巧。然而，包含了许多示例和讨论，以帮助读者决定哪种方法最适合他们自己的问题。
- en: 'Chapter 2 addresses what is one of the most commonly used techniques:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第二章讨论了最常用的技术之一：
- en: early stopping. Here Lutz Prechelt discusses the pitfalls of this seemingly
    simple technique. He quantifies the tradeoff between generalization and training
    time for various stopping criteria, which leads to a trick for picking an appropriate
    criterion.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 早停法。在这里，Lutz Prechelt 讨论了这一看似简单技术的陷阱。他量化了各种停止标准下泛化能力与训练时间之间的权衡，这导致了一种选择适当标准的技巧。
- en: Using a weight decay penalty term in the cost function is another common method
    for improving generalization. The difficulty, however, is in finding a good estimate
    of the weight decay parameter. In chapter 3, Thorsteinn Rögnvaldsson presents
    a fast technique for finding a good estimate, surprisingly, by using information
    measured at the early stopping point. Experimental evidence for its usefulness
    is given in several applications.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在成本函数中使用权重衰减惩罚项是改善泛化能力的另一种常见方法。然而，困难在于找到权重衰减参数的良好估计。在第三章中，Thorsteinn Rögnvaldsson
    介绍了一种通过使用在早停点测量的信息，快速找到良好估计的技术，令人惊讶的是，实验结果在多个应用中证明了其有效性。
- en: Tony Plate in chapter 4 treats the penalty terms along the lines of MacKay,
    i.e. as hyperparameters to be found through iterative search. He presents and
    compares tricks for making the hyperparameter search in classification networks
    work in practice by speeding it up and simplifying it. Key to his success is a
    control of the frequency of the hyperparameter updates and a better strategy in
    cases where the Hessian becomes out-of-bounds.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Tony Plate 在第四章中沿用 MacKay 的思路处理惩罚项，即将其视为通过迭代搜索找到的超参数。他展示并比较了加速和简化分类网络中超参数搜索的技巧。他成功的关键在于控制超参数更新的频率，以及在
    Hessian 变得超出界限时采用更好的策略。
- en: In chapter 5, Jan Larsen et al. present a trick for adapting regularization
    parameters by using simple gradient descent (with respect to the regularization
    parameters) on the validation error. The trick is tested on both classification
    and regression problems.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在第五章中，Jan Larsen 等人展示了一种通过在验证误差上使用简单的梯度下降（针对正则化参数）来调整正则化参数的技巧。该技巧在分类和回归问题中进行了测试。
- en: Averaging over multiple predictors is a well known method for improving generalization.
    Two questions that arise are how many predictors are "enough" and how does the
    number of predictors affect the stopping criteria for early stopping. In the final
    chapter of this section, David Horn et al. present solutions to these questions
    by providing a method for estimating the error of an infinite number of predictors.
    They then demonstrate this trick for a prediction task.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对多个预测器进行平均是一种众所周知的提高泛化能力的方法。出现的两个问题是“足够”的预测器有多少，以及预测器的数量如何影响早停法的停止标准。在本节的最后一章中，David
    Horn 等人通过提供一种估计无限数量预测器误差的方法，来解决这些问题。他们随后在一个预测任务中展示了这一技巧。
- en: Improving Network Models And Algorithmic Tricks
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进网络模型和算法技巧
- en: In this section we examine tricks that help improve the network model. Even
    though standard multilayer perceptrons (MLPs) are, in theory, universal approximators,
    other architectures may provide a more natural fit to a problem. A
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们研究了一些帮助改善网络模型的技巧。尽管标准的多层感知器（MLPs）在理论上是通用逼近器，但其他架构可能更自然地适应某个问题。
- en: better fit means that training is faster and that there is a greater likelihood
    of finding a good and stable solution. For example, radial basis functions (RBFs)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的拟合意味着训练更快，并且找到良好且稳定解决方案的可能性更大。例如，径向基函数（RBFs）。
- en: are preferred for problems that exhibit local features in a finite region. Of
    course, which architecture to choose is not always obvious.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 更适用于在有限区域内表现出局部特征的问题。当然，选择哪种架构并不总是显而易见的。
- en: In chapter 7, Gary Flake presents a trick that gives MLPs the power of both
    an MLP and an RBF so that one does not need to choose between them . This trick
    is simply to add extra inputs whose values are the square of the regular inputs.
    Both a theoretical and intuitive explanation are presented along with a number
    of simulation examples.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，Gary Flake展示了一种技巧，使得多层感知器（MLPs）同时具备MLP和RBF的能力，因此不需要在两者之间做出选择。这一技巧仅仅是添加额外的输入，其值为常规输入的平方。提供了理论和直观解释，并附有多个仿真示例。
- en: Rich Caruana in chapter 8 shows that performance can be improved on a main task
    by adding extra outputs to a network that predict related tasks. This technique,
    known as multi-task learning (MTL), trains these extra outputs in parallel with
    the main task. This chapter presents multiple examples of what one might use as
    these extra outputs as well as techniques for implementing MTL effectively. Empirical
    examples include mortality rankings for pneumonia and road-following in a network
    learning to steer a vehicle.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Rich Caruana在第8章中展示，通过向网络添加额外的输出以预测相关任务，可以提高主要任务的表现。这种被称为多任务学习（MTL）的技术，允许这些额外输出与主要任务并行训练。本章提供了多个可能用作这些额外输出的例子，以及有效实施MTL的技术。实证例子包括肺炎的死亡率排名和一个学习驾驶车辆的网络中的道路跟随。
- en: Patrick van der Smagt and Gerd Hirzinger consider in chapter 9 the illconditioning
    of the Hessian in neural network training and propose using what they call a linearly
    augmented feed-forward network, employing input/output short-cut connections that
    share the input/hidden weights. This gives rise to better conditioning of the
    learning problem and, thus, to faster learning, as shown in a simulation example
    with data from a robot arm.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在第9章中，Patrick van der Smagt和Gerd Hirzinger考虑了神经网络训练中Hessian矩阵的病态条件，并提出使用他们称之为线性增强前馈网络的方法，采用共享输入/隐藏权重的输入/输出捷径连接。这有助于改善学习问题的条件，从而加速学习，正如在一个使用机器人手臂数据的仿真示例中所示。
- en: 'In chapter 10, Nicol Schraudolph takes the idea of scaling and centering the
    inputs even further than chapter 1 by proposing to center all factors in the neural
    network gradient: inputs, activities, error signals and hidden unit slopes. He
    gives experimental evidence for the usefulness of this trick.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10章中，Nicol Schraudolph进一步扩展了输入的缩放和中心化的理念，提出将神经网络梯度中的所有因子中心化：输入、活动、误差信号和隐藏单元的斜率。他提供了这一技巧的实验证据。
- en: In chapter 11, Tony Plate's short note reports a numerical trick for computing
    derivatives more accurately with only a small memory overhead.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在第11章中，Tony Plate的短文报告了一种数值技巧，可以在仅有少量内存开销的情况下，更准确地计算导数。
- en: Representation And Incorporating Prior Knowledge In Neural Network Training
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示法和在神经网络训练中融入先验知识。
- en: 'Previous chapters (e.g. Chapter 1) present very general tricks for transforming
    inputs to improve learning: prior knowledge of the problem is not taken into account
    explicitly (of course regularization, as discussed in Chapters 2-5, implicitly
    assumes a prior but on the weight distribution). For complex, difficult problems,
    however, it is not enough to take a black box approach, no matter how good that
    black box might be. This section examines how prior knowledge about a problem
    can be used to greatly improve learning. The questions asked include how to best
    represent the data, how to make use of this representation for training, and how
    to take advantage of the invariances that are present. Such issues are key for
    proper neural network training. They are also at the heart of the tricks pointed
    out by Patrice Simard, et al. in the first chapter of this section. Here, the
    authors present a particularly interesting perspective on how to incorporate prior
    knowledge into data. They also give the first review of the tangent distance classification
    method and related techniques evolving from it such as tangent prop. These methods
    are applied to the difficult task of optical character recognition (OCR).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的章节（例如第1章）介绍了一些非常一般的技巧，用于转换输入以改善学习：问题的先验知识没有被明确考虑（当然，正则化，如第2-5章讨论的，隐含地假设了一个先验，但在权重分布上）。然而，对于复杂和困难的问题，仅仅采用黑箱方法是不够的，无论这个黑箱多么优秀。本节探讨了如何利用问题的先验知识来大幅改善学习。所提的问题包括如何最好地表示数据，如何利用这种表示进行训练，以及如何利用存在的不变性。这些问题对于神经网络的正确训练至关重要。它们也是Patrice
    Simard等人在本节第一章中指出的技巧的核心。在这里，作者呈现了一种特别有趣的视角，探讨如何将先验知识融入数据中。他们还首次回顾了切线距离分类方法及其相关技术，如切线传播。这些方法被应用于光学字符识别（OCR）的困难任务。
- en: In chapter 13, Larry Yaeger, et al. give an overview of the tricks and techniques
    for on-line handwritten character recognition that were eventually used in the
    Apple Computer's Newton MessagePad -Rand eMate-R. Anyone who has used these systems
    knows that their handwriting recognition capability works exceedingly well. Although
    many of the issues that are discussed in this chapter overlap with those in OCR,
    including representation and prior knowledge, the solutions are complementary.
    This chapter also gives a very nice overview of what design choices proved to
    be efficient as well as how different tricks such as choice of learning rate,
    over-representation of more difficult patterns, negative training, error emphasis
    and so on work together.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在第13章中，Larry Yaeger等人概述了最终在苹果公司的Newton MessagePad -Rand eMate-R中使用的在线手写字符识别的技巧和技术。任何使用过这些系统的人都知道，它们的手写识别能力非常出色。尽管本章讨论的许多问题与OCR中的问题重叠，包括表示和先验知识，但解决方案是互补的。本章还很好地概述了哪些设计选择被证明是有效的，以及不同技巧（如学习率选择、更难模式的过度表示、负训练、错误强调等）如何协同工作。
- en: Whether it be handwritten character recognition, speech recognition or medical
    applications, a particularly difficult problem encountered is the unbalanced class
    prior probabilities that occur, for example, when certain writing styles and subphoneme
    classes are uncommon or certain illnesses occur less frequently.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是手写字符识别、语音识别还是医疗应用，遇到的一个特别困难的问题是类先验概率不平衡，例如，当某些书写风格和子音素类别不常见或某些疾病发生频率较低时。
- en: Chapter 13 briefly discusses this problem in the context of handwriting recognition
    and presents a heuristic which controls the frequency with which samples are picked
    for training.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第13章简要讨论了手写识别中的这个问题，并提出了一种启发式方法，用于控制样本选择用于训练的频率。
- en: In chapter 14, Steve Lawrence, et al. discuss the issue of unbalanced class
    prior probabilities in greater depth. They present and compare several different
    heuristics (prior scaling, probabilistic sampling, post scaling and class membership
    equalization) one of which is similar to the one in chapter 13. They demonstrate
    their tricks solving an ECG classification problem and provide some theoretical
    explanations.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在第14章中，Steve Lawrence等人更深入地讨论了类先验概率不平衡的问题。他们提出并比较了几种不同的启发式方法（先验缩放、概率采样、后期缩放和类成员资格均衡），其中一种与第13章中的方法相似。他们展示了这些技巧在解决心电图分类问题中的应用，并提供了一些理论解释。
- en: Many training techniques work well for small to moderate size nets. However
    when problems consist of thousands of classes and millions of examples, not uncommon
    in applications such as speech recognition, many of these techniques break down.
    This chapter by Jürgen Fritsch and Michael Finke is devoted to the issue of large
    scale classification problems and representation design in general. Here the problem
    of unbalanced class prior probabilities is also tackled.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 许多训练技术在小到中等规模的网络中表现良好。然而，当问题包含成千上万的类别和数百万的示例时，这在语音识别等应用中并不罕见，许多这些技术就会失效。尤尔根·弗里奇和迈克尔·芬克的这一章专注于大规模分类问题和表示设计的一般问题。这里也解决了类先验概率不平衡的问题。
- en: Although Fritsch and Finke specifically exemplify their design approach for
    the problem of building a large vocabulary speech recognizer, it becomes clear
    that these techniques are also applicable to the general construction of an appropriate
    hierarchical decision tree. A particularly interesting result in this paper is
    that the structural design to incorporate prior knowledge about speech done by
    a human speech expert was outperformed by their machine learning technique using
    an agglomerative clustering algorithm to choose the structure of the decision
    tree.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然弗里奇和芬克特别例证了他们在构建大型词汇语音识别器问题上的设计方法，但显然这些技术也适用于构建适当的层次决策树。本文的一个特别有趣的结果是，结合人类语音专家的先验知识进行的结构设计被他们使用聚合聚类算法选择决策树结构的机器学习技术所超越。
- en: Tricks For Time Series
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列的技巧
- en: We close the book with two papers on the subject of time series and economic
    forecasting. In the first of these chapters, John Moody presents an excellent
    survey of both the challenges of macroeconomic forecasting as well a number of
    neural network solutions. The survey is followed by a more detailed description
    of smoothing regularizers, model selection methods (e.g. AIC, effective number
    of parameters, nonlinear cross-validation), and input selection via sensitivity-based
    input pruning. Model interpretation and visualization are also discussed.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以两篇关于时间序列和经济预测的论文结束本书。在第一章中，约翰·穆迪对宏观经济预测的挑战以及多种神经网络解决方案进行了出色的综述。该综述后面跟随的是对平滑正则化器、模型选择方法（如AIC、有效参数数量、非线性交叉验证）以及基于灵敏度的输入修剪的更详细描述。模型解释和可视化也有讨论。
- en: 'In the final chapter, Ralph Neuneier and Hans Georg Zimmermann present an impressive
    integrated system for neural network training of time series and economic forecasting.
    Every aspect of the system is discussed including input preprocessing, cost functions,
    handling of outliers, architecture, regularization techniques, as well as solutions
    for dealing with the problem of bottom-heavy networks, i.e. the input dimension
    is large while the output dimension is very small. There is also a thought-provoking
    discussion of the Observer-Observer dilemma: we want both to create a model based
    on observed data while, at the same time, use this model to judge the correctness
    of new incoming data. Even those people not interested specifically in economic
    forecasting are encouraged to read this very useful example of how to incorporate
    prior (system) knowledge into training.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一章，拉尔夫·诺伊内尔和汉斯·乔治·齐默尔曼展示了一个令人印象深刻的集成系统，用于时间序列和经济预测的神经网络训练。讨论了系统的每个方面，包括输入预处理、成本函数、异常值处理、架构、正则化技术，以及解决底重网络问题的方案，即输入维度很大而输出维度非常小。还有一个发人深省的讨论，涉及观察者-观察者困境：我们希望在观察数据的基础上创建模型，同时又利用该模型来判断新进数据的正确性。即使是那些对经济预测不感兴趣的人，也鼓励阅读这个如何将先验（系统）知识纳入训练的非常有用的例子。
- en: Final Remark
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最后备注
- en: As a final remark, we note that some of the views taken in the chapters are
    contradictory, e.g. some authors favor one regularization method over another,
    while other authors make exactly the opposite statement. On the one hand, one
    can explain these discrepancies by stating that the field is still very active
    and therefore opposing viewpoints will inevitably exist until more is understood.
    On the other hand, it may be that both (contradicting) views are correct but on
    different data sets and in different applications, e.g. an approach that considers
    noisy time-series needs algorithms with a completely different robustness than
    in, say, an OCR setting. In this sense, the present book mirrors an active field
    and a variety of applications with its diversity of views.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后需要指出的是，章节中某些观点存在矛盾，例如一些作者偏爱某种正则化方法，而其他作者则做出完全相反的声明。一方面，可以通过说明该领域仍然非常活跃来解释这些差异，因此对立观点不可避免地会存在，直到更多被理解。另一方面，可能两种（矛盾的）观点在不同的数据集和应用中都是正确的，例如，一个考虑噪声时间序列的方法需要与光学字符识别设置中完全不同的算法鲁棒性。从这个意义上讲，本书反映了一个活跃的领域和多样化的应用。
- en: August 1998 Jenny & Klaus Acknowledgements. We would like to thank all authors
    for their collaboration.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 1998年8月，珍妮和克劳斯致谢。我们要感谢所有作者的合作。
- en: Special thanks to Steven Lemm for considerable help with the typesetting. K.-
    R.M. acknowledges partial financial support from DFG (grant JA 379/51 and JA 379/7)
    and EU ESPRIT (grant 25387-STORM).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 特别感谢史蒂文·莱姆在排版方面提供的巨大帮助。K.-R.M. 感谢DFG（资助JA 379/51和JA 379/7）和欧盟ESPRIT（资助25387-STORM）的部分经济支持。
- en: Speeding Learning-
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加速学习-
- en: Preface
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: There are those who argue that developing fast algorithms is no longer necessary
    because computers have become so fast. However, we believe that the complexity
    of our algorithms and the size of our problems will always expand to consume all
    cycles available, regardless of the speed of our machines. Thus, there will never
    come a time when computational efficiency can or should be ignored. Besides, in
    the quest to find solutions faster, we also often find better and more stable
    solutions as well.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有人认为开发快速算法已不再必要，因为计算机变得如此快速。然而，我们相信算法的复杂性和问题的规模将始终扩大以占用所有可用的周期，无论我们机器的速度如何。因此，计算效率永远不会被忽视。此外，在寻找更快解决方案的过程中，我们往往也会发现更好、更稳定的解决方案。
- en: This section is devoted to techniques for making the learning process in backpropagation
    (BP) faster and more efficient. It contains a single chapter based on a workshop
    by Leon Bottou and Yann LeCun. While many alternative learning systems have emerged
    since the time BP was first introduced, BP is still the most widely used learning
    algorithm. The reason for this is its simplicity, efficiency, and its general
    effectiveness on a wide range of problems. Even so, there are many pitfalls in
    applying it, which is where all these tricks enter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本节专注于使反向传播（BP）学习过程更快更高效的技术。它包含一章基于Leon Bottou和Yann LeCun的研讨会。尽管自BP首次引入以来出现了许多替代学习系统，但BP仍然是使用最广泛的学习算法。原因在于其简单性、高效性以及在广泛问题上的一般有效性。即便如此，在应用它时仍有许多陷阱，这正是这些技巧的用武之地。
- en: 'Chapter 1 begins gently by introducing us to a few practical tricks that are
    very simple to implement. Included are easy to understand qualitative explanations
    of each. There is a discussion of stochastic (on-line) vs batch mode learning
    where the advantages and disadvantages of both are presented while making it clear
    that stochastic learning is most often preferred (p. 13). There is a trick that
    aims at maximizing the per iteration information presented to the network simply
    by knowing how best to shuffle the examples (p. 15). This is followed by an entire
    set of tricks that must be coordinated together for maximum effectiveness. These
    include:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第一章温和地开始，向我们介绍了一些非常简单易行的实用技巧。每个技巧都附有易于理解的定性解释。讨论了随机（在线）学习与批处理模式学习的优缺点，明确指出随机学习通常更受欢迎（第13页）。有一个技巧旨在通过知道如何最好地打乱示例，最大化每次迭代提供给网络的信息（第15页）。接下来是一整套必须协调以实现最大效果的技巧。这些包括：
- en: '- how to normalize, decorrelate, and scale the inputs (p. 16) - how to choose
    the sigmoid (p. 17) - how to set target values (classification) (p. 19) - how
    to initialize the weights (p. 20) - how to pick the learning rates (p. 20).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如何对输入进行归一化、去相关和缩放（第 16 页） - 如何选择 sigmoid（第 17 页） - 如何设置目标值（分类）（第 19 页） -
    如何初始化权重（第 20 页） - 如何选择学习率（第 20 页）。'
- en: Additional issues discussed include the effectiveness of momentum and the choice
    between radial basis units and sigmoid units (p. 21).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论的其他问题包括动量的有效性以及在径向基单元和 sigmoid 单元之间的选择（第 21 页）。
- en: Chapter 1 then introduces us to a little of the theory, providing deeper understanding
    of some of the preceding tricks. Included are discussions of the effect of learning
    rates on the speed of learning and of the relationship between the Hessian matrix,
    the error surface, and the learning rates. Simple examples of linear and multilayer
    nets are provided to illustrate the theoretical results.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 章介绍了一些理论，提供了对前述技巧的更深入理解。其中包括学习率对学习速度的影响以及 Hessian 矩阵、误差面和学习率之间的关系讨论。提供了简单的线性和多层网络示例，以说明理论结果。
- en: The chapter next enters more difficult territory by giving an overview of second
    order methods (p. 31). Quickly summarized here, they are
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本章接下来进入更困难的领域，概述二阶方法（第 31 页）。快速总结如下：
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '- 先前发布于：Orr, G.B. 和 Müller, K.-R.（编）：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0（1998年）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    7–8, 2012.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon 等（编）：《神经网络：实用技巧》，第 2 版，LNCS 7700，第 7–8 页，2012年。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: 'Newton method: generally impractical to use since it requires inverting the
    full Hessian and works only in batch mode.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 牛顿法：通常不切实际，因为它需要反转完整的 Hessian，并且仅在批处理模式下有效。
- en: 'conjugate gradient: an O(N) algorithm that doesn''t use the Hessian, but requires
    a line search and so works only in batch mode.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 共轭梯度法：一个 O(N) 算法，不使用 Hessian，但需要线性搜索，因此仅在批处理模式下有效。
- en: 'Quasi-Newton, Broyden-Fletcher-Goldfarb-Shanno (BFGS) method:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 准牛顿法，Broyden-Fletcher-Goldfarb-Shanno（BFGS）方法：
- en: an O(N2) algorithm that computes an estimate of the inverse Hessian. It requires
    line search and also only works in batch mode.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 O(N²) 算法，用于计算逆 Hessian 的估计。它需要线性搜索，且仅在批处理模式下有效。
- en: 'Gauss-Newton method: an O(N3) algorithm that uses the square Jacobi approximation
    of the Hessian. Mainly used for batch and works only for mean squared error loss
    functions.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯-牛顿法：一个 O(N³) 算法，使用 Hessian 的平方雅可比近似。主要用于批处理，仅适用于均方误差损失函数。
- en: 'Levenberg Marquardt method: extends the Gauss-Newton method to include a regularization
    parameter for stability.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 莱文伯格-马夸特法：扩展了高斯-牛顿法，加入了稳定性正则化参数。
- en: Second order methods can greatly speed learning at each iteration but often
    at an excessive computational cost. However, by replacing the exact Hessian with
    an approximation of either the full or partial Hessian, the benefits of second
    order information can still be reaped without incurring as great a computational
    cost.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶方法可以在每次迭代中大大加快学习速度，但通常会产生过高的计算成本。然而，通过用完整或部分 Hessian 的近似替代精确 Hessian，仍然可以在不产生如此高的计算成本的情况下获得二阶信息的好处。
- en: The first and most direct method for estimating the full Hessian is finite differences
    which simply requires little more than two backpropagations to compute each row
    of the Hessian (p. 35). Another is to use the square Jacobian approximation which
    guarantees a positive semi-definite matrix which may be beneficial for improving
    stability. If even more simplification is desired, one can just compute the diagonal
    elements of the Hessian. All of the methods mentioned here are easily implemented
    using BP.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 估计完整 Hessian 的第一种也是最直接的方法是有限差分，这只需要进行两次反向传播来计算 Hessian 的每一行（第 35 页）。另一种方法是使用平方雅可比近似，这保证了一个正半定矩阵，这可能有助于提高稳定性。如果需要更简单的计算，可以只计算
    Hessian 的对角元素。这里提到的所有方法都可以通过 BP 轻松实现。
- en: Unfortunately, for very large networks, many of the classical second order methods
    do not work well because storing the Hessian is far too expensive and because
    batch mode, required by most of the methods, is too slow. *On-line* second order
    methods are needed instead. One such technique presented here is a stochastic
    diagonal Levenberg Marquardt method (p. 40).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于非常大的网络，许多经典的二阶方法效果不佳，因为存储海森矩阵的成本太高，而且大多数方法要求的批处理模式速度太慢。相反，需要*在线*的二阶方法。这里提出的一种技术是随机对角Levenberg-Marquardt方法（第40页）。
- en: If all that is needed is the product of the Hessian with an arbitrary vector
    rather than the Hessian itself, then much time can be saved using a method that
    computes this entire product directly using only a single backpropagation step
    (p. 37). Such a technique can be used to compute the largest eigenvalue and associated
    eigenvector of the Hessian. The inverse of the largest eigenvalue can then be
    used to obtain a good estimate of the learning rate.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所需的仅仅是海森矩阵与任意向量的乘积，而不是海森矩阵本身，那么可以使用一种只需一个反向传播步骤即可直接计算整个乘积的方法，从而节省大量时间（第37页）。这种技术可以用于计算海森矩阵的最大特征值及其相关特征向量。最大特征值的倒数可以用来获得良好的学习率估计。
- en: 'Finally, three useful tricks are presented for computing the principal eigenvalue
    and vector without having to compute the Hessian: the power method, Taylor expansion,
    and an on-line method (p. 42).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，介绍了三种有用的技巧，用于计算主特征值和特征向量，而无需计算海森矩阵：幂法、泰勒展开和一种在线方法（第42页）。
- en: Jenny & Klaus
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Jenny & Klaus
- en: 1 Efficient Backprop-
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 高效反向传播-
- en: Yann A. LeCun1, Léon Bottou1, Genevieve B. Orr2, and Klaus-Robert Müller3 1
    Image Processing Research Department AT& T Labs - Research, 100 Schulz Drive,
    Red Bank, NJ 07701-7033, USA 2 Willamette University, 900 State Street, Salem,
    OR 97301, USA
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Yann A. LeCun1, Léon Bottou1, Genevieve B. Orr2 和 Klaus-Robert Müller3 1 AT&T实验室图像处理研究部，100
    Schulz Drive，Red Bank，NJ 07701-7033，美国 2 威拉米特大学，900 State Street，Salem，OR 97301，美国
- en: 3 GMD FIRST, Rudower Chaussee 5, 12489 Berlin, Germany
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 3 GMD FIRST，Rudower Chaussee 5，12489 柏林，德国
- en: '{yann,leonb}@research.att.com, gorr@willamette.edu, klaus@first.gmd.de Abstract.
    The convergence of back-propagation learning is analyzed so as to explain common
    phenomenon observed by practitioners. Many undesirable behaviors of backprop can
    be avoided with tricks that are rarely exposed in serious technical publications.
    This paper gives some of those tricks, and offers explanations of why they work.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '{yann,leonb}@research.att.com, gorr@willamette.edu, klaus@first.gmd.de 摘要。分析了反向传播学习的收敛性，以解释从业者观察到的常见现象。通过一些很少在严肃技术出版物中披露的技巧，可以避免许多反向传播的不良行为。本文给出了一些这些技巧，并解释了它们为何有效。'
- en: Many authors have suggested that second-order optimization methods are advantageous
    for neural net training. It is shown that most "classical" second-order methods
    are impractical for large neural networks. A few methods are proposed that do
    not have these limitations.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 许多作者建议二阶优化方法在神经网络训练中是有优势的。研究表明，大多数“经典”二阶方法对于大型神经网络来说并不实用。提出了一些没有这些限制的方法。
- en: 1.1 Introduction
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 引言
- en: Backpropagation is a very popular neural network learning algorithm because
    it is conceptually simple, computationally efficient, and because it often works.
    However, getting it to work well, and sometimes to work at all, can seem more
    of an art than a science. Designing and training a network using backprop requires
    making many seemingly arbitrary choices such as the number and types of nodes,
    layers, learning rates, training and test sets, and so forth. These choices can
    be critical, yet there is no foolproof recipe for deciding them because they are
    largely problem and data dependent. However, there are heuristics and some underlying
    theory that can help guide a practitioner to make better choices.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是一种非常流行的神经网络学习算法，因为它在概念上简单，计算上高效，并且通常能有效工作。然而，要让它运作良好，有时似乎更多是一种艺术而非科学。使用反向传播设计和训练网络需要做出许多看似任意的选择，比如节点和层的数量与类型、学习率、训练集和测试集等等。这些选择可能至关重要，但由于它们在很大程度上依赖于问题和数据，因此没有万无一失的决策方法。然而，有一些启发式方法和基本理论可以帮助从业者做出更好的选择。
- en: In the first section below we introduce standard backpropagation and discuss
    a number of simple heuristics or tricks for improving its performance. We next
    discuss issues of convergence. We then describe a few "classical" second-order
    non-linear optimization techniques and show that their application to neural network
    training is very limited, despite many claims to the contrary in the literature.
    Finally, we present a few second-order methods that do accelerate learning in
    certain cases.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的第一部分中，我们介绍标准的反向传播，并讨论一些简单的启发式方法或技巧，以提高其性能。接下来，我们讨论收敛性问题。然后，我们描述一些“经典”的二阶非线性优化技术，并表明尽管文献中有许多相反的说法，但其在神经网络训练中的应用非常有限。最后，我们介绍一些在某些情况下确实加速学习的二阶方法。
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '- 以前出版于：Orr, G.B. 和 Müller, K.-R. (Eds.): LNCS 1524, ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0 (1998)。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    9–48, 2012.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon 等（Eds.）：NN: Tricks of the Trade, 第 2 版，LNCS 7700，第 9–48 页，2012。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: 1.2 Learning And Generalization
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 学习与泛化
- en: There are several approaches to automatic machine learning, but much of the
    successful approaches can be categorized as *gradient-based learning methods*.
    The learning machine, as represented in Figure 1.1, computes a function M(Zp,
    W) where Zp is the p-th input pattern, and W represents the collection of adjustable
    parameters in the system. A cost function Ep = C(Dp, M(Zp, W)), measures the discrepancy
    between Dp, the "correct" or desired output for pattern Zp, and the output produced
    by the system. The average cost function E*train*(W) is the average of the errors
    Ep over a set of input/output pairs called the training set
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 自动机器学习有多种方法，但许多成功的方法可以归类为*基于梯度的学习方法*。如图 1.1 所示，学习机器计算一个函数 M(Zp, W)，其中 Zp 是第
    p 个输入模式，W 表示系统中可调参数的集合。成本函数 Ep = C(Dp, M(Zp, W))，衡量 Dp（模式 Zp 的“正确”或期望输出）与系统产生的输出之间的差异。平均成本函数
    E*train*(W) 是一组称为训练集的输入/输出对中错误 Ep 的平均值。
- en: '{(Z1, D1), ....(ZP , DP )}. In the simplest setting, the learning problem consists
    in finding the value of W that minimizes E*train*(W). In practice, the performance
    of the system on a training set is of little interest. The more relevant measure
    is the error rate of the system in the field, where it would be used in practice.
    This performance is estimated by measuring the accuracy on a set of samples disjoint
    from the training set, called the test set. The most commonly used cost function
    is the Mean Squared Error:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '{(Z1, D1), ....(ZP , DP )}。在最简单的情况下，学习问题在于找到 W 的值，使 E*train*(W) 最小。在实践中，系统在训练集上的性能并不重要。更相关的指标是系统在实际应用中的错误率。这种性能通过测量在与训练集不重合的一组样本上的准确性来估计，该组样本称为测试集。最常用的成本函数是均方误差：'
- en: Ep = 12
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Ep = 12
- en: '![17_image_0.png](17_image_0.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![17_image_0.png](17_image_0.png)'
- en: Ep
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Ep
- en: Fig. 1.1. Gradient-based learning machine
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1. 基于梯度的学习机器
- en: This chapter is focused on strategies for improving the process of minimizing
    the cost function. However, these strategies must be used in conjunction with
    methods for maximizing the network's ability to *generalize*, that is, to predict
    the correct targets for patterns the learning system has not previously seen (e.g.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本章专注于改进成本函数最小化过程的策略。然而，这些策略必须与最大化网络*泛化*能力的方法结合使用，即预测学习系统之前未见过的模式的正确目标（例如。
- en: see chapters 2, 3, 4, 5 for more detail).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见第 2、3、4、5 章以获取更多细节）。
- en: To understand generalization, let us consider how backpropagation works. We
    start with a set of samples each of which is an input/output pair of the function
    to be learned. Since the measurement process is often noisy, there may be errors
    in the samples. We can imagine that if we collected multiple *sets* of samples
    then each set would look a little different because of the noise and because of
    the different points sampled. Each of these data sets would also result in networks
    with minima that are slightly different from each other and from the true function.
    In this chapter, we concentrate on improving the process of finding the minimum
    for the particular set of examples that we are given. Generalization techniques
    try to correct for the errors introduced into the network as a result of our choice
    of dataset. Both are important.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解泛化，我们来考虑反向传播是如何工作的。我们从一组样本开始，每个样本都是要学习的函数的输入/输出对。由于测量过程通常是嘈杂的，样本中可能存在错误。我们可以想象，如果我们收集多个*样本集*，那么每个样本集由于噪声和不同的采样点而看起来会稍有不同。每个数据集也将导致具有轻微不同的最小值的网络，并且与真实函数有所不同。在本章中，我们专注于改善找到特定示例集的最小值的过程。泛化技术试图纠正由于选择数据集而引入到网络中的错误。这两者都是重要的。
- en: Several theoretical efforts have analyzed the process of learning by minimizing
    the error on a training set (a process sometimes called Empirical Risk Minimization)
    [40, 41].
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一些理论努力分析了通过最小化训练集上的错误来学习的过程（有时称为经验风险最小化）[40, 41]。
- en: 'Some of those theoretical analyses are based on decomposing the generalization
    error into two terms: bias and variance (see e.g. [12]). The bias is a measure
    of how much the network output, averaged over all possible data sets differs from
    the desired function. The variance is a measure of how much the network output
    varies between datasets. Early in training, the bias is large because the network
    output is far from the desired function. The variance is very small because the
    data has had little influence yet. Late in training, the bias is small because
    the network has learned the underlying function. However, if trained too long,
    the network will also have learned the noise specific to that dataset. This is
    referred to as overtraining. In such a case, the variance will be large because
    the noise varies between datasets. It can be shown that the minimum total error
    will occur when the sum of bias and variance are minimal.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一些理论分析基于将泛化误差分解为两个部分：偏差和方差（参见例如[12]）。偏差是一个度量，表示网络输出在所有可能的数据集上平均值与期望函数之间的差异。方差是一个度量，表示网络输出在不同数据集之间的变化程度。在训练初期，偏差较大，因为网络输出远离期望函数。方差非常小，因为数据对网络的影响还不大。在训练后期，偏差较小，因为网络已经学习了基本函数。然而，如果训练时间过长，网络也会学习到该数据集特有的噪声。这被称为过拟合。在这种情况下，方差会很大，因为噪声在不同数据集之间变化。可以证明，当偏差和方差的总和最小化时，最小总误差将会出现。
- en: There are a number of techniques (e.g. early stopping, regularization) for maximizing
    the generalization ability of a network when using backprop. Many of these techniques
    are described in later chapters 2, 3, 5, 4.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用反向传播时，有许多技术（例如提前停止、正则化）可以最大化网络的泛化能力。这些技术的许多内容将在后面的第2、3、5、4章中描述。
- en: The idea of this chapter, therefore, is to present minimization strategies (given
    a cost function) and the tricks associated with increasing the speed and quality
    of the minimization. It is however clear that the choice of the model (model selection),
    the architecture and the cost function is crucial for obtaining a network that
    generalizes well. So keep in mind that if the wrong model class is used and no
    proper model selection is done, then even a superb minimization will clearly not
    help very much. In fact, the existence of overtraining has led several authors
    to suggest that inaccurate minimization algorithms can be better than good ones.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本章的思想是提出最小化策略（给定一个成本函数）以及与提高最小化速度和质量相关的技巧。然而，很明显，模型的选择（模型选择）、架构和成本函数对于获得一个能够良好泛化的网络至关重要。因此，请记住，如果使用了错误的模型类且没有进行适当的模型选择，即使是卓越的最小化也显然不会有太大帮助。实际上，过拟合的存在使得一些作者建议，不准确的最小化算法可能比好的算法更有效。
- en: 1.3 Standard Backpropagation
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 标准反向传播
- en: Although the tricks and analyses in this paper are primarily presented in the
    context of "classical" multi-layer feed-forward neural networks, many of them
    also apply to most other gradient-based learning methods.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本文中的技巧和分析主要是在“经典”多层前馈神经网络的背景下提出的，但其中许多也适用于大多数其他基于梯度的学习方法。
- en: The simplest form of multilayer learning machine trained with gradient-based
    learning is simply a stack of modules, each of which implements a function Xn
    =
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度学习训练的最简单形式的多层学习机器仅仅是一个模块堆栈，每个模块实现一个函数 Xn =
- en: Fn(Wn, Xn−1), where Xn is a vector representing the output of the module, Wn
    is the vector of tunable parameters in the module (a subset of W), and Xn−1 is
    the module's input vector (as well as the previous module's output vector). The
    input X0 to the first module is the input pattern Zp. If the partial derivative
    of
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Fn(Wn, Xn−1)，其中 Xn 是表示模块输出的向量，Wn 是模块中可调参数的向量（W 的一个子集），而 Xn−1 是模块的输入向量（以及前一个模块的输出向量）。输入
    X0 到第一个模块是输入模式 Zp。如果可以计算相对于
- en: Ep with respect to Xn is known, then the partial derivatives of Ep with respect
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Ep 关于 Xn 已知，那么 Ep 关于
- en: to Wn and Xn−1 can be computed using the backward recurrence
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Wn 和 Xn−1 的偏导数，则可以使用向后递归计算
- en: $$\frac{\partial E^{p}}{\partial W_{n}}=\frac{\partial F}{\partial W}(W_{n},X_{n-1})\frac{\partial
    E^{p}}{\partial X_{n}}$$ $$\frac{\partial E^{p}}{\partial X_{n-1}}=\frac{\partial
    F}{\partial X}(W_{n},X_{n-1})\frac{\partial E^{p}}{\partial X_{n}}\tag{1.1}$$
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{\partial E^{p}}{\partial W_{n}}=\frac{\partial F}{\partial W}(W_{n},X_{n-1})\frac{\partial
    E^{p}}{\partial X_{n}}$$ $$\frac{\partial E^{p}}{\partial X_{n-1}}=\frac{\partial
    F}{\partial X}(W_{n},X_{n-1})\frac{\partial E^{p}}{\partial X_{n}}\tag{1.1}$$
- en: where ∂F
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ∂F
- en: ∂W (Wn, Xn−1) is the Jacobian of F with respect to W evaluated at the point
    (Wn, Xn−1), and ∂F
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ∂W (Wn, Xn−1) 是在点 (Wn, Xn−1) 处评估的 F 关于 W 的雅可比矩阵，且 ∂F
- en: ∂X (Wn, Xn−1) is the Jacobian of F with respect to X.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ∂X (Wn, Xn−1) 是 F 关于 X 的雅可比矩阵。
- en: The Jacobian of a vector function is a matrix containing the partial derivatives
    of all the outputs with respect to all the inputs. When the above equations are
    applied to the modules in reverse order, from layer N to layer 1, all the partial
    derivatives of the cost function with respect to all the parameters can be computed.
    The way of computing gradients is known as back-propagation.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 向量函数的雅可比矩阵是一个包含所有输出相对于所有输入的偏导数的矩阵。当上述方程以反向顺序应用于模块时，从层 N 到层 1，可以计算成本函数相对于所有参数的所有偏导数。计算梯度的方法称为反向传播。
- en: Traditional multi-layer neural networks are a special case of the above system
    where the modules are alternated layers of matrix multiplications (the weights)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的多层神经网络是上述系统的一种特殊情况，其中模块是交替的矩阵乘法层（权重）
- en: 'and component-wise sigmoid functions (the units):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以及分量-wise 的 sigmoid 函数（单位）：
- en: $Y_{n}=W_{n}X_{n-1}$  $X_{n}=F(Y_{n})$
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: $Y_{n}=W_{n}X_{n-1}$  $X_{n}=F(Y_{n})$
- en: $$\begin{array}{l}{(1.2)}\\ {(1.3)}\end{array}$$
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{l}{(1.2)}\\ {(1.3)}\end{array}$$
- en: where Wn is a matrix whose number of columns is the dimension of Xn−1, and number
    of rows is the dimension of Xn. F is a vector function that applies a sigmoid
    function to each component of its input. Yn is the vector of weighted sums, or
    *total inputs*, to layer n.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Wn 是一个矩阵，其列数是 Xn−1 的维度，行数是 Xn 的维度。F 是一个向量函数，对输入的每个分量应用一个 sigmoid 函数。Yn 是层
    n 的加权和向量，或称为 *总输入*。
- en: 'Applying the chain rule to the equation above, the classical backpropagation
    equations are obtained:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对上述方程应用链式法则，可以得到经典的反向传播方程：
- en: $$(1.4)$$
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.4)$$
- en: $$\begin{array}{l}{{{\frac{\partial E^{p}}{\partial y_{n}^{i}}}=f^{\prime}(y_{n}^{i}){\frac{\partial
    E^{p}}{\partial x_{n}^{i}}}}}\\ {{{\frac{\partial E^{p}}{\partial w_{n}^{i j}}}=x_{n-1}^{j}{\frac{\partial
    E^{p}}{\partial y_{n}^{i}}}}}\\ {{{\frac{\partial E^{p}}{\partial x_{n-1}^{k}}}=\sum_{i}w_{n}^{i
    k}{\frac{\partial E^{p}}{\partial y_{n}^{i}}}.}}\end{array}$$
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{l}{{{\frac{\partial E^{p}}{\partial y_{n}^{i}}}=f^{\prime}(y_{n}^{i}){\frac{\partial
    E^{p}}{\partial x_{n}^{i}}}}}\\ {{{\frac{\partial E^{p}}{\partial w_{n}^{i j}}}=x_{n-1}^{j}{\frac{\partial
    E^{p}}{\partial y_{n}^{i}}}}}\\ {{{\frac{\partial E^{p}}{\partial x_{n-1}^{k}}}=\sum_{i}w_{n}^{i
    k}{\frac{\partial E^{p}}{\partial y_{n}^{i}}}.}}\end{array}$$
- en: $$\quad(1.5)$$  $$\quad(1.6)$$
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: $$\quad(1.5)$$  $$\quad(1.6)$$
- en: 'The above equations can also be written in matrix form:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程也可以用矩阵形式表示：
- en: $$(1.7)$$  $$(1.8)$$
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.7)$$  $$(1.8)$$
- en: $\begin{array}{l}\dfrac{\partial E^p}{\partial Y_n}=F'(Y_n)\dfrac{\partial E^p}{\partial
    X_n}\\ \dfrac{\partial E^p}{\partial W_n}=X_{n-1}\dfrac{\partial E^p}{\partial
    Y_n}\\ \dfrac{\partial E^p}{\partial X_{n-1}}=W_n^T\dfrac{\partial E^p}{\partial
    Y_n}.\\ \end{array}$  In this case we have introduced the initial state.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: $\begin{array}{l}\dfrac{\partial E^p}{\partial Y_n}=F'(Y_n)\dfrac{\partial E^p}{\partial
    X_n}\\ \dfrac{\partial E^p}{\partial W_n}=X_{n-1}\dfrac{\partial E^p}{\partial
    Y_n}\\ \dfrac{\partial E^p}{\partial X_{n-1}}=W_n^T\dfrac{\partial E^p}{\partial
    Y_n}.\\ \end{array}$ 在这种情况下，我们引入了初始状态。
- en: $$(1.9)$$
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.9)$$
- en: $$(1.10)$$
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.10)$$
- en: 'The simplest learning (minimization) procedure in such a setting is the gradient
    descent algorithm where W is iteratively adjusted as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最简单的学习（最小化）过程是梯度下降算法，其中W按如下方式迭代调整：
- en: $$W(t)=W(t-1)-\eta{\frac{\partial E}{\partial W}}.$$
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: $$W(t)=W(t-1)-\eta{\frac{\partial E}{\partial W}}.$$
- en: In the simplest case, η is a scalar constant. More sophisticated procedures
    use variable η. In other methods η takes the form of a diagonal matrix, or is
    an estimate of the inverse Hessian matrix of the cost function (second derivative
    matrix) such as in the Newton and Quasi-Newton methods described later in the
    chapter. A proper choice of η is important and will be discussed at length later.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的情况下，η是一个标量常数。更复杂的程序使用可变η。在其他方法中，η的形式为对角矩阵，或是成本函数的逆海森矩阵（第二导数矩阵）的估计，如本章后面所述的牛顿和拟牛顿方法。η的适当选择很重要，后面将详细讨论。
- en: 1.4 A Few Practical Tricks
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 一些实用技巧
- en: Backpropagation can be very slow particularly for multilayered networks where
    the cost surface is typically non-quadratic, non-convex, and high dimensional
    with many local minima and/or flat regions. There is no formula to guarantee that
    (1) the network will converge to a good solution, (2) convergence is swift, or
    (3) convergence even occurs at all. However, in this section we discuss a number
    of tricks that can greatly improve the chances of finding a good solution while
    also decreasing the convergence time often by orders of magnitude. More detailed
    theoretical justifications will be given in later sections.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播在多层网络中特别慢，因为成本表面通常是非二次的、非凸的，并且维度高，存在许多局部最小值和/或平坦区域。没有公式可以保证（1）网络将收敛到良好的解决方案，（2）收敛速度快，或（3）收敛实际上发生。然而，在本节中，我们讨论了一些技巧，可以大大提高找到良好解决方案的机会，同时通常可以按数量级减少收敛时间。更详细的理论依据将在后面的部分中给出。
- en: 1.4.1 Stochastic Versus Batch Learning
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4.1 随机学习与批量学习
- en: 'At each iteration, equation (1.10) requires a complete pass through the entire
    dataset in order to compute the *average* or true gradient. This is referred to
    as batch learning since an entire "batch" of data must be considered before weights
    are updated. Alternatively, one can use stochastic (online) learning where a single
    example {Zt, Dt} is chosen (e.g. randomly) from the training set at each iteration
    t. An *estimate* of the true gradient is then computed based on the error Et of
    that example, and then the weights are updated:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，公式（1.10）需要对整个数据集进行完整遍历，以计算*平均*或真实梯度。这被称为批量学习，因为在更新权重之前必须考虑整个“批次”的数据。或者，可以使用随机（在线）学习，其中在每次迭代t中从训练集中选择单个示例{Zt,
    Dt}（例如随机选择）。然后基于该示例的误差Et计算真实梯度的*估计*，并更新权重：
- en: $$W(t+1)=W(t)-\eta{\frac{\partial E^{t}}{\partial W}}.$$
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: $$W(t+1)=W(t)-\eta{\frac{\partial E^{t}}{\partial W}}.$$
- en: 'Because this estimate of the gradient is noisy, the weights may not move precisely
    down the gradient at each iteration. As we shall see, this "noise" at each iteration
    can be advantageous. Stochastic learning is generally the preferred method for
    basic backpropagation for the following three reasons:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个梯度估计是嘈杂的，权重在每次迭代中可能不会精确沿梯度移动。正如我们将看到的，每次迭代中的“噪声”可能是有利的。随机学习通常是基本反向传播的首选方法，原因有三：
- en: $$(1.11)$$
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.11)$$
- en: Advantages of Stochastic Learning 1. Stochastic learning is usually *much* faster
    than batch learning. 2. Stochastic learning also often results in better solutions.
    3. Stochastic learning can be used for tracking changes.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 随机学习的优势 1. 随机学习通常比批量学习*快得多*。 2. 随机学习通常还会产生更好的解决方案。 3. 随机学习可用于跟踪变化。
- en: Stochastic learning is most often *much* faster than batch learning particularly
    on large redundant datasets. The reason for this is simple to show. Consider the
    simple case where a training set of size 1000 is inadvertently composed of 10
    identical copies of a set with 100 samples. Averaging the gradient over all 1000
    patterns gives the exact same result as computing the gradient based on just the
    first 100. Thus, batch gradient descent is wasteful because it recomputes the
    same quantity 10 times before one parameter update. On the other hand, stochastic
    gradient will see a full epoch as 10 iterations through a 100-long training set.
    In practice, examples rarely appear more than once in a dataset, but there are
    usually clusters of patterns that are very similar. For example in phoneme classification,
    all of the patterns for the phoneme /æ/ will (hopefully) contain much of the same
    information. It is this redundancy that can make batch learning much slower than
    on-line.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 随机学习通常比批量学习*快得多*，尤其是在大型冗余数据集上。这一点很简单。考虑一个大小为1000的训练集，它意外地由10个相同的100样本集合组成。对所有1000个模式的梯度进行平均，得到的结果与仅基于前100个计算的梯度完全相同。因此，批量梯度下降是浪费的，因为它在每次参数更新前会重复计算同一数量10次。另一方面，随机梯度会将一个完整的周期视为对一个100长的训练集进行10次迭代。在实践中，示例很少在数据集中出现超过一次，但通常会有非常相似的模式簇。例如，在音素分类中，音素/æ/的所有模式将（希望）包含大部分相同的信息。正是这种冗余使得批量学习的速度远不如在线学习。
- en: Stochastic learning also often results in better solutions because of the noise
    in the updates. Nonlinear networks usually have multiple local minima of differing
    depths. The goal of training is to locate one of these minima. Batch learning
    will discover the minimum of whatever basin the weights are initially placed.
    In stochastic learning, the noise present in the updates can result in the weights
    jumping into the basin of another, possibly deeper, local minimum. This has been
    demonstrated in certain simplified cases [15, 30].
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 随机学习通常因为更新中的噪声而导致更好的解决方案。非线性网络通常有多个不同深度的局部最小值。训练的目标是找到这些最小值之一。批量学习将发现权重最初放置的盆地的最小值。在随机学习中，更新中存在的噪声可能导致权重跳入另一个可能更深的局部最小值的盆地。这在某些简化情况下已被证明[15,
    30]。
- en: Stochastic learning is also useful when the function being modeled is changing
    over time, a quite common scenario in industrial applications where the data distribution
    changes gradually over time (e.g. due to wear and tear of the machines). If the
    learning machine does not detect and follow the change it is impossible to learn
    the data properly and large generalization errors will result. With batch learning,
    changes go undetected and we obtain rather bad results since we are likely to
    average over several rules, whereas on-line learning - if operated properly (see
    below in section 1.4.7) - will track the changes and yield good approximation
    results.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当被建模的函数随时间变化时，随机学习也很有用，这在工业应用中是相当常见的场景，因为数据分布会随着时间逐渐变化（例如，由于机器的磨损）。如果学习机器无法检测和跟随这些变化，就无法正确学习数据，结果会产生较大的泛化误差。批量学习会让变化未被检测到，从而得到相当糟糕的结果，因为我们可能会对多条规则进行平均，而在线学习如果正确操作（见下文1.4.7节）会跟踪变化并产生良好的近似结果。
- en: 'Despite the advantages of stochastic learning, there are still reasons why
    one might consider using batch learning:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随机学习有诸多优势，但仍有理由考虑使用批量学习：
- en: Advantages of Batch Learning
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 批量学习的优势
- en: '![21_image_0.png](21_image_0.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![21_image_0.png](21_image_0.png)'
- en: '![21_image_1.png](21_image_1.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![21_image_1.png](21_image_1.png)'
- en: '![21_image_2.png](21_image_2.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![21_image_2.png](21_image_2.png)'
- en: '![21_image_3.png](21_image_3.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![21_image_3.png](21_image_3.png)'
- en: '![21_image_4.png](21_image_4.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![21_image_4.png](21_image_4.png)'
- en: 1. Conditions of convergence are well understood.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 收敛条件理解得很好。
- en: 2. Many acceleration techniques (e.g. conjugate gradient) only operate in batch
    learning.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 许多加速技术（例如共轭梯度法）仅在批量学习中操作。
- en: 3. Theoretical analysis of the weight dynamics and convergence rates are simpler.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 权重动态和收敛速度的理论分析更简单。
- en: These advantages stem from the same noise that make stochastic learning
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这些优势源于同样的噪声，使得随机学习
- en: advantageous. This noise, which is so critical for finding better local minima
    also prevents full convergence to the minimum. Instead of converging to the
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这种噪声在寻找更好的局部最小值时至关重要，但也阻止了完全收敛到最小值。不是收敛到
- en: exact minimum, the convergence stalls out due to the weight fluctuations. The
    size of the fluctuations depend on the degree of noise of the stochastic updates.
    The variance of the fluctuations around the local minimum is proportional to the
    learning rate η [28, 27, 6]. So in order to reduce the fluctuations we can either
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在精确最小值处，收敛因权重波动而停滞。波动的大小取决于随机更新的噪声程度。局部最小值附近波动的方差与学习率η成正比[28, 27, 6]。因此，为了减少波动，我们可以选择
- en: decrease (anneal) the learning rate or have an adaptive batch size. In theory
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 降低（退火）学习率或使用自适应批量大小。从理论上讲
- en: '[13, 30, 36, 35] it is shown that the optimal annealing schedule of the learning
    rate is of the form'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[13, 30, 36, 35]显示学习率的最佳退火计划是以下形式'
- en: $$\eta\sim{\frac{c}{t}},$$
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: $$\eta\sim{\frac{c}{t}},$$
- en: ', (1.12)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ', (1.12)'
- en: $\left(1.12\right)^{2}$
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: $\left(1.12\right)^{2}$
- en: where t is the number of patterns presented and c is a constant. In practice,
    this may be too fast (see chapter 13).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 其中t是呈现的模式数量，c是一个常数。在实践中，这可能过快（见第13章）。
- en: Another method to remove noise is to use "mini-batches", that is, start with
    a small batch size and increase the size as training proceeds. Møller discusses
    one method for doing this [25] and Orr [31] discusses this for linear problems.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种去除噪声的方法是使用“迷你批量”，即从小批量开始，随着训练的进行逐步增加批量大小。Møller讨论了一种方法[25]，Orr [31]则讨论了线性问题的这种方法。
- en: However, deciding the rate at which to increase the batch size and which inputs
    to include in the small batches is as difficult as determining the proper learning
    rate. Effectively the size of the learning rate in stochastic learning corresponds
    to the respective size of the mini batch.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，决定批量大小的增加速率以及在小批量中包含哪些输入与确定适当的学习率一样困难。有效地说，随机学习中的学习率大小对应于小批量的相应大小。
- en: Note also that the problem of removing the noise in the data may be less critical
    than one thinks because of generalization. Overtraining may occur long before
    the noise regime is even reached.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，去除数据中的噪声的问题可能没有想象中那么关键，因为泛化的存在。在噪声阶段达到之前，过拟合可能早已发生。
- en: Another advantage of batch training is that one is able to use second order
    methods to speed the learning process. Second order methods speed learning by
    estimating not just the gradient but also the curvature of the cost surface. Given
    the curvature, one can estimate the approximate location of the actual minimum.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 批量训练的另一个优势是可以使用二阶方法加速学习过程。二阶方法通过估计不仅是梯度还有成本曲面的曲率来加速学习。考虑到曲率，可以估计实际最小值的近似位置。
- en: Despite the advantages of batch updates, stochastic learning is still often
    the preferred method particularly when dealing with very large data sets because
    it is simply much faster.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管批量更新有其优势，随机学习仍然是更受欢迎的方法，尤其是在处理非常大的数据集时，因为它简单得多。
- en: 1.4.2 Shuffling The Examples
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4.2 打乱示例
- en: Networks learn the fastest from the most unexpected sample. Therefore, it is
    advisable to choose a sample at each iteration that is the most unfamiliar to
    the system. Note, this applies only to stochastic learning since the order of
    input presentation is irrelevant for batch1. Of course, there is no simple way
    to know which inputs are information rich, however, a very simple trick that crudely
    implements this idea is to simply choose successive examples that are from *different*
    classes since training examples belonging to the same class will most likely contain
    similar information.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 网络从最意外的样本中学习得最快。因此，建议在每次迭代中选择一个对系统最不熟悉的样本。请注意，这仅适用于随机学习，因为输入呈现的顺序对批量学习1并不重要。当然，没有简单的方法可以知道哪些输入信息丰富，然而，一个非常简单的技巧是简单地选择来自*不同*类别的连续示例，因为属于同一类别的训练示例最有可能包含相似的信息。
- en: Another heuristic for judging how much new information a training example contains
    is to examine the error between the network output and the target value when this
    input is presented. A large error indicates that this input has not been learned
    by the network and so contains a lot of new information. Therefore, it makes sense
    to present this input more frequently. Of course, by "large" we mean relative
    to all of the other training examples. As the network trains, these relative errors
    will change and so should the frequency of presentation for a particular input
    pattern. A method that modifies the probability of appearance of each pattern
    is called an *emphasizing scheme*.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 评估训练示例包含多少新信息的另一个启发式方法是检查当输入被呈现时网络输出与目标值之间的误差。较大的误差表明网络尚未学习该输入，因此包含很多新信息。因此，更频繁地呈现此输入是有意义的。当然，所说的“较大”是相对于所有其他训练示例而言。随着网络训练，这些相对误差会变化，因此特定输入模式的呈现频率也应变化。修改每个模式出现概率的方法称为*强调方案*。
- en: 1 The order in which gradients are summed in batch may be affected by roundoff
    error if there is a significant range of gradient values.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 1 在批处理中梯度求和的顺序可能会受到舍入误差的影响，如果梯度值的范围显著。
- en: Choose Examples with Maximum Information Content
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 选择信息含量最大的示例
- en: '![22_image_0.png](22_image_0.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![22_image_0.png](22_image_0.png)'
- en: '![22_image_1.png](22_image_1.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![22_image_1.png](22_image_1.png)'
- en: '![22_image_2.png](22_image_2.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![22_image_2.png](22_image_2.png)'
- en: '![22_image_3.png](22_image_3.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![22_image_3.png](22_image_3.png)'
- en: '![22_image_4.png](22_image_4.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![22_image_4.png](22_image_4.png)'
- en: 1. Shuffle the training set so that successive training examples never (rarely)
    belong to the same class.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 洗牌训练集，以确保连续的训练示例很少（几乎不）属于同一类。
- en: 2. Present input examples that produce a large error more frequently than examples
    that produce a small error.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 比较频繁地呈现产生较大误差的输入示例，而不是产生较小误差的示例。
- en: However, one must be careful when perturbing the normal frequencies of input
    examples because this changes the relative importance that the network places
    on different examples. This may or may not be desirable. For example, this technique
    applied to data containing outliers can be disastrous because outliers can produce
    large errors yet should not be presented frequently. On the other hand, this technique
    can be particularly beneficial for boosting the performance for infrequently occurring
    inputs, e.g. /z/ in phoneme recognition (see chapter 13, 14).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在扰动输入示例的正常频率时必须小心，因为这会改变网络对不同示例的相对重视程度。这可能是可取的，也可能不可取。例如，将此技术应用于包含异常值的数据可能是灾难性的，因为异常值可能产生较大的误差，但不应频繁呈现。另一方面，这项技术对于提升不常出现输入的性能，特别有利，例如在音素识别中/z/（见第13章、第14章）。
- en: 1.4.3 Normalizing The Inputs
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4.3 标准化输入
- en: Convergence is usually faster if the average of each input variable over the
    training set is close to zero. To see this, consider the extreme case where all
    the inputs are positive. Weights to a particular node in the first weight layer
    are updated by an amount proportional to δx where δ is the (scalar) error at that
    node and x is the input vector (see equations (1.5) and (1.10)). When all of the
    components of an input vector are positive, all of the updates of weights that
    feed into a node will be the same sign (i.e. sign(δ)). As a result, these weights
    can only all decrease or all increase *together* for a given input pattern. Thus,
    if a weight vector must change direction it can only do so by zigzagging which
    is inefficient and thus very slow.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练集上每个输入变量的平均值接近零，收敛通常会更快。为此，考虑一个极端情况，所有输入都是正数。第一权重层中特定节点的权重按与δx成比例的量更新，其中δ是该节点的（标量）误差，x是输入向量（见方程（1.5）和（1.10））。当输入向量的所有分量都是正数时，输入到某个节点的所有权重更新将具有相同的符号（即sign(δ)）。因此，对于给定的输入模式，这些权重只能全部同时减少或增加。这样，如果权重向量必须改变方向，它只能通过锯齿形变化，这效率低下，因此非常缓慢。
- en: In the above example, the inputs were all positive. However, in general, any
    shift of the average input away from zero will bias the updates in a particular
    direction and thus slow down learning. Therefore, it is good to shift the inputs
    so that the average over the training set is close to zero. This heuristic should
    be applied at all layers which means that we want the average of the *outputs*
    of a node to be close to zero because these outputs are the inputs to the next
    layer [19], chapter 10. This problem can be addressed by coordinating how the
    inputs are transformed with the choice of sigmoidal activation function. Here
    we discuss the input transformation. The discussion of the sigmoid follows.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，所有输入都是正值。然而，通常，任何将平均输入偏离零的平移都会使更新在特定方向上产生偏差，从而减缓学习。因此，最好将输入平移，使得训练集的平均值接近零。这个启发式方法应适用于所有层，这意味着我们希望一个节点的*输出*的平均值接近零，因为这些输出是下一层的输入[19]，第10章。这个问题可以通过协调输入的转换方式与sigmoid激活函数的选择来解决。这里我们讨论输入转换，sigmoid的讨论随后进行。
- en: Convergence is faster not only if the inputs are shifted as described above
    but also if they are scaled so that all have about the same covariance, Ci, where
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果按照上述方式平移输入，同时缩放使得所有输入的协方差大致相同，Ci，收敛速度会更快，其中
- en: $$C_{i}={\frac{1}{P}}\sum_{p=1}^{P}(z_{i}^{p})^{2}.$$
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: $$C_{i}={\frac{1}{P}}\sum_{p=1}^{P}(z_{i}^{p})^{2}.$$
- en: $$(1.13)$$
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.13)$$
- en: 2. (1.13)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 2. (1.13)
- en: Here, P is the number of training examples, Ci is the covariance of the i th
    input variable and zpi is the i th component of the pth training example. Scaling
    speeds learning because it helps to balance out the rate at which the weights
    connected to the input nodes learn. The value of the covariance should be matched
    with that of the sigmoid used. For the sigmoid given below, a covariance of 1
    is a good choice.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，P是训练示例的数量，Ci是第i个输入变量的协方差，zpi是第p个训练示例的第i个分量。缩放加速学习，因为它有助于平衡连接到输入节点的权重学习的速率。协方差的值应与所使用的sigmoid匹配。对于下面给出的sigmoid，协方差为1是一个不错的选择。
- en: The exception to scaling all covariances to the same value occurs when it is
    known that some inputs are of less significance than others. In such a case, it
    can be beneficial to scale the less significant inputs down so that they are "less
    visible" to the learning process.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有协方差进行统一缩放的例外情况是当已知某些输入的重要性低于其他输入时。在这种情况下，将不太重要的输入缩小以使其对学习过程“更不明显”可能是有益的。
- en: Transforming the Inputs 1. The average of each input variable over the training
    set should be close to zero.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 转换输入 1. 每个输入变量在训练集上的平均值应接近零。
- en: 2. Scale input variables so that their covariances are about the same.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 缩放输入变量，使其协方差大致相同。
- en: 3. Input variables should be uncorrelated if possible.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 输入变量应尽可能不相关。
- en: The above two tricks of shifting and scaling the inputs are quite simple to
    implement. Another trick that is quite effective but more difficult to implement
    is to decorrelate the inputs. Consider the simple network in Figure 1.2. If inputs
    are uncorrelated then it is possible to solve for the value of w1 that minimizes
    the error without any concern for w2, and vice versa. In other words, the two
    variables are independent (the system of equations is diagonal). With correlated
    inputs, one must solve for both simultaneously which is a much harder problem.
    Principal component analysis (also known as the Karhunen-Loeve expansion)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输入的平移和缩放两个技巧相当简单易行。另一个相当有效但更难实施的技巧是去相关输入。考虑图1.2中的简单网络。如果输入不相关，则可以求解使误差最小化的w1的值，而不必考虑w2，反之亦然。换句话说，这两个变量是独立的（方程组是对角的）。对于相关输入，必须同时求解两者，这将是一个更困难的问题。主成分分析（也称为Karhunen-Loeve展开）。
- en: can be used to remove *linear* correlations in inputs [10].
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用来消除输入中的*线性*相关性[10]。
- en: Inputs that are linearly dependent (the extreme case of correlation) may also
    produce degeneracies which may slow learning. Consider the case where one input
    is always twice the other input (z2 = 2z1). The network output is constant along
    lines W2 = v − (1/2)W1, where v is a constant. Thus, the gradient is zero along
    these directions (see Figure 1.2). Moving along these lines has absolutely no
    effect on learning. We are trying to solve in 2-D what is effectively only a 1-D
    problem. Ideally we want to remove one of the inputs which will decrease the size
    of the network.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 线性相关的输入（相关性的极端情况）也可能产生简并，进而减缓学习。考虑一个输入始终是另一个输入的两倍的情况（z2 = 2z1）。网络输出沿着 W2 = v
    − (1/2)W1 的线是常数，其中 v 是一个常数。因此，在这些方向上的梯度为零（见图 1.2）。沿着这些线移动对学习绝对没有影响。我们在二维中试图解决实际上只有一维的问题。理想情况下，我们希望移除一个输入，从而减少网络的规模。
- en: Figure 1.3 shows the entire process of transforming inputs. The steps are
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 显示了转换输入的整个过程。步骤为
- en: (1) shift inputs so the mean is zero, (2) decorrelate inputs, and (3) equalize
    covariances.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 平移输入使得均值为零，(2) 去相关输入，以及 (3) 使协方差相等。
- en: '![24_image_0.png](24_image_0.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![24_image_0.png](24_image_0.png)'
- en: Fig. 1.2. Linearly dependent inputs
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2. 线性相关的输入
- en: 1.4.4 The Sigmoid
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4.4 Sigmoid 函数
- en: Nonlinear activation functions are what give neural networks their nonlinear
    capabilities. One of the most common forms of activation function is the sigmoid
    which is a monotonically increasing function that asymptotes at some finite value
    as ±∞ is approached. The most common examples are the standard logistic function
    f(x)=1/(1 + e−x) and hyperbolic tangent f(x) = tanh(x) shown in Figure 1.4. Sigmoids
    that are symmetric about the origin (e.g. see Figure 1.4b) are preferred for the
    same reason that inputs should be normalized, namely,
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性激活函数赋予神经网络非线性能力。最常见的激活函数形式之一是 sigmoid，它是一个单调递增的函数，在 ±∞ 接近时渐近于某个有限值。最常见的例子是标准逻辑函数
    f(x)=1/(1 + e−x) 和双曲正切 f(x) = tanh(x)，如图 1.4 所示。关于原点对称的 sigmoid 函数（例如，见图 1.4b）更受欢迎，原因与输入应标准化相同，即，
- en: '![25_image_0.png](25_image_0.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![25_image_0.png](25_image_0.png)'
- en: Fig. 1.3. Transformation of inputs
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3. 输入的转换
- en: because they are more likely to produce outputs (which are *inputs* to the next
    layer) that are on average close to zero. This is in contrast, say, to the logistic
    function whose outputs are always positive and so must have a mean that is positive.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它们更可能产生平均接近零的输出（这些输出是下一个层的*输入*）。这与逻辑函数形成对比，因为逻辑函数的输出总是正值，因此其均值必须为正。
- en: '![25_image_1.png](25_image_1.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![25_image_1.png](25_image_1.png)'
- en: 'Fig. 1.4. (a) Not recommended: the standard logistic function, f(x)=1/(1 +
    e−x).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4. (a) 不推荐：标准逻辑函数，f(x)=1/(1 + e−x)。
- en: (b) Hyperbolic tangent, f(x)=1.7159 tanh - 23 x
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 双曲正切，f(x)=1.7159 tanh - 23 x
- en: .
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: Sigmoids
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sigmoid 函数
- en: 1. Symmetric sigmoids such as hyperbolic tangent often converge faster than
    the standard logistic function.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 对称的 sigmoid 函数，如双曲正切，通常比标准逻辑函数收敛更快。
- en: '2. A recommended sigmoid [19] is: f(x)=1.7159 tanh  23x. Since the tanh function
    is sometimes computationally expensive, an approximation of it by a ratio of polynomials
    can be used instead.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 推荐的 sigmoid [19] 是：f(x)=1.7159 tanh  23x。由于 tanh 函数有时计算成本较高，因此可以使用多项式比率的近似来替代。
- en: 3. Sometimes it is helpful to add a small linear term, e.g. f(x) = tanh(x)+
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 有时添加一个小的线性项是有帮助的，例如 f(x) = tanh(x)+
- en: ax so as to avoid flat spots.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ax，以避免平坦区域。
- en: The constants in the recommended sigmoid given above have been chosen so that,
    *when used with transformed inputs* (see previous discussion), the variance of
    the outputs will also be close to 1 because the effective gain of the sigmoid
    is roughly 1 over its useful range. In particular, this sigmoid has the properties
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 上述推荐的 sigmoid 函数中的常数已被选择，使得*在使用转换后的输入时*（见前面的讨论），输出的方差也接近 1，因为 sigmoid 的有效增益在其有效范围内大约为
    1。特别是，这个 sigmoid 具有以下特性
- en: (a) f(±1) = ±1, (b) the second derivative is a maximum at x = 1, and (c) the
    effective gain is close to 1.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: (a) f(±1) = ±1，(b) 在 x = 1 时二阶导数达到最大，(c) 有效增益接近 1。
- en: One of the potential problems with using symmetric sigmoids is that the error
    surface can be *very* flat near the origin. For this reason it is good to avoid
    initializing with very small weights. Because of the saturation of the sigmoids,
    the error surface is also flat far from the origin. Adding a small linear term
    to the sigmoid can sometimes help avoid the flat regions (see chapter 9).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对称sigmoid的一个潜在问题是误差面在原点附近可能*非常*平坦。因此，最好避免以非常小的权重进行初始化。由于sigmoid的饱和，误差面在远离原点的地方也会很平坦。向sigmoid添加一个小的线性项有时可以帮助避免平坦区域（见第9章）。
- en: 1.4.5 Choosing Target Values
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4.5 选择目标值
- en: In classification problems, target values are typically binary (e.g. {-1,+1}).
    Common wisdom might seem to suggest that the target values be set at the value
    of the sigmoid's asymptotes. However, this has several drawbacks.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，目标值通常是二进制的（例如{-1,+1}）。常识似乎暗示目标值应设置在sigmoid的渐近值上。然而，这有几个缺点。
- en: First, instabilities can result. The training process will try to drive the
    output as close as possible to the target values, which can only be achieved asymptotically.
    As a result, the weights (output and even hidden) are driven to larger and larger
    values where the sigmoid derivative is close to zero. The very large weights increase
    the gradients, however, these gradients are then multiplied by an exponentially
    small sigmoid derivative (except when a twisting term2 is added to the sigmoid)
    producing a weight update close to zero. As a result, the weights may become stuck.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，可能会导致不稳定。训练过程会尽量将输出驱动到尽可能接近目标值，而这只能渐近实现。因此，权重（输出甚至隐藏）被驱动到越来越大的值，此时sigmoid的导数接近于零。然而，非常大的权重会增加梯度，但这些梯度随后会被一个指数级小的sigmoid导数乘以（除非向sigmoid添加了扭曲项），产生接近于零的权重更新。因此，权重可能会陷入停滞。
- en: Second, when the outputs saturate, the network gives no indication of confidence
    level. When an input pattern falls near a decision boundary the output class is
    uncertain. Ideally this should be reflected in the network by an output value
    that is in between the two possible target values, i.e. not near either asymptote.
    However, large weights tend to force all outputs to the tails of the sigmoid regardless
    of the uncertainty. Thus, the network may predict a wrong class without giving
    any indication of its low confidence in the result. Large weights that saturate
    the nodes make it impossible to differentiate between typical and nontypical examples.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，当输出饱和时，网络不会给出置信水平的指示。当输入模式接近决策边界时，输出类别是不确定的。理想情况下，这应该通过一个介于两个可能目标值之间的输出值在网络中反映出来，即不接近任何渐近线。然而，大权重往往会将所有输出强制到sigmoid的尾部，而不考虑不确定性。因此，网络可能会预测错误的类别，而没有表明其对结果的低置信度。使节点饱和的大权重使得无法区分典型和非典型示例。
- en: A solution to these problems is to set the target values to be within the range
    of the sigmoid, rather than at the asymptotic values. Care must be taken, however,
    to insure that the node is not restricted to only the linear part of the sigmoid.
    Setting the target values to the point of the maximum second derivative on the
    sigmoid is the best way to take advantage of the nonlinearity without saturating
    the sigmoid. This is another reason the sigmoid in Figure 1.4b is a good choice.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些问题的一种方法是将目标值设置在sigmoid的范围内，而不是在渐近值处。然而，必须小心，以确保节点不仅限于sigmoid的线性部分。在sigmoid的最大二阶导数点上设置目标值是充分利用非线性而不饱和sigmoid的最佳方式。这也是图1.4b中sigmoid是良好选择的另一个原因。
- en: It has maximum second derivative at ±1 which correspond to the binary target
    values typical in classification problems.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在±1处具有最大二阶导数，这对应于分类问题中的二进制目标值。
- en: Targets Choose target values at the point of the maximum second derivative on
    the sigmoid so as to avoid saturating the output units.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 目标选择在sigmoid的最大二阶导数点上设置目标值，以避免输出单元饱和。
- en: 2 A twisting term is a small linear term added to the node output, e.g.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 2 扭曲项是添加到节点输出的小线性项，例如：
- en: f(x) = tanh(x) + ax.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = tanh(x) + ax。
- en: 1.4.6 Initializing The Weights
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4.6 初始化权重
- en: The starting values of the weights can have a significant effect on the training
    process. Weights should be chosen randomly but in such a way that the sigmoid
    is primarily activated in its linear region. If weights are all very large then
    the sigmoid will saturate resulting in small gradients that make learning slow.
    If weights are very small then gradients will also be very small. Intermediate
    weights that range over the sigmoid's linear region have the advantage that (1)
    the gradients are large enough that learning can proceed and (2) the network will
    learn the linear part of the mapping before the more difficult nonlinear part.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 权重的初始值对训练过程有显著影响。权重应该随机选择，但方式应该使得 sigmoid 主要在其线性区域内被激活。如果权重都非常大，sigmoid 会饱和，导致小梯度，从而使学习变慢。如果权重非常小，梯度也会非常小。在
    sigmoids 的线性区域内范围的中间权重有两个优点：（1）梯度足够大，可以继续学习；（2）网络将在更困难的非线性部分之前学习映射的线性部分。
- en: Achieving this requires coordination between the training set normalization,
    the choice of sigmoid, and the choice of weight initialization. We start by requiring
    that the distribution of the outputs of each node have a standard deviation
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一目标需要训练集归一化、选择 sigmoid 函数和权重初始化之间的协调。我们首先要求每个节点输出的分布具有标准差
- en: (σ) of approximately 1. This is achieved at the input layer by normalizing the
    training set as described earlier. To obtain a standard deviation close to 1 at
    the output of the first hidden layer we just need to use the above recommended
    sigmoid together with the requirement that the input to the sigmoid also have
    a standard deviation σy = 1. Assuming the inputs, yi, to a unit are uncorrelated
    with variance 1, the standard deviation of the units weighted sum will be
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: （σ）约为 1。这通过如前所述对训练集进行归一化在输入层实现。为了在第一个隐藏层的输出处获得接近 1 的标准差，我们只需使用上述推荐的 sigmoid，并要求输入到
    sigmoid 的标准差也为 σy = 1。假设单元的输入 yi 与方差为 1 的无关，单元加权和的标准差将为
- en: $$\sigma_{y_{i}}=$$
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: $$\sigma_{y_{i}}=$$
- en: $$\left(\sum_{j}w_{i j}^{2}\right)^{1/2}\;.$$
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: $$\left(\sum_{j}w_{i j}^{2}\right)^{1/2}\;.$$
- en: $$(1.14)$$
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.14)$$
- en: $$(1.15)$$
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.15)$$
- en: . (1.14)
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: . (1.14)
- en: Thus, to insure that the σyi are approximately 1 the weights should be randomly
    drawn from a distribution with mean zero and a standard deviation given by
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了确保 σyi 约为 1，权重应该从均值为零且标准差为
- en: $$\sigma_{w}=m^{-1/2}$$
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: $$\sigma_{w}=m^{-1/2}$$
- en: σw = m−1/2 (1.15)
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: σw = m−1/2 (1.15)
- en: where m is the number of inputs to the unit.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 m 是输入单元的数量。
- en: Initializing Weights
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 权重初始化
- en: '![27_image_0.png](27_image_0.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![27_image_0.png](27_image_0.png)'
- en: 1. the training set has been normalized, and
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 训练集已经归一化，并且
- en: '![27_image_1.png](27_image_1.png) 2. the sigmoid from Figure 1.4b has been
    used'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '![27_image_1.png](27_image_1.png) 2. 使用了图 1.4b 中的 sigmoid 函数'
- en: then weights should be randomly drawn from a distribution (e.g. uniform) ![27_image_3.png](27_image_3.png)
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 然后权重应该从一种分布（例如均匀分布）中随机抽取。![27_image_3.png](27_image_3.png)
- en: '![27_image_2.png](27_image_2.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![27_image_2.png](27_image_2.png)'
- en: $\Pi$ Ironfi a 0  1  .
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: $\Pi$ Ironfi a 0  1  随机抽取。
- en: $$\sigma_{w}=m^{-1/2}$$
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: $$\sigma_{w}=m^{-1/2}$$
- en: $\left(1.16\right)^{2}$
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: $\left(1.16\right)^{2}$
- en: σw = m−1/2 (1.16)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: σw = m−1/2 (1.16)
- en: where m is the fan-in (the number of connections feeding *into* the node).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 m 是输入节点的连接数（即馈送到节点的连接数量）。
- en: 1.4.7 Choosing Learning Rates
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4.7 选择学习率
- en: There is at least one well-principled method (described in section 1.9.2) for
    estimating the ideal learning rate η. Many other schemes (most of them rather
    empirical) have been proposed in the literature to automatically adjust the learning
    rate. Most of those schemes decrease the learning rate when the weight vector
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 至少有一种合理的方法（在 1.9.2 节中描述）可以估计理想的学习率 η。文献中提出了许多其他方案（大多数相当经验性）来自动调整学习率。这些方案中的大多数在权重向量
- en: '"oscillates", and increase it when the weight vector follows a relatively steady
    direction. The main problem with these methods is that they are not appropriate
    for stochastic gradient or on-line learning because the weight vector fluctuates
    all the time.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: “振荡”，当权重向量沿相对稳定的方向移动时增加它。这些方法的主要问题是它们不适用于随机梯度或在线学习，因为权重向量总是在波动。
- en: Beyond choosing a single global learning rate, it is clear that picking a different
    learning rate ηi for each weight can improve the convergence. A well-principled
    way of doing this, based on computing second derivatives, is described in section
    1.9.1. The main philosophy is to make sure that all the weights in the network
    converge roughly at the same speed.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选择一个单一的全局学习率外，显然为每个权重选择不同的学习率ηi可以改善收敛性。一种基于计算二阶导数的良好方法在第1.9.1节中进行了描述。其主要理念是确保网络中的所有权重大致以相同的速度收敛。
- en: Depending upon the curvature of the error surface, some weights may require
    a small learning rate in order to avoid divergence, while others may require a
    large learning rate in order to converge at a reasonable speed. Because of this,
    learning rates in the lower layers should generally be larger than in the higher
    layers (see Figure 1.21). This corrects for the fact that in most neural net architectures,
    the second derivative of the cost function with respect to weights in the lower
    layers is generally smaller than that of the higher layers. The rationale for
    the above heuristics will be discussed in more detail in later sections along
    with suggestions for how to choose the actual value of the learning rate for the
    different weights (see section 1.9.1).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 根据误差面的曲率，有些权重可能需要小学习率以避免发散，而其他权重可能需要大学习率以合理速度收敛。因此，较低层的学习率通常应该大于较高层的学习率（见图1.21）。这纠正了在大多数神经网络架构中，较低层的代价函数对权重的二阶导数通常小于较高层的事实。上述启发式规则的依据将在后续章节中更详细地讨论，并提供选择不同权重学习率实际值的建议（见第1.9.1节）。
- en: If shared weights are used such as in time-delay neural networks (TDNN) [42]
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用共享权重，例如在时延神经网络(TDNN)中[42]，
- en: or convolutional networks [20], the learning rate should be proportional to
    the square root of the number of connections sharing that weight, because we know
    that the gradients are a sum of more-or-less independent terms.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 或卷积网络[20]，学习率应与共享该权重的连接数的平方根成正比，因为我们知道梯度是更多或较少独立项的总和。
- en: '![28_image_0.png](28_image_0.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![28_image_0.png](28_image_0.png)'
- en: 'Other tricks for improving the convergence include:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 其他提高收敛性的技巧包括：
- en: Momentum. Momentum
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 动量。动量
- en: $$\Delta w(t+1)=\eta{\frac{\partial E_{t+1}}{\partial w}}+\mu\Delta w(t),$$
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w(t+1)=\eta{\frac{\partial E_{t+1}}{\partial w}}+\mu\Delta w(t)，$$
- en: can increase speed when the cost surface is highly nonspherical because it damps
    the size of the steps along directions of high curvature thus yielding a larger
    effective learning rate along the directions of low curvature [43] (μ denotes
    the strength of the momentum term). It has been claimed that momentum generally
    helps more in batch mode than in stochastic mode, but no systematic study of this
    are known to the authors.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 当代价面高度非球形时，可以加快速度，因为它减小了沿高曲率方向的步长，从而在低曲率方向上产生更大的有效学习率[43]（μ表示动量项的强度）。有人声称，在批处理模式下动量通常比在随机模式下更有效，但作者并不知道对此进行系统研究的情况。
- en: Adaptive Learning Rates. Many authors, including Sompolinsky et al. [37],
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应学习率。许多作者，包括Sompolinsky等人[37]，
- en: Darken & Moody [9], Sutton [38], Murata et al. [28] have proposed rules for
    automatically adapting the learning rates (see also [16]). These rules control
    the speed of convergence by increasing or decreasing the learning rate based on
    the error.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Darken & Moody [9]、Sutton [38]、Murata等人[28]提出了自动调整学习率的规则（参见[16]）。这些规则通过根据误差增加或减少学习率来控制收敛速度。
- en: 'We assume the following facts for a learning rate adaptation algorithm: (1)
    the smallest eigenvalue of the Hessian (see Eq.(1.27)) is sufficiently smaller
    than the second smallest eigenvalue and (2) therefore after a large number of
    iterations, the parameter vector w(t) will approach the minimum from the direction
    of the minimum eigenvector of the Hessian (see Eq.(1.27), Figure 1.5). Under these
    conditions the evolution of the estimated parameter can be thought of as a onedimensional
    process and the minimum eigenvector v can be approximated (for a large number
    of iterations: see Figure 1.5) by'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设学习率自适应算法的以下事实：（1）Hessian的最小特征值（见方程(1.27)）显著小于第二小特征值；（2）因此，在经过大量迭代后，参数向量w(t)将从Hessian的最小特征向量方向接近最小值（见方程(1.27)，图1.5）。在这些条件下，估计参数的演变可以被视为一维过程，最小特征向量v可以通过以下方式进行近似（经过大量迭代后：见图1.5）：
- en: $$\mathbf{v}=\langle{\frac{\partial E}{\partial w}}\rangle/\|\langle{\frac{\partial
    E}{\partial w}}\rangle\|,$$
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbf{v}=\langle{\frac{\partial E}{\partial w}}\rangle/\|\langle{\frac{\partial
    E}{\partial w}}\rangle\|,$$
- en: where   denotes the L2 norm. Hence we can adopt a projection
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 其中   表示L2范数。因此我们可以采用投影
- en: $$\xi=\langle\mathbf{v}^{T}{\frac{\partial E}{\partial w}}\rangle=\|\langle{\frac{\partial
    E}{\partial w}}\rangle\|$$
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: $$\xi=\langle\mathbf{v}^{T}{\frac{\partial E}{\partial w}}\rangle=\|\langle{\frac{\partial
    E}{\partial w}}\rangle\|$$
- en: to the approximated minimum Eigenvector v as a one dimensional measure of the
    distance to the minimum. This distance can be used to control the learning rate
    (for details see [28])
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 以近似的最小特征向量v作为距离最小值的一维度量。该距离可用于控制学习率（详细信息见[28]）。
- en: $$w(t+1)=w(t+1)-\eta t\frac{\partial E_{t}}{\partial w},\tag{1.17}$$ $$\mathbf{r}(t+1)=(1-\delta)\mathbf{r}(t)+\delta\frac{\partial
    E_{t}}{\partial w},\quad(0<\delta<1)$$ (1.18) $$\eta(t+1)=\eta(t)+\alpha\eta(t)\left(\beta\|\mathbf{r}(t+1)\|-\eta(t)\right),\tag{1.19}$$
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: $$w(t+1)=w(t+1)-\eta t\frac{\partial E_{t}}{\partial w},\tag{1.17}$$ $$\mathbf{r}(t+1)=(1-\delta)\mathbf{r}(t)+\delta\frac{\partial
    E_{t}}{\partial w},\quad(0<\delta<1)$$ (1.18) $$\eta(t+1)=\eta(t)+\alpha\eta(t)\left(\beta\|\mathbf{r}(t+1)\|-\eta(t)\right),\tag{1.19}$$
- en: where δ controls the leak size of the average, *α, β* are constants and r is
    used as auxiliary variable to calculate the leaky average of the gradient ∂E
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 其中δ控制平均值的泄漏大小，*α, β*为常数，r作为辅助变量用于计算梯度∂E的泄漏平均值。
- en: ∂w .
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ∂w .
- en: 'Note that this set of rules is easy to compute and straightforward to implement.
    We simply have to keep track of an additional vector in Eq.(1.18): the averaged
    gradient r. The norm of this vector then controls the size of the learning rate
    (see Eq.(1.19)). The algorithm follows the simple intuition: far away from the
    minimum (large distance ξ) it proceeds in big steps and close to the minimum it
    anneals the learning rate (for theoretical details see [28]).'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这组规则易于计算并且实现简单。我们只需在公式(1.18)中跟踪一个额外的向量：平均梯度r。该向量的范数控制学习率的大小（见公式(1.19)）。该算法遵循简单的直觉：远离最小值时（大距离ξ）采取大步前进，而靠近最小值时则降低学习率（有关理论细节见[28]）。
- en: 1.4.8 Radial Basis Functions Vs Sigmoid Units
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4.8 径向基函数与sigmoid单元
- en: Although most systems use nodes based on dot products and sigmoids, many other
    types of units (or layers) can be used. A common alternative is the radial basis
    function (RBF) network (see [7, 26, 5, 32]) In RBF networks, the dot product of
    the weight and input vector is replaced with a Euclidean distance
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数系统使用基于点积和sigmoid的节点，但可以使用许多其他类型的单元（或层）。一种常见的替代方案是径向基函数（RBF）网络（见[7, 26,
    5, 32]）。在RBF网络中，权重和输入向量的点积被欧几里得距离所替代。
- en: '![30_image_0.png](30_image_0.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![30_image_0.png](30_image_0.png)'
- en: Fig. 1.5. Convergence of the flow. During the final stage of learning the average
    flow is approximately one dimensional towards the minimum w∗ and it is a good
    approximation of the minimum eigenvalue direction of the Hessian.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5. 收敛的流动。在学习的最后阶段，平均流动大约是一维的，朝向最小值w∗，并且这是Hessian的最小特征值方向的良好近似。
- en: between the input and weight and the sigmoid is replaced by an exponential.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入和权重之间，sigmoid被指数所替代。
- en: The output activity is computed, e.g. for one output, as
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 输出活动的计算，例如，对于一个输出，可以表示为
- en: $$g(x)=\sum_{i=1}^{N}w_{i}\exp\left(-\frac{1}{2\sigma_{i}^{2}}\|x-\nu_{i}\|^{2}\right),$$
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: $$g(x)=\sum_{i=1}^{N}w_{i}\exp\left(-\frac{1}{2\sigma_{i}^{2}}\|x-\nu_{i}\|^{2}\right),$$
- en: where νi (σi) is the mean (standard deviation) of the i-th Gaussian. These units
    can replace or coexist with the standard units and they are usually trained by
    combination of gradient descent (for output units) and unsupervised clustering
    for determining the means and widths of the RBF units.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 其中νi（σi）是第i个高斯的均值（标准差）。这些单元可以替代或与标准单元共存，通常通过梯度下降（用于输出单元）和无监督聚类来确定RBF单元的均值和宽度进行训练。
- en: Unlike sigmoidal units which can cover the entire space, a single RBF unit covers
    only a small local region of the input space. This can be an advantage because
    learning can be faster. RBF units may also form a better set of basis functions
    to model the input space than sigmoid units, although this is highly problem dependent
    (see chapter 7). On the negative side, the locality property of RBFs may be a
    disadvantage particularly in high dimensional spaces because may units are needed
    to cover the spaces. RBFs are more appropriate in (low dimensional) upper layers
    and sigmoids in (high dimensional) lower layers.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 与能够覆盖整个空间的 sigmoid 单元不同，单个 RBF 单元仅覆盖输入空间的小局部区域。这可以是一个优势，因为学习速度可能更快。RBF 单元也可能形成比
    sigmoid 单元更好的基函数集来建模输入空间，尽管这高度依赖于具体问题（见第 7 章）。另一方面，RBF 的局部特性在高维空间中可能是一个劣势，因为需要很多单元来覆盖这些空间。RBF
    更适合在（低维）上层，而 sigmoid 更适合在（高维）下层。
- en: 1.5 Convergence Of Gradient Descent 1.5.1 A Little Theory
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 梯度下降的收敛 1.5.1 一点理论
- en: In this section we examine some of the theory behind the tricks presented earlier.
    We begin in one dimension where the update equation for gradient descent can be
    written as
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究之前展示的技巧背后的理论。我们从一维开始，其中梯度下降的更新方程可以写成
- en: $$W(t+1)=W(t)-\eta{\frac{d E(W)}{d W}}.$$
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: $$W(t+1)=W(t)-\eta{\frac{d E(W)}{d W}}.$$
- en: $$(1.20)$$
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.20)$$
- en: dW . (1.20)
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: dW . (1.20)
- en: We would like to know how the value of η affects convergence and the learning
    speed. Figure 1.6 illustrates the learning behavior for several different sizes
    of η when the weight W starts out in the vicinity of a local minimum. In one dimension,
    it is easy to define the optimal learning rate, ηopt, as being the learning rate
    that will move the weight to the minimum, Wmin, in precisely one step (see Figure
    1.6(i)b). If η is smaller than ηopt then the stepsize will be smaller and convergence
    will take multiple timesteps. If η is between ηopt and 2ηopt then the weight will
    oscillate around Wmin but will eventually converge (Figure 1.6(i)c).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望了解 η 的值如何影响收敛和学习速度。图 1.6 展示了当权重 W 在局部最小值附近开始时，几种不同 η 大小时的学习行为。在一维中，容易定义最优学习率
    ηopt，作为在精确一步内将权重移动到最小值 Wmin 的学习率（见图 1.6(i)b）。如果 η 小于 ηopt，则步长会更小，收敛将需要多个时间步。如果
    η 在 ηopt 和 2ηopt 之间，则权重将在 Wmin 附近振荡，但最终会收敛（图 1.6(i)c）。
- en: If η is more than twice the size of ηopt (Figure 1.6(i)d) then the stepsize
    is so large that the weight ends up farther from Wmin than before. Divergence
    results.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 η 超过 ηopt 的两倍（图 1.6(i)d），那么步长太大，导致权重最终比之前更远离 Wmin，结果出现发散。
- en: '![31_image_0.png](31_image_0.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![31_image_0.png](31_image_0.png)'
- en: Fig. 1.6. Gradient descent for different learning rates
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6. 不同学习率下的梯度下降
- en: 'What is the optimal value of the learning rate ηopt? Let us first consider
    the case in 1-dimension. Assuming that E can be approximated by a quadratic function,
    ηopt can be derived by first expanding E in a Taylor series about the current
    weight, Wc:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率 ηopt 的最优值是多少？我们首先考虑一维的情况。假设 E 可以用二次函数近似，则 ηopt 可以通过首先围绕当前权重 Wc 对 E 进行泰勒级数展开来推导：
- en: $$E(W)=E(W_{c})+(W-W_{c})\frac{dE(W_{c})}{dW}+\frac{1}{2}(W-W_{c})^{2}\frac{d^{2}E(W_{c})}{dW^{2}}+\ldots,\tag{1.21}$$
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(W)=E(W_{c})+(W-W_{c})\frac{dE(W_{c})}{dW}+\frac{1}{2}(W-W_{c})^{2}\frac{d^{2}E(W_{c})}{dW^{2}}+\ldots,\tag{1.21}$$
- en: where we use the shorthand dE(Wc)
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了简写 dE(Wc)
- en: dW ≡ dE
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: dW ≡ dE
- en: dW
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: dW
- en: W=Wc
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: W=Wc
- en: . If E is quadratic the second order derivative is constant and the higher order
    terms vanish. Differentiating both sides with respect to W then gives
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 。如果 E 是二次的，则二阶导数是常数，高阶项消失。然后对 W 两边求导，得到
- en: $$\frac{dE(W)}{dW}=\frac{dE(W_{c})}{dW}+(W-W_{c})\frac{d^{2}E(W_{c})}{dW^{2}}.\tag{1.22}$$
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{dE(W)}{dW}=\frac{dE(W_{c})}{dW}+(W-W_{c})\frac{d^{2}E(W_{c})}{dW^{2}}.\tag{1.22}$$
- en: Setting W = Wmin and noting that dE(Wmin)/dW = 0, we are left after rearranging
    with
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在一步内达到最小值。设 W = Wmin，并注意到 dE(Wmin)/dW = 0，经过重新排列后我们得到
- en: $\mathbb{E}(W_{n+1})/\epsilon$
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathbb{E}(W_{n+1})/\epsilon$
- en: $$W_{m i n}=W_{c}-\left(\frac{d^{2}E(W_{c})}{d W^{2}}\right)^{-1}\frac{d E(W_{c})}{d
    W}.$$
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: $$W_{m i n}=W_{c}-\left(\frac{d^{2}E(W_{c})}{d W^{2}}\right)^{-1}\frac{d E(W_{c})}{d
    W}.$$
- en: $$(1.23)$$
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.23)$$
- en: $$(1.24)$$
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.24)$$
- en: Comparing this with the update equation (1.20), we find that we can reach a
    minimum in one step if
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 将此与更新方程 (1.20) 相比较，我们发现如果
- en: $$\eta_{o p t}=\left(\frac{d^{2}E(W_{c})}{d W^{2}}\right)^{-1}.$$
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: $$\eta_{o p t}=\left(\frac{d^{2}E(W_{c})}{d W^{2}}\right)^{-1}$$
- en: $$(1.25)$$
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.25)$$
- en: Perhaps an easier way to obtain this same result is illustrated in Figure 1.6(ii).
    The bottom graph plots the gradient of E as a function of W. Since E is quadratic,
    the gradient is simply a straight line with value zero at the minimum and ∂E(Wc)
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 也许更简单的方法如图1.6(ii)所示。底部图表将E的梯度绘制为W的函数。由于E是二次的，梯度简单地是一条直线，在最小值处值为零，∂E(Wc)
- en: ∂W at the current weight Wc. ∂2E/∂2W is simply the slope of this line and is
    computed using the standard slope formula
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 当前权重Wc的∂W。∂2E/∂2W只是这条线的斜率，使用标准斜率公式计算。
- en: $$\partial^{2}E/\partial^{2}W={\frac{\partial E(W_{c})/\partial W-0}{W_{c}-W_{m
    i n}}}.$$
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: $$\partial^{2}E/\partial^{2}W={\frac{\partial E(W_{c})/\partial W-0}{W_{c}-W_{m
    i n}}}.$$
- en: . (1.25)
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: . (1.25)
- en: Solving for Wmin then gives equation (1.23).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 解Wmin后得到方程(1.23)。
- en: While the learning rate that gives fastest convergence is ηopt, the largest
    learning rate that can be used without causing divergence is (also see Figure
    1.6(i)d)
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然给出最快收敛的学习率是ηopt，但可以使用而不导致发散的最大学习率是（另见图1.6(i)d）
- en: $$(1.26)$$
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.26)$$
- en: $$\eta_{m a x}=2\eta_{o p t}.$$
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: $$\eta_{m a x}=2\eta_{o p t}.$$
- en: ηmax = 2ηopt. (1.26)
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ηmax = 2ηopt. (1.26)
- en: If E is not exactly quadratic then the higher order terms in equation (1.21)
    are not precisely zero and (1.23) is only an approximation. In such a case, it
    may take multiple iterations to locate the minimum even when using ηopt, however,
    convergence can still be quite fast.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 如果E不是完全二次的，则方程(1.21)中的高阶项不精确为零，(1.23)仅是一个近似值。在这种情况下，即使使用ηopt，也可能需要多次迭代才能找到最小值，但收敛仍然可以非常快。
- en: In multiple dimensions, determining ηopt is a bit more difficult because the
    right side of (1.24) is a matrix H−1 where H is called the Hessian whose components
    are given by
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在多维情况下，确定ηopt稍微困难，因为(1.24)的右侧是矩阵H−1，其中H称为Hessian，其组件由下式给出：
- en: $$H_{i j}\equiv{\frac{\partial^{2}E}{\partial W_{i}\partial W_{j}}}$$
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: $$H_{i j}\equiv{\frac{\partial^{2}E}{\partial W_{i}\partial W_{j}}}$$
- en: $$(1.27)$$
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.27)$$
- en: with 1 ≤ *i, j* ≤ N, and N equal to the total number of weights.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 其中1 ≤ *i, j* ≤ N，N为权重总数。
- en: H is a measure of the curvature of E. In two dimensions, the lines of constant
    E for a quadratic cost are oval in shape as shown in Figure 1.7. The eigenvectors
    of H point in the directions of the major and minor axes. The eigenvalues measure
    the steepness of E along the corresponding eigendirection. Example. In the least
    mean square (LMS) algorithm, we have a single layer linear network with error
    function
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: H是E的曲率度量。在二维中，二次成本的常量E线呈椭圆形，如图1.7所示。H的特征向量指向主要和次要轴的方向。特征值度量E沿相应特征方向的陡峭程度。例子。在最小均方(LMS)算法中，我们有一个单层线性网络，误差函数为
- en: $$E(W)={\frac{1}{2P}}\sum_{p=1}^{P}|d^{p}-\sum_{i}w_{i}x_{i}^{p}|^{2}$$
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(W)={\frac{1}{2P}}\sum_{p=1}^{P}|d^{p}-\sum_{i}w_{i}x_{i}^{p}|^{2}$$
- en: $$(1.28)$$
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.28)$$
- en: where P is the number of training vectors. The Hessian in this case turns out
    the be the same as the covariance matrix of the inputs,
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 其中P是训练向量的数量。在这种情况下，Hessian恰好等于输入的协方差矩阵，
- en: $$H={\frac{1}{P}}\sum_{p}x^{p}x^{p\,T}\,.$$
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: $$H={\frac{1}{P}}\sum_{p}x^{p}x^{p\,T}\,.$$
- en: $$(1.29)$$
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.29)$$
- en: xpxpT . (1.29)
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: xpxpT . (1.29)
- en: '![33_image_0.png](33_image_0.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![33_image_0.png](33_image_0.png)'
- en: Fig. 1.7. Lines of constant E
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7. 常量E的线
- en: '![33_image_1.png](33_image_1.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![33_image_1.png](33_image_1.png)'
- en: $$(1.30)$$
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.30)$$
- en: Fig. 1.8. For the LMS algorithm, the eigenvectors and eigenvalues of H measure
    the spread of the inputs in input space
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8. 对于LMS算法，H的特征向量和特征值度量输入空间中的输入分散
- en: Thus, each eigenvalue of H is also a measure of the covariance or spread of
    the inputs along the corresponding eigendirection as shown in Figure 1.8.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，H的每个特征值也是沿相应特征方向输入协方差或分散程度的度量，如图1.8所示。
- en: Using a scalar learning rate is problematic in multiple dimensions. We want
    η to be large so that convergence is fast along the shallow directions of E (small
    eigenvalues of H), however, if η is too large the weights will diverge along the
    steep directions (large eigenvalues of H). To see this more specifically, let
    us again expand E, but this time about a minimum
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在多维中使用标量学习率是有问题的。我们希望η较大，以便在E的平坦方向（H的小特征值）快速收敛，但如果η过大，权重将在陡峭方向（H的大特征值）发散。为更具体地观察这一点，我们再次展开E，但这次围绕最小值。
- en: $$E(W)\approx E(W_{m i n})+\frac{1}{2}(W-W_{m i n})^{T}H_{(W_{m i n})}(W-W_{m
    i n}).$$
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(W)\approx E(W_{m i n})+\frac{1}{2}(W-W_{m i n})^{T}H_{(W_{m i n})}(W-W_{m
    i n}).$$
- en: Differentiating (1.30) and using the result in the update equation (1.20) gives
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 对(1.30)进行微分，并在更新方程(1.20)中使用该结果得到
- en: $$W(t+1)=W(t)-\eta\frac{\partial E(t)}{\partial W}$$ $$=W(t)-\eta H_{(W_{min})}(W(t)-W_{min}).$$
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: $$W(t+1)=W(t)-\eta\frac{\partial E(t)}{\partial W}$$ $$=W(t)-\eta H_{(W_{min})}(W(t)-W_{min}).$$
- en: $$(1.31)$$ $$(1.32)$$
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.31)$$ $$(1.32)$$
- en: Subtracting Wmin from both sides gives
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 从两边减去 Wmin 得到
- en: $$(W(t+1)-W_{min})=(I-\eta H(W_{min}))(W(t)-W_{min}).\tag{1.33}$$
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: $$(W(t+1)-W_{min})=(I-\eta H(W_{min}))(W(t)-W_{min}).\tag{1.33}$$
- en: If the prefactor (I − ηH(Wmin)) is a matrix transformation that always shrinks
    a vector (i.e. its eigenvalues all have magnitude less than 1) then the update
    equation will converge.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前因子 (I − ηH(Wmin)) 是一个始终缩小向量的矩阵变换（即其特征值的绝对值均小于 1），则更新方程将收敛。
- en: How does this help with choosing the learning rates? Ideally we want different
    learning rates along the different eigendirections. This is simple if the eigendirections
    are lined up with the coordinate axes of the weights. In such a case, the weights
    are uncoupled and we can assign each weight its own learning rate based on the
    corresponding eigenvalue. However, if the weights are coupled then we must first
    rotate H such that H is diagonal, i.e. the coordinate axes line up with the eigendirections
    (see Figure 1.7b). This is the purpose of diagonalizing the Hessian discussed
    earlier.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这对选择学习率有什么帮助？理想情况下，我们希望沿不同的特征方向使用不同的学习率。如果特征方向与权重的坐标轴对齐，这很简单。在这种情况下，权重是解耦的，我们可以根据相应的特征值为每个权重分配其自己的学习率。然而，如果权重是耦合的，那么我们必须先旋转
    H 使得 H 是对角矩阵，即坐标轴与特征方向对齐（见图 1.7b）。这正是之前讨论的对 Hessian 进行对角化的目的。
- en: Let Θ be the rotation matrix such that
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 设 Θ 为旋转矩阵，使得
- en: $$A=\Theta H\Theta^{T}$$
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: $$A=\Theta H\Theta^{T}$$
- en: $$(1.34)$$
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.34)$$
- en: Λ = ΘHΘT (1.34)
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: Λ = ΘHΘT (1.34)
- en: where Λ is diagonal and ΘT Θ = I. The cost function then can be written as
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Λ 是对角矩阵，ΘT Θ = I。成本函数可以写成
- en: "E(W) ≈ E(Wmin) + 12 \f(W − Wmin) T ΘT   \fΘH(Wmin)ΘT [Θ(W − Wmin)] . (1.35)\
    \ Making a change of coordinates to ν = Θ(W − Wmin) simplifies the above equation\
    \ to"
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: "E(W) ≈ E(Wmin) + 12 \f(W − Wmin) T ΘT   \fΘH(Wmin)ΘT [Θ(W − Wmin)] . (1.35)\
    \ 将坐标变换为 ν = Θ(W − Wmin) 将上述方程简化为"
- en: $$E(\nu)\approx E(0)+{\frac{1}{2}}\nu^{T}\Lambda\nu$$
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(\nu)\approx E(0)+{\frac{1}{2}}\nu^{T}\Lambda\nu$$
- en: $$(1.36)$$
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.36)$$
- en: and the transformed update equation becomes
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 变换后的更新方程变为
- en: $$\nu(t+1)=(I-\eta\Lambda)\nu(t).$$
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: $$\nu(t+1)=(I-\eta\Lambda)\nu(t).$$
- en: ν(t + 1) = (I − ηΛ)ν(t). (1.37)
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ν(t + 1) = (I − ηΛ)ν(t). (1.37)
- en: Note that I − ηΛ is diagonal with diagonal components 1 − ηλi. This equation
    will converge if |1 − ηλi| < 1, i.e. η < 2λi for all i. If constrained to have
    a *single* scalar learning rate for all weights then we must require
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 I − ηΛ 是对角矩阵，对角元素为 1 − ηλi。此方程将收敛当 |1 − ηλi| < 1，即 η < 2λi 对所有 i 都成立。如果限制为所有权重都有一个*单一*标量学习率，则必须要求
- en: $$(1.37)$$
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.37)$$
- en: $$\eta<\frac{2}{\lambda_{m a x}}$$
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: $$\eta<\frac{2}{\lambda_{m a x}}$$
- en: $$(1.38)$$
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.38)$$
- en: in order to avoid divergence, where λmax is the largest eigenvalue of H. For
    fastest convergence we have
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免发散，其中 λmax 是 H 的最大特征值。为了实现最快收敛，我们有
- en: $$\eta_{o p t}={\frac{1}{\lambda_{m a x}}}.$$
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: $$\eta_{o p t}={\frac{1}{\lambda_{m a x}}}.$$
- en: $$(1.39)$$
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.39)$$
- en: . (1.39)
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: . (1.39)
- en: If λmin is a lot smaller than λmax then convergence will be very slow along
    the λmin direction. In fact, convergence time is proportional to the condition
    number κ ≡ λmax/λmin so that it is desirable to have as small an eigenvalue spread
    as possible.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 λmin 远小于 λmax，那么沿着 λmin 方向的收敛将非常缓慢。实际上，收敛时间与条件数 κ ≡ λmax/λmin 成正比，因此希望特征值的差异尽可能小。
- en: However, since we have rotated H to be aligned with the coordinate axes,
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们已经将 H 旋转为与坐标轴对齐，
- en: (1.37) consists actually of N independent 1-dimensional equations. Therefore,
    we can choose a learning rate for each weight independent of the others. We see
    that the optimal rate for the i th weight νi is ηopt,i = 1λi .
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: (1.37) 实际上由 N 个独立的 1 维方程组成。因此，我们可以为每个权重选择独立于其他权重的学习率。我们看到第 i 个权重 νi 的最优学习率为
    ηopt,i = 1λi。
- en: 1.5.2 Examples
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5.2 示例
- en: Linear Network. Figure 1.10 displays a set of 100 examples drawn from two Gaussian
    distributed classes centered at (-0.4,-0.8) and (0.4,0.8). The eigenvalues of
    the covariance matrix are 0.84 and 0.036. We train a single layer linear network
    with 2 inputs, 1 output, 2 weights, and 1 bias (see Figure (1.9)) using the LMS
    algorithm in batch mode. Figure 1.11 displays the weight trajectory and error
    during learning when using a learning rates of η = 1.5 and 2.5. Note that the
    learning rate (see Eq. 1.38) ηmax = 2/λmax = 2/.84 = 2.38 will cause divergences
    as is evident for η = 2.5.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 线性网络。图 1.10 显示了一组来自两个高斯分布类的 100 个样本，中心分别为 (-0.4,-0.8) 和 (0.4,0.8)。协方差矩阵的特征值为
    0.84 和 0.036。我们使用 LMS 算法以批量模式训练一个具有 2 个输入、1 个输出、2 个权重和 1 个偏置的单层线性网络（见图 1.9）。图
    1.11 显示了使用学习率 η = 1.5 和 2.5 进行学习时的权重轨迹和误差。请注意，学习率（见公式 1.38）ηmax = 2/λmax = 2/.84
    = 2.38 会导致发散，显然在 η = 2.5 时就是如此。
- en: '![35_image_1.png](35_image_1.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![35_image_1.png](35_image_1.png)'
- en: '![35_image_0.png](35_image_0.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![35_image_0.png](35_image_0.png)'
- en: Fig. 1.9. Simple linear network Fig. 1.10. Two classes drawn from gaussian distributions
    centered at (-0.4,-0.8) and (0.4,0.8)
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9. 简单线性网络 图 1.10. 从高斯分布中提取的以 (-0.4,-0.8) 和 (0.4,0.8) 为中心的两个类
- en: Figure 1.12 shows the same example using stochastic instead of batch mode learning.
    Here, a learning rate of η = 0.2 is used. One can see that the trajectory is much
    noisier than in batch mode since only an estimate of the gradient is used at each
    iteration. The cost is plotted as a function of epoch. An epoch here is simply
    defined as 100 input presentations which, for stochastic learning, corresponds
    to 100 weight updates. In batch, an epoch corresponds to one weight update. Multilayer
    Network. Figure 1.14 shows the architecture for a very simple multilayer network.
    It has 1 input, 1 hidden, and 1 output node. There are 2 weights and 2 biases.
    The activation function is f(x)=1.71 tanh((2/3)x). The training set contains 10
    examples from each of 2 classes. Both classes are Gaussian distributed with standard
    deviation 0.4. Class 1 has a mean of -1 and class 2 has a mean of +1. Target values
    are -1 for class 1 and +1 for class 2. Figure 1.13 shows the stochastic trajectory
    for the example.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12 显示了使用随机学习而不是批量学习的相同示例。在这里，使用的学习率为 η = 0.2。可以看到，轨迹比批量模式下要嘈杂得多，因为在每次迭代中仅使用梯度的估计。成本作为一个周期的函数绘制。在这里，一个周期简单定义为
    100 次输入展示，对于随机学习，这对应于 100 次权重更新。在批量模式下，一个周期对应于一次权重更新。多层网络。图 1.14 显示了一个非常简单的多层网络的架构。它有
    1 个输入、1 个隐藏和 1 个输出节点。共有 2 个权重和 2 个偏置。激活函数为 f(x)=1.71 tanh((2/3)x)。训练集包含来自 2 个类别的
    10 个示例。这两个类别均为标准差 0.4 的高斯分布。类别 1 的均值为 -1，类别 2 的均值为 +1。类别 1 的目标值为 -1，类别 2 的目标值为
    +1。图 1.13 显示了该示例的随机轨迹。
- en: '![36_image_0.png](36_image_0.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![36_image_0.png](36_image_0.png)'
- en: Fig. 1.11. Weight trajectory and error curve during learning for (a) η = 1.5
    and (b)
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11. （a）η = 1.5 和（b）学习过程中的权重轨迹和误差曲线
- en: η = 2.5
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: η = 2.5
- en: '![36_image_2.png](36_image_2.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![36_image_2.png](36_image_2.png)'
- en: Fig. 1.12. Weight trajectory and error curve during stochastic learning for
    η =
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1.12. 使用随机学习时的权重轨迹和误差曲线，η = '
- en: '0.2'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '0.2'
- en: '![36_image_1.png](36_image_1.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![36_image_1.png](36_image_1.png)'
- en: Fig. 1.13. Weight trajectories and errors for 1-1-1 network trained using stochastic
    learning
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13. 使用随机学习训练的 1-1-1 网络的权重轨迹和误差
- en: '![37_image_0.png](37_image_0.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![37_image_0.png](37_image_0.png)'
- en: Fig. 1.14. The minimal multilayer network
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.14. 最小多层网络
- en: 1.5.3 Input Transformations And Error Surface Transformations Revisited
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5.3 输入变换与误差面变换回顾
- en: We can use the results of the previous section to justify several of the tricks
    discussed earlier.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用上一节的结果来证明之前讨论的几个技巧。
- en: Subtract The Means From The Input Variables
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从输入变量中减去均值
- en: The reason for the above trick is that a nonzero mean in the input variables
    creates a *very large* eigenvalue. This means the condition number will be large,
    i.e. the cost surface will be steep in some directions and shallow in others so
    that convergence will be very slow. The solution is to simply preprocess the inputs
    by subtracting their means.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 上述技巧的原因是输入变量的非零均值会产生一个*非常大的*特征值。这意味着条件数将很大，即成本面在某些方向上将陡峭，在其他方向上则较平缓，因此收敛会非常缓慢。解决方案是通过减去均值简单预处理输入。
- en: 'For a single linear neuron, the eigenvectors of the Hessian (with means subtracted)
    point along the principal axes of the cloud of training vectors (recall Figure
    1.8). Inputs that have a large variation in spread along different directions
    of the input space will have a large condition number and slow learning. And so
    we recommend:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个线性神经元，海森矩阵（减去均值后）的特征向量沿训练向量云的主轴指向（回想图1.8）。在输入空间不同方向上有大变异的输入将具有大的条件数和缓慢学习。因此，我们建议：
- en: Normalize The Variances Of The Input Variables.
  id: totrans-413
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准化输入变量的方差。
- en: If the input variables are correlated, this will not make the error surface
    spherical, but it will possibly reduce its eccentricity.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入变量是相关的，这不会使误差表面呈球形，但可能会减少其偏心率。
- en: 'Correlated input variables usually cause the eigenvectors of H to be rotated
    away from the coordinate axes (Figure 1.7a versus 1.7b) thus weight updates are
    not decoupled. Decoupled weights make the "one learning rate per weight" method
    optimal, thus, we have the following trick:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的输入变量通常会导致H的特征向量偏离坐标轴（图1.7a与1.7b），因此权重更新并未解耦。解耦的权重使得“每个权重一个学习率”方法最优，因此，我们有以下技巧：
- en: Decorrelate The Input Variables.
  id: totrans-416
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解耦输入变量。
- en: 'Now suppose that the input variables of a neuron have been decorrelated, the
    Hessian for this neuron is then diagonal and its eigenvalues point along the coordinate
    axes. In such a case the gradient is not the best descent direction as can be
    seen in Fig 1.7b. At the point P, an arrow shows that gradient does not point
    towards the minimum. However, if we instead assign each weight its own learning
    rate (equal the inverse of the corresponding eigenvalue) then the descent direction
    will be in the direction of the other arrow that points directly towards the minimum:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设一个神经元的输入变量已经解耦，则该神经元的海森矩阵为对角形式，其特征值沿坐标轴指向。在这种情况下，梯度并不是最佳下降方向，如图1.7b所示。在点P，箭头显示梯度并不指向最小值。然而，如果我们为每个权重分配自己的学习率（等于相应特征值的倒数），那么下降方向将指向另一个箭头，直接指向最小值：
- en: Use a separate learning rate for each weight.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个权重使用单独的学习率。
- en: 1.6 Classical Second Order Optimization Methods
  id: totrans-419
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 经典二阶优化方法
- en: In the following we will briefly introduce the Newton, conjugate gradient, GaussNewton,
    Levenberg Marquardt and the Quasi-Newton (BFGS) method (see also
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将简要介绍牛顿、共轭梯度、高斯-牛顿、列文伯格-马夸特和拟牛顿（BFGS）方法（另见
- en: '[11, 34, 3, 5]).'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '[11, 34, 3, 5]）。'
- en: '![38_image_0.png](38_image_0.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![38_image_0.png](38_image_0.png)'
- en: $$(1.40)$$
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.40)$$
- en: Fig. 1.15. Sketch of the whitening properties of the Newton algorithm
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.15. 牛顿算法的白化特性示意图
- en: 1.6.1 Newton Algorithm
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6.1 牛顿算法
- en: To get an understanding of the Newton method let us recapitulate the results
    from section 1.5.1. Assuming a quadratic loss function E (see Eq.(1.21)) as depicted
    in Figure 1.6(ii), we can compute the weight update along the lines of Eq.(1.21)-(1.23)
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解牛顿方法，让我们回顾一下1.5.1节的结果。假设一个二次损失函数E（见公式（1.21）），如图1.6（ii）所示，我们可以沿着公式（1.21）-（1.23）计算权重更新。
- en: $$\Delta w=\eta\left({\frac{\partial^{2}E}{\partial w^{2}}}\right)^{-1}{\frac{\partial
    E}{\partial w}}=\eta H(w)^{-1}{\frac{\partial E}{\partial w}},$$
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w=\eta\left({\frac{\partial^{2}E}{\partial w^{2}}}\right)^{-1}{\frac{\partial
    E}{\partial w}}=\eta H(w)^{-1}{\frac{\partial E}{\partial w}},$$
- en: ∂w , (1.40)
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: ∂w , (1.40)
- en: where η must to be chosen in the range 0 <η< 1 since E is in practice not perfectly
    quadratic. In this equation information about the Hessian H is taken into account.
    If the error function was quadratic one step would be sufficient to converge.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 其中η必须在0 <η< 1范围内选择，因为E在实际中并不是完全二次的。在这个方程中考虑了海森矩阵H的信息。如果误差函数是二次的，一步就足够收敛。
- en: Usually the energy surface around the minimum is rather ellipsoid, or in the
    extreme like a taco shell, depending on the conditioning of the Hessian. A whitening
    transform, well known from signal processing literature [29] can change this ellipsoid
    shape to a spherical shape through u = ΘΛ1/2w (see Figure 1.15 and
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，最小值周围的能量表面相当椭球，或者在极端情况下像玉米饼，这取决于海森矩阵的条件。一个众所周知的白化变换[29]可以通过u = ΘΛ1/2w将这种椭球形状转换为球形（见图1.15和
- en: '![39_image_0.png](39_image_0.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![39_image_0.png](39_image_0.png)'
- en: Fig. 1.16. Sketch of conjugate gradient directions in a 2D error surface
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.16. 2D误差表面中共轭梯度方向的示意图
- en: 'Eq.(1.34)). So the inverse Hessian in Eq.(1.40) basically spheres out the error
    surface locally. The following two approaches can be shown to be equivalent: (a)
    use the Newton algorithm in an untransformed weight space and (b) do usual gradient
    descent in a whitened coordinate system (see Figure 1.15) [19].'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: Eq.(1.34)). 因此，Eq.(1.40) 中的逆海森矩阵基本上局部地平滑了误差面。以下两种方法可以证明是等效的：（a）在未变换的权重空间中使用牛顿算法，以及（b）在白化坐标系统中进行常规梯度下降（见图
    1.15）[19]。
- en: Summarizing, the Newton algorithm converges in one step if the error function
    is quadratic and (unlike gradient descent) it is invariant with respect to linear
    transformations of the input vectors. This means that the convergence time is
    not affected by shifts, scaling and rotation of input vectors. However one of
    the main drawbacks is that an N × N Hessian matrix must be stored and inverted,
    which takes O(N3) per iterations and is therefore impractical for more than a
    few variables. Since the error function is in general non-quadratic, there is
    no guarantee of convergence. If the Hessian is not positive definite (if it has
    some zero or even negative Eigenvalues where the error surface is flat or some
    directions are curved downward), then the Newton algorithm will diverge, so the
    Hessian must be positive definite. Of course the Hessian matrix of multilayer
    networks is in general not positive definite everywhere. For these reasons the
    Newton algorithm in its original form is not usable for general neural network
    learning.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，当误差函数为二次时，牛顿算法在一步内收敛，并且（与梯度下降不同）对输入向量的线性变换是不变的。这意味着收敛时间不受输入向量的平移、缩放和旋转的影响。然而，主要缺点之一是必须存储和求逆一个
    N × N 的海森矩阵，这在每次迭代中需要 O(N³) 的时间，因此对于多个变量来说是不切实际的。由于误差函数通常不是二次的，因此没有收敛的保证。如果海森矩阵不是正定的（如果它有一些零或甚至负特征值，使得误差面是平坦的或某些方向向下弯曲），那么牛顿算法将会发散，因此海森矩阵必须是正定的。当然，多层网络的海森矩阵通常在所有地方都不是正定的。因此，牛顿算法在其原始形式下不适用于一般的神经网络学习。
- en: However it gives good insights for developing more sophisticated algorithms,
    as discussed in the following.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这为开发更复杂的算法提供了良好的洞见，如下文所述。
- en: 1.6.2 Conjugate Gradient
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6.2 共轭梯度
- en: 'There are several important properties in conjugate gradient optimization:
    (1) it is a O(N) method, (2) it doesn''t use the Hessian explicitly, (3) it attempts
    to find descent directions that try to minimally spoil the result achieved in
    the previous iterations, (4) it uses a line search, and most importantly, (5)
    it works only for batch learning.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 共轭梯度优化有几个重要特性：（1）它是一个 O(N) 方法，（2）它不显式使用海森矩阵，（3）它试图找到下降方向，以尽量减少对先前迭代结果的干扰，（4）它使用线搜索，最重要的是，（5）它仅适用于批量学习。
- en: $$(1.41)$$
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.41)$$
- en: $$(1.42)$$
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.42)$$
- en: The third property is shown in Figure 1.16. Assume we pick a descent direction,
    e.g. the gradient, then we minimize along a line in this direction (line search).
    Subsequently we should try to find a direction along which the gradient does not
    change its direction, but merely its length (conjugate direction),
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个特性在图 1.16 中显示。假设我们选择一个下降方向，例如梯度，然后在该方向上沿一条线进行最小化（线搜索）。随后，我们应该尝试找到一个方向，在这个方向上梯度不会改变其方向，只是改变其长度（共轭方向），
- en: because moving along this direction will not spoil the result of the previous
    iteration. The evolution of the descent directions ρk at iteration k is given
    as ρk = −∇E(wk) + βkρk−1, (1.41)
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 因为沿这个方向移动不会破坏前一次迭代的结果。迭代 k 时下降方向 ρk 的演变给出为 ρk = −∇E(wk) + βkρk−1, (1.41)
- en: where the choice of βk can be done either according to Fletcher and Reeves [34]
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 βk 的选择可以根据 Fletcher 和 Reeves [34] 来进行。
- en: $$\beta_{k}={\frac{\nabla E(w_{k})^{T}\nabla E(w_{k})}{\nabla E(w_{k-1})^{T}\nabla
    E(w_{k-1})}}$$
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: $$\beta_{k}={\frac{\nabla E(w_{k})^{T}\nabla E(w_{k})}{\nabla E(w_{k-1})^{T}\nabla
    E(w_{k-1})}}$$
- en: $\text{or}$ Polak and Ribiere.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: $\text{或}$ Polak 和 Ribiere。
- en: $$\rho_{k}=-\nabla E(w_{k})+\beta_{k}\rho_{k-1},$$
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: $$\rho_{k}=-\nabla E(w_{k})+\beta_{k}\rho_{k-1},$$
- en: $$\beta_{k}=\frac{(\nabla E(w_{k})-\nabla E(w_{k-1}))^{T}\nabla E(w_{k})}{\nabla
    E(w_{k-1})^{T}\nabla E(w_{k-1})}.$$  Two directions $\rho_{k}$ and $\rho_{k-1}$
    are defined as conjugate if
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: $$\beta_{k}=\frac{(\nabla E(w_{k})-\nabla E(w_{k-1}))^{T}\nabla E(w_{k})}{\nabla
    E(w_{k-1})^{T}\nabla E(w_{k-1})}.$$ 两个方向 $\rho_{k}$ 和 $\rho_{k-1}$ 被定义为共轭的，如果
- en: ∇E(wk−1)T ∇E(wk−1) . (1.43)
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: ∇E(wk−1)T ∇E(wk−1) . (1.43)
- en: $$(1.43)$$
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.43)$$
- en: $$\rho_{k}^{T}H\rho_{k-1}=0,$$
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: $$\rho_{k}^{T}H\rho_{k-1}=0,$$
- en: i.e. conjugate directions are orthogonal directions in the space of an identity
    Hessian matrix (see Figure 1.17). Very important for convergence in both choices
    is a good line search procedure. For a perfectly quadratic function with N variables
    a convergence within N steps can be proved. For non-quadratic functions Polak
    and Ribiere's choice seems more robust. Conjugate gradient (1.41) can also be
    viewed as a smart choice for choosing the momentum term known in neural network
    training. It has been applied with large success in multi-layer network training
    on problems that are moderate sized with rather low redundancy in the data. Typical
    applications range from function approximation, robotic control
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 即共轭方向是在单位 Hessian 矩阵空间中的正交方向（参见图 1.17）。在这两种选择中，良好的线搜索过程对于收敛非常重要。对于具有 N 个变量的完全二次函数，可以证明在
    N 步内收敛。对于非二次函数，Polak 和 Ribiere 的选择似乎更为稳健。共轭梯度 (1.41) 也可以被视为在神经网络训练中选择动量项的聪明选择。它在解决适中规模且数据冗余较低的问题的多层网络训练中取得了巨大成功。典型应用范围包括函数逼近、机器人控制
- en: '[39], time-series prediction and other real valued problems where high accuracy
    is wanted. Clearly on large and redundant (classification) problems stochastic
    backpropagation is faster. Although attempts have been made to define minibatches
    [25], the main disadvantage of conjugate gradient methods remains that it is a
    batch method (partly due to the precision requirements in line search procedure).'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '[39]、时间序列预测和其他需要高精度的实值问题。显然，对于大型冗余（分类）问题，随机反向传播速度更快。尽管已尝试定义小批量 [25]，但共轭梯度方法的主要缺点仍然是它是一种批处理方法（部分由于在线搜索过程中对精度的要求）。'
- en: '![40_image_0.png](40_image_0.png)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
  zh: '![40_image_0.png](40_image_0.png)'
- en: Fig. 1.17. Sketch of conjugate gradient directions in a 2D error surface
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.17. 共轭梯度方向在 2D 误差面上的示意图
- en: 1.6.3 Quasi-Newton (Bfgs)
  id: totrans-454
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6.3 准牛顿 (BFGS)
- en: The Quasi-Newton (BFGS) method (1) iteratively computes an estimate of the inverse
    Hessian, (2) is an O(N2) algorithm, (3) requires line search and (4) it works
    only for batch learning.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 准牛顿 (BFGS) 方法（1）迭代计算逆 Hessian 的估计，（2）是 O(N²) 算法，（3）需要线搜索，并且（4）仅适用于批量学习。
- en: 'The positive definite estimate of the inverse Hessian is done directly without
    requiring matrix inversion and by only using gradient information. Algorithmically
    this can be described as follows: (1) first a positive definite matrix M is chosen,
    e.g. M = I, (2) then the search direction is set to'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 逆 Hessian 的正定估计是直接完成的，无需矩阵求逆，仅使用梯度信息。从算法上讲，这可以描述为：（1）首先选择一个正定矩阵 M，例如 M = I，（2）然后设置搜索方向为
- en: $$\rho(t)=M(t)\nabla E(w(t)),$$
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: $$\rho(t)=M(t)\nabla E(w(t)),$$
- en: (3) a line search is performed along ρ, which gives the update for the parameters
    at time t
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 沿着 ρ 进行线搜索，以获得时间 t 的参数更新
- en: $$w(t)=w(t-1)-\eta(t)\rho(t).$$
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: $$w(t)=w(t-1)-\eta(t)\rho(t).$$
- en: Finally (4) the estimate of the inverse Hessian is updated. Compared to the
    Newton algorithm the Quasi-Newton approach only needs gradient information.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 最后（4）更新逆 Hessian 的估计。与牛顿算法相比，准牛顿方法仅需要梯度信息。
- en: The most successful Quasi-Newton algorithm is the Broyden-Fletcher-GoldfarbShanno
    (BFGS) method. The update rule for the estimate of the inverse Hessian is
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 最成功的准牛顿算法是 Broyden-Fletcher-Goldfarb-Shanno (BFGS) 方法。逆 Hessian 的估计更新规则为
- en: $$M(t)=M(t-1)\left(1+\frac{\phi^{T}M\phi}{\delta^{T}\phi}\right)\frac{\delta\delta^{T}}{\delta^{T}\phi}-\left(\frac{\delta\phi^{T}M+M\phi\delta^{T}}{\delta^{T}\phi}\right),$$
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: $$M(t)=M(t-1)\left(1+\frac{\phi^{T}M\phi}{\delta^{T}\phi}\right)\frac{\delta\delta^{T}}{\delta^{T}\phi}-\left(\frac{\delta\phi^{T}M+M\phi\delta^{T}}{\delta^{T}\phi}\right),$$
- en: $$(1.44)$$
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.44)$$
- en: $$(1.45)$$
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.45)$$
- en: ', (1.44)'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: ', (1.44)'
- en: where some abbreviations have been used for the following N × 1 vectors
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用了一些缩写，表示以下 N × 1 向量
- en: $$\begin{array}{l}{{\phi=\nabla E(w(t))-\nabla E(w(t-1))}}\\ {{\delta=w(t)-w(t-1).}}\end{array}$$
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{l}{{\phi=\nabla E(w(t))-\nabla E(w(t-1))}}\\ {{\delta=w(t)-w(t-1).}}\end{array}$$
- en: Although, as mentioned above, the complexity is only O(N2), we are still required
    to store a N × N matrix, so the algorithm is only practical for small networks
    with non-redundant training sets. Recently some variants exist that aim to reduce
    storage requirements (see e.g. [3]).
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如上所述，复杂度仅为 O(N²)，但我们仍然需要存储一个 N × N 的矩阵，因此该算法仅适用于具有非冗余训练集的小型网络。最近一些变体旨在减少存储需求（参见例如[3]）。
- en: 1.6.4 Gauss-Newton And Levenberg Marquardt
  id: totrans-469
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6.4 高斯-牛顿法和勒文贝格-马夸尔特法
- en: Gauss-Newton and Levenberg Marquardt algorithm (1) use the square Jacobi approximation,
    (2) are mainly designed for batch learning, (3) have a complexity of O(N3) and
    (4) most important, they work only for mean squared error loss functions. The
    Gauss-Newton algorithm is like the Newton algorithm, however the Hessian is approximated
    by the square of the Jacobian (see also section 1.7.2 for a further discussion)
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯-牛顿和列文伯格-马夸特算法 (1) 使用平方雅可比近似，(2) 主要设计用于批量学习，(3) 复杂度为 O(N3)，(4) 最重要的是，它们仅适用于均方误差损失函数。高斯-牛顿算法类似于牛顿算法，然而海森矩阵是通过雅可比的平方来近似的（见
    1.7.2 节以获取进一步讨论）。
- en: $$\Lambda w=\left(\sum_{p}\frac{\partial f(w,x_{p})}{\partial w}^{T}\frac{\partial
    f(w,x_{p})}{\partial w}\right)^{-1}\nabla E(w).\tag{1.46}$$
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Lambda w=\left(\sum_{p}\frac{\partial f(w,x_{p})}{\partial w}^{T}\frac{\partial
    f(w,x_{p})}{\partial w}\right)^{-1}\nabla E(w).\tag{1.46}$$
- en: The Levenberg Marquardt method is like the Gauss-Newton above, but it has a
    regularization parameter μ that prevents it from blowing up, if some eigenvalues
    are small
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 列文伯格-马夸特方法类似于上述高斯-牛顿方法，但它具有一个正则化参数 μ，可以防止在某些特征值较小时出现爆炸。
- en: T ∂f(w, xp) ∂w + μI−1∇E(w), (1.47) Δw = - ∂f(w, xp) ∂w p
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: T ∂f(w, xp) ∂w + μI−1∇E(w), (1.47) Δw = - ∂f(w, xp) ∂w p
- en: where I denotes the unity matrix. The Gauss Newton method is valid for quadratic
    cost functions however a similar procedure also works with KullbackLeibler cost
    and is called Natural Gradient (see e.g. [1, 44, 2]).
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 I 表示单位矩阵。高斯-牛顿方法适用于二次成本函数，但类似的程序在使用 Kullback-Leibler 成本时也有效，称为自然梯度（见 e.g.
    [1, 44, 2]）。
- en: 1.7 Tricks To Compute The Hessian Information In Multilayer Networks
  id: totrans-475
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7 在多层网络中计算海森矩阵信息的技巧
- en: We will now discuss several techniques aimed at computing full or partial Hessian
    information by (a) finite difference method, (b) square Jacobian approximation
    (for Gauss-Newton and Levenberg-Marquardt algorithm), (c) computation of the diagonal
    of the Hessian and (d) by obtaining a product of the Hessian and a vector without
    computing the Hessian. Other semi-analytical techniques that allow the computation
    of the full Hessian are omitted because they are rather complicated and also require
    many forward/backward propagation steps [5, 8].
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将讨论几种旨在通过 (a) 有限差分法，(b) 平方雅可比近似（用于高斯-牛顿和列文伯格-马夸特算法），(c) 计算海森矩阵的对角线以及 (d)
    在不计算海森矩阵的情况下获得海森矩阵与向量的乘积的技术。其他允许计算完整海森矩阵的半解析技术被省略，因为它们相当复杂，并且还需要许多前向/反向传播步骤 [5,
    8]。
- en: 1.7.1 Finite Difference
  id: totrans-477
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7.1 有限差分法
- en: We can write the k-th line of the Hessian
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以写出海森矩阵的第 k 行
- en: $$H^{(k)}=\frac{\partial(\nabla E(w))}{\partial w_{k}}\sim\frac{\nabla E(w+\delta\phi_{k})-\nabla
    E(w)}{\delta},$$
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: $$H^{(k)}=\frac{\partial(\nabla E(w))}{\partial w_{k}}\sim\frac{\nabla E(w+\delta\phi_{k})-\nabla
    E(w)}{\delta},$$
- en: 'where φk = (0, 0, 0,..., 1*,...,* 0) is a vector of zeros and only one 1 at
    the k-th position. This can be implemented with a simple recipe: (1) compute the
    total gradient by multiple forward and backward propagation steps. (2) Add δ to
    the k-th parameter and compute again the gradient, and finally (3) subtract both
    results and divide by δ. Due to numerical errors in this computation scheme the
    resulting Hessian might not be perfectly symmetric. In this case it should be
    symmetrized as described below.'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 φk = (0, 0, 0,..., 1*,...,* 0) 是一个零向量，只有在第 k 个位置有一个 1。可以用一个简单的步骤实现：（1）通过多次前向和反向传播计算总梯度；（2）在第
    k 个参数上添加 δ 并再次计算梯度；最后（3）相减两者结果并除以 δ。由于该计算方案中的数值误差，得到的海森矩阵可能不完全对称。在这种情况下，应按照下面描述的方式进行对称化。
- en: 1.7.2 Square Jacobian Approximation For The Gauss-Newton And Levenberg-Marquardt
    Algorithms
  id: totrans-481
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7.2 高斯-牛顿与列文伯格-马夸特算法的平方雅可比近似
- en: Assuming a mean squared cost function
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个均方成本函数
- en: $$E(w)={\frac{1}{2}}\sum_{p}(d_{p}-f(w,x_{p}))^{T}(d_{p}-f(w,x_{p}))$$
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(w)={\frac{1}{2}}\sum_{p}(d_{p}-f(w,x_{p}))^{T}(d_{p}-f(w,x_{p}))$$
- en: then the gradient is
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 那么梯度为
- en: $${\frac{\partial E(w)}{\partial w}}=-\sum_{p}(d_{p}-f(w,x_{p}))^{T}{\frac{\partial
    f(w,x_{p})}{\partial w}}$$
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: $${\frac{\partial E(w)}{\partial w}}=-\sum_{p}(d_{p}-f(w,x_{p}))^{T}{\frac{\partial
    f(w,x_{p})}{\partial w}}$$
- en: $$(1.48)$$
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.48)$$
- en: $$(1.49)$$
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.49)$$
- en: and the Hessian follows as
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 海森矩阵如下
- en: $$H(w)=\sum_{p}\frac{\partial f(w,x_{p})}{\partial w}^{T}\frac{\partial f(w,x_{p})}{\partial
    w}+\sum_{p}(d_{p}-f(w,x_{p}))^{T}\frac{\partial^{2}f(w,x_{p})}{\partial w\partial
    w}.$$
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: $$H(w)=\sum_{p}\frac{\partial f(w,x_{p})}{\partial w}^{T}\frac{\partial f(w,x_{p})}{\partial
    w}+\sum_{p}(d_{p}-f(w,x_{p}))^{T}\frac{\partial^{2}f(w,x_{p})}{\partial w\partial
    w}.$$
- en: ∂w∂w . (1.50)
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: ∂w∂w . (1.50)
- en: 'A simplifying approximation of the Hessian is the square of the Jacobian which
    is a positive semi-definite matrix of dimension: N × O'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian的简化近似是Jacobian的平方，它是一个维度为N × O的半正定矩阵。
- en: $$(1.50)$$
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.50)$$
- en: $$H(w)\sim\sum_{p}{\frac{\partial f(w,x_{p})}{\partial w}}^{T}{\frac{\partial
    f(w,x_{p})}{\partial w}},$$
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: $$H(w)\sim\sum_{p}{\frac{\partial f(w,x_{p})}{\partial w}}^{T}{\frac{\partial
    f(w,x_{p})}{\partial w}},$$
- en: $$(1.51)$$
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.51)$$
- en: 'where the second term from Eq.(1.50) was dropped. This is equivalent to assuming
    that the network is a linear function of the parameters w. Again this is readily
    implemented for the k-th column of the Jacobian: for all training patterns, (1)
    we forward propagate, then (2) set the activity of the output units to 0 and only
    the k-th output to 1, (3) a backpropagation step is taken and the gradient is
    accumulated.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Eq.(1.50)中的第二项被去掉。这相当于假设网络是参数w的线性函数。同样，这可以轻松实现Jacobian的第k列：对于所有训练样本，（1）我们向前传播，然后（2）将输出单元的活动设置为0，仅将第k个输出设置为1，（3）进行一次反向传播步骤并累积梯度。
- en: 1.7.3 Backpropagating Second Derivatives
  id: totrans-496
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7.3 反向传播二阶导数
- en: Let us consider a multi-layer system with some functional blocks with Ni inputs,
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个具有Ni输入的一些功能模块的多层系统，
- en: No outputs and N parameters of the form O = F(*W, X*). Now assume we knew
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 没有输出，N个参数的形式为O = F(*W, X*)。现在假设我们知道
- en: ∂2*E/∂O*2, which is a No × No matrix. Then it is straight forward to compute
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: ∂2*E/∂O*2，这是一个No × No矩阵。然后可以直接计算
- en: this matrix  $$\frac{\partial^{2}E}{\partial W^{2}}=\frac{\partial O}{\partial
    W}^{T}\frac{\partial^{2}E}{\partial O^{2}}\frac{\partial O}{\partial W}+\frac{\partial
    E}{\partial O}\frac{\partial^{2}O}{\partial W^{2}}.\tag{1.52}$$  We can drop the
    second term in Eq.(1.52) and the resulting estimate of the
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵$$\frac{\partial^{2}E}{\partial W^{2}}=\frac{\partial O}{\partial W}^{T}\frac{\partial^{2}E}{\partial
    O^{2}}\frac{\partial O}{\partial W}+\frac{\partial E}{\partial O}\frac{\partial^{2}O}{\partial
    W^{2}}.\tag{1.52}$$我们可以去掉Eq.(1.52)中的第二项，得到的估计
- en: Hessian is positive semi-definite. A further reduction is achieved, if we ignore
    all
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian是半正定的。如果忽略所有的
- en: but the diagonal terms of ∂2E
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 但∂2E的对角项
- en: '∂O2 :'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '∂O2 :'
- en: $$(1.52)$$
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.52)$$
- en: $${\frac{\partial^{2}E}{\partial w_{i}^{2}}}=\sum_{k}{\frac{\partial^{2}E}{\partial
    o_{k}^{2}}}\left({\frac{\partial o_{k}}{\partial w_{i}}}\right)^{2}.$$
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: $${\frac{\partial^{2}E}{\partial w_{i}^{2}}}=\sum_{k}{\frac{\partial^{2}E}{\partial
    o_{k}^{2}}}\left({\frac{\partial o_{k}}{\partial w_{i}}}\right)^{2}.$$
- en: $$(1.53)$$
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.53)$$
- en: A similar derivation can be done to obtain the Ni times Ni matrix ∂2*E/∂x*2.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的推导可以得到Ni次的Ni矩阵∂2*E/∂x*2。
- en: 1.7.4 Backpropagating The Diagonal Hessian In Neural Nets
  id: totrans-508
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7.4 在神经网络中反向传播对角Hessian
- en: Backpropagation procedures for computing the diagonal Hessian are well known
    [18, 4, 19]. It is assumed that each layer in the network has the functional form
    oi = f(yi) = f(j wijxj ) (see Figure 1.18 for the sigmoidal network). Using the
    Gauss-Newton approximation (dropping the term that contain f(y)) we obtain:∂2E
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 计算对角Hessian的反向传播过程是众所周知的[18, 4, 19]。假设网络中的每一层具有功能形式oi = f(yi) = f(j wijxj)(见图1.18中的sigmoidal网络)。使用高斯-牛顿近似（去掉包含f(y)的项），我们得到：∂2E
- en: '**Lemma (dropping the term that contain $f^{\prime}(x)$**  $$\frac{\partial^{2}E}{\partial
    y_{k}^{2}}=\frac{\partial^{2}E}{\partial o_{k}^{2}}\left(f^{\prime}(y_{k})\right)^{2},$$  $$\frac{\partial^{2}E}{\partial
    w_{ki}^{2}}=\frac{\partial^{2}E}{\partial y_{k}^{2}}x_{i}^{2}$$'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理（去掉包含$f^{\prime}(x)$的项）** $$\frac{\partial^{2}E}{\partial y_{k}^{2}}=\frac{\partial^{2}E}{\partial
    o_{k}^{2}}\left(f^{\prime}(y_{k})\right)^{2},$$ $$\frac{\partial^{2}E}{\partial
    w_{ki}^{2}}=\frac{\partial^{2}E}{\partial y_{k}^{2}}x_{i}^{2}$$'
- en: $$(1.54)$$
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.54)$$
- en: $$(1.55)$$
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.55)$$
- en: $$(1.56)$$
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.56)$$
- en: $$(1.57)$$
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.57)$$
- en: and∂2E
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 和∂2E
- en: $${\frac{\partial^{2}E}{\partial x_{i}^{2}}}\sum_{k}{\frac{\partial^{2}E}{\partial
    y_{k}^{2}}}w_{k i}^{2}.$$
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: $${\frac{\partial^{2}E}{\partial x_{i}^{2}}}\sum_{k}{\frac{\partial^{2}E}{\partial
    y_{k}^{2}}}w_{k i}^{2}.$$
- en: With $f$ being a Gaussian nonlinearity as shown in Figure 1.18 for the RBF networks
    we obtain $$\frac{\partial^2E}{\partial w_{ki}^2}=\frac{\partial^2E}{\partial
    y_k^2}(x_i-w_{ki})^2\eqno(1.57)$$  1.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 设$f$为高斯非线性，如图1.18中RBF网络所示，我们得到$$\frac{\partial^2E}{\partial w_{ki}^2}=\frac{\partial^2E}{\partial
    y_k^2}(x_i-w_{ki})^2\eqno(1.57)$$  1.
- en: and∂2E
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 和∂2E
- en: $${\frac{\partial^{2}E}{\partial x_{i}^{2}}}=\sum_{k}{\frac{\partial^{2}E}{\partial
    y_{k}^{2}}}(x_{i}-w_{k i})^{2}.$$
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: $${\frac{\partial^{2}E}{\partial x_{i}^{2}}}=\sum_{k}{\frac{\partial^{2}E}{\partial
    y_{k}^{2}}}(x_{i}-w_{k i})^{2}.$$
- en: $$(1.58)$$
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.58)$$
- en: The cost of computing the diagonal second derivatives by running these equations
    from the last layer to the first one is essentially the same as the regular backpropation
    pass used for the gradient, except that the square of the weights are used in
    the weighted sums. This technique is applied in the "optimal brain damage" pruning
    procedure (see [21]).
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从最后一层到第一层运行这些方程来计算对角二阶导数的成本基本上与用于梯度的常规反向传播过程相同，只是加权和中使用了权重的平方。这项技术应用于“最佳大脑损伤”修剪程序（见
    [21]）。
- en: '![44_image_0.png](44_image_0.png)'
  id: totrans-522
  prefs: []
  type: TYPE_IMG
  zh: '![44_image_0.png](44_image_0.png)'
- en: $$(1.59)$$
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.59)$$
- en: 'Fig. 1.18. Backpropagating the diagonal Hessian: sigmoids (left) and RBFs (right)'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.18. 反向传播对角 Hessian：sigmoid（左）和 RBF（右）
- en: 1.7.5 Computing The Product Of The Hessian And A Vector
  id: totrans-525
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7.5 计算 Hessian 和向量的乘积
- en: In many methods that make use of the Hessian, the Hessian is used exclusively
    in products with a vector. Interestingly, there is a way of computing such products
    without going through the trouble of computing the Hessian itself. The finite
    difference method can fulfill this task for an arbitrary vector Ψ
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多利用 Hessian 的方法中，Hessian 专门用于与向量的乘积。有趣的是，有一种方法可以在不计算 Hessian 本身的情况下计算这些乘积。有限差分法可以为任意向量
    Ψ 完成此任务。
- en: $$H\Psi\sim\frac{1}{\alpha}\left(\frac{\partial E}{\partial w}(w+\alpha\Psi)-\frac{\partial
    E}{\partial w}(w)\right),$$
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: $$H\Psi\sim\frac{1}{\alpha}\left(\frac{\partial E}{\partial w}(w+\alpha\Psi)-\frac{\partial
    E}{\partial w}(w)\right),$$
- en: using only two gradient computations (at point w and w + αΨ respectively),
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用两个梯度计算（在点 w 和 w + αΨ）。
- en: which can be readily computed with backprop (α is a small constant).
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过反向传播轻松计算（α 是一个小常数）。
- en: This method can be applied to compute the principal eigenvector and eigenvalue
    of H by the power method. By iterating and setting
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法可通过幂法计算 H 的主特征向量和特征值。通过迭代并设置
- en: $$\Psi(t+1)={\frac{H\Psi(t)}{\|\Psi(t)\|}},$$
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Psi(t+1)={\frac{H\Psi(t)}{\|\Psi(t)\|}},$$
- en: $$(1.60)$$
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.60)$$
- en: the vector Ψ(t) will converge to the largest eigenvector of H and Ψ(t) to the
    corresponding eigenvalue [23, 14, 10]. See also [33] for an even more accurate
    method that (1) does not use finite differences and (2) has similar complexity.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 Ψ(t) 将收敛到 H 的最大特征向量，Ψ(t) 将收敛到对应的特征值 [23, 14, 10]。有关不使用有限差分且复杂度相似的更精确方法，请参见
    [33]。
- en: 1.8 Analysis Of The Hessian In Multi-Layer Networks
  id: totrans-534
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.8 多层网络中 Hessian 的分析
- en: 'It is interesting to understand how some of the tricks shown previously influence
    on the Hessian, i.e. how does the Hessian change with architecture and details
    of the implementation. Typically, the eigenvalue distribution of the Hessian looks
    like the one sketched in Figure 1.20: a few small eigenvalues, many medium ones
    and few very large ones. We will now argue that the *large eigenvalues* will cause
    the trouble in the training process because [23, 22]'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 了解之前展示的一些技巧如何影响 Hessian 是很有趣的，即 Hessian 如何随着架构和实现细节而变化。通常，Hessian 的特征值分布看起来像图
    1.20 中的草图：一些小特征值，许多中等特征值和少量非常大的特征值。我们现在将论证 *大特征值* 会在训练过程中造成问题，因为 [23, 22]
- en: '- non-zero mean inputs or neuron states [22] (see also chapter 10) - wide variations
    of the second derivatives from layer to layer - correlation between state variables.'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '- 非零均值输入或神经元状态 [22]（另见第 10 章） - 从层到层的二阶导数有广泛变化 - 状态变量之间的相关性。'
- en: 'To exemplify this, we show the eigenvalue distribution of a network trained
    on OCR data in Figure 1.20. Clearly, there is a wide spread of eigenvalues (see
    Figure 1.19) and we observe that the ratio between e.g. the first and the eleventh
    eigenvalue is about 8. The long tail of the eigenvalue distribution (see Figure
    1.20) is rather painful because the ratio between the largest and smallest eigenvalue
    gives the conditioning of the learning problem. A large ratio corresponds to a
    big difference in the axis of the ellipsoidal shaped error function:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们在图 1.20 中展示了在 OCR 数据上训练的网络的特征值分布。显然，特征值的分布很广泛（见图 1.19），我们观察到例如第一和第十一特征值之间的比例约为
    8。特征值分布的长尾（见图 1.20）相当痛苦，因为最大和最小特征值之间的比例给出了学习问题的条件数。较大的比例对应于椭圆形误差函数的轴之间的巨大差异：
- en: the larger the ratio, the more we find a taco-shell shaped minima, which are
    extremely steep towards the small axis and very flat along the long axis.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 比例越大，我们越能发现塔可壳形状的极小值，这些极小值在短轴方向上非常陡峭，在长轴方向上则非常平坦。
- en: '![45_image_0.png](45_image_0.png)'
  id: totrans-539
  prefs: []
  type: TYPE_IMG
  zh: '![45_image_0.png](45_image_0.png)'
- en: Fig. 1.19. Eigenvalue spectrum in a 4 layer shared weights network (256×128×64×10)
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.19. 四层共享权重网络的特征值谱（256×128×64×10）
- en: trained on 320 handwritten digits
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 在320个手写数字上进行训练
- en: '![46_image_0.png](46_image_0.png)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
  zh: '![46_image_0.png](46_image_0.png)'
- en: Fig. 1.20. Eigenvalue spectrum in a 4 layer shared weights network (256×128×64×10)
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.20. 四层共享权重网络的特征值谱（256×128×64×10）
- en: '![46_image_1.png](46_image_1.png)'
  id: totrans-544
  prefs: []
  type: TYPE_IMG
  zh: '![46_image_1.png](46_image_1.png)'
- en: 'trained on 320 handwritten digits Fig. 1.21. Multilayered architecture: the
    second derivative is often smaller in lower layers'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 在320个手写数字上进行训练 图1.21. 多层架构：在下层的二阶导数通常较小
- en: Another general characteristic of the Hessian in multi-layer networks is the
    spread between layers. In Figure 1.21 we roughly sketch how the shape of the Hessian
    varies from being rather flat in the first layer to being quite steep in the last
    layer. This affects the learning speed and can provide an ingredient to explain
    the slow learning in lower layers and the fast (sometime oscillating) learning
    in the last layer. A trick to compensate this different scale of learning is to
    use the inverse diagonal Hessian to control the learning rate (see also section
    1.6, chapter 17).
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 在多层网络中，Hessian的另一个普遍特征是层间的差异。在图1.21中，我们大致描绘了Hessian的形状如何从第一层的相对平坦逐渐变为最后一层的陡峭。这影响了学习速度，并可以提供解释下层学习缓慢和最后一层学习快速（有时震荡）的一个因素。补偿这种不同学习规模的一个技巧是使用逆对角Hessian来控制学习速率（另见第1.6节，第17章）。
- en: 1.9 Applying Second Order Methods To Multilayer Networks
  id: totrans-547
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.9 将二阶方法应用于多层网络
- en: 'Before we concentrate in this section on how to tailor second order techniques
    for training large networks, let us first repeat some rather pessimistic facts
    about applying classical second order methods. Techniques using full Hessian information
    (Gauss -Newton, Levenberg-Marquardt and BFGS) can only apply to very small networks
    trained in batch mode, however those small networks are not the ones that need
    speeding up the most. Most second order methods (conjugate gradient, BFGS, . .
    . ) require a line-search and can therefore not be used in the stochastic mode.
    Many of the tricks discussed previously apply only to batch learning. From our
    experience we know that a carefully tuned stochastic gradient descent is hard
    to beat on large classification problems. For smaller problems that require accurate
    real-valued outputs like in function approximation or control problems, we see
    that conjugate gradient (with Polak-Ribiere Eq.(1.43)) offers the best combination
    of speed, reliability and simplicity. Several attempts using "mini batches" in
    applying conjugate gradient to large and redundant problems have been made recently
    [17, 25, 31]. A variant of conjugate gradient optimization (called scaled CG)
    seems interesting: here the line search procedure is replaced by a 1D Levenberg
    Marquardt type algorithm [24].'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们本节集中讨论如何为大规模网络定制二阶技术之前，让我们先重复一些关于应用经典二阶方法的相当悲观的事实。使用完整Hessian信息的技术（高斯-牛顿法、勒温伯格-马夸特法和BFGS）仅能应用于以批量模式训练的非常小的网络，而这些小网络恰恰不是最需要加速的。大多数二阶方法（共轭梯度、BFGS等）需要线搜索，因此无法在随机模式下使用。许多之前讨论的技巧仅适用于批量学习。根据我们的经验，我们知道仔细调优的随机梯度下降在大型分类问题上很难被击败。对于需要准确实值输出的小问题，如函数逼近或控制问题，我们发现共轭梯度（结合Polak-Ribiere公式(1.43)）提供了速度、可靠性和简单性的最佳组合。最近对在大规模冗余问题上应用共轭梯度的“迷你批量”方法进行了几次尝试[17,
    25, 31]。一种共轭梯度优化的变体（称为缩放CG）似乎很有趣：在这里，线搜索程序被1D勒温伯格-马夸特法算法替代[24]。
- en: 1.9.1 A Stochastic Diagonal Levenberg Marquardt Method
  id: totrans-549
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.9.1 随机对角勒温伯格-马夸特法
- en: To obtain a stochastic version of the Levenberg Marquardt algorithm the idea
    is to compute the diagonal Hessian through a running estimate of the second derivative
    with respect to each parameter. The instantaneous second derivative can be obtained
    via backpropagation as shown in the formulas of section 1.7. As soon as we have
    those running estimates we can use them to compute individual learning rates for
    each parameter
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得勒温伯格-马夸特法的随机版本，理念是通过对每个参数的二阶导数进行动态估计来计算对角Hessian。瞬时二阶导数可以通过反向传播获得，如第1.7节的公式所示。一旦我们得到了这些动态估计，就可以用它们为每个参数计算个别学习速率。
- en: $$\eta_{k i}=\frac{\epsilon}{\langle\frac{\partial^{2}E}{\partial w_{k i}^{2}}\rangle+\mu},$$
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: $$\eta_{k i}=\frac{\epsilon}{\langle\frac{\partial^{2}E}{\partial w_{k i}^{2}}\rangle+\mu},$$
- en: $$(1.61)$$
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: $$(1.61)$$
- en: ', (1.61)'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: ', (1.61)'
- en: where  denotes the global learning rate, and  ∂2E
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 其中  表示全局学习速率，和  ∂2E
- en: ∂w2ki is a running estimate of the diagonal second derivative with respect to
    wki. μ is a parameter to prevent ηki from blowing up in case the second derivative
    is small, i.e. when the optimization moves in flat parts of the error function.
    The running estimate is computed as
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: ∂w2ki是关于wki的对角二阶导数的运行估计。μ是一个参数，用于防止在二阶导数较小时（即优化在误差函数的平坦部分移动时）ηki爆炸。运行估计的计算为
- en: $$\langle\frac{\partial^{2}E}{\partial w_{ki}^{2}}\rangle_{new}=(1-\gamma)\langle\frac{\partial^{2}E}{\partial
    w_{ki}^{2}}\rangle_{old}+\gamma\frac{\partial^{2}E^{p}}{\partial w_{ki}^{2}},\tag{1.62}$$
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: $$\langle\frac{\partial^{2}E}{\partial w_{ki}^{2}}\rangle_{new}=(1-\gamma)\langle\frac{\partial^{2}E}{\partial
    w_{ki}^{2}}\rangle_{old}+\gamma\frac{\partial^{2}E^{p}}{\partial w_{ki}^{2}},\tag{1.62}$$
- en: where γ is a small constant that determines the amount of memory that is being
    used. The second derivatives can be computed prior to training over e.g. a subset
    of the training set. Since they change only very slowly they only need to be reestimated
    every few epochs. Note that the additional cost over regular backpropagation is
    negligible and convergence is - as a rule of thumb - about three times faster
    than a carefully tuned stochastic gradient algorithm.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 其中γ是一个小常数，用于确定使用的记忆量。二阶导数可以在训练之前计算，例如在训练集的一个子集上。由于它们变化非常缓慢，因此只需每几个周期重新估计一次。请注意，常规反向传播的额外成本微不足道，收敛速度——作为经验法则——约比精心调整的随机梯度算法快三倍。
- en: In Figure 1.22 and 1.23 we see the convergence of the stochastic diagonal Levenberg
    Marquardt method (1.61) for a toy example with two different sets of learning
    rates. Obviously the experiment shown Figure 1.22 contains fewer fluctuations
    than in Figure 1.23 due to smaller learning rates.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1.22和1.23中，我们看到随机对角Levenberg-Marquardt方法（1.61）在两个不同学习率的玩具示例中的收敛情况。显然，图1.22中的实验波动比图1.23要小，这是由于学习率较小。
- en: '![48_image_0.png](48_image_0.png)'
  id: totrans-559
  prefs: []
  type: TYPE_IMG
  zh: '![48_image_0.png](48_image_0.png)'
- en: Fig. 1.22. Stochastic diagonal Levenberg-Marquardt algorithm. Data set from
    2 Gaussians with 100 examples. The network has one linear unit, 2 inputs and 1
    output, i.e.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.22. 随机对角Levenberg-Marquardt算法。数据集来自2个高斯分布，包含100个示例。网络有一个线性单元，2个输入和1个输出，即
- en: three parameters (2 weights, 1 bias).
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 三个参数（2个权重，1个偏置）。
- en: '![49_image_0.png](49_image_0.png)'
  id: totrans-562
  prefs: []
  type: TYPE_IMG
  zh: '![49_image_0.png](49_image_0.png)'
- en: Fig. 1.23. Stochastic diagonal Levenberg-Marquardt algorithm. Data set from
    2 Gaussians with 100 examples. The network has one linear unit, 2 inputs and 1
    output, i.e.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.23. 随机对角Levenberg-Marquardt算法。数据集来自2个高斯分布，包含100个示例。网络有一个线性单元，2个输入和1个输出，即
- en: three parameters (2 weights, 1 bias).
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 三个参数（2个权重，1个偏置）。
- en: 1.9.2 Computing The Principal Eigenvalue/Vector Of The Hessian
  id: totrans-565
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.9.2 计算海森矩阵的主特征值/特征向量
- en: 'In the following we give three tricks for computing the principal eigenvalue/Vector
    of the Hessian without having to compute the Hessian itself. Remember that in
    section 1.4.7 we also introduced a method to approximate the smallest eigenvector
    of the Hessian (without having to compute the Hessian) through averaging (see
    also [28]). Power Method. We repeat the result of our discussion in section 1.7.5:
    starting from a random initial vector Ψ, the iteration'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们给出计算海森矩阵的主特征值/特征向量的三个技巧，而无需计算海森矩阵本身。请记住，在第1.4.7节中，我们还介绍了一种通过平均来近似海森矩阵最小特征向量的方法（另见[28]）。幂法。我们重申在第1.7.5节中的讨论结果：从一个随机初始向量Ψ开始，迭代
- en: $$\Psi_{n e w}=H\frac{\Psi_{o l d}}{\|\Psi_{o l d}\|},$$
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Psi_{n e w}=H\frac{\Psi_{o l d}}{\|\Psi_{o l d}\|},$$
- en: will eventually converge to the principal eigenvector (or a vector in the principal
    eigenspace) and Ψold will converge to the corresponding eigenvalue [14, 10].
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 最终将收敛到主特征向量（或主特征空间中的一个向量），而Ψold将收敛到相应的特征值[14, 10]。
- en: '![50_image_0.png](50_image_0.png)'
  id: totrans-569
  prefs: []
  type: TYPE_IMG
  zh: '![50_image_0.png](50_image_0.png)'
- en: Fig. 1.24. Evolution of the eigenvalue as a function of the number of pattern
    presentations for a shared weight network with 5 layers, 64638 connections and
    1278 free parameters. The training set consists of 1000 handwritten digits.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.24. 特征值随样本展示次数变化的演变，对于一个具有5层、64638个连接和1278个自由参数的共享权重网络。训练集包含1000个手写数字。
- en: Taylor Expansion. Another method makes use of the fact that small perturbations
    of the gradient also lead to the principal eigenvector of H
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 泰勒展开。另一种方法利用了梯度的小扰动也会导致H的主特征向量的事实。
- en: $$\Psi_{new}=\frac{1}{\alpha}\left(\frac{\partial E}{\partial w}(w+\alpha\frac{\Psi_{old}}{\|\Psi_{old}\|})-\frac{\partial
    E}{\partial w}(w)\right),\tag{1.63}$$
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Psi_{new}=\frac{1}{\alpha}\left(\frac{\partial E}{\partial w}(w+\alpha\frac{\Psi_{old}}{\|\Psi_{old}\|})-\frac{\partial
    E}{\partial w}(w)\right),\tag{1.63}$$
- en: where α is a small constant. One iteration of this procedure requires two forward
    and two backward propagation steps for each pattern in the training set.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 其中α是一个小常数。该程序的一次迭代需要对训练集中每个模式进行两次正向和两次反向传播步骤。
- en: Online Computation of Ψ. The following rule makes use of the running average
    to obtain the largest eigenvalue of the average Hessian very fast
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: Ψ的在线计算。以下规则利用运行平均值快速获取平均Hessian的最大特征值
- en: $$\Psi_{new}=(1-\gamma)\Psi+\frac{1}{\alpha}\left(\frac{\partial E^{p}}{\partial
    w}(w+\alpha\frac{\Psi_{old}}{\|\Psi_{old}\|})-\frac{\partial E}{\partial w}(w)\right).\tag{1.64}$$
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Psi_{new}=(1-\gamma)\Psi+\frac{1}{\alpha}\left(\frac{\partial E^{p}}{\partial
    w}(w+\alpha\frac{\Psi_{old}}{\|\Psi_{old}\|})-\frac{\partial E}{\partial w}(w)\right).\tag{1.64}$$
- en: '![51_image_0.png](51_image_0.png)'
  id: totrans-576
  prefs: []
  type: TYPE_IMG
  zh: '![51_image_0.png](51_image_0.png)'
- en: Learning Rate Predicted Optimal Learning Rate
  id: totrans-577
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率 预测的最优学习率
- en: Fig. 1.25. Mean squared error as a function of the ratio between learning rate
    and predicted optimal learning rate for a fully connected network (784 × 30 ×
    10). The training set consists of 300 handwritten digits.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.25. 平方误差作为学习率与预测最优学习率比值的函数，用于一个完全连接的网络（784 × 30 × 10）。训练集包含300个手写数字。
- en: 'To summarize, the eigenvalue/vector computations:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 总结特征值/特征向量计算：
- en: 1. a random vector is chosen for initialization of Ψ,
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 随机选择一个向量用于Ψ的初始化，
- en: 2. an input pattern is presented with desired output, a forward and backward
    propagation, step is performed and the gradients G(w) are stored, 3. α Ψold Ψold
    is added to the current weight vector w, 4. a forward and backward propagation
    step is performed with the perturbed weight vector and the gradients G(w) are
    stored, 5. the difference 1/α(G(w) − G(w)) is computed and the running average
    of the eigenvector is updated, 6. we loop from (2)-(6) until a reasonably stable
    result is obtained for Ψ,
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 提供一个输入模式和期望输出，执行正向和反向传播步骤，并存储梯度G(w)，3. α Ψold Ψold被添加到当前权重向量w，4. 使用扰动的权重向量执行正向和反向传播步骤，并存储梯度G(w)，5.
    计算差值1/α(G(w) − G(w))并更新特征向量的运行平均值，6. 从(2)-(6)循环，直到获得Ψ的合理稳定结果，
- en: 7. the optimal learning rate is then given as
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 最优学习率被定义为
- en: $\mathbf{a}$
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathbf{a}$
- en: $||\psi|$.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: $||\psi|$.
- en: '![52_image_0.png](52_image_0.png)'
  id: totrans-585
  prefs: []
  type: TYPE_IMG
  zh: '![52_image_0.png](52_image_0.png)'
- en: LEARNING RATE
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率
- en: PREDICTED OPTIMAL LEARNING RATE
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的最优学习率
- en: Fig. 1.26. Mean squared error as a function of the ratio between learning rate
    and predicted optimal learning rate for a shared weight network with 5 layers
    (1024 ×
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.26. 平方误差作为共享权重网络中学习率与预测最优学习率比值的函数，具有5层（1024 ×
- en: 1568 × 392 × 400 × 100 × 10), 64638 (local) connections and 1278 free parameters
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 1568 × 392 × 400 × 100 × 10)，64638（局部）连接和1278个自由参数
- en: (shared weights). The training set consists of 1000 handwritten digits.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: （共享权重）。训练集包含1000个手写数字。
- en: In Figure 1.24 we see the evolution of the eigenvalue as a function of the number
    of pattern presentations for a neural network in a handwritten character recognition
    task. In practice we adapt the leak size of the running average in order to get
    fewer fluctuations (as also indicated on the figure). In the figure we see that
    after fewer than 100 pattern presentations the correct order of magnitude for
    the eigenvalue, i.e the learning rate is reached. From the experiments we also
    observe that the fluctuations of the average Hessian over training are small.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1.24中，我们看到特征值随模式呈现次数的变化，用于手写字符识别任务的神经网络。实际上，我们调整运行平均值的泄漏大小以减少波动（如图中所示）。图中显示，在少于100次模式呈现后，特征值的正确量级即学习率达成。从实验中我们还观察到，训练过程中平均Hessian的波动很小。
- en: In Figure 1.25 and 1.26 we start with the same initial conditions, and perform
    a fixed number of epochs with learning rates computed by multiplying the predicted
    learning rate by a predefined constant. Choosing constant 1 (i.e. using the predicted
    optimal rate) always gives residual errors which are very close to the error achieved
    by the best choice of the constant. In other words, the "predicted optimal rate"
    is optimal enough.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1.25和1.26中，我们从相同的初始条件开始，并进行固定数量的训练周期，学习率通过将预测学习率与预定义常数相乘来计算。选择常数1（即使用预测的最优率）总是会产生与最佳常数选择下的残差误差非常接近的结果。换句话说，“预测的最优率”是足够最优的。
- en: 1.10 Discussion And Conclusion
  id: totrans-593
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.10 讨论与结论
- en: 'According to the recommendations mentioned above, a practitioner facing a multi-layer
    neural net training problem would go through the following steps:'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述建议，面临多层神经网络训练问题的从业者将经历以下步骤：
- en: '- shuffle the examples - center the input variables by subtracting the mean
    - normalize the input variable to a standard deviation of 1 - if possible, decorrelate
    the input variables. - pick a network with the sigmoid function shown in figure
    1.4 - set the target values within the range of the sigmoid, typically +1 and
    -1. - initialize the weights to random values as prescribed by 1.16.'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '- 随机打乱示例 - 通过减去均值来中心化输入变量 - 将输入变量标准化到标准差为1 - 如果可能，去相关输入变量。 - 选择图1.4中所示的sigmoid函数的网络
    - 将目标值设定在sigmoid的范围内，通常为+1和-1。 - 将权重初始化为1.16所规定的随机值。'
- en: 'The preferred method for training the network should be picked as follows:'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 选择训练网络的首选方法如下：
- en: '- if the training set is large (more than a few hundred samples) and redundant,
    and if the task is classification, use stochastic gradient with careful tuning,
    or use the stochastic diagonal Levenberg Marquardt method.'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果训练集很大（超过几百个样本）且冗余，并且如果任务是分类，使用经过仔细调整的随机梯度，或使用随机对角Levenberg-Marquardt方法。'
- en: '- if the training set is not too large, or if the task is regression, use conjugate
    gradient.'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果训练集不是太大，或者任务是回归，使用共轭梯度法。'
- en: Classical second-order methods are impractical in almost all useful cases.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的二阶方法在几乎所有有用的情况下都是不切实际的。
- en: The non-linear dynamics of stochastic gradient descent in multi-layer neural
    networks, particularly as it pertains to generalization, is still far from being
    well understood. More theoretical work and systematic experimental work is needed.
    Acknowledgement. Y.L. & L.B. & K.-R. M. gratefully acknowledge mutual exchange
    grants from DAAD and NSF.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 多层神经网络中随机梯度下降的非线性动态，特别是其与泛化的关系，仍然远未被充分理解。需要更多的理论研究和系统的实验工作。致谢。Y.L.、L.B.和K.-R.
    M.感激DAAD和NSF的互换资助。
- en: '[1] Amari, S.: Neural learning in structured parameter spaces - natural riemannian
    gradient. In: Mozer, M.C., Jordan, M.I., Petsche, T. (eds.) Advances in Neural
    Information Processing Systems, vol. 9, p. 127. MIT Press (1997)'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Amari, S.: 结构参数空间中的神经学习——自然黎曼梯度。在：Mozer, M.C., Jordan, M.I., Petsche, T.（编）神经信息处理系统进展，第9卷，p.
    127. MIT出版社（1997）'
- en: '[2] Amari, S.: Natural gradient works efficiently in learning. Neural Computation
    10(2), 251–276 (1998)'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Amari, S.: 自然梯度在学习中高效工作。神经计算 10(2)，251–276（1998）'
- en: '[3] Battiti, R.: First- and second-order methods for learning: Between steepest
    descent and newton''s method. Neural Computation 4, 141–166 (1992)'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Battiti, R.: 学习的第一和第二阶方法：在最陡下降与牛顿方法之间。神经计算 4，141–166（1992）'
- en: '[4] Becker, S., LeCun, Y.: Improving the convergence of backbropagation learning
    with second oder metho ds. In: Touretzky, D., Hinton, G., Sejnowski, T. (eds.)
    Proceedings of the 1988 Connectionist Models Summer School, pp. 29–37. Lawrence
    Erlbaum Associates (1989)'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Becker, S., LeCun, Y.: 通过二阶方法改善反向传播学习的收敛性。在：Touretzky, D., Hinton, G.,
    Sejnowski, T.（编）1988年连接主义模型暑期学校论文集，pp. 29–37. Lawrence Erlbaum Associates（1989）'
- en: '[5] Bishop, C.M.: Neural Networks for Pattern Recognition. Clarendon Press,
    Oxford'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Bishop, C.M.: 模式识别的神经网络。Clarendon出版社，牛津'
- en: (1995)
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: (1995)
- en: '[6] Bottou, L.: Online algorithms and stochastic approximations. In: Saad,
    D. (ed.)'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Bottou, L.: 在线算法和随机近似。在：Saad, D.（编）'
- en: Online Learning in Neural Networks (1997 Workshop at the Newton Institute).
    The Newton Institute Series. Cambridge University Press, Cambridge (1998)
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习中的神经网络（1997年牛顿研究所研讨会）。牛顿研究所系列。剑桥大学出版社，剑桥（1998）
- en: '[7] Broomhead, D.S., Lowe, D.: Multivariable function interpolation and adaptive
    networks. Complex Systems 2, 321–355 (1988)'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Broomhead, D.S., Lowe, D.: 多变量函数插值和自适应网络。复杂系统 2, 321–355（1988）'
- en: '[8] Buntine, W.L., Weigend, A.S.: Computing second order derivatives in FeedForward
    networks: A review. IEEE Transactions on Neural Networks (1993) (to appear)'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Buntine, W.L., Weigend, A.S.: 在前馈网络中计算二阶导数：综述。IEEE神经网络学报（1993）（待发表）'
- en: '[9] Darken, C., Moody, J.E.: Note on learning rate schedules for stochastic
    optimization. In: Lippmann, R.P., Moody, J.E., Touretzky, D.S. (eds.) Advances
    in Neural Information Processing Systems, vol. 3, pp. 832–838. Morgan Kaufmann,
    San Mateo (1991)'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Darken, C., Moody, J.E.: 关于随机优化的学习速率调度的说明。在：Lippmann, R.P., Moody, J.E.,
    Touretzky, D.S.（编）神经信息处理系统进展，第3卷，页832–838。摩根·考夫曼，旧金山（1991）'
- en: '[10] Diamantaras, K.I., Kung, S.Y.: Principal Component Neural Networks. Wiley,
    New York (1996)'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Diamantaras, K.I., Kung, S.Y.: 主成分神经网络。威利，纽约（1996）'
- en: '[11] Fletcher, R.: Practical Methods of Optimization, ch. 8.7: Polynomial time
    algorithms, 2nd edn., pp. 183–188. John Wiley & Sons, New York (1987)'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Fletcher, R.: 优化的实用方法，第8.7章：多项式时间算法，第2版，页183–188。约翰·威利父子公司，纽约（1987）'
- en: '[12] Geman, S., Bienenstock, E., Doursat, R.: Neural networks and the bias/variance
    dilemma. Neural Computation 4(1), 1–58 (1992)'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Geman, S., Bienenstock, E., Doursat, R.: 神经网络与偏差/方差困境。神经计算 4(1), 1–58（1992）'
- en: '[13] Goldstein, L.: Mean square optimality in the continuous time Robbins Monro
    procedure. Technical Report DRB-306, Dept. of Mathematics, University of Southern
    California, LA (1987)'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Goldstein, L.: 连续时间Robbins Monro程序中的均方最优性。技术报告DRB-306，南加州大学数学系，洛杉矶（1987）'
- en: '[14] Golub, G.H., Van Loan, C.F.: Matrix Computations, 2nd edn. Johns Hopkins
    University Press, Baltimore (1989)'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Golub, G.H., Van Loan, C.F.: 矩阵计算，第2版。约翰·霍普金斯大学出版社，巴尔的摩（1989）'
- en: '[15] Heskes, T.M., Kappen, B.: On-line learning processes in artificial neural
    networks.'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Heskes, T.M., Kappen, B.: 人工神经网络中的在线学习过程。'
- en: 'In: Tayler, J.G. (ed.) Mathematical Approaches to Neural Networks, vol. 51,
    pp. 199–233. Elsevier, Amsterdam (1993)'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 在：Tayler, J.G.（编）神经网络的数学方法，第51卷，页199–233。爱思唯尔，阿姆斯特丹（1993）
- en: '[16] Jacobs, R.A.: Increased rates of convergence through learning rate adaptation.'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Jacobs, R.A.: 通过学习速率调整提高收敛速率。'
- en: Neural Networks 1, 295–307 (1988)
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络 1, 295–307（1988）
- en: '[17] Kramer, A.H., Sangiovanni-Vincentelli, A.: Efficient parallel learning
    algorithms for neural networks. In: Touretzky, D.S. (ed.) Proceedings of the 1988
    Conference on Advances in Neural Information Processing Systems, pp. 40–48. Morgan
    Kaufmann, San Mateo (1989)'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Kramer, A.H., Sangiovanni-Vincentelli, A.: 用于神经网络的高效并行学习算法。在：Touretsky,
    D.S.（编）1988年神经信息处理系统进展会议论文集，页40–48。摩根·考夫曼，旧金山（1989）'
- en: '[18] LeCun, Y.: Modeles connexionnistes de l''apprentissage (connectionist
    learning models). PhD thesis, Universitè P. et M. Curie, Paris VI (1987)'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] LeCun, Y.: 连接主义学习模型。博士论文，巴黎六大P. et M. Curie大学（1987）'
- en: '[19] LeCun, Y.: Generalization and network design strategies. In: Pfeifer,
    R., Schreter, Z., Fogelman, F., Steels, L. (eds.) Proceedings of the International
    Conference Connectionism in Perspective, University of Zürich, October 10-13.
    Elsevier, Amsterdam (1988)'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] LeCun, Y.: 泛化与网络设计策略。在：Pfeifer, R., Schreter, Z., Fogelman, F., Steels,
    L.（编）国际会议“连接主义视角”论文集，苏黎世大学，10月10-13日。爱思唯尔，阿姆斯特丹（1988）'
- en: '[20] LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard,
    W.,'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard,
    W.,'
- en: 'Jackel, L.D.: Handwritten digit recognition with a backpropagation network.
    In:'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 'Jackel, L.D.: 使用反向传播网络进行手写数字识别。在：'
- en: Touretsky, D.S. (ed.) Advances in Neural Information Processing Systems, vol.
    2. Morgan Kaufmann, San Mateo (1990)
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: Touretsky, D.S.（编）神经信息处理系统进展，第2卷。摩根·考夫曼，旧金山（1990）
- en: '[21] LeCun, Y., Denker, J.S., Solla, S.A.: Optimal brain damage. In: Touretsky,
    D.S.'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] LeCun, Y., Denker, J.S., Solla, S.A.: 最优脑损伤。在：Touretsky, D.S.'
- en: (ed.) Advances in Neural Information Processing Systems, vol. 2, pp. 598–605
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: （编）神经信息处理系统进展，第2卷，页598–605
- en: (1990)
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: （1990）
- en: '[22] LeCun, Y., Kanter, I., Solla, S.A.: Second order properties of error surfaces.
    In:'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] LeCun, Y., Kanter, I., Solla, S.A.: 错误曲面的二阶特性。在：'
- en: Advances in Neural Information Processing Systems, vol. 3. Morgan Kaufmann,
    San Mateo (1991)
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 神经信息处理系统进展，第3卷。摩根·考夫曼，旧金山（1991）
- en: '[23] LeCun, Y., Simard, P.Y., Pearlmutter, B.: Automatic learning rate maximization
    by on-line estimation of the hessian''s eigenvectors. In: Giles, Hanson, Cowan
    (eds.) Advances in Neural Information Processing Systems, vol. 5. Morgan Kaufmann,
    San Mateo (1993)'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] LeCun, Y., Simard, P.Y., Pearlmutter, B.: 通过在线估计海森矩阵的特征向量自动最大化学习率。编者:
    Giles, Hanson, Cowan《神经信息处理系统进展》，第 5 卷。摩根·考夫曼出版社，圣马特奥 (1993)'
- en: '[24] Møller, M.: A scaled conjugate gradient algorithm for fast supervised
    learning.'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Møller, M.: 一种用于快速监督学习的缩放共轭梯度算法。'
- en: Neural Networks 6, 525–533 (1993)
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络 6, 525–533 (1993)
- en: '[25] Møller, M.: Supervised learning on large redundant training sets. International
    Journal of Neural Systems 4(1), 15–25 (1993)'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Møller, M.: 在大型冗余训练集上进行监督学习。国际神经系统期刊 4(1), 15–25 (1993)'
- en: '[26] Moody, J.E., Darken, C.J.: Fast learning in networks of locally-tuned
    processing units. Neural Computation 1, 281–294 (1989)'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Moody, J.E., Darken, C.J.: 快速学习局部调谐处理单元网络。神经计算 1, 281–294 (1989)'
- en: '[27] Murata, N.: PhD thesis, University of Tokyo (1992) (in Japanese) [28]
    Murata, N., Müller, K.-R., Ziehe, A., Amari, S.: Adaptive on-line learning in
    changing environments. In: Mozer, M.C., Jordan, M.I., Petsche, T. (eds.) Advances
    in Neural Information Processing Systems, vol. 9, p. 599. MIT Press (1997)'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Murata, N.: 博士论文，东京大学 (1992) (日语) [28] Murata, N., Müller, K.-R., Ziehe,
    A., Amari, S.: 在变化环境中的自适应在线学习。编者: Mozer, M.C., Jordan, M.I., Petsche, T.《神经信息处理系统进展》，第
    9 卷, 第 599 页。麻省理工学院出版社 (1997)'
- en: '[29] Oppenheim, A.V., Schafer, R.W.: Digital Signal Processing. Prentice-Hall,
    Englewood Cliffs (1975)'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] Oppenheim, A.V., Schafer, R.W.: 数字信号处理。普伦蒂斯霍尔，恩格尔伍德悬崖 (1975)'
- en: '[30] Orr, G.B.: Dynamics and Algorithms for Stochastic learning. PhD thesis,
    Oregon Graduate Institute (1995)'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] Orr, G.B.: 随机学习的动态与算法。博士论文，俄勒冈州研究所 (1995)'
- en: '[31] Orr, G.B.: Removing noise in on-line search using adaptive batch sizes.
    In: Mozer, M.C., Jordan, M.I., Petsche, T. (eds.) Advances in Neural Information
    Processing Systems, vol. 9, p. 232. MIT Press (1997)'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Orr, G.B.: 使用自适应批量大小在线搜索去噪。编者: Mozer, M.C., Jordan, M.I., Petsche, T.《神经信息处理系统进展》，第
    9 卷, 第 232 页。麻省理工学院出版社 (1997)'
- en: '[32] Orr, M.J.L.: Regularization in the selection of radial basis function
    centers. Neural Computation 7(3), 606–623 (1995)'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] Orr, M.J.L.: 在选择径向基函数中心时的正则化。神经计算 7(3), 606–623 (1995)'
- en: '[33] Pearlmutter, B.A.: Fast exact multiplication by the hessian. Neural Computation
    6, 147–160 (1994)'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '[33] Pearlmutter, B.A.: 通过海森矩阵进行快速精确乘法。神经计算 6, 147–160 (1994)'
- en: '[34] Press, W.H., Flannery, B.P., Teukolsky, S.A., Vetterling, W.T.: Numerical
    Recipies in C: The art of Scientific Programming. Cambridge University Press,
    Cambridge (1988)'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '[34] Press, W.H., Flannery, B.P., Teukolsky, S.A., Vetterling, W.T.: C语言数值计算：科学编程的艺术。剑桥大学出版社，剑桥
    (1988)'
- en: '[35] Saad, D. (ed.): Online Learning in Neural Networks (1997 Workshop at the
    Newton Institute). The Newton Institute Series. Cambridge University Press, Cambridge
    (1998)'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: '[35] Saad, D. (编者): 神经网络中的在线学习（1997年牛顿研究所研讨会）。牛顿研究所系列。剑桥大学出版社，剑桥 (1998)'
- en: '[36] Saad, D., Solla, S.A.: Exact solution for on-line learning in multilayer
    neural networks. Physical Review Letters 74, 4337–4340 (1995) [37] Sompolinsky,
    H., Barkai, N., Seung, H.S.: On-line learning of dichotomies: algorithms and learning
    curves. In: Oh, J.-H., Kwon, C., Cho, S. (eds.) Neural Networks: The Statistical
    Mechanics Perspective, pp. 105–130. World Scientific, Singapore (1995)'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '[36] Saad, D., Solla, S.A.: 多层神经网络在线学习的精确解。物理评论快报 74, 4337–4340 (1995) [37]
    Sompolinsky, H., Barkai, N., Seung, H.S.: 二分法的在线学习：算法与学习曲线。编者: Oh, J.-H., Kwon,
    C., Cho, S.《神经网络：统计力学视角》，第 105–130 页。世界科学出版社，新加坡 (1995)'
- en: '[38] Sutton, R.S.: Adapting bias by gradient descent: An incremental version
    of deltabar-delta. In: Swartout, W. (ed.) Proceedings of the 10th National Conference
    on Artificial Intelligence, pp. 171–176. MIT Press, San Jose (July 1992)'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '[38] Sutton, R.S.: 通过梯度下降调整偏差：一种增量版本的deltabar-delta。编者: Swartout, W.《第十届全国人工智能会议论文集》，第
    171–176 页。麻省理工学院出版社，圣荷西 (1992年7月)'
- en: '[39] van der Smagt, P.: Minimisation methods for training feed-forward networks.
    Neural Networks 7(1), 1–11 (1994)'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '[39] van der Smagt, P.: 训练前馈网络的最小化方法。神经网络 7(1), 1–11 (1994)'
- en: '[40] Vapnik, V.: The Nature of Statistical Learning Theory. Springer, New York
    (1995)'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: '[40] Vapnik, V.: 统计学习理论的本质。施普林格出版社，纽约 (1995)'
- en: '[41] Vapnik, V.: Statistical Learning Theory. Wiley, New York (1998)'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: '[41] Vapnik, V.: 统计学习理论。威利出版社，纽约 (1998)'
- en: '[42] Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., Lang, K.J.: Phoneme
    recognition using time-delay neural networks. IEEE Transactions on Acoustics,
    Speech, and Signal Processing ASSP-37, 328–339 (1989)'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: '[42] Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., Lang, K.J.: 使用时延神经网络的音素识别。IEEE声学、语音和信号处理交易
    ASSP-37, 328–339 (1989)'
- en: '[43] Wiegerinck, W., Komoda, A., Heskes, T.: Stochastic dynamics of learning
    with momentum in neural networks. Journal of Physics A 27, 4425–4437 (1994)'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: '[43] Wiegerinck, W., Komoda, A., Heskes, T.: 使用动量的学习随机动力学在神经网络中。物理学A杂志 27,
    4425–4437 (1994)'
- en: '[44] Yang, H.H., Amari, S.: The efficiency and the robustness of natural gradient
    descent learning rule. In: Jordan, M.I., Kearns, M.J., Solla, S.A. (eds.) Advances
    in Neural Information Processing Systems, vol. 10. MIT Press (1998)'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: '[44] Yang, H.H., Amari, S.: 自然梯度下降学习规则的效率和鲁棒性。在：Jordan, M.I., Kearns, M.J.,
    Solla, S.A.（编辑）《神经信息处理系统进展》，第10卷。麻省理工学院出版社（1998）'
- en: Regularization Techniques To Improve Generalization-
  id: totrans-653
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改善泛化的正则化技术-
- en: Preface
  id: totrans-654
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: 'Good tricks for regularization are extremely important for improving the generalization
    ability of neural networks. The first and most commonly used trick is early stopping,
    which was originally described in [11]. In its simplest version, the trick is
    as follows:'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 对于提高神经网络的泛化能力，良好的正则化技巧是极其重要的。第一个也是最常用的技巧是早期停止，这最初在[11]中描述。在其最简单的版本中，技巧如下：
- en: Take an independent validation set, e.g. take out a part of the training set,
    and monitor the error on this set during training. The error on the training set
    will decrease, whereas the error on the validation set will first decrease and
    then increase. The early stopping point occurs where the error on the validation
    set is the lowest. It is here that the network weights provide the best generalization.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 取一个独立的验证集，例如从训练集中取出一部分，并在训练期间监控该集的错误。训练集上的错误会减少，而验证集上的错误会先减少后增加。早期停止点发生在验证集的错误最低处。在这里，网络权重提供最佳泛化。
- en: As Lutz Prechelt points out in chapter 2, the above picture is highly idealized.
    In practice, the shape of the error curve on the validation set is more likely
    very ragged with multiple minima. Choosing the "best" early stopping point then
    involves a trade-off between (1) improvement of generalization and (2) speed of
    learning. If speed is not an issue then, clearly, the safest strategy is to train
    all the way until the minimum error on the training set is found, while monitoring
    the location of the lowest error rate on the validation set. Of course, this can
    take a prohibitive amount of computing time. This chapter presents less costly
    strategies employing a number of different stopping criteria, e.g. when the ratio
    between the *generalization loss* and the *progress* exceeds a given threshold
    (see p. 57). A large simulation study using various benchmark problems is used
    in the discussion and analysis of the differences (with respect to e.g. robustness,
    effectiveness, training time, . . . ) between these proposed stopping criteria
    (see p. 60ff.). So far theoretical studies [12, 1, 6] have not studied this trade-off.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 正如Lutz Prechelt在第2章指出的，上述图像是高度理想化的。实际上，验证集上的错误曲线形状更可能非常不规则，具有多个极小值。选择“最佳”早期停止点涉及（1）泛化改进与（2）学习速度之间的权衡。如果速度不是问题，那么显然，最安全的策略是训练直到找到训练集上的最小错误，同时监控验证集上最低错误率的位置。当然，这可能需要大量的计算时间。本章提出了一些成本较低的策略，采用多种不同的停止标准，例如当*泛化损失*与*进展*的比率超过给定阈值时（见第57页）。使用各种基准问题的大规模模拟研究用于讨论和分析这些提议的停止标准之间的差异（例如鲁棒性、有效性、训练时间等）（见第60页及后）。到目前为止，理论研究[12,
    1, 6]尚未研究这种权衡。
- en: Weight decay is also a commonly used technique for controlling capacity in neural
    networks. Early stopping is considered to be fast, but it is not well defined
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 权重衰减也是一种常用的控制神经网络能力的技术。早期停止被认为是快速的，但定义不够明确。
- en: (keep in mind the pitfalls mentioned in chapter 2). On the other hand, weight
    decay regularizers [5, 2] are well understood, but finding a suitable parameter
    λ to control the strength of the weight decay term can be tediously time consuming.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: （请记住第2章提到的陷阱）。另一方面，权重衰减正则化器[5, 2]被很好地理解，但找到合适的参数λ来控制权重衰减项的强度可能非常耗时。
- en: 'Thorsteinn Rögnvaldsson proposes a simple trick for estimating λ by making
    use of the *best of both worlds* (see p. 75): simply compute the gradient at the
    early stopping solution Wes and divide it by the norm of Wes, λˆ = ∇E(Wes)/2Wes.'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: Thorsteinn Rögnvaldsson提出了一种简单的技巧，通过利用*两全其美*（见第75页）来估计λ：简单地计算早期停止解Wes的梯度并将其除以Wes的范数，λˆ
    = ∇E(Wes)/2Wes。
- en: Other penalties are also possible. The trick is speedy, since we neither have
    to do a complete training nor a scan of the whole λ parameter space, and the accuracy
    of the determined λˆ is good, as seen from some interesting simulations.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 其他惩罚也可能存在。这个技巧快速有效，因为我们既不需要进行完整的训练，也不需要扫描整个λ参数空间，而且从一些有趣的模拟结果来看，确定的λˆ的准确性良好。
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: '- 先前发表在：Orr, G.B. 和 Müller, K.-R.（编）：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0 (1998)。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    49–51, 2012.'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon等人（编）：NN: Tricks of the Trade, 第2版，LNCS 7700，页49–51，2012年。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: 'Tony Plate in chapter 4 treats the penalty factors for the weights (hyperparameters)
    along the Bayesian framework of MacKay [8] and Neal [9]. There are two levels
    in searching for the best network. The inner loop is a minimization of the training
    error keeping the hyperparameters fixed, whereas the outer loop searches the hyperparameter
    space with the goal of maximizing the evidence of having generated the data. This
    whole procedure is rather slow and computationally expensive, since, in theory,
    the inner search needs to converge (to a local minimum) at each outer loop search
    step. When applied to classification networks using the cross-entropy error function
    the outer-loop search can be unstable with the hyperparameter values oscillating
    wildly or going to inappropriate extremes. To make this Bayesian framework work
    better in practice, Tony Plate proposes a number of tricks that speed and simplify
    the hyperparameter search strategies (see p. 96). In particular, his search strategies
    center around the questions: (1) how often (when) should the hyperparameters be
    updated (see p. 96) and (2) what should be done if the Hessian is out-of-bounds
    (see p. 97ff.). To discuss the effects of the choices made in (1) and (2), Tony
    Plate uses simulations based on artificial examples and concludes with a concise
    set of rules for making the hyperparameter framework work better.'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: Tony Plate在第四章中沿着MacKay [8]和Neal [9]的贝叶斯框架处理权重的惩罚因子（超参数）。寻找最佳网络有两个层次。内部循环是固定超参数的情况下最小化训练误差，而外部循环则搜索超参数空间，以最大化生成数据的证据。整个过程相对缓慢且计算开销大，因为理论上，内部搜索在每个外部循环搜索步骤中都需要收敛（到局部最小值）。当应用于使用交叉熵误差函数的分类网络时，外部循环搜索可能不稳定，超参数值可能剧烈波动或达到不当的极端。为了使这个贝叶斯框架在实践中更好地运作，Tony
    Plate提出了一些加速和简化超参数搜索策略的技巧（见第96页）。特别是，他的搜索策略围绕以下问题展开：(1) 超参数应该多频繁（何时）更新（见第96页）和(2)
    如果Hessian超出界限该怎么办（见第97页及后续）。为讨论在(1)和(2)中所作选择的影响，Tony Plate使用基于人工示例的模拟，并总结出一套简明的规则，以便使超参数框架更好地工作。
- en: 'In chapter 5, Jan Larsen et al. formulate an iterative gradient descent scheme
    for adapting their regularization parameters (note, different regularizers can
    be used for input/hidden and hidden/output weights). The trick is simple:'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 在第五章中，Jan Larsen等人制定了一个迭代梯度下降方案，用于调整他们的正则化参数（注意，不同的正则化器可以用于输入/隐藏和隐藏/输出权重）。这个技巧很简单：
- en: perform gradient descent on the validation set errors with respect to the regularization
    parameters, and iteratively use the results for updating the estimate of the regularization
    parameters (see p. 116). This method holds for a variety of penalty terms (e.g.
    weight decay). The computational overhead is negligible for computing the gradients,
    however, an inverse Hessian has to be estimated. If second order methods are used
    for training, then the inverse Hessian may already be available, so there is little
    additional effort. Otherwise obtaining full Hessian information is rather tedious
    and limits the approach to smaller applications (see discussion in chapter 1).
    Nevertheless approximations of the Hessian (e.g.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证集误差上执行梯度下降，以调整正则化参数，并迭代地使用结果更新正则化参数的估计（参见第116页）。此方法适用于多种惩罚项（例如权重衰减）。计算梯度的计算开销可以忽略不计，但必须估计逆海森矩阵。如果使用二阶方法进行训练，则逆海森矩阵可能已经可用，因此额外的工作量很少。否则，获取完整的海森信息相当繁琐，限制了该方法在较小应用中的使用（参见第1章的讨论）。尽管如此，海森的近似值（例如。
- en: diagonal) could also be used to limit the computation time. Jan Larsen, et al.,
    demonstrate the applicability of their trick on classification (vowel data) and
    regression (time-series prediction) problems.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 对角线）也可以用来限制计算时间。Jan Larsen等人展示了他们的技巧在分类（元音数据）和回归（时间序列预测）问题上的适用性。
- en: 'Averaging over multiple predictors is a well known method for improving generalization
    (see e.g. [10, 3, 7, 13]). David Horn et al. raises two questions in ensemble
    training: (1) how many predictors are "enough" and (2) how does the number of
    predictors affect the stopping criteria for early stopping (see p. 134). They
    present solutions for answering these questions by providing a method for estimating
    the error of an infinite number of predictors and they demonstrate the usefulness
    of their trick for the sunspot prediction task. Additional theoretical reasoning
    is given to explain their success in terms of variance minimization within the
    ensemble.'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 对多个预测器进行平均是一种众所周知的提高泛化能力的方法（参见例如[10, 3, 7, 13]）。David Horn等人在集成训练中提出了两个问题：（1）多少个预测器是“足够的”，以及（2）预测器的数量如何影响早停的停止标准（参见第134页）。他们通过提供一种估计无限多个预测器误差的方法，提出了回答这些问题的解决方案，并展示了他们的技巧在太阳黑子预测任务中的实用性。还给出了额外的理论推理，以解释他们在集成中通过最小化方差而取得的成功。
- en: Jenny & Klaus
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 珍妮与克劳斯
- en: '[1] Amari, S., Murata, N., Müller, K.-R., Finke, M., Yang, H.H.: Asymptotic
    statistical theory of overtraining and cross-validation. IEEE Transactions on
    Neural Networks 8(5), 985–996 (1997)'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Amari, S., Murata, N., Müller, K.-R., Finke, M., Yang, H.H.：过拟合与交叉验证的渐近统计理论。IEEE神经网络交易
    8(5), 985–996 (1997)'
- en: '[2] Bishop, C.M.: Neural Networks for Pattern Recognition. Clarendon Press,
    Oxford'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Bishop, C.M.：模式识别的神经网络。牛津大学出版社'
- en: (1995)
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: （1995）
- en: '[3] Breiman, L.: Bagging predictors. Machine Learning 26(2), 123–140 (1996)
    [4] Cowan, J.D., Tesauro, G., Alspector, J. (eds.): Advances in Neural Information
    Processing Systems 6, San Mateo, CA. Morgan Kaufman Publishers Inc. (1994)'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Breiman, L.：袋装预测器。机器学习 26(2), 123–140 (1996) [4] Cowan, J.D., Tesauro,
    G., Alspector, J.（主编）：神经信息处理系统进展 6，圣马特奥，加利福尼亚。摩根·考夫曼出版社（1994）'
- en: '[5] Girosi, F., Jones, M., Poggio, T.: Regularization theory and neural networks
    architectures. Neural Computation 7(2), 219–269 (1995)'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Girosi, F., Jones, M., Poggio, T.：正则化理论与神经网络架构。神经计算 7(2), 219–269 (1995)'
- en: '[6] Kearns, M.: A bound on the error of cross validation using the approximation
    and estimation rates, with consequences for the training-test split. Neural Computation
    9(5), 1143–1161 (1997)'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Kearns, M.：使用近似和估计速率的交叉验证误差界限，以及对训练测试划分的影响。神经计算 9(5), 1143–1161 (1997)'
- en: '[7] Lincoln, W.P., Skrzypek, J.: Synergy of clustering multiple back propagation
    networks. In: Touretzky, D.S. (ed.) Advances in Neural Information Processing
    Systems 2, San Mateo, CA, pp. 650–657. Morgan Kaufmann (1990)'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Lincoln, W.P., Skrzypek, J.：多重反向传播网络的聚类协同。在：Touretzky, D.S.（编）神经信息处理系统进展
    2，圣马特奥，加利福尼亚，第650–657页。摩根·考夫曼（1990）'
- en: '[8] McKay, D.J.C.: A practical Bayesian framework for backpropagation networks.'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] McKay, D.J.C.：一种实用的贝叶斯框架用于反向传播网络。'
- en: Neural Computation 4, 448–472 (1992)
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 神经计算 4, 448–472 (1992)
- en: '[9] Neal, R.M.: Bayesian Learning for Neural Networks. Lecture Notes in Statistics,
    vol. 118. Springer, New York (1996)'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Neal, R.M.：神经网络的贝叶斯学习。统计学讲义，卷118。施普林格出版社，纽约（1996）'
- en: '[10] Perrone, M.P.: Improving Regression Estimation: Averaging Methods for
    Variance Reduction with Extensions to General Convex Measure Optimization. PhD
    thesis, Brown University (May 1993)'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Perrone, M.P.：改善回归估计：用于方差减少的平均方法及其在一般凸度量优化中的扩展。博士论文，布朗大学（1993年5月）'
- en: '[11] Plaut, D.C., Nowlan, S.J., Hinton, G.E.: Experiments on learning by backpropagation.
    Technical Report Computer Science Dept. Tech. Report, Pittsburgh, PA (1986)'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Plaut, D.C., Nowlan, S.J., Hinton, G.E.：反向传播学习的实验。技术报告，计算机科学系，技术报告，匹兹堡，PA（1986年）'
- en: '[12] Wang, C., Venkatesh, S.S., Judd, J.S.: Optimal stopping and effective
    machine complexity in learning. In: [4] (1994)'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Wang, C., Venkatesh, S.S., Judd, J.S.：学习中的最优停止和有效机器复杂性。在：[4]（1994年）'
- en: '[13] Wolpert, D.H.: Stacked generalization. Neural Networks 5(2), 241–259 (1992)'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Wolpert, D.H.：堆叠泛化。神经网络 5(2)，241–259（1992年）'
- en: 2 Early Stopping - But When?∗
  id: totrans-686
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 提前停止 - 但何时？∗
- en: Lutz Prechelt Fakultät für Informatik, Universität Karlsruhe D-76128 Karlsruhe,
    Germany prechelt@ira.uka.de http://www.ipd.ira.uka.de/˜prechelt/
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: Lutz Prechelt，卡尔斯鲁厄大学计算机科学系，德国卡尔斯鲁厄 D-76128 prechelt@ira.uka.de http://www.ipd.ira.uka.de/˜prechelt/
- en: 'Abstract. Validation can be used to detect when overfitting starts during supervised
    training of a neural network; training is then stopped before convergence to avoid
    the overfitting ("early stopping"). The exact criterion used for validation-based
    early stopping, however, is usually chosen in an ad-hoc fashion or training is
    stopped interactively. This trick describes how to select a stopping criterion
    in a systematic fashion; it is a trick for either speeding learning procedures
    or improving generalization, whichever is more important in the particular situation.
    An empirical investigation on multi-layer perceptrons shows that there exists
    a tradeoff between training time and generalization: From the given mix of 1296
    training runs using different 12 problems and 24 different network architectures
    I conclude slower stopping criteria allow for small improvements in generalization
    (here: about 4% on average), but cost much more training time (here: about factor
    4 longer on average).'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要。验证可以用来检测在监督训练神经网络时何时开始过拟合；然后在收敛之前停止训练，以避免过拟合（“提前停止”）。然而，基于验证的提前停止所使用的确切标准通常是以临时方式选择的，或者训练是交互式停止的。这一技巧描述了如何以系统的方式选择停止标准；这是一种加速学习过程或改善泛化的技巧，具体情况取决于哪种更为重要。对多层感知器的实证研究表明，训练时间和泛化之间存在权衡：从1296次训练运行的给定组合中，使用不同的12个问题和24种不同的网络架构，我得出结论，较慢的停止标准允许泛化有小幅改进（此处：平均约4%），但需要花费更多的训练时间（此处：平均大约长4倍）。
- en: 2.1 Early Stopping Is Not Quite As Simple 2.1.1 Why Early Stopping?
  id: totrans-689
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 提前停止并不那么简单 2.1.1 为什么提前停止？
- en: 'When training a neural network, one is usually interested in obtaining a network
    with optimal generalization performance. However, all standard neural network
    architectures such as the fully connected multi-layer perceptron are prone to
    overfitting [10]: While the network *seems* to get better and better, i.e., the
    error on the training set decreases, at some point during training it actually
    begins to get worse again, i.e., the error on unseen examples increases. The idealized
    expectation is that during training the generalization error of the network evolves
    as shown in Figure 2.1. Typically the generalization error is estimated by a validation
    error, i.e., the average error on a *validation set*, a fixed set of examples
    not from the training set.'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，通常希望获得具有最佳泛化性能的网络。然而，所有标准的神经网络架构，如全连接的多层感知器，都容易过拟合[10]：虽然网络*看起来*越来越好，即训练集上的误差减少，但在训练的某个时刻，它实际上又开始变差，即在未见示例上的误差增加。理想的期望是，在训练过程中，网络的泛化误差如图2.1所示演变。通常，通过验证误差来估计泛化误差，即在*验证集*上的平均误差，该集是一个固定的示例集合，不来自训练集。
- en: 'There are basically two ways to fight overfitting: reducing the number of dimensions
    of the parameter space or reducing the effective size of each dimension.'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上有两种方法来对抗过拟合：减少参数空间的维度数量或减少每个维度的有效大小。
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '- 先前发表在：Orr, G.B. 和 Müller, K.-R. (编)：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0（1998年）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    53–67, 2012.'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon 等（编）：NN：行业技巧，第二版，LNCS 7700，第53–67页，2012年。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag 柏林海德堡 2012年
- en: Techniques for reducing the number of parameters are greedy constructive learning
    [7], pruning [5, 12, 14], or weight sharing [18]. Techniques for reducing the
    size of each parameter dimension are regularization, such as weight decay [13]
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 减少参数数量的技术包括贪婪构造学习[7]、剪枝[5, 12, 14]或权重共享[18]。减少每个参数维度大小的技术是正则化，例如权重衰减[13]。
- en: and others [25], or early stopping [17]. See also [8, 20] for an overview and
    [9] for an experimental comparison.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 以及其他[25]，或早停[17]。另见[8, 20]以获取概述，和[9]以获取实验比较。
- en: Early stopping is widely used because it is simple to understand and implement
    and has been reported to be superior to regularization methods in many cases,
    e.g. in [9].
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 早停法被广泛使用，因为它简单易懂且易于实现，并且在许多情况下被报告优于正则化方法，例如在[9]中。
- en: 2.1.2 The Basic Early Stopping Technique
  id: totrans-699
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1.2 基本早停技术
- en: 'In most introductory papers on supervised neural network training one can find
    a diagram similar to the one shown in Figure 2.1. It is claimed to show the evolution
    over time of the per-example error on the training set and on a validation set
    not used for training (the *training error curve* and the *validation error curve*).
    Given this behavior, it is clear how to do early stopping using validation:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数关于监督神经网络训练的入门论文中，可以找到类似于图2.1所示的图表。它声称展示了训练集和未用于训练的验证集的每个样本错误随时间的演变（*训练错误曲线*和*验证错误曲线*）。鉴于这种行为，很明显如何使用验证进行早停：
- en: '![60_image_0.png](60_image_0.png)'
  id: totrans-701
  prefs: []
  type: TYPE_IMG
  zh: '![60_image_0.png](60_image_0.png)'
- en: 'Fig. 2.1. Idealized training and validation error curves. Vertical: errors;
    horizontal:'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 理想化的训练和验证错误曲线。垂直：错误；水平：
- en: time.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 时间。
- en: 1. Split the training data into a training set and a validation set, e.g. in
    a 2-to-1 proportion.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 将训练数据分成训练集和验证集，例如以2比1的比例。
- en: 2. Train only on the training set and evaluate the per-example error on the
    validation set once in a while, e.g. after every fifth epoch.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 仅在训练集上训练，并偶尔评估验证集上的每个样本错误，例如每五个周期后。
- en: 3. Stop training as soon as the error on the validation set is higher than it
    was the last time it was checked.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 一旦验证集上的错误高于上次检查时的错误，立即停止训练。
- en: 4. Use the weights the network had in that previous step as the result of the
    training run.
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 将网络在上一步中的权重作为训练运行的结果。
- en: 'This approach uses the validation set to anticipate the behavior in real use
    (or on a test set), assuming that the error on both will be similar: The validation
    error is used as an estimate of the generalization error.'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法使用验证集来预测实际使用（或测试集）中的行为，假设两者的错误相似：验证错误被用作泛化错误的估计。
- en: 2.1.3 The Uglyness Of Reality
  id: totrans-709
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1.3 现实的丑陋
- en: However, for real neural network training the validation set error does not
    evolve as smoothly as shown in Figure 2.1, but looks more like in Figure 2.2.
    See Section 2.4 for a rough explanation of this behavior. As we see, the validation
    error can still go further down after it has begun to increase - plus in a realistic
    setting we do never know the exact generalization error but estimate it by the
    validation set error instead. There is no obvious rule for deciding when the minimum
    of the generalization error is obtained. Real validation error curves almost always
    have more than one local minimum. The above curve exhibits as many as 16 local
    minima before severe overfitting begins at about epoch 400. Of these local minima,
    4 are the global minimum up to where they occur. The optimal stopping point in
    this example would be epoch 205. Note that stopping in epoch 400 compared to stopping
    shortly after the first "deep" local minimum at epoch 45 trades an about sevenfold
    increase of learning time for an improvement of validation set error by 1.1% (by
    finding the minimum at epoch 205). If representative data is used, the validation
    error is an unbiased estimate of the actual network performance; so we expect
    a 1.1% decrease of the generalization error in this case. Nevertheless, overfitting
    might sometimes go undetected because the validation set is finite and thus not
    perfectly representative of the problem.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于真实的神经网络训练，验证集错误并不会像图2.1所示那样平滑演变，而更像图2.2所示。有关这种行为的粗略解释请见第2.4节。如我们所见，验证错误在开始增加后仍然可能进一步降低——而且在现实环境中，我们永远无法知道确切的泛化错误，而是通过验证集错误来估计。没有明显的规则可以决定何时达到泛化错误的最小值。真实的验证错误曲线几乎总是有多个局部最小值。上述曲线在大约第400个周期开始严重过拟合之前展现了多达16个局部最小值。在这些局部最小值中，有4个是它们出现时的全局最小值。在这个例子中，最佳的停止点是第205个周期。请注意，与在第45个周期的第一个“深”局部最小值后不久停止相比，在第400个周期停止将学习时间增加约七倍，以换取验证集错误改善1.1%（通过在第205个周期找到最小值）。如果使用代表性数据，验证错误是对实际网络性能的无偏估计；因此，我们期望在这种情况下泛化错误减少1.1%。然而，过拟合有时可能会被忽视，因为验证集是有限的，因此并不完美代表问题。
- en: Unfortunately, the above or any other validation error curve is not *typical*
    in the sense that all curves share the same qualitative behavior. Other curves
    might never reach a better minimum than the first, or than, say, the third; the
    mountains and valleys in the curve can be of very different width, height, and
    shape. The only thing all curves seem to have in common is that the differences
    between the first and the following local minima are not huge.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，上述或任何其他验证错误曲线并不是在所有曲线具有相同定性行为的意义上*典型*。其他曲线可能永远无法达到比第一条更好的最小值，或者说第三条的最小值；曲线中的山脉和谷底可能具有非常不同的宽度、高度和形状。所有曲线似乎唯一共同点是，第一局部最小值和后续局部最小值之间的差异并不大。
- en: As we see, choosing a stopping criterion predominantly involves a tradeoff between
    training time and generalization error. However, some stopping criteria may typically
    find better tradeoffs that others. This leads to the question of
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，选择停止准则主要涉及训练时间和泛化错误之间的权衡。然而，一些停止准则通常可能找到比其他准则更好的权衡。这就引出了一个问题：
- en: '![61_image_0.png](61_image_0.png)'
  id: totrans-713
  prefs: []
  type: TYPE_IMG
  zh: '![61_image_0.png](61_image_0.png)'
- en: 'Fig. 2.2. A real validation error curve. Vertical: validation set error; horizontal:
    time (in training epochs).'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2. 一个真实的验证错误曲线。纵轴：验证集错误；横轴：时间（以训练周期计）。
- en: which criterion to use with cross validation to decide when to stop training.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 应该使用哪种准则进行交叉验证，以决定何时停止训练。
- en: 'This is why we need the present trick: To tell us how to *really* do early
    stopping.'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要当前技巧的原因：告诉我们如何*真正*进行提前停止。
- en: 2.2 How To Do Early Stopping Best
  id: totrans-717
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 如何最好地进行提前停止
- en: What we need is a predicate that tells us when to stop training. We call such
    a predicate a *stopping criterion*. Among all possible stopping criteria we are
    searching for those which yield the lowest generalization error and also for those
    with the best "price-performance ratio", i.e., that require the least training
    for a given generalization error or that (on average) result in the lowest generalization
    error for a certain training time.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的是一个谓词，告诉我们何时停止训练。我们称这种谓词为*停止准则*。在所有可能的停止准则中，我们寻找那些能产生最低泛化错误的准则，同时也寻找那些具有最佳“性价比”的准则，即，对于给定的泛化错误，所需的训练最少，或者在某一训练时间内（平均）导致最低的泛化错误。
- en: 2.2.1 Some Classes Of Stopping Criteria
  id: totrans-719
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2.1 一些停止准则的类别
- en: 'There are a number of plausible stopping criteria and this work considers three
    classes of them. To formally describe the criteria, we need some definitions first.
    Let E be the objective function (error function) of the training algorithm, for
    example the squared error. Then Etr(t), the training set error (for short: training
    error), is the average error per example over the training set, measured after
    epoch t. Eva(t), the validation error, is the corresponding error on the validation
    set and is used by the stopping criterion. Ete(t), the test error, is the corresponding
    error on the test set; it is not known to the training algorithm but estimates
    the generalization error and thus benchmarks the quality of the network resulting
    from training. In real life, the generalization error is usually unknown and only
    the validation error can be used to estimate it.'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种合理的停止标准，本研究考虑了三类标准。要正式描述这些标准，我们首先需要一些定义。设E为训练算法的目标函数（误差函数），例如平方误差。然后，Etr(t)为训练集误差（简称：训练误差），是经过第t个周期后训练集上每个样本的平均误差。Eva(t)为验证误差，是验证集上的对应误差，并由停止标准使用。Ete(t)为测试误差，是测试集上的对应误差；它对训练算法是未知的，但可用于估计泛化误差，从而基准评估训练后网络的质量。在现实中，泛化误差通常是未知的，只有验证误差可以用来估计。
- en: 'The value Eopt(t) is defined to be the lowest validation set error obtained
    in epochs up to t:'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 值Eopt(t)被定义为在周期t之前获得的最低验证集误差：
- en: $$E_{o p t}(t):=\operatorname*{min}_{t^{\prime}\leq t}E_{v a}(t^{\prime})$$
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: $$E_{o p t}(t):=\operatorname*{min}_{t^{\prime}\leq t}E_{v a}(t^{\prime})$$
- en: 'Now we define the *generalization* at epoch t to be the relative increase of
    the validation error over the minimum-so-far (in percent):'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义第t个周期的*泛化*为验证误差相对于迄今为止的最小值的相对增加（以百分比表示）：
- en: $$G L(t)=100\cdot\left({\frac{E_{v a}(t)}{E_{o p t}(t)}}-1\right)$$
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: $$G L(t)=100\cdot\left({\frac{E_{v a}(t)}{E_{o p t}(t)}}-1\right)$$
- en: 'High generalization loss is one obvious candidate reason to stop training,
    because it directly indicates overfitting. This leads us to the first class of
    stopping criteria: stop as soon as the generalization loss exceeds a certain threshold.
    We define the class GLα as GLα : stop after first epoch t with GL(t) > α However,
    we might want to suppress stopping if the training is still progressing very rapidly.
    The reasoning behind this approach is that when the training error still decreases
    quickly, generalization losses have higher chance to be "repaired"; we assume
    that often overfitting does not begin until the error decreases only slowly. To
    formalize this notion we define a *training strip of length k* to be a sequence
    of k epochs numbered n + 1 *...n* + k where n is divisible by k. The training
    *progress* (in per thousand) measured after such a training strip is then'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 高泛化损失是停止训练的一个明显候选原因，因为它直接指示过拟合。这引导我们到第一类停止标准：一旦泛化损失超过某个阈值就停止。我们将类GLα定义为GLα：在第一个周期t后停止，满足GL(t)
    > α。然而，如果训练仍然快速进展，我们可能想抑制停止。这一方法背后的推理是，当训练误差仍然迅速下降时，泛化损失有更高的机会被“修复”；我们假设，通常在误差仅缓慢下降之前，过拟合并不会开始。为正式化这一概念，我们定义*长度为k的训练阶段*为一系列k个周期，编号为n
    + 1 *...n* + k，其中n可被k整除。此类训练*进展*（以千分之一计）在这样的训练阶段后测量为：
- en: $$P_{k}(t):=1000\cdot\left({\frac{\sum_{t^{\prime}=t-k+1}^{t}E_{t r}(t^{\prime})}{k\cdot\operatorname*{min}_{t^{\prime}=t-k+1}^{t}E_{t
    r}(t^{\prime})}}-1\right)$$
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: $$P_{k}(t):=1000\cdot\left({\frac{\sum_{t^{\prime}=t-k+1}^{t}E_{t r}(t^{\prime})}{k\cdot\operatorname*{min}_{t^{\prime}=t-k+1}^{t}E_{t
    r}(t^{\prime})}}-1\right)$$
- en: that is, "how much was the average training error during the strip larger than
    the minimum training error during the strip?" Note that this progress measure
    is high for unstable phases of training, where the training set error goes up
    instead of down. This is intended, because many training algorithms sometimes
    produce such "jitter" by taking inappropriately large steps in weight space. The
    progress measure is, however, guaranteed to approach zero in the long run unless
    the training is globally unstable (e.g. oscillating).
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，“在该阶段，平均训练误差比最低训练误差高了多少？”注意，这一进展度量在训练不稳定的阶段较高，此时训练集误差上升而非下降。这是有意为之，因为许多训练算法有时会因在权重空间中采取不适当的大步而产生这种“颤动”。然而，这一进展度量在长远来看会趋近于零，除非训练是全局不稳定的（例如振荡）。
- en: 'Now we can define the second class of stopping criteria: use the quotient of
    generalization loss and progress.'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义第二类停止标准：使用泛化损失和进展的比值。
- en: In the following we will always assume strips of length 5 and measure the validation
    error only at the end of each strip.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的内容中，我们将始终假设条带长度为5，并且只在每个条带的末尾测量验证误差。
- en: $$\mathrm{h}\ \frac{G L(t)}{P_{k}(t)}>\alpha$$
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{h}\ \frac{G L(t)}{P_{k}(t)}>\alpha$$
- en: '$PQ_{\alpha}$ : stop after first end-of-strip epoch $t$ with .'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: $PQ_{\alpha}$：在第一个结束条带的时期$t$后停止。
- en: A completely different kind of stopping criterion relies only on the sign of
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 完全不同类型的停止准则仅依赖于
- en: 'the changes in the generalization error. We define the third class of stopping
    criteria: stop when the generalization error increased in s successive strips.'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 一般化误差的变化。我们定义第三类停止准则：在连续的s个条带中，当一般化误差增加时停止。
- en: 'UPs : stop after epoch t iff UPs−1 stops after epoch t − k and Eva(t) > Eva(t
    − k)'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: UPs：当UPs−1在时期t − k后停止，并且Eva(t) > Eva(t − k)时在时期t后停止。
- en: 'UP1 : stop after first end-of-strip epoch t with Eva(t) > Eva(t − k)'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: UP1：在第一个结束条带的时期t后停止，条件为Eva(t) > Eva(t − k)
- en: The idea behind this definition is that when the validation error has increased
    not only once but during s consecutive strips, we assume that such increases indicate
    the beginning of final overfitting, independent of how large the increases actually
    are. The UP criteria have the advantage of measuring change locally so that they
    can be used in the context of pruning algorithms, where errors must be allowed
    to remain much higher than previous minima over long training periods.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义背后的想法是，当验证误差不仅一次而是在s个连续条带中增加时，我们假设这种增加表示最终过拟合的开始，无论实际增加有多大。UP准则的优点在于能够局部测量变化，因此可以在修剪算法的背景下使用，在这种情况下，误差必须允许在长时间训练期间保持高于先前的最低值。
- en: None of these criteria alone can guarantee termination. We thus complement them
    by the rule that training is stopped when the progress drops below 0.1 or after
    at most 3000 epochs.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 这些准则单独不能保证终止。因此，我们通过规则来补充，当进展下降到0.1以下或在最多3000个时期后停止训练。
- en: 'All stopping criteria are used in the same way: They decide to stop at some
    time t during training and the result of the training is then the set of weights
    that exhibited the lowest validation error Eopt(t). Note that in order to implement
    this scheme, only one duplicate weight set is needed.'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 所有停止准则的使用方式相同：它们在训练期间的某个时间t决定停止，训练结果是显示出最低验证误差Eopt(t)的权重集。注意，为了实现这个方案，只需要一个重复的权重集。
- en: '2.2.2 The Trick: Criterion Selection Rules'
  id: totrans-739
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2.2 这个技巧：准则选择规则
- en: These three classes of stopping criteria GL, UP, and P Q were evaluated on a
    variety of learning problems as described in Section 2.3 below. The results indicate
    that "slower" criteria, which stop later than others, on the average lead to improved
    generalization compared to "faster" ones. However, the training time that has
    to be expended for such improvements is rather large on average and also varies
    dramatically when slow criteria are used. The systematic differences between the
    criteria *classes* are only small.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 这三类停止准则GL、UP和P Q在第2.3节中描述的多种学习问题上进行了评估。结果表明，通常停止较晚的“较慢”准则平均上导致比“较快”准则更好的泛化。然而，为了获得这种改进，平均所需的训练时间相当大，并且在使用慢准则时变化也很剧烈。不同准则*类别*之间的系统性差异很小。
- en: 'For training setups similar to the one used in this work, the following rules
    can be used for selecting a stopping criterion:'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 对于类似于本工作中使用的训练设置，可以使用以下规则选择停止准则：
- en: 1. Use fast stopping criteria unless small improvements of network performance
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 使用快速停止准则，除非网络性能的小幅提升
- en: (e.g. 4%) are worth large increases of training time (e.g. factor 4).
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: （例如4%）值得大幅增加训练时间（例如增加4倍）。
- en: 2. To maximize the probability of finding a "good" solution (as opposed to maximizing
    the average quality of solutions), use a GL criterion.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 为了最大化找到“良好”解决方案的概率（与最大化解决方案的平均质量相对），使用GL准则。
- en: 3. To maximize the average quality of solutions, use a P Q criterion if the
    network overfits only very little or an UP criterion otherwise.
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 为了最大化解决方案的平均质量，如果网络过拟合仅很少，使用P Q准则，否则使用UP准则。
- en: 2.3 Where And How Well Does This Trick Work?
  id: totrans-746
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 这个技巧在哪些地方和怎样有效？
- en: As no mathematical analysis of the properties of stopping criteria is possible
    today (see Section 2.4 for the state of the art), we resort to an experimental
    evaluation.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 由于目前无法对停止准则的性质进行数学分析（参见第2.4节的最新进展），我们依靠实验评估。
- en: We want to find out which criteria will achieve how much generalization using
    how much training time on which kinds of problems. To achieve broad coverage,
    we use 12 different network topologies, 12 different learning tasks, and 14 different
    stopping criteria. To keep the experiment feasible, only one training algorithm
    is used.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想了解哪些标准在多大程度上实现了泛化，使用了多长训练时间以及在什么样的问题上。为了实现广泛覆盖，我们使用12种不同的网络拓扑、12种不同的学习任务和14种不同的停止标准。为了使实验可行，仅使用一种训练算法。
- en: 2.3.1 Concrete Questions
  id: totrans-749
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3.1 具体问题
- en: 'To derive and evaluate the stopping criteria selection rules presented above
    we need to answer the following questions:'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 为了推导和评估上述停止标准选择规则，我们需要回答以下问题：
- en: 1. *Training time:* How long will training take with each criterion, i.e., how
    *fast* or *slow* are they?
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 1. *训练时间：* 使用每个标准训练需要多长时间，即它们有多*快*或*慢*？
- en: 2. *Efficiency:* How much of this training time will be redundant, i.e., will
    occur after the to-be-chosen validation error minimum has been seen?
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 2. *效率：* 这段训练时间中有多少是冗余的，即在选择的验证误差最小值出现后，还会发生多少？
- en: 3. *Effectiveness:* How good will the resulting network performance be? 4. *Robustness:*
    How sensitive are the above qualities of a criterion to changes of the learning
    problem, network topology, or initial conditions?
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 3. *有效性：* 结果网络性能会有多好？ 4. *鲁棒性：* 上述标准的质量对学习问题、网络拓扑或初始条件的变化有多敏感？
- en: 5. *Tradeoffs:* Which criteria provide the best time-performance tradeoff? 6.
    *Quantification:* How can the tradeoff be quantified?
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 5. *权衡：* 哪些标准提供最佳的时间-性能权衡？ 6. *量化：* 如何量化这一权衡？
- en: The answers will directly lead to the rules already presented above in Section
    2.2.2. To find the answers to the questions we record for a large number of runs
    when each criterion would stop and what the associated network performance would
    be.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 答案将直接导致第2.2.2节中已提出的规则。为了解答这些问题，我们记录大量运行中每个标准会在何时停止，以及相关的网络性能。
- en: 2.3.2 Experimental Setup
  id: totrans-756
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3.2 实验设置
- en: 'Approach. To measure network performance, we partition each dataset into two
    disjoint parts: *Training data* and *test data*. The training data is further
    subdivided into a *training set* of examples used to adjust the network weights
    and a *validation set* of examples used to estimate network performance during
    training as required by the stopping criteria. The validation set is never used
    for weight adjustment. This decision was made in order to obtain pure stopping
    criteria results. In contrast, in a real application after a reasonable stopping
    time has been computed, one would include the validation set examples in the training
    set and retrain from scratch.'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 方法。为了测量网络性能，我们将每个数据集分为两个不相交的部分：*训练数据*和*测试数据*。训练数据进一步细分为*训练集*（用于调整网络权重的示例）和*验证集*（用于根据停止标准在训练期间估计网络性能的示例）。验证集从不用于权重调整。这个决定是为了获得纯粹的停止标准结果。相比之下，在实际应用中，计算出合理的停止时间后，会将验证集示例包含在训练集中并从头开始重新训练。
- en: Stopping Criteria. The stopping criteria examined were GL1, GL2, GL3, GL5, P
    Q0.5, P Q0.75, P Q1, P Q2, P Q3, UP2, UP3, UP4, UP6, and UP8. All criteria where
    evaluated simultaneously, i.e., each single training run returned one result for
    each of the criteria. This approach reduces the variance of the estimation.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 停止标准。检查的停止标准包括GL1、GL2、GL3、GL5、P Q0.5、P Q0.75、P Q1、P Q2、P Q3、UP2、UP3、UP4、UP6和UP8。所有标准同时评估，即每次训练运行为每个标准返回一个结果。这种方法减少了估计的方差。
- en: Learning Tasks. Twelve different problems were used, all from the Proben1 NN
    benchmark set [19]. All problems are real datasets from realistic application
    domains; they form a sample of a broad class of domains, but none of them exhibits
    extreme nonlinearity. The problems have between 8 and 120 inputs, between 1 and
    19 outputs, and between 214 and 7200 examples. All inputs and outputs are normalized
    to range 0. . . 1. Nine of the problems are classification tasks using 1-of-n
    output encoding (cancer, card, diabetes, gene, glass, heart, horse, soybean, and
    *thyroid*), three are approximation tasks (*building, flare*, and hearta). Datasets
    and Network Architectures. The examples of each problem were partitioned into
    training (50%), validation (25%), and test set (25% of examples)
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 学习任务。使用了十二个不同的问题，均来自 Proben1 NN 基准集 [19]。所有问题均为来自真实应用领域的真实数据集；它们形成了一个广泛类别的样本，但没有一个展现出极端非线性。这些问题的输入数量在
    8 到 120 之间，输出数量在 1 到 19 之间，样本数量在 214 到 7200 之间。所有输入和输出都归一化到范围 0...1。九个问题是使用 1-of-n
    输出编码的分类任务（癌症、信用卡、糖尿病、基因、玻璃、心脏、马、大豆和 *甲状腺*），三个是近似任务（*建筑、耀斑* 和心脏病）。数据集和网络架构。每个问题的示例被划分为训练集（50%）、验证集（25%）和测试集（25%
    的示例）。
- en: 'in three different random ways, resulting in 36 datasets. Each of these datasets
    was trained with 12 different feedforward network topologies: one hidden layer
    networks with 2, 4, 8, 16, 24, or 32 hidden nodes and two hidden layer networks
    with 2+2, 4+2, 4+4, 8+4, 8+8, or 16+8 hidden nodes in the first+second hidden
    layer, respectively; all these networks were fully connected including all possible
    shortcut connections. For each of the network topologies and each dataset, two
    runs were made with linear output units and one with sigmoidal output units using
    the activation function f(x) = x/(1 + |x|).'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 以三种不同的随机方式进行划分，生成 36 个数据集。每个数据集均使用 12 种不同的前馈网络拓扑进行训练：一个隐藏层网络，具有 2、4、8、16、24
    或 32 个隐藏节点，以及两个隐藏层网络，第一和第二隐藏层分别具有 2+2、4+2、4+4、8+4、8+8 或 16+8 个隐藏节点；所有这些网络均为完全连接，包括所有可能的快捷连接。对于每种网络拓扑和每个数据集，分别进行了两次线性输出单元的运行和一次使用激活函数
    f(x) = x/(1 + |x|) 的 sigmoid 输出单元的运行。
- en: Training Algorithm. All runs were done using the RPROP training algorithm
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 训练算法。所有运行均使用 RPROP 训练算法进行。
- en: '[21] using the squared error function and the parameters η+ = 1.1, η− = 0.5,
    Δ0 ∈ 0.05 ... 0.2 randomly per weight, Δmax = 50, Δmin = 0, initial weights'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] 使用平方误差函数和参数 η+ = 1.1, η− = 0.5, Δ0 ∈ 0.05 ... 0.2 随机生成每个权重，Δmax = 50,
    Δmin = 0, 初始权重。'
- en: −0.5 ... 0.5 randomly. RPROP is a fast backpropagation variant that is about
    as fast as quickprop [6] but more stable without adjustment of the parameters.
    RPROP requires epoch learning, i.e., the weights are updated only once per epoch.
    Therefore, the algorithm is fast without parameter tuning for small training sets
    but not recommendable for large training sets. Lack of parameter tuning helps
    to avoid the common methodological error of tuning parameters using the test error.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: −0.5 ... 0.5 随机生成。RPROP 是一种快速的反向传播变体，其速度与 quickprop [6] 相当，但在不调整参数的情况下更为稳定。RPROP
    需要以轮次为单位进行学习，即权重仅在每个轮次中更新一次。因此，对于小型训练集，该算法快速且不需要参数调整，但对于大型训练集则不推荐。缺乏参数调整有助于避免使用测试误差调节参数的常见方法论错误。
- en: '2.3.3 Experiment Results 2.3.4 Discussion: Answers To The Questions'
  id: totrans-764
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3.3 实验结果 2.3.4 讨论：问题的答案
- en: '| training time   | efficiency and effectiveness   |          |       |        |          |       |'
  id: totrans-765
  prefs: []
  type: TYPE_TB
  zh: '| 训练时间       | 效率和有效性                 |          |       |        |          |       |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-766
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| C               | Scˆ(C)                         | SGL2 (C) | r(C)  | Bcˆ(C)
    | BGL2 (C) | Pg(C) |'
  id: totrans-767
  prefs: []
  type: TYPE_TB
  zh: '| C               | Scˆ(C)                         | SGL2 (C) | r(C)  | Bcˆ(C)
    | BGL2 (C) | Pg(C) |'
- en: '| UP2             | 0.792                          | 0.766    | 0.277 | 1.055  |
    1.024    | 0.587 |'
  id: totrans-768
  prefs: []
  type: TYPE_TB
  zh: '| UP2             | 0.792                          | 0.766    | 0.277 | 1.055  |
    1.024    | 0.587 |'
- en: '| GL1             | 0.956                          | 0.823    | 0.308 | 1.044  |
    1.010    | 0.680       |'
  id: totrans-769
  prefs: []
  type: TYPE_TB
  zh: '| GL1             | 0.956                          | 0.823    | 0.308 | 1.044  |
    1.010    | 0.680       |'
- en: '| UP3             | 1.010                          | 1.264    | 0.419 | 1.026        |
    1.003    | 0.631 |'
  id: totrans-770
  prefs: []
  type: TYPE_TB
  zh: '| UP3             | 1.010                          | 1.264    | 0.419 | 1.026        |
    1.003    | 0.631 |'
- en: '| GL2             | 1.237                          | 1.000    | 0.514 | 1.034  |
    1.000    | 0.723       |'
  id: totrans-771
  prefs: []
  type: TYPE_TB
  zh: '| GL2             | 1.237                          | 1.000    | 0.514 | 1.034  |
    1.000    | 0.723       |'
- en: '| UP4             | 1.243                          | 1.566    | 0.599 | 1.020        |
    0.997    | 0.666 |'
  id: totrans-772
  prefs: []
  type: TYPE_TB
  zh: '| UP4             | 1.243                          | 1.566    | 0.599 | 1.020        |
    0.997    | 0.666 |'
- en: '| P Q0.5          | 1.253                          | 1.334    | 0.663 | 1.027  |
    1.002    | 0.658 |'
  id: totrans-773
  prefs: []
  type: TYPE_TB
  zh: '| P Q0.5          | 1.253                          | 1.334    | 0.663 | 1.027  |
    1.002    | 0.658 |'
- en: '| P Q0.75         | 1.466                          | 1.614    | 0.863 | 1.021  |
    0.998    | 0.682 |'
  id: totrans-774
  prefs: []
  type: TYPE_TB
  zh: '| P Q0.75         | 1.466                          | 1.614    | 0.863 | 1.021  |
    0.998    | 0.682 |'
- en: '| GL3             | 1.550                          | 1.450    | 0.712       |
    1.025  | 0.994    | 0.748       |'
  id: totrans-775
  prefs: []
  type: TYPE_TB
  zh: '| GL3             | 1.550                          | 1.450    | 0.712       |
    1.025  | 0.994    | 0.748       |'
- en: '| P Q1            | 1.635                          | 1.796    | 1.038 | 1.018  |
    0.994    | 0.704 |'
  id: totrans-776
  prefs: []
  type: TYPE_TB
  zh: '| P Q1            | 1.635                          | 1.796    | 1.038 | 1.018  |
    0.994    | 0.704 |'
- en: '| UP6             | 1.786                          | 2.381    | 1.125 | 1.012        |
    0.990    | 0.737 |'
  id: totrans-777
  prefs: []
  type: TYPE_TB
  zh: '| UP6             | 1.786                          | 2.381    | 1.125 | 1.012        |
    0.990    | 0.737 |'
- en: '| GL5             | 2.014                          | 2.013    | 1.162 | 1.021  |
    0.991    | 0.772       |'
  id: totrans-778
  prefs: []
  type: TYPE_TB
  zh: '| GL5             | 2.014                          | 2.013    | 1.162 | 1.021  |
    0.991    | 0.772       |'
- en: '| P Q2            | 2.184                          | 2.510    | 1.636 | 1.012  |
    0.990    | 0.768 |'
  id: totrans-779
  prefs: []
  type: TYPE_TB
  zh: '| P Q2            | 2.184                          | 2.510    | 1.636 | 1.012  |
    0.990    | 0.768 |'
- en: '| UP8             | 2.485                          | 3.259    | 1.823 | 1.010        |
    0.988    | 0.759 |'
  id: totrans-780
  prefs: []
  type: TYPE_TB
  zh: '| UP8             | 2.485                          | 3.259    | 1.823 | 1.010        |
    0.988    | 0.759 |'
- en: '| P Q3            | 2.614                          | 3.095    | 2.140 | 1.009  |
    0.988    | 0.800 |'
  id: totrans-781
  prefs: []
  type: TYPE_TB
  zh: '| P Q3            | 2.614                          | 3.095    | 2.140 | 1.009  |
    0.988    | 0.800 |'
- en: Altogether, 1296 training runs were made for the comparison, giving 18144 stopping
    criteria performance records for the 14 criteria. 270 of these records (or 1.5%)
    from 125 different runs reached the 3000 epoch limit instead of using the stopping
    criterion itself.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 总共进行了 1296 次训练以进行比较，产生了 14 个标准的 18144 条停止标准性能记录。这些记录中有 270 条（或 1.5%）来自 125 次不同的运行，达到了
    3000 epoch 限制，而不是使用停止标准本身。
- en: The results for each stopping criterion averaged over all 1296 runs are shown
    in Table 2.1. Figure 2.3 describes the variance embedded in the means given in
    the table. I will now explain and then interpret the entries in both, table and
    figure. Note that the discussion is biased by the particular collection of criteria
    chosen for the study.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 每个停止标准在所有 1296 次运行中的结果平均值显示在表 2.1 中。图 2.3 描述了表中给出的均值所包含的方差。我将现在解释并解释表格和图形中的条目。请注意，讨论受到所选标准特定集合的影响。
- en: 'Definitions. For each run, we define Eva(C) as the minimum validation error
    found until criterion C indicates to stop; it is the error after epoch number
    tm(C) (read: "time of minimum"). Ete(C) is the corresponding test error and characterizes
    network performance. Stopping occurs after epoch ts(C) (read:'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 定义。对于每次运行，我们定义 Eva(C) 为在标准 C 指示停止之前找到的最小验证误差；这是在 epoch 数 tm(C) 之后的误差（读作：“最小时间”）。Ete(C)
    是相应的测试误差，表征网络性能。停止发生在 epoch ts(C) 之后（读作：
- en: '"time of stop"). A *best* criterion Cˆ of a particular run is one with minimum
    ts of all those (among the examined) with minimum Eva, i.e., a criterion that
    found the best validation error fastest. There may be several best, because multiple
    criteria may stop at the same epoch. Note that there is no single criterion Cˆ
    because Cˆ changes from run to run. C is called *good* in a particular run if
    Eva(C) = Eva(Cˆ), i.e., if it is among those that found the lowest validation
    set error, no matter how fast or slow.'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: “停止时间”。一个 *最佳* 标准 Cˆ 的特定运行是指在所有这些（在检查中）中具有最小 ts 的标准，且具有最小 Eva，即找到最佳验证误差最快的标准。可能有多个最佳，因为多个标准可能在同一
    epoch 停止。请注意，没有单一的标准 Cˆ，因为 Cˆ 在不同运行中变化。标准 C 在特定运行中被称为 *良好*，如果 Eva(C) = Eva(Cˆ)，即如果它是找到最低验证集误差的标准之一，无论速度快慢。
- en: We now discuss the questions raised in Section 2.3.1.
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在讨论第 2.3.1 节提出的问题。
- en: Table 2.1. Behavior of stopping criteria. SGL2 is normalized training time,
    BGL2 is normalized test error (both relative to GL2). r is the training time redundancy,
    Pg is the probability of finding a good solution. For further description please
    refer to the text.
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.1. 停止标准的行为。SGL2 是标准化训练时间，BGL2 是标准化测试误差（均相对于 GL2）。r 是训练时间冗余，Pg 是找到良好解决方案的概率。有关进一步描述，请参阅文本。
- en: '![67_image_0.png](67_image_0.png)'
  id: totrans-788
  prefs: []
  type: TYPE_IMG
  zh: '![67_image_0.png](67_image_0.png)'
- en: 'BCˆ (C) (bottom) for each pair of learning problem and stopping criterion.
    In each of the 168 columns, the dot represents the mean computed from 108 runs:
    learning problem and stopping criterion are fixed, while three other parameters
    are varied (12 topologies'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: BCˆ (C) (底部) 每对学习问题和停止标准。每一列中的点表示从 108 次运行计算的均值：学习问题和停止标准是固定的，而三个其他参数是可变的（12
    种拓扑
- en: '× 3 runs × 3 dataset variants). The length of the line is twice the standard
    deviation within these 108 values. Within each block of dot-line plots, the plots
    represent (in order) the problems building, cancer, card, diabetes, flare, gene,
    glass, heart, hearta, horse, soybean, thyroid. The horizontal line marks the median
    of the means. Note:'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: × 3 次运行 × 3 个数据集变体)。该线的长度是这108个值的标准差的两倍。在每个点线图块内，图形按顺序代表问题构建、癌症、卡片、糖尿病、火焰、基因、玻璃、心脏、心脏a、马、大豆、甲状腺。水平线标记了均值的中位数。注意：
- en: When comparing the criteria groups, remember that overall the P Q criteria chosen
    are slower than the others. It is unfair to compare, for example, P Q0.5 to GL1
    and UP2.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较标准组时，请记住总体上所选的PQ标准比其他标准更慢。例如，将PQ0.5与GL1和UP2进行比较是不公平的。
- en: 1. *Training time:* The *slowness* of a criterion C in a run, relative to another
    criterion x is Sx(C) := ts(C)/ts(x), i.e., the relative total training time. As
    we see, the times relative to a fixed criterion as shown in column SGL2 (C) vary
    by more than factor 4. Therefore, the decision for a particular stopping criterion
    influences training times dramatically, even if one considers only the range of
    criteria used here. In contrast, even the slowest criteria train only about 2.5
    times as long as the fastest criterion of each run that finds the same result,
    as indicated in column SCˆ(C). This shows that the training times are not completely
    unreasonable even for the slower criteria, but do indeed pay off to some degree.
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 1. *训练时间：* 标准C在一次运行中的*缓慢性*相对于另一个标准x定义为 Sx(C) := ts(C)/ts(x)，即相对总训练时间。从SGL2 (C)列中可以看出，相对于固定标准的时间变化超过4倍。因此，特定停止标准的选择显著影响训练时间，即使仅考虑这里使用的标准范围。相比之下，即使是最慢的标准，相对于找到相同结果的每次运行中最快标准的训练时间也仅为约2.5倍，如SCˆ(C)列所示。这表明即使是较慢的标准，训练时间也并非完全不合理，确实在某种程度上是有回报的。
- en: 2. *Efficiency:* The *redundancy* of a criterion can be defined as r(C) :=
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: '2. *效率：* 标准的*冗余*可以定义为 r(C) := '
- en: (ts(C)/tm(C)) − 1. It characterizes how long the training continues after the
    final solution has been seen. r(C)=0 would be perfect, r(C)=1 means that the criterion
    trains twice as long as necessary. Low values indicate efficient criteria. As
    we see, the slower a criterion is, the less efficient it tends to get. Even the
    fastest criteria "waste" about one fifth of their overall training time. The slower
    criteria train twice as long as necessary to find the same solution.
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: (ts(C)/tm(C)) − 1。它描述了在看到最终解决方案后训练持续的时间。r(C)=0是完美的，r(C)=1意味着该标准的训练时间是必要时间的两倍。低值表示高效标准。正如我们所见，标准越慢，效率往往越低。即使是最快的标准“浪费”了大约五分之一的整体训练时间。较慢的标准训练时间是必要的两倍以找到相同的解决方案。
- en: 3. *Effectiveness:* We define the *badness* of a criterion C in a run relative
    to another criterion x as Bx(C) := Ete(C)/Ete(x), i.e., its relative error on
    the test set. Pg(C) is the fraction of the 1296 runs in which C was a good criterion.
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 3. *有效性：* 我们将运行中标准C相对于另一个标准x的*坏度*定义为 Bx(C) := Ete(C)/Ete(x)，即其在测试集上的相对误差。Pg(C)是1296次运行中C是良好标准的比例。
- en: This is an estimate of the probability that C is good in a run. As we see from
    the Pg column, even the fastest criteria are fairly effective. They reach a result
    as good as the best (of the same run) in about 60% of the cases. On the other
    hand, even the slowest criteria are not at all infallible; they achieve about
    80%.
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对C在一次运行中良好的概率的估计。从Pg列中可以看出，即使是最快的标准也相当有效。它们在约60%的情况下取得的结果与最佳（相同运行中的）一样好。另一方面，即使是最慢的标准也并非毫无错误；它们的成功率约为80%。
- en: 'However, Pg says nothing about how far from the optimum the non-good runs are.
    Columns BCˆ(C) and BGL2 (C) indicate that these differences are usually rather
    small: column BGL2 (C) shows that even the criteria with the lowest error achieve
    only about 1% lower error on the average than the relatively fast criterion GL2.
    In column BCˆ(C) we see that several only modestly slow criteria have just about
    2% higher error on the average than the best criteria of the same run. For obtaining
    the lowest possible generalization error, independent of training time, it appears
    that one has to use an extreme criterion such as GL50 or even use a conjunction
    of all three criteria classes with high parameter values.'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Pg并没有说明非良好运行距离最佳的远近。BCˆ(C)和BGL2 (C)列表明这些差异通常相当小：BGL2 (C)列显示，即使是误差最低的标准，其平均误差也仅比相对快速的GL2标准低约1%。在BCˆ(C)列中，我们看到几个仅略微缓慢的标准其平均误差比同一运行的最佳标准高出约2%。为了获得最低的可能泛化误差，无论训练时间如何，似乎必须使用极端标准，例如GL50，甚至结合所有三个标准类的高参数值。
- en: 4. *Robustness:* We call a criterion *robust* to the degree that its performance
    is independent of the learning problem and the learning environment (network topology,
    initial conditions etc.). Optimal robustness would mean that in Figure 2.3 all
    dots within a block are at the same height (problem independence)
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: 4. *稳健性：* 我们称标准为*稳健*，在于其性能独立于学习问题和学习环境（网络拓扑、初始条件等）。最佳稳健性意味着在图2.3中，所有点在一个块内的高度相同（问题独立性）。
- en: and all lines have length zero (environment independence). Note that slowness
    and badness are measured relative to the best criterion of the same program run.
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 所有行的长度为零（环境独立性）。请注意，缓慢和不良是相对于同一程序运行的最佳标准进行测量的。
- en: 'We observe the following:'
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到以下几点：
- en: '- With respect to slowness and redundancy, slower criteria are much less robust
    than faster ones. In particular the P Q criteria are quite sensitive to the learning
    problem, with the card and horse problems being worst in this experimental setting.'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 关于缓慢和冗余，较慢的标准比较快的标准稳健性差得多。特别是，P Q标准对学习问题非常敏感，在这种实验设置中，卡片和马匹问题是最糟的。
- en: '- With respect to badness, the picture is completely different: slower criteria
    tend to be slightly *more* robust than slower ones. P Q criteria are a little
    more robust than the others while GL criteria are significantly less robust.'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 关于不良情况，情况完全不同：较慢的标准往往比较快的标准稍微*更*稳健。P Q标准比其他标准稍微稳健，而GL标准则显著不稳健。
- en: All criteria are more or less instable for the building, cancer, and thyroid
    problems. In particular, all GL criteria have huge problems with the building
    problem, whose dataset 1 is the only one that is partitioned non-randomly; it
    uses chronological order of examples, see [19]. The slower variants of the other
    criteria types are nicely robust in this case.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 所有标准在建筑、癌症和甲状腺问题上或多或少都不稳定。特别是，所有GL标准在建筑问题上有巨大的问题，数据集1是唯一一个非随机划分的；它使用了示例的时间顺序，见[19]。在这种情况下，其他标准类型的较慢变体则表现得非常稳健。
- en: '- Similar statements apply when one analyzes the influence of only large or
    only small network topologies separately (not shown in any figure or table). One
    notable exception was the fact that for networks with very few hidden nodes the
    P Q criteria are more cost-effective than both the GL and the UP criteria for
    minimizing BCˆ(C). The explanation may be that such small networks do not overfit
    severely; in this case it is advantageous to take training progress into account
    as an additional factor to determine when to stop training.'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 当单独分析大型或小型网络拓扑的影响时，类似的陈述适用（未在任何图表中显示）。一个显著的例外是，对于具有极少隐藏节点的网络，P Q标准在最小化BCˆ(C)方面比GL和UP标准更具成本效益。解释可能是这些小型网络并没有严重过拟合；在这种情况下，将训练进展作为额外因素来决定何时停止训练是有利的。
- en: Overall, fast criteria improve the predictability of the training time, while
    slow ones improve the predictability of the solution quality.
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，快速标准提高了训练时间的可预测性，而缓慢标准提高了解决方案质量的可预测性。
- en: 5. *Best tradeoffs:* Despite the common overall trend, some criteria may be
    more cost-effective than others, i.e., provide better tradeoffs between training
    time and resulting network performance. Column Bcˆ of the table suggests that
    the best tradeoffs between test error and training time are (in order of increasing
    willingness to spend lots of training time) UP3, UP4, and UP6, if one wants to
    minimize the expected network performance from a single run. These criteria are
    also robust. If on the other hand one wants to make several runs and pick the
    network that seems to be best (based on its validation error), Pg is the relevant
    metric and the GL criteria are preferable. The best tradeoffs are marked with
    a star in the table. Figure 2.4 illustrates these results. The upper curve corresponds
    to column BCˆ of the table (plotted against column SCˆ); local minima indicate
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 5. *最佳权衡:* 尽管总体趋势相似，但一些标准可能比其他标准更具成本效益，即在训练时间和网络性能之间提供更好的权衡。表格中的列Bcˆ建议，如果想要最小化单次运行的预期网络性能，测试误差与训练时间之间的最佳权衡依次为UP3、UP4和UP6。这些标准也很稳健。另一方面，如果想进行多次运行并选择似乎表现最佳的网络（基于其验证误差），Pg是相关的指标，GL标准更可取。最佳权衡在表中用星号标记。图2.4说明了这些结果。上曲线对应于表格的列BCˆ（绘制对列SCˆ）；局部最小值指示
- en: '![69_image_0.png](69_image_0.png)'
  id: totrans-807
  prefs: []
  type: TYPE_IMG
  zh: '![69_image_0.png](69_image_0.png)'
- en: Fig. 2.4. Badness BCˆ (C) and Pg against slowness SCˆ(C) of criteria
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4. 标准的坏度BCˆ (C)和Pg与缓慢性SCˆ(C)的关系
- en: criteria with the best tradeoffs. The lower curve corresponds to column Pg;
    local maxima indicate the criteria with the best tradeoffs. All measurements are
    scaled by 1000.
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: 具有最佳权衡的标准。下曲线对应于列Pg；局部最大值指示具有最佳权衡的标准。所有测量均按1000进行缩放。
- en: '6. *Quantification:* From columns SGL2(C) and BGL2 (C) we can quantify the
    tradeoff involved in the selection of a stopping criterion as follows: In the
    range of criteria examined we can roughly trade a 4% decrease in test error (from
    1.024 to 0.988) for an about fourfold increase in training time (from 0.766 to
    3.095).'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 6. *量化:* 从列SGL2(C)和BGL2(C)我们可以量化选择停止标准所涉及的权衡如下：在检查的标准范围内，我们大致可以以4%的测试误差减少（从1.024减少到0.988）换取训练时间的大约四倍增加（从0.766增加到3.095）。
- en: Within this range, some criteria are somewhat better than others, but there
    is no panacea.
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个范围内，一些标准比其他标准稍微好一些，但没有灵丹妙药。
- en: 2.3.5 Generalization Of These Results
  id: totrans-812
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3.5 这些结果的概括
- en: It is difficult to say whether or how these results apply to different contexts
    than those of the above evaluation. Speculating though, I would expect that the
    behavior of the stopping criteria
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: 难以说这些结果是否以及如何适用于与上述评估不同的上下文。然而推测，我会期待停止标准的行为
- en: '- is similar for other learning rules, unless they frequently make rather extreme
    steps in parameter space,'
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对于其他学习规则是相似的，除非它们在参数空间中频繁做出相当极端的步伐，'
- en: '- is similar for other error functions, unless they are discontinuous, - is
    similar for other learning tasks, as long as they are in the same ballpark with
    respect to their nonlinearity, number of inputs and outputs, and amount of available
    training data.'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对于其他错误函数是相似的，除非它们是间断的，- 对于其他学习任务也是相似的，只要它们在非线性、输入和输出数量以及可用训练数据量方面大致相同。'
- en: Note however, that at least with respect to the learning task deviations do
    occur (see Figure 2.3). More research is needed in order to describe which properties
    of the learning tasks lead to which differences in stopping criteria behavior
    —
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 然而请注意，至少在学习任务方面会出现偏差（见图2.3）。需要更多研究来描述学习任务的哪些属性导致停止标准行为的哪些差异—
- en: 'or more generally: in order to understand how which features of tasks influence
    learning methods.'
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 或更一般地说：为了理解哪些任务特征影响学习方法。
- en: 2.4 Why This Works
  id: totrans-818
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 为什么这是有效的
- en: Detailed theoretical analyses of the error curves cannot yet be done for the
    most interesting cases such as sigmoidal multi-layer perceptrons trained on a
    modest number of examples; today they are possible for restricted scenarios only
    [1, 2, 3, 24] and do usually not aim at finding the optimal stopping criterion
    in a way comparable to the present work. However, a simplification of the analysis
    performed by Wang et al. [24] or the alternative view induced by the bias/variance
    decomposition of the error as described by Geman et al. [10] can give some insights
    why early stopping behaves as it does.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最有趣的案例，如在适量样本上训练的sigmoid多层感知器，目前尚不能进行详细的理论分析；今天的分析仅限于受限场景 [1, 2, 3, 24]，通常也不旨在以与本文相当的方式找到最佳停止标准。然而，Wang
    等人 [24] 进行的分析的简化，或 Geman 等人 [10] 描述的偏差/方差分解引发的替代视角，可以为为何提前停止以这样的方式运行提供一些见解。
- en: 'At the beginning of training (phase I), the error is dominated by what Wang
    et al. call the *approximation error* - the network has hardly learned anything
    and is still very biased. During training this part of the error is further and
    further reduced. At the same time, however, another component of the error increases:
    the *complexity error* that is induced by the increasing variance of the network
    model as the possible magnitude and diversity of the weights grows. If we train
    long enough, the error will be dominated by the complexity error'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练开始时（第一阶段），误差主要受到 Wang 等人所称的 *近似误差* 的主导——网络几乎没有学习任何东西，仍然非常偏见。在训练过程中，这部分误差会不断减少。然而，与此同时，误差的另一个组成部分却在增加：*复杂性误差*，这是由网络模型的方差随着权重的可能大小和多样性增加而引起的。如果我们训练足够长的时间，误差将主要受到复杂性误差的影响。
- en: '(phase III). Therefore, there is a phase during training, when the approximation
    and complexity (or: bias and variance) components of the error compete but none
    of them dominates (phase II). See Amari et al. [1, 2] for yet another view of
    the training process, using a geometrical interpretation. The task of early stopping
    as described in the present work is to detect when phase II ends and the dominance
    of the variance part begins.'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: （第三阶段）。因此，在训练期间，有一个阶段，近似和复杂性（或：偏差和方差）误差的组成部分相互竞争，但没有任何一个占主导地位（第二阶段）。有关训练过程的另一种观点，请参见
    Amari 等人的研究 [1, 2]，他们使用几何解释。本文所描述的提前停止的任务是检测第二阶段何时结束，以及方差部分的主导地位何时开始。
- en: 'Published theoretical results on early stopping appear to provide some nice
    techniques for practical application: Wang et al. [24] offer a method for computing
    the stopping point based on complexity considerations - without using a separate
    validation set at all. This could save precious training examples. Amari et al.
    [1, 2] compute the optimal split proportion of training data into training and
    validation set.'
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: 已发表的关于提前停止的理论结果似乎提供了一些适用于实际应用的良好技术：Wang 等人 [24] 提供了一种基于复杂性考虑计算停止点的方法——完全不使用单独的验证集。这可以节省宝贵的训练样本。Amari
    等人 [1, 2] 计算训练数据分割为训练集和验证集的最佳比例。
- en: On the other hand, unfortunately, the practical applicability of these theoretical
    analyses is severely restricted. Wang et al.'s analysis applies to networks where
    only output weights are being trained; no hidden layer training is captured. It
    is unclear to what degree the results apply to the multi-layer networks considered
    here. Amari et al.'s analysis applies to the asymptotic case of very many training
    examples. The analysis does not give advice on stopping criteria; it shows that
    early stopping is not useful when very many examples are available but does not
    cover the much more frequent case when training examples are scarce.
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，不幸的是，这些理论分析的实际适用性受到严重限制。Wang 等人的分析适用于仅训练输出权重的网络；没有捕捉到隐藏层的训练。目前尚不清楚结果在这里考虑的多层网络中的适用程度。Amari
    等人的分析适用于大量训练样本的渐近情况。该分析并未提供停止标准的建议；它表明，当有大量样本可用时，提前停止并没有用，但并未涵盖训练样本稀缺的更常见情况。
- en: There are several other theoretical works on early stopping, but none of them
    answers our practical questions. Thus, given these theoretic results, one is still
    left with making a good stopping decision for practical cases of multilayer networks
    with only few training examples and faced with a complicated evolution of the
    validation set error as shown in Figure 2.2. This is why the present empirical
    investigation was necessary.
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 有几篇关于早期停止的理论工作，但没有一篇能解答我们的实际问题。因此，鉴于这些理论结果，仍需为只有少量训练样本且面临复杂的验证集误差演变的多层网络的实际案例做出良好的停止决策，正如图
    2.2 所示。这就是进行本次实证研究的必要原因。
- en: The jagged form of the validation error curve during phase II arises because
    neither bias nor variance change monotonically, let alone smoothly. The bias error
    component may change abruptly because training algorithms never perform gradient
    descent, but take finite steps in parameter space that sometimes have severe results.
    The observed variance error component may change abruptly because, first, the
    validation set error is only an estimate of the actual generalization error and,
    second, the effect of a parameter change may be very different in different parts
    of parameter space.
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 在阶段 II 中，验证误差曲线的锯齿形状出现是因为偏差和方差都没有单调变化，更不用说平滑变化。偏差误差成分可能会突然变化，因为训练算法从未执行梯度下降，而是在参数空间中采取有限步骤，有时会导致严重后果。观察到的方差误差成分可能会突然变化，因为首先，验证集误差仅是实际泛化误差的估计，其次，参数变化的影响在参数空间的不同部分可能截然不同。
- en: Quantitatively, the different error minima that occur during phase II are quite
    close together in terms of size, but may be rather far apart in terms of training
    epoch. The exact validation error behavior seems rather unpredictable when only
    a short left section of the error curve is given. The behavior is also very different
    for different training situations.
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 从定量上看，阶段 II 中发生的不同误差最小值在大小上相当接近，但在训练周期方面可能相差较远。当只给出误差曲线的短左侧时，确切的验证误差行为似乎相当不可预测。不同的训练情况其行为也大相径庭。
- en: For these reasons no class of stopping criteria has any big advantage over another
    (on average, for the mix of situations considered here), but scaling the same
    criterion to be slower always tends to gain a little generalization.
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这些原因，没有一种停止准则在（这里考虑的各种情况的平均）上有显著优势，但将相同的准则调整得更慢往往会略微提高泛化能力。
- en: '[1] Amari, S., Murata, N., Müller, K.-R., Finke, M., Yang, H.: Statistical
    theory of overtraining - is cross-validation effective. In: [23], pp. 176–182
    (1996)'
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Amari, S., Murata, N., Müller, K.-R., Finke, M., Yang, H.：过拟合的统计理论——交叉验证有效吗。在：[23]，第
    176–182 页（1996）'
- en: '[2] Amari, S., Murata, N., Müller, K.-R., Finke, M., Yang, H.: Aymptotic statistical
    theory of overtraining and cross-validation. IEEE Trans. on Neural Networks 8(5),'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Amari, S., Murata, N., Müller, K.-R., Finke, M., Yang, H.：过拟合和交叉验证的渐近统计理论。IEEE
    神经网络学报 8(5)，'
- en: 985–996 (1997)
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 985–996（1997）
- en: '[3] Baldi, P., Chauvin, Y.: Temporal evolution of generalization during learning
    in linear networks. Neural Computation 3, 589–603 (1991)'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Baldi, P., Chauvin, Y.：线性网络学习过程中泛化的时间演变。神经计算 3，589–603（1991）'
- en: '[4] Cowan, J.D., Tesauro, G., Alspector, J. (eds.): Advances in Neural Information
    Processing Systems 6. Morgan Kaufman Publishers Inc., San Mateo (1994)'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Cowan, J.D., Tesauro, G., Alspector, J.（编）：神经信息处理系统进展 6。摩根·考夫曼出版公司，加州圣马特奥（1994）'
- en: '[5] Le Cun, Y., Denker, J.S., Solla, S.A.: Optimal brain damage. In: [22],
    pp. 598–605'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Le Cun, Y., Denker, J.S., Solla, S.A.：最佳脑损伤。在：[22]，第 598–605 页'
- en: (1990)
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: （1990）
- en: '[6] Fahlman, S.E.: An empirical study of learning speed in back-propagation
    networks.'
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Fahlman, S.E.：反向传播网络学习速度的实证研究。'
- en: Technical Report CMU-CS-88-162, School of Computer Science, Carnegie Mellon
    University, Pittsburgh, PA (September 1988)
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 技术报告 CMU-CS-88-162，卡内基梅隆大学计算机科学学院，匹兹堡，PA（1988年9月）
- en: '[7] Fahlman, S.E., Lebiere, C.: The Cascade-Correlation learning architecture.
    In:'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Fahlman, S.E., Lebiere, C.：级联-相关学习架构。在：'
- en: '[22], pp. 524–532 (1990)'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: '[22]，第 524–532 页（1990）'
- en: '[8] Fiesler, E.: Comparative bibliography of ontogenic neural networks (1994)
    (submitted for publication)'
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Fiesler, E.：发育神经网络的比较书目（1994）（已提交出版）'
- en: '[9] Finnoff, W., Hergert, F., Zimmermann, H.G.: Improving model selection by
    nonconvergent methods. Neural Networks 6, 771–783 (1993)'
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Finnoff, W., Hergert, F., Zimmermann, H.G.：通过非收敛方法改善模型选择。神经网络 6，771–783（1993）'
- en: '[10] Geman, S., Bienenstock, E., Doursat, R.: Neural networks and the bias/variance
    dilemma. Neural Computation 4, 1–58 (1992)'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Geman, S., Bienenstock, E., Doursat, R.：神经网络与偏差/方差困境。神经计算 4，1–58（1992）'
- en: '[11] Hanson, S.J., Cowan, J.D., Giles, C.L. (eds.): Advances in Neural Information
    Processing Systems 5. Morgan Kaufman Publishers Inc., San Mateo (1993)'
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Hanson, S.J., Cowan, J.D., Giles, C.L.（编辑）：神经信息处理系统的进展 5。摩根·考夫曼出版社，加州圣马特奥（1993）'
- en: '[12] Hassibi, B., Stork, D.G.: Second order derivatives for network pruning:
    Optimal brain surgeon. In: [11], pp. 164–171 (1993)'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Hassibi, B., Stork, D.G.：网络剪枝的二阶导数：最佳大脑外科医生。在：[11]，第164–171页（1993）'
- en: '[13] Krogh, A., Hertz, J.A.: A simple weight decay can improve generalization.
    In: [16],'
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Krogh, A., Hertz, J.A.：简单的权重衰减可以改善泛化。在：[16]，'
- en: pp. 950–957 (1992)
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 第950–957页（1992）
- en: '[14] Levin, A.U., Leen, T.K., Moody, J.E.: Fast pruning using principal components.'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Levin, A.U., Leen, T.K., Moody, J.E.：使用主成分的快速剪枝。'
- en: 'In: [4] (1994)'
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 在：[4]（1994）
- en: '[15] Lippmann, R.P., Moody, J.E., Touretzky, D.S. (eds.): Advances in Neural
    Information Processing Systems 3. Morgan Kaufman Publishers Inc., San Mateo (1991)'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Lippmann, R.P., Moody, J.E., Touretzky, D.S.（编辑）：神经信息处理系统的进展 3。摩根·考夫曼出版社，加州圣马特奥（1991）'
- en: '[16] Moody, J.E., Hanson, S.J., Lippmann, R.P. (eds.): Advances in Neural Information
    Processing Systems 4. Morgan Kaufman Publishers Inc., San Mateo (1992)'
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Moody, J.E., Hanson, S.J., Lippmann, R.P.（编辑）：神经信息处理系统的进展 4。摩根·考夫曼出版社，加州圣马特奥（1992）'
- en: '[17] Morgan, N., Bourlard, H.: Generalization and parameter estimation in feedforward
    nets: Some experiments. In: [22], pp. 630–637 (1990)'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Morgan, N., Bourlard, H.：前馈网络中的泛化与参数估计：一些实验。在：[22]，第630–637页（1990）'
- en: '[18] Nowlan, S.J., Hinton, G.E.: Simplifying neural networks by soft weight-sharing.'
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Nowlan, S.J., Hinton, G.E.：通过软权重共享简化神经网络。'
- en: Neural Computation 4(4), 473–493 (1992)
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 神经计算 4(4)，473–493（1992）
- en: '[19] Prechelt, L.: PROBEN1 - A set of benchmarks and benchmarking rules for
    neural network training algorithms. Technical Report 21/94, Fakultät für Informatik,
    Universität Karlsruhe, Germany, Anonymous, ftp://pub/papers/techreports/1994/1994-21.ps.gz
    on, ftp.ira.uka.de (September 1994)'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Prechelt, L.：PROBEN1 - 一套神经网络训练算法的基准和基准规则。技术报告 21/94，卡尔斯鲁厄大学计算机系，德国，匿名，ftp://pub/papers/techreports/1994/1994-21.ps.gz，ftp.ira.uka.de（1994年9月）'
- en: '[20] Reed, R.: Pruning algorithms - a survey. IEEE Transactions on Neural Networks
    4(5), 740–746 (1993)'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Reed, R.：剪枝算法 - 一项调查。IEEE神经网络汇刊 4(5)，740–746（1993）'
- en: '[21] Riedmiller, M., Braun, H.: A direct adaptive method for faster backpropagation
    learning: The RPROP algorithm. In: Proc. of the IEEE Intl. Conf. on Neural Networks,
    San Francisco, CA, pp. 586–591 (April 1993)'
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Riedmiller, M., Braun, H.：一种直接自适应方法用于更快的反向传播学习：RPROP算法。在：IEEE国际神经网络会议论文集，加州旧金山，第586–591页（1993年4月）'
- en: '[22] Touretzky, D.S. (ed.): Advances in Neural Information Processing Systems
    2. Morgan Kaufman Publishers Inc., San Mateo (1990)'
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Touretzky, D.S.（编辑）：神经信息处理系统的进展 2。摩根·考夫曼出版社，加州圣马特奥（1990）'
- en: '[23] Touretzky, D.S., Mozer, M.C., Hasselmo, M.E. (eds.): Advances in Neural
    Information Processing Systems 8. MIT Press, Cambridge (1996)'
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Touretzky, D.S., Mozer, M.C., Hasselmo, M.E.（编辑）：神经信息处理系统的进展 8。麻省理工学院出版社，剑桥（1996）'
- en: '[24] Wang, C., Venkatesh, S.S., Judd, J.S.: Optimal stopping and effective
    machine complexity in learning. In: [4] (1994)'
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Wang, C., Venkatesh, S.S., Judd, J.S.：学习中的最优停止和有效机器复杂度。在：[4]（1994）'
- en: '[25] Weigend, A.S., Rumelhart, D.E., Huberman, B.A.: Generalization by weightelimination
    with application to forecasting. In: [15], pp. 875–882 (1991)'
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Weigend, A.S., Rumelhart, D.E., Huberman, B.A.：通过权重消除进行泛化及其在预测中的应用。在：[15]，第875–882页（1991）'
- en: 3 A Simple Trick For Estimating The Weight Decay Parameter-
  id: totrans-860
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 一个简单的技巧来估计权重衰减参数-
- en: Thorsteinn S. Rögnvaldsson Centre for Computer Architecture (CCA), Halmstad
    University, P.O. Box 823, S-301 18 Halmstad, Sweden denni@cca.hh.se http://www.hh.se/staff/denni/
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: Thorsteinn S. Rögnvaldsson 计算机架构中心（CCA），哈姆斯塔大学，邮政信箱 823，S-301 18 哈姆斯塔，瑞典 denni@cca.hh.se
    http://www.hh.se/staff/denni/
- en: Abstract. We present a simple trick to get an approximate estimate of the weight
    decay parameter λ. The method combines early stopping and weight decay, into the
    estimate λˆ = ∇E(Wes)-/-2W es-,
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要。我们提出了一种简单的技巧来近似估计权重衰减参数 λ。该方法结合了早停和权重衰减，得到估计 λˆ = ∇E(Wes)-/-2W es-，
- en: where Wes is the set of weights at the early stopping point, and E(W)
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Wes 是早停点的权重集合，E(W)
- en: is the training data fit error.
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: 是训练数据的拟合误差。
- en: The estimate is demonstrated and compared to the standard crossvalidation procedure
    for λ selection on one synthetic and four real life data sets. The result is that
    λˆ is as good an estimator for the optimal weight decay parameter value as the
    standard search estimate, but orders of magnitude quicker to compute.
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 该估计通过与标准交叉验证程序进行比较，展示了在一个合成数据集和四个真实数据集上进行λ选择的结果。结果显示，λˆ作为最佳权重衰减参数值的估计器，与标准搜索估计同样有效，但计算速度快了几个数量级。
- en: The results also show that weight decay can produce solutions that are significantly
    superior to committees of networks trained with early stopping.
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: 结果还表明，权重衰减可以产生显著优于使用早停法训练的网络组合的解决方案。
- en: 3.1 Introduction
  id: totrans-867
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 引言
- en: A regression problem which does not put constraints on the model used is illposed
    [21], because there are infinitely many functions that can fit a finite set of
    training data perfectly. Furthermore, real life data sets tend to have noisy inputs
    and/or outputs, which is why models that fit the data perfectly tend to be poor
    in terms of out-of-sample performance. Since the modeler's task is to find a model
    for the underlying function while not overfitting to the noise, models have to
    be based on criteria which include other qualities besides their fit to the training
    data.
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不对模型施加约束的回归问题被认为是不适定的[21]，因为存在无限多的函数可以完美拟合有限的训练数据。此外，真实数据集往往具有噪声输入和/或输出，这就是为什么完美拟合数据的模型在样本外性能上往往较差。由于建模者的任务是为潜在函数找到一个模型，同时避免对噪声的过拟合，模型必须基于除了与训练数据的拟合度之外的其他标准。
- en: In the neural network community the two most common methods to avoid overfitting
    are *early stopping* and *weight decay* [17]. Early stopping has the advantage
    of being quick, since it shortens the training time, but the disadvantage
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络领域，避免过拟合的两种最常见的方法是*早停法*和*权重衰减*[17]。早停法的优点是速度快，因为它缩短了训练时间，但缺点是
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: '- 先前发表在：Orr, G.B. 和 Müller, K.-R. (编)：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0（1998年）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    69–89, 2012.'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon 等（编）：NN：行业技巧，第2版，LNCS 7700，第69–89页，2012年。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: of being poorly defined and not making full use of the available data. Weight
    decay, on the other hand, has the advantage of being well defined, but the disadvantage
    of being quite time consuming. This is because much time is spent with selecting
    a suitable value for the weight decay parameter (λ), by searching over several
    values of λ and estimating the out-of-sample performance using e.g. cross validation
    [25].
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: 由于定义不清，未能充分利用可用数据。另一方面，权重衰减的优点在于定义明确，但缺点是相当耗时。这是因为在选择合适的权重衰减参数值（λ）时，需要花费大量时间，需通过搜索多个λ值并使用例如交叉验证来估计样本外性能[25]。
- en: In this paper, we present a very simple method for estimating the weight decay
    parameter, for the standard weight decay case. This method combines early stopping
    with weight decay, thus merging the quickness of early stopping with the more
    well defined weight decay method, providing a weight decay parameter which is
    essentially as good as the standard search method estimate when tested empirically.
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种非常简单的估计权重衰减参数的方法，适用于标准的权重衰减情况。该方法将早停法与权重衰减结合，从而将早停法的快速性与定义更明确的权重衰减方法融合，提供了一个在实证测试中与标准搜索方法估计几乎一样有效的权重衰减参数。
- en: We also demonstrate in this paper that the arduous process of selecting λ can
    be rewarding compared to simpler methods, like e.g. combining networks into committees
    [16].
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在本文中演示了，与简单方法（例如，将网络组合成委员会[16]）相比，选择λ的艰巨过程是值得的。
- en: 'The paper is organized as follows: In section 2 we present the background of
    how and why weight decay or early stopping should be used. In section 3 we review
    the standard method for selecting λ and also introduce our new estimate. In section
    4 we give empirical evidence on how well the method works, and in section 5 we
    summarize our conclusions.'
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的组织结构如下：在第2节中，我们介绍了为何以及如何使用权重衰减或早停法的背景。在第3节中，我们回顾了选择λ的标准方法，并介绍了我们的新估计方法。在第4节中，我们提供了该方法有效性的实证证据，第5节中我们总结了我们的结论。
- en: 3.2 Ill-Posed Problems, Regularization, And Such Things... 3.2.1 Ill-Posed Problems
  id: totrans-878
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 不适定问题、正则化及相关事宜……3.2.1 不适定问题
- en: $$y(n)=\phi[\mathbf{x}(n)]+\varepsilon(n)$$
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: $$y(n)=\phi[\mathbf{x}(n)]+\varepsilon(n)$$
- en: In what follows, we denote the input data by x(n), the target data by y(n),
    and the model (neural network) output by f(W, x(n)), where W denotes the
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我们用x(n)表示输入数据，用y(n)表示目标数据，用f(W, x(n))表示模型（神经网络）输出，其中W表示
- en: parameters (weights) for the model. We assume a target data generating process
    of the form
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的参数（权重）。我们假设目标数据生成过程的形式为
- en: y(n) = φ[x(n)] + ε(n) (3.1)
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: y(n) = φ[x(n)] + ε(n) (3.1)
- en: where φ is the *underlying function* and ε(n) are sampled from a stationary
    uncorrelated (IID) zero mean noise process with variance σ2. We select models
    f
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: 其中φ是*基础函数*，ε(n)来自具有方差σ²的平稳无关（IID）零均值噪声过程。我们选择模型f
- en: from a model family F, e.g. multilayer perceptrons, to learn an approximation
    to the underlying function φ, based on the training data. That is, we are searching
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型家族F中，例如多层感知机，学习对基础函数φ的近似，基于训练数据。也就是说，我们正在寻找
- en: for
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: 对于
- en: $$f^{*}\equiv f(W^{*})\in F{\mathrm{~such~that~}}E(f^{*},\phi)\leq E(f,\phi)\
    \forall\ f\in F,$$
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: $$f^{*}\equiv f(W^{*})\in F{\mathrm{~such~that~}}E(f^{*},\phi)\leq E(f,\phi)\
    \forall\ f\in F,$$
- en: where E(f,φ) is a measure of the "distance" between the model f and the true
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: 其中E(f,φ)是模型f与真实模型之间的“距离”度量
- en: model φ. Since we only have access to the target values y, and not the underlying
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: 模型φ。由于我们只能访问目标值y，而无法访问基础值
- en: function φ, E(f,φ) is often taken to be the mean square error
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: 函数φ，E(f,φ)通常被视为均方误差
- en: $$E(f,\phi)\to E(f,y)=E(W)={\frac{1}{2N}}\sum_{n=1}^{N}[y(n)-f(W,x(n))]^{2}.$$
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(f,\phi)\to E(f,y)=E(W)={\frac{1}{2N}}\sum_{n=1}^{N}[y(n)-f(W,x(n))]^{2}.$$
- en: $$(3.1)$$
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.1)$$
- en: $$(3.2)$$
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.2)$$
- en: $$(3.3)$$
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.3)$$
- en: 'Unfortunately, minimizing (3.3) is, more often than not, an ill-posed problem.
    That is, it does not meet the following three requirements [21]:'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，最小化（3.3）通常是一个病态问题。也就是说，它不满足以下三个要求[21]：
- en: '- The model (e.g. neural network) can learn the function training data, i.e.'
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: '- 模型（例如神经网络）可以学习函数训练数据，即'
- en: there *exists* a solution f ∗ ∈ F.
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一个解f ∗ ∈ F。
- en: '- The solution is *unique*. - The solution is *stable* under small variations
    in the training data set. For instance, training with two slightly different training
    data sets sampled from the same process must result in similar solutions (similar
    when evaluated on e.g. test data).'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: '- 解是*唯一*的。 - 解在训练数据集的小变化下是*稳定*的。例如，使用从同一过程采样的两个略有不同的训练数据集进行训练，必须导致类似的解（在例如测试数据上评估时相似）。'
- en: The first and second of these requirements are often not considered serious
    problems. It is always possible to find a multilayer perceptron that learns the
    training data perfectly by using many internal units, since any continuous function
    can be constructed with a single hidden layer network with sigmoid units (see
    e.g. [6]), and we may be happy with any solution and ignore questions on uniqueness.
    However, a network that has learned the training data perfectly will be very sensitive
    to changes in the training data. Fulfilling the first requirement is thus usually
    in conflict with fulfilling the third requirement, which is a really important
    requirement. A solution which changes significantly with slightly different training
    sets will have very poor generalization properties.
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个要求中的第一个和第二个通常不会被视为严重问题。使用许多内部单元，始终可以找到一个完美学习训练数据的多层感知机，因为任何连续函数都可以用具有sigmoid单元的单隐藏层网络构造（见例如[6]），我们可能会对任何解感到满意，并忽略唯一性的问题。然而，完美学习训练数据的网络对训练数据的变化将非常敏感。因此，满足第一个要求通常与满足第三个要求相冲突，而第三个要求实际上是非常重要的。对略有不同的训练集显著变化的解将具有非常差的泛化特性。
- en: 3.2.2 Regularization
  id: totrans-899
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2.2 正则化
- en: It is common to introduce so-called regularizers1 in order to make the learning
    task well posed (or at least less ill-posed). That is, instead of only minimizing
    an error of fit measure like (3.3) we augment it with a regularization term λR(W)
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: 引入所谓的正则化项是很常见的，以使学习任务良好提出（或至少减少病态）。也就是说，我们不仅仅是最小化拟合误差度量如（3.3），而是用正则化项λR(W)来增强它。
- en: which expresses e.g. our prior beliefs about the solution.
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 这表达了我们对解的先验信念。
- en: The error functional then takes the form
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: 然后误差泛函的形式为
- en: $$E(\mathbf{W})=\frac{1}{2N}\sum_{n=1}^{N}[y(n)-f(\mathbf{W},\mathbf{x}(n))]^{2}+\lambda
    R(\mathbf{W})=E_{0}(\mathbf{W})+\lambda R(\mathbf{W}),\tag{3.4}$$
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(\mathbf{W})=\frac{1}{2N}\sum_{n=1}^{N}[y(n)-f(\mathbf{W},\mathbf{x}(n))]^{2}+\lambda
    R(\mathbf{W})=E_{0}(\mathbf{W})+\lambda R(\mathbf{W}),\tag{3.4}$$
- en: where λ is the *regularization parameter* which weighs the importance of R(W)
    relative to the error of fit E0(W).
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: 其中λ是*正则化参数*，它权衡R(W)相对于拟合误差E0(W)的重要性。
- en: The effect of the regularization term is to shrink the model family F, or make
    some models more likely than others. As a consequence, solutions become more stable
    to small perturbations in the training data.
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化项的作用是缩小模型家族F，或使某些模型比其他模型更可能。因此，解决方案对训练数据的小扰动变得更加稳定。
- en: The term "regularization" encompasses all techniques which make use of penalty
    terms added to the error measure to avoid overfitting. This includes e.g. weight
    decay [17], weight elimination [26], soft weight sharing [15], Laplacian weight
    decay [12] [27], and smoothness regularization [2] [9] [14] . Certain forms of
    "hints" [1] can also be called regularization.
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: “正则化”一词涵盖了所有利用添加到误差度量的惩罚项以避免过拟合的技术。这包括例如权重衰减[17]、权重消除[26]、软权重共享[15]、拉普拉斯权重衰减[12][27]和光滑正则化[2][9][14]。某些形式的“提示”[1]也可以称为正则化。
- en: 3.2.3 Bias And Variance
  id: totrans-907
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2.3 偏差与方差
- en: The benefit of regularization is often described in the context of *model bias*
    and model variance. This originates from the separation of the expected generalization
    error Egen into three terms [8]
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的好处通常在*模型偏差*和模型方差的背景下进行描述。这源于将期望的泛化误差Egen分成三个部分[8]。
- en: 1 Called "stabilizers" by Tikhonov [21].
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: 1 被Tikhonov称为“稳定器”[21]。
- en: $$\left\langle E_{gen}\right\rangle=\left\langle\int\left[y(\mathbf{x})-f(\mathbf{x})\right]^{2}p(\mathbf{x})d\mathbf{x}\right\rangle\tag{1}$$
    $$=\int\left[\phi(\mathbf{x})-\left\langle f(\mathbf{x})\right\rangle\right]^{2}p(\mathbf{x})d\mathbf{x}+\left\langle\int\left[f(\mathbf{x})-\left\langle
    f(\mathbf{x})\right\rangle\right]^{2}p(\mathbf{x})d\mathbf{x}\right\rangle+$$
    $$\left\langle\int\left[y(\mathbf{x})-\phi(\mathbf{x})\right]^{2}p(\mathbf{x})d\mathbf{x}\right\rangle$$
    $$=\text{Bias}^{2}+\text{Variance}+\sigma^{2},$$
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: $$\left\langle E_{gen}\right\rangle=\left\langle\int\left[y(\mathbf{x})-f(\mathbf{x})\right]^{2}p(\mathbf{x})d\mathbf{x}\right\rangle\tag{1}$$
    $$=\int\left[\phi(\mathbf{x})-\left\langle f(\mathbf{x})\right\rangle\right]^{2}p(\mathbf{x})d\mathbf{x}+\left\langle\int\left[f(\mathbf{x})-\left\langle
    f(\mathbf{x})\right\rangle\right]^{2}p(\mathbf{x})d\mathbf{x}\right\rangle+$$
    $$\left\langle\int\left[y(\mathbf{x})-\phi(\mathbf{x})\right]^{2}p(\mathbf{x})d\mathbf{x}\right\rangle$$
    $$=\text{Bias}^{2}+\text{Variance}+\sigma^{2},$$
- en: $$(3.5)$$
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.5)$$
- en: where  denotes taking the expectation over an ensemble of training sets. Here
    p(x) denotes the input data probability density.
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 其中表示对训练集集合取期望。这里p(x)表示输入数据的概率密度。
- en: A high sensitivity to training data noise corresponds to a large model variance.
    A large bias term means either that φ ∈/ F, or that φ is downweighted in favour
    of other models in F. We thus have a trade-off between model bias and model variance,
    which corresponds to the trade-off between the first and third requirements on
    well-posed problems.
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 对训练数据噪声的高敏感性对应于大的模型方差。大的偏差项意味着φ ∈/ F，或者φ在F中相对于其他模型被降低权重。因此，我们在模型偏差和模型方差之间有一个权衡，这对应于良好提出问题的第一和第三要求之间的权衡。
- en: Model bias is weighed versus model variance by selecting both a parametric form
    for R(W) and an optimal2 value for the regularization parameter λ.
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: 模型偏差与模型方差的权衡通过选择R(W)的参数形式和正则化参数λ的最佳值来实现。
- en: Many neural network practitioners ignore the first part and choose weight decay
    by default, which corresponds to a Gaussian parametric form for the prior on W.
    Weight decay is, however, not always the best choice (in fact, it is most certainly
    not the best choice for all problems). Weight decay does not for instance consider
    the function the network is producing, it only puts a constraint on the parameters.
    Another, perhaps more correct, choice would be to constrain the higher order derivatives
    of the network function (which is commonplace in statistics) like in e.g. [14].
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: 许多神经网络实践者忽略第一部分，默认选择权重衰减，这对应于W的高斯参数形式。然而，权重衰减并不总是最佳选择（实际上，对于所有问题，它绝对不是最佳选择）。例如，权重衰减并未考虑网络产生的函数，它仅对参数施加约束。另一个或许更正确的选择是约束网络函数的高阶导数（在统计学中是常见的），例如[14]。
- en: 3.2.4 Bayesian Framework
  id: totrans-916
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2.4 贝叶斯框架
- en: From a Bayesian and maximum likelihood perspective, *prior* information about
    the model (f) is weighed against the *likelihood* of the training data (D) through
    Bayes theorem (see [4] for a discussion on this). Denote the probability for observing
    data set D by p(D), the prior distribution of models f by p(f), and the likelihood
    for observing the data D, if f is the correct model, by p(D|f). We then have for
    the posterior probability p(f|D) for the model f given the observed data D
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: 从贝叶斯和最大似然的角度来看，关于模型 (f) 的 *先验* 信息与训练数据 (D) 的 *似然* 通过贝叶斯定理进行权衡（有关讨论，请参见 [4]）。观察数据集
    D 的概率用 p(D) 表示，模型 f 的先验分布用 p(f) 表示，如果 f 是正确模型，则观察数据 D 的似然用 p(D|f) 表示。我们可以得到给定观察数据
    D 的模型 f 的后验概率 p(f|D)。
- en: $$p(f|D)=\frac{p(D|f)p(f)}{p(D)}\Rightarrow$$ $$-\ln p(f|D)=-\log p(D|f)-\ln
    p(f)+\ln p(D)\Rightarrow$$ $$-\ln p(f|D)=\sum_{n=1}^{N}[y(n)-f(\mathbf{W},\mathbf{x}(n))]^{2}-\ln
    p(f)+\mbox{constant},\tag{3.6}$$
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(f|D)=\frac{p(D|f)p(f)}{p(D)}\Rightarrow$$ $$-\ln p(f|D)=-\log p(D|f)-\ln
    p(f)+\ln p(D)\Rightarrow$$ $$-\ln p(f|D)=\sum_{n=1}^{N}[y(n)-f(\mathbf{W},\mathbf{x}(n))]^{2}-\ln
    p(f)+\mbox{constant},\tag{3.6}$$
- en: where Gaussian noise ε is assumed in the last step. If we identify 2NλR(W) with
    the negative logarithm of the model prior, − ln p(f), then maximizing p(f|D) is
    equivalent to minimizing expression (3.4).
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步假设高斯噪声 ε。如果我们将 2NλR(W) 识别为模型先验的负对数，− ln p(f)，那么最大化 p(f|D) 等价于最小化表达式 (3.4)。
- en: 2 Optimality is usually measured via cross-validation or some similar method.
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: 2 最优性通常通过交叉验证或其他类似方法来衡量。
- en: From this perspective, choosing R(W) is equivalent to choosing a parameterized
    form for the model prior p(f), and selecting a value for λ corresponds to estimating
    the parameters for the prior.
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，选择 R(W) 相当于为模型的先验 p(f) 选择参数化形式，而选择 λ 的值对应于估计先验的参数。
- en: 3.2.5 Weight Decay
  id: totrans-922
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2.5 权重衰减
- en: Weight decay [17] is the neural network equivalent to the Ridge Regression [11]
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: 权重衰减 [17] 是神经网络的岭回归 [11] 等价物。
- en: method. In this case R(W) = W2 = k w2k and the error functional is
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: 方法。在这种情况下 R(W) = W2 = k w2k，误差泛函为
- en: $$E(\mathbf{W})=E_{0}(\mathbf{W})+\lambda R(\mathbf{W})=\frac{1}{2N}\sum_{n=1}^{N}[y(n)-f(\mathbf{W},\mathbf{x}(n))]^{2}+\lambda\|\mathbf{W}\|^{2},\tag{3.7}$$
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(\mathbf{W})=E_{0}(\mathbf{W})+\lambda R(\mathbf{W})=\frac{1}{2N}\sum_{n=1}^{N}[y(n)-f(\mathbf{W},\mathbf{x}(n))]^{2}+\lambda\|\mathbf{W}\|^{2},\tag{3.7}$$
- en: and λ is usually referred to as the *weight decay parameter*. In the Bayesian
    framework, weight decay means implicitly imposing the model prior
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: 而 λ 通常被称为 *权重衰减参数*。在贝叶斯框架中，权重衰减意味着隐含地施加模型先验。
- en: $$p[f(\mathbf{W})]=\sqrt{\frac{\lambda}{2\pi\sigma^{2}}}\exp\left(\frac{-\lambda\|\mathbf{W}\|^{2}}{2\sigma^{2}}\right)\tag{3.8}$$
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: $$p[f(\mathbf{W})]=\sqrt{\frac{\lambda}{2\pi\sigma^{2}}}\exp\left(\frac{-\lambda\|\mathbf{W}\|^{2}}{2\sigma^{2}}\right)\tag{3.8}$$
- en: where σ2 is the variance of the noise in the data.
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 σ2 是数据中噪声的方差。
- en: Weight decay often improves the generalization properties of neural network
    models, for reasons outlined above.
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: 权重衰减通常会改善神经网络模型的泛化特性，原因如上所述。
- en: 3.2.6 Early Stopping
  id: totrans-930
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2.6 提前停止
- en: Undoubtedly, the simplest and most widely used method to avoid overfitting is
    to stop training before the training set has been learned perfectly. This is done
    by setting aside a fraction of the training data for estimating the out-of-sample
    performance. This data set is called the validation data set. Training is then
    stopped when the error on the validation set starts to increase. Early stopping
    often shortens the training time significantly, but suffers from being ill-defined
    since there really is no well defined stopping point, and wasteful with data,
    since a part of the data is set aside.
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，避免过拟合最简单和最广泛使用的方法是，在训练集被完全学习之前停止训练。通过保留一部分训练数据来估计样本外性能来实现。这部分数据称为验证数据集。当验证集上的误差开始增加时，训练将停止。提前停止通常显著缩短训练时间，但由于没有明确定义的停止点而导致定义不明确，同时由于一部分数据被保留而造成数据浪费。
- en: There is a connection between early stopping and weight decay, if learning starts
    from small weights, since weight decay applies a potential which forces all weights
    towards zero. For instance, Sjöberg and Ljung [20] show that, if a constant learning
    rate η is used, the number of iterations n at which training is stopped is related
    to the weight decay parameter λ roughly as
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: 如果学习从小权重开始，则提前停止和权重衰减之间存在联系，因为权重衰减施加了一个势能，使所有权重趋向于零。例如，Sjöberg 和 Ljung [20]
    表示，如果使用恒定学习率 η，则停止训练的迭代次数 n 与权重衰减参数 λ 大致相关。
- en: $$\lambda\sim{\frac{1}{2m}}.$$
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: $$\lambda\sim{\frac{1}{2m}}.$$
- en: $$(3.9)$$
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.9)$$
- en: 2ηn. (3.9)
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: 2ηn. (3.9)
- en: This does not, however, mean that using early stopping is equivalent to using
    weight decay in practice. Expression (3.9) is based on a constant learning rate,
    a local expansion around the optimal stopping point, ignoring local minima, and
    assumes small input noise levels, which may not reflect the situation when overfitting
    is a serious problem. The choice of learning algorithm can also affect the early
    stopping point, and one cannot expect (3.9) to hold exactly in the practical case.
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不意味着在实践中使用早停等同于使用权重衰减。表达式(3.9)基于一个恒定的学习率，围绕最优停止点的局部扩展，忽略局部极小值，并假设输入噪声水平较小，这可能无法反映过拟合严重时的情况。学习算法的选择也可能影响早停点，因此不能期望(3.9)在实际案例中完全成立。
- en: Inspired by this connection between early stopping and weight decay, we use
    early stopping in the following section to estimate the weight decay parameter
    λ.
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: 受到早停和权重衰减之间联系的启发，我们在下一部分使用早停来估计权重衰减参数λ。
- en: 3.3 Estimating λ From a pure Bayesian point of view, the prior is something
    we know/assume in advance and do not use the training data to select (see e.g.
    [5]). There is consequently no such thing as "λ selection" in the pure Bayesian
    model selection scheme. This is of course perfectly fine if the prior is correct.
    However, if we suspect that our choice of prior is less than perfect, then we
    are better off if we take an "empirical Bayes" approach and use the data to tune
    the prior, through λ.
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: 3.3 从纯贝叶斯的角度来看，先验是我们事先知道/假设的东西，而不使用训练数据来选择（参见例如[5]）。因此，在纯贝叶斯模型选择方案中没有“λ选择”这一说法。如果先验是正确的，这当然是完全可以接受的。然而，如果我们怀疑选择的先验不够完美，那么采取“经验贝叶斯”方法，通过λ使用数据来调整先验将更为有效。
- en: Several options for selecting λ have been proposed. Weigend et al. [26] present,
    for a slightly different weight cost term, a set of heuristic rules for changing
    λ during the training. Although Weigend et al. demonstrate the use of these heuristics
    on a couple of time series problems, we cannot get these rules to work consistently
    to our satisfaction. A more principled approach is to try several values of λ
    and estimate the out-of-sample error, either by correcting the training error,
    with some factor or term, or by using cross-validation. The former is done in
    e.g. [10], [23], and [24] (see also references therein). The latter is done by
    e.g. [25].
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: 关于选择λ，有几种方案被提出。Weigend等人[26]为稍微不同的权重成本项提供了一套启发式规则，用于在训练过程中改变λ。尽管Weigend等人在几个时间序列问题上展示了这些启发式的使用，但我们无法让这些规则始终如愿地发挥作用。更为原则的方法是尝试多个λ值，并估计样本外误差，方法是用某些因子或项来修正训练误差，或通过交叉验证来实现。前者在例如[10]、[23]和[24]中进行了，后者由例如[25]完成。
- en: The method of using validation data for estimating the out-of-sample error is
    robust but slow since it requires training several models. We use cross-validation
    here because of its reliability.
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: 使用验证数据来估计样本外误差的方法稳健但速度较慢，因为它需要训练多个模型。我们在此使用交叉验证，因为它的可靠性。
- en: 3.3.1 Search Estimates
  id: totrans-941
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3.1 搜索估计
- en: Finding the optimal λ requires the use of a search algorithm, which must be
    robust because the validation error can be very noisy. A simple and straightforward
    way is to start at some large λ where the validation error is large, due to the
    large model bias, and step towards lower values until the out-of-sample error
    becomes large again, due to the large model variance. In our experience, it often
    makes sense to do the search in log λ (i.e. with equally spaced increments in
    log λ).
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找最优λ需要使用搜索算法，该算法必须稳健，因为验证误差可能非常嘈杂。一种简单直接的方法是从某个大的λ开始，由于模型偏差大，验证误差也大，并逐步向较低的值移动，直到样本外误差因模型方差大而再次增大。在我们的经验中，通常在对数λ中进行搜索（即以对数λ的均匀间隔进行增量）是有意义的。
- en: The result of such a search is a set of K values {λk} with corresponding average
    n-fold cross validation errors {log E*nCV,k*} and standard deviations {σ*nCV,k*}
    for the validation errors. These are defined as
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: 此类搜索的结果是一组K值{λk}，对应于验证误差的平均n折交叉验证误差{log E*nCV,k*}和标准差{σ*nCV,k*}。这些定义为
- en: $$\log E_{n C V,k}={\frac{1}{n}}\sum_{j=1}^{n}\log E_{j,k}$$ $$\sigma_{n C V,k}^{2}={\frac{1}{n-1}}\sum_{j=1}^{n}\left(\log
    E_{j,k}-\log E_{n C V,k}\right)^{2}$$
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: $$\log E_{n C V,k}={\frac{1}{n}}\sum_{j=1}^{n}\log E_{j,k}$$ $$\sigma_{n C V,k}^{2}={\frac{1}{n-1}}\sum_{j=1}^{n}\left(\log
    E_{j,k}-\log E_{n C V,k}\right)^{2}$$
- en: $$(3.10)$$
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.10)$$
- en: $$(3.11)$$
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.11)$$
- en: 2 (3.11)
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 2 (3.11)
- en: when λ = λk. The number of validation data sets is n and Ej,k denotes the validation
    error when λ = λk and we use validation set j. Taking logarithms is motivated
    by our observation that the validation error distribution looks approximately
    log-normal and we use this in our selection of the optimal λ value below.
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: 当λ = λk时，验证数据集的数量为n，Ej,k表示当λ = λk时使用验证集j的验证误差。取对数的动机是我们观察到验证误差分布大致呈对数正态分布，我们在下面选择最佳λ值时利用了这一点。
- en: Once the search is finished, the optimal λ is selected. This is not necessarily
    trivial since a large range of values may look equally good, or one value may
    have a small average cross-validation error with a large variation in this error,
    and another value may have a slightly higher average cross-validation error with
    a small variation in this error. The simplest approach is to look at a plot of
    the validation errors versus λ and make a judgement on where the optimal λ is,
    but this adds an undesired subjectiveness to the choice. Another is to take a
    weighted average over the different λ values, which is what we use here (see Ripley
    [19] for a discussion on variants of λ selection methods).
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦搜索完成，就选择最佳λ。这并不一定简单，因为大量值看起来可能同样不错，或者某个值的平均交叉验证误差较小而该误差的变化较大，而另一个值的平均交叉验证误差稍高但变化较小。最简单的方法是查看验证误差与λ的图形，并判断最佳λ的位置，但这会在选择中增加不必要的主观性。另一种方法是对不同的λ值进行加权平均，这正是我们在这里使用的方法（参见Ripley
    [19]关于λ选择方法变体的讨论）。
- en: Our estimate for the optimal λ is the value
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对最佳λ的估计值是
- en: $$\hat{\lambda}_{o p t}=\frac{\sum_{k=1}^{K}n_{k}\lambda_{k}}{\sum_{k=1}^{K}n_{k}}\,$$
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: $$\hat{\lambda}_{o p t}=\frac{\sum_{k=1}^{K}n_{k}\lambda_{k}}{\sum_{k=1}^{K}n_{k}}\,$$
- en: $\text{q.(3.4)}$ is minimized, then .
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: $\text{q.(3.4)}$ 被最小化时。
- en: $$(3.13)$$
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.13)$$
- en: $$(3.14)$$
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.14)$$
- en: $$(3.12)$$
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.12)$$
- en: where nk is the number of times λk corresponds to the minimum validation error
    when we sample validation errors from K log-normal distributions with means log
    E*nCV,k* and standard deviations σ*nCV,k*, assuming that the validation errors
    are independent. This is illustrated on a hypothetical example in Figure 3.1.
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: 其中nk是当我们从K个对数正态分布中抽样验证误差时，λk对应于最小验证误差的次数，假设验证误差是独立的。这个过程在图3.1中的假设示例中进行了说明。
- en: The choice (3.12) was done after confirming that it often agrees well with our
    subjective choice for λ. We refer to this below as a "Monte Carlo estimate" of
    λ.
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: 选择(3.12)是在确认它通常与我们对λ的主观选择一致后做出的。我们在下面称之为“蒙特卡洛估计”的λ。
- en: '#### 3.3.2  Twe'
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 3.3.2 Twe'
- en: 3.3.2 Two Early Stopping Estimates
  id: totrans-959
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3.2 两个早期停止估计
- en: $$\nabla E(\mathbf{W}^{*})=\nabla E_{0}(\mathbf{W}^{*})+\lambda\nabla R(\mathbf{W}^{*})=0,$$
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: $$\nabla E(\mathbf{W}^{*})=\nabla E_{0}(\mathbf{W}^{*})+\lambda\nabla R(\mathbf{W}^{*})=0,$$
- en: If W∗ is the set of weights when E(W) in eq. (3.4) is minimized, then
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: 如果W∗是在方程(3.4)中E(W)被最小化时的权重集合，那么
- en: ∇E(W∗) = ∇E0(W∗) + λ∇R(W∗)=0, (3.13)
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: ∇E(W∗) = ∇E0(W∗) + λ∇R(W∗)=0, (3.13)
- en: which implies
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着
- en: $$\mathrm{h\impl}$$
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{h\impl}$$
- en: $$\lambda=\frac{\|\nabla E_{0}(W^{*})\|}{\|\nabla R(W^{*})\|}$$
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: $$\lambda=\frac{\|\nabla E_{0}(W^{*})\|}{\|\nabla R(W^{*})\|}$$
- en: ∇R(W∗) (3.14)
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: ∇R(W∗) (3.14)
- en: for the regularization parameter λ. Thus, if we have a reasonable estimate of
    W∗, or of ∇E0(W∗) and ∇R(W∗), then we can use this to estimate λ. An appealingly
    simple way of estimating W∗ is to use early stopping, because of its connection
    with weight decay.
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正则化参数λ。因此，如果我们对W∗或∇E0(W∗)和∇R(W∗)有一个合理的估计，那么我们可以用它来估计λ。一个简单而有效的W∗估计方法是使用早期停止，因为它与权重衰减有联系。
- en: Denoting the set of weights at the early stopping point by Wes, we have
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: 将早期停止点的权重集合表示为Wes，我们有
- en: $$\hat{\lambda}_{1}=\frac{\|\nabla E_{0}(\mathbf{W}_{es})\|}{\|\nabla R(\mathbf{W}_{es})\|},\tag{3.15}$$  $\hat{\lambda}_{1}=\frac{\|\nabla
    E_{0}(\mathbf{W}_{es})\|}{\|\nabla R(\mathbf{W}_{es})\|},$ (3.15)  $\hat{\lambda}_{1}=\frac{\|\nabla
    E_{0}(\mathbf{W}_{es})\|}{\|\nabla R(\mathbf{W}_{es})\|},$ (3.15)  $\hat{\lambda}_{1}=\frac{\|\nabla
    E_{0}(\mathbf{W}_{es})\|}{\|\nabla R(\mathbf{W}_{es})\|},$ (3.15)
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: $$\hat{\lambda}_{1}=\frac{\|\nabla E_{0}(\mathbf{W}_{es})\|}{\|\nabla R(\mathbf{W}_{es})\|},\tag{3.15}$$  $\hat{\lambda}_{1}=\frac{\|\nabla
    E_{0}(\mathbf{W}_{es})\|}{\|\nabla R(\mathbf{W}_{es})\|},$ (3.15)  $\hat{\lambda}_{1}=\frac{\|\nabla
    E_{0}(\mathbf{W}_{es})\|}{\|\nabla R(\mathbf{W}_{es})\|},$ (3.15)  $\hat{\lambda}_{1}=\frac{\|\nabla
    E_{0}(\mathbf{W}_{es})\|}{\|\nabla R(\mathbf{W}_{es})\|},$ (3.15)
- en: as a simple estimate for λ. A second possibility is to consider the whole set
    of linear equations defined by (3.13) and minimize the squared error
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: 作为λ的一个简单估计。第二种可能性是考虑由(3.13)定义的整个线性方程组，并最小化平方误差
- en: $$\|\nabla E_{0}(\mathbf{W}_{es})+\lambda\nabla R(\mathbf{W}_{es})\|^{2}=$$
    $$\|\nabla E_{0}(\mathbf{W}_{es})\|^{2}+2\lambda\nabla E_{0}(\mathbf{W}_{es})\cdot\nabla
    R(\mathbf{W}_{es})+\lambda^{2}\|\nabla R(\mathbf{W}_{es})\|^{2}\tag{3.16}$$
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: $$\|\nabla E_{0}(\mathbf{W}_{es})+\lambda\nabla R(\mathbf{W}_{es})\|^{2}=$$
    $$\|\nabla E_{0}(\mathbf{W}_{es})\|^{2}+2\lambda\nabla E_{0}(\mathbf{W}_{es})\cdot\nabla
    R(\mathbf{W}_{es})+\lambda^{2}\|\nabla R(\mathbf{W}_{es})\|^{2}\tag{3.16}$$
- en: '![81_image_0.png](81_image_0.png)'
  id: totrans-972
  prefs: []
  type: TYPE_IMG
  zh: '![81_image_0.png](81_image_0.png)'
- en: Fig. 3.1. Illustration of the procedure for estimating λˆopt on a hypothetical
    example.
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1。假设例子中估计λˆopt过程的说明。
- en: From the search we have a set of K lognormal distributions with means log E*nCV,k*
    and variances σ2*nCV,k*, which is illustrated in the top plate. From these K distributions,
    we sample K error values and select the λ corresponding to the minimum error value
    as "winner". This is repeated several times (100 in the figure but 10,000 times
    in the experiments in the text) collecting statistics on how often different λ
    values are winners, and the mean log λ is computed. This is illustrated in the
    bottom plate, which shows the histogram resulting from sampling 100 times. From
    this we get log λˆopt = −2.48±1.15, which gives us λ = 10−2.48 = 0.003 for training
    the "best" network.
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: 从搜索中，我们得到一组K个对数正态分布，其均值为log E*nCV,k*，方差为σ2*nCV,k*，如顶部图板所示。从这些K个分布中，我们抽取K个误差值，并选择对应于最小误差值的λ作为“赢家”。这重复多次（图中为100次，但文本中的实验为10,000次），收集不同λ值成为赢家的统计数据，并计算平均log
    λ。这在底部图板中有所示，显示了从100次采样得到的直方图。从中我们得到log λˆopt = −2.48±1.15，这给我们训练“最佳”网络的λ = 10−2.48
    = 0.003。
- en: with respect to λ. That is, solving the equation
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: 关于λ。也就是说，解方程
- en: $$\frac{\partial}{\partial\lambda}\left\{\|\nabla E_{0}(\mathbf{W}_{es})+\lambda\nabla
    R(\mathbf{W}_{es})\|^{2}\right\}=0\tag{3.17}$$
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{\partial}{\partial\lambda}\left\{\|\nabla E_{0}(\mathbf{W}_{es})+\lambda\nabla
    R(\mathbf{W}_{es})\|^{2}\right\}=0\tag{3.17}$$
- en: which gives $\theta$
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出$\theta$
- en: $$\hat{\lambda}_{2}=\max\left[0,\frac{-\nabla E_{0}(\mathbf{W}_{es})\cdot\nabla
    R(\mathbf{W}_{es})}{\|\nabla R(\mathbf{W}_{es})\|^{2}}\right].\tag{3.18}$$
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: $$\hat{\lambda}_{2}=\max\left[0,\frac{-\nabla E_{0}(\mathbf{W}_{es})\cdot\nabla
    R(\mathbf{W}_{es})}{\|\nabla R(\mathbf{W}_{es})\|^{2}}\right].\tag{3.18}$$
- en: $$(3.19)$$
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.19)$$
- en: $$(3.20)$$
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.20)$$
- en: The estimate is bound from below since λ must be positive.
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: 该估计是有下限的，因为λ必须为正。
- en: The second estimate, λˆ2, corresponds to a linear regression without intercept
    term on the set of points {∂iE0(Wes), ∂iR(Wes)}, whereas the first estimate, λˆ1,
    is closer to the ratio max[|∂iE0(Wes)|]/ max[|∂iR(Wes)|]. It follows from the
    Cauchy-Schwartz inequality that
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个估计，λˆ2，对应于没有截距项的线性回归，数据点集为{∂iE0(Wes), ∂iR(Wes)}，而第一个估计，λˆ1，则更接近于比率max[|∂iE0(Wes)|]/
    max[|∂iR(Wes)|]。根据柯西-施瓦茨不等式可得
- en: $${\hat{\lambda}}_{1}\geq{\hat{\lambda}}_{2}.$$
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: $${\hat{\lambda}}_{1}\geq{\hat{\lambda}}_{2}.$$
- en: λˆ1 ≥ λˆ2. (3.19)
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: λˆ1 ≥ λˆ2. (3.19)
- en: For the specific case of weight decay, where R(W) = W2, expressions (3.15)
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: 对于权重衰减的特定情况，其中R(W) = W2，表达式(3.15)
- en: and (3.18) become
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: 和(3.18)变为
- en: $$\hat{\lambda}_{1}=\frac{\|\nabla E_{0}(W_{e s})\|}{2\|W_{e s}\|},$$
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: $$\hat{\lambda}_{1}=\frac{\|\nabla E_{0}(W_{e s})\|}{2\|W_{e s}\|},$$
- en: 2Wes , (3.20)
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: 2Wes , (3.20)
- en: $$\hat{\lambda}_{2}=\operatorname*{max}\left[0,\frac{-\nabla E_{0}(W_{e s})\cdot
    W_{e s}}{2\|W_{e s}\|^{2}}\right].$$
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: $$\hat{\lambda}_{2}=\operatorname*{max}\left[0,\frac{-\nabla E_{0}(W_{e s})\cdot
    W_{e s}}{2\|W_{e s}\|^{2}}\right].$$
- en: $$(3.21)$$
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.21)$$
- en: . (3.21)
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: . (3.21)
- en: These estimates are sensitive to the particularities of the training and validation
    data sets used, and possibly also to the training algorithm. One must therefore
    average them over different validation and training sets. It is, however, still
    quicker to do this than to do a search since early stopping training often is
    several orders of magnitude faster to do than a full minimization of (3.7).
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: 这些估计对使用的训练和验证数据集的特性非常敏感，可能还与训练算法有关。因此，必须在不同的验证和训练集上对其进行平均。然而，这比进行搜索要快，因为提前停止训练通常比完全最小化(3.7)快几个数量级。
- en: One way to view the estimates (3.15) and (3.18) is as the weight decay parameters
    that correspond to the early stopping point. However, our aim here is not to imitate
    early stopping with weight decay, but to use early stopping to estimate the weight
    decay parameter λ. We hope that using weight decay with this λ value will actually
    result in better out-of-sample performance than what we get from doing early stopping
    (the whole exercise becomes rather meaningless if this is not the case).
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: 查看估计值 (3.15) 和 (3.18) 的一种方式是将其视为对应于早期停止点的权重衰减参数。然而，我们在这里的目标并不是通过权重衰减模仿早期停止，而是使用早期停止来估计权重衰减参数
    λ。我们希望使用这个 λ 值的权重衰减实际上会导致比进行早期停止所得到的更好的样本外表现（如果不是这样，这整个过程就变得相当没有意义）。
- en: As a sidenote, we imagine that (3.15) and (3.18) could be used also to estimate
    weight decay parameters in cases when different weight decays are used for weights
    in different layers. This would then be done by considering these estimates for
    different groups of weights.
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: 作为附带说明，我们想象 (3.15) 和 (3.18) 也可以用于估计在不同层的权重使用不同权重衰减时的权重衰减参数。这将通过考虑不同权重组的这些估计来实现。
- en: 3.4 Experiments 3.4.1 Data Sets
  id: totrans-995
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 实验 3.4.1 数据集
- en: 'We here demonstrate the performance of our algorithm on a set of five regression
    problems. For each problem, we vary either the number of inputs, the number of
    hidden units, or the amount of training data to study the effects of the numbers
    of parameters relative to the number of training data points. The five problems
    are:'
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们演示了算法在一组五个回归问题上的性能。对于每个问题，我们变化输入数量、隐藏单元数量或训练数据量，以研究参数数量与训练数据点数量之间的关系。这五个问题是：
- en: $$(3.22)$$
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.22)$$
- en: $$\phi(x_{1},x_{2})=x_{1}x_{2}.$$
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: $$\phi(x_{1},x_{2})=x_{1}x_{2}.$$
- en: Synthetic Bilinear Problem. The task is to model a bilinear function of the
    form φ(x1, x2) = x1x2. (3.22)
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: 合成双线性问题。任务是建模形式为 φ(x1, x2) = x1x2 的双线性函数。(3.22)
- en: We use three different sizes of training data sets, M ∈ {20, 40, 100}, but a
    constant validation set size of 10 patterns. The validation patterns are in addition
    to the M training patterns. The test error, or generalization error, is computed
    by numerical integration over 201 × 201 data points on a two-dimensional lattice
    (x1, x2) ∈ [−1, 1]2. The target values (but not the inputs) are contaminated with
    three different levels of Gaussian noise with standard deviation σ ∈ {0.1, 0.2,
    0.5}. This gives a total of 3 × 3=9 different experiments on this particular problem,
    which we refer to as setup A1, A2, ..., and A9 below.
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用三种不同大小的训练数据集，M ∈ {20, 40, 100}，但验证集的大小始终为 10 个模式。验证模式是额外的，除了 M 个训练模式。测试误差或泛化误差是通过对二维晶格
    (x1, x2) ∈ [−1, 1]2 上的 201 × 201 个数据点进行数值积分计算得出的。目标值（但不是输入）被三种不同水平的标准差 σ ∈ {0.1,
    0.2, 0.5} 的高斯噪声污染。这在这个特定问题上给出了总共 3 × 3=9 个不同的实验，我们在下面称之为设置 A1, A2, ..., 和 A9。
- en: This allows controlled studies w.r.t. noise levels and training set sizes, while
    keeping the network architecture constant (2 inputs, 8 tanh hidden, and one linear
    output). Predicting Puget Sound Power and Light Co. Power Load between 7 and 8
    a.m. the Following Day. This data set is taken from the Puget Sound Power and
    Light Co's power prediction competition [3]. The winner of this competition used
    a set of linear models, one for each hour of the day. We have selected the subproblem
    of predicting the load between 7 and 8 a.m. 24 hrs. in advance. This hour shows
    the largest variation in power load. The training set consists of 844 weekdays
    between January 1985 and September 1990. Of these, 150 days are randomly selected
    and used for validation. We use 115 winter weekdays, from between November 1990
    and March 1992, for out-of-sample testing.
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许对噪声水平和训练集大小进行受控研究，同时保持网络架构不变（2 个输入、8 个 tanh 隐藏单元和一个线性输出）。预测次日早上 7 点到 8 点的普吉特海湾电力公司电力负荷。该数据集来自普吉特海湾电力公司的电力预测比赛[3]。该比赛的获胜者使用了一组线性模型，每个小时一个。我们选择了预测在提前
    24 小时的早上 7 点到 8 点之间负荷的子问题。这个小时显示了电力负荷最大的变化。训练集包含 1985 年 1 月至 1990 年 9 月之间的 844
    个工作日。其中 150 天被随机选择用于验证。我们使用 115 个冬季工作日，时间在 1990 年 11 月至 1992 年 3 月之间，进行样本外测试。
- en: 'The inputs are things like current load, average load during the last 24 hours,
    average load during the last week, time of the year, etc., giving a total of 15
    inputs. Three different numbers of internal units are tried on this task: 15,
    10, and 5, and we refer to these experiments as B1, B2, and B3 below.'
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: 输入包括当前负载、过去 24 小时的平均负载、过去一周的平均负载、年度时间等，共计 15 个输入。在这个任务上尝试三种不同数量的内部单元：15、10 和
    5，我们将这些实验称为 B1、B2 和 B3。
- en: Predicting Daily Riverflow in Two Icelandic Rivers. This problem is tabulated
    in [22], and the task is to model tomorrow's average flow of water in one of two
    Icelandic rivers, knowing today's and previous days' waterflow, temperature, and
    precipitation. The training set consists of 731 data points, corresponding to
    the years 1972 and 1973, out of which we randomly sample 150 datapoints for validation.
    The test set has 365 data points (the year 1974). We use two different lengths
    of lags, 8 or 4 days back, which correspond to 24 or 12 inputs, while the number
    of internal units is kept constant at 12. These experiments are referred to as
    C1, C2, C3, and C4 below.
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: 预测两条冰岛河流的日流量。这个问题在 [22] 中进行了表格化，任务是根据今天和前几天的水流、温度和降水量，模型化明天一条冰岛河流的平均水流。训练集包含
    731 个数据点，对应于 1972 和 1973 年，其中我们随机抽取 150 个数据点进行验证。测试集有 365 个数据点（1974 年）。我们使用两种不同的滞后长度，向后
    8 或 4 天，分别对应 24 或 12 个输入，同时保持内部单元数量恒定为 12。这些实验被称为 C1、C2、C3 和 C4。
- en: Predicting the Wolf Sunspots Time Series. This time series has been used several
    times in the context of demonstrating new regularization techniques, for instance
    by [15] and [26]. We try three different network architectures on this problem,
    always keeping 12 input units but using 4, 8, or 12 internal units in the network.
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: 预测狼日斑时间序列。这个时间序列在展示新正则化技术的上下文中已经被多次使用，例如 [15] 和 [26]。我们在这个问题上尝试三种不同的网络架构，总是保持
    12 个输入单元，但在网络中使用 4、8 或 12 个内部单元。
- en: 'These experiments are referred to as setup D1, D2, and D3 below. The training
    set size is kept constant at M = 221 (years 1700-1920), out of which we randomly
    pick 22 patterns for validation. We test our models under four different conditions:
    Single step prediction on "test set 1" with 35 data points (years 1921-1955),
    4-step iterated prediction on "test set 1", 8-step iterated prediction on all
    74 available test years (1921-1994), and 11-step iterated prediction on all available
    test years. These test conditions are coded as s1, m4, m8, and m11.'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验被称为设置 D1、D2 和 D3。训练集大小保持恒定为 M = 221（年份 1700-1920），其中我们随机选择 22 个模式进行验证。我们在四种不同条件下测试我们的模型：在“测试集
    1”上进行单步预测，包含 35 个数据点（年份 1921-1955），在“测试集 1”上进行 4 步迭代预测，在所有 74 个可用测试年份（1921-1994）上进行
    8 步迭代预测，以及在所有可用测试年份上进行 11 步迭代预测。这些测试条件被编码为 s1、m4、m8 和 m11。
- en: 'Estimating the Peak Pressure Position in a Combustion Engine. This is a data
    set with 4 input variables (ignition time, engine load, engine speed, and air/fuel
    ratio) and only 49 training data points, out of which we randomly pick 9 patterns
    for validation. The test set consists of 35 data points, which have been measured
    under slightly different conditions than the training data. We try four different
    numbers of internal units on this task: 2, 4, 8, or 12, and refer to these experiments
    as E1, E2, E3, and E4.'
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: 估计 combustion engine 中的峰值压力位置。这个数据集有 4 个输入变量（点火时间、发动机负载、发动机转速和空气/燃料比），只有 49
    个训练数据点，其中我们随机选择 9 个模式进行验证。测试集包含 35 个数据点，这些数据点是在与训练数据稍有不同的条件下测量的。我们在这个任务上尝试四种不同数量的内部单元：2、4、8
    或 12，并将这些实验称为 E1、E2、E3 和 E4。
- en: 3.4.2 Experimental Procedure
  id: totrans-1007
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4.2 实验程序
- en: 'The experimental procedure is the same for all problems: We begin by estimating
    λ in the "traditional" way by searching over the region log λ ∈ [−6.5, 1.0] in
    steps of Δ log λ = 0.5. For each λ value, we train 10 networks using the Rprop
    training algorithm 3 [18]. Each network is trained until the total error (3.7)
    is minimized, measured by'
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
  zh: 实验程序对所有问题都是相同的：我们首先以“传统”的方式估计 λ，通过在区域 log λ ∈ [−6.5, 1.0] 中以 Δ log λ = 0.5 的步长进行搜索。对于每个
    λ 值，我们使用 Rprop 训练算法训练 10 个网络 [18]。每个网络的训练直到总误差 (3.7) 最小化为止，测量方法为
- en: $$\log\left[{\frac{1}{100}}\sum_{i=1}^{100}{\frac{|\Delta E_{i}|}{\|\Delta W_{i}\|}}\right]<-5,$$
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: $$\log\left[{\frac{1}{100}}\sum_{i=1}^{100}{\frac{|\Delta E_{i}|}{\|\Delta W_{i}\|}}\right]<-5,$$
- en: $$(3.23)$$
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.23)$$
- en: < −5, (3.23)
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: < −5, (3.23)
- en: where the sum runs over the most recent 100 epochs, or until 105 epochs have
    passed, whichever occurs first. The convergence criterion (3.23) is usually fulfilled
    within 105 epochs. New validation and training sets are sampled for each of the
    10 networks, but the different validation sets are allowed to overlap. Means and
    standard deviations, log E*nCV,k* and σ*nCV,k*, for the errors are estimated from
    these 10 network runs, assuming a lognormal distribution for the validation errors.
    Figure 3.2 shows an example of such a search for the Wolf sunspot problem, using
    a neural network with 12 inputs, 8 internal units, and 1 linear output.
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: 其中和是最近100个纪元的总和，或者直到经过105个纪元，以先到者为准。收敛标准（3.23）通常在105个纪元内满足。每个网络的10个新验证和训练集是随机抽样的，但不同的验证集可以重叠。误差的均值和标准偏差，log
    E*nCV,k*和σ*nCV,k*，是从这10次网络运行中估计的，假设验证误差服从对数正态分布。图3.2展示了使用具有12个输入、8个内部单元和1个线性输出的神经网络进行Wolf太阳黑子问题的搜索示例。
- en: '![84_image_0.png](84_image_0.png)'
  id: totrans-1013
  prefs: []
  type: TYPE_IMG
  zh: '![84_image_0.png](84_image_0.png)'
- en: 'Fig. 3.2. Left panel: Training and validation errors on the Wolf sunspot time
    series, setup D2, plotted versus the weight decay parameter λ. Each point corresponds
    to an average over 10 runs with different validation and training sets. The error
    bars mark 95% confidence limits for the average validation and training errors,
    under the assumption that the errors are lognormally distributed. The objective
    Monte Carlo method gives log λˆopt = −2.00 ± 0.31. Right panel: The corresponding
    plot for the test error on the sunpots "test set 1". The network architecture
    is 12 inputs, 8 tanh internal units, and 1 linear output.'
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2. 左侧面板：在Wolf太阳黑子时间序列上，设置D2的训练和验证误差，按权重衰减参数λ绘制。每个点对应于10次不同验证和训练集的平均值。误差条标记了假设误差为对数正态分布时平均验证和训练误差的95%置信限。客观蒙特卡罗方法给出的log
    λˆopt = −2.00 ± 0.31。右侧面板：在太阳黑子“测试集1”上的测试误差的相应图。
- en: Using the objective Monte Carlo method described above, we estimate an optimal
    λˆopt value from this search. This value is then used to train 10 new
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述客观蒙特卡罗方法，我们从此次搜索中估计出最佳的λˆopt值。该值随后用于训练10个新的网络。
- en: 3 Initial tests showed that the Rprop algorithm was considerably more efficient
    and robust than e.g. backprop or conjugate gradients in minimizing the error.
    We did not, however, try true second order algorithms like Levenberg-Marquardt
    or QuasiNewton.
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: 初步测试表明，Rprop算法在最小化误差方面显著比例如反向传播或共轭梯度法更有效和稳健。然而，我们没有尝试真正的二阶算法，如Levenberg-Marquardt或QuasiNewton。
- en: '![85_image_0.png](85_image_0.png)'
  id: totrans-1017
  prefs: []
  type: TYPE_IMG
  zh: '![85_image_0.png](85_image_0.png)'
- en: 'Fig. 3.3. Left panel: Histogram showing the estimated values λˆ1 for 100 different
    training runs, using different training and validation sets each time. Right panel:
    Similar histogram for λˆ2. The problem (D2) is the same as that depicted in Figure
    3.2.'
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3. 左侧面板：显示100个不同训练运行的估计值λˆ1的直方图，每次使用不同的训练和验证集。右侧面板：λˆ2的类似直方图。问题（D2）与图3.2中所示的相同。
- en: networks with all the training data (no validation set). The test errors for
    these networks are then computed using the held out test set.
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有训练数据（没有验证集）的网络。这些网络的测试误差随后使用保留的测试集进行计算。
- en: A total of 16 × 10 = 160 network runs are thus done to select the λˆopt for
    each experiment. This corresponds to a few days' or a week's work, depending on
    available hardware and the size of the problem. Although this is in excess of
    what is really needed in practice (one could get away with about half as many
    runs in a real application) the time spent doing this is aggravating. The times
    needed for doing the searches described in this paper ranged from 10 up to 400
    cpu-hours, depending on the problem and the computer4. For comparison, the early
    stopping experiments described below took between 10 cpu-minutes and 14 cpu-hours.
    There was typically a ratio of 40 between the time needed for a search and the
    time needed for an early stopping estimate.
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总共进行了16 × 10 = 160次网络运行，以为每个实验选择λˆopt。这相当于几天或一周的工作，具体取决于可用的硬件和问题的规模。尽管这超过了实际应用中真正需要的数量（在实际应用中，约一半的运行次数就足够了），但所花的时间仍然令人沮丧。本文中描述的搜索所需时间从10到400
    cpu小时不等，具体取决于问题和计算机4。作为对比，下面描述的早期停止实验所需时间介于10 cpu分钟和14 cpu小时之间。搜索所需的时间与早期停止估计所需时间之间通常存在40倍的比率。
- en: We then estimate λˆ1 and λˆ2, by training 100 networks with early stopping.
    One problem here is that the stopping point is ill-defined, i.e. the first observed
    minimum in the validation error is not necessarily the minimum where one should
    stop.
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们通过训练100个带有早期停止的网络来估计λˆ1和λˆ2。这里的一个问题是停止点定义不明确，即在验证错误中观察到的第一个最小值不一定是应该停止的最小值。
- en: The validation error quite often decreases again beyond this point. To avoid
    such problems, we keep a record of the weights corresponding to the latest minimum
    validation error and continue training beyond that point. The training is stopped
    when as many epochs have passed as it took to find the validation error minimum
    without encountering a new minimum. The weights corresponding to the last validation
    error minimum are then used as the early stopping weights. For example, if the
    validation error has a minimum at say 250 epochs, we then wait until a total of
    500 epochs have passed before deciding on that particular stopping point. From
    the 100 networks, we get 100 estimates for λˆ1 and λˆ2. We take the logarithm
    of these and compute means log λˆ1 and log λˆ2, and corresponding standard deviations.
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 验证错误在这一点之后往往会再次下降。为了避免这种问题，我们记录与最新最小验证错误对应的权重，并在该点之后继续训练。当经过的轮数与找到验证错误最小值所需的轮数相同且没有遇到新最小值时，训练将停止。最后验证错误最小值对应的权重将作为早期停止权重使用。例如，如果验证错误在250轮时达到最小值，我们将等待总共500轮结束后再决定这一特定的停止点。从100个网络中，我们获得了100个对λˆ1和λˆ2的估计。我们取这些值的对数并计算均值log
    λˆ1和log λˆ2，以及相应的标准偏差。
- en: 4 A variety of computers were used for the simulations, including NeXT, Sun
    Sparc, DEC Alpha, and Pentium computers running Solaris.
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: 4 在模拟中使用了多种计算机，包括NeXT、Sun Sparc、DEC Alpha和运行Solaris的Pentium计算机。
- en: The resulting arithmetic mean values are taken as the estimates for λ and the
    standard deviations are used as measures of the estimation error. The arithmetic
    means are then used to train 10 networks which use all the training data. Figure
    3.3 shows the histograms corresponding to the problem presented in Figure 3.2.
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的算术平均值被作为λ的估计值，标准偏差则用作估计误差的度量。然后使用这些算术平均值训练10个使用所有训练数据的网络。图3.3显示了与图3.2中呈现的问题对应的直方图。
- en: When comparing test errors achieved with different methods, we use the Wilcoxon
    rank test [13], also called the Mann-Whitney test, and report differences at 95%
    confidence level.
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较不同方法得到的测试错误时，我们使用Wilcoxon秩检验[13]，也称为Mann-Whitney检验，并报告95%置信水平下的差异。
- en: 3.4.3 Quality of the λ Estimates As a first test of the quality of the estimates
    λˆ1 and λˆ2, we check how well they agree with the λˆopt estimate, which can be
    considered a "truth". The estimates for all the problem setups are tabulated in
    table 3.1 and plotted in Figure 3.4.
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: 3.4.3 λ估计的质量 作为对λˆ1和λˆ2估计质量的第一次测试，我们检查它们与λˆopt估计的符合程度，这可以视为一种“真值”。所有问题设置的估计值在表3.1中列出，并在图3.4中绘制。
- en: Table 3.1. Estimates of λ for the 23 different problem setups. Code A corresponds
    to the synthetic problem, code B to the Power prediction, code C to the riverflow
    prediction, code D to the Sunspots series, and code E to the maximum pressure
    position problem. For the log λˆopt column, errors are the standard deviations
    of the Monte Carlo estimate. For the early stopping estimates, errors are the
    standard deviations of the estimates.
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1. 针对23种不同问题设置的λ估计。代码A对应于合成问题，代码B对应于电力预测，代码C对应于河流流量预测，代码D对应于太阳黑子系列，代码E对应于最大压力位置问题。对于log
    λˆopt列，误差是Monte Carlo估计的标准偏差。对于早期停止估计，误差是估计的标准偏差。
- en: '| Problem                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
    log λˆopt                              | log λˆ1   | log λˆ2   |'
  id: totrans-1028
  prefs: []
  type: TYPE_TB
  zh: '| 问题                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
    log λˆopt                              | log λˆ1   | log λˆ2   |'
- en: '| --- | --- | --- | --- |'
  id: totrans-1029
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| A1 (M = 20, σ = 0.1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    -2.82 ± 0.04 -2.71 ± 0.66 -3.44 ± 1.14 |           |           |'
  id: totrans-1030
  prefs: []
  type: TYPE_TB
  zh: '| A1 (M = 20, σ = 0.1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    -2.82 ± 0.04 -2.71 ± 0.66 -3.44 ± 1.14 |           |           |'
- en: '| A2 (M = 20, σ = 0.2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    -2.67 ± 0.42 -2.32 ± 0.58 -3.20 ± 0.96 |           |           |'
  id: totrans-1031
  prefs: []
  type: TYPE_TB
  zh: '| A2 (M = 20, σ = 0.2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    -2.67 ± 0.42 -2.32 ± 0.58 -3.20 ± 0.96 |           |           |'
- en: '| A3 (M = 20, σ = 0.5)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    -0.49 ± 1.01 -1.93 ± 0.78 -3.14 ± 1.15 |           |           |'
  id: totrans-1032
  prefs: []
  type: TYPE_TB
  zh: '| A3 (M = 20, σ = 0.5)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    -0.49 ± 1.01 -1.93 ± 0.78 -3.14 ± 1.15 |           |           |'
- en: '| A4 (M = 40, σ = 0.1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    -2.93 ± 0.49 -2.85 ± 0.73 -3.56 ± 0.87 |           |           |'
  id: totrans-1033
  prefs: []
  type: TYPE_TB
  zh: '| A4 (M = 40, σ = 0.1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    -2.93 ± 0.49 -2.85 ± 0.73 -3.56 ± 0.87 |           |           |'
- en: '| A5 (M = 40, σ = 0.2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    -2.53 ± 0.34 -2.41 ± 0.64 -2.91 ± 0.68 |           |           |'
  id: totrans-1034
  prefs: []
  type: TYPE_TB
  zh: '| A5 (M = 40, σ = 0.2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    -2.53 ± 0.34 -2.41 ± 0.64 -2.91 ± 0.68 |           |           |'
- en: '| A6 (M = 40, σ = 0.5)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    -2.43 ± 0.44 -2.13 ± 0.74 -2.85 ± 0.77 |           |           |'
  id: totrans-1035
  prefs: []
  type: TYPE_TB
  zh: '| A6 (M = 40, σ = 0.5)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    -2.43 ± 0.44 -2.13 ± 0.74 -2.85 ± 0.77 |           |           |'
- en: '| A7 (M = 100, σ = 0.1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
    -3.45 ± 0.78 -3.01 ± 0.86 -3.74 ± 0.93 |           |           |'
  id: totrans-1036
  prefs: []
  type: TYPE_TB
  zh: '| A7 (M = 100, σ = 0.1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
    -3.45 ± 0.78 -3.01 ± 0.86 -3.74 ± 0.93 |           |           |'
- en: '| A8 (M = 100, σ = 0.2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
    -3.34 ± 0.71 -2.70 ± 0.73 -3.33 ± 0.92 |           |           |'
  id: totrans-1037
  prefs: []
  type: TYPE_TB
  zh: '| A8 (M = 100, σ = 0.2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
    -3.34 ± 0.71 -2.70 ± 0.73 -3.33 ± 0.92 |           |           |'
- en: '| A9 (M = 100, σ = 0.5)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
    -3.31 ± 0.82 -2.34 ± 0.63 -3.13 ± 1.06 |           |           |'
  id: totrans-1038
  prefs: []
  type: TYPE_TB
  zh: '| A9（M = 100, σ = 0.5）                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
    -3.31 ± 0.82 -2.34 ± 0.63 -3.13 ± 1.06 |           |           |'
- en: '| B1 (Power, 15 hidden)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
    -3.05 ± 0.21 -3.82 ± 0.42 -5.20 ± 0.70 |           |           |'
  id: totrans-1039
  prefs: []
  type: TYPE_TB
  zh: '| B1（功率，15个隐藏层）                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
    -3.05 ± 0.21 -3.82 ± 0.42 -5.20 ± 0.70 |           |           |'
- en: '| B2 (Power, 10 hidden)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
    -3.57 ± 0.35 -3.75 ± 0.45 -4.93 ± 0.50 |           |           |'
  id: totrans-1040
  prefs: []
  type: TYPE_TB
  zh: '| B2（功率，10个隐藏层）                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
    -3.57 ± 0.35 -3.75 ± 0.45 -4.93 ± 0.50 |           |           |'
- en: '| B3 (Power, 5 hidden)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    -4.35 ± 0.66 -3.78 ± 0.52 -5.03 ± 0.74 |           |           |'
  id: totrans-1041
  prefs: []
  type: TYPE_TB
  zh: '| B3 (功率, 5隐藏)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    -4.35 ± 0.66 -3.78 ± 0.52 -5.03 ± 0.74 |           |           |'
- en: '| C1 (Jökulsá Eystra, 8 lags) -2.50 ± 0.10 -3.10 ± 0.33 -4.57 ± 0.59 C2 (Jökulsá
    Eystra, 4 lags) -2.53 ± 0.12 -3.15 ± 0.40 -4.20 ± 0.59 C3 (Vatnsdalsá, 8 lags)
    -2.48 ± 0.11 -2.65 ± 0.40 -3.92 ± 0.56 C4 (Vatnsdalsá, 4 lags) -2.39 ± 0.55 -2.67
    ± 0.45 -3.70 ± 0.62 D1 (Sunspots, 12 hidden) -2.48 ± 0.12 -2.48 ± 0.50 -3.70 ±
    0.42 D2 (Sunspots, 8 hidden) -2.00 ± 0.31 -2.43 ± 0.45 -3.66 ± 0.60 D3 (Sunspots,
    4 hidden) -2.51 ± 0.44 -2.39 ± 0.48 -3.54 ± 0.65 E1 (Pressure, 12 hidden) -3.13
    ± 0.43 -3.03 ± 0.70 -4.69 ± 0.91 E2 (Pressure, 8 hidden) -3.01 ± 0.52 -3.02 ±
    0.64 -4.72 ± 0.82 E3 (Pressure, 4 hidden) -3.83 ± 0.80 -3.07 ± 0.71 -4.50 ± 1.24
    E4 (Pressure, 2 hidden) -4.65 ± 0.78 -3.46 ± 1.34 -4.21 ± 1.40 |                                        |           |           |'
  id: totrans-1042
  prefs: []
  type: TYPE_TB
  zh: '| C1 (约库尔萨·埃斯特拉, 8滞后) -2.50 ± 0.10 -3.10 ± 0.33 -4.57 ± 0.59 C2 (约库尔萨·埃斯特拉,
    4滞后) -2.53 ± 0.12 -3.15 ± 0.40 -4.20 ± 0.59 C3 (瓦茨达尔萨, 8滞后) -2.48 ± 0.11 -2.65
    ± 0.40 -3.92 ± 0.56 C4 (瓦茨达尔萨, 4滞后) -2.39 ± 0.55 -2.67 ± 0.45 -3.70 ± 0.62 D1
    (太阳黑子, 12隐藏) -2.48 ± 0.12 -2.48 ± 0.50 -3.70 ± 0.42 D2 (太阳黑子, 8隐藏) -2.00 ± 0.31
    -2.43 ± 0.45 -3.66 ± 0.60 D3 (太阳黑子, 4隐藏) -2.51 ± 0.44 -2.39 ± 0.48 -3.54 ± 0.65
    E1 (压力, 12隐藏) -3.13 ± 0.43 -3.03 ± 0.70 -4.69 ± 0.91 E2 (压力, 8隐藏) -3.01 ± 0.52
    -3.02 ± 0.64 -4.72 ± 0.82 E3 (压力, 4隐藏) -3.83 ± 0.80 -3.07 ± 0.71 -4.50 ± 1.24
    E4 (压力, 2隐藏) -4.65 ± 0.78 -3.46 ± 1.34 -4.21 ± 1.40 |                                        |           |           |'
- en: The linear correlation between log λˆ1 and log λˆopt is 0.71, which is more
    than three standard deviations larger than the expected correlation between 23
    random points. Furthermore, a linear regression with intercept gives the result
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: log λˆ1 和 log λˆopt 之间的线性相关性为 0.71，超过了 23 个随机点之间期望相关性的三个标准差。此外，带有截距的线性回归结果为
- en: $$\hat{\lambda}_{o p t}=0.30+1.13\hat{\lambda}_{1}.$$
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: $$\hat{\lambda}_{o p t}=0.30+1.13\hat{\lambda}_{1}.$$
- en: $$(3.24)$$
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.24)$$
- en: λˆopt = 0.30 + 1.13λˆ1. (3.24)
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: λˆopt = 0.30 + 1.13λˆ1. (3.24)
- en: Thus, λˆ1 is a fairly good estimator of λˆopt.
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，λˆ1 是 λˆopt 的一个相当好的估计量。
- en: The linear correlation between λˆ2 and λˆopt is 0.48, more than two standard
    deviations from the random correlation. A linear regression gives
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: λˆ2 和 λˆopt 之间的线性相关性为 0.48，超过了与随机相关性的两个标准差。线性回归得出
- en: $$\hat{\lambda}_{o p t}=-0.66+0.57\hat{\lambda}_{2},$$
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
  zh: $$\hat{\lambda}_{o p t}=-0.66+0.57\hat{\lambda}_{2},$$
- en: λˆopt = −0.66 + 0.57λˆ2, (3.25)
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: λˆopt = −0.66 + 0.57λˆ2, (3.25)
- en: and the second estimator λˆ2 is clearly a less good estimator of λˆopt.
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: 而第二个估计量 λˆ2 显然是 λˆopt 的一个不太好的估计量。
- en: '![87_image_0.png](87_image_0.png)'
  id: totrans-1052
  prefs: []
  type: TYPE_IMG
  zh: '![87_image_0.png](87_image_0.png)'
- en: $$(3.25)$$
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
  zh: $$(3.25)$$
- en: 'Fig. 3.4. Plot of the results in Table 3.1. Left plate: The λˆ1 estimate plotted
    versus λˆopt. The linear correlation between log λˆ1 and log λˆopt is 0.71. Right
    plate: λˆ2 plotted versus λˆopt. The linear correlation between log λˆ2 and log
    λˆopt is 0.48. The sizes of the crosses correspond to the error bars in Table
    3.1.'
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4. 表 3.1 中结果的绘图。左图：λˆ1 估计值与 λˆopt 的绘制。log λˆ1 和 log λˆopt 之间的线性相关性为 0.71。右图：λˆ2
    与 λˆopt 的绘制。log λˆ2 和 log λˆopt 之间的线性相关性为 0.48。交叉的大小对应于表 3.1 中的误差条。
- en: We next compare the out-of-sample performances of these different λ estimates,
    which is what really matters to the practitioner. Table 3.2 lists the differences
    in out-of-sample performance when using the early stopping estimates or the search
    estimate. A "+" means that using the early stop estimate results in significantly
    (95% significance level) lower test error than if λˆopt is used. Similarly, a
    "–" means that the search estimate gives significantly lower test error than the
    early stopping estimates. A "0" means there is no significant difference.
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们比较这些不同 λ 估计的样本外性能，这对从业者来说才是真正重要的。表 3.2 列出了使用早停估计或搜索估计时的样本外性能差异。“+”表示使用早停估计导致的测试误差显著（95%
    显著性水平）低于使用 λˆopt 的情况。类似地，“–”表示搜索估计的测试误差显著低于早停估计。“0”表示没有显著差异。
- en: The conclusion from Table 3.2 is that λˆ2 is significantly worse than λˆopt,
    but that there is no consistent difference between λˆ1 and λˆopt. The two estimates
    are essentially equal, in terms of test error. In some cases, like the power prediction
    problem, it would have been beneficial to do a small search around the early stop
    estimate to check for a possibly better value.
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: 从表 3.2 的结论是，λˆ2 显著劣于 λˆopt，但 λˆ1 和 λˆopt 之间没有一致的差异。这两个估计在测试误差方面基本相等。在某些情况下，比如功率预测问题，围绕早停估计进行小范围搜索将是有益的，以检查是否存在更好的值。
- en: The test errors for the combustion engine (setups E) are not included in Tables
    3.2 (and 3.3) because the test set is too different from the training set to provide
    relevant results. In fact, no regularized network is significantly better than
    an unregularized network on this problem.
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: 由于测试集与训练集差异太大，导致不能提供相关结果，燃烧发动机（设置 E）的测试误差未包含在表 3.2（和 3.3）中。实际上，在这个问题上，没有正则化网络显著优于无正则化网络。
- en: 'Table 3.2. Relative performance of single networks trained using the estimates
    λˆ1 and λˆ2, for the weight decay parameter, and the performance of single networks
    trained using the search estimate λˆopt. The relative performances are reported
    as: "+" means that using λˆi results in a test error which is significantly lower
    than what the search estimate λˆopt gives, "0" means that the performances are
    equivalent, and "–" means that using λˆopt results in a lower test error than
    when using λˆi. All results are reported for a 95% confidence level when using
    the Wilcoxon test. See the text on why the E'
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.2. 使用估计 λˆ1 和 λˆ2 训练的单个网络相对性能，针对权重衰减参数，以及使用搜索估计 λˆopt 训练的单个网络的性能。相对性能报告为：“+”
    表示使用 λˆi 的测试误差显著低于搜索估计 λˆopt 的结果，“0” 表示性能相当，“–” 表示使用 λˆopt 的测试误差低于使用 λˆi 的情况。所有结果都是在
    95% 的置信水平下使用 Wilcoxon 测试报告的。有关 E 的具体信息，请参阅文本。
- en: results are left out.
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: 结果被省略。
- en: '| Problem Setup                | λˆ1 vs. λˆopt λˆ2 vs. λˆopt   |    |'
  id: totrans-1060
  prefs: []
  type: TYPE_TB
  zh: '| 问题设置                | λˆ1 与 λˆopt λˆ2 与 λˆopt   |    |'
- en: '| --- | --- | --- |'
  id: totrans-1061
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| A1 (M = 20, σ = 0.1)         | 0                             | 0  |'
  id: totrans-1062
  prefs: []
  type: TYPE_TB
  zh: '| A1 (M = 20, σ = 0.1)         | 0                             | 0  |'
- en: '| A2 (M = 20, σ = 0.2)         | 0                             | -  |'
  id: totrans-1063
  prefs: []
  type: TYPE_TB
  zh: '| A2 (M = 20, σ = 0.2)         | 0                             | -  |'
- en: '| A3 (M = 20, σ = 0.5)         | 0                             | 0  |'
  id: totrans-1064
  prefs: []
  type: TYPE_TB
  zh: '| A3 (M = 20, σ = 0.5)         | 0                             | 0  |'
- en: '| A4 (M = 40, σ = 0.1)         | 0                             | 0  |'
  id: totrans-1065
  prefs: []
  type: TYPE_TB
  zh: '| A4 (M = 40, σ = 0.1)         | 0                             | 0  |'
- en: '| A5 (M = 40, σ = 0.2)         | 0                             | -  |'
  id: totrans-1066
  prefs: []
  type: TYPE_TB
  zh: '| A5 (M = 40, σ = 0.2)         | 0                             | -  |'
- en: '| A6 (M = 40, σ = 0.5)         | 0                             | 0  |'
  id: totrans-1067
  prefs: []
  type: TYPE_TB
  zh: '| A6 (M = 40, σ = 0.5)         | 0                             | 0  |'
- en: '| A7 (M = 100, σ = 0.1)        | -                             | 0  |'
  id: totrans-1068
  prefs: []
  type: TYPE_TB
  zh: '| A7 (M = 100, σ = 0.1)        | -                             | 0  |'
- en: '| A8 (M = 100, σ = 0.2)        | 0                             | 0  |'
  id: totrans-1069
  prefs: []
  type: TYPE_TB
  zh: '| A8 (M = 100, σ = 0.2)        | 0                             | 0  |'
- en: '| A9 (M = 100, σ = 0.5)        | +                             | 0  |'
  id: totrans-1070
  prefs: []
  type: TYPE_TB
  zh: '| A9 (M = 100, σ = 0.5)        | +                             | 0  |'
- en: '| B1 (Power, 15 hidden)        | -                             | -  |'
  id: totrans-1071
  prefs: []
  type: TYPE_TB
  zh: '| B1 (功率, 15 个隐藏层)        | -                             | -  |'
- en: '| B2 (Power, 10 hidden)        | -                             | -  |'
  id: totrans-1072
  prefs: []
  type: TYPE_TB
  zh: '| B2 (功率, 10 个隐藏层)        | -                             | -  |'
- en: '| B3 (Power, 5 hidden)         | +                             | -  |'
  id: totrans-1073
  prefs: []
  type: TYPE_TB
  zh: '| B3 (功率, 5 个隐藏层)         | +                             | -  |'
- en: '| C1 (Jökulsá Eystra, 8 lags)  | -                             | -  |'
  id: totrans-1074
  prefs: []
  type: TYPE_TB
  zh: '| C1 (Jökulsá Eystra, 8 延迟)  | -                             | -  |'
- en: '| C2 (Jökulsá Eystra, 4 lags)  | 0                             | -  |'
  id: totrans-1075
  prefs: []
  type: TYPE_TB
  zh: '| C2 (Jökulsá Eystra, 4 延迟)  | 0                             | -  |'
- en: '| C3 (Vatnsdalsá, 8 lags)      | -                             | -  |'
  id: totrans-1076
  prefs: []
  type: TYPE_TB
  zh: '| C3 (Vatnsdalsá, 8 延迟)      | -                             | -  |'
- en: '| C4 (Vatnsdalsá, 4 lags)      | 0                             | -  |'
  id: totrans-1077
  prefs: []
  type: TYPE_TB
  zh: '| C4 (Vatnsdalsá, 4 个滞后)      | 0                             | -  |'
- en: '| D1.s1 (Sunspots, 12 hidden)  | 0                             | -  |'
  id: totrans-1078
  prefs: []
  type: TYPE_TB
  zh: '| D1.s1 (太阳黑子, 12 个隐藏)  | 0                             | -  |'
- en: '| D2.s1 (Sunspots, 8 hidden)   | +                             | -  |'
  id: totrans-1079
  prefs: []
  type: TYPE_TB
  zh: '| D2.s1 (太阳黑子, 8 个隐藏)   | +                             | -  |'
- en: '| D3.s1 (Sunspots, 4 hidden)   | 0                             | -  |'
  id: totrans-1080
  prefs: []
  type: TYPE_TB
  zh: '| D3.s1 (太阳黑子, 4 个隐藏)   | 0                             | -  |'
- en: '| D1.m4 (Sunspots, 12 hidden)  | 0                             | -  |'
  id: totrans-1081
  prefs: []
  type: TYPE_TB
  zh: '| D1.m4 (太阳黑子, 12 个隐藏)  | 0                             | -  |'
- en: '| D2.m4 (Sunspots, 8 hidden)   | +                             | -  |'
  id: totrans-1082
  prefs: []
  type: TYPE_TB
  zh: '| D2.m4 (太阳黑子, 8 个隐藏)   | +                             | -  |'
- en: '| D3.m4 (Sunspots, 4 hidden)   | +                             | -  |'
  id: totrans-1083
  prefs: []
  type: TYPE_TB
  zh: '| D3.m4 (太阳黑子, 4 个隐藏)   | +                             | -  |'
- en: '| D1.m8 (Sunspots, 12 hidden)  | 0                             | -  |'
  id: totrans-1084
  prefs: []
  type: TYPE_TB
  zh: '| D1.m8 (太阳黑子, 12 个隐藏)  | 0                             | -  |'
- en: '| D2.m8 (Sunspots, 8 hidden)   | -                             | -  |'
  id: totrans-1085
  prefs: []
  type: TYPE_TB
  zh: '| D2.m8 (太阳黑子, 8 个隐藏)   | -                             | -  |'
- en: '| D3.m8 (Sunspots, 4 hidden)   | +                             | -  |'
  id: totrans-1086
  prefs: []
  type: TYPE_TB
  zh: '| D3.m8 (太阳黑子, 4 个隐藏)   | +                             | -  |'
- en: '| D1.m11 (Sunspots, 12 hidden) | 0                             | 0  |'
  id: totrans-1087
  prefs: []
  type: TYPE_TB
  zh: '| D1.m11 (太阳黑子, 12 个隐藏) | 0                             | 0  |'
- en: '| D2.m11 (Sunspots, 8 hidden)  | +                             | -  |'
  id: totrans-1088
  prefs: []
  type: TYPE_TB
  zh: '| D2.m11 (太阳黑子, 8 个隐藏)  | +                             | -  |'
- en: '| D3.m11 (Sunspots, 4 hidden)  | 0                             | -  |'
  id: totrans-1089
  prefs: []
  type: TYPE_TB
  zh: '| D3.m11 (太阳黑子, 4 个隐藏)  | 0                             | -  |'
- en: 3.4.4 Weight Decay Versus Early Stopping Committees
  id: totrans-1090
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4.4 权重衰减与早停委员会
- en: Having trained all these early stopping networks, it is reasonable to ask if
    using them to estimate λ for a weight decay network is the optimal use of these
    networks? Another possible use is, for instance, to construct a committee [16]
    from them.
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练所有这些早停网络后，合理的问题是使用它们来估计 λ 以用于权重衰减网络是否是这些网络的**最佳**用途？另一种可能的用途是，例如，从它们构建一个委员会
    [16]。
- en: To test this, we compare the test errors for our regularized networks with those
    when using a committee of 10 networks trained with early stopping. The results
    are listed in Table 3.3.
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这一点，我们将我们正则化网络的测试误差与使用经过早停训练的 10 个网络的委员会的测试误差进行比较。结果列在表 3.3 中。
- en: 'Some observations from Table 3.3, bearing in mind that the set of problems
    is small, are: Early stopping committees seem like the better option when the
    problem is very noisy (setups A3, A6, and A9), and when the network does not have
    very many degrees of freedom (setups B3, C4, and D3). Weight decay networks, on
    the other hand, seem to work better than committees on problems with many degrees
    of freedom (setups B1 and C3), problems with low noise levels and much data (setup
    A7), and problems where the prediction is iterated through the network (m4, m8,
    and m11 setups). We emphasize, however, that these conclusions are drawn from
    a limited set of problems and that all problems tend to have their own set of
    weird characteristics.'
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
  zh: 从表 3.3 中的一些观察，考虑到问题集较小，有：当问题非常嘈杂时（设置 A3、A6 和 A9），早停委员会似乎是更好的选择；而当网络自由度不高时（设置
    B3、C4 和 D3）。另一方面，权重衰减网络在自由度较高的问题（设置 B1 和 C3）、低噪声水平且数据较多的问题（设置 A7），以及预测通过网络迭代的问题（m4、m8
    和 m11 设置）上表现更好。然而，我们强调，这些结论是基于有限的问题集得出的，所有问题都有其独特的特征。
- en: We also check which model works best on each problem. On the power prediction,
    the best overall model is a large network (B1) which is trained with weight decay.
    On the river prediction problems, the best models are small (C2 and C4) and trained
    with either weight decay (Jökulsá Eystra) or early stopping and then combined
    into committees (Vatnsdalsá). On the sunspot problem, the best overall model is
    a large network (D1) trained with weight decay.
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还检查了哪个模型在每个问题上表现最佳。在电力预测中，最佳整体模型是一个大型网络（B1），经过权重衰减训练。在河流预测问题中，最佳模型是小型网络（C2
    和 C4），经过权重衰减（Jökulsá Eystra）或早停训练并组合成委员会（Vatnsdalsá）。在太阳黑子问题中，最佳整体模型是经过权重衰减训练的大型网络（D1）。
- en: These networks are competitive with previous results on the same data sets.
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络与在相同数据集上之前的结果具有竞争力。
- en: The performance of the power load B1 weight decay networks, using λˆopt, are
    significantly better than what a human expert produces, and also significantly
    better than the results by the winner of the Puget Sound Power and Light Co.
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 λˆopt 的 B1 权重衰减网络在性能上显著优于人类专家的表现，也显著优于普吉特海湾电力和光明公司获胜者的结果。
- en: Power Load Competition [7], although the difference is small. The test results
    are summarized in Figure 3.5. The performance of the sunspot D1 weight decay network
    is comparable with the network by Weigend et al., listed in [26]. Figure 3.6 shows
    the performance of the D1 network trained with weight decay, λ =
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: 功率负荷竞赛 [7]，尽管差异较小。测试结果在图 3.5 中总结。太阳黑子 D1 权重衰减网络的性能与 Weigend 等人的网络相当，列在 [26]
    中。图 3.6 显示了使用权重衰减训练的 D1 网络的性能，λ =
- en: λˆ1, and compares it to the results by Weigend et al. [26]. The weight decay
    network produces these results using a considerably simpler λ selection method
    and regularization cost than the one presented in [26].
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: λˆ1，并将其与 Weigend 等人的结果进行比较 [26]。权重衰减网络使用比 [26] 中展示的简单得多的 λ 选择方法和正则化成本产生这些结果。
- en: From these anecdotal results, one could be bold and say that weight decay shows
    a slight edge over early stopping committees. However, it is fair to say that
    it is a good idea to try both committees and weight decay when constructing predictor
    models.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些轶事结果来看，可以大胆地说权重衰减相比早停委员会略有优势。然而，公平地说，在构建预测模型时，同时尝试这两种委员会和权重衰减是个好主意。
- en: It is emphasized that these results are from a small set of problems, but that
    these problems (except perhaps for the synthetic data) are all realistic in the
    sense that the datasets are small and noisy.
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: 强调这些结果来自一小组问题，但这些问题（也许合成数据除外）在数据集较小且嘈杂的意义上都是现实的。
- en: Table 3.3. Relative performance of single networks trained using weight decay
    and early stopping committees with 10 members. The relative performance of weight
    decay
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.3. 使用权重衰减和 10 个成员的早停委员会训练的单网络的相对性能。权重衰减的相对性能
- en: '(WD) and 10 member early stopping committees are reported as: "+" means that
    weight decay is significantly better than committees, "0" means that weight decay
    and committees are equivalent, and "–" means that committees are better than weight
    decay. All results are reported for a 95% confidence level when using the Wilcoxon
    test. See the text on why the E results are left out.'
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: (WD) 和 10 个成员的早停委员会的报告如下：“+”表示权重衰减显著优于委员会，“0”表示权重衰减与委员会等效，而“–”表示委员会优于权重衰减。所有结果均以
    95% 的置信水平通过 Wilcoxon 检验报告。请参阅文本了解为何 E 结果被省略。
- en: '| Problem Setup                | WD(λˆopt) vs. Comm. WD(λˆ1) vs. Comm.   |    |'
  id: totrans-1103
  prefs: []
  type: TYPE_TB
  zh: '| 问题设置                     | WD(λˆopt) vs. Comm. WD(λˆ1) vs. Comm.   |    |'
- en: '| --- | --- | --- |'
  id: totrans-1104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| A1 (M = 20, σ = 0.1)         | 0                                       |
    +  |'
  id: totrans-1105
  prefs: []
  type: TYPE_TB
  zh: '| A1 (M = 20, σ = 0.1)         | 0                                       |
    +  |'
- en: '| A2 (M = 20, σ = 0.2)         | 0                                       |
    0  |'
  id: totrans-1106
  prefs: []
  type: TYPE_TB
  zh: '| A2 (M = 20, σ = 0.2)         | 0                                       |
    0  |'
- en: '| A3 (M = 20, σ = 0.5)         | -                                       |
    -  |'
  id: totrans-1107
  prefs: []
  type: TYPE_TB
  zh: '| A3 (M = 20, σ = 0.5)         | -                                       |
    -  |'
- en: '| A4 (M = 40, σ = 0.1)         | 0                                       |
    0  |'
  id: totrans-1108
  prefs: []
  type: TYPE_TB
  zh: '| A4 (M = 40, σ = 0.1)         | 0                                       |
    0  |'
- en: '| A5 (M = 40, σ = 0.2)         | +                                       |
    +  |'
  id: totrans-1109
  prefs: []
  type: TYPE_TB
  zh: '| A5 (M = 40, σ = 0.2)         | +                                       |
    +  |'
- en: '| A6 (M = 40, σ = 0.5)         | -                                       |
    -  |'
  id: totrans-1110
  prefs: []
  type: TYPE_TB
  zh: '| A6 (M = 40, σ = 0.5)         | -                                       |
    -  |'
- en: '| A7 (M = 100, σ = 0.1)        | +                                       |
    +  |'
  id: totrans-1111
  prefs: []
  type: TYPE_TB
  zh: '| A7 (M = 100, σ = 0.1)        | +                                       |
    +  |'
- en: '| A8 (M = 100, σ = 0.2)        | 0                                       |
    0  |'
  id: totrans-1112
  prefs: []
  type: TYPE_TB
  zh: '| A8 (M = 100, σ = 0.2)        | 0                                       |
    0  |'
- en: '| A9 (M = 100, σ = 0.5)        | -                                       |
    0  |'
  id: totrans-1113
  prefs: []
  type: TYPE_TB
  zh: '| A9 (M = 100, σ = 0.5)        | -                                       |
    0  |'
- en: '| B1 (Power, 15 hidden)        | +                                       |
    -  |'
  id: totrans-1114
  prefs: []
  type: TYPE_TB
  zh: '| B1 (功率, 15 个隐藏层)        | +                                       | -  |'
- en: '| B2 (Power, 10 hidden)        | 0                                       |
    -  |'
  id: totrans-1115
  prefs: []
  type: TYPE_TB
  zh: '| B2 (功率, 10 个隐藏层)        | 0                                       | -  |'
- en: '| B3 (Power, 5 hidden)         | -                                       |
    -  |'
  id: totrans-1116
  prefs: []
  type: TYPE_TB
  zh: '| B3 (功率, 5 个隐藏层)         | -                                       | -  |'
- en: '| C1 (Jökulsá Eystra, 8 lags)  | +                                       |
    0  |'
  id: totrans-1117
  prefs: []
  type: TYPE_TB
  zh: '| C1 (Jökulsá Eystra, 8 个滞后)  | +                                       | 0  |'
- en: '| C2 (Jökulsá Eystra, 4 lags)  | +                                       |
    0  |'
  id: totrans-1118
  prefs: []
  type: TYPE_TB
  zh: '| C2 (Jökulsá Eystra, 4 个滞后)  | +                                       | 0  |'
- en: '| C3 (Vatnsdalsá, 8 lags)      | +                                       |
    +  |'
  id: totrans-1119
  prefs: []
  type: TYPE_TB
  zh: '| C3 (Vatnsdalsá, 8 个滞后)      | +                                       | +  |'
- en: '| C4 (Vatnsdalsá, 4 lags)      | -                                       |
    -  |'
  id: totrans-1120
  prefs: []
  type: TYPE_TB
  zh: '| C4 (Vatnsdalsá, 4 个滞后)      | -                                       | -  |'
- en: '| D1.s1 (Sunspots, 12 hidden)  | 0                                       |
    0  |'
  id: totrans-1121
  prefs: []
  type: TYPE_TB
  zh: '| D1.s1 (太阳黑子, 12 个隐藏层) | 0                                       | 0  |'
- en: '| D2.s1 (Sunspots, 8 hidden)   | -                                       |
    0  |'
  id: totrans-1122
  prefs: []
  type: TYPE_TB
  zh: '| D2.s1 (太阳黑子，8个隐藏层)   | -                                       | 0  |'
- en: '| D3.s1 (Sunspots, 4 hidden)   | -                                       |
    -  |'
  id: totrans-1123
  prefs: []
  type: TYPE_TB
  zh: '| D3.s1 (太阳黑子，4个隐藏层)   | -                                       | -  |'
- en: '| D1.m4 (Sunspots, 12 hidden)  | +                                       |
    +  |'
  id: totrans-1124
  prefs: []
  type: TYPE_TB
  zh: '| D1.m4 (太阳黑子，12个隐藏层)  | +                                       | +  |'
- en: '| D2.m4 (Sunspots, 8 hidden)   | +                                       |
    +  |'
  id: totrans-1125
  prefs: []
  type: TYPE_TB
  zh: '| D2.m4 (太阳黑子，8个隐藏层)   | +                                       | +  |'
- en: '| D3.m4 (Sunspots, 4 hidden)   | 0                                       |
    +  |'
  id: totrans-1126
  prefs: []
  type: TYPE_TB
  zh: '| D3.m4 (太阳黑子，4个隐藏层)   | 0                                       | +  |'
- en: '| D1.m8 (Sunspots, 12 hidden)  | +                                       |
    +  |'
  id: totrans-1127
  prefs: []
  type: TYPE_TB
  zh: '| D1.m8 (太阳黑子，12个隐藏层)  | +                                       | +  |'
- en: '| D2.m8 (Sunspots, 8 hidden)   | +                                       |
    0  |'
  id: totrans-1128
  prefs: []
  type: TYPE_TB
  zh: '| D2.m8 (太阳黑子，8个隐藏层)   | +                                       | 0  |'
- en: '| D3.m8 (Sunspots, 4 hidden)   | 0                                       |
    0  |'
  id: totrans-1129
  prefs: []
  type: TYPE_TB
  zh: '| D3.m8 (太阳黑子，4个隐藏层)   | 0                                       | 0  |'
- en: '| D1.m11 (Sunspots, 12 hidden) | +                                       |
    +  |'
  id: totrans-1130
  prefs: []
  type: TYPE_TB
  zh: '| D1.m11 (太阳黑子，12个隐藏层) | +                                       | +  |'
- en: '| D2.m11 (Sunspots, 8 hidden)  | 0                                       |
    +  |'
  id: totrans-1131
  prefs: []
  type: TYPE_TB
  zh: '| D2.m11 (太阳黑子，8个隐藏层)  | 0                                       | +  |'
- en: '| D3.m11 (Sunspots, 4 hidden)  | +                                       |
    +  |'
  id: totrans-1132
  prefs: []
  type: TYPE_TB
  zh: '| D3.m11 (太阳黑子，4个隐藏层)  | +                                       | +  |'
- en: '![91_image_0.png](91_image_0.png)'
  id: totrans-1133
  prefs: []
  type: TYPE_IMG
  zh: '![91_image_0.png](91_image_0.png)'
- en: Fig. 3.5. The performance of the 10 neural networks with 15 inputs, 15 hidden
    units, and one output unit, trained with weight decay using λ = λˆopt, on the
    power prediction problem. "Human expert" denotes the prediction result by the
    human expert at Puget Sound Power and Light Co., and "Competition winner" denotes
    the result by the model that won the Puget Sound Power and Light Co.'s Power Prediction
    Competition.
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5. 使用λ = λˆopt进行权重衰减训练的10个神经网络在电力预测问题上的表现，输入15，隐藏单元15，输出单元1。“人类专家”指的是普吉特海湾电力与光照公司的专家预测结果，而“竞赛获胜者”指的是在普吉特海湾电力与光照公司的电力预测竞赛中获胜模型的结果。
- en: '![91_image_1.png](91_image_1.png)'
  id: totrans-1135
  prefs: []
  type: TYPE_IMG
  zh: '![91_image_1.png](91_image_1.png)'
- en: Fig. 3.6. The performance of a neural network with 12 inputs, 12 hidden units,
    and one output unit, trained with weight decay using λ = λˆ1, on iterated predictions
    for the sunspot problem. The error bars denote one standard deviation for the
    10 trained networks. The dashed line shows the results when using the network
    listed in [26]. Note that these results are achieved with a simple weight decay
    cost and a very simple method for selecting λ, whereas [26] use weight elimination
    and a complicated heuristic scheme for setting λ.
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6. 使用λ = λˆ1进行权重衰减训练的一个神经网络在太阳黑子问题上的迭代预测表现，输入12，隐藏单元12，输出单元1。误差条表示10个训练网络的一个标准差。虚线显示使用[26]中列出的网络时的结果。请注意，这些结果是通过简单的权重衰减成本和非常简单的λ选择方法实现的，而[26]使用了权重消除和复杂的启发式方案来设置λ。
- en: 3.5 Conclusions
  id: totrans-1137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 结论
- en: The established connection between early stopping and weight decay regularization
    naturally leads to the idea of using early stopping to estimate the weight decay
    parameter. In this paper we have shown how this can be done and that the resulting
    λ results in as low test errors as achieved with the standard crossvalidation
    method, although this varies between problems. In practical applications, this
    means replacing a search which may take days or weeks, with a computation that
    usually does not require more than a few minutes or hours. This value can also
    be used as a starting point for a more extensive cross-validation search.
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
  zh: 早停法与权重衰减正则化之间建立的联系自然引出了使用早停法来估计权重衰减参数的想法。本文展示了如何实现这一点，并且得出的λ值在测试误差上与标准交叉验证方法相当，尽管这在不同问题间有所不同。在实际应用中，这意味着将可能需要几天或几周的搜索替换为通常只需几分钟或几小时的计算。这个值也可以用作更广泛交叉验证搜索的起点。
- en: We have also shown that using several early stopping networks to estimate λ
    can be smarter than combining the networks into committees. The conclusion from
    this is that although there is a correspondence between early stopping and weight
    decay under asymptotic conditions this does not mean that early stopping and weight
    decay give equivalent results in real life situations.
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了使用多个早停网络来估计λ可能比将网络组合成委员会更为聪明。由此得出的结论是，尽管在渐近条件下早停法与权重衰减之间存在对应关系，但这并不意味着早停法和权重衰减在实际情况下给出等效的结果。
- en: The method unfortunately only works for regularization terms that have a connection
    with early stopping, like quadratic weight decay or "weight decay like" regularizers
    where the weights are constrained towards the origin in weight space (but using
    e.g. a Laplacian prior instead of the usual Gaussian prior). The method does not
    carry over to regularizers which do not have any connection to early stopping
    (like e.g. Tikhonov smoothing regularizers).
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，该方法仅适用于与提前停止相关的正则化项，例如二次权重衰减或“权重衰减类”正则化器，其中权重在权重空间中向原点受限（但使用例如拉普拉斯先验而不是通常的高斯先验）。该方法不适用于与提前停止无关的正则化器（例如，Tikhonov平滑正则化器）。
- en: Acknowledgements. David B. Rosen is thanked for a very inspiring dinner conversation
    during the 1996 "Machines that Learn" Workshop in Snowbird, Utah. Milan Casey
    Brace of Puget Sound Power and Light Co. is thanked for supplying the power load
    data. Financial support is gratefully acknowledged from NSF (grant CDA-9503968),
    Olle and Edla Ericsson's Foundation, the Swedish Institute, and the Swedish Research
    Council for Engineering Sciences (grant TFR-282-95-847).
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢。感谢David B. Rosen在1996年“学习机器”研讨会期间在犹他州Snowbird的激励性晚餐谈话。感谢Puget Sound Power
    and Light Co.的Milan Casey Brace提供电力负荷数据。特别感谢NSF（拨款CDA-9503968）、Olle和Edla Ericsson基金会、瑞典研究所以及瑞典工程科学研究委员会（拨款TFR-282-95-847）的财政支持。
- en: '[1] Abu-Mustafa, Y.S.: Hints. Neural Computation 7, 639–671 (1995) [2] Bishop,
    C.M.: Curvature-driven smoothing: A learning algorithm for feedforward networks.
    IEEE Transactions on Neural Networks 4(5), 882–884 (1993)'
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Abu-Mustafa, Y.S.：提示。《神经计算》7，639–671 (1995) [2] Bishop, C.M.：曲率驱动的平滑：前馈网络的学习算法。《IEEE神经网络交易》4(5)，882–884
    (1993)'
- en: '[3] Brace, M.C., Schmidt, J., Hadlin, M.: Comparison of the forecast accuracy
    of neural networks with other established techniques. In: Proceedings of the First
    International Form on Application of Neural Networks to Power System, Seattle
    WA, pp. 31–35 (1991)'
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Brace, M.C., Schmidt, J., Hadlin, M.：神经网络与其他已建立技术的预测准确性比较。在：第一次国际神经网络在电力系统应用论坛论文集，西雅图，华盛顿州，第31–35页
    (1991)'
- en: '[4] Buntine, W.L., Weigend, A.S.: Bayesian back-propagation. Complex Systems
    5, 603–643 (1991)'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Buntine, W.L., Weigend, A.S.：贝叶斯反向传播。《复杂系统》5，603–643 (1991)'
- en: '[5] Cheeseman, P.: On Bayesian model selection. In: The Mathematics of Generalization
    - The Proceedings of the SFI/CNLS Workshop on Formal Approaches to Supervised
    Learning, pp. 315–330. Addison-Wesley, Reading (1995)'
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Cheeseman, P.：关于贝叶斯模型选择。在：《推广数学 - SFI/CNLS监督学习形式方法研讨会论文集》，第315–330页。阿迪森-韦斯利，雷丁
    (1995)'
- en: '[6] Cybenko, G.: Approximation by superpositions of a sigmoidal function. Mathematics
    of Control, Signals and Systems 2, 304–314 (1989)'
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Cybenko, G.：通过sigmoidal函数的叠加进行逼近。《控制、信号和系统数学》2，304–314 (1989)'
- en: '[7] Engle, R., Clive, F., Granger, W.J., Ramanathan, R., Vahid, F., Werner,
    M.:'
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Engle, R., Clive, F., Granger, W.J., Ramanathan, R., Vahid, F., Werner,
    M.：'
- en: Construction of the puget sound forecasting model. EPRI Project \# RP2919, Quantitative
    Economics Research Institute, San Diego, CA (1991)
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: 皮吉特海峡预测模型的构建。EPRI项目 \# RP2919，定量经济研究所，圣地亚哥，加利福尼亚州 (1991)
- en: '[8] Geman, S., Bienenstock, E., Doursat, R.: Neural networks and the bias/variance
    dilemma. Neural Computation 4(1), 1–58 (1992)'
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Geman, S., Bienenstock, E., Doursat, R.：神经网络与偏差/方差困境。《神经计算》4(1)，1–58 (1992)'
- en: '[9] Girosi, F., Jones, M., Poggio, T.: Regularization theory and neural networks
    architectures. Neural Computation 7, 219–269 (1995)'
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Girosi, F., Jones, M., Poggio, T.：正则化理论和神经网络架构。《神经计算》7，219–269 (1995)'
- en: '[10] Hansen, L.K., Rasmussen, C.E., Svarer, C., Larsen, J.: Adaptive regularization.
    In:'
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Hansen, L.K., Rasmussen, C.E., Svarer, C., Larsen, J.：自适应正则化。在：'
- en: Vlontzos, J., Hwang, J.-N., Wilson, E. (eds.) Proceedings of the IEEE Workshop
    on Neural Networks for Signal Processing IV, pp. 78–87. IEEE Press, Piscataway
    (1994)
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: Vlontzos, J., Hwang, J.-N., Wilson, E. (编辑)。《IEEE信号处理神经网络研讨会论文集IV》，第78–87页。IEEE出版社，Piscataway
    (1994)
- en: '[11] Hoerl, A.E., Kennard, R.W.: Ridge regression: Biased estimation of nonorthogonal
    problems. Technometrics 12, 55–67 (1970)'
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Hoerl, A.E., Kennard, R.W.：岭回归：非正交问题的偏估计。《技术计量学》12，55–67 (1970)'
- en: '[12] Ishikawa, M.: A structural learning algorithm with forgetting of link
    weights. Technical Report TR-90-7, Electrotechnical Laboratory, Information Science
    Division, 1-1-4 Umezono, Tsukuba, Ibaraki 305, Japan (1990)'
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Ishikawa, M.：一种具有链接权重遗忘的结构学习算法。技术报告TR-90-7，电气实验室，信息科学部，1-1-4 Umezono，筑波，茨城县305，日本
    (1990)'
- en: '[13] Kendall, M.G., Stuart, A.: The Advanced Theory of Statistics, 3rd edn.
    Hafner Publishing Co., New York (1972)'
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Kendall, M.G., Stuart, A.: 高级统计理论，第3版。哈夫纳出版社，纽约（1972）'
- en: '[14] Moody, J.E., Rögnvaldsson, T.S.: Smoothing regularizers for projective
    basis function networks. In: Advances in Neural Information Processing Systems
    9. MIT'
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Moody, J.E., Rögnvaldsson, T.S.: 投影基函数网络的平滑正则化器。在：《神经信息处理系统进展 9》。MIT'
- en: Press, Cambridge (1997)
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
  zh: 剑桥出版社（1997）
- en: '[15] Nowlan, S., Hinton, G.: Simplifying neural networks by soft weight-sharing.
    Neural Computation 4, 473–493 (1992)'
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Nowlan, S., Hinton, G.: 通过软权重共享简化神经网络。《神经计算》4，473–493（1992）'
- en: '[16] Perrone, M.P., Cooper, L.C.: When networks disagree: Ensemble methods
    for hybrid neural networks. In: Artificial Neural Networks for Speech and Vision,
    pp.'
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Perrone, M.P., Cooper, L.C.: 当网络意见不一致时：混合神经网络的集成方法。在：《用于语音和视觉的人工神经网络》，页。'
- en: 126–142. Chapman and Hall, London (1993)
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: 126–142. 查普曼与霍尔出版社，伦敦（1993）
- en: '[17] Plaut, D., Nowlan, S., Hinton, G.: Experiments on learning by backpropagation.
    Technical Report CMU-CS-86-126, Carnegie Mellon University, Pittsburg, PA (1986)'
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Plaut, D., Nowlan, S., Hinton, G.: 反向传播学习实验。技术报告 CMU-CS-86-126，卡内基梅隆大学，匹兹堡，宾夕法尼亚（1986）'
- en: '[18] Riedmiller, M., Braun, H.: A direct adaptive method for faster backpropagation
    learning: The RPROP algorithm. In: Ruspini, H. (ed.) Proc. of the IEEE Intl. Conference
    on Neural Networks, San Fransisco, California, pp. 586–591 (1993)'
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Riedmiller, M., Braun, H.: 一种更快的反向传播学习的直接自适应方法：RPROP算法。在：Ruspini, H.（编）《IEEE国际神经网络会议论文集》，旧金山，加利福尼亚，页586–591（1993）'
- en: '[19] Ripley, B.D.: Pattern Recognition and Neural Networks. Cambridge University
    Press, Cambridge (1996)'
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Ripley, B.D.: 模式识别与神经网络。剑桥大学出版社，剑桥（1996）'
- en: '[20] Sjöberg, J., Ljung, L.: Overtraining, regularization, and searching for
    minimum with application to neural nets. Int. J. Control 62(6), 1391–1407 (1995)'
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Sjöberg, J., Ljung, L.: 过拟合、正则化与最小值搜索及其在神经网络中的应用。《控制国际期刊》62(6)，1391–1407（1995）'
- en: '[21] Tikhonov, A.N., Arsenin, V.Y.: Solutions of Ill-Posed problems. V. H.
    Winston &'
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Tikhonov, A.N., Arsenin, V.Y.: 解决不适定问题。V. H. Winston &'
- en: Sons, Washington D.C. (1977)
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
  zh: 维尔逊，华盛顿特区（1977）
- en: '[22] Tong, H.: Non-linear Time Series: A Dynamical System Approach. Clarendon
    Press, Oxford (1990)'
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Tong, H.: 非线性时间序列：一种动力系统方法。克拉伦登出版社，牛津（1990）'
- en: '[23] Utans, J., Moody, J.E.: Selecting neural network architectures via the
    prediction risk: Application to corporate bond rating prediction. In: Proceedings
    of the First International Conference on Artificial Intelligence Applications
    on Wall Street. IEEE Computer Society Press, Los Alamitos (1991)'
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Utans, J., Moody, J.E.: 通过预测风险选择神经网络架构：应用于公司债券评级预测。在：第一届华尔街人工智能应用国际会议论文集。IEEE计算机学会出版社，洛杉矶（1991）'
- en: '[24] Wahba, G., Gu, C., Wang, Y., Chappell, R.: Soft classification, a.k.a.
    risk estimation, via penalized log likelihood and smoothing spline analysis of
    variance. In:'
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Wahba, G., Gu, C., Wang, Y., Chappell, R.: 软分类，亦即风险估计，通过惩罚对数似然与平滑样条方差分析。
    在：'
- en: The Mathematics of Generalization - The Proceedings of the SFI/CNLS Workshop
    on Formal Approaches to Supervised Learning, pp. 331–359. Addison-Wesley, Reading
    (1995)
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化的数学 - SFI/CNLS关于监督学习正式方法研讨会论文集，页331–359。阿迪森-韦斯利，雷丁（1995）
- en: '[25] Wahba, G., Wold, S.: A completely automatic french curve. Communications
    in Statistical Theory & Methods 4, 1–17 (1975)'
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Wahba, G., Wold, S.: 一种完全自动的法式曲线。统计理论与方法通讯 4，1–17（1975）'
- en: '[26] Weigend, A., Rumelhart, D., Hubermann, B.: Back-propagation, weightelimination
    and time series prediction. In: Sejnowski, T., Hinton, G., Touretzky, D. (eds.)
    Proc. of the Connectionist Models Summer School. Morgan Kaufmann Publishers, San
    Mateo (1990)'
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Weigend, A., Rumelhart, D., Hubermann, B.: 反向传播、权重消除与时间序列预测。在：Sejnowski,
    T., Hinton, G., Touretzky, D.（编）《连接主义模型夏季学校论文集》。摩根·考夫曼出版社，圣马特奥（1990）'
- en: '[27] Williams, P.M.: Bayesian regularization and pruning using a Laplace prior.
    Neural Computation 7, 117–143 (1995)'
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Williams, P.M.: 使用拉普拉斯先验的贝叶斯正则化与剪枝。《神经计算》7，117–143（1995）'
- en: 4 Controlling The Hyperparameter Search In Mackay'S Bayesian Neural Network
    Framework-
  id: totrans-1174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 控制麦凯贝叶斯神经网络框架中的超参数搜索-
- en: Tony Plate School of Mathematical and Computing Sciences, Victoria University,
    Wellington, New Zealand tap@mcs.vuw.ac.nz http://www.mcs.vuw.ac.nz/˜tap/
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
  zh: 托尼·普莱特 数学与计算科学学院，维多利亚大学，惠灵顿，新西兰 tap@mcs.vuw.ac.nz http://www.mcs.vuw.ac.nz/˜tap/
- en: Abstract. In order to achieve good generalization with neural networks overfitting
    must be controlled. Weight penalty factors are one common method of providing
    this control. However, using weight penalties creates the additional search problem
    of finding the optimal penalty factors.
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要。为了实现神经网络的良好泛化，必须控制过拟合。权重惩罚因子是提供这种控制的一种常见方法。然而，使用权重惩罚会产生寻找最优惩罚因子的额外搜索问题。
- en: MacKay [5] proposed an approximate Bayesian framework for training neural networks,
    in which penalty factors are treated as hyperparameters and found in an iterative
    search. However, for classification networks trained with cross-entropy error,
    this search is slow and unstable, and it is not obvious how to improve it. This
    paper describes and compares several strategies for controlling this search. Some
    of these strategies greatly improve the speed and stability of the search. Test
    runs on a range of tasks are described.
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: MacKay [5] 提出了一个用于训练神经网络的近似贝叶斯框架，其中惩罚因子被视为超参数并在迭代搜索中找到。然而，对于使用交叉熵误差训练的分类网络，这种搜索过程缓慢且不稳定，且如何改善这一点并不明显。本文描述并比较了几种控制该搜索的策略。其中一些策略极大提高了搜索的速度和稳定性。描述了在一系列任务上的测试运行。
- en: 4.1 Introduction
  id: totrans-1178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 引言
- en: Neural networks can provide useful flexible statistical models for non-linear
    regression and classification. However, as with all such models, the flexibility
    must be controlled to avoid overfitting. One way of doing this in neural networks
    is to use weight penalty factors ( regularization parameters). This creates the
    problem of finding the values of the penalty factors which will maximize performance
    on new data. As various researchers have pointed out, including MacKay [5], Neal
    [10] and Bishop [1], it is generally advantageous to use more than one penalty
    factor, in order to differentially penalize weights between different layers of
    the network. However, doing this makes it computationally infeasible to choose
    optimal penalty factors by k-fold cross validation.
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以为非线性回归和分类提供有用的灵活统计模型。然而，正如所有此类模型一样，必须控制灵活性以避免过拟合。在神经网络中做到这一点的一种方法是使用权重惩罚因子（正则化参数）。这就产生了寻找惩罚因子的值以最大化新数据性能的问题。正如包括
    MacKay [5]、Neal [10] 和 Bishop [1] 在内的各种研究人员指出的那样，通常使用多个惩罚因子是有利的，以便对网络不同层之间的权重进行差别惩罚。然而，这样做使得通过
    k 折交叉验证选择最优惩罚因子在计算上变得不可行。
- en: MacKay [5] describes a Bayesian framework for training neural networks and choosing
    optimal penalty factors (which are hyperparameters in his framework). In this
    framework, we choose point estimates of hyperparameters to maximize the
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
  zh: MacKay [5] 描述了一个用于训练神经网络并选择最优惩罚因子的贝叶斯框架（在他的框架中这些是超参数）。在这个框架中，我们选择超参数的点估计以最大化
- en: '"evidence" of the network. Parameters (i.e., weights) can be assigned into
    different'
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的“证据”。参数（即权重）可以分配到不同的
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: '- 先前发表在：Orr, G.B. 和 Müller, K.-R.（编者）：LNCS 1524, ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0（1998年）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    91–110, 2012.'
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon 等（编者）：NN: Tricks of the Trade, 第2版，LNCS 7700，页91–110，2012年。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: groups, and each controlled by a separate hyperparameter. This allows weights
    between different layers to be penalized differently. MacKay [6, 8] and Neal [10]
    have shown that it also provides a way of implementing "Automatic Relevance Detection"
    (ARD), in which connections emerging from different units in the input layer are
    assigned to different regularization groups. The idea is that hyperparameters
    controlling weights for irrelevant inputs should become large, driving those weights
    to zero, while hyperparameters for relevant inputs stabilize at small to moderate
    values. This can help generalization by causing the network to ignore irrelevant
    inputs and also makes it possible to see at a glance which inputs are important.
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
  zh: 组，每个组由单独的超参数控制。这允许不同层之间的权重受到不同的惩罚。MacKay [6, 8] 和 Neal [10] 显示，这也提供了一种实现“自动相关检测”（ARD）的方法，其中来自输入层不同单元的连接被分配到不同的正则化组。其思想是，控制无关输入权重的超参数应变得较大，从而将这些权重驱动至零，而相关输入的超参数则稳定在小到中等的值。这有助于泛化，使网络忽略无关输入，同时也使人一目了然哪些输入是重要的。
- en: In this framework the search for an optimal network has two levels. The inner
    level is a standard search for weights which minimize error on the training data,
    with fixed hyperparameters. The outer level is a search for hyperparameters which
    maximize the evidence. For the Bayesian theory to apply, the inner level search
    should be allowed to converge to a local minima at each step of the outer level
    search. However, this can be expensive and slow. Problems with speed and stability
    of the search seem especially severe with classification networks trained with
    cross-entropy error.
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个框架中，寻找最佳网络分为两个层级。内层是标准的权重搜索，旨在最小化训练数据上的误差，超参数固定。外层则是寻找最大化证据的超参数。为了使贝叶斯理论适用，内层搜索应允许在外层搜索的每一步收敛到局部最小值。然而，这可能成本高昂且缓慢。与交叉熵误差训练的分类网络相比，搜索的速度和稳定性问题似乎尤其严重。
- en: This paper describes experiments with different control strategies for updating
    the hyperparameters in the outer level search. These experiments show that the
    simple "let it run to convergence and then update" strategy often does not work
    well, and that other strategies can generally work better. In previous work, the
    current author successfully employed one of these strategies in an application
    of neural networks to epidemiological data analysis [11]. The experiments reported
    here confirm the necessity for update strategies and also demonstrate that although
    the strategy used in this previous work is reasonably effective in some situations,
    there are simpler and better strategies which work in a wider range of situations.
    These experiments also furnish data on the relationship between the evidence and
    the generalization error. This data confirms theoretical expectations about when
    the evidence should and should not be a good indication of generalization error.
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: 本文描述了在外层搜索中更新超参数的不同控制策略的实验。这些实验表明，简单的“让它收敛后再更新”策略往往效果不佳，而其他策略通常表现更好。在之前的工作中，当前作者成功地在神经网络应用于流行病学数据分析中采用了这些策略之一
    [11]。这里报告的实验确认了更新策略的必要性，并展示了尽管在某些情况下前述工作的策略是合理有效的，但存在更简单且在更广泛情况下有效的策略。这些实验还提供了证据与泛化误差之间关系的数据。这些数据确认了关于何时证据应该或不应该是泛化误差良好指示的理论预期。
- en: In the second section of this chapter, the update formulas for hyperparameters
    are given. Network propagation and weight update formulas are not given, as they
    are well known and available elsewhere, e.g., in Bishop [1]. Different control
    strategies for the outer level hyperparameter search are described in the third
    section. In the fourth section, the simulation experiments are described, and
    the results are reported in the fifth section. The experimental relationships
    between evidence and generalization error are reported in the sixth section.
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
  zh: 本章第二节给出了超参数的更新公式。网络传播和权重更新公式未给出，因为它们众所周知且可以在其他地方找到，例如 Bishop [1]。第三节描述了外层超参数搜索的不同控制策略。第四节描述了模拟实验，第五节报告了结果。第六节报告了证据与泛化误差之间的实验关系。
- en: 4.2 Hyperparameter Updates
  id: totrans-1190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 超参数更新
- en: The update formulas for the hyperparameters (weight penalty factors) in the
    outer level search are quite simple. Before describing them we need some terminology.
    For derivations and background theory see Bishop [1], MacKay [5, 7], or Thodberg
    [13].
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: 外层搜索中超参数（权重惩罚因子）的更新公式相当简单。在描述它们之前，我们需要一些术语。有关推导和背景理论，请参见 Bishop [1]、MacKay [5,
    7] 或 Thodberg [13]。
- en: '- n is the total number of weight in the network.'
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: '- n 是网络中的权重总数。'
- en: '- wi the value of the ith weight.'
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
  zh: '- wi 是第 i 个权重的值。'
- en: '- K is the number of hyperparameters.'
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: '- K 是超参数的数量。'
- en: '- Ic is the set of indices of the weights in the cth hyperparameter group.
    - αc is the value of the hyperparameter controlling the cth hyperparameter group;
    it specifies the prior distribution on the weights in that group. α[i] denotes
    the value of the hyperparameter controlling the group to which weight i belongs.'
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
  zh: '- Ic 是第 c 个超参数组中权重的索引集合。- αc 是控制第 c 个超参数组的超参数值；它指定该组权重的先验分布。α[i] 表示控制权重 i 所属组的超参数值。'
- en: '- nc is the number of weights in the cth hyperparameter group.'
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: '- nc 是第 c 个超参数组中的权重数量。'
- en: '- C is the weight cost (penalty term) for the network: C = 12 ni=1 α[i]w2i
    .'
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
  zh: '- C 是网络的权重成本（惩罚项）：C = 12 ni=1 α[i]w2i 。'
- en: '- m is the total number of training examples.'
  id: totrans-1198
  prefs: []
  type: TYPE_NORMAL
  zh: '- m是训练样本的总数。'
- en: '- yj and t j are the network outputs and target values, respectively, for the
    jth training example.'
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: '- yj和tj分别是第j个训练样本的网络输出和目标值。'
- en: '- E is the error term of the network. For the classification networks described
    here, the modified cross-entropy (Bishop [1], p.232) is used:'
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: '- E是网络的误差项。对于这里描述的分类网络，使用修改的交叉熵（Bishop [1], p.232）：'
- en: $$E=-\sum_{j=1}^{m}\{t^{j}\log{\frac{y^{j}}{t^{j}}}+(1-t^{j})\log{\frac{1-y^{j}}{1-t^{j}}}\}.$$
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
  zh: $$E=-\sum_{j=1}^{m}\{t^{j}\log{\frac{y^{j}}{t^{j}}}+(1-t^{j})\log{\frac{1-y^{j}}{1-t^{j}}}\}$$。
- en: Note that all graphs and tables of test set performance use the "deviance",
    which is twice the error.
  id: totrans-1202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，所有测试集性能的图表和表格使用“偏差”，即误差的两倍。
- en: '- H is the Hessian of the network (the second partial derivatives of the sum
    of the error and weight cost). hij denotes the ijth element of this matrix, and
    h−1 ij denotes the ijth element of H−1:'
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
  zh: '- H是网络的海森矩阵（误差和权重成本之和的二阶偏导数）。hij表示该矩阵的ij元素，h−1 ij表示H−1的ij元素：'
- en: $$h_{i j}={\frac{\partial^{2}(E+C)}{\partial w_{i}\partial w_{j}}}.$$
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
  zh: $$h_{i j}={\frac{\partial^{2}(E+C)}{\partial w_{i}\partial w_{j}}}$$。
- en: HE is the matrix of second partial derivatives of just the error, and HC is
    the matrix of second partial derivatives of just the weight cost.
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
  zh: HE是仅针对误差的二阶偏导数矩阵，HC是仅针对权重成本的二阶偏导数矩阵。
- en: '- Tr(H−1) is the trace of the inverse of H: Tr(H−1) = ni=1 h−1 ii .'
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
  zh: '- Tr(H−1)是H的逆的迹：Tr(H−1) = ni=1 h−1 ii 。'
- en: '- Trc(H−1) is the trace of the inverse Hessian for just those elements of the
    cth regularization group: Trc(H−1) = i∈Ic h−1 ii .'
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
  zh: '- Trc(H−1)是第c个正则化组的逆海森矩阵的迹：Trc(H−1) = i∈Ic h−1 ii 。'
- en: '- γc is a derived parameter which can be seen as an estimate of the number
    of well-determined parameters in the cth regularization group, i.e., the number
    of parameters determined by the data rather than by the prior.'
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
  zh: '- γc是一个派生参数，可以视为组c中确定参数数量的估计，即由数据而非先验确定的参数数量。'
- en: The overall training procedure is shown in Figure 4.1.
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
  zh: 整个训练过程见图4.1。
- en: The updates for the hyperparameters αc depend on the estimate γc (the number
    of well-determined parameters in group c) which is calculated as follows
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数αc的更新依赖于估计的γc（组c中的确定参数数量），其计算方法如下
- en: '(Eqn 27 in [7]; derivable from Eqn 10.140 in [1]):'
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
  zh: （在[7]中的方程27；可从[1]中的方程10.140推导）：
- en: $$\gamma_{c}=n_{c}-\alpha_{c}\mathrm{Tr}_{c}(\mathbf{H}^{-1}).$$
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
  zh: $$\gamma_{c}=n_{c}-\alpha_{c}\mathrm{Tr}_{c}(\mathbf{H}^{-1})$$。
- en: $$(4.1)$$
  id: totrans-1213
  prefs: []
  type: TYPE_NORMAL
  zh: $$(4.1)$$
- en: γc = nc − αcTrc(H−1). (4.1)
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
  zh: γc = nc − αcTrc(H−1)。 (4.1)
- en: If a Gaussian distribution is a reasonable approximation to the posterior weight
    distribution, γc should be between 0 and nc. Furthermore, we expect each parameter
    in group c to contribute between 0 and 1 to γc. Hence, we expect h−1 ii to always
    be in the range [0, 1/α[i]].
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果高斯分布是对后验权重分布的合理近似，则γc应介于0和nc之间。此外，我们期望组c中的每个参数对γc的贡献介于0和1之间。因此，我们期望h−1 ii始终在范围[0,
    1/α[i]]内。
- en: set the αc to initial values set wi to initial random values repeat repeat make
    an optimization step for weights to minimize E + C
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
  zh: 设置αc为初始值，设置wi为初始随机值，重复进行优化步骤以最小化E + C
- en: 'until finished weight optimization re-estimate the αc until finished max number
    of passes through training data Fig. 4.1. The training procedure The updates for
    the αc is as follows (Eqn 22 in [7]; Eqn 10.74 in [1]):'
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
  zh: 直到完成权重优化，重新估计αc，直到完成最大训练数据遍历次数，见图4.1。训练过程中的αc更新如下（在[7]中的方程22；在[1]中的方程10.74）：
- en: $$\mathbf{0}$$
  id: totrans-1218
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbf{0}$$
- en: ⎥⎥⎦
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
  zh: ⎥⎥⎦
- en: $$\alpha_{c}^{\prime}=\frac{\gamma_{c}}{\sum_{i\in\mathcal{I}_{c}}w_{i}^{2}}$$
  id: totrans-1220
  prefs: []
  type: TYPE_NORMAL
  zh: $$\alpha_{c}^{\prime}=\frac{\gamma_{c}}{\sum_{i\in\mathcal{I}_{c}}w_{i}^{2}}$$
- en: '|; Eqn 10.74 in [1]\).'
  id: totrans-1221
  prefs: []
  type: TYPE_NORMAL
  zh: '|；[1]中的方程10.74）。'
- en: $$(4.2)$$
  id: totrans-1222
  prefs: []
  type: TYPE_NORMAL
  zh: $$(4.2)$$
- en: 'MacKay [7] remarks that this formula can be seen as matching the prior to the
    data: 1/αc is an estimate of the variance for the weights in group c, taking into
    account the effective number of well determined parameters (effective degrees
    of freedom) in that group.'
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
  zh: MacKay [7]指出，这个公式可以看作是将先验与数据相匹配：1/αc是对组c中权重方差的估计，考虑到该组中有效确定参数的有效自由度。
- en: 4.2.1 Difficulties With Using The Update Formulas
  id: totrans-1224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2.1 使用更新公式的困难
- en: 'The difficulties with using these update formulas arise when the assumption
    that the error plus cost surface is a quadratic bowl is false. This assumption
    can fail in two ways: the error plus cost surface may not be quadratic, or it
    may not be a bowl (i.e., the Hessian is not positive definite). In either of these
    situations, it is possible for γc to be out of the range [0, nc]. To illustrate,
    consider a single diagonal element of the Hessian in the situation where off-diagonal
    elements are zero:'
  id: totrans-1225
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些更新公式的困难在于假设误差加成本曲面是一个二次碗的前提是错误的。这个假设可能以两种方式失败：误差加成本曲面可能不是二次的，或者它可能不是一个碗（即，海森矩阵不是正定的）。在这两种情况下，γc
    可能超出范围 [0, nc]。为了说明这一点，考虑海森矩阵的一个对角元素，当非对角元素为零时：
- en: $$\mathbf{H}=\mathbf{\mu}$$
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbf{H}=\mathbf{\mu}$$
- en: $$\left[\begin{array}{c c c}{{\ddots}}&{{}}&{{0}}\\ {{}}&{{}}&{{}}\\ {{h_{i
    i}}}&{{}}\\ {{}}&{{}}&{{}}\\ {{0}}&{{}}\end{array}\right]=\left[\begin{array}{c
    c c}{{\ddots}}&{{}}&{{}}&{{0}}\\ {{}}&{{}}&{{h_{i i}^{E}+\alpha_{[i]}}}\\ {{}}&{{}}&{{}}\\
    {{0}}&{{}}&{{}}\end{array}\right]$$
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
  zh: $$\left[\begin{array}{c c c}{{\ddots}}&{{}}&{{0}}\\ {{}}&{{}}&{{}}\\ {{h_{i
    i}}}&{{}}\\ {{}}&{{}}&{{}}\\ {{0}}&{{}}\end{array}\right]=\left[\begin{array}{c
    c c}{{\ddots}}&{{}}&{{}}&{{0}}\\ {{}}&{{}}&{{h_{i i}^{E}+\alpha_{[i]}}}\\ {{}}&{{}}&{{}}\\
    {{0}}&{{}}&{{}}\end{array}\right]$$
- en: 'Since the off-diagonal elements are zero, the inverse Hessian is simple to
    write down:'
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
  zh: 由于非对角元素为零，逆海森矩阵简单易写：
- en: $$\mathbf{H}^{-1}={\left[\begin{array}{l l}{\cdot\cdot}\\ {}&{}\\ {}&{{}{\frac{1}{h_{i
    i}^{E}+\alpha_{[i]}}}}\end{array}\right]}$$
  id: totrans-1229
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbf{H}^{-1}={\left[\begin{array}{l l}{\cdot\cdot}\\ {}&{}\\ {}&{{}{\frac{1}{h_{i
    i}^{E}+\alpha_{[i]}}}}\end{array}\right]}$$
- en: $$\bot\,\,\,0$$
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
  zh: $$\bot\,\,\,0$$
- en: 0 ...
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
  zh: 0 ...
- en: ⎤
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
  zh: ⎤
- en: 'Suppose the i parameter is in the cth regularization group, by itself. Then
    the number of well-determined parameters in this group is given by:'
  id: totrans-1233
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 i 参数独自在 cth 正则化组中。那么该组中的良好确定参数的数量为：
- en: $$\gamma_{[i]}=1-\alpha_{[i]}h_{i i}^{-1}=1-\frac{\alpha_{[i]}}{h_{i i}^{E}+\alpha_{[i]}}=\frac{h_{i
    i}^{E}}{h_{i i}^{E}+\alpha_{[i]}}$$
  id: totrans-1234
  prefs: []
  type: TYPE_NORMAL
  zh: $$\gamma_{[i]}=1-\alpha_{[i]}h_{i i}^{-1}=1-\frac{\alpha_{[i]}}{h_{i i}^{E}+\alpha_{[i]}}=\frac{h_{i
    i}^{E}}{h_{i i}^{E}+\alpha_{[i]}}$$
- en: $$(4.3)$$
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
  zh: $$(4.3)$$
- en: If hEii is positive, γ[i] will be between 0 and 1. γ[i] will be large if hEii
    is large relative to α[i], which means that wi is well determined by the data,
    i.e., small moves of wi will make a large difference to E. γ[i] will be small
    if hEii is small relative to α[i], which means that wi is poorly determined by
    the data.
  id: totrans-1236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 hEii 为正，γ[i] 将在 0 和 1 之间。如果 hEii 相对于 α[i] 很大，γ[i] 将会很大，这意味着 wi 由数据良好确定，即，wi
    的小变化会对 E 产生很大的影响。如果 hEii 相对于 α[i] 很小，γ[i] 将会很小，这意味着 wi 由数据确定得不好。
- en: 'The expectation that h−1 ii is in the range [0, 1/αc] (and hence contributes
    between 0 and 1 well determined parameter to γ[i]) can fail even if the model
    is at a local minima of E + C. Being at a local minimum of E + C does not guarantee
    that hEii will be positive: it is possible for the hyperparameter to "pin" the
    weight value to a convex portion of a non-quadratic E surface. Consider the case
    where the Hessian is diagonal and positive definite, but hEii is negative. From
    Eqn 4.3, we can see that h−1 ii can make a negative contribution1 to γc, which
    makes little sense in terms of "numbers of well-determined parameters". This situation2
    is illustrated in Figure 4.2: at the minimum of the E +C function the E function
    is convex ( d2E'
  id: totrans-1237
  prefs: []
  type: TYPE_NORMAL
  zh: 即使模型处于 E + C 的局部极小值，h−1 ii 在范围 [0, 1/αc] 内的期望（因此在 γ[i] 上贡献一个介于 0 和 1 之间的良好确定参数）也可能失败。处于
    E + C 的局部极小值并不保证 hEii 为正：超参数可能会将权重值“固定”在非二次 E 曲面的凸部分。考虑海森矩阵是对角且正定的情况，但 hEii 为负。从公式
    4.3，我们可以看到 h−1 ii 可以对 γc 产生负贡献1，这在“良好确定参数的数量”方面没有意义。这种情况2在图 4.2 中得到说明：在 E + C
    函数的最小值处，E 函数是凸的（ d2E
- en: dw2 is negative). Here, negative degrees of freedom would be calculated under
    the (incorrect) assumption that error plus cost is quadratic.
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
  zh: dw2 为负）。在此，负的自由度将根据（不正确的）假设误差加成本是二次的进行计算。
- en: This is important for neural networks, because even if sum-squared error is
    used, non-linearities in the sigmoids can cause the the error plus cost function
    to be not a quadratic function of weights.
  id: totrans-1239
  prefs: []
  type: TYPE_NORMAL
  zh: 这对神经网络很重要，因为即使使用平方误差，sigmoid 中的非线性也可能导致误差加成本函数不是权重的二次函数。
- en: '![99_image_0.png](99_image_0.png)'
  id: totrans-1240
  prefs: []
  type: TYPE_IMG
  zh: '![99_image_0.png](99_image_0.png)'
- en: Fig. 4.2. In minimizing E + C, a weight cost function C can pin a weight value
    to a convex portion of the error surface E. The plot on the left shows the surfaces,
    the plot on the right shows the derivatives in the region of the minimum.
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2。在最小化 E + C 时，权重成本函数 C 可以将权重值固定在误差面 E 的凸部分。左侧的图显示了表面，右侧的图显示了在最小值区域的导数。
- en: If the model is not at a local minimum of E + C all bets are off. H may not
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型不在 E + C 的局部最小值，所有的猜测都将失效。H 可能不是
- en: even be positive definite (i.e., the Hessian of a quadratic bowl), and if this
    is the case it is almost certain that some h−1 ii will be out of the range [0,
    1/α[i]]. Even 1 With general matrices it is possible that h−1 ii < −α[i], in which
    case the contribution will be an unbounded positive number.
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至可能是正定的（即，一个二次碗的海森矩阵），如果是这种情况，几乎可以肯定某些 h−1 ii 将超出范围 [0, 1/α[i]]。即使是一般矩阵也有可能出现
    h−1 ii < −α[i]，在这种情况下，贡献将是一个无界正数。
- en: 2 In Figure 4.2, E = w(w − 1)(w + 1)2 + 1, C = 4w2, d(E+C)
  id: totrans-1244
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 4.2 中，E = w(w − 1)(w + 1)² + 1，C = 4w²，d(E+C)
- en: dw 0.152645≈ 0, and d2E
  id: totrans-1245
  prefs: []
  type: TYPE_NORMAL
  zh: dw 0.152645≈ 0，和 d²E
- en: dw2 0.152645
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
  zh: dw² 0.152645
- en: ≈ −0.8036.
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
  zh: ≈ −0.8036。
- en: if H is positive definite, and HE is also positive definite, it can still be
    the case that some h−1 ii are out-of-bounds, and thus make contributions of less
    than zero or more than one "well-determined parameter" each.
  id: totrans-1248
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 H 是正定的，并且 HE 也是正定的，仍然可能存在一些 h−1 ii 超出范围，因此每个“确定参数”的贡献可能小于零或大于一。
- en: 'These difficulties leave us with two problems:'
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
  zh: 这些困难使我们面临两个问题：
- en: 1. When should the hyperparameters be updated?
  id: totrans-1250
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 何时应该更新超参数？
- en: 2. What should be done when h−1 ii is not in [0, 1/α[i]] (i.e., is "out-of-bounds").
  id: totrans-1251
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 当 h−1 ii 不在 [0, 1/α[i]] 中时（即，处于“超出范围”状态）应该怎么做。
- en: Bishop [1] suggests updating hyperparameters after every few weight updates.
  id: totrans-1252
  prefs: []
  type: TYPE_NORMAL
  zh: Bishop [1] 建议在每几次权重更新后更新超参数。
- en: 'Thodberg [13] suggests updating the αc after every weight update, but only
    recalculating the γc occasionally (at five evenly-spaced intervals throughout
    the whole process). While updating hyperparameters after every weight update is
    not feasible when using conjugate gradient or other second-order training methods,
    common practice seems to be more or less in line with Thodberg''s recommendations:
    train the network more or less to convergence before each hyperparameter update.
    However, this strategy can result in an extremely slow overall search.'
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
  zh: Thodberg [13] 建议在每次权重更新后更新 αc，但仅偶尔重新计算 γc（在整个过程中均匀分布的五个间隔）。虽然在使用共轭梯度或其他二阶训练方法时，在每次权重更新后更新超参数并不现实，但常见做法似乎或多或少与
    Thodberg 的建议一致：在每次超参数更新之前，将网络训练到收敛。然而，这种策略可能导致整体搜索速度极慢。
- en: Furthermore, training to convergence does not eliminate the problem of outof-bounds
    h−1 ii values. In the remainder of this chapter, various strategies for choosing
    when to update hyperparameters, and for dealing with out-of-bounds h−1 ii values
    are described and compared in experiments.
  id: totrans-1254
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，训练到收敛并不能消除超出范围的 h−1 ii 值的问题。本章其余部分将描述并比较各种选择何时更新超参数以及处理超出范围的 h−1 ii 值的策略。
- en: 4.3 Control Strategies
  id: totrans-1255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 控制策略
- en: 'The strategies tested here fall into three groups: strategies for when to update
    hyperparameters, strategies for dealing with out-of-bounds h−1 ii values, and
    special strategies for dealing with exceptional cases. For each task, searches
    were run a fixed number of minimization steps, with different combinations of
    control strategies. Each step of the inner minimization loop was a step of a conjugate
    gradient method, and could involve a number of passes through the training data,
    though the average was just over two.'
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
  zh: 此处测试的策略分为三类：何时更新超参数的策略、处理超出范围的 h−1 ii 值的策略，以及处理特殊情况的特殊策略。对于每个任务，运行了固定数量的最小化步骤，结合不同的控制策略。内层最小化循环的每一步都是共轭梯度方法的一步，并可能涉及对训练数据的多次遍历，尽管平均次数仅稍超过两次。
- en: 4.3.1 Choosing When To Update Hyperparameters
  id: totrans-1257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3.1 选择何时更新超参数
- en: 'Four different strategies for choosing when to break out of the inner loop
    and update hyperparameters were employed:'
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
  zh: 采用了四种不同的策略来选择何时跳出内循环并更新超参数：
- en: 'rare: Update at 10 evenly spaced intervals. medium: Update at 30 evenly spaced
    intervals. often: Update at 100 evenly spaced intervals. patience: Update when
    the improvement in the last n steps was less than 1% of the improvement since
    the start of the inner loop, or when the improvement in the last n steps was less
    than 0.01% of the null error (the minimum error that can be achieved with a constant
    output value). At least n steps of the inner loop are taken.'
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
  zh: 'rare: 每10个均匀间隔进行一次更新。medium: 每30个均匀间隔进行一次更新。often: 每100个均匀间隔进行一次更新。patience:
    当最后 n 步的改进小于自内循环开始以来改进的 1% 时更新，或当最后 n 步的改进小于 0.01% 的零误差（可以通过恒定输出值达到的最小误差）时更新。至少进行
    n 步的内循环。'
- en: Convergence was difficult to test when using a number of different datasets
    and networks. One standard way of detecting convergence is to test whether the
    ratio |g¯|/|w¯| is less than some threshold (|g¯| is the Euclidean length of the
    vector of weight derivatives, and |w¯| is the Euclidean length of the vector of
    weights.)
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多种不同的数据集和网络时，收敛性测试较为困难。检测收敛的一种标准方法是测试比率 |g¯|/|w¯| 是否小于某个阈值（|g¯| 是权重导数向量的欧几里得长度，|w¯|
    是权重向量的欧几里得长度）。
- en: However, the appropriate threshold varied greatly among different tasks. In
    any case, with all strategies, if a convergence test was met (|g¯|/|w¯| < 10−6),
    the inner loop terminated, and the hyperparameters were updated. This did not
    occur often in the experiments described here.
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，适当的阈值在不同任务之间差异很大。在任何情况下，使用所有策略时，如果满足收敛测试（|g¯|/|w¯| < 10−6），内循环将终止，并更新超参数。这在此处描述的实验中并不常见。
- en: 'The "patience" strategy is intended to be a surrogate for convergence: the
    inner loop runs out of patience when the improvement achieved in the last n steps
    is minuscule. With "patience", the inner loop is guaranteed to terminate if the
    error is bounded below. In practice, the inner loop runs out of patience reasonable
    quickly: the update rate is somewhere between "rare" and "medium" depending on
    the difficulty of the optimization problem.'
  id: totrans-1262
  prefs: []
  type: TYPE_NORMAL
  zh: “patience”策略旨在作为收敛的替代：当最后 n 步的改进微不足道时，内循环会失去耐心。使用“patience”，如果误差下限有界，则内循环保证终止。在实践中，内循环很快就会失去耐心：更新速率介于“rare”和“medium”之间，具体取决于优化问题的难度。
- en: 4.3.2 Dealing With Out-Of-Bounds Estimates Of Numbers Of Well-Determined Parameters
  id: totrans-1263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3.2 处理已确定参数数量的越界估计
- en: In each of the experiments, one of the following strategies was used to deal
    with out-of-bounds h−1 ii values. In describing these strategies, h−1ii or γc
    are used to denote values which are used instead of the originally calculated
    ones.
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个实验中，使用以下策略之一来处理越界的 h−1 ii 值。在描述这些策略时，h−1ii 或 γc 用于表示用来代替原始计算值的值。
- en: 'none: This strategy allows γc to take on unreasonably large values, but not
    negative ones (h−1 ii values are not checked):'
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: 'none: 该策略允许 γc 取非常大的值，但不允许为负值（h−1 ii 值不被检查）：'
- en: $$\gamma_{c}^{\prime}=\left\{\begin{array}{l l}{{0}}&{{\mathrm{if~}\gamma_{c}<0}}\\
    {{\gamma_{c}\ \mathrm{otherwise}}}\end{array}\right.$$
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
  zh: $$\gamma_{c}^{\prime}=\left\{\begin{array}{l l}{{0}}&{{\mathrm{如果~}\gamma_{c}<0}}\\
    {{\gamma_{c}\ \mathrm{其他情况}}}\end{array}\right.$$
- en: 'group: This strategy forces the total number of well-determined parameters
    in a regularization group to be reasonable:'
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
  zh: 'group: 该策略强制正则化组中的已确定参数总数保持合理：'
- en: $$\gamma_{c}^{\prime}=\left\{\begin{array}{l l}{{0}}&{{\mathrm{if~}\gamma_{c}<0}}\\
    {{n_{c}\mathrm{~if~}\gamma_{c}>n_{c}}}\\ {{\gamma_{c}\mathrm{~otherwise}}}\end{array}\right.$$
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
  zh: $$\gamma_{c}^{\prime}=\left\{\begin{array}{l l}{{0}}&{{\mathrm{如果~}\gamma_{c}<0}}\\
    {{n_{c}\mathrm{~如果~}\gamma_{c}>n_{c}}}\\ {{\gamma_{c}\mathrm{~其他情况}}}\end{array}\right.$$
- en: 'trim: This strategy forces the contribution of each h−1 ii to be reasonable.
    If h−1 ii is out-of-bounds, it is assumed to represent zero well-determined parameters:'
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
  zh: 'trim: 该策略强制每个 h−1 ii 的贡献保持合理。如果 h−1 ii 超出范围，则假设其代表零个已确定参数：'
- en: $h_{ii}^{-1'}=\begin{cases}1/\alpha_{[i]}\text{if}h_{ii}^{-1}\text{is not in}[0,1/\alpha_{[i]}]\\
    h_{ii}^{-1}\text{otherwise}\end{cases}$.
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
  zh: $h_{ii}^{-1'}=\begin{cases}1/\alpha_{[i]}\text{如果}h_{ii}^{-1}\text{不在}[0,1/\alpha_{[i]}]\\
    h_{ii}^{-1}\text{其他情况}\end{cases}$。
- en: 'snip: This strategy forces the contribution of each h−1 ii to be reasonable.
    If h−1 ii is out-of-bounds, it is assumed to represent one well-determined parameter:'
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
  zh: 'snip: 该策略强制每个 h−1 ii 的贡献保持合理。如果 h−1 ii 超出范围，则假设其代表一个已确定的参数：'
- en: $h_{ii}^{-1'}=\begin{cases}0&\text{if}h_{ii}^{-1}\text{is not in}[0,1/\alpha_{[i]}]\\
    h_{ii}^{-1}&\text{otherwise}\end{cases}$.
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
  zh: $h_{ii}^{-1'}=\begin{cases}0&\text{如果}h_{ii}^{-1}\text{不在}[0,1/\alpha_{[i]}]\\
    h_{ii}^{-1}&\text{其他情况}\end{cases}$。
- en: 'useold: This strategy forces the contribution of each h−1 ii to be reasonable.
    If h−1 ii is out-of-bounds the last good estimate of the well-determinedness of
    parameter i is used:'
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
  zh: useold：此策略强制每个 h−1 ii 的贡献合理。如果 h−1 ii 超出范围，则使用参数 i 的最后一个良好估计的确定性：
- en: $$\gamma_{c}^{\prime}=\sum_{i=1}^{n_{c}}\left\{\begin{array}{l l}{{1-\alpha_{c}^{*}h_{i
    i}^{-1^{*}}\mathrm{~if~}h_{i i}^{-1}\mathrm{~is~not~in~}[0,1/\alpha_{[i]}]}}\\
    {{1-\alpha_{c}h_{i i}^{-1}}}&{{\mathrm{~otherwise}}}\end{array}\right.$$
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
  zh: $$\gamma_{c}^{\prime}=\sum_{i=1}^{n_{c}}\left\{\begin{array}{l l}{{1-\alpha_{c}^{*}h_{i
    i}^{-1^{*}}\mathrm{~如果~}h_{i i}^{-1}\mathrm{~不在~}[0,1/\alpha_{[i]}]}}\\ {{1-\alpha_{c}h_{i
    i}^{-1}}}&{{\mathrm{~否则}}}\end{array}\right.$$
- en: where α∗c and h−1∗
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 α∗c 和 h−1∗
- en: ii are most recent values such that h−1 ii is in [0, 1/α[i]], or if there are
    no such values, h−1∗
  id: totrans-1276
  prefs: []
  type: TYPE_NORMAL
  zh: ii 是最近的值，使得 h−1 ii 在 [0, 1/α[i]] 中，或者如果没有这样的值，h−1∗
- en: ii = 0.
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
  zh: ii = 0。
- en: 'cond: This strategy only updates αc, using γc or a snipped version of γc, under
    the following conditions: (a) all eigenvalues of H are positive, and'
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
  zh: cond：此策略仅更新 αc，使用 γc 或 γc 的简化版本，在以下条件下：（a）H 的所有特征值都是正的，并且
- en: (b) for all i ∈ Ic, h−1 ii is in [0, 1/α[i]], or a snipped version of γc will
    result in a change in αc in the same direction as the last change when all the
    eigenvalues of H were positive and all the h−1 ii in group c were in bounds, and
    there have not been more than five such changes since all of the h−1 ii for group
    c have been in range.
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: （b）对于所有 i ∈ Ic，h−1 ii 在 [0, 1/α[i]] 中，或者 γc 的简化版本将导致 αc 在最后一次变化时的方向变化，当 H 的所有特征值为正且组
    c 中所有 h−1 ii 在范围内时，并且自从组 c 的所有 h−1 ii 一直在范围内以来没有超过五次这样的变化。
- en: 'cheap: This is Mackay''s [7] "cheap and cheerful" method, in which all parameters
    are assumed to be well determined, i.e.,'
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
  zh: 便宜：这是 Mackay 的 [7] “便宜而愉快”的方法，其中所有参数都假定为确定，即，
- en: $\gamma_c'=n_{c\cdots}$
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
  zh: $\gamma_c'=n_{c\cdots}$
- en: The advantage of this method is that Hessian need not be calculated. Mackay
    remarks that this method can be expected to perform poorly when there are a large
    number of poorly determined parameters.
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的优点是无需计算 Hessian。Mackay 指出，当有大量参数不确定时，该方法的表现可能不佳。
- en: 4.3.3 Further Generally Applicable Strategies
  id: totrans-1283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3.3 进一步通用的策略
- en: 'Several further strategies which could be combined with any of the ones already
    mentioned were also employed: nza: (no zero alphas) Do not accept an updated alpha
    value of zero (retain the old value).'
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: 还使用了几种可以与之前提到的任何策略结合的进一步策略：nza：（不接受零阿尔法）不接受更新的阿尔法值为零（保留旧值）。
- en: 'limit: Limit the the change in an alpha value to have a magnitude of no more
    than 10, i.e., round αc to be in the interval [0.1αc, 10αc].'
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
  zh: limit：限制阿尔法值的变化幅度不超过 10，即将 αc 四舍五入为在 [0.1αc, 10αc] 区间内。
- en: 'omit: If there are any h−1 ii not in [0, 1/α[i]], omit the corresponding rows
    and columns from H to give the smaller matrix H, and use the diagonal elements
    of H−1 for h−1 ii . The idea is to omit troublesome components of the model so
    that they do not interfere with estimates of well-determinedness for wellbehaved
    parameters. Strategies from the previous section are used to assign well-determinedness
    values for parameters with out-of-bound h−1 ii values in H−1 or H−1.'
  id: totrans-1286
  prefs: []
  type: TYPE_NORMAL
  zh: omit：如果有任何 h−1 ii 不在 [0, 1/α[i]] 中，则从 H 中省略相应的行和列，以得到更小的矩阵 H，并使用 H−1 的对角元素作为
    h−1 ii。其思想是省略模型中麻烦的组成部分，以免干扰良好参数的确定性估计。前一部分的策略用于为在 H−1 或 H−1 中超出范围的 h−1 ii 值的参数分配确定性值。
- en: 4.4 Experimental Setup
  id: totrans-1287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 实验设置
- en: 'The experiments reported here used seven different test functions, based on
    the five 2-dimensional functions used by Hwang et al. [4] and Roosen and Hastie
    [12]:'
  id: totrans-1288
  prefs: []
  type: TYPE_NORMAL
  zh: 此处报告的实验使用了七个不同的测试函数，基于 Hwang 等人 [4] 和 Roosen 与 Hastie [12] 使用的五个二维函数：
- en: $$1-0.5)^{2}$$
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
  zh: $$1-0.5)^{2}$$
- en: $${\mathfrak{h}})$$
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
  zh: $${\mathfrak{h}})$$
- en: $1\ \mathbb{F}^{\dagger}$
  id: totrans-1291
  prefs: []
  type: TYPE_NORMAL
  zh: $1\ \mathbb{F}^{\dagger}$
- en: 'linear function:'
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
  zh: 线性函数：
- en: 'f0(x1, x2) = (2x1 + x2)/0.6585 simple interaction:'
  id: totrans-1293
  prefs: []
  type: TYPE_NORMAL
  zh: f0(x1, x2) = (2x1 + x2)/0.6585 简单交互：
- en: f1(x1, x2) = 10.391((x1 − 0.4)(x2 − 0.6) + 0.36)
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
  zh: f1(x1, x2) = 10.391((x1 − 0.4)(x2 − 0.6) + 0.36)
- en: 'radial function:'
  id: totrans-1295
  prefs: []
  type: TYPE_NORMAL
  zh: 径向函数：
- en: f2(x1, x2) = 24.234((x1 − 0.5)2 + (x2 − 0.5)2)(0.75 − ((x1 − 0.5)2
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
  zh: f2(x1, x2) = 24.234((x1 − 0.5)2 + (x2 − 0.5)2)(0.75 − ((x1 − 0.5)2
- en: +(x2 − 0.5)2))
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
  zh: +(x2 − 0.5)2))
- en: 'harmonic function:'
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
  zh: 调和函数：
- en: f3(x1, x2) = 42.659(0.1+(x1 − 0.5)(0.05 − 10(x1 − 0.5)2(x2 − 0.5)2
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
  zh: f3(x1, x2) = 42.659(0.1+(x1 − 0.5)(0.05 − 10(x1 − 0.5)2(x2 − 0.5)2
- en: +(x1 − 0.5)4 + 5(x2 − 0.5)4))
  id: totrans-1300
  prefs: []
  type: TYPE_NORMAL
  zh: +(x1 − 0.5)4 + 5(x2 − 0.5)4))
- en: 'additive function:'
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
  zh: 加法函数：
- en: f4(x1, x2)=1.3356(1.5(1 − x1) + e(2x1−1) sin(3π(x1 − 0.6)2)
  id: totrans-1302
  prefs: []
  type: TYPE_NORMAL
  zh: f4(x1, x2)=1.3356(1.5(1 − x1) + e(2x1−1) sin(3π(x1 − 0.6)2)
- en: +e3(x2−0.5) sin(4π(x2 − 0.9)2))
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
  zh: +e3(x2−0.5) sin(4π(x2 − 0.9)2))
- en: 'complicated interaction:'
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂交互：
- en: f5(x1, x2)=1.9(1.35 + ex1 sin(13(x1 − 0.6)2)e−x2 sin(7x2))
  id: totrans-1305
  prefs: []
  type: TYPE_NORMAL
  zh: f5(x1, x2)=1.9(1.35 + ex1 sin(13(x1 − 0.6)2)e−x2 sin(7x2))
- en: 'interaction plus linear:'
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
  zh: 交互作用加线性：
- en: f6(x1, x2)=0.83045(f0(x1, x2) + f1(x1, x2))
  id: totrans-1307
  prefs: []
  type: TYPE_NORMAL
  zh: f6(x1, x2)=0.83045(f0(x1, x2) + f1(x1, x2))
- en: 'Training data for the binary classification tasks was generated by choosing
    250 xj1 and xj2 points from a uniform distribution over [0, 1]. Another 250 sj
    points were chosen from the uniform [0, 1] distribution to determine whether the
    target should be 0 or 1. The {0, 1} target t j i for case j for function i depended
    on the probability pji of a 1 calculated as the sigmoid of the function value
    (with the mean of the function subtracted):'
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制分类任务的训练数据是通过从[0, 1]均匀分布中选择250个 xj1 和 xj2 点生成的。另有250个 sj 点从均匀的[0, 1]分布中选择，以确定目标应为0还是1。针对函数
    i 的案例 j 的{0, 1}目标 t j i 依赖于计算得到的1的概率 pji，该概率为函数值的sigmoid（减去函数均值）：
- en: $$p_{i}^{j}={\frac{1}{1+e^{-\left(f_{i}(x_{1}^{j},x_{2}^{j})-\mu_{i}\right)}}}$$
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
  zh: $$p_{i}^{j}={\frac{1}{1+e^{-\left(f_{i}(x_{1}^{j},x_{2}^{j})-\mu_{i}\right)}}}$$
- en: $$t_{i}^{j}={\left\{\begin{array}{l l}{0}&{{\mathrm{~if~}}p_{i}^{j}<s^{j}}\\
    {1}&{{\mathrm{~otherwise~}}}\end{array}\right.}$$
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
  zh: $$t_{i}^{j}={\left\{\begin{array}{l l}{0}&{{\mathrm{~如果~}}p_{i}^{j}<s^{j}}\\
    {1}&{{\mathrm{~否则~}}}\end{array}\right.}$$
- en: where μi is the mean of each function over the unit square (μ0 = 2.28, μ1 =
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 μi 是每个函数在单位正方形上的均值（μ0 = 2.28, μ1 =
- en: 3.6, μ2 = 2.3, μ3 = 4.4, μ4 = 2.15, μ5 = 2.7, μ6 = 4.92).
  id: totrans-1312
  prefs: []
  type: TYPE_NORMAL
  zh: 3.6, μ2 = 2.3, μ3 = 4.4, μ4 = 2.15, μ5 = 2.7, μ6 = 4.92).
- en: A further 5 random distractor points chosen from a uniform [0, 1] distribution
    were concatenated with each (xj1, xj2) pair, to give a 7 dimensional functionapproximation
    task. These extra points were added so that in order for the neural network to
    generalize well it would be necessary that the automatic relevance determination
    set the weights from irrelevant inputs to zero (by driving the α's for those weights
    to high values).
  id: totrans-1313
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步选择的5个随机干扰点来自均匀[0, 1]分布，并与每个 (xj1, xj2) 对连接，以形成一个7维函数逼近任务。这些额外点的添加是为了使神经网络能够很好地进行泛化，因此需要自动相关性决定将来自无关输入的权重设为零（通过将这些权重的α值驱动到高值）。
- en: For testing, the probabilities were used as targets, rather than stochastic
    binary values. This was done to reduce the noise in measuring the test error.
    The test set inputs consisted of 400 (x1, x2) points from the uniform grid over
    [0, 1],
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试中，使用概率作为目标，而不是随机二进制值。这是为了减少测量测试误差时的噪声。测试集输入包含来自[0, 1]均匀网格的400个(x1, x2)点，
- en: 'and 400 vectors of distractors chosen from a uniform distribution over [0,
    1]. In order that test and training errors be comparable the test error was calculated
    as the expected error over the test points, assuming actual targets had been chosen
    randomly with the given target probabilities. The expected error for one test
    case was calculated as follows:'
  id: totrans-1315
  prefs: []
  type: TYPE_NORMAL
  zh: 以及从[0, 1]均匀分布中选择的400个干扰向量。为了使测试误差和训练误差可比，测试误差被计算为在测试点上的期望误差，假设实际目标是随机选择的，具有给定的目标概率。一个测试案例的期望误差计算如下：
- en: $$E_{i}^{j}=-\left(p_{i}^{j}\log y_{i}^{j}+(1-p_{i}^{j})\log(1-y_{i}^{j})\right)$$
  id: totrans-1316
  prefs: []
  type: TYPE_NORMAL
  zh: $$E_{i}^{j}=-\left(p_{i}^{j}\log y_{i}^{j}+(1-p_{i}^{j})\log(1-y_{i}^{j})\right)$$
- en: where yji was the prediction of the network for test case j.
  id: totrans-1317
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 yji 是网络对测试案例 j 的预测。
- en: The seven learning tasks are called I0 through I6 (the "I" is for "impure" in
    Roosen and Hastie's terminology [12].) It should be noted that these learning
    tasks are much harder than the tasks used by Hwang et al. [4], as the binomial
    outputs make the training data much noisier, and irrelevant inputs are present.
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
  zh: 七个学习任务被称为 I0 到 I6（“I”在 Roosen 和 Hastie 的术语中代表“杂质”[12]）。值得注意的是，这些学习任务比 Hwang
    等人使用的任务要难得多，因为二项输出使得训练数据更加嘈杂，并且存在无关输入。
- en: 4.4.1 Targets For "Good" Performance
  id: totrans-1319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4.1 "良好"表现的目标
- en: 'Various simple modeling strategies were applied to the data to give an indication
    of what level of test error performance was achievable, and to demonstrate how
    important it was to ignore the distractor inputs. Three different models were
    tried: linear and quadratic logistic models, and generalized additive models (GAMs)
    [3], each with and without the distractor inputs. No attempt was made to prevent
    overfitting, as the intention of these models was to show how much overfitting
    can occur if distractor are not ignored.'
  id: totrans-1320
  prefs: []
  type: TYPE_NORMAL
  zh: 采用了多种简单建模策略对数据进行处理，以表明可以实现的测试误差性能水平，并展示忽略干扰输入的重要性。尝试了三种不同的模型：线性和二次逻辑模型，以及广义加法模型（GAMs）[3]，每种模型都有和没有干扰输入。没有尝试防止过拟合，因为这些模型的意图是展示如果忽略干扰输入会发生多少过拟合。
- en: 'null: No inputs are used. The predicted output is the average of the targets
    (1 parameter).'
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
  zh: 'null: 不使用任何输入。预测输出是目标的平均值（1 个参数）。'
- en: 'true: The actual function value. This is included to indicate the level of
    noise in the data.'
  id: totrans-1322
  prefs: []
  type: TYPE_NORMAL
  zh: 'true: 实际函数值。包含此项是为了指示数据中的噪声水平。'
- en: 'lin: A logistic (linear) model fit using only x1 and x2 as inputs (3 parameters).'
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
  zh: 'lin: 一个仅使用 x1 和 x2 作为输入的逻辑（线性）模型（3 个参数）。'
- en: 'lin.D: A logistic (linear) model fit using all inputs, including the distractors
    (8 parameters).'
  id: totrans-1324
  prefs: []
  type: TYPE_NORMAL
  zh: 'lin.D: 一个使用所有输入，包括干扰项的逻辑（线性）模型（8 个参数）。'
- en: 'quad: A logistic model (with quadratic terms) fit using only x1 and x2 as inputs'
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
  zh: 'quad: 一个仅使用 x1 和 x2 作为输入的逻辑模型（带有二次项）。'
- en: (6 parameters).
  id: totrans-1326
  prefs: []
  type: TYPE_NORMAL
  zh: （6 个参数）。
- en: 'quad.D: A logistic model (with quadratic terms) fit using all inputs, including
    the distractors (36 parameters).'
  id: totrans-1327
  prefs: []
  type: TYPE_NORMAL
  zh: 'quad.D: 一个使用所有输入，包括干扰项的逻辑模型（36 个参数）。'
- en: 'gam: A generalized additive model fit using only x1 and x2 as inputs, with
    three degrees of freedom for each dimension (approx 7 parameters).'
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
  zh: 'gam: 一个仅使用 x1 和 x2 作为输入的广义加性模型，每个维度有三个自由度（大约 7 个参数）。'
- en: 'gam.D: A generalized additive model fit using all inputs, with three degrees
    of freedom for each dimension (approx 22 parameters).'
  id: totrans-1329
  prefs: []
  type: TYPE_NORMAL
  zh: 'gam.D: 一个使用所有输入的广义加性模型，每个维度有三个自由度（大约 22 个参数）。'
- en: 'T: The target for "good" network performance. The test set deviances for the
    various models and tasks are shown in Table 4.1, ordered by error for each task.
    Some tasks are easy, while others are very difficult'
  id: totrans-1330
  prefs: []
  type: TYPE_NORMAL
  zh: 'T: "好" 网络性能的目标。各种模型和任务的测试集偏差在表 4.1 中显示，按每个任务的错误排序。有些任务很简单，而另一些任务则非常困难。'
- en: (finding good solutions for task I3 appears to extremely difficult, as Roosen
    and Hastie [12] also discovered.) Targets for "good" neural network performance
    were derived from the errors achieved by any model. The targets were chosen to
    be achievable by neural network models and yet lower than the test set deviance
    achieved by any of the above simple models using all the inputs (except for I3,
    which neural networks had great difficulty with).
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
  zh: （寻找任务 I3 的好解决方案似乎极其困难，正如 Roosen 和 Hastie [12] 也发现的那样。）"好" 神经网络性能的目标是根据任何模型所达到的错误得出的。目标选择为神经网络模型可实现的，同时低于上述所有简单模型在使用所有输入（除了
    I3，神经网络对此非常困难）时所达到的测试集偏差。
- en: Table 4.1. Test set deviances (twice the error) for various models. Names ending
    in ".D"
  id: totrans-1332
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1. 各种模型的测试集偏差（误差的两倍）。
- en: are those of models which used both the distractor and relevant inputs. The
    number in parentheses beside the target for good network performance (T) is the
    number of networks which achieved this target at the end of training (out of 540).
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
  zh: 是使用干扰项和相关输入的模型的结果。括号中的数字代表在训练结束时达到这一"好"网络性能目标（T）的网络数量（共 540 个）。
- en: '| I0                                                                   | I1                                                                  |
    I2                                                         | I3                                                                                  |
    I4                                                                   | I5                                                                  |
    I6                                                                  |'
  id: totrans-1334
  prefs: []
  type: TYPE_TB
  zh: '| I0                                                                   | I1                                                                  |
    I2                                                         | I3                                                                                  |
    I4                                                                   | I5                                                                  |
    I6                                                                  |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1335
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| true 478 lin 487                                                     |                                                                     |                                                            |                                                                                     |                                                                      |                                                                     |                                                                     |'
  id: totrans-1336
  prefs: []
  type: TYPE_TB
  zh: '| true 478 lin 487                                                     |                                                                     |                                                            |                                                                                     |                                                                      |                                                                     |                                                                     |'
- en: '| T (165) 493 lin.D 498 quad 501 gam 502 gam.D 519 null 555 quad.D 556 | true
    488 quad 495                                                   |                                                            |                                                                                     |                                                                      |                                                                     |                                                                     |'
  id: totrans-1337
  prefs: []
  type: TYPE_TB
  zh: '| T (165) 493 lin.D 498 quad 501 gam 502 gam.D 519 null 555 quad.D 556 | true
    488 quad 495                                                   |                                                            |                                                                                     |                                                                      |                                                                     |                                                                     |'
- en: '|                                                                      | T
    (258) 520 lin 545 gam 548 null 556 lin.D 558 quad.D 595 gam.D 608 | true 475 quad
    494 gam 507                                  |                                                                                     |                                                                      |                                                                     |                                                                     |'
  id: totrans-1338
  prefs: []
  type: TYPE_TB
  zh: '|                                                                      | T
    (258) 520 lin 545 gam 548 null 556 lin.D 558 quad.D 595 gam.D 608 | true 475 quad
    494 gam 507                                  |                                                                                     |                                                                      |                                                                     |                                                                     |'
- en: '|                                                                      |                                                                     |
    T (44) 530 gam.D 551 null 555 lin 561 quad.D 575 lin.D 577 | true 495 gam 546
    T (7) 555 lin 556 lin.D 557 null 558 quad 566 gam.D 574 quad.D 669 | true 480
    gam 498                                                     |                                                                     |                                                                     |'
  id: totrans-1339
  prefs: []
  type: TYPE_TB
  zh: '|                                                                      |                                                                     |
    T (44) 530 gam.D 551 null 555 lin 561 quad.D 575 lin.D 577 | true 495 gam 546
    T (7) 555 lin 556 lin.D 557 null 558 quad 566 gam.D 574 quad.D 669 | true 480
    gam 498                                                     |                                                                     |                                                                     |'
- en: '|                                                                      |                                                                     |                                                            |                                                                                     |
    T (114) 530 quad 533 gam.D 537 lin 554 null 556 lin.D 558 quad.D 597 | true 491
    gam 539                                                    |                                                                     |'
  id: totrans-1340
  prefs: []
  type: TYPE_TB
  zh: '|                                                                      |                                                                     |                                                            |                                                                                     |
    T (114) 530 quad 533 gam.D 537 lin 554 null 556 lin.D 558 quad.D 597 | true 491
    gam 539                                                    |                                                                     |'
- en: '|                                                                      |                                                                     |                                                            |                                                                                     |                                                                      |
    T (58) 545 quad 547 lin 547 lin.D 555 null 555 gam.D 572 quad.D 620 | true 478
    quad 485                                                   |'
  id: totrans-1341
  prefs: []
  type: TYPE_TB
  zh: '|                                                                      |                                                                     |                                                            |                                                                                     |                                                                      |
    T (58) 545 quad 547 lin 547 lin.D 555 null 555 gam.D 572 quad.D 620 | true 478
    quad 485                                                   |'
- en: '|                                                                      |                                                                     |                                                            |                                                                                     |                                                                      |                                                                     |
    T (233) 500 lin 517 lin.D 523 gam 529 quad.D 540 gam.D 550 null 557 |'
  id: totrans-1342
  prefs: []
  type: TYPE_TB
  zh: '|                                                                      |                                                                     |                                                            |                                                                                     |                                                                      |                                                                     |
    T (233) 500 lin 517 lin.D 523 gam 529 quad.D 540 gam.D 550 null 557 |'
- en: 4.4.2 Network Architecture And Training
  id: totrans-1343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4.2 网络架构与训练
- en: Standard feed-forward networks were used, with details as follows. All networks
    had 3 to 15 hidden units, which computed the tanh function (a symmetric sigmoid).
    Inputs were in the range 0 to 1. The output unit computed the logistic function
    of its total input. Weights were initialized to random values drawn from a Gaussian
    distribution with variance 0.5.
  id: totrans-1344
  prefs: []
  type: TYPE_NORMAL
  zh: 标准前馈网络被使用，具体如下。所有网络都有3到15个隐藏单元，这些单元计算tanh函数（对称sigmoid）。输入范围为0到1。输出单元计算其总输入的logistic函数。权重初始化为从方差为0.5的高斯分布中抽取的随机值。
- en: 'Networks had nine hyperparameters (weight penalties): one for the weights for
    each input, one for hidden unit biases, and one for hidden to output weights.
    There was no penalty on the output bias. All hyperparameters were initialized
    to 0.5. Hyperparameters were allowed to increase to maximum value of 10,000.'
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
  zh: 网络有九个超参数（权重惩罚）：一个用于每个输入的权重，一个用于隐藏单元的偏置，一个用于隐藏到输出的权重。输出偏置没有惩罚。所有超参数初始化为0.5。超参数允许增加到最大值10,000。
- en: Networks were trained using a conjugate algorithm for the number of steps specified
    in Table 4.2. These numbers of steps were chosen give ample time for reasonably
    good methods to converge on some solution (the harder problems required more steps).
    Each step of the conjugate gradient algorithm involved one or more passes through
    the training set (the average was just over two). Training was terminated if the
    total number of passes through the training set exceeded 2.3 times the maximum
    allowed number of conjugate gradient steps.
  id: totrans-1346
  prefs: []
  type: TYPE_NORMAL
  zh: 网络使用共轭算法进行训练，步骤数量如表4.2所示。这些步骤数量的选择给予合理方法充分的时间去收敛到某个解（更困难的问题需要更多步骤）。每一步共轭梯度算法涉及一次或多次通过训练集（平均超过两次）。如果通过训练集的总次数超过2.3倍的共轭梯度步骤的最大允许次数，则训练将终止。
- en: 'The Hessian was calculated using the exact analytical method described in Buntine
    and Weigend [2]. This requires h + 1 passes through the training data, where h
    is the number of hidden units. This is usually far faster than a finitedifferences
    method, which requires n + 1 for the forward differences method and 2n+1 passes
    for the more accurate central differences method. The total number of floating
    point operations involved in the exact calculation of the Hessian is dominated
    by the update of the Hessian matrix (Eqn 15c in [2]). For a network with one output
    unit it is approximately 3.5(h + 1)Nn2 (there are 7 operations in each Hessian
    element update, but only half the elements need be computed as the matrix is symmetric).
    Eigendecomposition and inversion of the Hessian takes approximately 4/3n3 operations,
    but this is usually small compared to the calculation of the Hessian matrix. As
    long as hyperparameters are not updated too frequently, the time taken by Hessian
    evaluation and inversion is generally not an excessive amount on top of the time
    taken by the standard weight-training part of the procedure. For example, in the
    easier tasks (I0, I1, and I6) with the most frequent Hessian calculations (the
    "often" updates: 100 during), approximately one-third of the computation time
    was spent in Hessian calculations. In the more difficult tasks, relatively less
    time was spent in Hessian calculations because the training times were longer.'
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian 是使用 Buntine 和 Weigend [2] 中描述的精确分析方法计算的。这需要对训练数据进行 h + 1 次遍历，其中 h 是隐藏单元的数量。这通常比有限差分法快得多，后者对于前向差分法需要
    n + 1 次，而更准确的中心差分法需要 2n + 1 次遍历。Hessian 的精确计算所涉及的浮点运算总数主要由 Hessian 矩阵的更新主导（见 [2]
    中的公式 15c）。对于一个有一个输出单元的网络，大约需要 3.5(h + 1)Nn²（每个 Hessian 元素更新中有 7 次运算，但只需计算一半的元素，因为矩阵是对称的）。Hessian
    的特征分解和求逆大约需要 4/3n³ 次运算，但这通常相比于计算 Hessian 矩阵而言较小。只要超参数更新不太频繁，Hessian 评估和求逆所花费的时间通常不会超过标准权重训练过程所需的时间。例如，在较简单的任务
    (I0, I1, 和 I6) 中，Hessian 计算最频繁（“经常”更新：100 次），大约三分之一的计算时间用于 Hessian 计算。在较困难的任务中，由于训练时间较长，相对花费在
    Hessian 计算上的时间较少。
- en: Thirty six different combinations of the hyperparameter update strategies discussed
    in Section 4.3 were tested. Training on each problem was repeated five times with
    different initial weights. The same five sets of random initial weights were used
    for each strategy. This means that there were a total of 160 attempts to train
    each sized network for each problem.
  id: totrans-1348
  prefs: []
  type: TYPE_NORMAL
  zh: 测试了第 4.3 节讨论的三十六种不同的超参数更新策略组合。每个问题的训练重复了五次，使用了不同的初始权重。每种策略都使用相同的五组随机初始权重。这意味着每个问题的每个规模网络总共进行了
    160 次训练尝试。
- en: Table 4.2. Number of conjugate gradient steps allowed for different sized networks
    on the different tasks
  id: totrans-1349
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.2. 不同任务中不同规模网络允许的共轭梯度步数
- en: '| Number of hidden units     |      |            |       |    |'
  id: totrans-1350
  prefs: []
  type: TYPE_TB
  zh: '| 隐藏单元数               |      |            |       |    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-1351
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Task                       | 3    | 5          | 10    | 15 |'
  id: totrans-1352
  prefs: []
  type: TYPE_TB
  zh: '| 任务                     | 3    | 5          | 10    | 15 |'
- en: '| I0, I1, I6 1800 3000       | 6000 | -          |       |    |'
  id: totrans-1353
  prefs: []
  type: TYPE_TB
  zh: '| I0, I1, I6 1800 3000       | 6000 | -          |       |    |'
- en: '| I2, I3, I4 3000 6000 12000 | -    |            |       |    |'
  id: totrans-1354
  prefs: []
  type: TYPE_TB
  zh: '| I2, I3, I4 3000 6000 12000 | -    |            |       |    |'
- en: '| I5                         | -    | 6000 12000 | 30000 |    |'
  id: totrans-1355
  prefs: []
  type: TYPE_TB
  zh: '| I5                         | -    | 6000 12000 | 30000 |    |'
- en: 4.5 Effectiveness Of Control Strategies
  id: totrans-1356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 控制策略的有效性
- en: 'Effective of various combinations of control strategies was judged by whether
    or not the test error at the end of training was acceptable (using the deviance
    targets in Table 4.1). Good performance on any of the tasks was not possible without
    setting the hyperparameters in the appropriate ranges: low for the weights coming
    from the x1 and x2 inputs, and high for the weights coming from the distractor
    inputs. Figure 4.3 shows example plots of hyperparameter values versus test deviance
    for networks trained on task I6 (with jitter added to make dense clouds of points
    visible). Task I6 was a reasonably easy task: nearly all networks ended with appropriate
    low values for the relevant-input hyperparameters. Finding appropriate high values
    for distractor hyperparameters was more difficult, and those networks which did
    not did not perform well. All such plots had the same tendencies as those in Figure
    4.3: low test set deviance was achieved only be networks with low values for relevant-input
    hyperparameters and high values for distractor hyperparameters. Some of the other
    tasks were more difficult, e.g., I5, and poor search strategies would set relevant-input
    hyperparameters to high values, resulting in poor performance from ignoring the
    relevant inputs.'
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
  zh: 各种控制策略组合的有效性通过训练结束时的测试误差是否可接受来判断（使用表 4.1 中的偏差目标）。在未将超参数设置在适当范围内的情况下，任何任务的良好表现都是不可能的：来自
    x1 和 x2 输入的权重需要低值，而来自干扰输入的权重需要高值。图 4.3 显示了训练于任务 I6 的网络的超参数值与测试偏差的示例图（添加了抖动以使密集的点云可见）。任务
    I6 是一个相对简单的任务：几乎所有网络的相关输入超参数均以适当的低值结束。找到干扰超参数的适当高值更为困难，未能找到的网络表现不佳。所有此类图的趋势与图
    4.3 中的相同：只有相关输入超参数低值且干扰超参数高值的网络才能实现低测试集偏差。其他一些任务则更为困难，例如 I5，糟糕的搜索策略会将相关输入超参数设置为高值，从而导致忽视相关输入而表现不佳。
- en: '![107_image_0.png](107_image_0.png)'
  id: totrans-1358
  prefs: []
  type: TYPE_IMG
  zh: '![107_image_0.png](107_image_0.png)'
- en: Fig. 4.3. Final hyperparameter values versus test set deviances for networks
    with 5 hidden units trained on task I6
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3. 训练于任务 I6 的 5 个隐藏单元网络的最终超参数值与测试集偏差的关系
- en: The number of successes for each of the 36 (combinations of) strategies on each
    task is shown in Table 4.3. Each asterisk represents one success, i.e., a network
    which ended up with test set performance lower than the target for good performance
    (Table 4.1.) The maximum number of successes for any cell is 15, as 5 random starts
    were used for each of three different sized networks. The row totals are out of
    105, and the column totals are out of 540. The grand total (987 successes) is
    out of 3780.
  id: totrans-1360
  prefs: []
  type: TYPE_NORMAL
  zh: 每个任务的 36 种（组合）策略的成功次数如表 4.3 所示。每个星号代表一次成功，即网络在测试集上的表现低于良好表现的目标（表 4.1）。任何单元格的最大成功次数为
    15，因为每个大小不同的网络进行了 5 次随机启动。行总计为 105，列总计为 540。总成功次数（987 次）来自 3780。
- en: The clear overall best strategy is "snip+often". The special strategies "nza"
    and "limit" seem to help a little. These conclusions are strengthened by examining
    plots of the evolution of test deviance during training. Figures 4.4 and 4.5 show
    plots of test deviance during training for networks with 5 hidden units on task
    I6, and for networks with 10 hidden units on task I4. There are five lines in
    each plot because five random starts were used for each network. The ideal network
    has a test deviance which descends rapidly and then stays below the target performance
    (the dotted line) - this shows that the strategy quickly found good values for
    hyperparameters (the tasks were set up so that it was not possible to achieve
    low test deviance without having appropriate values for the hyperparameters).
    Lines which flatten out at around 557 are for networks whose hyperparameters have
    all been set to high values, so that the network ignores all inputs and thus has
    the same performance as the null model (see Table 4.1). The search was terminated
    early for many of these networks, because no hyperparameters or weights were changing.
    Note that few of the networks with 10 hidden units reached the target for good
    performance on task I4 though networks with 5 hidden units did much better. This
    indicates that the redundancy in these networks could not be effectively controlled.
  id: totrans-1361
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的整体策略是“剪切+频繁”。特别策略“nza”和“limit”似乎有一点帮助。通过检查训练过程中测试偏差的演变图，这些结论得到了加强。图4.4和4.5展示了在任务I6上具有5个隐藏单元的网络以及在任务I4上具有10个隐藏单元的网络的测试偏差图。每个图中有五条线，因为每个网络使用了五次随机启动。理想网络的测试偏差迅速下降，然后保持在目标性能（虚线）以下——这表明策略迅速找到了超参数的良好值（这些任务设置使得在没有适当的超参数值时无法实现低测试偏差）。在大约557处平坦的线条是超参数都设置为高值的网络，因此网络忽略所有输入，其性能与零模型相同（见表4.1）。由于许多这些网络没有超参数或权重变化，搜索提前终止。注意，尽管具有5个隐藏单元的网络表现得更好，但很少有10个隐藏单元的网络在任务I4上达到了良好性能的目标。这表明这些网络中的冗余无法有效控制。
- en: '|                            | total          | I0                         |
    I1              | I2 I3       | I4                              | I5         |
    I6       |       |'
  id: totrans-1362
  prefs: []
  type: TYPE_TB
  zh: '|                            | total          | I0                         |
    I1              | I2 I3       | I4                              | I5         |
    I6       |       |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1363
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| cheap+often                | 4              | ****                       |                 |             |                                 |            |          |       |'
  id: totrans-1364
  prefs: []
  type: TYPE_TB
  zh: '| cheap+often                | 4              | ****                       |                 |             |                                 |            |          |       |'
- en: '| group+medium               | 10             | ***                        |
    ***             | *           | ***                             |            |          |       |'
  id: totrans-1365
  prefs: []
  type: TYPE_TB
  zh: '| group+medium               | 10             | ***                        |
    ***             | *           | ***                             |            |          |       |'
- en: '| group+patience             | 10             | *                          |
    ***             | *           | *****                           |            |          |       |'
  id: totrans-1366
  prefs: []
  type: TYPE_TB
  zh: '| group+patience             | 10             | *                          |
    ***             | *           | *****                           |            |          |       |'
- en: '| none+patience              | 10             | ****                       |
    *               | *****       |                                 |            |          |       |'
  id: totrans-1367
  prefs: []
  type: TYPE_TB
  zh: '| none+patience              | 10             | ****                       |
    *               | *****       |                                 |            |          |       |'
- en: '| none+rare                  | 11             | ***                        |
    **              | ******      |                                 |            |          |       |'
  id: totrans-1368
  prefs: []
  type: TYPE_TB
  zh: '| none+rare                  | 11             | ***                        |
    **              | ******      |                                 |            |          |       |'
- en: '| group+rare                 | 12             | ***                        |
    ***             | ******      |                                 |            |          |       |'
  id: totrans-1369
  prefs: []
  type: TYPE_TB
  zh: '| group+rare                 | 12             | ***                        |
    ***             | ******      |                                 |            |          |       |'
- en: '| snip+rare                  | 14             | **                         |
    **              | ****        | ******                          |            |          |       |'
  id: totrans-1370
  prefs: []
  type: TYPE_TB
  zh: '| snip+rare                  | 14             | **                         |
    **              | ****        | ******                          |            |          |       |'
- en: '| trim+often                 | 16             | *****                      |
    ****            | ****        | ***                             |            |          |       |'
  id: totrans-1371
  prefs: []
  type: TYPE_TB
  zh: '| 修剪+偶尔                   | 16             | *****                      | ****            |
    ****        | ***                             |            |          |       |'
- en: '| trim+patience              | 17             | ***                        |
    ******          | *           | *                               | *          |
    *****    |       |'
  id: totrans-1372
  prefs: []
  type: TYPE_TB
  zh: '| 修剪+耐心                   | 17             | ***                        | ******          |
    *           | *                               | *          | *****    |       |'
- en: '| none+medium                | 18             | ****                       |
    ******          | *           | *                               | **         |
    ****     |       |'
  id: totrans-1373
  prefs: []
  type: TYPE_TB
  zh: '| 无+中等                     | 18             | ****                       |
    ******          | *           | *                               | **         |
    ****     |       |'
- en: '| omit+snip+patience         | 18             | *******                    |
    ******          | *****       |                                 |            |          |       |'
  id: totrans-1374
  prefs: []
  type: TYPE_TB
  zh: '| 省略+剪切+耐心              | 18             | *******                    | ******          |
    *****       |                                 |            |          |       |'
- en: '| omit+snip+patience+limit   | 18             | *******                    |
    ******          | *****       |                                 |            |          |       |'
  id: totrans-1375
  prefs: []
  type: TYPE_TB
  zh: '| 省略+剪切+耐心+限制        | 18             | *******                    | ******          |
    *****       |                                 |            |          |       |'
- en: '| trim+medium                | 20             | *****                      |
    ******          | ***         | ***                             | ***        |          |       |'
  id: totrans-1376
  prefs: []
  type: TYPE_TB
  zh: '| 修剪+中等                   | 20             | *****                      | ******          |
    ***         | ***                             | ***        |          |       |'
- en: '| trim+rare                  | 20             | **                         |
    ******          | ***         | ****                            | *****      |          |       |'
  id: totrans-1377
  prefs: []
  type: TYPE_TB
  zh: '| 修剪+稀有                   | 20             | **                         | ******          |
    ***         | ****                            | *****      |          |       |'
- en: '| useold+patience            | 20             | ****                       |
    ******          | *           | *                               | **         |
    *        | ***** |'
  id: totrans-1378
  prefs: []
  type: TYPE_TB
  zh: '| 使用旧版+耐心              | 20             | ****                       | ******          |
    *           | *                               | **         | *        | *****
    |'
- en: '| omit+useold+patience       | 21             | ****                       |
    *******         | ***         | **                              | *****      |          |       |'
  id: totrans-1379
  prefs: []
  type: TYPE_TB
  zh: '| 省略+使用旧版+耐心         | 21             | ****                       | *******         |
    ***         | **                              | *****      |          |       |'
- en: '| omit+useold+patience+limit | 22             | ****                       |
    ******          | ****        | ***                             | *****      |          |       |'
  id: totrans-1380
  prefs: []
  type: TYPE_TB
  zh: '| 省略+使用旧版+耐心+限制    | 22             | ****                       | ******          |
    ****        | ***                             | *****      |          |       |'
- en: '| cheap+medium               | 22             | *****                      |
    *****           | ***         | *                               | ********   |          |       |'
  id: totrans-1381
  prefs: []
  type: TYPE_TB
  zh: '| 便宜+中等                   | 22             | *****                      | *****           |
    ***         | *                               | ********   |          |       |'
- en: '| cheap+patience             | 23             | *****                      |
    *****           | *           | **                              | ********** |          |       |'
  id: totrans-1382
  prefs: []
  type: TYPE_TB
  zh: '| 便宜+耐心                   | 23             | *****                      | *****           |
    *           | **                              | ********** |          |       |'
- en: '| none+often                 | 26             | ********                   |
    *********       | *           | ***                             | **         |
    ***      |       |'
  id: totrans-1383
  prefs: []
  type: TYPE_TB
  zh: '| 无+经常                     | 26             | ********                   |
    *********       | *           | ***                             | **         |
    ***      |       |'
- en: '| snip+medium                | 26             | ******                     |
    *********       | *****       | **                              | ****       |          |       |'
  id: totrans-1384
  prefs: []
  type: TYPE_TB
  zh: '| 剪切+中等                   | 26             | ******                     | *********       |
    *****       | **                              | ****       |          |       |'
- en: '| snip+patience              | 26             | ******                     |
    *****           | *           | *****                           | ****       |
    *****    |       |'
  id: totrans-1385
  prefs: []
  type: TYPE_TB
  zh: '| 剪切+耐心                   | 26             | ******                     | *****           |
    *           | *****                           | ****       | *****    |       |'
- en: '| omit+trim+patience         | 27             | **********                 |
    ******          | **          | **                              | **         |
    *****    |       |'
  id: totrans-1386
  prefs: []
  type: TYPE_TB
  zh: '| 省略+修剪+耐心             | 27             | **********                 | ******          |
    **          | **                              | **         | *****    |       |'
- en: '| snip+patience+nza+limit    | 27             | *****                      |
    *******         | *           | *****                           | ****       |
    *****    |       |'
  id: totrans-1387
  prefs: []
  type: TYPE_TB
  zh: '| 剪切+耐心+nza+限制         | 27             | *****                      | *******         |
    *           | *****                           | ****       | *****    |       |'
- en: '| omit+useold+often          | 28             | *********                  |
    ********        | ***         | **                              | ******     |          |       |'
  id: totrans-1388
  prefs: []
  type: TYPE_TB
  zh: '| omit+useold+often          | 28             | *********                  |
    ********        | ***         | **                              | ******     |          |       |'
- en: '| omit+useold+often+limit    | 29             | *********                  |
    ********        | **          | ***                             | *******    |          |       |'
  id: totrans-1389
  prefs: []
  type: TYPE_TB
  zh: '| omit+useold+often+limit    | 29             | *********                  |
    ********        | **          | ***                             | *******    |          |       |'
- en: '| snip+patience+nza          | 29             | *******                    |
    *******         | *           | *****                           | ****       |
    *****    |       |'
  id: totrans-1390
  prefs: []
  type: TYPE_TB
  zh: '| snip+patience+nza          | 29             | *******                    |
    *******         | *           | *****                           | ****       |
    *****    |       |'
- en: '| group+often                | 31             | ********                   |
    ***********     | *           | *                               | **         |
    ******** |       |'
  id: totrans-1391
  prefs: []
  type: TYPE_TB
  zh: '| group+often                | 31             | ********                   |
    ***********     | *           | *                               | **         |
    ******** |       |'
- en: '| cond+often+limit+nza       | 32             | *************              |
    **********      | *********   |                                 |            |          |       |'
  id: totrans-1392
  prefs: []
  type: TYPE_TB
  zh: '| cond+often+limit+nza       | 32             | *************              |
    **********      | *********   |                                 |            |          |       |'
- en: '| omit+snip+often+limit      | 32             | ************               |
    *********       | *********** |                                 |            |          |       |'
  id: totrans-1393
  prefs: []
  type: TYPE_TB
  zh: '| omit+snip+often+limit      | 32             | ************               |
    *********       | *********** |                                 |            |          |       |'
- en: '| useold+often               | 32 *********** | ********                   |
    ****            | **          | *******                         |            |          |       |'
  id: totrans-1394
  prefs: []
  type: TYPE_TB
  zh: '| useold+often               | 32 *********** | ********                   |
    ****            | **          | *******                         |            |          |       |'
- en: '| cheap+rare                 | 34             | *********                  |
    ********* ***** | **          | *********                       |            |          |       |'
  id: totrans-1395
  prefs: []
  type: TYPE_TB
  zh: '| cheap+rare                 | 34             | *********                  |
    ********* ***** | **          | *********                       |            |          |       |'
- en: '| omit+snip+often            | 36             | **                         |
    *************   | ********    | *************                   |            |          |       |'
  id: totrans-1396
  prefs: []
  type: TYPE_TB
  zh: '| omit+snip+often            | 36             | **                         |
    *************   | ********    | *************                   |            |          |       |'
- en: '| snip+often                 | 50             | ******** ***************   |
    ** **           | ****        | ***** **************            |            |          |       |'
  id: totrans-1397
  prefs: []
  type: TYPE_TB
  zh: '| snip+often                 | 50             | ******** ***************   |
    ** **           | ****        | ***** **************            |            |          |       |'
- en: '| snip+often+nza             | 54             | ********** ***************
    | ** **           | *****       | ****** **************           |            |          |       |'
  id: totrans-1398
  prefs: []
  type: TYPE_TB
  zh: '| snip+often+nza             | 54             | ********** ***************
    | ** **           | *****       | ****** **************           |            |          |       |'
- en: '| snip+often+nza+limit       | 54             | **********                 |
    **************  | *           | ******* ******** ************** |            |          |       |'
  id: totrans-1399
  prefs: []
  type: TYPE_TB
  zh: '| snip+often+nza+limit       | 54             | **********                 |
    **************  | *           | ******* ******** ************** |            |          |       |'
- en: '|                            | 987            | 165                        |
    258             | 44          | 7                               | 114        |
    58       | 233   |'
  id: totrans-1400
  prefs: []
  type: TYPE_TB
  zh: '|                            | 987            | 165                        |
    258             | 44          | 7                               | 114        |
    58       | 233   |'
- en: Table 4.3. Number of successes for each strategy on each task
  id: totrans-1401
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.3. 各策略在每个任务上的成功次数
- en: Update strategies other than "snip" tend to be very unstable. Frequent updates
    seem essential for achieving good final performance within a reasonable amount
    of time. The more complex strategies for getting or retaining good estimates of
    γ's, i.e., omit and useold, seemed to be of little benefit.
  id: totrans-1402
  prefs: []
  type: TYPE_NORMAL
  zh: 除“snip”外的更新策略往往非常不稳定。频繁的更新似乎对于在合理的时间内实现良好的最终性能至关重要。获取或保留良好的γ估计的更复杂策略，如omit和useold，似乎没有太大好处。
- en: 'Finding good hyperparameter values is difficult. If updates are too frequent,
    α''s can rise uncontrollably or become unstable. If updates are too infrequent,
    the search is too slow. Uncontrollable rises in α''s can occur when the network
    has not started using the weights it needs, and those weights are small. In these
    circumstances α is overestimated, which forces weights to become smaller in a
    runaway-feedback process. Instability results because of the feedback between
    γ and α: a change in γ causes a same direction change in α, and a change in alpha
    causes an opposite direction change in γ. Thus, if γ is overestimated, this leads
    to an overestimation in α, which lead to a lower reestimate of γ, which in turn
    can lead to a lower reestimate of α. While this process sometimes results in stable
    self-correcting behavior, other times it results in uncontrolled oscillations
    (which is what is happening with the "none+often" strategy: second row, third
    column in Figures 4.4 and 4.5). In general, while it is slow to raise an α from
    a value that is too low, it is more difficult to lower an α from a value which
    is too high (because the weights are forced to zero immediately). The reasons
    for the differing behavior of the various strategies appear to be as follows:'
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找良好的超参数值是困难的。如果更新过于频繁，α可能会失控或变得不稳定。如果更新过于不频繁，搜索就太慢。当网络尚未开始使用所需权重且这些权重较小时，可能会发生α的失控上升。在这些情况下，α被高估，这迫使权重在失控的反馈过程中变得更小。由于γ和α之间的反馈，导致不稳定：γ的变化导致α同方向变化，α的变化则导致γ反方向变化。因此，如果γ被高估，这将导致α的高估，进一步导致γ的低估，这反过来又可能导致α的低估。虽然这个过程有时会导致稳定的自我修正行为，但其他时候则会导致失控的振荡（这正是“无+经常”策略发生的情况：见图4.4和4.5的第二行第三列）。一般来说，从过低的值提高α是缓慢的，而从过高的值降低α则更加困难（因为权重会立即被迫降为零）。各种策略的不同表现的原因如下：
- en: 'none, group: Frequently give out-of-bounds or very poor estimates for γ, and
    cause instability in the search.'
  id: totrans-1404
  prefs: []
  type: TYPE_NORMAL
  zh: 无，组：经常给出超出范围或非常糟糕的γ估计，并导致搜索的不稳定。
- en: 'cond: Results in fairly stable behavior, but often does not find the optimal
    values for all α''s because it stops updating them.'
  id: totrans-1405
  prefs: []
  type: TYPE_NORMAL
  zh: 调整：导致相当稳定的行为，但通常无法找到所有α的最优值，因为它停止了对它们的更新。
- en: 'cheap: Overestimates γ and kills weights too quickly. trim: Sometimes underestimates
    γ because it assumes zero degrees of freedom when the γ estimates from the Hessian
    are out-of-bounds, and thus reduces α values which should be large, resulting
    in instability.'
  id: totrans-1406
  prefs: []
  type: TYPE_NORMAL
  zh: 便宜：高估了γ，并过快地消耗权重。修剪：有时低估了γ，因为它假设在Hessian中的γ估计超出范围时自由度为零，从而降低了本应较大的α值，导致不稳定。
- en: 'snip: Sometimes overestimates γ because it assumes one degree of freedom for
    a parameter whose γ estimate from the Hessian is out-of-bounds. However, this
    keeps α values high without raising them too much: overestimation of γ appears
    to be much less harmful than underestimation.'
  id: totrans-1407
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪：有时高估γ，因为它假设某个参数的自由度为一，而其Hessian中γ的估计超出范围。然而，这使得α值保持较高，而不会过多提升：高估γ似乎远比低估更不具害。
- en: Update frequency was also important. With good α calculation strategies, updating
    frequently (i.e., "often") gave fastest convergence to good α values and best
    overall performance. Waiting for some degree of convergence in the conjugate gradient
    search before updating α values (i.e., "patience") was of no benefit at all.
  id: totrans-1408
  prefs: []
  type: TYPE_NORMAL
  zh: 更新频率也很重要。通过良好的α计算策略，频繁更新（即“经常”）能最快收敛到良好的α值，并获得最佳整体表现。在更新α值之前，等待共轭梯度搜索中的某种收敛程度（即“耐心”）根本没有任何好处。
- en: 4.6 Relationship Of Test Set Error To Evidence
  id: totrans-1409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 测试集错误与证据的关系
- en: 'If we have trained a number of networks, we often want to know which will have
    the lowest generalization error. The "evidence" value calculated for each network
    can be used choose networks which will perform well. The evidence is the log likelihood
    of the data given the values of the hyperparameters, integrated over weight values
    based on the assumption of a Gaussian distribution for the posterior of the weights.
    The evidence for a network evaluated with cross-entropy error is as follows (Eqn
    10.67 in [1]; Eqn 30 in [7]):'
  id: totrans-1410
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们训练了多个网络，我们通常想知道哪个网络的泛化误差最低。为每个网络计算的“证据”值可用于选择表现良好的网络。证据是给定超参数值的数据的对数似然，基于权重值的假设集成在权重的后验高斯分布下。使用交叉熵误差评估的网络的证据如下（见[1]中的公式10.67；[7]中的公式30）：
- en: $$\ln p(D|\alpha)=-\frac{1}{2}\sum_{i=1}^{n}\alpha_{[i]}w_{i}^{2}-E-\frac{1}{2}|H|+\sum_{c}\frac{n_{c}}{2}\ln\alpha_{c}-\frac{N}{2}\ln(2\Pi)\tag{4.4}$$
  id: totrans-1411
  prefs: []
  type: TYPE_NORMAL
  zh: $$\ln p(D|\alpha)=-\frac{1}{2}\sum_{i=1}^{n}\alpha_{[i]}w_{i}^{2}-E-\frac{1}{2}|H|+\sum_{c}\frac{n_{c}}{2}\ln\alpha_{c}-\frac{N}{2}\ln(2\Pi)\tag{4.4}$$
- en: Whether or not the evidence is a good guide to which networks will perform well
    is questionable, as various assumptions on which evidence calculations are based
    are often violated in particular networks. The simulations performed offer a good
    opportunity to examine how accurately high evidence indicates good test set performance.
  id: totrans-1412
  prefs: []
  type: TYPE_NORMAL
  zh: 证据是否是判断哪些网络表现良好的好指南是有问题的，因为证据计算所基于的各种假设在特定网络中常常被违反。所进行的模拟提供了一个良好的机会来检查高证据如何准确地指示良好的测试集性能。
- en: The evidence is particularly sensitive to low eigenvalues in the Hessian of
    the network. The validity of the evidence value is doubtful in cases where the
    Hessian has low or negative eigenvalues. Bishop [1] recommends omitting eigenvalues
    which are lower than some threshold from the calculation of the evidence.
  id: totrans-1413
  prefs: []
  type: TYPE_NORMAL
  zh: 证据对网络Hessian中的低特征值特别敏感。在Hessian具有低或负特征值的情况下，证据值的有效性值得怀疑。Bishop [1] 建议在计算证据时省略低于某个阈值的特征值。
- en: 'Figure 4.6 shows plots of test deviances that would be achieved by selecting
    twenty networks based on their evidence values. Two different methods of calculating
    the evidence were used: (a) ignore negative eigenvalues of the Hessian, and'
  id: totrans-1414
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 显示了通过根据证据值选择二十个网络而获得的测试偏差的图。使用了两种不同的方法来计算证据：(a) 忽略Hessian的负特征值，和
- en: (b) if an eigenvalue is lower than the smallest non-zero αc, replace it with
    that αc
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 如果特征值低于最小的非零αc，则用该αc替代它。
- en: '("clipped evidence"). Because the evidence is sensitive to low eigenvalues,
    four different filters were applied to networks: (a) use all networks; (b) throw
    out networks'
  id: totrans-1416
  prefs: []
  type: TYPE_NORMAL
  zh: (“剪裁证据”). 由于证据对低特征值敏感，四种不同的过滤器被应用于网络：(a) 使用所有网络；(b) 丢弃网络
- en: '![110_image_0.png](110_image_0.png)'
  id: totrans-1417
  prefs: []
  type: TYPE_IMG
  zh: '![110_image_0.png](110_image_0.png)'
- en: I6. The horizontal axis is the number of passes through the training set. The
    dotted line is the target for good performance. The minimum value quoted is the
    minimum deviance at any point in training.
  id: totrans-1418
  prefs: []
  type: TYPE_NORMAL
  zh: I6. 横轴是通过训练集的次数。虚线是良好性能的目标。引用的最小值是在训练的任何时刻的最小偏差。
- en: '![111_image_0.png](111_image_0.png)'
  id: totrans-1419
  prefs: []
  type: TYPE_IMG
  zh: '![111_image_0.png](111_image_0.png)'
- en: I4. The horizontal axis is the number of passes through the training set. The
    dotted line is the target for good performance. The minimum value quoted is the
    minimum deviance at any point in training.
  id: totrans-1420
  prefs: []
  type: TYPE_NORMAL
  zh: I4. 横轴是通过训练集的次数。虚线是良好性能的目标。引用的最小值是在训练的任何时刻的最小偏差。
- en: '![112_image_0.png](112_image_0.png)'
  id: totrans-1421
  prefs: []
  type: TYPE_IMG
  zh: '![112_image_0.png](112_image_0.png)'
- en: Fig. 4.6. Distribution of test errors for the 20 "best" nets selected according
    to evidence and network diagnostics
  id: totrans-1422
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6. 根据证据和网络诊断选出的20个“最佳”网络的测试误差分布
- en: with negative eigenvalues; (c) throw out networks with eigenvalues lower than
    the the smallest αc; and (d) throw out networks with out-of-bounds h−1 ii values.
  id: totrans-1423
  prefs: []
  type: TYPE_NORMAL
  zh: 带有负特征值；(c) 丢弃特征值低于最小的αc的网络；(d) 丢弃具有超出界限的h−1 ii值的网络。
- en: The first row in each plot of Figure 4.6 shows the distribution of the actual
    best 20 test deviances (this would normally be impossible to calculate, but we
    can calculate it here because we have artificial tasks.) If evidence were a perfect
    predictor of test set performance, the 20 networks with highest evidence would
    be the same 20 networks, but it is not, as the rest of the figure shows. The remaining
    eight rows show the distribution of test errors for nets with the highest evidence,
    with evidence calculated in both of the ways described above. Lower pairs of rows
    rules more nets out of consideration, based on diagnostics of the Hessian and
    inverse Hessian.
  id: totrans-1424
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 中每个图的第一行显示了实际最佳的20个测试偏差的分布（这通常是无法计算的，但我们在这里可以计算，因为我们有人工任务）。如果证据是测试集性能的完美预测器，那么证据最高的20个网络将是同样的20个网络，但事实并非如此，剩余的图示明了这一点。其余八行显示了证据最高的网络的测试误差分布，证据是按照上述两种方法计算的。较低的行对更多网络进行排除，基于Hessian和逆Hessian的诊断。
- en: The two different ways of calculating the evidence did not make any discernible
    difference. The box-and-whisker plots show the median as a solid circle. The box
    shows the upper and lower quartiles (i.e., 50% of points are in the box).
  id: totrans-1425
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种计算证据的方法没有明显的差异。箱线图以实心圆表示中位数。箱体显示上四分位数和下四分位数（即，50%的数据点位于箱体内）。
- en: The whiskers show the furthest points within 1.5 times the interquartile range
    (the length of the box) of the quartiles. Points outside the whiskers are plotted
    individually with hollow circles.
  id: totrans-1426
  prefs: []
  type: TYPE_NORMAL
  zh: 胡须表示四分位数范围（箱子的长度）内1.5倍的最远点。胡须之外的点用空心圆单独绘制。
- en: Plots of the evidence versus test deviance show a strong correlation, which
    seems strongest and most reliable for networks which have no low eigenvalues.
    However, the plots in Figure 4.6 show that using the evidence to select networks
    is not always reliable. Networks with high evidence but with poorly conditioned
    Hessians have a wide range of test error, but some networks with the lowest test
    error have poorly conditioned Hessians. This means that allowing networks with
    poorly conditioned Hessians resulted in choosing too many networks with high test
    error, whereas rejecting networks with poorly conditioned Hessians rejected too
    many of the networks with low test error. This seemed to be more of a problem
    for the more difficult tasks. On the easier tasks (e.g., I6), networks with high
    evidence and no low eigenvalues had very low test deviance.
  id: totrans-1427
  prefs: []
  type: TYPE_NORMAL
  zh: 证据与测试偏差的图表显示出强烈的相关性，这在没有低特征值的网络中似乎最强且最可靠。然而，图4.6中的图表显示，使用证据选择网络并不总是可靠。具有高证据但条件不良的海森矩阵的网络，其测试误差范围很广，但一些测试误差最低的网络则具有条件不良的海森矩阵。这意味着允许具有条件不良海森矩阵的网络导致选择过多的高测试误差网络，而拒绝条件不良海森矩阵的网络则拒绝了过多的低测试误差网络。这在更困难的任务中似乎更成问题。在较简单的任务（例如，I6）中，具有高证据且没有低特征值的网络具有非常低的测试偏差。
- en: 4.7 Conclusion
  id: totrans-1428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.7 结论
- en: For classification networks, the search for optimal hyperparameters can be slow
    and unstable. However, the search can be improved by using the strategies summarized
    below. Similar experiments with regression networks (with linear output units
    and sum-square errors, on tasks with continuous targets and Gaussian noise) revealed
    that the hyperparameter search in such tasks is generally faster and more stable
    than in classification tasks. Only when the tasks had very high noise in the training
    cases (with variance of 4 or greater) did the hyperparameter search become at
    all difficult. Under these conditions, none of the strategies tried was clearly
    superior.
  id: totrans-1429
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类网络，优化超参数的搜索可能缓慢且不稳定。然而，通过使用下面总结的策略，可以改善搜索。类似的回归网络实验（具有线性输出单元和平方和误差，处理具有连续目标和高斯噪声的任务）表明，在这些任务中，超参数搜索通常比分类任务更快且更稳定。只有当任务在训练案例中噪声非常高（方差为4或更大）时，超参数搜索才会变得困难。在这些条件下，尝试的策略没有明显的优越性。
- en: One of the main causes of instability in the hyperparameter search is low eigenvalues
    in the Hessian. In turn, one of the main causes of this is redundancies in the
    network. It is easily verified that the Hessian for a network with redundancies
    can have zero or close to zero eigenvalues - the only thing that prevents the
    eigenvalues being zero is non-zero hyperparameters. In neural networks both additive
    (parallel) and multiplicative (serial) redundancies can occur.
  id: totrans-1430
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数搜索不稳定的主要原因之一是海森矩阵中的低特征值。反过来，这主要是由网络中的冗余引起的。可以轻易验证，具有冗余的网络的海森矩阵可能具有零或接近零的特征值——防止特征值为零的唯一因素是非零的超参数。在神经网络中，既可以发生加法（并行）冗余，也可以发生乘法（串行）冗余。
- en: Larger networks are more likely to have redundancies. Hence we could expect
    the search to be more stable for smaller networks, and this is what was observed
    (though of course some small networks did not perform well as they did not have
    sufficient capacity to model the task).
  id: totrans-1431
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的网络更可能存在冗余。因此，我们可以预期较小网络的搜索会更稳定，而这也是观察到的结果（当然，一些小网络表现不佳，因为它们没有足够的能力来建模任务）。
- en: The initial hyperparameter and weight values are important. If the initial hyperparameters
    are too high, they can force all the weights to zero before the network has a
    chance to learn anything. If the initial hyperparameters are too low the network
    can get trapped in an overfitting mode. Thodberg [13] makes the reasonable suggestion
    that the initial hyperparameters should be set so that the weight cost is 10%
    of the error at the beginning of training.
  id: totrans-1432
  prefs: []
  type: TYPE_NORMAL
  zh: 初始超参数和权重值很重要。如果初始超参数过高，可能会在网络有机会学习之前就迫使所有权重变为零。如果初始超参数过低，网络可能会陷入过拟合模式。Thodberg
    [13] 合理建议初始超参数应设置为权重成本在训练开始时占误差的10%。
- en: 'The results described in this chapter lead to the following recommendations
    for hyperparameter updates:'
  id: totrans-1433
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所述的结果导致以下超参数更新建议：
- en: '- update hyperparameters frequently'
  id: totrans-1434
  prefs: []
  type: TYPE_NORMAL
  zh: '- 频繁更新超参数'
- en: '- if any h−1 ii values are out-of-bounds, replace them with zero in the calculation
    of γ[i] (so that each out-of-bounds h−1 ii contributes one well-determined parameter)'
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果任何 h−1 ii 值超出范围，在计算 γ[i] 时将其替换为零（以使每个超出范围的 h−1 ii 贡献一个确定的参数）'
- en: '- ignore updates for αc which suggest a zero value - limit changes in αc to
    have a maximum magnitude of 10.'
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
  zh: '- 忽略建议零值的 αc 更新 - 将 αc 的变化限制在最大幅度为10。'
- en: '- ignore negative eigenvalues in calculations of the evidence - regard evidence
    values as unreliable for networks with eigenvalues lower than the lowest αc Those
    interested in Bayesian approaches to neural network modeling should also consider
    Neal''s [9] Markov-chain Monte Carlo methods. Although these Monte Carlo methods
    sometimes require longer computation times than methods based on Mackay''s approximate
    Bayesian framework, they have some theoretical advantages and also require less
    tweaking.'
  id: totrans-1437
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在证据的计算中忽略负特征值 - 对于特征值低于最低 αc 的网络，将证据值视为不可靠。对贝叶斯神经网络建模感兴趣的人还应考虑 Neal 的 [9]
    马尔可夫链蒙特卡洛方法。尽管这些蒙特卡洛方法有时需要比基于 MacKay 的近似贝叶斯框架的方法更长的计算时间，但它们具有一些理论优势，并且也需要更少的调整。'
- en: '[1] Bishop, C.: Neural Networks for Pattern Recognition. Oxford University
    Press'
  id: totrans-1438
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Bishop, C.：模式识别的神经网络。牛津大学出版社'
- en: (1995)
  id: totrans-1439
  prefs: []
  type: TYPE_NORMAL
  zh: （1995）
- en: '[2] Buntine, W.L., Weigend, A.S.: Computing second derivatives in feed-forward
    networks: A review. IEEE Transactions on Neural Networks 5(3), 480–488 (1994)'
  id: totrans-1440
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Buntine, W.L., Weigend, A.S.：前馈网络中的二阶导数计算：综述。IEEE神经网络学报 5(3), 480–488（1994）'
- en: '[3] Hastie, T.J., Tibshirani, R.J.: Generalized additive models. Chapman and
    Hall, London (1990)'
  id: totrans-1441
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Hastie, T.J., Tibshirani, R.J.：广义加法模型。查普曼与霍尔，伦敦（1990）'
- en: '[4] Hwang, J.N., Lay, S.-R., Maechler, M., Martin, R.D., Schimert, J.: Regression
    modeling in back-propagation and projection pursuit learning. IEEE Transactions
    on Neural Networks 5(3), 342–353 (1994)'
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Hwang, J.N., Lay, S.-R., Maechler, M., Martin, R.D., Schimert, J.：反向传播和投影追踪学习中的回归建模。IEEE神经网络学报
    5(3), 342–353（1994）'
- en: '[5] MacKay, D.J.C.: A practical Bayesian framework for backpropagation networks.'
  id: totrans-1443
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] MacKay, D.J.C.：一个实用的贝叶斯框架用于反向传播网络。'
- en: Neural Computation 4(3), 448–472 (1992)
  id: totrans-1444
  prefs: []
  type: TYPE_NORMAL
  zh: 神经计算 4(3), 448–472（1992）
- en: '[6] MacKay, D.J.C.: Bayesian methods for backpropagation networks. In: Domany,
    E., van Hemmen, J.L., Schulten, K. (eds.) Models of Neural Networks III, ch. 6,
    Springer, New York (1994)'
  id: totrans-1445
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] MacKay, D.J.C.：反向传播网络的贝叶斯方法。在：Domany, E., van Hemmen, J.L., Schulten, K.（编）《神经网络模型
    III》，第6章，施普林格，纽约（1994）'
- en: '[7] MacKay, D.J.C.: Probable networks and plausible predictions - a review
    of practical Bayesian methods for supervised neural networks. Network: Computation
    in Neural Systems 6, 469–505 (1995)'
  id: totrans-1446
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] MacKay, D.J.C.：可能网络与可信预测 - 关于监督神经网络的实用贝叶斯方法的综述。网络：神经系统中的计算 6, 469–505（1995）'
- en: '[8] MacKay, D.J.C.: Bayesian non-linear modelling for the 1993 energy prediction
    competition. In: Heidbreder, G. (ed.) Maximum Entropy and Bayesian Methods, Santa
    Barbara 1993, pp. 221–234. Kluwer, Dordrecht (1996)'
  id: totrans-1447
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] MacKay, D.J.C.：1993年能源预测竞赛的贝叶斯非线性建模。在：Heidbreder, G.（编）《最大熵与贝叶斯方法》，圣巴巴拉1993，第221–234页。克鲁维尔，朵德雷赫特（1996）'
- en: '[9] Neal, R.M.: Monte carlo implementation of gaussian process models for bayesian
    regression and classification. Technical Report TR9702, Dept. of Statistics, University
    of Toronto (1997), Software, http://www.cs.utoronto.ca/~radford/'
  id: totrans-1448
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Neal, R.M.：用于贝叶斯回归和分类的高斯过程模型的蒙特卡洛实现。技术报告 TR9702，多伦多大学统计系（1997），软件，http://www.cs.utoronto.ca/~radford/'
- en: '[10] Neal, R.M.: Bayesian Learning for Neural Networks. Springer, New York
    (1996)'
  id: totrans-1449
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Neal, R.M.：神经网络的贝叶斯学习。施普林格，纽约（1996）'
- en: '[11] Plate, T., Bert, J., Grace, J., Band, P.: A comparison between neural
    networks and other statistical techniques for modeling the relationship between
    tobacco and alcohol and cancer. In: Mozer, M.C., Jordan, M.I., Petsche, T. (eds.)
    Advances in Neural Information Processing 9 (NIPS 1996). MIT Press (1997)'
  id: totrans-1450
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Plate, T., Bert, J., Grace, J., Band, P.：神经网络与其他统计技术在建模烟草、酒精与癌症关系方面的比较。在：Mozer,
    M.C., Jordan, M.I., Petsche, T.（编）《神经信息处理的进展 9》（NIPS 1996）。MIT出版社（1997）'
- en: '[12] Roosen, C., Hastie, T.: Logistic response projection pursuit. Technical
    report, AT&T Bell Laboratories (1993)'
  id: totrans-1451
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Roosen, C., Hastie, T.：逻辑响应投影追踪。技术报告，AT&T贝尔实验室（1993）'
- en: '[13] Thodberg, H.H.: A review of bayesian neural networks with an application
    to near infrared spectroscopy. IEEE Transactions on Neural Networks 7(1), 56–72
    (1996)'
  id: totrans-1452
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Thodberg, H.H.: 关于贝叶斯神经网络的综述及其在近红外光谱中的应用。IEEE神经网络交易 7(1), 56–72 (1996)'
- en: 5 Adaptive Regularization In Neural Network Modeling-
  id: totrans-1453
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 自适应正则化在神经网络建模中的应用-
- en: Jan Larsen1, Claus Svarer2, Lars Nonboe Andersen1, and Lars Kai Hansen1 1 Department
    of Mathematical Modeling, Building 321 Technical University of Denmark DK-2800
    Lyngby, Denmark 2 Neurobiology Research Unit Department of Neurology, Building
    9201 Copenhagen University Hospital Blegdamsvej 9 DK-2100 Copenhagen, Denmark
  id: totrans-1454
  prefs: []
  type: TYPE_NORMAL
  zh: Jan Larsen1, Claus Svarer2, Lars Nonboe Andersen1，以及Lars Kai Hansen1 1 丹麦技术大学数学建模系，321号楼
    DK-2800 Lyngby, 丹麦 2 哥本哈根大学医院神经病学系神经生物学研究单位，9201号楼 Blegdamsvej 9 DK-2100 哥本哈根,
    丹麦
- en: '{jl,lna,lkhansen}@imm.dtu.dk, csvarer@pet.rh.dk http://eivind.imm.dtu.dk, http://neuro.pet.rh.dk
    Abstract. In this paper we address the important problem of optimizing regularization
    parameters in neural network modeling. The suggested optimization scheme is an
    extended version of the recently presented algorithm [25]. The idea is to minimize
    an empirical estimate - like the cross-validation estimate - of the generalization
    error with respect to regularization parameters. This is done by employing a simple
    iterative gradient descent scheme using virtually no additional programming overhead
    compared to standard training. Experiments with feed-forward neural network models
    for time series prediction and classification tasks showed the viability and robustness
    of the algorithm. Moreover, we provided some simple theoretical examples in order
    to illustrate the potential and limitations of the proposed regularization framework.'
  id: totrans-1455
  prefs: []
  type: TYPE_NORMAL
  zh: '{jl,lna,lkhansen}@imm.dtu.dk, csvarer@pet.rh.dk http://eivind.imm.dtu.dk, http://neuro.pet.rh.dk
    摘要。本论文探讨了神经网络建模中优化正则化参数的重要问题。建议的优化方案是最近提出的算法[25]的扩展版本。其思路是最小化关于正则化参数的泛化误差的经验估计——例如交叉验证估计。通过使用几乎不增加额外编程开销的简单迭代梯度下降方案来实现。对于时间序列预测和分类任务的前馈神经网络模型的实验表明了该算法的可行性和鲁棒性。此外，我们提供了一些简单的理论例子，以说明所提出的正则化框架的潜力和局限性。'
- en: 5.1 Introduction
  id: totrans-1456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 引言
- en: Neural networks are flexible tools for time series processing and pattern recognition.
    By increasing the number of hidden neurons in a 2-layer architecture any relevant
    target function can be approximated arbitrarily close [19]. The associated risk
    of overfitting on noisy data is of major concern in neural network design, which
    find expression in the ubiquitous bias-variance dilemma, see e.g., [9].
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是处理时间序列和模式识别的灵活工具。通过增加2层架构中的隐藏神经元数量，任何相关的目标函数都可以任意接近地进行逼近[19]。在神经网络设计中，过拟合噪声数据的相关风险是一个主要关注点，这在无处不在的偏差-方差困境中得到了体现，参见例如[9]。
- en: 'The need for regularization is two-fold: First, it remedies numerical problems
    in the training process by smoothing the cost function and by introducing curvature
    in low (possibly zero) curvature regions of cost function. Secondly, regularization
    is a tool for reducing variance by introducing extra bias. The overall'
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的需求有两个方面：首先，它通过平滑成本函数和在成本函数的低（可能为零）曲率区域引入曲率来解决训练过程中的数值问题。其次，正则化是一种通过引入额外偏差来减少方差的工具。总体上
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-1459
  prefs: []
  type: TYPE_NORMAL
  zh: '- 先前发表在：Orr, G.B. 和 Müller, K.-R. (主编)：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-1460
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0 (1998)。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    111–130, 2012.'
  id: totrans-1461
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon 等（主编）：NN: Tricks of the Trade, 第2版，LNCS 7700，第111–130页，2012年。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-1462
  prefs: []
  type: TYPE_NORMAL
  zh: -c 施普林格-维尔拉赫 柏林 海德堡 2012年
- en: objective of architecture optimization is to minimize the generalization error.
  id: totrans-1463
  prefs: []
  type: TYPE_NORMAL
  zh: 架构优化的目标是最小化泛化误差。
- en: The architecture can be optimized *directly* by stepwise selection procedures
    (including pruning techniques) or *indirectly* using regularization. In general,
    one would prefer a hybrid scheme; however, a very flexible regularization may
    substitute the need for selection procedures. The numerical experiments we consider
    mainly hybrid pruning/adaptive regularization schemes.
  id: totrans-1464
  prefs: []
  type: TYPE_NORMAL
  zh: 架构可以通过逐步选择程序（包括剪枝技术）*直接*优化，或者通过正则化*间接*优化。一般来说，人们会倾向于使用混合方案；然而，非常灵活的正则化可能会替代选择程序的需要。我们考虑的数值实验主要是混合剪枝/自适应正则化方案。
- en: The trick presented in this communication addresses the problem of adapting
    regularization parameters.
  id: totrans-1465
  prefs: []
  type: TYPE_NORMAL
  zh: 本通讯中提出的技巧解决了适应正则化参数的问题。
- en: The trick consists in formulating a simple iterative gradient descent scheme
    for adapting the regularization parameters aiming at minimizing the generalization
    error.
  id: totrans-1466
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技巧在于制定一个简单的迭代梯度下降方案，以适应正则化参数，旨在最小化泛化误差。
- en: We suggest to use an empirical estimate1 of the generalization error, viz. the
    K-fold cross-validation [8], [39]. In [25] and [3] the proposed scheme was studied
    using the hold-out validation estimator.
  id: totrans-1467
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议使用泛化误差的经验估计，即 K 折交叉验证 [8]，[39]。在 [25] 和 [3] 中，所提出的方案使用了保持验证估计器进行研究。
- en: In addition to empirical estimators for the generalization error a number of
    algebraic estimators like FPE [1], FPER [23], GEN [21], GPE [31] and NIC [33]
    have been developed in recent years. These estimates, however, depend on a number
    of statistical assumptions which can be quite hard to justify. In particular,
    they are o(1/Nt) estimators where Nt is the number of training examples.
  id: totrans-1468
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对泛化误差的经验估计，近年来还开发了一些代数估计量，如 FPE [1]、FPER [23]、GEN [21]、GPE [31] 和 NIC [33]。然而，这些估计依赖于一些统计假设，这些假设可能很难得到证明。特别地，它们是
    o(1/Nt) 估计量，其中 Nt 是训练示例的数量。
- en: However, for many practical modeling set-ups it is hard to meet the large training
    set assumption.
  id: totrans-1469
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于许多实际建模设置，满足大训练集假设是困难的。
- en: In [14] properties of adaptive regularization is studied in the simple case
    of estimating the mean of a random variable using an algebraic estimate of the
    average2 generalization error and [15] proposed an adaptive regularization scheme
    for neural networks based on an algebraic estimate. However, experiments indicate
    that this scheme has a drawback regarding robustness. In addition, the requirement
    of a large training set may not be met.
  id: totrans-1470
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [14] 中，研究了自适应正则化在使用代数估计的随机变量均值估计中的简单案例的属性，[15] 提出了基于代数估计的神经网络自适应正则化方案。然而，实验表明，该方案在鲁棒性方面存在缺陷。此外，可能无法满足大训练集的要求。
- en: The Bayesian approach to adapt regularization parameters is to minimize the
    so-called evidence [5, Ch. 10], [30]. The evidence, however, does not in a simple
    way relate to the generalization error which is our primary object of interest.
  id: totrans-1471
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应正则化参数的贝叶斯方法是最小化所谓的证据 [5, Ch. 10]，[30]。然而，证据并不简单地与我们主要关注的泛化误差相关。
- en: Furthermore [2] and [38] consider the use of a validation set to tune the amount
    of regularization, in particular when using the early-stop technique.
  id: totrans-1472
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，[2] 和 [38] 考虑使用验证集来调整正则化量，特别是在使用早停技术时。
- en: Section 5.2 considers training and empirical generalization assessment. In Section
    5.3 the framework for optimization of regularization parameters is presented.
  id: totrans-1473
  prefs: []
  type: TYPE_NORMAL
  zh: 第 5.2 节考虑了训练和经验泛化评估。在第 5.3 节中，提出了正则化参数优化的框架。
- en: The experimental section 5.4 deals with examples of feed-forward neural networks
    models for classification and time series prediction. Further, in order to study
    the theoretical potential/limitations of the proposed framework, we include some
    simulations on a simple toy problem.
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
  zh: 实验部分 5.4 处理用于分类和时间序列预测的前馈神经网络模型示例。此外，为了研究所提出框架的理论潜力/限制，我们包括了一些关于简单玩具问题的模拟。
- en: 5.2 Training And Generalization
  id: totrans-1475
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 训练与泛化
- en: Suppose that the neural network is described by the vector function f(x; w)
    where x is the input vector and w is the vector of network weights and thresholds
    1 For further discussion on empirical generalization assessment, see e.g., [24].
    2 Average w.r.t. to different training sets.
  id: totrans-1476
  prefs: []
  type: TYPE_NORMAL
  zh: 假设神经网络由向量函数 f(x; w) 描述，其中 x 是输入向量，w 是网络权重和阈值的向量。有关经验泛化评估的进一步讨论，请参见，例如，[24]。2
    针对不同训练集的平均值。
- en: with dimensionality m. The objective is to use the network model for approximating
    the true conditional input-output distribution p(y|x), or some moments hereof.
    For regression and signal processing problems we normally model the conditional
    expectation E{y|x}.
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
  zh: 维度为 m。目标是使用网络模型来逼近真实的条件输入-输出分布 p(y|x)，或其某些矩。对于回归和信号处理问题，我们通常建模条件期望 E{y|x}。
- en: 'Assume that we have available a data set D = {x(k); y(k)}Nk=1 of N inputoutput
    examples. In order to both train and empirically estimate the generalization performance
    we follow the idea of K-fold cross-validation [8], [39] and split the data set
    into K randomly chosen disjoint sets of approximately equal size, i.e., D = ∪Kj=1Vj
    and ∀ i = j : Vi ∩ Vj = ∅. Training and validation is replicated K times, and
    in the j''th run training is done on the set Tj = D\Vj and validation is performed
    on Vj .'
  id: totrans-1478
  prefs: []
  type: TYPE_NORMAL
  zh: '假设我们有一个数据集 D = {x(k); y(k)}Nk=1，共有 N 个输入输出示例。为了训练并经验性地估计泛化性能，我们遵循 K 折交叉验证 [8],
    [39] 的思路，将数据集随机分割成 K 个大致相等大小的互不重叠的集合，即 D = ∪Kj=1Vj 且 ∀ i = j : Vi ∩ Vj = ∅。训练和验证重复
    K 次，在第 j 次运行中，训练在集合 Tj = D\Vj 上进行，验证在 Vj 上进行。'
- en: The cost function, CTj , for network training on Tj , is supposed to be the
    sum of a loss function (or training error), STj (w), and a regularization term
    R(w, κ) parameterized by a set of regularization parameters κ, i.e.,
  id: totrans-1479
  prefs: []
  type: TYPE_NORMAL
  zh: 网络在 Tj 上训练的成本函数 CTj 被认为是损失函数（或训练误差）STj (w) 和由一组正则化参数 κ 参数化的正则化项 R(w, κ) 的总和，即，
- en: $$C_{{\mathcal{T}}_{j}}(\mathbf{w})=S_{{\mathcal{T}}_{j}}(\mathbf{w})+R(\mathbf{w},\mathbf{\kappa})={\frac{1}{N_{t
    j}}}\sum_{k=1}^{N_{t j}}\ell\left(\mathbf{y}(k),{\widehat{\mathbf{y}}}(k);\mathbf{w}\right)+R(\mathbf{w},\mathbf{\kappa})$$
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
  zh: $$C_{{\mathcal{T}}_{j}}(\mathbf{w})=S_{{\mathcal{T}}_{j}}(\mathbf{w})+R(\mathbf{w},\mathbf{\kappa})={\frac{1}{N_{t
    j}}}\sum_{k=1}^{N_{t j}}\ell\left(\mathbf{y}(k),{\widehat{\mathbf{y}}}(k);\mathbf{w}\right)+R(\mathbf{w},\mathbf{\kappa})$$
- en: $$(5.1)$$
  id: totrans-1481
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.1)$$
- en: where (·) measures the distance between the output y(k) and the network prediction
    y%(k) = f(x(k); w). In section 5.4 we will consider log-likelihood and the square
    error loss function  = |y − y%| 2. Ntj *≡ |T*j | defines the number of training
    examples in Tj and k is used to index the k'th example [x(k), y(k)]. Training
    provides the estimated weight vector w% j = arg minw CTj (w).
  id: totrans-1482
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 (·) 衡量输出 y(k) 和网络预测 y%(k) = f(x(k); w) 之间的距离。在第 5.4 节中，我们将考虑对数似然和平方误差损失函数
    = |y − y%| ²。Ntj *≡ |T*j | 定义了 Tj 中的训练样本数量，k 用于索引第 k 个示例 [x(k), y(k)]。训练提供了估计的权重向量
    w% j = arg minw CTj (w)。
- en: The j'th validation set Vj consist of Nvj = N − Ntj examples and the validation
    error3 of the trained network reads
  id: totrans-1483
  prefs: []
  type: TYPE_NORMAL
  zh: 第 j 个验证集 Vj 包含 Nvj = N − Ntj 个示例，训练网络的验证误差3 表示为
- en: $$S_{\mathcal{V}_{j}}(\widehat{\mathbf{w}}_{j})=\frac{1}{N_{v j}}\sum_{k=1}^{N_{v
    j}}\ell\left(\mathbf{y}(k),\widehat{\mathbf{y}}(k);\widehat{\mathbf{w}}_{j}\right)\tag{1}$$
  id: totrans-1484
  prefs: []
  type: TYPE_NORMAL
  zh: $$S_{\mathcal{V}_{j}}(\widehat{\mathbf{w}}_{j})=\frac{1}{N_{v j}}\sum_{k=1}^{N_{v
    j}}\ell\left(\mathbf{y}(k),\widehat{\mathbf{y}}(k);\widehat{\mathbf{w}}_{j}\right)\tag{1}$$
- en: $$(5.2)$$
  id: totrans-1485
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.2)$$
- en: where the sum runs over the Nvj validation examples. SVj (w% j ) is thus an
    estimate of the generalization error, defined as the expected loss,
  id: totrans-1486
  prefs: []
  type: TYPE_NORMAL
  zh: 其中求和是在 Nvj 验证示例上进行的。因此，SVj (w% j ) 是对泛化误差的估计，定义为期望损失，
- en: $$G(\widehat{\mathbf{w}}_{j})=E_{\mathbf{x},\mathbf{y}}\{\ell(\mathbf{y},\widehat{\mathbf{y}};\widehat{\mathbf{w}}_{j})\}=\int\ell(\mathbf{y},\widehat{\mathbf{y}};\widehat{\mathbf{w}}_{j})\cdot
    p(\mathbf{x},\mathbf{y})\,d\mathbf{x}d\mathbf{y}\tag{5.3}$$
  id: totrans-1487
  prefs: []
  type: TYPE_NORMAL
  zh: $$G(\widehat{\mathbf{w}}_{j})=E_{\mathbf{x},\mathbf{y}}\{\ell(\mathbf{y},\widehat{\mathbf{y}};\widehat{\mathbf{w}}_{j})\}=\int\ell(\mathbf{y},\widehat{\mathbf{y}};\widehat{\mathbf{w}}_{j})\cdot
    p(\mathbf{x},\mathbf{y})\,d\mathbf{x}d\mathbf{y}\tag{5.3}$$
- en: where p(x, y) is the unknown joint input-output probability density. Generally,
    SVj (w% j ) = G(w% j ) + O(1/&Nvj ) where O(·) is the Landau order function4.
    Thus we need large Nvj to achieve an accurate estimate of the generalization error.
    On the other hand, this leaves only few data for training thus the true generalization
    G(w% j ) increases. Consequently there exist a trade-off among the two conflicting
    aims which calls for finding an optimal split ratio. The optimal split ratio5
    is an interesting open and difficult problem since it depends on the total algorithm
    in which the validation error enters. Moreover, it depends on the learning curve6
    [17].
  id: totrans-1488
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 p(x, y) 是未知的联合输入输出概率密度。一般来说，SVj (w% j ) = G(w% j ) + O(1/&Nvj )，其中 O(·) 是兰道阶函数4。因此，我们需要较大的
    Nvj 来实现对泛化误差的准确估计。另一方面，这仅为训练留下了少量数据，因此真实的泛化 G(w% j ) 会增加。因此，在这两个相互矛盾的目标之间存在权衡，这需要寻找一个最佳的分割比例。最佳分割比例5
    是一个有趣且困难的问题，因为它依赖于验证误差进入的总算法。此外，它还依赖于学习曲线6 [17]。
- en: 3 That is, the loss function on the validation set.
  id: totrans-1489
  prefs: []
  type: TYPE_NORMAL
  zh: 3 也就是说，验证集上的损失函数。
- en: 4 If h(x) = O(g(x)) then |h(x)|/|*g(x)*| < ∞ for x → 0. 5 For more elaborations
    on the split of data, see e.g., [2], [20], [24] and [26].
  id: totrans-1490
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 h(x) = O(g(x))，那么 |h(x)|/|*g(x)*| < ∞ 当 x → 0。关于数据分割的更多详细信息，请参见例如 [2], [20],
    [24] 和 [26]。
- en: 6 Defined as the average generalization error as a function of the number of
    training examples.
  id: totrans-1491
  prefs: []
  type: TYPE_NORMAL
  zh: 6 定义为泛化误差与训练示例数量的函数关系。
- en: The final K-fold cross-validation estimate is given by the average validation
    error estimates,
  id: totrans-1492
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的K折交叉验证估计由平均验证误差估计给出，
- en: $${\hat{\Gamma}}={\frac{1}{K}}\sum_{j=1}^{K}S_{\mathcal{V}_{j}}({\hat{\mathbf{w}}}_{j}).$$
  id: totrans-1493
  prefs: []
  type: TYPE_NORMAL
  zh: $${\hat{\Gamma}}={\frac{1}{K}}\sum_{j=1}^{K}S_{\mathcal{V}_{j}}({\hat{\mathbf{w}}}_{j}).$$
- en: $$(5.4)$$
  id: totrans-1494
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.4)$$
- en: $$(5.5)$$
  id: totrans-1495
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.5)$$
- en: $$\Gamma=E_{\mathcal{T}}\{G({\hat{\mathbf{w}}}_{j})\}.$$
  id: totrans-1496
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Gamma=E_{\mathcal{T}}\{G({\hat{\mathbf{w}}}_{j})\}.$$
- en: SVj (w% j ). (5.4)
  id: totrans-1497
  prefs: []
  type: TYPE_NORMAL
  zh: SVj (w% j )。(5.4)
- en: Γ% is an estimate of the *average* generalization error over all possible training
    sets of size Ntj, Γ = ET {G(w% j )}. (5.5)
  id: totrans-1498
  prefs: []
  type: TYPE_NORMAL
  zh: Γ%是所有可能训练集大小Ntj的*平均*泛化误差的估计，Γ = ET {G(w% j )}。(5.5)
- en: Γ% is an unbiased estimate of Γ if the data of D are independently distributed7,
    see e.g., [16].
  id: totrans-1499
  prefs: []
  type: TYPE_NORMAL
  zh: 如果D的数据是独立分布的，则Γ%是Γ的无偏估计，见例如[16]。
- en: The idea is now to optimize the amount of regularization by minimizing Γ%
  id: totrans-1500
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的想法是通过最小化Γ来优化正则化的数量。
- en: w.r.t. the regularization parameters κ. An algorithm for this purpose is described
    in Section 5.3. Furthermore, we might consider optimizing regularization using
    the hold-out validation estimate corresponding to K = 1. In this case one has
    to choose a split ratio. Without further ado, we will recommend a 50/50 splitting.
  id: totrans-1501
  prefs: []
  type: TYPE_NORMAL
  zh: 关于正则化参数κ。为此目的的算法在第5.3节中描述。此外，我们可能会考虑使用对应于K = 1的保留验证估计来优化正则化。在这种情况下，必须选择一个分割比例。言归正传，我们将推荐50/50的分割。
- en: Suppose that we found the optimal κ using the cross-validation estimate.
  id: totrans-1502
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们通过交叉验证估计找到了最佳κ。
- en: Replications of training result in K different weight estimates w% j which might
    be viewed as an ensemble of networks. In [16] we showed under certain mild conditions
    that when considering a o(1/N) approximation, the average generalization error
    of the ensemble network fens(x) = Kj=1 βj · f(x, w% j ) equals that of the network
    trained on all examples in D where βj weights the contribution from the j'th network
    and j βj = 1. If K is a divisor in N then ∀*j, β*j = 1/K,
  id: totrans-1503
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的重复导致K个不同的权重估计w% j，这可以视为一个网络的集成。在[16]中，我们在某些温和条件下证明，当考虑o(1/N)近似时，集成网络fens(x)
    = Kj=1 βj · f(x, w% j )的平均泛化误差等于在D中对所有示例进行训练的网络的平均泛化误差，其中βj权重来自第j个网络的贡献，并且j βj
    = 1。如果K是N的约数，则∀*j, β*j = 1/K，
- en: otherwise βj = (N − Nvj )/N(K − 1). Consequently, one might use the ensemble
    network to compensate for the increase in generalization error due to only training
    on Ntj = N − Nvj data. Alternatively, one might retrain on the full data set D
    using the optimal κ. We use the latter approach in the experimental section.
  id: totrans-1504
  prefs: []
  type: TYPE_NORMAL
  zh: 否则βj = (N − Nvj )/N(K − 1)。因此，可以使用集成网络来弥补仅在Ntj = N − Nvj数据上训练所导致的泛化误差增加。或者，也可以使用最佳κ在完整数据集D上重新训练。我们在实验部分采用后者的方法。
- en: 'A minimal necessary requirement for a procedure which estimates the network
    parameters on the training set and optimizes the amount of regularization from
    a cross-validation set is: the generalization error of the regularized network
    should be smaller than that of the unregularized network trained on the full data
    set D. However, this is not always the case, and is the quintessence of various
    "no free lunch" theorems [12], [44], [46]:'
  id: totrans-1505
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一种在训练集上估计网络参数并从交叉验证集中优化正则化数量的过程，最小必要要求是：正则化网络的泛化误差应小于在完整数据集D上训练的未正则化网络的泛化误差。然而，这并不总是如此，这正是各种“没有免费午餐”定理的精髓[12]、[44]、[46]：
- en: '- If the regularizer is parameterized using many parameters, κ, there is a
    potential risk of over-fitting on the cross-validation data. A natural way to
    avoid this situation is to limit the number of regularization parameters.'
  id: totrans-1506
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果正则化器使用许多参数κ进行参数化，则在交叉验证数据上存在过拟合的潜在风险。避免这种情况的一种自然方法是限制正则化参数的数量。'
- en: Another recipe is to impose constraints on κ (hyper regularization).
  id: totrans-1507
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是对κ施加约束（超常规化）。
- en: '- The specific choice of the regularizers functional form impose prior constraints
    on the functions to be implemented by the network8. If the prior information is
    mismatched to the actual problem it might be better not to use regularization.'
  id: totrans-1508
  prefs: []
  type: TYPE_NORMAL
  zh: '- 正则化器的特定选择形式对网络要实现的函数施加了先验约束。如果先验信息与实际问题不匹配，可能更好的是不使用正则化。'
- en: '- The de-biasing procedure described above which compensate for training only
    on Ntj < N examples might fail to yield better performance since the 7 That is,
    [x(k1), y(k1)] is independent of [x(k2), y(k2)] for all k1 = k2. 8 The functional
    constraints are through the penalty imposed on the weights.'
  id: totrans-1509
  prefs: []
  type: TYPE_NORMAL
  zh: 上述去偏置程序对于仅在 Ntj < N 的样本上进行训练的情况可能无法产生更好的性能。也就是说，对于所有 k1 ≠ k2，[x(k1), y(k1)]
    与 [x(k2), y(k2)] 是独立的。功能约束是通过对权重施加的惩罚实现的。
- en: weights now are optimized using all data, including those which where left out
    exclusively for optimizing regularization parameters.
  id: totrans-1510
  prefs: []
  type: TYPE_NORMAL
  zh: 权重现在使用所有数据进行优化，包括那些专门用于优化正则化参数而被排除的数据。
- en: '- The split among training/validation data, and consequently the number of
    folds, K, may not be chosen appropriately.'
  id: totrans-1511
  prefs: []
  type: TYPE_NORMAL
  zh: '- 训练/验证数据的划分，因此折数 K，可能没有被适当选择。'
- en: These problems are further addressed in Section 5.4.1.
  id: totrans-1512
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题在第 5.4.1 节中进一步讨论。
- en: 5.3 Adapting Regularization Parameters
  id: totrans-1513
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 调整正则化参数
- en: The choice of regularizer may be motivated by
  id: totrans-1514
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正则化器的动机可能来自于
- en: '- the fact that the minimization of the cost function is normally an ill- posed
    task. Regularization smoothens the cost function and thereby facilitates the training.
    The weight decay regularizer9, originally suggested by Hinton in the neural networks
    literature, is a simple way to accomplish this task, see e.g., [35].'
  id: totrans-1515
  prefs: []
  type: TYPE_NORMAL
  zh: '- 最小化成本函数通常是一个不适定的问题。正则化平滑了成本函数，从而促进了训练。权重衰减正则化，最初由 Hinton 在神经网络文献中提出，是实现此任务的一种简单方法，参见例如
    [35]。'
- en: '- a priori knowledge of the weights, e.g., in terms of a prior distribution
    (when using a Bayesian approach). In this case the regularization term normally
    plays the role of a log-prior distribution. Weight decay regularization may be
    viewed as a Gaussian prior, see e.g., [2]. Other types of priors, e.g., the Laplacian
    [13], [43] and soft weight sharing [34] has been considered. Moreover, priors
    have been developed for the purpose of restricting the number of weights (pruning),
    e.g., the so-called weight elimination [42].'
  id: totrans-1516
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对权重的先验知识，例如，以先验分布的形式（使用贝叶斯方法时）。在这种情况下，正则化项通常充当对数先验分布的角色。权重衰减正则化可以被视为高斯先验，参见例如
    [2]。其他类型的先验，例如拉普拉斯 [13]、[43] 和软权重共享 [34] 也已被考虑。此外，为了限制权重的数量（剪枝），已经开发了先验，例如所谓的权重消除
    [42]。'
- en: '- a desired characteristics of the functional mapping performed by the network.'
  id: totrans-1517
  prefs: []
  type: TYPE_NORMAL
  zh: '- 这是网络执行的功能映射的一个期望特征。'
- en: Typically, a smooth mapping is preferred. Regularizers which penalizes curvature
    of the mapping has been suggested in [4], [7], [32], [45], [10].
  id: totrans-1518
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，更偏爱平滑映射。文献 [4]、[7]、[32]、[45]、[10] 中提出了惩罚映射曲率的正则化器。
- en: In the experimental section we consider weight decay regularization and some
    generalizations hereof. Without further ado, weight decay regularization has proven
    to be useful in many neural network applications.
  id: totrans-1519
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验部分，我们考虑权重衰减正则化及其一些一般化。不用多说，权重衰减正则化在许多神经网络应用中证明是有用的。
- en: The standard approach for estimation of regularization parameters is more and
    less systematic search and evaluation of the cross-validation error. However,
    this is not viable for multiple regularization parameters. On the other hand,
    as will be demonstrated, it is possible to derive an optimization algorithm based
    on gradient descent.
  id: totrans-1520
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化参数的估计标准方法是系统地搜索和评估交叉验证误差。然而，对于多个正则化参数，这并不可行。另一方面，正如将要演示的，可以基于梯度下降推导出一种优化算法。
- en: Consider a regularization term R(w, κ) which depends on q regularization parameters
    contained in the vector κ. Since the estimated weights w% j = arg minw CTj (w)
    are controlled by the regularization term, we may in fact consider the cross-validation
    error (5.4) as an *implicit function* of the regularization parameters, i.e.,
  id: totrans-1521
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个正则化项 R(w, κ)，它依赖于包含在向量 κ 中的 q 个正则化参数。由于估计的权重 w% j = arg minw CTj (w) 受正则化项控制，我们实际上可以将交叉验证误差
    (5.4) 视为正则化参数的*隐式函数*，即，
- en: $${\widehat{\Gamma}}(\kappa)={\frac{1}{K}}\sum_{j=1}^{K}S\nu_{j}\left({\widehat{\mathbf{w}}}_{j}(\kappa)\right)$$
  id: totrans-1522
  prefs: []
  type: TYPE_NORMAL
  zh: $${\widehat{\Gamma}}(\kappa)={\frac{1}{K}}\sum_{j=1}^{K}S\nu_{j}\left({\widehat{\mathbf{w}}}_{j}(\kappa)\right)$$
- en: $$(5.6)$$
  id: totrans-1523
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.6)$$
- en: SVj (w% j (κ)) (5.6)
  id: totrans-1524
  prefs: []
  type: TYPE_NORMAL
  zh: SVj (w% j (κ)) (5.6)
- en: 9 Also known as ridge regression.
  id: totrans-1525
  prefs: []
  type: TYPE_NORMAL
  zh: 9 也称为岭回归。
- en: where w% j (κ) is the κ-dependent vector of weights estimated from training
    set Tj . The optimal regularization can be found by using gradient descent10,
  id: totrans-1526
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 w% j (κ) 是从训练集 Tj 中估计的依赖于 κ 的权重向量。可以通过使用梯度下降法来找到最佳正则化，
- en: $$\kappa_{(n+1)}=\kappa_{(n)}-\eta\frac{\partial\widehat{T}}{\partial\kappa}(\widehat{w}(\kappa_{(n)}))$$
  id: totrans-1527
  prefs: []
  type: TYPE_NORMAL
  zh: $$\kappa_{(n+1)}=\kappa_{(n)}-\eta\frac{\partial\widehat{T}}{\partial\kappa}(\widehat{w}(\kappa_{(n)}))$$
- en: ∂κ(w% (κ(n))) (5.7)
  id: totrans-1528
  prefs: []
  type: TYPE_NORMAL
  zh: ∂κ(w% (κ(n))) (5.7)
- en: where η > 0 is a step-size (learning rate) and κ(n) is the estimate of the regularization
    parameters in iteration n.
  id: totrans-1529
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 η > 0 是步长（学习率），κ(n) 是迭代 n 中正则化参数的估计值。
- en: Suppose the regularization term is linear in the regularization parameters,
  id: totrans-1530
  prefs: []
  type: TYPE_NORMAL
  zh: 假设正则化项在正则化参数上是线性的，
- en: $$(5.7)$$
  id: totrans-1531
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.7)$$
- en: $$R(\mathbf{w},\mathbf{\kappa})=\mathbf{\kappa}^{\top}\mathbf{r}(\mathbf{w})=\sum_{i=1}^{q}\kappa_{i}r_{i}(\mathbf{w})$$
  id: totrans-1532
  prefs: []
  type: TYPE_NORMAL
  zh: $$R(\mathbf{w},\mathbf{\kappa})=\mathbf{\kappa}^{\top}\mathbf{r}(\mathbf{w})=\sum_{i=1}^{q}\kappa_{i}r_{i}(\mathbf{w})$$
- en: $$(5.8)$$
  id: totrans-1533
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.8)$$
- en: where κi are the regularization parameters and ri(w) the associated regularization
    functions. Many suggested regularizers are linear in the regularization parameters,
    this includes the popular weight decay regularization as well as regularizers
    imposing smooth functions such as the Tikhonov regularizer [4], [2] and the smoothing
    regularizer for neural networks [32], [45]. However, there exist exceptions such
    as weight-elimination [42] and soft weight sharing [34]. In this case the presented
    method needs few modifications.
  id: totrans-1534
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 κi 是正则化参数，ri(w) 是相关的正则化函数。许多建议的正则化器在正则化参数上是线性的，这包括流行的权重衰减正则化以及施加平滑函数的正则化器，如
    Tikhonov 正则化器 [4]、[2] 和神经网络的平滑正则化器 [32]、[45]。然而，也存在例外，例如权重消除 [42] 和软权重共享 [34]。在这种情况下，所提出的方法需要一些修改。
- en: Using the results of the Appendix, the gradient of the cross-validation error
    equals
  id: totrans-1535
  prefs: []
  type: TYPE_NORMAL
  zh: 使用附录中的结果，交叉验证误差的梯度等于
- en: $$\frac{\partial\widehat{T}}{\partial\mathbf{\kappa}}(\mathbf{\kappa})=\frac{1}{K}\sum_{j=1}^{K}\frac{\partial
    S\nu_{j}}{\partial\mathbf{\kappa}}(\widehat{\mathbf{w}}_{j}),\tag{5.9}$$ $$\frac{\partial
    S\nu_{j}}{\partial\mathbf{\kappa}}(\widehat{\mathbf{w}}_{j})=-\frac{\partial\mathbf{r}}{\partial\mathbf{w}^{\top}}(\widehat{\mathbf{w}}_{j})\cdot\mathbf{J}_{j}^{-1}(\widehat{\mathbf{w}}_{j})\cdot\frac{\partial
    S\nu_{j}}{\partial\mathbf{w}}(\widehat{\mathbf{w}}_{j}).\tag{5.10}$$  where $\mathbf{J}_{j}=\partial^{2}C_{T_{j}}/\partial\mathbf{w}\partial\mathbf{w}^{\top}$
    is the Hessian of the cost function. As an example,
  id: totrans-1536
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{\partial\widehat{T}}{\partial\mathbf{\kappa}}(\mathbf{\kappa})=\frac{1}{K}\sum_{j=1}^{K}\frac{\partial
    S\nu_{j}}{\partial\mathbf{\kappa}}(\widehat{\mathbf{w}}_{j}),\tag{5.9}$$ $$\frac{\partial
    S\nu_{j}}{\partial\mathbf{\kappa}}(\widehat{\mathbf{w}}_{j})=-\frac{\partial\mathbf{r}}{\partial\mathbf{w}^{\top}}(\widehat{\mathbf{w}}_{j})\cdot\mathbf{J}_{j}^{-1}(\widehat{\mathbf{w}}_{j})\cdot\frac{\partial
    S\nu_{j}}{\partial\mathbf{w}}(\widehat{\mathbf{w}}_{j}).\tag{5.10}$$，其中 $\mathbf{J}_{j}=\partial^{2}C_{T_{j}}/\partial\mathbf{w}\partial\mathbf{w}^{\top}$
    是成本函数的 Hessian 矩阵。作为一个例子，
- en: $$(5.10)$$
  id: totrans-1537
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.10)$$
- en: $$(5.12)$$
  id: totrans-1538
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.12)$$
- en: where Jj = ∂2CTj /∂w∂w is the Hessian of the cost function. As an example,
  id: totrans-1539
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Jj = ∂2CTj /∂w∂w 是成本函数的 Hessian 矩阵。作为一个例子，
- en: consider the case of weight decay regularization with separate weight decays
    for
  id: totrans-1540
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑使用单独权重衰减的权重衰减正则化的情况
- en: two group of weights, e.g., the input-to-hidden and hidden-to output weights,
  id: totrans-1541
  prefs: []
  type: TYPE_NORMAL
  zh: 两组权重，例如，从输入到隐层的权重和从隐层到输出的权重，
- en: i.e.,  $$R(\mathbf{w},\mathbf{\kappa})=\kappa^{I}\cdot|\mathbf{w}^{I}|^{2}+\kappa^{H}\cdot|\mathbf{w}^{H}|^{2}\tag{5.11}$$  where
    $\mathbf{\kappa}=[\kappa^{I},\kappa^{H}]$, $\mathbf{w}=[\mathbf{w}^{I},\mathbf{w}^{H}]$
    with $\mathbf{w}^{I}$, $\mathbf{w}^{H}$ denoting the input-to-hidden
  id: totrans-1542
  prefs: []
  type: TYPE_NORMAL
  zh: 即，$$R(\mathbf{w},\mathbf{\kappa})=\kappa^{I}\cdot|\mathbf{w}^{I}|^{2}+\kappa^{H}\cdot|\mathbf{w}^{H}|^{2}\tag{5.11}$$，其中
    $\mathbf{\kappa}=[\kappa^{I},\kappa^{H}]$，$\mathbf{w}=[\mathbf{w}^{I},\mathbf{w}^{H}]$，$\mathbf{w}^{I}$
    和 $\mathbf{w}^{H}$ 表示从输入到隐层的权重
- en: where κ = [κI , κH], w = [wI , wH] with wI , wH denoting the input-to-hidden
    and hidden-to output weights, respectively. The gradient of the validation error
    then yields,
  id: totrans-1543
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 κ = [κI , κH], w = [wI , wH]，wI , wH 分别表示从输入到隐层的权重和从隐层到输出的权重。验证误差的梯度为，
- en: $$\frac{\partial S_{\mathcal{V}_{j}}}{\partial\kappa^{I}}(\hat{\mathbf{w}}_{j})=-2(\hat{\mathbf{w}}_{j}^{I})^{\top}\cdot\mathbf{g}_{j}^{I},\;\;\frac{\partial
    S_{\mathcal{V}_{j}}}{\partial\kappa^{H}}(\hat{\mathbf{w}}_{j})=-2(\hat{\mathbf{w}}_{j}^{H})^{\top}\cdot\mathbf{g}_{j}^{H}$$  which
    we get
  id: totrans-1544
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{\partial S_{\mathcal{V}_{j}}}{\partial\kappa^{I}}(\hat{\mathbf{w}}_{j})=-2(\hat{\mathbf{w}}_{j}^{I})^{\top}\cdot\mathbf{g}_{j}^{I},\;\;\frac{\partial
    S_{\mathcal{V}_{j}}}{\partial\kappa^{H}}(\hat{\mathbf{w}}_{j})=-2(\hat{\mathbf{w}}_{j}^{H})^{\top}\cdot\mathbf{g}_{j}^{H}$$，我们得到
- en: · gHj (5.12)
  id: totrans-1545
  prefs: []
  type: TYPE_NORMAL
  zh: · gHj (5.12)
- en: where gj is the vector
  id: totrans-1546
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 gj 是向量
- en: $$\mathbf{g}_{j}=[\mathbf{g}_{j}^{I},\mathbf{g}_{j}^{H}]=\mathbf{J}_{j}^{-1}({\hat{\mathbf{w}}}_{j})\cdot{\frac{\partial
    S_{\mathcal{V}_{j}}}{\partial\mathbf{w}}}({\hat{\mathbf{w}}}_{j}).$$
  id: totrans-1547
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbf{g}_{j}=[\mathbf{g}_{j}^{I},\mathbf{g}_{j}^{H}]=\mathbf{J}_{j}^{-1}({\hat{\mathbf{w}}}_{j})\cdot{\frac{\partial
    S_{\mathcal{V}_{j}}}{\partial\mathbf{w}}}({\hat{\mathbf{w}}}_{j}).$$
- en: $$(5.13)$$
  id: totrans-1548
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.13)$$
- en: ∂w (w% j ). (5.13)
  id: totrans-1549
  prefs: []
  type: TYPE_NORMAL
  zh: ∂w (w% j ). (5.13)
- en: 'In summary, the algorithm for adapting regularization parameters consists of
    the following 8 steps:'
  id: totrans-1550
  prefs: []
  type: TYPE_NORMAL
  zh: 总结，自适应正则化参数的算法包括以下 8 个步骤：
- en: 10 We have recently extended this algorithm incorporating second order information
    via the Conjugate Gradient technique [11].
  id: totrans-1551
  prefs: []
  type: TYPE_NORMAL
  zh: 10 我们最近通过共轭梯度技术 [11] 扩展了该算法，纳入了二阶信息。
- en: 1. Choose the split ratio; hence, the number of folds, K.
  id: totrans-1552
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 选择拆分比率，因此，折叠数 K。
- en: 2. Initialize κ and the weights of the network11.
  id: totrans-1553
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 初始化 κ 和网络的权重11。
- en: 3. Train the K networks with fixed κ on Tj to achieve w% j (κ), j = 1, 2, ···
    , K.
  id: totrans-1554
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 使用固定的 κ 在 Tj 上训练 K 个网络，以达到 w% j (κ)，j = 1, 2, ··· , K。
- en: Calculate the validation errors SVj and the cross-validation estimate Γ%.
  id: totrans-1555
  prefs: []
  type: TYPE_NORMAL
  zh: 计算验证误差 SVj 和交叉验证估计 Γ%。
- en: 4. Calculate the gradients ∂SVj /∂κ and *∂Γ /∂* % κ cf. (5.9) and (5.10). Initialize
    the step-size η.
  id: totrans-1556
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 计算梯度 ∂SVj /∂κ 和 *∂Γ /∂* % κ，参考 (5.9) 和 (5.10)。初始化步长 η。
- en: 5. Update κ using (5.7).
  id: totrans-1557
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 使用 (5.7) 更新 κ。
- en: 6. Retrain the K networks from the previous weight estimates and recalculate
    the cross-validation error Γ%.
  id: totrans-1558
  prefs: []
  type: TYPE_NORMAL
  zh: 6. 从之前的权重估计中重训练 K 个网络，并重新计算交叉验证误差 Γ%。
- en: 7. If no decrease in cross-validation error then perform a bisection of η and
    go to step 5; otherwise, continue.
  id: totrans-1559
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 如果交叉验证误差没有下降，则对 η 进行二分并回到第 5 步；否则，继续。
- en: 8. Repeat steps 4–7 until the relative change in cross-validation error is below
    a small percentage or, e.g., the 2-norm of the gradient *∂Γ /∂* % κ is below a
    small number.
  id: totrans-1560
  prefs: []
  type: TYPE_NORMAL
  zh: 8. 重复步骤 4–7，直到交叉验证误差的相对变化低于一个小百分比，或者，例如，梯度 *∂Γ /∂* % κ 的 2-范数低于一个小数字。
- en: 'Compared to standard neural network training the above algorithm does generally
    not lead to severe computational overhead. First of all, the standard approach
    of tuning regularization parameters by, more or less systematic search, requires
    a lot of training sessions. The additional terms to be computed in the adaptive
    algorithm are: 1) the derivative of the regularization functions w.r.t. the weights,
    ∂r/∂w, 2) the gradient of the validation errors, ∂SVj /∂w, and 3) the inverse
    Hessians, J−1 j . The first term is often a simple function of the weights12 and
    computationally inexpensive. In the case of feed-forward neural networks, the
    second term is computed by one pass of the validation examples through a standard
    back-propagation algorithm. The third term is computationally more expensive.
    However, if the network is trained using a second order scheme, which requires
    computation of the inverse Hessian13, there is no computational overhead.'
  id: totrans-1561
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准神经网络训练相比，上述算法通常不会导致严重的计算开销。首先，通过或多或少的系统搜索来调节正则化参数的标准方法需要大量的训练会话。自适应算法中需要计算的附加项是：1)
    正则化函数相对于权重的导数 ∂r/∂w，2) 验证误差的梯度 ∂SVj /∂w，以及 3) 逆海森矩阵 J−1 j。第一个项通常是权重的简单函数，计算上开销不大。在前馈神经网络的情况下，第二个项通过标准反向传播算法对验证样本进行一次传递来计算。第三个项的计算成本较高。然而，如果网络使用二阶方案进行训练，这需要计算逆海森矩阵，则没有计算开销。
- en: The adaptive algorithm requires of the order of K·itr κ·itr η weight retrainings.
  id: totrans-1562
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应算法需要大约 K·itr κ·itr η 次权重重训练。
- en: Here itrκ is the number of iterations in the gradient descent scheme for κ and
    itr η is the average number of bisections of η in step 7 of the algorithm. In
    the experiments carried out the number of retrainings is approx. 100–300 times
    K.
  id: totrans-1563
  prefs: []
  type: TYPE_NORMAL
  zh: 此处 itrκ 是 κ 的梯度下降方案中的迭代次数，itr η 是算法第 7 步中 η 的平均二分次数。在进行的实验中，重训练的次数大约是 100–300
    倍 K。
- en: Recall, since we keep on retraining from the current weight estimate, the number
    of training epochs is generally small.
  id: totrans-1564
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于我们持续从当前权重估计中进行重训练，训练周期的数量通常较小。
- en: The number of weight retrainings is somewhat higher than that involved when
    optimizing the network by using a pruning technique like validation set based
    Optimal Brain Damage (vOBD) [25], [27]. vOBD based on K-fold cross-validation
    requires of the order of K ·m retrainings, where m = dim(w). The adaptive regularization
    algorithm is easily integrated with the pruning algorithm as demonstrated in the
    experimental section.
  id: totrans-1565
  prefs: []
  type: TYPE_NORMAL
  zh: 权重重训练的次数略高于使用像基于验证集的最佳脑损伤 (vOBD) [25]，[27] 这样的修剪技术优化网络时涉及的次数。基于 K 折交叉验证的 vOBD
    需要大约 K ·m 次重训练，其中 m = dim(w)。自适应正则化算法易于与修剪算法结合，正如实验部分所示。
- en: 11 In Sec. 5.4.1 a practical initialization procedure for κ is described. 12
    For weight decay, it is 2w. 13 Often the computations are reduced by using a Hessians
    approximation, e.g.,
  id: totrans-1566
  prefs: []
  type: TYPE_NORMAL
  zh: 11 在第 5.4.1 节中描述了一种 κ 的实用初始化程序。12 对于权重衰减，它为 2w。13 通常通过使用 Hessians 近似减少计算，例如，
- en: the Gauss-Newton approximation. Many studies have reported significant training
    speed-up by using second order methods, see e.g., [22], [35].
  id: totrans-1567
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯-牛顿近似。许多研究报告了通过使用二阶方法显著加快训练速度，参见例如 [22], [35]。
- en: 5.4 Numerical Experiments 5.4.1 Potentials And Limitations In The Approach
  id: totrans-1568
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 数值实验 5.4.1 方法的潜力和局限性
- en: The purpose of the section is to demonstrate the potential and limitations of
    the suggested adaptive regularization framework. We consider the simple linear
    data generating *system*, viz. estimating the mean of a Gaussian variable,
  id: totrans-1569
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目的是展示建议的自适应正则化框架的潜力和局限性。我们考虑简单的线性数据生成 *系统*，即估计高斯变量的均值，
- en: $$y(k)=w^{\circ}+\varepsilon(k)$$
  id: totrans-1570
  prefs: []
  type: TYPE_NORMAL
  zh: $$y(k)=w^{\circ}+\varepsilon(k)$$
- en: $$(5.14)$$
  id: totrans-1571
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.14)$$
- en: y(k) = w◦ + ε(k) (5.14)
  id: totrans-1572
  prefs: []
  type: TYPE_NORMAL
  zh: y(k) = w◦ + ε(k) (5.14)
- en: where w◦ is the true mean and the noise ε(k) ∼ N (0, σ2ε ).
  id: totrans-1573
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 w◦ 是真实均值，噪声 ε(k) ∼ N (0, σ2ε)。
- en: We employ 2-fold cross-validation, i.e., D = T1 ∪ T2, where Tj , j = 1, 2 denote
    the two training sets in the validation procedure containing approximately half
    the examples14. The linear *model* y(k) = w + e(k) is trained using the mean square
    cost function augmented by simple weight decay, as shown by
  id: totrans-1574
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用 2 倍交叉验证，即 D = T1 ∪ T2，其中 Tj，j = 1, 2 表示验证过程中包含大约一半示例的两个训练集14。线性 *模型* y(k)
    = w + e(k) 使用均方成本函数进行训练，并通过简单的权重衰减进行增强，如下所示
- en: $$C_{\mathcal{T}_{j}}(w)={\frac{1}{N_{t j}}}\sum_{k=1}^{N_{t j}}(y(k)-w)^{2}+\kappa\cdot
    w^{2}$$
  id: totrans-1575
  prefs: []
  type: TYPE_NORMAL
  zh: $$C_{\mathcal{T}_{j}}(w)={\frac{1}{N_{t j}}}\sum_{k=1}^{N_{t j}}(y(k)-w)^{2}+\kappa\cdot
    w^{2}$$
- en: where k runs over examples of the data set in question. The estimated weights
    are w%j = ¯yj/(1 + κ) where y¯j = N −1 tjNtj k=1 y(k) are the estimated mean.
    For this simple case, the minimization of the cross-validation error given by,
  id: totrans-1576
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 k 遍历相关数据集的示例。估计的权重为 w%j = ¯yj/(1 + κ)，其中 y¯j = N −1 tjNtj k=1 y(k) 是估计的均值。对于这个简单情况，交叉验证误差的最小化由以下给出，
- en: $$\widehat{\Gamma}(\kappa)=\frac{1}{2}\sum_{j=1}^{2}S_{\mathcal{V}_{j}}(\widehat{\mathbf{w}}_{j}(\kappa)),\
    \ \ \ S_{\mathcal{V}_{j}}(\widehat{\mathbf{w}}_{j}(\kappa))=\frac{1}{N_{v j}}\sum_{k=1}^{N_{v
    j}}(y(k)-\widehat{w}_{j})^{2},$$
  id: totrans-1577
  prefs: []
  type: TYPE_NORMAL
  zh: $$\widehat{\Gamma}(\kappa)=\frac{1}{2}\sum_{j=1}^{2}S_{\mathcal{V}_{j}}(\widehat{\mathbf{w}}_{j}(\kappa)),\
    \ \ \ S_{\mathcal{V}_{j}}(\widehat{\mathbf{w}}_{j}(\kappa))=\frac{1}{N_{v j}}\sum_{k=1}^{N_{v
    j}}(y(k)-\widehat{w}_{j})^{2},$$
- en: $$(5.15)$$
  id: totrans-1578
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.15)$$
- en: $$(5.16)$$
  id: totrans-1579
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.16)$$
- en: $$(5.17)$$
  id: totrans-1580
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.17)$$
- en: $$(5.18)$$
  id: totrans-1581
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.18)$$
- en: 2, (5.16)
  id: totrans-1582
  prefs: []
  type: TYPE_NORMAL
  zh: 2, (5.16)
- en: can be done exactly. The optimal κ is given by
  id: totrans-1583
  prefs: []
  type: TYPE_NORMAL
  zh: 可以精确计算。最优的 κ 由以下给出
- en: $$\kappa_{\mathrm{opt}}={\frac{{\bar{y}}_{1}^{2}+{\bar{y}}_{2}^{2}}{2{\bar{y}}_{1}{\bar{y}}_{2}}}-1.$$
  id: totrans-1584
  prefs: []
  type: TYPE_NORMAL
  zh: $$\kappa_{\mathrm{opt}}={\frac{{\bar{y}}_{1}^{2}+{\bar{y}}_{2}^{2}}{2{\bar{y}}_{1}{\bar{y}}_{2}}}-1.$$
- en: Assuming N to be even, the ensemble average of the estimated weights15, w%j
    (κopt), leads to the final estimate
  id: totrans-1585
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 N 为偶数，估计权重的集合平均15，w%j (κopt)，导致最终估计
- en: $$\widehat w_{\mathrm{reg}}=\frac{1}{2}\left(\widehat w_{1}(\kappa_{\mathrm{opt}})+\widehat
    w_{2}(\kappa_{\mathrm{opt}})\right)=\frac{\bar{y}_{1}\bar{y}_{2}(\bar{y}_{1}+\bar{y}_{2})}{\bar{y}_{1}^{2}+\bar{y}_{2}^{2}}.$$
  id: totrans-1586
  prefs: []
  type: TYPE_NORMAL
  zh: $$\widehat w_{\mathrm{reg}}=\frac{1}{2}\left(\widehat w_{1}(\kappa_{\mathrm{opt}})+\widehat
    w_{2}(\kappa_{\mathrm{opt}})\right)=\frac{\bar{y}_{1}\bar{y}_{2}(\bar{y}_{1}+\bar{y}_{2})}{\bar{y}_{1}^{2}+\bar{y}_{2}^{2}}.$$
- en: . (5.18)
  id: totrans-1587
  prefs: []
  type: TYPE_NORMAL
  zh: . (5.18)
- en: 'Notice two properties: First, the estimate is self-consistent as limN→∞ w%reg
    ='
  id: totrans-1588
  prefs: []
  type: TYPE_NORMAL
  zh: 注意两个特性：首先，估计是自洽的，因为 limN→∞ w%reg =
- en: limN→∞ w%D = w◦ where w%D = N −1 Nk=1 y(k) = (¯y1 + ¯y2)/2 is the unregularized
    estimate trained on all data. Secondly, it is easy to verify that y¯j ∼ N (w◦,
    2σ2ε /N). That is, if the *normalized true weight* θ ≡ w◦/A where A = &2/N · σε
    is large then y¯j ≈ w◦ which means, w%reg ≈ w%D.
  id: totrans-1589
  prefs: []
  type: TYPE_NORMAL
  zh: limN→∞ w%D = w◦，其中 w%D = N −1 Nk=1 y(k) = (¯y1 + ¯y2)/2 是基于所有数据的未正则化估计。其次，容易验证
    y¯j ∼ N (w◦, 2σ2ε /N)。也就是说，如果 *标准化真实权重* θ ≡ w◦/A，其中 A = &2/N · σε 较大，则 y¯j ≈ w◦，这意味着
    w%reg ≈ w%D。
- en: "14 That is, Nt1 = N/2\t and Nt2 = N − Nt1. Note that these training sets are\
    \ also the two validation sets, V1 = T2, and vice versa. 15 The ensemble average\
    \ corresponds to retraining on all data using κopt. The weighting of the two estimates\
    \ is only valid for N even (see Sec. 5.2 for the general case)."
  id: totrans-1590
  prefs: []
  type: TYPE_NORMAL
  zh: 14 即，Nt1 = N/2 和 Nt2 = N − Nt1。注意，这些训练集也是两个验证集，V1 = T2，反之亦然。15 集合平均对应于使用 κopt
    在所有数据上重新训练。两个估计的加权仅在 N 为偶数时有效（参见第 5.2 节的一般情况）。
- en: The objective is now to test whether using w%reg results in lower generalization
    error than employing the unregularized estimate w%D. The generalization error
    associated with using the weight w is given by
  id: totrans-1591
  prefs: []
  type: TYPE_NORMAL
  zh: 目标现在是测试使用 w%reg 是否能比采用未正则化估计 w%D 产生更低的泛化误差。与使用权重 w 相关的泛化误差由以下公式给出：
- en: $$G(w)=\sigma_{\varepsilon}^{2}+(w-w^{\circ})^{2}.$$
  id: totrans-1592
  prefs: []
  type: TYPE_NORMAL
  zh: $$G(w)=\sigma_{\varepsilon}^{2}+(w-w^{\circ})^{2}.$$
- en: $$(5.19)$$
  id: totrans-1593
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.19)$$
- en: $$(5.20)$$
  id: totrans-1594
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.20)$$
- en: 2. (5.19)
  id: totrans-1595
  prefs: []
  type: TYPE_NORMAL
  zh: 2. (5.19)
- en: Further define the generalization error improvement,
  id: totrans-1596
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步定义泛化误差的改进：
- en: $$Z=G({\hat{w}}_{\mathcal{D}})-G({\hat{w}}_{\mathrm{reg}})=({\hat{w}}_{\mathcal{D}}-w^{\circ})^{2}-({\hat{w}}_{\mathrm{reg}}-w^{\circ})^{2}.$$
  id: totrans-1597
  prefs: []
  type: TYPE_NORMAL
  zh: $$Z=G({\hat{w}}_{\mathcal{D}})-G({\hat{w}}_{\mathrm{reg}})=({\hat{w}}_{\mathcal{D}}-w^{\circ})^{2}-({\hat{w}}_{\mathrm{reg}}-w^{\circ})^{2}.$$
- en: 2. (5.20)
  id: totrans-1598
  prefs: []
  type: TYPE_NORMAL
  zh: 2. (5.20)
- en: Note that Z merely is a function of the random variables y¯1, y¯2 and the true
    weight w◦, i.e., it suffices to get samples of y¯1, y¯2 when evaluating properties
    of Z. Define the normalized variables
  id: totrans-1599
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 Z 只是随机变量 y¯1、y¯2 和真实权重 w◦ 的一个函数，即在评估 Z 的性质时，仅需获取 y¯1、y¯2 的样本。定义归一化变量：
- en: $$\widetilde{y}_{j}=\frac{\widetilde{y}_{j}}{A}\sim\mathcal{N}\left(\frac{w^{\circ}}{\sigma_{\varepsilon}}\cdot\sqrt{\frac{N}{2}},1\right)=\mathcal{N}(\theta,1).$$
  id: totrans-1600
  prefs: []
  type: TYPE_NORMAL
  zh: $$\widetilde{y}_{j}=\frac{\widetilde{y}_{j}}{A}\sim\mathcal{N}\left(\frac{w^{\circ}}{\sigma_{\varepsilon}}\cdot\sqrt{\frac{N}{2}},1\right)=\mathcal{N}(\theta,1).$$
- en: $$(5.21)$$
  id: totrans-1601
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.21)$$
- en: It is easily shown that the normalized generalization error improvement Z/A2
    is a function of y'1, y'2 and θ; hence, the distribution of Z/A2 is parameterized
    solely by θ.
  id: totrans-1602
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，归一化的泛化误差改进 Z/A2 是 y'1、y'2 和 θ 的函数；因此，Z/A2 的分布仅由 θ 参数化。
- en: As a quality measure we consider the *probability of improvement* in generalization
    error given by Prob{Z > 0}. Note that Prob{Z > 0} = 1/2 corresponds to equal preference
    of the two estimates. The probability of improvement depends only on the normalized
    weight θ since Prob{Z > 0} = Prob{Z/A2 > 0}.
  id: totrans-1603
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个质量度量，我们考虑 *泛化误差改善的概率*，由 Prob{Z > 0} 给出。注意 Prob{Z > 0} = 1/2 对应于两个估计的相等偏好。改进的概率仅依赖于归一化权重
    θ，因为 Prob{Z > 0} = Prob{Z/A2 > 0}。
- en: Moreover, we consider the *relative generalization error improvement*, defined
    as
  id: totrans-1604
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们考虑 *相对泛化误差的改进*，定义为：
- en: $$\mathrm{RGI}=100\%\cdot{\frac{Z}{G({\hat{w}}_{\mathcal{D}})}}.$$
  id: totrans-1605
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{RGI}=100\%\cdot{\frac{Z}{G({\hat{w}}_{\mathcal{D}})}}.$$
- en: $$(5.22)$$
  id: totrans-1606
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.22)$$
- en: $$(5.23)$$
  id: totrans-1607
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.23)$$
- en: In particular, we focus on the probability that the relative improvement in
    generalization is bigger than16 x, i.e., Prob(RGI > x). Optimally Prob(RGI > x)
  id: totrans-1608
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们关注相对改进大于 16 x 的概率，即 Prob(RGI > x)。最佳情况为 Prob(RGI > x)：
- en: should be close to 1 for x ≤ 0% and slowly decaying towards zero for 0% < x
    ≤
  id: totrans-1609
  prefs: []
  type: TYPE_NORMAL
  zh: 应该在 x ≤ 0% 时接近 1，并在 0% < x ≤ 时缓慢趋近于零。
- en: 100%. Using the notation w'reg = w%reg/A, w'D = w%D/A, RGI can be written as
  id: totrans-1610
  prefs: []
  type: TYPE_NORMAL
  zh: 100%。使用符号 w'reg = w%reg/A，w'D = w%D/A，RGI 可以写为：
- en: $$\mathrm{RGI}=100\%\cdot{\frac{({\tilde{w}}_{\mathcal{D}}-\theta)^{2}-({\tilde{w}}_{\mathrm{reg}}-\theta)^{2}}{N/2+({\tilde{w}}_{\mathcal{D}}-\theta)^{2}}}.$$
  id: totrans-1611
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{RGI}=100\%\cdot{\frac{({\tilde{w}}_{\mathcal{D}}-\theta)^{2}-({\tilde{w}}_{\mathrm{reg}}-\theta)^{2}}{N/2+({\tilde{w}}_{\mathcal{D}}-\theta)^{2}}}.$$
- en: N/2+(w'D − θ)2 . (5.23)
  id: totrans-1612
  prefs: []
  type: TYPE_NORMAL
  zh: N/2+(w'D − θ)2 . (5.23)
- en: Thus, the distribution of RGI is parameterized by θ and N.
  id: totrans-1613
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RGI 的分布由 θ 和 N 参数化。
- en: The quality measures are computed by generating Q independent realizations of
    y'1, y'2, i.e., {y'
  id: totrans-1614
  prefs: []
  type: TYPE_NORMAL
  zh: 质量度量是通过生成 Q 个独立的 y'1、y'2 实现的，即 {y'
- en: (i)
  id: totrans-1615
  prefs: []
  type: TYPE_NORMAL
  zh: (i)
- en: 1 , y'
  id: totrans-1616
  prefs: []
  type: TYPE_NORMAL
  zh: 1 , y'
- en: (i)
  id: totrans-1617
  prefs: []
  type: TYPE_NORMAL
  zh: (i)
- en: 2 }Qi=1. E.g., the probability of improvement is estimated by Pimp = Q−1 Qi=1
    μ(Z(i)) where μ(x)=1 for x > 0, and zero otherwise.
  id: totrans-1618
  prefs: []
  type: TYPE_NORMAL
  zh: 2 }Qi=1. 例如，改善的概率由 Pimp = Q−1 Qi=1 μ(Z(i)) 估计，其中 μ(x)=1 当 x > 0 时，其他情况为零。
- en: The numerical results of comparing w%reg to the unregularized estimate w%D is
    summarized in Fig. 5.1.
  id: totrans-1619
  prefs: []
  type: TYPE_NORMAL
  zh: 比较 w%reg 和未正则化估计 w%D 的数值结果总结在图 5.1 中。
- en: $$0)=\mathrm{Prob}(Z>0).$$
  id: totrans-1620
  prefs: []
  type: TYPE_NORMAL
  zh: $$0)=\mathrm{Prob}(Z>0).$$
- en: 16 Note that, Prob(RGI > 0) = Prob(Z > 0).
  id: totrans-1621
  prefs: []
  type: TYPE_NORMAL
  zh: 16 注意，Prob(RGI > 0) = Prob(Z > 0)。
- en: '![124_image_0.png](124_image_0.png)'
  id: totrans-1622
  prefs: []
  type: TYPE_IMG
  zh: '![124_image_0.png](124_image_0.png)'
- en: Fig. 5.1. Result of comparing the optimally regularized estimate wreg of the
    mean of a Gaussian variable to the unregularized estimate wD. The results are
    based on Q = 105 independent realizations. The probability of improvement Pimp,
    shown in panel (a), is one for when the normalized true weight θ = N/2 · w◦/σε
    = 0, and above 0.5 for θ <∼ 0.8. That is, when the prior information of the weight
    decay regularizer is correct
  id: totrans-1623
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1. 比较高效正则化估计 wreg 和未正则化估计 wD 的结果。结果基于 Q = 105 次独立实现。改善的概率 Pimp，如面板 (a) 所示，当归一化真实权重
    θ = N/2 · w◦/σε = 0 时为 1，且对于 θ <∼ 0.8 时大于 0.5。也就是说，当权重衰减正则器的先验信息正确时：
- en: (true weight close to zero), when N is small or when σε is large. As θ becomes
    large Pimp tends to 0.5 due to the fact that w ≈ wD. Panel (b)–(d) display Prob(RGI
    > x)
  id: totrans-1624
  prefs: []
  type: TYPE_NORMAL
  zh: （真实权重接近于零），当N较小或σε较大时。随着θ增大，Pimp趋向于0.5，因为w ≈ wD。面板(b)–(d)显示Prob(RGI > x)
- en: for θ ∈ {0, 2, 10}. The ideal probability curve is 1 for x < 0 and a slow decay
    towards zero for x > 0. The largest improvement is attained for small θ and small
    N. Panel (c) and (d) indicate that small N gives the largest probability for x
    > 0; however, also the smallest probability for negative x. That is, a higher
    chance of getting a good improvement also increases the change of deterioration.
    Notice, even though Pimp < 0.5 for θ = 2, 10 there is still a reasonable probability
    of getting a significant improvement.
  id: totrans-1625
  prefs: []
  type: TYPE_NORMAL
  zh: 对于θ ∈ {0, 2, 10}。理想的概率曲线在x < 0时为1，而在x > 0时缓慢衰减至零。对于小θ和小N，获得的最大改善效果。面板(c)和(d)表明，小N在x
    > 0时提供了最大的概率；然而，在负x时则是最小的概率。也就是说，获得良好改善的机会越高，恶化的风险也随之增加。请注意，尽管对于θ = 2, 10时Pimp
    < 0.5，但仍然有合理的概率获得显著改善。
- en: 5.4.2 Classification
  id: totrans-1626
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4.2 分类
- en: We test the performance of the adaptive regularization algorithm on a vowel
    classification problem. The data are based on the Peterson and Barney database
  id: totrans-1627
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一个元音分类问题上测试自适应正则化算法的性能。数据基于彼得森和巴尼数据库
- en: '[36]. The classes are vowel sounds characterized by the first four formant
    frequencies. 76 persons (33 male, 28 female and 15 children) have pronounced c
    = 10 different vowels (IY IH EH AE AH AA AO UH UW ER) two times. This results
    in a data base of totally 1520 examples. The database is the verified database
    described in [41] where all data17 are used, including examples where utterance
    failed of unanimous identification in the listening test (26 listeners). All examples
    were included to make the task more difficult.'
  id: totrans-1628
  prefs: []
  type: TYPE_NORMAL
  zh: '[36]。类别是由前四个共振峰频率特征化的元音。76人（33名男性，28名女性和15名儿童）发出了c = 10种不同的元音（IY IH EH AE AH
    AA AO UH UW ER），每种发音两次。这导致总共1520个示例的数据集。该数据库是[41]中描述的经过验证的数据库，所有数据都被使用，包括在听力测试中未能一致识别的发声示例（26名听众）。所有示例都被纳入，以增加任务的难度。'
- en: The regularization was adapted using a hold-out validation error estimator,
    thus the examples were split into a data set, D, consisting of N = 760 examples
  id: totrans-1629
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是通过使用保留验证误差估计器进行适配的，因此示例被划分为一个数据集D，包含N = 760个示例
- en: (16 male, 14 female and 8 children) and an independent test set of the remaining
    760 examples. The regularization was adapted by splitting the data set D equally
    into a validation set of Nv = 380 examples and a training set of Nt = 380 examples
    (8 male, 7 female and 4 children in each set).
  id: totrans-1630
  prefs: []
  type: TYPE_NORMAL
  zh: （16名男性，14名女性和8名儿童）和剩余760个示例的独立测试集。正则化通过将数据集D均分为一个包含Nv = 380个示例的验证集和一个包含Nt =
    380个示例的训练集（每个集合中8名男性，7名女性和4名儿童）进行适配。
- en: We used a feed-forward 2-layer neural network with hyperbolic tangent neurons
    in the hidden layer and modified SoftMax normalized outputs, y%i, see e.g.,
  id: totrans-1631
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个前馈的2层神经网络，隐藏层采用双曲正切神经元，输出采用修改后的SoftMax规范化输出y%i，见例如，
- en: '[2], [18], [3]. Thus, the outputs estimates the posterior class probabilities
    p(Ci|x),'
  id: totrans-1632
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]，[18]，[3]。因此，输出估计后验类别概率p(Ci|x)，'
- en: where Ci denotes the i'th class, i = 1, 2, ··· , c. Bayes rule (see e.g., [2])
    is used to assign Ci to input x if i = arg maxj p(Cj |x). Suppose that the network
    weights are given by w = [wI , wIbias, wH, wHbias] where wI , wH are input-to-hidden
    and hidden-to-output weights, respectively, and the bias weights are assembled
    in wIbias and wHbias. Suppose that the targets yi(k)=1 if x(k) ∈ Ci, and zero
    otherwise. The network is optimized using a log-likelihood loss function augmented
    by a weight decay regularizer using 4 regularization parameters,
  id: totrans-1633
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Ci表示第i类，i = 1, 2, ··· , c。贝叶斯规则（参见例如[2]）用于将Ci分配给输入x，如果i = arg maxj p(Cj |x)。假设网络权重由w
    = [wI , wIbias, wH, wHbias]给出，其中wI，wH分别是输入到隐藏层和隐藏层到输出层的权重，而偏差权重则组合在wIbias和wHbias中。假设目标yi(k)=1如果x(k)
    ∈ Ci，否则为零。该网络使用对数似然损失函数进行优化，并通过4个正则化参数增强了权重衰减正则化器，
- en: $$C(\mathbf{w})=\frac{1}{N_{t}}\sum_{k=1}^{N_{t}}\sum_{i=1}^{c}y_{i}(k)\log(\hat{y}_{i}(k,\mathbf{w}))\tag{5.24}$$
    $$+\kappa^{I}\cdot|\mathbf{w}^{I}|^{2}+\kappa_{\rm bias}^{I}\cdot|\mathbf{w}_{\rm
    bias}^{I}|^{2}+\kappa^{H}\cdot|\mathbf{w}^{H}|^{2}+\kappa_{\rm bias}^{H}\cdot|\mathbf{w}_{\rm
    bias}^{H}|^{2}.$$
  id: totrans-1634
  prefs: []
  type: TYPE_NORMAL
  zh: $$C(\mathbf{w})=\frac{1}{N_{t}}\sum_{k=1}^{N_{t}}\sum_{i=1}^{c}y_{i}(k)\log(\hat{y}_{i}(k,\mathbf{w}))\tag{5.24}$$
    $$+\kappa^{I}\cdot|\mathbf{w}^{I}|^{2}+\kappa_{\rm bias}^{I}\cdot|\mathbf{w}_{\rm
    bias}^{I}|^{2}+\kappa^{H}\cdot|\mathbf{w}^{H}|^{2}+\kappa_{\rm bias}^{H}\cdot|\mathbf{w}_{\rm
    bias}^{H}|^{2}.$$
- en: We further define unnormalized weight decays as α ≡ κ · Nt. This regularizer
    is motivated by the fact that the bias, input and hidden layer weights play a
    different role, e.g., the input, hidden and bias signals normally have different
    scale (see also [2, Ch. 9.2]).
  id: totrans-1635
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步定义未归一化的权重衰减为α ≡ κ · Nt。这个正则化器的动机在于偏差、输入和隐藏层权重扮演着不同的角色，例如，输入、隐藏和偏差信号通常具有不同的尺度（见[2,
    Ch. 9.2]）。
- en: 'The simulation set-up is:'
  id: totrans-1636
  prefs: []
  type: TYPE_NORMAL
  zh: 仿真设置为：
- en: '- Network: 4 inputs, 5 hidden neurons, 9 outputs18.'
  id: totrans-1637
  prefs: []
  type: TYPE_NORMAL
  zh: '- 网络：4个输入，5个隐藏神经元，9个输出18。'
- en: '- Weights are initialized uniformly over [−0.5, 0.5], regularization parameters
    are initialized at zero. One step in a gradient descent training algorithm (see
    e.g., [29]) is performed and the weight decays are re-initialized at λmax/102,
    where λmax is the max. eigenvalue of the Hessian matrix of the cost function.'
  id: totrans-1638
  prefs: []
  type: TYPE_NORMAL
  zh: '- 权重在[−0.5, 0.5]范围内均匀初始化，正则化参数初始化为零。执行梯度下降训练算法中的一步（见例如，[29]），并且权重衰减在λmax/102处重新初始化，其中λmax是代价函数Hessian矩阵的最大特征值。'
- en: 'This initialization scheme is motivated by the following observations:'
  id: totrans-1639
  prefs: []
  type: TYPE_NORMAL
  zh: 该初始化方案的动机来自以下观察：
- en: '- Weight decays should be so small that they do not reduce the approximation
    capabilities of the network significantly.'
  id: totrans-1640
  prefs: []
  type: TYPE_NORMAL
  zh: '- 权重衰减应足够小，以免显著降低网络的近似能力。'
- en: '- They should be so large that the algorithm is prevented from being trapped
    in a local optimum and numerical instabilities are eliminated.'
  id: totrans-1641
  prefs: []
  type: TYPE_NORMAL
  zh: '- 它们应足够大，以防止算法陷入局部最优，并消除数值不稳定性。'
- en: 17 The database can be retrieved from ftp://eivind.imm.dtu.dk/dist/data/vowel/
  id: totrans-1642
  prefs: []
  type: TYPE_NORMAL
  zh: 17 数据库可以从ftp://eivind.imm.dtu.dk/dist/data/vowel/检索。
- en: PetersonBarney.tar.Z 18 We only need 9 outputs since the posterior class probability
    of the 10th class is given by 1 − 9j=1 p(Cj |x).
  id: totrans-1643
  prefs: []
  type: TYPE_NORMAL
  zh: PetersonBarney.tar.Z 18 我们只需要9个输出，因为第10类的后验类概率由1 − 9j=1 p(Cj |x)给出。
- en: '- Training is now done using a Gauss-Newton algorithm (see e.g., [29]). The
    Hessian is inverted using the Moore-Penrose pseudo inverse ensuring that the eigenvalue
    spread19 is less than 108.'
  id: totrans-1644
  prefs: []
  type: TYPE_NORMAL
  zh: '- 训练现在使用高斯-牛顿算法进行（见例如，[29]）。Hessian使用Moore-Penrose伪逆进行反演，确保特征值分布19小于108。'
- en: '- The regularization step-size η is initialized at 1. - When the adaptive regularization
    scheme has terminated 3% of the weights are pruned using a validation set based
    version of the Optimal Brain Damage'
  id: totrans-1645
  prefs: []
  type: TYPE_NORMAL
  zh: '- 正则化步长η初始化为1。- 当自适应正则化方案终止时，使用基于验证集的Optimal Brain Damage修剪3%的权重。'
- en: (vOBD) recipe [25], [27].
  id: totrans-1646
  prefs: []
  type: TYPE_NORMAL
  zh: (vOBD) 配方 [25], [27].
- en: '- Alternation between pruning and adaptive regularization continues until the
    validation error has reached a minimum.'
  id: totrans-1647
  prefs: []
  type: TYPE_NORMAL
  zh: '- 修剪与自适应正则化之间的交替持续进行，直到验证误差达到最小值。'
- en: '- Finally, remaining weights are retrained on all data using the optimized
    weight decay parameters.'
  id: totrans-1648
  prefs: []
  type: TYPE_NORMAL
  zh: '- 最后，剩余的权重在所有数据上使用优化的权重衰减参数进行重新训练。'
- en: Table 5.1. Probability of misclassification (pmc) and log-likelihood cost function
  id: totrans-1649
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1. 错误分类概率（pmc）和对数似然代价函数
- en: (without reg. term, see (5.24)) for the classification example. The neural network
    averages and standard deviations are computed from 10 runs. In the case of small
    fixed regularization, weight decays were set at initial values equal to λmax/106
    where λmax is the largest eigenvalue of the Hessian matrix of the cost function.
    Optimal regularization refers to the case of optimizing 4 weight decay parameters.
    Pruning refers to validation set based OBD. KNN refers to k-nearest-neighbor classification.
  id: totrans-1650
  prefs: []
  type: TYPE_NORMAL
  zh: （无正则项，见(5.24)）用于分类示例。神经网络的均值和标准差是从10次运行中计算得出的。在小固定正则化的情况下，权重衰减的初始值设定为λmax/106，其中λmax是代价函数Hessian矩阵的最大特征值。最佳正则化是指优化4个权重衰减参数的情况。修剪是指基于验证集的OBD。KNN是指k近邻分类。
- en: '|                                          | NN            | NN            |
    KNN   |'
  id: totrans-1651
  prefs: []
  type: TYPE_TB
  zh: '|                                          | NN            | NN            |
    KNN   |'
- en: '| --- | --- | --- | --- |'
  id: totrans-1652
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| small fixed reg. opt. reg.+prun. (k = 9) |               |               |       |'
  id: totrans-1653
  prefs: []
  type: TYPE_TB
  zh: '| 小固定正则化 优化正则化+修剪 (k = 9) |               |               |       |'
- en: '| Training Set                             | 0.075 ±0.026  | 0.107 ± 0.008
    | 0.150 |'
  id: totrans-1654
  prefs: []
  type: TYPE_TB
  zh: '| 训练集                             | 0.075 ±0.026  | 0.107 ± 0.008 | 0.150 |'
- en: '| Validation Set                           | 0.143 ± 0.014 | 0.115 ± 0.004
    | 0.158 |'
  id: totrans-1655
  prefs: []
  type: TYPE_TB
  zh: '| 验证集                           | 0.143 ± 0.014 | 0.115 ± 0.004 | 0.158 |'
- en: '| Test Set                                 | 0.146 ± 0.010 | 0.124 ± 0.006
    | 0.199 |'
  id: totrans-1656
  prefs: []
  type: TYPE_TB
  zh: '| 测试集                                 | 0.146 ± 0.010 | 0.124 ± 0.006 | 0.199
    |'
- en: '| Test Set (train. on all data)            | 0.126 ± 0.010 | 0.119 ± 0.004
    | 0.153 |'
  id: totrans-1657
  prefs: []
  type: TYPE_TB
  zh: '| 测试集（在所有数据上训练）            | 0.126 ± 0.010 | 0.119 ± 0.004 | 0.153 |'
- en: '|                                                               | NN                              |
    NN   |'
  id: totrans-1658
  prefs: []
  type: TYPE_TB
  zh: '|                                                               | NN                              |
    NN   |'
- en: '| --- | --- | --- |'
  id: totrans-1659
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| small fixed reg. opt. reg.+prun.                              |                                 |      |'
  id: totrans-1660
  prefs: []
  type: TYPE_TB
  zh: '| 小固定正则化. 优化正则化 + 修剪                              |                                 |      |'
- en: '| Training Set                                                  | 0.2002 ±
    0.0600 0.2881 ± 0.0134 |      |'
  id: totrans-1661
  prefs: []
  type: TYPE_TB
  zh: '| 训练集                                                      | 0.2002 ± 0.0600
    0.2881 ± 0.0134 |      |'
- en: '| Validation Set                                                | 0.7016 ±
    0.2330 0.3810 ± 0.0131 |      |'
  id: totrans-1662
  prefs: []
  type: TYPE_TB
  zh: '| 验证集                                                      | 0.7016 ± 0.2330
    0.3810 ± 0.0131 |      |'
- en: '| Test Set                                                      | 0.6687 ±
    0.2030 0.3773 ± 0.0143 |      |'
  id: totrans-1663
  prefs: []
  type: TYPE_TB
  zh: '| 测试集                                                      | 0.6687 ± 0.2030
    0.3773 ± 0.0143 |      |'
- en: '| Test Set (train. on all data) 0.4426 ± 0.0328 0.3518 ± 0.0096 |                                 |      |'
  id: totrans-1664
  prefs: []
  type: TYPE_TB
  zh: '| 测试集 (在所有数据上训练) 0.4426 ± 0.0328 0.3518 ± 0.0096 |                                 |      |'
- en: '|                                                               |                                 |      |'
  id: totrans-1665
  prefs: []
  type: TYPE_TB
  zh: '|                                                               |                                 |      |'
- en: Probability of Misclassification (pmc)
  id: totrans-1666
  prefs: []
  type: TYPE_NORMAL
  zh: 错误分类概率 (pmc)
- en: Log-likelihood Cost Function
  id: totrans-1667
  prefs: []
  type: TYPE_NORMAL
  zh: 对数似然成本函数
- en: Table 5.1 reports the average and standard deviations of the probability of
    misclassification (pmc) and log-likelihood cost function over 10 runs for pruned
    networks using the optimal regularization parameters. Note that retraining on
  id: totrans-1668
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1报告了使用最优正则化参数的修剪网络在10次运行中错误分类概率 (pmc) 和对数似然成本函数的平均值和标准差。请注意重新训练
- en: 19 Eigenvalue spread should not be larger than the square root of the machine
    precision
  id: totrans-1669
  prefs: []
  type: TYPE_NORMAL
  zh: 19 特征值分布不应大于机器精度的平方根
- en: '[6].'
  id: totrans-1670
  prefs: []
  type: TYPE_NORMAL
  zh: '[6].'
- en: the full data set decreases the test pmc slightly on the average. In fact, improvement
    was noticed in 9 out of 10 runs. The table further shows the gain of the combined
    adaptive regularization/pruning algorithm relative to using a small fixed weight
    decay. However, recall, cf. Sec. 5.4.1, that the actual gain is *very* dependent
    on the noise level, data set size, etc. The objective is not to demonstrate high
    gain for a specific problem, rather to demonstrate that algorithm runs fairly
    robust in a classification set-up. For comparison we used a k-nearestneighbor
    (KNN) classification (see e.g., [2]) and found that k = 9 neighbors was optimal
    by minimizing pmc on the validation set. The neural network performed significantly
    better. Contrasting the obtained results to other work is difficult. In [37] results
    on the Peterson-Barney vowel problem are reported, but their data are not exactly
    the same; only the first 2 formant frequencies were used. Furthermore, different
    test sets have been used for the different methods presented.
  id: totrans-1671
  prefs: []
  type: TYPE_NORMAL
  zh: 完整数据集平均略微降低了测试pmc。事实上，在10次运行中，有9次观察到改进。表格进一步显示了组合自适应正则化/修剪算法相对于使用小的固定权重衰减的收益。然而，请回顾，第5.4.1节提到，实际收益*非常*依赖于噪声水平、数据集大小等。目标并不是展示某个特定问题的高收益，而是展示算法在分类设置中相对稳健的运行。为了比较，我们使用了k-近邻
    (KNN) 分类（参见例如[2]），发现k = 9邻居在验证集上最小化pmc是最优的。神经网络表现得明显更好。将获得的结果与其他工作进行对比是困难的。在[37]中报告了Peterson-Barney元音问题的结果，但他们的数据并不完全相同；仅使用了前2个共振峰频率。此外，针对不同方法使用了不同的测试集。
- en: The best result reported [28] is obtained by using KNN and reach pmc = 0.186
    which is significantly higher than our results.
  id: totrans-1672
  prefs: []
  type: TYPE_NORMAL
  zh: 报告的最佳结果[28]是通过使用KNN获得的，达到pmc = 0.186，显著高于我们的结果。
- en: Fig. 5.2 shows the evolution of the adaptive regularization as well as the pruning
    algorithm.
  id: totrans-1673
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2展示了自适应正则化以及修剪算法的演变。
- en: 5.4.3 Time Series Prediction
  id: totrans-1674
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4.3 时间序列预测
- en: We tested the performance of the adaptive regularization schemes on the MackeyGlass
    chaotic time series prediction problem, see e.g., [22], [40]. The goal is to predict
    the series 100 steps ahead based on previous observations. The feed-forward net
    configuration is an input lag-space x(k)=[x(k), x(k−6), x(k−12), x(k−18)]
  id: totrans-1675
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测试了自适应正则化方案在MackeyGlass混沌时间序列预测问题上的表现，参见例如[22]、[40]。目标是基于之前的观察预测未来100步的序列。前馈网络配置为输入延迟空间
    x(k)=[x(k), x(k−6), x(k−12), x(k−18)]
- en: of 4 inputs, 25 hidden hyperbolic tangent neurons, and a single linear output
    unit y%(k) which predicts y(k) = x(k + 100). The cost function is the squared
    error, N −1 t Nt k=1(y(k)−y%(k, w))2, augmented by a weight decay regularizer
    using 4 different weight decays as described in Section 5.4.2.
  id: totrans-1676
  prefs: []
  type: TYPE_NORMAL
  zh: 输入为4个，25个隐藏的双曲正切神经元，以及一个预测y(k) = x(k + 100)的线性输出单元y%(k)。成本函数为平方误差，N −1 t Nt
    k=1(y(k)−y%(k, w))²，使用4个不同的权重衰减的正则化器进行增强，如5.4.2节所述。
- en: 'The simulation set-up is:'
  id: totrans-1677
  prefs: []
  type: TYPE_NORMAL
  zh: 仿真设置为：
- en: '- The data set, D, has N = 500 examples and an independent test has 8500 examples.'
  id: totrans-1678
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集D包含N = 500个示例，独立测试集有8500个示例。
- en: '- The regularization parameters are optimized using a hold-out validation error
    with an even split20 of the data set into training and validation sets each having
    250 examples.'
  id: totrans-1679
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化参数使用持出验证误差进行优化，将数据集平均分为训练集和验证集，每个集各有250个示例。
- en: '- Weight decays are initialized at zero and one Gauss-Newton iteration is performed,
    then weight decays were re-initialized at λmax/106, where λmax is the max. eigenvalue
    of the Hessian matrix of the cost function.'
  id: totrans-1680
  prefs: []
  type: TYPE_NORMAL
  zh: 权重衰减初始化为零并执行一次高斯-牛顿迭代，然后权重衰减重新初始化为λmax/106，其中λmax为成本函数Hessian矩阵的最大特征值。
- en: '- The network is trained using a Gauss-Newton training scheme. The Hessian
    is inverted using the Moore-Penrose pseudo inverse ensuring that the eigenvalue
    spread is less than 108.'
  id: totrans-1681
  prefs: []
  type: TYPE_NORMAL
  zh: 网络使用高斯-牛顿训练方案进行训练。Hessian矩阵通过摩尔-彭若斯伪逆被逆转，确保特征值的分散度小于108。
- en: '- The regularization step-size η is initialized at 10−2.'
  id: totrans-1682
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化步长η初始化为10−2。
- en: 20 The sensitivity to different splits are considered in [25].
  id: totrans-1683
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到不同分割的敏感性[25]。
- en: '![128_image_1.png](128_image_1.png)'
  id: totrans-1684
  prefs: []
  type: TYPE_IMG
  zh: '![128_image_1.png](128_image_1.png)'
- en: '![128_image_0.png](128_image_0.png)'
  id: totrans-1685
  prefs: []
  type: TYPE_IMG
  zh: '![128_image_0.png](128_image_0.png)'
- en: Fig. 5.2. Classification example. Panels (a), (b) and (c) show the evolution
    of the adaptive regularization algorithm in a typical run (fully connected network).
    The weight decays are optimized aiming at minimizing the validation error in panel
    (a). Note that also the test error decreases. This tendency is also evident in
    panel (b) displaying pmc even though a small increase noticed. In panel (c) the
    convergence unnormalized weight decays, α = κ · Nt, are depicted. Panels (d) and
    (e) show the evolution of errors and pmc during the pruning session. The optimal
    network is chosen as the one with minimal validation error, as indicated by the
    vertical line. There is only a marginal effect of pruning in this run. Finally,
    in panel (f), the variation of the optimal (end of pruning)
  id: totrans-1686
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2. 分类示例。面板(a)、(b)和(c)显示了自适应正则化算法在典型运行中的演变（全连接网络）。权重衰减经过优化，以最小化面板(a)中的验证误差。注意测试误差也在下降。这个趋势在面板(b)中同样明显，尽管注意到小幅增加。在面板(c)中，收敛的非标准化权重衰减α
    = κ · Nt被描绘出来。面板(d)和(e)显示了剪枝阶段的误差和pmc的演变。最佳网络选择为验证误差最小的网络，如垂直线所示。在此运行中剪枝的效果仅为边际。最后，在面板(f)中，最优（剪枝结束时）的变化。
- en: α's in different runs is demonstrated. A clear similarity over runs is noticed.
  id: totrans-1687
  prefs: []
  type: TYPE_NORMAL
  zh: 不同运行中的α值得到了展示，注意到运行之间有明显的相似性。
- en: Table 5.2. Normalized squared error performance for the time series prediction
    examples. All figures are in units of 10−3σ2x and averages and standard deviations
    are computed from 10 runs. In the case of small fixed regularization, weight decays
    were set at initial values equal to λmax/106 where λmax is the largest eigenvalue
    of the Hessian matrix of the cost function. Optimal regularization refers to the
    case of optimizing 4 weight decay parameters. Pruning refers to validation set
    based OBD.
  id: totrans-1688
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.2. 时间序列预测示例的标准化平方误差表现。所有数字的单位为10−3σ²x，平均值和标准差是从10次运行中计算得出的。在小的固定正则化情况下，权重衰减的初始值设定为λmax/106，其中λmax是成本函数的Hessian矩阵的最大特征值。最佳正则化是指优化4个权重衰减参数的情况。剪枝是指基于验证集的OBD。
- en: '|                                                         | NN          | NN          |
    NN          |'
  id: totrans-1689
  prefs: []
  type: TYPE_TB
  zh: '|                                                       | NN          | NN          |
    NN          |'
- en: '| --- | --- | --- | --- |'
  id: totrans-1690
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| small fixed reg. small fixed reg.+prun. opt. reg.+prun. |             |             |             |'
  id: totrans-1691
  prefs: []
  type: TYPE_TB
  zh: '| 小固定正则化  小固定正则化+剪枝  最优正则化+剪枝 |             |             |             |'
- en: '| Training Set                                            | 0.17 ± 0.07 | 0.12
    ± 0.04 | 0.10 ± 0.07 |'
  id: totrans-1692
  prefs: []
  type: TYPE_TB
  zh: '| 训练集                                            | 0.17 ± 0.07 | 0.12 ± 0.04
    | 0.10 ± 0.07 |'
- en: '| Validation Set                                          | 0.53 ± 0.26 | 0.36
    ± 0.07 | 0.28 ± 0.14 |'
  id: totrans-1693
  prefs: []
  type: TYPE_TB
  zh: '| 验证集                                              | 0.53 ± 0.26 | 0.36 ± 0.07
    | 0.28 ± 0.14 |'
- en: '| Test Set                                                | 1.91 ± 0.68 | 1.58
    ± 0.21 | 1.29 ± 0.46 |'
  id: totrans-1694
  prefs: []
  type: TYPE_TB
  zh: '| 测试集                                               | 1.91 ± 0.68 | 1.58 ±
    0.21 | 1.29 ± 0.46 |'
- en: '| Test Set (train. on all data)                           | 1.33 ± 0.43 | 1.34
    ± 0.26 | 1.17 ± 0.48 |'
  id: totrans-1695
  prefs: []
  type: TYPE_TB
  zh: '| 测试集（在所有数据上训练）                          | 1.33 ± 0.43 | 1.34 ± 0.26 | 1.17
    ± 0.48 |'
- en: '- Alternation between adapting the 4 weight decays and validation set based
    pruning [25].'
  id: totrans-1696
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在调整4个权重衰减和验证集基础上的修剪之间进行交替[25]。'
- en: '- The pruned network is retrained on all data using the optimized weight decay
    parameters.'
  id: totrans-1697
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪后的网络使用优化的权重衰减参数在所有数据上重新训练。
- en: 'Table 5.2 reports the average and standard deviations of the normalized squared
    error (i.e., the squared error normalized with the estimated variance of x(k),
    denoted σ%2x) over 10 runs for optimal regularization parameters. Retraining on
    the full data set decreases the test error somewhat on the average. Improvement
    was noticed in 10 out of 10 runs. We tested 3 different cases: small fixed regularization,
    small fixed regularization assisted by pruning and combined adaptive regularization/pruning.
    It turns that pruning alone does not improve performance; however, supplementing
    by adaptive regularization gives a test error reduction.'
  id: totrans-1698
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.2报告了在10次运行中，最优正则化参数下归一化平方误差的平均值和标准差（即，归一化的平方误差与x(k)的估计方差σ%2x）。在完整数据集上重新训练平均会稍微减少测试误差。在10次运行中有10次观察到了改善。我们测试了3种不同的情况：小的固定正则化、小的固定正则化辅以修剪和组合自适应正则化/修剪。结果表明，仅修剪并没有改善性能；然而，辅以自适应正则化则减少了测试误差。
- en: We furthermore tried a flexible regularization scheme, viz. individual weight
    decay where R(w, κ) = mi=1 κiw2i and κi ≥ 0 are imposed. In the present case it
    turned out that the flexible regularizer was not able to outperform the joint
    adaptive regularization/pruning scheme; possibly due to training and validation
    set sizes.
  id: totrans-1699
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步尝试了一种灵活的正则化方案，即个别权重衰减，其中R(w, κ) = mi=1 κiw2i且κi ≥ 0被施加。在当前情况下，灵活的正则化器未能超越联合自适应正则化/修剪方案；这可能与训练和验证集的大小有关。
- en: Fig. 5.3 demonstrates adaptive regularization and pruning in a typical case
    using 4 weight decays.
  id: totrans-1700
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3展示了在使用4个权重衰减的典型情况下的自适应正则化和修剪。
- en: 5.5 Conclusions
  id: totrans-1701
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 结论
- en: In this paper it was suggested to adapt regularization parameters by minimizing
    the cross-validation error or a simple hold-out validation error. We derived a
    simple gradient descent scheme for optimizing regularization parameters which
    has a small programming overhead and an acceptable computational overhead compared
    to standard training. Numerical examples with a toy linear model showed limitations
    and advantages of the adaptive regularization approach. Moreover, numerical experiments
    on classification and time series prediction problems successfully demonstrated
    the functionality of the algorithm. Adaptation of regularization parameters resulted
    in lower generalization error; however, it should
  id: totrans-1702
  prefs: []
  type: TYPE_NORMAL
  zh: 本文建议通过最小化交叉验证误差或简单的留出验证误差来调整正则化参数。我们推导了一种简单的梯度下降方案来优化正则化参数，与标准训练相比，它具有较小的编程开销和可接受的计算开销。使用玩具线性模型的数值示例展示了自适应正则化方法的局限性和优势。此外，在分类和时间序列预测问题上的数值实验成功演示了算法的功能。正则化参数的调整导致了更低的泛化误差；然而，它应该。
- en: '![130_image_0.png](130_image_0.png)'
  id: totrans-1703
  prefs: []
  type: TYPE_IMG
  zh: '![130_image_0.png](130_image_0.png)'
- en: Fig. 5.3. Time series prediction example. Panels (a) and (b) show a typical
    evolution of errors and unnormalized weight decay values α when running the adaptive
    regularization algorithm using 4 weight decays. The normalized validation error
    drops approx.
  id: totrans-1704
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3. 时间序列预测示例。面板(a)和(b)显示了在使用4个权重衰减的自适应正则化算法运行时，误差和未标准化的权重衰减值α的典型演变。标准化验证误差大约下降。
- en: a factor of 2 when adapting weight decays. It turns out that some regularization
    of the input-to-hidden and output bias weights are needed whereas the other weights
    essentially requires no regularization22. In panel (c) and (d) it is demonstrated
    that pruning reduces the test error slightly. The optimal network is chosen as
    the one with minimal validation error, as indicated by the vertical line.
  id: totrans-1705
  prefs: []
  type: TYPE_NORMAL
  zh: 当调整权重衰减时，出现了2倍的因素。事实证明，输入到隐藏层和输出偏置权重的一些正则化是必要的，而其他权重基本上不需要正则化22。在面板(c)和(d)中，证明了修剪稍微减少了测试误差。最佳网络被选择为具有最小验证误差的网络，如垂直线所示。
- en: be emphasized that the actual yield is very dependent on the problem and the
    choice of the regularizers functional form. Acknowledgments. This research was
    supported by the Danish Natural Science and Technical Research Councils through
    the Computational Neural Network Center. JL furthermore acknowledge the Radio
    Parts Foundation for financial support. Mads Hintz-Madsen and Morten With Pedersen
    are acknowledged for stimulating discussions.
  id: totrans-1706
  prefs: []
  type: TYPE_NORMAL
  zh: 应强调的是，实际产量在很大程度上依赖于问题和正则化器函数形式的选择。致谢。该研究得到了丹麦自然科学与技术研究委员会通过计算神经网络中心的支持。JL还感谢无线电零部件基金会的资金支持。Mads
    Hintz-Madsen和Morten With Pedersen因富有启发性的讨论而被感谢。
- en: 22 Recall that if a weight decay κ is below λmax/108 it does not influence the
    MoorePenrose pseudo inversion of the Hessian.
  id: totrans-1707
  prefs: []
  type: TYPE_NORMAL
  zh: 22 记住，如果权重衰减κ低于λmax/108，它不会影响海森矩阵的Moore-Penrose伪逆。
- en: Appendix
  id: totrans-1708
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Assume that the regularization term is linear in the regularization parameters,
    i.e.,
  id: totrans-1709
  prefs: []
  type: TYPE_NORMAL
  zh: 假设正则化项在正则化参数中是线性的，即，
- en: $$R(\mathbf{w},\mathbf{\kappa})=\mathbf{\kappa}^{\top}\mathbf{r}(\mathbf{w})=\sum_{i=1}^{q}\kappa_{i}r_{i}(\mathbf{w})$$
  id: totrans-1710
  prefs: []
  type: TYPE_NORMAL
  zh: $$R(\mathbf{w},\mathbf{\kappa})=\mathbf{\kappa}^{\top}\mathbf{r}(\mathbf{w})=\sum_{i=1}^{q}\kappa_{i}r_{i}(\mathbf{w})$$
- en: The gradient of the cross-validation error (5.4) is
  id: totrans-1711
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证误差(5.4)的梯度为
- en: $$\frac{\partial\widehat{\Gamma}}{\partial\mathbf{\kappa}}(\mathbf{\kappa})=\frac{1}{K}\sum_{j=1}^{K}\frac{\partial
    S_{\mathcal{V}_{j}}}{\partial\mathbf{\kappa}}(\widehat{\mathbf{w}}_{j}(\mathbf{\kappa}))$$
  id: totrans-1712
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{\partial\widehat{\Gamma}}{\partial\mathbf{\kappa}}(\mathbf{\kappa})=\frac{1}{K}\sum_{j=1}^{K}\frac{\partial
    S_{\mathcal{V}_{j}}}{\partial\mathbf{\kappa}}(\widehat{\mathbf{w}}_{j}(\mathbf{\kappa}))$$
- en: $$(5.25)$$
  id: totrans-1713
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.25)$$
- en: $$(5.26)$$
  id: totrans-1714
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.26)$$
- en: $$(5.27)$$
  id: totrans-1715
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.27)$$
- en: Using the chain rule the gradient vector of the validation error, SVj , can
    be
  id: totrans-1716
  prefs: []
  type: TYPE_NORMAL
  zh: 使用链式法则，验证误差SVj的梯度向量可以
- en: written as∂SVj
  id: totrans-1717
  prefs: []
  type: TYPE_NORMAL
  zh: 表示为∂SVj
- en: $$\frac{\partial S_{\mathcal{V}_{j}}}{\partial\mathbf{\kappa}}(\widehat{\mathbf{w}}_{j}(\mathbf{\kappa}))=\frac{\partial\mathbf{w}^{\mathsf{T}}}{\partial\mathbf{\kappa}}(\widehat{\mathbf{w}}_{j}(\mathbf{\kappa}))\cdot\frac{\partial
    S_{\mathcal{V}_{j}}}{\partial\mathbf{w}}(\widehat{\mathbf{w}}_{j}(\mathbf{\kappa}))$$
  id: totrans-1718
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{\partial S_{\mathcal{V}_{j}}}{\partial\mathbf{\kappa}}(\widehat{\mathbf{w}}_{j}(\mathbf{\kappa}))=\frac{\partial\mathbf{w}^{\mathsf{T}}}{\partial\mathbf{\kappa}}(\widehat{\mathbf{w}}_{j}(\mathbf{\kappa}))\cdot\frac{\partial
    S_{\mathcal{V}_{j}}}{\partial\mathbf{w}}(\widehat{\mathbf{w}}_{j}(\mathbf{\kappa}))$$
- en: ∂w (w% j (κ)) (5.27)
  id: totrans-1719
  prefs: []
  type: TYPE_NORMAL
  zh: ∂w (w% j (κ)) (5.27)
- en: where ∂w/∂κ is the q × m derivative matrix of the estimated weights w.r.t.
  id: totrans-1720
  prefs: []
  type: TYPE_NORMAL
  zh: 其中∂w/∂κ是估计权重的q × m导数矩阵。
- en: the regularization parameters and m = dim(w). In order to find this derivative
  id: totrans-1721
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化参数和m = dim(w)。为了找到这个导数
- en: matrix, consider the gradient of the cost function w.r.t. to the weights as
    a
  id: totrans-1722
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵，考虑成本函数相对于权重的梯度作为
- en: function of κ and use the following expansion around the current estimate κ(n),
  id: totrans-1723
  prefs: []
  type: TYPE_NORMAL
  zh: κ的函数，并使用以下围绕当前估计κ(n)的展开，
- en: $$\frac{\partial C\tau_{\!_{I}}}{\partial\mathbf{w}}(\mathbf{\kappa})=\frac{\partial
    C\tau_{\!_{I}}}{\partial\mathbf{w}}(\mathbf{\kappa}_{(n)})+\frac{\partial^{2}C\tau_{\!_{I}}}{\partial\mathbf{w}\partial\mathbf{\kappa}^{\top}}(\mathbf{\kappa}_{(n)})\cdot(\mathbf{\kappa}-\mathbf{\kappa}_{(m)})+o(|\mathbf{\kappa}-\mathbf{\kappa}_{(n)}|).\tag{5.28}$$  Requiring
    $\widehat{\mathbf{w}}(\mathbf{\kappa}_{(n+1)})$ in the next iteration to be an
    optimal weight vector, i.e.,
  id: totrans-1724
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{\partial C\tau_{\!_{I}}}{\partial\mathbf{w}}(\mathbf{\kappa})=\frac{\partial
    C\tau_{\!_{I}}}{\partial\mathbf{w}}(\mathbf{\kappa}_{(n)})+\frac{\partial^{2}C\tau_{\!_{I}}}{\partial\mathbf{w}\partial\mathbf{\kappa}^{\top}}(\mathbf{\kappa}_{(n)})\cdot(\mathbf{\kappa}-\mathbf{\kappa}_{(m)})+o(|\mathbf{\kappa}-\mathbf{\kappa}_{(n)}|).\tag{5.28}$$
    要求下一次迭代中的$\widehat{\mathbf{w}}(\mathbf{\kappa}_{(n+1)})$为最优权重向量，即，
- en: Requiring w% (κ(n+1)) in the next iteration to be an optimal weight vector,
    i.e.,
  id: totrans-1725
  prefs: []
  type: TYPE_NORMAL
  zh: 要求下一次迭代中的w% (κ(n+1))为最优权重向量，即，
- en: ∂CTj /∂w(κ(n+1)) = 0 implies
  id: totrans-1726
  prefs: []
  type: TYPE_NORMAL
  zh: ∂CTj /∂w(κ(n+1)) = 0意味着
- en: ∂w∂κ (w% (κ(n))) = 0. (5.29)
  id: totrans-1727
  prefs: []
  type: TYPE_NORMAL
  zh: ∂w∂κ (w% (κ(n))) = 0。(5.29)
- en: $$\frac{\partial^{2}C_{\mathcal{T}_{j}}}{\partial w\partial\boldsymbol{\kappa}^{\top}}(\hat{\boldsymbol{w}}(\boldsymbol{\kappa}_{(n)}))=\mathbf{0}.\tag{1}$$
  id: totrans-1728
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{\partial^{2}C_{\mathcal{T}_{j}}}{\partial w\partial\boldsymbol{\kappa}^{\top}}(\hat{\boldsymbol{w}}(\boldsymbol{\kappa}_{(n)}))=\mathbf{0}.\tag{1}$$
- en: Recall that ∂CTj /∂w(κ(n)) = 0 by assumption. (5.29) can be used for determining
    ∂w/∂κ. Recognizing that the cost function CTj (w% (κ)) = STj (w% (κ)) +
  id: totrans-1729
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，由于假设，∂CTj /∂w(κ(n)) = 0。(5.29)可用于确定∂w/∂κ。识别成本函数CTj (w% (κ)) = STj (w% (κ))
    +
- en: R(w% (κ), κ) depends *implicitly* (thorough w% (κ)) and *explicitly* on κ it
    is possible,
  id: totrans-1730
  prefs: []
  type: TYPE_NORMAL
  zh: R(w% (κ), κ *隐含* (通过w% (κ)) 和 *显式* 依赖于κ。
- en: 'by using (5.25), to derive the following relation23:'
  id: totrans-1731
  prefs: []
  type: TYPE_NORMAL
  zh: 使用(5.25)导出以下关系23：
- en: $$(5.29)$$
  id: totrans-1732
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.29)$$
- en: $${\frac{\partial\mathbf{w}^{\top}}{\partial\mathbf{\kappa}}}({\hat{\mathbf{w}}}_{j})=-{\frac{\partial\mathbf{r}}{\partial\mathbf{w}^{\top}}}({\hat{\mathbf{w}}}_{j})\cdot\mathbf{J}_{j}^{-1}({\hat{\mathbf{w}}}_{j})$$
  id: totrans-1733
  prefs: []
  type: TYPE_NORMAL
  zh: $${\frac{\partial\mathbf{w}^{\top}}{\partial\mathbf{\kappa}}}({\hat{\mathbf{w}}}_{j})=-{\frac{\partial\mathbf{r}}{\partial\mathbf{w}^{\top}}}({\hat{\mathbf{w}}}_{j})\cdot\mathbf{J}_{j}^{-1}({\hat{\mathbf{w}}}_{j})$$
- en: $$(5.30)$$
  id: totrans-1734
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.30)$$
- en: $$(5.31)$$
  id: totrans-1735
  prefs: []
  type: TYPE_NORMAL
  zh: $$(5.31)$$
- en: where Jj = ∂2CTj /∂w∂w is the Hessian of the cost function which e.g., might
    be evaluated using the Gauss-Newton approximation [29]. Finally, substituting
    (5.30) into (5.27) gives
  id: totrans-1736
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Jj = ∂2CTj /∂w∂w 是成本函数的海森矩阵，可能使用高斯-牛顿近似进行评估 [29]。最后，将 (5.30) 代入 (5.27) 得到
- en: $$\frac{\partial S_{\mathcal{V}_{j}}}{\partial\mathbf{\kappa}}(\widehat{\mathbf{w}}_{j})=-\frac{\partial\mathbf{r}}{\partial\mathbf{w}^{\intercal}}(\widehat{\mathbf{w}}_{j})\cdot\mathbf{J}_{j}^{-1}(\widehat{\mathbf{w}}_{j})\cdot\frac{\partial
    S_{\mathcal{V}_{j}}}{\partial\mathbf{w}}(\widehat{\mathbf{w}}_{j})$$
  id: totrans-1737
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{\partial S_{\mathcal{V}_{j}}}{\partial\mathbf{\kappa}}(\widehat{\mathbf{w}}_{j})=-\frac{\partial\mathbf{r}}{\partial\mathbf{w}^{\intercal}}(\widehat{\mathbf{w}}_{j})\cdot\mathbf{J}_{j}^{-1}(\widehat{\mathbf{w}}_{j})\cdot\frac{\partial
    S_{\mathcal{V}_{j}}}{\partial\mathbf{w}}(\widehat{\mathbf{w}}_{j})$$
- en: ∂w (w% j ) (5.31)
  id: totrans-1738
  prefs: []
  type: TYPE_NORMAL
  zh: ∂w (w% j ) (5.31)
- en: ∂SVj /∂w is found by ordinary back- propagation on the validation set while
  id: totrans-1739
  prefs: []
  type: TYPE_NORMAL
  zh: ∂SVj /∂w 通过普通反向传播在验证集上找到，而
- en: ∂r/∂w is calculated from the specific assumptions on the regularizer.
  id: totrans-1740
  prefs: []
  type: TYPE_NORMAL
  zh: ∂r/∂w 根据对正则化器的具体假设进行计算。
- en: 23 For convenience, here w's explicit κ-dependence is omitted.
  id: totrans-1741
  prefs: []
  type: TYPE_NORMAL
  zh: 23 为了方便，这里省略了 w 的显式 κ 依赖。
- en: '[1] Akaike, H.: Fitting Autoregressive Models for Prediction. Annals of the
    Institute of Statistical Mathematics 21, 243–247 (1969)'
  id: totrans-1742
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Akaike, H.: 自回归模型的拟合与预测。统计数学研究所年报 21, 243–247 (1969)'
- en: '[2] Amari, S., Murata, N., Müller, K.R., Finke, M., Yang, H.: Asymptotic Statistical
    Theory of Overtraining and Cross-Validation. Technical report METR 95-06 and IEEE
    Transactions on Neural Networks 8(5), 985–996 (1995)'
  id: totrans-1743
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Amari, S., Murata, N., Müller, K.R., Finke, M., Yang, H.: 过拟合与交叉验证的渐近统计理论。技术报告METR
    95-06及《IEEE神经网络汇刊》8(5)，985–996 (1995)'
- en: '[3] Nonboe Andersen, L., Larsen, J., Hansen, L.K., Hintz-madsen, M.: Adaptive
    Regularization of Neural Classifiers. In: Principe, J., et al. (eds.) Proceedings
    of the IEEE Workshop on Neural Networks for Signal Processing VII, pp. 24–33.
    IEEE, Piscataway (1997)'
  id: totrans-1744
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Nonboe Andersen, L., Larsen, J., Hansen, L.K., Hintz-madsen, M.: 神经分类器的自适应正则化。见：Principe,
    J. 等（编辑）《IEEE信号处理神经网络研讨会会议录 VII》，第24–33页。IEEE，皮斯卡塔维 (1997)'
- en: '[4] Bishop, C.M.: Curvature-Driven Smoothing: A Learning Algorithm for Feedforward
    Neural Networks. IEEE Transactions on Neural Networks 4(4), 882–884'
  id: totrans-1745
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Bishop, C.M.: 曲率驱动平滑：前馈神经网络的学习算法。《IEEE神经网络汇刊》4(4)，882–884'
- en: (1993)
  id: totrans-1746
  prefs: []
  type: TYPE_NORMAL
  zh: (1993)
- en: '[5] Bishop, C.M.: Neural Networks for Pattern Recognition. Oxford University
    Press, Oxford (1995)'
  id: totrans-1747
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Bishop, C.M.: 用于模式识别的神经网络。牛津大学出版社，牛津 (1995)'
- en: '[6] Dennis, J.E., Schnabel, R.B.: Numerical Methods for Unconstrained Optimization
    and Non-linear Equations. Prentice- Hall, Englewood Cliffs (1983)'
  id: totrans-1748
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Dennis, J.E., Schnabel, R.B.: 无约束优化与非线性方程的数值方法。普伦蒂斯-霍尔，恩格尔伍德悬崖 (1983)'
- en: '[7] Drucker, H., Le Cun, Y.: Improving Generalization Performance in Character
    Recognition. In: Juang, B.H., et al. (eds.) Neural Networks for Signal Processing:
    Proceedings of the 1991 IEEE-SP Workshop, pp. 198–207. IEEE, Piscataway'
  id: totrans-1749
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Drucker, H., Le Cun, Y.: 提高字符识别中的泛化性能。见：Juang, B.H. 等（编辑）《信号处理的神经网络：1991年IEEE-SP研讨会会议录》，第198–207页。IEEE，皮斯卡塔维'
- en: (1991)
  id: totrans-1750
  prefs: []
  type: TYPE_NORMAL
  zh: (1991)
- en: '[8] Geisser, S.: The Predictive Sample Reuse Method with Applications. Journal
    of the American Statistical Association 50, 320–328 (1975)'
  id: totrans-1751
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Geisser, S.: 可预测样本重用方法及其应用。《美国统计协会杂志》50, 320–328 (1975)'
- en: '[9] Geman, S., Bienenstock, E., Doursat, R.: Neural Networks and the Bias/Variance
    Dilemma. Neural Computation 4, 1–58 (1992)'
  id: totrans-1752
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Geman, S., Bienenstock, E., Doursat, R.: 神经网络与偏差/方差困境。神经计算 4, 1–58 (1992)'
- en: '[10] Girosi, F., Jones, M., Poggio, T.: Regularization Theory and Neural Networks
    Architectures. Neural Computation 7(2), 219–269 (1995)'
  id: totrans-1753
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Girosi, F., Jones, M., Poggio, T.: 正则化理论与神经网络架构。神经计算 7(2), 219–269 (1995)'
- en: '[11] Goutte, C., Larsen, J.: Adaptive Regularization of Neural Networks using
    Conjugate Gradient. In: Proceedings of ICASSP 1998, Seattle, USA, vol. 2, pp.
    1201–'
  id: totrans-1754
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Goutte, C., Larsen, J.: 使用共轭梯度的神经网络自适应正则化。见：ICASSP 1998 会议录，西雅图，美国，第2卷，第1201–'
- en: 1204 (1998)
  id: totrans-1755
  prefs: []
  type: TYPE_NORMAL
  zh: 1204 (1998)
- en: '[12] Goutte, C.: Note on Free Lunches and Cross-Validation. Neural Computation
    9(6),'
  id: totrans-1756
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Goutte, C.: 关于免费午餐与交叉验证的说明。神经计算 9(6),'
- en: 1211–1215 (1997)
  id: totrans-1757
  prefs: []
  type: TYPE_NORMAL
  zh: 1211–1215 (1997)
- en: '[13] Goutte, C.: Regularization with a Pruning Prior. Neural Networks (1997)
    (to appear)'
  id: totrans-1758
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Goutte, C.: 带有修剪先验的正则化。神经网络 (1997)（待发表）'
- en: '[14] Hansen, L.K., Rasmussen, C.E.: Pruning from Adaptive Regularization. Neural
    Computation 6, 1223–1232 (1994)'
  id: totrans-1759
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Hansen, L.K., Rasmussen, C.E.: 从自适应正则化中剪枝。神经计算 6, 1223–1232 (1994)'
- en: '[15] Hansen, L.K., Rasmussen, C.E., Svarer, C., Larsen, J.: Adaptive Regularization.'
  id: totrans-1760
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Hansen, L.K., Rasmussen, C.E., Svarer, C., Larsen, J.: 自适应正则化。'
- en: 'In: Vlontzos, J., Hwang, J.-N., Wilson, E. (eds.) Proceedings of the IEEE Workshop
    on Neural Networks for Signal Processing IV, pp. 78–87. IEEE, Piscataway'
  id: totrans-1761
  prefs: []
  type: TYPE_NORMAL
  zh: 在：Vlontzos, J., Hwang, J.-N., Wilson, E.（编）《IEEE信号处理神经网络研讨会IV的会议论文集》，第78–87页。IEEE,
    Piscataway
- en: (1994)
  id: totrans-1762
  prefs: []
  type: TYPE_NORMAL
  zh: （1994）
- en: '[16] Hansen, L.K., Larsen, J.: Linear Unlearning for Cross-Validation. Advances
    in Computational Mathematics 5, 269–280 (1996)'
  id: totrans-1763
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Hansen, L.K., Larsen, J.: 跨验证的线性非学习。计算数学进展 5, 269–280 (1996)'
- en: '[17] Hertz, J., Krogh, A., Palmer, R.G.: Introduction to the Theory of Neural
    Computation. Addison-Wesley Publishing Company, Redwood City (1991) [18] Hintz-Madsen,
    M., With Pedersen, M., Hansen, L.K., Larsen, J.: Design and Evaluation of Neural
    Classifiers. In: Usui, S., Tohkura, Y., Katagiri, S., Wilson, E.'
  id: totrans-1764
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Hertz, J., Krogh, A., Palmer, R.G.: 神经计算理论导论。 Addison-Wesley出版社，Redwood
    City (1991) [18] Hintz-Madsen, M., 与Pedersen, M., Hansen, L.K., Larsen, J.: 神经分类器的设计与评估。在：Usui,
    S., Tohkura, Y., Katagiri, S., Wilson, E.'
- en: (eds.) Proceedings of the IEEE Workshop on Neural Networks for Signal Processing
    VI, pp. 223–232. IEEE, Piscataway (1996)
  id: totrans-1765
  prefs: []
  type: TYPE_NORMAL
  zh: （编）《IEEE信号处理神经网络研讨会VI的会议论文集》，第223–232页。IEEE, Piscataway (1996)
- en: '[19] Hornik, K.: Approximation Capabilities of Multilayer Feedforward Networks.
    Neural Networks 4, 251–257 (1991)'
  id: totrans-1766
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Hornik, K.: 多层前馈网络的逼近能力。神经网络 4, 251–257 (1991)'
- en: '[20] Kearns, M.: A Bound on the Error of Cross Validation Using the Approximation
    and Estimation Rates, with Consequences for the Training-Test Split. Neural Computation
    9(5), 1143–1161 (1997)'
  id: totrans-1767
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Kearns, M.: 使用近似和估计速率的交叉验证误差界限，以及对训练-测试分割的影响。神经计算 9(5), 1143–1161 (1997)'
- en: '[21] Larsen, J.: A Generalization Error Estimate for Nonlinear Systems. In:
    Kung, S.Y.,'
  id: totrans-1768
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Larsen, J.: 非线性系统的泛化误差估计。在：Kung, S.Y.,'
- en: et al. (eds.) Proceedings of the 1992 IEEE-SP Workshop on Neural Networks for
    Signal Processing, vol. 2, pp. 29–38. IEEE, Piscataway (1992)
  id: totrans-1769
  prefs: []
  type: TYPE_NORMAL
  zh: 等（编）《1992年IEEE-SP信号处理神经网络研讨会的会议论文集》，第2卷，第29–38页。IEEE, Piscataway (1992)
- en: '[22] Larsen, J.: Design of Neural Network Filters, Ph.D. Thesis, Electronics
    Institute, Technical University of Denmark (1993),'
  id: totrans-1770
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Larsen, J.: 神经网络滤波器的设计，博士论文，丹麦技术大学电子学院 (1993)'
- en: ftp://eivind.imm.dtu.dk/dist/PhD_thesis/jlarsen.thesis.ps.Z
  id: totrans-1771
  prefs: []
  type: TYPE_NORMAL
  zh: ftp://eivind.imm.dtu.dk/dist/PhD_thesis/jlarsen.thesis.ps.Z
- en: '[23] Larsen, J., Hansen, L.K.: Generalization Performance of Regularized Neural
    Network Models. In: Vlontzos, J., et al. (eds.) Proceedings of the IEEE Workshop
    on Neural Networks for Signal Processing IV, pp. 42–51. IEEE, Piscataway (1994)'
  id: totrans-1772
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Larsen, J., Hansen, L.K.: 正则化神经网络模型的泛化性能。在：Vlontzos, J. 等（编）《IEEE信号处理神经网络研讨会IV的会议论文集》，第42–51页。IEEE,
    Piscataway (1994)'
- en: '[24] Larsen, J., Hansen, L.K.: Empirical Generalization Assessment of Neural
    Network Models. In: Girosi, F., et al. (eds.) Proceedings of the IEEE Workshop
    on Neural Networks for Signal Processing V, pp. 30–39. IEEE, Piscataway (1995)'
  id: totrans-1773
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Larsen, J., Hansen, L.K.: 神经网络模型的实证泛化评估。在：Girosi, F. 等（编）《IEEE信号处理神经网络研讨会V的会议论文集》，第30–39页。IEEE,
    Piscataway (1995)'
- en: '[25] Larsen, J., Hansen, L.K., Svarer, C., Ohlsson, M.: Design and Regularization
    of Neural Networks: The Optimal Use of a Validation Set. In: Usui, S., Tohkura,
    Y., Katagiri, S., Wilson, E. (eds.) Proceedings of the IEEE Workshop on Neural
    Networks for Signal Processing VI, pp. 62–71. IEEE, Piscataway (1996)'
  id: totrans-1774
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Larsen, J., Hansen, L.K., Svarer, C., Ohlsson, M.: 神经网络的设计与正则化：验证集的最佳使用。在：Usui,
    S., Tohkura, Y., Katagiri, S., Wilson, E.（编）《IEEE信号处理神经网络研讨会VI的会议论文集》，第62–71页。IEEE,
    Piscataway (1996)'
- en: '[26] Larsen, J., et al.: Optimal Data Set Split Ratio for Empirical Generalization
    Error Estimates (in preparation)'
  id: totrans-1775
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Larsen, J., 等：实证泛化误差估计的最佳数据集分割比（准备中）'
- en: '[27] Le Cun, Y., Denker, J.S., Solla, S.A.: Optimal Brain Damage. In: Touretzky,
    D.S. (ed.) Proceedings of the 1989 Conference on Advances in Neural Information
    Processing Systemsshers, vol. 2, pp. 598–605. Morgan Kaufmann Publishers, San
    Mateo (1990)'
  id: totrans-1776
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Le Cun, Y., Denker, J.S., Solla, S.A.: 最优脑损伤。在：Touretzky, D.S.（编）《1989年神经信息处理系统进展会议的论文集》，第2卷，第598–605页。Morgan
    Kaufmann出版社，San Mateo (1990)'
- en: '[28] Lowe, D.: Adaptive Radial Basis Function Nonlinearities and the Problem
    of Generalisation. In: Proc. IEE Conf. on Artificial Neural Networks, pp. 171–175
    (1989)'
  id: totrans-1777
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] Lowe, D.: 自适应径向基函数非线性及泛化问题。在：IEE会议论文集，人工神经网络，第171–175页 (1989)'
- en: '[29] Ljung, L.: System Identification: Theory for the User. Prentice-Hall,
    Englewood Cliffs (1987)'
  id: totrans-1778
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] Ljung, L.: 系统识别：用户的理论. 普伦蒂斯-霍尔, 英格尔伍德悬崖 (1987)'
- en: '[30] MacKay, D.J.C.: A Practical Bayesian Framework for Backprop Networks.
    Neural Computation 4(3), 448–472 (1992)'
  id: totrans-1779
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] MacKay, D.J.C.: 实用贝叶斯框架用于反向传播网络. 神经计算 4(3), 448–472 (1992)'
- en: '[31] Moody, J.: Prediction Risk and Architecture Selection for Neural Networks.
    In:'
  id: totrans-1780
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Moody, J.: 神经网络的预测风险与架构选择. 在：'
- en: 'Cherkassky, V., et al. (eds.) From Statistics to Neural Networks: Theory and
    Pattern Recognition Applications, vol. 136. Springer-Verlag Series F, Berlin (1994)'
  id: totrans-1781
  prefs: []
  type: TYPE_NORMAL
  zh: Cherkassky, V., 等编 (eds.) 从统计到神经网络：理论与模式识别应用, 第136卷. 斯普林格-费尔拉格系列 F, 柏林 (1994)
- en: '[32] Moody, J., Rögnvaldsson, T.: Smoothing Regularizers for Projective Basis
    Function Networks. In: Proceedings of the 1996 Conference on Advances in Neural
    Information Processing Systems, vol. 9. MIT Press, Cambridge (1997)'
  id: totrans-1782
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] Moody, J., Rögnvaldsson, T.: 投影基函数网络的平滑正则化器. 在：1996年神经信息处理系统进展会议论文集, 第9卷.
    MIT出版社, 剑桥 (1997)'
- en: '[33] Murata, N., Yoshizawa, S., Amari, S.: Network Information Criterion -
    Determining the Number of Hidden Units for an Artificial Neural Network Model.
    IEEE'
  id: totrans-1783
  prefs: []
  type: TYPE_NORMAL
  zh: '[33] Murata, N., Yoshizawa, S., Amari, S.: 网络信息标准 - 确定人工神经网络模型的隐藏单元数量. IEEE'
- en: Transactions on Neural Networks 5(6), 865–872 (1994)
  id: totrans-1784
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络学报 5(6), 865–872 (1994)
- en: '[34] Nowlan, S., Hinton, G.: Simplifying Neural Networks by Soft Weight Sharing.'
  id: totrans-1785
  prefs: []
  type: TYPE_NORMAL
  zh: '[34] Nowlan, S., Hinton, G.: 通过软权重共享简化神经网络.'
- en: Neural Computation 4(4), 473–493 (1992)
  id: totrans-1786
  prefs: []
  type: TYPE_NORMAL
  zh: 神经计算 4(4), 473–493 (1992)
- en: '[35] With Pedersen, M.: Training Recurrent Networks. In: Proceedings of the
    IEEE'
  id: totrans-1787
  prefs: []
  type: TYPE_NORMAL
  zh: '[35] 与 Pedersen, M.: 训练递归网络. 在：IEEE会议论文集'
- en: Workshop on Neural Networks for Signal Processing VII. IEEE, Piscataway (1997)
  id: totrans-1788
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络信号处理研讨会 VII. IEEE, 皮斯卡特维 (1997)
- en: '[36] Peterson, G.E., Barney, H.L.: Control Methods Used in a Study of the Vowels.'
  id: totrans-1789
  prefs: []
  type: TYPE_NORMAL
  zh: '[36] Peterson, G.E., Barney, H.L.: 在元音研究中使用的控制方法.'
- en: JASA 24, 175–184 (1952)
  id: totrans-1790
  prefs: []
  type: TYPE_NORMAL
  zh: JASA 24, 175–184 (1952)
- en: '[37] Shadafan, R.S., Niranjan, M.: A Dynamic Neural Network Architecture by
    Sequential Partitioning of the Input Space. Neural Computation 6(6), 1202–1222'
  id: totrans-1791
  prefs: []
  type: TYPE_NORMAL
  zh: '[37] Shadafan, R.S., Niranjan, M.: 通过对输入空间的顺序划分构建动态神经网络架构. 神经计算 6(6), 1202–1222'
- en: (1994)
  id: totrans-1792
  prefs: []
  type: TYPE_NORMAL
  zh: (1994)
- en: '[38] Sjöberg, J.: Non-Linear System Identification with Neural Networks, Ph.D.
    Thesis no. 381, Department of Electrical Engineering, Linköping University, Sweden'
  id: totrans-1793
  prefs: []
  type: TYPE_NORMAL
  zh: '[38] Sjöberg, J.: 用神经网络进行非线性系统识别, 博士学位论文编号381, 瑞典林雪平大学电气工程系'
- en: (1995)
  id: totrans-1794
  prefs: []
  type: TYPE_NORMAL
  zh: (1995)
- en: '[39] Stone, M.: Cross-validatory Choice and Assessment of Statistical Predictors.
    Journal of the Royal Statistical Society B 36(2), 111–147 (1974)'
  id: totrans-1795
  prefs: []
  type: TYPE_NORMAL
  zh: '[39] Stone, M.: 交叉验证的选择与统计预测因子的评估. 皇家统计学会B期刊 36(2), 111–147 (1974)'
- en: '[40] Svarer, C., Hansen, L.K., Larsen, J., Rasmussen, C.E.: Designer Networks
    for Time Series Processing. In: Kamm, C.A., et al. (eds.) Proceedings of the IEEE
    Workshop on Neural Networks for Signal Processing, vol. 3, pp. 78–87. IEEE, Piscataway
    (1993)'
  id: totrans-1796
  prefs: []
  type: TYPE_NORMAL
  zh: '[40] Svarer, C., Hansen, L.K., Larsen, J., Rasmussen, C.E.: 时间序列处理的设计网络. 在：Kamm,
    C.A., 等编 (eds.) IEEE神经网络信号处理研讨会论文集, 第3卷, 第78–87页. IEEE, 皮斯卡特维 (1993)'
- en: '[41] Watrous, R.L.: Current Status of PetersonBarney Vowel Formant Data. JASA
    89, 2459–2460 (1991)'
  id: totrans-1797
  prefs: []
  type: TYPE_NORMAL
  zh: '[41] Watrous, R.L.: PetersonBarney 元音共振峰数据的当前状态. JASA 89, 2459–2460 (1991)'
- en: '[42] Weigend, A.S., Huberman, B.A., Rumelhart, D.E.: Predicting the Future:
    A Connectionist Approach. International Journal of Neural Systems 1(3), 193–209
    (1990) [43] Williams, P.M.: Bayesian Regularization and Pruning using a Laplace
    Prior. Neural Computation 7(1), 117–143 (1995)'
  id: totrans-1798
  prefs: []
  type: TYPE_NORMAL
  zh: '[42] Weigend, A.S., Huberman, B.A., Rumelhart, D.E.: 预测未来：一种连接主义方法. 国际神经系统期刊
    1(3), 193–209 (1990) [43] Williams, P.M.: 使用拉普拉斯先验的贝叶斯正则化与剪枝. 神经计算 7(1), 117–143
    (1995)'
- en: '[44] Wolpert, D.H., Macready, W.G.: The Mathematics of Search. Technical Report
    SFI-TR-95-02-010, Santa Fe Instute (1995)'
  id: totrans-1799
  prefs: []
  type: TYPE_NORMAL
  zh: '[44] Wolpert, D.H., Macready, W.G.: 搜索的数学. 技术报告 SFI-TR-95-02-010, 圣塔菲研究所 (1995)'
- en: '[45] Wu, L., Moody, J.: A Smoothing Regularizer for Feedforward and Recurrent
    Neural Networks. Neural Computation 8(3) (1996)'
  id: totrans-1800
  prefs: []
  type: TYPE_NORMAL
  zh: '[45] Wu, L., Moody, J.: 用于前馈与递归神经网络的平滑正则化器. 神经计算 8(3) (1996)'
- en: '[46] Zhu, H., Rohwer, R.: No Free Lunch for Cross Validation. Neural Computation
    8(7), 1421–1426 (1996)'
  id: totrans-1801
  prefs: []
  type: TYPE_NORMAL
  zh: '[46] Zhu, H., Rohwer, R.: 交叉验证无免费午餐. 神经计算 8(7), 1421–1426 (1996)'
- en: 6 Large Ensemble Averaging-
  id: totrans-1802
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 大型集成平均-
- en: David Horn1, Ury Naftaly1, and Nathan Intrator2 1 School of Physics and Astronomy
    2 School of Mathematical Sciences Raymond and Beverly Sackler Faculty of Exact
    Sciences Tel Aviv University, Tel Aviv 69978, Israel horn@neuron.tau.ac.il http://neuron.tau.ac.il/~horn/
  id: totrans-1803
  prefs: []
  type: TYPE_NORMAL
  zh: David Horn1，Ury Naftaly1 和 Nathan Intrator2 1 物理与天文学系 2 数学科学系 雷蒙德与贝弗利·萨克勒精确科学学院
    特拉维夫大学，以色列特拉维夫 69978 horn@neuron.tau.ac.il http://neuron.tau.ac.il/~horn/
- en: Abstract. Averaging over many predictors leads to a reduction of the variance
    portion of the error. We present a method for evaluating the mean squared error
    of an infinite ensemble of predictors from finite (small size) ensemble information.
    We demonstrate it on ensembles of networks with different initial choices of synaptic
    weights. We find that the optimal stopping criterion for large ensembles occurs
    later in training time than for single networks. We test our method on the suspots
    data set and obtain excellent results.
  id: totrans-1804
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要。对多个预测器进行平均可减少误差的方差部分。我们提出了一种方法，从有限（小规模）集合信息中评估无限集合预测器的均方误差。我们在不同初始选择的突触权重的网络集合上演示了该方法。我们发现，对于大集合，最佳停止标准出现在训练时间晚于单个网络的情况。我们在太阳黑子数据集上测试了我们的方法，并获得了优秀的结果。
- en: 6.1 Introduction
  id: totrans-1805
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 引言
- en: Ensemble averaging has been proposed in the literature as a means to improve
    the generalization properties of a neural network predictor[3, 11, 7]. We follow
    this line of thought and consider averaging over a set of networks that differ
    from one another just by the initial values of their synaptic weights.
  id: totrans-1806
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中提出了集合平均作为改善神经网络预测器泛化特性的手段[3, 11, 7]。我们遵循这一思路，考虑对一组仅因其突触权重的初始值不同的网络进行平均。
- en: We introduce a method to extract the performance of large ensembles from that
    of finite size ones. This is explained in the next section, and is demonstrated
    on the sunspots data set. Ensemble averaging over the initial conditions of the
    neural networks leads to a lower prediction error, which is obtained for a later
    training time than that expected from single networks. Our method outperforms
    the best published results for the sunspots problem [6].
  id: totrans-1807
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了一种从有限规模集合的性能中提取大集合性能的方法。下一节将对此进行解释，并在太阳黑子数据集上进行演示。对神经网络初始条件的集合平均导致较低的预测误差，且在训练时间上晚于单个网络的预期。我们的方法在太阳黑子问题上超越了已发布的最佳结果[6]。
- en: The theoretical setting of the method is provided by the bias/variance decomposition.
    Within this framework, we define a particular bias/variance decomposition for
    networks differing by their initial conditions only. While the bias of the ensemble
    of networks with different initial conditions remains unchanged, the variance
    error decreases considerably.
  id: totrans-1808
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的理论背景由偏差/方差分解提供。在这个框架内，我们定义了一个特定的偏差/方差分解，适用于仅因初始条件不同的网络。虽然不同初始条件的网络集的偏差保持不变，但方差误差显著减少。
- en: 6.2 Extrapolation To Large-Ensemble Averages
  id: totrans-1809
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 外推到大集合平均
- en: The training procedure of neural networks starts out with some choice of initial
    values of the connection weights. We consider ensembles of networks that differ
  id: totrans-1810
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的训练过程始于某些连接权重初始值的选择。我们考虑的是相互之间仅因初始值不同的网络集合。
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-1811
  prefs: []
  type: TYPE_NORMAL
  zh: '- 先前发表于：Orr, G.B. 和 Müller, K.-R.（编）：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-1812
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0（1998）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    131–137, 2012.'
  id: totrans-1813
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon 等（编）：NN: Tricks of the Trade，第2版，LNCS 7700，第131–137页，2012。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-1814
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: from one another just by their initial values and average over them. Since the
    space of initial conditions is very large we develop a technique which allows
    us to approximate averaging over the whole space.
  id: totrans-1815
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过其初始值彼此不同并对其进行平均。由于初始条件的空间非常大，我们开发了一种技术，可以近似地对整个空间进行平均。
- en: Our technique consists of constructing groups of a fixed number of networks,
    Q. All networks differ from one another by the random choice of their initial
    weights. For each group we define our predictor to be the average of the output
    of all Q networks. Choosing several different groups of the same size Q, and averaging
    over their predictions for the test set, defines the finite size average that
    is displayed in Fig. 1. Then we perform a parametric estimate of the limit Q →
    ∞. A simple regression in 1/Q suffices to obtain this limit in the suspots problem,
    as shown in Fig. 2. In general one may encounter a more complicated inverse power
    behavior, indicating correlations between networks with different initial weights.
  id: totrans-1816
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的技术包括构建固定数量的网络组Q。所有网络通过随机选择其初始权重而彼此不同。对于每个组，我们将预测器定义为所有Q个网络输出的平均值。选择多个相同大小Q的不同组，并对它们在测试集上的预测取平均值，定义了图1中显示的有限大小平均值。然后我们进行Q
    → ∞的参数估计。通过1/Q的简单回归即可获得在太阳黑子问题中的极限，如图2所示。通常可能会遇到更复杂的反幂行为，表明不同初始权重的网络之间存在相关性。
- en: '![136_image_0.png](136_image_0.png)'
  id: totrans-1817
  prefs: []
  type: TYPE_IMG
  zh: '![136_image_0.png](136_image_0.png)'
- en: 'Fig. 6.1. ARV of test set Prediction error (ARV) is plotted vs. training time
    in kilo epochs (KE). The curves correspond to different choices of group sizes:
    Q = 1, 2, 4, 5, 10, 20 from top to bottom. The lowest curve is the extrapolation
    to Q → ∞.'
  id: totrans-1818
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1。测试集的ARV预测误差（ARV）相对于训练时间（千次纪元KE）绘制的曲线。曲线对应于不同的组大小选择：从上到下，Q = 1, 2, 4, 5,
    10, 20。最低曲线是对Q → ∞的外推。
- en: 6.2.1 Application To The Sunspots Problem
  id: totrans-1819
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2.1 对太阳黑子问题的应用
- en: Yearly sunspot statistics have been gathered since 1700. These data have been
    extensively studied and have served as a benchmark in the statistical literature
    [9, 10, 4]. Following previous publications [10, 6, 8] we choose the training
    set to
  id: totrans-1820
  prefs: []
  type: TYPE_NORMAL
  zh: 自1700年以来，已收集了每年的太阳黑子统计数据。这些数据经过广泛研究，并作为统计文献中的基准[9, 10, 4]。遵循之前的出版物[10, 6, 8]，我们选择训练集来
- en: '![137_image_0.png](137_image_0.png)'
  id: totrans-1821
  prefs: []
  type: TYPE_IMG
  zh: '![137_image_0.png](137_image_0.png)'
- en: Fig. 6.2. Extrapolation method used for extracting the Q → ∞ prediction The
    results for different ensemble size Q at two different training periods, t = 70KE
    (dots) and 140KE (circles) lie on straight lines as a function of 1/Q. For each
    curve, the first three points from the right represent ensemble sizes of 1, 2,
    and 4 respectively. While the three points of 140KE all lie above the corresponding
    ones of 70KE, an extrapolation to larger ensemble sizes suggests that the overall
    performance will be better for 140KE as is observed from the fit to the line.
  id: totrans-1822
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2。用于提取Q → ∞预测的外推法。不同集合大小Q在两个不同训练期t = 70KE（点）和140KE（圈）下的结果作为1/Q的函数呈直线分布。每条曲线从右边开始的前三个点分别代表集合大小为1、2和4。虽然140KE的三个点都高于70KE的相应点，但对更大集合大小的外推表明，整体性能会更好，如线性拟合所示。
- en: 'contain the period between 1701 and 1920, and the test-set to contain the years
    1921 to 1955. Following [10], we calculate the prediction error according to the
    average relative variance (ARV) of the data set S:'
  id: totrans-1823
  prefs: []
  type: TYPE_NORMAL
  zh: 包含1701年至1920年间的时期，测试集包含1921年至1955年。遵循[10]，我们根据数据集S的平均相对方差（ARV）计算预测误差：
- en: $$\text{ARV}=\frac{\sum_{k\in S}\left(y_{k}-f(\mathbf{x}_{k})\right)^{2}}{\sum_{k\in
    S}\left(y_{k}-E\left[y_{k}\right]\right)^{2}}\tag{6.1}$$
  id: totrans-1824
  prefs: []
  type: TYPE_NORMAL
  zh: $$\text{ARV}=\frac{\sum_{k\in S}\left(y_{k}-f(\mathbf{x}_{k})\right)^{2}}{\sum_{k\in
    S}\left(y_{k}-E\left[y_{k}\right]\right)^{2}}\tag{6.1}$$
- en: yk(xk) are the data values and f(xk) is the predictor. In our time series problem,
    for any given time point t = k, the input vector xk has component values taken
    from the series at times t − 1, t − 2, ··· , t − 12 (as in [10]). The denominator
    in Eq. 1 is σ2 = 1535 for the training set. The same value is used for the test
    set.
  id: totrans-1825
  prefs: []
  type: TYPE_NORMAL
  zh: yk(xk)是数据值，f(xk)是预测器。在我们的时间序列问题中，对于任何给定的时间点t = k，输入向量xk的分量值取自t − 1、t − 2、···、t
    − 12（如[10]所示）。公式1中的分母σ2 = 1535用于训练集。相同的值也用于测试集。
- en: We use neural networks with 12 inputs, one sigmoidal hidden layer consisting
    of 4 units and a linear output. They are then enlarged to form recurrent networks
    (SRN) [1] in which the input layer is increased by adding to it the hidden layer
    of the previous point in the time series. The learning algorithm consists of back
    propagation applied to an error function which is the MSE of the training set.
    A
  id: totrans-1826
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用具有12个输入的神经网络，一个由4个单元组成的sigmoidal隐藏层，以及一个线性输出。然后将其扩大以形成递归网络（SRN）[1]，其中输入层通过添加时间序列中前一个点的隐藏层而增加。学习算法包括对训练集的均方误差（MSE）应用反向传播。
- en: validation set containing 35 randomly chosen points was left out during training
    to serve for performance validation.
  id: totrans-1827
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集包含35个随机选择的点，在训练期间被留出以进行性能验证。
- en: Fig. 6.1 displays our results on the test set as a function of the number of
    training epochs. We observe a descending order of Q = 1, 2, 4, 5, 10, 20 followed
    by the extrapolation to Q → ∞. All of these curves correspond to averages over
    groups of size Q, calculated by running 60 networks. To demonstrate how the extrapolation
    is carried out we display in Fig. 6.2 the points obtained for t = 70 and t = 140
    KE as a function of 1Q . It is quite clear that a linear extrapolation is very
    satisfactory. Moreover, the results for Q = 20 are not far from the extrapolated
    Q → ∞ results. Note that the minimum of the Q → ∞
  id: totrans-1828
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 显示了我们在测试集上的结果，作为训练轮次的函数。我们观察到 Q = 1、2、4、5、10、20 的降序，然后是对 Q → ∞ 的外推。这些曲线都对应于大小为
    Q 的组的平均值，计算方式是运行 60 个网络。为了展示外推是如何进行的，我们在图 6.2 中显示了 t = 70 和 t = 140 KE 对 1/Q 的点。很明显，线性外推非常令人满意。此外，Q
    = 20 的结果与外推的 Q → ∞ 结果相差不远。请注意，Q → ∞ 的最小值
- en: curve in Fig.1 occurs at a much higher training time than that of the Q = 1
    single network curve. This is also evident from the crossing of the t = 70 and
    t = 140 KE lines on Fig. 2. An important conclusion is that the stopping criterion
    for ensemble training (to be applied, of course, to every network in the group)
    is very different from that of single network training.
  id: totrans-1829
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1 中的曲线在训练时间上显著高于 Q = 1 单网络曲线。这在图 2 中 t = 70 和 t = 140 的 KE 线交叉中也很明显。一个重要的结论是，集成训练的停止标准（当然要应用于组中的每个网络）与单网络训练的标准非常不同。
- en: 6.2.2 Best Result
  id: totrans-1830
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2.2 最佳结果
- en: The curves shown in Fig. 1 were obtained with a learning rate of 0.003. Lower
    learning rates lead to lower errors. In that case the effect of ensemble averaging
    is not as dramatic. We obtained our best result by changing our input vector into
    the six dimensional choice of Pi & Peterson [8] that consists of xt−1, xt−2, xt−3,
    xt−4, xt−9 and xt−10. Using a learning rate of 0.0005 on the otherwise unchanged
    SRN described above, we obtain the minimum of the prediction error at 0.0674,
    which is better than any previously reported result.
  id: totrans-1831
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1 中所示的曲线是通过学习率为 0.003 获得的。较低的学习率会导致较低的误差。在这种情况下，集成平均的效果并没有那么显著。我们通过将输入向量更改为
    Pi & Peterson 的六维选择 [8] 来获得最佳结果，该选择由 xt−1、xt−2、xt−3、xt−4、xt−9 和 xt−10 组成。在上述保持不变的
    SRN 上使用学习率 0.0005，我们的预测误差最小值为 0.0674，这优于任何先前报告的结果。
- en: 6.3 Theoretical Analysis
  id: totrans-1832
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 理论分析
- en: The theoretical setting of the method is provided by the bias/variance decomposition.
    Within this framework, we define a particular bias/variance decomposition for
    networks differing by their initial conditions only. This is a particularly useful
    subset of the general set of all sources of variance.
  id: totrans-1833
  prefs: []
  type: TYPE_NORMAL
  zh: 方法的理论背景由偏差/方差分解提供。在这个框架内，我们定义了一种特定的偏差/方差分解，适用于仅因初始条件不同而异的网络。这是所有方差来源的一种特别有用的子集。
- en: The performance of an estimator is commonly evaluated by the Mean Square Error
    (MSE) defined as
  id: totrans-1834
  prefs: []
  type: TYPE_NORMAL
  zh: 估计量的性能通常通过均方误差（MSE）来评估，其定义为
- en: $$\operatorname{MSE}(f)\equiv E\left[(y-f(\mathbf{x}))^{2}\right]$$
  id: totrans-1835
  prefs: []
  type: TYPE_NORMAL
  zh: $$\operatorname{MSE}(f)\equiv E\left[(y-f(\mathbf{x}))^{2}\right]$$
- en: $$(6.2)$$
  id: totrans-1836
  prefs: []
  type: TYPE_NORMAL
  zh: $$(6.2)$$
- en: where the average is over test sets for the predictor f, and y are the target
    values of the data in x. Assuming the expectation E is taken with respect to the
    true probability of x and y, the MSE can be decomposed into
  id: totrans-1837
  prefs: []
  type: TYPE_NORMAL
  zh: 其中平均值是针对预测器 f 的测试集，y 是数据 x 的目标值。假设期望 E 是相对于 x 和 y 的真实概率进行的，MSE 可以分解为
- en: $$E\left[\left(y-f(\mathbf{x})\right)^{2}\right]=E\left[\left(y-E[y|\mathbf{x}]\right)^{2}\right]+E\left[\left(f(\mathbf{x})-E[y|\mathbf{x}]\right)^{2}\right].\tag{6.3}$$
  id: totrans-1838
  prefs: []
  type: TYPE_NORMAL
  zh: $$E\left[\left(y-f(\mathbf{x})\right)^{2}\right]=E\left[\left(y-E[y|\mathbf{x}]\right)^{2}\right]+E\left[\left(f(\mathbf{x})-E[y|\mathbf{x}]\right)^{2}\right].\tag{6.3}$$
- en: The first RHS term represents the variability or the noise in the data and is
    independent of the estimator f. It suffices therefore to concentrate on the second
  id: totrans-1839
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项 RHS 表示数据中的变异性或噪声，且与估计量 f 无关。因此，只需集中于第二项
- en: '![139_image_0.png](139_image_0.png)'
  id: totrans-1840
  prefs: []
  type: TYPE_IMG
  zh: '![139_image_0.png](139_image_0.png)'
- en: $$(6.4)$$
  id: totrans-1841
  prefs: []
  type: TYPE_NORMAL
  zh: $$(6.4)$$
- en: $$(6.5)$$
  id: totrans-1842
  prefs: []
  type: TYPE_NORMAL
  zh: $$(6.5)$$
- en: $$(6.6)$$
  id: totrans-1843
  prefs: []
  type: TYPE_NORMAL
  zh: $$(6.6)$$
- en: $$(6.7)$$
  id: totrans-1844
  prefs: []
  type: TYPE_NORMAL
  zh: $$(6.7)$$
- en: Fig. 6.3. Our best results for the test set of the sunspots problem. Plotted
    here are Q = 1 results for various choices of initial conditions, represented
    by their averages with error-bars extending over a standard deviation, and Q =
    20 results (the thinner points), as a function of training time in K-epochs. The
    network is based on the Pi & Peterson variables, and the learning rate is 0.0005.
  id: totrans-1845
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3. 我们在日斑问题测试集上的最佳结果。这里绘制的是 Q = 1 对于各种初始条件选择的结果，以它们的平均值表示，并用误差条扩展至一个标准偏差，以及
    Q = 20 的结果（较细的点），作为训练时间（K-epochs）的函数。网络基于 Pi & Peterson 变量，学习率为 0.0005。
- en: 'term. Any given predictor f(x) is naturally limited by the set of data on which
    it is trained. Considering a typical error one may average over all data space
    [2] and decompose this error into Bias and Variance components:'
  id: totrans-1846
  prefs: []
  type: TYPE_NORMAL
  zh: 项。任何给定的预测器 f(x) 自然受到其训练数据集的限制。考虑一个典型误差，可以对所有数据空间 [2] 进行平均，并将该误差分解为偏差和方差组成部分：
- en: $$E_{\mathcal{D}}\left[\left(f(\mathbf{x})-E[y|\mathbf{x}]\right)^{2}\right]=B_{\mathcal{D}}+V_{\mathcal{D}}$$
  id: totrans-1847
  prefs: []
  type: TYPE_NORMAL
  zh: $$E_{\mathcal{D}}\left[\left(f(\mathbf{x})-E[y|\mathbf{x}]\right)^{2}\right]=B_{\mathcal{D}}+V_{\mathcal{D}}$$
- en: where
  id: totrans-1848
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: BD(f(x)) = (ED [f(x)] − E[y|x])2 (6.5)
  id: totrans-1849
  prefs: []
  type: TYPE_NORMAL
  zh: BD(f(x)) = (ED [f(x)] − E[y|x])² (6.5)
- en: $$B_{\mathcal{D}}(f(\mathbf{x}))=\left(E_{\mathcal{D}}\left[f(\mathbf{x})\right]-E[y|\mathbf{x}]\right)^{2}$$
  id: totrans-1850
  prefs: []
  type: TYPE_NORMAL
  zh: $$B_{\mathcal{D}}(f(\mathbf{x}))=\left(E_{\mathcal{D}}\left[f(\mathbf{x})\right]-E[y|\mathbf{x}]\right)^{2}$$
- en: $$V_{\mathcal{D}}(f(\mathbf{x}))=E_{\mathcal{D}}\left[\left(f(\mathbf{x})-E_{\mathcal{D}}\left[f(\mathbf{x})\right]\right)^{2}\right].$$
  id: totrans-1851
  prefs: []
  type: TYPE_NORMAL
  zh: $$V_{\mathcal{D}}(f(\mathbf{x}))=E_{\mathcal{D}}\left[\left(f(\mathbf{x})-E_{\mathcal{D}}\left[f(\mathbf{x})\right]\right)^{2}\right].$$
- en: In our application we use as our predictor EI [f(x)] where the subscript I denotes
    the space of initial weights of the neural network that serves as f(x). To understand
    the effect of averaging over initial weights let us construct a space R by the
    direct product of D and I. It can then be shown that
  id: totrans-1852
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的应用中，我们将 EI [f(x)] 作为我们的预测器，其中下标 I 表示作为 f(x) 的神经网络的初始权重空间。为了理解对初始权重进行平均的影响，让我们通过
    D 和 I 的直接积构造一个空间 R。然后可以证明
- en: $$B_{\mathcal{R}}(f(\mathbf{x}))=B_{\mathcal{D}}(E_{\mathcal{I}}\left[f(\mathbf{x})\right]),$$
  id: totrans-1853
  prefs: []
  type: TYPE_NORMAL
  zh: $$B_{\mathcal{R}}(f(\mathbf{x}))=B_{\mathcal{D}}(E_{\mathcal{I}}\left[f(\mathbf{x})\right])$$，
- en: $$V_{\mathcal{R}}(f(\mathbf{x}))\geq V_{\mathcal{D}}(E_{\mathcal{I}}\left[f(\mathbf{x})\right]).$$
  id: totrans-1854
  prefs: []
  type: TYPE_NORMAL
  zh: $$V_{\mathcal{R}}(f(\mathbf{x}))\geq V_{\mathcal{D}}(E_{\mathcal{I}}\left[f(\mathbf{x})\right])$$。
- en: $$(6.8)$$
  id: totrans-1855
  prefs: []
  type: TYPE_NORMAL
  zh: $$(6.8)$$
- en: BR(f(x)) = BD(EI [f(x)]), (6.7)
  id: totrans-1856
  prefs: []
  type: TYPE_NORMAL
  zh: BR(f(x)) = BD(EI [f(x)])， (6.7)
- en: and
  id: totrans-1857
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: VR(f(x)) ≥ VD(EI [f(x)]). (6.8)
  id: totrans-1858
  prefs: []
  type: TYPE_NORMAL
  zh: VR(f(x)) ≥ VD(EI [f(x)])。 (6.8)
- en: This means that using EI [f(x)] as the predictor, the characteristic error has
    reduced variance but unchanged bias.
  id: totrans-1859
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着使用 EI [f(x)] 作为预测器时，特征误差的方差降低，但偏差不变。
- en: The bias term may also be represented as BR = ED [BI] where
  id: totrans-1860
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差项也可以表示为 BR = ED [BI]，其中
- en: $$B_{\mathcal{I}}(f(\mathbf{x}))=\left(E_{\mathcal{I}}\left[f(\mathbf{x})\right]-E[y|\mathbf{x}]\right)^{2}.$$
  id: totrans-1861
  prefs: []
  type: TYPE_NORMAL
  zh: $$B_{\mathcal{I}}(f(\mathbf{x}))=\left(E_{\mathcal{I}}\left[f(\mathbf{x})\right]-E[y|\mathbf{x}]\right)^{2}.$$
- en: BI(f(x)) = (EI [f(x)] − E[y|x])2 . (6.9)
  id: totrans-1862
  prefs: []
  type: TYPE_NORMAL
  zh: BI(f(x)) = (EI [f(x)] − E[y|x])²。 (6.9)
- en: BI is unaffected when f(x) is replaced by its average. The analogously defined
    variance term, VI, gets eliminated by such averaging. In other words, by averaging
    over all I we eliminated all variance due to the choice of initial weights.
  id: totrans-1863
  prefs: []
  type: TYPE_NORMAL
  zh: 当 f(x) 被其平均值替代时，BI 不受影响。类似定义的方差项 VI 通过这种平均被消除。换句话说，通过对所有 I 进行平均，我们消除了因初始权重选择而导致的所有方差。
- en: The difference between the Q = 1 and Q → ∞ curves in Fig. 1 represents this
    reduction of variance.
  id: totrans-1864
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1 中 Q = 1 和 Q → ∞ 曲线之间的差异代表了方差的减少。
- en: To understand the Q dependence of Fig. 2 consider an average defined by ¯f(x)
    over functions that represent independent identically distributed random variables.
    It can then be shown that
  id: totrans-1865
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解图 2 中的 Q 依赖性，可以考虑一个由 ¯f(x) 定义的平均值，该平均值代表独立同分布的随机变量。然后可以证明
- en: $$(6.9)$$
  id: totrans-1866
  prefs: []
  type: TYPE_NORMAL
  zh: $$(6.9)$$
- en: $$B(\bar{f})=\overline{{{B(f)}}}\qquad V(\bar{f})=\overline{{{V(f)}}}/Q.$$
  id: totrans-1867
  prefs: []
  type: TYPE_NORMAL
  zh: $$B(\bar{f})=\overline{{{B(f)}}}\qquad V(\bar{f})=\overline{{{V(f)}}}/Q.$$
- en: $$(6.10)$$
  id: totrans-1868
  prefs: []
  type: TYPE_NORMAL
  zh: $$(6.10)$$
- en: B( ¯f) = B(f) V ( ¯f) = V (f)/Q. (6.10)
  id: totrans-1869
  prefs: []
  type: TYPE_NORMAL
  zh: B( ¯f) = B(f) V ( ¯f) = V (f)/Q。 (6.10)
- en: Hence we can interpret the 1/Q behavior displayed in Fig. 2 as a demonstration
    that the choice of initial conditions in that analysis acts as effective random
    noise. In general this is not necessarily the case, since networks with different
    initial conditions may have non-trivial correlations. For a more thorough discussion
    of this and other points see [5].
  id: totrans-1870
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将图 2 中显示的 1/Q 行为解释为在该分析中初始条件的选择作为有效随机噪声的示范。一般来说，这并不一定成立，因为具有不同初始条件的网络可能存在非平凡的相关性。有关这一点及其他问题的更全面讨论见
    [5]。
- en: In conclusion, we see that averaging over networks with different initial weights
    is helpful in reducing the prediction error by eliminating the variance induced
    by initial conditions. Performing this average over groups of finite size Q one
    finds out from the Q dependence if the errors induced by initial conditions are
    correlated or not. Moreover, one may estimate the Q needed to eliminate this source
    of variance.
  id: totrans-1871
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们看到，在不同初始权重的网络上进行平均有助于减少预测误差，消除由初始条件引起的方差。在有限大小 Q 的组上进行这种平均，可以通过 Q 依赖性判断初始条件引起的误差是否相关。此外，可以估计消除此方差源所需的
    Q。
- en: '[1] Elman, J.L., Zipser, D.: Learning the Hidden Structure of Speech. J. Acoust.
    Soc.'
  id: totrans-1872
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Elman, J.L., Zipser, D.: 学习语音的隐含结构。声学学会杂志。'
- en: Amer. 83, 1615–1626 (1988)
  id: totrans-1873
  prefs: []
  type: TYPE_NORMAL
  zh: 美国 83, 1615–1626 (1988)
- en: '[2] Geman, S., Bienenstock, E., Doursat, R.: Neural networks and the bias/variance
    dilemma. Neural Comp. 4(1), 1–58 (1992)'
  id: totrans-1874
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Geman, S., Bienenstock, E., Doursat, R.: 神经网络与偏差/方差困境。神经计算 4(1), 1–58 (1992)'
- en: '[3] Lincoln, W.P., Skrzypek, J.: Synergy of clustering multiple back propagation
    networks. In: Touretzky, D.S. (ed.) Advances in Neural Information Processing
    Systems, vol. 2, pp. 650–657. Morgan Kaufmann, SanMateo (1990)'
  id: totrans-1875
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Lincoln, W.P., Skrzypek, J.: 多个反向传播网络的协同作用。见：Touretzky, D.S.（主编）《神经信息处理系统的进展》，第2卷，第650–657页。摩根·考夫曼，圣马特奥
    (1990)'
- en: '[4] Morris, J.: Forecasting the sunspot cycle. J. Roy. Stat. Soc. Ser. A 140,
    437–447'
  id: totrans-1876
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Morris, J.: 预测太阳黑子周期。皇家统计学会系列 A 140, 437–447'
- en: (1977)
  id: totrans-1877
  prefs: []
  type: TYPE_NORMAL
  zh: (1977)
- en: '[5] Naftaly, U., Intrator, N., Horn, D.: Optimal Ensemble Averaging of Neural
    Networks. Network, Comp. Neural Sys. 8, 283–296 (1997)'
  id: totrans-1878
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Naftaly, U., Intrator, N., Horn, D.: 神经网络的最优集成平均。网络，计算神经系统 8, 283–296 (1997)'
- en: '[6] Nowlan, S.J., Hinton, G.E.: Simplifying neural networks by soft weight-sharing.'
  id: totrans-1879
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Nowlan, S.J., Hinton, G.E.: 通过软权重共享简化神经网络。'
- en: Neural Computation 4, 473–493 (1992)
  id: totrans-1880
  prefs: []
  type: TYPE_NORMAL
  zh: 神经计算 4, 473–493 (1992)
- en: '[7] Perrone, P.M.: Improving regression estimation: averaging methods for variance
    reduction with extensions to general convex measure optimization. PhD thesis,
    Brown University, Institute for Brain and Neural Systems (1993)'
  id: totrans-1881
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Perrone, P.M.: 改进回归估计：用于方差减少的平均方法及其在一般凸测度优化中的扩展。博士论文，布朗大学，大脑与神经系统研究所 (1993)'
- en: '[8] Pi, H., Peterson, C.: Finding the Embedding Dimension and Variable Dependencies
    in Time Series. Neural Comp. 6, 509–520 (1994)'
  id: totrans-1882
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Pi, H., Peterson, C.: 在时间序列中寻找嵌入维度和变量依赖性。神经计算 6, 509–520 (1994)'
- en: '[9] Priestley, M.B.: Spectral Analysis and Time Series. Academic Press (1981)'
  id: totrans-1883
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Priestley, M.B.: 频谱分析与时间序列。学术出版社 (1981)'
- en: '[10] Weigend, A.S., Huberman, B.A., Rumelhart, D.: Predicting the future: A
    connectionist approach. Int. J. Neural Syst. 1, 193–209 (1990)'
  id: totrans-1884
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Weigend, A.S., Huberman, B.A., Rumelhart, D.: 预测未来：一种连接主义方法。国际神经系统期刊 1,
    193–209 (1990)'
- en: '[11] Wolpert, D.H.: Stacked generalization. Neural Networks 5, 241–259 (1992)'
  id: totrans-1885
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Wolpert, D.H.: 堆叠泛化。神经网络 5, 241–259 (1992)'
- en: Improving Network Models And Algorithmic Tricks-
  id: totrans-1886
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进网络模型和算法技巧-
- en: Preface
  id: totrans-1887
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: This section contains 5 chapters presenting easy to implement tricks which modify
    either the architecture and/or the learning algorithm so as to enhance the network's
    modeling ability. Better modeling means better solutions in less time.
  id: totrans-1888
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含5章，展示了易于实现的技巧，这些技巧修改了架构和/或学习算法，以增强网络的建模能力。更好的建模意味着更快的解决方案。
- en: In chapter 7, Gary Flake presents a trick that gives an MLP the additional power
    of an RBF. Trivial to implement, one simply adds extra inputs whose values are
    the square of the original inputs (p. 144). While adding higher order terms as
    inputs is not a new idea, this chapter contributes new insight by providing (1)
    a good summary of previous work, (2) simple clear examples illustrating this trick,
    (3) a theoretical justification showing that one need only add the higher order
    squared terms, and (4) a thorough comparison with numerous other network models.
    The need for *only* the squared terms is significant because it means that we
    gain this extra power without having the number of inputs grow excessively large.
    We remark that this idea can be extended further by including relevant features
    other than squared inputs e.g. by using kernel PCA [2] to obtain the non-linear
    features.
  id: totrans-1889
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，Gary Flake提出了一种技巧，使MLP具备RBF的额外能力。实现简单，只需添加额外输入，其值为原始输入的平方（第144页）。虽然将高阶项作为输入并不是一个新主意，但本章通过提供（1）对以前工作的良好总结，（2）简单明了的示例说明这个技巧，（3）理论上的证明，显示只需添加高阶平方项，以及（4）与众多其他网络模型的全面比较，贡献了新的见解。*仅*需要平方项的意义在于，这意味着我们在不让输入数量过于庞大的情况下获得了额外的能力。我们指出，这一思路可以通过包括除了平方输入以外的相关特征进一步扩展，例如通过使用核主成分分析（PCA）[2]来获得非线性特征。
- en: Rich Caruana in chapter 8 presents multi-task learning (MTL) (p. 163)
  id: totrans-1890
  prefs: []
  type: TYPE_NORMAL
  zh: Rich Caruana在第8章中介绍了多任务学习（MTL）（第163页）。
- en: 'where extra outputs are added to a network to predict tasks separate but related
    to the primary task. To introduce the trick, the chapter begins with an example
    and detailed discussion of a simple boolean function of binary inputs. The author
    then presents several of what one might use as extra outputs in practice. These
    include, among others: (1) features that are available only after predictions
    must be made, but which are available offline at *training* time (p. 170), (2)
    the same task but with a different metric (p. 175), and (3) the same task but
    with different output representations (p. 176). Empirical results are presented
    for (1) mortality rankings for pneumonia where the extra outputs are test results
    not available when the patient first enters the hospital but which are available
    a posteriori to complement the training data, and (2) a vehicle steering task
    where other outputs include location of centerline and road edges, etc. The last
    part of the chapter is devoted to topics for implementing MTL effectively, such
    as size of hidden layers (p. 181), early stopping (p. 181), and learning rates
    (p. 185).'
  id: totrans-1891
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，额外的输出被添加到网络中，以预测与主要任务分开但相关的任务。为了引入这个技巧，本章以一个简单的二元输入布尔函数的示例和详细讨论开始。作者随后展示了一些在实践中可能使用的额外输出。这些包括：
    (1) 只有在做出预测后才能获得的特征，但在*训练*时是可用的（第170页）， (2) 但使用不同度量的相同任务（第175页），以及 (3) 使用不同输出表示的相同任务（第176页）。提供了(1)肺炎的死亡率排名的实证结果，其中额外输出是患者首次进入医院时不可用的测试结果，但在训练数据的补充后可用，以及(2)一个车辆转向任务，其中其他输出包括中心线和道路边缘的位置等。本章的最后部分专门讨论有效实施MTL的主题，例如隐藏层的大小（第181页）、提前停止（第181页）和学习率（第185页）。
- en: The next chapter by Patrick van der Smagt and Gerd Hirzinger presents a trick
    to reduce the problem of ill-conditioning in the Hessian. If a unit has a very
    small outgoing weight then the influence of the incoming weights to that unit
    will be severely diminished (see chapter 1 for other sources of Hessian illconditioning).
    This results in flat spots in the error surface, which translates into
  id: totrans-1892
  prefs: []
  type: TYPE_NORMAL
  zh: 由Patrick van der Smagt和Gerd Hirzinger撰写的下一章介绍了一种减少Hessian病态问题的技巧。如果一个单元的外部权重非常小，则该单元的输入权重的影响将大大减弱（有关Hessian病态的其他来源，请参见第1章）。这导致误差表面出现平坦区域，进而转化为
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-1893
  prefs: []
  type: TYPE_NORMAL
  zh: '- 先前发表在：Orr, G.B.和Müller, K.-R.（编）：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-1894
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0（1998年）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    139–141, 2012.'
  id: totrans-1895
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon等人（编）：神经网络：行业技巧，第2版，LNCS 7700，第139-141页，2012年。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-1896
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: slow training (see also [1]). The trick is to add linear shortcut connections
    (p.
  id: totrans-1897
  prefs: []
  type: TYPE_NORMAL
  zh: 慢速训练（另见[1]）。这个技巧是增加线性快捷连接（第页）。
- en: 196) from the input to the output nodes to create what the authors refer to
    as a linearly augmented feed-forward network. These connections share the weights
    with the input to hidden connections so that no new weight parameters are added.
    This trick enhances the sensitivity of the network to those incoming weights thus
    removing or reducing the flat spots in the error surface. The improvement in the
    quality of the error surface is illustrated in a toy example.
  id: totrans-1898
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入节点到输出节点创建作者所称的线性增强前馈网络。这些连接与输入到隐藏层的连接共享权重，因此没有添加新的权重参数。这个技巧提高了网络对这些输入权重的敏感性，从而消除或减少误差表面上的平坦区域。误差表面质量的改善在一个玩具示例中得到了说明。
- en: Simulations with data from a robot arm are also shown.
  id: totrans-1899
  prefs: []
  type: TYPE_NORMAL
  zh: 还展示了使用机器人手臂数据的模拟。
- en: 'The trick discussed by Nicol Schraudolph in chapter 10 is to center the various
    factors comprising the neural network''s gradient (p. 208): input and hidden unit
    activities (see chapter 1), error signals, and the slope of the hidden units''
    nonlinear activation functions (p. 208). To give an example: *activity centering*'
  id: totrans-1900
  prefs: []
  type: TYPE_NORMAL
  zh: Nicol Schraudolph在第10章中讨论的技巧是对神经网络梯度的各种因素进行中心化（第208页）：输入和隐藏单元的活动（见第1章）、误差信号以及隐藏单元非线性激活函数的斜率（第208页）。举个例子：*活动中心化*
- en: (p. 207) is done by simply transforming the values of the components xi into
  id: totrans-1901
  prefs: []
  type: TYPE_NORMAL
  zh: （第207页）通过简单地将组件xi的值转换为
- en: $$\tilde{x}_{i}\,=\,x_{i}-\langle x_{i}\rangle\,,$$
  id: totrans-1902
  prefs: []
  type: TYPE_NORMAL
  zh: $$\tilde{x}_{i}\,=\,x_{i}-\langle x_{i}\rangle\,,$$
- en: 'where · denotes averaging over training samples. All different centering strategies
    can be implemented efficiently for a stochastic, batch or mini batch learning
    scenario (p. 209). He also uses shortcut connections (p. 208) but quite differently
    from the previous chapter: the shortcut connections contain new weights'
  id: totrans-1903
  prefs: []
  type: TYPE_NORMAL
  zh: 其中·表示对训练样本的平均。所有不同的中心化策略都可以高效实现于随机、批处理或小批量学习场景（第209页）。他还使用了快捷连接（第208页），但与前一章的使用方式有所不同：这些快捷连接包含新的权重。
- en: (not shared) which complement slope centering by carrying the linear component
    of the signal, making it possible for the rest of the network to concentrate on
    the nonlinear component of the problem. So, with respect to shortcut connections,
    the approaches in chapters 9 and 10 appear complementary. Centering gives a nice
    speed-up without much harm to the generalization error, as seen in the simulations
    on toy and vowel data (p. 211).
  id: totrans-1904
  prefs: []
  type: TYPE_NORMAL
  zh: （不共享）通过传递信号的线性分量来补充斜率中心化，使网络的其余部分能够集中于问题的非线性分量。因此，就快捷连接而言，第9章和第10章中的方法似乎是互补的。中心化提供了良好的加速，对泛化误差几乎没有伤害，正如在玩具和元音数据上的模拟所示（第211页）。
- en: In chapter 11, Tony Plate presents a trick requiring only minimal memory overhead
    that reduces numerical round-off in backpropagation networks.
  id: totrans-1905
  prefs: []
  type: TYPE_NORMAL
  zh: 在第11章中，Tony Plate提出了一种只需极少内存开销的技巧，减少反向传播网络中的数值舍入误差。
- en: Round-off error can occur in the standard method for computing the derivative
    of the logistic function since it requires calculating the product
  id: totrans-1906
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算逻辑函数的导数的标准方法中可能会出现舍入误差，因为它需要计算乘积。
- en: Y(1 − Y)
  id: totrans-1907
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Y(1 − Y)
- en: where y is the output of either a hidden or output unit. When the value of y
    is close to 1 then the limited precision of single or even double precision floating
    point numbers can result in the product being zero. This may not be a serious
    problem for on-line learning but can cause significant problems for networks using
    batch mode, particularly when second order methods are used. Such round-off can
    occur in other types of units as well. This chapter provides formulas for reducing
    such round-off errors in the computation of
  id: totrans-1908
  prefs: []
  type: TYPE_NORMAL
  zh: 其中y是隐藏单元或输出单元的输出。当y的值接近1时，单精度甚至双精度浮点数的有限精度可能导致乘积为零。这对于在线学习可能不是一个严重的问题，但对于使用批处理模式的网络，尤其是在使用二阶方法时，可能会造成显著问题。其他类型的单元也可能出现这种舍入误差。本章提供了减少计算中这种舍入误差的公式。
- en: '- derivatives of the error for logistic units or tanh units (p. 226 and 229)
    - derivatives in a one-of-k classification problem with cross-entropy error and
    softmax (p. 228)'
  id: totrans-1909
  prefs: []
  type: TYPE_NORMAL
  zh: '- 逻辑单元或tanh单元的误差导数（第226和229页） - 在一个使用交叉熵误差和softmax的k类分类问题中的导数（第228页）'
- en: '- derivatives and errors in a two-class classification problem using a single
    logistic output unit with cross entropy error and 0/1 targets (p. 228).'
  id: totrans-1910
  prefs: []
  type: TYPE_NORMAL
  zh: '- 使用单个逻辑输出单元和交叉熵误差及0/1目标的二类分类问题中的导数和误差（第228页）。'
- en: '[1] Hochreiter, S., Schmidhuber, J.: Flat minima. Neural Computation 9(1),
    1–42'
  id: totrans-1911
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Hochreiter, S., Schmidhuber, J.: 平坦极小值。神经计算 9(1), 1–42'
- en: (1997)
  id: totrans-1912
  prefs: []
  type: TYPE_NORMAL
  zh: (1997)
- en: '[2] Schölkopf, B., Smola, A., Müller, K.-R.: Nonlinear component analysis as
    a kernel eigenvalue problem. Neural Computation 10, 1299–1319 (1998)'
  id: totrans-1913
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Schölkopf, B., Smola, A., Müller, K.-R.：非线性成分分析作为核特征值问题。《神经计算》10，1299–1319
    (1998)'
- en: 7 Square Unit Augmented, Radially Extended, Multilayer Perceptrons-
  id: totrans-1914
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7平方单元增强、径向扩展的多层感知器-
- en: Gary William Flake Siemens Corporate Research, Inc.
  id: totrans-1915
  prefs: []
  type: TYPE_NORMAL
  zh: Gary William Flake 西门子企业研究公司
- en: 755 College Road East Princeton, NJ 08540 flake@scr.siemens.com http://mitpress.mit.edu/books/FLAOH/cbnhtml/author.html
    Abstract. Consider a multilayer perceptron (MLP) with d inputs, a single hidden
    sigmoidal layer and a linear output. By adding an additional d inputs to the network
    with values set to the square of the first d inputs, properties reminiscent of
    higher-order neural networks and radial basis function networks (RBFN) are added
    to the architecture with little added expense in terms of weight requirements.
    Of particular interest, this architecture has the ability to form localized features
    in a d-dimensional space with a single hidden node but can also span large volumes
    of the input space; thus, the architecture has the localized properties of an
    RBFN
  id: totrans-1916
  prefs: []
  type: TYPE_NORMAL
  zh: 755 College Road East Princeton, NJ 08540 flake@scr.siemens.com http://mitpress.mit.edu/books/FLAOH/cbnhtml/author.html
    摘要。考虑一个具有d个输入、一个隐藏的sigmoid层和一个线性输出的多层感知器（MLP）。通过向网络添加额外的d个输入，其值设为前d个输入的平方，架构中添加了类似于高阶神经网络和径向基函数网络（RBFN）的特性，而在权重需求上几乎没有额外开销。特别有趣的是，这种架构能够在d维空间中形成局部特征，尽管只有一个隐藏节点，但也可以跨越输入空间的大体积；因此，该架构具有RBFN的局部特性。
- en: but does not suffer as badly from the curse of dimensionality. I refer to a
    network of this type as a SQuare Unit Augmented, Radially Extended, MultiLayer
    Perceptron (SQUARE-MLP or SMLP).
  id: totrans-1917
  prefs: []
  type: TYPE_NORMAL
  zh: 但不太受维数诅咒的困扰。我将这种类型的网络称为SQuare Unit Augmented, Radially Extended, MultiLayer
    Perceptron（SQUARE-MLP或SMLP）。
- en: 7.1 Introduction And Motivation
  id: totrans-1918
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 引言与动机
- en: When faced with a new and challenging problem, the most crucial decision that
    a neural network researcher must make is in the choice of the model class to pursue.
    Several different types of neural architectures are commonly found in most model
    building tool-boxes, with two of the more familiar, radial basis function networks
    (RBFNs) [15, 16, 13, 1, 17] and multilayer perceptrons (MLPs) [25, 20],
  id: totrans-1919
  prefs: []
  type: TYPE_NORMAL
  zh: 当面临一个新的挑战性问题时，神经网络研究者必须做出的最关键决定是选择追求的模型类别。在大多数模型构建工具箱中，常见几种不同类型的神经架构，其中两种较为熟悉的是径向基函数网络（RBFNs）[15,
    16, 13, 1, 17]和多层感知器（MLPs）[25, 20]，
- en: 'exemplifying the differences found between global and local model types. Specifically,
    an MLP is an example of a global model that builds approximations with features
    that alter the entire input-output response, while an RBFN is a local model that
    uses features confined to a finite region in the input-space. This single difference
    between the two model types has major implications for many architectural and
    algorithmic issues. While MLPs are slow learners, have low memory retention, typically
    use homogeneous learning rules and are relatively less troubled by the curse of
    dimensionality, RBFNs are nearly opposite in every way:'
  id: totrans-1920
  prefs: []
  type: TYPE_NORMAL
  zh: 例证了全局模型和局部模型类型之间的差异。具体而言，MLP是构建整个输入输出响应特征近似的全局模型的一个例子，而RBFN是使用有限输入空间区域内特征的局部模型。这两种模型类型之间的这一单一差异对许多架构和算法问题具有重要意义。虽然MLP的学习速度较慢，记忆保留率低，通常使用同质学习规则，且相对不那么受维数诅咒的困扰，但RBFN几乎在各个方面都是相反的：
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-1921
  prefs: []
  type: TYPE_NORMAL
  zh: '- 以前发表在：Orr, G.B.和Müller, K.-R.（编）：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998)
  id: totrans-1922
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0 (1998)
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    143–161, 2012.'
  id: totrans-1923
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon等（编）：NN：行业诀窍，第2版，LNCS 7700，第143–161页，2012。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-1924
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: they are fast learners, have high memory retention, typically use heterogeneous
    learning rules and are greatly troubled by the curse of dimensionality.
  id: totrans-1925
  prefs: []
  type: TYPE_NORMAL
  zh: 它们是快速学习者，具有高记忆保留率，通常使用异质学习规则，并且受到维数诅咒的严重困扰。
- en: Because of these differences, it is often tempting to use one or more heuristics
    to make the choice, e.g., RBFNs (MLPs) for low (high) dimensional problems, or
    RBFNs (MLPs) for continuous function approximation (pattern classification) problems.
    While these rules-of-thumb are often sufficient for simple problems there are
    many exceptions that defy the rules (e.g., see [12]). Moreover, more challenging
    problems from industrial settings often have high-dimensional input-spaces that
    are locally well-behaved and may not be clearly defined as either function approximation
    or pattern classification problems. This means that choosing the best architectures
    for a particular problem can be a nontrivial problem in itself.
  id: totrans-1926
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些差异，往往会诱使人们使用一个或多个启发式方法来做出选择，例如，低（高）维问题使用 RBFN（MLP），或连续函数逼近（模式分类）问题使用 RBFN（MLP）。虽然这些经验法则通常足以应对简单问题，但也有许多例外情况违反了这些规则（例如，参见
    [12]）。此外，来自工业环境的更具挑战性的问题往往具有局部良好的高维输入空间，可能并不明确被定义为函数逼近或模式分类问题。这意味着，为特定问题选择最佳架构本身可能是一个非平凡的问题。
- en: Ironically, a good compromise to this dilemma has been known for quite some
    time but only recently has the elegance of the trick been appreciated. For years,
    researchers have commonly used augmented MLPs by adding the squares of the inputs
    as auxiliary inputs. The justification for this has always been fairly casual
    and has usually boiled down to the argument that using this trick couldn't possibly
    hurt. But as it turns out, an MLP augmented in this way with n hidden nodes can
    almost perfectly approximate an RBFN with n basis functions. The "almost" comes
    from the fact that the radial basis function in the augmented MLP is not Gaussian
    but quasi-Gaussian (which is an admittedly undefined term that I simply use to
    mean "so close to Gaussian that it really doesn't matter."). This means that an
    MLP augmented with the squares of it's inputs can easily form local features with
    a single hidden node but can also span vast regions of the input-space, thereby
    effectively ignoring inputs when needed. Thus, the best of both architectural
    approaches is retained by using this amazingly simple trick.
  id: totrans-1927
  prefs: []
  type: TYPE_NORMAL
  zh: 具有讽刺意味的是，这一困境的良好折衷方案已经被知道了一段时间，但直到最近，人们才真正欣赏到这一技巧的优雅。多年来，研究人员通常通过将输入的平方作为辅助输入来使用增强的多层感知机（MLP）。这一做法的合理性一直比较随意，通常归结为使用这个技巧不可能造成伤害。但事实证明，以这种方式增强的具有
    n 个隐藏节点的 MLP几乎可以完美地逼近具有 n 个基函数的径向基函数网络（RBFN）。这个“几乎”源于增强 MLP 中的径向基函数不是高斯型而是准高斯型（这确实是一个未定义的术语，我用它只是为了表示“接近高斯，以至于实际上无关紧要”）。这意味着，使用输入平方增强的
    MLP 可以轻松形成局部特征，即使只有一个隐藏节点，但也可以覆盖输入空间的广阔区域，从而在需要时有效地忽略输入。因此，通过使用这一惊人的简单技巧，结合了两种架构方法的最佳之处。
- en: The remainder of this chapter is divided into 5 more sections. Section 7.2contains
    a description of the trick and briefly gives a comparison of the proposed architecture
    to other classes of well-known models. In Section 7.3, a function approximation
    problem and a pattern classification problem are used as examples to demonstrate
    the effectiveness of the trick. Afterwards, a well-known and challenging vowel
    classification problem is studied in greater detail. Section 7.4 theoretically
    justifies the trick by showing the equivalence of the resulting architecture and
    RBFNs, while Section 7.5 gives a more intuitive justification for the proposed
    trick by illustrating the types of surfaces and boundaries that can be formed
    by a single node with auxiliary square inputs. Finally, in Section 7.6, I give
    my conclusions.
  id: totrans-1928
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分分为五个部分。第 7.2 节描述了这一技巧，并简要比较了所提架构与其他知名模型类别的异同。在第 7.3 节中，使用函数逼近问题和模式分类问题作为示例来演示这一技巧的有效性。随后，对一个著名且具有挑战性的元音分类问题进行了更详细的研究。第
    7.4 节通过展示结果架构与 RBFN 的等价性来理论上证明这一技巧，而第 7.5 节则通过说明使用辅助平方输入的单个节点可以形成的表面和边界类型，给出这一技巧的更直观的解释。最后，在第
    7.6 节中，我给出我的结论。
- en: '7.2 The Trick: A Square-Mlp'
  id: totrans-1929
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 这个技巧：平方-多层感知机
- en: 'The proposed trick involves only a simple modification to the standard MLP
    architecture: the input layer of an MLP is augmented with an extra set of inputs
    that are coupled to the squares of the original inputs. This trick can be implemented
    in at least two different ways. The first technique is to simply augment a data
    set with the extra inputs. Thus, if one had a set with each input pattern having
    d components, then a new data set can be made from the original that has 2d inputs
    with the extra d inputs set equal to the squares of the original inputs. Implementing
    the trick in this way is expensive from a memory point of view but has the advantage
    that not a single line of new source code need be written to try it out. Moreover,
    this allows the trick to be tried even on a commercial simulator where one may
    not have access to source code.'
  id: totrans-1930
  prefs: []
  type: TYPE_NORMAL
  zh: 所提出的技巧仅涉及对标准 MLP 架构的简单修改：MLP 的输入层通过与原始输入的平方相联系的额外输入集进行增强。这个技巧可以以至少两种不同的方式实现。第一种技术是简单地用额外的输入增强数据集。因此，如果一个数据集中的每个输入模式有
    d 个组件，则可以从原始数据集中创建一个新的数据集，该数据集有 2d 个输入，额外的 d 个输入设置为原始输入的平方。以这种方式实现技巧从内存角度来看是昂贵的，但有一个优点，就是不需要编写任何新源代码来尝试。此外，这使得即使在没有源代码访问权限的商业模拟器上也可以尝试该技巧。
- en: 'The second way to implement the trick is to explicitly code the actual changes
    into the architecture:'
  id: totrans-1931
  prefs: []
  type: TYPE_NORMAL
  zh: 实现该技巧的第二种方式是将实际变化显式编码到架构中：
- en: $$y=\sum_{i}w_{i}g\left(\sum_{j}u_{ij}x_{j}+\sum_{k}v_{ik}x_{k}^{2}+a_{i}\right)+b,\tag{7.1}$$
  id: totrans-1932
  prefs: []
  type: TYPE_NORMAL
  zh: $$y=\sum_{i}w_{i}g\left(\sum_{j}u_{ij}x_{j}+\sum_{k}v_{ik}x_{k}^{2}+a_{i}\right)+b,\tag{7.1}$$
- en: with g(x) = tanh(x) or 1/(1+exp(−x)). I call such a network a SQuare Unit Augmented,
    Radially Extended, MultiLayer Perceptron (SQUARE-MLP or SMLP).
  id: totrans-1933
  prefs: []
  type: TYPE_NORMAL
  zh: 设 g(x) = tanh(x) 或 1/(1+exp(−x))。我称这种网络为**平方单元增强、径向扩展、多层感知器**（SQUARE-MLP 或 SMLP）。
- en: The "square unit augmented" portion of the name comes from the newly added vikx2k
    terms. The reason behind the "radially extended" portion of the name will become
    clear in Sections 7.4 and 7.5. All experiments in Section 7.3 use the architecture
    described by Equation 7.1. The history of this trick is rather difficult to trace
    primarily because it is such a trivial trick; however, a brief list of some related
    ideas is presented below.
  id: totrans-1934
  prefs: []
  type: TYPE_NORMAL
  zh: “平方单元增强”部分的名称来源于新添加的 vikx2k 项。名称中“径向扩展”部分的原因将在第 7.4 和 7.5 节中阐明。第 7.3 节中的所有实验都使用方程
    7.1 描述的架构。这个技巧的历史相当难以追踪，主要是因为它实在是一个微不足道的技巧；然而，下面列出了一些相关想法的简要清单。
- en: Engineering and Statistics. Very early related ideas have been pursued in the
    statistics community in the form of polynomial regression and Volterra filters
    in the engineering community [24, 22]. However, in both of these related approaches
    the model output is always linear in the polynomial terms, which is not the case
    with the SMLP architecture or in the other neural architectures discussed below.
  id: totrans-1935
  prefs: []
  type: TYPE_NORMAL
  zh: 工程与统计。早期相关的想法在统计学界以多项式回归和在工程界的 Volterra 滤波器的形式得到了追求 [24, 22]。然而，在这两种相关方法中，模型输出始终在线性多项式项中，这与
    SMLP 架构或下面讨论的其他神经架构不同。
- en: Sigma-Pi Networks. Some neural network architectures which are much more complicated
    than Equation 7.1 have the SMLP as a special case. Perhaps the earliest reference
    to a similar idea in the neural network literature can be traced back to Sigma-Pi
    networks [20], which extends an MLP's linear net input function with a summation
    of products, i wji *k xik. One could imagine a multilayer Sigma-Pi network that
    manages to compute the squares of the xik terms prior to them being passed through
    to a sigmoidal activation function. This would be a rather clumsy way of calculating
    the squares of the inputs, but it is possible to do it, nonetheless.
  id: totrans-1936
  prefs: []
  type: TYPE_NORMAL
  zh: Sigma-Pi 网络。一些比方程 7.1 更复杂的神经网络架构将 SMLP 视为特例。也许神经网络文献中最早提及类似想法的可以追溯到 Sigma-Pi
    网络 [20]，它通过对乘积的求和 i wji *k xik 扩展了 MLP 的线性网络输入函数。人们可以想象一个多层 Sigma-Pi 网络，它在将 xik
    传递到 Sigmoid 激活函数之前计算 xik 项的平方。虽然这是一种计算输入平方的相当笨拙的方法，但无论如何，仍然可以实现。
- en: 'Higher-Order Networks. Perhaps the closest example is the higher-order network,
    proposed by Lee *et al.* [14], which is similar to Equation 7.1 but uses the full
    quadratic net input function:'
  id: totrans-1937
  prefs: []
  type: TYPE_NORMAL
  zh: 高阶网络。或许最接近的例子是由 Lee *等* [14] 提出的高阶网络，它类似于方程 7.1，但使用完整的二次网络输入函数：
- en: $$y=\sum_{i}w_{i}g\left(\sum_{j}u_{ij}x_{j}+\sum_{k}\sum_{l}v_{ikl}x_{k}x_{l}+a_{i}\right)+b.\tag{7.2}$$
  id: totrans-1938
  prefs: []
  type: TYPE_NORMAL
  zh: $$y=\sum_{i}w_{i}g\left(\sum_{j}u_{ij}x_{j}+\sum_{k}\sum_{l}v_{ikl}x_{k}x_{l}+a_{i}\right)+b.\tag{7.2}$$
- en: With vikl set to zero when k = l a SMLP is recovered. Thus, a SMLP is actually
    a higher-order network with a diagonal quadratic term. Higher-order networks have
    been shown to be very powerful extensions of MLPs. They can form both local and
    global features but only at the cost of squaring the number of weights for each
    hidden node. The memory requirements become an even greater issue when more sophisticated
    optimization routines are applied to an architecture such as Newton's or quasi-Newton
    methods which require memory proportional to the square of the number of weights
    in the network. Functional Link Networks. Another related architecture is the
    functional-link network [18], which is similar to a standard MLP but explicitly
    augments the network with the results of scalar functions applied to the inputs.
    For example, in some applications it may be known in advance that the desired
    output of the network is a function of the sine and cosine of one or more inputs
    (e.g., the inputs may correspond to angles of a robot arm). In this case, one
    would do well to include these values explicitly as inputs into the network instead
    of forcing the network to learn a potentially difficult-to-model concept. Functionallink
    networks may use any scalar function that, in the end, essentially performs a
    type of preprocessing on the data. Usually, expert knowledge is used to determine
    which extra scalar functions are to be incorporated into the network; that is,
    there is no general technique for choosing the best preprocessor functions a priori.
    However, given a set of nonlinear transformations on the input data one can perform
    principal component analysis (PCA) on the nonlinear feature space to select a
    subset that carries the most variance. A computationally feasible version of this
    technique has been proposed in [23], which they refer to as kernel PCA. In any
    event, by using the square function to augment a functional-link network, the
    SMLP is once again recovered.
  id: totrans-1939
  prefs: []
  type: TYPE_NORMAL
  zh: 当 k = l 时，vikl 设置为零，恢复了 SMLP。因此，SMLP 实际上是具有对角二次项的高阶网络。高阶网络已被证明是 MLP 的强大扩展。它们能够形成局部和全局特征，但代价是每个隐藏节点的权重数量需要平方。尤其在对诸如牛顿法或拟牛顿法等架构应用更复杂的优化例程时，内存需求变得更加突出，这些方法需要的内存与网络中权重数量的平方成正比。功能连接网络。另一个相关架构是功能连接网络
    [18]，它类似于标准 MLP，但明确通过对输入应用标量函数的结果来增强网络。例如，在某些应用中，可能事先知道网络的期望输出是一个或多个输入的正弦和余弦的函数（例如，输入可能对应于机器人手臂的角度）。在这种情况下，显式将这些值作为输入纳入网络会更有利，而不是强迫网络学习一个潜在难以建模的概念。功能连接网络可以使用任何标量函数，最终本质上对数据进行某种预处理。通常，专家知识用于确定应纳入网络的额外标量函数；也就是说，没有通用技术来事先选择最佳预处理函数。然而，给定输入数据上的一组非线性变换，可以对非线性特征空间进行主成分分析（PCA），以选择携带最多方差的子集。在
    [23] 中提出了一种计算上可行的该技术版本，他们称之为核 PCA。无论如何，通过使用平方函数来增强功能连接网络，再次恢复了 SMLP。
- en: While I have tried to assign proper credit, it is generally accepted that the
    basic idea of adding the squares of the inputs to a model is at least as old as
    the sage advice "preprocessing is everything."
  id: totrans-1940
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我尽力给予适当的引用，但一般认为将输入的平方添加到模型中的基本思想至少与“预处理是关键”这一明智建议同样古老。
- en: 7.3 Example Applications
  id: totrans-1941
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 示例应用
- en: The following three examples demonstrate problem domains in which an SMLP
  id: totrans-1942
  prefs: []
  type: TYPE_NORMAL
  zh: 以下三个示例展示了 SMLP 可在其中超越 MLP 或 RBFN 的问题领域。
- en: can conceivably outperform an MLP or an RBFN. All of the examples are wellknown
    benchmarks. In each case, the output response of the models must form local features
    while simultaneously spanning a large region of the input-space.
  id: totrans-1943
  prefs: []
  type: TYPE_NORMAL
  zh: 所有示例都是著名的基准。在每种情况下，模型的输出响应必须形成局部特征，同时覆盖输入空间的大区域。
- en: In general, the MLPs will have difficulty forming the local features, while
    the RBFNs will have trouble spanning the flat regions of the input space.
  id: totrans-1944
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，MLP 在形成局部特征方面会遇到困难，而 RBFN 在涵盖输入空间的平坦区域时会遇到问题。
- en: 7.3.1 Hill-Plateau Function Approximation
  id: totrans-1945
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3.1 山丘-高原函数近似
- en: The first problem is an admittedly contrived example that was chosen precisely
    because it is difficult for both MLPs and RBFNs. The "Hill-Plateau" surface [21],
    displayed in Figure 7.1, has a single local bump on a sigmoidal ridge. Training
    data for this problem consists of a two-dimensional uniform sampling on a 21×21
    grid of the region shown in the figure while the testing data comes from a finer
    41 × 41 grid.
  id: totrans-1946
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题是一个承认的矫揉造作的例子，选择它的原因正是因为它对MLP和RBFN都很困难。图7.1中展示的“山丘-高原”表面[21]，有一个单一的局部凸起，位于一个S形的山脊上。这个问题的训练数据由图中所示区域的21×21网格上的二维均匀采样组成，而测试数据来自一个更细的41×41网格。
- en: '![149_image_0.png](149_image_0.png)'
  id: totrans-1947
  prefs: []
  type: TYPE_IMG
  zh: '![149_image_0.png](149_image_0.png)'
- en: Fig. 7.1. A hill-plateau exemplifies the differences between local and global
    architectures. MLPs can easily form the plateau but have a hard time on the hill,
    while RBFNs trivially form the hill but are troubled by the plateau.
  id: totrans-1948
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1. 山丘-高原示例展示了局部和全局架构之间的差异。MLP可以轻松形成高原，但在山丘上遇到困难，而RBFN则轻松形成山丘，但在高原上遇到麻烦。
- en: '| Model   | # of Nodes # of Weights Test RMSE   |    |          |'
  id: totrans-1949
  prefs: []
  type: TYPE_TB
  zh: '| 模型   | 节点数量 权重数量 测试RMSE   |    |          |'
- en: '| --- | --- | --- | --- |'
  id: totrans-1950
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| RBFN    | 2                                   | 9  | 0.333406 |'
  id: totrans-1951
  prefs: []
  type: TYPE_TB
  zh: '| RBFN    | 2                                   | 9  | 0.333406 |'
- en: '|         | 3                                   | 13 | 0.071413 |'
  id: totrans-1952
  prefs: []
  type: TYPE_TB
  zh: '|         | 3                                   | 13 | 0.071413 |'
- en: '|         | 4                                   | 17 | 0.042067 |'
  id: totrans-1953
  prefs: []
  type: TYPE_TB
  zh: '|         | 4                                   | 17 | 0.042067 |'
- en: '|         | 5                                   | 21 | 0.002409 |'
  id: totrans-1954
  prefs: []
  type: TYPE_TB
  zh: '|         | 5                                   | 21 | 0.002409 |'
- en: '| MLP     | 2                                   | 9  | 0.304800 |'
  id: totrans-1955
  prefs: []
  type: TYPE_TB
  zh: '| MLP     | 2                                   | 9  | 0.304800 |'
- en: '|         | 3                                   | 13 | 0.015820 |'
  id: totrans-1956
  prefs: []
  type: TYPE_TB
  zh: '|         | 3                                   | 13 | 0.015820 |'
- en: '|         | 4                                   | 17 | 0.001201 |'
  id: totrans-1957
  prefs: []
  type: TYPE_TB
  zh: '|         | 4                                   | 17 | 0.001201 |'
- en: '| SMLP    | 2                                   | 13 | 0.000025 |'
  id: totrans-1958
  prefs: []
  type: TYPE_TB
  zh: '| SMLP    | 2                                   | 13 | 0.000025 |'
- en: Table 7.1. Best of twenty runs for the Hill-Plateau surface
  id: totrans-1959
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1. 山丘-高原表面的二十次运行的最佳结果
- en: 'Besides a standard MLP, a normalized RBFN (NRBFN) was used for this problem,
    which is described by the two equations:'
  id: totrans-1960
  prefs: []
  type: TYPE_NORMAL
  zh: 除了标准MLP外，还使用了一个归一化的RBFN（NRBFN）来解决这个问题，具体由以下两个方程描述：
- en: $$y=\frac{\sum_{i}w_{i}r_{i}({\bf x})}{\sum_{j}r_{j}({\bf x})}+a\quad\mbox{and}\tag{7.3}$$  $r_{i}({\bf
    x})=\exp(-||{\bf x}-{\bf c}_{i}||^{2}/\sigma_{i}^{2})$, (7.4)
  id: totrans-1961
  prefs: []
  type: TYPE_NORMAL
  zh: $$y=\frac{\sum_{i}w_{i}r_{i}({\bf x})}{\sum_{j}r_{j}({\bf x})}+a\quad\mbox{和}\tag{7.3}$$  $r_{i}({\bf
    x})=\exp(-||{\bf x}-{\bf c}_{i}||^{2}/\sigma_{i}^{2})$, (7.4)
- en: with ci and σi being the ith basis center and width, respectively. To train
    the NRBFNs the centers were first clustered in the input-space of the training
    patterns with the k-means clustering algorithm. The width of each basis function
    was then set proportional to the distance to the nearest neighbor. Afterwards,
    the least-mean-square solution of the linear terms, wi and a, were solved for
  id: totrans-1962
  prefs: []
  type: TYPE_NORMAL
  zh: 其中ci和σi分别为第i个基中心和宽度。为了训练NRBFNs，首先使用k均值聚类算法在训练模式的输入空间中对中心进行了聚类。每个基函数的宽度与最近邻的距离成比例设置。随后，线性项wi和a的最小均方解被求解
- en: '![150_image_0.png](150_image_0.png)'
  id: totrans-1963
  prefs: []
  type: TYPE_IMG
  zh: '![150_image_0.png](150_image_0.png)'
- en: Fig. 7.2. Data for the Two-Spiral problem
  id: totrans-1964
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2. 两螺旋问题的数据
- en: exactly using a singular value decomposition to compute the pseudo-inverse.
    All of this formed the initial set of weights for the quasi-Newton's method (BFGS)
    optimization routine which was used on all weights simultaneously for up to 200
    epochs.
  id: totrans-1965
  prefs: []
  type: TYPE_NORMAL
  zh: 确切使用奇异值分解计算伪逆。所有这些形成了拟牛顿法（BFGS）优化例程的初始权重集，该例程用于所有权重同时训练，最多200个周期。
- en: For the MLPs, a single hidden layer with the tanh(x) activation function and
    a linear output was used. All weights were initially set to uniform random values
    in a -0.1 to 0.1 range. All weights were then trained with quasi-Newton method
    (BFGS) for up to 200 epochs. The SMLPs were setup and trained exactly as the MLPs.
  id: totrans-1966
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MLP，使用了单一隐藏层，激活函数为tanh(x)，线性输出。所有权重最初设置为-0.1到0.1范围内的均匀随机值。然后，所有权重通过拟牛顿法（BFGS）进行训练，最多200个周期。SMLP的设置和训练与MLP完全相同。
- en: Table 7.1 shows the results for all three architectures. As can be seen, both
    the RBFNs and the MLPs have a fair amount of difficulty with this task, even though
    the training data is noise free and the training procedures are fairly sophisticated.
    Contrary to this, the SMLP manages to nail the surface with only two hidden nodes.
    Moreover, the testing error is orders of magnitude better than the best results
    from the other two architectures.
  id: totrans-1967
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1显示了所有三种架构的结果。如可以看到，RBFNs和MLPs在此任务上都有相当大的困难，即使训练数据是无噪声的，训练过程也相当复杂。与此相反，SMLP仅用两个隐层节点就能成功解决问题。此外，测试误差比其他两种架构的最佳结果好几个数量级。
- en: 7.3.2 Two-Spirals Classification
  id: totrans-1968
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3.2 双螺旋分类
- en: The two-spirals classification problem is a well-known benchmark that is extremely
    challenging for all neural network architectures; additionally, virtually no results
    have been reported for RBFNs as the problem is such that local models would have
    to memorize the training data with many basis functions (> 100) in order to come
    even close to solving it. Thus, this is an example of a problem that RBFNs are
    not even viable candidates, which is why they are not considered further. Figure
    7.2 shows the data for the two-spirals problem, which consists of 194 points on
    the x-y-plane that belong to one of two spirals, each of which rotates around
    the origin three times.
  id: totrans-1969
  prefs: []
  type: TYPE_NORMAL
  zh: 双螺旋分类问题是一个著名的基准，对所有神经网络架构来说都极具挑战性；此外，几乎没有关于RBFNs的结果报告，因为这个问题使得局部模型必须使用很多基函数（>
    100）来记忆训练数据，以便接近解决。因此，这是一个RBFNs根本无法胜任的问题，这也是它们不再进一步考虑的原因。图7.2显示了双螺旋问题的数据，包含194个点位于x-y平面，属于两个螺旋之一，每个螺旋绕原点旋转三次。
- en: 'Previous results by other researchers for this problem have mostly focused
    on traditional MLPs and MLPs with shortcut connections. The best reported results
    for 100% classification accuracy are summarized below:'
  id: totrans-1970
  prefs: []
  type: TYPE_NORMAL
  zh: 其他研究者对这个问题的先前结果主要集中在传统MLP和带快捷连接的MLP上。100%分类准确率的最佳报告结果总结如下：
- en: '- (Lang & Witbrock [9]): 2-5-5-5-1 MLP with shortcut connections and 138 total
    weights. Average convergence time of 20,000 batched backpropagation epochs.'
  id: totrans-1971
  prefs: []
  type: TYPE_NORMAL
  zh: '- (Lang & Witbrock [9]): 具有快捷连接和138个总权重的2-5-5-5-1 MLP。平均收敛时间为20,000个批量反向传播周期。'
- en: '- (Lang & Witbrock [9]): 2-5-5-5-1 MLP with shortcut connections, 138 total
    weights, and cross-entropy error function. Average convergence time of 12,000
    batched backpropagation epochs.'
  id: totrans-1972
  prefs: []
  type: TYPE_NORMAL
  zh: '- (Lang & Witbrock [9]): 具有快捷连接、138个总权重和交叉熵误差函数的2-5-5-5-1 MLP。平均收敛时间为12,000个批量反向传播周期。'
- en: '- (Lang & Witbrock [9]): 2-5-5-5-1 MLP with shortcut connections and 138 total
    weights. Average of 7,900 quickprop [3] epochs.'
  id: totrans-1973
  prefs: []
  type: TYPE_NORMAL
  zh: '- (Lang & Witbrock [9]): 具有快捷连接和138个总权重的2-5-5-5-1 MLP。平均为7,900个quickprop [3]周期。'
- en: '- (Frostrom [unpublished]): 2-20-10-1 MLP with no shortcut connections and
    281 weights. Required 13,900 batched backpropagation with momentum epochs.'
  id: totrans-1974
  prefs: []
  type: TYPE_NORMAL
  zh: '- (Frostrom [未发表]): 具有281个权重且没有快捷连接的2-20-10-1 MLP。需要13,900个带动量的批量反向传播周期。'
- en: '- (Fahlman and Lebiere [4]): Cascade-Correlation MLP using 12 to 19 hidden
    units (15 average) and an average of 1700 quickprop epochs. Because of the cascade
    correlation topology, these networks used between 117 and 250 weights.'
  id: totrans-1975
  prefs: []
  type: TYPE_NORMAL
  zh: '- (Fahlman和Lebiere [4]): 使用12到19个隐层单元（平均15）的级联相关MLP，平均1700个quickprop周期。由于级联相关拓扑，这些网络使用了117到250个权重。'
- en: As these results show, the two-spiral problem is exceptionally difficult, requiring
    both complicated network topologies and long training times.
  id: totrans-1976
  prefs: []
  type: TYPE_NORMAL
  zh: 如这些结果所示，双螺旋问题极其困难，需要复杂的网络拓扑和较长的训练时间。
- en: Compared to the results above, the SMLP architecture seems to be very wellsuited
    to this problem. An SMLP with 15 hidden hyperbolic tangent units (for a total
    of only 91 weights) was trained with a conjugate gradient optimization routine.
    In ten out of ten trials, the SMLP solved the two-spirals problem with an average
    of 2500 training epochs (but as few as 800). Notice that the architecture for
    the SMLP, both topologically and in the number of weights, is much simpler than
    those used in the studies with the MLPs. As a result, the optimization algorithms
    can be much more efficient. This is a case of the representation power of an architecture
    simplifying the learning, thereby making weight optimization a faster process.
  id: totrans-1977
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述结果相比，SMLP架构似乎非常适合这个问题。一个具有15个隐藏双曲正切单元的SMLP（总共只有91个权重）通过共轭梯度优化例程进行了训练。在十次实验中，SMLP平均用了2500个训练周期解决了双螺旋问题（最少仅需800个）。注意，SMLP的架构在拓扑上和权重数量上都比在MLP研究中使用的架构简单得多。因此，优化算法的效率可以大大提高。这是架构的表示能力简化学习的一个案例，从而使权重优化过程更快。
- en: Although it was not always possible to consistently train a simpler SMLP
  id: totrans-1978
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管并不总是能够一致地训练一个更简单的SMLP
- en: to solve this problem, an SMLP with 10 hidden nodes (and only 61 weights) succeeded
    on three separate trials, taking an average of 1500 epochs. The output response
    surface of this SMLP is shown in Figure 7.3. In Section 7.5 we will examine the
    different types of surfaces that can be formed by a single SMLP hidden node. We
    shall see that the the SMLP's ability to easily form local and global features
    is crucial to its ability to rapidly solve the two-spiral problem.
  id: totrans-1979
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，一个具有10个隐藏节点（仅61个权重）的SMLP在三次独立试验中成功，平均使用了1500个周期。该SMLP的输出响应面如图7.3所示。在第7.5节中，我们将考察由单个SMLP隐藏节点形成的不同类型的表面。我们将看到，SMLP轻松形成局部和全局特征的能力对于其快速解决双螺旋问题至关重要。
- en: '![152_image_0.png](152_image_0.png)'
  id: totrans-1980
  prefs: []
  type: TYPE_IMG
  zh: '![152_image_0.png](152_image_0.png)'
- en: Fig. 7.3. SMLP reconstructed Two-Spiral surface from only ten hidden nodes
  id: totrans-1981
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3. SMLP仅使用十个隐藏节点重建的双螺旋表面
- en: 7.3.3 Vowel Classification
  id: totrans-1982
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3.3 元音分类
- en: The Deterding vowel recognition data set [2] is another widely studied benchmark
    that is much more difficult than the two earlier problems and is also more indicative
    of the type of problem that a neural network practitioner could be faced with.
    The data consists of auditory features of steady state vowels spoken by British
    English speakers. There are 528 training patterns and 462 test patterns with each
    pattern consisting of 10 features and belonging to exactly one of 11 classes that
    correspond to the spoken vowel. The speakers are of both genders, making this
    a very interesting problem.
  id: totrans-1983
  prefs: []
  type: TYPE_NORMAL
  zh: Deterding元音识别数据集[2]是另一个被广泛研究的基准，其难度远高于前两个问题，更能体现神经网络从业者可能面临的问题类型。该数据集包含由英国英语发音者发出的稳态元音的听觉特征。共有528个训练模式和462个测试模式，每个模式由10个特征组成，并且属于恰好一个对应于所发元音的11个类别中的一个。发音者包括男女，使得这个问题非常有趣。
- en: All results for this section use an architecture with 10 inputs, a varying number
    of hidden units, and 11 outputs. Some results from previous studies are summarized
    in Table 7.2. Some of the earlier studies are somewhat anecdotal in that they
    used either a single experiment or only a few experiments but they are informative
    as they demonstrate what the sophisticated neural network practitioner could expect
    to achieve on this problem with a wide number of architectures.
  id: totrans-1984
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的所有结果使用的是一个具有10个输入、不同数量的隐藏单元和11个输出的架构。以前研究的一些结果在表7.2中进行了汇总。一些早期研究在某种程度上是轶事性的，因为它们仅使用了单个实验或仅几个实验，但它们提供了有价值的信息，展示了复杂神经网络从业者在广泛架构下可以在这个问题上实现的期望结果。
- en: Interestingly, Robinson's results show that a nearest neighbor classifier is
    very difficult to beat for this problem. With a 56% correct classification rate,
    a nearest neighbor classifier outperforms all of Robinson's neural solutions.
    However, nearest neighbor approaches require vast amounts of data to be stored
    as a look-up table, so this is not a particularly encouraging result.
  id: totrans-1985
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，Robinson的结果表明，对于这个问题，最近邻分类器非常难以被超越。其正确分类率为56%，最近邻分类器的表现优于Robinson的所有神经网络解决方案。然而，最近邻方法需要存储大量数据作为查找表，因此这并不是一个特别令人鼓舞的结果。
- en: The best score, reported by Hastie and Tibshirani [7], was achieved with a Discriminant
    Adaptive Nearest Neighbor (DANN) classifier. The score of 61.7% was from the best
    classifier found in a number of simulation studies; hence, this score represents
    the best known prior result as found by an expert attempting multiple solutions.
  id: totrans-1986
  prefs: []
  type: TYPE_NORMAL
  zh: Hastie和Tibshirani [7]报告的最佳得分由一个判别自适应最近邻（DANN）分类器实现。61.7%的得分来自多次模拟研究中找到的最佳分类器；因此，该得分代表了专家尝试多种解决方案所找到的最佳已知结果。
- en: Table 7.3 lists the results of the experiments done specifically for this work.
  id: totrans-1987
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.3列出了专门为本研究进行的实验结果。
- en: Five different model/optimization combinations are shown in the table, and each
    row in the table corresponds to fifty separate trials started with different random
    initial conditions. For the major entries labeled as "trained" the weights of
    the model were determined by conjugate gradient for models with more than 1,000
    weights and quasi-Newton's method for models with fewer than 1,000 weights.
  id: totrans-1988
  prefs: []
  type: TYPE_NORMAL
  zh: 表中显示了五种不同的模型/优化组合，表中的每一行对应于以不同随机初始条件开始的五十次独立实验。对于标记为“训练”的主要条目，模型的权重是通过共轭梯度法确定的，适用于权重超过1,000的模型，而对于权重少于1,000的模型则使用拟牛顿法。
- en: The optimization routines were set to minimize an error function of the form
    E = e2+λ||w||2 where e is the difference between the actual and desired outputs
    and λ is a weight decay term that penalizes large weights. λ was set to 10−4 for
    all experiments. Values of λ equal to 10−3 and 10−5 consistently gave worse results
    for all architectures, so 10−4 was a fair compromise.1 All MLP and SMLP
  id: totrans-1989
  prefs: []
  type: TYPE_NORMAL
  zh: 优化程序被设置为最小化形式为E = e2+λ||w||2的误差函数，其中e是实际输出与期望输出之间的差异，λ是惩罚大权重的权重衰减项。所有实验中λ设置为10−4。λ值为10−3和10−5的结果在所有架构中一致较差，因此10−4是一个合理的折中。
- en: architectures used a tanh(x) activation function, while the RBFN is the same
    as described in Section 7.3.1 but is unnormalized (The normalized RBFN gave consistently
    inferior results). The optimization routines were always allowed to run until
    convergence (change in error measure is less 10−20) unless otherwise noted.
  id: totrans-1990
  prefs: []
  type: TYPE_NORMAL
  zh: 架构使用了tanh(x)激活函数，而RBFN与第7.3.1节中描述的一样，但未归一化（归一化的RBFN consistently给出的结果较差）。优化程序通常允许运行至收敛（误差度量变化小于10−20），除非另有说明。
- en: 'The weights in the RBFN and NRBFN models were "solved" with a three step process:
    1) set the centers to the cluster centers generated by the k-means clustering
    algorithm applied to the input vectors of the training data; 2) set the widths
    proportional to the distance to the nearest neighbor of each center; and 3) solve
    the remaining linear weights as a least mean square problem with a matrix pseudo-inverse.'
  id: totrans-1991
  prefs: []
  type: TYPE_NORMAL
  zh: RBFN和NRBFN模型中的权重通过三步过程“求解”：1）将中心设置为由k均值聚类算法生成的聚类中心，应用于训练数据的输入向量；2）将宽度设为与每个中心的最近邻的距离成比例；3）将剩余的线性权重作为带矩阵伪逆的最小均方问题求解。
- en: The SMLP architecture can be solved for in a manner similar to how the RBFN
    and NRBFN networks are solved. The details of this procedure are covered in Section
    7.4, but it suffices to say at this point that the procedure is nearly identical
    with the exception that the weights corresponding to the centers and the widths
    must be slightly transformed.
  id: totrans-1992
  prefs: []
  type: TYPE_NORMAL
  zh: SMLP架构可以通过类似于RBFN和NRBFN网络的方式来求解。该过程的细节在第7.4节中进行了说明，但此时可以说，该过程几乎完全相同，唯一的例外是与中心和宽度对应的权重必须稍作变换。
- en: Interestingly, the SMLP can use this "solve" procedure to compute an initial
    set of weights for an SMLP that is then trained with a gradient-based method.
    This has the effect of predisposing the SMLP to a very good initial solution that
    can be refined by the gradient-based optimization routines. The row in Table 7.3
    with the parenthetical label "hybrid" corresponds to SMLPs trained in this way.
  id: totrans-1993
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，SMLP可以使用此“求解”过程计算SMLP的初始权重集，然后通过基于梯度的方法进行训练。这会使SMLP倾向于一个非常好的初始解，之后可以通过基于梯度的优化程序进行精细调整。表7.3中带有“混合”标签的行对应于以这种方式训练的SMLP。
- en: Three of the columns in Table 7.3 show three different ways of measuring success
    for the various models, of which the only statistically significant measure is
    the column labeled "% Correct (Average)," which is the average test set score
    achieved after the optimization procedure halted on the training data. The scores
    reported under the column heading "% Correct (Best)" correspond to the best
  id: totrans-1994
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.3中的三列显示了测量各种模型成功的三种不同方式，唯一具有统计显著性的度量是标记为“% 正确（平均）”的列，代表在优化程序停止训练数据后获得的平均测试集分数。列标题“%
    正确（最佳）”下报告的分数对应于最佳
- en: 1 Note that since the SMLP has an extra set of weights, care must be taken to
    control the capacity and avoid over-fitting the data.
  id: totrans-1995
  prefs: []
  type: TYPE_NORMAL
  zh: 1 注意，由于SMLP有额外的一组权重，因此必须小心控制容量，避免过拟合数据。
- en: Table 7.2. Previous result on the vowel data as summarized in [19], [12], [5]
    [8], [6],
  id: totrans-1996
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.2. 关于元音数据的先前结果总结见于[19]，[12]，[5]，[8]，[6]，
- en: and [7]. All entries are either deterministic techniques or are the best scores
    reported, unless the score appears with a "," in which case the score represents
    an average over multiple runs.
  id: totrans-1997
  prefs: []
  type: TYPE_NORMAL
  zh: 和[7]。所有条目要么是确定性技术，要么是报告的最佳分数，除非分数后面带有“，”的情况下，该分数表示多次运行的平均值。
- en: '| multiple runs.                  | Model   | Number of   | Number of   | Percent   |'
  id: totrans-1998
  prefs: []
  type: TYPE_TB
  zh: '| 多次运行。                  | 模型   | 运行次数   | 数据数量   | 百分比   |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-1999
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Hidden                          | Weights | Correct     |             |           |'
  id: totrans-2000
  prefs: []
  type: TYPE_TB
  zh: '| 隐藏                          | 权重 | 正确率     |             |           |'
- en: '| Single-Layer Perceptron         | -       | 11          | 33          |           |'
  id: totrans-2001
  prefs: []
  type: TYPE_TB
  zh: '| 单层感知器         | -       | 11          | 33          |           |'
- en: '| Multilayer Perceptron [19]      | 11      | 253         | 44          |           |'
  id: totrans-2002
  prefs: []
  type: TYPE_TB
  zh: '| 多层感知器 [19]      | 11      | 253         | 44          |           |'
- en: '| 22                              | 495     | 45          |             |           |'
  id: totrans-2003
  prefs: []
  type: TYPE_TB
  zh: '| 22                              | 495     | 45          |             |           |'
- en: '| 88                              | 1,947   | 51          |             |           |'
  id: totrans-2004
  prefs: []
  type: TYPE_TB
  zh: '| 88                              | 1,947   | 51          |             |           |'
- en: '| Multilayer Perceptron [12]      | 5       | 121         | 50.1             |           |'
  id: totrans-2005
  prefs: []
  type: TYPE_TB
  zh: '| 多层感知器 [12]      | 5       | 121         | 50.1             |           |'
- en: '| (with renormalization)          | 10      | 231         | 57.5             |           |'
  id: totrans-2006
  prefs: []
  type: TYPE_TB
  zh: '| （带重正化）          | 10      | 231         | 57.5             |           |'
- en: '| 20                              | 451     | 50.6             |             |           |'
  id: totrans-2007
  prefs: []
  type: TYPE_TB
  zh: '| 20                              | 451     | 50.6             |             |           |'
- en: '| Stochastic Network [5]          | 8       | 297         | 54             |           |'
  id: totrans-2008
  prefs: []
  type: TYPE_TB
  zh: '| 随机网络 [5]          | 8       | 297         | 54             |           |'
- en: '| (FF-R classifier)               | 16      | 473         | 56             |           |'
  id: totrans-2009
  prefs: []
  type: TYPE_TB
  zh: '| （FF-R分类器）               | 16      | 473         | 56             |           |'
- en: '| 32                              | 825     | 57.9             |             |           |'
  id: totrans-2010
  prefs: []
  type: TYPE_TB
  zh: '| 32                              | 825     | 57.9             |             |           |'
- en: '| Radial Basis Function           | 88      | 1,936       | 48          |           |'
  id: totrans-2011
  prefs: []
  type: TYPE_TB
  zh: '| 径向基函数           | 88      | 1,936       | 48          |           |'
- en: '| 528                             | 11,616  | 53          |             |           |'
  id: totrans-2012
  prefs: []
  type: TYPE_TB
  zh: '| 528                             | 11,616  | 53          |             |           |'
- en: '| Gaussian Node Network           | 11      | 253         | 47          |           |'
  id: totrans-2013
  prefs: []
  type: TYPE_TB
  zh: '| 高斯节点网络           | 11      | 253         | 47          |           |'
- en: '| 22                              | 495     | 54          |             |           |'
  id: totrans-2014
  prefs: []
  type: TYPE_TB
  zh: '| 22                              | 495     | 54          |             |           |'
- en: '| 88                              | 1,947   | 53          |             |           |'
  id: totrans-2015
  prefs: []
  type: TYPE_TB
  zh: '| 88                              | 1,947   | 53          |             |           |'
- en: '| 528                             | 11,627  | 55          |             |           |'
  id: totrans-2016
  prefs: []
  type: TYPE_TB
  zh: '| 528                             | 11,627  | 55          |             |           |'
- en: '| Square Node Network             | 11      | 253         | 50          |           |'
  id: totrans-2017
  prefs: []
  type: TYPE_TB
  zh: '| 方形节点网络             | 11      | 253         | 50          |           |'
- en: '| (not an SMLP)                   | 22      | 495         | 51          |           |'
  id: totrans-2018
  prefs: []
  type: TYPE_TB
  zh: '| （不是SMLP）                   | 22      | 495         | 51          |           |'
- en: '| 88                              | 1,947   | 55          |             |           |'
  id: totrans-2019
  prefs: []
  type: TYPE_TB
  zh: '| 88                              | 1,947   | 55          |             |           |'
- en: '| Modified Kanerva Model          | 88      | 968         | 43          |           |'
  id: totrans-2020
  prefs: []
  type: TYPE_TB
  zh: '| 修改的卡内瓦模型          | 88      | 968         | 43          |           |'
- en: '| 528                             | 5808    | 50          |             |           |'
  id: totrans-2021
  prefs: []
  type: TYPE_TB
  zh: '| 528                             | 5808    | 50          |             |           |'
- en: '| Local Approximation             | 2       | 5808        | 50.0        |           |'
  id: totrans-2022
  prefs: []
  type: TYPE_TB
  zh: '| 局部近似             | 2       | 5808        | 50.0        |           |'
- en: '|                                 | 3       | 5808        | 52.8        |           |'
  id: totrans-2023
  prefs: []
  type: TYPE_TB
  zh: '|                                 | 3       | 5808        | 52.8        |           |'
- en: '|                                 | 5       | 5808        | 53.0        |           |'
  id: totrans-2024
  prefs: []
  type: TYPE_TB
  zh: '|                                 | 5       | 5808        | 53.0        |           |'
- en: '| 10                              | 5808    | 48.3        |             |           |'
  id: totrans-2025
  prefs: []
  type: TYPE_TB
  zh: '| 10                              | 5808    | 48.3        |             |           |'
- en: '| 20                              | 5808    | 45.0        |             |           |'
  id: totrans-2026
  prefs: []
  type: TYPE_TB
  zh: '| 20                              | 5808    | 45.0        |             |           |'
- en: '| Nearest Neighbor                | -       | (5,808)     | 56          |           |'
  id: totrans-2027
  prefs: []
  type: TYPE_TB
  zh: '| 最近邻居                        | -       | (5,808)     | 56          |           |'
- en: '| Linear Discriminant Analysis    | -       | 715         | 44          |           |'
  id: totrans-2028
  prefs: []
  type: TYPE_TB
  zh: '| 线性判别分析    | -       | 715         | 44          |           |'
- en: '| Softmax                         | -       | -?-         | 33          |           |'
  id: totrans-2029
  prefs: []
  type: TYPE_TB
  zh: '| Softmax                         | -       | -?-         | 33          |           |'
- en: '| Quadratic Discriminant Analysis | -       | -?-         | 47          |           |'
  id: totrans-2030
  prefs: []
  type: TYPE_TB
  zh: '| 二次判别分析 | -       | -?-         | 47          |           |'
- en: '| CART                            | -       | -?-         | 44          |           |'
  id: totrans-2031
  prefs: []
  type: TYPE_TB
  zh: '| CART                            | -       | -?-         | 44          |           |'
- en: '| CART (linear comb. splits)      | -       | -?-         | 46          |           |'
  id: totrans-2032
  prefs: []
  type: TYPE_TB
  zh: '| CART（线性组合划分）            | -       | -?-         | 46          |           |'
- en: '| FDA / BRUTO                     | -       | -?-         | 56          |           |'
  id: totrans-2033
  prefs: []
  type: TYPE_TB
  zh: '| FDA / BRUTO                     | -       | -?-         | 56          |           |'
- en: '| Softmax / BRUTO                 | -       | -?-         | 50          |           |'
  id: totrans-2034
  prefs: []
  type: TYPE_TB
  zh: '| Softmax / BRUTO                 | -       | -?-         | 50          |           |'
- en: '| FDA / MARS (degree 1)           | -       | -?-         | 55          |           |'
  id: totrans-2035
  prefs: []
  type: TYPE_TB
  zh: '| FDA / MARS（1级）               | -       | -?-         | 55          |           |'
- en: '| FDA / MARS (degree 2)           | -       | -?-         | 58          |           |'
  id: totrans-2036
  prefs: []
  type: TYPE_TB
  zh: '| FDA / MARS（2级）               | -       | -?-         | 58          |           |'
- en: '| Softmax / MARS (degree 1)       | -       | -?-         | 52          |           |'
  id: totrans-2037
  prefs: []
  type: TYPE_TB
  zh: '| Softmax / MARS（1级）           | -       | -?-         | 52          |           |'
- en: '| Softmax / MARS (degree 2)       | -       | -?-         | 50          |           |'
  id: totrans-2038
  prefs: []
  type: TYPE_TB
  zh: '| Softmax / MARS（2级）           | -       | -?-         | 50          |           |'
- en: '| LOCOCODE / Backprop             | 11      | 473         | 58             |           |'
  id: totrans-2039
  prefs: []
  type: TYPE_TB
  zh: '| LOCOCODE / 反向传播             | 11      | 473         | 58             |           |'
- en: '| (30 inputs) DANN                | -       | -?-         | 61.7        |           |'
  id: totrans-2040
  prefs: []
  type: TYPE_TB
  zh: '| （30 个输入） DANN              | -       | -?-         | 61.7        |           |'
- en: '![155_image_1.png](155_image_1.png)'
  id: totrans-2041
  prefs: []
  type: TYPE_IMG
  zh: '![155_image_1.png](155_image_1.png)'
- en: '![155_image_2.png](155_image_2.png)'
  id: totrans-2042
  prefs: []
  type: TYPE_IMG
  zh: '![155_image_2.png](155_image_2.png)'
- en: '![155_image_0.png](155_image_0.png)'
  id: totrans-2043
  prefs: []
  type: TYPE_IMG
  zh: '![155_image_0.png](155_image_0.png)'
- en: 'Fig. 7.4. The results from Table 7.3 shown in a graphical format: (a) the average
    results shown for all models types; (b) the average of the SMLP and MLP models
    with error bars shown; (c) the average of the SMLP and RBFN models with error
    bars shown'
  id: totrans-2044
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4. 表 7.3 的结果以图形格式显示：（a）所有模型类型的平均结果；（b）SMLP 和 MLP 模型的平均值，并显示误差条；（c）SMLP 和
    RBFN 模型的平均值，并显示误差条。
- en: '| Model          | # of       | # of   | % Correct % Correct % Correct)   |
    Standard   |       |      |'
  id: totrans-2045
  prefs: []
  type: TYPE_TB
  zh: '| 模型          | # of       | # of   | % 正确 % 正确 % 正确）       | 标准   |       |      |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-2046
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Hidden Weights | (Cheating) | (Best) | (Average)                        |
    Deviation  |       |      |'
  id: totrans-2047
  prefs: []
  type: TYPE_TB
  zh: '| 隐藏权重 | （作弊） | （最佳） | （平均）                        | 偏差  |       |      |'
- en: '| MLP            | 11         | 253    | 53.24                            |
    51.08      | 44.53 | 3.34 |'
  id: totrans-2048
  prefs: []
  type: TYPE_TB
  zh: '| MLP            | 11         | 253    | 53.24                            |
    51.08      | 44.53 | 3.34 |'
- en: '| (trained)      | 44         | 979    | 58.00                            |
    54.76      | 49.54 | 2.55 |'
  id: totrans-2049
  prefs: []
  type: TYPE_TB
  zh: '| （训练过的）      | 44         | 979    | 58.00                            | 54.76      |
    49.54 | 2.55 |'
- en: '| 88             | 1947       | 57.57  | 57.14                            |
    50.67      | 3.69  |      |'
  id: totrans-2050
  prefs: []
  type: TYPE_TB
  zh: '| 88             | 1947       | 57.57  | 57.14                            |
    50.67      | 3.69  |      |'
- en: '| SMLP           | 11         | 363    | 63.63                            |
    60.82      | 53.25 | 5.02 |'
  id: totrans-2051
  prefs: []
  type: TYPE_TB
  zh: '| SMLP           | 11         | 363    | 63.63                            |
    60.82      | 53.25 | 5.02 |'
- en: '| (trained)      | 22         | 715    | 64.93                            |
    63.63      | 55.11 | 3.31 |'
  id: totrans-2052
  prefs: []
  type: TYPE_TB
  zh: '| （训练过的）      | 22         | 715    | 64.93                            | 63.63      |
    55.11 | 3.31 |'
- en: '| 33             | 1067       | 65.15  | 65.15                            |
    56.54      | 3.92  |      |'
  id: totrans-2053
  prefs: []
  type: TYPE_TB
  zh: '| 33             | 1067       | 65.15  | 65.15                            |
    56.54      | 3.92  |      |'
- en: '| 44             | 1419       | 66.66  | 65.15                            |
    58.50      | 2.51  |      |'
  id: totrans-2054
  prefs: []
  type: TYPE_TB
  zh: '| 44             | 1419       | 66.66  | 65.15                            |
    58.50      | 2.51  |      |'
- en: '| RBFN           | 11         | 253    | ——                               |
    56.92      | 52.11 | 2.64 |'
  id: totrans-2055
  prefs: []
  type: TYPE_TB
  zh: '| RBFN           | 11         | 253    | ——                               |
    56.92      | 52.11 | 2.64 |'
- en: '| (solved)       | 22         | 495    | ——                               |
    63.20      | 57.36 | 2.89 |'
  id: totrans-2056
  prefs: []
  type: TYPE_TB
  zh: '| (solved)       | 22         | 495    | ——                               |
    63.20      | 57.36 | 2.89 |'
- en: '| 33             | 737        | ——     | 66.88                            |
    59.03      | 3.45  |      |'
  id: totrans-2057
  prefs: []
  type: TYPE_TB
  zh: '| 33             | 737        | ——     | 66.88                            |
    59.03      | 3.45  |      |'
- en: '| 44             | 979        | ——     | 67.53                            |
    61.38      | 2.66  |      |'
  id: totrans-2058
  prefs: []
  type: TYPE_TB
  zh: '| 44             | 979        | ——     | 67.53                            |
    61.38      | 2.66  |      |'
- en: '| 66             | 1463       | ——     | 65.80                            |
    61.79      | 2.23  |      |'
  id: totrans-2059
  prefs: []
  type: TYPE_TB
  zh: '| 66             | 1463       | ——     | 65.80                            |
    61.79      | 2.23  |      |'
- en: '| 88             | 1947       | ——     | 67.09                            |
    61.58      | 2.38  |      |'
  id: totrans-2060
  prefs: []
  type: TYPE_TB
  zh: '| 88             | 1947       | ——     | 67.09                            |
    61.58      | 2.38  |      |'
- en: '| SMLP           | 11         | 363    | ——                               |
    58.87      | 54.14 | 2.79 |'
  id: totrans-2061
  prefs: []
  type: TYPE_TB
  zh: '| SMLP           | 11         | 363    | ——                               |
    58.87      | 54.14 | 2.79 |'
- en: '| (solved)       | 22         | 715    | ——                               |
    63.63      | 56.68 | 3.23 |'
  id: totrans-2062
  prefs: []
  type: TYPE_TB
  zh: '| (solved)       | 22         | 715    | ——                               |
    63.63      | 56.68 | 3.23 |'
- en: '| 33             | 1067       | ——     | 63.85                            |
    57.17      | 3.09  |      |'
  id: totrans-2063
  prefs: []
  type: TYPE_TB
  zh: '| 33             | 1067       | ——     | 63.85                            |
    57.17      | 3.09  |      |'
- en: '| 44             | 1419       | ——     | 67.31                            |
    59.41      | 2.89  |      |'
  id: totrans-2064
  prefs: []
  type: TYPE_TB
  zh: '| 44             | 1419       | ——     | 67.31                            |
    59.41      | 2.89  |      |'
- en: '| 66             | 2123       | ——     | 68.39                            |
    60.36      | 3.04  |      |'
  id: totrans-2065
  prefs: []
  type: TYPE_TB
  zh: '| 66             | 2123       | ——     | 68.39                            |
    60.36      | 3.04  |      |'
- en: '| 88             | 2827       | ——     | 67.09                            |
    60.30      | 2.57  |      |'
  id: totrans-2066
  prefs: []
  type: TYPE_TB
  zh: '| 88             | 2827       | ——     | 67.09                            |
    60.30      | 2.57  |      |'
- en: '| SMLP           | 11         | 363    | 66.66                            |
    63.85      | 57.19 | 2.98 |'
  id: totrans-2067
  prefs: []
  type: TYPE_TB
  zh: '| SMLP           | 11         | 363    | 66.66                            |
    63.85      | 57.19 | 2.98 |'
- en: '| (hybrid)       | 22         | 715    | 66.88                            |
    63.41      | 59.22 | 2.50 |'
  id: totrans-2068
  prefs: []
  type: TYPE_TB
  zh: '| (hybrid)       | 22         | 715    | 66.88                            |
    63.41      | 59.22 | 2.50 |'
- en: '| 33             | 1067       | 66.45  | 64.71                            |
    59.64      | 2.81  |      |'
  id: totrans-2069
  prefs: []
  type: TYPE_TB
  zh: '| 33             | 1067       | 66.45  | 64.71                            |
    59.64      | 2.81  |      |'
- en: '| 44             | 1419       | 68.18  | 66.88                            |
    60.59      | 2.74  |      |'
  id: totrans-2070
  prefs: []
  type: TYPE_TB
  zh: '| 44             | 1419       | 68.18  | 66.88                            |
    60.59      | 2.74  |      |'
- en: 'Table 7.3. Results from this study: All averages are computed from 50 trials
    with any result less than 33% (the score of a perceptron) being discarded as non-convergent.
    See the text for an explanation of the terms "Cheating," "Best," "and "Average".'
  id: totrans-2071
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.3. 本研究结果：所有平均值是从50次试验中计算的，任何结果低于33%（感知器的分数）均被丢弃，视为不收敛。有关术语“作弊”、“最佳”和“平均”的解释，请参见正文。
- en: final test score achieved from the 50 runs, while the "% Correct (Cheating)"
    is the best test score achieved at any time during the training by any of the
    models in the fifty runs. Since the "solved" models have their weights computed
    in a single step, the "cheating" score only has meaning for the iterative techniques.
    One way of interpreting the "cheating" score is that it is the best score that
    could be achieved if one had a perfect cross validation set to use for the purpose
    of early stopping.
  id: totrans-2072
  prefs: []
  type: TYPE_NORMAL
  zh: 从50次运行中获得的最终测试分数，而“% 正确（作弊）”是任何模型在五十次运行中训练期间任何时候获得的最佳测试分数。由于“已解决”模型的权重是在一步中计算的，因此“作弊”分数仅对迭代技术有意义。“作弊”分数的一种解释是，如果有一个完美的交叉验证集可用于提前停止，那么可以达到的最佳分数。
- en: Some of the information in Table 7.3 is graphically summarized in Figure 7.3.3
    and can, therefore, be better appreciated. In every case the SMLPs and the RBFNs
    outperform the MLPs by a statistically significant margin. However, the difference
    between the SMLPs and the RBFNs is much narrower, with the RBFNs and the hybrid
    SMLPs being nearly identical performance-wise. Also note that the hybrid training
    scheme appears to offer some improvement over both the trained and the solved
    SMLPs.
  id: totrans-2073
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.3中的一些信息在图 7.3.3中以图形方式进行了总结，因此更容易理解。在每种情况下，SMLP和RBFN的表现均显著优于MLP。然而，SMLP和RBFN之间的差异要小得多，RBFN和混合SMLP在性能上几乎相同。同时请注意，混合训练方案似乎在训练和已解决的SMLP上都有所改进。
- en: '![157_image_0.png](157_image_0.png)'
  id: totrans-2074
  prefs: []
  type: TYPE_IMG
  zh: '![157_image_0.png](157_image_0.png)'
- en: Fig. 7.5. A "quasi-Gaussian" activation function as a affine transformed sigmoidal
  id: totrans-2075
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5. 一个“准高斯”激活函数，作为仿射变换的sigmoidal
- en: 7.4 Theoretical Justification
  id: totrans-2076
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 理论依据
- en: Given the context of the numerical experiments from the previous section, we
    are now ready to see how an SMLP can be thought of as a "radially extended" version
    of an MLP. In this section, I will rewrite Equation 7.1 into a form that is equivalent
    to an RBFN; thus, we will see how it is possible for an SMLP to almost perfectly
    approximate an RBFN.
  id: totrans-2077
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于上一节的数值实验背景，我们现在准备看看 SMLP 如何被视为 MLP 的“径向扩展”版本。在本节中，我将把方程 7.1 重写为与 RBFN 等价的形式；因此，我们将看到
    SMLP 如何几乎完美地逼近 RBFN。
- en: The first step is to more closely examine the sigmoidal activation function.
  id: totrans-2078
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是更仔细地检查 sigmoid 激活函数。
- en: 'Let sigmoid(x)=1/(1 + exp(−x)) and gauss(x) = exp(−x2). We can define a quasi-Gaussian
    function as:'
  id: totrans-2079
  prefs: []
  type: TYPE_NORMAL
  zh: 设 sigmoid(x)=1/(1 + exp(−x)) 且 gauss(x) = exp(−x2)。我们可以定义一个准高斯函数为：
- en: $$\mathrm{q}(x)=2-2/(\operatorname{gauss}(x)+1)=2-2\operatorname{sigmoid}(x^{2}).$$
  id: totrans-2080
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{q}(x)=2-2/(\operatorname{gauss}(x)+1)=2-2\operatorname{sigmoid}(x^{2}).$$
- en: $$(7.5)$$
  id: totrans-2081
  prefs: []
  type: TYPE_NORMAL
  zh: $$(7.5)$$
- en: This means that a local kernel function can be formed from an affine transformation
    of a sigmoid whose input has been squared.
  id: totrans-2082
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着可以通过对平方输入的 sigmoid 进行仿射变换来形成局部核函数。
- en: Figure 7.5 shows how the quasi-Gaussian function relates to the true Gaussian
    function. Both functions are unimodal and exponentially decay in both directions.
    Moreover, a similar transformation can be applied to a hyperbolic tangent function;
    hence, it really doesn't matter which of the two common sigmoid functions are
    used as either can be transformed into a basis function.
  id: totrans-2083
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 显示了准高斯函数与真实高斯函数之间的关系。两者都是单峰的，并且在两个方向上都呈指数衰减。此外，可以对双曲正切函数应用类似的变换；因此，使用这两种常见的
    sigmoid 函数中的任何一种都没有关系，因为它们都可以转化为基函数。
- en: Since a basis function has a center and a width, we want to be able to form
    local features of arbitrary size at an arbitrary location. Typically, this means
    that a basis function incorporates a distance measure such as Euclidean distance.
  id: totrans-2084
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基函数具有中心和宽度，我们希望能够在任意位置形成任意大小的局部特征。通常，这意味着基函数包含诸如欧几里得距离等距离度量。
- en: 'With a center denoted by ci and a width proportional to σ, we can rewrite a
    normalized Euclidean distance function as follows:'
  id: totrans-2085
  prefs: []
  type: TYPE_NORMAL
  zh: 以中心 ci 和与 σ 成比例的宽度，我们可以将归一化的欧几里得距离函数重写如下：
- en: "1 σ2i ||x − ci||2 = 1σ2i (x · x − 2ci · x + ci · ci) = \t− 2σ2i ci · x + \t\
    \ 1 σ2i x · x + \t 1 σ2i ci · ci  \t− 2σ2i cijxj +- k \t 1 σ2i x2k + \t 1 σ2i\
    \ ci · ci (7.6) = - j"
  id: totrans-2086
  prefs: []
  type: TYPE_NORMAL
  zh: "1 σ2i ||x − ci||2 = 1σ2i (x · x − 2ci · x + ci · ci) = \t− 2σ2i ci · x + \t\
    \ 1 σ2i x · x + \t 1 σ2i ci · ci  \t− 2σ2i cijxj +- k \t 1 σ2i x2k + \t 1 σ2i\
    \ ci · ci (7.6) = - j"
- en: Thus, the equation
  id: totrans-2087
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该方程
- en: $$2-2\,{\rm sigmoid}\left(\sum_{j}u_{ij}x_{j}+\sum_{k}v_{ik}x_{k}^{2}+a_{i},\right)\tag{7.7}$$
  id: totrans-2088
  prefs: []
  type: TYPE_NORMAL
  zh: $$2-2\,{\rm sigmoid}\left(\sum_{j}u_{ij}x_{j}+\sum_{k}v_{ik}x_{k}^{2}+a_{i},\right)\tag{7.7}$$
- en: looks a lot like a radial basis function. By comparing Equation 7.6 to Equation
    7.7 it is trivial to set the uij , vij , and ai terms in such a way that a local
    "bump" is placed at a specific location with a specific width. This means that
    a single hidden node in an SMLP network can form a local feature in an input of
    any dimension.
  id: totrans-2089
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来很像一个径向基函数。通过将方程 7.6 与方程 7.7 进行比较，可以轻松地设置 uij、vij 和 ai 项，以便在特定位置放置特定宽度的局部“隆起”。这意味着
    SMLP 网络中的一个隐藏节点可以在任意维度的输入中形成局部特征。
- en: By way of comparison, Lapedes and Farber [10, 11] similarly constructed local
    features with standard MLPs. However, in a d-dimensional input space, one would
    need an MLP with two hidden layers, 2d hidden nodes in the first hidden layers,
    and another hidden node in the second hidden layer, just to form a single local
    "bump".
  id: totrans-2090
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，Lapedes 和 Farber [10, 11] 也同样使用标准 MLP 构建了局部特征。然而，在 d 维输入空间中，需要一个具有两个隐藏层的
    MLP，第一隐藏层中有 2d 个隐藏节点，以及第二隐藏层中的另一个隐藏节点，仅用于形成单个局部“隆起”。
- en: This simple analysis shows that local features are exceptionally easy to form
    in an SMLP but are potentially very difficult to form in an MLP. As mentioned
    in Section 7.3.3, it is possible to exploit the similarity between SMLPs and RBFNs
    and "solve" the weights in an SMLP with a non-iterative procedure. The first step
    is to choose a set of basis centers that can be determined by sub-sampling or
    clustering the input-space of the training data. After the centers are chosen,
    the nearest neighbor of each center with respect to the other centers can be calculated.
    These distances can be used as the widths of the basis centers. Next, the centers
    ci and widths σi can be plugged into Equation 7.6 to determine the values of the
    uij , vik and ai weights. Finally, the linear weights in the SMLP, wi and b from
    Equation 7.1, can be solved for exactly by using a matrix pseudoinverse procedure.
  id: totrans-2091
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的分析表明，局部特征在SMLP中异常容易形成，但在MLP中可能非常困难。如第7.3.3节所述，可以利用SMLP和RBFN之间的相似性，通过非迭代程序“解决”SMLP中的权重。第一步是选择一组基中心，这可以通过对训练数据输入空间的子采样或聚类来确定。在选择中心后，可以计算每个中心相对于其他中心的最近邻。可以将这些距离用作基中心的宽度。接下来，可以将中心ci和宽度σi代入方程7.6，以确定uij、vik和ai权重的值。最后，可以通过使用矩阵伪逆程序精确解决SMLP中的线性权重wi和方程7.1中的b。
- en: Thus, one can train an SMLP as one would an MLP or one could solve an SMLP as
    one would an RBFN. It is also possible to combine the approaches and let the solved
    weights be the initial weights for a training procedure. Using the procedure to
    solve the weights can sometimes cut the computational overhead for computing the
    weight by orders of magnitude compared to typical training methods. Moreover,
    as was found in the numerical experiments in Section 7.3.3, solutions found with
    this hybrid scheme may easily exceed the quality of solutions found with more
    traditional approaches.
  id: totrans-2092
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以像训练MLP一样训练SMLP，或者像解决RBFN一样解决SMLP。也可以结合这两种方法，让解决后的权重成为训练程序的初始权重。使用该程序解决权重有时可以大幅度减少计算权重的计算开销，与典型的训练方法相比。此外，如第7.3.3节中的数值实验所发现，使用这种混合方案找到的解决方案的质量可能轻松超越传统方法。
- en: '![159_image_0.png](159_image_0.png)'
  id: totrans-2093
  prefs: []
  type: TYPE_IMG
  zh: '![159_image_0.png](159_image_0.png)'
- en: Fig. 7.6. Several difference types of surfaces and decision boundaries that
    can be formed by a single SMLP node
  id: totrans-2094
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6. 可以由单个SMLP节点形成的几种不同类型的表面和决策边界
- en: '![160_image_0.png](160_image_0.png)'
  id: totrans-2095
  prefs: []
  type: TYPE_IMG
  zh: '![160_image_0.png](160_image_0.png)'
- en: Fig. 7.7. Output response surfaces of the 10 hidden nodes in the SMLP network
    that solved the two spiral problem
  id: totrans-2096
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7. 解决两螺旋问题的SMLP网络中10个隐藏节点的输出响应表面
- en: 7.5 Intuitive And Topological Justification
  id: totrans-2097
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 直观和拓扑证明
- en: While the previous analysis shows that an SMLP can efficiently approximate an
    RBFN, the transformation from an SMLP into an RBFN examined only a single special
    case of the type of features that can be formed by an SMLP node. In fact, a single
    SMLP node can form many other types of features besides hyper-sigmoids and local
    bumps. To show that this is indeed the case, one only needs to compose a two-dimensional
    diagonal quadratic equation into a sigmoidal function to see what type of surfaces
    are possible.
  id: totrans-2098
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管之前的分析表明，SMLP可以有效地近似RBFN，但将SMLP转换为RBFN仅检查了SMLP节点可以形成的特征类型的一个特殊案例。事实上，单个SMLP节点可以形成许多其他类型的特征，除了超
    sigmoid 和局部波动。为了证明这一点，只需将二维对角二次方程组合成 sigmoid 函数，以查看可能的表面类型。
- en: Figure 7.6 shows some familiar surfaces and decision boundaries that can be
    formed with a single SMLP node. As expected, hyper-sigmoids and bumps can be formed.
    What is interesting, however, is that there are many types of SMLP features that
    are neither local nor global. For example, a ridge, wedge or saddle may look like
    a local or global feature if it is projected onto one lower dimension; however,
    whether this projection is local or global depends on the subspace that the projection
    is formed.
  id: totrans-2099
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6展示了一些可以通过单个SMLP节点形成的熟悉表面和决策边界。如预期，超 sigmoid 和波动可以形成。然而有趣的是，还有许多类型的SMLP特征既不是局部也不是全局。例如，脊、楔或鞍在投影到一个较低维度时可能看起来像局部或全局特征；然而，这种投影是否局部或全局取决于投影形成的子空间。
- en: How important is it for an architecture to be able to form these features? The
    very fact that these types of boundaries and surfaces have names means that they
    are important enough that one may need a type of model that can efficiently form
    them. However, if we reexamine the two-spirals problem from Section 7.3.2 it is
    possible to dissect the decision boundary formed by the SMLP (shown in Figure
    7.3) to see how the surface was formed. What it truly interesting is that radial
    boundaries, wedges, and a sigmoid were all crucial to forming the entire decision
    surface. If the SMLP lacked the ability to form any of these features, then it
    is easily possible that the hidden node requirements for this problem would explode.
  id: totrans-2100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个架构能够形成这些特征的重要性有多大？这些类型的边界和表面都有名称，说明它们的重要性足以需要一种能有效形成它们的模型。然而，如果我们重新审视第7.3.2节的双螺旋问题，可以对SMLP形成的决策边界（见图7.3）进行剖析，以了解表面的形成过程。真正有趣的是，径向边界、楔形和S形曲线对于形成整个决策表面都是至关重要的。如果SMLP缺乏形成这些特征的能力，那么隐藏节点的需求可能会急剧增加。
- en: There is, however, one caveat with the variety of features that can be formed
    with a single SMLP node. The wedge, ridge, ellipse and saddle structures shown
    in Figure 7.6 must always be aligned with one of the input axes. In other words,
    it is impossible to make a ridge that would run parallel to the line defined by
    x = y. We can see that this is true by noting that (x−y)2 has the term −2xy in
    its expansion, which means that the quadratic form is non-diagonal. In general,
    in order to have the ability to rotate all of the features in Figure 7.6, one
    would need the full quadratic form, thus requiring a higher-order network instead
    of the SMLP. Hence, while eliminating the off-diagonal terms in a higher-order
    network saves a considerable number of weights, there is a cost in the types of
    features that can be formed.
  id: totrans-2101
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，单个SMLP节点可以形成的特征种类多样，但有一个注意事项。图7.6中显示的楔形、脊、椭圆和鞍形结构必须始终与输入轴之一对齐。换句话说，不可能形成与x
    = y定义的直线平行的脊。我们可以通过注意到(x−y)²在展开中有项−2xy来证明这一点，这意味着二次形式是非对角的。一般来说，要能够旋转图7.6中的所有特征，需要完整的二次形式，因此需要比SMLP更高阶的网络。因此，虽然在高阶网络中消除非对角项可以节省大量权重，但在可形成特征的类型上会有所损失。
- en: 7.6 Conclusions
  id: totrans-2102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.6 结论
- en: We have seen that there are problems whose solutions require features of both
    local and global scope. MLPs excel at forming global features but have a difficult
    time forming local features. RBFNs are exactly the opposite. The SMLP architecture
    can efficiently form both types of features with only a small penalty in the number
    of weights. However, the increase in the number of weights is more than compensated
    for by improvements in the network's ability to form features. This often results
    in simpler networks with fewer weights that can learn much faster and approximate
    more accurately.
  id: totrans-2103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，有些问题的解决方案需要局部和全局特征。MLP在形成全局特征方面表现出色，但在形成局部特征时困难重重。RBFN则正好相反。SMLP架构能够高效地形成这两种特征，同时权重的增加很小。然而，权重数量的增加被网络在形成特征能力上的提升所补偿。这通常导致更简单的网络，权重更少，学习速度更快，近似更准确。
- en: For the two main numerical studies in this work, it was found that the SMLP
  id: totrans-2104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本研究中的两个主要数值研究，发现SMLP
- en: architecture performed as well or better than the best known techniques for
    the two-spirals problem and the Deterding vowel recognition data. Moreover, these
    results are strengthened by the fact that the average performance of the SMLP
    was often superior to the best known results for the other techniques.
  id: totrans-2105
  prefs: []
  type: TYPE_NORMAL
  zh: 架构在解决双螺旋问题和德特丁元音识别数据方面的表现与最佳已知技术相当或更好。此外，SMLP的平均表现通常优于其他技术的最佳已知结果，这一点进一步加强了这些结果。
- en: It was also found that the dual nature of the SMLP can be exploited in the form
    of hybrid algorithms. SMLPs can be "solved" like an RBFN, trained like an MLP,
    or both. It is also noteworthy that the nonlinear weights in an SMLP are only
    "moderately" nonlinear. For example, gradient-based learning algorithms can be
    used on RBFNs but it is known that the high degree of nonlinearity found in the
    weights which correspond to centers and widths can often make gradient training
    very slow for RBFNs when the nonlinear weight are included. Similarly, since MLPs
    require two hidden layers of nodes to form local features efficiently, the first
    layer of weights in an MLP are exceedingly nonlinear because they are eventually
    passed through two layers of nonlinear nodes. Counter to this, the nonlinear nodes
    in an SMLP are only passed through a single layer of nonlinear nodes. Although
    it is unproven at this time, it seems like a reasonable conjecture that SMLP networks
    may be intrinsically better conditioned for gradient-based learning of local "bumps"
    than MLPs with two hidden layers. Acknowledgements. I thank Frans Coetzee, Chris
    Darken, Lee Giles, Jenny Orr, Ray Watrous, and the anonymous reviewers for many
    helpful comments and discussions.
  id: totrans-2106
  prefs: []
  type: TYPE_NORMAL
  zh: 还发现，SMLP 的双重性质可以以混合算法的形式加以利用。SMLP 可以像 RBFN 一样“求解”，像 MLP 一样训练，或两者皆可。同样值得注意的是，SMLP
    中的非线性权重仅是“适度”非线性。例如，基于梯度的学习算法可以应用于 RBFN，但已知与中心和宽度对应的权重中的高度非线性常常会使得在包含非线性权重时 RBFN
    的梯度训练非常缓慢。类似地，由于 MLP 需要两个隐藏层的节点以有效形成局部特征，MLP 中的第一层权重极为非线性，因为它们最终会经过两个非线性节点层。与此相反，SMLP
    中的非线性节点仅经过一层非线性节点。虽然目前尚未证明，但可以合理推测，SMLP 网络在局部“波动”的基于梯度的学习方面可能内在地比具有两个隐藏层的 MLP
    更具条件优越性。致谢。我感谢 Frans Coetzee、Chris Darken、Lee Giles、Jenny Orr、Ray Watrous 及匿名审稿人提供的许多有益意见和讨论。
- en: '[1] Casdagli, M.: Nonlinear prediction of chaotic time series. Physica D 35,
    335–356'
  id: totrans-2107
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Casdagli, M.: 混沌时间序列的非线性预测。物理学 D 35，335–356'
- en: (1989)
  id: totrans-2108
  prefs: []
  type: TYPE_NORMAL
  zh: (1989)
- en: '[2] Deterding, D.H.: Speaker Normalisation for Automatic Speech Recognition.
    PhD'
  id: totrans-2109
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Deterding, D.H.: 自动语音识别的说话者标准化。博士学位'
- en: thesis, University of Cambridge (1989)
  id: totrans-2110
  prefs: []
  type: TYPE_NORMAL
  zh: 论文，剑桥大学 (1989)
- en: '[3] Fahlman, S.E.: Faster-learning variations on back-propagation: An empirical
    study. In: Proceedings of the 1988 Connectionist Models Summer School. Morgan
    Kaufmann (1988)'
  id: totrans-2111
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Fahlman, S.E.: 关于反向传播的快速学习变体：一项实证研究。在：1988 年连接主义模型暑期学校会议录。摩根·考夫曼 (1988)'
- en: '[4] Fahlman, S.E., Lebiere, C.: The cascade-correlation learning architecture.
    In:'
  id: totrans-2112
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Fahlman, S.E., Lebiere, C.: 级联相关学习架构。在：'
- en: Touretzky, S. (ed.) Advances in Neural Information Processing Systems, vol.
    2. Morgan Kaufmann (1990)
  id: totrans-2113
  prefs: []
  type: TYPE_NORMAL
  zh: Touretzky, S. (编) 《神经信息处理系统进展》，第2卷。摩根·考夫曼 (1990)
- en: '[5] Finke, M., Müller, K.-R.: Estimating a-posteriori probabilities using stochastic
    network models. In: Mozer, M., Smolensky, P., Touretzky, D.S., Elman, J.L., Weigend,
    A.S. (eds.) Proceedings of the 1993 Connectionist Models Summer School, pp. 324–331.
    Erlenbaum Associates, Hillsdale (1994)'
  id: totrans-2114
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Finke, M., Müller, K.-R.: 使用随机网络模型估计后验概率。在：Mozer, M., Smolensky, P., Touretzky,
    D.S., Elman, J.L., Weigend, A.S. (编) 《1993 年连接主义模型暑期学校会议录》，第 324–331 页。Erlenbaum
    Associates, Hillsdale (1994)'
- en: '[6] Hastie, T., Tibshirani, R.: Flexible discriminant analysis by optimal scoring.
    Technical report, AT&T Bell Labs, Murray Hill, New Jersey (1993)'
  id: totrans-2115
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Hastie, T., Tibshirani, R.: 通过最佳评分进行灵活的判别分析。技术报告，AT&T 贝尔实验室，穆雷山，新泽西 (1993)'
- en: '[7] Hastie, T., Tibshirani, R.: Discriminant adaptive nearest neighbor classification.'
  id: totrans-2116
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Hastie, T., Tibshirani, R.: 判别自适应最近邻分类。 '
- en: IEEE Transactions on Pattern Analysis and Machine Intelligence 18(6), 607–616
    (1996)
  id: totrans-2117
  prefs: []
  type: TYPE_NORMAL
  zh: IEEE 模式分析与机器智能杂志 18(6)，607–616 (1996)
- en: '[8] Hochreiter, S., Schmidhuber, J.: Lococode. Technical Report FKI-222-97,
    Fakultät für Informatik, Technische Universität München (1997)'
  id: totrans-2118
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Hochreiter, S., Schmidhuber, J.: Lococode. 技术报告 FKI-222-97，慕尼黑工业大学计算机系
    (1997)'
- en: '[9] Lang, K.J., Witbrock, M.J.: Learning to tell two spirals apart. In: Proceedings
    of the 1988 Connectionist Models Summer School. Morgan Kaufmann, San Francisco
    (1988)'
  id: totrans-2119
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Lang, K.J., Witbrock, M.J.: 学习区分两个螺旋。在：1988 年连接主义模型暑期学校会议录。摩根·考夫曼，旧金山 (1988)'
- en: '[10] Lapedes, A., Farber, R.: Nonlinear signal processing using neural networks:
    Prediction and system modelling. Technical Report LA-UR-87-2662, Los Alamos National
    Laboratory, Los Alamos, NM (1987)'
  id: totrans-2120
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Lapedes, A., Farber, R.: 使用神经网络的非线性信号处理：预测和系统建模。技术报告 LA-UR-87-2662，洛斯阿拉莫斯国家实验室，洛斯阿拉莫斯，NM
    (1987)'
- en: '[11] Lapedes, A., Farber, R.: How neural nets work. In: Anderson, D.Z. (ed.)
    Neural Information Processing Sysytems, pp. 442–456. American Institute of Physics,
    New York (1988)'
  id: totrans-2121
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Lapedes, A., Farber, R.: 神经网络是如何工作的。在：Anderson, D.Z. (编.) 神经信息处理系统，第442–456页。美国物理学会，纽约
    (1988)'
- en: '[12] Lawrence, S., Tsoi, A.C., Back, A.D.: Function approximation with neural
    networks and local methods: Bias, variance and smoothness. In: Bartlett, P., Burkitt,
    A., Williamson, R. (eds.) Australian Conference on Neural Networks, pp. 16–21.'
  id: totrans-2122
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Lawrence, S., Tsoi, A.C., Back, A.D.: 使用神经网络和局部方法的函数逼近：偏差、方差和平滑性。在：Bartlett,
    P., Burkitt, A., Williamson, R. (编.) 澳大利亚神经网络会议，第16–21页。'
- en: Australian National University (1996)
  id: totrans-2123
  prefs: []
  type: TYPE_NORMAL
  zh: 澳大利亚国立大学 (1996)
- en: '[13] Lee, S., Kil, R.M.: Multilayer feedforward potential function networks.
    In: IEEE'
  id: totrans-2124
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Lee, S., Kil, R.M.: 多层前馈潜能函数网络。在：IEEE'
- en: international Conference on Neural Networks, pp. 1:161–1:171. SOS Printing,
    San Diego (1988)
  id: totrans-2125
  prefs: []
  type: TYPE_NORMAL
  zh: 国际神经网络会议，第1:161–1:171页。SOS Printing, 圣地亚哥 (1988)
- en: '[14] Lee, Y.C., Doolen, G., Chen, H.H., Sun, G.Z., Maxwell, T., Lee, H.Y.,
    Giles, C.L.: Machine learning using higher order correlation networks. Physica
    D 22-D,'
  id: totrans-2126
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Lee, Y.C., Doolen, G., Chen, H.H., Sun, G.Z., Maxwell, T., Lee, H.Y.,
    Giles, C.L.: 使用高阶相关网络的机器学习。物理学 D 22-D，'
- en: 276–306 (1986)
  id: totrans-2127
  prefs: []
  type: TYPE_NORMAL
  zh: 276–306 (1986)
- en: '[15] Moody, J., Darken, C.: Learning with localized receptive fields. In: Touretsky,
    D.,'
  id: totrans-2128
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Moody, J., Darken, C.: 使用局部感受野的学习。见：Touretsky, D.,'
- en: Hinton, G., Sejnowski, T. (eds.) Proceedings of the 1988 Connectionist Models
    Summer School, Morgan Kaufmann (1988)
  id: totrans-2129
  prefs: []
  type: TYPE_NORMAL
  zh: Hinton, G., Sejnowski, T. (编.) 1988年连接主义模型暑期学校论文集，摩根·考夫曼（1988年）
- en: '[16] Moody, J., Darken, C.: Fast learning in networks of locally-tuned processing
    units.'
  id: totrans-2130
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Moody, J., Darken, C.: 在局部调谐处理单元网络中的快速学习。'
- en: Neural Computation 1, 281–294 (1989)
  id: totrans-2131
  prefs: []
  type: TYPE_NORMAL
  zh: 神经计算 1, 281–294 (1989)
- en: '[17] Niranjan, M., Fallside, F.: Neural networks and radial basis functions
    in classifying static speech patterns. Computer Speech and Language 4, 275–289
    (1990)'
  id: totrans-2132
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Niranjan, M., Fallside, F.: 在分类静态语音模式中使用神经网络和径向基函数。计算机语音与语言 4, 275–289
    (1990)'
- en: '[18] Pao, Y.H.: Adaptive Pattern Recognition and Neural Networks. Addison-Wesley
    Publishing Company, Inc., Reading (1989)'
  id: totrans-2133
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Pao, Y.H.: 自适应模式识别与神经网络。阿迪生-韦斯利出版社，阅读 (1989)'
- en: '[19] Robinson, A.J.: Dynamic Error Propagation Networks. PhD thesis, Cambridge
    University (1989)'
  id: totrans-2134
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Robinson, A.J.: 动态误差传播网络。博士论文，剑桥大学 (1989)'
- en: '[20] Rumelhart, D.E., McClelland, J.L.: the PDP Research Group. In: Parallel
    Distributed Processing: Explorations in the Microstructure of Cognition, vol.
    2. MIT'
  id: totrans-2135
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Rumelhart, D.E., McClelland, J.L.: PDP研究组。在：并行分布处理：认知微观结构的探索，第2卷。麻省理工学院'
- en: Press (1986)
  id: totrans-2136
  prefs: []
  type: TYPE_NORMAL
  zh: Press (1986)
- en: '[21] Sarle, W.: The comp.ai.neural-nets Frequently Asked Questions List (1997)'
  id: totrans-2137
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Sarle, W.: comp.ai.neural-nets 常见问题解答列表 (1997)'
- en: '[22] Schetzen, M.: The Volterra and Wiener Theories of Nonlinear Systems. John
    Wiley and Sons, New York (1980)'
  id: totrans-2138
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Schetzen, M.: 非线性系统的沃尔特拉和维纳理论。约翰·威利与儿子，纽约 (1980)'
- en: '[23] Schölkopf, B., Smola, A., Müller, K.-R.: Nonlinear component analysis
    as a kernel eigenvalue problem. Technical report, Max-Planck-Institut für biologische
    Kybernetik, 1996. Neural Computation 10(5), 1299–1319 (1998)'
  id: totrans-2139
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Schölkopf, B., Smola, A., Müller, K.-R.: 非线性成分分析作为一个核特征值问题。技术报告，马克斯·普朗克生物控制研究所，1996年。神经计算
    10(5), 1299–1319 (1998)'
- en: '[24] Volterra, V.: Theory of Functionals and of Integro-differential Equations.
    Dover'
  id: totrans-2140
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Volterra, V.: 泛函与积分微分方程的理论。多佛'
- en: (1959)
  id: totrans-2141
  prefs: []
  type: TYPE_NORMAL
  zh: (1959)
- en: '[25] Werbos, P.: Beyond Regression: New Tools for Prediction and Analysis in
    the Behavioral Sciences. PhD thesis, Harvard University (1974)'
  id: totrans-2142
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Werbos, P.: 超越回归：行为科学预测与分析的新工具。博士论文，哈佛大学 (1974)'
- en: 8 A Dozen Tricks With Multitask Learning-
  id: totrans-2143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 种多任务学习的技巧-
- en: Rich Caruana Just Research and Carnegie Mellon University, 4616 Henry Street,
    Pittsburgh, PA 15213 caruana@cs.cmu.edu http://www.cs.cmu.edu/~caruana/
  id: totrans-2144
  prefs: []
  type: TYPE_NORMAL
  zh: Rich Caruana 仅研究与卡内基梅隆大学，4616 Henry Street, Pittsburgh, PA 15213 caruana@cs.cmu.edu
    http://www.cs.cmu.edu/~caruana/
- en: Abstract. Multitask Learning is an inductive transfer method that improves generalization
    accuracy on a main task by using the information contained in the training signals
    of other *related* tasks. It does this by learning the extra tasks in parallel
    with the main task while using a shared representation; what is learned for each
    task can help other tasks be learned better. This chapter describes a dozen opportunities
    for applying multitask learning in real problems. At the end of the chapter we
    also make several suggestions for how to get the most our of multitask learning
    on real-world problems.
  id: totrans-2145
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要。多任务学习是一种归纳迁移方法，通过利用其他*相关*任务的训练信号中的信息，提升主要任务的泛化准确性。它通过与主要任务并行学习额外任务，同时使用共享表示；为每个任务所学到的知识可以帮助其他任务更好地学习。本章描述了在实际问题中应用多任务学习的十几个机会。在本章末尾，我们还提出了一些建议，以帮助你在现实问题中充分利用多任务学习。
- en: When tackling real problems, one often encounters valuable information that
    is not easily incorporated in the learning process. This chapter shows a dozen
    ways to benefit from the information that often gets ignored. The basic trick
    is to create extra tasks that get trained on the same net with the main task.
    This Multitask Learning is a form of inductive transfer1 that improves performance
    on the main task by using the information contained in the training signals of
    other *related* tasks. It does this by learning the main task in parallel with
    the extra tasks while using a shared representation; what is learned for each
    task can help other tasks be learned better.
  id: totrans-2146
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决实际问题时，常常会遇到难以纳入学习过程的有价值信息。本章展示了十几种利用这些常被忽视的信息的方法。基本技巧是创建额外任务，与主要任务在同一网络上进行训练。这种多任务学习是一种归纳迁移的形式，通过利用其他*相关*任务的训练信号中的信息来提高主要任务的性能。它通过与额外任务并行学习主要任务，同时使用共享表示；为每个任务所学到的知识可以帮助其他任务更好地学习。
- en: We use the term "task" to refer to a function that will be learned from a training
    set. We call the important task that we wish to learn better the main task. Other
    tasks whose training signals will be used by multitask learning to learn the main
    task better are the extra tasks. Often, we do not care how well extra tasks are
    learned. Their sole purpose is to help the main task be learned better. We call
    the union of the main task and the extra tasks a domain. Here we restrict ourselves
    to domains where the tasks are defined on a common set of input features, though
    some of the extra tasks may be functions of only a subset of these input features.
  id: totrans-2147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用“任务”一词来指代将从训练集学习的功能。我们称希望更好学习的重要任务为主要任务。那些其训练信号将被多任务学习用来更好学习主要任务的其他任务称为额外任务。通常，我们不关心额外任务的学习效果。它们的唯一目的是帮助主要任务更好地学习。我们称主要任务和额外任务的集合为领域。这里我们限制在任务定义在共同输入特征集上的领域，尽管一些额外任务可能仅是这些输入特征子集的函数。
- en: This chapter shows that most real-world domains present a number of opportunities
    for multitask learning (MTL). Because these opportunities are not
  id: totrans-2148
  prefs: []
  type: TYPE_NORMAL
  zh: 本章表明，大多数现实领域都存在许多多任务学习（MTL）的机会。因为这些机会并不
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-2149
  prefs: []
  type: TYPE_NORMAL
  zh: '- 先前发表在：Orr, G.B. 和 Müller, K.-R.（主编）：LNCS 1524, ISBN'
- en: 978-3-540-65311-0 (1998). 1 Inductive transfer is the process of transfering
    anything learned for one problem to help learning of other related problems.
  id: totrans-2150
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0 (1998). 1 归纳迁移是将为一个问题学习到的知识转移到帮助学习其他相关问题的过程。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    163–189, 2012.'
  id: totrans-2151
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon 等（主编）：神经网络：行业技巧，第2版，LNCS 7700，第163–189页，2012。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-2152
  prefs: []
  type: TYPE_NORMAL
  zh: -c 施普林格-弗朗克 柏林 海德堡 2012
- en: always obvious, most of the chapter is dedicated to showing different ways useful
    extra tasks arise in real problems. We demonstrate several of these opportunities
    using real data. The chapter ends with a few suggestions that help you get the
    most out of multitask learning. Some of these suggestions are so important that
    if you don't follow them, MTL can easily hurt performance on the main task instead
    of helping it.
  id: totrans-2153
  prefs: []
  type: TYPE_NORMAL
  zh: 并非总是显而易见，本章大部分内容致力于展示在实际问题中如何出现有用的额外任务。我们使用真实数据演示了其中几个机会。本章最后提供了一些建议，帮助你充分利用多任务学习。这些建议中有些非常重要，如果不遵循，它们可能会对主要任务的性能造成伤害，而不是帮助。
- en: 8.1 Introduction To Multitask Learning In Backprop Nets
  id: totrans-2154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 多任务学习导论 在反向传播网络中
- en: 'Consider the following boolean functions defined on eight bits, B1 ··· B8:'
  id: totrans-2155
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下在八个位 B1 ··· B8 上定义的布尔函数：
- en: T ask1 = B1 ∨ P arity(B2 ··· B6)
  id: totrans-2156
  prefs: []
  type: TYPE_NORMAL
  zh: T ask1 = B1 ∨ P arity(B2 ··· B6)
- en: T ask2 = ¬B1 ∨ P arity(B2 ··· B6) T ask3 = B1 ∧ P arity(B2 ··· B6) T ask4 =
    ¬B1 ∧ P arity(B2 ··· B6)
  id: totrans-2157
  prefs: []
  type: TYPE_NORMAL
  zh: T ask2 = ¬B1 ∨ P arity(B2 ··· B6) T ask3 = B1 ∧ P arity(B2 ··· B6) T ask4 =
    ¬B1 ∧ P arity(B2 ··· B6)
- en: 'where "Bi" represents the ith bit, "¬" is logical negation, "∨" is disjunction,
    "∧" is conjunction, and "P arity(B2 ··· B6)" is the parity of bits 2–6. Bits B7
    and B8 are not used by the functions. These four tasks are related in several
    ways:'
  id: totrans-2158
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 "Bi" 代表第 i 位，"¬" 是逻辑非，"∨" 是析取，"∧" 是合取，"P arity(B2 ··· B6)" 是第 2 到第 6 位的奇偶性。位
    B7 和 B8 不被这些函数使用。这四个任务在多个方面相关：
- en: '- they are all defined on the same inputs, bits B1 ··· B8;'
  id: totrans-2159
  prefs: []
  type: TYPE_NORMAL
  zh: '- 它们都在相同的输入位 B1 ··· B8 上定义；'
- en: '- they all ignore the same inputs, bits B7 and B8;'
  id: totrans-2160
  prefs: []
  type: TYPE_NORMAL
  zh: '- 它们都忽略相同的输入位，B7 和 B8；'
- en: '- each uses a common computed subfeature, P arity(B2 ··· B6);'
  id: totrans-2161
  prefs: []
  type: TYPE_NORMAL
  zh: '- 每个任务都使用一个共同计算的子特征，P arity(B2 ··· B6)；'
- en: '- when B1 = 0, Task 1 needs P arity(B2 ··· B6), but Task 2 does not, and vice
    versa;'
  id: totrans-2162
  prefs: []
  type: TYPE_NORMAL
  zh: '- 当 B1 = 0 时，任务 1 需要 P arity(B2 ··· B6)，而任务 2 不需要，反之亦然；'
- en: '- as with Tasks 1 and 2, when Task 3 needs P arity(B2 ··· B6), Task 4 does
    not need it, and vice versa.'
  id: totrans-2163
  prefs: []
  type: TYPE_NORMAL
  zh: '- 与任务 1 和 2 一样，当任务 3 需要 P arity(B2 ··· B6) 时，任务 4 不需要，反之亦然。'
- en: We can train artificial neural nets on these tasks with backprop. Bits B1 ···
    B8 are the inputs to the net. The task values computed by the four functions are
    the target outputs. We create a data set by enumerating all 256 combinations of
    the eight input bits, and computing for each setting of the bits the task signals
    for Tasks 1, 2, 3, and 4 using the definitions above. This yields 256 different
    cases, with four different training signals for each case.
  id: totrans-2164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用反向传播在这些任务上训练人工神经网络。位 B1 ··· B8 是网络的输入。由四个函数计算的任务值是目标输出。我们通过列举八个位的所有 256
    种组合，创建一个数据集，并根据上述定义为每种位设置计算任务 1、2、3 和 4 的信号。这产生了 256 个不同的案例，每个案例有四个不同的训练信号。
- en: 8.1.1 Single And Multitask Learning Of Task 1
  id: totrans-2165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1.1 任务 1 的单任务与多任务学习
- en: Consider Task 1 the main task. Tasks 2, 3, and 4 are the extra tasks. That is,
    we are interested only in improving the accuracy of models trained for Task 1.
    We've done an experiment where we train Task 1 on the three nets shown in Figure
    8.1. All the nets are fully connected feed-forward nets with 8 inputs, 100 hidden
    units, and 1–4 outputs. Where there are multiple outputs, each output is fully
    connected to the hidden units. Nets were trained in batch mode using backprop
    with MITRE's Aspirin/MIGRAINES 6.0 with learning rate = 0.1 and momentum = 0.9.
  id: totrans-2166
  prefs: []
  type: TYPE_NORMAL
  zh: 将任务 1 视为主要任务。任务 2、3 和 4 是额外任务。也就是说，我们只关心提高为任务 1 训练的模型的准确性。我们进行了一个实验，训练任务 1 使用图
    8.1 中显示的三个网络。所有网络都是完全连接的前馈网络，具有 8 个输入、100 个隐藏单元和 1–4 个输出。在多个输出的情况下，每个输出都完全连接到隐藏单元。网络采用批处理模式训练，使用
    MITRE 的 Aspirin/MIGRAINES 6.0，学习率 = 0.1，动量 = 0.9。
- en: Task 1 is trained alone on the net on the left of Figure 8.1. This is a backprop
    net trained on a single task. We refer to this as single task learning (STL) or
    single task backprop (STL-backprop). The net in the center of Figure 8.1 trains
    Task 1 on a net that is also trained on Task 2. The hidden layer of this net is
    shared by Tasks 1 and 2. This is multitask backprop (MTL-backprop) with two tasks.
    The net on the right side of Figure 8.1 trains Task 1 with Tasks 2, 3, and 4.
    The hidden layer of this net is shared by all four tasks. This is MTL-backprop
    with four tasks. How well will Task 1 be learned by the different nets?
  id: totrans-2167
  prefs: []
  type: TYPE_NORMAL
  zh: 任务 1 在图 8.1 左侧的网络上单独训练。这是一个在单一任务上训练的反向传播网络。我们称之为单任务学习（STL）或单任务反向传播（STL-backprop）。图
    8.1 中间的网络在一个同时训练任务 2 的网络上训练任务 1。这个网络的隐藏层由任务 1 和 2 共享。这是具有两个任务的多任务反向传播（MTL-backprop）。图
    8.1 右侧的网络在任务 2、3 和 4 的情况下训练任务 1。这个网络的隐藏层由所有四个任务共享。这是具有四个任务的 MTL-backprop。任务 1
    会在不同的网络上学习得多好吗？
- en: '![166_image_0.png](166_image_0.png)'
  id: totrans-2168
  prefs: []
  type: TYPE_IMG
  zh: '![166_image_0.png](166_image_0.png)'
- en: Fig. 8.1. Three Neural Net Architectures for Learning Task 1
  id: totrans-2169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1. 学习任务 1 的三种神经网络架构
- en: We performed 25 independent trials by resampling training and test sets from
    the 256 cases. From the 256 cases, we randomly sample 128 cases for the training
    set, and use the remaining 128 cases as a test set. (For now we ignore the complexity
    of early stopping, which can be tricky with MTL nets. See section 8.3.2 for a
    thorough discussion of early stopping in MTL nets.)
  id: totrans-2170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过从 256 个案例中重新抽样训练集和测试集进行了 25 次独立试验。从 256 个案例中，我们随机抽取 128 个案例作为训练集，剩余 128
    个案例作为测试集。（目前我们忽略早停的复杂性，这在多任务学习网络中可能很棘手。有关多任务学习网络中早停的详细讨论，请参见第 8.3.2 节。）
- en: 'For each trial, we trained three nets: an STL net for Task 1, an MTL net for
    Tasks 1 and 2, and an MTL net for Tasks 1–4. We measure performance only on the
    output for Task 1. When there are extra outputs for Task 2 or Tasks 2–4, these
    are trained with backprop, but ignored when the net is evaluated. The sole purpose
    of the extra outputs is to affect what is learned in the hidden layer these outputs
    share with Task 1.'
  id: totrans-2171
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次试验，我们训练了三种网络：一个用于任务1的STL网络，一个用于任务1和2的MTL网络，以及一个用于任务1–4的MTL网络。我们仅测量任务1的输出表现。当存在任务2或任务2–4的额外输出时，这些输出使用反向传播进行训练，但在评估网络时被忽略。额外输出的唯一目的是影响这些输出与任务1共享的隐藏层中的学习内容。
- en: 8.1.2 Results
  id: totrans-2172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1.2 结果
- en: Every 5000 epochs we evaluated the performance of the nets on the test set.
    We measured the RMS error of the output with respect to the target values, the
    criterion being optimized by backprop. We also measured the accuracy of the output
    in predicting the boolean function values. If the net output is less than 0.5,
    it was treated as a prediction of 0, otherwise it was treated as a prediction
    of 1.
  id: totrans-2173
  prefs: []
  type: TYPE_NORMAL
  zh: 每5000个周期，我们评估了网络在测试集上的表现。我们测量了输出相对于目标值的均方根误差，反向传播优化的标准就是这个。我们还测量了输出在预测布尔函数值上的准确性。如果网络输出小于0.5，则视为预测为0，否则视为预测为1。
- en: Figure 8.2 shows the RMSE for Task 1 on the test set during training. The three
    curves in the graph are each the average of 25 trials.2 RMSE on the main task,
    Task 1, is reduced when Task 1 is trained on a net simultaneously trained on other
    related tasks. RMSE is reduced when Task 1 is trained with extra Task 2, and is
    further reduced when extra Tasks 3 and 4 are added. *Training multiple* tasks
    on one net does not increase the number of training patterns seen by the net.
    Each net sees exactly the same training cases. The MTL nets do not see more training
    cases; they receive more training signals with each case.
  id: totrans-2174
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2显示了在训练期间任务1在测试集上的均方根误差。这三条曲线是25次试验的平均值。2 当任务1在一个同时训练其他相关任务的网络上进行训练时，主要任务（任务1）的均方根误差会减少。当任务1与额外的任务2一起训练时，均方根误差会减少，当再增加额外的任务3和4时，均方根误差进一步降低。*在一个网络上同时训练多个*任务并不会增加网络看到的训练模式数量。每个网络看到的训练案例完全相同。MTL网络并没有看到更多的训练案例；它们在每个案例中接收更多的训练信号。
- en: '![167_image_0.png](167_image_0.png)'
  id: totrans-2175
  prefs: []
  type: TYPE_IMG
  zh: '![167_image_0.png](167_image_0.png)'
- en: Fig. 8.2. RMSE Test-set Performance of Three Different Nets on Task 1
  id: totrans-2176
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2. 三种不同网络在任务1上的均方根误差测试集表现
- en: Figure 8.3 shows the average test-set accuracy on Task 1 for 25 trials with
    the three different nets. Task 1 has boolean value 1 about 75% of the time. A
    simple learner that learned to predict 1 all the time should achieve about 75%
    accuracy on Task 1. When trained alone (STL), performance on Task 1 is about 80%.
    When Task 1 is trained with Task 2, performance increases to about 88%. When Task1
    is trained with Tasks 2, 3, and 4, performance increases further to about 90%.
    Table 8.1 summarizes the results of examining the training curve from each trial.
  id: totrans-2177
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3显示了在三种不同网络的25次试验中，任务1的平均测试集准确性。任务1的布尔值在大约75%的时间内为1。一个学习者如果总是预测为1，则在任务1上的准确率应达到约75%。当单独训练时（STL），任务1的表现约为80%。当任务1与任务2一起训练时，表现提高至约88%。当任务1与任务2、3和4一起训练时，表现进一步提高至约90%。表8.1总结了每次试验训练曲线的结果。
- en: 2 Average training curves can be misleading, particularly if training curves
    are not monotonic. For example, it is possible for method A to always achieve
    better error than method B, but for the average of method A to be everywhere worse
    than the average of method B because the regions where performance on method A
    is best do not align, but do align for method B. Before presenting average training
    curves, we always examine the individual curves to make sure the average curve
    is not misleading.
  id: totrans-2178
  prefs: []
  type: TYPE_NORMAL
  zh: 平均训练曲线可能具有误导性，尤其是在训练曲线不是单调的情况下。例如，方法A可能总是比方法B取得更好的误差，但方法A的平均值却在所有地方都比方法B的平均值差，因为方法A表现最佳的区域不重合，而方法B的区域则重合。在呈现平均训练曲线之前，我们始终检查各个曲线，以确保平均曲线不具误导性。
- en: '![168_image_0.png](168_image_0.png)'
  id: totrans-2179
  prefs: []
  type: TYPE_IMG
  zh: '![168_image_0.png](168_image_0.png)'
- en: Fig. 8.3. Test-set Percent Correct of Three Different Nets on Task 1 Table 8.1.
    Test-set performance on Task 1 of STL of Task 1, MTL of Tasks 1 and 2, and MTL
    of Tasks 1, 2, 3, and 4. *** indicates performance is statistically better than
    STL at .001 or better, respectively.
  id: totrans-2180
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3. 三种不同网络在任务1上的测试集正确率 表8.1. STL任务1、MTL任务1和2以及MTL任务1、2、3和4的测试集表现。***表示在.001或更好情况下，表现统计上优于STL。
- en: '| NET                     | STL: 1   | MTL: 1+2   | MTL: 1+2+3+4   |'
  id: totrans-2181
  prefs: []
  type: TYPE_TB
  zh: '| 网络                     | STL: 1   | MTL: 1+2   | MTL: 1+2+3+4   |'
- en: '| --- | --- | --- | --- |'
  id: totrans-2182
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Root-Mean-Squared-Error | 0.211    | 0.134 ***  | 0.122 ***      |'
  id: totrans-2183
  prefs: []
  type: TYPE_TB
  zh: '| 均方根误差              | 0.211    | 0.134 ***  | 0.122 ***      |'
- en: '| Percent Correct         | 79.7%    | 87.5% ***  | 88.9% ***      |'
  id: totrans-2184
  prefs: []
  type: TYPE_TB
  zh: '| 正确率                   | 79.7%    | 87.5% ***  | 88.9% ***      |'
- en: 8.1.3 Discussion
  id: totrans-2185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1.3 讨论
- en: Why is the main task learned better if it is trained on a net learning other
    related tasks at the same time? We ran a number of experiments to verify that
    the performance increase with MTL is due to the fact that the tasks are related,
    and not just a side effect of training multiple outputs on one net.
  id: totrans-2186
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么在同时训练其他相关任务的网络上，主要任务的学习效果更好？我们进行了多项实验，以验证MTL带来的性能提升是由于任务之间的相关性，而不仅仅是训练多个输出的副作用。
- en: Adding noise to neural nets sometimes improves their generalization performance
    [22]. To the extent that MTL tasks are *uncorrelated*, their contribution to the
    aggregate gradient may appear as noise to other tasks and this might improve generalization.
    To see if this explains the benefits we see from MTL, in one experiment we train
    Task 1 on a net with three *random* tasks.
  id: totrans-2187
  prefs: []
  type: TYPE_NORMAL
  zh: 向神经网络添加噪声有时会提高其泛化性能[22]。在MTL任务*不相关*的情况下，它们对聚合梯度的贡献可能会对其他任务显得像噪声，这可能会改善泛化。为了查看这是否解释了我们从MTL中看到的好处，在一个实验中，我们在一个具有三个*随机*任务的网络上训练任务1。
- en: A second effect to be concerned about is that adding tasks tends to increase
    the effective learning rate on the input-to-hidden layer weights because the gradients
    from the multiple outputs add at the hidden layer, and this might favor nets with
    multiple outputs. To test this, we train an MTL net with four copies of Task 1.
    Each of the four outputs receives exactly the same training signal. This is a
    degenerate form of MTL where no extra information is given to the net by the extra
    tasks.
  id: totrans-2188
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要关注的影响是，增加任务往往会提高输入到隐含层权重的有效学习速率，因为多个输出的梯度在隐含层叠加，这可能有利于具有多个输出的网络。为了测试这一点，我们训练了一个MTL网络，其中包含四个任务1的副本。四个输出接收完全相同的训练信号。这是一种退化形式的MTL，其中额外任务没有向网络提供额外的信息。
- en: A third effect that needs to be ruled out is net capacity. 100 hidden units
    is a lot for these tasks. Does the MTL net, which has to share the 100 hidden
    units among four tasks, generalize better because each task has fewer hidden units?
    To test for this, we train Task 1 on STL nets with 200 hidden units and with 25
    hidden units. This will tell us if generalization would be better with more or
    less capacity.
  id: totrans-2189
  prefs: []
  type: TYPE_NORMAL
  zh: 需要排除的第三个影响是网络容量。对于这些任务，100个隐含单元已经很多。MTL网络是否因为每个任务拥有较少的隐含单元而在四个任务中共享这100个隐含单元时更具泛化能力？为了测试这一点，我们在具有200个隐含单元和25个隐含单元的STL网络上训练任务1。这将告诉我们，更多或更少的容量是否会带来更好的泛化能力。
- en: Finally, we ran a fourth experiment based on the heuristic used in [37]. We
    shuffle the training signals (the target output values) for Tasks 2, 3, and 4
    before training an MTL net on the four tasks. Shuffling reassigns the target values
    to the input vectors in the training set for Tasks 2, 3, and 4. The main task,
    Task 1, is not affected. The distributions of the training signals for outputs
    2–4 have not changed, but the training signals are no longer related to Task 1.
    This is a powerful test that has the potential to rule-out many mechanisms that
    do not depend on relationships between the tasks.
  id: totrans-2190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们进行了第四个实验，基于[37]中使用的启发式方法。我们在对四个任务训练MTL网络之前，先对任务2、3和4的训练信号（目标输出值）进行了随机打乱。打乱将目标值重新分配给任务2、3和4的输入向量。主要任务任务1不受影响。任务2-4的训练信号分布没有变化，但训练信号与任务1不再相关。这是一个强有力的测试，有可能排除许多不依赖于任务间关系的机制。
- en: We ran each experiment 25 times using exactly the same data sets used in the
    previous section. Figure 8.4 shows the generalization performance on Task 1 in
    the four experiments. For comparison, the performance of of STL, MTL with Tasks
    1 and 2, and MTL with Tasks 1–4 from the previous section are also shown.
  id: totrans-2191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与前一部分相同的数据集进行了25次实验。图8.4显示了四个实验中任务1的泛化性能。为进行比较，前一部分中STL、MTL（任务1和2）及MTL（任务1-4）的性能也进行了展示。
- en: When Task 1 is trained with random extra tasks, performance on Task 1 drops
    below the performance on Task 1 when it is trained alone on an STL net. We conclude
    MTL of Tasks 1–4 probably does not learn Task 1 better by adding noise to the
    learning process through the extra outputs.
  id: totrans-2192
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务 1 与随机额外任务一起训练时，任务 1 的性能低于在 STL 网络上单独训练时的性能。我们得出结论，任务 1-4 的 MTL 可能并没有通过增加额外输出中的噪音来更好地学习任务
    1。
- en: When Task 1 is trained with three additional copies of Task 1, the performance
    is comparable to that when Task 1 is trained alone with STL.3 We conclude that
    MTL does not learn Task 1 better just because backprop works better with multiple
    outputs.
  id: totrans-2193
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务 1 与三个额外的任务 1 副本一起训练时，性能与任务 1 在 STL 中单独训练时相当。我们得出结论，MTL 之所以没有更好地学习任务 1，仅仅是因为反向传播在多个输出上效果更好。
- en: When Task 1 is trained on an STL net with 25 hidden units, performance is comparable
    to the performance with 100 hidden units. Moreover, when Task 1 is trained on
    an STL net with 200 hidden units, it is slightly better. (The differences between
    STL with 25, 100, and 200 hidden units are not statistically significant.) We
    conclude that performance on Task 1 is relatively insensitive to net size for
    nets between 25 and 200 hidden units, and, if anything, Task 1 would benefit from
    a net with more capacity, not one with less capacity. Thus it is unlikely that
    MTL on Tasks 1–4 performs better on Task 1 because Tasks 2–4 are using up extra
    capacity that is hurting Task 1.
  id: totrans-2194
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务 1 在具有 25 个隐藏单元的 STL 网络上训练时，性能与在 100 个隐藏单元时的性能相当。此外，当任务 1 在具有 200 个隐藏单元的
    STL 网络上训练时，表现略好。（25、100 和 200 个隐藏单元的 STL 之间的差异在统计上并不显著。）我们得出结论，任务 1 的性能对 25 到
    200 个隐藏单元的网络大小相对不敏感，如果说有任何影响，任务 1 会从更大容量的网络中受益，而不是更小容量的网络。因此，任务 1-4 的 MTL 更好地执行任务
    1 的可能性不大，因为任务 2-4 正在占用多余的容量，从而对任务 1 造成伤害。
- en: 3 We sometimes observe that training multiple copies of a task on one net does
    improve performance. When we have observed this, the benefit is never large enough
    to explain away the benefits observed with MTL. But it is interesting and surprising,
    as the improvement is gained without any additional information being given to
    the net. The most likely explanation is that the multiple connections to the hidden
    layer allow different hidden layer predictions to be averaged and thus act as
    a weak boosting mechanism.
  id: totrans-2195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有时观察到在一个网络上训练多个任务副本确实会提高性能。当我们观察到这一点时，收益从未足够大以解释 MTL 的收益。但这是有趣且令人惊讶的，因为这种改进是在没有额外信息提供给网络的情况下获得的。最可能的解释是多个与隐藏层的连接允许不同的隐藏层预测被平均，从而起到弱增强机制的作用。
- en: '![170_image_0.png](170_image_0.png)'
  id: totrans-2196
  prefs: []
  type: TYPE_IMG
  zh: '![170_image_0.png](170_image_0.png)'
- en: 'Fig. 8.4. RMSE test-set performance of Task 1 when trained with: MTL with three
    random tasks; MTL with three more copies of Task 1; MTL with shuffled training
    signals for Tasks 2–4; STL on nets with 25 or 200 hidden units.'
  id: totrans-2197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4. 任务 1 在以下情况下的 RMSE 测试集性能：与三个随机任务的多任务学习（MTL）；与三个额外的任务 1 副本的 MTL；任务 2-4
    的训练信号经过洗牌的 MTL；在具有 25 或 200 个隐藏单元的网络上的单任务学习（STL）。
- en: When Task 1 is trained with training signals for Tasks 2–4 that have been shuffled,
    the performance of MTL drops below the performance of Task 1 trained alone on
    an STL net. Clearly the benefit we see with MTL on these problems is not due to
    some accident caused by the distribution of the extra outputs. The extra outputs
    must be *related* to the main task to help it.
  id: totrans-2198
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务 1 与经过洗牌的任务 2-4 的训练信号一起训练时，MTL 的性能低于在 STL 网络上单独训练的任务 1 的性能。显然，我们在这些问题上看到的
    MTL 的益处并不是由于额外输出的分布造成的某种意外。额外输出必须与主要任务*相关*，才能对其有所帮助。
- en: These experiments rule out most explanations for why MTL outperforms STL
  id: totrans-2199
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验排除了大多数关于为什么 MTL 优于 STL 的解释。
- en: on Task 1 that do not require Tasks 2–4 be related to Task 1. So why is the
    main task learned better when trained in parallel with Tasks 2–4?
  id: totrans-2200
  prefs: []
  type: TYPE_NORMAL
  zh: 在任务 1 中，不需要任务 2-4 与任务 1 相关。那么为什么当任务 1 与任务 2-4 并行训练时，主要任务的学习效果更好呢？
- en: One reason is that Task 1 needs to learn the subfeature P arity(B2 ··· B6)
  id: totrans-2201
  prefs: []
  type: TYPE_NORMAL
  zh: 一个原因是任务 1 需要学习子特征 P 归一化(B2 ··· B6)。
- en: that it shares with Tasks 2–4. Tasks 2–4 give the net information about this
    subfeature that it would not get from Task 1 alone. For example, when B1 = 1,
    the training signal for Task 1 contains no information about P arity(B2 ··· B6).
    We say B1 masks P arity(B2 ··· B6) when B1 = 1. But the training signals for Task
    2 provide information about the Parity subfeature in exactly those cases where
    Task 1 is masked. Thus the hidden layer in a net trained on both Tasks 1 and 2
    gets twice as much information about the Parity subfeature as a net trained on
    one of these tasks, despite the fact that they see exactly the same training cases.
    The MTL net is getting more information with each training case.
  id: totrans-2202
  prefs: []
  type: TYPE_NORMAL
  zh: 它与任务2–4共享的信息。任务2–4提供了有关这个子特征的网络信息，而这些信息在单独的任务1中无法获得。例如，当B1 = 1时，任务1的训练信号不包含关于P
    arity(B2 ··· B6)的信息。当B1 = 1时，我们说B1屏蔽了P arity(B2 ··· B6)。但是，任务2的训练信号在任务1被屏蔽的情况下，正好提供了关于Parity子特征的信息。因此，在同时训练任务1和任务2的网络中的隐藏层，获得的关于Parity子特征的信息是仅训练这两个任务之一的网络的两倍，尽管它们看到的训练案例完全相同。MTL网络在每个训练案例中获得更多信息。
- en: Another reason why MTL helps Task 1 is that all the tasks are functions of the
    same inputs, bits B1 ··· B6, and ignore the same inputs, B7 and B8. Because the
    tasks overlap on the features they use and don't use, the MTL net is better able
    select which input features to use.
  id: totrans-2203
  prefs: []
  type: TYPE_NORMAL
  zh: MTL有助于任务1的另一个原因是所有任务都是相同输入（位B1 ··· B6）的函数，并忽略相同的输入B7和B8。由于任务在使用和不使用的特征上存在重叠，MTL网络更能选择哪些输入特征来使用。
- en: 'A third reason why MTL helps Task 1 is that there are relationships between
    the way the different tasks use the inputs that promote learning good internal
    representations. For example, all the tasks logically combine input B1 with a
    function of inputs B2 ··· B6. This similarity tends to prevent the net from learning
    internal representations that, for example, directly combine bits B1 and B2. A
    net trained on all the tasks together is biased to learn more modular, in this
    case more correct, internal representations that support the multiple tasks. This
    bias towards modular internal representations reduces the net''s tendency to learn
    spurious correlations that occur in any finite training sample: there may be a
    random correlation between bit B3 and the output for Task 1 that looks fairly
    strong in this one training set, but if that spurious correlation does not also
    help the other tasks, it is less likely to be learned.'
  id: totrans-2204
  prefs: []
  type: TYPE_NORMAL
  zh: MTL有助于任务1的第三个原因是不同任务使用输入的方式之间存在关系，这促进了良好的内部表示的学习。例如，所有任务在逻辑上将输入B1与输入B2 ··· B6的函数结合在一起。这种相似性往往防止网络学习直接将位B1和B2结合的内部表示。一个同时训练所有任务的网络会倾向于学习更模块化的、在这种情况下更正确的内部表示，以支持多个任务。这种对模块化内部表示的偏向减少了网络学习任何有限训练样本中出现的虚假相关性的倾向：例如，在这个训练集中的位B3与任务1的输出之间可能存在一种看似相当强的随机相关性，但如果这种虚假相关性没有帮助其他任务，它被学习的可能性就会降低。
- en: 8.2 Tricks For Using Multitask Learning In The Real World
  id: totrans-2205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 在现实世界中使用多任务学习的技巧
- en: The previous section introduced multitask learning (MTL) in backprop nets using
    four tasks carefully designed to have relationships that make learning them in
    parallel work better than learning them in isolation. How often will real problems
    present extra tasks that allow multitask learning to improve performance on the
    main task?
  id: totrans-2206
  prefs: []
  type: TYPE_NORMAL
  zh: 前一部分介绍了在反向传播网络中使用的多任务学习（MTL），采用四个精心设计的任务，使得并行学习比孤立学习效果更好。现实问题中多任务学习能提高主要任务性能的额外任务出现的频率有多高？
- en: This section shows that many real world problems yield opportunities for multitask
    learning. We present a dozen prototypical real-world applications where the training
    signals for related extra tasks are available and can be leveraged. We believe
    most real-world problems fall into one or more of these prototypical classes.
    This claim might sound surprising given that few of the test problems traditionally
    used in machine learning are multitask problems. We believe most of the problems
    used in machine learning so far have been heavily preprocessed to fit the single
    task learning mold. Most of the opportunities for MTL in these problems were eliminated
    as the problems were defined.
  id: totrans-2207
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了许多现实世界的问题提供了多任务学习的机会。我们呈现了十多个典型的现实世界应用，其中相关额外任务的训练信号可用，并且可以被利用。我们相信大多数现实世界的问题都属于这些典型类别中的一个或多个。这个说法可能让人感到惊讶，因为传统上用于机器学习的测试问题中很少是多任务问题。我们认为迄今为止用于机器学习的大多数问题都经过了大量预处理，以适应单一任务学习的模型。这些问题中MTL的机会在定义问题时就被消除了。
- en: 8.2.1 Using The Future To Predict The Present
  id: totrans-2208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.1 使用未来预测现在
- en: Often valuable features become available *after* predictions must be made. If
    learning is done offline, these features can be collected for the training set
    and used for learning. These features can't be used as inputs, because they will
    not be available when making predictions for future test cases. They can, however,
    be used as extra outputs for multitask learning. The predictions the learner makes
    for these extra tasks will be ignored when the system is used to make predictions
    for the main task. Their sole function is to provide extra information to the
    learner during training so that it can learn the main task better.
  id: totrans-2209
  prefs: []
  type: TYPE_NORMAL
  zh: 有价值的特征往往在必须做出预测之后才会变得可用。如果离线学习，这些特征可以收集到训练集中并用于学习。这些特征不能作为输入使用，因为在对未来测试案例做出预测时，它们不会可用。然而，它们可以作为多任务学习的额外输出。学习者为这些额外任务所做的预测在系统用于主要任务的预测时将被忽略。它们的唯一功能是在训练期间为学习者提供额外信息，从而使其能够更好地学习主要任务。
- en: One source of applications of learning from the future is sequential decision
    making in medicine. Given the initial symptoms, decisions are made about what
    tests to make and what treatment to begin. New information becomes available when
    the tests are completed and as the patient responds (or fails to respond) to the
    treatment. From this new information, new decisions are made. Should more tests
    be made? Should the treatment be changed? Has the patient's condition changed?
    Is this patient now high risk, or low risk? Does the patient need to be hospitalized?
    Etc.
  id: totrans-2210
  prefs: []
  type: TYPE_NORMAL
  zh: 从未来学习的应用来源之一是医学中的顺序决策。根据初始症状，决定进行哪些检测和开始何种治疗。当检测完成并且患者对治疗做出反应（或未反应）时，会有新的信息可用。从这些新信息中，做出新的决策。是否需要进行更多检测？治疗是否需要更改？患者的病情是否有所变化？该患者现在是高风险还是低风险？患者需要住院吗？等等。
- en: 'When machine learning is applied to early stages in the decision making process,
    only those input features that typically would be available for patients at this
    stage of the process are usually used. This is unfortunate. In an historical database,
    all of the patients may have run the full course of medical testing and treatment
    and their final outcome may be known. Must we ignore the results of lab tests
    and other valuable features in the database just because these will not be available
    for patients at the stage of medical decision making for which we wish to learn
    a model? The Pneumonia Risk Prediction Problem. Consider pneumonia. There are
    3,000,000 cases of pneumonia each year in the U.S., 900,000 of which get hospitalized.
    Most pneumonia patients recover given appropriate treatment, and many can be treated
    effectively without hospitalization. Nonetheless, pneumonia is serious: 100,000
    of those hospitalized for pneumonia die from it, and many more are at elevated
    risk if not hospitalized.'
  id: totrans-2211
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器学习应用于决策过程的早期阶段时，通常仅使用在这一阶段对患者可用的输入特征。这很不幸。在历史数据库中，所有患者可能已经完成了全面的医学检测和治疗，其最终结果可能是已知的。我们是否必须忽视实验室检测结果和数据库中其他有价值的特征，仅仅因为这些特征在我们希望学习模型的医学决策阶段对患者不可用？肺炎风险预测问题。考虑肺炎。在美国，每年有3,000,000例肺炎，其中900,000例住院。大多数肺炎患者在适当治疗下能够康复，许多患者在不住院的情况下也能有效治疗。尽管如此，肺炎是严重的：100,000例因肺炎住院的患者死于此病，还有更多人如果不住院则面临更高的风险。
- en: Consider the problem of predicting a patient's risk from pneumonia before they
    are hospitalized. (The problem is not to diagnose if the patient has pneumonia,
    but to determine how much risk the pneumonia poses to the patient.) A
  id: totrans-2212
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在患者住院之前预测其肺炎风险的问题。（问题不是诊断患者是否患有肺炎，而是确定肺炎对患者构成的风险程度。）
- en: primary goal in medical decision making is to accurately, swiftly, and economically
    identify patients at high risk from diseases like pneumonia so that they may be
    hospitalized to receive aggressive testing and treatment; patients at low risk
    may be more comfortably, safely, and economically treated at home.
  id: totrans-2213
  prefs: []
  type: TYPE_NORMAL
  zh: 医学决策的主要目标是准确、迅速和经济地识别高风险患者，以便他们能够住院接受积极的检测和治疗；低风险患者则可以在家中更舒适、安全和经济地治疗。
- en: Some of the most useful features for assessing risk are the lab tests that become
    available only after a patient is hospitalized. It is the *extra* lab tests made
    after patients are admitted to the hospital that we use as extra tasks for MTL;
    they cannot be used as inputs because they will not be available for most future
    patients when making the decision to hospitalize.4 The most useful decision aid
    for this problem would be to predict which patients will live or die. This is
    too difficult. In practice, the best that can be achieved is to estimate a probability
    of death (POD) from the observed symptoms. In fact, it is sufficient to learn
    to order patients by POD so lower-risk patients can be discriminated from higher
    risk patients; patients at least risk may then be considered for outpatient care.
  id: totrans-2214
  prefs: []
  type: TYPE_NORMAL
  zh: 一些评估风险最有用的特征是只有在患者住院后才能进行的实验室测试。我们使用的*额外*实验室测试是在患者入院后进行的，作为MTL的额外任务；因为在做出住院决定时，这些测试对于大多数未来患者将不可用。这个问题最有用的决策辅助工具是预测哪些患者会生存或死亡。这太困难了。实际上，最好的做法是从观察到的症状中估计死亡概率（POD）。事实上，仅需学习根据POD对患者进行排序，以便能够将低风险患者与高风险患者区分开；然后可以考虑将最低风险的患者进行门诊治疗。
- en: 4 Other researchers who tackled this problem ignored the the lab tests because
    they knew they would not be available at run time and did not see ways to use
    them other than as inputs.
  id: totrans-2215
  prefs: []
  type: TYPE_NORMAL
  zh: 其他研究者在处理这个问题时忽略了实验室测试，因为他们知道这些测试在运行时不会可用，并且未能看到除作为输入之外的其他使用方式。
- en: The performance criteria used by others working with this database [15] is the
    accuracy with which one can select prespecified fractions of a patient population
    who will live. For example, given a population of 10,000 patients, find the 20%
  id: totrans-2216
  prefs: []
  type: TYPE_NORMAL
  zh: 其他在该数据库上工作的人使用的性能标准是能够准确选择预先指定的生存患者比例。例如，给定一个10,000名患者的人群，找到20%。
- en: of this population at *least* risk. To do this we learn a risk model, and a
    threshold for this risk model, that allows 20% of the population (2000 patients)
    to fall below it. If 30 of the 2000 patients below this threshold die, the error
    rate is 30/2000 = 0.015. We say that the error rate for FOP 0.20 is 0.015 (FOP
    stands for "fraction of population"). Here we consider FOPs 0.1, 0.2, 0.3, 0.4,
    and 0.5. Our goal is to learn models and thresholds such that the error rate at
    each FOP is minimized.
  id: totrans-2217
  prefs: []
  type: TYPE_NORMAL
  zh: 这个人群的*最低*风险。为此，我们学习一个风险模型，以及这个风险模型的阈值，使得20%的人口（2000名患者）低于该阈值。如果这2000名低于阈值的患者中有30人死亡，错误率为30/2000
    = 0.015。我们说FOP 0.20的错误率为0.015（FOP代表“人群比例”）。在这里，我们考虑FOP 0.1、0.2、0.3、0.4和0.5。我们的目标是学习模型和阈值，使每个FOP的错误率最小化。
- en: Multitask Learning and Pneumonia Risk Prediction. The straightforward approach
    to this problem is to use backprop to train an STL net to learn to predict which
    patients live or die, and then use the real-valued predictions of this net to
    sort patients by risk. This STL net has 30 inputs for the 30 basic pre-hospitalization
    measurements, a single hidden layer, and a single output trained with targets
    0=lived, 1=died.5 Given a large training set, a net trained this way should learn
    to predict the probability of death for each patient, not which patients live
    or die. If the training sample is small, the net will overfit and learn a very
    nonlinear function that outputs values near 0/1 for cases in the training set,
    but which does not generalize well. It is critical to use early stopping to halt
    training before this happens.
  id: totrans-2218
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习与肺炎风险预测。解决这个问题的直接方法是使用反向传播训练一个STL网络，学习预测哪些患者生存或死亡，然后利用该网络的实值预测按风险对患者进行排序。该STL网络有30个输入，用于30个基本的入院前测量，包含一个隐藏层，输出一个目标为0=生存，1=死亡的单一输出。给定一个大的训练集，这种训练方式的网络应该能够预测每个患者的死亡概率，而不是哪些患者生存或死亡。如果训练样本较小，网络会过拟合，学习一个非常非线性的函数，对训练集中的案例输出接近0或1的值，但无法很好地进行泛化。关键是要使用早停法，在发生这种情况之前停止训练。
- en: We developed a method called *Rankprop* specifically for this domain. Rankprop
    learns to rank patients without learning to predict mortality (0=lived,1=died).
    Figure 8.5 compares the performance of squared error on 0/1 targets with rankprop
    on this problem. Rankprop outperforms traditional backprop using squared error
    on targets 0=lived,1=died by 10%-40% on this domain, depending on which FOP is
    used for comparison. See [9] for details about rankprop.6 There are 35 future
    lab values that we use as extra backprop *outputs,* as shown in Figure 8.6. The
    expectation is that these extra outputs will bias the shared hidden layer toward
    representations that better capture important features of each patient's condition,
    and that this will lead to more accurate predictions of patient risk at the main
    task output.
  id: totrans-2219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这个领域开发了一种名为*Rankprop*的方法。Rankprop学习如何对患者进行排名，而无需学习预测死亡率（0=存活，1=死亡）。图8.5比较了在该问题上平方误差的0/1目标与rankprop的表现。根据用于比较的FOP，Rankprop在本领域上超越了传统的平方误差反向传播，提升幅度在10%-40%之间。有关rankprop的详细信息，请参见[9]。我们使用35个未来实验室值作为额外的反向传播*输出*，如图8.6所示。期望这些额外的输出将使共享的隐藏层倾向于更好地捕捉每个患者病情的重要特征，从而在主要任务输出中实现更准确的患者风险预测。
- en: The STL net has 30 inputs, 8 hidden units, and one output trained to predict
    risk with rankprop. The MTL net has the same 30 inputs, 64 hidden units, one output
    for rankprop, and 35 extra outputs trained with squared error. (Preliminary experiments
    suggested 8–32 hidden units was optimal for STL, and that MTL performs best with
    nets as large as 512 hidden units. We used 8 and 64 hidden units so that we could
    run many experiments.) The 35 extra outputs on
  id: totrans-2220
  prefs: []
  type: TYPE_NORMAL
  zh: STL网络有30个输入、8个隐藏单元和一个用于通过rankprop预测风险的输出。MTL网络有相同的30个输入、64个隐藏单元、一个用于rankprop的输出和35个使用平方误差训练的额外输出。（初步实验表明，STL的最佳隐藏单元数为8-32，而MTL在隐藏单元数达到512时表现最佳。我们使用8个和64个隐藏单元，以便能够进行更多实验。）额外的35个输出
- en: 5 We tried both squared error and cross entropy. The difference between the
    two was small. Squared error performed slightly better. 6 We use rankprop for
    the rest of our experiments on this domain because it is the best performer we
    know of on this problem. We want to see if MTL can make the best method better.
  id: totrans-2221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试了平方误差和交叉熵。两者之间的差异很小。平方误差的表现略好一些。我们在该领域的其他实验中使用rankprop，因为这是我们所知道的在此问题上表现最佳的方法。我们希望查看MTL是否能使最佳方法更进一步。
- en: '![174_image_0.png](174_image_0.png)'
  id: totrans-2222
  prefs: []
  type: TYPE_IMG
  zh: '![174_image_0.png](174_image_0.png)'
- en: Fig. 8.5. The performance of SSE (0/1 targets) and rankprop on the 5 FOPs in
    the pneumonia domain. Lower error indicates better performance.
  id: totrans-2223
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5。SSE（0/1目标）和rankprop在肺炎领域的5个FOP上的表现。较低的错误表示更好的性能。
- en: the MTL net (see Figure 8.6) are trained at the same time the net is trained
    to predict risk.
  id: totrans-2224
  prefs: []
  type: TYPE_NORMAL
  zh: MTL网络（见图8.6）与用于预测风险的网络同时训练。
- en: We train the net using training and validation sets containing 1000 patients
    randomly drawn from the database. Training is halted on both the STL and MTL nets
    when overfitting is observed on the main rankprop risk task. On the MTL net, the
    performance of the extra tasks is not taken into account for early stopping. Only
    the performance of the output for the main task is considered when deciding when
    to stop training. (See section 8.3.2 for more discussion of early stopping with
    MTL nets.) Once training is halted, the net is tested on the remaining unused
    patients in the database.
  id: totrans-2225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用包含从数据库中随机抽取的1000名患者的训练和验证集来训练网络。当在主要的rankprop风险任务上观察到过拟合时，STL和MTL网络的训练将会停止。在MTL网络中，额外任务的表现不用于早停判断。决定停止训练时，仅考虑主要任务的输出表现。（关于MTL网络的早停更多讨论见第8.3.2节。）一旦训练停止，网络将在数据库中剩余未使用的患者上进行测试。
- en: Results. Table 8.2 shows the mean performance of ten runs of rankprop using
    STL and MTL. The bottom row shows the percent improvement in performance obtained
    on this problem by using the future lab measurements as extra MTL
  id: totrans-2226
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。表8.2显示了使用STL和MTL的rankprop十次运行的平均性能。底行显示通过使用未来实验室测量作为额外MTL所获得的性能提升百分比。
- en: outputs. Negative percentages indicate MTL reduces error. Although MTL lowers
    the error at each FOP compared with STL, only the differences at FOP 0.3, 0.4,
    and 0.5 are statistically significant with ten trials using a standard t-test.
  id: totrans-2227
  prefs: []
  type: TYPE_NORMAL
  zh: 输出。负百分比表示MTL降低了错误。尽管与STL相比，MTL在每个FOP上的错误都降低了，但只有在使用标准t检验的十次试验中，FOP 0.3、0.4和0.5的差异是统计显著的。
- en: The improvement from MTL is 5–10%. This improvement can be of considerable consequence
    in medical domains. To verify that the benefit from MTL is due to relationships
    between what is learned for the future labs and the main task, we ran the shuffle
    test (see section 8.1.3). We shuffled the training signals for the extra tasks
    in the training sets before training the nets with MTL.
  id: totrans-2228
  prefs: []
  type: TYPE_NORMAL
  zh: MTL带来的改进为5–10%。这一改进在医学领域可能具有重要意义。为了验证MTL带来的好处是否源于未来实验室学习内容与主要任务之间的关系，我们进行了洗牌测试（见第8.1.3节）。在用MTL训练网络之前，我们对训练集中的额外任务的训练信号进行了洗牌。
- en: '![175_image_0.png](175_image_0.png)'
  id: totrans-2229
  prefs: []
  type: TYPE_IMG
  zh: '![175_image_0.png](175_image_0.png)'
- en: Fig. 8.6. Using future lab results as extra outputs to bias learning for the
    main risk prediction task. The lab tests would help most if they could be used
    as inputs, but will not yet have been measured when risk must be predicted, so
    we use them as extra outputs for MTL instead.
  id: totrans-2230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6. 使用未来实验室结果作为额外输出，以便对主要风险预测任务进行学习偏向。如果实验室测试能够作为输入使用，将会有很大帮助，但在必须进行风险预测时，实验室测试尚未被测量，因此我们将它们作为MTL的额外输出。
- en: Table 8.2. Error Rates (fraction deaths) for STL with Rankprop and MTL with
    Rankprop on Fractions of the Population predicted to be at low risk (FOP) between
    0.0 and 0.5. MTL makes 5–10% fewer errors than STL.
  id: totrans-2231
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.2. 使用Rankprop的STL和使用Rankprop的MTL在预测为低风险（FOP）的人群比例（介于0.0和0.5之间）上的错误率（死亡比例）。MTL的错误率比STL少5–10%。
- en: '| FOP          | 0.1    | 0.2    | 0.3     | 0.4     | 0.5     |'
  id: totrans-2232
  prefs: []
  type: TYPE_TB
  zh: '| FOP          | 0.1    | 0.2    | 0.3     | 0.4     | 0.5     |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-2233
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| STL Rankprop | .0083  | .0144  | .0210   | .0289   | .0386   |'
  id: totrans-2234
  prefs: []
  type: TYPE_TB
  zh: '| STL Rankprop | .0083  | .0144  | .0210   | .0289   | .0386   |'
- en: '| MTL Rankprop | .0074  | .0127  | .0197   | .0269   | .0364   |'
  id: totrans-2235
  prefs: []
  type: TYPE_TB
  zh: '| MTL Rankprop | .0074  | .0127  | .0197   | .0269   | .0364   |'
- en: '| % Change     | -10.8% | -11.8% | -6.2% * | -6.9% * | -5.7% * |'
  id: totrans-2236
  prefs: []
  type: TYPE_TB
  zh: '| % 变化       | -10.8% | -11.8% | -6.2% * | -6.9% * | -5.7% * |'
- en: Figure 8.7 shows the results of MTL with shuffled training signals for the extra
    tasks. The results of STL and MTL with unshuffled extra tasks are also shown.
    Shuffling the training signals for the extra tasks reduces the performance of
    MTL below that of STL. We conclude that it is the relationship between the main
    task and the extra tasks that lets MTL perform better on the main task; the benefit
    disappears when these relationships are broken by shuffling the extra task signals.
  id: totrans-2237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7显示了带有洗牌训练信号的MTL的结果。也展示了STL和未洗牌额外任务的MTL结果。对额外任务的训练信号进行洗牌使得MTL的性能低于STL。我们得出的结论是，主要任务与额外任务之间的关系使得MTL在主要任务上表现更好；当这些关系通过洗牌额外任务信号被打破时，益处消失。
- en: We have also run experiments where we use the future lab tests as inputs to
    a net trained to predict risk, and impute the values for the lab tests when they
    are missing on future test cases. Imputing missing values for the lab tests did
    not yield performance comparable to MTL on this problem. Similar experiments with
    feature nets [17] also failed to yield improvements comparable to MTL.
  id: totrans-2238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还进行了实验，使用未来的实验室测试作为输入，训练一个预测风险的网络，并在未来的测试案例中缺失时对实验室测试的值进行插补。对实验室测试缺失值的插补未能在这个问题上达到与MTL相当的性能。与特征网络的类似实验[17]也未能获得与MTL相当的改进。
- en: '![176_image_0.png](176_image_0.png)'
  id: totrans-2239
  prefs: []
  type: TYPE_IMG
  zh: '![176_image_0.png](176_image_0.png)'
- en: Fig. 8.7. Performance of STL, MTL, and MTL with shuffled extra task signals
    on pneumonia risk prediction at the five FOPs
  id: totrans-2240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7. STL、MTL以及带有洗牌额外任务信号的MTL在五个FOP下的肺炎风险预测性能
- en: Future measurements are available in many *offline* learning problems. As just
    one very different example, a robot or autonomous vehicle can more accurately
    measure the size, location, and identity of objects if it passes nearer them in
    the future. For example, road stripes and the edge of the road can be detected
    reliably as a vehicle passes alongside them, but detecting them far ahead of the
    vehicle is hard. Since driving brings future road closer to the car, stripes and
    road borders can be measured accurately as the car passes them. Dead reckoning
    allows these future measurements to be added to the training set. They can't be
    used as *inputs*; They won't be available in time while driving. But they can
    be used to augment a training set. We suspect that using future measurements as
    extra outputs will be a frequent source of extra tasks in real problems.
  id: totrans-2241
  prefs: []
  type: TYPE_NORMAL
  zh: 未来测量在许多*离线*学习问题中是可用的。作为一个非常不同的例子，机器人或自主车辆如果在未来接近物体时，可以更准确地测量物体的大小、位置和身份。例如，当车辆经过时，路面标线和路边缘可以被可靠地检测，但在车辆前方远处检测它们是困难的。由于驾驶将未来的道路带得离车更近，车辆经过时可以准确测量标线和路边。死算允许将这些未来测量添加到训练集中。它们不能作为*输入*使用；在驾驶时不会及时可用。但可以用来增强训练集。我们怀疑使用未来测量作为额外输出将在实际问题中频繁成为额外任务的来源。
- en: 8.2.2 Multiple Metrics
  id: totrans-2242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.2 多种度量
- en: Sometimes it is hard to capture everything that is important in one error metric.
    When alternate metrics capture different, but useful, aspects of a problem, MTL
  id: totrans-2243
  prefs: []
  type: TYPE_NORMAL
  zh: 有时很难在一个错误度量中捕捉到所有重要内容。当不同的替代度量捕捉到问题的不同但有用的方面时，MTL
- en: can be used to benefit from the multiple metrics. One example of this is the
    pneumonia problem in the previous section. Rankprop outperforms backprop using
    traditional squared error on this problem, but has trouble learning to rank cases
    at such low risk that virtually all patients survive because these cases provide
    little ordering information. Interestingly, squared error performs best when cases
    have high purity, such as in regions of feature space where most cases have low
    risk. *Squared error is at its best where rankprop is weakest.* Adding an extra
    output trained with squared error to a net learning to predict pneumonia risk
    with rankprop improves the accuracy of the rankprop output an additional 5-10%
    for the least-risk cases. The earliest example of using multiple output representations
    we know of is [38] which uses both SSE and cross-entropy outputs for the same
    task.
  id: totrans-2244
  prefs: []
  type: TYPE_NORMAL
  zh: 可以利用多种度量来获益。一个例子是上一节提到的肺炎问题。在这个问题上，Rankprop在使用传统平方误差时表现优于反向传播，但在处理几乎所有患者都生存的低风险病例时却难以学习排序，因为这些病例提供的信息很少。有趣的是，当病例具有高纯度时，例如在大多数病例风险较低的特征空间区域，平方误差表现最佳。*平方误差在Rankprop最弱的地方表现最佳。*
    将一个使用平方误差训练的额外输出添加到学习使用Rankprop预测肺炎风险的网络中，可以将最低风险病例的Rankprop输出的准确度提高额外的5-10%。我们所知的使用多种输出表示的最早例子是[38]，它为同一任务使用了SSE和交叉熵输出。
- en: 8.2.3 Multiple Output Representations
  id: totrans-2245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.3 多种输出表示
- en: Sometimes it is not apparent what output encoding is best for a problem. Distributed
    output representations often help *parts* of a problem be learned well because
    the parts have separate error gradients. But non-distributed output representations
    are sometimes more accurate. Consider the problem of learning to classify a face
    as one of twenty faces. One output representation is to have one output code for
    each face. Another representation is to have outputs code for features such as
    beard/no_beard, glasses/no_glasses, long_hair/short, eye_color(blue, brown), male/female,
    that are sufficient to distinguish the faces.
  id: totrans-2246
  prefs: []
  type: TYPE_NORMAL
  zh: 有时不明显什么样的输出编码最适合某个问题。分布式输出表示通常有助于*部分*问题的良好学习，因为这些部分具有独立的误差梯度。但非分布式输出表示有时更为准确。考虑学习将面孔分类为二十张面孔之一的问题。一种输出表示是为每张面孔设定一个输出码。另一种表示是对足以区分面孔的特征进行编码，如胡须/无胡须、眼镜/无眼镜、长发/短发、眼睛颜色（蓝色、棕色）、男性/女性。
- en: Correct classification, however, may require that each feature be correctly
    predicted. The non-distributed output coding that uses one output for each individual
    may be more reliable. But training the net to recognize specific traits should
    help, too. MTL is one way to merge these conflicting requirements in one net by
    using both output representations, even if only one representation will be used
    for prediction.
  id: totrans-2247
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正确的分类可能要求每个特性都被正确预测。对每个个体使用一个输出的非分布式输出编码可能更可靠。但训练网络以识别特定特征也应该有帮助。MTL是一种通过使用两种输出表示来将这些相互冲突的需求合并到一个网络中的方法，即使只有一种表示会用于预测。
- en: A related approach to multiple output encodings is error correcting codes
  id: totrans-2248
  prefs: []
  type: TYPE_NORMAL
  zh: 一种与多输出编码相关的方法是纠错编码。
- en: '[18]. Here, multiple encodings for the outputs are designed so that the combined
    prediction is less sensitive to occasional errors in some of the outputs. It is
    not clear how much ECOC benefits from MTL-like mechanisms. In fact, ECOC may benefit
    from being trained on STL nets (instead of MTL nets) so that different outputs
    do not share the same hidden layer and thus are less correlated. But see [27]
    for ways of using MTL to *decorrelate* errors in multiple outputs to boost committee
    machine performance.'
  id: totrans-2249
  prefs: []
  type: TYPE_NORMAL
  zh: '[18]。在这里，为输出设计了多个编码，以便组合预测对某些输出中偶发错误的敏感度较低。尚不清楚ECOC在多任务学习（MTL）机制下能受益多少。实际上，ECOC可能会受益于在单任务学习（STL）网络上进行训练（而不是MTL网络），以便不同的输出不共享同一隐藏层，从而减少相关性。但请参见[27]以了解如何使用MTL来*去相关*多个输出中的错误，以提升委员会机器的性能。'
- en: 8.2.4 Time Series Prediction
  id: totrans-2250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.4 时间序列预测
- en: The simplest way to use MTL for time series prediction is to use a single net
    with multiple outputs, each output corresponding to the same task at a different
    time. This net makes predictions for the same task at different times. We tested
    this on a robot domain where the goal is to predict what the robot will sense
    1, 2, 4, and 8 meters in the future as it moves forward. Training all four of
    these distances on one MTL net improved the accuracy of the long range predictions
    about 10% (see chapter 17 where MTL is used in a time series application).
  id: totrans-2251
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MTL进行时间序列预测的最简单方法是使用一个具有多个输出的单一网络，每个输出对应于在不同时间的相同任务。这个网络对同一任务在不同时间进行预测。我们在一个机器人领域进行了测试，目标是预测机器人在向前移动时1、2、4和8米后的感知。将这四个距离在一个MTL网络上进行训练，使得长距离预测的准确性提高了约10%（参见第17章，其中MTL用于时间序列应用）。
- en: 8.2.5 Using Non-Operational Features
  id: totrans-2252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.5 使用非操作性特征
- en: Some features are impractical to use at run time, either because they are too
    expensive to compute, or because they need human expertise that won't be available.
    We usually have more time, however, to prepare our training sets.
  id: totrans-2253
  prefs: []
  type: TYPE_NORMAL
  zh: 一些特性在运行时使用起来不够实用，要么是因为计算成本太高，要么是因为需要人类专业知识，而这些知识在运行时无法获得。然而，我们通常有更多时间来准备训练集。
- en: When it is impractical to compute some features on the fly at run time, but
    practical to compute them for the training set, these features can be used as
    extra outputs to help learning. Pattern recognition provides a good example of
    this. We tested MTL on a door recognition problem where the goal is to recognize
    doorways and doorknobs. The extra tasks were features such as the location of
    door edges and doorway centers that required laborious hand labelling that would
    not be applied to the test set. The MTL nets that were trained to predict these
    additional hand-labelled features were 25% more accurate at locating doors and
    doorknobs. Other domains where hand-labelling can be used to augment training
    sets this way include text domains, medical domains, acoustic domains, and speech
    domains.
  id: totrans-2254
  prefs: []
  type: TYPE_NORMAL
  zh: 当在运行时动态计算某些特性不切实际，但为训练集计算这些特性是可行时，可以将这些特性作为额外的输出用于帮助学习。模式识别就是一个很好的例子。我们在一个门识别问题上测试了MTL，目标是识别门和门把手。额外的任务包括门边缘和门口中心的位置，这些特性需要繁琐的人工标记，而这些标记不会应用于测试集。经过训练以预测这些额外手工标记特性的MTL网络在定位门和门把手方面的准确性提高了25%。其他可以通过这种方式使用手工标记来增强训练集的领域包括文本领域、医学领域、声学领域和语音领域。
- en: 8.2.6 Using Extra Tasks To Focus Attention
  id: totrans-2255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.6 使用额外任务来集中注意力
- en: Learning often uses large, ubiquitous patterns in the inputs, while ignoring
    small or less common inputs that might also be useful. MTL can be used to coerce
    the learner to attend to patterns in the input it would otherwise ignore. This
    is done by forcing it to learn internal representations to support related tasks
    that depend on these patterns.
  id: totrans-2256
  prefs: []
  type: TYPE_NORMAL
  zh: 学习通常使用输入中的大而普遍的模式，同时忽略可能也有用的小或不常见的输入。MTL可以被用来强制学习者关注它本来会忽略的输入模式。这是通过强迫它学习支持这些模式相关任务的内部表征来实现的。
- en: A good example is road following. Here, STL nets often ignore lane markings
    when learning to steer because lane markings are usually a small part of the image,
    are constantly changing, and are often difficult to see (even for humans).
  id: totrans-2257
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的例子是道路跟随。在这里，STL网络在学习转向时往往忽略车道标记，因为车道标记通常是图像中的小部分，持续变化，并且通常难以识别（即使是人类也如此）。
- en: If a net learning to steer is also required to learn to recognize road stripes,
    the net will learn to attend to those parts of the image where stripes occur.
    To the extent that the stripe tasks are learnable, the net will develop internal
    representations to support them. Since the net is also learning to steer using
    the same hidden layer, the steering task can use the parts of the stripe hidden
    representation that are useful for steering.
  id: totrans-2258
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个学习转向的网络还需要学习识别道路条纹，那么该网络将学会关注图像中出现条纹的部分。在条纹任务可以学习的程度上，网络将发展出支持这些任务的内部表征。由于网络也在使用相同的隐藏层学习转向，因此转向任务可以利用对转向有用的条纹隐藏表征部分。
- en: We tested this idea using a road image simulator developed by Pomerleau to permit
    rapid testing of learning methods for road-following domains [28].
  id: totrans-2259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Pomerleau开发的道路图像模拟器测试了这个想法，以快速测试道路跟随领域的学习方法[28]。
- en: Figure 8.8 shows several 2-D road images.
  id: totrans-2260
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8展示了几张二维道路图像。
- en: 'The principal task is to predict steering direction. For the MTL experiments,
    we used eight additional tasks:'
  id: totrans-2261
  prefs: []
  type: TYPE_NORMAL
  zh: 主要任务是预测转向方向。在MTL实验中，我们使用了八个附加任务：
- en: '- whether the road is one or two lanes - location of centerline (if any) -
    location of left edge of road - location of right edge of road'
  id: totrans-2262
  prefs: []
  type: TYPE_NORMAL
  zh: '- 道路是单车道还是双车道 - 中线的位置（如果有） - 道路左边缘的位置 - 道路右边缘的位置'
- en: '- location of road center - intensity of road surface - intensity of region
    bordering road - intensity of centerline (if any)'
  id: totrans-2263
  prefs: []
  type: TYPE_NORMAL
  zh: '- 道路中心的位置 - 道路表面的强度 - 边缘区域的强度 - 中线的强度（如果有）'
- en: These additional tasks are all computable from the internal variables in the
    simulator. Table 8.3 shows the average performance of ten runs of single and multitask
    learning on each of these tasks. The MTL net has 32 inputs, 16 hidden units, and
    9 outputs. The 36 STL nets have 32 inputs, 2, 4, 8 or 16 hidden units, and 1 output
    each.
  id: totrans-2264
  prefs: []
  type: TYPE_NORMAL
  zh: 这些附加任务都可以从模拟器的内部变量计算得出。表8.3显示了在每个任务上进行单任务和多任务学习的十次运行的平均性能。MTL网络有32个输入，16个隐藏单元和9个输出。36个STL网络各有32个输入，2、4、8或16个隐藏单元，每个网络有1个输出。
- en: The last two columns compare STL and MTL. The first is the percent reduction
    in error of MTL over the best STL run. Negative percentages indicate MTL
  id: totrans-2265
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两列比较了STL和MTL。第一列是MTL相对于最佳STL运行的误差百分比减少。负百分比表示MTL
- en: '![179_image_0.png](179_image_0.png)'
  id: totrans-2266
  prefs: []
  type: TYPE_IMG
  zh: '![179_image_0.png](179_image_0.png)'
- en: Fig. 8.8. Sample single and two lane roads generated with Pomerleau's road simulator
  id: totrans-2267
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8. 使用Pomerleau的道路模拟器生成的单车道和双车道的样本图像。
- en: performs better. The last column is the percent improvement of MTL over the
    average STL performance. On the important steering task, MTL outperforms STL 15–30%.
  id: totrans-2268
  prefs: []
  type: TYPE_NORMAL
  zh: 表现更好。最后一列是MTL相对于平均STL性能的百分比改善。在重要的转向任务上，MTL的表现优于STL，提升幅度为15%至30%。
- en: We ran a follow-up experiment to test how important centerstripes are to the
    STL and MTL nets. We eliminated the stripes from the images in a test set. If
    MTL learned more about centerstripes than STL, and uses what it learned about
    centerstripes for the main steering task, we expect to see steering performance
    degrade more for MTL than for STL when we remove the centerstripes from the images.
    Error increased more for the MTL nets than for the STL nets, suggesting the MTL
    nets are making more use of the stripes in the images.
  id: totrans-2269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了后续实验，以测试中心条纹对STL和MTL网络的重要性。我们从测试集中去除了图像中的条纹。如果MTL对中心条纹的学习超过了STL，并且利用所学的中心条纹来完成主要的转向任务，那么当我们从图像中去除中心条纹时，我们预计MTL的转向性能会比STL更差。MTL网络的误差增加幅度大于STL网络，这表明MTL网络在图像中更充分地利用了条纹。
- en: Table 8.3. Performance of STL and MTL on the road following domain. The underlined
    entries in the STL columns are the STL runs that performed best. Differences statistically
    significant at .05 or better are marked with an *.
  id: totrans-2270
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.3. STL和MTL在道路跟踪领域的表现。STL列中带下划线的条目是表现最佳的STL运行。差异在统计上显著的在0.05或更好水平上标记为*。
- en: '| ROOT-MEAN SQUARED ERROR ON TEST SET   |                            |      |            |            |             |             |          |'
  id: totrans-2271
  prefs: []
  type: TYPE_TB
  zh: '| 测试集上的均方根误差                  |                            |      |            |            |             |             |          |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-2272
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| TASK                                  | Single Task Backprop (STL) | MTL  |
    Change MTL | Change MTL |             |             |          |'
  id: totrans-2273
  prefs: []
  type: TYPE_TB
  zh: '| 任务                                 | 单任务反向传播（STL）      | MTL  | 改变MTL   |
    改变MTL   |             |             |          |'
- en: '| 2HU                                   | 4HU                        | 8HU  |
    16HU       | 16HU       | to Best STL | to Mean STL |          |'
  id: totrans-2274
  prefs: []
  type: TYPE_TB
  zh: '| 2HU                                   | 4HU                        | 8HU  |
    16HU       | 16HU       | 到最佳STL  | 到均值STL  |          |'
- en: '| 1 or 2 Lanes                          | .201                       | .209
    | .207       | .178       | .156        | -12.4% *    | -21.5% * |'
  id: totrans-2275
  prefs: []
  type: TYPE_TB
  zh: '| 1或2车道                             | .201                       | .209 | .207       |
    .178       | .156        | -12.4% *   | -21.5% * |'
- en: '| Left Edge                             | .069                       | .071
    | .073       | .073       | .062        | -10.1% *    | -13.3% * |'
  id: totrans-2276
  prefs: []
  type: TYPE_TB
  zh: '| 左边缘                               | .069                       | .071 | .073       |
    .073       | .062        | -10.1% *   | -13.3% * |'
- en: '| Right Edge                            | .076                       | .062
    | .058       | .056       | .051        | -8.9% *     | -19.0% * |'
  id: totrans-2277
  prefs: []
  type: TYPE_TB
  zh: '| 右边缘                               | .076                       | .062 | .058       |
    .056       | .051        | -8.9% *    | -19.0% * |'
- en: '| Line Center                           | .153                       | .152
    | .152       | .152       | .151        | -0.7%       | -0.8%    |'
  id: totrans-2278
  prefs: []
  type: TYPE_TB
  zh: '| 行中心                               | .153                       | .152 | .152       |
    .152       | .151        | -0.7%       | -0.8%    |'
- en: '| Road Center                           | .038                       | .037
    | .039       | .042       | .034        | -8.1% *     | -12.8% * |'
  id: totrans-2279
  prefs: []
  type: TYPE_TB
  zh: '| 道路中心                             | .038                       | .037 | .039       |
    .042       | .034        | -8.1% *    | -12.8% * |'
- en: '| Road Greylevel                        | .054                       | .055
    | .055       | .054       | .038        | -29.6% *    | -30.3% * |'
  id: totrans-2280
  prefs: []
  type: TYPE_TB
  zh: '| 道路灰度级                           | .054                       | .055 | .055       |
    .054       | .038        | -29.6% *   | -30.3% * |'
- en: '| Edge Greylevel                        | .037                       | .038
    | .039       | .038       | .038        | 2.7%        | 0.0%     |'
  id: totrans-2281
  prefs: []
  type: TYPE_TB
  zh: '| 边缘灰度级                           | .037                       | .038 | .039       |
    .038       | .038        | 2.7%        | 0.0%     |'
- en: '| Line Greylevel                        | .054                       | .054
    | .054       | .054       | .054        | 0.0%        | 0.0%     |'
  id: totrans-2282
  prefs: []
  type: TYPE_TB
  zh: '| 行灰度级                             | .054                       | .054 | .054       |
    .054       | .054        | 0.0%        | 0.0%     |'
- en: '| Steering                              | .093                       | .069
    | .087       | .072       | .058        | -15.9% *    | -27.7% * |'
  id: totrans-2283
  prefs: []
  type: TYPE_TB
  zh: '| 转向                                 | .093                       | .069 |
    .087       | .072       | .058        | -15.9% *   | -27.7% * |'
- en: '8.2.7 Hints: Tasks Hand-Crafted By A Domain Expert'
  id: totrans-2284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.7 提示：由领域专家手工制作的任务
- en: Extra outputs can be used to *inject rule hints* into nets about what they should
    learn [32, 33]. This is MTL where the extra tasks are carefully engineered to
    coerce the net to learn specific internal representations. Hints can also be provided
    to backprop nets via extra terms in the error signal backpropagated for the main
    task output [1, 2]. The extra error terms constrain what is learned to satisfy
    desired properties of main task such as monotonicity [31], symmetry, or transitivity
    with respect to certain sets of inputs. MTL, which does not use extra error terms
    on task outputs, could be used in concert with these techniques.
  id: totrans-2285
  prefs: []
  type: TYPE_NORMAL
  zh: 额外输出可以用来*注入规则提示*到网络中，指引它们应该学习什么[32, 33]。这是MTL，其中额外任务经过精心设计，以强迫网络学习特定的内部表示。提示也可以通过在主任务输出的误差信号中反向传播的额外项提供给反向传播网络[1,
    2]。额外的误差项限制了所学内容，以满足主任务所需的性质，例如单调性[31]、对称性或与某些输入集的传递性。MTL如果不在任务输出上使用额外的误差项，可以与这些技术配合使用。
- en: 8.2.8 Handling Other Categories In Classification
  id: totrans-2286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.8 分类中的其他类别处理
- en: In real-world applications of digit recognition, some of the images given to
    the classifier may be alphabetic characters or punctuation marks instead of digits.
    One way to prevent accidentally classifying a "t" as a one or seven is to create
    an "other" category that is the correct classification for non-digit images. The
    large variety of characters mapped to this "other" class makes learning this class
    potentially very difficult. MTL suggests an alternate way to do this. Split the
    "other" class into separate classes for the individual characters that are trained
    in parallel with the main digit tasks. A single output coding for the "other"
    class can be used, as well. Breaking the "other" category into multiple tasks
    gives the net more learnable error signal for these cases [26].
  id: totrans-2287
  prefs: []
  type: TYPE_NORMAL
  zh: 在数字识别的实际应用中，提供给分类器的某些图像可能是字母字符或标点符号，而不是数字。防止意外将“t”分类为一或七的一种方法是创建一个“其他”类别，以便正确分类非数字图像。映射到这个“其他”类别的大量字符使得学习这个类别变得非常困难。MTL建议一种替代方法。将“其他”类别分为针对个别字符的单独类别，并与主要数字任务并行训练。“其他”类别的单一输出编码也可以使用。将“其他”类别分解为多个任务，为这些情况提供了更多可学习的错误信号[26]。
- en: 8.2.9 Sequential Transfer
  id: totrans-2288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.9 顺序迁移
- en: MTL is parallel transfer. Often tasks arise serially and we can't wait for all
    of them to begin learning. In these cases we can use parallel transfer to perform
    sequential transfer. If the training data can be stored, do MTL using whatever
    tasks are available when it is time to start learning, and re-train as new tasks
    or new data arise. If training data cannot be stored, or if we already have models
    for which data is not available, we can still use MTL. Use the models to generate
    synthetic data that is then used as extra training signals. This approach to sequential
    transfer avoids catastrophic interference (forgetting old tasks while learning
    new ones). Moreover, it is applicable where the analytical methods of evaluating
    domain theories required by some serial transfer methods [29, 34] are not available.
    For example, the domain theory need not be differentiable, it only needs to make
    predictions. One issue that arises when synthesizing data from prior models is
    what distribution to sample from. See [16] for a discussion of synthetic sampling.
  id: totrans-2289
  prefs: []
  type: TYPE_NORMAL
  zh: MTL是并行迁移。通常任务是顺序产生的，我们无法等到所有任务开始学习。在这些情况下，我们可以使用并行迁移来执行顺序迁移。如果可以存储训练数据，则在开始学习时使用可用任务进行MTL，并在出现新任务或新数据时重新训练。如果无法存储训练数据，或者我们已经有模型而数据不可用，我们仍然可以使用MTL。利用模型生成合成数据，然后将其作为额外的训练信号。这个顺序迁移的方法避免了灾难性干扰（在学习新任务时忘记旧任务）。此外，这适用于某些顺序迁移方法[29,
    34]所需的评估领域理论的分析方法不可用的情况。例如，领域理论不必是可微的，只需能够做出预测。在从先前模型合成数据时出现的一个问题是要从哪个分布中采样。有关合成采样的讨论，请参见[16]。
- en: 8.2.10 Similar Tasks With Different Data Distributions
  id: totrans-2290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.10 不同数据分布下的相似任务
- en: Sometimes there are multiple instances of the same problem, but the distribution
    of samples differs for each instantiation. For example, most hospitals diagnose
    and treat the same diseases, but the demographics of the patients each hospital
    serves is different. Hospitals in Florida see older patients, urban hospitals
    see poorer patients, etc. Models trained separately for each hospital would perform
    best, but often there is insufficient data to train a separate model for each
    hospital. Pooling the data, however, may not lead to models that are accurate
    for each hospital. MTL provides one solution to this problem. Use one net to make
    predictions for each hospital, using a different output on the net for each hospital.
    Because each patient is a training case for only one hospital, error can be backpropagated
    only through the one output that has a target value for each input vector.
  id: totrans-2291
  prefs: []
  type: TYPE_NORMAL
  zh: 有时同一个问题会有多个实例，但每个实例的样本分布却不同。例如，大多数医院诊断和治疗相同的疾病，但每个医院服务的患者人群特征不同。佛罗里达的医院接诊年长患者，城市医院则接诊经济条件较差的患者等。为每个医院单独训练的模型效果最佳，但往往缺乏足够的数据为每个医院训练单独的模型。然而，汇总数据可能不会产生对每个医院准确的模型。MTL提供了一个解决方案。使用一个网络对每个医院进行预测，为每个医院使用不同的输出。因为每位患者仅是一个医院的训练案例，错误只能通过具有每个输入向量目标值的那一个输出进行反向传播。
- en: 8.2.11 Learning With Hierarchical Data
  id: totrans-2292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.11 使用层次数据学习
- en: In many domains, the data falls in a hierarchy of classes. Most applications
    of machine learning to hierarchical data make little use of the hierarchy. MTL
    provides one way of exploiting hierarchical information. When training a model
    to classify data at one level in the hierarchy, include as extra tasks the classification
    tasks that arise for ancestors, descendants, and siblings of the current classification
    task. The easiest way to to accomplish this is to train one MTL net to predict
    all class distinctions in the hierarchy at the same time.
  id: totrans-2293
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多领域，数据处于类的层次结构中。大多数将机器学习应用于层次数据的应用很少利用层次结构。MTL提供了一种利用层次信息的方法。当训练一个模型来对层次结构中的某一层次数据进行分类时，将当前分类任务的祖先、后代和兄弟的分类任务作为额外任务包含在内。实现这一点的最简单方法是训练一个MTL网络，同时预测层次结构中的所有类别区分。
- en: 8.2.12 Some Inputs Work Better As Outputs
  id: totrans-2294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.12 一些输入作为输出效果更佳
- en: The common practice in backprop nets is to use all features that will be available
    for test cases as inputs, and have outputs only for tasks that need to be predicted.
    On real problems, however, learning often works better given a carefully selected
    subset of the features to use inputs[7, 23, 24]. One way to benefit from features
    not used as inputs is to use them as extra outputs for MTL. We've done experiments
    with both synthetic and real problems where moving some features from the input
    side of the net to the output side of the net improves performance on the main
    task. We use feature selection to select those features that should be used as
    inputs, and then treat some of the remaining features as extra tasks.
  id: totrans-2295
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播网络中，常见的做法是将所有可用于测试案例的特征作为输入，仅对需要预测的任务生成输出。然而，在实际问题中，给定经过精心选择的特征子集作为输入时，学习效果通常更好[7,
    23, 24]。从未用作输入的特征中获益的一种方法是将其作为MTL的额外输出。我们在合成和真实问题上进行了实验，发现将一些特征从网络的输入侧移至输出侧可以提高主要任务的性能。我们使用特征选择来选择应作为输入的特征，然后将一些剩余特征视为额外任务。
- en: Figure 8.9 shows the ROC Area on a pneumonia problem as the number of input
    features on the backprop net varies.7 ROC Areas closer to 1 indicate better performance.
    There are 192 features available for most patients. Using all 192 features as
    inputs (Net1) is suboptimal. Better performance is obtained by using the first
    50 features selected with feature selection (Net2). The horizontal line at the
    top of the graph (Net3) shows the ROC Area obtained by using the first 50 features
    as inputs, and the *next* 100 features as extra *outputs*. Using these same 150
    features all as inputs (Net4) yields worse performance.8
  id: totrans-2296
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9显示了肺炎问题的ROC面积，随着反向传播网络输入特征数量的变化而变化。ROC面积越接近1，性能越好。大多数患者有192个特征可用。将所有192个特征作为输入（Net1）并不是最优的。通过使用前50个通过特征选择选出的特征（Net2）可以获得更好的性能。图表顶部的水平线（Net3）显示了使用前50个特征作为输入，以及*接下来的*100个特征作为额外的*输出*所获得的ROC面积。将这150个特征全部作为输入（Net4）会导致性能下降。8
- en: 7 This is not the same pneumonia problem used in section 8.2.1. 8 Although the
    95% confidence intervals for Net2 and Net3 overlap with ten trials, a paired t-test
    shows the results are significant at .01.
  id: totrans-2297
  prefs: []
  type: TYPE_NORMAL
  zh: 7 这与8.2.1节中使用的肺炎问题不同。8 尽管Net2和Net3的95%置信区间在十次试验中重叠，但配对t检验显示结果在.01水平上显著。
- en: '![182_image_0.png](182_image_0.png)'
  id: totrans-2298
  prefs: []
  type: TYPE_IMG
  zh: '![182_image_0.png](182_image_0.png)'
- en: Fig. 8.9. ROC Area on the Pneumonia Risk Prediction Task vs. the number of input
    features used by the backprop net
  id: totrans-2299
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9。肺炎风险预测任务的ROC面积与反向传播网络使用的输入特征数量的关系
- en: 8.3 Getting The Most Out Of Mtl
  id: totrans-2300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 最大限度地利用MTL
- en: The basic machinery for doing multitask learning in neural nets is present in
    backprop. Backprop, however, was not designed to do MTL well. This chapter presents
    suggestions for how to make MTL in backprop nets work better. Some of the suggestions
    are counterintuitive, but if not used, can cause MTL to hurt generalization on
    the main task instead of helping it.
  id: totrans-2301
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中进行多任务学习的基本机制在反向传播中是存在的。然而，反向传播并不是为了很好地执行MTL而设计的。本章提供了如何使反向传播网络中的MTL更有效的建议。其中一些建议可能违反直觉，但如果不使用，可能会导致MTL对主要任务的泛化造成伤害，而不是帮助它。
- en: 8.3.1 Use Large Hidden Layers
  id: totrans-2302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3.1 使用大隐藏层
- en: The basic idea behind MTL in backprop nets is that what is learned in the hidden
    layer for one task can be useful to other tasks. MTL works when tasks share hidden
    units. One might think that small hidden layers would help MTL by promoting sharing
    between tasks.
  id: totrans-2303
  prefs: []
  type: TYPE_NORMAL
  zh: MTL在反向传播网络中的基本理念是，隐藏层中为一个任务学习到的知识对其他任务也有用。当任务共享隐藏单元时，MTL有效。有人可能认为小的隐藏层通过促进任务间的共享将有助于MTL。
- en: For the kinds of problems we've examined here, this usually does not work.
  id: totrans-2304
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们在这里研究的这类问题，这通常是无效的。
- en: Usually, tasks are different enough that much of what each task needs to learn
    does not transfer to many (or any) other tasks. Using a large hidden layer insures
    that there are enough hidden units for tasks to learn independent hidden layer
    representations when they need to. Sharing can still occur, but only when the
    overlap between the hidden layer representations for different tasks is strong.
    In many real-world problems, the loss in accuracy that results from forcing tasks
    to share by keeping the hidden layer small is larger than the benefit that arises
    from the sharing. Usually it is important to use large hidden layers with MTL.
  id: totrans-2305
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，任务之间的差异足够大，以至于每个任务需要学习的许多内容不会转移到其他（或任何）任务。使用大隐藏层可以确保有足够的隐藏单元，让任务在需要时学习独立的隐藏层表示。共享仍然可以发生，但只有当不同任务的隐藏层表示之间的重叠非常强时。在许多实际问题中，因强制任务共享而保持隐藏层较小所导致的准确性损失，往往大于由共享带来的收益。通常，在MTL中使用大型隐藏层非常重要。
- en: 8.3.2 Do Early Stopping For Each Task Separately
  id: totrans-2306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3.2 对每个任务单独进行早停
- en: The classic NETtalk application [30] used one trained both phonemes and stresses
    on one backprop net. NETtalk is an early example of MTL. But the builders
  id: totrans-2307
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的NETtalk应用[30]在一个反向传播网络上训练了音素和重音。NETtalk是多任务学习（MTL）的早期示例。但其构建者
- en: '![183_image_0.png](183_image_0.png)'
  id: totrans-2308
  prefs: []
  type: TYPE_IMG
  zh: '![183_image_0.png](183_image_0.png)'
- en: Fig. 8.10. On NETtalk, the Stress task trains very quickly and overfits long
    before the Phoneme task reaches peak performance
  id: totrans-2309
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10. 在NETtalk中，重音任务训练得非常快，并且在音素任务达到峰值性能之前就已经过拟合。
- en: of NETtalk viewed the multiple outputs as codings for a single problem, not
    as independent tasks that benefited each other by being trained together.
  id: totrans-2310
  prefs: []
  type: TYPE_NORMAL
  zh: NETtalk的构建者将多个输出视为单个问题的编码，而不是作为通过共同训练相互受益的独立任务。
- en: Figure 8.10 shows the learning curves for the phoneme and stress subtasks separately.
    It is clear that the stress tasks begin to overfit before the phoneme tasks reach
    peak performance. Better performance could be obtained on NETtalk by doing early
    stopping on the stress and phoneme tasks individually, or by balancing their learning
    rates so they reach peak performance at the same time.
  id: totrans-2311
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10展示了音素和重音子任务的学习曲线。很明显，重音任务在音素任务达到峰值性能之前就开始出现过拟合。通过对重音和音素任务分别进行早停，或者通过平衡它们的学习率，使它们同时达到峰值性能，可以获得更好的性能。
- en: Early stopping prevents overfitting by halting the training of error-driven
    procedures like backprop before they achieve minimum error on the training set
  id: totrans-2312
  prefs: []
  type: TYPE_NORMAL
  zh: 早停通过在错误驱动程序（如反向传播）达到训练集的最小误差之前停止训练，从而防止过拟合。
- en: (see chapter 2). Recall the steering prediction problem from section 8.2.6.
    We applied MTL to this problem by training a net on eight extra tasks in addition
    to the main steering task. Figure 8.11 shows nine learning curves, one for each
    of the tasks on this MTL net. Each graph is the validation set error during training.
  id: totrans-2313
  prefs: []
  type: TYPE_NORMAL
  zh: （见第2章）。回想8.2.6节的转向预测问题。我们通过在主要转向任务之外训练一个网络来解决八个额外的任务，应用了MTL。图8.11显示了九条学习曲线，每条曲线对应于这个MTL网络的一个任务。每个图表显示了训练期间验证集的误差。
- en: Table 8.4 shows the best place to halt each task. There is no one epoch where
    training can be stopped so as to achieve maximum performance on all tasks. If
    all tasks are important, and one net is used to predict all the tasks, halting
    training where the error summed across all outputs is minimized is the best you
    can do. Figure 8.12 shows the combined RMS error of the nine tasks. The best average
    RMSE occurs at 75,000 backprop passes.
  id: totrans-2314
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.4显示了每个任务停止的最佳时机。没有一个周期可以停止训练，以便在所有任务上实现最大性能。如果所有任务都很重要，并且使用一个网络来预测所有任务，那么在所有输出的误差总和最小化的位置停止训练是你能做的最佳选择。图8.12显示了九个任务的综合均方根误差（RMS误差）。最佳平均RMSE出现在75,000次反向传播传递时。
- en: '![184_image_0.png](184_image_0.png)'
  id: totrans-2315
  prefs: []
  type: TYPE_IMG
  zh: '![184_image_0.png](184_image_0.png)'
- en: Fig. 8.11. Test-Set Performance of MTL Net Trained on Nine Tasks
  id: totrans-2316
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11. 在九个任务上训练的MTL网络的测试集性能
- en: But using one net to make predictions for all the tasks is suboptimal. Better
    performance is achieved by using the validation set to do early stopping on each
    output individually. The trick is to make a copy of the net at the epoch where
    performance on each task is best, and use this copy to make predictions for that
    task. After making each copy, continue training the net until the other tasks
    reach peak performance. Sometimes, it is best to continue training all outputs
    on the net, including those that have begun to overfit. Sometimes, it is better
    to stop training (or use a lower learning rate) for outputs that have begun to
    overfit. Keep in mind that once an output has begun to overfit, we no longer care
    how well the net performs on that task because we have a copy of the net from
    an earlier epoch when performance on that task was best. The only reason to continue
    training the task is because it may benefit other tasks that have not reached
    peak performance yet.
  id: totrans-2317
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，使用一个网络对所有任务进行预测并不是最佳选择。通过使用验证集对每个输出进行单独的提前停止，可以获得更好的性能。诀窍是在每个任务表现最佳的时期复制网络，并使用这个副本对该任务进行预测。在制作每个副本后，继续训练网络，直到其他任务达到最佳性能。有时，最好继续训练所有输出，包括那些已经开始过拟合的输出。有时，对于那些已经开始过拟合的输出，停止训练（或使用较低的学习率）会更好。请记住，一旦某个输出开始过拟合，我们就不再关心网络在该任务上的表现，因为我们有一个早期时期的网络副本，当时该任务的表现最佳。继续训练该任务的唯一原因是它可能有利于尚未达到最佳性能的其他任务。
- en: '![185_image_0.png](185_image_0.png)'
  id: totrans-2318
  prefs: []
  type: TYPE_IMG
  zh: '![185_image_0.png](185_image_0.png)'
- en: Fig. 8.12. Combined Test-Set Performance on all Nine Tasks Table 8.4. Performance
    of MTL on the nine tasks in the steering domain when training is halted on each
    task individually compared with halting using the combined error across all tasks
  id: totrans-2319
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12. 所有九个任务的组合测试集性能 表 8.4. 在每个任务单独停止训练时，与通过所有任务的合并误差停止训练MTL的九个任务的性能对比。
- en: '| TASK              | Halted Individually   | Halted Combined   | Difference   |             |       |'
  id: totrans-2320
  prefs: []
  type: TYPE_TB
  zh: '| 任务              | 单独停止             | 合并停止         | 差异         |             |       |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-2321
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|                   | BP Pass               | Performance       | BP Pass      |
    Performance |       |'
  id: totrans-2322
  prefs: []
  type: TYPE_TB
  zh: '|                   | BP通过               | 性能             | BP通过      | 性能       |       |'
- en: '| 1: 1 or 2 Lanes   | 100000                | 0.444             | 75000        |
    0.456       | 2.7%  |'
  id: totrans-2323
  prefs: []
  type: TYPE_TB
  zh: '| 1: 1或2车道      | 100000                | 0.444             | 75000        |
    0.456       | 2.7%  |'
- en: '| 2: Left Edge      | 100000                | 0.309             | 75000        |
    0.321       | 3.9%  |'
  id: totrans-2324
  prefs: []
  type: TYPE_TB
  zh: '| 2: 左边缘        | 100000                | 0.309             | 75000        |
    0.321       | 3.9%  |'
- en: '| 3: Right Edge     | 100000                | 0.376             | 75000        |
    0.381       | 1.3%  |'
  id: totrans-2325
  prefs: []
  type: TYPE_TB
  zh: '| 3: 右边缘         | 100000                | 0.376             | 75000        |
    0.381       | 1.3%  |'
- en: '| 4: Line Center    | 75000                 | 0.486             | 75000        |
    0.486       | 0.0%  |'
  id: totrans-2326
  prefs: []
  type: TYPE_TB
  zh: '| 4: 中心线        | 75000                 | 0.486             | 75000        |
    0.486       | 0.0%  |'
- en: '| 5: Road Center    | 200000                | 0.208             | 75000        |
    0.239       | 14.9% |'
  id: totrans-2327
  prefs: []
  type: TYPE_TB
  zh: '| 5: 道路中心      | 200000                | 0.208             | 75000        |
    0.239       | 14.9% |'
- en: '| 6: Road Greylevel | 750000                | 0.552             | 75000        |
    0.680       | 23.2% |'
  id: totrans-2328
  prefs: []
  type: TYPE_TB
  zh: '| 6: 道路灰度级    | 750000                | 0.552             | 75000        |
    0.680       | 23.2% |'
- en: '| 7: Edge Greylevel | 375000                | 0.518             | 75000        |
    0.597       | 15.3% |'
  id: totrans-2329
  prefs: []
  type: TYPE_TB
  zh: '| 7: 边缘灰度级    | 375000                | 0.518             | 75000        |
    0.597       | 15.3% |'
- en: '| 8: Line Greylevel | 1                     | 1.010             | 75000        |
    1.158       | 14.7% |'
  id: totrans-2330
  prefs: []
  type: TYPE_TB
  zh: '| 8: 线条灰度级    | 1                     | 1.010             | 75000        |
    1.158       | 14.7% |'
- en: '| 9: Steering       | 125000                | 0.276             | 75000        |
    0.292       | 5.8%  |'
  id: totrans-2331
  prefs: []
  type: TYPE_TB
  zh: '| 9: 转向          | 125000                | 0.276             | 75000        |
    0.292       | 5.8%  |'
- en: Table 8.4 compares the performance of early stopping done per task with the
    performance one obtains by halting training for the entire MTL net at one place
    using the combined error. On average, early stopping for tasks individually reduces
    error 9.0%. This is a large difference. For some tasks, the performance of the
    MTL net is worse than the performance of STL on this task if the MTL net is not
    halted on that task individually.
  id: totrans-2332
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.4比较了按任务进行的提前停止性能与通过使用合并误差在一个地方停止整个MTL网络训练时获得的性能。平均而言，单独对任务进行的提前停止减少了9.0%的误差。这是一个很大的差异。对于某些任务，如果MTL网络在该任务上没有单独停止，其性能可能不如STL在该任务上的表现。
- en: Before leaving this topic, it should be mentioned that the training curves for
    the individual outputs on an MTL net are not necessarily monotonic. While it is
    not unheard of for the test-set error of an STL net to be multimodal, the trainingset
    error for an STL net should descend monotonically or become flat. This is not
    true for the errors of individual outputs on an MTL net. The training-set error
    summed across all outputs should never increase, but any one output may exhibit
    more complex behavior. The graph for road_greylevel (graph number 6)
  id: totrans-2333
  prefs: []
  type: TYPE_NORMAL
  zh: 在离开这个话题之前，需要提到的是，MTL网络上各个输出的训练曲线不一定是单调的。虽然STL网络的测试集误差出现多模态并不罕见，但STL网络的训练集误差应该是单调下降或变平。这对于MTL网络上各个输出的误差并不成立。所有输出的训练集误差总和不应增加，但任何一个输出可能会表现出更复杂的行为。图6显示了road_greylevel的情况。
- en: in Figure 8.11 shows a multimodal test-set curve. The training set curve for
    this output is similar. This makes judging when to halt training more difficult
    with MTL nets. Because of this, we always do early stopping on MTL nets by training
    past the epoch where performance on each task appears to be best, and either retrain
    the net a second time (with the same random seed) to get the copies, or are careful
    to keep enough copies during the first run that we have whatever copies we will
    need.
  id: totrans-2334
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11展示了一个多模态测试集曲线。该输出的训练集曲线类似。这使得在MTL网络中判断何时停止训练变得更加困难。因此，我们总是在MTL网络上进行提前停止，通过训练超过每个任务表现最佳的时代，并在第一次运行中小心保留足够的副本，或者再次以相同的随机种子重新训练网络，以获取副本。
- en: 8.3.3 Use Different Learning Rates For Different Tasks
  id: totrans-2335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3.3 为不同任务使用不同的学习率
- en: Is it possible to control the rates at which different tasks train so they each
    reach their best performance at the same time? Would best performance on each
    task be achieved if each task reached peak performance at the same time? If not,
    is it better for extra tasks to learn slower or faster than the main task?
  id: totrans-2336
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有可能控制不同任务的训练速度，使它们同时达到最佳表现？如果每个任务同时达到峰值表现，是否能够实现最佳表现？如果不能，对于额外任务来说，学习速度比主任务慢或快更好？
- en: The rate at which different tasks learn using vanilla backprop is rarely optimal
    for MTL. Task that train slower than the main task will not have learned enough
    to help the main task when training on the main task is stopped. Tasks that train
    faster than the main task may overfit so much before the main task is learned
    well that either what they have learned is no longer useful to the main task,
    or they may drive the main task into premature overfitting.
  id: totrans-2337
  prefs: []
  type: TYPE_NORMAL
  zh: 使用普通反向传播时，不同任务的学习速度很少是MTL的最佳选择。训练速度慢于主任务的任务在主任务训练停止时不会学到足够的知识来帮助主任务。训练速度快于主任务的任务可能在主任务学习良好之前过拟合，导致它们学到的东西对主任务不再有用，或者可能使主任务提前过拟合。
- en: The most direct method of controlling the rate at which different tasks learn
    is to use a different learning rate for each task, i.e., for each output. We have
    experimented with using gradient descent to find learning rates for each extra
    output to maximize the generalization performance on the main task. Table 8.5
    shows the performance on the main steering task before and after optimizing the
    learning rates of the other eight extra tasks. Optimizing the learning rates for
    the extra MTL tasks improved performance on the main task an additional 11.5%.
    This improvement is over and above the original improvement of 15%–25% for MTL
    over STL.
  id: totrans-2338
  prefs: []
  type: TYPE_NORMAL
  zh: 控制不同任务学习速度的最直接方法是为每个任务使用不同的学习率，即为每个输出设置学习率。我们尝试使用梯度下降为每个额外输出寻找学习率，以最大化主任务的泛化性能。表8.5显示了优化其他八个额外任务的学习率前后主转向任务的性能。为额外的MTL任务优化学习率使主任务的性能额外提高了11.5%。这个提升是MTL相较于STL的原始提升15%-25%之上的。
- en: Examining the training curves for all the tasks as the learning rates are optimized
    shows that the changes in the learning rates of the extra tasks has a significant
    effect on the rate at which the extra tasks are learned. Interestingly, it also
    has a significant effect on the rate at which the main task is learned.
  id: totrans-2339
  prefs: []
  type: TYPE_NORMAL
  zh: 随着学习率优化，检查所有任务的训练曲线显示额外任务的学习率变化对额外任务学习速度有显著影响。有趣的是，这也对主任务的学习速度有显著影响。
- en: Table 8.5. Performance of MTL on the main Steering Direction task before and
    after optimizing the learning rates of the other eight extra tasks
  id: totrans-2340
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.5。MTL在主转向方向任务上优化其他八个额外任务的学习率前后的性能。
- en: '| TRIAL   | Before Optimization   | After Optimization   | Difference   |'
  id: totrans-2341
  prefs: []
  type: TYPE_TB
  zh: '| 试验   | 优化前   | 优化后   | 差异   |'
- en: '| --- | --- | --- | --- |'
  id: totrans-2342
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Trial 1 | 0.227                 | 0.213                | -6.2%        |'
  id: totrans-2343
  prefs: []
  type: TYPE_TB
  zh: '| 试验 1 | 0.227                 | 0.213                | -6.2%        |'
- en: '| Trial 2 | 0.276                 | 0.241                | -12.7%       |'
  id: totrans-2344
  prefs: []
  type: TYPE_TB
  zh: '| 试验 2 | 0.276                 | 0.241                | -12.7%       |'
- en: '| Trial 3 | 0.249                 | 0.236                | -5.2%        |'
  id: totrans-2345
  prefs: []
  type: TYPE_TB
  zh: '| 试验 3 | 0.249                 | 0.236                | -5.2%        |'
- en: '| Trial 4 | 0.276                 | 0.231                | -16.3%       |'
  id: totrans-2346
  prefs: []
  type: TYPE_TB
  zh: '| 试验 4 | 0.276                 | 0.231                | -16.3%       |'
- en: '| Trial 5 | 0.276                 | 0.234                | -15.2%       |'
  id: totrans-2347
  prefs: []
  type: TYPE_TB
  zh: '| 试验 5 | 0.276                 | 0.234                | -15.2%       |'
- en: '| Average | 0.261                 | 0.231                | -11.5% *     |'
  id: totrans-2348
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 0.261                 | 0.231                | -11.5% *     |'
- en: This is surprising because we keep the learning rate for the main task fixed
    during optimization. Perhaps even more interesting is the fact that optimizing
    the learning rate to maximize the generalization accuracy of the main task also
    improved generalization on the extra tasks nearly as much as it helped the main
    task. What is good for the goose appears to be good for the gander.
  id: totrans-2349
  prefs: []
  type: TYPE_NORMAL
  zh: 这令人惊讶，因为我们在优化过程中保持主任务的学习率不变。更有趣的是，优化学习率以最大化主任务的泛化准确性，也几乎同样改善了额外任务的泛化。这对鹅有益的，似乎对雁也有益。
- en: 8.3.4 Use A Private Hidden Layer For The Main Task
  id: totrans-2350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3.4 为主任务使用私有隐藏层
- en: Sometimes the optimal number of hidden units is 100 hidden units or more per
    output. If there are hundreds of extra tasks this translates to thousands of hidden
    units. This not only creates computational difficulties, but degrades performance
    on the main task because most of the hidden layer repersentation is constructed
    for other tasks. The main task output unit has a massive hidden unit selection
    problem as it tries to use only those few hidden units that are useful to it.
  id: totrans-2351
  prefs: []
  type: TYPE_NORMAL
  zh: 有时每个输出的最佳隐藏单元数为100个或更多。如果有数百个额外任务，这将转化为数千个隐藏单元。这不仅会造成计算困难，还会降低主任务的性能，因为大部分隐藏层表示是为其他任务构建的。主任务的输出单元在试图仅使用对其有用的少数隐藏单元时面临巨大的隐藏单元选择问题。
- en: '![187_image_0.png](187_image_0.png)'
  id: totrans-2352
  prefs: []
  type: TYPE_IMG
  zh: '![187_image_0.png](187_image_0.png)'
- en: Fig. 8.13. MTL Architecture With a Private Hidden Layer for the Main Task(s),
    and a Shared Hidden Layer Used by the Main Task(s) and the Extra Tasks
  id: totrans-2353
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13. 为主任务和额外任务使用私有隐藏层的MTL架构，以及主任务和额外任务共享的隐藏层
- en: Figure 8.13 shows a net architecture that solves this problem. Instead of one
    hidden layer shared equally by all tasks, there are two disjoint hidden layers.
    Hidden layer 1 is a private hidden layer used only by the main task(s). Hidden
    layer 2 is shared by the main task(s) and the extra tasks. This is the hidden
    layer that supports MTL transfer. Because the main task sees and affects the shared
    hidden layer, but the extra tasks do not affect the hidden layer reserved for
    the main tasks(s), hidden layer 2 can be kept small without hurting the main task.
  id: totrans-2354
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13展示了解决此问题的网络架构。与一个共享所有任务的隐藏层不同，这里有两个不相交的隐藏层。隐藏层1是仅用于主任务的私有隐藏层。隐藏层2则被主任务和额外任务共享。这是支持MTL转移的隐藏层。由于主任务可以看到并影响共享的隐藏层，而额外任务不影响保留给主任务的隐藏层，因此隐藏层2可以保持较小而不影响主任务。
- en: 8.4 Chapter Summary
  id: totrans-2355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 章节总结
- en: We usually think of the inputs to a backprop net as the place where information
    is given to the net, and the outputs as the place where the net outputs predictions.
    Backprop, however, pushes information into the net through the *outputs* during
    training. The information fed into a net through its outputs is as important as
    the information fed into it through its inputs.
  id: totrans-2356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常认为反向传播网络的输入是信息传递给网络的地方，而输出是网络输出预测的地方。然而，在训练过程中，反向传播通过*输出*将信息推入网络。通过输出输入到网络的信息与通过输入输入到网络的信息同样重要。
- en: Multitask Learning is a way of using the outputs of a backprop net to push additional
    information into the net during training. If the net architecture allows sharing
    of what is learned for different outputs, this additional information can help
    the main task be learned better. (See [5, 6, 4, 8, 3, 9, 11, 10, 20, 35, 36, 12,
    21, 13, 14] for additional discussion about multitask learning.)
  id: totrans-2357
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习是一种在训练过程中使用反向传播网络的输出将额外信息推入网络的方法。如果网络架构允许不同输出之间共享所学内容，这些额外信息可以帮助主任务得到更好的学习。（有关多任务学习的更多讨论，请参见[5,
    6, 4, 8, 3, 9, 11, 10, 20, 35, 36, 12, 21, 13, 14]。）
- en: MTL trains multiple tasks in parallel not because this is a more efficient way
    to learn multiple tasks, but because the information in the training signals for
    the extra tasks can help the main task be learned better. Sometimes what is optimal
    for the main task is not optimal for the extra tasks. It is important to optimize
    the technique so that performance on the *main* task is best, even if this hurts
    performance on the extra tasks. If the extra tasks are important too, it may be
    best to rerun learning for each important extra task, with the technique optimized
    for each task one at a time.
  id: totrans-2358
  prefs: []
  type: TYPE_NORMAL
  zh: MTL并行训练多个任务并不是因为这种方法更高效，而是因为额外任务的训练信号中的信息可以帮助主要任务更好地学习。有时主要任务的最佳方案并不适用于额外任务。优化技术以确保主要任务的性能最佳是重要的，即使这可能会影响额外任务的性能。如果额外任务同样重要，最好为每个重要的额外任务重新运行学习，且为每个任务单独优化技术。
- en: This chapter presented a number of opportunities for using extra outputs to
    leverage information that is available in real domains. The trick in most of these
    applications is to view the outputs of the net as inputs that are used only during
    learning. Any information that is available when the net is trained, but which
    would not be available later when the net is used for prediction, can potentially
    be used as extra outputs. There are many domains where useful extra tasks will
    be available. The list of prototypical domains provided in this chapter is not
    complete. More kinds of extra tasks will be identified in the future. Acknowledgements.
    R. Caruana was supported by ARPA grant F33615-931-1330, NSF grant BES-9315428,
    and Agency for Health Care Policy grant HS06468. The work to find input features
    that worked better when used as extra outputs is joint work with Virginia de Sa,
    who was supported by postdoctoral fellowship from the Sloan Foundation. We thank
    the University of Toronto for the Xerion Simulator, and D. Koller and M. Sahami
    for the use of their feature selector.
  id: totrans-2359
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了利用额外输出来利用实际领域中可用信息的多种机会。这些应用中的关键是将网络的输出视为仅在学习过程中使用的输入。任何在网络训练时可用的信息，但在网络用于预测时不可用的信息，都可以作为额外输出。许多领域将有有用的额外任务。本章提供的原型领域列表并不完整，未来将识别出更多种类的额外任务。致谢。R.
    Caruana获得了ARPA资助F33615-931-1330，NSF资助BES-9315428，以及医疗政策机构资助HS06468。寻找在作为额外输出时表现更好的输入特征的工作与Virginia
    de Sa共同完成，她获得了斯隆基金会的博士后奖学金。我们感谢多伦多大学提供的Xerion模拟器，以及D. Koller和M. Sahami提供的特征选择器的使用。
- en: '[1] Abu-Mostafa, Y.S.: Learning from Hints in Neural Networks. Journal of Complexity
    6(2), 192–198 (1990)'
  id: totrans-2360
  prefs: []
  type: TYPE_NORMAL
  zh: '[从神经网络中的提示学习](https://example.org)，作者为Y.S. Abu-Mostafa，发表在《复杂性杂志》6(2)，页码192–198（1990）。'
- en: '[2] Abu-Mostafa, Y.S.: Hints. Neural Computation 7, 639–671 (1995) [3] Baxter,
    J.: Learning Internal Representations. In: COLT 1995, Santa Cruz, CA'
  id: totrans-2361
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示](https://example.org)，作者为Y.S. Abu-Mostafa，发表在《神经计算》7，页码639–671（1995）；[学习内部表示](https://example.org)，作者为J.
    Baxter，收录于COLT 1995，位于加利福尼亚州圣克鲁斯。'
- en: (1995)
  id: totrans-2362
  prefs: []
  type: TYPE_NORMAL
  zh: (1995)
- en: '[4] Baxter, J.: Learning Internal Representations. Ph.D. Thesis, The Flinders
    Univeristy of South Australia (December 1994)'
  id: totrans-2363
  prefs: []
  type: TYPE_NORMAL
  zh: '[学习内部表示](https://example.org)，作者为J. Baxter，博士论文，南澳大利亚弗林德斯大学（1994年12月）。'
- en: '[5] Caruana, R.: Multitask Learning: A Knowledge-Based Source of Inductive
    Bias.'
  id: totrans-2364
  prefs: []
  type: TYPE_NORMAL
  zh: '[多任务学习：基于知识的归纳偏置来源](https://example.org)，作者为R. Caruana。'
- en: 'In: Proceedings of the 10th International Conference on Machine Learning, ML'
  id: totrans-2365
  prefs: []
  type: TYPE_NORMAL
  zh: 收录于第十届国际机器学习会议，ML。
- en: 1993, University of Massachusetts, Amherst, pp. 41–48 (1993)
  id: totrans-2366
  prefs: []
  type: TYPE_NORMAL
  zh: 1993，马萨诸塞大学，阿默斯特，页码41–48（1993）。
- en: '[6] Caruana, R.: Multitask Connectionist Learning. In: Proceedings of the 1993
    Connectionist Models Summer School, pp. 372–379 (1994)'
  id: totrans-2367
  prefs: []
  type: TYPE_NORMAL
  zh: '[多任务连接主义学习](https://example.org)，作者为R. Caruana，收录于1993年连接主义模型暑期学校的论文集中，页码372–379（1994）。'
- en: '[7] Caruana, R., Freitag, D.: Greedy Attribute Selection. In: ICML 1994, Rutgers,
    NJ, pp. 28–36 (1994)'
  id: totrans-2368
  prefs: []
  type: TYPE_NORMAL
  zh: '[贪婪属性选择](https://example.org)的作者为R. Caruana和D. Freitag，收录于ICML 1994，位于新泽西州的拉德格斯大学，页码28–36（1994）。'
- en: '[8] Caruana, R.: Learning Many Related Tasks at the Same Time with Backpropagation.
    In: NIPS 1994, pp. 656–664 (1995)'
  id: totrans-2369
  prefs: []
  type: TYPE_NORMAL
  zh: '[同时学习许多相关任务的反向传播](https://example.org)，作者为R. Caruana，收录于NIPS 1994，页码656–664（1995）。'
- en: '[9] Caruana, R., Baluja, S., Mitchell, T.: Using the Future to "Sort Out" the
    Present:'
  id: totrans-2370
  prefs: []
  type: TYPE_NORMAL
  zh: '[利用未来“理清”现在](https://example.org)，作者为R. Caruana、S. Baluja和T. Mitchell。'
- en: 'Rankprop and Multitask Learning for Medical Risk Prediction. In: Proceedings
    of Advances in Neural Information Processing Systems, NIPS 1995, pp. 959–965'
  id: totrans-2371
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗风险预测的Rankprop和多任务学习，收录于NIPS 1995的神经信息处理系统进展论文集，页码959–965。
- en: (1996)
  id: totrans-2372
  prefs: []
  type: TYPE_NORMAL
  zh: (1996)
- en: '[10] Caruana, R., de Sa, V.R.: Promoting Poor Features to Supervisors: Some
    Inputs Work Better As Outputs. In: NIPS 1996 (1997)'
  id: totrans-2373
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Caruana, R., de Sa, V.R.: 促进劣特征给监督者：某些输入作为输出更有效。收录于：NIPS 1996 (1997)'
- en: '[11] Caruana, R.: Multitask Learning. Machine Learning 28, 41–75 (1997)'
  id: totrans-2374
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Caruana, R.: 多任务学习。机器学习 28, 41–75 (1997)'
- en: '[12] Caruana, R.: Multitask Learning. Ph.D. thesis, Carnegie Mellon University,
    CMUCS-97-203 (1997)'
  id: totrans-2375
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Caruana, R.: 多任务学习。博士论文，卡内基梅隆大学，CMUCS-97-203 (1997)'
- en: '[13] Caruana, R., O''Sullivan, J.: Multitask Pattern Recognition for Autonomous
    Robots. In: The Proceedings of the IEEE Intelligent Robots and Systems Conference
    (IROS 1998), Victoria (1998) (to appear)'
  id: totrans-2376
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Caruana, R., O''Sullivan, J.: 自主机器人多任务模式识别。收录于：IEEE 智能机器人与系统会议论文集 (IROS
    1998)，维多利亚 (1998)（待发表）'
- en: '[14] Caruana, R., de Sa, V.R.: Using Feature Selection to Find Inputs that
    Work Better as Outputs. In: The Proceedings of the International Conference on
    Neural Nets'
  id: totrans-2377
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Caruana, R., de Sa, V.R.: 使用特征选择寻找更有效的输入作为输出。收录于：国际神经网络会议论文集'
- en: (ICANN 1998), Sweden (1998) (to appear)
  id: totrans-2378
  prefs: []
  type: TYPE_NORMAL
  zh: （ICANN 1998），瑞典 (1998)（待发表）
- en: '[15] Cooper, G.F., Aliferis, C.F., Ambrosino, R., Aronis, J., Buchanan, B.G.,
    Caruana, R., Fine, M.J., Glymour, C., Gordon, G., Hanusa, B.H., Janosky, J.E.,
    Meek, C., Mitchell, T., Richardson, T., Spirtes, P.: An Evaluation of Machine
    Learning Methods for Predicting Pneumonia Mortality. Artificial Intelligence in
    Medicine 9, 107–138 (1997)'
  id: totrans-2379
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Cooper, G.F., Aliferis, C.F., Ambrosino, R., Aronis, J., Buchanan, B.G.,
    Caruana, R., Fine, M.J., Glymour, C., Gordon, G., Hanusa, B.H., Janosky, J.E.,
    Meek, C., Mitchell, T., Richardson, T., Spirtes, P.: 评估用于预测肺炎死亡率的机器学习方法。医学人工智能
    9, 107–138 (1997)'
- en: '[16] Craven, M., Shavlik, J.: Using Sampling and Queries to Extract Rules from
    Trained Neural Networks. In: Proceedings of the 11th International Conference
    on Machine Learning, ML 1994, Rutgers University, New Jersey, pp. 37–45 (1994)'
  id: totrans-2380
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Craven, M., Shavlik, J.: 使用采样和查询从训练的神经网络中提取规则。收录于：第11届国际机器学习会议论文集，ML 1994，罗格斯大学，新泽西州，页码
    37–45 (1994)'
- en: '[17] Davis, I., Stentz, A.: Sensor Fusion for Autonomous Outdoor Navigation
    Using Neural Networks. In: Proceedings of IEEE''s Intelligent Robots and Systems
    Conference (1995)'
  id: totrans-2381
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Davis, I., Stentz, A.: 使用神经网络进行自主户外导航的传感器融合。收录于：IEEE 智能机器人与系统会议论文集 (1995)'
- en: '[18] Dietterich, T.G., Bakiri, G.: Solving Multiclass Learning Problems via
    ErrorCorrecting Output Codes. Journal of Artificial Intelligence Research 2, 263–286'
  id: totrans-2382
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Dietterich, T.G., Bakiri, G.: 通过错误纠正输出编码解决多类学习问题。人工智能研究杂志 2, 263–286'
- en: (1995)
  id: totrans-2383
  prefs: []
  type: TYPE_NORMAL
  zh: （1995）
- en: '[19] Fine, M.J., Singer, D., Hanusa, B.H., Lave, J., Kapoor, W.: Validation
    of a Pneumonia Prognostic Index Using the MedisGroups Comparative Hospital Database.'
  id: totrans-2384
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Fine, M.J., Singer, D., Hanusa, B.H., Lave, J., Kapoor, W.: 使用 MedisGroups
    比较医院数据库验证肺炎预后指数。'
- en: American Journal of Medicine (1993)
  id: totrans-2385
  prefs: []
  type: TYPE_NORMAL
  zh: 美国医学杂志 (1993)
- en: '[20] Ghosn, J., Bengio, Y.: Multi-Task Learning for Stock Selection. In: NIPS
    1996'
  id: totrans-2386
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Ghosn, J., Bengio, Y.: 股票选择的多任务学习。收录于：NIPS 1996'
- en: (1997)
  id: totrans-2387
  prefs: []
  type: TYPE_NORMAL
  zh: （1997）
- en: '[21] Heskes, T.: Solving a Huge Number of Similar Tasks: A Combination of Multitask
    Learning and a Hierarchical Bayesian Approach. In: Proceedings of the 15th International
    Conference on Machine Learning, Madison, Wisconsin, pp. 233–241'
  id: totrans-2388
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Heskes, T.: 解决大量相似任务：多任务学习与层次贝叶斯方法的结合。收录于：第15届国际机器学习会议论文集，麦迪逊，威斯康星州，页码
    233–241'
- en: (1998)
  id: totrans-2389
  prefs: []
  type: TYPE_NORMAL
  zh: （1998）
- en: '[22] Holmstrom, L., Koistinen, P.: Using Additive Noise in Back-propagation
    Training.'
  id: totrans-2390
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Holmstrom, L., Koistinen, P.: 在反向传播训练中使用附加噪声。'
- en: IEEE Transactions on Neural Networks 3(1), 24–38 (1992)
  id: totrans-2391
  prefs: []
  type: TYPE_NORMAL
  zh: IEEE 神经网络学报 3(1), 24–38 (1992)
- en: '[23] John, G., Kohavi, R., Pfleger, K.: Irrelevant Features and the Subset
    Selection Problem. In: ICML 1994, Rutgers, NJ, pp. 121–129 (1994)'
  id: totrans-2392
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] John, G., Kohavi, R., Pfleger, K.: 无关特征与子集选择问题。收录于：ICML 1994，罗格斯大学，新泽西州，页码
    121–129 (1994)'
- en: '[24] Koller, D., Sahami, M.: Towards Optimal Feature Selection. In: ICML 1996,
    Bari, Italy, pp. 284–292 (1996)'
  id: totrans-2393
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Koller, D., Sahami, M.: 朝着最优特征选择迈进。收录于：ICML 1996，巴里，意大利，页码 284–292 (1996)'
- en: '[25] Le Cun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard,
    W., Jackal, L.D.: Backpropagation Applied to Handwritten Zip-Code Recognition.
    Neural Computation 1, 541–551 (1989)'
  id: totrans-2394
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Le Cun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard,
    W., Jackal, L.D.: 反向传播应用于手写邮政编码识别。神经计算 1, 541–551 (1989)'
- en: '[26] Le Cun, Y.: Private communication (1997)'
  id: totrans-2395
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Le Cun, Y.: 私人通信 (1997)'
- en: '[27] Munro, P.W., Parmanto, B.: Competition Among Networks Improves Committee
    Performance. In: Proceedings of Advances in Neural Information Processing Systems,
    NIPS 1996, vol. 9 (1997) (to appear)'
  id: totrans-2396
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Munro, P.W., Parmanto, B.: 网络之间的竞争改善委员会绩效。收录于：神经信息处理系统进展论文集，NIPS 1996，第9卷
    (1997)（待发表）'
- en: '[28] Pomerleau, D.A.: Neural Network Perception for Mobile Robot Guidance.
    Doctoral Thesis, Carnegie Mellon University: CMU-CS-92-115 (1992)'
  id: totrans-2397
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] Pomerleau, D.A.：《移动机器人引导的神经网络感知》。博士论文，卡内基梅隆大学：CMU-CS-92-115（1992）'
- en: '[29] Pratt, L.Y., Mostow, J., Kamm, C.A.: Direct Transfer of Learned Information
    Among Neural Networks. In: Proceedings of AAAI 1991 (1991)'
  id: totrans-2398
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] Pratt, L.Y., Mostow, J., Kamm, C.A.：《神经网络之间学习信息的直接转移》。载于《AAAI 1991会议论文集》（1991）'
- en: '[30] Sejnowski, T.J., Rosenberg, C.R.: NETtalk: A Parallel Network that Learns
    to Read Aloud. John Hopkins: JHU/EECS-86/01 (1986)'
  id: totrans-2399
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] Sejnowski, T.J., Rosenberg, C.R.：《NETtalk：一个学习朗读的并行网络》。约翰·霍普金斯大学：JHU/EECS-86/01（1986）'
- en: '[31] Sill, J., Abu-Mostafa, Y.: Monotonicity Hints. In: Proceedings of Neural
    Information Processing Systems, NIPS 1996, vol. 9 (1997) (to appear)'
  id: totrans-2400
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Sill, J., Abu-Mostafa, Y.：《单调性提示》。载于《神经信息处理系统会议论文集》，NIPS 1996，第9卷（1997）（待刊）'
- en: '[32] Suddarth, S.C., Holden, A.D.C.: Symbolic-neural Systems and the Use of
    Hints for Developing Complex Systems. International Journal of Man-Machine Studies
    35(3), 291–311 (1991)'
  id: totrans-2401
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] Suddarth, S.C., Holden, A.D.C.：《符号-神经系统及其在开发复杂系统中的提示使用》。国际人机研究杂志 35(3)，291–311（1991）'
- en: '[33] Suddarth, S.C., Kergosien, Y.L.: Rule-injection Hints as a Means of Improving
    Network Performance and Learning Time. In: Proceedings of EURASIP Workshop on
    Neural Nets, pp. 120–129 (1990)'
  id: totrans-2402
  prefs: []
  type: TYPE_NORMAL
  zh: '[33] Suddarth, S.C., Kergosien, Y.L.：《规则注入提示作为提高网络性能和学习时间的手段》。载于《EURASIP神经网络研讨会论文集》，第120-129页（1990）'
- en: '[34] Thrun, S.: Explanation-Based Neural Network Learning: A Lifelong Learning
    Approach. Kluwer Academic Publisher (1996)'
  id: totrans-2403
  prefs: []
  type: TYPE_NORMAL
  zh: '[34] Thrun, S.：《基于解释的神经网络学习：一种终身学习方法》。Kluwer 学术出版社（1996）'
- en: '[35] Thrun, S., Pratt, L. (eds.): Machine Learning. Second Special Issue on
    Inductive Transfer (1997)'
  id: totrans-2404
  prefs: []
  type: TYPE_NORMAL
  zh: '[35] Thrun, S., Pratt, L.（主编）：机器学习。第二期关于归纳转移的特刊（1997）'
- en: '[36] Thrun, S., Pratt, L. (eds.): Learning to Learn. Kluwer (1997)'
  id: totrans-2405
  prefs: []
  type: TYPE_NORMAL
  zh: '[36] Thrun, S., Pratt, L.（主编）：学习如何学习。Kluwer（1997）'
- en: '[37] Valdes-Perez, R., Simon, H.A.: A Powerful Heuristic for the Discovery
    of Complex Patterned Behavior. In: Proceedings of the 11th International Conference
    on Machine Learning, ML 1994, Rutgers University, New Jersey, pp. 326–334 (1994)'
  id: totrans-2406
  prefs: []
  type: TYPE_NORMAL
  zh: '[37] Valdes-Perez, R., Simon, H.A.：《发现复杂模式行为的强大启发式方法》。载于第11届国际机器学习会议论文集，ML
    1994，新泽西州罗格斯大学，第326-334页（1994）'
- en: '[38] Weigend, A., Rumelhart, D., Huberman, B.: Generalization by WeightElimination
    with Application to Forecasting. In: Proceedings of Advances in Neural Information
    Processing Systems, NIPS 1990, vol. 3, pp. 875–882 (1991)'
  id: totrans-2407
  prefs: []
  type: TYPE_NORMAL
  zh: '[38] Weigend, A., Rumelhart, D., Huberman, B.：《通过权重消除实现泛化及其在预测中的应用》。载于《神经信息处理系统进展论文集》，NIPS
    1990，第3卷，第875-882页（1991）'
- en: 9 Solving The Ill-Conditioning In Neural Network Learning-
  id: totrans-2408
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 解决神经网络学习中的不适定性
- en: Patrick van der Smagt and Gerd Hirzinger German Aerospace Research Establishment,
    Institute of Robotics and System Dynamics, P.O. Box 1116, D–82230 Wessling, Germany
    smagt@dlr.de http://www.robotic.dlr.de/Smagt/
  id: totrans-2409
  prefs: []
  type: TYPE_NORMAL
  zh: Patrick van der Smagt 和 Gerd Hirzinger 德国航空航天研究院，机器人与系统动力学研究所，邮政信箱 1116，D–82230
    Wessling，德国 smagt@dlr.de http://www.robotic.dlr.de/Smagt/
- en: Abstract. In this paper we investigate the feed-forward learning problem. The
    well-known ill-conditioning which is present in most feed-forward learning problems
    is shown to be the result of the structure of the network. Also, the well-known
    problem that weights between 'higher' layers in the network have to settle before
    'lower' weights can converge is addressed. We present a solution to these problems
    by modifying the structure of the network through the addition of linear connections
    which carry shared weights. We call the new network structure the linearly augmented
    feed-forward network, and it is shown that the universal approximation theorems
    are still valid. Simulation experiments show the validity of the new method, and
    demonstrate that the new network is less sensitive to local minima and learns
    faster than the original network.
  id: totrans-2410
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要。本文研究了前馈学习问题。大多数前馈学习问题中存在的众所周知的不适定性被证明是网络结构造成的。同时，网络中“高层”之间的权重必须在“低层”权重收敛之前稳定的著名问题也得到了讨论。我们通过增加共享权重的线性连接来修改网络结构，从而提出了解决这些问题的方法。我们将这种新的网络结构称为线性增强前馈网络，并证明通用逼近定理仍然有效。仿真实验表明了新方法的有效性，并展示了新网络对局部最小值的敏感性较低，学习速度比原始网络快。
- en: 9.1 Introduction
  id: totrans-2411
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 引言
- en: One of the major problems with feed-forward network learning remains the accuracy
    and speed of the learning algorithms. Since the learning problem is a complex
    and highly nonlinear one [12, 4], iterative learning procedures must be used to
    solve the optimisation problem [2, 14]. A continuing desire to improve the behavior
    of the learning algorithm has lead to many excellent optimisation algorithms which
    are especially tailored for feed-forward network learning.
  id: totrans-2412
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈网络学习的主要问题之一仍然是学习算法的准确性和速度。由于学习问题是一个复杂且高度非线性的难题[12, 4]，必须使用迭代学习过程来解决优化问题[2,
    14]。对改进学习算法行为的持续渴望导致了许多优秀的优化算法，这些算法特别适用于前馈网络学习。
- en: However, an important problem is the particular form of the error function that
    represents the learning problem. It has long been noted [10, 16] that the derivatives
    of the error function are usually ill-conditioned. This ill-conditioning is reflected
    in error landscapes which contain many saddle points and flat areas.
  id: totrans-2413
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个重要问题是表示学习问题的误差函数的特定形式。早已注意到[10, 16]，误差函数的导数通常条件不好。这种条件不良反映在包含许多鞍点和平坦区域的误差景观中。
- en: Although this problem can be solved by using stochastic learning methods
  id: totrans-2414
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可以通过使用随机学习方法解决这个问题
- en: (e.g., [9, 1, 13]), these methods require many learning iterations in order
    to find an optimum, and are therefore not suited for problems where fast learning
    is a requirement. We therefore remain focused on gradient-based learning methods.
  id: totrans-2415
  prefs: []
  type: TYPE_NORMAL
  zh: （例如，[9, 1, 13]），这些方法需要许多学习迭代才能找到最佳解，因此不适合快速学习为要求的问题。因此，我们仍然专注于基于梯度的学习方法。
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-2416
  prefs: []
  type: TYPE_NORMAL
  zh: '- 之前发表于：Orr, G.B. 和 Müller, K.-R.（编辑）：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-2417
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0（1998年）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    191–203, 2012.'
  id: totrans-2418
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon 等（编辑）：NN：行业技巧，第2版，LNCS 7700，第191–203页，2012年。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-2419
  prefs: []
  type: TYPE_NORMAL
  zh: -c 施普林格-维尔海德2012年
- en: Algorithms exist which attempt to find well-behaving minima [7], yet an important
    factor of the learning problem remains the structure of the feed-forward network.
  id: totrans-2420
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一些算法试图寻找良好行为的最小值[7]，然而学习问题的一个重要因素仍然是前馈网络的结构。
- en: In this chapter an explanation of the ill-conditioned learning problem is provided
    as well as a solution to alleviate this problem. Section 9.2 formally introduces
    the learning problem, and describes the problem of singularities in the learn
    matrices. In section 9.3 the cause of the singularities are analyzed and an adapted
    learning rule is introduced which alleviates this problem. Section 9.4 discusses
    a few applications.
  id: totrans-2421
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了对条件不良学习问题的解释，并提出了解决此问题的方案。第9.2节正式介绍学习问题，并描述学习矩阵中的奇点问题。在第9.3节中分析了奇点的原因，并介绍了一种适应性学习规则，以减轻此问题。第9.4节讨论了一些应用。
- en: 9.2 The Learning Process
  id: totrans-2422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 学习过程
- en: 'With a neural network N : N × n → M we create an approximation to a set of
    p learning samples {(x1, y1), (x2, y2)*, ...,* (xp, yp)}, with xi ∈ N and yi ∈
    M , for which holds that ∀1 ≤ i ≤ p : F(xi) = yi. The function F : N → M is called
    the model function.'
  id: totrans-2423
  prefs: []
  type: TYPE_NORMAL
  zh: '使用神经网络 N : N × n → M，我们创建对一组 p 个学习样本 {(x1, y1), (x2, y2)*, ...,* (xp, yp)}
    的近似，其中 xi ∈ N 和 yi ∈ M，对于所有 1 ≤ i ≤ p : F(xi) = yi。函数 F : N → M 被称为模型函数。'
- en: Let n be the number of free parameters W of the network. In this particular
    case we are interested in approximating the learning samples rather than the underlying
    function F, or assume that the p learning samples are representative for F.
  id: totrans-2424
  prefs: []
  type: TYPE_NORMAL
  zh: 设 n 为网络的自由参数 W 的数量。在这种情况下，我们感兴趣的是近似学习样本，而不是潜在的函数 F，或者假设 p 个学习样本是 F 的代表。
- en: The oth output of the function that is represented by the neural network can
    be written as
  id: totrans-2425
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络表示的函数的输出可以写作
- en: $${\cal N}(\mathbf{x},W)_{o}=\sum_{h}w_{ho}s\left(\sum_{i}w_{ih}x_{i}+\theta_{h}\right)+\theta_{o}\tag{9.1}$$
  id: totrans-2426
  prefs: []
  type: TYPE_NORMAL
  zh: $${\cal N}(\mathbf{x},W)_{o}=\sum_{h}w_{ho}s\left(\sum_{i}w_{ih}x_{i}+\theta_{h}\right)+\theta_{o}\tag{9.1}$$
- en: $$(9.2)$$
  id: totrans-2427
  prefs: []
  type: TYPE_NORMAL
  zh: $$(9.2)$$
- en: where s(x) the transfer function, o indicates an output unit, h a hidden unit,
    i an input unit. The symbol who indicates an element of W corresponding with the
    connection from hidden unit h to output unit o; wih is similarly used for a connection
    from input unit i to hidden unit h. Finally, θ is a bias weight and therefore
    an element of W. An exemplar feed-forward network with one input and output unit
    and two hidden units is depicted in figure 9.1.
  id: totrans-2428
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 s(x) 为传输函数，o 表示输出单元，h 为隐藏单元，i 为输入单元。符号表示与从隐藏单元 h 到输出单元 o 的连接相对应的 W 的元素；wih
    类似用于从输入单元 i 到隐藏单元 h 的连接。最后，θ 是偏置权重，因此也是 W 的一个元素。图 9.1 展示了一个示例前馈网络，具有一个输入和输出单元以及两个隐藏单元。
- en: The learning task consists of minimizing an approximation error, which is usually
    defined as
  id: totrans-2429
  prefs: []
  type: TYPE_NORMAL
  zh: 学习任务包括最小化近似误差，通常定义为
- en: $$E_{\mathcal{N}}(W)=\sum_{i=1}^{p}\|{\mathcal{N}}(\mathbf{x}_{i},W)-\mathbf{y}_{i}\|$$
  id: totrans-2430
  prefs: []
  type: TYPE_NORMAL
  zh: $$E_{\mathcal{N}}(W)=\sum_{i=1}^{p}\|{\mathcal{N}}(\mathbf{x}_{i},W)-\mathbf{y}_{i}\|$$
- en: $$(9.3)$$
  id: totrans-2431
  prefs: []
  type: TYPE_NORMAL
  zh: $$(9.3)$$
- en: where for · we prefer to use the L2 norm. We will leave the subscript N out
    when no confusion arises. E(W) is (highly) nonlinear in W, such that iterative
    search techniques are required to find the W for which E(W) is sufficiently small.
  id: totrans-2432
  prefs: []
  type: TYPE_NORMAL
  zh: 对于·我们更倾向于使用 L2 范数。当没有混淆时，我们将省略下标 N。E(W) 在 W 中是（高度）非线性的，因此需要迭代搜索技术来找到 E(W) 足够小的
    W。
- en: Finally we define the residual pattern error
  id: totrans-2433
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义残差模式误差
- en: $$e_{M(i-1)+j}=\|{\mathcal{N}}(\mathbf{x}_{i},W)_{j}-\mathbf{y}_{i j}\|,$$
  id: totrans-2434
  prefs: []
  type: TYPE_NORMAL
  zh: $$e_{M(i-1)+j}=\|{\mathcal{N}}(\mathbf{x}_{i},W)_{j}-\mathbf{y}_{i j}\|,$$
- en: i.e., the error in the j'th output value for the i'th learning sample.
  id: totrans-2435
  prefs: []
  type: TYPE_NORMAL
  zh: 即，第 i 个学习样本的 j'th 输出值的误差。
- en: '![193_image_0.png](193_image_0.png)'
  id: totrans-2436
  prefs: []
  type: TYPE_IMG
  zh: '![193_image_0.png](193_image_0.png)'
- en: Fig. 9.1. An exemplar feed-forward neural network. The circles represent neurons;
    the black filled circle is a bias unit, and always carries an activation value
    of '1'.
  id: totrans-2437
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1. 一个示例前馈神经网络。圆圈代表神经元；黑色实心圆为偏置单元，始终携带激活值“1”。
- en: 9.2.1 Learning Methodology
  id: totrans-2438
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2.1 学习方法论
- en: Gradient based learning methods are characterized by considering low-order terms
    from the Taylor expansion to the approximation error,
  id: totrans-2439
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的学习方法的特点是考虑泰勒展开中低阶项对近似误差的影响，
- en: $$E(W+W_{0})=E(W_{0})+\left.\nabla E\right|_{W_{0}}W+W^{T}\left.\nabla^{2}E\right|_{W_{0}}W+\ldots\tag{9.4}$$
  id: totrans-2440
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(W+W_{0})=E(W_{0})+\left.\nabla E\right|_{W_{0}}W+W^{T}\left.\nabla^{2}E\right|_{W_{0}}W+\ldots\tag{9.4}$$
- en: In most cases more than the second order term is neglected. We define
  id: totrans-2441
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，忽略高于二阶的项。我们定义
- en: $$N{\mathrm{e~define}}$$
  id: totrans-2442
  prefs: []
  type: TYPE_NORMAL
  zh: $$N{\mathrm{e~define}}$$
- en: $$\tilde{E}_{1}(W+W_{0})=E(W_{0})+\left.\nabla E\right|_{W_{0}}W$$
  id: totrans-2443
  prefs: []
  type: TYPE_NORMAL
  zh: $$\tilde{E}_{1}(W+W_{0})=E(W_{0})+\left.\nabla E\right|_{W_{0}}W$$
- en: $$(9.5)$$
  id: totrans-2444
  prefs: []
  type: TYPE_NORMAL
  zh: $$(9.5)$$
- en: $$(9.6)$$
  id: totrans-2445
  prefs: []
  type: TYPE_NORMAL
  zh: $$(9.6)$$
- en: E˜1(W + W0) = E(W0) + ∇E|W0 W (9.5)
  id: totrans-2446
  prefs: []
  type: TYPE_NORMAL
  zh: E˜1(W + W0) = E(W0) + ∇E|W0 W (9.5)
- en: and
  id: totrans-2447
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: $$\tilde{E}_{2}(W+W_{0})=\tilde{E}_{1}(W)+W^{T}\left.\nabla^{2}E\right|_{W_{0}}W$$
  id: totrans-2448
  prefs: []
  type: TYPE_NORMAL
  zh: $$\tilde{E}_{2}(W+W_{0})=\tilde{E}_{1}(W)+W^{T}\left.\nabla^{2}E\right|_{W_{0}}W$$
- en: W0 W (9.6)
  id: totrans-2449
  prefs: []
  type: TYPE_NORMAL
  zh: W0 W (9.6)
- en: being the first-order and second-order approximation to E, respectively.
  id: totrans-2450
  prefs: []
  type: TYPE_NORMAL
  zh: 分别是 E 的一阶和二阶近似。
- en: By locally considering the approximation error as a first- or second-order function
    of W, we can use several existing approximation methodologies to minimize
  id: totrans-2451
  prefs: []
  type: TYPE_NORMAL
  zh: 通过局部考虑近似误差作为 W 的一阶或二阶函数，我们可以使用几种现有的近似方法来最小化
- en: E. When, according to the local second-order approximation information, E is
    minimized, the local information is updated and a second minimization step is
  id: totrans-2452
  prefs: []
  type: TYPE_NORMAL
  zh: E。当根据局部二阶近似信息最小化 E 时，局部信息被更新，并进行第二次最小化步骤。
- en: carried out. This process is repeated until a minimum is found.
  id: totrans-2453
  prefs: []
  type: TYPE_NORMAL
  zh: 完成。该过程重复进行，直到找到一个最小值。
- en: Well-known minimization methods are steepest descent (a variant of which is
  id: totrans-2454
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知的最小化方法包括最速下降法（其变体是
- en: known as error backpropagation), conjugate gradient optimisation, LevenbergMarquardt
    optimisation, variable metric methods, and (quasi-) Newton methods. Each of these
    methods has its advantages and disadvantages which are
  id: totrans-2455
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为误差反向传播）、共轭梯度优化、Levenberg-Marquardt 优化、变量度量方法和（拟）牛顿方法。这些方法各有优缺点，
- en: discussed elsewhere [14].
  id: totrans-2456
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他地方讨论过 [14]。
- en: The optimisation methods work in principle as follows. By considering the
  id: totrans-2457
  prefs: []
  type: TYPE_NORMAL
  zh: 优化方法的原则如下。通过考虑
- en: second-order approximation to E, an optimum can be analytically found if ∇E
  id: totrans-2458
  prefs: []
  type: TYPE_NORMAL
  zh: E 的二阶近似，如果 ∇E
- en: and ∇2E are known. After the optimum has been located, ∇E and ∇2E are recomputed
    using the local information, and the minimum is relocated. This process is repeated
    until a minimum is found.
  id: totrans-2459
  prefs: []
  type: TYPE_NORMAL
  zh: 而 ∇2E 是已知的。在找到最优点后，使用局部信息重新计算 ∇E 和 ∇2E，并重新定位最小值。该过程重复进行，直到找到一个最小值。
- en: Naturally, the success of this approach stands or falls with the form of the
    error function. If the error function is not too complex but smooth, and can be
    reasonably approximated by a quadratic function, the discussed optimisation methods
    are a reliable and fast way of finding minima. In feed-forward network training,
    however, the error functions appear to have a large number of flat areas where
    minimization is a difficult task due to limited floating point accuracy.
  id: totrans-2460
  prefs: []
  type: TYPE_NORMAL
  zh: 自然，这种方法的成功取决于误差函数的形式。如果误差函数不太复杂且光滑，并且可以合理地用二次函数近似，则所讨论的优化方法是寻找极小值的可靠和快速的方法。然而，在前馈网络训练中，误差函数似乎存在许多平坦区域，使得最小化任务因浮点精度有限而变得困难。
- en: 9.2.2 Condition Of The Learning Problem
  id: totrans-2461
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2.2 学习问题的条件
- en: The flat areas of the error function can be formalized as follows. We define
    the
  id: totrans-2462
  prefs: []
  type: TYPE_NORMAL
  zh: 误差函数的平坦区域可以形式化如下。我们定义
- en: (M p) × n Jacobian matrix as
  id: totrans-2463
  prefs: []
  type: TYPE_NORMAL
  zh: (M p) × n 雅可比矩阵为
- en: $$J\equiv$$
  id: totrans-2464
  prefs: []
  type: TYPE_NORMAL
  zh: $$J\equiv$$
- en: ⎛
  id: totrans-2465
  prefs: []
  type: TYPE_NORMAL
  zh: ⎛
- en: $$\mathbf{\tau}_{i}^{\mu}\left(\begin{array}{c}{{\nabla e_{1}^{i}}}\\ {{\nabla
    e_{2}^{T}}}\\ {{\vdots}}\\ {{\vdots}}\\ {{\nabla e_{M p}^{T}}}\end{array}\right)$$
  id: totrans-2466
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbf{\tau}_{i}^{\mu}\left(\begin{array}{c}{{\nabla e_{1}^{i}}}\\ {{\nabla
    e_{2}^{T}}}\\ {{\vdots}}\\ {{\vdots}}\\ {{\nabla e_{M p}^{T}}}\end{array}\right)$$
- en: ⎞
  id: totrans-2467
  prefs: []
  type: TYPE_NORMAL
  zh: ⎞
- en: where $\nabla\mathbf{e}_{k}\equiv\left(\begin{array}{c}\frac{\partial e_{k}}{\partial
    w_{1}}\\ \frac{\partial e_{k}}{\partial w_{2}}\\ \vdots\\ \frac{\partial e_{k}}{\partial
    w_{n}}\end{array}\right)$
  id: totrans-2468
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\nabla\mathbf{e}_{k}\equiv\left(\begin{array}{c}\frac{\partial e_{k}}{\partial
    w_{1}}\\ \frac{\partial e_{k}}{\partial w_{2}}\\ \vdots\\ \frac{\partial e_{k}}{\partial
    w_{n}}\end{array}\right)$
- en: '![194_image_0.png](194_image_0.png)'
  id: totrans-2469
  prefs: []
  type: TYPE_IMG
  zh: '![194_image_0.png](194_image_0.png)'
- en: ⎞
  id: totrans-2470
  prefs: []
  type: TYPE_NORMAL
  zh: ⎞
- en: $$(9.7)$$
  id: totrans-2471
  prefs: []
  type: TYPE_NORMAL
  zh: $$(9.7)$$
- en: ⎟⎟⎟⎟⎠(9.7)
  id: totrans-2472
  prefs: []
  type: TYPE_NORMAL
  zh: ⎟⎟⎟⎟⎠(9.7)
- en: such that we can write J as
  id: totrans-2473
  prefs: []
  type: TYPE_NORMAL
  zh: 使得我们可以将 J 写为
- en: $$J\equiv1$$
  id: totrans-2474
  prefs: []
  type: TYPE_NORMAL
  zh: $$J\equiv1$$
- en: '![194_image_1.png](194_image_1.png)'
  id: totrans-2475
  prefs: []
  type: TYPE_IMG
  zh: '![194_image_1.png](194_image_1.png)'
- en: $$(9.8)$$
  id: totrans-2476
  prefs: []
  type: TYPE_NORMAL
  zh: $$(9.8)$$
- en: ⎟⎟⎟⎟⎠ . (9.8)
  id: totrans-2477
  prefs: []
  type: TYPE_NORMAL
  zh: ⎟⎟⎟⎟⎠ . (9.8)
- en: In the learning process we thus encounter that ∇E = 2JT e.
  id: totrans-2478
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习过程中，我们遇到 ∇E = 2JT e。
- en: We furthermore define the Hessian H = ∇2E, which is the matrix of second order
    derivatives of E. We are interested in the eigenvalues and eigenvectors of H.
    Since H is a positive semidefinite symmetric matrix close to a local minimum,
    its eigenvalues are all positive real numbers. When an eigenvalue is very small,
    the effect of moving in the direction of the corresponding eigenvector on the
    approximation error is very small. This means that, in that direction, the error
    function is (nearly) singular. The singularity of the error function can be expressed
    in the condition of H, which is defined as the quotient of its largest and its
    smallest eigenvalues.
  id: totrans-2479
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步定义 Hessian H = ∇2E，它是 E 的二阶导数矩阵。我们对 H 的特征值和特征向量感兴趣。由于 H 是一个接近局部最小值的正半定对称矩阵，其特征值均为正实数。当一个特征值非常小的时候，沿着对应特征向量的移动对近似误差的影响非常小。这意味着，在该方向上，误差函数是（几乎）奇异的。误差函数的奇异性可以通过
    H 的条件表示，该条件定义为其最大特征值与最小特征值的比值。
- en: As mentioned above, a bad condition of H may occur at minima or flat points
    in the error function E(W). It appears that feed-forward learning tasks are generally
    characterized by having a singular or near-singular Hessian matrix. Although the
    said learning methods are mathematically not influenced by a badly conditioned
    Hessian, it does lead to inaccuracies due to the limited floating point accuracy
    of the digital computer.
  id: totrans-2480
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，H 的不良状态可能发生在误差函数 E(W) 的极小值或平坦点。前馈学习任务通常具有奇异或接近奇异的 Hessian 矩阵。虽然上述学习方法在数学上不受不良条件
    Hessian 的影响，但由于数字计算机的浮点精度有限，它确实会导致不准确性。
- en: 9.2.3 What Causes The Singularities
  id: totrans-2481
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2.3 奇异性的原因
- en: Reference [10] lists a few cases in which the Hessian may become singular or
    nearsingular. The listed reasons are associated with the ill character of the
    sigmoid transfer function. Typical for this function is the fact that limx→∞ s(x)
    = c+ and limx→−∞ s(x) = c−. Also, bad conditioning can be the result of uncentered
    data, and can also be alleviated [11].
  id: totrans-2482
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献 [10] 列出了一些 Hessian 可能变为奇异或接近奇异的情况。列出的原因与 sigmoid 转移函数的不良特性有关。该函数的典型特征是
    limx→∞ s(x) = c+ 和 limx→−∞ s(x) = c−。此外，不良条件也可能是由于数据未中心化所导致的，并且可以得到缓解 [11]。
- en: Assuming that some neuron is in this 'saturated' state for all learning patterns,
    its input weights will have a delta equal to 0 (according to the backpropagation
    rule) such that these weights will never change. For each of the incoming weights
    of this neuron, this leads to a 0-row in H, and therefore singularity.
  id: totrans-2483
  prefs: []
  type: TYPE_NORMAL
  zh: 假设某个神经元在所有学习模式中处于这种“饱和”状态，其输入权重的增量将等于0（根据反向传播规则），使得这些权重永远不会改变。对于该神经元的每个输入权重，这导致H中的0行，从而形成奇异性。
- en: However, there is another important reason for singularity, which may especially
    occur after network initialization. When a multi-layer feed-forward network has
    a small (e.g., less than 0.1) weight leaving from a hidden unit, the influence
    of the *weights that feed into this hidden unit* is significantly reduced. Therefore
    the *∂e/∂w*k will be close to 0, leading to near-null rows in J and a near-singular
    H.
  id: totrans-2484
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有另一个重要的奇异性原因，这种情况可能特别发生在网络初始化后。当一个多层前馈网络从隐藏单元发出的权重很小（例如，小于0.1）时，*流入该隐藏单元的权重*的影响显著减小。因此，*∂e/∂w*k将接近于0，导致J中接近于零的行以及接近奇异的H。
- en: 'We observe that this kind of singularity is very common and touches a characteristic
    problem in feed-forward network learning: The gradients in the lowerlayer weights
    are influenced by the higher-layer weights. A related problem is the influence
    that the change of the weights between the hidden and output units have on the
    change of the input weights; when they rapidly and frequently change, as will
    be the case during the initial stages of learning, the lower weights will have
    nonsensical repeated perturbations.'
  id: totrans-2485
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，这种奇异性非常普遍，并触及前馈网络学习中的一个特征问题：下层权重的梯度受到上层权重的影响。相关的问题是隐藏单元和输出单元之间权重变化对输入权重变化的影响；当它们快速且频繁地变化时，就像学习初始阶段的情况一样，较低的权重会出现无意义的重复扰动。
- en: 9.2.4 Definition Of Minimum
  id: totrans-2486
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2.4 最小值的定义
- en: A minimum is said to be reached when the derivative of the error function is
    zero, i.e., *∂E/∂w*1≤k≤n = 0. Since the gradient of the error function equals
    the column-sum of the Jacobian, a minimum has been reached when
  id: totrans-2487
  prefs: []
  type: TYPE_NORMAL
  zh: 当误差函数的导数为零时，称为达到最小值，即*∂E/∂w*1≤k≤n = 0。由于误差函数的梯度等于雅可比矩阵的列和，因此当达到最小值时
- en: $\forall1\leq k\leq n:\sum_{i=1}^{Mp}\frac{\partial e_{i}}{\partial w_{k}}=0,$  $\forall1\leq
    k\leq n:\sum_{i=1}^{Mp}\frac{\partial e_{i}}{\partial w_{k}}=0,$
  id: totrans-2488
  prefs: []
  type: TYPE_NORMAL
  zh: $\forall1\leq k\leq n:\sum_{i=1}^{Mp}\frac{\partial e_{i}}{\partial w_{k}}=0,$  $\forall1\leq
    k\leq n:\sum_{i=1}^{Mp}\frac{\partial e_{i}}{\partial w_{k}}=0,$
- en: $$(9.9)$$
  id: totrans-2489
  prefs: []
  type: TYPE_NORMAL
  zh: $$(9.9)$$
- en: 'i.e., when each of the columns adds up to 0. Eq. (9.9) defines the minimum
    for a batch-learning system: the gradient, when summed over all learning samples,
    should be 0. This also means, however, that it may occur that elements of a column-sum
    cancel each other out, even when not all elements of the Jacobian are 0. Differently
    put, it may occur that the gradient for some patterns are non-zero, whereas the
    gradients sum up to zero.'
  id: totrans-2490
  prefs: []
  type: TYPE_NORMAL
  zh: 即，当每一列的和为0时。公式(9.9)定义了批量学习系统的最小值：对所有学习样本求和时，梯度应为0。然而，这也意味着，某些列的和可能相互抵消，即使雅可比矩阵的所有元素并不全为0。换句话说，某些模式的梯度可能非零，而梯度的总和却为零。
- en: The optimal case is reached when all elements of the Jacobian are 0. This means,
    of course, that the residual error of each pattern is 0.
  id: totrans-2491
  prefs: []
  type: TYPE_NORMAL
  zh: 当雅可比矩阵的所有元素均为0时，达到最佳情况。这当然意味着每个模式的残差误差为0。
- en: 9.3 Local Minima Are Caused By Backpropagation
  id: totrans-2492
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 局部最小值由反向传播造成
- en: 'In this section we propose a new neural network structure which alleviates
    the above problems. In the standard backpropagation learning procedure, the gradient
    of the error function with respect to the weights is determined by computing the
    following steps for each learning pattern:'
  id: totrans-2493
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提出了一种新的神经网络结构，以缓解上述问题。在标准反向传播学习程序中，关于权重的误差函数梯度通过对每个学习模式计算以下步骤来确定：
- en: 1. For each output unit o, compute the delta δo = yo − ao where ao is the activation
    value for that unit, when a learning sample is presented.
  id: totrans-2494
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 对于每个输出单元o，当呈现学习样本时，计算增量δo = yo − ao，其中ao是该单元的激活值。
- en: '2. Compute the weight derivative for the weights who from the hidden to output
    units:'
  id: totrans-2495
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 计算从隐藏单元到输出单元的权重导数：
- en: $$\Delta w_{h o}=\delta_{o}a_{h}$$
  id: totrans-2496
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w_{h o}=\delta_{o}a_{h}$$
- en: where ah is the activation value for the hidden unit.
  id: totrans-2497
  prefs: []
  type: TYPE_NORMAL
  zh: 其中ah是隐藏单元的激活值。
- en: '3. Compute the delta for the hidden unit:'
  id: totrans-2498
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 计算隐藏单元的增量：
- en: $$\delta_{h}=\sum_{o}\delta_{o}w_{h o}s^{\prime}(a_{h}).$$
  id: totrans-2499
  prefs: []
  type: TYPE_NORMAL
  zh: $$\delta_{h}=\sum_{o}\delta_{o}w_{h o}s^{\prime}(a_{h}).$$
- en: $$(9.10)$$
  id: totrans-2500
  prefs: []
  type: TYPE_NORMAL
  zh: $$(9.10)$$
- en: 4. Compute the weight derivative for the weights wih from the input to hidden
  id: totrans-2501
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 计算从输入到隐藏层的权重wih的权重导数
- en: 'units:'
  id: totrans-2502
  prefs: []
  type: TYPE_NORMAL
  zh: 单元：
- en: $$\Delta w_{i h}=\delta_{h}a_{i}=\sum_{o}\delta_{o}w_{h o}s^{\prime}(a_{h})a_{i}.$$
  id: totrans-2503
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w_{i h}=\delta_{h}a_{i}=\sum_{o}\delta_{o}w_{h o}s^{\prime}(a_{h})a_{i}.$$
- en: The gradient is then computed as the summation of the Δw's.
  id: totrans-2504
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，梯度作为Δw的总和计算。
- en: 'From (9.10) we can see that there are four cases when the gradient for a weight
    from an input to a hidden unit is negligible, such that the corresponding row
    and column in the Hessian are near-zero:'
  id: totrans-2505
  prefs: []
  type: TYPE_NORMAL
  zh: 从（9.10）我们可以看到，当输入到隐藏单元的权重的梯度微不足道时，会出现四种情况，以至于Hessian的对应行和列接近零：
- en: '- When δo is small. This is correct, since that case means that the output
    of the network is close to its desired output.'
  id: totrans-2506
  prefs: []
  type: TYPE_NORMAL
  zh: '- 当δo很小。这是正确的，因为该情况意味着网络的输出接近其期望输出。'
- en: '- When who is small. This is an undesired situation: A small weight from hidden
    to output unit paralyzes weights from input to hidden units. This is especially
    important since the weight might have to change its value later.'
  id: totrans-2507
  prefs: []
  type: TYPE_NORMAL
  zh: '- 当谁很小。这是一种不理想的情况：隐藏层到输出单元的小权重使输入到隐藏单元的权重瘫痪。这一点尤为重要，因为权重可能需要在后续改变其值。'
- en: '- When s(ah) is small; this occurs when the weight wih is large. Again, this
    saturation type of paralysis is undesired.'
  id: totrans-2508
  prefs: []
  type: TYPE_NORMAL
  zh: '- 当s(ah)很小；这发生在权重wih很大时。这种饱和类型的瘫痪是不希望的。'
- en: '- Finally, when ai is small. This is desired: When the input value is insignificant,
    it should have no influence on the output.'
  id: totrans-2509
  prefs: []
  type: TYPE_NORMAL
  zh: '- 最后，当ai很小。这是期望的：当输入值微不足道时，它不应对输出产生影响。'
- en: 9.3.1 A New Neural Network Structure
  id: totrans-2510
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3.1 一种新的神经网络结构
- en: 'In order to alleviate these problems, we propose a change to the learning system
    of (9.10) as follows:'
  id: totrans-2511
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这些问题，我们建议对（9.10）的学习系统进行如下更改：
- en: $$\Delta w_{i h}=\sum_{o}\delta_{o}(w_{h o}s^{\prime}[a_{h}]+c)a_{i}=\delta_{h}a_{i}+c\sum_{o}\delta_{o}a_{i}.$$
  id: totrans-2512
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w_{i h}=\sum_{o}\delta_{o}(w_{h o}s^{\prime}[a_{h}]+c)a_{i}=\delta_{h}a_{i}+c\sum_{o}\delta_{o}a_{i}.$$
- en: By adding a constant c to the middle term, we can solve both paralysis problems.
    In effect, an extra connection from each input unit to each output unit is created,
    with a weight value coupled to the weight from the input to hidden unit.
  id: totrans-2513
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在中间项添加一个常数c，我们可以解决这两个瘫痪问题。实际上，从每个输入单元到每个输出单元创建了一个额外的连接，其权重值与从输入到隐藏单元的权重相耦合。
- en: '![197_image_0.png](197_image_0.png)'
  id: totrans-2514
  prefs: []
  type: TYPE_IMG
  zh: '![197_image_0.png](197_image_0.png)'
- en: Fig. 9.2. An exemplar adapted feed-forward neural network
  id: totrans-2515
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2. 一种示例适应的前馈神经网络
- en: Although c can be made a learnable parameter, in the sequel we will assume c
    = 1. The o'th output of the neural network is now computed as
  id: totrans-2516
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然c可以作为可学习参数，但在后续内容中我们将假设c = 1。神经网络的第o个输出现在计算为
- en: $${\cal M}(\mathbf{x},W)_{o}=\sum_{h}\left(w_{ho}s\left[\sum_{i}w_{ih}x_{i}+\theta_{h}\right]+\sum_{i}w_{ih}x_{i}\right)+\theta_{o}.\tag{9.11}$$
  id: totrans-2517
  prefs: []
  type: TYPE_NORMAL
  zh: $${\cal M}(\mathbf{x},W)_{o}=\sum_{h}\left(w_{ho}s\left[\sum_{i}w_{ih}x_{i}+\theta_{h}\right]+\sum_{i}w_{ih}x_{i}\right)+\theta_{o}.\tag{9.11}$$
- en: We call the new network the linearly augmented feed-forward network. The structure
    of this network is depicted in figure 9.2. Note the equivalence of N and M, viz.,
  id: totrans-2518
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称新的网络为线性增强前馈网络。该网络的结构如图9.2所示。注意N和M的等价性，即，
- en: $${\mathcal{M}}(\mathbf{x},W)_{o}\equiv{\mathcal{N}}(\mathbf{x},W)_{o}+\sum_{h}\sum_{i}w_{i
    h}x_{i}.$$
  id: totrans-2519
  prefs: []
  type: TYPE_NORMAL
  zh: $${\mathcal{M}}(\mathbf{x},W)_{o}\equiv{\mathcal{N}}(\mathbf{x},W)_{o}+\sum_{h}\sum_{i}w_{i
    h}x_{i}.$$
- en: $$(9.12)$$
  id: totrans-2520
  prefs: []
  type: TYPE_NORMAL
  zh: $$(9.12)$$
- en: 9.3.2 Influence on the Approximation Error E
  id: totrans-2521
  prefs: []
  type: TYPE_NORMAL
  zh: 9.3.2 对近似误差E的影响
- en: Although the optimal W will be different for the networks N and M, we can still
    compare the forms of the error functions EN vs. EM. Using (9.2) and (9.11)
  id: totrans-2522
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最优W在网络N和M中会有所不同，但我们仍然可以比较误差函数EN与EM的形式。使用（9.2）和（9.11）
- en: we can compute the error in the approximation of a single learning sample (x,
    y)
  id: totrans-2523
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算单个学习样本（x, y）的近似误差
- en: 'with N inputs, κ hidden units, and a single output:'
  id: totrans-2524
  prefs: []
  type: TYPE_NORMAL
  zh: 有N个输入，κ个隐藏单元和一个输出：
- en: $$E_{\mathcal{M}}(W)^{2}=\left(\mathcal{M}[\mathbf{x},W]-y\right)^{2}$$
  id: totrans-2525
  prefs: []
  type: TYPE_NORMAL
  zh: $$E_{\mathcal{M}}(W)^{2}=\left(\mathcal{M}[\mathbf{x},W]-y\right)^{2}$$
- en: $${}^{2}=({\cal M}[\mathbf{x},W]-y)^{2}\tag{9.13}$$ $$=\left({\cal N}(\mathbf{x},W)-y+\sum_{i}\sum_{h}w_{ih}x_{i}\right)^{2}$$
    $$=E_{\cal N}(W)^{2}+2\sum_{i}\sum_{h}w_{ih}x_{i}({\cal N}[\mathbf{x},W]-y)+\left(\sum_{i}\sum_{h}w_{ih}x_{i}\right)^{2}.$$
  id: totrans-2526
  prefs: []
  type: TYPE_NORMAL
  zh: $${}^{2}=({\cal M}[\mathbf{x},W]-y)^{2}\tag{9.13}$$ $$=\left({\cal N}(\mathbf{x},W)-y+\sum_{i}\sum_{h}w_{ih}x_{i}\right)^{2}$$
    $$=E_{\cal N}(W)^{2}+2\sum_{i}\sum_{h}w_{ih}x_{i}({\cal N}[\mathbf{x},W]-y)+\left(\sum_{i}\sum_{h}w_{ih}x_{i}\right)^{2}.$$
- en: 'The error for the network M differs from the error for N in two terms. When
    we consider EM at those values of W where EN is minimal, we can see that the difference
    between EN and EM consists of a normalization term h wTh x; wh is the vector of
    weights connecting the inputs to hidden unit h. This non-negative term will only
    be zero when the vectors wT1 x, wT2 x, ..., wTκ x cancel each other out for each
    input vector x which is in the training set. In words: EM(W)2 introduces a penalty
    for hidden units doing the same thing, thus making the network use its resources
    more efficiently.'
  id: totrans-2527
  prefs: []
  type: TYPE_NORMAL
  zh: 网络M的误差与网络N的误差在两个项上有所不同。当我们在EN最小的那些W值下考虑EM时，可以看出EN和EM之间的差异由一个归一化项h wTh x组成；wh是连接输入与隐藏单元h的权重向量。只有当向量wT1
    x, wT2 x, ..., wTκ x对于每个训练集中的输入向量x相互抵消时，这个非负项才会为零。换句话说：EM(W)²对隐藏单元执行相同操作施加了惩罚，从而使网络更加高效地使用其资源。
- en: 9.3.3 M And The Universal Approximation Theorems
  id: totrans-2528
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3.3 M与通用逼近定理
- en: It has been shown in various publications [3, 6, 8] that the ordinary feed-forward
    neural network N can represent any Borel-measurable function with a single layer
    of hidden units which have sigmoidal or Gaussian activation functions in the hidden
    layer.
  id: totrans-2529
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种出版物中已显示[3, 6, 8]，普通前馈神经网络N可以用具有sigmoid或高斯激活函数的单层隐藏单元表示任何Borel可测函数。
- en: Theorem 1. The network M *can represent any Borel-measurable function with*
    a single layer of hidden units which have sigmoidal or Gaussian activation functions
    in the hidden layer.
  id: totrans-2530
  prefs: []
  type: TYPE_NORMAL
  zh: 定理1. 网络M*可以用*具有sigmoid或高斯激活函数的单层隐藏单元表示任何Borel可测函数。
- en: Proof. We show that any network N can be written as a network M; therefore,
    the class of networks M *are universal approximators.*
  id: totrans-2531
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。我们展示任何网络N可以写成网络M；因此，网络M的类别*是通用逼近器*。
- en: By using (9.1) and (9.11),
  id: totrans-2532
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用(9.1)和(9.11)，
- en: $$\mathcal{N}(\mathbf{x},W)_{o}=\sum_{h=1}^{\kappa}w_{ho}s\left(\sum_{i}w_{ih}x_{i}+\theta_{h}\right)+\theta_{o}$$
    $$=\sum_{h=1}^{\kappa}\left(w_{ho}s\left[\sum_{i}w_{ih}x_{i}+\theta_{h}\right]+\sum_{i}w_{ih}x_{i}\right)+\theta_{o}-\sum_{h=1}^{\kappa}\sum_{i}w_{ih}x_{i}$$
    $$=\mathcal{M}(\mathbf{x},W)_{o}+\sum_{l=\kappa+1}^{2\kappa}\left(0\left[\sum_{i}-w_{i,l-\kappa}x_{i}+0\right]+\sum_{i}-w_{i,l-\kappa}x_{i}\right)+0$$
    $$=\mathcal{M}(\mathbf{x},W)_{o}+\mathcal{M}(\mathbf{x},V)_{o}$$
  id: totrans-2533
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathcal{N}(\mathbf{x},W)_{o}=\sum_{h=1}^{\kappa}w_{ho}s\left(\sum_{i}w_{ih}x_{i}+\theta_{h}\right)+\theta_{o}$$
    $$=\sum_{h=1}^{\kappa}\left(w_{ho}s\left[\sum_{i}w_{ih}x_{i}+\theta_{h}\right]+\sum_{i}w_{ih}x_{i}\right)+\theta_{o}-\sum_{h=1}^{\kappa}\sum_{i}w_{ih}x_{i}$$
    $$=\mathcal{M}(\mathbf{x},W)_{o}+\sum_{l=\kappa+1}^{2\kappa}\left(0\left[\sum_{i}-w_{i,l-\kappa}x_{i}+0\right]+\sum_{i}-w_{i,l-\kappa}x_{i}\right)+0$$
    $$=\mathcal{M}(\mathbf{x},W)_{o}+\mathcal{M}(\mathbf{x},V)_{o}$$
- en: where V is a weight matrix such that the elements of V corresponding to the
    weights from hidden to output units are 0, and the other weights equal the negation
    of its W counterparts. Furthermore, bias weights are set to 0.
  id: totrans-2534
  prefs: []
  type: TYPE_NORMAL
  zh: 其中V是一个权重矩阵，使得V中与从隐藏单元到输出单元的权重相对应的元素为0，其他权重等于其W对应项的负值。此外，偏置权重设置为0。
- en: The sum M(x, W)o + M(x, V )o represents two M-networks, which can also be written
    as a single M(x, W)-network with the double amount of hidden units, where W =
    [W V ].
  id: totrans-2535
  prefs: []
  type: TYPE_NORMAL
  zh: 和M(x, W)o + M(x, V)o表示两个M网络，也可以写成一个具有双倍隐藏单元的单个M(x, W)-网络，其中W = [W V]。
- en: Using the theorems from [3, 6, 8] the proof is complete.
  id: totrans-2536
  prefs: []
  type: TYPE_NORMAL
  zh: 利用[3, 6, 8]中的定理，证明已完成。
- en: Note that it is also possible to write each M-network as an N -network, by doubling
    the number of hidden units and using infinitesimal weights from the input units
    to these hidden units, and their multiplicative inverse for the weights from these
    hidden to the output units.
  id: totrans-2537
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，也可以通过将隐藏单元的数量加倍并使用从输入单元到这些隐藏单元的无穷小权重，以及从这些隐藏单元到输出单元的权重的乘法逆数，将每个M网络写为N网络。
- en: Figure 9.3 shows the equivalence of an N and M network for the two-hidden unit
    case.
  id: totrans-2538
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3展示了在两个隐藏单元情况下N网络和M网络的等价性。
- en: '![199_image_0.png](199_image_0.png)'
  id: totrans-2539
  prefs: []
  type: TYPE_IMG
  zh: '![199_image_0.png](199_image_0.png)'
- en: Fig. 9.3. Equivalence of an N -network (left) and an M-network (right). Note
    that bias units are left out for clarity.
  id: totrans-2540
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3. N网络（左）和M网络（右）的等价性。为清晰起见，偏置单元未列出。
- en: 9.3.4 Example
  id: totrans-2541
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3.4 示例
- en: 'As an example, we train a network with a single hidden unit, and no bias connections,
    to represent the learning samples (1, 1) and (2, 2). The hidden unit has a sigmoid
    activation function. The function that is computed by the network thus is N (*x,
    W*) = w2s(w1x) for the original neural network, and M(*x, W*) = w2s(w1x)+w1x for
    the adapted neural network. We use the sigmoid function s(x)=1/(1 + e−x) as activation
    function, which has the following wellknown properties: limx→∞ s(x)=1, limx→−∞
    s(x)=0, and limx→±∞ s(x)=0.'
  id: totrans-2542
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，我们训练一个具有单个隐藏单元且没有偏置连接的网络，以表示学习样本(1, 1)和(2, 2)。隐藏单元采用sigmoid激活函数。因此，网络计算的函数为N
    (*x, W*) = w2s(w1x)（原始神经网络），而适应后的神经网络为M(*x, W*) = w2s(w1x) + w1x。我们使用sigmoid函数s(x)=1/(1
    + e−x)作为激活函数，其具有以下众所周知的性质：limx→∞ s(x)=1，limx→−∞ s(x)=0，以及limx→±∞ s(x)=0。
- en: Figure 9.4 shows the error function and its derivatives for this neural network.
  id: totrans-2543
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4显示了该神经网络的误差函数及其导数。
- en: 'In the top row of this figure we see the original neural network. Notice in
    the top middle figure, depicting *∂E/∂w*1, that *∂E/∂w*1 ≈ 0 for small values
    of w2. In other words, when w2 is small, w1 will hardly change its value. Similarly,
    when w1 is large, then *∂E/∂w*1 will be small due to the fact that the derivative
    of the transfer function is nearly 0. In the bottom row the modified neural network
    is depicted. Left, again, the error function. The middle figure clearly shows
    that the derivative has no areas anymore which are zero or very small. The right
    figure still shows that, if w1 has a large negative value, *∂E/∂w*2 is negligible:'
  id: totrans-2544
  prefs: []
  type: TYPE_NORMAL
  zh: 在该图的顶行中，我们看到了原始神经网络。注意在中间上方的图中，描述*∂E/∂w*1时，*∂E/∂w*1对于小值的w2近似为0。换句话说，当w2较小时，w1几乎不会改变其值。类似地，当w1较大时，*∂E/∂w*1也会很小，因为传递函数的导数几乎为0。在底行中，展示了修改后的神经网络。左侧再次是误差函数。中间的图清楚地显示，导数不再有零或非常小的区域。右侧的图仍然表明，如果w1有一个大的负值，*∂E/∂w*2是可以忽略的：
- en: after all, the activation value of the hidden unit is near-zero.
  id: totrans-2545
  prefs: []
  type: TYPE_NORMAL
  zh: 毕竟，隐藏单元的激活值接近于零。
- en: 9.4 Applications
  id: totrans-2546
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 应用
- en: We have applied the new learning scheme to several approximation problems. In
    all problems, each network has been run 3,000 times with different initial random
    values for the weights. In order to train the network, we used a Polak-Ribière
    conjugate gradient optimization technique with Powell restarts [14].
  id: totrans-2547
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将新的学习方案应用于多个逼近问题。在所有问题中，每个网络都以不同的初始随机权重值运行了3000次。为了训练网络，我们使用了Polak-Ribière共轭梯度优化技术，并进行了Powell重启[14]。
- en: 'The applications with real data (problems 3 and 4) use two independent sets
    of data: a learn set and a cross-validation set. In all cases, the network was'
  id: totrans-2548
  prefs: []
  type: TYPE_NORMAL
  zh: 使用真实数据的应用（问题3和4）使用两个独立的数据集：学习集和交叉验证集。在所有情况下，网络被
- en: '![200_image_0.png](200_image_0.png)'
  id: totrans-2549
  prefs: []
  type: TYPE_IMG
  zh: '![200_image_0.png](200_image_0.png)'
- en: w 1 Fig. 9.4. Error function and derivatives using the original and adapted
    learning rule. The top row shows the error function for the original learning
    rule (left), as well as its derivative *∂E/∂w*1 (centre) and *∂E/∂w*2 (right).
    The bottom row shows the same graphs for the adapted learning rule. The contour
    lines have a distance of 0.5 (left graphs) and 0.2 (middle and right graphs).
  id: totrans-2550
  prefs: []
  type: TYPE_NORMAL
  zh: w 1 图9.4。使用原始和适应学习规则的误差函数和导数。顶行显示了原始学习规则的误差函数（左），以及其导数*∂E/∂w*1（中）和*∂E/∂w*2（右）。底行显示了适应学习规则的相同图形。等高线的距离为0.5（左图）和0.2（中间和右图）。
- en: trained until the error over the cross-validation set was minimal (i.e., up
    to but not beyond the point where the network started to over-fit). The approximation
    errors that are reported are computed using the samples in the cross-validation
    set.
  id: totrans-2551
  prefs: []
  type: TYPE_NORMAL
  zh: 训练直到交叉验证集上的误差最小（即，达到但不超过网络开始过拟合的点）。报告的逼近误差是使用交叉验证集中的样本计算得出的。
- en: 'Problem 1: (synthetic data) the XOR problem. The well-known XOR problem consists
    of representing four learning samples [(0, 0), 0], [(0, 1), 1], [(1, 0), 1], and
    [(1, 1), 0]. The network has two inputs, two hidden units, and one output.'
  id: totrans-2552
  prefs: []
  type: TYPE_NORMAL
  zh: 问题1：（合成数据）异或问题。众所周知的异或问题由四个学习样本组成：[(0, 0), 0]，[(0, 1), 1]，[(1, 0), 1]，和[(1,
    1), 0]。该网络有两个输入，两个隐藏单元和一个输出。
- en: It has been noted [5] that the XOR problem is very atypical in neural network
    learning, because it is penalized for generalization. Nevertheless, the XOR problem
    is generally considered as a small standard learning benchmark problem.
  id: totrans-2553
  prefs: []
  type: TYPE_NORMAL
  zh: 已经注意到[5]，异或问题在神经网络学习中是非常不典型的，因为它会因为泛化而受到惩罚。尽管如此，异或问题通常被视为一个小型标准学习基准问题。
- en: Whereas the network N reaches local minima in 22.4% of the runs, the linearly
    augmented network M always reached a global minimum. For N , the average number
    of steps to obtain an approximation error of 0.0 (within the 32-bit floating point
    accuracy of the computer) equals 189.1; for M, 98.3 steps were required.
  id: totrans-2554
  prefs: []
  type: TYPE_NORMAL
  zh: 而网络N在22.4%的运行中达到局部最小值，线性增强网络M始终达到了全局最小值。对于N，获得0.0的近似误差（在计算机的32位浮点精度范围内）所需的平均步数为189.1；对于M，则需要98.3步。
- en: When using the ordinary error backpropagation learning rule (i.e., no conjugate
    gradient learning), the XOR learning problem has been reported to lead to 8.7%
    local minima [14].
  id: totrans-2555
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用普通的误差反向传播学习规则时（即，没有共轭梯度学习），据报道XOR学习问题导致8.7%的局部最小值[14]。
- en: 'Problem 2: (synthetic data) approximating tan(x). As a second test, the networks
    have been used for the approximation of the function F(x) = tan(x).'
  id: totrans-2556
  prefs: []
  type: TYPE_NORMAL
  zh: 问题2：（合成数据）逼近tan(x)。作为第二个测试，网络用于函数F(x) = tan(x)的逼近。
- en: The function has been uniformly sampled in the domain [0, π] using 20 learning
    samples in total. For the approximation we used a network with one input, five
    hidden units in a single hidden layer, and one output.
  id: totrans-2557
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数在区间[0, π]中均匀采样，总共使用了20个学习样本。为了进行逼近，我们使用了一个输入、一个隐藏层中有五个隐藏单元和一个输出的网络。
- en: With network N , 14.6% of the runs lead to a local minimum. The linearly augmented
    neural network is not perfect; it is stuck in a local minimum in 6.2%
  id: totrans-2558
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络N中，14.6%的运行导致局部最小值。线性增强神经网络并不完美；在6.2%的情况下，它陷入了局部最小值。
- en: 'of the runs. In all other cases, a global minimum of very close to 0.0 was
    found. Problem 3: (real data) approximating robot arm inverse kinematics. Thirdly
    we approximated data describing a hand-eye coordination learning problem with
    a Manutec R2 robot arm.'
  id: totrans-2559
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有其他情况下，发现了非常接近0.0的全局最小值。问题3：（真实数据）逼近机器人手臂的逆运动学。第三，我们逼近了描述手眼协调学习问题的数据，使用了Manutec
    R2机器人手臂。
- en: The data is organized as follows. There are five input variables, describing
    the current position of the robot arm in joint angles θ2, θ3, as well as the visual
    position of an object with respect to the hand-held camera in pixel coordinates
    x, y, z. The output part of the data consists of the required joint rotations
    Δθ1, Δθ2, and Δθ3 necessary to reach the object. See [15] for further description
    of this hand-eye coordination problem. In this particular problem, 1103 learning
    samples are used; 1103 samples are used for cross-validation. The optimal of six
    hidden units in a single layer [15] is used. Only the cross-validating data is
    used for evaluating the network.
  id: totrans-2560
  prefs: []
  type: TYPE_NORMAL
  zh: 数据组织如下。有五个输入变量，描述机器人手臂在关节角度θ2、θ3处的当前位置，以及相对于手持摄像头的物体在像素坐标x、y、z中的视觉位置。数据的输出部分由所需的关节旋转Δθ1、Δθ2和Δθ3组成，以到达目标。有关手眼协调问题的进一步描述，请参见[15]。在这个特定问题中，使用了1103个学习样本；1103个样本用于交叉验证。使用单层中六个隐藏单元的最优[15]。仅使用交叉验证数据来评估网络。
- en: 'With the normal network N , in 2.3% of the cases the network got stuck in a
    minimum with an unacceptably high error for both the learn and test sets. This
    never occurred in the 3,000 trials in which the linearly augmented network was
    used. The cross-validated approximation error after 10,000 learning steps equals
    4.20 · 10−4 for network N , and 4.04 · 10−4 for network M (both values are average
    per learning sample). The new method shows a slight improvement here. Problem
    4: (real data) gear box deformation data. The final test consists of the following
    problem, which is encountered in the control of a newly developed lightweight
    robot arm. A gear box connects a DC motor with a robot arm segment. In order to
    position the robot arm segment at a desired joint angle θa, the DC motor has to
    exert a force τ. However, in the normal setup a joint angle decoder is only available
    at the DC motor side, which measures the angle θm.'
  id: totrans-2561
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常网络N中，2.3%的情况下，网络陷入了一个不可接受的高误差的局部最小值，学习集和测试集都如此。在使用线性增强网络的3,000次试验中，从未发生过这种情况。经过10,000个学习步骤后，网络N的交叉验证近似误差为4.20
    · 10−4，网络M为4.04 · 10−4（这两个值是每个学习样本的平均值）。新方法在这里显示出轻微的改进。问题4：（真实数据）齿轮箱变形数据。最后的测试由以下问题组成，该问题出现在控制新开发的轻量机器人手臂时。一个齿轮箱将直流电机与机器人手臂段连接起来。为了将机器人手臂段定位到所需的关节角度θa，直流电机必须施加一个力τ。然而，在正常设置中，关节角度解码器仅在直流电机一侧可用，用于测量角度θm。
- en: By mounting an extra decoder at the arm segment in a laboratory setup we can
    measure θa. The actual joint angle θa differs slightly from θm because of the
    gear-box elasticity. We attempt to learn θa from (θm, ˙θm, τ).
  id: totrans-2562
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在实验室设置中在臂段上安装额外的解码器，我们可以测量 θa。实际关节角度 θa 与 θm 稍有不同，因为齿轮箱的弹性。我们尝试从 (θm, ˙θm,
    τ) 中学习 θa。
- en: In order to learn these data, we use a network with 3 inputs, 6 hidden units
    in a single layer, and one output. The data consists of 4,000 learning samples
    as well as 4,000 samples used for the cross-validation. In each run, up to 10,000
    learning iterations are performed but not beyond the point of overfitting.
  id: totrans-2563
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习这些数据，我们使用一个具有 3 个输入、1 层 6 个隐藏单元和 1 个输出的网络。数据包括 4,000 个学习样本和 4,000 个用于交叉验证的样本。在每次运行中，最多进行
    10,000 次学习迭代，但不超过过拟合的点。
- en: Both network N as M do not get stuck in a minimum with an unacceptably high
    error for both the learn and test sets. A surprise, however, is encountered in
    the cross-validated approximation error that is computed after 10,000 learning
    steps. For network N , this error equals 2.85 · 10−4 per learning sample; for
    M,
  id: totrans-2564
  prefs: []
  type: TYPE_NORMAL
  zh: 网络 N 和 M 都没有在学习集和测试集中陷入不可接受高误差的局部最小值。然而，在经过 10,000 次学习步骤后计算的交叉验证近似误差中遇到了一些意外。对于网络
    N，这个误差每个学习样本等于 2.85 · 10−4；对于 M，
- en: however, this error is as low as 1.87 · 10−6 per sample. Further data analysis
    has shown that the data has strong linear components, which explains the fact
    that the approximation error is two orders of magnitude lower.
  id: totrans-2565
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个误差每个样本低至 1.87 · 10−6。进一步的数据分析显示，数据具有强线性成分，这解释了近似误差低两个数量级的事实。
- en: Results are summarized in table 9.1.
  id: totrans-2566
  prefs: []
  type: TYPE_NORMAL
  zh: 结果总结在表 9.1 中。
- en: Table 9.1. Results of the ordinary feed-forward network N and the linearly augmented
    feed-forward network M on the four problems
  id: totrans-2567
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.1. 普通前馈网络 N 和线性增强前馈网络 M 在四个问题上的结果
- en: '|                         |                | N                       | M    |'
  id: totrans-2568
  prefs: []
  type: TYPE_TB
  zh: '|                         |                | N                       | M    |'
- en: '| --- | --- | --- | --- |'
  id: totrans-2569
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| XOR                     | % local minima | 22.4                    | 0.0  |'
  id: totrans-2570
  prefs: []
  type: TYPE_TB
  zh: '| XOR                     | % 局部最小值 | 22.4                    | 0.0  |'
- en: '| avg. steps              | 189.1          |                         | 98.3
    |'
  id: totrans-2571
  prefs: []
  type: TYPE_TB
  zh: '| 平均步骤              | 189.1          |                         | 98.3 |'
- en: '| tan                     | % local minima | 14.6                    | 6.2  |'
  id: totrans-2572
  prefs: []
  type: TYPE_TB
  zh: '| tan                     | % 局部最小值 | 14.6                    | 6.2  |'
- en: '| robot                   | % local minima | 2.3                     | 0.0  |'
  id: totrans-2573
  prefs: []
  type: TYPE_TB
  zh: '| 机器人                   | % 局部最小值 | 2.3                     | 0.0  |'
- en: '| 3D data                 | avg. error     | 4.20 · 10−4 4.04 · 10−4 |      |'
  id: totrans-2574
  prefs: []
  type: TYPE_TB
  zh: '| 3D 数据                 | 平均误差     | 4.20 · 10−4 4.04 · 10−4 |      |'
- en: '| gear box % local minima |                | 0.0                     | 0.0  |'
  id: totrans-2575
  prefs: []
  type: TYPE_TB
  zh: '| 齿轮箱 % 局部最小值 |                | 0.0                     | 0.0  |'
- en: '| data                    | avg. error     | 2.85 · 10−4 1.87 · 10−6 |      |'
  id: totrans-2576
  prefs: []
  type: TYPE_TB
  zh: '| 数据                    | 平均误差     | 2.85 · 10−4 1.87 · 10−6 |      |'
- en: 9.5 Conclusion
  id: totrans-2577
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 结论
- en: It has been shown that the ordinary backpropagation learning rule leads to bad
    conditioning in the matrix of second-order derivatives of the error function which
    is encountered in feed-forward neural network learning. This again leads to local
    minima and saddle points in the error landscape. In order to alleviate this problem,
    we have introduced an adaptation to the backpropagation rule, which can be implemented
    as an adapted feed-forward neural network structure. We call this network the
    the *linearly augmented feed-forward neural network*. The adaptation leads to
    a learning rule which obtains stable values for the weights which connect the
    input units with the hidden units, even while the weights from hidden to output
    units change.
  id: totrans-2578
  prefs: []
  type: TYPE_NORMAL
  zh: 已证明，普通反向传播学习规则在前馈神经网络学习中导致了误差函数的二阶导数矩阵的条件不良。这又导致误差空间中的局部最小值和鞍点。为了缓解这个问题，我们引入了对反向传播规则的适应，这可以实现为一种适应的前馈神经网络结构。我们称这个网络为
    *线性增强前馈神经网络*。这种适应导致了一种学习规则，使连接输入单元和隐藏单元的权重获得稳定值，即使从隐藏单元到输出单元的权重发生变化。
- en: A mathematical analysis shows the validity of the method; in particular, the
    universal approximation theorems are shown to remain valid with the new neural
    network structure. The application of the new method to two sets of synthetic
    data and two sets of real data shows that the new method is much less sensitive
    to local minima, and reaches an optimum in fewer iterations. Acknowledgments.
    The authors acknowledge the help of Alin Albu-Schäffer in supplying the gear box
    data.
  id: totrans-2579
  prefs: []
  type: TYPE_NORMAL
  zh: 数学分析表明该方法的有效性；特别是，普遍逼近定理在新的神经网络结构下仍然有效。将新方法应用于两组合成数据和两组真实数据表明，新方法对局部极小值的敏感性大大降低，并且在更少的迭代中达到最优。致谢。作者感谢Alin
    Albu-Schäffer在提供齿轮箱数据方面的帮助。
- en: '[1] Aarts, E., Korst, J.: Simulated Annealing and Boltzmann Machines. John
    Wiley'
  id: totrans-2580
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Aarts, E., Korst, J.：模拟退火与玻尔兹曼机。约翰·威利'
- en: '& Sons (1989)'
  id: totrans-2581
  prefs: []
  type: TYPE_NORMAL
  zh: '& Sons（1989年）'
- en: '[2] Battiti, R.: First- and second-order methods for learning: Between steepest
    descent and Newton''s method. Neural Computation 4, 141–166 (1992)'
  id: totrans-2582
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Battiti, R.：学习的第一阶和第二阶方法：在最陡下降法和牛顿法之间。神经计算 4，141–166（1992年）'
- en: '[3] Cybenko, G.: Approximation by superpositions of a sigmoidal function. Mathematics
    of Control, Signals, and Systems 2(4), 303–314 (1989)'
  id: totrans-2583
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Cybenko, G.：通过 sigmoid 函数的叠加进行逼近。控制、信号与系统的数学 2(4)，303–314（1989年）'
- en: '[4] DasGupta, B., Siegelmann, H.T., Sontag, E.D.: On the complexity of training
    neural networks with continuous activation functions. IEEE Transactions on Neural
    Networks 6(6), 1490–1504 (1995)'
  id: totrans-2584
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] DasGupta, B., Siegelmann, H.T., Sontag, E.D.：关于训练具有连续激活函数的神经网络的复杂性。IEEE神经网络汇刊
    6(6)，1490–1504（1995年）'
- en: '[5] Fahlman, S.E.: An empirical study of learning speed in back-propagation
    networks. Technical Report CMU–CS–88-0-162, Carnegie Mellon University (September
    1988)'
  id: totrans-2585
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Fahlman, S.E.：对反向传播网络学习速度的实证研究。技术报告 CMU–CS–88-0-162，卡内基梅隆大学（1988年9月）'
- en: '[6] Funahashi, K.-I.: On the approximate realization of continuous mappings
    by neural networks. Neural Networks 2(3), 183–192 (1989)'
  id: totrans-2586
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Funahashi, K.-I.：关于神经网络对连续映射的近似实现。神经网络 2(3)，183–192（1989年）'
- en: '[7] Hochreiter, S., Schmidhuber, J.: Flat minima. Neural Computation 9(1),
    1–42'
  id: totrans-2587
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Hochreiter, S., Schmidhuber, J.：平坦极小值。神经计算 9(1)，1–42'
- en: (1997)
  id: totrans-2588
  prefs: []
  type: TYPE_NORMAL
  zh: （1997年）
- en: '[8] Hornik, K., Stinchcombe, M., White, H.: Multilayer feedforward networks
    are universal approximators. Neural Networks 2(5), 359–366 (1989)'
  id: totrans-2589
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Hornik, K., Stinchcombe, M., White, H.：多层前馈网络是通用逼近器。神经网络 2(5)，359–366（1989年）'
- en: '[9] Robbins, H., Monro, S.: A stochastic approximation method. Annals of Mathematical
    Statistics 22(1), 400–407 (1951)'
  id: totrans-2590
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Robbins, H., Monro, S.：一种随机逼近方法。数学统计年鉴 22(1)，400–407（1951年）'
- en: '[10] Saarinen, S., Bramley, R., Cybenko, G.: Ill-conditioning in neural network
    training problems. Siam Journal of Scientific Computing 14(3), 693–714 (1993)'
  id: totrans-2591
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Saarinen, S., Bramley, R., Cybenko, G.：神经网络训练问题中的病态条件。SIAM科学计算杂志 14(3)，693–714（1993年）'
- en: '[11] Schraudolph, N.N.: On centering neural network weight updates. Technical
    Report IDSIA-19-97, IDSIA (April 1997)'
  id: totrans-2592
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Schraudolph, N.N.：关于中心化神经网络权重更新。技术报告 IDSIA-19-97，IDSIA（1997年4月）'
- en: '[12] Sontag, E.D., Sussmann, H.J.: Backpropagation can give rise to spurious
    local minima even for networks without hidden layers. Complex Systems 3(1), 91–106
    (1989)'
  id: totrans-2593
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Sontag, E.D., Sussmann, H.J.：反向传播即使在没有隐层的网络中也会产生虚假的局部极小值。复杂系统 3(1)，91–106（1989年）'
- en: '[13] Unnikrishnan, K.P., Venugopal, K.P.: Alopex: A correlation based learning
    algorithm for feedforward and recurrent neural networks. Neural Computation 6,
    469–490 (1994)'
  id: totrans-2594
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Unnikrishnan, K.P., Venugopal, K.P.：Alopex：一种基于相关性的前馈和递归神经网络学习算法。神经计算
    6，469–490（1994年）'
- en: '[14] van der Smagt, P.: Minimisation methods for training feed-forward networks.
    Neural Networks 7(1), 1–11 (1994)'
  id: totrans-2595
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] van der Smagt, P.：训练前馈网络的最小化方法。神经网络 7(1)，1–11（1994年）'
- en: '[15] van der Smagt, P.: Visual Robot Arm Guidance using Neural Networks. PhD'
  id: totrans-2596
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] van der Smagt, P.：使用神经网络的视觉机器人手臂引导。博士论文'
- en: thesis, Dept of Computer Systems, University of Amsterdam (March 1995)
  id: totrans-2597
  prefs: []
  type: TYPE_NORMAL
  zh: 论文，阿姆斯特丹大学计算机系统系（1995年3月）
- en: '[16] Zhang, Q.J., Zhang, Y.J., Ye, W.: Local-sparse connection multilayer networks.'
  id: totrans-2598
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Zhang, Q.J., Zhang, Y.J., Ye, W.：局部稀疏连接多层网络。'
- en: 'In: Proc. IEEE Conf. Neural Networks, pp. 1254–1257. IEEE (1995)'
  id: totrans-2599
  prefs: []
  type: TYPE_NORMAL
  zh: 在：IEEE神经网络会议论文集，第1254–1257页。IEEE（1995年）
- en: 10 Centering Neural Network Gradient Factors-
  id: totrans-2600
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 中心化神经网络梯度因子-
- en: Nicol N. Schraudolph IDSIA, Corso Elvezia 36 6900 Lugano, Switzerland nic@idsia.ch
    http://www.idsia.ch/
  id: totrans-2601
  prefs: []
  type: TYPE_NORMAL
  zh: Nicol N. Schraudolph IDSIA，科尔索·埃尔维齐亚36号，6900卢加诺，瑞士 nic@idsia.ch http://www.idsia.ch/
- en: Abstract. It has long been known that neural networks can learn faster when
    their input and hidden unit activities are centered about zero; recently we have
    extended this approach to also encompass the centering of error signals [15].
    Here we generalize this notion to all factors involved in the network's gradient,
    leading us to propose centering the slope of hidden unit activation functions
    as well. Slope centering removes the linear component of backpropagated error;
    this improves credit assignment in networks with shortcut connections. Benchmark
    results show that this can speed up learning significantly without adversely affecting
    the trained network's generalization ability.
  id: totrans-2602
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要。早已知道，当神经网络的输入和隐层单元活动以零为中心时，学习速度可以加快；最近，我们扩展了这一方法，也涵盖了误差信号的中心化[15]。在这里，我们将这一概念推广到网络梯度中涉及的所有因素，提出隐层单元激活函数的斜率也应进行中心化。斜率中心化消除了反向传播误差的线性成分；这改善了具有快捷连接的网络中的信用分配。基准结果表明，这可以显著加快学习，而不会对训练后的网络的泛化能力产生不利影响。
- en: 10.1 Introduction
  id: totrans-2603
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 引言
- en: 'Centering is a general methodology for accelerating learning in adaptive systems
    of the type exemplified by neural networks - that is, systems that are typically
    nonlinear, continuous, and redundant; that learn incrementally from examples,
    generally by some form of gradient descent. Its basic tenet is:'
  id: totrans-2604
  prefs: []
  type: TYPE_NORMAL
  zh: 中心化是一种通用的方法论，用于加速自适应系统的学习，这类系统以神经网络为例——即通常是非线性、连续且冗余的系统；通过某种形式的梯度下降，从实例中逐步学习。其基本原则是：
- en: All pattern-dependent factors entering the update equation for a neural network
    weight should be centered, i.e.*, have their average over patterns* subtracted
    out.
  id: totrans-2605
  prefs: []
  type: TYPE_NORMAL
  zh: 进入神经网络权重更新方程的所有与模式相关的因素都应该进行中心化，即*应减去其在模式上的平均值*。
- en: Prior Work. It is well-known that the inputs to an LMS adaptive filter should
    be centered to permit rapid yet stable adaptation [22], and it has been argued
  id: totrans-2606
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的工作。众所周知，LMS自适应滤波器的输入应该进行中心化，以便快速而稳定地适应[22]，并且有论据表明
- en: '[12] that the same applies to input and hidden unit activity in a multi-layer
    network. Although Sejnowski [16] proposed a variant of Hebbian learning in which
    both the pre- and postsynaptic factors of the weight update are centered, the
    idea was not taken up when backpropagation became popular. The benefits of centering
    error signals in multi-layer networks were thus reported only recently [15]; here
    we finally suggest centering as a general methodology, and present backpropagation
    equations in which all factors are centered.'
  id: totrans-2607
  prefs: []
  type: TYPE_NORMAL
  zh: '[12]同样适用于多层网络中的输入和隐层单元活动。尽管Sejnowski [16]提出了一种Hebbian学习的变体，其中权重更新的前后突触因素均被中心化，但当反向传播变得流行时，这一思想并未被采纳。因此，多层网络中误差信号中心化的好处直到最近才被报道[15]；在这里，我们最终建议中心化作为一种通用的方法论，并提出所有因素均被中心化的反向传播方程。'
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-2608
  prefs: []
  type: TYPE_NORMAL
  zh: '- 先前发表在：Orr, G.B. 和 Müller, K.-R.（编）：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-2609
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0（1998）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    205–223, 2012.'
  id: totrans-2610
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon等（编）：NN: Tricks of the Trade, 第二版，LNCS 7700，第205–223页，2012年。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-2611
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: 'Independence of Architecture. Although centering is introduced here in the
    context of feedforward networks with sigmoid activation functions, the approach
    itself has a far wider reach. The implementation details may vary, but in essence
    centering is not tied to any particular architecture: its principles are equally
    applicable to feedforward and recurrent networks, with sigmoid, radial, or other
    basis functions, with or without topological structure, time delays, multiplicative
    gates, *etc.* - in short, the host of architectural elements used in neural network
    design. Independence of Learning Algorithm. Similarly, centering is not wedded
    to any particular learning algorithm either. It may be applied to deterministic
    (batch) or stochastic (online) gradient descent; more importantly, it may be freely
    combined with more sophisticated optimization techniques such as expectation maximization,
    conjugate gradient and quasi-Newton methods. It also leaves available the many
    useful tricks often employed with stochastic gradient descent, such as momentum,
    learning rate adaptation, gradient normalization, and so forth. Due to this flexibility,
    centering has the potential to further accelerate even the fastest of these methods.'
  id: totrans-2612
  prefs: []
  type: TYPE_NORMAL
  zh: 架构的独立性。尽管这里在具有sigmoid激活函数的前馈网络的背景下引入了中心化方法，但其本身的适用范围更广。实现细节可能有所不同，但从本质上讲，中心化并不依赖于任何特定架构：其原则同样适用于前馈和递归网络，使用sigmoid、径向或其他基函数，无论是否具备拓扑结构、时间延迟、乘法门等——简而言之，神经网络设计中使用的各种架构元素。学习算法的独立性。同样，中心化也不依赖于任何特定的学习算法。它可以应用于确定性（批量）或随机（在线）梯度下降；更重要的是，它可以自由与更复杂的优化技术结合，如期望最大化、共轭梯度和准牛顿法。此外，它还保留了许多常用于随机梯度下降的实用技巧，例如动量、学习率调整、梯度归一化等等。由于这种灵活性，中心化有潜力进一步加速这些方法中最快的。
- en: Overview. Section 10.2 introduces the centering approach in terms of the modifications
    it mandates for ordinary backpropagation learning. We then discuss implementation
    details in Section 10.3 before presenting benchmark results in Section 10.4. Section
    10.5 concludes our paper with a brief analysis of how centering facilitates faster
    learning.
  id: totrans-2613
  prefs: []
  type: TYPE_NORMAL
  zh: 概述。第10.2节介绍了中心化方法对普通反向传播学习所要求的修改。我们随后在第10.3节讨论实现细节，然后在第10.4节展示基准结果。第10.5节以简要分析中心化如何促进更快学习结束我们的论文。
- en: 10.2 Centered Backpropagation
  id: totrans-2614
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 中心化反向传播
- en: 'The backpropagation learning algorithm is characterized by three equations,
    describing the forward propagation of activity, the backpropagation of error,
    and the modification of weights, respectively. Here are the implications of centering
    for each of these three equations:'
  id: totrans-2615
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播学习算法的特点在于三个方程，分别描述活动的前向传播、误差的反向传播和权重的修改。以下是关于这三个方程中心化的含义：
- en: 10.2.1 Activity Propagation
  id: totrans-2616
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2.1 活动传播
- en: Conventional. Consider a neural network with activation of node j given by
  id: totrans-2617
  prefs: []
  type: TYPE_NORMAL
  zh: 常规。考虑一个神经网络，节点j的激活由下式给出：
- en: $$x_{j}\,=\,f_{j}(y_{j})\,,\quad y_{j}\,=\,\sum_{i\in A_{j}}w_{i j}\,\tilde{x}_{i}\,,$$
  id: totrans-2618
  prefs: []
  type: TYPE_NORMAL
  zh: $$x_{j}\,=\,f_{j}(y_{j})\,,\quad y_{j}\,=\,\sum_{i\in A_{j}}w_{i j}\,\tilde{x}_{i}\,.$$
- en: $$(10.1)$$
  id: totrans-2619
  prefs: []
  type: TYPE_NORMAL
  zh: $$(10.1)$$
- en: where fj is a nonlinear (typically sigmoid) activation function, wij are the
    synaptic weights, and Aj denotes the set of *anterior* nodes feeding their activity
    xˇi into node j. Conventionally, the anterior nodes' output is fed forward directly,
    *i.e.*,
  id: totrans-2620
  prefs: []
  type: TYPE_NORMAL
  zh: 其中fj是一个非线性（通常是sigmoid）激活函数，wij是突触权重，Aj表示将其活动xˇi输入节点j的*前置*节点集合。通常，前置节点的输出直接向前馈入，即，
- en: '(∀i) ˇxi ≡ xi. We imply that nodes are activated via Equation 10.1 in appropriately
    ordered (feedforward) sequence, and that some have their values clamped so as
    to represent external inputs to the network. In particular, we posit a bias input
    x0 ≡1 and require that all nodes are connected to it: (∀j >0) 0 ∈Aj .'
  id: totrans-2621
  prefs: []
  type: TYPE_NORMAL
  zh: (∀i) ˇxi ≡ xi。我们意味着节点通过方程10.1以适当的顺序（前馈）激活，并且一些节点的值被固定，以表示网络的外部输入。特别地，我们假设偏置输入x0
    ≡1，并要求所有节点与之相连：（∀j >0）0 ∈Aj。
- en: $$(10.2)$$
  id: totrans-2622
  prefs: []
  type: TYPE_NORMAL
  zh: $$(10.2)$$
- en: Centered. As suggested by LeCun et al. [12], the activity of the network's input
    and hidden units should be centered to permit faster learning (see Chapter 1).
    We do so by setting
  id: totrans-2623
  prefs: []
  type: TYPE_NORMAL
  zh: 中心化。正如LeCun等人所建议的[12]，网络输入和隐含单元的活动应中心化，以允许更快的学习（见第1章）。我们通过设置实现这一点。
- en: $$(\forall i>0)\;\;\;\check{x}_{i}\;=\;x_{i}-\langle x_{i}\rangle\,,\tag{1}$$
  id: totrans-2624
  prefs: []
  type: TYPE_NORMAL
  zh: $$(\forall i>0)\;\;\;\check{x}_{i}\;=\;x_{i}-\langle x_{i}\rangle\,,\tag{1}$$
- en: where · denotes averaging over training samples (see Section 10.3 for ways to
    implement this operator). Note that the bias input must not be centered - since
    x0 = x0 = 1, it would otherwise become inoperative.
  id: totrans-2625
  prefs: []
  type: TYPE_NORMAL
  zh: 其中·表示对训练样本的平均（有关实现此操作符的方法，请参见第10.3节）。请注意，偏置输入必须不被中心化——因为x0 = x0 = 1，否则它将变得无效。
- en: 10.2.2 Weight Modification
  id: totrans-2626
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2.2 权重修改
- en: Conventional. The weights wij of the network given in Equation 10.1 are typically
    optimized by gradient descent in some objective function E. With a local step
    size η ij for each weight, this results in the weight update equation
  id: totrans-2627
  prefs: []
  type: TYPE_NORMAL
  zh: 常规。公式10.1中给出的网络权重wij通常通过梯度下降在某个目标函数E中进行优化。对于每个权重的局部步长η ij，这导致了权重更新方程
- en: $$\Delta w_{i j}\,=\,\eta_{\,i j}\,\delta_{j}\,\dot{x}_{i}\,,\;\;\mathrm{where}\;\;\delta_{j}\,=\,-\partial
    E/\partial y_{j}\,.$$
  id: totrans-2628
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w_{i j}\,=\,\eta_{\,i j}\,\delta_{j}\,\dot{x}_{i}\,,\;\;\mathrm{其中}\;\;\delta_{j}\,=\,-\partial
    E/\partial y_{j}\,.$$
- en: $$(10.3)$$
  id: totrans-2629
  prefs: []
  type: TYPE_NORMAL
  zh: $$(10.3)$$
- en: Centered. We have recently proposed [15] that the error signals δj should be
    centered as well to achieve even faster convergence. This is done by updating
    all non-bias weights via
  id: totrans-2630
  prefs: []
  type: TYPE_NORMAL
  zh: 中心化。我们最近提出[15]误差信号δj也应该被中心化，以实现更快的收敛。这是通过更新所有非偏置权重来完成的：
- en: ($\forall i>0$) $\Delta w_{ij}=\eta_{\,ij}\,\delta_{j}\,\tilde{x}_{i}\,,$ where
    $\delta_{j}=\delta_{j}-\langle\delta_{j}\rangle\,.$ (10.4)
  id: totrans-2631
  prefs: []
  type: TYPE_NORMAL
  zh: ($\forall i>0$) $\Delta w_{ij}=\eta_{\,ij}\,\delta_{j}\,\tilde{x}_{i}\,,$ 其中
    $\delta_{j}=\delta_{j}-\langle\delta_{j}\rangle\,.$ (10.4)
- en: 'As before, this centered update must not be used for bias weights, for otherwise
    they would remain forever stuck (except for stochastic fluctuations) at their
    present values:'
  id: totrans-2632
  prefs: []
  type: TYPE_NORMAL
  zh: 和以前一样，这种中心更新不能用于偏置权重，否则它们将永远停留在当前值（除了随机波动）：
- en: $\mathbf{M}$
  id: totrans-2633
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathbf{M}$
- en: '* Δw0j  ∝ -ˇδj'
  id: totrans-2634
  prefs: []
  type: TYPE_NORMAL
  zh: '* Δw0j  ∝ -ˇδj'
- en: $$\langle\Delta w_{0j}\rangle\,\propto\,\langle\check{\delta}_{j}\rangle\,=\,\langle\delta_{j}\rangle-\langle\delta_{j}\rangle\,=\,0\,.$$
  id: totrans-2635
  prefs: []
  type: TYPE_NORMAL
  zh: $$\langle\Delta w_{0j}\rangle\,\propto\,\langle\check{\delta}_{j}\rangle\,=\,\langle\delta_{j}\rangle-\langle\delta_{j}\rangle\,=\,0\,.$$
- en: Instead, bias weights are updated conventionally (Equation 10.3). Since this
    means that the average error δj  is given exclusively to the bias weight w0j ,
    we have previously called this technique *d.c. error shunting* [15].
  id: totrans-2636
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，偏置权重按照常规进行更新（公式10.3）。由于这意味着平均误差δj专门给偏置权重w0j，因此我们之前称这种技术为*d.c.误差分流*[15]。
- en: 10.2.3 Error Backpropagation
  id: totrans-2637
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2.3 误差反向传播
- en: Conventional. For output units, the error δj can be computed directly from
  id: totrans-2638
  prefs: []
  type: TYPE_NORMAL
  zh: 常规。对于输出单元，误差δj可以直接计算：
- en: 'the objective function; for hidden units, it must be derived through error
    backpropagation:'
  id: totrans-2639
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数；对于隐藏单元，必须通过误差反向传播来推导：
- en: $$\delta_{j}\,=\,f_{j}^{\prime}(y_{j})\,\gamma_{j}\,,\quad\gamma_{j}\,=\,\sum_{k\in
    P_{j}}w_{j k}\,\delta_{k}\,,$$
  id: totrans-2640
  prefs: []
  type: TYPE_NORMAL
  zh: $$\delta_{j}\,=\,f_{j}^{\prime}(y_{j})\,\gamma_{j}\,,\quad\gamma_{j}\,=\,\sum_{k\in
    P_{j}}w_{j k}\,\delta_{k}\,,$$
- en: $$(10.5)$$
  id: totrans-2641
  prefs: []
  type: TYPE_NORMAL
  zh: $$(10.5)$$
- en: $$(10.6)$$
  id: totrans-2642
  prefs: []
  type: TYPE_NORMAL
  zh: $$(10.6)$$
- en: 'where Pj denotes the set of *posterior* nodes fed from node j, and fj(yj )
    is the node''s current *slope* - the derivative of its nonlinearity fj at the
    present level of activation. Centered. By inserting Equation 10.6 into Equation
    10.4, we can express the weight update for hidden units as a triple product of
    their activity, backpropagated error, and slope:'
  id: totrans-2643
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Pj表示来自节点j的*后验*节点集合，fj(yj)是节点当前的*斜率*——其非线性fj在当前激活水平的导数。中心化。通过将公式10.6插入公式10.4，我们可以将隐藏单元的权重更新表示为其活动、反向传播误差和斜率的三重乘积：
- en: $$\Delta w_{i j}\,\propto\,f_{j}^{\prime}(y_{j})\,\dot{\gamma}_{j}\,\ddot{x}_{i}\,,\tag{1}$$
  id: totrans-2644
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w_{i j}\,\propto\,f_{j}^{\prime}(y_{j})\,\dot{\gamma}_{j}\,\ddot{x}_{i}\,,\tag{1}$$
- en: where γˇj denotes backpropagated *centered* errors. It is not necessary to center
    the γˇj explicitly since
  id: totrans-2645
  prefs: []
  type: TYPE_NORMAL
  zh: 其中γˇj表示反向传播的*中心*误差。显式中心化γˇj并不是必要的，因为
- en: $$\langle\tilde{\gamma}_{j}\rangle\,=\,\left\langle\sum_{k\in P_{j}}w_{jk}\,\tilde{\delta}_{k}\right\rangle\,=\,\sum_{k\in
    P_{j}}w_{jk}\,\langle\tilde{\delta}_{k}\rangle\,=\,0\,.\tag{10.8}$$
  id: totrans-2646
  prefs: []
  type: TYPE_NORMAL
  zh: $$\langle\tilde{\gamma}_{j}\rangle\,=\,\left\langle\sum_{k\in P_{j}}w_{jk}\,\tilde{\delta}_{k}\right\rangle\,=\,\sum_{k\in
    P_{j}}w_{jk}\,\langle\tilde{\delta}_{k}\rangle\,=\,0\,.\tag{10.8}$$
- en: $$(10.7)$$
  id: totrans-2647
  prefs: []
  type: TYPE_NORMAL
  zh: $$(10.7)$$
- en: By centering activity and error signals we have so far addressed two of the
    three factors in Equation 10.7, leaving the remaining third factor - the node's
    slope - to be dealt with. This is done by modifying the nonlinear part of error
    backpropagation (Equation 10.6) to
  id: totrans-2648
  prefs: []
  type: TYPE_NORMAL
  zh: 通过中心化活动和误差信号，我们至今已经解决了公式10.7中的三个因素中的两个，剩下的第三个因素——节点的斜率——待处理。这是通过修改误差反向传播的非线性部分（公式10.6）来实现的：
- en: $$\delta_{j}\,=\,\tilde{f}^{\prime}_{j}(y_{j})\,\dot{\gamma}_{j}\,,\,\,\,\,\,\mbox{where}\,\,\,\,\,\tilde{f}^{\prime}_{j}(y_{j})\,=\,f^{\prime}_{j}(y_{j})-\left\langle
    f^{\prime}_{j}(y_{j})\right\rangle.\tag{10.9}$$
  id: totrans-2649
  prefs: []
  type: TYPE_NORMAL
  zh: $$\delta_{j}\,=\,\tilde{f}^{\prime}_{j}(y_{j})\,\dot{\gamma}_{j}\,,\,\,\,\,\,\mbox{其中}\,\,\,\,\,\tilde{f}^{\prime}_{j}(y_{j})\,=\,f^{\prime}_{j}(y_{j})-\left\langle
    f^{\prime}_{j}(y_{j})\right\rangle.\tag{10.9}$$
- en: Decimation of Linear Errors. Note that for a *linear* node n we would have fn(yn)≡
    1, and Equation 10.9 would always yield δn ≡ 0. In other words, slope centering
    (for any node) blocks backpropagation of the *linear component* of error signals
    - that component which a linear node in the same position would receive.
  id: totrans-2650
  prefs: []
  type: TYPE_NORMAL
  zh: 线性误差的削减。注意，对于一个*线性*节点n，我们有fn(yn)≡ 1，方程10.9将始终导致δn ≡ 0。换句话说，斜率中心化（对于任何节点）阻止了误差信号的*线性成分*的反向传播——同一位置的线性节点将接收的那个成分。
- en: Viewed in terms of error decimation, we have thus taken the logical next step
    past error centering, which removed the d.c. (constant) component of error signals.
  id: totrans-2651
  prefs: []
  type: TYPE_NORMAL
  zh: 从误差削减的角度来看，我们已迈出了超越误差中心化的逻辑下一步，后者去除了误差信号的d.c.（常数）成分。
- en: Shortcuts. It was important then to have a parameter - the bias weight - to
    receive and correct the d.c. error component about to be eliminated. Likewise,
    we now require additional weights to implement the linear mapping from anterior
    to posterior nodes that the unit in question is itself no longer capable of. Formally,
    we demand that for each node j for which Equation 10.9 is used, we have
  id: totrans-2652
  prefs: []
  type: TYPE_NORMAL
  zh: 捷径。因此，重要的是有一个参数——偏置权重——来接收和修正即将被消除的d.c.误差成分。同样，我们现在需要额外的权重来实现从前节点到后节点的线性映射，而相关单元自己已经无法实现。正式地，我们要求对于每个使用方程10.9的节点j，我们有
- en: $$(\forall i\in A_{j})\ \ P_{j}\subseteq P_{i}\,.$$
  id: totrans-2653
  prefs: []
  type: TYPE_NORMAL
  zh: $$(\forall i\in A_{j})\ \ P_{j}\subseteq P_{i}\,.$$
- en: $$(10.10)$$
  id: totrans-2654
  prefs: []
  type: TYPE_NORMAL
  zh: $$(10.10)$$
- en: (∀i ∈ Aj ) Pj ⊆ Pi . (10.10)
  id: totrans-2655
  prefs: []
  type: TYPE_NORMAL
  zh: (∀i ∈ Aj ) Pj ⊆ Pi . (10.10)
- en: We refer to connections that bypass a node (or layer) in this fashion as *shortcuts*.
    It has been noted before that neural network learning sometimes improves with
    the addition of shortcut weights. In our own experiments (see Section 10.4), however,
    we find that it is slope centering that makes shortcut weights genuinely useful.
  id: totrans-2656
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以这种方式绕过节点（或层）的连接称为*捷径*。之前已经指出，神经网络学习在增加捷径权重时有时会改善。然而，在我们自己的实验中（见第10.4节），我们发现是斜率中心化使捷径权重真正有用。
- en: 'A Complementary Approach? In Chapter 9, van der Smagt and Hirzinger also advocate
    shortcuts as a means for accelerating neural network learning. Note, however,
    that their use of shortcuts is quite different from ours: in order to improve
    the conditioning of a neural network, they add shortcut connections whose weights
    are coupled to (shared with) *existing* weights. They thus suitably modify the
    network''s topology without adding new weight parameters, or deviating from a
    strict gradient-based optimization framework. By contrast, we deliberately decimate
    the linear component of the gradient for hidden units in order to focus them on
    their nonlinear task. We then use shortcuts with *additional* weight parameters
    to take care of the linear mapping that the hidden units now ignore.'
  id: totrans-2657
  prefs: []
  type: TYPE_NORMAL
  zh: 一种互补的方法？在第9章中，van der Smagt和Hirzinger也主张使用捷径来加速神经网络学习。然而，请注意，他们对捷径的使用与我们的不同：为了改善神经网络的条件，他们添加了权重与*现有*权重耦合（共享）的捷径连接。因此，他们适当地修改了网络的拓扑，而没有增加新的权重参数，也没有偏离严格的基于梯度的优化框架。相反，我们故意削弱隐藏单元的梯度线性成分，以便让它们专注于非线性任务。然后，我们使用带有*附加*权重参数的捷径来处理隐藏单元现在忽略的线性映射。
- en: 'While both these approaches use shortcuts to achieve their ends, from another
    perspective they appear almost complementary: whereas we eliminate the linear
    component from our gradient, van der Smagt and Hirzinger in fact add just such
    a component to theirs. It may even be possible to profitably combine the two approaches
    in a single - admittedly rather complicated - neural network architecture.'
  id: totrans-2658
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这两种方法都使用捷径来实现目标，但从另一个角度看，它们几乎是互补的：我们从梯度中消除了线性成分，而van der Smagt和Hirzinger实际上在他们的梯度中增加了这样的成分。甚至可能将这两种方法有益地结合在一个单一的——诚然相当复杂的——神经网络架构中。
- en: 10.3 Implementation Techniques
  id: totrans-2659
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 实施技术
- en: 'We can distinguish a variety of approaches to centering a variable in a neural
    network in terms of how the averaging operator · is implemented. Specifically,
    averaging may be performed either exactly or approximately, and applied either
    a priori, or adaptively during learning in either batch (deterministic) or online
    (stochastic) settings:'
  id: totrans-2660
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据平均运算符·的实现方式，区分多种中心化变量的方法。在具体上，平均可以精确或近似地进行，并在学习过程中以先验或自适应的方式应用于批量（确定性）或在线（随机）设置：
- en: '| centering method:   | approximate     | exact          |                       |'
  id: totrans-2661
  prefs: []
  type: TYPE_TB
  zh: '| 中心化方法：   | 近似     | 精确          |                       |'
- en: '| --- | --- | --- | --- |'
  id: totrans-2662
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| a priori            | by design       | extrinsic      |                       |'
  id: totrans-2663
  prefs: []
  type: TYPE_TB
  zh: '| 先验            | 设计       | 外在      |                       |'
- en: '| online              | running average | -              |                       |'
  id: totrans-2664
  prefs: []
  type: TYPE_TB
  zh: '| 在线              | 滚动平均 | -              |                       |'
- en: '| adaptive                      | batch           | previous batch | two-pass,
    single-pass |'
  id: totrans-2665
  prefs: []
  type: TYPE_TB
  zh: '| 自适应                      | 批量           | 上一批次 | 两遍，单遍 |'
- en: 10.3.1 A Priori Methods
  id: totrans-2666
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3.1 先验方法
- en: By Design. Some of the benefits of centering may be reaped without any modification
    of the learning algorithm, simply by setting up the system appropriately.
  id: totrans-2667
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设计。某些中心化的好处可以在不修改学习算法的情况下获得，仅需适当设置系统即可。
- en: For instance, the hyperbolic tangent (tanh) function with its symmetric range
    from -1 to 1 will typically produce better-centered output than the commonly used
    logistic sigmoid f(y)=1/(1 + e−y) ranging from 0 to 1, and is therefore the preferred
    activation function for hidden units [12]. Similarly, the input representation
    can (and should) be chosen such that inputs will be roughly centered.
  id: totrans-2668
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，具有对称范围-1到1的双曲正切（tanh）函数，通常会产生比常用的逻辑 sigmoid f(y)=1/(1 + e−y)（范围为0到1）更好中心化的输出，因此它是隐藏单元的首选激活函数[12]。类似地，输入表示可以（并且应该）选择使输入大致中心化。
- en: When using shortcuts, one may even choose *a priori* to subtract a constant
    (say, half their maximum) from hidden unit slopes to improve their centering.
  id: totrans-2669
  prefs: []
  type: TYPE_NORMAL
  zh: 使用捷径时，甚至可以选择*先验*从隐藏单元的斜率中减去一个常数（例如，最大值的一半），以改善其中心化。
- en: 'We refer to these approximate methods as centering *by design*. Though inexact,
    they provide convenient and easily implemented tricks to speed up neural network
    learning. Regardless of whether further acceleration techniques will be required
    or not, it is generally a good idea to keep centering in mind as a design principle
    when setting up learning tasks for neural networks. Extrinsic. Quantities that
    are extrinsic to the network - *i.e.*, not affected by its weight changes - may
    often be centered exactly prior to learning. In particular, for any given training
    set the network''s inputs can be centered in this fashion. Even in online settings
    where the training set is not known in advance, it is sometimes possible to perform
    such *extrinsic* centering based upon prior knowledge of the training data: instead
    of a time series x(t) one might for instance present the temporal difference signal
    x(t) − x(t−1) as input to the network, which will be centered if x(t) is stationary.'
  id: totrans-2670
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些近似方法称为*设计中心化*。尽管不精确，它们提供了方便且易于实现的技巧，以加速神经网络的学习。无论是否需要进一步的加速技术，通常在为神经网络设置学习任务时，将中心化作为设计原则是个好主意。外在。对网络而言外在的量
    - *即*，不受其权重变化影响的量 - 通常可以在学习前准确中心化。特别是，对于任何给定的训练集，网络的输入可以以这种方式进行中心化。即使在训练集未知的在线设置中，有时也可以基于对训练数据的先验知识进行这种*外在*中心化：例如，不是将时间序列x(t)作为输入，可能会将时间差信号x(t)
    − x(t−1)呈现给网络，如果x(t)是平稳的，这将被中心化。
- en: 10.3.2 Adaptive Methods
  id: totrans-2671
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3.2 自适应方法
- en: Online. When learning online, the immediate environment of a single weight within
    a multi-layer network is highly non-stationary, due to the simultaneous adaptation
    of other weights, if not due to the learning task itself. A uniquely defined average
    of some signal x(t) to be centered is therefore not available online, and we must
    make do with *running averages* - smoothed versions of the signal itself. A popular
    smoother is the *exponential trace*
  id: totrans-2672
  prefs: []
  type: TYPE_NORMAL
  zh: 在线。当进行在线学习时，由于其他权重的同时适应，单层网络中单个权重的即时环境是高度非平稳的，甚至由于学习任务本身也是如此。因此，某些信号x(t)的唯一定义的平均值在在线情况下是不可用的，我们必须依靠*滚动平均*
    - 信号本身的平滑版本。一个流行的平滑器是*指数平滑*
- en: $${\bar{\mathbf{x}}}(t\!+\!1)\,=\,\alpha\,{\bar{\mathbf{x}}}(t)\,+\,(1\!-\!\alpha)\,\mathbf{x}(t)\,,$$
  id: totrans-2673
  prefs: []
  type: TYPE_NORMAL
  zh: '$${\bar{\mathbf{x}}}(t\!+\!1)\,=\,\alpha\,{\bar{\mathbf{x}}}(t)\,+\,(1\!-\!\alpha)\,\mathbf{x}(t)\,,$$ '
- en: $$(10.11)$$
  id: totrans-2674
  prefs: []
  type: TYPE_NORMAL
  zh: '$$(10.11)$$ '
- en: x¯(t+1) = α x¯(t) + (1−α) x(t), (10.11)
  id: totrans-2675
  prefs: []
  type: TYPE_NORMAL
  zh: 'x¯(t+1) = α x¯(t) + (1−α) x(t), (10.11) '
- en: 'which has the advantage of being *history-free* and causal, *i.e.*, requiring
    neither past nor future values of x for the present update. The free parameter
    α (with 0 ≤ α ≤ 1) determines the time scale of averaging. Its choice is not trivial:
    if it is too small, x¯ will be too noisy; if it is too large, the average will
    lag too far behind the (drifting) signal.'
  id: totrans-2676
  prefs: []
  type: TYPE_NORMAL
  zh: '该方法的优点是*无历史*和因果性，*即*，当前更新不需要 x 的过去或未来值。自由参数 α（0 ≤ α ≤ 1）决定了平均的时间尺度。选择并不简单：如果太小，x¯
    会太嘈杂；如果太大，平均值会滞后于（漂移）信号。 '
- en: Note that the computational cost of centering by this method is proportional
    to the number of nodes in the network. In densely connected networks, this is
    dwarfed by the number of weights, so that the propagation of activities and error
    signals through these weights dominates the computation. The cost of online centering
    will therefore make itself felt in small or sparsely connected networks only.
  id: totrans-2677
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，通过这种方法进行中心化的计算成本与网络中的节点数量成正比。在密集连接的网络中，这一成本被权重数量所掩盖，因此通过这些权重传播活动和误差信号的计算占主导地位。因此，在线中心化的成本只会在小型或稀疏连接的网络中显现。 '
- en: 'Two-Pass Batch. A simple way to implement exact centering in a batch learning
    context is to perform two passes through the training set for each weight update:
    the first to calculate the required averages, the second to compute the resulting
    weight changes. This obviously may increase the computational cost of network
    training by almost a factor of two. For relatively small networks and training
    sets, the activity and error for each node and pattern can be stored during the
    first pass, so that the second pass only consists of the weight update'
  id: totrans-2678
  prefs: []
  type: TYPE_NORMAL
  zh: 双次批次。在批量学习上下文中实现精确中心化的简单方法是对每次权重更新进行两次传递：第一次计算所需的平均值，第二次计算结果权重变化。这显然可能会将网络训练的计算成本增加近一倍。对于相对较小的网络和训练集，可以在第一次传递中存储每个节点和模式的活动和误差信号，因此第二次传递仅由权重更新组成。
- en: (Equation 10.4). Where this is not possible, a feedforward-only first pass (Equation
    10.1) is sufficient to compute average activities and slopes; error centering
    may then be implemented via one of the other methods described here. Previous
    Batch. To avoid the computational overhead of a two-pass method, one can use the
    averages collected over the *previous* batch in the computation of weight changes
    for the current batch. This approximation assumes that the averages involved do
    not change too much from batch to batch; this may result in stability problems
    in conjunction with very high learning rates. Computationally this method is quite
    attractive in that it is cheaper still than the online technique described above.
    When mini-batches are used for training, both approaches can be combined profitably
    by centering with an exponential trace over mini-batch averages. Single-Pass Batch.
    It is possible to perform exact centering in just a *single* pass through a batch
    of training patterns. This is done by expanding the triple product of the fully
    centered batch weight update (cf. Equation 10.7). Using fj as a shorthand for
    fj(yj ), we have
  id: totrans-2679
  prefs: []
  type: TYPE_NORMAL
  zh: '（方程 10.4）。在无法实现这一点的情况下，仅进行前馈的第一次传递（方程 10.1）即可计算平均活动和斜率；然后可以通过这里描述的其他方法之一实现误差中心化。之前的批次。为了避免两次传递方法的计算开销，可以在计算当前批次的权重变化时使用收集到的*之前*批次的平均值。这个近似假设所涉及的平均值在批次之间变化不大；这可能会导致在非常高的学习率下出现稳定性问题。从计算上讲，这种方法相当吸引人，因为它比上述在线技术更便宜。当使用小批次进行训练时，可以通过对小批次平均值进行指数平滑来有利地结合这两种方法。单次批次。只需在训练模式的批次中进行一次*单次*传递就可以实现精确的中心化。这是通过扩展完全中心化批量权重更新的三重乘积来完成的（参见方程
    10.7）。使用 fj 作为 fj(yj) 的简写，我们有 '
- en: "Δwij ∝ \t(xi − xi)(γj − γj )(fj − \tfj )  = \txiγj fj − \txi γj fj − \txi γj\
    \  fj − \txiγj \tfj  + \txi γj  fj + \txi γj \tfj  + \txi γj  \tfj  − \txi γj\
    \  \tfj  = \txiγj fj − xi \tγj fj − γj  \txifj − xiγj  \tfj + 2 xi γj  \tfj  (10.12)"
  id: totrans-2680
  prefs: []
  type: TYPE_NORMAL
  zh: 'Δwij ∝ (xi − xi)(γj − γj)(fj − fj) = xiγj fj − xi γj fj − xi γj fj − xiγj fj
    + xi γj fj + xi γj fj + xi γj fj − xi γj fj = xiγj fj − xi γj fj − γj xi fj −
    xiγj fj + 2 xi γj fj (10.12) '
- en: In addition to the ordinary (uncentered) batch weight update term -xiγjfj
  id: totrans-2681
  prefs: []
  type: TYPE_NORMAL
  zh: '除了普通的（非中心化）批量权重更新项 -xiγjfj '
- en: .
  id: totrans-2682
  prefs: []
  type: TYPE_NORMAL
  zh: '。 '
- en: and the individual averages xi, γj , and -fj
  id: totrans-2683
  prefs: []
  type: TYPE_NORMAL
  zh: '以及各个平均值 xi、γj 和 -fj '
- en: ., the single-pass centered update (10.12) thus also requires collection of
    the sub-products xiγj ,-xifj
  id: totrans-2684
  prefs: []
  type: TYPE_NORMAL
  zh: ., 单次中心更新 (10.12) 因此也需要收集子产品 xiγj ,-xifj。
- en: ., and
  id: totrans-2685
  prefs: []
  type: TYPE_NORMAL
  zh: ., 和
- en: -γjfj
  id: totrans-2686
  prefs: []
  type: TYPE_NORMAL
  zh: -γjfj
- en: .. Due to the extra computation involved, the single-pass batch update is not
    necessarily more efficient than a two-pass method. It is simplified considerably,
    however, when not all factors are involved - for instance, when activities have
    already been centered *a priori* so that xi ≈ 0.
  id: totrans-2687
  prefs: []
  type: TYPE_NORMAL
  zh: .. 由于涉及额外的计算，单次批处理更新并不一定比两次方法更高效。然而，当并非所有因素都涉及时，它会大大简化——例如，当活动已经*先验*中心化，使得 xi
    ≈ 0 时。
- en: Note that the expansion technique shown here may be used to derive an exact
    single-pass batch method for any weight update that involves the addition (or
    subtraction) of some quantity that must be computed from the entire batch of training
    patterns. This includes algorithms such as BCM learning [4, 10] and binary information
    gain optimization [14].
  id: totrans-2688
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里展示的扩展技术可用于推导任何涉及从整个训练模式计算某个数量的确切单次批处理方法。这包括如BCM学习[4, 10]和二元信息增益优化[14]等算法。
- en: 10.4 Empirical Results
  id: totrans-2689
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 实证结果
- en: While activity centering has long been part of backpropagation lore, and empirical
    results for error centering have been reported previously [15], slope centering
    is being proposed for the first time here. It is thus too early to assess its
    general applicability or utility; here we present a number of experiments designed
    to show the typical effect that centering has on speed and reliability of convergence
    as well as generalization performance in feedforward neural networks trained by
    accelerated backpropagation methods.
  id: totrans-2690
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管活动中心化长期以来是反向传播知识的一部分，且之前已有关于误差中心化的实证结果[15]，但坡度中心化在这里首次被提出。因此，现在评估其普遍适用性或效用还为时尚早；在此我们展示了一些旨在表明中心化对速度和收敛的可靠性及在通过加速反向传播方法训练的前馈神经网络中的泛化性能的典型影响的实验。
- en: 'The next section describes the general setup and acceleration techniques used
    in all our experiments. Subsequent sections then present our respective results
    for two well-known benchmarks: the toy problem of symmetry detection in binary
    patterns, and a difficult vowel recognition task.'
  id: totrans-2691
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分描述了我们所有实验中使用的一般设置和加速技术。随后的部分将呈现我们在两个著名基准测试中的各自结果：二元模式中的对称检测玩具问题和一个困难的元音识别任务。
- en: 10.4.1 Setup Of Experiments
  id: totrans-2692
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4.1 实验设置
- en: Benchmark Design. For each benchmark task we performed a number of experiments
    to compare performance with vs. without various forms of centering. Each experiment
    consisted of 100 *runs* starting from different initial weights but identical
    in all other respects. For each run, networks were initialized with random weights
    from a zero-mean Gaussian distribution with standard deviation 0.3. All experiments
    were given the same sequence of random numbers for their 100 weight initializations;
    the seed for this sequence was picked only after the design of the benchmark had
    been finalized.
  id: totrans-2693
  prefs: []
  type: TYPE_NORMAL
  zh: 基准设计。对于每个基准任务，我们进行了多项实验，以比较有无各种中心化形式的性能。每个实验由100次*运行*组成，起始权重不同，但在其他方面完全相同。每次运行中，网络使用来自零均值高斯分布的随机权重初始化，标准差为0.3。所有实验都使用相同的随机数序列进行100次权重初始化；该序列的种子在基准设计最终确定后才选定。
- en: Training Modality. In order to make the results as direct an assessment of centering
    as possible, training was done in batch mode so as to avoid the additional free
    parameters (*e.g.*, smoothing time constants) required by online methods.
  id: totrans-2694
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模式。为了使结果尽可能直接评估中心化，训练采用批处理模式，以避免在线方法所需的额外自由参数（*例如*，平滑时间常数）。
- en: Where not done *a priori*, centering was then implemented with the exact twopass
    batch method. In addition, we always updated the hidden-to-output weights of the
    network *before* backpropagating error through them. This is known to sometimes
    improve convergence behavior [17], and we have found it to increase stability
    at the large step sizes we desire.
  id: totrans-2695
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有*先验*完成，则使用确切的两次批处理方法来实现中心化。此外，我们总是在反向传播误差之前更新网络的隐层到输出层的权重。这被认为有时可以改善收敛行为[17]，我们发现这可以在我们期望的大步长下提高稳定性。
- en: Competitive Controls. The ordinary backpropagation (plain gradient descent)
    algorithm has many known defects, and a large number of acceleration techniques
    has been proposed for it. We informally tested a number of such techniques, then
    picked the combination that achieved the fastest reliable convergence. This combination
    - *vario-*η and *bold driver* - was then used for all experiments reported here.
    Thus any performance advantage for centering reported thereafter has been realized
    *on top of* a state-of-the-art accelerated gradient method as control.
  id: totrans-2696
  prefs: []
  type: TYPE_NORMAL
  zh: 竞争控制。普通的反向传播（简单的梯度下降）算法有许多已知缺陷，提出了大量加速技术。我们非正式地测试了多种技术，然后选择了实现最快可靠收敛的组合。这种组合——*vario-*η和*bold
    driver*——被用于这里报告的所有实验。因此，之后报告的居中性能优势是在一种先进的加速梯度方法之上实现的。
- en: Vario-η [23, page 48]. This interesting technique - also described in Chapter
    17 - sets the local learning rate for each weight inversely proportional to the
    standard deviation of its stochastic gradient. The weight change thus becomes
  id: totrans-2697
  prefs: []
  type: TYPE_NORMAL
  zh: Vario-η [23, page 48]。这种有趣的技术——在第17章中也有描述——将每个权重的局部学习率设置为与其随机梯度的标准差成反比。因此，权重变化变为
- en: $$\Delta w_{ij}\,=\,\frac{-\eta\,g_{ij}}{\varrho+\sigma(g_{ij})}\,,\,\,\,\mbox{where}\,\,\,\,g_{ij}\,\equiv\,\frac{\partial
    E}{\partial w_{ij}}\,\,\,\,\,\mbox{and}\,\,\,\,\sigma(u)\,\equiv\,\sqrt{\left\langle
    u^{2}\right\rangle-\left\langle u\right\rangle^{2}}\,,\tag{10}$$
  id: totrans-2698
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w_{ij}\,=\,\frac{-\eta\,g_{ij}}{\varrho+\sigma(g_{ij})}\,,\,\,\,\mbox{where}\,\,\,\,g_{ij}\,\equiv\,\frac{\partial
    E}{\partial w_{ij}}\,\,\,\,\,\mbox{and}\,\,\,\,\sigma(u)\,\equiv\,\sqrt{\left\langle
    u^{2}\right\rangle-\left\langle u\right\rangle^{2}}\,,\tag{10}$$
- en: $$(10.13)$$
  id: totrans-2699
  prefs: []
  type: TYPE_NORMAL
  zh: $$(10.13)$$
- en: with the small positive constant  preventing division by near-zero values. Vario-η
    can be used in both batch and online modes, and is quite effective in that it
    not only performs gradient normalization, but also adapts step sizes to the level
    of noise in the local gradient signal.
  id: totrans-2700
  prefs: []
  type: TYPE_NORMAL
  zh: 通过小的正常数防止接近零值的除法。Vario-η可以在批处理和在线模式中使用，并且非常有效，它不仅执行梯度归一化，还根据局部梯度信号中的噪声水平调整步长。
- en: 'We used vario-η for all experiments reported here, with  = 0.1. In a batch
    implementation this leaves only one free parameter to be determined: the global
    learning rate η. Bold Driver [11, 21, 2, 3]. This algorithm for adapting the global
    learning rate η is simple and effective, but only works for batch learning. Starting
    from some initial value, η is increased by a certain factor after each batch in
    which the error did not increase by more than a very small constant ε (required
    for numerical stability). Whenever the error rises by more than ε, however, the
    last weight change is undone, and η decreased sharply.'
  id: totrans-2701
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里报告的所有实验中使用了vario-η，= 0.1。在批处理实现中，这仅留下一个待确定的自由参数：全局学习率η。Bold Driver [11,
    21, 2, 3]。该算法用于调整全局学习率η，简单而有效，但仅适用于批学习。从某个初始值开始，η在每个误差没有超过非常小的常数ε（为数值稳定性所需）的批次后增加一定因子。然而，若误差超过ε，则撤销上一次权重变化，并迅速降低η。
- en: All experiments reported here were performed using bold driver with a learning
    rate increment of 2%, a decrement of 50%, and ε= 10−10. These values were found
    to provide fast, reliable convergence across all experiments. Due to the amount
    of recomputation they require, we do count the "failed" epochs (whose weight changes
    are subsequently undone) in our performance figures.
  id: totrans-2702
  prefs: []
  type: TYPE_NORMAL
  zh: 这里报告的所有实验均使用带有2%学习率增量、50%减量和ε= 10−10的bold driver进行。这些值被发现能够在所有实验中提供快速、可靠的收敛。由于它们所需的重新计算量，我们在性能数据中计算“失败”的纪元（其权重变化随后被撤销）。
- en: 10.4.2 Symmetry Detection Problem
  id: totrans-2703
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4.2 对称检测问题
- en: 'In our first benchmark, a fully connected feedforward network with 8 inputs,
    8 hidden units and a single output is to learn the symmetry detection task: given
    an 8-bit binary pattern at the input, it is to signal at the output whether the
    pattern is symmetric about its middle axis (target = 1) or not (target = 0). This
    is admittedly a toy problem, although not a trivial one.'
  id: totrans-2704
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一次基准测试中，一个完全连接的前馈网络有8个输入、8个隐藏单元和一个输出，旨在学习对称检测任务：给定输入的8位二进制模式，输出信号指示模式是否在其中轴对称（目标
    = 1）或不对称（目标 = 0）。这虽然是一个玩具问题，但并非简单。
- en: Since the target is binary, we used a logistic output unit and cross-entropy
    loss function. For each run the network was trained on all 256 possible patterns
    until the root-mean-square error of its output over the batch fell below 0.01.
    We recorded the number of epochs required to reach this criterion, but did not
    test for generalization ability on this task. Error and Activity Centering. In
    our first set of experiments we examined the separate and combined effect of centering
    the network's activity and/or error signals. For convenience, activity centering
    was performed *a priori* by using -1 and 1 as input levels, and the hyperbolic
    tangent (tanh) as activation function for hidden units. The off-center control
    experiments were done with 0 and 1 as input levels and the logistic activation
    function f(y)=1/(1 + e−y). Note that all differences between the tanh and logistic
    nonlinearities are eliminated by the vario-η algorithm, *except* for the eccentricity
    of their respective outputs.
  id: totrans-2705
  prefs: []
  type: TYPE_NORMAL
  zh: 由于目标是二元的，我们使用了逻辑输出单元和交叉熵损失函数。每次运行时，网络在所有256种可能的模式上进行训练，直到其批次输出的均方根误差降到0.01以下。我们记录了达到此标准所需的纪元数，但没有测试该任务的泛化能力。错误和活动居中。在我们的第一组实验中，我们考察了居中网络活动和/或错误信号的单独和组合效果。为了方便，活动居中通过将-1和1作为输入水平，并使用双曲正切（tanh）作为隐藏单元的激活函数进行*a
    priori*处理。偏离中心的对照实验则使用0和1作为输入水平，以及逻辑激活函数f(y)=1/(1 + e−y)。请注意，通过vario-η算法消除了tanh和逻辑非线性之间的所有差异，*除了*各自输出的离心率。
- en: 'Results. Table 10.1 shows that centering either activity or error signals produced
    an approximate 7-fold increase in convergence speed. In no instance was a run
    that used one (or both) of these centering methods slower than the corresponding
    control without centering. The similar magnitude of the speed-up suggests that
    it may be due to the improved conditioning of the Hessian achieved by centering
    either errors or activities (see Section 10.5). Note, however, that activity centering
    beat error centering almost 2/3 of the time in the direct comparison. On the other
    hand, error centering appeared to improve the *reliability* of convergence: it
    cut the convergence time''s coefficient of variation (the ratio between its standard
    deviation and mean, henceforth: c.v.) in half while activity centering Table 10.1.
    The effect of centering activities and/or error signals on the symmetry detection
    task without shortcuts. Reported are the empirical mean, standard deviation, and
    25th/50th/75th percentile (rounded to three significant digits) of the number
    of epochs required to converge to criterion. Also shown is the result of directly
    comparing runs with identical random seeds. The number of runs in each comparison
    may sum to less than 100 due to ties.'
  id: totrans-2706
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。表10.1显示，居中活动或错误信号均能将收敛速度提高约7倍。在任何情况下，使用一种（或两种）居中方法的运行速度都没有慢于不进行居中的对应对照组。速度提升的相似幅度表明，这可能是由于居中错误或活动所改善的Hessian条件（见第10.5节）。然而，请注意，在直接比较中，活动居中几乎在2/3的时间里优于错误居中。另一方面，错误居中似乎提高了收敛的*可靠性*：它将收敛时间的变异系数（标准差与均值的比率，以下称为c.v.）减半，而活动居中则未能做到这一点。表10.1。居中活动和/或错误信号对对称性检测任务的影响，没有捷径。报告的是达到标准所需的纪元数的经验均值、标准差，以及第25/50/75百分位（四舍五入到三个有效数字）。还显示了对具有相同随机种子的运行进行直接比较的结果。由于并列情况，每次比较中的运行次数可能总和小于100。
- en: '| error signals:   | conventional       | centered         |           |          |    |             |'
  id: totrans-2707
  prefs: []
  type: TYPE_TB
  zh: '| 错误信号：   | 常规              | 居中            |           |          |    |             |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-2708
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| mean ± st.d.     | direct comparison: | mean ± st.d.     |           |          |    |             |'
  id: totrans-2709
  prefs: []
  type: TYPE_TB
  zh: '| 均值 ± 标准差     | 直接比较：      | 均值 ± 标准差     |           |          |    |             |'
- en: '| activities:      | quartiles          | # of faster runs | quartiles |          |    |             |'
  id: totrans-2710
  prefs: []
  type: TYPE_TB
  zh: '| 活动：         | 四分位数          | 更快运行的次数   | 四分位数 |          |    |             |'
- en: '| 669 ± 308        | 0 - 100            | 97.5 ± 21.8      |           |          |    |             |'
  id: totrans-2711
  prefs: []
  type: TYPE_TB
  zh: '| 669 ± 308        | 0 - 100            | 97.5 ± 21.8      |           |          |    |             |'
- en: '| off-center (0/1) | 453/580/852        | 0                | 0         | 35       |
    7  | 82/95.5/109 |'
  id: totrans-2712
  prefs: []
  type: TYPE_TB
  zh: '| 偏离中心 (0/1) | 453/580/852        | 0                | 0         | 35       |
    7  | 82/95.5/109 |'
- en: '|                  | |                  |                  |           |          |    |             |'
  id: totrans-2713
  prefs: []
  type: TYPE_TB
  zh: '|                  | |                  |                  |           |          |    |             |'
- en: '|                  | |                  | 63 × 100         | 93        |          |    |             |'
  id: totrans-2714
  prefs: []
  type: TYPE_TB
  zh: '|                  | |                  | 63 × 100         | 93        |          |    |             |'
- en: '| 93.1 ± 46.7      | 100                | 65.4 ± 15.9      |           |          |    |             |'
  id: totrans-2715
  prefs: []
  type: TYPE_TB
  zh: '| 93.1 ± 46.7      | 100                | 65.4 ± 15.9      |           |          |    |             |'
- en: '| centered (-1/1)  | 67.5/79.5/94       | 14 -             | 84        | 57/62/70
    |    |             |'
  id: totrans-2716
  prefs: []
  type: TYPE_TB
  zh: '| 中心化 (-1/1)  | 67.5/79.5/94       | 14 -             | 84        | 57/62/70
    |    |             |'
- en: left it unchanged. We speculate that this may be the beneficial effect of centering
    on the *backpropagated* error, which does not occur for activity centering.
  id: totrans-2717
  prefs: []
  type: TYPE_NORMAL
  zh: 保持不变。我们推测这可能是中心化对*反向传播*误差的有益影响，而这种影响在活动中心化时并不存在。
- en: Finally, a further speedup of 50% (while maintaining the lower c.v.) occurred
    when both activity and error signals were centered. This may be attributed to
    the fact that our centering of hidden unit activity by design (cf. Section 10.3)
    was only approximate. To assess the significance of these effects, note that since
    the data was collected over 100 runs, the standard error of the reported mean
    time to convergence is 1/
  id: totrans-2718
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当活动和误差信号都被中心化时，进一步加速了50%（同时保持较低的变异系数）。这可能归因于我们设计上对隐藏单元活动的中心化（参见第10.3节）只是近似的。为了评估这些效应的显著性，请注意，由于数据是在100次运行中收集的，因此报告的收敛时间的标准误差为1/
- en: √100 = 1/10 its reported standard deviation.
  id: totrans-2719
  prefs: []
  type: TYPE_NORMAL
  zh: √100 = 1/10 其报告的标准差。
- en: Shortcuts and Slope Centering. In the second set of experiments we left both
    activity and error signals centered, and examined the separate and combined effect
    of adding shortcuts and/or slope centering. Note that since the complement of
    a symmetric bit pattern is also symmetric, the symmetry detection task has no
    linear component at all - we would therefore expect shortcuts to be of minimal
    benefit here.
  id: totrans-2720
  prefs: []
  type: TYPE_NORMAL
  zh: 短路径和斜率中心化。在第二组实验中，我们保持活动和误差信号均衡，并研究了添加短路径和/或斜率中心化的单独及组合效果。请注意，由于对称比特模式的补集也是对称的，因此对称性检测任务根本没有线性成分——因此我们预期短路径在这里的好处最小。
- en: 'Results. Table 10.2 shows that indeed adding shortcuts alone was not beneficial
    - in fact it slowed down convergence in over 80% of the cases, and significantly
    increased the c.v. Subsequent addition of slope centering, however, brought about
    an almost 3-fold increase in learning speed, and restored the original c.v. of
    about 1/4. When used together, slope centering and shortcuts never increased convergence
    time, and on average cut it in half. By contrast, slope centering without shortcuts
    failed to converge at all about 1/3 of the time. This may come as a surprise,
    considering that the given task had no linear component. However, consider the
    following:'
  id: totrans-2721
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。表10.2显示，单独添加短路径并没有带来好处——实际上在超过80%的情况下，它减慢了收敛速度，并显著增加了变异系数。然而，后续添加的斜率中心化几乎使学习速度提高了3倍，并恢复了大约1/4的原始变异系数。当同时使用时，斜率中心化和短路径从未增加收敛时间，平均缩短了一半。相比之下，没有短路径的斜率中心化在大约1/3的时间里完全没有收敛。这可能令人惊讶，因为所给的任务没有线性成分。然而，请考虑以下几点：
- en: Table 10.2. The effect of centering slopes and/or adding shortcuts on the symmetry
    detection task with centered activity and error signals. Results are shown in
    the same manner as in Table 10.1.
  id: totrans-2722
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.2。斜率中心化和/或添加短路径对对称性检测任务的影响，其中活动和误差信号被中心化。结果以与表10.1相同的方式显示。
- en: '| slopes:      | conventional       | centered         |            |      |             |    |           |'
  id: totrans-2723
  prefs: []
  type: TYPE_TB
  zh: '| 斜率：       | 常规               | 中心化           |            |      |             |    |           |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-2724
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| mean ± st.d. | direct comparison: | mean ± st.d.     |            |      |             |    |           |'
  id: totrans-2725
  prefs: []
  type: TYPE_TB
  zh: '| 平均 ± 标准差 | 直接比较：         | 平均 ± 标准差   |            |      |             |    |           |'
- en: '| topology:    | quartiles          | # of faster runs | quartiles  |      |             |    |           |'
  id: totrans-2726
  prefs: []
  type: TYPE_TB
  zh: '| 拓扑：       | 四分位数           | 更快运行的数量   | 四分位数    |      |             |    |           |'
- en: '| 65.4 ± 15.9  | 52                 | -                | 48         | *    |
    51.6 ± 16.2 |    |           |'
  id: totrans-2727
  prefs: []
  type: TYPE_TB
  zh: '| 65.4 ± 15.9  | 52                 | -                | 48         | *    |
    51.6 ± 16.2 |    |           |'
- en: '| short-       | no                 | 57/62/70         | 81         | 0    |
    61          | 4  | 43/64.5/∞ |'
  id: totrans-2728
  prefs: []
  type: TYPE_TB
  zh: '| 短的         | 无                  | 57/62/70         | 81         | 0    |
    61          | 4  | 43/64.5/∞ |'
- en: '|              | |                  | 39 ×             | 99         | 95 |
    |             |    |           |'
  id: totrans-2729
  prefs: []
  type: TYPE_TB
  zh: '|              | |                  | 39 ×             | 99         | 95 |
    |             |    |           |'
- en: '| cuts?        | 90.4 ± 31.1        | 17               | 33.1 ± 8.6 |      |             |    |           |'
  id: totrans-2730
  prefs: []
  type: TYPE_TB
  zh: '| 切分？       | 90.4 ± 31.1        | 17               | 33.1 ± 8.6 |      |             |    |           |'
- en: '| yes          | 69.5/80/102        | 0 - 100          | 28/31/35   |      |             |    |           |'
  id: totrans-2731
  prefs: []
  type: TYPE_TB
  zh: '| 是           | 69.5/80/102        | 0 - 100          | 28/31/35   |      |             |    |           |'
- en: '* Mean and standard deviation exclude 34 runs which did not converge.'
  id: totrans-2732
  prefs: []
  type: TYPE_NORMAL
  zh: '* 均值和标准差不包括34次未收敛的运行。'
- en: 'Need for Shortcuts. Due to the monotonicity of their nonlinear transfer function,
    hidden units always carry some linear moment, in the sense of a positive correlation
    between their net input and output. In the absence of shortcuts, the hidden units
    must arrange themselves so that their linear moments together match the overall
    linear component of the task (here: zero). This adaptation process is normally
    driven by the linear component of the error - which slope centering removes.'
  id: totrans-2733
  prefs: []
  type: TYPE_NORMAL
  zh: 对捷径的需求。由于其非线性传递函数的单调性，隐藏单元总是携带一定的线性动量，即它们的净输入与输出之间存在正相关。在没有捷径的情况下，隐藏单元必须排列自身，使得它们的线性动量整体上与任务的线性分量（此处为零）相匹配。这个适应过程通常由误差的线性分量驱动，而坡度中心化则消除了这一点。
- en: 'The remaining nonlinear error signals can still jostle the hidden units into
    an overall solution, but such an indirect process is bound to be unreliable: as
    it literally removes slope from the error surface, slope centering creates numerous
    local minima. Shortcut weights turn these local minima into global ones by modeling
    the missing (linear) component of the gradient, thereby freeing the hidden units
    from any responsibility to do so.'
  id: totrans-2734
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的非线性误差信号仍然可以促使隐藏单元达到整体解决方案，但这种间接过程必然不可靠：因为它从误差面上字面上移除了坡度，坡度中心化会产生大量局部极小值。捷径权重通过对缺失的（线性）梯度分量建模，将这些局部极小值转变为全局极小值，从而解放了隐藏单元的责任。
- en: In summary, while a network without shortcuts trained with slope centering may
    converge to a solution, the addition of shortcut weights is necessary to ensure
    that slope centering will not be detrimental to the learning process. Conversely,
    slope centering can prevent shortcuts from acting as redundant "detractors" that
    impede learning instead of assisting it. These two techniques should therefore
    always be used in conjunction.
  id: totrans-2735
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，虽然没有捷径的网络经过坡度中心化训练后可能收敛到一个解决方案，但添加捷径权重是必要的，以确保坡度中心化不会对学习过程产生不利影响。相反，坡度中心化可以防止捷径充当冗余的“干扰者”，阻碍学习而不是促进学习。因此，这两种技术应始终结合使用。
- en: 10.4.3 Vowel Recognition Problem
  id: totrans-2736
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4.3 元音识别问题
- en: 'Our positive experiences with centering on the symmetry detection task immediately
    raise two further questions: 1) will these results transfer to more challenging,
    realistic problems, and 2) is the gain in learning speed - as often happens -
    bought at the expense of generalization ability? In order to address these questions,
    we conducted further experiments with the speaker-independent vowel recognition
    data due to Deterding [5], a popular benchmark for which good generalization performance
    is rather difficult to achieve. The Task. The network''s task is to recognize
    the eleven steady-state vowels of British English in a speaker-independent fashion,
    given 10 spectral features (specifically: LPC-derived log area ratios) of the
    speech signal. The data consists of 990 patterns to be classified: 6 instances
    for each of the 11 vowels spoken by each of 15 speakers. We follow the convention
    of splitting it into a training set containing the data from the first 8 (4 male,
    4 female) speakers, and a test set containing those of the remaining 7 (4 male,
    3 female). Note that there is no separate validation set available.'
  id: totrans-2737
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在对称检测任务中的积极经验立即引发了两个进一步的问题：1）这些结果能否转移到更具挑战性的现实问题上，2）学习速度的提升——如常发生的那样——是否以泛化能力为代价？为了回答这些问题，我们进行了进一步的实验，使用了Deterding提供的说话者独立元音识别数据[5]，这是一个较为流行的基准，其良好的泛化性能相当难以实现。任务。网络的任务是以说话者独立的方式识别英国英语的十一种稳态元音，给定语音信号的10个谱特征（具体为：LPC导出的对数面积比）。数据由990个模式组成：每个发言者说的11个元音各有6个实例，发言者共有15位。我们遵循将数据划分为训练集和测试集的惯例，训练集中包含前8位（4位男性，4位女性）发言者的数据，测试集中包含其余7位（4位男性，3位女性）的数据。请注意，没有单独的验证集可用。
- en: Prior Work. Robinson [13] pioneered the use of Deterding's data as a benchmark
    by comparing the performance of a number of neural network architectures on it.
    Interestingly, none of his methods could outperform the primitive single nearest
    neighbor approach (which misclassifies 44% of test patterns), thus posing a challenge
    to the pattern recognition community. Trained on the task as formulated above,
    conventional backpropagation networks in fact appear to reach their limits at
    error rates of around 42% [6, 9], while an adaptive nearest neighbor technique
    can achieve 38% [7]. In Chapter 7, Flake reports comparably favorable results
    for RBF networks as well as his own hybrid architectures. Even better performance
    can be obtained by using speaker sex/identity information [19, 20],
  id: totrans-2738
  prefs: []
  type: TYPE_NORMAL
  zh: 先前工作。Robinson [13] 首先使用 Deterding 的数据作为基准，通过比较多种神经网络架构在其上的表现。有趣的是，他的方法没有一个能够超越原始的单一最近邻方法（其误分类率为
    44%），这对模式识别社区提出了挑战。在上述任务的训练中，传统的反向传播网络实际上在错误率约为 42%[6, 9] 时似乎达到了极限，而自适应最近邻技术可以达到
    38%[7]。在第 7 章中，Flake 报告了 RBF 网络以及他自己混合架构的比较有利的结果。使用说话者性别/身份信息[19, 20]，可以获得更好的性能。
- en: or by training a separate model for each vowel [8]. By combining these two approaches,
    a test set error of 23% has been reached [18], the lowest we are aware of to date.
    Training and Testing. We trained fully connected feedforward networks with 10
    inputs, 22 hidden units, and 11 logistic output units by minimization of crossentropy
    loss. The target was 1 for the output corresponding to the correct vowel, 0 for
    all others. Activity centering was done *a priori* by explicitly centering the
    inputs (separately for training and test set), and by using the tanh nonlinearity
    for hidden units. The uncentered control experiments used the original input data,
    and logistic activation functions.
  id: totrans-2739
  prefs: []
  type: TYPE_NORMAL
  zh: 或通过为每个元音训练一个单独的模型[8]。通过结合这两种方法，测试集错误率达到了 23%[18]，这是我们目前所知的最低值。训练和测试。我们通过最小化交叉熵损失，训练了一个具有
    10 个输入、22 个隐藏单元和 11 个逻辑输出单元的全连接前馈网络。正确元音对应的输出目标为 1，其他所有输出目标为 0。活动中心化是通过显式中心化输入（分别针对训练和测试集）以及使用
    tanh 非线性函数进行的。未中心化的对照实验使用原始输入数据和逻辑激活函数。
- en: 'The relatively small size of our networks enabled us to run all experiments
    out to 2 000 epochs of training. After each epoch, the network''s generalization
    ability was measured in terms of its misclassification rate on the test set. For
    the purpose of testing, a maximum likelihood approach was adopted: the network''s
    highest output for a given test pattern was taken to indicate its classification
    of that pattern.'
  id: totrans-2740
  prefs: []
  type: TYPE_NORMAL
  zh: 我们网络的相对较小的规模使我们能够将所有实验运行到 2000 个训练周期。每个周期后，网络的泛化能力通过其在测试集上的误分类率进行测量。为了测试，采用了最大似然的方法：网络对给定测试模式的最高输出被视为其对该模式的分类。
- en: 'First Results. Figure 10.1 shows how the average test set error (over 100 runs)
    evolved during training in each of the 8 experiments we performed for this benchmark.
    For all curves, error bars were at most the size of the marks shown along the
    curve, and have therefore been omitted for clarity. Following our experience on
    the symmetry detection task, shortcuts and slope centering were always used in
    conjunction whereas activity and error centering were examined independently.
    The following effects can be discerned:'
  id: totrans-2741
  prefs: []
  type: TYPE_NORMAL
  zh: 首次结果。图 10.1 显示了在我们为该基准测试进行的 8 次实验中，平均测试集错误率（经过 100 次运行）在训练期间的演变。所有曲线的误差条的大小至多为曲线上所示标记的大小，因此为了清晰起见已省略。根据我们在对称检测任务中的经验，快捷方式和斜率中心化始终是结合使用的，而活动和错误中心化则是独立进行的。可以辨别出以下效果：
- en: '![216_image_0.png](216_image_0.png)'
  id: totrans-2742
  prefs: []
  type: TYPE_IMG
  zh: '![216_image_0.png](216_image_0.png)'
- en: Fig. 10.1. Evolution of the average test set error while learning the vowel
    recognition task with activity centering (triangular marks), error centering (filled
    marks), and/or slope centering with shortcut weights (solid lines), vs. their
    uncentered controls. Experiments are denoted a)–h) as in Table 10.3.
  id: totrans-2743
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1. 在学习元音识别任务时，平均测试集错误率的演变，采用活动中心化（三角标记）、错误中心化（填充标记）和/或带有快捷权重的斜率中心化（实线），与未中心化的对照组相比。实验标记为
    a)–h)，如表 10.3 所示。
- en: 1. All experiments with activity centering (triangular marks) clearly outperformed
    all experiments without it (circular marks) in both average convergence speed
    and minimum average test set error.
  id: totrans-2744
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 所有采用活动中心化的实验（三角标记）在平均收敛速度和最小平均测试集错误率上明显优于所有不采用活动中心化的实验（圆形标记）。
- en: 2. All experiments with shortcuts and slope centering (solid lines) outperformed
    the corresponding experiment without them (dashed lines).
  id: totrans-2745
  prefs: []
  type: TYPE_NORMAL
  zh: 所有带有快捷方式和斜率中心化（实线）的实验均优于没有这些方法的对应实验（虚线）。
- en: 3. With one notable exception (experiment d), error centering (filled marks)
  id: totrans-2746
  prefs: []
  type: TYPE_NORMAL
  zh: 除了一个显著的例外（实验d），误差中心化（填充标记）
- en: sped up convergence significantly. Its effect was greatest in the experiments
    without activity centering.
  id: totrans-2747
  prefs: []
  type: TYPE_NORMAL
  zh: 显著加快了收敛速度。在没有活动中心化的实验中，其效果最为显著。
- en: 4. The best experiment in terms of both convergence speed and minimum average
    test set error was e), the fully centered one; the worst was a), the fully conventional
    one.
  id: totrans-2748
  prefs: []
  type: TYPE_NORMAL
  zh: 在收敛速度和最小平均测试集误差方面，表现最佳的实验是e)，完全中心化的实验；表现最差的是a)，完全传统的实验。
- en: The qualitative picture that emerges is that centering appears to significantly
    speed up convergence without adversely affecting the trained network's generalization
    ability. We will now attempt to quantify this finding.
  id: totrans-2749
  prefs: []
  type: TYPE_NORMAL
  zh: 显现出的定性图景是，中心化显著加快了收敛速度，而不对训练网络的泛化能力产生不利影响。我们现在将尝试量化这一发现。
- en: 'Quantifying the Effect. Since the curves in Figure 10.1 are in fact superpositions
    of 100 nonlinear curves each, they are ill-suited to quantitative analysis:'
  id: totrans-2750
  prefs: []
  type: TYPE_NORMAL
  zh: 量化效果。由于图10.1中的曲线实际上是100条非线性曲线的叠加，它们不适合进行定量分析：
- en: value and location of the minimum average test set error do not tell us anything
    about the distribution of such minima across individual runs - not even their
    average value or location. In order to obtain such quantitative results, we need
    to identify an appropriate minimum in test set error for each run. This will allow
    us to directly compare runs with identical initial weights across experiments,
    as well as to characterize the distribution of minima within each experiment by
    aggregate statistics (*e.g.*, mean, standard deviation, quartiles) for both the
    minimum test set error, and the time taken to reach it.
  id: totrans-2751
  prefs: []
  type: TYPE_NORMAL
  zh: 最小平均测试集误差的值和位置并不能告诉我们关于这些最小值在各个运行中的分布的信息——甚至连它们的平均值或位置也无法得知。为了获得这样的定量结果，我们需要为每次运行确定一个合适的测试集误差最小值。这将使我们能够在实验中直接比较具有相同初始权重的运行，并通过汇总统计（*例如*，均值、标准差、四分位数）来表征每个实验内最小值的分布，包括最小测试集误差和达到该误差所需的时间。
- en: A fair and consistent strategy to identify minima suitable for the quantitative
    comparisons we have in mind is not trivial to design. Individual runs may have
    multiple minima in test set error, or none at all. If we were to just use the
    global minimum over the duration of the run (2 000 epochs), we would not be able
    to distinguish a fast method which makes some insignificant improvement to a long-found
    minimum late in the run from a slow method which takes that long to reach its
    first minimum. Given that we are concerned with both the quality of generalization
    performance and the speed with which it is achieved, a greedy strategy for picking
    appropriate minima is indicated.
  id: totrans-2752
  prefs: []
  type: TYPE_NORMAL
  zh: 设计一个合理且一致的策略来识别适合我们想要的定量比较的最小值并非易事。单个运行可能在测试集误差中有多个最小值，或者根本没有。如果我们仅使用运行期间的全局最小值（2000个周期），那么我们将无法区分一个在运行后期对一个早已找到的最小值进行微小改善的快速方法与一个花费较长时间达到其第一个最小值的慢速方法。鉴于我们关心的是泛化性能的质量以及实现的速度，因此选择合适最小值的贪婪策略是有必要的。
- en: 'Identification of Minima. We follow the evolution of test set error over the
    course of each run, noting new minima as we encounter them. If the best value
    found so far is not improved upon within a certain period of time, we pick it
    as the minimum of that run for the purpose of quantitative analysis. The appropriate
    length of waiting period before giving up on further improvement is a difficult
    issue - see Chapter 2 for a discussion. For a fair comparison between faster and
    slower optimization methods, it should be proportional to the time it took to
    reach the minimum in question: a slow run then has correspondingly more time to
    improve its solution than a fast one.'
  id: totrans-2753
  prefs: []
  type: TYPE_NORMAL
  zh: 最小值的识别。我们跟踪每次运行过程中测试集误差的演变，记录我们遇到的新最小值。如果迄今为止找到的最佳值在一定时间内没有得到改善，我们将其选为该运行的最小值，以便进行定量分析。放弃进一步改进前的适当等待时间是一个棘手的问题——有关讨论见第2章。为了公平比较快速和慢速优化方法，这个时间应该与达到相关最小值所需的时间成正比：慢速运行因此有更多时间改善其解决方案。
- en: 'Unfortunately this approach fails if a minimum of test set error occurs during
    the initial transient, within the first few epochs of training: the waiting period
    would then be too short, causing us to give up prematurely. On the other hand,
    we cannot wait longer than the overall duration of the run. We therefore stop
    looking for further improvement in a run after min(2 000, 2+100) epochs, where
    records when the network first achieved the lowest test set error seen so far
    in that run. Only 9 out of the 800 runs reported here expired at the upper limit
    of 2 000 epochs, so we are confident that its imposition did not significantly
    skew our results.'
  id: totrans-2754
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，如果在训练的最初几个轮次内出现最小测试集错误，该方法会失败：等待时间可能过短，导致我们过早放弃。另一方面，我们无法等待超过整体运行的持续时间。因此，我们在达到min(2
    000, 2+100)轮次后停止寻找进一步的改进，此时记录网络首次达到的当前最低测试集错误。这里报告的800次运行中，只有9次达到了2 000轮次的上限，因此我们相信这一限制并没有显著扭曲我们的结果。
- en: 'Test Set Used as Validation Set. Note that since we use the test set to determine
    at which point to compare performance, we have effectively appropriated it as
    a validation set. The minimum test set errors reported below are therefore not
    unbiased estimators for the network''s ability to generalize to novel speakers,
    and should not be compared to proper measurements of this ability (for which the
    test set must not affect the training procedure in any way). Nonetheless, let
    us not forget that the lowest test set error does measure the network''s generalization
    ability in a consistent fashion after all: even though these scores are all biased
    to favor a particular set of novel speakers (the test set), by no means does this
    render their comparison *against each other* insignificant. Overview of Results.
    Table 10.3 summarizes quantitative results obtained in this fashion for the vowel
    recognition problem. To assess their significance, recall Table 10.3. Minimum
    test set error (misclassification rate in %), and the number of epochs required
    to reach it, for the vowel recognition task. Except for the different layout,
    results are reported in the same manner as in Tables 10.1 and 10.2. Due to space
    limitations, only selected pairs of experiments are compared directly.'
  id: totrans-2755
  prefs: []
  type: TYPE_NORMAL
  zh: 使用测试集作为验证集。请注意，由于我们使用测试集来确定何时比较性能，因此实际上将其视为验证集。以下报告的最小测试集错误因此并不是网络对新说话者泛化能力的无偏估计，且不应与这种能力的正确测量进行比较（测试集在任何情况下都不得影响训练过程）。尽管如此，我们不应忘记，最低测试集错误确实以一致的方式衡量了网络的泛化能力：尽管这些分数都倾向于
    favor 特定的新说话者集合（测试集），但这绝不会使它们之间的比较显得*微不足道*。结果概述。表10.3总结了针对元音识别问题以这种方式获得的定量结果。为了评估其重要性，请回顾表10.3。元音识别任务的最小测试集错误（错误分类率
    %）以及达到该错误所需的轮次。除不同的布局外，结果的报告方式与表10.1和10.2相同。由于空间限制，仅直接比较了选定的实验对。
- en: '| features:    | performance            | measure:        |                  |                |                |                  |             |                  |              |    |    |    |'
  id: totrans-2756
  prefs: []
  type: TYPE_TB
  zh: '| 特征：      | 性能                  | 测量：          |                  |                |                |                  |             |                  |              |    |    |    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-2757
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| centering    | minimum test set error | epochs required |                  |                |                |                  |             |                  |              |    |    |    |'
  id: totrans-2758
  prefs: []
  type: TYPE_TB
  zh: '| 居中        | 最小测试集错误         | 所需轮次        |                  |                |                |                  |             |                  |              |    |    |    |'
- en: '| experiment   | activ.                 | error           | slope            |
    shortcuts      | quartiles      | # of better runs | quartiles   | # of faster
    runs |              |    |    |    |'
  id: totrans-2759
  prefs: []
  type: TYPE_TB
  zh: '| 实验        | 激活                   | 错误            | 斜率            | 简化           |
    四分位数       | 更好运行的数量    | 四分位数     | 更快运行的数量    |              |    |    |    |'
- en: '| mean ± st.d. | dir. comparison:       | mean ± st.d.    | dir. comparison:
    |                |                |                  |             |                  |              |    |    |    |'
  id: totrans-2760
  prefs: []
  type: TYPE_TB
  zh: '| 平均 ± 标准差 | 方向比较：           | 平均 ± 标准差   | 方向比较：       |                |                |                  |             |                  |              |    |    |    |'
- en: '| 48.0 ± 3.6   | 554 ± 321              |                 |                  |                |                |                  |             |                  |              |    |    |    |'
  id: totrans-2761
  prefs: []
  type: TYPE_TB
  zh: '| 48.0 ± 3.6   | 554 ± 321              |                 |                  |                |                |                  |             |                  |              |    |    |    |'
- en: '| a)           | 45.7/47.3/50.0         | 67              | 17               |
    13             | 19             | 37               | 365/486/691 | 3                |
    10           | 4  | 1  | 14 |'
  id: totrans-2762
  prefs: []
  type: TYPE_TB
  zh: '| a)           | 45.7/47.3/50.0         | 67              | 17               |
    13             | 19             | 37               | 365/486/691 | 3                |
    10           | 4  | 1  | 14 |'
- en: '|              | |                      |                 |                  |                |                |                  |             |                  |              |    |    |    |'
  id: totrans-2763
  prefs: []
  type: TYPE_TB
  zh: '|              | |                      |                 |                  |                |                |                  |             |                  |              |    |    |    |'
- en: '| 49.1 ± 2.9   | 31 |                   | 125 ± 82        | 97               |                |                |                  |             |                  |              |    |    |    |'
  id: totrans-2764
  prefs: []
  type: TYPE_TB
  zh: '| 49.1 ± 2.9   | 31 |                   | 125 ± 82        | 97               |                |                |                  |             |                  |              |    |    |    |'
- en: '| b)           | √                      | 47.0/49.6/50.9  | 10               |
    67.5/104/163   | 51             |                  |             |                  |              |    |    |    |'
  id: totrans-2765
  prefs: []
  type: TYPE_TB
  zh: '| b)           | √                      | 47.0/49.6/50.9  | 10               |
    67.5/104/163   | 51             |                  |             |                  |              |    |    |    |'
- en: '| 43.9 ± 2.5   | 82                     | 156 ± 110       | 90               |                |                |                  |             |                  |              |    |    |    |'
  id: totrans-2766
  prefs: []
  type: TYPE_TB
  zh: '| 43.9 ± 2.5   | 82                     | 156 ± 110       | 90               |                |                |                  |             |                  |              |    |    |    |'
- en: '| c)           | √                      | 42.3/43.9/46.0  | 51               |
    75/137/215     | 47             |                  |             |                  |              |    |    |    |'
  id: totrans-2767
  prefs: []
  type: TYPE_TB
  zh: '| c)           | √                      | 42.3/43.9/46.0  | 51               |
    75/137/215     | 47             |                  |             |                  |              |    |    |    |'
- en: '| |            | |                      |                 |                  |                |                |                  |             |                  |              |    |    |    |'
  id: totrans-2768
  prefs: []
  type: TYPE_TB
  zh: '| |            | |                      |                 |                  |                |                |                  |             |                  |              |    |    |    |'
- en: '| 44.3 ± 2.3   | 89                     | 46              | 84               |
    158 ± 141      | 48             | 52               | 96          |                  |              |    |    |    |'
  id: totrans-2769
  prefs: []
  type: TYPE_TB
  zh: '| 44.3 ± 2.3   | 89                     | 46              | 84               |
    158 ± 141      | 48             | 52               | 96          |                  |              |    |    |    |'
- en: '| d)           | √                      | √               | 42.9/44.2/45.9   |
    49             | 65             | 72/124/186       | 21          | 85               |              |    |    |    |'
  id: totrans-2770
  prefs: []
  type: TYPE_TB
  zh: '| d)           | √                      | √               | 42.9/44.2/45.9   |
    49             | 65             | 72/124/186       | 21          | 85               |              |    |    |    |'
- en: '| |            | |                      |                 |                  |                |                |                  |             |                  |              |    |    |    |'
  id: totrans-2771
  prefs: []
  type: TYPE_TB
  zh: '| |            | |                      |                 |                  |                |                |                  |             |                  |              |    |    |    |'
- en: '| 44.2 ± 2.5   | 49                     | 80              | 72.4 ± 55.5      |
    78             | 99             |                  |             |                  |              |    |    |    |'
  id: totrans-2772
  prefs: []
  type: TYPE_TB
  zh: '| 44.2 ± 2.5   | 49                     | 80              | 72.4 ± 55.5      |
    78             | 99             |                  |             |                  |              |    |    |    |'
- en: '| e)           | √                      | √               | √                |
    √              | 42.3/44.4/46.3 | 70               | 47          | 68               |
    37.5/51.5/81 | 76 | 75 | 92 |'
  id: totrans-2773
  prefs: []
  type: TYPE_TB
  zh: '| e)           | √                      | √               | √                |
    √              | 42.3/44.4/46.3 | 70               | 47          | 68               |
    37.5/51.5/81 | 76 | 75 | 92 |'
- en: '| |            | 113 ± 64               | 24 |            |                  |                |                |                  |             |                  |              |    |    |    |'
  id: totrans-2774
  prefs: []
  type: TYPE_TB
  zh: '| |            | 113 ± 64               | 24 |            |                  |                |                |                  |             |                  |              |    |    |    |'
- en: '| 44.2 ± 2.8   | 51                     |                 |                  |                |                |                  |             |                  |              |    |    |    |'
  id: totrans-2775
  prefs: []
  type: TYPE_TB
  zh: '| 44.2 ± 2.8   | 51                     |                 |                  |                |                |                  |             |                  |              |    |    |    |'
- en: '| f)           | √                      | √               | √                |
    42.3/44.4/46.1 | 68             | 69.5/97.5/148    | 88          |                  |              |    |    |    |'
  id: totrans-2776
  prefs: []
  type: TYPE_TB
  zh: '| f)           | √                      | √               | √                |
    42.3/44.4/46.1 | 68             | 69.5/97.5/148    | 88          |                  |              |    |    |    |'
- en: '| 46.8 ± 3.7   | 27                     | 126 ± 139       | 22               |                |                |                  |             |                  |              |    |    |    |'
  id: totrans-2777
  prefs: []
  type: TYPE_TB
  zh: '| 46.8 ± 3.7   | 27                     | 126 ± 139       | 22               |                |                |                  |             |                  |              |    |    |    |'
- en: '| g)           | √                      | √               | √                |
    44.0/47.0/48.9 | 43             | 64/94/138        | 84          |                  |              |    |    |    |'
  id: totrans-2778
  prefs: []
  type: TYPE_TB
  zh: '| g)           | √                      | √               | √                |
    44.0/47.0/48.9 | 43             | 64/94/138        | 84          |                  |              |    |    |    |'
- en: '| |            | |                      |                 |                  |                |                |                  |             |                  |              |    |    |    |'
  id: totrans-2779
  prefs: []
  type: TYPE_TB
  zh: '| |            | |                      |                 |                  |                |                |                  |             |                  |              |    |    |    |'
- en: '| 46.5 ± 3.1   | 56                     | 31              | 29               |
    30             | 61             | 270 ± 164        | 15          | 12               |
    15           | 8  | 86 |    |'
  id: totrans-2780
  prefs: []
  type: TYPE_TB
  zh: '| 46.5 ± 3.1   | 56                     | 31              | 29               |
    30             | 61             | 270 ± 164        | 15          | 12               |
    15           | 8  | 86 |    |'
- en: '| h)           | √                      | √               | 44.5/46.8/48.5   |
    162/235/316    |                |                  |             |                  |              |    |    |    |'
  id: totrans-2781
  prefs: []
  type: TYPE_TB
  zh: '| h)           | √                      | √               | 44.5/46.8/48.5   |
    162/235/316    |                |                  |             |                  |              |    |    |    |'
- en: that the standard error in the mean of a performance measure reported here is
    1/
  id: totrans-2782
  prefs: []
  type: TYPE_NORMAL
  zh: 这里报告的性能指标的均值标准误差为 1/
- en: √100 = 1/10 of its reported standard deviation. Figure 10.2 depicts the same
    data (except for the direct comparisons) graphically in form of cumulative histograms
    for the minimum test set error and the number of epochs required to reach it.
  id: totrans-2783
  prefs: []
  type: TYPE_NORMAL
  zh: √100 = 报告的标准偏差的 1/10。图 10.2 以累积直方图的形式图示了相同数据（除了直接比较），展示了最小测试集错误及达到该错误所需的训练轮数。
- en: The results generally confirm the trends observed in Figure 10.1. Runs in the
    fully centered experiment e) clearly converged most rapidly - and to test set
    errors that were among the best. Compared to the conventional setup a), full centering
    converged almost 8 times faster on average while generalizing better 80% of the
    time.
  id: totrans-2784
  prefs: []
  type: TYPE_NORMAL
  zh: 结果一般确认了图 10.1 中观察到的趋势。完全中心化实验 e) 中的运行明显收敛得更快——并且测试集错误在最佳之列。与常规设置 a) 相比，完全中心化平均收敛速度快了近
    8 倍，同时在 80% 的情况下泛化效果更佳。
- en: 'Generalization Performance. Average misclassification rates on the test set
    ranged from 44% to 49%, which we consider a fair result given our comparatively
    small networks. They cluster into three groups: networks with activity centering'
  id: totrans-2785
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化性能。测试集上的平均误分类率范围从 44% 到 49%，考虑到我们相对较小的网络，这被视为公平结果。它们聚集为三组：以活动为中心的网络
- en: '![219_image_0.png](219_image_0.png)'
  id: totrans-2786
  prefs: []
  type: TYPE_IMG
  zh: '![219_image_0.png](219_image_0.png)'
- en: Fig. 10.2. Cumulative histograms for the minimum test set error (left), and
    the number of epochs required to reach it (right), for the vowel recognition task.
    Curves are labelled as in Figure 10.1, and marked every 10th percentile.
  id: totrans-2787
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2。最小测试集错误的累积直方图（左）和达到该错误所需的训练轮数（右），用于元音识别任务。曲线的标记与图 10.1 相同，每 10 个百分位标记一次。
- en: achieved around 44%, the two others with shortcuts and slope centering came
    in under 47%, while the remaining two only reached 48–49%. The cumulative histogram
    (Figure 10.2, left) shows that all activity-centered networks had an almost identical
    distribution of minimum test set errors.
  id: totrans-2788
  prefs: []
  type: TYPE_NORMAL
  zh: 达到约 44%，另外两个使用快捷方式和斜率中心的结果低于 47%，而其余两个仅达到 48-49%。累积直方图（图 10.2，左）显示所有以活动为中心的网络具有几乎相同的最小测试集错误分布。
- en: 'Note that centering the inputs changes the task, and that the addition of shortcuts
    changes the network topology. It is possible that this - rather than centering
    *per se* - accounts for their beneficial effect on generalization. Error centering
    was the one feature in our experiments that changed the dynamics of learning exclusively.
    Its addition appeared to slightly *worsen* generalization, particularly in the
    absence of other forms of centering. This could be caused by a reduction (due
    to centering) of the effective number of parameters in what is already a rather
    small model. Such an effect should not overly concern us: one could easily recoup
    the lost degrees of freedom by slightly increasing the number of hidden units
    for centered networks. Convergence Speed. All three forms of centering examined
    here clearly sped up convergence, both individually and in combination. A slight
    anomaly appeared in that the addition of error centering in going from experiment
    c) to d)'
  id: totrans-2789
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，居中输入会改变任务，添加快捷方式会改变网络拓扑。可能正是这个 - 而不是居中 *本身* - 导致了它们对泛化的有益效果。在我们的实验中，错误居中是唯一一个仅改变学习动态的特征。它的添加似乎稍微*恶化*了泛化，特别是在缺乏其他形式的居中时。这可能是由于居中导致有效参数数量减少（因为模型本身已经相当小）。这种影响不应过于担忧：可以通过稍微增加中心网络的隐藏单元数量轻松弥补失去的自由度。收敛速度。这里考察的所有三种居中形式明显加快了收敛，无论是单独还是结合使用。从实验c)到d)时，添加错误居中出现了轻微的异常。
- en: had no significant effect on the average number of epochs required. A look at
    the cumulative histogram (Figure 10.2, right) reveals that while experiment d)
    is ahead between the 20th and 80th percentile, c) had fewer unusually slow runs
    than d), and a few exceptionally fast ones.
  id: totrans-2790
  prefs: []
  type: TYPE_NORMAL
  zh: 对所需的平均周期数没有显著影响。累积直方图（图10.2，右侧）显示，尽管实验d)在20到80百分位之间领先，但c)的异常慢运行次数少于d)，而且有几次异常快的运行。
- en: 'With the other forms of centering in place, the addition of error centering
    was unequivocally beneficial: average convergence time decreased from 113 epochs
    in f) to 72.4 epochs in e). The histogram shows that the fully centered e) is
    far ahead of the competition through almost the entire percentile range.'
  id: totrans-2791
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他形式的居中存在的情况下，添加错误居中无疑是有益的：平均收敛时间从f)的113个周期减少到e)的72.4个周期。直方图显示，完全居中的e)在几乎整个百分位范围内远远领先于竞争对手。
- en: Finally, it is interesting to note that the addition of shortcuts and slope
    centering, both on their own and to a network with activity and error centering,
    roughly doubled the convergence speed - the same magnitude of effect as observed
    on the symmetry detection task.
  id: totrans-2792
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得注意的是，添加快捷方式和斜率居中，无论是单独使用还是与活动和错误居中结合使用，都大约将收敛速度翻倍 - 这种效应的大小与对称检测任务中的观察结果相同。
- en: 10.5 Discussion
  id: totrans-2793
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.5 讨论
- en: The preceding section has shown that centering can indeed have beneficial effects
    on the learning speed and generalization ability of a neural network. Why is this
    so? In what follows, we offer an explanation from three (partially overlapping)
    perspectives, considering in turn the effect of centering on the condition number
    of the Hessian, the level of noise in the local gradient, and the credit assignment
    between different parts of the network.
  id: totrans-2794
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的章节已显示，居中确实对神经网络的学习速度和泛化能力有益。那么这是为什么呢？接下来，我们将从三个（部分重叠的）角度进行解释，依次考虑居中对Hessian条件数的影响、局部梯度中的噪声水平，以及网络不同部分之间的信用分配。
- en: Conditioning the Hessian. It is well known that the minimal convergence time
    for first-order gradient descent on quadratic error surfaces is inversely related
    to the condition number of the Hessian matrix, *i.e.*, the ratio between its largest
    and its smallest eigenvalue. A common strategy for accelerating gradient descent
    is therefore to seek to improve the condition number of the Hessian.
  id: totrans-2795
  prefs: []
  type: TYPE_NORMAL
  zh: 对Hessian进行条件处理。众所周知，基于二次误差曲面的首次梯度下降的最小收敛时间与Hessian矩阵的条件数成反比，*即*，其最大特征值与最小特征值之间的比率。因此，加速梯度下降的常见策略是寻求改善Hessian的条件数。
- en: 'For a single linear node y = wTx with squared loss function, the Hessian is
    simply the covariance matrix of the inputs: H =-x xT .. Its largest eigenvalue
    is typically caused by the d.c. component of x [12]. Centering the inputs removes
    that eigenvalue, thus conditioning the Hessian and permitting larger step sizes
    (cf. Chapter 1). For batch learning, error centering has exactly the same effect
    on the local weight update:'
  id: totrans-2796
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个线性节点 y = wTx 和平方损失函数，Hessian 矩阵简单地是输入的协方差矩阵：H =-x xT .. 它的最大特征值通常是由 x 的直流成分引起的
    [12]。中心化输入去除了该特征值，从而对 Hessian 进行了条件化，并允许更大的步长（参见第 1 章）。对于批量学习，误差中心化对局部权重更新的影响完全相同：
- en: $$\Delta\mathbf{w}\,\propto\,\langle\left(\delta-\langle\delta\rangle\right)\mathbf{x}\rangle\,=\,\langle\delta\,\mathbf{x}\rangle-\langle\delta\rangle\,\langle\mathbf{x}\rangle\,=\,\langle\delta\left(\mathbf{x}-\langle\mathbf{x}\rangle\right)\rangle\tag{10.14}$$
  id: totrans-2797
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta\mathbf{w}\,\propto\,\langle\left(\delta-\langle\delta\rangle\right)\mathbf{x}\rangle\,=\,\langle\delta\,\mathbf{x}\rangle-\langle\delta\rangle\,\langle\mathbf{x}\rangle\,=\,\langle\delta\left(\mathbf{x}-\langle\mathbf{x}\rangle\right)\rangle\tag{10.14}$$
- en: Error centering does go further than activity centering, however, in that it
    also affects the error backpropagated to anterior nodes. Moreover, Equation 10.14
    does not hold for online learning, where the gradient is noisy.
  id: totrans-2798
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，误差中心化比活动中心化更进一步，因为它还影响反向传播到前节点的误差。此外，公式 10.14 对于在线学习不成立，因为梯度是噪声的。
- en: Noise Reduction. It can be shown that centering improves the signal-to-noise
    ratio of the local gradient. Omitting the slope factor for the sake of simplicity,
    consider the noisy weight update
  id: totrans-2799
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声减少。可以证明，中心化提高了局部梯度的信噪比。为了简化，忽略斜率因子，考虑噪声权重更新
- en: $$\Delta w_{ij}\,\propto\,(\delta_{j}+\phi)(x_{i}+\xi)\,=\,\delta_{j}x_{i}+\xi\delta_{j}+\phi
    x_{i}+\phi\xi\tag{10.15}$$
  id: totrans-2800
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w_{ij}\,\propto\,(\delta_{j}+\phi)(x_{i}+\xi)\,=\,\delta_{j}x_{i}+\xi\delta_{j}+\phi
    x_{i}+\phi\xi\tag{10.15}$$
- en: where φ and ξ are the noise terms, presumed to be zero-mean, and independent
    of activity, error, and each other. In the expansion on the right-hand side, the
    first term is the desired (noise-free) weight update while the others represent
    noise that contaminates it. While the last (pure noise) term cannot be helped,
    we can reduce the variance of the two mixed terms by centering δj and xi so as
    to minimize -δ 2j
  id: totrans-2801
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 φ 和 ξ 是噪声项，假定为零均值，并且相互独立于活动、误差和彼此。在右侧展开中，第一个项是期望的（无噪声）权重更新，而其他项代表污染它的噪声。虽然最后一个（纯噪声）项无法避免，但我们可以通过中心化
    δj 和 xi 来减少这两个混合项的方差，以最小化 -δ 2j。
- en: . and -x2i
  id: totrans-2802
  prefs: []
  type: TYPE_NORMAL
  zh: . 和 -x2i
- en: ., respectively.
  id: totrans-2803
  prefs: []
  type: TYPE_NORMAL
  zh: ., 分别。
- en: 'One might of course contend that in doing so, we are also shrinking the signal
    δjxi, so that in terms of the signal-to-noise ratio we are no better - in fact,
    worse - off than before. This cuts right to the heart of the matter, for centering
    rests upon the notion that the error signal relevant to a non-bias, non-shortcut
    weight is the fully centered weight update, and that any d.c. components in δjxi
    should therefore also be regarded as a form of noise. This presumption can of
    course be maintained only because we do have bias and shortcut weights to address
    the error components that centering removes. Improved Credit Assignment. From
    the perspective of a network that has these additional parameters, then, centering
    is a way to improve the assignment of responsibility for the network''s errors:
    constant errors are shunted to the bias weights, linear errors to the shortcut
    weights, and the remainder of the network is bothered only with those parts of
    the error signal that actually require a nonlinearity. Centering thus views hidden
    units as a scarce resource that should only be called upon where necessary. Given
    the computational complications that arise in the training of nonlinear nodes,
    we submit that this is an appropriate and productive viewpoint.'
  id: totrans-2804
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，有人可能会争辩说，这样做也缩小了信号 δjxi，因此就信噪比而言，我们并没有更好——实际上，甚至更糟。这直接切入了问题的核心，因为中心化基于这样一种观念：与非偏置、非捷径权重相关的误差信号是完全中心化的权重更新，δjxi
    中的任何直流成分因此也应被视为一种噪声。这一假设当然只有在我们确实有偏置和捷径权重来处理中心化所去除的误差成分时才能维持。从具有这些额外参数的网络的角度来看，中心化是一种改善网络误差责任分配的方法：恒定误差被转移到偏置权重，线性误差转移到捷径权重，网络的其余部分仅处理那些实际需要非线性的误差信号部分。因此，中心化将隐藏单元视为稀缺资源，只有在必要时才调用。鉴于在训练非线性节点时出现的计算复杂性，我们认为这是一个合适且富有成效的观点。
- en: Future Work. While the results reported here are quite promising, more experiments
    are required to assess the general applicability and effectiveness of centering.
    For feedforward networks, we would like to explore the use of centering with multiple
    hidden layers, stochastic (online) gradient descent, and for function approximation
    (rather than classification) problems. The centering approach *per se*, however,
    is rather more general than that, and so further ahead we anticipate its application
    to a range of more sophisticated network architectures, learning algorithms, and
    problem domains. Acknowledgments. I would like to thank the editors of this book
    as well as my colleagues Jürgen Schmidhuber, Marco Wiering, and Rafał Sałustowicz
    for their helpful comments. This work was supported by the Swiss National Science
    Foundation under grant numbers 2100–045700.95/1 and 2000–052678.97/1.
  id: totrans-2805
  prefs: []
  type: TYPE_NORMAL
  zh: 未来工作。尽管此处报告的结果非常有前景，但仍需更多实验以评估中心化的普遍适用性和有效性。对于前馈网络，我们希望探索在多个隐藏层、随机（在线）梯度下降和函数近似（而非分类）问题中使用中心化的方法。然而，中心化方法*本身*实际上更为一般，因此我们预期它将应用于更复杂的网络架构、学习算法和问题领域。致谢。我要感谢本书的编辑以及我的同事Jürgen
    Schmidhuber、Marco Wiering和Rafał Sałustowicz的宝贵意见。此项工作得到了瑞士国家科学基金会的资助，资助编号为2100–045700.95/1和2000–052678.97/1。
- en: '[1] Anderson, J., Rosenfeld, E. (eds.): Neurocomputing: Foundations of Research.'
  id: totrans-2806
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Anderson, J., Rosenfeld, E.（主编）：神经计算：研究基础。'
- en: MIT Press, Cambridge (1988)
  id: totrans-2807
  prefs: []
  type: TYPE_NORMAL
  zh: 麻省理工学院出版社，剑桥（1988）
- en: '[2] Battiti, R.: Accelerated back-propagation learning: Two optimization methods.'
  id: totrans-2808
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Battiti, R.：加速反向传播学习：两种优化方法。'
- en: Complex Systems 3, 331–342 (1989)
  id: totrans-2809
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂系统3, 331–342 (1989)
- en: '[3] Battiti, R.: First- and second-order methods for learning: Between steepest
    descent and Newton''s method. Neural Computation 4(2), 141–166 (1992)'
  id: totrans-2810
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Battiti, R.：学习的一阶和二阶方法：介于最陡下降法和牛顿法之间。神经计算4(2), 141–166 (1992)'
- en: '[4] Bienenstock, E., Cooper, L., Munro, P.: Theory for the development of neuron
    selectivity: Orientation specificity and binocular interaction in visual cortex.
    Journal of Neuroscience 2 (1982); Reprinted in [1]'
  id: totrans-2811
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Bienenstock, E., Cooper, L., Munro, P.：神经元选择性的发展的理论：视觉皮层中的方向特异性和双眼交互。神经科学杂志2（1982）；重印于[1]'
- en: '[5] Deterding, D.H.: Speaker Normalisation for Automatic Speech Recognition.
    PhD'
  id: totrans-2812
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Deterding, D.H.：自动语音识别中的发言者归一化。博士学位论文'
- en: thesis, University of Cambridge (1989)
  id: totrans-2813
  prefs: []
  type: TYPE_NORMAL
  zh: 论文，剑桥大学（1989）
- en: '[6] Finke, M., Müller, K.-R.: Estimating a-posteriori probabilities using stochastic
    network models. In: Mozer, M.C., Smolensky, P., Touretzky, D.S., Elman, J.L.,'
  id: totrans-2814
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Finke, M., Müller, K.-R.：使用随机网络模型估计后验概率。在：Mozer, M.C., Smolensky, P., Touretzky,
    D.S., Elman, J.L.,'
- en: Weigend, A.S. (eds.) Proceedings of the 1993 Connectionist Models Summer School,
    Boulder, CO. Lawrence Erlbaum Associates, Hillsdale (1994)
  id: totrans-2815
  prefs: []
  type: TYPE_NORMAL
  zh: Weigend, A.S.（主编）1993年联结模型夏季学校会议记录，博尔德，科罗拉多州。劳伦斯·厄尔鲍姆协会，希尔斯代尔（1994）
- en: '[7] Hastie, T.J., Tibshirani, R.J.: Discriminant adaptive nearest neighbor
    classification. IEEE Transactions on Pattern Analysis and Machine Intelligence
    18(6),'
  id: totrans-2816
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Hastie, T.J., Tibshirani, R.J.：判别自适应最近邻分类。IEEE模式分析与机器智能交易18(6)，'
- en: 607–616 (1996)
  id: totrans-2817
  prefs: []
  type: TYPE_NORMAL
  zh: 607–616 (1996)
- en: '[8] Herrmann, M.: On the merits of topography in neural maps. In: Kohonen,
    T.'
  id: totrans-2818
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Herrmann, M.：神经地图中地形的优点。在：Kohonen, T.'
- en: (ed.) Proceedings of the Workshop on Self-Organizing Maps, pp. 112–117. Helsinki
    University of Technology (1997)
  id: totrans-2819
  prefs: []
  type: TYPE_NORMAL
  zh: （主编）自组织地图研讨会记录，页112–117。赫尔辛基理工大学（1997）
- en: '[9] Hochreiter, S., Schmidhuber, J.: Feature extraction through lococode. Neural
    Computation (1998) (to appear)'
  id: totrans-2820
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Hochreiter, S., Schmidhuber, J.：通过lococode进行特征提取。神经计算（1998）（待发表）'
- en: '[10] Intrator, N.: Feature extraction using an unsupervised neural network.
    Neural Computation 4(1), 98–107 (1992)'
  id: totrans-2821
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Intrator, N.：使用无监督神经网络进行特征提取。神经计算4(1), 98–107 (1992)'
- en: '[11] Lapedes, A., Farber, R.: A self-optimizing, nonsymmetrical neural net
    for content addressable memory and pattern recognition. Physica D 22, 247–259
    (1986)'
  id: totrans-2822
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Lapedes, A., Farber, R.：一种自优化的非对称神经网络，用于内容寻址存储器和模式识别。物理学D 22, 247–259
    (1986)'
- en: '[12] LeCun, Y., Kanter, I., Solla, S.A.: Eigenvalues of covariance matrices:
    Application to neural-network learning. Physical Review Letters 66(18), 2396–2399
    (1991)'
  id: totrans-2823
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] LeCun, Y., Kanter, I., Solla, S.A.：协方差矩阵的特征值：应用于神经网络学习。物理评论快报66(18), 2396–2399
    (1991)'
- en: '[13] Robinson, A.J.: Dynamic Error Propagation Networks. PhD thesis, University
    of Cambridge (1989)'
  id: totrans-2824
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Robinson, A.J.：动态误差传播网络。博士学位论文，剑桥大学（1989）'
- en: '[14] Schraudolph, N.N., Sejnowski, T.J.: Unsupervised discrimination of clustered
    data via optimization of binary information gain. In: Hanson, S.J., Cowan, J.D.,
    Giles, C.L. (eds.) Advances in Neural Information Processing Systems, vol. 5,
    pp. 499– 506. Morgan Kaufmann, San Mateo (1993)'
  id: totrans-2825
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Schraudolph, N.N., Sejnowski, T.J.: 通过优化二元信息增益对聚类数据进行无监督区分。在：Hanson, S.J.,
    Cowan, J.D., Giles, C.L.（编）《神经信息处理系统进展》，卷5, pp. 499–506. Morgan Kaufmann, 圣马特奥
    (1993)'
- en: '[15] Schraudolph, N.N., Sejnowski, T.J.: Tempering backpropagation networks:
    Not all weights are created equal. In: Touretzky, D.S., Mozer, M.C., Hasselmo,
    M.E. (eds.) Advances in Neural Information Processing Systems, vol. 8, pp. 563–569.'
  id: totrans-2826
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Schraudolph, N.N., Sejnowski, T.J.: 调整反向传播网络：不是所有权重都是相同的。在：Touretzky,
    D.S., Mozer, M.C., Hasselmo, M.E.（编）《神经信息处理系统进展》，卷8, pp. 563–569.'
- en: MIT Press, Cambridge (1996)
  id: totrans-2827
  prefs: []
  type: TYPE_NORMAL
  zh: MIT出版社，剑桥 (1996)
- en: '[16] Sejnowski, T.J.: Storing covariance with nonlinearly interacting neurons.
    Journal of Mathematical Biology 4, 303–321 (1977)'
  id: totrans-2828
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Sejnowski, T.J.: 用非线性交互神经元存储协方差。《数学生物学杂志》4, 303–321 (1977)'
- en: '[17] Shah, S., Palmieri, F., Datum, M.: Optimal filtering algorithms for fast
    learning in feedforward neural networks. Neural Networks 5, 779–787 (1992)'
  id: totrans-2829
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Shah, S., Palmieri, F., Datum, M.: 用于前馈神经网络快速学习的最优滤波算法。《神经网络》5, 779–787
    (1992)'
- en: '[18] Tenenbaum, J.B., Freeman, W.T.: Separating style and content. In: Mozer,
    M.C.,'
  id: totrans-2830
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Tenenbaum, J.B., Freeman, W.T.: 风格与内容的分离。在：Mozer, M.C.,'
- en: Jordan, M.I., Petsche, T. (eds.) Advances in Neural Information Processing Systems,
    vol. 9, pp. 662–668. The MIT Press, Cambridge (1997)
  id: totrans-2831
  prefs: []
  type: TYPE_NORMAL
  zh: Jordan, M.I., Petsche, T.（编）《神经信息处理系统进展》，卷9, pp. 662–668. MIT出版社，剑桥 (1997)
- en: '[19] Turney, P.D.: Exploiting Context When Learning to Classify. In: Brazdil,
    P.B.'
  id: totrans-2832
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Turney, P.D.: 在分类学习中利用上下文。在：Brazdil, P.B.'
- en: (ed.) ECML 1993. LNCS, vol. 667, pp. 402–407. Springer, Heidelberg (1993)
  id: totrans-2833
  prefs: []
  type: TYPE_NORMAL
  zh: （编）ECML 1993. LNCS, 卷667, pp. 402–407. Springer, 海德堡 (1993)
- en: '[20] Turney, P.D.: Robust classification with context-sensitive features. In:
    Proceedings of the Sixth International Conference on Industrial and Engineering
    Applications of Artificial Intelligence and Expert Systems, pp. 268–276 (1993)'
  id: totrans-2834
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Turney, P.D.: 具有上下文敏感特征的鲁棒分类。在：第六届国际人工智能及专家系统工业与工程应用会议论文集，pp. 268–276
    (1993)'
- en: '[21] Vogl, T.P., Mangis, J.K., Rigler, A.K., Zink, W.T., Alkon, D.L.: Accelerating
    the convergence of the back-propagation method. Biological Cybernetics 59, 257–263'
  id: totrans-2835
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Vogl, T.P., Mangis, J.K., Rigler, A.K., Zink, W.T., Alkon, D.L.: 加速反向传播方法的收敛。《生物网络》59,
    257–263'
- en: (1988)
  id: totrans-2836
  prefs: []
  type: TYPE_NORMAL
  zh: （1988）
- en: '[22] Widrow, B., McCool, J.M., Larimore, M.G., Johnson Jr., C.R.: Stationary
    and nonstationary learning characteristics of the LMS adaptive filter. Proceedings
    of the IEEE 64(8), 1151–1162 (1976)'
  id: totrans-2837
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Widrow, B., McCool, J.M., Larimore, M.G., Johnson Jr., C.R.: LMS自适应滤波器的静态和非静态学习特性。《IEEE会议录》64(8),
    1151–1162 (1976)'
- en: '[23] Zimmermann, H.G.: Neuronale Netze als Entscheidungskalkül. In: Rehkugler,
    H.,'
  id: totrans-2838
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Zimmermann, H.G.: 神经网络作为决策计算。在：Rehkugler, H.,'
- en: 'Zimmermann, H.G. (eds.) Neuronale Netze in der Ökonomie: Grundlagen und finanzwirtschaftliche
    Anwendungen, pp. 1–87. Vahlen Verlag, Munich (1994)'
  id: totrans-2839
  prefs: []
  type: TYPE_NORMAL
  zh: Zimmermann, H.G.（编）《经济中的神经网络：基础与金融应用》，pp. 1–87. Vahlen Verlag, 慕尼黑 (1994)
- en: 11 Avoiding Roundoff Error In Backpropagating Derivatives-
  id: totrans-2840
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 避免反向传播导数中的舍入误差
- en: Tony Plate School of Mathematical and Computing Sciences, Victoria University,
    Wellington, New Zealand tap@mcs.vuw.ac.nz http://www.mcs.vuw.ac.nz/~tap/
  id: totrans-2841
  prefs: []
  type: TYPE_NORMAL
  zh: Tony Plate，维多利亚大学数学与计算科学学院，新西兰惠灵顿，tap@mcs.vuw.ac.nz http://www.mcs.vuw.ac.nz/~tap/
- en: Abstract. One significant source of roundoff error in backpropagation networks
    is the calculation of derivatives of unit outputs with respect to their total
    inputs. The roundoff error can lead result in high relative error in derivatives,
    and in particular, derivatives being calculated to be zero when in fact they are
    small but non-zero. This roundoff error is easily avoided with a simple programming
    trick which has a small memory overhead (one or two extra floating point numbers
    per unit) and an insignificant computational overhead.
  id: totrans-2842
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要：反向传播网络中的一个显著的舍入误差来源是计算单元输出相对于其总输入的导数。舍入误差可能导致导数的相对误差很高，尤其是当导数实际为小但非零时被计算为零。通过一个简单的编程技巧可以轻松避免这种舍入误差，该技巧的内存开销很小（每个单元一个或两个额外的浮点数）且计算开销微不足道。
- en: 11.1 Introduction
  id: totrans-2843
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 引言
- en: Backpropagating derivatives is an essential part of training multilayer networks.
    Accuracy of these derivatives is important to many training methods, especially
    ones which use second-order information, such as conjugate gradients. The standard
    formula for backpropagating error derivatives (eg., as given in Ripley [3],
  id: totrans-2844
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播导数是训练多层网络的一个重要部分。这些导数的准确性对许多训练方法至关重要，尤其是使用二阶信息的方法，如共轭梯度法。反向传播误差导数的标准公式（例如，见于Ripley
    [3]，
- en: Bishop [1], and Rumelhart, Hinton, and Williams [4]) use floating point arithmetic
    in such a way that can result in significant roundoff error. In particular, small
    derivatives are rounded off to zero. These errors can cause second-order methods
    to become confused (because of inaccurate gradients) and can also cause the weight
    search to stop or be very slow because some derivatives are calculated to be zero
    when in fact they are small but non-zero. This chapter explains how this particular
    source of roundoff error can be avoided simply and cheaply. The method applies
    to both logistic and tanh units, and to the sum-squared and cross-entropy error
    functions.
  id: totrans-2845
  prefs: []
  type: TYPE_NORMAL
  zh: Bishop [1] 和 Rumelhart, Hinton, 和 Williams [4]以浮点算术的方式进行计算，可能导致显著的舍入误差。尤其是，小导数被舍入为零。这些误差可能导致二阶方法出现混乱（由于梯度不准确），也可能导致权重搜索停止或变得非常缓慢，因为某些导数被计算为零，而实际上它们是小但非零的。
    本章解释了如何简单而廉价地避免这种特定的舍入误差来源。该方法适用于逻辑单元和tanh单元，以及平方和和交叉熵误差函数。
- en: In this chapter, the symbol "=" is used to denote mathematical equality, and
    the symbol "←" is used to denote an assignment to some floating-point variable.
  id: totrans-2846
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，符号“=”用于表示数学等式，符号“←”用于表示赋值给某个浮点变量。
- en: Floating point values are denoted by an asterisk, eg., x∗i is the floating point
    version of xi.
  id: totrans-2847
  prefs: []
  type: TYPE_NORMAL
  zh: 浮点值用星号表示，例如，x∗i是xi的浮点版本。
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-2848
  prefs: []
  type: TYPE_NORMAL
  zh: '- 之前发布于：Orr, G.B. 和 Müller, K.-R.（编）：LNCS 1524, ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-2849
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0（1998）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    225–230, 2012.'
  id: totrans-2850
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon 等（编）：NN: Tricks of the Trade, 第2版，LNCS 7700，第225–230页，2012年。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-2851
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: 11.2 Roundoff Error In Sigmoid Units
  id: totrans-2852
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 Sigmoid单元的舍入误差
- en: 'Consider a non-input unit whose output yi is computed as the logistic function
    of its total input xi:'
  id: totrans-2853
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个非输入单元，其输出yi被计算为其总输入xi的逻辑函数：
- en: $$y_{i}=\frac{1}{1+\exp(-x_{i})}\qquad\quad\mathrm{and}\qquad\quad y_{i}^{*}\leftarrow\frac{1}{1+\exp(-x_{i}^{*})}.$$
  id: totrans-2854
  prefs: []
  type: TYPE_NORMAL
  zh: $$y_{i}=\frac{1}{1+\exp(-x_{i})}\qquad\quad\mathrm{and}\qquad\quad y_{i}^{*}\leftarrow\frac{1}{1+\exp(-x_{i}^{*})}.$$
- en: 'In the backpropagation phase of training, we need to calculate the partial
    derivative of the error with respect to the total input. This is done using the
    chain rule:'
  id: totrans-2855
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练的反向传播阶段，我们需要计算误差相对于总输入的偏导数。这是通过链式法则完成的：
- en: $${\frac{\partial E}{\partial x_{i}}}={\frac{\partial E}{\partial y_{i}}}{\frac{\partial
    y_{i}}{\partial x_{i}}}.$$
  id: totrans-2856
  prefs: []
  type: TYPE_NORMAL
  zh: $${\frac{\partial E}{\partial x_{i}}}={\frac{\partial E}{\partial y_{i}}}{\frac{\partial
    y_{i}}{\partial x_{i}}}.$$
- en: The standard way of calculating ∂yi
  id: totrans-2857
  prefs: []
  type: TYPE_NORMAL
  zh: 计算∂yi的标准方法
- en: ∂xi is to use the formula relating it to yi, which for the logistic function
    is
  id: totrans-2858
  prefs: []
  type: TYPE_NORMAL
  zh: ∂xi是使用与yi相关的公式，对于逻辑函数为
- en: $$\left({\frac{\partial y_{i}}{\partial x_{i}}}\right)^{*}\gets y_{i}^{*}(1-y_{i}^{*}).$$
  id: totrans-2859
  prefs: []
  type: TYPE_NORMAL
  zh: $$\left({\frac{\partial y_{i}}{\partial x_{i}}}\right)^{*}\gets y_{i}^{*}(1-y_{i}^{*}).$$
- en: 'This computation is a potential source of roundoff error: if the actual value
    of yi is so close to one that y∗i is exactly one, then ( ∂yi'
  id: totrans-2860
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算是潜在的舍入误差来源：如果yi的实际值接近于一，以至于y∗i恰好为一，那么（∂yi
- en: ∂xi
  id: totrans-2861
  prefs: []
  type: TYPE_NORMAL
  zh: ∂xi
- en: )∗ will equal zero.
  id: totrans-2862
  prefs: []
  type: TYPE_NORMAL
  zh: ）*将等于零。
- en: Figures 11.1 show the values of this expression calculated in single and double
    precision floating point arithmetic. In single precision, when xi is greater than
    about 17.33, y∗i (1 − y∗i ) evaluates to zero. For xi values slightly lower than
    17.33 there is significant quantization. In double precision y∗i (1 − y∗i ) evaluates
    to zero when xi is greater than about 36.74.
  id: totrans-2863
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1显示了在单精度和双精度浮点算术中计算该表达式的值。在单精度中，当xi大于约17.33时，y∗i (1 − y∗i)的计算结果为零。对于略低于17.33的xi值，存在显著的量化。在双精度中，当xi大于约36.74时，y∗i
    (1 − y∗i)的计算结果为零。
- en: '![224_image_0.png](224_image_0.png)'
  id: totrans-2864
  prefs: []
  type: TYPE_IMG
  zh: '![224_image_0.png](224_image_0.png)'
- en: Fig. 11.1. Quantization in calculated values of unit derivatives ( ∂y
  id: totrans-2865
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1. 单位导数（∂y）计算值的量化
- en: ∂x ) for logistic units, computed using the formula y∗(1 − y∗) where y∗ ← 1/(1
    + exp(−x∗)). Roundoff error causes quantization on the right hand side of each
    plot.
  id: totrans-2866
  prefs: []
  type: TYPE_NORMAL
  zh: ∂x）对于逻辑单元，使用公式 y∗(1 − y∗) 计算，其中 y∗ ← 1/(1 + exp(−x∗))。舍入误差导致每个图的右侧发生量化。
- en: Note that in Figure 11.1 the roundoff error due to the calculation of 1 − y∗
  id: totrans-2867
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在图 11.1 中，由于计算 1 − y∗ 而产生的舍入误差
- en: 'only occurs for positive x. For negative x, y∗ approaches zero, and is accurately
    represented, and the relative roundoff error in 1−y∗ is insignificant. This provides
    a clue as to how to avoid the roundoff error for positive x: don''t compute 1
    − y∗'
  id: totrans-2868
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在正 x 的情况下发生。对于负 x，y∗ 接近零并被准确表示，而在 1−y∗ 中的相对舍入误差则微不足道。这提供了一个线索，说明如何避免正 x 的舍入误差：不要计算
    1 − y∗
- en: 'when y∗ is close to one. Indeed, these roundoff errors can be avoided entirely
    by storing an extra value with each unit, which is zi = 1 − yi, calculated accurately
    in floating point arithmetic as follows:'
  id: totrans-2869
  prefs: []
  type: TYPE_NORMAL
  zh: 当 y∗ 接近 1 时，确实，可以通过为每个单元存储一个额外的值完全避免这些舍入误差，该值为 zi = 1 − yi，按以下方式准确计算：
- en: $$y_{i}^{*}\leftarrow\frac{1}{1+\exp(-x_{i}^{*})}\qquad\mathrm{and}\qquad z_{i}^{*}\leftarrow\frac{\exp(-x_{i}^{*})}{1+\exp(-x_{i}^{*})}.$$
  id: totrans-2870
  prefs: []
  type: TYPE_NORMAL
  zh: $$y_{i}^{*}\leftarrow\frac{1}{1+\exp(-x_{i}^{*})}\qquad\mathrm{and}\qquad z_{i}^{*}\leftarrow\frac{\exp(-x_{i}^{*})}{1+\exp(-x_{i}^{*})}.$$
- en: 'Together, these two floating points numbers accurately represent the unit output
    at its extremes: y∗i is an accurate representation when the output is close to
    zero, and z∗i is an accurate representation of one minus the output when the output
    is close to one. With them, ∂yi'
  id: totrans-2871
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个浮点数共同准确表示单元输出的极值：当输出接近零时，y∗i 是一个准确的表示，而当输出接近一时，z∗i 是输出的 1 减去值的准确表示。有了它们，∂yi
- en: '∂xi is simply and accurately calculated as follows:'
  id: totrans-2872
  prefs: []
  type: TYPE_NORMAL
  zh: ∂xi 可以简单而准确地计算如下：
- en: $$\left({\frac{\partial y_{i}}{\partial x_{i}}}\right)^{*}\gets y_{i}^{*}z_{i}^{*}.$$
  id: totrans-2873
  prefs: []
  type: TYPE_NORMAL
  zh: $$\left({\frac{\partial y_{i}}{\partial x_{i}}}\right)^{*}\gets y_{i}^{*}z_{i}^{*}.$$
- en: Implementing these calculations requires the storage of one extra floating point
    number per unit (ie., z∗i ), and the computation of one extra division per unit.
    This extra resource usage is insignificant because the total resource requirements
    for a forward- and back-propagation pass are proportional to the number of weights
    in the network, which is usually of the order of the square of the number of units.
  id: totrans-2874
  prefs: []
  type: TYPE_NORMAL
  zh: 实施这些计算需要为每个单元存储一个额外的浮点数（即 z∗i），并且每个单元计算一个额外的除法。这个额外的资源使用是微不足道的，因为前向和反向传播的总资源需求与网络中权重的数量成正比，通常是单位数量的平方级别。
- en: The value yi is also used in calculating partial derivatives for weights on
    connections emerging from unit i. However, the errors in the representations of
    values of yi close to one do not cause high relative errors in these weight derivatives
    (except in special circumstances where derivatives from different examples cancel
    each other, but there is no simple remedy in these rare situations). Hence, it
    is generally safe to use just y∗i for these calculations.
  id: totrans-2875
  prefs: []
  type: TYPE_NORMAL
  zh: 值 yi 还用于计算从单元 i 发出的连接的权重的偏导数。然而，接近 1 的 yi 值的表示中的误差并不会导致这些权重导数中的高相对误差（除非在不同示例的导数互相抵消的特殊情况下，但在这些少见情况下没有简单的补救办法）。因此，通常可以安全地仅使用
    y∗i 进行这些计算。
- en: 11.2.1 Sum-Squared Error Computations
  id: totrans-2876
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2.1 平方和误差计算
- en: Roundoff errors can also occur in the calculation of errors and their derivatives.
    This is unlikely to be important if targets are rounded-off versions of true targets,
    eg., targets like 0.3129871 or 0.9817523, because such targets have as much roundoff
    error as the values of unit outputs.
  id: totrans-2877
  prefs: []
  type: TYPE_NORMAL
  zh: 舍入误差也可能发生在错误及其导数的计算中。如果目标是实际目标的四舍五入版本，则这不太可能重要，例如，像 0.3129871 或 0.9817523 的目标，因为这样的目标具有与单元输出值相同的舍入误差。
- en: 'However, if sum-squared error is used, and targets are all 0 or 1 and are accurate
    (i.e., 1 is not a rounded-off version of 0.99999999 or 1.00000001), and unit i
    computes a logistic function of its total input, then the following formulas can
    be used (where ti is the target for unit i, E = 12 (ti − yi)2, and zi = 1 − yi,
    calculated as before):'
  id: totrans-2878
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果使用平方和误差，并且目标值都是 0 或 1 且是准确的（即 1 不是 0.99999999 或 1.00000001 的四舍五入版本），且单元
    i 计算其总输入的逻辑函数，则可以使用以下公式（其中 ti 是单元 i 的目标，E = 12 (ti − yi)²，zi = 1 − yi，按之前的方式计算）：
- en: $$\begin{array}{c}{{E^{*}\leftarrow\left\{\begin{array}{l l}{{\frac{1}{2}(t_{i}^{*}-y_{i}^{*})^{2}}}\\
    {{\frac{1}{2}((t_{i}^{*}-1)+z_{i}^{*})^{2}}}\end{array}\right.}}\\ {{\left({\frac{\partial
    E}{\partial y_{i}}}\right)^{*}\leftarrow\left\{\begin{array}{l l}{{y_{i}^{*}-t_{i}^{*}}}\\
    {{(1-t_{i}^{*})-z_{i}^{*}}}\end{array}\right.}}\end{array}$$
  id: totrans-2879
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{c}{{E^{*}\leftarrow\left\{\begin{array}{l l}{{\frac{1}{2}(t_{i}^{*}-y_{i}^{*})^{2}}}\\
    {{\frac{1}{2}((t_{i}^{*}-1)+z_{i}^{*})^{2}}}\end{array}\right.}}\\ {{\left({\frac{\partial
    E}{\partial y_{i}}}\right)^{*}\leftarrow\left\{\begin{array}{l l}{{y_{i}^{*}-t_{i}^{*}}}\\
    {{(1-t_{i}^{*})-z_{i}^{*}}}\end{array}\right.}}\end{array}$$
- en: $\text{if}y_i^*\leq0.5$  $\text{if}y_i^*>0.5$  $\text{if}y_i^*\leq0.5$  $\text{if}y_i^*>0.5$  ...
  id: totrans-2880
  prefs: []
  type: TYPE_NORMAL
  zh: $\text{如果}y_i^*\leq0.5$  $\text{如果}y_i^*>0.5$  $\text{如果}y_i^*\leq0.5$  $\text{如果}y_i^*>0.5$  ...
- en: 2 (t∗i − y∗i )2 if y∗i ≤ 0.5
  id: totrans-2881
  prefs: []
  type: TYPE_NORMAL
  zh: 2 (t∗i − y∗i )2 如果 y∗i ≤ 0.5
- en: ∗i − 1) + z∗i )2 if y∗i > 0.5
  id: totrans-2882
  prefs: []
  type: TYPE_NORMAL
  zh: ∗i − 1) + z∗i )2 如果 y∗i > 0.5
- en: ∗i if y∗i ≤ 0.5
  id: totrans-2883
  prefs: []
  type: TYPE_NORMAL
  zh: ∗i 如果 y∗i ≤ 0.5
- en: ∗i ) − z∗i if y∗i > 0.5
  id: totrans-2884
  prefs: []
  type: TYPE_NORMAL
  zh: ∗i ) − z∗i 如果 y∗i > 0.5
- en: These formulas could be split into a greater number of simpler cases if ti were
    always 0 or 1, but as they are they are correct for general values of ti and accurate
    when ti is 0 or 1.
  id: totrans-2885
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ti 始终为 0 或 1，则这些公式可以拆分为更多更简单的情况，但就目前而言，它们对一般的 ti 值是正确的，并且当 ti 为 0 或 1 时是准确的。
- en: 11.2.2 Single Logistic-Output Cross-Entropy Computations
  id: totrans-2886
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2.2 单逻辑输出交叉熵计算
- en: 'If the network has a single output unit (unit i) which represents probability
    in a two-class classification problem, then the cross-entropy error function [1]
    for one example is as follows:'
  id: totrans-2887
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网络有一个输出单元（单元 i），它代表一个二分类问题中的概率，那么一个示例的交叉熵误差函数 [1] 如下所示：
- en: $$E=t_{i}\mathrm{log}y_{i}+(1-t_{i})\mathrm{log}(1-y_{i}).$$
  id: totrans-2888
  prefs: []
  type: TYPE_NORMAL
  zh: $$E=t_{i}\mathrm{log}y_{i}+(1-t_{i})\mathrm{log}(1-y_{i}).$$
- en: This error function is most appropriately used with a logistic function on the
    output unit. In this case, errors and partial derivatives can be calculated from
    the following formulas, and the calculations will be accurate for ti = 0 or 1
    (where zi = 1 − yi, calculated as before). To get accurate results, it is necessary
    to use the function1 log1p(x), which computes log(1 + x) accurately for tiny x.
  id: totrans-2889
  prefs: []
  type: TYPE_NORMAL
  zh: 该误差函数最适合与输出单元上的逻辑函数一起使用。在这种情况下，可以根据以下公式计算误差和偏导数，并且当 ti = 0 或 1 时计算将是准确的（其中 zi
    = 1 − yi，按之前计算）。为了获得准确的结果，有必要使用函数1 log1p(x)，该函数可以准确计算 log(1 + x) 对于微小的 x。
- en: $$\begin{aligned} E^{*}&\leftarrow\left\{\begin{aligned} &t_{i}^{*}\text{log}y_{i}^{*}+(1-t_{i}^{*})\text{log}1\text{p}(-y_{i}^{*})&\text{if}y_{i}^{*}\leq
    0.5\\ &t_{i}^{*}\text{log}1\text{p}(-z_{i}^{*})+(1-t_{i}^{*})\text{log}z_{i}^{*}&\text{if}y_{i}^{*}>0.5\\
    \end{aligned}\right.\\ \left(\frac{\partial E}{\partial x_{i}}\right)^{*}&\leftarrow\left\{\begin{aligned}
    &y_{i}^{*}-t_{i}^{*}&\text{if}y_{i}^{*}\leq 0.5\\ &1-t_{i}^{*}-z_{i}^{*}&\text{if}y_{i}^{*}>0.5\\
    \end{aligned}\right.\\ \end{aligned}$$
  id: totrans-2890
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{aligned} E^{*}&\leftarrow\left\{\begin{aligned} &t_{i}^{*}\text{log}y_{i}^{*}+(1-t_{i}^{*})\text{log}1\text{p}(-y_{i}^{*})&\text{如果}y_{i}^{*}\leq
    0.5\\ &t_{i}^{*}\text{log}1\text{p}(-z_{i}^{*})+(1-t_{i}^{*})\text{log}z_{i}^{*}&\text{如果}y_{i}^{*}>0.5\\
    \end{aligned}\right.\\ \left(\frac{\partial E}{\partial x_{i}}\right)^{*}&\leftarrow\left\{\begin{aligned}
    &y_{i}^{*}-t_{i}^{*}&\text{如果}y_{i}^{*}\leq 0.5\\ &1-t_{i}^{*}-z_{i}^{*}&\text{如果}y_{i}^{*}>0.5\\
    \end{aligned}\right.\\ \end{aligned}$$
- en: 11.2.3 Other Approaches To Avoiding Zero-Derivatives With The Logistic Function
  id: totrans-2891
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2.3 避免逻辑函数零导数的其他方法
- en: Fahlman [2] suggested adding 0.1 to ∂yi
  id: totrans-2892
  prefs: []
  type: TYPE_NORMAL
  zh: Fahlman [2] 建议向 ∂yi 添加 0.1。
- en: ∂xi in order to decrease learning time by eliminating flat spots in the training
    surface (ie., spots with small derivatives), and also to avoid zero-derivatives
    due to roundoff. In Fahlman's experiments this technique improved the learning
    time. However, it also makes the derivatives incorrect with respect to the error
    function. This may not matter in networks where the actual output numbers do not
    mean very much other than "high" or
  id: totrans-2893
  prefs: []
  type: TYPE_NORMAL
  zh: ∂xi 以通过消除训练表面中的平坦点（即，具有小导数的点）来减少学习时间，并且还避免由于舍入而导致的零导数。在 Fahlman 的实验中，这种技术改善了学习时间。然而，它也使导数相对于误差函数不正确。在实际输出数值除了“高”或“低”之外并没有太大意义的网络中，这可能无关紧要。
- en: '"low". However, in networks where achieving accurate estimates of the targets
    is important, eg., where the targets are probabilities, or targets are continuous
    values in a modeling task, this technique is undesirable as it causes the gradient
    search to not minimize the error, but to minimize some other quantity instead.'
  id: totrans-2894
  prefs: []
  type: TYPE_NORMAL
  zh: “低”。然而，在准确估计目标非常重要的网络中，例如，当目标是概率，或目标在建模任务中是连续值时，这种技术是不可取的，因为它导致梯度搜索未能最小化误差，而是最小化其他量。
- en: Another technique sometimes used with neural networks is to transform targets
    from the range [0, 1] to [0.1, 0.9]. Again, a secondary motivation is to avoid
    zero-derivatives, and again, this technique should not be used where actual output
    values have any more significance than "high" or "low".
  id: totrans-2895
  prefs: []
  type: TYPE_NORMAL
  zh: 有时与神经网络一起使用的另一种技术是将目标值从范围 [0, 1] 转换为 [0.1, 0.9]。再次，次要动机是避免零导数，而再次，这种技术不应在实际输出值具有比“高”或“低”更重要的意义时使用。
- en: 11.3 Softmax And Cross-Entropy Computations
  id: totrans-2896
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 softmax 和交叉熵计算
- en: 'In networks which must classify instances into one of k mutually exclusive
    classes, cross-entropy is often used as the error measure, together with the softmax
    output function. The softmax output function allows the outputs of the k output
    units to be interpreted as probabilities: it forces each output to be between
    zero 1 log1p(x) is available in Unix math libraries.'
  id: totrans-2897
  prefs: []
  type: TYPE_NORMAL
  zh: 在必须将实例分类为 k 个相互排斥类别之一的网络中，交叉熵通常用作误差度量，结合 softmax 输出函数。softmax 输出函数允许 k 个输出单元的输出被解释为概率：它强制每个输出在零与一之间，`log1p(x)`
    在 Unix 数学库中可用。
- en: 'and one, and forces their total to be one. Assuming that outputs units are
    numbered 1 to k, the equations for softmax and cross-entropy (for one example)
    are as follows:'
  id: totrans-2898
  prefs: []
  type: TYPE_NORMAL
  zh: 并且一，并强制它们的总和为一。假设输出单元编号为 1 到 k，softmax 和交叉熵（对于一个示例）的方程如下：
- en: $$y_{i}={\frac{\exp(x_{i})}{\sum_{j=1}^{k}\exp(x_{j})}}\qquad{\mathrm{~and~}}\qquad
    E=\sum_{j=1}^{k}t_{j}{\mathrm{log}}y_{j}.$$
  id: totrans-2899
  prefs: []
  type: TYPE_NORMAL
  zh: $$y_{i}={\frac{\exp(x_{i})}{\sum_{j=1}^{k}\exp(x_{j})}}\qquad{\mathrm{~and~}}\qquad
    E=\sum_{j=1}^{k}t_{j}{\mathrm{log}}y_{j}.$$
- en: 'The derivative of the error with respect to x has a very simple form:'
  id: totrans-2900
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 x 的误差导数具有非常简单的形式：
- en: $${\frac{\partial E}{\partial x_{i}}}=y_{i}-t_{i}.$$
  id: totrans-2901
  prefs: []
  type: TYPE_NORMAL
  zh: $${\frac{\partial E}{\partial x_{i}}}=y_{i}-t_{i}.$$
- en: This equation is subject to high relative error due to roundoff when ti is exactly
    one and yi is close to one. This can happen often in training neural nets, and
    can lead to derivatives being calculated as zero when in fact they are just small.
  id: totrans-2902
  prefs: []
  type: TYPE_NORMAL
  zh: 当 ti 正好为一且 yi 接近一时，该方程受舍入引起的相对误差影响很大。这在训练神经网络时经常发生，并可能导致导数被计算为零，而实际上它们只是很小。
- en: 'This roundoff error, and also possible overflow in the computation of yi, can
    be avoided by using the following computations, which use extra floating point
    variables to store the values of zi = 1 − yi and xi = xi − maxj xj :'
  id: totrans-2903
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用以下计算，可以避免这种舍入误差以及在计算 yi 时可能的溢出，这些计算使用额外的浮点变量来存储值 zi = 1 − yi 和 xi = xi −
    maxj xj：
- en: "z∗i ← 21 − y∗i if y∗i ≤ 0.5 1 Q∗ j\f=i exp(xj ∗) if y∗i > 0.5 m∗ ← max j x∗j\
    \ xi ∗ ← x∗i − m∗ Q∗ ← - j exp(xj ∗) \t ∂E ∂xi ∗← 2y∗i − t∗i if y∗i ≤ 0.5 (1 −\
    \ t ∗i ) − z∗i if y∗i > 0.5 y∗i ← 1 Q∗ exp(xi ∗) 2t∗j log(y∗j ) if y∗i ≤ 0.5 t\
    \ ∗j log1p(−z∗j ) if y∗i > 0.5 E∗ ← - j"
  id: totrans-2904
  prefs: []
  type: TYPE_NORMAL
  zh: "z∗i ← 21 − y∗i 如果 y∗i ≤ 0.5 1 Q∗ j\f=i exp(xj ∗) 如果 y∗i > 0.5 m∗ ← max j x∗j\
    \ xi ∗ ← x∗i − m∗ Q∗ ← - j exp(xj ∗) \t ∂E ∂xi ∗← 2y∗i − t∗i 如果 y∗i ≤ 0.5 (1 −\
    \ t ∗i ) − z∗i 如果 y∗i > 0.5 y∗i ← 1 Q∗ exp(xi ∗) 2t∗j log(y∗j ) 如果 y∗i ≤ 0.5 t\
    \ ∗j log1p(−z∗j ) 如果 y∗i > 0.5 E∗ ← - j"
- en: 'The space overhead is low: at most two extra floating point variables per output
    unit, depending on the implementation. The time overhead is also low.'
  id: totrans-2905
  prefs: []
  type: TYPE_NORMAL
  zh: 空间开销很低：每个输出单元最多两个额外的浮点变量，具体取决于实现。时间开销也很低。
- en: The only lengthy additional computation is the second alternative for z∗i ,
    which can be performed for at most one i because it is impossible for more than
    one yi to be greater than 0.5.
  id: totrans-2906
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的冗长附加计算是 z∗i 的第二种替代方案，最多只能为一个 i 执行，因为不可能有多个 yi 大于 0.5。
- en: 11.4 Roundoff Error In Tanh Units
  id: totrans-2907
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 tanh 单元中的舍入误差
- en: 'The same ideas apply to the calculation of derivatives in tanh units, which
    are described by the following equations:'
  id: totrans-2908
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的思想适用于 tanh 单元中导数的计算，这些单元由以下方程描述：
- en: $$y_{i}={\frac{1-\exp(-2x_{i})}{1+\exp(-2x_{i})}}\qquad{\mathrm{and}}\qquad{\frac{\partial
    y_{i}}{\partial x_{i}}}=(1-y_{i})(1+y_{i}).$$
  id: totrans-2909
  prefs: []
  type: TYPE_NORMAL
  zh: $$y_{i}={\frac{1-\exp(-2x_{i})}{1+\exp(-2x_{i})}}\qquad{\mathrm{and}}\qquad{\frac{\partial
    y_{i}}{\partial x_{i}}}=(1-y_{i})(1+y_{i}).$$
- en: 'For tanh units, the output yi can take on values between -1 and 1. Hence, to
    represent outputs accurately at the extremes we need two extra floating point
    numbers to store the values zi = 1 − yi and ui = 1+ yi. Then the following expressions
    evaluated in floating point arithmetic will avoid unnecessary roundoff error:'
  id: totrans-2910
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 tanh 单元，输出 yi 可以取 -1 到 1 之间的值。因此，为了准确表示极值输出，我们需要两个额外的浮点数来存储值 zi = 1 − yi
    和 ui = 1 + yi。然后，以下在浮点算术中计算的表达式将避免不必要的舍入误差：
- en: $$\begin{array}{l}{{v_{i}^{*}\leftarrow\exp(-2x_{i}^{*})}}\\ {{u_{i}^{*}\gets\frac{2}{1+v_{i}^{*}}}}\\
    {{z_{i}^{*}\gets v_{i}^{*}u_{i}^{*}}}\end{array}$$
  id: totrans-2911
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{l}{{v_{i}^{*}\leftarrow\exp(-2x_{i}^{*})}}\\ {{u_{i}^{*}\gets\frac{2}{1+v_{i}^{*}}}}\\
    {{z_{i}^{*}\gets v_{i}^{*}u_{i}^{*}}}\end{array}$$
- en: $$\begin{array}{c}{{y_{i}^{*}\gets u_{i}^{*}-1}}\\ {{\left(\frac{\partial y_{i}}{\partial
    x_{i}}\right)^{*}\gets z_{i}^{*}u_{i}^{*}}}\end{array}$$
  id: totrans-2912
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{c}{{y_{i}^{*}\gets u_{i}^{*}-1}}\\ {{\left(\frac{\partial y_{i}}{\partial
    x_{i}}\right)^{*}\gets z_{i}^{*}u_{i}^{*}}}\end{array}$$
- en: '![228_image_0.png](228_image_0.png)'
  id: totrans-2913
  prefs: []
  type: TYPE_IMG
  zh: '![228_image_0.png](228_image_0.png)'
- en: The tanh function is not often used on output units, but if desired, formulas
    for accurately calculating errors and derivatives when targets are always 1 or
    -1 are easily derived.
  id: totrans-2914
  prefs: []
  type: TYPE_NORMAL
  zh: tanh函数在输出单元上不常使用，但如果需要，当目标始终为1或-1时，计算误差和导数的公式可以轻松推导。
- en: 11.5 Why Bother?
  id: totrans-2915
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.5 为什么要费心？
- en: 'Since the derivatives which are affected by roundoff error are very small,
    the proposal to calculate them accurately might provoke the response "why bother?"
    For on-line learning methods (stochastic gradient), there is probably no point,
    as computing small gradients to high accuracy is unlikely to make any difference.
    However, for nets trained in batch mode with second-order methods such as conjugate
    gradients, small derivatives can be quite important, for the following reasons:'
  id: totrans-2916
  prefs: []
  type: TYPE_NORMAL
  zh: 由于受舍入误差影响的导数非常小，提议准确计算它们可能会引发“何必呢？”的反应。对于在线学习方法（随机梯度），可能没有意义，因为高精度计算小梯度不太可能带来任何改变。然而，对于使用共轭梯度等二阶方法在批处理模式下训练的网络，小导数可能非常重要，原因如下：
- en: 1. Quantization in error or derivatives can confuse line search routines, so
    avoiding quantization is a good thing.
  id: totrans-2917
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 误差或导数的量化可能会混淆线搜索例程，因此避免量化是明智之举。
- en: 2. Many small derivatives can add up.
  id: totrans-2918
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 许多小导数可能会累加。
- en: 3. Computing small but non-zero derivatives allows training methods to continue
    as opposed to stopping because of zero derivatives. Some training methods can
    make significant progress with small derivatives, so it is possible that the weights
    will move out of the flat area of the error function.
  id: totrans-2919
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 计算小但非零的导数允许训练方法继续进行，而不是因零导数而停止。一些训练方法可以在小导数下取得显著进展，因此权重可能会从误差函数的平坦区域移动。
- en: Indeed, there is little reason not to compute these values accurately, as the
    extra storage and computations are insignificant.
  id: totrans-2920
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，没有理由不准确计算这些值，因为额外的存储和计算量微不足道。
- en: '[1] Bishop, C.: Neural Networks for Pattern Recognition. Oxford University
    Press'
  id: totrans-2921
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Bishop, C.: 用于模式识别的神经网络。牛津大学出版社'
- en: (1995)
  id: totrans-2922
  prefs: []
  type: TYPE_NORMAL
  zh: （1995）
- en: '[2] Fahlman, S.E.: Fast-learning variations on back-propagation: An empirical
    study.'
  id: totrans-2923
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Fahlman, S.E.: 关于反向传播的快速学习变体：一项实证研究。'
- en: 'In: Touretzky, D., Hinton, G., Sejnowski, T. (eds.) Proceedings of the 1988
    Connectionist Models Summer School, pp. 38–51. Morgan Kaufmann, San Mateo (1989)'
  id: totrans-2924
  prefs: []
  type: TYPE_NORMAL
  zh: 在：Touretzky, D., Hinton, G., Sejnowski, T.（编）1988年连接主义模型夏季学校论文集，pp. 38–51。摩根·考夫曼，圣马特奥（1989）
- en: '[3] Ripley, B.D.: Pattern Recognition and Neural Networks. Cambridge University
    Press (1995)'
  id: totrans-2925
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Ripley, B.D.: 模式识别与神经网络。剑桥大学出版社（1995）'
- en: '[4] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning internal representations
    by error propagation. In: Parallel Distributed Processing: Explorations in the
    Microstructure of Cognition, vol. 1, pp. 318–362. MIT Press, Cambridge (1986)'
  id: totrans-2926
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: 通过误差传播学习内部表示。在：并行分布处理：对认知微观结构的探索，第一卷，pp.
    318–362。MIT出版社，剑桥（1986）'
- en: Representing And Incorporating Prior Knowledge In Neural Network Training-
  id: totrans-2927
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在神经网络训练中表示和融入先前知识-
- en: Preface
  id: totrans-2928
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: 'The present section focuses on tricks for four important aspects in learning:
    (1) incorporation of prior knowledge, (2) choice of representation for the learning
    task, (3) unequal class prior distributions, and finally (4) large network training.'
  id: totrans-2929
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点关注学习中的四个重要方面的技巧：（1）先前知识的融入，（2）学习任务的表示选择，（3）不平等的类先验分布，最后（4）大型网络训练。
- en: Patrice Simard, et al. review the famous tangent distance and tangent propagation
    algorithms. Included are tricks for speeding and increasing the stability that
    were not published in previous work. One important trick for obtaining good performance
    in their methods is smoothing (see p. 249) of the input data, which is illustrated
    for 2D handwritten character recognition images. To obtain the tangent we must
    compute the derivative, however, this obviously cannot be calculated for discrete
    images. To compute tangent vectors, it must be possible to interpolate between
    the pixel values of the images (or the features of the images). Since many interpolations
    are possible, a "smoothing" regularizer is used because it has the extra benefit
    of imposing some control on the locality of the transformation invariance. However,
    care must be taken not to over smooth (or useful features are washed away). Another
    trick is to use the so-called elastic tangent distance (see p. 249) which eliminates
    the problem of singular systems which arises when there is zero distance between
    two patterns, or when their tangent vectors are parallel. Finally, through a very
    refined hierarchy of resolution and accuracy (see p. 251) the tangent distance
    algorithm can be sped up by two or tree orders of magnitude over a straight forward
    implementation.
  id: totrans-2930
  prefs: []
  type: TYPE_NORMAL
  zh: Patrice Simard等人回顾了著名的切线距离和切线传播算法。包括了一些加速和提高稳定性的技巧，这些在之前的工作中没有发表。获取良好性能的一个重要技巧是对输入数据进行平滑处理（见第249页），这在二维手写字符识别图像中得到了说明。为了获得切线，我们必须计算导数，但显然这对于离散图像是无法计算的。要计算切线向量，必须能够在图像的像素值（或图像的特征）之间进行插值。由于可能的插值方法很多，因此使用了一种“平滑”正则化器，因为它还具有对变换不变性的局部性施加控制的额外好处。然而，必须小心不要过度平滑（否则有用特征会被抹去）。另一个技巧是使用所谓的弹性切线距离（见第249页），它消除了当两个模式之间的距离为零时，或当它们的切线向量平行时出现的奇异系统问题。最后，通过非常精细的分辨率和准确性层次（见第251页），切线距离算法的速度可以比简单实现提高两个或三个数量级。
- en: Tangentprop takes the invariance idea even further by making it possible to
    incorporate local or global invariances and prior knowledge directly into the
    loss function of backpropagation and to backpropagate tangents efficiently (see
    p.
  id: totrans-2931
  prefs: []
  type: TYPE_NORMAL
  zh: Tangentprop进一步推动了不变性理念，使得可以将局部或全局不变性和先验知识直接纳入反向传播的损失函数中，并高效地反向传播切线（见第259页）。
- en: 259). The chapter concludes with a digression on the mathematical background
    (Lie groups) and the simplifications that follow from the theory of this very
    successful (record breaking OCR) algorithm.
  id: totrans-2932
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以对数学背景（李群）及其成功（创纪录OCR）算法理论所带来的简化的题外话作为结束。
- en: In the next chapter Larry Yaeger, et al. present an entire collection of tricks
    for an application that also is of strong commercial interest, on-line handwritten
    character recognition. These tricks were used by the authors to develop the recognizer
    in the Apple Computer's Newton MessagePad-R and eMate-R products.
  id: totrans-2933
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，Larry Yaeger等人展示了一整套针对在线手写字符识别的应用技巧，这一应用也具有强大的商业兴趣。这些技巧被作者用于开发苹果电脑的Newton
    MessagePad-R和eMate-R产品中的识别器。
- en: 'Their recognizer consists of three main components: a segmenter, a classifier,
    and a context driven search component. Of particular interest is the intricate
    representation of stroke information (see p. 274) that serves as input to the
    classifier. Prior knowledge has led the authors to include specific stroke'
  id: totrans-2934
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的识别器由三个主要组件组成：一个分割器、一个分类器和一个上下文驱动搜索组件。特别值得关注的是对笔画信息的复杂表示（见第274页），作为分类器的输入。先验知识促使作者包括特定的笔画
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-2935
  prefs: []
  type: TYPE_NORMAL
  zh: '- 之前发表在：Orr, G.B. 和 Müller, K.-R.（编）：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-2936
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0（1998）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    231–233, 2012.'
  id: totrans-2937
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon等人（编）：《神经网络：行业窍门》，第2版，LNCS 7700，第231–233页，2012。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-2938
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: features, grayscale character images, and coarse character attributes (stroke
    count and character aspect ratio), all combined into an ensemble of networks.
  id: totrans-2939
  prefs: []
  type: TYPE_NORMAL
  zh: 特征、灰度字符图像和粗略字符属性（笔画数和字符纵横比），所有这些都结合成一个网络集成。
- en: The ensemble decisions are then combined in algorithmic search through a hypothesis
    space of dictionaries and combinations of dictionaries comprising a broad coverage,
    weakly applied language model.
  id: totrans-2940
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，集成决策通过对字典的假设空间和字典组合进行算法搜索，结合了广泛覆盖的、应用较弱的语言模型。
- en: 'An extensive set of tricks for training the network are also discussed, including:'
  id: totrans-2941
  prefs: []
  type: TYPE_NORMAL
  zh: 还讨论了一套广泛的训练网络技巧，包括：
- en: 'Normalizing output error: Assist output activities of secondary choices to
    have non-zero values (*NormOutErr*). This enhances the robustness for the subsequent
    integrated search procedure since the search can now also take the n best choices
    into account (p. 276).'
  id: totrans-2942
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化输出错误：帮助次要选择的输出活动具有非零值（*NormOutErr*）。这增强了后续集成搜索过程的鲁棒性，因为搜索现在也可以考虑前n个最佳选择（第p.
    276）。
- en: 'Negative training: Reduce the effect of invalid character segmentation (p.'
  id: totrans-2943
  prefs: []
  type: TYPE_NORMAL
  zh: 负训练：减少无效字符分割的影响（第p.）。
- en: 278).
  id: totrans-2944
  prefs: []
  type: TYPE_NORMAL
  zh: 278).
- en: 'Stroke warping: Generate randomly warped versions of patterns to increase the
    generalization ability (p. 279).'
  id: totrans-2945
  prefs: []
  type: TYPE_NORMAL
  zh: 笔画扭曲：生成随机扭曲的模式版本，以增加泛化能力（第p. 279）。
- en: 'Frequency balancing: Reduce the problem of unbalanced class priors using the
    simple trick of repeating low frequency classes more often in order to force the
    network to allocate more resources to these cases (p. 280).'
  id: totrans-2946
  prefs: []
  type: TYPE_NORMAL
  zh: 频率平衡：使用简单的技巧，通过更频繁地重复低频类别来减少不平衡类别先验的问题，以强迫网络为这些情况分配更多资源（第p. 280）。
- en: 'Error emphasis: Account for different and uncommon writing *styles* by presenting
    difficult or unusual patterns more often (p. 281).'
  id: totrans-2947
  prefs: []
  type: TYPE_NORMAL
  zh: 错误强调：通过更频繁地呈现困难或不寻常的模式来考虑不同和不常见的写作*风格*（第p. 281）。
- en: 'Quantized weights: Enable the neural network classifier to run with only onebyte
    weights and train with a temporary additional two bytes (p. 282).'
  id: totrans-2948
  prefs: []
  type: TYPE_NORMAL
  zh: 量化权重：使神经网络分类器仅使用一字节权重运行，并以临时额外的两个字节进行训练（第p. 282）。
- en: In chapter 14, Steve Lawrence, et al. discuss several different tricks for alleviating
    the problem of unbalanced class prior probabilities. Some theoretical explanations
    are also provided. In the first trick, prior scaling (p. 296), weight updates
    are scaled so that the total expected update for each class is equal. The second
    trick, probabilistic sampling (p. 298), slightly modifies the frequency balancing
    in chapter 13. Here, a class is first chosen and then from within this class a
    sample is drawn. The next trick is referred to as post scaling (p. 298). The network
    is trained as usual but the network outputs are rescaled after training. This
    method can also be used to optimize other criteria that are different from the
    loss function the network has been trained with. Finally the authors propose to
    equalize class memberships (p. 299) by either subsampling the class with higher
    frequency or by duplicating patterns from the class with lower frequency (however
    they report that this trick works least efficiently). The effectiveness of each
    of these tricks is examined and compared for an ECG classification problem.
  id: totrans-2949
  prefs: []
  type: TYPE_NORMAL
  zh: 在第14章中，史蒂夫·劳伦斯等人讨论了几种缓解不平衡类别先验概率问题的不同技巧。还提供了一些理论解释。在第一个技巧中，先验缩放（第p. 296），权重更新被缩放，使得每个类别的总期望更新相等。第二个技巧，概率采样（第p.
    298），稍微修改了第13章中的频率平衡。这里，首先选择一个类别，然后从该类别中抽取样本。下一个技巧称为后缩放（第p. 298）。网络如常训练，但训练后对网络输出进行重新缩放。这种方法还可以用于优化与网络训练时使用的损失函数不同的其他标准。最后，作者建议通过对高频类别进行子采样或从低频类别中复制模式来平衡类别成员资格（然而他们报告说这个技巧的效率最低）。这些技巧在心电图分类问题中的有效性得到了检验和比较。
- en: Training problems with thousands of classes and millions of examples, as are
    common for speech and handwritten character recognition problems, pose a major
    challenge. While many of the training techniques discussed so far work well for
    moderate size nets, they can fail miserably for these extremely large problems.
    In chapter 15, Jürgen Fritsch and Michael Finke design a representation and architecture
    for such large scale learning problems and, like the previous two chapters, they
    also tackle the problem of unbalanced class priors (since not all of the 24k subphonemes
    are equally probable). They exemplify their approach by building a large vocabulary
    speech recognizer. In the first step they break down the task into a hierarchy
    of smaller decision problems of controllable size (divide and conquer, p. 313)
    and estimate the conditional probabilities for each node of the decision tree
    with a neural network. The network training uses mini batches and individual adaptive
    learning rates that are increased if progress is made in weight space and decreased
    if the fluctuations in weight space are too high (p. 332). These estimated probabilities
    - modeled by every single neural network node - are combined to give an overall
    estimate of the class decision probabilities.
  id: totrans-2950
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有成千上万类别和数百万示例的训练问题，如语音和手写字符识别问题，带来了重大挑战。虽然到目前为止讨论的许多训练技术对于中等规模的网络效果良好，但在这些极大的问题上它们可能会失败。在第15章中，Jürgen
    Fritsch 和 Michael Finke 为此类大规模学习问题设计了一种表示和架构，并像前两章一样，他们还解决了不平衡类别先验的问题（因为并非所有的24k亚音素都是同样可能的）。他们通过构建一个大型词汇语音识别器来举例说明他们的方法。在第一步中，他们将任务分解为可控规模的小决策问题的层次结构（分而治之，p.
    313），并使用神经网络估计决策树每个节点的条件概率。网络训练使用小批量和个体自适应学习率，当在权重空间取得进展时增加，当权重空间的波动过高时减少（p. 332）。这些估计的概率——由每个神经网络节点建模——结合起来给出类别决策概率的总体估计。
- en: The authors either determine the decision tree structure manually or estimate
    it by their ACID clustering algorithm (p. 324). Interestingly, the manual structure
    design was outperformed by the proposed agglomerative clustering scheme. No doubt
    that prior knowledge helps to achieve better classification results. However,
    this astonishing result indicates that human prior knowledge, although helpful
    in general, is suboptimal for structuring such a large task, particularly since
    automatic clustering allows for fine-grain subdivision of the classification task
    and aims for uniformity of priors. This desirable goal is hardly achievable by
    manual construction of the classification hierarchy. Furthermore, the human prior
    knowledge also does not provide the best basis from which a machine learning algorithm
    can learn optimally, a fact that is important to keep in mind for other applications
    as well.
  id: totrans-2951
  prefs: []
  type: TYPE_NORMAL
  zh: 作者要么手动确定决策树结构，要么通过他们的 ACID 聚类算法进行估计（p. 324）。有趣的是，手动结构设计的效果不如所提出的聚合聚类方案。毫无疑问，先验知识有助于获得更好的分类结果。然而，这一惊人的结果表明，尽管人类的先验知识在一般情况下是有益的，但对于构建如此庞大的任务而言是次优的，特别是因为自动聚类允许对分类任务进行细粒度细分，并旨在实现先验的均匀性。这个理想目标通过手动构建分类层次结构几乎无法实现。此外，人类的先验知识也并不是机器学习算法最佳学习的基础，这一事实在其他应用中也值得记住。
- en: Jenny & Klaus
  id: totrans-2952
  prefs: []
  type: TYPE_NORMAL
  zh: Jenny & Klaus
- en: 12 Transformation Invariance In Pattern Recognition - Tangent Distance And Tangent
    Propagation-
  id: totrans-2953
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 转换不变性在模式识别中的应用 - 切线距离和切线传播-
- en: Patrice Y. Simard1, Yann A. LeCun1, John S. Denker1, and Bernard Victorri2 1
    Image Processing Services Research Lab, AT& T Labs - Research, 100 Schulz Drive,
    Red Bank, NJ 07701-7033, USA
  id: totrans-2954
  prefs: []
  type: TYPE_NORMAL
  zh: Patrice Y. Simard1, Yann A. LeCun1, John S. Denker1 和 Bernard Victorri2 1 图像处理服务研究实验室，AT&T
    实验室 - 研究，100 Schulz Drive, Red Bank, NJ 07701-7033, 美国
- en: patrice@research.att.com http://www.research.att.com/info/patrice 2 CNRS, ELSAP,
    ENS, 1 rue Maurice Arnoux, F-92120 MONTROUGE, France Abstract. In pattern recognition,
    statistical modeling, or regression, the amount of data is a critical factor affecting
    the performance. If the amount of data and computational resources are unlimited,
    even trivial algorithms will converge to the optimal solution. However, in the
    practical case, given limited data and other resources, satisfactory performance
    requires sophisticated methods to regularize the problem by introducing *a priori*
    knowledge. Invariance of the output with respect to certain transformations of
    the input is a typical example of such *a priori* knowledge. In this chapter,
    we introduce the concept of tangent vectors, which compactly represent the essence
    of these transformation invariances, and two classes of algorithms, "tangent distance"
    and "tangent propagation", which make use of these invariances to improve performance.
  id: totrans-2955
  prefs: []
  type: TYPE_NORMAL
  zh: patrice@research.att.com http://www.research.att.com/info/patrice 2 CNRS, ELSAP,
    ENS, 1 rue Maurice Arnoux, F-92120 MONTROUGE, 法国 摘要。在模式识别、统计建模或回归中，数据量是影响性能的关键因素。如果数据量和计算资源是无限的，即使是微不足道的算法也会收敛到最优解。然而，在实际情况下，考虑到有限的数据和其他资源，令人满意的性能需要复杂的方法，通过引入*先验*知识来正则化问题。输出相对于输入某些变换的不变性是这种*先验*知识的典型例子。在本章中，我们引入切向量的概念，它紧凑地表示这些变换不变性的本质，以及两类算法：“切向距离”和“切向传播”，它们利用这些不变性来提高性能。
- en: 12.1 Introduction
  id: totrans-2956
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 引言
- en: Pattern Recognition is one of the main tasks of biological information processing
    systems, and a major challenge of computer science. The problem of pattern recognition
    is to classify objects into categories, given that objects in a particular category
    may have widely-varying features, while objects in different categories may have
    quite similar features. A typical example is handwritten digit recognition. Characters,
    typically represented as fixed-size images (say 16 by 16 pixels),
  id: totrans-2957
  prefs: []
  type: TYPE_NORMAL
  zh: 模式识别是生物信息处理系统的主要任务之一，也是计算机科学的一大挑战。模式识别的问题在于将对象分类，考虑到同一类别中的对象可能具有广泛不同的特征，而不同类别中的对象可能具有相似的特征。一个典型的例子是手写数字识别。字符通常表示为固定大小的图像（例如
    16 x 16 像素），
- en: must be classified into one of 10 categories using a *classification function*.
    Building such a classification function is a major technological challenge, as
    irrelevant variabilities among objects of the same class must be eliminated, while
    meaningful differences between objects of different classes must be identified.
    These classification functions for most real pattern recognition tasks are too
    complicated to be synthesized "by hand" using only what humans know about the
    task.
  id: totrans-2958
  prefs: []
  type: TYPE_NORMAL
  zh: 必须使用*分类函数*将其分类为 10 个类别之一。构建这样的分类函数是一个重大的技术挑战，因为同一类别对象之间的不相关变异必须被消除，而不同类别对象之间的有意义差异必须被识别。大多数真实模式识别任务的这些分类函数过于复杂，无法仅凭人类对任务的了解“手动”合成。
- en: Instead, we use sophisticated techniques that combine humans' *a priori* knowledge
    with information automatically extracted from a set of labeled examples
  id: totrans-2959
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们使用复杂的技术，将人类的*先验*知识与从一组标记示例中自动提取的信息相结合。
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-2960
  prefs: []
  type: TYPE_NORMAL
  zh: '- 先前发表在：Orr, G.B. 和 Müller, K.-R.（编）：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-2961
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0（1998）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    235–269, 2012.'
  id: totrans-2962
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon 等（编）：NN: 行业技巧，第 2 版，LNCS 7700，第 235-269 页，2012 年。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-2963
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: '(the training set). These techniques can be divided into two camps, according
    to the number of parameters they require: the "memory based" algorithms, which
    in effect store a sizeable subset of the entire training set, and the "learned-function"
    techniques, which learn by adjusting a comparatively small number of parameters.
    This distinction is arbitrary because the patterns stored by a memory-based algorithm
    can be considered the parameters of a very complex learned function.'
  id: totrans-2964
  prefs: []
  type: TYPE_NORMAL
  zh: （训练集）。这些技术可以根据所需参数的数量分为两个阵营：“基于记忆”的算法，它实际上存储了整个训练集的一个相当大的子集，以及“学习函数”技术，它通过调整相对少量的参数进行学习。这个区分是任意的，因为基于记忆的算法存储的模式可以视为非常复杂的学习函数的参数。
- en: The distinction is however useful in this work, because memory based algorithms
    often rely on a metric which can be modified to incorporate transformation invariances,
    while learned-function algorithms consist of selecting a classification function,
    the derivatives of which can be constrained to reflect the same transformation
    invariances. The two methods for incorporating invariances are different enough
    to justify two independent sections.
  id: totrans-2965
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在本研究中，这一区分是有用的，因为基于记忆的算法通常依赖于可以修改的度量，以纳入变换不变性，而学习函数算法则由选择分类函数组成，其导数可以被约束以反映相同的变换不变性。这两种纳入不变性的方法足够不同，因此有必要划分为两个独立的部分。
- en: 12.1.1 Memory Based Algorithms
  id: totrans-2966
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1.1 基于记忆的算法
- en: 'To compute the classification function, many practical pattern recognition
    systems, and several biological models, simply store all the examples, together
    with their labels, in a memory. Each incoming pattern can then be compared to
    all the stored prototypes, and the labels associated with the prototypes that
    best match the input determine the output. The above method is the simplest example
    of the *memory-based* models. Memory-based models require three things: a distance
    measure to compare inputs to prototypes, an *output function* to produce an output
    by combining the labels of the prototypes, and a *storage scheme* to build the
    set of prototypes.'
  id: totrans-2967
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算分类函数，许多实际的模式识别系统和若干生物模型，简单地将所有示例及其标签存储在内存中。每个输入模式随后可以与所有存储的原型进行比较，与输入最佳匹配的原型关联的标签确定输出。上述方法是*基于记忆*模型的最简单例子。基于记忆的模型需要三样东西：一个距离度量，用于将输入与原型进行比较，一个*输出函数*，通过组合原型的标签生成输出，以及一个*存储方案*，用于构建原型集。
- en: All three aspects have been abundantly treated in the literature. Output functions
    range from simply voting the labels associated with the k closest prototypes (K-Nearest
    Neighbors), to computing a score for each class as a linear combination of the
    distances to all the prototypes, using fixed [21] or learned [5]
  id: totrans-2968
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个方面在文献中已经得到了充分的探讨。输出函数范围从简单投票与 k 个最近原型相关联的标签（K-最近邻），到计算每个类别的得分，作为与所有原型的距离的线性组合，使用固定的[21]或学习得到的[5]。
- en: coefficients. Storage schemes vary from storing the entire training set, to
    picking appropriate subsets of it (see [8], chapter 6, for a survey) to learned-functions
    such as learning vector quantization (LVQ) [17] and gradient descent. Distance
    measures can be as simple as the Euclidean distance, assuming the patterns and
    prototypes are represented as vectors, or more complex as in the generalized quadratic
    metric [10] or in elastic matching methods [15].
  id: totrans-2969
  prefs: []
  type: TYPE_NORMAL
  zh: 系数。存储方案从存储整个训练集到选择合适的子集（见[8]，第6章的综述），再到学习函数，例如学习向量量化（LVQ）[17]和梯度下降。距离度量可以简单到欧几里得距离，假设模式和原型表示为向量，或者复杂到广义二次度量[10]或弹性匹配方法[15]。
- en: '![233_image_0.png](233_image_0.png)'
  id: totrans-2970
  prefs: []
  type: TYPE_IMG
  zh: '![233_image_0.png](233_image_0.png)'
- en: '![233_image_1.png](233_image_1.png)'
  id: totrans-2971
  prefs: []
  type: TYPE_IMG
  zh: '![233_image_1.png](233_image_1.png)'
- en: be classified **Prototype A Prototype B**
  id: totrans-2972
  prefs: []
  type: TYPE_NORMAL
  zh: 被分类 **原型 A 原型 B**
- en: Fig. 12.1. According to the Euclidean distance the pattern to be classified
    is more similar to prototype B. A better distance measure would find that prototype
    A is closer because it differs mainly by a rotation and a thickness transformation,
    two transformations which should leave the classification invariant.
  id: totrans-2973
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1。根据欧几里得距离，被分类的模式与原型 B 更为相似。一个更好的距离度量方法会发现原型 A 更近，因为它主要通过旋转和厚度变换来区分，这两种变换应该保持分类不变。
- en: A simple but inefficient pattern recognition method is to use a simple distance
    measure, such as Euclidean distance between vectors representing the raw input,
    combined with a very large set of prototypes. This method is inefficient because
    almost all possible instances of a category must be present in the prototype set.
    In the case of handwritten digit recognition, this means that digits of each class
    in all possible positions, sizes, angles, writing styles, line thicknesses, skews,
    etc... must be stored. In real situations, this approach leads to impractically
    large prototype sets or to mediocre recognition accuracy as illustrated in Figure
    12.1. An unlabeled image of a thick, slanted "9" must be classified by finding
    the closest prototype image out of two images representing respectively a thin,
    upright "9" and a thick, slanted"4". According to the Euclidean distance (sum
    of the squares of the pixel to pixel differences), the "4" is closer. The result
    is an incorrect classification. The classical way of dealing with this problem
    is to use a so-called feature extractor whose purpose is to compute a representation
    of the patterns that is minimally affected by transformations of the patterns
    that do not modify their category. For character recognition, the representation
    should be invariant with respect to position, size changes, slight rotations,
    distortions, or changes in line thickness. The design and implementation of feature
    extractors is the major bottleneck of building a pattern recognition system. For
    example, the problem illustrated in Figure 12.1 can be solved by deslanting and
    thinning the images.
  id: totrans-2974
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单但低效的模式识别方法是使用简单的距离度量，比如表示原始输入的向量之间的欧几里得距离，结合一个非常大的原型集。这种方法效率低下，因为几乎所有可能类别的实例必须存在于原型集中。在手写数字识别的情况下，这意味着必须存储每个类别在所有可能位置、大小、角度、书写风格、线条粗细、倾斜等的数字。在实际情况下，这种方法会导致原型集过于庞大，或者识别准确率平庸，如图12.1所示。一个厚实、倾斜的“9”的无标签图像必须通过找到最接近的原型图像来分类，而这两个原型图像分别代表一个细长、直立的“9”和一个厚实、倾斜的“4”。根据欧几里得距离（像素间差异的平方和），"4"更接近。结果是错误分类。处理这个问题的传统方法是使用所谓的特征提取器，其目的是计算一个模式的表示，这种表示最小限度地受那些不改变其类别的模式变换的影响。对于字符识别，表示应对位置、大小变化、轻微旋转、扭曲或线条粗细变化保持不变。特征提取器的设计和实现是构建模式识别系统的主要瓶颈。例如，图12.1中说明的问题可以通过去斜和细化图像来解决。
- en: An alternative to this is to use an invariant *distance measure* constructed
    in such a way that the distance between a prototype and a pattern will not be
    affected by irrelevant transformations of the pattern or of the prototype. With
    an invariant distance measure, each prototype can match many possible instances
    of pattern, thereby greatly reducing the number of prototypes required.
  id: totrans-2975
  prefs: []
  type: TYPE_NORMAL
  zh: 这种替代方案是使用一种不变的*距离度量*，以确保原型和模式之间的距离不受模式或原型的无关变换影响。使用不变的距离度量，每个原型可以匹配多种可能的模式实例，从而大大减少所需的原型数量。
- en: The natural way of doing this is to use "deformable" prototypes. During the
    matching process, each prototype is deformed so as to best fit the incoming pattern.
    The quality of the fit, possibly combined with a measure of the amount of deformation,
    is then used as the distance measure [15]. With the example of Figure 12.1, the
    "9" prototype would be rotated and thickened so as to best match the incoming
    "9". This approach has two shortcomings. First, a set of allowed deformations
    must be designed based on *a priori* knowledge. Fortunately, this is feasible
    for many tasks, including character recognition. Second, the search for the best-matching
    deformation is often enormously expensive, and/or unreliable. Consider the case
    of patterns that can be represented by vectors. For example, the pixel values
    of a 16 by 16 pixel character image can be viewed as the components of a 256-dimensional
    vector. One pattern, or one prototype, is a point in this 256-dimensional space.
    Assuming that the set of allowable transformations is continuous, the set of all
    the patterns that can be obtained by transforming one prototype using one or a
    combination of allowable transformations is a surface in the 256-D pixel space.
    More precisely, when a pattern P is transformed (e.g.
  id: totrans-2976
  prefs: []
  type: TYPE_NORMAL
  zh: 自然的做法是使用“可变形”原型。在匹配过程中，每个原型都会变形，以最佳地适应传入模式。拟合的质量，可能结合变形程度的量度，随后被用作距离度量[15]。以图12.1为例，“9”原型将被旋转并加粗，以最佳匹配传入的“9”。这种方法有两个缺点。首先，必须根据*先验*知识设计一组允许的变形。幸运的是，这对于许多任务，包括字符识别，是可行的。其次，寻找最佳匹配变形的过程通常代价昂贵，和/或不可靠。考虑可以用向量表示的模式的情况。例如，一个16乘16像素的字符图像的像素值可以视为一个256维向量的分量。一个模式或一个原型，是这个256维空间中的一个点。假设允许的变换集合是连续的，通过使用一个或多个允许的变换变换一个原型所能获得的所有模式的集合是在256维像素空间中的一个表面。更确切地说，当一个模式P被变换时（例如，
- en: rotated) according to a transformation s(*P, α*) which depends on one parameter
    α (e.g. the angle of the rotation), the set of all the transformed patterns
  id: totrans-2977
  prefs: []
  type: TYPE_NORMAL
  zh: 根据一个依赖于单个参数α（例如旋转角度）的变换s(*P, α*)进行旋转，所有变换后的模式集
- en: $$\mathbf{\partial}(\mathbf{k})\}$$
  id: totrans-2978
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbf{\partial}(\mathbf{k})\}$$
- en: SP = {x | ∃α for which x = s(*P, α*)} (12.1)
  id: totrans-2979
  prefs: []
  type: TYPE_NORMAL
  zh: SP = {x | ∃α使得x = s(*P, α*)} (12.1)
- en: is a one-dimensional curve in the vector space of the inputs. In the remainder
    of
  id: totrans-2980
  prefs: []
  type: TYPE_NORMAL
  zh: 是输入向量空间中的一维曲线。在其余部分
- en: this chapter, we will always assume that we have chosen s be differentiable
    with respect to both P and α and such that s(P, 0) = P.
  id: totrans-2981
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将始终假设选择s对P和α都是可微分的，并且满足s(P, 0) = P。
- en: When the set of transformations is parameterized by n parameters αi (rotation,
    translation, scaling, etc.), the intrinsic dimension of the manifold SP is
  id: totrans-2982
  prefs: []
  type: TYPE_NORMAL
  zh: 当变换集合由n个参数αi（旋转、平移、缩放等）进行参数化时，流形SP的内在维度是
- en: at most n. For example, if the allowable transformations of character images
  id: totrans-2983
  prefs: []
  type: TYPE_NORMAL
  zh: 最多为n。例如，如果字符图像的允许变换
- en: are horizontal and vertical shifts, rotations, and scaling, the surface will
    be a
  id: totrans-2984
  prefs: []
  type: TYPE_NORMAL
  zh: 是水平和垂直位移、旋转和缩放，表面将是一个
- en: 4-dimensional manifold.
  id: totrans-2985
  prefs: []
  type: TYPE_NORMAL
  zh: 4维流形。
- en: In general, the manifold will not be linear. Even a simple image translation
  id: totrans-2986
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，流形不会是线性的。即使是简单的图像平移
- en: corresponds to a highly non-linear transformation in the high-dimensional pixel
    space. For example, if the image of an "8" is translated upward, some pixels
  id: totrans-2987
  prefs: []
  type: TYPE_NORMAL
  zh: 对应于高维像素空间中的高度非线性变换。例如，如果一个"8"的图像向上平移，一些像素
- en: oscillate from white to black and back several times. Matching a deformable
  id: totrans-2988
  prefs: []
  type: TYPE_NORMAL
  zh: 在白色和黑色之间反复振荡。匹配一个可变形的
- en: prototype to an incoming pattern now amounts to finding the point on the surface
    that is at a minimum distance from the point representing the incoming pattern.
    This non-linearity makes the matching much more expensive and unreliable. Simple
    minimization methods such as gradient descent (or conjugate
  id: totrans-2989
  prefs: []
  type: TYPE_NORMAL
  zh: 将原型与传入模式匹配现在相当于找到表面上与代表传入模式的点的最小距离的点。这种非线性使得匹配变得更加昂贵和不可靠。像梯度下降（或共轭
- en: gradient) can be used to find the minimum-distance point, however, these methods
    only converge to a *local* minimum. In addition, running such an iterative
  id: totrans-2990
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度）可以用来找到最小距离点，然而，这些方法只会收敛到*局部*最小值。此外，运行这样的迭代
- en: procedure for each prototype is usually prohibitively expensive.
  id: totrans-2991
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个原型的程序通常是代价高昂的。
- en: If the set of transformations happens to be linear in pixel space, then the
  id: totrans-2992
  prefs: []
  type: TYPE_NORMAL
  zh: 如果变换集合在像素空间中恰好是线性的，那么
- en: manifold is a linear subspace (a hyperplane). The matching procedure is then
  id: totrans-2993
  prefs: []
  type: TYPE_NORMAL
  zh: 流形是线性子空间（超平面）。匹配过程随后为
- en: reduced to finding the shortest distance between a point (vector) and a hyperplane,
    which is an easy-to-solve quadratic minimization problem. This special
  id: totrans-2994
  prefs: []
  type: TYPE_NORMAL
  zh: 简化为寻找一个点（向量）与一个超平面之间的最短距离，这是一个易于解决的二次最小化问题。这种特殊的
- en: case has been studied in the statistical literature and is sometimes referred
    to as Procrustes analysis [24]. It has been applied to signature verification
    [12] and
  id: totrans-2995
  prefs: []
  type: TYPE_NORMAL
  zh: 此案例已在统计文献中研究，有时被称为Procrustes分析[24]。它已应用于签名验证[12]和
- en: on-line character recognition [26].
  id: totrans-2996
  prefs: []
  type: TYPE_NORMAL
  zh: 在线字符识别[26]。
- en: This chapter considers the more general case of non-linear transformations
  id: totrans-2997
  prefs: []
  type: TYPE_NORMAL
  zh: 本章考虑非线性变换的更一般情况。
- en: such as geometric transformations of gray-level images. Remember that even a
    simple image translation corresponds to a highly non-linear transformation in
  id: totrans-2998
  prefs: []
  type: TYPE_NORMAL
  zh: 例如灰度图像的几何变换。请记住，即使是简单的图像平移也对应于高度非线性的变换。
- en: the high-dimensional pixel space. The main idea of the chapter is to approximate
    the surface of possible transforms of a pattern by its tangent plane at
  id: totrans-2999
  prefs: []
  type: TYPE_NORMAL
  zh: 高维像素空间。本章的主要思想是通过在某一点的切平面来近似模式可能变换的表面。
- en: the pattern, thereby reducing the matching to finding the shortest distance
    between two planes. This distance is called the *tangent distance*. The result
    of the
  id: totrans-3000
  prefs: []
  type: TYPE_NORMAL
  zh: 模式，从而将匹配简化为寻找两个平面之间的最短距离。这个距离被称为*tangent distance*。该结果的
- en: approximation is shown in Figure 12.2, in the case of rotation for handwritten
    digits. At the top of the figure, is the theoretical curve in pixel space which
    represents equation (12.1), together with its linear approximation. Points of
    the transformation curve are depicted below for various amounts of rotation (each
    angle corresponds to a value of α). The bottom of Figure 12.2 depicts the linear
  id: totrans-3001
  prefs: []
  type: TYPE_NORMAL
  zh: 近似在图12.2中展示，针对手写数字的旋转情况。图的顶部是像素空间中的理论曲线，表示方程(12.1)，以及其线性近似。变换曲线的点在下方展示，对于不同的旋转量（每个角度对应一个α值）。图12.2的底部描绘了线性
- en: approximation of the curve s(*P, α*) given by the Taylor expansion of s around
  id: totrans-3002
  prefs: []
  type: TYPE_NORMAL
  zh: 由围绕s的泰勒展开给出的曲线s(*P, α*)的近似。
- en: 'α = 0:'
  id: totrans-3003
  prefs: []
  type: TYPE_NORMAL
  zh: α = 0：
- en: $$s(P,\alpha)=s(P,0)+\alpha{\frac{\partial s(P,\alpha)}{\partial\alpha}}+O(\alpha^{2})\approx
    P+\alpha T.$$
  id: totrans-3004
  prefs: []
  type: TYPE_NORMAL
  zh: $$s(P,\alpha)=s(P,0)+\alpha{\frac{\partial s(P,\alpha)}{\partial\alpha}}+O(\alpha^{2})\approx
    P+\alpha T.$$
- en: $$(12.2)^{\frac{1}{2}}$$
  id: totrans-3005
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.2)^{\frac{1}{2}}$$
- en: This linear approximation is completely characterized by the point P and the
  id: totrans-3006
  prefs: []
  type: TYPE_NORMAL
  zh: 该线性近似完全由点P和
- en: tangent vector T = ∂s(P,α)
  id: totrans-3007
  prefs: []
  type: TYPE_NORMAL
  zh: 切向量T = ∂s(P,α)
- en: ∂α . Tangent vectors, also called the Lie derivatives of
  id: totrans-3008
  prefs: []
  type: TYPE_NORMAL
  zh: ∂α。切向量，也称为Lie导数
- en: '![236_image_0.png](236_image_0.png)'
  id: totrans-3009
  prefs: []
  type: TYPE_IMG
  zh: '![236_image_0.png](236_image_0.png)'
- en: 'Fig. 12.2. Top: Representation of the effect of the rotation in pixel space.
    Middle: Small rotations of an original digitized image of the digit "2", for different
    angle values of α. Bottom: Images obtained by moving along the tangent to the
    transformation curve for the same original digitized image P by adding various
    amounts (α) of the tangent vector T .'
  id: totrans-3010
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2。顶部：像素空间中旋转的效果的表示。中间：原始数字化图像“2”的小角度旋转，对于不同的角度值α。底部：通过添加各种量（α）的切向量T，沿变换曲线的切线移动得到的图像。
- en: the transformation s, will be the subject of section 12.4. As can be seen from
    Figure 12.2, for reasonably small angles (α < 1), the approximation is very good.
  id: totrans-3011
  prefs: []
  type: TYPE_NORMAL
  zh: 变换s，将在12.4节中讨论。从图12.2可以看出，对于合理小的角度（α < 1），近似非常好。
- en: Figure 12.3 illustrates the difference between the Euclidean distance, the full
    invariant distance (minimum distance between manifolds) and the tangent distance.
    In the figure, both the prototype and the pattern are deformable (twosided distance),
    but for simplicity or efficiency reasons, it is also possible to deform only the
    prototype or only the unknown pattern (one-sided distance).
  id: totrans-3012
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3说明了欧几里得距离、全不变距离（流形之间的最小距离）和切向距离之间的区别。在图中，原型和模式都是可变形的（双向距离），但出于简化或效率的原因，也可以仅变形原型或仅变形未知模式（单向距离）。
- en: 'Although in the following we will concentrate on using tangent distance to
    recognize images, the method can be applied to many different types of signals:
    temporal signals, speech, sensor data...'
  id: totrans-3013
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管接下来我们将集中使用切向距离来识别图像，但该方法也可以应用于许多不同类型的信号：时间信号、语音、传感器数据等。
- en: '![237_image_0.png](237_image_0.png)'
  id: totrans-3014
  prefs: []
  type: TYPE_IMG
  zh: '![237_image_0.png](237_image_0.png)'
- en: Fig. 12.3. Illustration of the Euclidean distance and the tangent distance between
    P
  id: totrans-3015
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3. 欧几里得距离和 P 之间的切向距离的示意图
- en: and E. The curves Sp and Se represent the sets of points obtained by applying
    the chosen transformations (for example translations and rotations) to P and E.
    The lines going through P and E represent the tangent to these curves. Assuming
    that working space has more dimensions than the number of chosen transformations
    (on the diagram, assume one transformation in a 3-D space) the tangent spaces
    do not intersect and the tangent distance is uniquely defined.
  id: totrans-3016
  prefs: []
  type: TYPE_NORMAL
  zh: 和 E。曲线 Sp 和 Se 代表通过对 P 和 E 应用所选变换（例如平移和旋转）获得的点集。通过 P 和 E 的直线代表这些曲线的切线。假设工作空间的维度大于所选变换的数量（在图中，假设在三维空间中有一个变换），切空间不相交，切向距离是唯一定义的。
- en: 12.1.2 Learned-Function Algorithms
  id: totrans-3017
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1.2 学习函数算法
- en: Rather than trying to keep a representation of the training set, it is also
    possible to choose a classification function by learning a set of parameters.
    This is the approach taken in neural networks, curve fitting, regression, et cetera.
  id: totrans-3018
  prefs: []
  type: TYPE_NORMAL
  zh: 与其试图保持训练集的表示，还可以通过学习一组参数来选择分类函数。这是神经网络、曲线拟合、回归等所采取的方法。
- en: We assume all data is drawn independently from a given statistical distribution
    P, and our learning machine is characterized by the set of functions it can implement,
    Gw(x), indexed by the vector of parameters w. We write F(x)
  id: totrans-3019
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设所有数据独立于给定的统计分布 P 提取，我们的学习机器由它可以实现的函数集合 Gw(x) 表征，该集合由参数向量 w 索引。我们写 F(x)
- en: to represent the "correct" or "desired" labeling of the point x. The task is
    to find a value for w such that Gw best approximates F. We can use a finite set
    of training data to help find this vector. We assume the correct labeling F(x)
  id: totrans-3020
  prefs: []
  type: TYPE_NORMAL
  zh: 表示点 x 的“正确”或“期望”标记。任务是找到一个 w 的值，使得 Gw 最好地逼近 F。我们可以使用有限的训练数据来帮助找到这个向量。我们假设正确的标记为
    F(x)
- en: is known for all points in the training set. For example, Gw may be the function
    computed by a neural net having weights w, or Gw may be a polynomial having coefficients
    w. Without additional information, finding a value for w is an ill-posed problem
    unless the number of parameters is small and/or the size of the training set is
    large. This is because the training set does not provide enough information to
    distinguish the best solution among all the candidate ws. This problem is illustrated
    in Figure 12.4 (left). The desired function F (solid line) is to be approximated
    by a functions Gw (dotted line) from four examples {(xi, F(xi))}i=1,2,3,4. As
    exemplified in the picture, the fitted function Gw largely disagrees with the
    desired function F between the examples, but it is not possible to infer this
    from the training set alone. Many values of w can generate many different functions
    Gw, some of which may be terrible approximations of F, even though they are in
    complete agreement with the training set. Because of this, it is customary to
    add "regularizers", or additional constraints, to restrict the search of an acceptable
    w. For example, we may require the function Gw to be "smooth", by adding the constraint
    that w2 should be minimized. It is important that the regularizer reflects a property
    of F, hence regularizers depend on *a priori* knowledge about the function to
    be modeled.
  id: totrans-3021
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练集中的所有点都是已知的。例如，Gw 可能是一个具有权重 w 的神经网络计算的函数，或者 Gw 可能是具有系数 w 的多项式。如果没有额外信息，寻找
    w 的值是一个不适定的问题，除非参数的数量很小和/或训练集的大小很大。这是因为训练集提供的信息不足以区分所有候选 w 中的最佳解决方案。这个问题在图 12.4（左）中得到了说明。期望函数
    F（实线）需要通过四个例子 {(xi, F(xi))}i=1,2,3,4 来逼近函数 Gw（虚线）。如图所示，拟合函数 Gw 在示例之间与期望函数 F 大相径庭，但仅凭训练集无法推断这一点。许多
    w 的值可以生成许多不同的函数 Gw，其中一些可能是 F 的糟糕近似，即使它们与训练集完全一致。因此，通常会添加“正则化器”或附加约束，以限制可接受 w 的搜索。例如，我们可能要求函数
    Gw 是“光滑的”，通过增加 w2 应该被最小化的约束。正则化器反映了 F 的特性，因此正则化器依赖于对要建模的函数的 *a priori* 知识。
- en: '![238_image_0.png](238_image_0.png)'
  id: totrans-3022
  prefs: []
  type: TYPE_IMG
  zh: '![238_image_0.png](238_image_0.png)'
- en: 'Fig. 12.4. Learning a given function (solid line) from a limited set of examples
    (x1 to x4). The fitted curves are shown by dotted line. Left: The only constraint
    is that the fitted curve goes through the examples. Right: The fitted curves not
    only go through each example but also its derivatives evaluated at the examples
    agree with the derivatives of the given function.'
  id: totrans-3023
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4. 从有限的例子集（x1 到 x4）学习给定函数（实线）。拟合曲线由虚线表示。左侧：唯一的约束是拟合曲线通过这些例子。右侧：拟合曲线不仅通过每个例子，而且在例子处的导数也与给定函数的导数一致。
- en: Selecting a good family G = {Gw, w ∈ q} of functions is a difficult task, sometimes
    known as "model selection" [16, 14]. If G contains a large family of functions,
    it is more likely that it will contain a good approximation of F (the function
    we are trying to approximate), but it is also more likely that the selected candidate
    (using the training set) will generalize poorly because many functions in G will
    agree with the training data and take outrageous values between the training samples.
    If, on the other hand, G contains a small family of functions, it is more likely
    that a function Gw which fits the data will be a good approximation of F. The
    capacity of the family of functions G is often referred to as the VC dimension
    [28, 27]. If a large amount of data is available, G should contain a large family
    of functions (high VC dimension), so that more functions can be approximated,
    and in particular, F. If, on the other hand, the data is scarce, G
  id: totrans-3024
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个好的函数族 G = {Gw, w ∈ q} 是一项困难的任务，有时被称为“模型选择”[16, 14]。如果 G 包含一个大规模的函数族，它更有可能包含
    F（我们试图逼近的函数）的良好近似，但也更可能导致所选候选函数（使用训练集）泛化效果差，因为 G 中的许多函数会与训练数据一致，并在训练样本之间取非常大的值。另一方面，如果
    G 包含一个小规模的函数族，那么适合数据的函数 Gw 更有可能是 F 的良好近似。函数族 G 的容量通常被称为 VC 维度[28, 27]。如果可用数据量很大，G
    应该包含一个大规模的函数族（高 VC 维度），以便能够逼近更多的函数，特别是 F。如果数据稀缺，G
- en: should be restricted to a small family of functions (low VC dimension), to control
    the values between the (more distant) samples1. The VC dimension can also be
  id: totrans-3025
  prefs: []
  type: TYPE_NORMAL
  zh: 应该限制在一个小规模的函数族（低 VC 维度）中，以控制（较远）样本之间的值1。VC 维度也可以
- en: 1 Note that this point of view also applies to memory based systems. In the
    case where all the training data can be kept in memory, however, the VC dimension
    is infinite, and the formalism is meaningless. The VC dimension is a learning
    paradigm and is not useful unless learning is involved.
  id: totrans-3026
  prefs: []
  type: TYPE_NORMAL
  zh: 1 请注意，这种观点也适用于基于记忆的系统。然而，在所有训练数据都可以保存在内存中的情况下，VC 维度是无限的，形式化是没有意义的。VC 维度是一种学习范式，除非涉及学习，否则没有用。
- en: controlled by putting a knob on how much effect is given to some regularizers.
  id: totrans-3027
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调节某些正则化器的效果强度来进行控制。
- en: For instance it is possible to control the capacity of a neural network by adding
    "weight decay" as a regularizer. Weight decay is a heuristic that favors smooth
    classification functions, by making a tradeoff by decreasing w2 at the cost, usually,
    of slightly increased error on the training set. Since the optimal classification
    function is not necessarily smooth, for instance at a decision boundary, the weight
    decay regularizer can have adverse effects.
  id: totrans-3028
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，可以通过添加“权重衰减”作为正则化器来控制神经网络的容量。权重衰减是一种启发式方法，它通过降低 w² 来偏好平滑的分类函数，通常以稍微增加训练集上的错误为代价。由于最优分类函数在决策边界处不一定平滑，因此权重衰减正则化器可能会产生不利影响。
- en: As mentioned earlier, the regularizer should reflect interesting properties
    (a priori knowledge) of the function to be learned. If the functions F and Gw
    are assumed to be differentiable, which is generally the case, the search for
    Gw can be greatly improved by requiring that Gw's derivatives evaluated at the
    points {xi}
  id: totrans-3029
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，正则化器应反映要学习的函数的有趣属性（先验知识）。如果假设函数 F 和 Gw 可微分，这通常是情况，那么通过要求 Gw 在点 {xi} 处的导数，可以大大改善对
    Gw 的搜索。
- en: are more or less equal (this is the regularizer knob) to the derivatives of
    F at the same points (Figure 12.4 right). This result can be extended to multidimensional
    inputs. In this case, we can impose the equality of the derivatives of F and Gw
    in *certain directions*, not necessarily in all directions of the input space.
    Such constraints find immediate use in traditional pattern recognition problems.
    It is often the case that *a priori* knowledge is available on how the desired
    function varies with respect to some transformations of the input. It is straightforward
    to derive the corresponding constraint on the directional derivatives of the fitted
    function Gw in the directions of the transformations (previously named tangent
    vectors). Typical examples can be found in pattern recognition where the desired
    classification function is known to be invariant with respect to some transformation
    of the input such as translation, rotation, scaling, etc., in other words, the
    directional derivatives of the classification function in the directions of these
    transformations is zero.
  id: totrans-3030
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种程度上，这与同一点处F的导数（图12.4右侧）是相等的（这就是正则化器的调节旋钮）。这个结果可以扩展到多维输入。在这种情况下，我们可以在*某些方向*上施加F和Gw导数的相等性，而不必在输入空间的所有方向上。这样的约束在传统模式识别问题中有着直接的应用。通常情况下，*先验*知识可以用来了解所需函数相对于输入某些变换的变化方式。推导出拟合函数Gw在这些变换方向上的方向导数的相应约束是非常简单的（先前称之为切向量）。在模式识别中，典型的例子是所需的分类函数已知对某些输入变换（如平移、旋转、缩放等）是保持不变的；换句话说，这些变换方向上的分类函数的方向导数为零。
- en: This is illustrated in Figure 12.4. The right part of the figure shows how the
    additional constraints on Gw help generalization by constraining the values of
    Gw outside the training set. For every transformation which has a known effect
    on the classification function, a regularizer can be added in the form of a constraint
    on the directional derivative of Gw in the direction of the tangent vector (such
    as the one depicted in Figure 12.2), computed from the curve of transformation.
  id: totrans-3031
  prefs: []
  type: TYPE_NORMAL
  zh: 这在图12.4中得到了说明。图的右侧部分显示了对Gw施加额外约束如何通过限制训练集之外Gw的值来帮助泛化。对于每个已知对分类函数有影响的变换，可以以对Gw的切向量方向导数的约束形式添加正则化器（如图12.2所示的那种），该导数是从变换曲线计算得出的。
- en: The next section will analyze in detail how to use a distance based on tangent
    vector in memory based algorithms. The subsequent section will discuss the use
    of tangent vectors in neural network, with the tangent propagation algorithm.
    The last section will compare different algorithms to compute tangent vectors.
  id: totrans-3032
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将详细分析如何在基于记忆的算法中使用基于切向量的距离。后续部分将讨论切向量在神经网络中的应用，特别是切向传播算法。最后一节将比较计算切向量的不同算法。
- en: 12.2 Tangent Distance
  id: totrans-3033
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 切向距离
- en: The Euclidean distance between two patterns P and E is in general not appropriate
    because it is sensitive to irrelevant transformations of P and of E. In contrast,
    the transformed distance D(E,P) is defined to be the minimal distance between
    the two manifolds SP and SE, and is therefore invariant with respect to the transformation
    used to generate SP and SE (see Figure 12.3). Unfortunately, these manifolds have
    no analytic expression in general, and finding the distance between them is a
    difficult optimization problem with multiple local minima.
  id: totrans-3034
  prefs: []
  type: TYPE_NORMAL
  zh: 两个模式P和E之间的欧几里得距离通常不适用，因为它对P和E的无关变换非常敏感。相反，转化后的距离D(E,P)被定义为两个流形SP和SE之间的最小距离，因此对于生成SP和SE所使用的变换是保持不变的（见图12.3）。不幸的是，这些流形通常没有解析表达式，找到它们之间的距离是一个具有多个局部最小值的困难优化问题。
- en: Besides, true invariance is not necessarily desirable since a rotation of a
    "6" into a "9" does not preserve the correct classification.
  id: totrans-3035
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，真正的不变性并不一定是可取的，因为将“6”旋转为“9”并没有保持正确的分类。
- en: 'Our approach consists of computing the minimum distance between the linear
    surfaces that best approximate the non-linear manifolds SP and SE. This solves
    three problems at once: 1) linear manifolds have simple analytical expressions
    which can be easily computed and stored, 2) finding the minimum distance between
    linear manifolds is a simple least squares problem which can be solved efficiently
    and, 3) this distance is locally invariant but not globally invariant. Thus the
    distance between a "6" and a slightly rotated "6" is small but the distance between
    a "6" and a "9" is large. The different distances between P and E are represented
    schematically in Figure 12.3.'
  id: totrans-3036
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法包括计算最佳近似非线性流形 SP 和 SE 的线性表面之间的最小距离。这同时解决了三个问题：1）线性流形具有简单的解析表达式，易于计算和存储，2）寻找线性流形之间的最小距离是一个简单的最小二乘问题，可以高效求解，3）该距离在局部是不变的，但在全局上是不变的。因此，"6"
    和稍微旋转的 "6" 之间的距离较小，但 "6" 和 "9" 之间的距离较大。P 和 E 之间的不同距离在图 12.3 中以示意图表示。
- en: The figure represents two patterns P and E in 3-dimensional space. The manifolds
    generated by s are represented by one-dimensional curves going through E
  id: totrans-3037
  prefs: []
  type: TYPE_NORMAL
  zh: 图中表示了三维空间中的两个模式 P 和 E。由 s 生成的流形由经过 E 的一维曲线表示。
- en: and P respectively. The linear approximations to the manifolds are represented
    by lines tangent to the curves at E and P. These lines do not intersect in 3 dimensions
    and the shortest distance between them (uniquely defined) is D(E,P).
  id: totrans-3038
  prefs: []
  type: TYPE_NORMAL
  zh: 和 P 分别。流形的线性近似由在 E 和 P 处切于曲线的切线表示。这些切线在三维空间中不相交，它们之间的最短距离（唯一定义）为 D(E,P)。
- en: The distance between the two non-linear transformation curves D(E,P) is also
    shown on the figure.
  id: totrans-3039
  prefs: []
  type: TYPE_NORMAL
  zh: 两个非线性变换曲线 D(E,P) 之间的距离也在图中显示。
- en: An efficient implementation of the tangent distance D(E,P) will be given in
    the next section, using image recognition as an illustration. We then compare
    our methods with the best known competing methods. Finally we will discuss possible
    variations on the tangent distance and how it can be generalized to problems other
    than pattern recognition.
  id: totrans-3040
  prefs: []
  type: TYPE_NORMAL
  zh: 下节将给出切线距离 D(E,P) 的有效实现，使用图像识别作为示例。然后，我们将我们的方法与已知的最佳竞争方法进行比较。最后，我们将讨论切线距离的可能变体，以及如何将其推广到模式识别以外的问题。
- en: 12.2.1 Implementation
  id: totrans-3041
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2.1 实现
- en: In this section we describe formally the computation of the tangent distance.
    Let the function s transform an image P to s(*P, α*) according to the parameter
    α. We require s to be differentiable with respect to α and P, and require s(P,
    0) = P. If P is a 2 dimensional image for instance, s(*P, α*) could be a rotation
    of P by the angle α. If we are interested in all transformations of images which
    conserve distances (isometry), s(*P, α*) would be a rotation by αθ followed by
    a translation by αx, αy of the image P. In this case α = (αθ, αx, αy) is a vector
    of parameters of dimension 3. In general, α = (α1*,...,α*m) is of dimension m.
  id: totrans-3042
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们正式描述切线距离的计算。设函数 s 根据参数 α 将图像 P 转换为 s(*P, α*)。我们要求 s 对 α 和 P 可微，并要求 s(P,
    0) = P。如果 P 是一个二维图像，则 s(*P, α*) 可以是 P 旋转角度 α。如果我们对所有保留距离的图像变换（等距变换）感兴趣，则 s(*P,
    α*) 将是对 P 旋转 αθ 后再平移 αx, αy。在这种情况下，α = (αθ, αx, αy) 是一个三维参数向量。一般而言，α = (α1*,...,α*m)
    的维度为 m。
- en: Since s is differentiable, the set SP = {x | ∃α for which x = s(*P, α*)} is
    a differentiable manifold which can be approximated to the first order by a hyperplane
    TP . This hyperplane is tangent to SP at P and is generated by the columns of
    matrix
  id: totrans-3043
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 s 可微，集合 SP = {x | ∃α 使得 x = s(*P, α*)} 是一个可微流形，可以通过超平面 TP 一阶近似。该超平面在 P 处与
    SP 切于，并由矩阵的列生成。
- en: $$L_{P}=\left.{\frac{\partial s(P,\alpha)}{\partial\alpha}}\right|_{\alpha={\bf0}}=\left[{\frac{\partial
    s(P,\alpha)}{\partial\alpha_{1}}},\ldots,{\frac{\partial s(P,\alpha)}{\partial\alpha_{m}}}\right]_{\alpha={\bf0}}$$
  id: totrans-3044
  prefs: []
  type: TYPE_NORMAL
  zh: $$L_{P}=\left.{\frac{\partial s(P,\alpha)}{\partial\alpha}}\right|_{\alpha={\bf0}}=\left[{\frac{\partial
    s(P,\alpha)}{\partial\alpha_{1}}},\ldots,{\frac{\partial s(P,\alpha)}{\partial\alpha_{m}}}\right]_{\alpha={\bf0}}$$
- en: $$(12.3)$$
  id: totrans-3045
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.3)$$
- en: which are vectors tangent to the manifold. If E and P are two patterns to be
    compared, the respective tangent planes TE and TP can be used to define a new
    distance D between these two patterns. The tangent distance D(E,P) between E and
    P is defined by
  id: totrans-3046
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是与流形切的向量。如果 E 和 P 是要比较的两个模式，则各自的切平面 TE 和 TP 可以用来定义这两个模式之间的新距离 D。E 和 P 之间的切线距离
    D(E,P) 定义为
- en: $$D(E,P)=\operatorname*{min}_{x\in T_{E},y\in T_{P}}\|x-y\|^{2}.$$
  id: totrans-3047
  prefs: []
  type: TYPE_NORMAL
  zh: $$D(E,P)=\operatorname*{min}_{x\in T_{E},y\in T_{P}}\|x-y\|^{2}.$$
- en: x − y2. (12.4)
  id: totrans-3048
  prefs: []
  type: TYPE_NORMAL
  zh: x − y². (12.4)
- en: 'The equation of the tangent planes TE and TP is given by:'
  id: totrans-3049
  prefs: []
  type: TYPE_NORMAL
  zh: 切平面TE和TP的方程为：
- en: $$\begin{array}{l l}{{E^{\prime}(\alpha_{E})\;=\;E+L_{E}\alpha_{E}}}\\ {{P^{\prime}(\alpha_{P})\;=\;P+L_{P}\alpha_{P}}}\end{array}$$
  id: totrans-3050
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{l l}{{E^{\prime}(\alpha_{E})\;=\;E+L_{E}\alpha_{E}}}\\ {{P^{\prime}(\alpha_{P})\;=\;P+L_{P}\alpha_{P}}}\end{array}$$
- en: where LE and LP are the matrices containing the tangent vectors (see equation
    (12.3)) and the vectors αE and αP are the coordinates of E and P (using bases
    LE and LP ) in the corresponding tangent planes. Note that E, E, LE
  id: totrans-3051
  prefs: []
  type: TYPE_NORMAL
  zh: 其中LE和LP是包含切向量的矩阵（见方程（12.3）），向量αE和αP是E和P在相应切平面中的坐标（使用基LE和LP）。注意E, E, LE
- en: and αE denote vectors and matrices in linear equations (12.5). For example,
    if the pixel space was of dimension 5, and there were two tangent vectors, we
    could rewrite equation (12.5) as
  id: totrans-3052
  prefs: []
  type: TYPE_NORMAL
  zh: αE表示线性方程（12.5）中的向量和矩阵。例如，如果像素空间的维度为5，并且有两个切向量，我们可以将方程（12.5）重写为
- en: $\begin{bmatrix}E'_1\\ E'_2\\ E'_3\\ E'_4\\ E'_5\end{bmatrix}\;=\;\begin{bmatrix}E_1\\
    E_2\\ E_3\\ E_4\\ E_5\end{bmatrix}\;+\;\begin{bmatrix}\\ \\ \\ \end{bmatrix}$  and
    $L_P\text{are attributes.}$
  id: totrans-3053
  prefs: []
  type: TYPE_NORMAL
  zh: $\begin{bmatrix}E'_1\\ E'_2\\ E'_3\\ E'_4\\ E'_5\end{bmatrix}\;=\;\begin{bmatrix}E_1\\
    E_2\\ E_3\\ E_4\\ E_5\end{bmatrix}\;+\;\begin{bmatrix}\\ \\ \\ \end{bmatrix}$  且$L_P$是属性。
- en: $\begin{array}{l}\begin{array}{*{20}{c}}{L_{11}}&{L_{12}}\end{array}\\ {L_{21}}&{L_{22}}\end{array}\\
    {L_{31}}&{L_{32}}\\ {L_{41}}&{L_{42}}\end{array}\left[\begin{array}{l}{\alpha_1}\\
    {\alpha_2}\end{array}\right].\\ {L_{51}}&{L_{52}}\end{array}$  of the patterns
    so in many case.
  id: totrans-3054
  prefs: []
  type: TYPE_NORMAL
  zh: $\begin{array}{l}\begin{array}{*{20}{c}}{L_{11}}&{L_{12}}\end{array}\\ {L_{21}}&{L_{22}}\end{array}\\
    {L_{31}}&{L_{32}}\\ {L_{41}}&{L_{42}}\end{array}\left[\begin{array}{l}{\alpha_1}\\
    {\alpha_2}\end{array}\right].\\ {L_{51}}&{L_{52}}\end{array}$  的模式，因此在许多情况下。
- en: $$(12.4)$$
  id: totrans-3055
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.4)$$
- en: $$(12.5)$$ $$(12.6)$$
  id: totrans-3056
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.5)$$ $$(12.6)$$
- en: $$(12.7)$$
  id: totrans-3057
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.7)$$
- en: $$(12.8)$$
  id: totrans-3058
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.8)$$
- en: $$(12.9)$$
  id: totrans-3059
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.9)$$
- en: $$(12.10)$$
  id: totrans-3060
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.10)$$
- en: . (12.7)
  id: totrans-3061
  prefs: []
  type: TYPE_NORMAL
  zh: . (12.7)
- en: The quantities LE and LP are attributes of the patterns so in many cases they
    can be precomputed and stored.
  id: totrans-3062
  prefs: []
  type: TYPE_NORMAL
  zh: 量LE和LP是模式的属性，因此在许多情况下它们可以预先计算并存储。
- en: Computing the tangent distance
  id: totrans-3063
  prefs: []
  type: TYPE_NORMAL
  zh: 计算切向距离
- en: D(E,P) = min αE,αP E(αE) − P(αP )2 (12.8)
  id: totrans-3064
  prefs: []
  type: TYPE_NORMAL
  zh: D(E,P) = min αE,αP E(αE) − P(αP )² (12.8)
- en: 'amounts to solving a linear least squares problem. The optimality condition
    is that the partial derivatives of D(E,P) with respect to αP and αE should be
    zero:'
  id: totrans-3065
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于解决一个线性最小二乘问题。最优性条件是D(E,P)关于αP和αE的偏导数应为零：
- en: $$\frac{\partial D(E,P)}{\partial\alpha_{E}}=2(E^{\prime}(\alpha_{E})-P^{\prime}(\alpha_{P}))^{\top}L_{E}=0$$
    $$\frac{\partial D(E,P)}{\partial\alpha_{P}}=2(P^{\prime}(\alpha_{P})-E^{\prime}(\alpha_{E}))^{\top}L_{P}=0.\tag{1}$$
  id: totrans-3066
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{\partial D(E,P)}{\partial\alpha_{E}}=2(E^{\prime}(\alpha_{E})-P^{\prime}(\alpha_{P}))^{\top}L_{E}=0$$
    $$\frac{\partial D(E,P)}{\partial\alpha_{P}}=2(P^{\prime}(\alpha_{P})-E^{\prime}(\alpha_{E}))^{\top}L_{P}=0.\tag{1}$$
- en: 'Substituting E and P by their expressions yields to the following linear system
    of equations, which we must solve for αP and αE :'
  id: totrans-3067
  prefs: []
  type: TYPE_NORMAL
  zh: 用E和P的表达式替换得到以下线性方程组，我们必须为αP和αE求解：
- en: $$\begin{array}{l}{{L_{P}^{\top}(E-P-L_{P}\alpha_{P}+L_{E}\alpha_{E})=0}}\\
    {{L_{E}^{\top}(E-P-L_{P}\alpha_{P}+L_{E}\alpha_{E})=0.}}\end{array}$$
  id: totrans-3068
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{l}{{L_{P}^{\top}(E-P-L_{P}\alpha_{P}+L_{E}\alpha_{E})=0}}\\
    {{L_{E}^{\top}(E-P-L_{P}\alpha_{P}+L_{E}\alpha_{E})=0.}}\end{array}$$
- en: $$(12.11)$$ $$(12.12)$$
  id: totrans-3069
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.11)$$ $$(12.12)$$
- en: $$(12.13)$$ $$(12.14)$$
  id: totrans-3070
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.13)$$ $$(12.14)$$
- en: The solution of this system is
  id: totrans-3071
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统的解为
- en: $$(L_{PE}L_{EE}^{-1}L_{E}^{\top}-L_{P}^{\top})(E-P)=(L_{PE}L_{EE}^{-1}L_{EP}-L_{PP})\alpha_{P}\tag{12.13}$$
    $$(L_{EP}L_{PP}^{-1}L_{P}^{\top}-L_{E}^{\top})(E-P)=(L_{EE}-L_{EP}L_{PP}^{-1}L_{PP})\alpha_{E}\tag{12.14}$$  where
    $L_{EE}=L_{E}^{\top}L_{E}$, $L_{PE}=L_{P}^{\top}L_{E}$, $L_{EP}=L_{E}^{\top}L_{P}$
    and $L_{PP}=L_{P}^{\top}L_{P}$. LU
  id: totrans-3072
  prefs: []
  type: TYPE_NORMAL
  zh: $$(L_{PE}L_{EE}^{-1}L_{E}^{\top}-L_{P}^{\top})(E-P)=(L_{PE}L_{EE}^{-1}L_{EP}-L_{PP})\alpha_{P}\tag{12.13}$$
    $$(L_{EP}L_{PP}^{-1}L_{P}^{\top}-L_{E}^{\top})(E-P)=(L_{EE}-L_{EP}L_{PP}^{-1}L_{PP})\alpha_{E}\tag{12.14}$$
    其中$L_{EE}=L_{E}^{\top}L_{E}$，$L_{PE}=L_{P}^{\top}L_{E}$，$L_{EP}=L_{E}^{\top}L_{P}$和$L_{PP}=L_{P}^{\top}L_{P}$。LU
- en: decompositions of LEE and LP P can be precomputed. The most expensive part
  id: totrans-3073
  prefs: []
  type: TYPE_NORMAL
  zh: LEE和LPP的分解可以预先计算。最耗时的部分
- en: in solving this system is evaluating LEP (LP E can be obtained by transposing
    LEP ). It requires mE × mP dot products, where mE is the number of tangent vectors
    for E and mP is the number of tangent vectors for P. Once LEP has been computed,
    αP and αE can be computed by solving two (small) linear systems of respectively
    mE and mP equations. The tangent distance is obtained by computing E(αE)−P(αP
    ) using the value of αP and αE in equations (12.5) and
  id: totrans-3074
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决这个系统时，评估LEP是最耗时的（LP E可以通过转置LEP获得）。这需要mE × mP的点积，其中mE是E的切向量数量，mP是P的切向量数量。一旦计算出LEP，αP和αE可以通过解决两个（小）线性方程组分别计算mE和mP个方程。切向距离通过使用方程（12.5）中的αP和αE值计算E(αE)−P(αP
    )获得。
- en: (12.6). If n is the dimension of the input space (i.e. the length of vector
    E and P),
  id: totrans-3075
  prefs: []
  type: TYPE_NORMAL
  zh: (12.6)。如果 n 是输入空间的维度（即向量 E 和 P 的长度），
- en: the algorithm described above requires roughly n(mE +1)(mP +1)+3(m3E +m3P )
  id: totrans-3076
  prefs: []
  type: TYPE_NORMAL
  zh: 上述算法大约需要 n(mE +1)(mP +1)+3(m3E +m3P )
- en: multiply-adds. Approximations to the tangent distance can however be computed
    more efficiently.
  id: totrans-3077
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法-加法。不过，可以更有效地计算切向距离的近似值。
- en: 12.2.2 Some Illustrative Results
  id: totrans-3078
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2.2 一些说明性结果
- en: Local Invariance. The "local2 invariance" of tangent distance can be illustrated
    by transforming a reference image by various amounts and measuring its distance
    to a set of prototypes.
  id: totrans-3079
  prefs: []
  type: TYPE_NORMAL
  zh: 局部不变性。切向距离的“局部不变性”可以通过以不同量变换参考图像并测量其与一组原型的距离来说明。
- en: The bottom of Figure 12.5 shows 10 typical handwritten digit images. One of
    them - the digit "3" - is chosen to be the reference. The reference is translated
    horizontally by the amount indicated in the abscissa. There are ten curves for
    Euclidean distance and ten more curves for tangent distance, measuring the distance
    between the translated reference and one of the 10 digits.
  id: totrans-3080
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5的底部显示了10个典型的手写数字图像。其中一个——数字“3”——被选择作为参考。参考图像水平平移，平移量在横坐标中指示。对于欧几里得距离有十条曲线，对于切向距离也有十条曲线，测量平移参考与10个数字之一之间的距离。
- en: Since the reference was chosen from the 10 digits, it is not surprising that
    the curve corresponding to the digit "3" goes to 0 when the reference is not translated
    (0 pixel translation). It is clear from the figure that if the reference (the
    image "3") is translated by more than 2 pixels, the Euclidean distance will confuse
    it with other digits, namely "8" or "5". In contrast, there is no possible confusion
    when tangent distance is used. As a matter of fact, in this example, the tangent
    distance correctly identifies the reference up to a translation of 5 pixels! Similar
    curves were obtained with all the other transformations (rotation, scaling, etc...).
  id: totrans-3081
  prefs: []
  type: TYPE_NORMAL
  zh: 由于参考是从10个数字中选择的，因此当参考未被平移（0 像素平移）时，数字“3”对应的曲线下降到0并不令人惊讶。从图中可以清楚地看出，如果参考（图像“3”）平移超过2个像素，欧几里得距离会将其与其他数字混淆，即“8”或“5”。相反，使用切向距离时没有可能的混淆。事实上，在这个例子中，切向距离在平移达到5个像素时仍然正确识别参考！所有其他变换（旋转、缩放等）也获得了类似的曲线。
- en: The "local" invariance of tangent distance with respect to small transformations
    generally implies more accurate classification for much larger transformations.
    This is the single most important feature of tangent distance.
  id: totrans-3082
  prefs: []
  type: TYPE_NORMAL
  zh: 切向距离对于小变换的“局部”不变性通常意味着对于更大变换的更准确分类。这是切向距离最重要的特性。
- en: 'The locality of the invariance has another important benefit: Local invariance
    can be enforced with *very few* tangent vectors. The reason is that for infinitesimal
    (local) transformations, there is a direct correspondence3 between the tangent
    vectors of the tangent plane and the various compositions of transformations.
    For example, the three tangent vectors for X-translation, Y-translation and'
  id: totrans-3083
  prefs: []
  type: TYPE_NORMAL
  zh: 不变性的局部性还有另一个重要好处：可以用*很少的*切向向量来强制执行局部不变性。原因在于，对于无穷小（局部）变换，切向平面上的切向量与各种变换组合之间存在直接对应关系3。例如，X方向平移、Y方向平移的三个切向量以及
- en: 2 Local invariance refers to invariance with respect to small transformations
    (i.e. a rotation of a very small angle). In contrast, global invariance refers
    to invariance with respect to arbitrarily large transformations (i.e. a rotation
    of 180 degrees). Global invariance is not desirable in digit recognition, since
    we need to distinguish
  id: totrans-3084
  prefs: []
  type: TYPE_NORMAL
  zh: 2 局部不变性是指对小变换的不变性（即非常小角度的旋转）。相对而言，全局不变性是指对任意大变换的不变性（即180度的旋转）。在数字识别中，全局不变性并不是理想的，因为我们需要区分
- en: '"6" from a "9". 3 An isomorphism actually, see "Lie algebra" in [6].'
  id: totrans-3085
  prefs: []
  type: TYPE_NORMAL
  zh: 从“9”得出“6”。3 实际上是一个同构，参见“李代数”在[6]中。
- en: '![243_image_0.png](243_image_0.png)'
  id: totrans-3086
  prefs: []
  type: TYPE_IMG
  zh: '![243_image_0.png](243_image_0.png)'
- en: Fig. 12.5. Euclidean and tangent distances between 10 typical images of handwritten
    digits and a translated image of the digit "3". The abscissa represents the amount
    of horizontal translation (measured in pixels).
  id: totrans-3087
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5。手写数字的10个典型图像与数字“3”的平移图像之间的欧几里得距离和切向距离。横坐标表示水平平移的量（以像素为单位）。
- en: rotations around the origin, generate a tangent plane corresponding to all the
    possible compositions of horizontal translations, vertical translations and rotations.
    The resulting tangent distance is then locally invariant to all the translations
    and all the rotations (around any center). Figure 12.6 further illustrates this
    phenomenon by displaying points in the tangent plane generated from only 5 tangent
    vectors. Each of these images looks like it has been obtained by applying various
    combinations of scaling, rotation, horizontal and vertical skewing, and thickening.
    Yet, the tangent distance between any of these points and the original image is
    0.
  id: totrans-3088
  prefs: []
  type: TYPE_NORMAL
  zh: 围绕原点的旋转生成了一个切平面，对应于所有可能的水平平移、垂直平移和旋转的组合。结果的切距在局部上对所有的平移和旋转（围绕任何中心）都是不变的。图12.6进一步通过显示仅由5个切向量生成的切平面中的点来说明这一现象。每个图像看起来似乎是通过应用各种缩放、旋转、水平和垂直倾斜及加粗的组合获得的。然而，这些点与原始图像之间的切距为0。
- en: 'Handwritten Digit Recognition. Experiments were conducted to evaluate the performance
    of tangent distance for handwritten digit recognition. An interesting characteristic
    of digit images is that we can readily identify a set of local transformations
    which do not affect the identity of the character, while covering a large portion
    of the set of possible *instances* of the character. Seven such image transformations
    were identified: X and Y translations, rotation, scaling, two hyperbolic transformations
    (which can generate shearing and squeezing), and line thickening or thinning.
    The first six transformations were chosen to span the set of all possible linear
    coordinate transforms in the image plane. (Nevertheless, they correspond to highly
    non-linear transforms in pixel space.) Additional'
  id: totrans-3089
  prefs: []
  type: TYPE_NORMAL
  zh: 手写数字识别。进行实验以评估切距在手写数字识别中的表现。数字图像的一个有趣特征是，我们可以很容易地识别出一组不会影响字符身份的局部变换，同时覆盖字符所有可能的*实例*的大部分。这些图像变换包括：X和Y平移、旋转、缩放、两个双曲变换（可以生成剪切和挤压），以及线条加粗或细化。前六个变换被选为覆盖图像平面中所有可能的线性坐标变换集合。（然而，它们对应于像素空间中的高度非线性变换。）额外
- en: '![244_image_0.png](244_image_0.png)'
  id: totrans-3090
  prefs: []
  type: TYPE_IMG
  zh: '![244_image_0.png](244_image_0.png)'
- en: 'Fig. 12.6. Left: Original image. Middle: 5 tangent vectors corresponding respectively
    to the 5 transformations: scaling, rotation, expansion of the X axis while compressing
    the Y axis, expansion of the first diagonal while compressing the second diagonal
    and thickening. Right: 32 points in the tangent space generated by adding or subtracting
    each of the 5 tangent vectors.'
  id: totrans-3091
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6。左：原始图像。中：分别对应于5个变换的5个切向量：缩放、旋转、在压缩Y轴的同时扩展X轴、在压缩第二对角线的同时扩展第一对角线和加粗。右：通过加或减去每个切向量生成的切空间中的32个点。
- en: 'transformations have been tried with less success. Three databases were used
    to test our algorithm:'
  id: totrans-3092
  prefs: []
  type: TYPE_NORMAL
  zh: 变换的尝试效果较差。我们使用了三个数据库来测试我们的算法：
- en: 'US Postal Service Database: The database consisted of 16 × 16 pixel sizenormalized
    images of handwritten digits, coming from US mail envelopes. The training and
    testing set had respectively 9709 and 2007 examples. NIST1 Database: The second
    experiment was a competition organized by the National Institute of Standards
    and Technology (NIST) in Spring 1992. The object of the competition was to classify
    a test set of 59,000 handwritten digits, given a training set of 223,000 patterns.
    NIST2 Database: The third experiment was performed on a database made out of the
    training and testing database provided by NIST (see above). NIST'
  id: totrans-3093
  prefs: []
  type: TYPE_NORMAL
  zh: 美国邮政服务数据库：该数据库包含来自美国邮件信封的16 × 16像素大小标准化的手写数字图像。训练集和测试集分别有9709和2007个样本。NIST1数据库：第二个实验是由国家标准与技术研究所（NIST）于1992年春季组织的比赛。比赛的目标是根据223,000个模式的训练集对59,000个手写数字的测试集进行分类。NIST2数据库：第三个实验是在由NIST提供的训练和测试数据库（见上文）上进行的。NIST
- en: had divided the data into two sets which unfortunately had different distributions.
    The training set (223,000 patterns) was easier than the testing set (59,000 patterns).
    In our experiments we combined these two sets 50/50 to make a training set of
    60,000 patterns and testing/validation sets of 10,000 patterns each, all having
    the same characteristics.
  id: totrans-3094
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据分为两个集合，但不幸的是它们的分布不同。训练集（223,000个模式）比测试集（59,000个模式）更简单。在实验中，我们将这两个集合以50/50的比例结合，形成了一个60,000个模式的训练集和各10,000个模式的测试/验证集，所有集合均具有相同特征。
- en: For each of these three databases we tried to evaluate human performance to
    benchmark the difficulty of the database. For USPS, two members of our group went
    through the test set and both obtained a 2.5% raw error performance. The human
    performance on NIST1 was provided by the National Institute of Standard and Technology.
    The human performance on NIST2 was measured on a small subsample of the database
    and must therefore be taken with caution.
  id: totrans-3095
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这三个数据库，我们尝试评估人类表现以基准数据库的难度。对于USPS，我们组的两名成员通过了测试集，并均获得了2.5%的原始错误表现。NIST1的人类表现由国家标准与技术研究所提供。NIST2的人类表现是在数据库的小子样本上测量的，因此需要谨慎对待。
- en: Several of the leading algorithms where tested on each of these databases.
  id: totrans-3096
  prefs: []
  type: TYPE_NORMAL
  zh: 多个领先算法在这些数据库上进行了测试。
- en: The first experiment used the K-Nearest Neighbor algorithm, using the ordinary
    Euclidean distance. The prototype set consisted of all available training examples.
    A 1-Nearest Neighbor rule gave optimal performance in USPS while a 3-Nearest Neighbors
    rule performed better in NIST2.
  id: totrans-3097
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个实验使用了K-近邻算法，采用普通的欧几里得距离。原型集由所有可用的训练样本组成。1-近邻规则在USPS中表现最佳，而3-近邻规则在NIST2中表现更好。
- en: The second experiment was similar to the first, but the distance function was
    changed to tangent distance with 7 transformations. For the USPS and NIST2 databases,
    the prototype set was constructed as before, but for NIST1 it was constructed
    by cycling through the training set. Any patterns which were misclassified were
    added to the prototype set. After a few cycles, no more prototypes are added (the
    training error was 0). This resulted in 10,000 prototypes. A 3- Nearest Neighbors
    rule gave optimal performance on this set.
  id: totrans-3098
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个实验与第一个实验相似，但距离函数改为切线距离并进行了7次变换。对于USPS和NIST2数据库，原型集与之前构建相同，但对于NIST1，则通过循环训练集构建。任何被误分类的模式都被添加到原型集中。经过几轮后，不再添加更多原型（训练误差为0）。最终得到了10,000个原型。3-近邻规则在这个集合上表现最佳。
- en: Other algorithms such as neural nets [18, 20], optimal margin classifier [7],
  id: totrans-3099
  prefs: []
  type: TYPE_NORMAL
  zh: 其他算法如神经网络 [18, 20]、最优边际分类器 [7]、
- en: local learning [3] and boosting [9] were also used on these databases. A case
    study can be found in [20].
  id: totrans-3100
  prefs: []
  type: TYPE_NORMAL
  zh: 局部学习 [3] 和提升 [9] 也用于这些数据库。案例研究可以在 [20] 中找到。
- en: 'Table 12.1. Results: Performances in % of errors for (in order) human, K-nearest
    neighbor, tangent distance, Lenet1 (simple neural network), Lenet4 (large neural
    network), optimal margin classifier (OMC), local learning (LL) and boosting (Boost)'
  id: totrans-3101
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.1。结果：人类、K-近邻、切线距离、Lenet1（简单神经网络）、Lenet4（大型神经网络）、最优边际分类器（OMC）、局部学习（LL）和提升（Boost）各自的错误率表现（%）
- en: '| Human   | K-NN   | T.D.   | Lenet1   | Lenet4   | OMC   | LL   | Boost   |     |'
  id: totrans-3102
  prefs: []
  type: TYPE_TB
  zh: '| 人类   | K-NN   | 切线距离   | Lenet1   | Lenet4   | OMC   | LL   | 提升   |     |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-3103
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| USPS    | 2.5    | 5.7    | 2.5      | 4.2      | 4.3   | 3.3  | 2.6     |     |'
  id: totrans-3104
  prefs: []
  type: TYPE_TB
  zh: '| USPS    | 2.5    | 5.7    | 2.5      | 4.2      | 4.3   | 3.3  | 2.6     |     |'
- en: '| NIST1   | 1.6    | 3.2    | 3.7      | 4.1      |       |      |         |     |'
  id: totrans-3105
  prefs: []
  type: TYPE_TB
  zh: '| NIST1   | 1.6    | 3.2    | 3.7      | 4.1      |       |      |         |     |'
- en: '| NIST2   | 0.2    | 2.4    | 1.1      | 1.7      | 1.1   | 1.1  | 1.1     |
    0.7 |'
  id: totrans-3106
  prefs: []
  type: TYPE_TB
  zh: '| NIST2   | 0.2    | 2.4    | 1.1      | 1.7      | 1.1   | 1.1  | 1.1     |
    0.7 |'
- en: 'The results are summarized in Table 12.1. As illustrated in the table, the
    tangent distance algorithm equals or outperforms all other algorithms we tested,
    in all cases except one: Boosted LeNet 4 was the winner on the NIST2 database.
    This is not surprising. The K-nearest neighbor algorithm (with no preprocessing)
    is very unsophisticated in comparison to local learning, optimal margin classifier,
    and boosting. The advantange of tangent distance is the *a priori* knowledge of
    transformation invariance embedded into the distance. When the training data is
    sufficiently large, as is the case in NIST2, some of this knowledge can be picked
    up from the data by the more sophisticated algorithms. In other words, the value
    of *a priori* knowledge decreases as the size of the training set increases.'
  id: totrans-3107
  prefs: []
  type: TYPE_NORMAL
  zh: 结果总结在表12.1中。如表中所示，切线距离算法在所有测试的算法中在所有情况下（除了一个例外）都相当或超越了其他算法：在NIST2数据库上，Boosted
    LeNet 4获胜。这并不令人惊讶。K近邻算法（无预处理）与局部学习、最优边际分类器和提升相比，显得非常简单。切线距离的优势在于距离中嵌入的*先验*变换不变性知识。当训练数据足够大时，正如NIST2所示，这些知识可以被更复杂的算法从数据中获取。换句话说，*先验*知识的价值随着训练集规模的增加而减少。
- en: 12.2.3 How To Make Tangent Distance Work
  id: totrans-3108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2.3 如何使切线距离工作
- en: This section is dedicated to the technological "know how" which is necessary
    to make tangent distance work with various applications. "Tricks"of this sort
    are usually not published for various reasons (they are not always theoretically
    sound, page area is too valuable, the tricks are specific to one particular application,
    commercial competitive considerations discourage telling everyone how to reproduce
    the result, etc.), but they are often a determining factor in making the technology
    a success. Several of these techniques will be discussed here.
  id: totrans-3109
  prefs: []
  type: TYPE_NORMAL
  zh: 本节专门讨论使切线距离能够与各种应用一起工作的技术“诀窍”。此类“技巧”通常由于各种原因未被公布（它们并不总是理论上合理，页面空间过于珍贵，技巧特定于某一特定应用，商业竞争考虑不鼓励告诉所有人如何重现结果等），但它们往往是使技术成功的决定性因素之一。这里将讨论其中的几种技术。
- en: 'Smoothing the Input Space: This is the single most important factor in obtaining
    good performance with tangent distance. By definition, the tangent vectors are
    the Lie derivatives of the transformation function s(*P, α*) with respect to α.
    They can be written as:'
  id: totrans-3110
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑输入空间：这是获得切线距离良好性能的最重要因素。根据定义，切线向量是变换函数s(*P, α*)相对于α的李导数。它们可以写成：
- en: $$L_{P}=\left.{\frac{\partial s(P,\alpha)}{\partial\alpha}}\right|=\operatorname*{lim}_{\epsilon\rightarrow0}{\frac{s(P,\epsilon)-s(P,0)}{\epsilon}}.$$
  id: totrans-3111
  prefs: []
  type: TYPE_NORMAL
  zh: $$L_{P}=\left.{\frac{\partial s(P,\alpha)}{\partial\alpha}}\right|=\operatorname*{lim}_{\epsilon\rightarrow0}{\frac{s(P,\epsilon)-s(P,0)}{\epsilon}}.$$
- en: $$(12.15)$$
  id: totrans-3112
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.15)$$
- en: It is therefore very important that s be differentiable (and well behaved) with
    respect to α. In particular, it is clear from equation (12.15) that s(P, ) must
    be computed for  arbitrarily small. Fortunately, even when P can only take discrete
    values, it is easy to make s differentiable. The trick is to use a smoothing interpolating
    function Cσ as a preprocessing for P, such that s(Cσ(P), α) is differentiable
    (with respect to Cσ(P) and α, not with respect to P). For instance, if the input
    space for P is binary images, Cσ(P) can be a convolution of P with a Gaussian
    function of standard deviation σ. If s(Cσ(P), α) is a translation of α pixels,
    the derivative of s(Cσ(P), α) can easily be computed since s(Cσ(P), )
  id: totrans-3113
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，s相对于α必须是可微的（且表现良好）是非常重要的。特别地，从方程(12.15)可以清楚看出，s(P, )必须在任意小的情况下计算。幸运的是，即使P只能取离散值，仍然可以轻松使s可微。诀窍是使用平滑插值函数Cσ作为P的预处理，使得s(Cσ(P),
    α)在Cσ(P)和α方面是可微的，而不是相对于P。举例来说，如果P的输入空间是二值图像，Cσ(P)可以是P与标准偏差为σ的高斯函数的卷积。如果s(Cσ(P),
    α)是平移α个像素，则可以很容易计算s(Cσ(P), α)的导数，因为s(Cσ(P), )。
- en: can be obtained by translating Gaussian functions. This preprocessing will be
    discussed in more details in section 12.4.
  id: totrans-3114
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过平移高斯函数获得。该预处理将在12.4节中详细讨论。
- en: The smoothing factor σ controls the locality of the invariance. The smoother
    the transformation curve defined by s is, the longer the linear approximation
    will be valid. In general the best smoothing is the maximum smoothing which does
    not blur the features. For example, in handwritten character recognition with
    16x16 pixel images, a Gaussian function with a standard deviation of 1 pixel yielded
    the best results. Increased smoothing led to confusion (such as a "5" mistaken
    for "6" because the lower loop had been closed by the smoothing)
  id: totrans-3115
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑因子σ控制不变性的局部性。由s定义的变换曲线越平滑，线性近似有效的时间就越长。一般而言，最佳平滑是最大平滑，但不会模糊特征。例如，在16x16像素图像的手写字符识别中，标准偏差为1像素的高斯函数得到了最佳效果。增加平滑导致了混淆（例如，"5"被误认为"6"，因为下环已被平滑关闭）。
- en: and decreased smoothing didn't make full use of the invariance properties.
  id: totrans-3116
  prefs: []
  type: TYPE_NORMAL
  zh: 而减少平滑并未充分利用不变性特性。
- en: If the available computation time allows it, the best strategy is to extract
    features first, smooth shamelessly, and then compute the tangent distance on the
    smoothed features.
  id: totrans-3117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可用的计算时间允许，最佳策略是先提取特征，毫不犹豫地平滑，然后在平滑特征上计算切线距离。
- en: 'Controlled Deformation: The linear system given in equation (12.8) is singular
    if some of the tangent vectors for E or P are parallel. Although the probability
    of this happening is zero when the data is taken from a real-valued continuous
    distribution (as is the case in handwritten character recognition), it is possible
    that a pattern may be duplicated in both the training and the test set, resulting
    in a division by zero error. The fix is quite simple and elegant. Equation (12.8)
    can be replaced by equation:'
  id: totrans-3118
  prefs: []
  type: TYPE_NORMAL
  zh: 受控变形：方程 (12.8) 中给出的线性系统是奇异的，如果E或P的一些切向量是平行的。尽管在从实值连续分布中提取数据时，这种情况发生的概率为零（如手写字符识别中的情况），但模式可能在训练集和测试集中重复，导致除零错误。解决方法相当简单而优雅。方程
    (12.8) 可以被替换为方程：
- en: $$D(E,P)=\min_{\alpha_{E},\alpha_{P}}\|E+L_{E}\alpha_{E}-P-L_{P}\alpha_{P}\|^{2}+k\|L_{E}\alpha_{E}\|^{2}+k\|L_{P}\alpha_{P}\|^{2}.\tag{12.16}$$
  id: totrans-3119
  prefs: []
  type: TYPE_NORMAL
  zh: $$D(E,P)=\min_{\alpha_{E},\alpha_{P}}\|E+L_{E}\alpha_{E}-P-L_{P}\alpha_{P}\|^{2}+k\|L_{E}\alpha_{E}\|^{2}+k\|L_{P}\alpha_{P}\|^{2}.\tag{12.16}$$
- en: The physical interpretation of this equation, depicted in Figure 12.7, is that
    the point E(αE) on the tangent plane TE is attached to E with a spring with spring
    constant k and to P(αp) (on the tangent plane TP ) with spring constant 1, and
    P(αp) is also attached to P with spring constant k. (All three springs
  id: totrans-3120
  prefs: []
  type: TYPE_NORMAL
  zh: 此方程的物理解释如图12.7所示，点E(αE)与弹簧常数为k的弹簧连接在切向平面TE上，并与弹簧常数为1的P(αP)（在切向平面TP上）连接，P(αP)也通过弹簧常数为k与P相连。（所有三根弹簧
- en: '![247_image_0.png](247_image_0.png)'
  id: totrans-3121
  prefs: []
  type: TYPE_IMG
  zh: '![247_image_0.png](247_image_0.png)'
- en: $\left(12.17\right)$  $\left(12.18\right)$  .
  id: totrans-3122
  prefs: []
  type: TYPE_NORMAL
  zh: $\left(12.17\right)$  $\left(12.18\right)$ 。
- en: Fig. 12.7. The tangent distance between E and P is the elastic energy stored
    in each of the three springs connecting P, P, E and E. P and E can move without
    friction along the tangent planes. The spring constants are indicated on the figure.
  id: totrans-3123
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7。E和P之间的切向距离是连接P、P、E和E的三根弹簧储存的弹性能。P和E可以在切向平面上无摩擦移动。弹簧常数在图中标出。
- en: have zero natural length.) The new tangent distance is the total potential elastic
    energy stored of all three springs at equilibrium. As for the standard tangent
    distance, the solution can easily be obtained by differentiating equation (12.16)
  id: totrans-3124
  prefs: []
  type: TYPE_NORMAL
  zh: 有零自然长度。）新的切向距离是所有三根弹簧在平衡时储存的总潜在弹性能。至于标准切向距离，解可以通过对方程 (12.16) 求导轻松获得。
- en: 'with respect to αE and αP . The differentiation yields:'
  id: totrans-3125
  prefs: []
  type: TYPE_NORMAL
  zh: 关于αE和αP。求导结果为：
- en: $$\begin{array}{l}{{L_{P}^{\top}(E-P-L_{P}(1+k)\alpha_{P}+L_{E}\alpha_{E})=0}}\\
    {{L_{E}^{\top}(E-P-L_{P}\alpha_{P}+L_{E}(1+k)\alpha_{E})=0.}}\end{array}$$
  id: totrans-3126
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{l}{{L_{P}^{\top}(E-P-L_{P}(1+k)\alpha_{P}+L_{E}\alpha_{E})=0}}\\
    {{L_{E}^{\top}(E-P-L_{P}\alpha_{P}+L_{E}(1+k)\alpha_{E})=0.}}\end{array}$$
- en: The solution of this system is
  id: totrans-3127
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统的解是
- en: $$(L_{PE}L_{EE}^{-1}L_{E}^{\top}-(1+k)L_{P}^{\top})(E-P)=(L_{PE}L_{EE}^{-1}L_{EP}-(1+k)^{2}L_{PP})\alpha_{P}\tag{12.19}$$
    $$(L_{EP}L_{PP}^{-1}L_{P}^{\top}-(1+k)L_{E}^{\top})(E-P)=((1+k)^{2}L_{EE}-L_{EP}L_{PP}^{-1}L_{PE})\alpha_{E}\tag{12.20}$$
  id: totrans-3128
  prefs: []
  type: TYPE_NORMAL
  zh: $$(L_{PE}L_{EE}^{-1}L_{E}^{\top}-(1+k)L_{P}^{\top})(E-P)=(L_{PE}L_{EE}^{-1}L_{EP}-(1+k)^{2}L_{PP})\alpha_{P}\tag{12.19}$$
    $$(L_{EP}L_{PP}^{-1}L_{P}^{\top}-(1+k)L_{E}^{\top})(E-P)=((1+k)^{2}L_{EE}-L_{EP}L_{PP}^{-1}L_{PE})\alpha_{E}\tag{12.20}$$
- en: where LEE = LELE, LP E = LP LE, LEP = LELP and LP P = LP LP . The system has
    the same complexity as the vanilla tangent distance except that, it always has
    a solution for k ≥ 0, and is more numerically stable. Note that for k = 0, it
    is equivalent to the standard tangent distance, while for k = ∞,
  id: totrans-3129
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 LEE = LELE，LP E = LP LE，LEP = LELP，LP P = LP LP。该系统的复杂性与普通切向距离相同，唯一不同的是，它总是对
    k ≥ 0 有解，并且数值稳定性更高。注意，对于 k = 0，它相当于标准切向距离，而对于 k = ∞，
- en: we have the Euclidean distance. This approach is also very useful when the number
    of tangent vectors is greater or equal than the number of dimensions of the space.
    The standard tangent distance would most likely be zero (when the tangent spaces
    intersect), but the "spring" tangent distance still expresses valuable information
    about the invariances.
  id: totrans-3130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有欧几里得距离。当切向量的数量大于或等于空间的维度时，这种方法也非常有用。标准的切向距离在切向空间相交时最有可能为零，但“弹簧”切向距离仍然表达了关于不变性的重要信息。
- en: If the number of dimension of the input space is large compared to the number
    of tangent vectors, keeping k as small as possible is better because it doesn't
    interfere with the "sliding" along the tangent plane (E and P are less constrained).
  id: totrans-3131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入空间的维度与切向量的数量相比很大，保持k尽可能小是更好的，因为这不会干扰在切向平面上的“滑动”（E和P的约束较少）。
- en: Contrary to intuition, there is no danger of sliding too far in high dimensional
    space because tangent vectors are always roughly orthogonal and they could only
    slide far if they were parallel.
  id: totrans-3132
  prefs: []
  type: TYPE_NORMAL
  zh: 与直觉相反，在高维空间中滑动得过远是没有危险的，因为切线向量总是大致正交，只有在它们平行时才可能滑动得过远。
- en: 'Hierarchy of Distances: If several invariances are used, classification using
    tangent distance alone would be quite expensive. Fortunately, if a typical memory
    based algorithm is used, for example, K-nearest neighbors, it is quite unnecessary
    to compute the full tangent distance between the unclassified pattern and all
    the labeled samples. In particular, if a crude estimate of the tangent distance
    indicates with a sufficient confidence that a sample is very far from the pattern
    to be classified, no more computation is needed to know that this sample is not
    one of the K-nearest neighbors. Based on this observation one can build a hierarchy
    of distances which can greatly reduce the computation of each classification.
    Let''s assume, for instance, that we have m approximations Di of the tangent distance,
    ordered such that D1 is the crudest approximation of the tangent distance and
    Dm is exactly tangent distance (for instance D1 to D5 could be the Euclidean distance
    with increasing resolution, and D6 to D10 each add a tangent vector at full resolution).'
  id: totrans-3133
  prefs: []
  type: TYPE_NORMAL
  zh: 距离层次：如果使用多个不变性，仅使用切线距离进行分类会非常昂贵。幸运的是，如果使用典型的基于记忆的算法，例如 K 最近邻，则完全没有必要计算未分类模式与所有标记样本之间的完整切线距离。特别是，如果粗略估计的切线距离以足够的置信度表明某个样本远离要分类的模式，则无需更多计算即可知道该样本不是
    K 最近邻之一。基于这一观察，可以建立一个距离层次，极大地减少每次分类的计算。假设我们有 m 个切线距离的近似值 Di，按顺序排列，使得 D1 是切线距离的最粗略近似，而
    Dm 则是准确的切线距离（例如，D1 到 D5 可能是具有递增分辨率的欧几里得距离，D6 到 D10 则每个都在全分辨率下添加一个切线向量）。
- en: 'The basic idea is to keep a pool of all the prototypes which could potentially
    be the K-nearest neighbors of the unclassified pattern. Initially the pool contains
    all the samples. Each of the distances Di corresponds to a stage of the classification
    process. The classification algorithm has 3 steps at each stage, and proceeds
    from stage 1 to stage m or until the classification is complete: Step 1: the distance
    Di between all the samples in the pool and the unclassified pattern is computed.'
  id: totrans-3134
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是保持一个包含所有原型的池，这些原型可能是未分类模式的 K 最近邻。最初，池中包含所有样本。每个距离 Di 对应于分类过程的一个阶段。分类算法在每个阶段有
    3 个步骤，并从阶段 1 进行到阶段 m，或者直到分类完成：步骤 1：计算池中所有样本与未分类模式之间的距离 Di。
- en: 'Step 2: A classification and a confidence score is computed with these distances.'
  id: totrans-3135
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 2：使用这些距离计算分类和置信度评分。
- en: 'If the confidence is good enough, let''s say better than Ci (for instance,
    if all the samples left in the pool are in the same class) the classification
    is complete, otherwise proceed to step 3. Step 3: The Ki closest samples, according
    to distance Di are kept in the pool, while the remaining samples are discarded.'
  id: totrans-3136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果置信度足够好，假设好于 Ci（例如，如果池中剩下的所有样本都在同一类中），则分类完成；否则，继续进行步骤 3。步骤 3：根据距离 Di 保留 Ki
    个最近的样本在池中，而丢弃剩余样本。
- en: Finding the Ki closest samples can be done in O(p) (where p is the number of
    samples in the pool) since these elements need not to be sorted [22, 2]. The reduced
    pool is then passed to stage i + 1.
  id: totrans-3137
  prefs: []
  type: TYPE_NORMAL
  zh: 找到 Ki 个最近的样本可以在 O(p) 的时间内完成（其中 p 是池中样本的数量），因为这些元素不需要排序 [22, 2]。然后将缩减后的池传递到阶段
    i + 1。
- en: 'The two constants Ci and Ki must be determined in advance using a validation
    set. This can easily be done graphically by plotting the error as a function of
    Ki and Ci at each stage (starting with all Ki equal to the number of labeled samples
    and Ci = 1 for all stages). At each stage there is a minimum Ki and minimum Ci
    which give optimal performance on the validation set. By taking larger values,
    we can decrease the probability of making errors on the test sets. The slightly
    worse performance of using a hierarchy of distances is often well worth the speed-up.
    The computational cost of a pattern classification is then equal to:'
  id: totrans-3138
  prefs: []
  type: TYPE_NORMAL
  zh: 两个常数 Ci 和 Ki 必须使用验证集提前确定。这可以通过在每个阶段绘制 Ki 和 Ci 的误差作为函数的图形轻松完成（从所有 Ki 等于标记样本的数量和
    Ci = 1 开始）。在每个阶段都有一个最小的 Ki 和最小的 Ci，它们能在验证集上提供最佳性能。通过取更大的值，我们可以减少在测试集上出错的概率。使用距离层次的略微较差的性能通常是值得的，因为它加快了速度。模式分类的计算成本等于：
- en: $${\mathrm{computational~cost}}\approx\sum_{i}$$
  id: totrans-3139
  prefs: []
  type: TYPE_NORMAL
  zh: $${\mathrm{计算~成本}}\approx\sum_{i}$$
- en: $$(12.21)$$
  id: totrans-3140
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.21)$$
- en: '| number of   | distance   | probability   |'
  id: totrans-3141
  prefs: []
  type: TYPE_TB
  zh: '| 数字数量   | 距离   | 概率   |'
- en: '| --- | --- | --- |'
  id: totrans-3142
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| prototypes  | complexity | to reach      |'
  id: totrans-3143
  prefs: []
  type: TYPE_TB
  zh: '| 原型      | 复杂度 | 达成      |'
- en: '| ×           | ×          |               |'
  id: totrans-3144
  prefs: []
  type: TYPE_TB
  zh: '| ×           | ×          |               |'
- en: '| at stage i  | at stage i | stage i       |'
  id: totrans-3145
  prefs: []
  type: TYPE_TB
  zh: '| 在阶段 i  | 在阶段 i | 阶段 i       |'
- en: All this is better illustrated with an example as in Figure 12.8. This system
    was used for the USPS experiment described in a previous section. In classification
  id: totrans-3146
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些通过图 12.8 中的示例更好地说明。该系统用于前面章节描述的 USPS 实验。在分类中
- en: '![249_image_0.png](249_image_0.png)'
  id: totrans-3147
  prefs: []
  type: TYPE_IMG
  zh: '![249_image_0.png](249_image_0.png)'
- en: Fig. 12.8. Pattern recognition using a hierarchy of distances. The filter proceeds
    from left (starting with the whole database) to right (where only a few prototypes
    remain). At each stage distances between prototypes and the unknown pattern are
    computed and sorted; then the best candidate prototypes are selected for the next
    stage. As the complexity of the distance increases, the number of prototypes decreases,
    making computation feasible. At each stage a classification is attempted and a
    confidence score is computed. If the confidence score is high enough, the remaining
    stages are skipped.
  id: totrans-3148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.8. 使用距离层次进行模式识别。过滤器从左侧（从整个数据库开始）移动到右侧（仅剩少数原型）。在每个阶段，计算和排序原型与未知模式之间的距离；然后选择最佳候选原型进行下一阶段。随着距离复杂性的增加，原型数量减少，从而使计算变得可行。在每个阶段尝试分类并计算置信分数。如果置信分数足够高，则跳过剩余阶段。
- en: of handwritten digits (16x16 pixel images), D1, D2, and D3, were the Euclidean
    distances at resolution 2 × 2, 4 × 4 and 8 × 8 respectively. D4 was the one sided
    tangent distance with X-translation, on the sample side only, at resolution 8×8.
  id: totrans-3149
  prefs: []
  type: TYPE_NORMAL
  zh: 手写数字（16x16 像素图像）的 D1、D2 和 D3，分别是在 2 × 2、4 × 4 和 8 × 8 分辨率下的欧几里得距离。D4 是在 8×8
    分辨率下，样本侧唯一的切线距离，带有 X 平移。
- en: D5 was the double sided tangent distance with X-translation at resolution 16×16.
  id: totrans-3150
  prefs: []
  type: TYPE_NORMAL
  zh: D5 是在 16×16 分辨率下带有 X 平移的双侧切线距离。
- en: Each of the subsequent distances added one tangent vector on each side (Ytranslation,
    scaling, rotation, hyperbolic deformation1, hyperbolic deformation2 and thickness)
    until the full tangent distance was computed (D11).
  id: totrans-3151
  prefs: []
  type: TYPE_NORMAL
  zh: 后续的每个距离在每一侧添加了一个切线向量（Y 平移、缩放、旋转、双曲变形1、双曲变形2 和厚度），直到计算出完整的切线距离（D11）。
- en: Table 12.2 shows the expected number of multiply-adds at each of the stages.
  id: totrans-3152
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12.2 显示了各个阶段的预期乘加次数。
- en: 'It should be noted that the full tangent distance need only be computed for
    1 in 20 unknown patterns (probability 0.05), and only with 5 samples out of the
    original 10, 000. The net speed up was in the order of 500, compared with computing
    the full tangent distance between every unknown pattern and every sample (this
    is 6 times faster than computing the Euclidean distance at full resolution). Multiple
    Iterations: Tangent distance can be viewed as one iteration of a Newton-type algorithm
    which finds the points of minimum distance on the true transformation manifolds.
    The vectors αE and αP are the coordinates of the two closest points in the respective
    tangent spaces, but they can also be interpreted as the value for the real (non-linear)
    transformations. In other words, we can use αE and αP to compute the points s(E,αE)
    and s(*P, α*P ), the real nonlinear transformation of E and P. From these new
    points, we can recompute the tangent vectors, and the tangent distance and reiterate
    the process. If the appropriate conditions are met, this process can converge
    to a local minimum in the distance between the two transformation manifolds of
    P and E.'
  id: totrans-3153
  prefs: []
  type: TYPE_NORMAL
  zh: 应注意，完整的切线距离仅需对 20 个未知模式中的 1 个（概率 0.05）进行计算，且只需从原始的 10,000 个样本中选取 5 个。与对每个未知模式和每个样本计算完整切线距离相比，净加速约为
    500（这比以全分辨率计算欧几里得距离快 6 倍）。多次迭代：切线距离可以视为一种牛顿类型算法的单次迭代，该算法在真实变换流形上找到最小距离的点。向量 αE
    和 αP 是各自切空间中两个最近点的坐标，但它们也可以被解释为真实（非线性）变换的值。换句话说，我们可以使用 αE 和 αP 计算点 s(E,αE) 和 s(*P,
    α*P )，即 E 和 P 的真实非线性变换。从这些新点，我们可以重新计算切线向量、切线距离并重新迭代该过程。如果满足适当条件，该过程可以收敛到 P 和 E
    的两个变换流形之间的局部最小值。
- en: This process did not improve handwritten character recognition, but it yielded
    impressive results in face recognition [29]. In that case, each successive iteration
    was done at increasing resolution (hence combining hierarchical distances and
    multiple iterations), making the whole process computationally efficient.
  id: totrans-3154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程没有改善手写字符识别，但在面部识别中产生了令人印象深刻的结果[29]。在这种情况下，每次迭代都是以更高的分辨率进行的（因此结合了分层距离和多次迭代），使整个过程在计算上更为高效。
- en: 'Table 12.2. Summary computation for the classification of 1 pattern: The first
    column is the distance index, the second column indicates the number of tangent
    vectors (0 for the Euclidean distance), and the third column indicates the resolution
    in pixels, the fourth is Ki or the number of prototypes on which the distance
    Di must be computed, the fifth column indicates the number of additional dot products
    which must be computed to evaluate distance Di, the sixth column indicates the
    probability of not skipping that stage after the confidence score has been used,
    and the last column indicates the total average number of multiply-adds which
    must be performed (product of column 3 to 6) at each stage.'
  id: totrans-3155
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12.2. 对1个模式的分类的汇总计算：第一列是距离索引，第二列表示切线向量的数量（欧几里得距离为0），第三列表示以像素为单位的分辨率，第四列是Ki或必须计算的距离Di的原型数量，第五列表示为评估距离Di而必须计算的附加点积的数量，第六列表示在使用置信度评分后不跳过该阶段的概率，最后一列表示在每个阶段必须执行的总平均乘法加法数量（第3到6列的乘积）。
- en: '| i   | # of T.V.   | Reso   | # of proto (Ki)   | # of prod   | Probab   |
    # of mul/add   |'
  id: totrans-3156
  prefs: []
  type: TYPE_TB
  zh: '| i   | 电视数量   | 分辨率   | 原型数量 (Ki)   | 产品数量   | 概率   | 乘法/加法数量   |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-3157
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1   | 0           | 4      | 9709              | 1           | 1.00     |
    40,000         |'
  id: totrans-3158
  prefs: []
  type: TYPE_TB
  zh: '| 1   | 0           | 4      | 9709              | 1           | 1.00     |
    40,000         |'
- en: '| 2   | 0           | 16     | 3500              | 1           | 1.00     |
    56,000         |'
  id: totrans-3159
  prefs: []
  type: TYPE_TB
  zh: '| 2   | 0           | 16     | 3500              | 1           | 1.00     |
    56,000         |'
- en: '| 3   | 0           | 64     | 500               | 1           | 1.00     |
    32,000         |'
  id: totrans-3160
  prefs: []
  type: TYPE_TB
  zh: '| 3   | 0           | 64     | 500               | 1           | 1.00     |
    32,000         |'
- en: '| 4   | 1           | 64     | 125               | 2           | 0.90     |
    14,000         |'
  id: totrans-3161
  prefs: []
  type: TYPE_TB
  zh: '| 4   | 1           | 64     | 125               | 2           | 0.90     |
    14,000         |'
- en: '| 4   | 2           | 256    | 50                | 5           | 0.60     |
    40,000         |'
  id: totrans-3162
  prefs: []
  type: TYPE_TB
  zh: '| 4   | 2           | 256    | 50                | 5           | 0.60     |
    40,000         |'
- en: '| 6   | 4           | 256    | 45                | 7           | 0.40     |
    32,000         |'
  id: totrans-3163
  prefs: []
  type: TYPE_TB
  zh: '| 6   | 4           | 256    | 45                | 7           | 0.40     |
    32,000         |'
- en: '| 7   | 6           | 256    | 25                | 9           | 0.20     |
    11,000         |'
  id: totrans-3164
  prefs: []
  type: TYPE_TB
  zh: '| 7   | 6           | 256    | 25                | 9           | 0.20     |
    11,000         |'
- en: '| 8   | 8           | 256    | 15                | 11          | 0.10     |
    4,000          |'
  id: totrans-3165
  prefs: []
  type: TYPE_TB
  zh: '| 8   | 8           | 256    | 15                | 11          | 0.10     |
    4,000          |'
- en: '| 9   | 10          | 256    | 10                | 13          | 0.10     |
    3,000          |'
  id: totrans-3166
  prefs: []
  type: TYPE_TB
  zh: '| 9   | 10          | 256    | 10                | 13          | 0.10     |
    3,000          |'
- en: '| 10  | 12          | 256    | 5                 | 15          | 0.05     |
    1,000          |'
  id: totrans-3167
  prefs: []
  type: TYPE_TB
  zh: '| 10  | 12          | 256    | 5                 | 15          | 0.05     |
    1,000          |'
- en: '| 11  | 14          | 256    | 5                 | 17          | 0.05     |
    1,000          |'
  id: totrans-3168
  prefs: []
  type: TYPE_TB
  zh: '| 11  | 14          | 256    | 5                 | 17          | 0.05     |
    1,000          |'
- en: 12.3 Tangent Propagation
  id: totrans-3169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 切线传播
- en: The previous section dealt with memory-based techniques. We now apply tangent-distance
    principles to learned-function techniques.
  id: totrans-3170
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节讨论了基于记忆的技术。我们现在将切线距离原则应用于学习函数技术。
- en: The key idea is to incorporate the invariance directly into the learned classification
    function. In this section, we present an algorithm, called "tangent propagation",
    in which gradient descent is used to propagate information about the invariances
    of the training data. The process is a generalization of the widelyused "back
    propagation" method, which propagates information about the training data itself.
  id: totrans-3171
  prefs: []
  type: TYPE_NORMAL
  zh: 关键思想是将不变性直接纳入学习到的分类函数。在本节中，我们提出了一种算法，称为“切线传播”，在该算法中，使用梯度下降来传播关于训练数据不变性的信息。这个过程是广泛使用的“反向传播”方法的推广，后者传播的是关于训练数据本身的信息。
- en: We again assume all data is drawn independently from a given statistical distribution
    P, and our learning machine is characterized by the set of functions in can implement,
    Gw(x), indexed by the vector of parameters w. Ideally, we would like to find w
    which minimizes the energy function
  id: totrans-3172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次假设所有数据独立地从给定的统计分布 P 中抽取，我们的学习机器由可以实现的函数集 Gw(x) 特征化，参数向量 w 索引。理想情况下，我们希望找到能够最小化能量函数
    w
- en: $${\mathcal{E}}=\int\|G_{w}(x)-F(x)\|^{2}d{\mathcal{P}}(x)$$
  id: totrans-3173
  prefs: []
  type: TYPE_NORMAL
  zh: $${\mathcal{E}}=\int\|G_{w}(x)-F(x)\|^{2}d{\mathcal{P}}(x)$$
- en: $$(12.22)$$
  id: totrans-3174
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.22)$$
- en: where F(x) represents the "correct" or "desired" labeling of the point x. In
    the real world we must estimate this integral using only a finite set of training
    points B drawn the distribution P. That is, we try to minimize
  id: totrans-3175
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 F(x) 代表点 x 的“正确”或“期望”标签。在现实世界中，我们必须仅使用从分布 P 中抽取的有限训练点 B 来估计这个积分。也就是说，我们尝试最小化
- en: $$E_{p}=\sum_{i=1}^{p}\|G_{w}(x_{i})-F(x_{i})\|$$
  id: totrans-3176
  prefs: []
  type: TYPE_NORMAL
  zh: $$E_{p}=\sum_{i=1}^{p}\|G_{w}(x_{i})-F(x_{i})\|$$
- en: 'where the sum runs over the training set B. An estimate of w can be computed
    by following a gradient descent using the weight-update rule:'
  id: totrans-3177
  prefs: []
  type: TYPE_NORMAL
  zh: 其中和在训练集 B 上进行。可以通过遵循使用权重更新规则的梯度下降来计算 w 的估计：
- en: $$\Delta w=-\eta{\frac{\partial E_{p}}{\partial w}}.$$
  id: totrans-3178
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w=-\eta{\frac{\partial E_{p}}{\partial w}}.$$
- en: Let's consider an input transformation s(*x, α*) controlled by a parameter α.
    As always, we require that s is differentiable and that s(x, 0) = x. Now, in addition
    to the known labels of the training data, we assume that ∂F (s(xi,α))
  id: totrans-3179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个由参数 α 控制的输入变换 s(*x, α*)。和往常一样，我们要求 s 是可微的，并且 s(x, 0) = x。现在，除了已知的训练数据标签外，我们假设
    ∂F (s(xi,α))
- en: '∂α is known at α = 0 for each point x in the training set. To incorporate the
    invariance property into Gw(x), we add that the following constraint on the derivative:'
  id: totrans-3180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练集中的每个点 x，∂α 在 α = 0 时已知。为了将不变性属性纳入 Gw(x)，我们增加以下关于导数的约束：
- en: $$E_{r}=\sum_{i=1}^{p}\left|\frac{\partial G_{w}(s(x_{i},\alpha))}{\partial\alpha}-\frac{\partial
    F(s(x_{i},\alpha))}{\partial\alpha}\right|_{\alpha=0}^{2}\tag{1}$$
  id: totrans-3181
  prefs: []
  type: TYPE_NORMAL
  zh: $$E_{r}=\sum_{i=1}^{p}\left|\frac{\partial G_{w}(s(x_{i},\alpha))}{\partial\alpha}-\frac{\partial
    F(s(x_{i},\alpha))}{\partial\alpha}\right|_{\alpha=0}^{2}\tag{1}$$
- en: $$(12.23)$$
  id: totrans-3182
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.23)$$
- en: $$(12.24)$$
  id: totrans-3183
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.24)$$
- en: $$(12.25)$$
  id: totrans-3184
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.25)$$
- en: 'should be small at α = 0. In many pattern classification problems, we are interested
    in the local classification invariance property for F(x) with respect to the transformation
    s (the classification does not change when the input is slightly transformed),
    so we can simplify equation (12.25) to:'
  id: totrans-3185
  prefs: []
  type: TYPE_NORMAL
  zh: 应该在 α = 0 时很小。在许多模式分类问题中，我们对 F(x) 的局部分类不变性属性感兴趣，关于变换 s（当输入稍微变化时分类不变），因此我们可以将方程（12.25）简化为：
- en: $$E_{r}=\sum_{i=1}^{p}\left|{\frac{\partial G_{w}(s(x_{i},\alpha))}{\partial\alpha}}\right|_{\alpha=0}^{2}$$
  id: totrans-3186
  prefs: []
  type: TYPE_NORMAL
  zh: $$E_{r}=\sum_{i=1}^{p}\left|{\frac{\partial G_{w}(s(x_{i},\alpha))}{\partial\alpha}}\right|_{\alpha=0}^{2}$$
- en: $$(12.27)$$
  id: totrans-3187
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.27)$$
- en: $$(12.26)$$
  id: totrans-3188
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.26)$$
- en: since ∂F (s(xi,α))
  id: totrans-3189
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 ∂F (s(xi,α))
- en: ∂α = 0. To minimize this term we can modify the gradient descent rule to use
    the energy function
  id: totrans-3190
  prefs: []
  type: TYPE_NORMAL
  zh: ∂α = 0。为了最小化这个项，我们可以修改梯度下降规则，以使用能量函数
- en: $$E=\eta E_{p}+\mu E_{r}$$
  id: totrans-3191
  prefs: []
  type: TYPE_NORMAL
  zh: $$E=\eta E_{p}+\mu E_{r}$$
- en: E = ηEp + μEr (12.27)
  id: totrans-3192
  prefs: []
  type: TYPE_NORMAL
  zh: E = ηEp + μEr (12.27)
- en: 'with the weight update rule:'
  id: totrans-3193
  prefs: []
  type: TYPE_NORMAL
  zh: 与权重更新规则：
- en: $$\Delta w=-{\frac{\partial E}{\partial w}}.$$
  id: totrans-3194
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w=-{\frac{\partial E}{\partial w}}.$$
- en: The learning rates (or regularization parameters) η and μ are tremendously important,
    because they determine the tradeoff between learning the invariances
  id: totrans-3195
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率（或正则化参数）η和μ非常重要，因为它们决定了学习不变性之间的权衡
- en: (based on the chosen directional derivatives) versus learning the label itself
    (i.e.
  id: totrans-3196
  prefs: []
  type: TYPE_NORMAL
  zh: （基于所选择的方向导数）而不是直接学习标签本身（即
- en: the zeroth derivative) at each point in the training set.
  id: totrans-3197
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练集的每个点处的零阶导数。
- en: 'The local variation of the classification function, which appears in equation
    (12.26) can be written as:'
  id: totrans-3198
  prefs: []
  type: TYPE_NORMAL
  zh: 分类函数的局部变化，出现在方程（12.26）中，可以写成：
- en: 'On (12.26) can be written as:  $\left.\dfrac{\partial G_w(s(x,\alpha))}{\partial\alpha}\right|_{\alpha=0}=\left.\dfrac{\partial
    G_w(s(x,\alpha))}{\partial s(x,\alpha)}\dfrac{\partial s(x,\alpha)}{\partial\alpha}\right|_{\alpha=0}=\nabla_xG_w(x).$
    $\left.\dfrac{\partial s(x,\alpha)}{\partial\alpha}\right|_{\alpha=0}=\left.\dfrac{\partial}{\partial\alpha}\right|_{\alpha=0}=0$
    (12.29)'
  id: totrans-3199
  prefs: []
  type: TYPE_NORMAL
  zh: 在（12.26）中可以写为： $\left.\dfrac{\partial G_w(s(x,\alpha))}{\partial\alpha}\right|_{\alpha=0}=\left.\dfrac{\partial
    G_w(s(x,\alpha))}{\partial s(x,\alpha)}\dfrac{\partial s(x,\alpha)}{\partial\alpha}\right|_{\alpha=0}=\nabla_xG_w(x).$
    $\left.\dfrac{\partial s(x,\alpha)}{\partial\alpha}\right|_{\alpha=0}=\left.\dfrac{\partial}{\partial\alpha}\right|_{\alpha=0}=0$
    (12.29)
- en: $$(12.28)$$
  id: totrans-3200
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.28)$$
- en: since s(*x, α*) = x if α = 0 and where ∇xGw(x) is the Jacobian of Gw(x) for
    pattern x, and ∂s(α, x)/∂α is the *tangent vector* associated with transformation
    s as described in the previous section. Multiplying the tangent vector by the
    Jacobian involves one forward propagation through a "linearized" version of the
    network. If α is multi-dimensional, the forward propagation must be repeated for
    each tangent vector.
  id: totrans-3201
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 s(*x, α*) = x 当 α = 0，且 ∇xGw(x) 是 Gw(x) 对模式 x 的雅可比矩阵，而 ∂s(α, x)/∂α 是与前一节所述变换
    s 相关的 *切向量*。将切向量乘以雅可比矩阵涉及通过网络的“线性化”版本进行一次正向传播。如果 α 是多维的，则必须为每个切向量重复正向传播。
- en: The theory of Lie algebras [11] ensures that compositions of local (small)
  id: totrans-3202
  prefs: []
  type: TYPE_NORMAL
  zh: 李代数的理论 [11] 确保了局部（小）组合的构成。
- en: transformations correspond to linear combinations of the corresponding tangent
    vectors (this result will be discussed further in section 12.4). Consequently,
    if Er(x)=0 is verified, the network derivative in the direction of a linear combination
    of the tangent vectors is equal to the same linear combination of the desired
    derivatives. In other words, if the network is successfully trained to be locally
    invariant with respect to, say, horizontal translations and vertical translations,
    it will be invariant with respect to compositions thereof.
  id: totrans-3203
  prefs: []
  type: TYPE_NORMAL
  zh: 变换对应于相应切向量的线性组合（这一结果将在第12.4节进一步讨论）。因此，如果 Er(x)=0 得到验证，则网络在切向量的线性组合方向上的导数等于所需导数的相同线性组合。换句话说，如果网络成功训练为在水平平移和垂直平移方面局部不变，它将对这些变换的组合保持不变。
- en: It is possible to devise an efficient algorithm, "tangent prop", for performing
    the weight update (equation (12.28)). It is analogous to ordinary backpropagation,
    but in addition to propagating neuron activations, it also propagates the tangent
    vectors. The equations can be easily derived from Figure 12.9.
  id: totrans-3204
  prefs: []
  type: TYPE_NORMAL
  zh: 可以设计出一个高效的算法“切线传播”，用于执行权重更新（方程（12.28））。它类似于普通的反向传播，但除了传播神经元激活外，还传播切向量。这些方程可以很容易地从图12.9中推导出来。
- en: '![252_image_0.png](252_image_0.png)'
  id: totrans-3205
  prefs: []
  type: TYPE_IMG
  zh: '![252_image_0.png](252_image_0.png)'
- en: Fig. 12.9. Forward propagated variables (*a, x, γ, ξ*), and backward propagated
    variables (*b, y, β, ψ*) in the regular network (roman symbols) and the Jacobian
    (linearized)
  id: totrans-3206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.9. 正向传播变量 (*a, x, γ, ξ*) 和反向传播变量 (*b, y, β, ψ*) 在常规网络（罗马符号）和雅可比（线性化）中。
- en: network (greek symbols). Converging forks (in the direction in which the signal
    is traveling) are sums, diverging forks just duplicate the values.
  id: totrans-3207
  prefs: []
  type: TYPE_NORMAL
  zh: 网络（希腊符号）。收敛分叉（信号传播的方向）是求和，发散分叉则只复制值。
- en: 12.3.1 Local Rule
  id: totrans-3208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3.1 局部规则
- en: 'The forward propagation equation is:'
  id: totrans-3209
  prefs: []
  type: TYPE_NORMAL
  zh: 正向传播方程为：
- en: $$a_{i}^{l}=\sum_{j}w_{i j}^{l}x_{j}^{l-1}\qquad x_{i}^{l}=\sigma(a_{i}^{l})$$
  id: totrans-3210
  prefs: []
  type: TYPE_NORMAL
  zh: $$a_{i}^{l}=\sum_{j}w_{i j}^{l}x_{j}^{l-1}\qquad x_{i}^{l}=\sigma(a_{i}^{l})$$
- en: 'where σ is a non linear differentiable function (typically a sigmoid). The
    forward propagation starts at the first layer (l = 1), with x0 being the input
    layer, and ends at the output layer (l = L). Similarly, The tangent forward propagation
    (tangent prop) is defined by:'
  id: totrans-3211
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 σ 是一个非线性可微函数（通常是 sigmoid）。正向传播从第一层（l = 1）开始，以 x0 为输入层，结束于输出层（l = L）。同样，切线正向传播（切线传播）由以下公式定义：
- en: $$\gamma_{i}^{l}=\sum_{j}w_{i j}^{l}\xi_{j}^{l-1}\qquad\quad\xi_{i}^{l}=\sigma^{\prime}(a_{i}^{l})\gamma_{i}^{l}.$$
  id: totrans-3212
  prefs: []
  type: TYPE_NORMAL
  zh: $$\gamma_{i}^{l}=\sum_{j}w_{i j}^{l}\xi_{j}^{l-1}\qquad\quad\xi_{i}^{l}=\sigma^{\prime}(a_{i}^{l})\gamma_{i}^{l}.$$
- en: j
  id: totrans-3213
  prefs: []
  type: TYPE_NORMAL
  zh: j
- en: The tangent forward propagation starts at the first layer (l = 1), with ξ0 being
    the tangent vector ∂s(x,α)
  id: totrans-3214
  prefs: []
  type: TYPE_NORMAL
  zh: 切线正向传播从第一层（l = 1）开始，以 ξ0 为切向量 ∂s(x,α)。
- en: '∂α , and ends at the output layer (l = L). The tangent gradient backpropagation
    can be computed using the chain rule:'
  id: totrans-3215
  prefs: []
  type: TYPE_NORMAL
  zh: ∂α，结束于输出层（l = L）。切线梯度反向传播可以使用链式法则计算：
- en: $$\begin{array}{l l l}{{\partial E}}&{{}}&{{}}\\ {{\frac{\partial E}{\partial\xi_{i}^{l}}=\sum_{k}\frac{\partial
    E}{\partial\gamma_{k}^{l+1}}\frac{\partial\gamma_{k}^{l+1}}{\partial\xi_{i}^{l}}}}&{{}}&{{\frac{\partial
    E}{\partial\gamma_{i}^{l}}=\frac{\partial E}{\partial\xi_{i}^{l}}\frac{\partial\xi_{i}^{l}}{\partial\gamma_{i}^{l}}}}\\
    {{}}&{{}}&{{}}\\ {{\beta_{i}^{l}=\sum_{k}\psi_{k}^{l+1}w_{k i}^{l+1}}}&{{}}&{{\psi_{i}^{l}=\beta_{i}^{l}\sigma^{\prime}(a_{i}^{l}).}}\end{array}$$
  id: totrans-3216
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{l l l}{{\partial E}}&{{}}&{{}}\\ {{\frac{\partial E}{\partial\xi_{i}^{l}}=\sum_{k}\frac{\partial
    E}{\partial\gamma_{k}^{l+1}}\frac{\partial\gamma_{k}^{l+1}}{\partial\xi_{i}^{l}}}}&{{}}&{{\frac{\partial
    E}{\partial\gamma_{i}^{l}}=\frac{\partial E}{\partial\xi_{i}^{l}}\frac{\partial\xi_{i}^{l}}{\partial\gamma_{i}^{l}}}}\\
    {{}}&{{}}&{{}}\\ {{\beta_{i}^{l}=\sum_{k}\psi_{k}^{l+1}w_{k i}^{l+1}}}&{{}}&{{\psi_{i}^{l}=\beta_{i}^{l}\sigma^{\prime}(a_{i}^{l}).}}\end{array}$$
- en: $$(12.30)$$
  id: totrans-3217
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.30)$$
- en: $$(12.31)$$
  id: totrans-3218
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.31)$$
- en: $$(12.33)$$
  id: totrans-3219
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.33)$$
- en: $$(12.32)$$
  id: totrans-3220
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.32)$$
- en: The tangent backward propagation starts at the output layer (l = L), with ξL
  id: totrans-3221
  prefs: []
  type: TYPE_NORMAL
  zh: 切线反向传播从输出层（l = L）开始，ξL
- en: being the network variation ∂Gw(s(x,α))
  id: totrans-3222
  prefs: []
  type: TYPE_NORMAL
  zh: 网络变化为∂Gw(s(x,α))
- en: '∂α , and ends at the input layer. Similarly, the gradient backpropagation equation
    is:'
  id: totrans-3223
  prefs: []
  type: TYPE_NORMAL
  zh: ∂α，结束于输入层。类似地，梯度反向传播方程为：
- en: ∂al+1 k ∂xli ∂E ∂ali = ∂E ∂xli ∂xli ∂ali + ∂E ∂ξli ∂ξli ∂ali ∂E ∂xli = - k ∂E
    ∂al+1 k bli = - k yl+1 k wl+1 ki yli = bliσ(ali) + βiσ(ali)γli . (12.35)
  id: totrans-3224
  prefs: []
  type: TYPE_NORMAL
  zh: ∂al+1 k ∂xli ∂E ∂ali = ∂E ∂xli ∂xli ∂ali + ∂E ∂ξli ∂ξli ∂ali ∂E ∂xli = - k ∂E
    ∂al+1 k bli = - k yl+1 k wl+1 ki yli = bliσ(ali) + βiσ(ali)γli . (12.35)
- en: $$(12.34)$$
  id: totrans-3225
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.34)$$
- en: 'The standard backward propagation starts at the output layer (l = L), with
    xL = Gw(x0) being the network output, and ends at the input layer. Finally, the
    weight update is:'
  id: totrans-3226
  prefs: []
  type: TYPE_NORMAL
  zh: 标准反向传播从输出层（l = L）开始，xL = Gw(x0)为网络输出，并结束于输入层。最后，权重更新为：
- en: $$\Delta w_{i j}^{l}=-\frac{\partial E}{\partial a_{i}^{l}}\frac{\partial a_{i}^{l}}{\partial
    w_{i j}^{l}}-\frac{\partial E}{\partial\gamma_{i}^{l}}\frac{\partial\gamma_{i}^{l}}{\partial
    w_{i j}^{l}}$$
  id: totrans-3227
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w_{i j}^{l}=-\frac{\partial E}{\partial a_{i}^{l}}\frac{\partial a_{i}^{l}}{\partial
    w_{i j}^{l}}-\frac{\partial E}{\partial\gamma_{i}^{l}}\frac{\partial\gamma_{i}^{l}}{\partial
    w_{i j}^{l}}$$
- en: $$(12.35)$$
  id: totrans-3228
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.35)$$
- en: $$(12.37)$$
  id: totrans-3229
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.37)$$
- en: $$(12.36)$$
  id: totrans-3230
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.36)$$
- en: $$\Delta w_{i j}^{l}=-y_{i}^{l}x_{j}^{l-1}-\psi_{i}^{l}\xi_{j}^{l-1}.$$
  id: totrans-3231
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w_{i j}^{l}=-y_{i}^{l}x_{j}^{l-1}-\psi_{i}^{l}\xi_{j}^{l-1}.$$
- en: j . (12.37)
  id: totrans-3232
  prefs: []
  type: TYPE_NORMAL
  zh: j . (12.37)
- en: The computation requires one forward propagation and one backward propagation
    per pattern and per tangent vector during training. After the network is trained,
    it is approximately locally invariant with respect to the chosen transformation.
    After training, the evaluation of the learned function is in all ways identical
    to a network which is not trained for invariance (except that the weights have
    different values).
  id: totrans-3233
  prefs: []
  type: TYPE_NORMAL
  zh: 计算在训练期间每个样本和每个切线向量需要一次前向传播和一次后向传播。网络训练完成后，对于所选择的变换，它在局部上是近似不变的。训练后，学习到的函数的评估在各方面上与未针对不变性训练的网络相同（除了权重的值不同）。
- en: '![254_image_0.png](254_image_0.png)'
  id: totrans-3234
  prefs: []
  type: TYPE_IMG
  zh: '![254_image_0.png](254_image_0.png)'
- en: Fig. 12.10. Generalization performance curve as a function of the training set
    size for the tangent prop and the backprop algorithms
  id: totrans-3235
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.10。切线传播和反向传播算法训练集大小与泛化性能曲线的关系
- en: 12.3.2 Results
  id: totrans-3236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3.2 结果
- en: Two experiments illustrate the advantages of tangent prop. The first experiment
    is a classification task, using a small (linearly separable) set of 480 binary
    images of handwritten digits. The training sets consist of 10, 20, 40, 80, 160
    or 320 patterns, and the test set contains 160 patterns. The patterns are smoothed
    using a Gaussian kernel with standard deviation of one half pixel. For each of
    the training set patterns, the tangent vectors for horizontal and vertical translation
    are computed. The network has two hidden layers with locally connected shared
    weights, and one output layer with 10 units (5194 connections, 1060 free parameters)
    [19]. The generalization performance as a function of the training set size for
    traditional backprop and tangent prop are compared in Figure 12.10. We have conducted
    additional experiments in which we implemented not only translations but also
    rotations, expansions and hyperbolic deformations. This set of 6 generators is
    a basis for all linear transformations of coordinates for two dimensional images.
    It is straightforward to implement other generators including gray-level-shifting,
    "smooth" segmentation, local continuous coordinate transformations and independent
    image segment transformations.
  id: totrans-3237
  prefs: []
  type: TYPE_NORMAL
  zh: 两个实验说明了切线传播的优势。第一个实验是一个分类任务，使用480个手写数字的二进制图像的小（线性可分）集合。训练集包含10、20、40、80、160或320个样本，测试集包含160个样本。这些样本使用标准差为半个像素的高斯核进行平滑。对于每个训练集样本，计算水平和垂直平移的切线向量。网络有两个隐藏层，具有局部连接共享权重，以及一个包含10个单元的输出层（5194个连接，1060个自由参数）[19]。图12.10比较了传统反向传播和切线传播的泛化性能与训练集大小的关系。我们还进行了额外的实验，实现了不仅是平移，还有旋转、扩展和双曲变形。这组6个生成器是二维图像坐标所有线性变换的基础。实现其他生成器非常简单，包括灰度级偏移、“平滑”分割、本地连续坐标变换和独立图像段变换。
- en: The next experiment is designed to show that in applications where data is highly
    correlated, tangent prop yields a large speed advantage. Since the distortion
    model implies adding lots of highly correlated data, the advantage of tangent
    prop over the distortion model becomes clear.
  id: totrans-3238
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个实验旨在表明，在数据高度相关的应用中，切线传播提供了巨大的速度优势。由于失真模型意味着添加大量高度相关的数据，切线传播相对于失真模型的优势变得显而易见。
- en: The task is to approximate a function that has plateaus at three locations.
    We want to enforce local invariance near each of the training points (Figure 12.11,
    bottom). The network has one input unit, 20 hidden units and one output unit.
  id: totrans-3239
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是逼近一个在三个位置有平台的函数。我们希望在每个训练点附近强制局部不变性（图12.11，底部）。该网络有一个输入单元、20个隐藏单元和一个输出单元。
- en: '![255_image_0.png](255_image_0.png)'
  id: totrans-3240
  prefs: []
  type: TYPE_IMG
  zh: '![255_image_0.png](255_image_0.png)'
- en: Fig. 12.11. Comparison of the distortion model (left column) and tangent prop
    (right column). The top row gives the learning curves (error versus number of
    sweeps through the training set). The bottom row gives the final input-output
    function of the network; the dashed line is the result for unadorned back prop.
  id: totrans-3241
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.11。失真模型（左列）与切线传播（右列）的比较。顶部行给出了学习曲线（误差与训练集遍历次数的关系）。底部行给出了网络的最终输入-输出函数；虚线是未修饰反向传播的结果。
- en: 'Two strategies are possible: either generate a small set of training points
    covering each of the plateaus (open squares on Figure 12.11 bottom), or generate
    one training point for each plateau (closed squares), and enforce local invariance
    around them (by setting the desired derivative to 0). The training set of the
    former method is used as a measure of performance for both methods. All parameters
    were adjusted for approximately optimal performance in all cases. The learning
    curves for both models are shown in Figure 12.11 (top). Each sweep through the
    training set for tangent prop is a little faster since it requires only 6 forward
    propagations, while it requires 9 in the distortion model. As can be seen, stable
    performance is achieved after 1300 sweeps for the tangent prop, versus 8000 for
    the distortion model. The overall speedup is therefore about 10.'
  id: totrans-3242
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种策略：要么生成一个小的训练点集，覆盖每个平台（图12.11底部的开放方块），要么为每个平台生成一个训练点（闭合方块），并在它们周围强制局部不变性（通过将所需导数设为0）。前一种方法的训练集用作两种方法的性能衡量。所有参数都调整为在所有情况下接近最佳性能。两个模型的学习曲线如图12.11（顶部）所示。切线传播每次通过训练集的速度稍快，因为仅需6次正向传播，而失真模型则需9次。可以看出，切线传播在1300次遍历后实现了稳定的性能，而失真模型则需8000次。因此，整体加速大约为10。
- en: Tangent prop in this example can take advantage of a very large regularization
    term. The distortion model is at a disadvantage because the only parameter that
    effectively controls the amount of regularization is the magnitude of the distortions,
    and this cannot be increased to large values because the right answer is only
    invariant under *small* distortions.
  id: totrans-3243
  prefs: []
  type: TYPE_NORMAL
  zh: 此例中的切线传播可以利用一个非常大的正则化项。失真模型处于不利地位，因为唯一有效控制正则化量的参数是失真的幅度，而这不能增加到大值，因为正确答案仅在*小*失真的情况下不变。
- en: 12.3.3 How To Make Tangent Prop Work
  id: totrans-3244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3.3 如何使切线传播有效
- en: 'Large Network Capacity: Relatively few experiments have been done with tangent
    propagation. It is clear, however, that the invariance constraint can be extremely
    beneficial. If the network does not have enough capacity, it will not benefit
    from the extra knowledge introduced by the invariance.'
  id: totrans-3245
  prefs: []
  type: TYPE_NORMAL
  zh: 大网络容量：切线传播的实验相对较少，但很明显，不变性约束可以极其有益。如果网络容量不足，则无法从不变性引入的额外知识中受益。
- en: 'Interleaving of the Tangent Vectors: Since the tangent vectors introduce even
    more correlation inside the training set, a substantial speed up can be obtained
    by alternating a regular forward and backward propagation with a tangent forward
    and backward propagation (even if there are several tangent vectors, only one
    is used at each pattern). For instance, if there were 3 tangent vectors, the training
    sequence could be:'
  id: totrans-3246
  prefs: []
  type: TYPE_NORMAL
  zh: 切线向量的交替：由于切线向量在训练集中引入了更多的相关性，通过交替常规的正向和反向传播与切线正向和反向传播可以获得显著的速度提升（即使有多个切线向量，在每个模式下只使用一个）。例如，如果有3个切线向量，则训练序列可以是：
- en: $$x_{1},t_{1}(x_{1}),x_{2},t_{2}(x_{2}),x_{3},t_{3}(x_{3}),x_{4},t_{1}(x_{4}),x_{5},t_{2}(x_{5}),\ldots\tag{12.38}$$
  id: totrans-3247
  prefs: []
  type: TYPE_NORMAL
  zh: $$x_{1},t_{1}(x_{1}),x_{2},t_{2}(x_{2}),x_{3},t_{3}(x_{3}),x_{4},t_{1}(x_{4}),x_{5},t_{2}(x_{5}),\ldots\tag{12.38}$$
- en: where xi means a forward and backward propagation for pattern i and tj (xi)
  id: totrans-3248
  prefs: []
  type: TYPE_NORMAL
  zh: 其中xi表示模式i的正向和反向传播，以及tj（xi）
- en: means a tangent forward and backward propagation of tangent vector j of pattern
    i. With such interleaving, the learning converges faster than grouping all the
    tangent vectors together. Of course, this only makes sense with on-line updates
    as opposed to batch updates.
  id: totrans-3249
  prefs: []
  type: TYPE_NORMAL
  zh: 代表模式i的切向量j的切线前向和后向传播。通过这样的交错，学习比将所有切向量聚集在一起收敛得更快。当然，这只有在在线更新而非批量更新时才有意义。
- en: 12.4 Tangent Vectors
  id: totrans-3250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.4 切向量
- en: In this section, we consider the general paradigm for transformation invariance
    and for the tangent vectors which have been used in the two previous sections.
  id: totrans-3251
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们考虑变换不变性和在前两节中使用的切向量的一般范例。
- en: Before we introduce each transformation and their corresponding tangent vectors,
    a brief explanation is given of the theory behind the practice. There are two
    aspects to the problem. First it is possible to establish a formal connection
    between groups of transformations of the input space (such as translation, rotation,
    etc. of 2) and their effect on a functional of that space (such as a mapping of
    2 to , which may represent an image, in continuous form). The theory of Lie groups
    and Lie algebra [6] allows us to do this. The second problem has to do with coding.
    Computer images are finite vectors of discrete variables. How can a theory which
    was developed for differentiable functionals of 2 to  be applied to these vectors?
    We first give a brief explanation of the theorems of Lie groups and Lie algebras
    which are applicable to pattern recognition. Next, we explore solutions to the
    coding problem. Finally some examples of transformation and coding are given for
    particular applications.
  id: totrans-3252
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们介绍每个变换及其相应的切向量之前，简要解释一下背后的理论。这个问题有两个方面。首先，可以在输入空间的变换群（例如2的平移、旋转等）与该空间函数（例如将2映射到，可能代表图像的连续形式）之间建立正式联系。李群和李代数的理论[6]使我们能够做到这一点。第二个问题与编码有关。计算机图像是有限的离散变量向量。如何将为可微函数开发的理论应用于这些向量？我们首先简要说明适用于模式识别的李群和李代数定理。接下来，我们探索编码问题的解决方案。最后，针对特定应用给出一些变换和编码的例子。
- en: 12.4.1 Lie Groups And Lie Algebras
  id: totrans-3253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.4.1 李群与李代数
- en: Consider an input space I (the plane 2 for example) and a differentiable function
    f which maps points of I to .
  id: totrans-3254
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个输入空间I（例如平面2）和一个可微函数f，它将I的点映射到。
- en: $$f:X\in I\longmapsto f(X)\in{\mathfrak{R}}.$$
  id: totrans-3255
  prefs: []
  type: TYPE_NORMAL
  zh: $$f:X\in I\longmapsto f(X)\in{\mathfrak{R}}.$$
- en: $\left(12.39\right)^{\frac{1}{2}}$
  id: totrans-3256
  prefs: []
  type: TYPE_NORMAL
  zh: $\left(12.39\right)^{\frac{1}{2}}$
- en: 'f : X ∈ I −→ f(X) ∈ . (12.39)'
  id: totrans-3257
  prefs: []
  type: TYPE_NORMAL
  zh: 'f : X ∈ I −→ f(X) ∈ . (12.39)'
- en: The function f(X) = f(*x, y*) can be interpreted as the continuous (defined
    for all points of 2) equivalent of the discrete computer image P[*i, j*].
  id: totrans-3258
  prefs: []
  type: TYPE_NORMAL
  zh: 函数f(X) = f(*x, y*)可以被解释为离散计算机图像P[*i, j*]的连续（在2的所有点上定义的）等价物。
- en: Next, consider a family of transformations tα, parameterized by α, which maps
    bijectively a point of I to a point of I
  id: totrans-3259
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，考虑一系列由α参数化的变换tα，它将输入空间I中的点双射地映射到I中的点。
- en: $$t_{\alpha}:X\in{\mathcal{I}}\longmapsto t_{\alpha}(X)\in{\mathcal{I}}.$$
  id: totrans-3260
  prefs: []
  type: TYPE_NORMAL
  zh: $$t_{\alpha}:X\in{\mathcal{I}}\longmapsto t_{\alpha}(X)\in{\mathcal{I}}.$$
- en: $$(12.40)$$
  id: totrans-3261
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.40)$$
- en: 'We assume that tα is differentiable with respect to α and X, and that t0 is
    the identity. For example tα could be the group of affine transformations of 2:'
  id: totrans-3262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设tα相对于α和X是可微的，并且t0是单位元。例如，tα可以是2的仿射变换群：
- en: $$t_{\alpha}:\left(\begin{matrix}x\\ y\end{matrix}\right)\longmapsto\left(\begin{matrix}x+\alpha_{1}x+\alpha_{2}y+\alpha_{5}\\
    \alpha_{3}x+y+\alpha_{4}y+\alpha_{6}\end{matrix}\right)\quad\text{with}\quad\left|\begin{matrix}1+\alpha_{1}&\alpha_{2}\\
    \alpha_{3}&1+\alpha_{4}\end{matrix}\right|\neq0.\tag{12.41}$$
  id: totrans-3263
  prefs: []
  type: TYPE_NORMAL
  zh: $$t_{\alpha}:\left(\begin{matrix}x\\ y\end{matrix}\right)\longmapsto\left(\begin{matrix}x+\alpha_{1}x+\alpha_{2}y+\alpha_{5}\\
    \alpha_{3}x+y+\alpha_{4}y+\alpha_{6}\end{matrix}\right)\quad\text{且}\quad\left|\begin{matrix}1+\alpha_{1}&\alpha_{2}\\
    \alpha_{3}&1+\alpha_{4}\end{matrix}\right|\neq0.\tag{12.41}$$
- en: 'This is a Lie group4 with 6 parameters. Another example is the group of direct
    isometry:'
  id: totrans-3264
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个具有6个参数的李群。另一个例子是直接等距群：
- en: $$t_{\alpha}:{\binom{x}{y}}\longmapsto{\binom{x\cos\theta-y\sin\theta+a}{x\sin\theta+y\cos\theta+b}}$$
  id: totrans-3265
  prefs: []
  type: TYPE_NORMAL
  zh: $$t_{\alpha}:{\binom{x}{y}}\longmapsto{\binom{x\cos\theta-y\sin\theta+a}{x\sin\theta+y\cos\theta+b}}$$
- en: $$(12.42)$$
  id: totrans-3266
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.42)$$
- en: which is a Lie group with 3 parameters.
  id: totrans-3267
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个具有3个参数的李群。
- en: We now consider the functional s(f,α), defined by
  id: totrans-3268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在考虑由以下定义的函数s(f,α)：
- en: $$s(f,\alpha)=f\circ t_{\alpha}^{-1}.$$
  id: totrans-3269
  prefs: []
  type: TYPE_NORMAL
  zh: $$s(f,\alpha)=f\circ t_{\alpha}^{-1}.$$
- en: α . (12.43)
  id: totrans-3270
  prefs: []
  type: TYPE_NORMAL
  zh: α . (12.43)
- en: This functional s, which takes another functional f as an argument, should remind
    the reader of Figure 12.2 where P, the discrete equivalent of f, is the argument
    of s.
  id: totrans-3271
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数 s，以另一个函数 f 作为参数，应该让读者想起图 12.2，其中 P 是 f 的离散等价物，是 s 的参数。
- en: 'The Lie algebra associated with the action of tα on f is the space generated
    by the m local transformations Lαi of f defined by:'
  id: totrans-3272
  prefs: []
  type: TYPE_NORMAL
  zh: 与 tα 对 f 的作用相关的李代数是由 f 的 m 个局部变换 Lαi 生成的空间，定义为：
- en: $$(12.43)$$
  id: totrans-3273
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.43)$$
- en: $$L_{a_{i}}(f)=\left.\frac{\partial s(f,\alpha)}{\partial\alpha_{i}}\right|_{\alpha=0}.$$
  id: totrans-3274
  prefs: []
  type: TYPE_NORMAL
  zh: $$L_{a_{i}}(f)=\left.\frac{\partial s(f,\alpha)}{\partial\alpha_{i}}\right|_{\alpha=0}.$$
- en: $$(12.44)$$
  id: totrans-3275
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.44)$$
- en: $$(12.47)$$
  id: totrans-3276
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.47)$$
- en: 'We can now write the local approximation of s as:'
  id: totrans-3277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以写出 s 的局部近似为：
- en: s(f,α) = f + α1Lα1 (f) + α2Lα2 (f) + ··· + αmLαm(f) + o(α2)(f). (12.45)
  id: totrans-3278
  prefs: []
  type: TYPE_NORMAL
  zh: s(f,α) = f + α1Lα1 (f) + α2Lα2 (f) + ··· + αmLαm(f) + o(α2)(f)。 (12.45)
- en: This equation is the continuous equivalent of equation (12.2) used in the introduction.
  id: totrans-3279
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程是引言中使用的方程 (12.2) 的连续等价物。
- en: The following example illustrates how Lαi can be computed from tα. Let's consider
    the group of direct isometry defined in equation (12.42) (with parameter α = (*θ,
    a, b*) as before, and X = (*x, y*))
  id: totrans-3280
  prefs: []
  type: TYPE_NORMAL
  zh: 以下例子说明如何从 tα 计算 Lαi。让我们考虑方程 (12.42) 中定义的直接等距群（参数 α = (*θ, a, b*），与之前相同，X = (*x,
    y*)）。
- en: $$((x-6))$$
  id: totrans-3281
  prefs: []
  type: TYPE_NORMAL
  zh: $$((x-6))$$
- en: s(f,α)(X) = f((x−a) cos θ + (y −b) sin θ, −(x−a) sin θ + (y −b) cos θ). (12.46)
  id: totrans-3282
  prefs: []
  type: TYPE_NORMAL
  zh: s(f,α)(X) = f((x−a) cos θ + (y −b) sin θ, −(x−a) sin θ + (y −b) cos θ)。 (12.46)
- en: If we differentiate around α = (0, 0, 0) with respect to θ, we obtain
  id: totrans-3283
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在 α = (0, 0, 0) 附近对 θ 进行微分，我们得到
- en: $${\frac{\partial s(f,\alpha)}{\partial\theta}}(X)=y{\frac{\partial f}{\partial
    x}}(x,y)+(-x){\frac{\partial f}{\partial y}}(x,y)$$
  id: totrans-3284
  prefs: []
  type: TYPE_NORMAL
  zh: $${\frac{\partial s(f,\alpha)}{\partial\theta}}(X)=y{\frac{\partial f}{\partial
    x}}(x,y)+(-x){\frac{\partial f}{\partial y}}(x,y)$$
- en: ∂y (*x, y*) (12.47)
  id: totrans-3285
  prefs: []
  type: TYPE_NORMAL
  zh: ∂y (*x, y*) (12.47)
- en: 4 A Lie group is a group that is also a differentiable manifold such that the
    differentiable structure is compatible with the group structure.
  id: totrans-3286
  prefs: []
  type: TYPE_NORMAL
  zh: 4 李群是一个也是可微流形的群，且可微结构与群结构兼容。
- en: that is
  id: totrans-3287
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说
- en: $$L_{\theta}=y{\frac{\partial}{\partial x}}+(-x){\frac{\partial}{\partial y}}.$$
  id: totrans-3288
  prefs: []
  type: TYPE_NORMAL
  zh: $$L_{\theta}=y{\frac{\partial}{\partial x}}+(-x){\frac{\partial}{\partial y}}.$$
- en: $$(12.48)$$
  id: totrans-3289
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.48)$$
- en: The transformation La = − ∂∂x and Lb = − ∂∂y can be obtained in a similar fashion.
    All local transformations of the group can be written as
  id: totrans-3290
  prefs: []
  type: TYPE_NORMAL
  zh: 变换 La = − ∂∂x 和 Lb = − ∂∂y 可以以类似的方式获得。群的所有局部变换可以写成
- en: in. An local transformation of the group can be written as  $$s(f,\alpha)=f+\theta(y\frac{\partial
    f}{\partial x}+(-x)\frac{\partial f}{\partial y})-a\frac{\partial f}{\partial
    x}-b\frac{\partial f}{\partial y}+o(\|\alpha\|^{2})(f)\tag{12.49}$$  $\alpha$
    is a constant, $\alpha$ is a constant, $\alpha$ is a constant.
  id: totrans-3291
  prefs: []
  type: TYPE_NORMAL
  zh: 在。一个群的局部变换可以写成 $$s(f,\alpha)=f+\theta(y\frac{\partial f}{\partial x}+(-x)\frac{\partial
    f}{\partial y})-a\frac{\partial f}{\partial x}-b\frac{\partial f}{\partial y}+o(\|\alpha\|^{2})(f)\tag{12.49}$$  $\alpha$
    是一个常数，$\alpha$ 是一个常数，$\alpha$ 是一个常数。
- en: which corresponds to a linear combination of the 3 basic operators Lθ, La and
    Lb 5. The property which is most important to us is that the 3 operators generate
    the whole space of local transformations. The result of applying the operators
    to a function f, such as a 2D image for example, is the set of vectors which we
    have been calling "tangent vector" in the previous sections. Each point in the
    tangent space correspond to a unique transformation and conversely any transformation
    of the Lie group (in the example all rotations of any angle and center together
    with all translations) corresponds to a point in the tangent plane.
  id: totrans-3292
  prefs: []
  type: TYPE_NORMAL
  zh: 这对应于 3 个基本算子 Lθ、La 和 Lb 的线性组合。对我们来说最重要的特性是这 3 个算子生成了局部变换的整个空间。将算子应用于一个函数 f 的结果，例如一个
    2D 图像，是我们在前几节中所称的“切向量”的集合。切空间中的每个点对应于一个唯一的变换，反之，李群的任何变换（在这个例子中是任意角度和中心的所有旋转以及所有平移）对应于切平面中的一个点。
- en: 12.4.2 Tangent Vectors
  id: totrans-3293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.4.2 切向量
- en: The last problem which remains to be solved is the problem of coding. Computer
    images, for instance, are coded as a finite set of discrete (even binary) values.
  id: totrans-3294
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个需要解决的问题是编码问题。例如，计算机图像被编码为有限的一组离散（甚至是二进制）值。
- en: These are hardly the differentiable mappings of I to  which we have been assuming
    in the previous subsection.
  id: totrans-3295
  prefs: []
  type: TYPE_NORMAL
  zh: 这些几乎不是我们在上一小节假设的从 I 到 的可微映射。
- en: To solve this problem we introduce a smooth interpolating function C which maps
    the discrete vectors to a continuous mapping of I to . For example, if P
  id: totrans-3296
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们引入一个光滑的插值函数 C，它将离散向量映射到 I 的连续映射。例如，如果 P
- en: is a image of n pixels, it can be mapped to a continuously valued function f
    over 2 by convolving it with a two dimensional Gaussian function gσ of standard
    deviation σ. This is because gσ is a differentiable mapping of 2 to , and P
  id: totrans-3297
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个 n 像素的图像，它可以通过与标准差为 σ 的二维高斯函数 gσ 进行卷积，映射为一个连续值的函数 f。这是因为 gσ 是从 2 到 的可微映射，而
    P
- en: 'can be interpreted as a sum of impulse functions. In the two dimensional case
    we can write the new interpretation of P as:'
  id: totrans-3298
  prefs: []
  type: TYPE_NORMAL
  zh: 可以被解释为脉冲函数的和。在二维情况下，我们可以将 P 的新解释写为：
- en: $$P^{\prime}(x,y)=\sum_{i,j}P[i][j]\delta(x-i)\delta(y-j)$$
  id: totrans-3299
  prefs: []
  type: TYPE_NORMAL
  zh: $$P^{\prime}(x,y)=\sum_{i,j}P[i][j]\delta(x-i)\delta(y-j)$$
- en: $$(12.50)$$
  id: totrans-3300
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.50)$$
- en: 'where P[i][j] denotes the finite vector of discrete values, as stored in a
    computer. The result of the convolution is of course differentiable because it
    is a sum of Gaussian functions. The Gaussian mapping is given by:'
  id: totrans-3301
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 P[i][j] 表示计算机中存储的离散值的有限向量。卷积的结果当然是可微的，因为它是高斯函数的和。高斯映射由以下给出：
- en: $$C_{\sigma}:P\longmapsto f=P^{\prime}*g_{\sigma}.$$
  id: totrans-3302
  prefs: []
  type: TYPE_NORMAL
  zh: $$C_{\sigma}:P\longmapsto f=P^{\prime}*g_{\sigma}.$$
- en: 'Cσ : P −→ f = P ∗ gσ. (12.51)'
  id: totrans-3303
  prefs: []
  type: TYPE_NORMAL
  zh: 'Cσ : P −→ f = P ∗ gσ. (12.51)'
- en: 'In the two dimensional case, the function f can be written as:'
  id: totrans-3304
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维情况下，函数 f 可以写为：
- en: 'In case, the function ƒ can be written as:  $ f(x,y)=\sum_{i,j}P[i][j]g_\sigma(x-i,y-j)$.'
  id: totrans-3305
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，函数 ƒ 可以写为： $ f(x,y)=\sum_{i,j}P[i][j]g_\sigma(x-i,y-j)$。
- en: $$(12.51)$$
  id: totrans-3306
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.51)$$
- en: $$(12.52)$$
  id: totrans-3307
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.52)$$
- en: 5 These operators are said to generate a Lie algebra, because on top of the
    addition and multiplication by a scalar, there is a special multiplication called
    "Lie bracket" defined by [L1, L2] = L1 ◦L2 − L2 ◦L1. In the above example we have
    [Lθ, La] = Lb,
  id: totrans-3308
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算子被称为生成一个李代数，因为在加法和标量乘法之上，还有一种称为“李括号”的特殊乘法，定义为 [L1, L2] = L1 ◦L2 − L2 ◦L1。在上面的例子中，我们有
    [Lθ, La] = Lb，
- en: '[La, Lb]=0, and [Lb, Lθ] = La.'
  id: totrans-3309
  prefs: []
  type: TYPE_NORMAL
  zh: '[La, Lb]=0，且 [Lb, Lθ] = La。'
- en: '![259_image_0.png](259_image_0.png)'
  id: totrans-3310
  prefs: []
  type: TYPE_IMG
  zh: '![259_image_0.png](259_image_0.png)'
- en: Fig. 12.12. Graphic illustration of the computation of f and two tangent vectors
    corresponding to Lx = *∂/∂x* (X-translation) and Lx = *∂/∂y* (Y-translation),
    from a binary image I. The Gaussian function *g(x, y*) = exp(−x2+y2 2σ2 ) has
    a standard deviation of σ = 0.9 in this example although its graphic representation
    (small images on the right)
  id: totrans-3311
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.12。图形示例展示了从二值图像 I 计算 f 和两个切向量的过程，分别对应于 Lx = *∂/∂x*（X 方向平移）和 Lx = *∂/∂y*（Y
    方向平移）。高斯函数 *g(x, y)* = exp(−x²+y² 2σ²) 在这个例子中标准差 σ = 0.9，尽管它的图形表示（右侧的小图像）
- en: have been rescaled for clarity.
  id: totrans-3312
  prefs: []
  type: TYPE_NORMAL
  zh: 已经为了清晰度进行了重新缩放。
- en: Other coding functions C can be used, such as cubic spline or even bilinear
    interpolation. Bilinear interpolation between the pixels yields a function f which
    is differentiable almost everywhere. The fact that the derivatives have two values
    at the integer locations (because the bilinear interpolation is different on both
    side of each pixels) is not a problem in practice - just choose one of the two
    values.
  id: totrans-3313
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用其他编码函数 C，例如立方样条或双线性插值。像素之间的双线性插值产生的函数 f 在几乎所有地方都是可微的。由于导数在整数位置有两个值（因为双线性插值在每个像素的两侧不同），在实践中这并不是问题——只需选择两个值中的一个即可。
- en: 'The Gaussian mapping is preferred for two reasons: First, the smoothing parameter
    σ can be used to control the locality of the invariance. This is because when
    f is smoother, the local approximation of equation (12.45) is valid for larger
    transformations. And second, when combined with the transformation operator L,
    the derivative can be applied on the closed form of the Gaussian function.'
  id: totrans-3314
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯映射被优先选择有两个原因：首先，平滑参数 σ 可用于控制不变性的局部性。这是因为当 f 更平滑时，方程 (12.45) 的局部近似在更大变换下有效。其次，当与变换算子
    L 结合时，导数可以在高斯函数的封闭形式上应用。
- en: 'For instance, if the X-translation operator L = ∂∂x is applied to f = P ∗ gσ,
    the actual computation becomes:'
  id: totrans-3315
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果将 X 平移算子 L = ∂/∂x 应用于 f = P ∗ gσ，则实际计算变为：
- en: $$L_{X}(f)=\frac{\partial}{\partial x}(P^{\prime}*g_{\sigma})=P^{\prime}*\frac{\partial
    g_{\sigma}}{\partial x}.\tag{12.53}$$
  id: totrans-3316
  prefs: []
  type: TYPE_NORMAL
  zh: $$L_{X}(f)=\frac{\partial}{\partial x}(P^{\prime}*g_{\sigma})=P^{\prime}*\frac{\partial
    g_{\sigma}}{\partial x}.\tag{12.53}$$
- en: because of the differentiation properties of convolution when the support is
    compact. This is easily done by convolving the original image with the X-derivative
    of the Gaussian function gσ. This operation is illustrated in Figure 12.12. Similarly,
    the tangent vector for scaling can be computed with
  id: totrans-3317
  prefs: []
  type: TYPE_NORMAL
  zh: 由于当支撑是紧致时卷积的微分性质。这可以通过将原始图像与高斯函数 gσ 的 X 导数进行卷积来轻松实现。此操作在图 12.12 中有所示例。类似地，缩放的切向量可以通过计算得出。
- en: $$L_{S}(f)=\left(x\frac{\partial}{\partial x}+y\frac{\partial}{\partial y}\right)(I*g_{\sigma})=x(I*\frac{\partial
    g_{\sigma}}{\partial x})+y(I*\frac{\partial g_{\sigma}}{\partial y}).\tag{12.54}$$
  id: totrans-3318
  prefs: []
  type: TYPE_NORMAL
  zh: $$L_{S}(f)=\left(x\frac{\partial}{\partial x}+y\frac{\partial}{\partial y}\right)(I*g_{\sigma})=x(I*\frac{\partial
    g_{\sigma}}{\partial x})+y(I*\frac{\partial g_{\sigma}}{\partial y}).\tag{12.54}$$
- en: '![260_image_0.png](260_image_0.png)'
  id: totrans-3319
  prefs: []
  type: TYPE_IMG
  zh: '![260_image_0.png](260_image_0.png)'
- en: This operation is illustrated in Figure 12.13.
  id: totrans-3320
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作在图12.13中进行了说明。
- en: Fig. 12.13. Graphic illustration of the computation of the tangent vector Tu
    = DxSx+
  id: totrans-3321
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.13。切向量 Tu = DxSx+ 的计算图示
- en: DySy (bottom image). In this example the displacement for each pixel is proportional
    to the distance of the pixel to the center of the image (Dx(*x, y*) = x−x0 and
    Dy(*x, y*) =
  id: totrans-3322
  prefs: []
  type: TYPE_NORMAL
  zh: DySy（底部图像）。在这个例子中，每个像素的位移与该像素到图像中心的距离成正比（Dx(*x, y*) = x−x0 和 Dy(*x, y*) =
- en: y −y0). The two multiplications (horizontal lines) as well as the addition (vertical
    right column) are done pixel by pixel.
  id: totrans-3323
  prefs: []
  type: TYPE_NORMAL
  zh: y −y0）。这两次乘法（横线）以及加法（垂直右列）是逐像素完成的。
- en: 12.4.3 Important Transformations In Image Processing
  id: totrans-3324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.4.3 图像处理中的重要变换
- en: 'This section summarizes how to compute the tangent vectors for image processing
    (in 2D). Each discrete image Ii is convolved with a Gaussian of standard deviation
    gσ to obtain a representation of the continuous image fi, according to equation:'
  id: totrans-3325
  prefs: []
  type: TYPE_NORMAL
  zh: 本节总结了如何计算图像处理（在二维中）的切向量。每个离散图像 Ii 与标准差为 gσ 的高斯进行卷积，以根据公式获得连续图像 fi 的表示：
- en: fi = Ii ∗ gσ. (12.55)
  id: totrans-3326
  prefs: []
  type: TYPE_NORMAL
  zh: fi = Ii ∗ gσ. (12.55)
- en: 'The resulting image fi will be used in all the computations requiring Ii (except
    for computing the tangent vector). For each image Ii, the tangent vectors are
    computed by applying the operators corresponding to the transformations of interest
    to the expression Ii ∗ gσ. The result, which can be precomputed, is an image which
    is the tangent vector. The following list contains some of the most useful tangent
    vectors: X-translation: This transformation is useful when the classification
    function is known to be invariant with respect to the input transformation:'
  id: totrans-3327
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图像 fi 将用于所有需要 Ii 的计算（除计算切向量外）。对于每个图像 Ii，切向量通过将相应于感兴趣变换的算子应用于表达式 Ii ∗ gσ 来计算。结果可以预计算，是一个切向量图像。以下列表包含一些最有用的切向量：X-平移：当分类函数已知对输入变换不变时，此变换非常有用：
- en: $$f_{i}=I_{i}*g_{\sigma}.$$
  id: totrans-3328
  prefs: []
  type: TYPE_NORMAL
  zh: $$f_{i}=I_{i}*g_{\sigma}.$$
- en: $$t_{\alpha}:{\binom{x}{y}}\longmapsto{\binom{x+\alpha}{y}}\,.$$
  id: totrans-3329
  prefs: []
  type: TYPE_NORMAL
  zh: $$t_{\alpha}:{\binom{x}{y}}\longmapsto{\binom{x+\alpha}{y}}\,.$$
- en: $\left(12.55\right)$ .
  id: totrans-3330
  prefs: []
  type: TYPE_NORMAL
  zh: $\left(12.55\right)$ 。
- en: $$(12.56)$$
  id: totrans-3331
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.56)$$
- en: . (12.56)
  id: totrans-3332
  prefs: []
  type: TYPE_NORMAL
  zh: . (12.56)
- en: 'The Lie operator is defined by:'
  id: totrans-3333
  prefs: []
  type: TYPE_NORMAL
  zh: Lie算子定义为：
- en: $$L_{X}={\frac{\partial}{\partial x}}.$$
  id: totrans-3334
  prefs: []
  type: TYPE_NORMAL
  zh: $$L_{X}={\frac{\partial}{\partial x}}.$$
- en: $$(12.57)$$
  id: totrans-3335
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.57)$$
- en: LX = ∂∂x. (12.57)
  id: totrans-3336
  prefs: []
  type: TYPE_NORMAL
  zh: LX = ∂∂x. (12.57)
- en: 'Y-translation: This transformation is useful when the classification function
    is known to be invariant with respect to the input transformation:'
  id: totrans-3337
  prefs: []
  type: TYPE_NORMAL
  zh: Y-平移：当分类函数已知对输入变换不变时，此变换非常有用：
- en: $$t_{\alpha}:{\binom{x}{y}}\longmapsto{\binom{x}{y+\alpha}}\,.$$
  id: totrans-3338
  prefs: []
  type: TYPE_NORMAL
  zh: $$t_{\alpha}:{\binom{x}{y}}\longmapsto{\binom{x}{y+\alpha}}\,.$$
- en: . (12.58)
  id: totrans-3339
  prefs: []
  type: TYPE_NORMAL
  zh: . (12.58)
- en: 'The Lie operator is defined by:'
  id: totrans-3340
  prefs: []
  type: TYPE_NORMAL
  zh: Lie算子定义为：
- en: $$(12.58)$$
  id: totrans-3341
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.58)$$
- en: $$L_{Y}={\frac{\partial}{\partial y}}.$$
  id: totrans-3342
  prefs: []
  type: TYPE_NORMAL
  zh: $$L_{Y}={\frac{\partial}{\partial y}}.$$
- en: $$(12.59)$$
  id: totrans-3343
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.59)$$
- en: $$(12.60)$$
  id: totrans-3344
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.60)$$
- en: LY = ∂∂y . (12.59)
  id: totrans-3345
  prefs: []
  type: TYPE_NORMAL
  zh: LY = ∂∂y . (12.59)
- en: 'Rotation: This transformation is useful when the classification function is
    known to be invariant with respect to the input transformation:'
  id: totrans-3346
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转：当分类函数已知对输入变换不变时，此变换非常有用：
- en: $$t_{\alpha}:{\binom{x}{y}}\longmapsto{\binom{x\cos\alpha-y\sin\alpha}{x\sin\alpha+y\cos\alpha}}\,.$$
  id: totrans-3347
  prefs: []
  type: TYPE_NORMAL
  zh: $$t_{\alpha}:{\binom{x}{y}}\longmapsto{\binom{x\cos\alpha-y\sin\alpha}{x\sin\alpha+y\cos\alpha}}\,.$$
- en: . (12.60)
  id: totrans-3348
  prefs: []
  type: TYPE_NORMAL
  zh: . (12.60)
- en: 'The Lie operator is defined by:'
  id: totrans-3349
  prefs: []
  type: TYPE_NORMAL
  zh: Lie算子定义为：
- en: $$L_{R}=y{\frac{\partial}{\partial x}}+(-x){\frac{\partial}{\partial y}}.$$
  id: totrans-3350
  prefs: []
  type: TYPE_NORMAL
  zh: $$L_{R}=y{\frac{\partial}{\partial x}}+(-x){\frac{\partial}{\partial y}}.$$
- en: $$(12.61)$$
  id: totrans-3351
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.61)$$
- en: 'Scaling: This transformation is useful when the classification function is
    known to be invariant with respect to the input transformation:'
  id: totrans-3352
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放：当分类函数已知对输入变换不变时，此变换非常有用：
- en: $$t_{\alpha}:{\binom{x}{y}}\longmapsto{\binom{x+\alpha x}{y+\alpha y}}.$$
  id: totrans-3353
  prefs: []
  type: TYPE_NORMAL
  zh: $$t_{\alpha}:{\binom{x}{y}}\longmapsto{\binom{x+\alpha x}{y+\alpha y}}.$$
- en: y + αy . (12.62)
  id: totrans-3354
  prefs: []
  type: TYPE_NORMAL
  zh: y + αy . (12.62)
- en: 'The Lie operator is defined by:'
  id: totrans-3355
  prefs: []
  type: TYPE_NORMAL
  zh: Lie算子定义为：
- en: $$(12.62)$$
  id: totrans-3356
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.62)$$
- en: $$L_{S}=x{\frac{\partial}{\partial x}}+y{\frac{\partial}{\partial y}}.$$
  id: totrans-3357
  prefs: []
  type: TYPE_NORMAL
  zh: $$L_{S}=x{\frac{\partial}{\partial x}}+y{\frac{\partial}{\partial y}}.$$
- en: $$(12.63)$$
  id: totrans-3358
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.63)$$
- en: ∂y . (12.63)
  id: totrans-3359
  prefs: []
  type: TYPE_NORMAL
  zh: ∂y . (12.63)
- en: 'Parallel hyperbolic transformation: This transformation is useful when the
    classification function is known to be invariant with respect to the input transformation:'
  id: totrans-3360
  prefs: []
  type: TYPE_NORMAL
  zh: 平行双曲变换：当分类函数已知对输入变换不变时，这种变换非常有用：
- en: $$t_{\alpha}:{\binom{x}{y}}\longmapsto{\binom{x+\alpha x}{y-\alpha y}}.$$
  id: totrans-3361
  prefs: []
  type: TYPE_NORMAL
  zh: $$t_{\alpha}:{\binom{x}{y}}\longmapsto{\binom{x+\alpha x}{y-\alpha y}}.$$
- en: y − αy . (12.64)
  id: totrans-3362
  prefs: []
  type: TYPE_NORMAL
  zh: y − αy . (12.64)
- en: 'The Lie operator is defined by:'
  id: totrans-3363
  prefs: []
  type: TYPE_NORMAL
  zh: 李算子定义为：
- en: $$(12.64)$$
  id: totrans-3364
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.64)$$
- en: $$L_{S}=x{\frac{\partial}{\partial x}}-y{\frac{\partial}{\partial y}}.$$
  id: totrans-3365
  prefs: []
  type: TYPE_NORMAL
  zh: $$L_{S}=x{\frac{\partial}{\partial x}}-y{\frac{\partial}{\partial y}}.$$
- en: $$(12.65)$$
  id: totrans-3366
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.65)$$
- en: ∂y . (12.65)
  id: totrans-3367
  prefs: []
  type: TYPE_NORMAL
  zh: ∂y . (12.65)
- en: 'Diagonal hyperbolic transformation: This transformation is useful when the
    classification function is known to be invariant with respect to the input transformation:'
  id: totrans-3368
  prefs: []
  type: TYPE_NORMAL
  zh: 对角双曲变换：当分类函数已知对输入变换不变时，这种变换非常有用：
- en: $$t_{\alpha}:\left(\begin{matrix}x\\ y\end{matrix}\right)\longmapsto\left(\begin{matrix}x+\alpha
    y\\ y+\alpha x\end{matrix}\right).\tag{12.66}$$
  id: totrans-3369
  prefs: []
  type: TYPE_NORMAL
  zh: $$t_{\alpha}:\left(\begin{matrix}x\\ y\end{matrix}\right)\longmapsto\left(\begin{matrix}x+\alpha
    y\\ y+\alpha x\end{matrix}\right).\tag{12.66}$$
- en: 'The Lie operator is defined by:'
  id: totrans-3370
  prefs: []
  type: TYPE_NORMAL
  zh: 李算子定义为：
- en: $L_{S}=y\frac{\partial}{\partial x}+x\frac{\partial}{\partial y}$. (12.67)
  id: totrans-3371
  prefs: []
  type: TYPE_NORMAL
  zh: $L_{S}=y\frac{\partial}{\partial x}+x\frac{\partial}{\partial y}$. (12.67)
- en: The resulting tangent vector is is the norm of the gradient of the image, which
    is very easy to compute.
  id: totrans-3372
  prefs: []
  type: TYPE_NORMAL
  zh: 结果切向量是图像的梯度的范数，这个计算非常简单。
- en: 'Thickening: This transformation is useful when the classification function
    is known to be invariant with respect to variation of thickness. This is known
    in morphology as dilation and its inverse, erosion. It is very useful in certain
    domains (such as handwritten character recognition because) thickening and thinning
    are natural variations which correspond to the pressure applied on a pen, or to
    different absorbtion properties of the ink on the paper. A dilation (resp. erosion)
    can be defined as the operation of replacing each value f(x,y)'
  id: totrans-3373
  prefs: []
  type: TYPE_NORMAL
  zh: 加厚：当分类函数已知对厚度变化不变时，这种变换非常有用。在形态学中，这被称为膨胀及其逆过程，腐蚀。在某些领域（如手写字符识别）非常有用，因为加厚和减薄是自然变化，对应于施加在笔上的压力，或墨水在纸上的不同吸收特性。膨胀（或腐蚀）可以定义为替换每个值
    f(x,y) 的操作。
- en: 'by the largest (resp. smallest) value of f(x, y) found within a neighborhood
    of a certain shape, centered at (*x, y*). The region is called the structural
    element. We will assume that the structural element is a sphere of radius α. We
    define the thickening transformation as the function which takes the function
    f and generates the function fα defined by:'
  id: totrans-3374
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在以 (*x, y*) 为中心的某种形状的邻域内找到 f(x, y) 的最大（或最小）值。这个区域称为结构元素。我们将假设结构元素是半径为 α 的球体。我们将加厚变换定义为将函数
    f 转换为函数 fα 的函数，其定义为：
- en: $$f_{\alpha}^{\prime}(X)=\operatorname*{max}_{\|r\|\leq\alpha}f(X+r)\quad{\mathrm{for~}}\alpha\geq0$$
  id: totrans-3375
  prefs: []
  type: TYPE_NORMAL
  zh: $$f_{\alpha}^{\prime}(X)=\operatorname*{max}_{\|r\|\leq\alpha}f(X+r)\quad{\mathrm{for~}}\alpha\geq0$$
- en: $$(12.68)$$
  id: totrans-3376
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.68)$$
- en: $$(12.69)$$
  id: totrans-3377
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.69)$$
- en: $$f_{\alpha}^{\prime}(X)=\operatorname*{min}_{\|r\|\leq-\alpha}f(X+r)\quad{\mathrm{for~}}\alpha\leq0.$$
  id: totrans-3378
  prefs: []
  type: TYPE_NORMAL
  zh: $$f_{\alpha}^{\prime}(X)=\operatorname*{min}_{\|r\|\leq-\alpha}f(X+r)\quad{\mathrm{for~}}\alpha\leq0.$$
- en: $$(12.70)$$
  id: totrans-3379
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.70)$$
- en: 'The derivative of the thickening for α ≥ 0 can be written as:'
  id: totrans-3380
  prefs: []
  type: TYPE_NORMAL
  zh: 当 α ≥ 0 时，加厚的导数可以写为：
- en: $$\operatorname*{lim}_{\alpha\longrightarrow0}{\frac{f^{\prime}(X)-f(X)}{\alpha}}=\operatorname*{lim}_{\alpha\longrightarrow0}{\frac{\operatorname*{max}_{\|r\|\leq\alpha}f(X+r)-f(X)}{\alpha}}.$$
  id: totrans-3381
  prefs: []
  type: TYPE_NORMAL
  zh: $$\operatorname*{lim}_{\alpha\longrightarrow0}{\frac{f^{\prime}(X)-f(X)}{\alpha}}=\operatorname*{lim}_{\alpha\longrightarrow0}{\frac{\operatorname*{max}_{\|r\|\leq\alpha}f(X+r)-f(X)}{\alpha}}.$$
- en: α . (12.70)
  id: totrans-3382
  prefs: []
  type: TYPE_NORMAL
  zh: α . (12.70)
- en: 'f(X) can be put within the max expression because it does not depend on r.
    Since α tends toward 0, we can write:'
  id: totrans-3383
  prefs: []
  type: TYPE_NORMAL
  zh: f(X) 可以放入最大值表达式中，因为它不依赖于 r。由于 α 趋向于 0，我们可以写：
- en: $$f(X+r)-f(X)=r\cdot\nabla f(X)+O(\|r\|^{2})\approx r\cdot\nabla f(X).$$
  id: totrans-3384
  prefs: []
  type: TYPE_NORMAL
  zh: $$f(X+r)-f(X)=r\cdot\nabla f(X)+O(\|r\|^{2})\approx r\cdot\nabla f(X).$$
- en: The maximum of
  id: totrans-3385
  prefs: []
  type: TYPE_NORMAL
  zh: 最大值为
- en: $$\operatorname*{max}_{\|r\|\leq\alpha}f(X+r)-f(X)=\operatorname*{max}_{\|r\|\leq\alpha}r\cdot\nabla
    f(X)$$
  id: totrans-3386
  prefs: []
  type: TYPE_NORMAL
  zh: $$\operatorname*{max}_{\|r\|\leq\alpha}f(X+r)-f(X)=\operatorname*{max}_{\|r\|\leq\alpha}r\cdot\nabla
    f(X)$$
- en: r · ∇f(X) (12.72)
  id: totrans-3387
  prefs: []
  type: TYPE_NORMAL
  zh: r · ∇f(X) (12.72)
- en: is attained when r and ∇f(X) are co-linear, that is when
  id: totrans-3388
  prefs: []
  type: TYPE_NORMAL
  zh: 当 r 和 ∇f(X) 处于共线时，即当
- en: $$(12.71)$$
  id: totrans-3389
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.71)$$
- en: $$(12.72)$$
  id: totrans-3390
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.72)$$
- en: $$r=\alpha{\frac{\nabla f(X)}{\|\nabla f(X)\|}}$$
  id: totrans-3391
  prefs: []
  type: TYPE_NORMAL
  zh: $$r=\alpha{\frac{\nabla f(X)}{\|\nabla f(X)\|}}$$
- en: $$(12.73)$$
  id: totrans-3392
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.73)$$
- en: ∇f(X) (12.73)
  id: totrans-3393
  prefs: []
  type: TYPE_NORMAL
  zh: ∇f(X) (12.73)
- en: 'assuming α ≥ 0. It can easily be shown that this equation holds when α is negative,
    because we then try to minimize equation (12.69). We therefore have:'
  id: totrans-3394
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 α ≥ 0。可以很容易地证明，当 α 为负时该方程仍然成立，因为我们试图最小化方程 (12.69)。因此，我们有：
- en: $$\operatorname*{lim}_{\alpha\longrightarrow0}{\frac{f_{\alpha}^{\prime}(X)-f(X)}{\alpha}}=\|\nabla
    f(X)\|$$
  id: totrans-3395
  prefs: []
  type: TYPE_NORMAL
  zh: $$\operatorname*{lim}_{\alpha\longrightarrow0}{\frac{f_{\alpha}^{\prime}(X)-f(X)}{\alpha}}=\|\nabla
    f(X)\|$$
- en: 'which is the tangent vector of interest. Note that this is true for α positive
    or negative. The same tangent vector describes both thickening and thinning. Alternatively,
    we can use our computation of the displacement r and define the following transformation
    of the input:'
  id: totrans-3396
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们关注的切向量。请注意，对于 α 无论是正还是负，这都是成立的。相同的切向量描述了加厚和变薄。或者，我们可以利用对位移 r 的计算，并定义输入的以下变换：
- en: $$(12.74)$$
  id: totrans-3397
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.74)$$
- en: $$t_{\alpha}(f):{\binom{x}{y}}\longmapsto{\binom{x+\alpha r_{x}}{y+\alpha r_{y}}}$$
  id: totrans-3398
  prefs: []
  type: TYPE_NORMAL
  zh: $$t_{\alpha}(f):{\binom{x}{y}}\longmapsto{\binom{x+\alpha r_{x}}{y+\alpha r_{y}}}$$
- en: $$(12.75)$$
  id: totrans-3399
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.75)$$
- en: where
  id: totrans-3400
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: $$(r_{x},r_{y})=r=\alpha{\frac{\nabla f(X)}{\|\nabla f(X)\|}}.$$
  id: totrans-3401
  prefs: []
  type: TYPE_NORMAL
  zh: $$(r_{x},r_{y})=r=\alpha{\frac{\nabla f(X)}{\|\nabla f(X)\|}}.$$
- en: . (12.76)
  id: totrans-3402
  prefs: []
  type: TYPE_NORMAL
  zh: . (12.76)
- en: 'This transformation of the input space is different for each pattern f (we
    do not have a Lie group of transformations, but the field structure generated
    by the (pseudo Lie) operator is still useful. The operator used to find the tangent
    vector is defined by:'
  id: totrans-3403
  prefs: []
  type: TYPE_NORMAL
  zh: 输入空间的这种变换对每个模式 f 都不同（我们没有一个变换的李群，但由（伪李）算子生成的场结构仍然是有用的。用于寻找切向量的算子定义为：
- en: $$(12.76)$$
  id: totrans-3404
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.76)$$
- en: $$L_{T}=\|\nabla\|$$
  id: totrans-3405
  prefs: []
  type: TYPE_NORMAL
  zh: $$L_{T}=\|\nabla\|$$
- en: $$(12.77)$$
  id: totrans-3406
  prefs: []
  type: TYPE_NORMAL
  zh: $$(12.77)$$
- en: LT = ∇ (12.77)
  id: totrans-3407
  prefs: []
  type: TYPE_NORMAL
  zh: LT = ∇ (12.77)
- en: '![263_image_0.png](263_image_0.png)'
  id: totrans-3408
  prefs: []
  type: TYPE_IMG
  zh: '![263_image_0.png](263_image_0.png)'
- en: Fig. 12.14. Illustration of 5 tangent vectors (top), with corresponding displacements
  id: totrans-3409
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.14. 5 个切向量的示意图（顶部），以及对应的位移
- en: (middle) and transformation effects (bottom). The displacement Dx and Dy are
    represented in the form of vector field. It can be noted that the tangent vector
    for the thickness deformation (right column) correspond to the norm of the gradient
    of the gray level image.
  id: totrans-3410
  prefs: []
  type: TYPE_NORMAL
  zh: （中部）和变换效果（底部）。位移 Dx 和 Dy 以向量场的形式表示。可以注意到，厚度变形的切向量（右列）对应于灰度图像的梯度的范数。
- en: which means that the tangent vector image is obtained by computing the normalized
    gray level gradient of the image at each point (the gradient at each point is
    normalized).
  id: totrans-3411
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着切向量图像是通过在每个点计算图像的归一化灰度梯度获得的（每个点的梯度都是归一化的）。
- en: The last 5 transformations are depicted in Figure 12.14 with the tangent vector.
  id: totrans-3412
  prefs: []
  type: TYPE_NORMAL
  zh: 最后 5 个变换在图 12.14 中与切向量一起展示。
- en: The last operator corresponds to a thickening or thinning of the image. This
    unusual transformation is extremely useful for handwritten character recognition.
  id: totrans-3413
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的算子对应于图像的加厚或变薄。这种不寻常的变换在手写字符识别中非常有用。
- en: 12.5 Conclusion
  id: totrans-3414
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.5 结论
- en: The basic tangent distance algorithm is quite easy to understand and implement.
  id: totrans-3415
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的切线距离算法相当容易理解和实现。
- en: Even though hardly any preprocessing or learning is required, the performance
    is surprisingly good and compares well to the best competing algorithms. We believe
    that the main reason for this success is its ability to incorporate *a priori*
    knowledge into the distance measure. The only algorithm which performed better
    than tangent distance on one of the three databases was boosting, which has similar
    *a priori* knowledge about transformations built into it.
  id: totrans-3416
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管几乎不需要任何预处理或学习，但性能令人惊讶地好，且与最佳竞争算法相比表现良好。我们认为，这一成功的主要原因在于其将 *a priori* 知识纳入距离测量的能力。在三个数据库中，唯一比切线距离表现更好的算法是提升算法，该算法内置了类似的
    *a priori* 变换知识。
- en: Many improvements are of course possible. For instance, smart preprocessing
    can allow us to measure the tangent distance in a more appropriate "feature" space,
    instead of the original pixel space. In image classification, for example, the
    features could be horizontal and vertical edges. This would most likely further
    improve the performance6 The only requirement is that the preprocessing must be
    differentiable, so that the tangent vectors can be computed (propagated) into
    the feature space.
  id: totrans-3417
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，许多改进是可能的。例如，智能预处理可以让我们在更合适的“特征”空间中测量切线距离，而不是在原始像素空间中。在图像分类中，例如，特征可以是水平和垂直边缘。这很可能进一步提高性能。唯一的要求是预处理必须是可微的，以便切向量可以计算（传播）到特征空间中。
- en: It is also straightforward to modify more complex algorithms such as LVQ
  id: totrans-3418
  prefs: []
  type: TYPE_NORMAL
  zh: 修改更复杂的算法，例如 LVQ 也很简单。
- en: (learning vector quantization) to use a tangent distance. In this case even
    the tangent vectors can be trained. The derivation has been done for batch training
    [13] and for on-line training [23] of the tangent vectors. When such training
    is performed, the *a priori* knowledge comes from other constraints imposed on
    the tangent vectors (for instance how many tangent vectors are allowed, which
    classes of transformation do they represent, etc).
  id: totrans-3419
  prefs: []
  type: TYPE_NORMAL
  zh: （学习向量量化）使用切向距离。在这种情况下，即使是切向量也可以进行训练。已为切向量的批量训练[13]和在线训练[23]完成了推导。当进行这种训练时，*先验*知识来自对切向量施加的其他约束（例如，允许多少个切向量，它们表示哪些变换类别等）。
- en: Finally, many optimizations which are commonly used in distance based algorithms
    can be used as successfully with tangent distance to speed up computation. The
    multi-resolution approach have already been tried successfully [25]. Other methods
    like "multi-edit-condensing" [1, 30] and K-d tree [4] are also possible.
  id: totrans-3420
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，许多在基于距离的算法中常用的优化也可以同样成功地用于切向距离，以加快计算速度。多分辨率方法已经成功尝试过[25]。其他方法如“多编辑凝缩”[1,
    30]和K-d树[4]也是可行的。
- en: The main advantage of tangent distance is that it is a modification of a standard
    distance measure to allow it to incorporate *a priori* knowledge that is specific
    to the problem at hand. Any algorithms based on a common distance measure (as
    it is often the case in classification, vector quantization, predictions, etc...)
    can potentially benefit from a more problem-specific distance. Many of these "distance
    based" algorithms do not require any learning, which means that they can be adapted
    instantly by just adding new patterns in the database. These additions are leveraged
    by the *a priori* knowledge put in the tangent distance.
  id: totrans-3421
  prefs: []
  type: TYPE_NORMAL
  zh: 切向距离的主要优势在于它是对标准距离度量的修改，允许其纳入特定于手头问题的*先验*知识。任何基于通用距离度量的算法（在分类、向量量化、预测等情况下通常如此）都可能受益于更具问题特异性的距离。这些“基于距离”的算法中的许多不需要任何学习，这意味着它们可以通过仅在数据库中添加新模式来立即适应。这些添加得益于放入切向距离的*先验*知识。
- en: 6 There may be an additional cost for computing the tangent vectors in the feature
    space if the feature space is very complex.
  id: totrans-3422
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征空间非常复杂，计算特征空间中的切向量可能会产生额外的成本。
- en: 'The two drawbacks of tangent distance are its memory and computational requirements.
    The most computationally and memory efficient algorithms generally involve learning
    [20]. Fortunately, the concept of tangent vectors can also be used in learning.
    This is the basis for the tangent propagation algorithm. The concept is quite
    simple: instead of learning a classification function from examples of its values,
    one can also use information about its derivatives. This information is provided
    by the tangent vectors. Unfortunately, not many experiments have been done in
    this direction. The two main problems with tangent propagation are that the capacity
    of the learning machine has to be adjusted to incorporate the additional information
    pertinent to the tangent vectors, and that training time must be increased. After
    training, the classification time and complexity are unchanged, but the classifier''s
    performance is improved.'
  id: totrans-3423
  prefs: []
  type: TYPE_NORMAL
  zh: 切向距离的两个缺点是其内存和计算需求。计算和内存效率最高的算法通常涉及学习[20]。幸运的是，切向量的概念也可以用于学习。这是切向传播算法的基础。这个概念相当简单：与其从其值的示例中学习分类函数，不如使用其导数的信息。这些信息由切向量提供。不幸的是，在这个方向上进行的实验不多。切向传播的两个主要问题是学习机器的容量必须调整以纳入与切向量相关的附加信息，且训练时间必须增加。训练后，分类时间和复杂性保持不变，但分类器的性能得到了提高。
- en: To a first approximation, using tangent distance or tangent propagation is like
    having a much larger database. If the database was plenty large to begin with,
    tangent distance or tangent propagation would not improve the performance. To
    a better approximation, tangent vectors are like using a distortion model to magnify
    the size of the training set. In many cases, using tangent vectors will be preferable
    to collecting (and labeling!) vastly more training data, and preferable (especially
    for memory-based classifiers) to dealing with all the data generated by the distortion
    model. Tangent vectors provide a compact and powerful representation of *a priori*
    knowledge which can easily be integrated in the most popular algorithms.
  id: totrans-3424
  prefs: []
  type: TYPE_NORMAL
  zh: 初步估计，使用切线距离或切线传播就像拥有一个更大的数据库。如果数据库一开始就很大，切线距离或切线传播并不会提高性能。更好的估计是，切线向量就像使用失真模型来放大训练集的大小。在许多情况下，使用切线向量比收集（和标记！）大量训练数据更可取，尤其对于基于记忆的分类器来说，也比处理失真模型生成的所有数据更可取。切线向量提供了一种紧凑而强大的表示，能够轻松整合到最流行的算法中，*先验*知识。
- en: Acknowledgement. P.S. and Y.L. gratefully acknowledge NSF grant No. INT9726745.
  id: totrans-3425
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢。P.S.和Y.L.衷心感谢NSF资助号INT9726745。
- en: '[1] Devijver, P.A., Kittler, J.: Pattern Recognition, A Statistical Approache.
    PrenticeHall, Englewood Cliffs (1982)'
  id: totrans-3426
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Devijver, P.A.，Kittler, J.：模式识别，统计方法。普伦蒂斯霍尔，恩格尔伍德悬崖（1982年）'
- en: '[2] Aho, A.V., Hopcroft, J.E., Ullman, J.D.: Data Structure and Algorithms.
    AddisonWesley (1983)'
  id: totrans-3427
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Aho, A.V.，Hopcroft, J.E.，Ullman, J.D.：数据结构与算法。阿迪森-韦斯利（1983年）'
- en: '[3] Bottou, L., Vapnik, V.N.: Local learning algorithms. Neural Computation
    4(6),'
  id: totrans-3428
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Bottou, L.，Vapnik, V.N.：局部学习算法。《神经计算》4(6)，'
- en: 888–900 (1992)
  id: totrans-3429
  prefs: []
  type: TYPE_NORMAL
  zh: 888–900（1992年）
- en: '[4] Broder, A.J.: Strategies for efficient incremental nearest neighbor search.
    Pattern Recognition 23, 171–178 (1990)'
  id: totrans-3430
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Broder, A.J.：高效增量最近邻搜索策略。《模式识别》23，171–178（1990年）'
- en: '[5] Broomhead, D.S., Lowe, D.: Multivariable functional interpolation and adaptive
    networks. Complex Systems 2, 321–355 (1988)'
  id: totrans-3431
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Broomhead, D.S.，Lowe, D.：多变量函数插值和自适应网络。《复杂系统》2，321–355（1988年）'
- en: '[6] Choquet-Bruhat, Y., DeWitt-Morette, C., Dillard-Bleick, M.: Analysis, Manifolds
    and Physics. North-Holland, Amsterdam (1982)'
  id: totrans-3432
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Choquet-Bruhat, Y.，DeWitt-Morette, C.，Dillard-Bleick, M.：分析、流形与物理学。北荷兰出版社，阿姆斯特丹（1982年）'
- en: '[7] Cortes, C., Vapnik, V.: Support vector networks. Machine Learning 20, 273–297'
  id: totrans-3433
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Cortes, C.，Vapnik, V.：支持向量网络。《机器学习》20，273–297'
- en: (1995)
  id: totrans-3434
  prefs: []
  type: TYPE_NORMAL
  zh: （1995年）
- en: '[8] Dasarathy, B.V.: Nearest Neighbor (NN) Norms: NN Pattern classification
    Techniques. IEEE Computer Society Press, Los Alamitos (1991)'
  id: totrans-3435
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Dasarathy, B.V.：最近邻（NN）规范：NN模式分类技术。IEEE计算机学会出版社，洛斯阿拉米托斯（1991年）'
- en: '[9] Drucker, H., Schapire, R., Simard, P.Y.: Boosting performance in neural
    networks.'
  id: totrans-3436
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Drucker, H.，Schapire, R.，Simard, P.Y.：神经网络性能提升。'
- en: International Journal of Pattern Recognition and Artificial Intelligence 7(4),
    705– 719 (1993)
  id: totrans-3437
  prefs: []
  type: TYPE_NORMAL
  zh: 《国际模式识别与人工智能杂志》7(4)，705–719（1993年）
- en: '[10] Fukunaga, K., Flick, T.E.: An optimal global nearest neighbor metric.
    IEEE transactions on Pattern analysis and Machine Intelligence 6(3), 314–318 (1984)'
  id: totrans-3438
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Fukunaga, K.，Flick, T.E.：最佳全局最近邻度量。《IEEE模式分析与机器智能交易》6(3)，314–318（1984年）'
- en: '[11] Gilmore, R.: Lie Groups, Lie Algebras and some of their Applications.
    Wiley, New York (1974)'
  id: totrans-3439
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Gilmore, R.：李群、李代数及其一些应用。威立出版社，纽约（1974年）'
- en: '[12] Hastie, T., Kishon, E., Clark, M., Fan, J.: A model for signature verification.'
  id: totrans-3440
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Hastie, T.，Kishon, E.，Clark, M.，Fan, J.：签名验证模型。'
- en: Technical Report 11214-910715-07TM, AT&T Bell Laboratories (July 1991)
  id: totrans-3441
  prefs: []
  type: TYPE_NORMAL
  zh: 技术报告11214-910715-07TM，AT&T贝尔实验室（1991年7月）
- en: '[13] Hastie, T., Simard, P.Y.: Metrics and models for handwritten character
    recognition. Statistical Science 13 (1998)'
  id: totrans-3442
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Hastie, T.，Simard, P.Y.：手写字符识别的度量和模型。《统计科学》13（1998年）'
- en: '[14] Hastie, T.J., Tibshirani, R.J.: Generalized Linear Models. Chapman and
    Hall, London (1990)'
  id: totrans-3443
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Hastie, T.J.，Tibshirani, R.J.：广义线性模型。查普曼与霍尔，伦敦（1990年）'
- en: '[15] Hinton, G.E., Williams, C.K.I., Revow, M.D.: Adaptive elastic models for
    handprinted character recognition. In: Advances in Neural Information Processing
    Systems, pp. 512–519. Morgan Kaufmann Publishers (1992)'
  id: totrans-3444
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Hinton, G.E.，Williams, C.K.I.，Revow, M.D.：手写字符识别的自适应弹性模型。见：神经信息处理系统进展，页512–519。摩根·考夫曼出版社（1992年）'
- en: '[16] Hoerl, A.E., Kennard, R.W.: Ridge regression: Biased estimation for nonorthogonal
    problems. Technometrics 12, 55–67 (1970)'
  id: totrans-3445
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Hoerl, A.E.，Kennard, R.W.：岭回归：非正交问题的偏差估计。《技术统计》12，55–67（1970年）'
- en: '[17] Kohonen, T.: Self-organization and associative memory. Springer Series
    in Information Sciences, vol. 8. Springer (1984)'
  id: totrans-3446
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Kohonen, T.: 自组织与联想记忆。信息科学系列，卷 8。Springer (1984)'
- en: '[18] Le Cun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard,
    W.,'
  id: totrans-3447
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Le Cun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard,
    W.,'
- en: 'Jackel, L.D.: Handwritten digit recognition with a back-propagation network.
    In: Touretzky, D. (ed.) Advances in Neural Information Processing Systems, vol.
    2, Morgan Kaufmann, Denver (1989)'
  id: totrans-3448
  prefs: []
  type: TYPE_NORMAL
  zh: 'Jackel, L.D.: 使用反向传播网络进行手写数字识别。在：Touretzky, D. (编) 神经信息处理系统进展，卷 2，Morgan Kaufmann，丹佛
    (1989)'
- en: '[19] LeCun, Y.: Generalization and network design strategies. In: Pfeifer,
    R., Schreter, Z., Fogelman, F., Steels, L. (eds.) Connectionism in Perspective,
    Zurich, Switzerland (1989); Elsevier, An extended version was published as a technical
    report of the University of Toronto'
  id: totrans-3449
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] LeCun, Y.: 泛化和网络设计策略。在：Pfeifer, R., Schreter, Z., Fogelman, F., Steels,
    L. (编) 连接主义的视角，瑞士苏黎世 (1989)；Elsevier，扩展版作为多伦多大学的技术报告发布'
- en: '[20] LeCun, Y., Jackel, L.D., Bottou, L., Cortes, C., Denker, J.S., Drucker,
    H., Guyon, I., Muller, U.A., Sackinger, E., Simard, P., Vapnik, V.: Learning algorithms
    for classification: A comparison on handwritten digit recognition. In: Oh, J.H.,
    Kwon, C., Cho, S. (eds.) Neural Networks: The Statistical Mechanics Perspective,
    pp.'
  id: totrans-3450
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] LeCun, Y., Jackel, L.D., Bottou, L., Cortes, C., Denker, J.S., Drucker,
    H., Guyon, I., Muller, U.A., Sackinger, E., Simard, P., Vapnik, V.: 分类的学习算法：关于手写数字识别的比较。在：Oh,
    J.H., Kwon, C., Cho, S. (编) 神经网络：统计力学视角，第'
- en: 261–276. World Scientific (1995)
  id: totrans-3451
  prefs: []
  type: TYPE_NORMAL
  zh: 261–276。世界科学出版社 (1995)
- en: '[21] Parzen, E.: On estimation of a probability density function and mode.
    Ann. Math.'
  id: totrans-3452
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Parzen, E.: 概率密度函数和模式的估计。数学年鉴。'
- en: Stat. 33, 1065–1076 (1962)
  id: totrans-3453
  prefs: []
  type: TYPE_NORMAL
  zh: Stat. 33, 1065–1076 (1962)
- en: '[22] Press, W.H., Flannery, B.P., Teukolsky, S.A., Vetterling, W.T.: Numerical
    Recipes.'
  id: totrans-3454
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Press, W.H., Flannery, B.P., Teukolsky, S.A., Vetterling, W.T.: 数值食谱。'
- en: Cambridge University Press, Cambridge (1988)
  id: totrans-3455
  prefs: []
  type: TYPE_NORMAL
  zh: 剑桥大学出版社，剑桥 (1988)
- en: '[23] Schwenk, H.: The diabolo classifier. Neural Computation (1998) (in press)'
  id: totrans-3456
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Schwenk, H.: diabolo分类器。神经计算 (1998)（待出版）'
- en: '[24] Sibson, R.: Studies in the robustness of multidimensional scaling: Procrustes
    statistices. J. R. Statist. Soc. 40, 234–238 (1978)'
  id: totrans-3457
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Sibson, R.: 多维标度稳健性研究：Procrustes统计。J. R. Statist. Soc. 40, 234–238 (1978)'
- en: '[25] Simard, P.Y.: Efficient computation of complex distance metrics using
    hierarchical filtering. In: Advances in Neural Information Processing Systems.
    Morgan Kaufmann Publishers (1994)'
  id: totrans-3458
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Simard, P.Y.: 使用层次过滤有效计算复杂距离度量。在：神经信息处理系统进展。Morgan Kaufmann Publishers
    (1994)'
- en: '[26] Sinden, F., Wilfong, G.: On-line recognition of handwritten symbols. Technical
    Report 11228-910930-02IM, AT&T Bell Laboratories (June 1992)'
  id: totrans-3459
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Sinden, F., Wilfong, G.: 在线手写符号识别。技术报告 11228-910930-02IM，AT&T 贝尔实验室（1992年6月）'
- en: '[27] Vapnik, V.N.: Estimation of dependences based on empirical data. Springer
    (1982)'
  id: totrans-3460
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Vapnik, V.N.: 基于经验数据的依赖性估计。Springer (1982)'
- en: '[28] Vapnik, V.N., Chervonenkis, A.Y.: On the uniform convergence of relative
    frequencies of events to their probabilities. Th. Prob. and its Applications 17(2),'
  id: totrans-3461
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] Vapnik, V.N., Chervonenkis, A.Y.: 事件相对频率均匀收敛于其概率的研究。概率理论及其应用 17(2),'
- en: 264–280 (1971)
  id: totrans-3462
  prefs: []
  type: TYPE_NORMAL
  zh: 264–280 (1971)
- en: '[29] Vasconcelos, N., Lippman, A.: Multiresolution tangent distance for affine-invariant
    classification. In: Advances in Neural Information Processing Systems, vol. 10,
    pp. 843–849. Morgan Kaufmann Publishers (1998)'
  id: totrans-3463
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] Vasconcelos, N., Lippman, A.: 用于仿射不变分类的多分辨率切线距离。在：神经信息处理系统进展，卷 10，第 843–849
    页。Morgan Kaufmann Publishers (1998)'
- en: '[30] Voisin, J., Devijver, P.: An application of the multiedit-condensing technique
    to the reference selection problem in a print recognition system. Pattern Recogntion
    20(5), 465–474 (1987)'
  id: totrans-3464
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] Voisin, J., Devijver, P.: 多编辑凝聚技术在打印识别系统中的参考选择问题的应用。模式识别 20(5), 465–474
    (1987)'
- en: 13 Combining Neural Networks And Context-Driven Search For On-Line, Printed
    Handwriting Recognition In The Newton-
  id: totrans-3465
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 将神经网络与上下文驱动搜索相结合，用于在线打印手写识别在牛顿-
- en: Larry S. Yaeger1, Brandyn J. Webb2, and Richard F. Lyon3 1 Apple Computer 5540
    Bittersweet Rd. Beanblossom, IN 46160 2 The Future 4578 Fieldgate Rd. Oceanside,
    CA 92056 3 Foveonics, Inc. 10131-B Bubb Rd. Cupertino, CA 95014 larryy@pobox.com
    http://www.beanblossom.in.us/larryy/
  id: totrans-3466
  prefs: []
  type: TYPE_NORMAL
  zh: Larry S. Yaeger1, Brandyn J. Webb2, 和 Richard F. Lyon3 1 苹果公司 5540 Bittersweet
    Rd. Beanblossom, IN 46160 2 未来公司 4578 Fieldgate Rd. Oceanside, CA 92056 3 Foveonics,
    Inc. 10131-B Bubb Rd. Cupertino, CA 95014 larryy@pobox.com http://www.beanblossom.in.us/larryy/
- en: Abstract. While on-line handwriting recognition is an area of long-standing
    and ongoing research, the recent emergence of portable, pen-based computers has
    focused urgent attention on usable, practical solutions. We discuss a combination
    and improvement of classical methods to produce robust recognition of hand-printed
    English text, for a recognizer shipping in new models of Apple Computer's Newton
    MessagePadRand eMateR. Combining an artificial neural network
  id: totrans-3467
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要：虽然在线手写识别是一个长期存在且持续研究的领域，但便携式笔记本电脑的近期出现使得对可用、实用解决方案的关注变得紧迫。我们讨论了一种经典方法的组合与改进，以实现对手写英文文本的稳健识别，适用于即将发布的新款苹果计算机的Newton
    MessagePadRand eMateR。结合人工神经网络
- en: (ANN), as a character classifier, with a context-driven search over segmentation
    and word recognition hypotheses provides an effective recognition system. Long-standing
    issues relative to training, generalization, segmentation, models of context,
    probabilistic formalisms, etc., need to be resolved, however, to get excellent
    performance. We present a number of recent innovations in the application of ANNs
    as character classifiers for word recognition, including integrated multiple representations,
    normalized output error, negative training, stroke warping, frequency balancing,
    error emphasis, and quantized weights. User-adaptation and extension to cursive
    recognition pose continuing challenges.
  id: totrans-3468
  prefs: []
  type: TYPE_NORMAL
  zh: （ANN），作为字符分类器，通过对分割和单词识别假设进行上下文驱动的搜索，提供了一种有效的识别系统。然而，相对于训练、泛化、分割、上下文模型、概率形式等的长期问题，需要解决，以获得卓越的性能。我们介绍了一些近期在应用ANN作为单词识别字符分类器方面的创新，包括集成多种表示、标准化输出误差、负训练、笔画扭曲、频率平衡、误差强调和量化权重。用户适应和扩展到草书识别仍然是持续的挑战。
- en: 13.1 Introduction
  id: totrans-3469
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 引言
- en: Pen-based hand-held computers are heavily dependent upon fast and accurate handwriting
    recognition, since the pen serves as the primary means for inputting data to such
    devices. Some earlier attempts at handwriting recognition have utilized strong,
    limited language models to maximize accuracy. However, this approach has proven
    to be unacceptable in real-world applications, generating disturbing and seemingly
    random word substitutions - known colloquially within Apple and Newton as "The
    Doonesbury Effect", due to Gary Trudeau's satirical look at first-generation Newton
    recognition performance. But the original handwriting recognition technology in
    the Newton, and the current, much-improved
  id: totrans-3470
  prefs: []
  type: TYPE_NORMAL
  zh: 基于笔的手持电脑在很大程度上依赖于快速且准确的手写识别，因为笔是输入数据到这些设备的主要手段。一些早期的手写识别尝试利用强大且有限的语言模型来最大化准确性。然而，这种方法在现实应用中被证明是不可接受的，产生了令人不安且似乎随机的单词替代现象——在苹果和Newton内部被称为“杜恩斯伯里效应”，因加里·特鲁多对第一代Newton识别性能的讽刺描述。然而，Newton中的原始手写识别技术，以及当前大大改进的
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-3471
  prefs: []
  type: TYPE_NORMAL
  zh: '- 先前发表于：Orr, G.B. 和 Müller, K.-R.（编）：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-3472
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0 (1998)。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    271–293, 2012.'
  id: totrans-3473
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon等（编）：NN：行业诀窍，第2版，LNCS 7700，第271-293页，2012。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-3474
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: '"Cursive Recognizer" technology, both of which were licensed from ParaGraph
    International, Inc., are not the subject of this article.'
  id: totrans-3475
  prefs: []
  type: TYPE_NORMAL
  zh: “草书识别器”技术，这两者均已从ParaGraph International, Inc.获得许可，但并非本文讨论的主题。
- en: In Apple's Advanced Technology Group (aka Apple Research Labs), we pursued a
    different approach, using bottom-up classification techniques based on trainable
    artificial neural networks (ANNs), in combination with comprehensive but weakly-applied
    language models. To focus our work on a subproblem that was tractable enough to
    lead to usable products in a reasonable time, we initially restricted the domain
    to hand-printing, so that strokes are clearly delineated by pen lifts. By simultaneously
    providing accurate character-level recognition, dictionaries exhibiting very wide
    coverage of the language, and the ability to write entirely outside those dictionaries,
    we have produced a hand-print recognizer that some have called the "first usable"
    handwriting recognition system. The ANN character classifier required some innovative
    training techniques to perform its task well. The dictionaries required large
    word lists, a regular expression grammar (to describe special constructs such
    as date, time, phone numbers, etc.), and a means of combining all these dictionaries
    into a comprehensive language model. And well balanced prior probabilities had
    to be determined for in-dictionary and out-of-dictionary writing. Together with
    a maximum-likelihood search engine, these elements form the basis of the so-called
    "Print Recognizer", that was first shipped in Newton OS 2.0 based MessagePad 120
    units in December, 1995, and has shipped in all subsequent Newton devices. In
    the MessagePad 2000 and 2100, despite retaining its label as a "Print Recognizer",
    it has been extended to handle connected characters (as well as a full Western
    European character set).
  id: totrans-3476
  prefs: []
  type: TYPE_NORMAL
  zh: 在苹果的先进技术组（又名苹果研究实验室），我们采用了不同的方法，使用基于可训练人工神经网络（ANN）的自下而上的分类技术，结合全面但应用较弱的语言模型。为了将我们的工作集中在一个可行的子问题上，以便在合理的时间内产生可用的产品，我们最初将领域限制在手写打印，从而使笔画通过笔抬起清晰划分。通过同时提供准确的字符级识别、具有广泛语言覆盖的字典，以及完全超出这些字典的书写能力，我们生产了一种被称为“首个可用”的手写识别系统的手写打印识别器。ANN字符分类器需要一些创新的训练技术来有效地完成其任务。字典需要大量的单词列表、一个正则表达式语法（以描述日期、时间、电话号码等特殊结构），以及将所有这些字典结合成一个全面的语言模型的方法。并且需要为字典内和字典外书写确定平衡的先验概率。结合最大似然搜索引擎，这些元素构成了所谓的“打印识别器”的基础，该识别器于1995年12月首次在Newton
    OS 2.0基础的MessagePad 120设备中发布，并在所有后续的Newton设备中使用。在MessagePad 2000和2100中，尽管保留了“打印识别器”的标签，但它已经扩展以处理连接字符（以及完整的西欧字符集）。
- en: There is ample prior work in combining low-level classifiers with dynamic time
    warping, hidden Markov models, Viterbi algorithms, and other search strategies
    to provide integrated segmentation and recognition for writing [15] and speech
  id: totrans-3477
  prefs: []
  type: TYPE_NORMAL
  zh: 在结合低级分类器与动态时间扭曲、隐马尔可夫模型、维特比算法及其他搜索策略以提供集成的书写[15]和语音分割与识别方面，有大量的前期工作。
- en: '[11]. And there is a rich background in the use of ANNs as classifiers, including
    their use as low-level character classifiers in a higher-level word recognition
    system [2]. But these approaches leave a large number of open-ended questions
    about how to achieve acceptable (to a real user) levels of performance. In this
    paper, we survey some of our experiences in exploring refinements and improvements
    to these techniques.'
  id: totrans-3478
  prefs: []
  type: TYPE_NORMAL
  zh: '[11]。使用人工神经网络（ANN）作为分类器的背景非常丰富，包括它们作为高级单词识别系统中的低级字符分类器的应用[2]。但是这些方法仍然存在许多未解的问题，关于如何达到可接受的（对真实用户）性能水平。在本文中，我们回顾了探索这些技术的改进与完善的一些经验。'
- en: 13.2 System Overview
  id: totrans-3479
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 系统概述
- en: Apple's print recognizer (APR) consists of three conceptual stages - Tentative
    Segmentation, Classification, and Context-Driven Search - as indicated in Figure
    13.1. The primary data upon which we operate are simple sequences of (x,y)
  id: totrans-3480
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果的打印识别器（APR）由三个概念阶段组成——暂定分割、分类和上下文驱动搜索——如图13.1所示。我们操作的主要数据是简单的(x,y)序列。
- en: coordinate pairs, plus pen-up/down information, thus defining stroke primitives.
  id: totrans-3481
  prefs: []
  type: TYPE_NORMAL
  zh: 坐标对，加上笔抬起/放下的信息，从而定义了笔画原语。
- en: The Segmentation stage decides which strokes will be combined to produce *segments*
    - the tentative groupings of strokes that will be treated as possible characters
    - and produces a sequence of these segments together with legal transitions between
    them. This process builds an implicit graph which is then labeled in the Classification
    stage and examined for a maximum likelihood interpretation in the Search stage.
    The Classification stage evaluates each segment using the ANN classifier, and
    produces a vector of output activations that are used as letter-class probabilities.
    The Search stage then uses these class probabilities together with models of lexical
    and geometric context to find the N most likely word or sentence hypotheses.
  id: totrans-3482
  prefs: []
  type: TYPE_NORMAL
  zh: 分割阶段决定将哪些笔画组合以生成*片段*——这些将被视为可能字符的笔画暂定分组，并生成这些片段的序列以及它们之间的合法转换。此过程构建了一个隐式图，然后在分类阶段进行标记，并在搜索阶段进行最大似然解释。分类阶段使用
    ANN 分类器评估每个片段，并生成作为字母类别概率的输出激活向量。搜索阶段随后使用这些类别概率以及词汇和几何上下文模型，找到 N 个最可能的单词或句子假设。
- en: '![269_image_0.png](269_image_0.png)'
  id: totrans-3483
  prefs: []
  type: TYPE_IMG
  zh: '![269_image_0.png](269_image_0.png)'
- en: Fig. 13.1. A simplified block diagram of our hand-print recognizer
  id: totrans-3484
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1 我们手写识别器的简化框图
- en: 13.3 Tentative Segmentation
  id: totrans-3485
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.3 暂定分割
- en: Character segmentation - the process of deciding which strokes comprise which
    characters - is inherently ambiguous. Ultimately this decision must be made, but,
    short of writing in boxes, it is impossible to do so (with any accuracy)
  id: totrans-3486
  prefs: []
  type: TYPE_NORMAL
  zh: 字符分割——决定哪些笔画组成哪些字符的过程——本质上是模糊的。最终必须做出这个决定，但除了用方框写，无法做到这一点（任何准确性）。
- en: in advance, external to the recognition process. Hence the initial segmentation
    stage in APR produces multiple, tentative groupings of strokes, and defers the
    final segmentation decisions until the search stage, thus integrating those segmentation
    decisions with the overall recognition process.
  id: totrans-3487
  prefs: []
  type: TYPE_NORMAL
  zh: 预先，在识别过程之外。因此，APR 的初始分割阶段产生多个暂定的笔画分组，并推迟最终的分割决定，直到搜索阶段，从而将这些分割决定与整体识别过程整合。
- en: APR uses a potentially exhaustive, sequential enumeration of stroke combinations
    to generate a sequence of viable character-segmentation hypotheses. These segments
    are subjected to some obvious constraints (such as "all strokes must be used"
    and "no strokes may be used twice"), and some less obvious filters (to cull "impossible"
    segmentations for the sake of efficiency). The resulting algorithm produces the
    actual segments that will be processed as possible characters, along with the
    legal transitions between these segments.
  id: totrans-3488
  prefs: []
  type: TYPE_NORMAL
  zh: APR 使用潜在的详尽、顺序枚举笔画组合，以生成一系列可行的字符分割假设。这些片段受到一些明显的约束（例如“所有笔画必须使用”和“没有笔画可以重复使用”），以及一些不那么明显的过滤器（以排除“不可行”的分割以提高效率）。生成的算法产生将作为可能字符处理的实际片段，以及这些片段之间的合法转换。
- en: The legal transitions are defined by *forward* and *reverse delays*. The forward
    delay indicates the next possible segment in the sequence (though later segments
    may also be legal), pointing just past the last segment that shares the trailing
    stroke of the current segment. The reverse delay indicates the start of the current
    batch of segments, all of which share the same leading stroke. Due to the enumeration
    scheme, a segment's reverse delay is the same as its stroke count minus one, unless
    preceeding segments (sharing the same leading stroke) were eliminated by the filters
    mentioned previously. These two simple delay parameters
  id: totrans-3489
  prefs: []
  type: TYPE_NORMAL
  zh: 合法的转换由*前向*和*反向延迟*定义。前向延迟表示序列中下一个可能的片段（尽管后来的片段也可能是合法的），指向与当前片段共享尾部笔画的最后一个片段之后。反向延迟表示当前片段组的起始点，所有这些片段共享相同的前导笔画。由于枚举方案，片段的反向延迟与其笔画数减一相同，除非前面的片段（共享相同的前导笔画）被之前提到的过滤器消除。这两个简单的延迟参数
- en: (per segment) suffice to define an implicit graph of all legal segment transitions.
    For a transition from segment number i to segment number j to be legal, the sum
    of segment i's forward delay plus segment j's reverse delay must be equal to j
    - i. Figure 13.2 provides an example of some ambiguous ink and the segments that
    might be generated from its strokes, supporting interpretations of "dog", "clog",
    "cbg", or even "%g".
  id: totrans-3490
  prefs: []
  type: TYPE_NORMAL
  zh: （每个片段）足以定义一个所有合法片段转换的隐式图。为了使从片段编号 i 到片段编号 j 的转换合法，片段 i 的前向延迟与片段 j 的反向延迟之和必须等于
    j - i。图 13.2 提供了一些模糊墨迹及其笔画可能生成的片段的示例，支持“狗”、“堵塞”、“cbg”或甚至“%g”的解释。
- en: '| Segment   | Stroke   | Forward Reverse   |       |       |       |'
  id: totrans-3491
  prefs: []
  type: TYPE_TB
  zh: '| 段落     | 笔画     | 正向 反向       |       |       |       |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-3492
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Ink       | Number   | Segment           | Count | Delay | Delay |'
  id: totrans-3493
  prefs: []
  type: TYPE_TB
  zh: '| 墨水      | 数量    | 段落             | 计数 | 延迟 | 延迟 |'
- en: '| 1         | 1        | 3                 | 0     |       |       |'
  id: totrans-3494
  prefs: []
  type: TYPE_TB
  zh: '| 1         | 1        | 3                 | 0     |       |       |'
- en: '| 2         | 2        | 4                 | 1     |       |       |'
  id: totrans-3495
  prefs: []
  type: TYPE_TB
  zh: '| 2         | 2        | 4                 | 1     |       |       |'
- en: '| 3         | 3        | 4                 | 2     |       |       |'
  id: totrans-3496
  prefs: []
  type: TYPE_TB
  zh: '| 3         | 3        | 4                 | 2     |       |       |'
- en: '| 4         | 1        | 2                 | 0     |       |       |'
  id: totrans-3497
  prefs: []
  type: TYPE_TB
  zh: '| 4         | 1        | 2                 | 0     |       |       |'
- en: '| 5         | 2        | 2                 | 1     |       |       |'
  id: totrans-3498
  prefs: []
  type: TYPE_TB
  zh: '| 5         | 2        | 2                 | 1     |       |       |'
- en: '| 6         | 1        | 1                 | 0     |       |       |'
  id: totrans-3499
  prefs: []
  type: TYPE_TB
  zh: '| 6         | 1        | 1                 | 0     |       |       |'
- en: '| 7         | 1        | 0                 | 0     |       |       |'
  id: totrans-3500
  prefs: []
  type: TYPE_TB
  zh: '| 7         | 1        | 0                 | 0     |       |       |'
- en: Fig. 13.2. Segmentation of strokes into tentative characters or segments
  id: totrans-3501
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2. 笔画分段为临时字符或段落
- en: 13.4 Character Classification
  id: totrans-3502
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4 字符分类
- en: The output of the segmentation stage is a stream of segments that are then passed
    to an ANN for classification as characters. Except for the architecture and training
    specifics detailed below, a fairly standard multi-layer perceptron trained with
    error back-propagation (BP) provides the ANN character classifier at the heart
    of APR. A large body of prior work exists to indicate the general applicability
    of ANN technology as a classifier providing good estimates of a posteriori probabilities
    of each class given the input ([5, 12, 11], and others cited herein). Compelling
    arguments have been made for why ANNs providing posterior probabilities in a probabilistic
    recognition formulation should be expected to outperform other recognition approaches
    [8], and ANNs have performed well as the core of speech recognition systems [10].
  id: totrans-3503
  prefs: []
  type: TYPE_NORMAL
  zh: 分段阶段的输出是一串段落，随后将其传递给人工神经网络（ANN）进行字符分类。除了下面详细描述的架构和训练细节之外，一个经过错误反向传播（BP）训练的相当标准的多层感知器提供了APR核心的ANN字符分类器。已有大量先前研究表明，ANN技术作为分类器的通用适用性，能够为每个类别在给定输入下提供良好的后验概率估计（[5,
    12, 11]及本文引用的其他文献）。已有强有力的论据指出，在概率识别公式中，提供后验概率的ANN应该优于其他识别方法[8]，而且ANN在语音识别系统中表现良好[10]。
- en: 13.4.1 Representation
  id: totrans-3504
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4.1 表示
- en: A recurring theme in ANN research is the extreme importance of the representation
    of the data that is given as input to the network. We experimented with a variety
    of input representations, including stroke features both anti-aliased (grayscale)
    and not (binary), and images both anti-aliased and not, and with various schemes
    for positioning and scaling the ink within the image input window.
  id: totrans-3505
  prefs: []
  type: TYPE_NORMAL
  zh: 在ANN研究中，一个反复出现的主题是输入网络的数据表示的重要性。我们实验了多种输入表示，包括抗锯齿的笔画特征（灰度）和非抗锯齿的（二进制），以及抗锯齿的和非抗锯齿的图像，并尝试了各种在图像输入窗口内定位和缩放墨水的方案。
- en: In every case, anti-aliasing was a significant win. This is consistent with
    others' findings, that ANNs perform better when presented with smoothly varying,
    distributed inputs than they do when presented with binary, localized inputs.
    Almost the simplest image representation possible, a non-aspect-ratio-preserving,
    expand-to-fill-the-window image (limited only by a maximum scale factor to keep
    from blowing dots up to the full window size), together with either a single unit
    or a thermometer code (some number of units turned on in sequence to represent
    larger values) for the aspect ratio, proved to be the most effective single-classifier
    solution. However, the best overall classifier accuracy was ultimately obtained
    by combining multiple distinct representations into nearly independent, parallel
    classifiers, joined at a final output layer. Hence representation proved not only
    to be as important as architecture, but, ultimately, to help define the architecture
    of our nets. For our final, hand-optimized system, we utilize four distinct inputs,
    as indicated in Figure 13.3. The stroke count representation was dithered (changed
    randomly at a small probability), to expand the effective training set, prevent
    the network from fixating on this simple input, and thereby improve the network's
    ability to generalize. A schematic of the various input representations can be
    seen as part of the architecture drawing in Figure 13.4 in the next section.
  id: totrans-3506
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种情况下，抗锯齿都是一个显著的优势。这与其他研究的发现一致，即当提供平滑变化的分布式输入时，人工神经网络的表现优于提供二元化、局部化输入时。几乎最简单的图像表示方式，即不保留纵横比的扩展填充窗口图像（仅受最大缩放因子的限制，以避免将点放大到全窗口大小），结合一个单元或温度计编码（按顺序开启的若干单元以表示更大的值）作为纵横比，证明是最有效的单分类器解决方案。然而，最佳的整体分类器准确率最终是通过将多个不同的表示组合成几乎独立的并行分类器，并在最终输出层连接而获得的。因此，表示证明不仅与架构同样重要，而且最终有助于定义我们网络的架构。对于我们最终的手动优化系统，我们使用四个不同的输入，如图13.3所示。笔划计数表示被抖动（以小概率随机改变），以扩展有效的训练集，防止网络专注于这个简单输入，从而提高网络的泛化能力。各种输入表示的示意图可以在下一节图13.4的架构图中看到。
- en: Input Feature Resolution Description
  id: totrans-3507
  prefs: []
  type: TYPE_NORMAL
  zh: 输入特征 分辨率 描述
- en: '| Input Feature   | Resolution   | Description                                                                         |'
  id: totrans-3508
  prefs: []
  type: TYPE_TB
  zh: '| 输入特征    | 分辨率      | 描述                                                                             |'
- en: '| --- | --- | --- |'
  id: totrans-3509
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Image           | 14x14        | anti-aliased, scale-to-window, scale-limited                                        |'
  id: totrans-3510
  prefs: []
  type: TYPE_TB
  zh: '| 图像        | 14x14        | 抗锯齿，缩放至窗口，缩放限制                                                      |'
- en: '| Stroke          | 20x9         | anti-aliased, limited resolution tangent
    slope, resampled to fixed number of points |'
  id: totrans-3511
  prefs: []
  type: TYPE_TB
  zh: '| 笔划        | 20x9         | 抗锯齿，限制分辨率的切线斜率，重新采样为固定数量的点                          |'
- en: '| Aspect Ratio    | 1x1          | normalized and capped to [0,1]                                                      |'
  id: totrans-3512
  prefs: []
  type: TYPE_TB
  zh: '| 纵横比      | 1x1          | 标准化并限制在[0,1]                                                               |'
- en: '| Stroke Count    | 5x1          | dithered thermometer code                                                           |'
  id: totrans-3513
  prefs: []
  type: TYPE_TB
  zh: '| 笔划计数    | 5x1          | 抖动的温度计编码                                                                  |'
- en: Fig. 13.3. Input representations used in APR
  id: totrans-3514
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3. APR中使用的输入表示
- en: 13.4.2 Architecture
  id: totrans-3515
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4.2 架构
- en: As with representations, we experimented with a variety of architectures, including
    simple fully-connected layers, receptive fields, shared weights, multiple hidden
    layers, and, ultimately, multiple nearly independent classifiers tied to a common
    output layer. The final choice of architecture includes multiple input representations,
    a first hidden layer (separate for each input representation) using receptive
    fields, fully connected second hidden layers (again distinct for each representation),
    and a final, shared, fully-connected output layer. Simple scalar features - aspect
    ratio and stroke count - connect to both second hidden layers. The final network
    architecture, for our original English-language system, is shown in Figure 13.4.
  id: totrans-3516
  prefs: []
  type: TYPE_NORMAL
  zh: 与表示方法一样，我们尝试了多种架构，包括简单的全连接层、感受野、共享权重、多隐藏层，最终选择了多个几乎独立的分类器连接到一个共同的输出层。最终选择的架构包括多个输入表示、一个第一个隐藏层（针对每个输入表示分开）使用感受野、全连接的第二个隐藏层（同样对每个表示独立），以及一个最终的、共享的全连接输出层。简单的标量特征——纵横比和笔划计数——同时连接到这两个第二隐藏层。我们原始英语系统的最终网络架构如图13.4所示。
- en: Layers are fully connected, except for the inputs to the first hidden layer
    on the image side. This first hidden layer on the image side consists of 8 separate
    grids, each of which accepts inputs from the image input grid with its own receptive
    field sizes and strides, shown parenthetically in Figure 13.4 as (x-size x y-size;
    xstride, y-stride). A stride is the number of units (pixels) in the input image
    space between sequential positionings of the receptive fields, in a given direction.
    The 7x2 and 2x7 side panels (surrounding the central 7x7 grid) pay special attention
    to the edges of the image. The 9x1 and 1x9 side panels specifically examine fullsize
    vertical and horizontal features, respectively. The 5x5 grid observes features
    at a different spatial scale than the 7x7 grid.
  id: totrans-3517
  prefs: []
  type: TYPE_NORMAL
  zh: 层完全连接，除了图像侧第一个隐藏层的输入。图像侧的第一个隐藏层由8个独立的网格组成，每个网格接受来自图像输入网格的输入，具有自己接收域的大小和步幅，在图13.4中以括号形式显示为（x-size
    x y-size; xstride, y-stride）。步幅是在给定方向上，输入图像空间中接收域的顺序位置之间的单位（像素）数量。7x2和2x7的侧面面板（环绕中心7x7网格）特别关注图像的边缘。9x1和1x9的侧面面板分别专门检查全尺寸的垂直和水平特征。5x5网格以不同的空间尺度观察特征，与7x7网格不同。
- en: Combining the two classifiers at the output layer, rather than, say, averaging
    the outputs of completely independent classifiers, allows generic BP to learn
    the best way to combine them, which is both convenient and powerful. But our *integrated
    multiple-representations* architecture is conceptually related to and motivated
    by prior experiments at combining nets such as Steve Nowlan's "mixture of experts"
    [7].
  id: totrans-3518
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出层结合这两个分类器，而不是例如平均完全独立分类器的输出，使得通用BP能够学习最佳的组合方式，这既方便又强大。但我们的*综合多重表示*架构在概念上与先前的结合网络实验相关，并受到其启发，例如史蒂夫·诺兰的“专家组合”[7]。
- en: '![272_image_0.png](272_image_0.png)'
  id: totrans-3519
  prefs: []
  type: TYPE_IMG
  zh: '![272_image_0.png](272_image_0.png)'
- en: Fig. 13.4. Final English-language net architecture. (See the text for an explanation
    of the notation.)
  id: totrans-3520
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4. 最终的英文网络架构。（有关符号的解释，请参见文本。）
- en: 13.4.3 Normalizing Output Error
  id: totrans-3521
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4.3 输出误差归一化
- en: Analyzing a class of errors involving words that were misrecognized due to perhaps
    a single misclassified character, we realized that the net was doing a poor job
    of representing second and third choice probabilities. Essentially, the net was
    being forced to attempt unambiguous classification of intrinsically ambiguous
    patterns due to the nature of the mean squared error minimization in BP,
  id: totrans-3522
  prefs: []
  type: TYPE_NORMAL
  zh: 分析一类涉及由于可能的单个错误分类字符而导致词语被错误识别的错误时，我们意识到网络在表示第二和第三选择概率方面表现不佳。实际上，由于均方误差最小化在BP中的性质，网络被迫尝试对本质上模糊的模式进行明确分类，
- en: coupled with the typical training vector which consists of all 0's except for
    the single 1 of the target. Lacking any viable means of encoding legitimate probabilistic
    ambiguity into the training vectors, we decided to try "normalizing" the
  id: totrans-3523
  prefs: []
  type: TYPE_NORMAL
  zh: 与典型训练向量相结合，该向量由除目标的单个1以外的所有0组成。由于缺乏任何可行的方法将合法的概率模糊编码到训练向量中，我们决定尝试“归一化”该
- en: '"pressure towards 0" vs. the "pressure towards 1" introduced by the output
    error during training. We refer to this technique as *NormOutErr*, due to its
    normalizing effect on target versus non-target output error.'
  id: totrans-3524
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，由输出误差引入的“向0的压力”与“向1的压力”。我们将此技术称为*NormOutErr*，因为它对目标与非目标输出误差的归一化效果。
- en: 'We reduce the BP error for non-target classes relative to the target class
    by a factor that normalizes the total non-target error seen at a given output
    unit relative to the total target error seen at that unit. Assuming a training
    set with equal representation of classes, this normalization should then be based
    on the number of non-target versus target classes in a typical training vector,
    or, simply, the number of output units (minus one). Hence for non-target output
    units, we scale the error at each unit by a constant:'
  id: totrans-3525
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个因子来减少相对于目标类的非目标类的BP误差，该因子归一化了在给定输出单元上看到的总非目标误差相对于在该单元上看到的总目标误差。假设训练集具有类的均等表示，那么这个归一化应该基于典型训练向量中的非目标类与目标类的数量，或者，简单地说，就是输出单元的数量（减去一）。因此，对于非目标输出单元，我们以一个常数缩放每个单元的误差：
- en: E = Ae
  id: totrans-3526
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E = Ae
- en: '![273_image_0.png](273_image_0.png)'
  id: totrans-3527
  prefs: []
  type: TYPE_IMG
  zh: '![273_image_0.png](273_image_0.png)'
- en: Fig. 13.5. Empirical p vs. y histogram for a net trained with A = 0.11 (d =
    0.1), with the corresponding theoretical curve
  id: totrans-3528
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5. 经过 A = 0.11（d = 0.1）训练的网络的经验 p 与 y 直方图，以及相应的理论曲线
- en: 'where e is the error at an output unit, and A is defined to be:'
  id: totrans-3529
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 e 是输出单元的误差，A 被定义为：
- en: $$A={\frac{1}{d(N_{o u t p u t s}-1)}}$$
  id: totrans-3530
  prefs: []
  type: TYPE_NORMAL
  zh: $$A={\frac{1}{d(N_{o u t p u t s}-1)}}$$
- en: where N*outputs* is the number of output units, and d is our tuning parameter,
    typically ranging from 0.1 to 0.2. Error at the target output unit is unchanged.
  id: totrans-3531
  prefs: []
  type: TYPE_NORMAL
  zh: 其中N*outputs*是输出单元的数量，d是我们的调节参数，通常范围在0.1到0.2之间。目标输出单元的误差保持不变。
- en: Overall, this raises the activation values at the output units, due to the reduced
    pressure towards zero, particularly for low-probability samples. Thus the learning
    algorithm no longer converges to a least mean-squared error (LMSE)
  id: totrans-3532
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这提高了输出单元的激活值，因减少了对零的压力，特别是对于低概率样本。因此，学习算法不再收敛于最小均方误差（LMSE）。
- en: estimate of p(class|*input*), but to an LMSE estimate of a nonlinear function
    f(p(class|input), A) depending on the factor A by which we reduced the error pressure
    toward zero.
  id: totrans-3533
  prefs: []
  type: TYPE_NORMAL
  zh: 对p(class|*input*)的估计，但对于依赖于我们减少误差压力向零的因子A的非线性函数f(p(class|input), A)的LMSE估计。
- en: Using a simple version of the technique of [3], we worked out what that resulting
    nonlinear function is. The net will attempt to converge to minimize the modified
    quadratic error function
  id: totrans-3534
  prefs: []
  type: TYPE_NORMAL
  zh: 使用技术[3]的简单版本，我们推导出了结果非线性函数是什么。网络将尝试收敛以最小化修改后的二次误差函数。
- en: $$\langle{\hat{E}}^{2}\rangle=p(1-y)^{2}+A(1-p)y^{2}$$
  id: totrans-3535
  prefs: []
  type: TYPE_NORMAL
  zh: $$\langle{\hat{E}}^{2}\rangle=p(1-y)^{2}+A(1-p)y^{2}$$
- en: by setting its output y for a particular class to
  id: totrans-3536
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为特定类设置其输出y为
- en: $$y={\frac{p}{A-A p+p}}$$
  id: totrans-3537
  prefs: []
  type: TYPE_NORMAL
  zh: $$y={\frac{p}{A-A p+p}}$$
- en: where p = p(class|*input*), and A is as defined above. For small values of p,
    the activation y is increased by a factor of nearly 1/A relative to the conventional
    case of y = p, and for high values of p the activation is closer to 1 by nearly
    a factor of A. The inverse function, useful for converting back to a probability,
    is
  id: totrans-3538
  prefs: []
  type: TYPE_NORMAL
  zh: 其中p = p(class|*input*)，A如上所述。对于小值的p，激活y相对于传统情况y = p增加了近1/A的因子，而对于高值的p，激活接近于1，几乎是A的因子。反函数有助于转换回概率，表达为
- en: $$p={\frac{y A}{y A+1-y}}$$
  id: totrans-3539
  prefs: []
  type: TYPE_NORMAL
  zh: $$p={\frac{y A}{y A+1-y}}$$
- en: '![274_image_0.png](274_image_0.png)'
  id: totrans-3540
  prefs: []
  type: TYPE_IMG
  zh: '![274_image_0.png](274_image_0.png)'
- en: Fig. 13.6. Character and word error rates for two different values of *NormOutErr(d)*.
  id: totrans-3541
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6。两种不同值的*NormOutErr(d)*的字符和单词错误率。
- en: A value of 0.0 disables *NormOutErr*, yielding normal BP. The unusually high
    value of 0.8 (A = 0.013) produces nearly equal pressures towards 0 and 1.
  id: totrans-3542
  prefs: []
  type: TYPE_NORMAL
  zh: 值为0.0将禁用*NormOutErr*，产生正常的BP。异常高的值0.8（A = 0.013）使得对0和1的压力几乎相等。
- en: We verified the fit of this function by looking at histograms of character-level
    empirical percentage-correct versus y, as in Figure 13.5.
  id: totrans-3543
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过查看字符级的经验正确率与y的直方图，验证了该函数的拟合，如图13.5所示。
- en: Even for this moderate amount of output error normalization, it is clear that
    the lower-probability samples have their output activations raised significantly,
    relative to the 45◦ line that A = 1 yields.
  id: totrans-3544
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对于这种适度的输出误差归一化，显然低概率样本的输出激活相对于A = 1所产生的45°线显著提高。
- en: 'The primary benefit derived from this technique is that the net does a much
    better job of representing second and third choice probabilities, and low probabilities
    in general. Despite a small drop in top choice character accuracy when using *NormOutErr*,
    we obtain a very significant increase in word accuracy by this technique. Figure
    13.6 shows an exaggerated example of this effect, for an atypically large value
    of d (0.8), which overly penalizes character accuracy; however, the 30% decrease
    in word error rate is normal for this technique. (Note: These data are from a
    multi-year-old experiment, and are not necessarily representative of current levels
    of performance on any absolute scale.)'
  id: totrans-3545
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术带来的主要好处是网络在表示第二和第三选择概率以及低概率方面表现更好。尽管使用*NormOutErr*时最高选择字符的准确率略有下降，但通过该技术我们获得了显著的单词准确率提升。图13.6显示了这种效果的夸张例子，针对一个非典型的大值d（0.8），过度惩罚字符准确率；然而，单词错误率减少30%对于该技术来说是正常的。（注意：这些数据来自数年前的实验，不一定代表当前的性能水平。）
- en: 13.4.4 Negative Training
  id: totrans-3546
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4.4 负训练
- en: The previously discussed inherent ambiguities in character segmentation necessarily
    result in the generation and testing of a large number of invalid segments.
  id: totrans-3547
  prefs: []
  type: TYPE_NORMAL
  zh: 前面讨论的字符分割固有模糊性必然导致生成和测试大量无效片段。
- en: During recognition, the network must classify these invalid segments just as
    it would any valid segment, with no knowledge of which are valid or invalid. A
    significant increase in word-level recognition accuracy was obtained by performing
    negative training with these invalid segments. This consists of presenting invalid
    segments to the net during training, with all-zero target vectors. We retain control
    over the degree of negative training in two ways. First is a *negative-training*
    factor (ranging from 0.2 to 0.5) that modulates the learning rate (equivalently
    by modulating the error at the output layer) for these negative patterns. This
    reduces the impact of negative training on positive training, thus modulating
    the impact on characters that specifically look like elements of multi-stroke
    characters (e.g., I, 1, l, o, O, 0). Secondly, we control a *negative-training
    probability*
  id: totrans-3548
  prefs: []
  type: TYPE_NORMAL
  zh: 在识别过程中，网络必须像处理任何有效段一样对待这些无效段，而不需要知道哪些是有效的，哪些是无效的。通过对这些无效段进行负训练，显著提高了词级识别的准确性。这包括在训练过程中向网络展示无效段，并使用全零目标向量。我们通过两种方式控制负训练的程度。首先是一个*负训练*因子（范围从0.2到0.5），它调节这些负模式的学习速率（等效于调节输出层的误差）。这减少了负训练对正训练的影响，从而调节了对特别类似于多笔画字符元素的字符的影响（例如，I、1、l、o、O、0）。其次，我们控制一个*负训练概率*
- en: (ranging between 0.05 and 0.3), which determines the probability that a particular
    negative sample will actually be trained on (for a given presentation). This both
    reduces the overall impact of negative training, and significantly reduces training
    time, since invalid segments are more numerous than valid segments.
  id: totrans-3549
  prefs: []
  type: TYPE_NORMAL
  zh: （范围在0.05到0.3之间），这决定了特定负样本在给定展示下实际被训练的概率。这不仅减少了负训练的整体影响，还显著减少了训练时间，因为无效段的数量远多于有效段。
- en: As with *NormOutErr*, this modification hurts character-level accuracy a little
    bit, but helps word-level accuracy a lot.
  id: totrans-3550
  prefs: []
  type: TYPE_NORMAL
  zh: 与*NormOutErr*一样，这一修改稍微影响了字符级的准确性，但大大提高了词级的准确性。
- en: 13.4.5 Stroke Warping
  id: totrans-3551
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4.5 笔划变形
- en: During training (but not during recognition), we produce random variations in
    stroke data, consisting of small changes in skew, rotation, and x and y linear
    and quadratic scalings. This produces alternate character forms that are consistent
    with stylistic variations within and between writers, and induces an explicit
    aspect ratio and rotation invariance within the framework of standard back-propagation.
    The amounts of each distortion to apply were chosen through cross-validation experiments,
    as just the amount needed to yield optimum generalization. (*Cross-validation*
    is a standard technique for *early stopping* of ANN
  id: totrans-3552
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中（而不是在识别过程中），我们会产生笔划数据的随机变异，包含偏斜、旋转以及x和y线性和二次缩放的小变化。这产生了与作家之间及作家内部的风格变异一致的替代字符形式，并在标准反向传播框架内引入了显式的长宽比和旋转不变性。每种扭曲的应用量是通过交叉验证实验选择的，以获得最佳的泛化效果。（*交叉验证*是一种用于ANN的*提前停止*的标准技术）
- en: training, to prevent over-learning of the training set, and thus reduced accuracy
    on new data outside that training set. The technique consists of keeping aside
    some subset of the available data, the cross-validation set, and testing on it
    at some interval, but never training on it, and then stopping the training when
    accuracy ceases to improve on this cross-validation set, despite the fact that
    accuracy might continue to improve on the training set.) We chose relative amounts
    of the various transformations by testing for optimal final, converged accuracy
    on a cross-validation set. We then increased the amount of all stroke warping
    being applied to the training set, just to the point at which accuracy on the
    training set ceased to diverge from accuracy on the cross-validation set.
  id: totrans-3553
  prefs: []
  type: TYPE_NORMAL
  zh: 训练，以防止对训练集的过度学习，从而在新的数据上降低准确性，该数据超出了训练集。该技术包括保留可用数据的某个子集，即交叉验证集，并在某个间隔进行测试，但从不对其进行训练，然后在这个交叉验证集上的准确性不再改善时停止训练，尽管在训练集上的准确性可能继续改善。我们通过测试在交叉验证集上获得最佳最终收敛准确性的相对变换量，来选择各种变换的相对数量。然后，我们增加对训练集应用的所有笔划变形的数量，直到训练集上的准确性不再与交叉验证集上的准确性偏离。
- en: We also examined a number of such samples by eye to verify that they represent
    a natural range of variation. A small set of such variations is shown in Figure
    13.7.
  id: totrans-3554
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过目测检查了一些这样的样本，以验证它们代表了自然的变异范围。图13.7中显示了一小部分这样的变异。
- en: Fig. 13.7. A few random stroke warpings of the same original "m" data
  id: totrans-3555
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7。同一原始“m”数据的几个随机笔划变换
- en: '![275_image_0.png](275_image_0.png)'
  id: totrans-3556
  prefs: []
  type: TYPE_IMG
  zh: '![275_image_0.png](275_image_0.png)'
- en: '![275_image_1.png](275_image_1.png)'
  id: totrans-3557
  prefs: []
  type: TYPE_IMG
  zh: '![275_image_1.png](275_image_1.png)'
- en: '![275_image_2.png](275_image_2.png)'
  id: totrans-3558
  prefs: []
  type: TYPE_IMG
  zh: '![275_image_2.png](275_image_2.png)'
- en: Our stroke warping scheme is somewhat related to the ideas of Tangent Dist and
    Tangent Prop [14, 13] (see chapter 12), in terms of the use of predetermined families
    of transformations, but we believe it is much easier to implement. It is also
    somewhat distinct in applying transformations on the original coordinate data,
    as opposed to using distortions of images. The voice transformation scheme of
    [4] is also related, but they use a static replication of the training set through
    a small number of transformations, rather than dynamic random transformations
    of an essentially infinite variety.
  id: totrans-3559
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的笔划变形方案在某种程度上与切线距离和切线传播的思想[14, 13]（见第12章）相关，涉及使用预定的变换系列，但我们认为实现起来要容易得多。它还在应用于原始坐标数据的变换方面有些不同，而不是使用图像的失真。文献[4]中的语音变换方案也相关，但他们通过少量变换使用静态复制训练集，而不是动态随机变换本质上无限种类的样本。
- en: 13.4.6 Frequency Balancing
  id: totrans-3560
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4.6 频率平衡
- en: Training data from natural English words and phrases exhibit very non-uniform
    priors for the various character classes, and ANNs readily model these priors.
    However, as with *NormOutErr*, we find that reducing the effect of these priors
    on the net, in a controlled way, and thus forcing the net to allocate more of
    its resources to low-frequency, low-probability classes is of significant benefit
    to the overall word recognition process. To this end, we explicitly (partially)
    balance the frequencies of the classes during training. We do this by probabilistically
    skipping and repeating patterns, based on a precomputed repetition factor. Each
    presentation of a repeated pattern is "warped" uniquely, as discussed previously.
  id: totrans-3561
  prefs: []
  type: TYPE_NORMAL
  zh: 来自自然英语单词和短语的训练数据在各种字符类别上表现出非常不均匀的先验，而人工神经网络可以轻松建模这些先验。然而，与*NormOutErr*一样，我们发现以受控方式减少这些先验对网络的影响，从而迫使网络将更多资源分配给低频、低概率类别，对整体词识别过程有显著益处。为此，我们在训练过程中明确（部分）平衡类别的频率。我们通过基于预计算的重复因子以概率方式跳过和重复模式来实现这一点。每次展示的重复模式都是“独特变形”的，如前所述。
- en: 'To compute the repetition factor for a class i, we first compute a normalized
    frequency of that class:'
  id: totrans-3562
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算类别i的重复因子，我们首先计算该类别的归一化频率：
- en: $$F_{i}={\frac{S_{i}}{S}}$$
  id: totrans-3563
  prefs: []
  type: TYPE_NORMAL
  zh: $$F_{i}={\frac{S_{i}}{S}}$$
- en: 'where Si is the number of samples in class i, and S¯ is the average number
    of samples over all classes, computed in the obvious way:'
  id: totrans-3564
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Si是类别i中的样本数，S¯是所有类别样本的平均数量，以明显的方式计算：
- en: $${\bar{S}}={\frac{1}{C}}\sum_{i=1}^{C}S_{i}$$
  id: totrans-3565
  prefs: []
  type: TYPE_NORMAL
  zh: $${\bar{S}}={\frac{1}{C}}\sum_{i=1}^{C}S_{i}$$
- en: 'with C being the number of classes. Our repetition factor is then defined to
    be:'
  id: totrans-3566
  prefs: []
  type: TYPE_NORMAL
  zh: 其中C是类别的数量。我们的重复因子定义为：
- en: $$R_{i}=\left({\frac{a}{F_{i}}}\right)^{b}$$
  id: totrans-3567
  prefs: []
  type: TYPE_NORMAL
  zh: $$R_{i}=\left({\frac{a}{F_{i}}}\right)^{b}$$
- en: with a and b being adjustable controls over the amount of skipping vs. repeating
    and the degree of prior normalization, respectively. Typical values of a range
    from 0.2 to 0.8, while b ranges from 0.5 to 0.9. The factor a < 1 lets us do more
    skipping than repeating; e.g. for a = 0.5, classes with relative frequency equal
    to half the average will neither skip nor repeat; more frequent classes will skip,
    and less frequent classes will repeat. A value of 0.0 for b would do nothing,
    giving Ri = 1.0 for all classes, while a value of 1.0 would provide "full" normalization.
  id: totrans-3568
  prefs: []
  type: TYPE_NORMAL
  zh: 其中a和b分别是对跳过与重复的数量以及先验归一化程度的可调控制。a的典型值范围从0.2到0.8，而b的范围从0.5到0.9。因子a < 1让我们做更多的跳过而不是重复；例如，对于a
    = 0.5，相对频率等于平均值一半的类别既不会跳过也不会重复；频率更高的类别将被跳过，而频率较低的类别将被重复。b的值为0.0将不会起作用，所有类别的Ri
    = 1.0，而b的值为1.0将提供“完全”归一化。
- en: A value of b somewhat less than one seems to be the best choice, letting the
    net keep some bias in favor of classes with higher prior probabilities.
  id: totrans-3569
  prefs: []
  type: TYPE_NORMAL
  zh: b的值稍低于1似乎是最佳选择，让网络保持对具有更高先验概率类别的一些偏见。
- en: This explicit prior-bias reduction is conceptually related to Lippmann's [8]
  id: totrans-3570
  prefs: []
  type: TYPE_NORMAL
  zh: 这种显式的先验偏差减少在概念上与Lippmann的[8]有关。
- en: and Morgan and Bourlard's [10] recommended method for converting from the net's
    estimate of posterior probability, p(class|*input*), to the value needed in an
    HMM or Viterbi search, p(input|*class*), which is to divide by p(*class*) priors.
    Using that technique, however, should produce noisier estimates for low frequency
    classes, due to the divisions by low frequencies, resulting in a set of estimates
    that are not really optimized in a LMSE sense (as the net outputs are). In addition,
    output activations that are naturally bounded between 0 and 1, due to the sigmoid,
    convert to potentially very large probability estimates, requiring a re-normalization
    step. Our method of frequency balancing during training eliminates both of these
    concerns. Perhaps more significantly, frequency balancing also allows the standard
    BP training process to dedicate more network resources to the classification of
    the lower-frequency classes, though we have no current method for characterizing
    or quantifying this benefit.
  id: totrans-3571
  prefs: []
  type: TYPE_NORMAL
  zh: Morgan和Bourlard推荐的方法是将网络对后验概率p(class|*input*)的估计转换为HMM或Viterbi搜索中所需的值p(input|*class*)，方法是除以p(*class*)的先验值。然而，使用该技术会由于低频类的除法而产生噪声较大的估计，导致一组估计在LMSE意义上并未真正优化（就像网络输出那样）。此外，由于sigmoid的作用，输出激活自然限制在0和1之间，可能转换为非常大的概率估计，需要重新归一化步骤。我们在训练期间的频率平衡方法消除了这两个问题。或许更重要的是，频率平衡还允许标准BP训练过程将更多网络资源专用于低频类的分类，尽管我们目前没有方法来表征或量化这种好处。
- en: 13.4.7 Error Emphasis
  id: totrans-3572
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4.7 错误强调
- en: While frequency balancing corrects for under-represented classes, it cannot
    account for under-represented writing styles. We utilize a conceptually related
    probabilistic skipping of patterns, but this time for just those patterns that
    the net correctly classifies in its forward/recognition pass, as a form of "error
    emphasis", to address this problem. We define a *correct-train probability* (ranging
    from 0.1 to 1.0) that is used as a biased coin to determine whether a particular
    pattern, having been correctly classified, will also be used for the backward/training
    pass or not. This only applies to correctly segmented, or "positive" patterns,
    and misclassified patterns are never skipped.
  id: totrans-3573
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然频率平衡纠正了不足表示的类别，但它无法解决不足表示的书写风格。我们利用一个概念上相关的模式概率跳过方法，但这次仅针对网络在其前向/识别传递中正确分类的那些模式，作为一种“错误强调”的形式，以解决此问题。我们定义了一个*正确训练概率*（范围从0.1到1.0），用于作为偏置硬币决定某个已正确分类的模式是否也会用于反向/训练传递。这仅适用于正确分割的或“正面”模式，而错误分类的模式从不被跳过。
- en: Especially during early stages of training, we set this parameter fairly low
  id: totrans-3574
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其在训练的早期阶段，我们将此参数设置得相对较低。
- en: (around 0.1), thus concentrating most of the training time and the net's learning
    capability on patterns that are more difficult to correctly classify. This is
    the only way we were able to get the net to learn to correctly classify unusual
    character variants, such as a 3-stroke "5" as written by only one training writer.
  id: totrans-3575
  prefs: []
  type: TYPE_NORMAL
  zh: （约0.1），因此将大部分训练时间和网络的学习能力集中在更难以正确分类的模式上。这是我们能够让网络学习正确分类不寻常字符变体（例如，仅由一个训练作者书写的3笔“5”）的唯一方法。
- en: Variants of this scheme are possible in which misclassified patterns would be
    repeated, or different learning rates would apply to correctly and incorrectly
    classified patterns. It is also related to techniques that use a training subset,
    from which easily-classified patterns are replaced by randomly selected patterns
    from the full training set [6].
  id: totrans-3576
  prefs: []
  type: TYPE_NORMAL
  zh: 该方案的变体是可能的，其中错误分类的模式会被重复，或者不同的学习率适用于正确和错误分类的模式。这也与使用训练子集的技术有关，从中容易分类的模式被从完整训练集中随机选择的模式替换。
- en: 13.4.8 Annealing
  id: totrans-3577
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4.8 退火
- en: Though some discussions of back-propagation espouse explicit formulae for modulating
    the learning rate over time, many seem to assume the use of a single, fixed learning
    rate. We view the stochastic back-propagation process as a kind of simulated annealing,
    with a learning rate starting very high and decreasing only slowly to a very low
    value. But rather than using any prespecified formula to decelerate learning,
    the rate at which the learning rate decreases is determined by the dynamics of
    the learning process itself. We typically start with a rate near 1.0 and reduce
    the rate by a multiplicative *decay factor* of 0.9 until it gets down to about
    0.001. The rate decay factor is applied following any epoch in which the total
    squared error increased on the training set, relative to the previous epoch. This
    "total squared error" is summed over all output units and over all patterns in
    one full epoch, and normalized by those counts. So even though we are using "online"
    or stochastic gradient descent, we have a measure of performance over whole epochs
    that can be used to guide the "annealing" of the learning rate. Repeated tests
    indicate that this approach yields better results than low (or even moderate)
    initial learning rates, which we speculate to be related to a better ability to
    escape local minima.
  id: totrans-3578
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管关于反向传播的一些讨论主张使用显式公式来调节学习率，但许多人似乎假设使用单一的固定学习率。我们将随机反向传播过程视为一种模拟退火，其中学习率开始时非常高，仅缓慢下降到一个非常低的值。但与其使用任何预先指定的公式来减缓学习，我们决定学习率下降的速度由学习过程本身的动态决定。我们通常从接近
    1.0 的速率开始，并以 0.9 的乘法*衰减因子*逐步降低，直到降到大约 0.001。学习率衰减因子在任何一个迭代中应用，其中训练集上的总平方误差相较于前一个迭代有所增加。这个“总平方误差”是针对所有输出单元和一个完整迭代中的所有模式求和的，并根据这些计数进行标准化。因此，即使我们使用“在线”或随机梯度下降，我们也有一个可以用来指导学习率“退火”的整体迭代性能衡量。反复测试表明，这种方法产生的结果优于较低（甚至适度）的初始学习率，我们推测这与更好地逃避局部极小值的能力有关。
- en: In addition, we find that we obtain best overall results when we also allow
    some of our many training parameters to change over the course of a training run.
    In particular, the correct train probability needs to start out very low to give
    the net a chance to learn unusual character styles, but it should finish up near
    1.0 in order to not introduce a general posterior probability bias in favor of
    classes with lots of ambiguous examples. We typically train a net in four "phases"
    according to parameters such as in Figure 13.8.
  id: totrans-3579
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们发现，当我们允许一些训练参数在训练过程中变化时，整体结果最佳。特别是，正确的训练概率需要开始时非常低，以便给网络一个机会去学习不寻常的字符风格，但它应该在接近
    1.0 时结束，以避免引入有利于包含大量模糊示例类别的一般后验概率偏差。我们通常根据图 13.8 中的参数，在四个“阶段”中训练网络。
- en: '|        |               | Negative Train Prob   |      |      |'
  id: totrans-3580
  prefs: []
  type: TYPE_TB
  zh: '|        |              | 负训练概率          |      |      |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-3581
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Epochs |               | Correct Train Prob    |      |      |'
  id: totrans-3582
  prefs: []
  type: TYPE_TB
  zh: '| 迭代次数 |          | 正确训练概率       |      |      |'
- en: '| Phase  | Learning Rate |                       |      |      |'
  id: totrans-3583
  prefs: []
  type: TYPE_TB
  zh: '| 阶段  | 学习率      |                       |      |      |'
- en: '| 1      | 25            | 1.0 - 0.5             | 0.1  | 0.05 |'
  id: totrans-3584
  prefs: []
  type: TYPE_TB
  zh: '| 1      | 25           | 1.0 - 0.5             | 0.1  | 0.05 |'
- en: '| 2      | 25            | 0.5 - 0.1             | 0.25 | 0.1  |'
  id: totrans-3585
  prefs: []
  type: TYPE_TB
  zh: '| 2      | 25           | 0.5 - 0.1             | 0.25 | 0.1  |'
- en: '| 3      | 50            | 0.1 - 0.01            | 0.5  | 0.18 |'
  id: totrans-3586
  prefs: []
  type: TYPE_TB
  zh: '| 3      | 50           | 0.1 - 0.01            | 0.5  | 0.18 |'
- en: '| 4      | 30            | 0.01 - 0.001          | 1.0  | 0.3  |'
  id: totrans-3587
  prefs: []
  type: TYPE_TB
  zh: '| 4      | 30           | 0.01 - 0.001          | 1.0  | 0.3  |'
- en: Fig. 13.8. Typical multi-phase schedule of learning rates and other parameters
    for training a character-classifier net
  id: totrans-3588
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8. 训练字符分类网络的学习率及其他参数的典型多阶段时间表
- en: 13.4.9 Quantized Weights
  id: totrans-3589
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4.9 量化权重
- en: The work of Asanovic and Morgan [1] shows that two-byte (16-bit) weights are
    about the smallest that can be tolerated in training large ANNs via backpropagation.
    But memory is expensive in small devices, and RISC processors, such as the ARM-610
    in the first devices in which this technology was deployed, are much more efficient
    doing one-byte loads and multiplies than two-byte loads and multiplies, so we
    were motivated to make one-byte weights work.
  id: totrans-3590
  prefs: []
  type: TYPE_NORMAL
  zh: Asanovic 和 Morgan 的研究[1]表明，两个字节（16 位）的权重是训练大型人工神经网络时可以容忍的最小值。但是在小设备中，内存是昂贵的，而
    RISC 处理器（例如在首次部署此技术的设备中使用的 ARM-610）在进行单字节加载和乘法时要比进行双字节加载和乘法高效得多，因此我们有动力使单字节权重有效。
- en: Running the net for recognition demands significantly less precision than does
    training the net. It turns out that one-byte weights provide adequate precision
    for recognition, if the weights are trained appropriately. In particular, a dynamic
    range should be fixed, and weights limited to that legal range during training,
    and then rounded to the requisite precision after training. For example, we find
    that a range of weight values from (almost) -8 to +8 in steps of 1/16 does a good
    job. Figure 13.9 shows a typical resulting distribution of weight values. If the
    weight limit is enforced during high-precision training, the resources of the
    net will be adapted to make up for the limit. Since bias weights are few in number,
    however, and very important, we allow them to use two bytes with essentially unlimited
    range. Performing our forward/recognition pass with low-precision, one-byte weights
    (a ±3.4 fixed-point representation), we find no noticeable degradation relative
    to floating-point, four-byte, or two-byte weights using this scheme.
  id: totrans-3591
  prefs: []
  type: TYPE_NORMAL
  zh: 运行网络进行识别所需的精度显著低于训练网络所需的精度。结果表明，如果权重经过适当训练，单字节权重可以提供足够的识别精度。具体来说，动态范围应该是固定的，并且在训练过程中权重必须限制在该合法范围内，训练后再四舍五入到所需的精度。例如，我们发现从（几乎）-8到+8，以1/16为步长的权重值范围效果很好。图13.9显示了权重值的典型分布。如果在高精度训练期间强制执行权重限制，网络的资源将会适应以弥补限制。然而，由于偏置权重数量较少且非常重要，我们允许它们使用两个字节，范围几乎没有限制。使用低精度的单字节权重（±3.4的定点表示）进行前向/识别传递时，相对于使用这种方案的浮点数、四字节或两字节权重，我们没有发现明显的性能下降。
- en: '![279_image_0.png](279_image_0.png)'
  id: totrans-3592
  prefs: []
  type: TYPE_IMG
  zh: '![279_image_0.png](279_image_0.png)'
- en: Fig. 13.9. Distribution of weight values in a net with one-byte weights, on
    a log count scale. Weights with magnitudes greater than 4 are sparse, but important.
  id: totrans-3593
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.9. 使用单字节权重的网络中权重值的分布，采用对数计数比例。绝对值大于4的权重很稀疏，但很重要。
- en: We have also developed a scheme for training with augmented one-byte weights.
    It uses a temporary augmentation of the weight values with two additional low-order
    bytes to achieve precision in training, but runs the forward pass of the net using
    only the one-byte high-order part. Thus any cumulative effect of the one-byte
    rounded weights in the forward pass can be compensated through further training.
    Small weight changes accumulate in the low-order bytes, and only occasionally
    carry into a change in the one-byte weights used by the net. In a personal product,
    this scheme could be used for adaptation to the user, after which the low-order
    residuals could be discarded and the temporary memory reclaimed.
  id: totrans-3594
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还开发了一种使用增强单字节权重进行训练的方案。它临时增强权重值，添加两个低位字节以实现训练中的精度，但在前向传递中仅使用单字节的高位部分。因此，在前向传递中，单字节四舍五入权重的任何累积效应都可以通过进一步的训练来补偿。小的权重变化累积在低位字节中，只有偶尔会影响网络使用的单字节权重。在个人产品中，该方案可以用于用户的适应，之后低位残余可以被丢弃并回收临时内存。
- en: 13.5 Context-Driven Search
  id: totrans-3595
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.5 上下文驱动搜索
- en: The output of the ANN classifier is a stream of probability vectors, one vector
    for each segmentation hypothesis, with as many potentially nonzero probability
    elements in each vector as there are characters (that the system is capable of
    recognizing). In practice, we typically only pass the top ten (or fewer) scored
    character-class hypotheses, per segment, to the search engine, for the sake of
    efficiency. The search engine then looks for a minimum-cost path through this
    vector stream, abiding by the legal transitions between segments, as defined in
    the tentative-segmentation step discussed previously. This minimum-cost path is
    the APR system's best interpretation of the ink input by the user, and is returned
    to the system in which APR is embedded as the recognition result for whole words
    or sentences of the user's input.
  id: totrans-3596
  prefs: []
  type: TYPE_NORMAL
  zh: ANN分类器的输出是一个概率向量流，每个分段假设一个向量，每个向量中潜在非零概率元素的数量与系统能够识别的字符数量相同。实际上，为了提高效率，我们通常仅将每个分段的前十个（或更少）得分的字符类别假设传递给搜索引擎。搜索引擎然后在这个向量流中寻找最低成本路径，遵循前面讨论的初步分段步骤中定义的合法段间转换。这个最低成本路径是APR系统对用户输入的墨水的最佳解释，并作为用户输入的完整单词或句子的识别结果返回到嵌入APR的系统中。
- en: The search is driven by a somewhat *ad hoc*, generative language model, which
    consists of a set of graphs that are searched in parallel. We use a simple beam
    search in a negative-log-probability (or *penalty*) space for the best N hypotheses.
    The beam is based on a fixed maximum number of hypotheses, rather than a particular
    value. Each possible transition token (character) emitted by one of the graphs
    is scored not only by the ANN, but by the language model itself, by a simple letter-case
    model, and by geometric-context models discussed below.
  id: totrans-3597
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索由一种有点*临时*的生成语言模型驱动，该模型由一组并行搜索的图组成。我们在负对数概率（或*惩罚*）空间中使用简单的束搜索来获取最佳N个假设。束基于固定的最大假设数量，而不是特定值。由其中一个图发出的每个可能的转换标记（字符）不仅由ANN评分，还由语言模型本身、简单的字母大小写模型以及下面讨论的几何上下文模型评分。
- en: The fully integrated search process takes place over a space of character- and
    word-segmentation hypotheses, as well as character-class hypotheses.
  id: totrans-3598
  prefs: []
  type: TYPE_NORMAL
  zh: 完全集成的搜索过程发生在字符和词分割假设以及字符类假设的空间中。
- en: 13.5.1 Lexical Context
  id: totrans-3599
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.5.1 词汇上下文
- en: Context is essential to accurate recognition, even if that context takes the
    form of a very broad language model. Humans achieve just 90% accuracy on isolated
    characters from our database. Lacking any context this would translate to a word
    accuracy of not much more than 60% (0.95), assuming an average word length of
    5 characters. We obviously need to do much better, with even lower isolated-character
    accuracy, and we accomplish this by the application of our context models.
  id: totrans-3600
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文对于准确识别至关重要，即使该上下文以非常广泛的语言模型形式存在。人类在我们数据库中对孤立字符的准确率仅为90%。在缺乏任何上下文的情况下，这将转化为不超过60%的词准确率（0.95），假设平均单词长度为5个字符。显然，我们需要做得更好，甚至在孤立字符的准确率更低的情况下，我们通过应用我们的上下文模型来实现这一点。
- en: A simple model of letter case and adjacency - penalizing case transitions except
    between the first and second characters, penalizing alphabetic-to-numeric transitions,
    and so on - together with the geometric-context models discussed later, is sufficient
    to raise word-level accuracy up to around 77%.
  id: totrans-3601
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的字母大小写和邻接模型——惩罚大小写转换，除了在第一个和第二个字符之间，惩罚字母到数字的转换，等等——结合后面讨论的几何上下文模型，足以将词级准确率提升至约77%。
- en: The next large gain in accuracy requires a genuine language model. We provide
    this model by means of dictionary graphs, and assemblages of those graphs combined
    into what we refer to as *BiGrammars*. BiGrammars are essentially scored lists
    of dictionaries, together with specified legal (scored) transitions between those
    dictionaries. This scheme allows us to use word lists, prefix and suffix lists,
    and punctuation models, and to enable appropriate transitions between them. Some
    dictionary graphs are derived from a regular-expression grammar that permits us
    to easily model phone numbers, dates, times, etc., as shown in Figure 13.10.
  id: totrans-3602
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率的下一次大幅提升需要一个真正的语言模型。我们通过字典图和将这些图组合成我们称之为*双语法*的集合提供该模型。双语法本质上是字典的评分列表，以及这些字典之间指定的合法（评分）转换。该方案使我们能够使用词列表、前缀和后缀列表以及标点模型，并在它们之间进行适当的转换。一些字典图是从正则表达式语法派生的，这使我们能够轻松地建模电话号码、日期、时间等，如图13.10所示。
- en: dig = [0123456789] digm01 = [23456789] acodenums = (digm01 [01] dig) acode =
    { ("1-"? acodenums "-"):40 , ("1"? "(" acodenums ")"):60 } phone = (acode? digm01
    dig dig "-" dig dig dig dig)
  id: totrans-3603
  prefs: []
  type: TYPE_NORMAL
  zh: dig = [0123456789] digm01 = [23456789] acodenums = (digm01 [01] dig) acode =
    { ("1-"? acodenums "-"):40 , ("1"? "(" acodenums ")"):60 } phone = (acode? digm01
    dig dig "-" dig dig dig dig)
- en: Fig. 13.10. Sample of the regular-expression language used to define a simple
    telephone-number grammar. Symbols are defined by the equal operator; square brackets
    enclose multiple, alternative characters; parentheses enclose sequences of symbols;
    curly braces enclose multiple, alternative symbols; an appended colon followed
    by numbers designates a prior probability of that alternative; an appended question
    mark means "zero or one occurrence"; and the final symbol definition represents
    the graph or grammar expressed by this dictionary.
  id: totrans-3604
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.10. 用于定义简单电话号码语法的正则表达式语言示例。符号由等号运算符定义；方括号括起来多个可选字符；圆括号括起来符号序列；大括号括起来多个可选符号；附加的冒号后跟数字表示该选项的先验概率；附加的问号表示“零或一次出现”；最终的符号定义表示由此字典表达的图形或语法。
- en: All of these dictionaries can be searched in parallel by combining them into
    a general-purpose BiGrammar that is suitable for most applications. It is also
    possible to combine subsets of these dictionaries, or special-purpose dictionaries,
    into special BiGrammars targeted at more limited contexts. A very simple
  id: totrans-3605
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些字典可以通过将它们组合成适合大多数应用的通用双语法来并行搜索。也可以将这些字典的子集或特殊用途字典组合成针对更有限上下文的特殊双语法。一种非常简单的
- en: Bigrammar Phone
  id: totrans-3606
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双语法电话
- en: '[Phone.lang 1. 1. 1.]'
  id: totrans-3607
  prefs: []
  type: TYPE_NORMAL
  zh: '[电话.lang 1. 1. 1.]'
- en: Fig. 13.11. Sample of a simple BiGrammar describing a telephone-only context.
  id: totrans-3608
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.11. 描述仅限于电话的上下文的简单双语法示例。
- en: The BiGrammar is first named (Phone), and then specified as a list of dictionaries
  id: totrans-3609
  prefs: []
  type: TYPE_NORMAL
  zh: 双语法首先被命名为（电话），然后指定为字典列表
- en: (Phone.lang), together with the probability of starting with this dictionary,
    ending with this dictionary, and cycling within this dictionary (the three numerical
    values).
  id: totrans-3610
  prefs: []
  type: TYPE_NORMAL
  zh: （电话.lang），连同以该字典开始、结束以及在该字典内循环的概率（这三个数值）。
- en: BiGrammar FairlyGeneral
  id: totrans-3611
  prefs: []
  type: TYPE_NORMAL
  zh: 双语法相当通用
- en: '![281_image_1.png](281_image_1.png)'
  id: totrans-3612
  prefs: []
  type: TYPE_IMG
  zh: '![281_image_1.png](281_image_1.png)'
- en: '![281_image_2.png](281_image_2.png)'
  id: totrans-3613
  prefs: []
  type: TYPE_IMG
  zh: '![281_image_2.png](281_image_2.png)'
- en: '![281_image_3.png](281_image_3.png)'
  id: totrans-3614
  prefs: []
  type: TYPE_IMG
  zh: '![281_image_3.png](281_image_3.png)'
- en: '![281_image_4.png](281_image_4.png)'
  id: totrans-3615
  prefs: []
  type: TYPE_IMG
  zh: '![281_image_4.png](281_image_4.png)'
- en: '![281_image_0.png](281_image_0.png)'
  id: totrans-3616
  prefs: []
  type: TYPE_IMG
  zh: '![281_image_0.png](281_image_0.png)'
- en: (.6
  id: totrans-3617
  prefs: []
  type: TYPE_NORMAL
  zh: (.6
- en: '[WordList.dict .5 .8 1. EndPunct.lang .2]'
  id: totrans-3618
  prefs: []
  type: TYPE_NORMAL
  zh: '[单词列表.dict .5 .8 1. 结束标点.lang .2]'
- en: '[User.dict .5 .8 1. EndPunct.lang .2]'
  id: totrans-3619
  prefs: []
  type: TYPE_NORMAL
  zh: '[用户.dict .5 .8 1. 结束标点.lang .2]'
- en: )
  id: totrans-3620
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: (.4
  id: totrans-3621
  prefs: []
  type: TYPE_NORMAL
  zh: (.4
- en: '[Phone.lang .5 .8 1. EndPunct.lang .2]'
  id: totrans-3622
  prefs: []
  type: TYPE_NORMAL
  zh: '[电话.lang .5 .8 1. 结束标点.lang .2]'
- en: '[Date.lang .5 .8 1. EndPunct.lang .2] ) ![281_image_6.png](281_image_6.png)'
  id: totrans-3623
  prefs: []
  type: TYPE_NORMAL
  zh: '[日期.lang .5 .8 1. 结束标点.lang .2] ) ![281_image_6.png](281_image_6.png)'
- en: '![281_image_5.png](281_image_5.png)'
  id: totrans-3624
  prefs: []
  type: TYPE_IMG
  zh: '![281_image_5.png](281_image_5.png)'
- en: '![281_image_8.png](281_image_8.png)'
  id: totrans-3625
  prefs: []
  type: TYPE_IMG
  zh: '![281_image_8.png](281_image_8.png)'
- en: '![281_image_9.png](281_image_9.png)'
  id: totrans-3626
  prefs: []
  type: TYPE_IMG
  zh: '![281_image_9.png](281_image_9.png)'
- en: '![281_image_10.png](281_image_10.png)'
  id: totrans-3627
  prefs: []
  type: TYPE_IMG
  zh: '![281_image_10.png](281_image_10.png)'
- en: '![281_image_11.png](281_image_11.png)'
  id: totrans-3628
  prefs: []
  type: TYPE_IMG
  zh: '![281_image_11.png](281_image_11.png)'
- en: '[OpenPunct.lang 1. 0. .5'
  id: totrans-3629
  prefs: []
  type: TYPE_NORMAL
  zh: '[开放标点.lang 1. 0. .5'
- en: (.6
  id: totrans-3630
  prefs: []
  type: TYPE_NORMAL
  zh: (.6
- en: '![281_image_7.png](281_image_7.png) )'
  id: totrans-3631
  prefs: []
  type: TYPE_IMG
  zh: '![281_image_7.png](281_image_7.png) )'
- en: (.4 Date.lang .5
  id: totrans-3632
  prefs: []
  type: TYPE_NORMAL
  zh: (.4 日期.lang .5
- en: )
  id: totrans-3633
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ']'
  id: totrans-3634
  prefs: []
  type: TYPE_NORMAL
  zh: ']'
- en: )
  id: totrans-3635
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '[EndPunct.lang 0. .9 .5 EndPunct.lang .1]'
  id: totrans-3636
  prefs: []
  type: TYPE_NORMAL
  zh: '[结束标点.lang 0. .9 .5 结束标点.lang .1]'
- en: '![281_image_12.png](281_image_12.png)'
  id: totrans-3637
  prefs: []
  type: TYPE_IMG
  zh: '![281_image_12.png](281_image_12.png)'
- en: '![281_image_13.png](281_image_13.png)'
  id: totrans-3638
  prefs: []
  type: TYPE_IMG
  zh: '![281_image_13.png](281_image_13.png)'
- en: Fig. 13.12. Sample of a slightly more complex BiGrammar describing a fairly
    general context. The BiGrammar is first named (FairlyGeneral), and then specified
    as a list of dictionaries (the *.dict and *.lang entries), together with the probability
    of starting with this dictionary, ending with this dictionary, and cycling within
    this dictionary (the first three numerical values following each dictionary name),
    plus any dictionaries to which this dictionary may legally transition, along with
    the probability of taking that transition. The parentheses permit easy specification
    of multiplicative prior probabilities for all dictionaries contained within them.
    Note that in this simple example, it is not possible (starting probability = 0)
    to start a string with the EndPunct (end punctuation)
  id: totrans-3639
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.12. 描述相当通用上下文的稍微复杂一些的双语法示例。双语法首先被命名为（FairlyGeneral），然后指定为字典列表（*.dict和*.lang条目），连同以该字典开始、结束以及在该字典内循环的概率（每个字典名称后面的前三个数值），加上该字典可以合法过渡到的任何字典，以及进行该过渡的概率。括号便于为其中包含的所有字典指定乘法先验概率。请注意，在这个简单的例子中，无法以结束标点（end
    punctuation）开始字符串（起始概率= 0）。
- en: dictionary, just as it is not possible to end a string with the OpenPunct dictionary.
  id: totrans-3640
  prefs: []
  type: TYPE_NORMAL
  zh: 字典，就像以开放标点字典结束字符串也是不可能的。
- en: BiGrammar, which might be useful to specify context for a field that only accepts
    telephone numbers, is shown in Figure 13.11. A more complex BiGrammar
  id: totrans-3641
  prefs: []
  type: TYPE_NORMAL
  zh: 用于指定只接受电话号码的领域上下文的双语法，如图13.11所示。更复杂的双语法
- en: (though still far short of the complexity of our final general-input context)
    is shown in Figure 13.12.
  id: totrans-3642
  prefs: []
  type: TYPE_NORMAL
  zh: (尽管仍然远未达到我们最终通用输入上下文的复杂性)如图13.12所示。
- en: We refer to our language model as being "weakly applied" because in parallel
    with all of the wordlist-based dictionaries and regular-expression grammars, we
    simultaneously search both an alphabetic-characters grammar ("wordlike") and a
    completely general, any-character-anywhere grammar ("symbols"). These more flexible
    models, though given fairly low *a priori* probabilities, permit users to write
    any unusual character string they might desire. When the prior probabilities for
    the various dictionaries are properly balanced, the recognizer is able to benefit
    from the language model, and deliver the desired level of accuracy for common
    in-dictionary words (and special constructs like phone numbers, etc.), yet can
    also recognize arbitrary, non-dictionary character strings, especially if they
    are written neatly enough that the character classifier can be confident of its
    classifications.
  id: totrans-3643
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的语言模型称为“弱应用”，因为在所有基于词表的字典和正则表达式语法的并行搜索中，我们同时搜索字母字符语法（“类似单词”）和完全通用的任意字符任意位置语法（“符号”）。这些更灵活的模型虽然具有相对较低的*a
    priori* 概率，但允许用户编写他们可能希望的任何不寻常字符字符串。当各种字典的先验概率得到合理平衡时，识别器能够从语言模型中受益，并为常见的字典内单词（以及特殊构造，如电话号码等）提供所需的准确性，同时也能够识别任意的非字典字符字符串，尤其是当它们写得足够整齐以使字符分类器对其分类有信心时。
- en: We have also experimented with bi-grams, tri-grams, N-grams, and we are continuing
    experiments with other, more data-driven language models; so far, however, our
    generative approach has yielded the best results.
  id: totrans-3644
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还实验了二元组、三元组、N-元组，并继续进行其他更数据驱动的语言模型的实验；然而，到目前为止，我们的生成方法已经产生了最佳结果。
- en: 13.5.2 Geometric Context
  id: totrans-3645
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.5.2 几何上下文
- en: We have never found a way to reliably estimate a baseline or topline for characters,
    independent of classifying those characters in a word. Non-recognitionintegrated
    estimates of these line positions, based on strictly geometric features, have
    too many pathological failure modes, which produce erratic recognition failures.
    Yet the geometric positioning of characters most certainly bears information important
    to the recognition process. Our system factors the problem by letting the ANN
    classify representations that are independent of baseline and size, and then using
    separate modules to score both the absolute size of individual characters, and
    the relative size and position of adjacent characters.
  id: totrans-3646
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从未找到一种方法来可靠地估计字符的基线或顶线，独立于在单词中对这些字符的分类。基于严格几何特征的非识别集成这些行位置的估计，具有太多病态故障模式，从而产生不稳定的识别失败。然而，字符的几何定位无疑承载着对识别过程重要的信息。我们的系统通过让人工神经网络（ANN）分类独立于基线和大小的表示来分解问题，然后使用独立模块为单个字符的绝对大小以及相邻字符的相对大小和位置打分。
- en: The scoring based on absolute size is derived from a set of simple Gaussian
    models of individual character heights, relative to some running scale parameters
    computed both during learning and during recognition. This *CharHeight* score
    directly multiplies the scores emitted by the ANN classifier, and helps significantly
    in case disambiguation.
  id: totrans-3647
  prefs: []
  type: TYPE_NORMAL
  zh: 基于绝对大小的评分来自一组简单的高斯模型，描述个别字符高度，相对于在学习和识别过程中计算的一些运行规模参数。这个*CharHeight*评分直接乘以ANN分类器发出的评分，并在情况消歧中提供了显著帮助。
- en: We also employ a *GeoContext* module that scores adjacent characters, based
    on the classification hypotheses for those characters and on their relative size
    and placement. GeoContext scores each tentative character based on its class and
    the class of the immediately preceding letter (for the current search hypothesis).
    The character classes are used to look up expected character sizes and positions
    in a standardized space (baseline=0.0, topline=1.0). The ink being evaluated provides
    actual sizes and positions that can be compared directly to the expected values,
    subject only to a scale factor and offset, which are chosen so as to minimize
    the estimated error of fit between data and model. This same quadratic error term,
    computed from the inverse covariance matrix of a full multivariate Gaussian model
    of these sizes and positions, is used directly as GeoContext's score (or penalty,
    since it is applied in the -log probability space of the search engine). Figure
    13.13 illustrates the bounding boxes derived from the user's ink vs. the table-driven
    model, with the associated error measures for our GeoContext module.
  id: totrans-3648
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用一个*GeoContext*模块，根据相邻字符的分类假设及其相对大小和位置，对相邻字符进行评分。GeoContext 根据当前搜索假设中每个临时字符的类别及其前一个字母的类别来评分。字符类别用于在标准化空间中查找预期的字符大小和位置（基线=0.0，顶线=1.0）。被评估的墨水提供了实际大小和位置，可以直接与预期值进行比较，仅受缩放因子和偏移量的影响，这些因子和偏移量的选择旨在最小化数据与模型之间的拟合误差。这个同样的二次误差项是从这些大小和位置的全多变量高斯模型的逆协方差矩阵计算而来的，直接用作
    GeoContext 的得分（或惩罚，因为它在搜索引擎的 -log 概率空间中应用）。图 13.13 说明了用户墨水与基于表格的模型之间的边界框，以及我们
    GeoContext 模块的相关误差度量。
- en: '![283_image_0.png](283_image_0.png)'
  id: totrans-3649
  prefs: []
  type: TYPE_IMG
  zh: '![283_image_0.png](283_image_0.png)'
- en: Fig. 13.13. The eight measurements that contribute to the GeoContext error vector
    and corresponding score for each letter pair
  id: totrans-3650
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.13. 贡献于 GeoContext 误差向量的八个测量及每对字母的相应得分
- en: GeoContext's multivariate Gaussian model is learned directly from data. The
    problem in doing so was to find a good way to train per-character parameters of
    top, bottom, width, space, etc., in our standardized space, from data that had
    no labeled baselines, or other absolute referent points. Since we had a technique
    for generating an error vector from the table of parameters, we decided to use
    a back-propagation variant to train the table of parameters to minimize the squared
    error terms in the error vectors, given all the pairs of adjacent characters and
    correct class labels from the training set.
  id: totrans-3651
  prefs: []
  type: TYPE_NORMAL
  zh: GeoContext 的多变量高斯模型直接从数据中学习。这样做的问题是如何在没有标记基线或其他绝对参照点的数据中，找到一种良好的方法来训练每个字符的顶部、底部、宽度、间距等参数。在拥有从参数表中生成误差向量的技术后，我们决定使用反向传播变体来训练参数表，以最小化给定所有相邻字符对和训练集中的正确类别标签的误差向量中的平方误差项。
- en: GeoContext plays a major role in properly recognizing punctuation, in disambiguating
    case, and in recognition in general. A more extended discussion of GeoContext
    has been provided by Lyon and Yaeger [9].
  id: totrans-3652
  prefs: []
  type: TYPE_NORMAL
  zh: GeoContext 在正确识别标点符号、消歧义大小写及一般识别中发挥着重要作用。Lyon 和 Yaeger [9] 对 GeoContext 提供了更为详细的讨论。
- en: 13.5.3 Integration With Word Segmentation
  id: totrans-3653
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.5.3 与单词分割的整合
- en: Just as it is necessary to integrate character segmentation with recognition
    via the search process, so is it essential to integrate word segmentation with
    recognition and search, in order to obtain accurate estimates of word boundaries,
    and to reduce the large class of errors associated with missegmented words. To
    perform this integration, we first need a means of estimating the probability
    of a word break between each pair of tentative characters. We use a simple statistical
    model of gap sizes and stroke-centroid spacing to compute this probability (*spaceProb*).
    Gaussian density distributions, based on means and standard deviations computed
    from a large training corpus, together with a prior probability scale factor,
    provide the basis for the word-gap and stroke-gap (non-word-gap)
  id: totrans-3654
  prefs: []
  type: TYPE_NORMAL
  zh: 正如将字符分割与识别通过搜索过程整合在一起是必要的，整合单词分割与识别和搜索也是至关重要的，以获得准确的单词边界估计，并减少与错误分割单词相关的大量错误类。为了进行这种整合，我们首先需要一种方法来估计每对临时字符之间的单词断裂概率。我们使用一个简单的统计模型来计算这个概率（*spaceProb*），该模型基于从大量训练语料库中计算的均值和标准差的高斯密度分布，以及先验概率缩放因子，提供单词间隙和笔画间隙（非单词间隙）的基础。
- en: models, as illustrated in Figure 13.14. Since any given gap is, by definition,
    either a word gap or a non-word gap, the simple ratio defined in Figure 13.14
    provides a convenient, self-normalizing estimate of the word-gap probability.
    In practice, that equation further reduces to a simple sigmoid form, thus allowing
    us to take advantage of a lookup-table-based sigmoid derived for use in the ANN.
    In a thresholding, non-integrated word-segmentation model, word breaks would be
    introduced when spaceProb exceeds 0.5; i.e., when a particular gap is more likely
    to be a word-gap than a non-word-gap. For our integrated system, both word-break
    and non-word-break hypotheses are generated at each segment transition, and weighted
    by spaceProb and (1-spaceProb), respectively. The search process then proceeds
    over this larger hypothesis space to produce best estimates of whole phrases or
    sentences, thus integrating word segmentation as well as character segmentation.
  id: totrans-3655
  prefs: []
  type: TYPE_NORMAL
  zh: 模型，如图13.14所示。由于任何给定的间隙，根据定义要么是单词间隙，要么是非单词间隙，因此图13.14中定义的简单比率提供了单词间隙概率的方便自归一化估计。在实践中，该方程进一步简化为简单的sigmoid形式，从而使我们能够利用为ANN使用的查找表生成的sigmoid。在一个阈值化的非集成单词分割模型中，当spaceProb超过0.5时，会引入单词断裂；即，当特定间隙更可能是单词间隙而不是非单词间隙时。在我们的集成系统中，在每个段落转换处生成单词断裂和非单词断裂假设，并分别按spaceProb和(1-spaceProb)加权。搜索过程随后在这个更大的假设空间中进行，以生成整个短语或句子的最佳估计，从而实现单词分割和字符分割的集成。
- en: '![284_image_0.png](284_image_0.png)'
  id: totrans-3656
  prefs: []
  type: TYPE_IMG
  zh: '![284_image_0.png](284_image_0.png)'
- en: Fig. 13.14. Gaussian density distributions yield a simple statistical model
    of wordbreak probability, which is applied in the region between the peaks of
    the StrokeGap and WordGap distributions. Hashed areas indicate regions of clear
    cut decisions, where P*WordBreak* is set to either 0.0 or 1.0, to avoid problems
    dealing with tails of these simple distributions.
  id: totrans-3657
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.14. 高斯密度分布产生了单词断裂概率的简单统计模型，应用于StrokeGap和WordGap分布的峰值之间的区域。哈希区域指示清晰决策的区域，其中P*WordBreak*被设置为0.0或1.0，以避免处理这些简单分布尾部的问题。
- en: 13.6 Discussion
  id: totrans-3658
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.6 讨论
- en: The combination of elements described in the preceeding sections produces a
    powerful, integrated approach to character segmentation, word segmentation, and
    recognition. Users' experiences with APR are almost uniformly positive, unlike
    experiences with previous handwriting recognition systems. Writing within the
    dictionary is remarkably accurate, yet the ease with which people can write outside
    the dictionary has fooled many people into thinking that the Newton's
  id: totrans-3659
  prefs: []
  type: TYPE_NORMAL
  zh: 前面章节中描述的元素组合产生了一种强大的、综合的方法，用于字符分割、单词分割和识别。用户对APR的体验几乎都是积极的，这与以前的手写识别系统的体验形成鲜明对比。在字典内的书写非常准确，但人们在字典外书写的轻松程度使许多人误以为这是牛顿的。
- en: '"Print Recognizer" does not use dictionaries. As discussed previously, our
    recognizer certainly does use dictionaries. Indeed, the broad-coverage language
    model, though weakly applied, is essential for high accuracy recognition. Curiously,
    there seems to be little problem with dictionary *perplexity* - little difficulty
    as a result of using very large, very complex language models. We attribute this
    fortunate behavior to the excellent performance of the neural network character
    classifier at the heart of the system. One of the side benefits of the weak application
    of the language model is that even when recognition fails and produces the wrong
    result, the answer that is returned to the user is typically understandable by
    the user - perhaps involving substitution of a single character. Two useful phenomena
    ensue as a result. First, the user learns what works and what doesn''t, especially
    when she refers back to the ink that produced the misrecognition, so the system
    trains the user gracefully over time. Second, the meaning is not lost the way
    it can be, all too easily, with whole word substitutions - with that "Doonesbury
    Effect" found in first-generation, strong-language-model recognizers.'
  id: totrans-3660
  prefs: []
  type: TYPE_NORMAL
  zh: “打印识别器”不使用字典。如前所述，我们的识别器确实使用字典。实际上，广覆盖语言模型虽然应用较弱，但对高准确率识别至关重要。有趣的是，字典的*困惑度*似乎几乎没有问题
    - 使用非常大、非常复杂的语言模型几乎没有困难。我们将这种幸运的表现归因于系统核心的神经网络字符分类器的优秀性能。语言模型应用较弱的一个副作用是，即使在识别失败并产生错误结果时，返回给用户的答案通常是用户可以理解的
    - 可能只涉及一个字符的替换。由此产生两个有用的现象。首先，用户学习什么有效、什么无效，尤其是当她回顾导致误识别的墨水时，因此系统随着时间的推移优雅地训练用户。其次，意义并没有像整体单词替换时那样容易丢失
    - 这种现象在第一代强语言模型识别器中被称为“Doonesbury效应”。
- en: Though we have provided legitimate accuracy statistics for certain comparative
    tests of some of our algorithms, we have deliberately shied away from claiming
    specific levels of accuracy in general. Neat printers, who are familiar with the
    system, can achieve 100% accuracy if they are careful. Testing on data from complete
    novices, writing for the first time using a metal pen on a glass surface, without
    any feedback from the recognition system, and with ambiguous instructions about
    writing with "disconnected characters" (intended to mean printing, but often interpreted
    to mean writing with otherwise cursive characters but separated by large spaces
    in a wholely unnatural style), can yield word-level accuracies as low as 80%.
    Of course, the entire interesting range of recognition accuracies lies between
    these two extremes. Perhaps a slightly more meaningful statistic comes from common
    reports on usenet newsgroups, and some personal testing, that suggest accuracies
    of 97% to 98% in regular use. But for scientific purposes, none of these numbers
    have any real meaning, since our testing datasets are proprietary, and the only
    valid tests between different recognizers would have to be based on results obtained
    by processing the exact same bits, or by analyzing large numbers of experienced
    users of the systems in the field - a difficult project which has not been undertaken.
  id: totrans-3661
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们为某些算法的比较测试提供了合法的准确性统计，但我们故意避免在一般情况下声称具体的准确性水平。熟悉系统的熟练用户如果小心谨慎，可以实现100%的准确率。在完全新手的数据上进行测试，这些新手第一次使用金属笔在玻璃表面上书写，没有来自识别系统的反馈，且对于用“断开字符”书写的模糊指示（意图是指打印，但通常被解释为用连笔书写但之间留有大间隔，以一种完全不自然的风格书写），可能导致单词级别的准确率低至80%。当然，整个有趣的识别准确率范围介于这两个极端之间。也许稍微更有意义的统计数据来自于usenet新闻组的常见报告和一些个人测试，这些测试表明在正常使用中的准确率为97%到98%。但对于科学目的而言，这些数字并没有任何实际意义，因为我们的测试数据集是专有的，而不同识别器之间唯一有效的测试必须基于处理相同位数据所获得的结果，或通过分析在现场使用系统的大量经验用户进行
    - 这是一个尚未开展的艰巨项目。
- en: 'One of the key reasons for the success of APR is the suite of innovative neural
    network training techniques that help the network encode better class probabilities,
    especially for under-represented classes and writing styles. Many of these techniques
    - stroke count dithering, normalization of output error, frequency balancing,
    error emphasis - share a unifying theme: Reducing the effect of *a priori* biases
    in the training data on network learning significantly improves the network''s
    performance in an integrated recognition system, despite a modest reduction in
    the network''s accuracy for individual characters. Normalization of output error
    prevents over-represented non-target classes from biasing the net against under-represented
    target classes. Frequency balancing prevents over-represented classes from biasing
    the net against under-represented classes.'
  id: totrans-3662
  prefs: []
  type: TYPE_NORMAL
  zh: APR成功的关键原因之一是创新神经网络训练技术的套件，这些技术帮助网络更好地编码类概率，特别是针对欠代表类和写作风格。这些技术中的许多——笔画计数抖动、输出错误标准化、频率平衡、错误强调——都共享一个统一主题：减少训练数据中的*先验*偏见对网络学习的影响显著提高了网络在集成识别系统中的表现，尽管这对单个字符的网络准确性有适度的降低。输出错误的标准化防止过度代表的非目标类对欠代表目标类造成偏见。频率平衡防止过度代表的类对欠代表的类造成偏见。
- en: And stroke-count dithering and error emphasis prevent over-represented writing
    styles from biasing the net against under-represented writing styles. One could
    even argue that negative training eliminates an absolute bias towards properly
    segmented characters, and that stroke warping reduces the bias towards those writing
    styles found in the training data, although these techniques also provide wholly
    new information to the system.
  id: totrans-3663
  prefs: []
  type: TYPE_NORMAL
  zh: 笔画计数抖动和错误强调防止过度代表的写作风格对网络造成对欠代表写作风格的偏见。甚至可以说，负训练消除了对适当分段字符的绝对偏见，而笔画变形则减少了对训练数据中那些写作风格的偏见，尽管这些技术也为系统提供了全新的信息。
- en: Though we've offered arguments for why each of these techniques, individually,
    helps the overall recognition process, it is unclear why prior-bias reduction,
    in general, should be so consistently valuable. The general effect may be related
    to the technique of dividing out priors, as is sometimes done to convert from
    p(class|input) to p(input|*class*). But we also believe that forcing the net,
    during learning, to allocate resources to represent less frequent sample types
    may be directly beneficial. In any event, it is clear that paying attention to
    such biases and taking steps to modulate them is a vital component of effective
    training of a neural network serving as a classifier in a maximum-likelihood recognition
    system.
  id: totrans-3664
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已提供论据说明这些技术各自如何帮助整体识别过程，但尚不清楚为什么*先验*偏见减少在一般情况下如此一致地有价值。其一般效果可能与消除先验的技术有关，这在将p(class|input)转换为p(input|*class*)时有时会使用。但我们也相信，在学习过程中强迫网络分配资源以表示较少频繁的样本类型可能是直接有益的。无论如何，关注这些偏见并采取措施调节它们显然是作为最大似然识别系统分类器的神经网络有效训练的重要组成部分。
- en: The majority of this paper describes a sort of snapshot of the system and its
    architecture as it was deployed in its first commercial release, when it was,
    indeed, purely a "Print Recognizer". Letters had to be fully "disconnected"; i.e.,
    the pen had to be lifted between each pair of characters. The characters could
    overlap to some extent, but the ink could not be continuous. Connected characters
    proved to be the largest remaining class of errors for most of our users, since
    even a person who normally prints (as opposed to writing in cursive script)
  id: totrans-3665
  prefs: []
  type: TYPE_NORMAL
  zh: 本文大部分内容描述了系统及其架构的快照，特别是在其首次商业发布时，那时它确实只是一个“打印识别器”。字母必须完全“断开”；即在每对字符之间必须抬起笔。字符可以在一定程度上重叠，但墨水不能连续。连接字符被证明是我们大多数用户剩余的最大错误类别，因为即使是通常打印的人（而不是书写草体）也会遇到这个问题。
- en: may occasionally connect a pair of characters - the cross-bar of a "t" with
    the "h" in "the", the "o" and "n" in any word ending in "ion", and so on. To address
    this issue, we experimented with some fairly straightforward modifications to
    our recognizer, involving the *fragmenting* of user-strokes into multiple systemstrokes,
    or *fragments*. Once the ink representing the connected characters is broken up
    into fragments, we then allow our standard integrated segmentation and recognition
    process to stitch them back together into the most likely character and word hypotheses,
    as always. This technique has proven itself to work quite well, and the version
    of the "Print Recognizer" in the MessagePad 2000 and 2100 supports recognition
    of printing with connected characters. This capability was added without significant
    modification of the main recognition algorithms as presented in this paper. Due
    to certain assumptions and constraints in the current release of the software,
    APR is not yet a full cursive recognizer, though that is an obvious next direction
    to explore.
  id: totrans-3666
  prefs: []
  type: TYPE_NORMAL
  zh: 有时可能会连接一对字符——比如“t”的横杠与“the”中的“h”，“ion”结尾的任何单词中的“o”和“n”，等等。为了解决这个问题，我们对我们的识别器进行了相当简单的修改，涉及将用户笔画*分解*为多个系统笔画或*碎片*。一旦表示连接字符的墨水被分解成碎片，我们就允许标准的集成分段和识别过程将它们重新缝合成最可能的字符和单词假设，始终如此。该技术已被证明效果良好，而MessagePad
    2000和2100中的“打印识别器”版本支持对连接字符的打印进行识别。这项功能的增加并未对本论文中提出的主要识别算法进行重大修改。由于当前软件版本中的某些假设和限制，APR尚未成为完整的草书识别器，尽管这是显而易见的下一个研究方向。
- en: The net architecture discussed in section 13.4.2 and shown in Figure 13.4 also
    corresponds to the true printing-only recognizer. The final output layer has 95
    elements corresponding to the full printable ASCII character set plus the British
    Pound sign. Initially for the German market, and now even in English units, we
    have extended APR to handle diacritical marks and the special symbols needed for
    most European languages (although there is only very limited coverage of foreign
    languages in the dictionaries of English units). The main innovation that permitted
    this extended character set was an explicit handling of any compound character
    as a *base* plus an *accent*. This way only a few nodes needed to be added to
    the neural network output layer, representing just the bases and accents, rather
    than all combinations and permutations of same. And training data for all compound
    characters sharing a common base or a common accent contributed to the network's
    ability to learn that base or accent, as opposed to contributing only to the explicit
    base+accent combination. Here again, however, the fundamental recognizer technology
    has not changed significantly from that presented in this paper.
  id: totrans-3667
  prefs: []
  type: TYPE_NORMAL
  zh: 第13.4.2节讨论的网络架构以及图13.4中所示的网络也对应于真正的仅打印识别器。最终输出层有95个元素，分别对应于完整的可打印ASCII字符集及英镑符号。最初针对德国市场，现在甚至在英制单位中，我们已经扩展APR以处理变音符号和大多数欧洲语言所需的特殊符号（尽管在英制单位的字典中对外语的覆盖非常有限）。使这一扩展字符集得以实现的主要创新是对任何复合字符的显式处理，将其视为*基本*字符加上*重音*。这样，神经网络输出层只需添加少数节点，表示基本字符和重音，而不是所有可能的组合和排列。而所有共享共同基本字符或共同重音的复合字符的训练数据有助于网络学习该基本字符或重音，而不是仅仅贡献于显式的基本+重音组合。然而，这里根本的识别技术与本论文中提出的内容没有显著变化。
- en: 13.7 Future Extensions
  id: totrans-3668
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.7 未来扩展
- en: We are optimistic that our algorithms, having proven themselves to work essentially
    as well for connected characters as for disconnected characters, may extend gracefully
    to full cursive.
  id: totrans-3669
  prefs: []
  type: TYPE_NORMAL
  zh: 我们乐观地认为，我们的算法在处理连接字符和不连接字符时表现基本相同，因此可能会优雅地扩展到完整的草书书写。
- en: On a more speculative note, we believe that the technique may extend well to
    ideographic languages, substituting radicals for characters, and ideographic characters
    for words.
  id: totrans-3670
  prefs: []
  type: TYPE_NORMAL
  zh: 在更具推测性的层面上，我们相信该技术可能很好地扩展到表意文字，使用部首替代字符，使用表意字符替代单词。
- en: 'Finally, a note about learning and user adaptation: For a learning technology
    such as ANNs, user adaptation is an obvious and natural fit, and was planned as
    part of the system from its inception. However, due to RAM constraints in the
    initial shipping product, and the subsequent prioritization of European character
    sets and connected characters, we have not yet deployed a learning system. We
    have, however, done some testing of user adaptation, and believe it to be of considerable
    value. Figure 13.15 shows a comparison of the average performance on an old user-independent
    net trained on data from 45 writers, and the performance for three individuals
    using (A) the user-independent net,'
  id: totrans-3671
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，关于学习和用户适应的一点说明：对于像人工神经网络这样的学习技术，用户适应显然是一个自然的契合，并且从一开始就被纳入系统的设计中。然而，由于初始发布产品时内存的限制，以及后续对欧洲字符集和连接字符的优先考虑，我们尚未部署学习系统。不过，我们确实进行了用户适应的一些测试，并相信它具有相当的价值。图13.15显示了在一个针对45位作者训练的旧用户独立网络上的平均表现，与三位个体使用的（A）用户独立网络的表现的比较。
- en: '(B) a net trained on data exclusively from that individual, and (C) a copy
    of the user-independent net adapted to the specific user by some incremental training.
    (Note: These data are from a multi-year-old experiment, and are not necessarily
    representative of current levels of performance on any absolute scale.)'
  id: totrans-3672
  prefs: []
  type: TYPE_NORMAL
  zh: (B) 一个仅针对该个体数据训练的网络，以及 (C) 一个通过一些增量训练适应特定用户的用户独立网络的副本。（注意：这些数据来自多年前的实验，并不一定能代表当前任何绝对标准上的表现水平。）
- en: '![287_image_0.png](287_image_0.png)'
  id: totrans-3673
  prefs: []
  type: TYPE_IMG
  zh: '![287_image_0.png](287_image_0.png)'
- en: '![287_image_1.png](287_image_1.png)'
  id: totrans-3674
  prefs: []
  type: TYPE_IMG
  zh: '![287_image_1.png](287_image_1.png)'
- en: Fig. 13.15. User-adaptation test results for three individual writers with three
    different nets each, plus the overall results for 45 writers tested on a user-independent
    net trained on all 45 writers
  id: totrans-3675
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.15。三位不同作者在各自不同网络上的用户适应测试结果，以及在一个针对所有45位作者训练的用户独立网络上的45位作者的总体结果。
- en: An important distinction is being made here between "user-adapted" and
  id: totrans-3676
  prefs: []
  type: TYPE_NORMAL
  zh: 这里做了一个重要的区分，即“用户适应”和
- en: '"user-specific" nets. "User-specific" nets have been trained with a relatively
    large corpus of data exclusively from that specific user. "User-adapted" nets
    were based on the user-independent net, with some additional training using limited
    data from the user in question. All testing was performed with data held out from
    all training sets.'
  id: totrans-3677
  prefs: []
  type: TYPE_NORMAL
  zh: “用户特定”网络是基于来自特定用户的相对大量的数据训练而成的。“用户适应”网络则基于用户独立网络，使用有限的来自该用户的数据进行了额外训练。所有测试均是在所有训练集中保留的数据上进行的。
- en: One obvious thing to note is the reduction in error rate ranging from a factor
    of 2 to a factor of 5 that both user-specific and user-adapted nets provide.
  id: totrans-3678
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显而易见的事实是，无论是用户特定网络还是用户适应网络，错误率都减少了2倍到5倍。
- en: An equally important thing to note is that the user-adapted net performs essentially
    as well as a user-specific net - in fact, slightly *better* for two of the three
    writers. Given ANNs' penchant for local minima, we were concerned that this might
    not be the case. But it appears that the features learned during the user-independent
    net training served the user-adapted net well. We believe that a very small amount
    of training data from an individual will allow us to adapt the user-independent
    net to that user, and improve the overall accuracy for that user significantly,
    especially for individuals with more stylized writing, or whose writing style
    is underrepresented in our user-independent training corpus. And even for writers
    with common and/or neat writing styles, there is inherently less ambiguity in
    a single writer's style than in a corpus of data necessarily doing its best to
    represent essentially all possible writing styles.
  id: totrans-3679
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个同样重要的事情是，用户适应网络的表现基本上与用户特定网络一样——实际上，对于三位作者中的两位，表现稍微*更好*。考虑到人工神经网络对局部最小值的偏好，我们曾担心情况可能并非如此。但看起来，在用户独立网络训练期间学习到的特征为用户适应网络提供了良好的支持。我们相信，从个体获取的非常少量的训练数据将使我们能够将用户独立网络适应于该用户，并显著提高该用户的整体准确性，尤其是对于那些书写风格更具个性化或在我们的用户独立训练语料库中表现不足的个体。即使是对于书写风格普通和/或整洁的作者，单个作者的风格本质上也比一个尽力代表所有可能书写风格的数据语料库要少一些模糊性。
- en: These results may be exaggerated somewhat by the limited data in the userindependent
    training corpus at the time these tests were performed (just 45 writers), and
    at least two of the three writers in question had particularly problematic writing
    styles. We have also made significant advances in our user-independent recognition
    accuracies since these tests were performed. Nonetheless, we believe these results
    are suggestive of the significant value of user adaptation, even in preference
    to a user-specific solution. Acknowledgements. This work was done in collaboration
    with Bill Stafford, Apple Computer, and Les Vogel, Angel Island Technologies.
    We would also like to acknowledge the contributions of Michael Kaplan, Rus Maxham,
    Kara Hayes, Gene Ciccarelli, and Stuart Crawford. We also received support and
    assistance from the Newton Recognition, Software Testing, and Hardware Support
    groups. We are also indebted to our many colleagues in the connectionist community
    for their advice, help, and encouragement over the years, as well as our many
    colleagues at Apple who pitched in to help throughout the life of this project.
  id: totrans-3680
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果可能由于在进行测试时用户独立训练语料库的数据有限（仅有45位写作者）而被夸大，且至少三位写作者中的两位有特别问题的写作风格。自这些测试进行以来，我们在用户独立识别准确性方面也取得了显著进展。尽管如此，我们相信这些结果暗示了用户适应的重要价值，即使优先考虑用户特定的解决方案。致谢。本工作与比尔·斯塔福德（Apple
    Computer）和莱斯·沃戈尔（Angel Island Technologies）合作完成。我们还要感谢迈克尔·卡普兰、拉斯·马克汉、卡拉·海耶斯、基恩·西卡雷利和斯图尔特·克劳福德的贡献。我们还得到了牛顿识别、软件测试和硬件支持团队的支持与帮助。多年来，我们也要感谢连接主义社区中的许多同事的建议、帮助和鼓励，以及苹果公司许多同事在项目整个生命周期中的支持。
- en: Some of the techniques described in this paper are the subject of pending U.S.
  id: totrans-3681
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中描述的一些技术正是美国待审的主题。
- en: and foreign patent applications.
  id: totrans-3682
  prefs: []
  type: TYPE_NORMAL
  zh: 以及外国专利申请。
- en: '[1] Asanovic, K., Morgan, N.: Experimental determination of precision requirements
    for back-propagation training of artificial neural networks. Technical Report
    TR91-036, International Computer Science Institute, Berkeley, CA (June 1991)'
  id: totrans-3683
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Asanovic, K., Morgan, N.: 人工神经网络反向传播训练精度要求的实验性确定。技术报告 TR91-036，加州伯克利国际计算机科学研究所（1991年6月）'
- en: '[2] Bengio, Y., LeCun, Y., Nohl, C., Burges, C.: LeRec: A NN/HMM hybrid for
    online handwriting recognition. Neural Computation 7(6), 1289–1303 (1995)'
  id: totrans-3684
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Bengio, Y., LeCun, Y., Nohl, C., Burges, C.: LeRec：一种用于在线手写识别的NN/HMM混合。神经计算
    7(6)，1289–1303（1995年）'
- en: '[3] Bourlard, H., Wellekens, C.J.: Links between markov models and multilayer
    perceptrons. IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI12(12),
    1167–1178 (1990)'
  id: totrans-3685
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Bourlard, H., Wellekens, C.J.: 马尔可夫模型与多层感知器之间的联系。IEEE模式分析与机器智能交易 PAMI12(12)，1167–1178（1990年）'
- en: '[4] Chang, E.I., Lippmann, R.P.: Using voice transformations to create additional
    training talkers for word spotting. In: Tesauro, G., Touretzky, D., Leen, T. (eds.)'
  id: totrans-3686
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Chang, E.I., Lippmann, R.P.: 使用语音变换创造额外的训练说话者以进行词检测。收录于：Tesauro, G., Touretzky,
    D., Leen, T.（编）'
- en: Advances in Neural Information Processing Systems, vol. 7, pp. 875–882. The
    MIT Press (1995)
  id: totrans-3687
  prefs: []
  type: TYPE_NORMAL
  zh: 神经信息处理系统的进展，第7卷，pp. 875–882。麻省理工学院出版社（1995年）
- en: '[5] Gish, H.: A probabilistic approach to the understanding and training of
    neural network classifiers. In: Proceedings of the IEEE Conference on Acoustics,
    Speech and Signal Processing, pp. 1361–1364. IEEE Press (1990)'
  id: totrans-3688
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Gish, H.: 理解和训练神经网络分类器的概率方法。收录于：IEEE声学、语音与信号处理会议论文集，pp. 1361–1364。IEEE出版社（1990年）'
- en: '[6] Guyon, I., Henderson, D., Albrecht, P., LeCun, Y., Denker, P.: Writer independent
    and writer adaptive neural network for on-line character recognition. In: Impedovo,
    S. (ed.) From Pixels to Features III, pp. 493–506. Elsevier, Amsterdam (1992)'
  id: totrans-3689
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Guyon, I., Henderson, D., Albrecht, P., LeCun, Y., Denker, P.: 用于在线字符识别的写作者独立和适应性神经网络。收录于：Impedovo,
    S.（编），从像素到特征 III，pp. 493–506。爱思唯尔，阿姆斯特丹（1992年）'
- en: '[7] Jacobs, R.A., Jordan, M.I., Nowlan, S.J., Hinton, G.E.: Adaptive mixtures
    of local experts. Neural Computation 3(1), 79–87 (1991)'
  id: totrans-3690
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Jacobs, R.A., Jordan, M.I., Nowlan, S.J., Hinton, G.E.: 自适应局部专家混合。神经计算
    3(1)，79–87（1991年）'
- en: '[8] Lippmann, R.P.: Neural networks, bayesian a posteriori probabilities, and
    pattern classification. In: Cherkassky, V., Friedman, J.H., Wechsler, H. (eds.)
    From Statistics to Neural Networks - Theory and Pattern Recognition Applications,
    pp. 83–104. Springer, Berlin (1994)'
  id: totrans-3691
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Lippmann, R.P.: 神经网络、贝叶斯后验概率和模式分类。收录于：Cherkassky, V., Friedman, J.H., Wechsler,
    H.（编），从统计学到神经网络——理论与模式识别应用，pp. 83–104。施普林格出版社，柏林（1994年）'
- en: '[9] Lyon, R., Yaeger, L.: On-line hand-printing recognition with neural networks.
    In:'
  id: totrans-3692
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Lyon, R., Yaeger, L.: 使用神经网络的在线手写识别。在：'
- en: Fifth International Conference on Microelectronics for Neural Networks and Fuzzy
    Systems, Lausanne, Switzerland. IEEE Computer Society Press (1996)
  id: totrans-3693
  prefs: []
  type: TYPE_NORMAL
  zh: 第五届国际微电子神经网络与模糊系统会议，瑞士洛桑。IEEE计算机学会出版社 (1996)
- en: '[10] Morgan, N., Bourlard, H.: Continuous speech recognition - an introduction
    to the hybrid HMM/connectionist approach. IEEE Signal Processing Mag 13(3), 24–42
    (1995)'
  id: totrans-3694
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Morgan, N., Bourlard, H.: 连续语音识别——混合HMM/连接主义方法的介绍。IEEE信号处理杂志 13(3), 24–42
    (1995)'
- en: '[11] Renals, S., Morgan, N., Cohen, M., Franco, H.: Connectionist probability
    estimation in the Decipher speech recognition system. In: Proceedings of the IEEE'
  id: totrans-3695
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Renals, S., Morgan, N., Cohen, M., Franco, H.: 在解码语音识别系统中的连接主义概率估计。在：IEEE会议论文集'
- en: International Conference on Acoustics, Speech, and Signal Processing, vol. I,
    pp. 601–604 (1992)
  id: totrans-3696
  prefs: []
  type: TYPE_NORMAL
  zh: 国际声学、语音与信号处理会议，卷 I，第 601–604 页 (1992)
- en: '[12] Richard, M.D., Lippmann, R.P.: Neural Network Classifiers Estimate Bayesian
    a posteriori probabilities. Neural Computation 3(4), 461–483 (1991)'
  id: totrans-3697
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Richard, M.D., Lippmann, R.P.: 神经网络分类器估计贝叶斯后验概率。神经计算 3(4), 461–483 (1991)'
- en: '[13] Simard, P., LeCun, Y., Denker, J.: Efficient pattern recognition using
    a new transformation distance. In: Hanson, S.J., Cowan, J.D., Giles, C.L. (eds.)
    Proceedings of the 1992 Conference on Advances in Neural Information Processing
    Systems, vol. 5, pp. 50–58. Morgan Kaufmann, San Mateo (1992)'
  id: totrans-3698
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Simard, P., LeCun, Y., Denker, J.: 使用新变换距离的高效模式识别。在：Hanson, S.J., Cowan,
    J.D., Giles, C.L. (编辑)《1992年神经信息处理系统进展会议论文集》，第 5 卷，第 50–58 页。摩根考夫曼，旧金山 (1992)'
- en: '[14] Simard, P., Victorri, B., LeCun, Y., Denker, J.: Tangent prop—A formalism
    for specifying selected invariances in an adaptive network. In: Moody, J.E., Hanson,
    S.J., Lippmann, R.P. (eds.) Advances in Neural Information Processing Systems,
    vol. 4, pp. 895–903. Morgan Kaufmann Publishers, Inc. (1992)'
  id: totrans-3699
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Simard, P., Victorri, B., LeCun, Y., Denker, J.: 切线传播——一种在自适应网络中指定选择不变性的形式。收录于：Moody,
    J.E., Hanson, S.J., Lippmann, R.P. (编辑)《神经信息处理系统进展》，第 4 卷，第 895–903 页。摩根考夫曼出版社
    (1992)'
- en: '[15] Tappert, C.C., Suen, C.Y., Wakahara, T.: The state of the art in on-line
    handwriting recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence
    PAMI-12(8), 787–808 (1990)'
  id: totrans-3700
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Tappert, C.C., Suen, C.Y., Wakahara, T.: 在线手写识别的最新进展。IEEE模式分析与机器智能交易 PAMI-12(8),
    787–808 (1990)'
- en: 14 Neural Network Classification And Prior Class Probabilities-
  id: totrans-3701
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14 神经网络分类和先验类概率-
- en: Steve Lawrence1, Ian Burns2, Andrew Back3, Ah Chung Tsoi4, and C. Lee Giles1,
  id: totrans-3702
  prefs: []
  type: TYPE_NORMAL
  zh: Steve Lawrence1, Ian Burns2, Andrew Back3, Ah Chung Tsoi4, 和 C. Lee Giles1，
- en: 1 NEC Research Institute, 4 Independence Way, Princeton, NJ 08540 http://www.neci.nj.nec.com
    2 Open Access Pty Ltd, Level 2, 7–9 Albany St, St. Leonards, NSW 2065, Australia
    3 RIKEN Brain Science Institute, 2-1 Hirosawa, Wako-shi, Saitama, 351-0198, Japan
    4 Faculty of Informatics, University of Wollongong, Northfields Ave, Wollongong,
    NSW 2522, Australia
  id: totrans-3703
  prefs: []
  type: TYPE_NORMAL
  zh: 1 NEC研究所，4 Independence Way, Princeton, NJ 08540 http://www.neci.nj.nec.com
    2 Open Access Pty Ltd, Level 2, 7–9 Albany St, St. Leonards, NSW 2065, Australia
    3 RIKEN脑科学研究所，2-1 Hirosawa, Wako-shi, Saitama, 351-0198, Japan 4 悉尼大学信息学院，Northfields
    Ave, Wollongong, NSW 2522, Australia
- en: '{lawrence,giles}@research.nj.nec.com, ian.burns@oa.com.au, back@brain.riken.go.jp,
    Ah_Chung_Tsoi@uow.edu.au http://www.neci.nj.nec.com/homepages/lawrence/'
  id: totrans-3704
  prefs: []
  type: TYPE_NORMAL
  zh: '{lawrence,giles}@research.nj.nec.com, ian.burns@oa.com.au, back@brain.riken.go.jp,
    Ah_Chung_Tsoi@uow.edu.au http://www.neci.nj.nec.com/homepages/lawrence/'
- en: Abstract. A commonly encountered problem in MLP (multi-layer perceptron) classification
    problems is related to the prior probabilities of the individual classes - if
    the number of training examples that correspond to each class varies significantly
    between the classes, then it may be harder for the network to learn the rarer
    classes in some cases. Such practical experience does not match theoretical results
    which show that MLPs approximate Bayesian *a posteriori* probabilities (independent
    of the prior class probabilities). Our investigation of the problem shows that
    the difference between the theoretical and practical results lies with the assumptions
    made in the theory (accurate estimation of Bayesian a posteriori probabilities
    requires the network to be large enough, training to converge to a global minimum,
    infinite training data, and the *a priori* class probabilities of the test set
    to be correctly represented in the training set). Specifically, the problem can
    often be traced to the fact that efficient MLP training mechanisms lead to sub-optimal
    solutions for most practical problems. In this chapter, we demonstrate the problem,
    discuss possible methods for alleviating it, and introduce new heuristics which
    are shown to perform well on a sample ECG classification problem. The heuristics
    may also be used as a simple means of adjusting for unequal misclassification
    costs.
  id: totrans-3705
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要。在多层感知器（MLP）分类问题中，常见的问题与各个类别的先验概率有关——如果对应每个类别的训练样本数量在类别之间显著不同，那么网络在某些情况下可能更难学习稀有类别。这种实际经验与理论结果不符，后者显示MLPs可以逼近贝叶斯*a
    posteriori*概率（与先验类别概率无关）。我们对该问题的研究表明，理论与实际结果之间的差异在于理论中做出的假设（准确估计贝叶斯*a posteriori*概率需要网络足够大，训练收敛到全局最小值，无限的训练数据，以及测试集的*a
    priori*类概率在训练集中得到正确表示）。具体来说，问题往往可以追溯到高效的MLP训练机制导致大多数实际问题的次优解。在本章中，我们展示了这个问题，讨论了缓解方法，并介绍了在样本ECG分类问题上表现良好的新启发式方法。这些启发式方法也可以作为调整不等误分类成本的简单手段。
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-3706
  prefs: []
  type: TYPE_NORMAL
  zh: '- 之前出版于：Orr, G.B. 和 Müller, K.-R. (编)：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998). - Lee Giles is also with the Institute for Advanced
    Computer Studies, University of Maryland, College Park, MD 20742.
  id: totrans-3707
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0 (1998)。 - Lee Giles也在马里兰大学学院公园的高级计算机研究所工作，邮政编码20742。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    295–309, 2012.'
  id: totrans-3708
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon等（编）：NN: Tricks of the Trade，第二版，LNCS 7700，第295–309页，2012年。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-3709
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: 14.1 Introduction
  id: totrans-3710
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.1 引言
- en: 'It has been shown theoretically that MLPs approximate Bayesian *a posteriori*
    probabilities when the desired network outputs are *1 of M* and squared-error
    or cross-entropy cost functions are used [6, 11, 12, 15, 23, 25, 26, 28, 29, 32].
    This result relies on a number of assumptions for accurate estimation: the network
    must be large enough and training must find a global minimum, infinite training
    data is required, and the *a priori* class probabilities of the test set must
    be correctly represented in the training set.'
  id: totrans-3711
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上已经证明，当所需网络输出为*1 of M*且使用平方误差或交叉熵成本函数时，MLPs可以逼近贝叶斯*a posteriori*概率[6, 11,
    12, 15, 23, 25, 26, 28, 29, 32]。这一结果依赖于准确估计所需的多个假设：网络必须足够大，训练必须找到全局最小值，需要无限的训练数据，测试集的*a
    priori*类概率必须在训练集中得到正确表示。
- en: In practice, MLPs have also been shown to accurately estimate Bayesian a posteriori
    probabilities for certain experiments [10]. However, a commonly encountered problem
    in MLP classification is related to the case when the frequency of the classes
    in the training set varies significantly1. If the number of training examples
    for each class varies significantly between classes then there may be a bias towards
    predicting the more common classes [3, 4], leading to worse classification performance
    for the rarer classes. In [5] it was observed that classes with low *a priori*
    probability in a speech application were "ignored" (no samples were classified
    as these classes after training). Such problems indicate that either the estimation
    of Bayesian *a posteriori* probabilities is inaccurate, or that such estimation
    may not be desired (e.g. due to varying misclassification costs (this is explained
    further in section 14.4)). Bourlard and Morgan [7] have demonstrated inaccurate
    estimation of Bayesian *a posteriori* probabilities in speech recognition. This
    chapter discusses how the problem may occur along with methods of dealing with
    the problem.
  id: totrans-3712
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，MLP在某些实验中也被证明能够准确估计贝叶斯后验概率。然而，MLP分类中常见的问题与训练集中类别频率显著变化的情况有关。如果每个类别的训练样本数量在类别之间差异显著，则可能会偏向于预测更常见的类别，从而导致较少类别的分类性能更差。在一项研究中观察到，在语音应用中，具有低*先验*概率的类别被“忽略”（训练后没有样本被分类为这些类别）。这些问题表明，贝叶斯*后验*概率的估计要么不准确，要么不需要这种估计（例如，由于误分类成本的变化，这在14.4节中进一步解释）。Bourlard和Morgan已经证明了在语音识别中贝叶斯*后验*概率的估计不准确。本章讨论了问题可能发生的方式以及解决该问题的方法。
- en: 14.2 The Trick
  id: totrans-3713
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.2 诀窍
- en: This section describes the tricks for alleviating the aforementioned problem.
    Motivation for their use and experimental results are provided in the following
    sections. The methods all consider some kind of scaling which is performed on
    a class by class basis2.
  id: totrans-3714
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了缓解上述问题的技巧。它们使用的动机和实验结果将在接下来的章节中提供。所有方法都考虑某种按类别缩放的方式。
- en: 14.2.1 Prior Scaling
  id: totrans-3715
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.2.1 先验缩放
- en: 'A method of scaling weight updates on a class by class basis according to the
    prior class probabilities is proposed in this section. Consider gradient descent
    weight updates for each pattern: wlki(new) = wlki(old) + Δwlki(p) where Δwlki(p)
    ='
  id: totrans-3716
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提出了一种根据先验类别概率按类别缩放权重更新的方法。考虑每个模式的梯度下降权重更新：wlki(new) = wlki(old) + Δwlki(p)，其中Δwlki(p)
    =
- en: −η
  id: totrans-3717
  prefs: []
  type: TYPE_NORMAL
  zh: −η
- en: ∂E(p)
  id: totrans-3718
  prefs: []
  type: TYPE_NORMAL
  zh: ∂E(p)
- en: ∂wlki
  id: totrans-3719
  prefs: []
  type: TYPE_NORMAL
  zh: ∂wlki
- en: ', p is the pattern index, and wki is the weight between neuron k in layer l
    and neuron i in layer l − 1. Scaling the weight updates on a pattern by pattern'
  id: totrans-3720
  prefs: []
  type: TYPE_NORMAL
  zh: ', p是模式索引，wki是层l中神经元k与层l − 1中神经元i之间的权重。按模式进行权重更新的缩放'
- en: 1 For the data in general. Others have considered the case of different class
    probabilities between the training and test sets, e.g. [23]. 2 Anand et al. [2]
    have also presented an algorithm related to unequal prior class probabilities.
    However, their algorithm aims only to improve convergence speed.
  id: totrans-3721
  prefs: []
  type: TYPE_NORMAL
  zh: 1 一般数据。其他人考虑了训练集和测试集之间不同类别概率的情况，例如[23]。2 Anand等人还提出了一种与不平等先验类别概率相关的算法。然而，他们的算法仅旨在提高收敛速度。
- en: Additionally, their algorithm is only for two class problems and batch update.
  id: totrans-3722
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，他们的算法仅适用于二类问题和批量更新。
- en: 'basis is considered such that the total expected update for patterns belonging
    to each class is equal (i.e. independent of the number of patterns in the class):'
  id: totrans-3723
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这样的基础，以使属于每个类别的模式的总期望更新相等（即与类别中的模式数量无关）：
- en: $$\left\langle\sum_{p=1}^{N_{p}}|s_{x}\Delta w_{k i}^{l}(p)|_{p_{c}=x}\right\rangle=c_{1},\,\forall
    x\in X$$
  id: totrans-3724
  prefs: []
  type: TYPE_NORMAL
  zh: $$\left\langle\sum_{p=1}^{N_{p}}|s_{x}\Delta w_{k i}^{l}(p)|_{p_{c}=x}\right\rangle=c_{1},\,\forall
    x\in X$$
- en: $$(14.1)$$
  id: totrans-3725
  prefs: []
  type: TYPE_NORMAL
  zh: $$(14.1)$$
- en: 'where pc is the target classification of pattern p, c1 is a constant, sx is
    a scaling factor, x ranges over all classes X, <> denotes expectation, and the
    pc = x subscript indicates that the sum is only over the patterns in a particular
    class x. This effectively scales the updates for lower frequency classes so that
    they are higher - the aim is to account for the fact that lower frequency classes
    tend to be "ignored" in certain situations. We assume that the expected weight
    update for individual patterns in each class is equal:'
  id: totrans-3726
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 pc 是模式 p 的目标分类，c1 是常数，sx 是缩放因子，x 遍历所有类别 X，<> 表示期望，pc = x 下标表示求和仅限于特定类 x 中的模式。这有效地缩放了较低频率类别的更新，以便它们更高——目的是考虑到较低频率类别在某些情况下往往会被“忽视”。我们假设每个类别中单个模式的期望权重更新是相等的：
- en: $$\left\langle|\Delta w_{k i}^{l}(p)|_{p_{c}=x}\right\rangle=c_{2},\,\forall
    x\in X$$
  id: totrans-3727
  prefs: []
  type: TYPE_NORMAL
  zh: $$\left\langle|\Delta w_{k i}^{l}(p)|_{p_{c}=x}\right\rangle=c_{2},\,\forall
    x\in X$$
- en: .= c2, ∀x ∈ X (14.2)
  id: totrans-3728
  prefs: []
  type: TYPE_NORMAL
  zh: .= c2, ∀x ∈ X (14.2)
- en: 'where c2 is a constant not related to c1. The scaling factor required is therefore:'
  id: totrans-3729
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 c2 是与 c1 无关的常数。因此，所需的缩放因子为：
- en: $$(14.2)$$
  id: totrans-3730
  prefs: []
  type: TYPE_NORMAL
  zh: $$(14.2)$$
- en: $$s_{x}={\frac{1}{p_{x}N_{c}}}$$
  id: totrans-3731
  prefs: []
  type: TYPE_NORMAL
  zh: $$s_{x}={\frac{1}{p_{x}N_{c}}}$$
- en: $$(14.3)$$
  id: totrans-3732
  prefs: []
  type: TYPE_NORMAL
  zh: $$(14.3)$$
- en: where sx is the scaling factor for all weight updates associated with a pattern
    belonging to class x, Nc is the number of classes, and px is the prior probability
    of class x.
  id: totrans-3733
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 sx 是与属于类 x 的模式相关的所有权重更新的缩放因子，Nc 是类别的数量，px 是类 x 的先验概率。
- en: Scaling as defined above invalidates the Bayesian *a posteriori* probability
    proofs
  id: totrans-3734
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所定义的缩放使得贝叶斯 *后验* 概率证明失效。
- en: '(for example, scaling a class by two can be compared with duplicating every
    pattern in the data for that class - causing changes in probability distributions),
    i.e. there is no reason to expect that the scaling strategy will be optimal. This,
    and the empirical result that the scaling may improve performance, leads to the
    hypothesis that there may be a point between no prior scaling and prior scaling
    as defined above which produces performance better than either of the two extremes.
    The following scaling rule can be used to select a degree of scaling between the
    two extremes:'
  id: totrans-3735
  prefs: []
  type: TYPE_NORMAL
  zh: （例如，缩放一个类别为两倍可以与在该类别的数据中复制每个模式进行比较——导致概率分布的变化），即没有理由期待缩放策略是最优的。这一点，加上缩放可能提高性能的经验结果，导致假设在没有先验缩放和上述定义的先验缩放之间可能存在一个点，其性能优于两个极端。以下缩放规则可用于在两个极端之间选择缩放程度：
- en: $$s_{x}^{\prime}=1-c_{s}+{\frac{c_{s}}{p_{x}N_{c}}}$$
  id: totrans-3736
  prefs: []
  type: TYPE_NORMAL
  zh: $$s_{x}^{\prime}=1-c_{s}+{\frac{c_{s}}{p_{x}N_{c}}}$$
- en: $$(14.4)$$
  id: totrans-3737
  prefs: []
  type: TYPE_NORMAL
  zh: $$(14.4)$$
- en: where 0 ≤ cs ≤ 1 is a constant specifying the amount of prior scaling to use.
    cs = 0 corresponds to no scaling according to prior probabilities, and cs = 1
    corresponds to scaling as above. Prior scaling in this form can be expressed as
    training with the following alternative cost function3
  id: totrans-3738
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 0 ≤ cs ≤ 1 是一个常数，指定要使用的先验缩放量。cs = 0 表示不根据先验概率进行缩放，cs = 1 表示如上所述进行缩放。这种形式的先验缩放可以用以下替代成本函数进行训练表示3。
- en: 3 A cost function with similar motivation, the "classification figure-of-merit"
    (CFM) proposed by Hampshire and Waibel [13], has been suggested as a possible
    improvement when prior class probabilities vary [3]. In [13], the CFM cost function
    leads to networks whichmake different errors to those trained with theMSE criterion,
    and can therefore be useful forimproving performance by combining classifiers
    trained with the CFM and the MSE. However, networks trained with the CFM criterion
    do not result in higher classification performance than networks trained with
    the MSE criterion for the experiments reported in [13].
  id: totrans-3739
  prefs: []
  type: TYPE_NORMAL
  zh: 3 一种具有类似动机的成本函数，哈姆郊和怀贝尔提出的“分类质量指标”（CFM）[13]，在先验类概率变化时被建议作为一种可能的改进[3]。在[13]中，CFM
    成本函数导致的网络与用MSE标准训练的网络产生不同的错误，因此可以通过结合用CFM和MSE训练的分类器来提高性能。然而，在[13]报告的实验中，用CFM标准训练的网络并未比用MSE标准训练的网络产生更高的分类性能。
- en: Definition 1
  id: totrans-3740
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 1
- en: $$E={\frac{1}{2}}\sum_{k=1}^{N_{p}}\sum_{j=1}^{N_{c}}s_{x}^{\prime}(d_{k j}-y_{k
    j})^{2}$$
  id: totrans-3741
  prefs: []
  type: TYPE_NORMAL
  zh: $$E={\frac{1}{2}}\sum_{k=1}^{N_{p}}\sum_{j=1}^{N_{c}}s_{x}^{\prime}(d_{k j}-y_{k
    j})^{2}$$
- en: $$(14.5)$$
  id: totrans-3742
  prefs: []
  type: TYPE_NORMAL
  zh: $$(14.5)$$
- en: 2 (14.5)
  id: totrans-3743
  prefs: []
  type: TYPE_NORMAL
  zh: 2 (14.5)
- en: where the network has one output for each of the Nc classes, Np is the number
    of patterns, d is the desired or target output, y *is the predicted output, and*
    x is the class of pattern k.
  id: totrans-3744
  prefs: []
  type: TYPE_NORMAL
  zh: 其中网络为每个 Nc 类具有一个输出，Np 是模式的数量，d 是期望或目标输出，y *是预测输出，* x 是模式 k 的类别。
- en: When using prior scaling as defined in this section, the individual sx values
    can be large for classes with low prior probability. This may lead to the requirement
    of decreasing the learning rate in order to prevent the relatively large weight
    updates interfering with the gradient descent process. Comparing the use of prior
    scaling and not using prior scaling then becomes problematic because the optimal
    learning rate is different for each case. An alternative is to normalize the sx
    values so that the maximum is 1. Another possibility is to present patterns repeatedly
    to the network instead of scaling weight updates, i.e. for a class with a scaling
    factor of 2 each pattern would be presented twice. This would have the advantage
    of reducing the range of weight updates in terms of magnitude, e.g. an update
    of magnitude x might be repeated twice rather than using a single update of magnitude
    2x. This may allow the use of a higher learning rate, and therefore reduce the
    number of epochs required. However, a disadvantage of repeating patterns is that
    the effective training set would be larger, resulting in longer training times
    for the same number of epochs. Such a technique could be done probabilistically,
    and this is the subject of the next technique.
  id: totrans-3745
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用本节定义的先前缩放时，低先前概率类别的个体 sx 值可能很大。这可能导致需要降低学习率，以防止相对较大的权重更新干扰梯度下降过程。比较使用先前缩放和不使用先前缩放就变得复杂，因为每种情况的最优学习率不同。另一种选择是对
    sx 值进行归一化，使最大值为 1。另一种可能性是重复呈现模式给网络，而不是缩放权重更新，即对于缩放因子为 2 的类别，每个模式将被呈现两次。这将有助于减少权重更新在幅度上的范围，例如，幅度为
    x 的更新可能重复两次，而不是使用幅度为 2x 的单次更新。这可能允许使用更高的学习率，从而减少所需的训练周期数量。然而，重复模式的一个缺点是有效训练集会更大，从而导致在相同训练周期下的训练时间更长。这种技术可以以概率方式进行，这也是下一个技术的主题。
- en: 14.2.2 Probabilistic Sampling
  id: totrans-3746
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.2.2 概率采样
- en: Yaeger et al. [33] (chapter 13) have proposed a method called *frequency balancing*
    which is similar to the prior scaling method above. In frequency balancing, Yaeger
    et al. use all training samples in random order for each training epoch and allow
    each sample to be presented to the network a random number of times, which may
    be zero or more and is computed probabilistically. A balancing factor is included,
    which is analogous to the scaling factor above (cs).
  id: totrans-3747
  prefs: []
  type: TYPE_NORMAL
  zh: Yaeger 等人 [33]（第 13 章）提出了一种称为*频率平衡*的方法，类似于上述的先前缩放方法。在频率平衡中，Yaeger 等人以随机顺序使用所有训练样本进行每个训练周期，并允许每个样本以随机次数呈现给网络，这个次数可以是零次或多次，且是概率计算得出的。包含一个平衡因子，类似于上述的缩放因子
    (cs)。
- en: 'We introduce a very similar method here called *probabilistic sampling* whereby
    training patterns are chosen randomly in the following manner: the class is chosen
    randomly with the probability of choosing each class x, being (1 − cs)px + csNc
    .'
  id: totrans-3748
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里引入一种非常相似的方法，称为*概率采样*，其方式如下：类别是随机选择的，每个类别 x 的选择概率为 (1 − cs)px + csNc。
- en: A training sample is then chosen randomly from among all training samples for
    the chosen class.
  id: totrans-3749
  prefs: []
  type: TYPE_NORMAL
  zh: 然后从所选类别的所有训练样本中随机选择一个训练样本。
- en: 14.2.3 Post Scaling
  id: totrans-3750
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.2.3 后缩放
- en: Instead of scaling weight updates or altering the effective class frequencies,
    it is possible to train the network as usual and then scale the outputs of the
    network after training. For example, the network could be trained as usual and
    then the outputs scaled according to the prior probabilities in a similar fashion
    to the prior scaling method (using equation 14.3 or 14.4). Experiments with this
    technique alone show that it is not always as successful as prior scaling of the
    weight updates. This may be because the estimation of the lower frequency classes
    can be less accurate than that of the higher frequency classes [24] (the deviations
    of the network outputs from the true values in regions with a higher number of
    data points influence the squared error cost function more than the deviations
    in regions with a lower number of points [23]).
  id: totrans-3751
  prefs: []
  type: TYPE_NORMAL
  zh: 不必缩放权重更新或改变有效类别频率，可以像往常一样训练网络，然后在训练后缩放网络的输出。例如，网络可以像往常一样训练，然后根据先前概率以类似于先前缩放方法的方式缩放输出（使用方程
    14.3 或 14.4）。单独使用这种技术的实验表明，它并不总是像权重更新的先前缩放那样成功。这可能是因为低频类别的估计不如高频类别的估计准确 [24]（网络输出与真实值之间的偏差在数据点较多的区域对平方误差成本函数的影响大于在数据点较少的区域的偏差
    [23]）。
- en: The post scaling technique introduced here can also be used to optimize a given
    criterion, e.g. the outputs may be scaled so that the probability of predicting
    each class matches the prior probabilities in the training set as closely as possible.
    Post scaling to minimize a different criterion is demonstrated in the results
    section. For the results in this chapter, the minimization is performed using
    a simple hill-climbing algorithm which adjusts a scaling factor associated with
    each of the outputs of the network.
  id: totrans-3752
  prefs: []
  type: TYPE_NORMAL
  zh: 这里介绍的后处理缩放技术也可以用来优化给定的标准，例如，输出可以缩放，以使每个类别的预测概率尽可能接近训练集中的先验概率。结果部分展示了最小化不同标准的后处理缩放。对于本章中的结果，最小化使用简单的爬山算法进行，该算法调整与网络每个输出相关的缩放因子。
- en: 14.2.4 Equalizing Class Membership
  id: totrans-3753
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.2.4 类别成员资格平衡
- en: A simple method for alleviating difficulty with unequal prior class probabilities
    is to adjust (e.g. equalize) the number of patterns in each class, either by subsampling
    [24] (removing patterns from higher frequency classes), or by duplication
  id: totrans-3754
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法来缓解不平等先验类别概率的问题是调整（例如，平衡）每个类别中的模式数量，方法是子采样[24]（从高频类别中移除模式）或通过重复。
- en: (of patterns in lower frequency classes)4. For subsampling, patterns can be
    removed randomly, or heuristics may be used to remove patterns in regions of low
    ambiguity. Subsampling involves a loss of information which can be detrimental.
    Duplication involves a larger dataset and longer training times for the same number
    of training epochs (the convergence time may be longer or shorter).
  id: totrans-3755
  prefs: []
  type: TYPE_NORMAL
  zh: （低频类别中的模式）4。对于子采样，可以随机移除模式，或者可以使用启发式方法在低歧义区域移除模式。子采样涉及信息损失，可能会造成不利影响。重复涉及更大的数据集以及相同训练周期数量的更长训练时间（收敛时间可能更长或更短）。
- en: 14.3 Experimental Results
  id: totrans-3756
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.3 实验结果
- en: Results on an ECG classification problem are reported in this section after
    discussing the use of alternative performance measures. Results on a simple artificial
    problem are also included in the explanation section.
  id: totrans-3757
  prefs: []
  type: TYPE_NORMAL
  zh: 本节报告了在讨论替代性能度量的使用后，关于ECG分类问题的结果。解释部分还包括一个简单人工问题的结果。
- en: 14.3.1 Performance Measures
  id: totrans-3758
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.3.1 性能度量
- en: When the interclass prior probabilities of the classes vary significantly, then
    the overall classification error may not be the most appropriate performance criterion.
    For example, a model may always predict the most common class and still provide
    relatively high performance. Statistics such as the Sensitivity, Positive Predictivity,
    and False Positive Rate can provide more meaningful results [1].
  id: totrans-3759
  prefs: []
  type: TYPE_NORMAL
  zh: 当类别之间的先验概率差异显著时，整体分类错误可能不是最合适的性能标准。例如，模型可能始终预测最常见的类别，仍然提供相对较高的性能。灵敏度、正预测率和假阳性率等统计数据可以提供更有意义的结果[1]。
- en: 'These are defined on a class by class basis as follows:'
  id: totrans-3760
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是逐类定义的，如下所示：
- en: The Sensitivity of a class is the proportion of events labeled as that class
    which are correctly detected. For the two class confusion matrix shown in table
    14.1 the sensitivity of class 1 is c11 c11+c12 .
  id: totrans-3761
  prefs: []
  type: TYPE_NORMAL
  zh: 一个类别的**灵敏度**是被标记为该类别的事件中正确检测到的比例。对于表14.1中显示的两个类别混淆矩阵，类别1的灵敏度为c11 c11+c12。
- en: 4 The heuristic of adding noise during training [22] could be useful here as
    with the other techniques in this chapter.
  id: totrans-3762
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间添加噪声的启发式方法[22]在这里可能会有用，就像本章的其他技术一样。
- en: The Positive Predictivity of a class is the proportion of events which were
    predicted to be the class and were labeled as that class. For the two class confusion
    matrix shown in table 14.1 the positive predictivity of class 1 is c11 c11+c21
    .
  id: totrans-3763
  prefs: []
  type: TYPE_NORMAL
  zh: 一个类别的**正预测率**是被预测为该类别并被标记为该类别的事件的比例。对于表14.1中显示的两个类别混淆矩阵，类别1的正预测率为c11 c11+c21。
- en: The False Positive Rate of a class is the proportion of all patterns for other
    classes which were incorrectly classified as that class. For the two class confusion
    matrix shown in table 14.1 the false positive rate of class 1 is c21 c11+c21 .
  id: totrans-3764
  prefs: []
  type: TYPE_NORMAL
  zh: 一个类别的**假阳性率**是所有被错误分类为该类别的其他类别模式的比例。对于表14.1中显示的两个类别混淆矩阵，类别1的假阳性率为c21 c11+c21。
- en: Table 14.1. A sample confusion matrix which is used to illustrate sensitivity,
    positive predictivity, and false positive rate. Rows correspond to the desired
    classes and columns correspond to the predicted classes.
  id: totrans-3765
  prefs: []
  type: TYPE_NORMAL
  zh: 表14.1. 一个示例混淆矩阵，用于说明灵敏度、正预测率和假阳性率。行对应于期望类别，列对应于预测类别。
- en: '![295_image_0.png](295_image_0.png)'
  id: totrans-3766
  prefs: []
  type: TYPE_IMG
  zh: '![295_image_0.png](295_image_0.png)'
- en: No single performance criterion can be labeled as the best for comparing algorithms
    or models because the best criterion to use is problem dependent.
  id: totrans-3767
  prefs: []
  type: TYPE_NORMAL
  zh: 没有单一的性能标准可以被标记为比较算法或模型的最佳标准，因为使用的最佳标准依赖于问题。
- en: 'Here, we take the sensitivity as defined above, and create a single performance
    measure, the mean squared sensitivity error (MSSE). We define the MSSE as follows:
    Definition 2'
  id: totrans-3768
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们采用上述定义的敏感性，并创建一个单一性能度量——均方敏感性误差 (MSSE)。我们将 MSSE 定义如下：定义 2
- en: $$M S S E={\frac{1}{N_{c}}}\sum_{i=1}^{N_{c}}(1-S_{i})^{2}$$
  id: totrans-3769
  prefs: []
  type: TYPE_NORMAL
  zh: $$M S S E={\frac{1}{N_{c}}}\sum_{i=1}^{N_{c}}(1-S_{i})^{2}$$
- en: $$(14.6)$$
  id: totrans-3770
  prefs: []
  type: TYPE_NORMAL
  zh: $$(14.6)$$
- en: where Nc = the number of classes and Si = sensitivity of class i as defined
    earlier. Sensitivities range from 0 (no examples of the class correctly classified)
    to 1 (all examples correctly classified). Thus, a lower MSSE corresponds to better
    performance. We choose this criterion because each class is given equal importance
    and the square causes lower individual sensitivities to be penalized more (e.g.
    for a two class problem, class sensitivities of 100% and 0% produce a higher MSSE
    than sensitivities of 50% and 50%). Note that this is only one possible criterion,
    and other criterion could be used in order to reflect different requirements,
    e.g. specific misclassification costs for each class. The post scaling heuristic
    can be used with any criterion (and doing so may be simpler than reformulating
    the neural network training algorithm for the new criterion).
  id: totrans-3771
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Nc = 类别数，Si = 之前定义的类别 i 的敏感性。敏感性范围从 0（没有正确分类的类别示例）到 1（所有示例均正确分类）。因此，较低的 MSSE
    对应于更好的性能。我们选择这个标准是因为每个类别都被赋予了同等重要性，平方使得较低的个体敏感性受到更大惩罚（例如，对于二分类问题，类别敏感性为 100% 和
    0% 的 MSSE 高于 50% 和 50% 的敏感性）。请注意，这只是一个可能的标准，还可以使用其他标准以反映不同的需求，例如每个类别的特定错误分类成本。后缩放启发式方法可以与任何标准一起使用（这样做可能比重新制定神经网络训练算法以适应新标准更简单）。
- en: 14.3.2 Ecg Classification Problem
  id: totrans-3772
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.3.2 ECG 分类问题
- en: 'This section presents results using the beforementioned techniques on an ECG
    classification problem. The database used is the MIT-BIH Arrhythmia database [21]
    - a common publicly available ECG database which contains a large number of ECG
    records that have been carefully annotated by experts. Detection of the following
    four beat types is considered: Normal (N), Premature Ventricular Contraction (PVC),
    Supraventricular Contraction (S), and Fusion (F) [21], i.e. there are four output
    classes. The four classes are denoted 1 (N), 2 (PVC), 3 (S),'
  id: totrans-3773
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了使用上述技术在 ECG 分类问题上的结果。所用数据库为 MIT-BIH 心律失常数据库 [21]——一个常见的公开 ECG 数据库，包含大量经过专家仔细注释的
    ECG 记录。考虑检测以下四种心跳类型：正常 (N)、早期室性收缩 (PVC)、上心室收缩 (S) 和融合 (F) [21]，即有四个输出类别。这四个类别分别表示为
    1 (N)、2 (PVC)、3 (S)。
- en: and 4 (F). An autoregressive model is calculated for a window of 200 samples
    centered over the peak of the R-wave of each beat. The inputs are the polar coordinates
    of each pole in the z-plane, i.e. frequency changes are reflected in the angular
    variation of the poles and damping is reflected in the magnitude variations. The
    model order was four corresponding to eight input variables. The prior probability
    of the classes (according to the training data) is (0.737, 0.191, 0.0529, 0.0196)
    corresponding to beat types (N, PVC, S, F).
  id: totrans-3774
  prefs: []
  type: TYPE_NORMAL
  zh: 和 4 (F)。为每个心跳的 R 波峰计算一个窗口为 200 个样本的自回归模型。输入为 z 平面中每个极点的极坐标，即频率变化反映在极点的角度变化中，阻尼则反映在幅度变化中。模型的阶数为四，对应于八个输入变量。类别的先验概率（根据训练数据）为
    (0.737, 0.191, 0.0529, 0.0196)，对应于心跳类型 (N, PVC, S, F)。
- en: MLPs with 20 hidden units were trained with stochastic backpropagation
  id: totrans-3775
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 20 个隐藏单元的多层感知器通过随机反向传播进行训练
- en: (update after each pattern) using an initial learning rate of 0.02 which was
    linearly reduced to zero over the training period of 500,000 updates. We used
    5,000 points in each of the training, validation and test sets. The validation
    set was used for early stopping. The following algorithms were used - a) prior
    scaling with the degree of scaling, cs, varied from 0 to 1, b) probabilistic sampling
    with the degree of scaling, cs, varied from 0 to 1, c) as per a) and b) with the
    addition of post scaling, and d) equalizing the number of cases in each class
    by removing cases in more common classes. The post scaling attempted to minimize
    the MSSE on the training set5. 10 trials were performed for each case.
  id: totrans-3776
  prefs: []
  type: TYPE_NORMAL
  zh: （在每个模式后更新）使用初始学习率 0.02，该学习率在 500,000 次更新的训练期间线性减小至零。我们在训练、验证和测试集各使用 5,000 个点。验证集用于提前停止。使用了以下算法
    - a) 先前缩放，缩放程度 cs 从 0 到 1 变化，b) 概率采样，缩放程度 cs 从 0 到 1 变化，c) 根据 a) 和 b) 加入后期缩放，以及
    d) 通过移除更常见类别中的案例来均衡每个类别中的案例数量。后期缩放试图最小化训练集上的 MSSE5。每种情况进行了 10 次试验。
- en: The median test set MSSE for d) was 0.195. The results for probabilistic sampling
    and probabilistic sampling plus post scaling are shown with box-whiskers plots6
    in figure 14.1. For probabilistic sampling, the best scaling results correspond
    to a degree of scaling in between no scaling and scaling according to the prior
    probabilities (cs ≈ 0.8). When cs is larger, the sensitivity of class 1 drops
    significantly and results in higher false positive rates for the other classes.
    When cs is lower, the sensitivity of classes 3 and 4 drops significantly. It can
    be seen that the addition of post scaling appears to almost always improve performance
    for this problem. The optimal degree of scaling, cs ≈ 0.8, is difficult to determine
    *a priori*. However, it can be seen that the addition of post scaling makes the
    selection of cs far less critical (cs = 0.3 to cs = 1.0 result in similar performance).
    Figure 14.2 shows confusion matrices (in graphical form). Without
  id: totrans-3777
  prefs: []
  type: TYPE_NORMAL
  zh: d) 的中位测试集 MSSE 为 0.195。概率采样和概率采样加后期缩放的结果以箱线图6展示在图 14.1 中。对于概率采样，最佳缩放结果对应于没有缩放和根据先前概率缩放之间的缩放程度（cs
    ≈ 0.8）。当 cs 较大时，类别 1 的敏感性显著下降，导致其他类别的假阳性率增加。当 cs 较小时，类别 3 和 4 的敏感性显著下降。可以看出，增加后期缩放几乎总是能改善该问题的性能。最佳缩放程度
    cs ≈ 0.8 很难*事先*确定。然而，可以看出，增加后期缩放使得选择 cs 的重要性大大降低（cs = 0.3 到 cs = 1.0 的表现相似）。图
    14.2 显示了混淆矩阵（图形形式）。没有
- en: 5 400 steps were used for the hill climbing algorithm where each step corresponded
    to either multiplying or dividing an individual output scale factor by a constant
    which was reduced linearly over time from 1.5 to 1. The time taken was short compared
    to the overall training time. 6 The distribution of results is often not Gaussian
    and alternative means of presenting results other than the mean and standard deviation
    can be more informative. Boxwhiskers plots show the interquartile range (IQR)
    with a box and the median as a bar across the box. The whiskers extend from the
    ends of the box to the minimum and maximum values. The median and the IQR are
    simple statistics which are not as sensitive to outliers as the mean and the standard
    deviation [31]. The median is the value in the middle when arranging the distribution
    in order from the smallest to the largest value. If the data is divided into two
    equal groups about the median, then the IQR is the difference between the medians
    of these groups. The IQR contains 50% of the points.
  id: totrans-3778
  prefs: []
  type: TYPE_NORMAL
  zh: 采用 5400 步用于爬山算法，每一步对应于将单个输出缩放因子乘以或除以一个常数，该常数随着时间线性减少，从 1.5 减少到 1。与整体训练时间相比，所用时间较短。6
    结果的分布通常不是高斯分布，除了均值和标准差外，其他呈现结果的方法可能更具信息性。箱线图显示了以箱子表示的四分位数范围（IQR），以及作为箱子中横杆的中位数。触须从箱子两端延伸到最小值和最大值。中位数和
    IQR 是简单的统计量，对异常值的敏感度不如均值和标准差[31]。中位数是按从小到大排列分布时中间的值。如果数据被分为两个关于中位数的相等组，则 IQR 是这两个组中位数之间的差异。IQR
    包含 50% 的点。
- en: scaling (cs = 0), it can be seen that classes 3 & 4 have low sensitivity. With
    scaling using cs = 1 all classes are now recognized, however the sensitivity of
    class 1 is worse and the false positive rate of classes 3 & 4 is significantly
    worse.
  id: totrans-3779
  prefs: []
  type: TYPE_NORMAL
  zh: 在缩放（cs = 0）时，可以看出类别 3 和 4 的敏感性较低。使用 cs = 1 的缩放时，所有类别均被识别，但类别 1 的敏感性更差，类别 3 和
    4 的假阳性率显著更差。
- en: 'The results for prior scaling and prior scaling combined with post scaling
    were very similar but slightly worse than the results with probabilistic sampling.
    The prior scaling results are not plotted in order to make the graph easier to
    follow, however the qualitative results are as follows: for low cs, prior scaling
    and probabilistic sampling perform very similarly. However, for high cs, probabilistic
    sampling has a clear advantage for this problem. This is perhaps just as expected
    - the relatively high variation in prior class probabilities leads to a high variation
    in weight update magnitudes across the classes when using high cs. Results for
    all methods can be seen in table 14.2.'
  id: totrans-3780
  prefs: []
  type: TYPE_NORMAL
  zh: 先验缩放和先验缩放结合后缩放的结果非常相似，但略逊于概率性取样的结果。为了使图形更易于理解，先验缩放的结果没有被绘制，但定性结果如下：在低 cs 的情况下，先验缩放和概率性取样的表现非常相似。然而，在高
    cs 的情况下，概率性取样在此问题上有明显优势。这或许是意料之中的——先验类别概率的相对高变异性导致在使用高 cs 时，各类别的权重更新幅度具有较高的变异性。所有方法的结果见表
    14.2。
- en: Table 14.2. Results for the various methods. We show the average results for
    the best selection of cs and also an average across all selections of cs. Note
    that selection of the optimal value of cs is less critical when using post scaling
    in addition to either the prior scaling or probabilistic sampling methods.
  id: totrans-3781
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14.2. 各种方法的结果。我们展示了最佳 cs 选择的平均结果，以及所有 cs 选择的平均结果。请注意，当使用后缩放与先验缩放或概率性取样方法结合时，选择最佳
    cs 值的重要性降低。
- en: '| Method        | Prior        | Prior Scaling + Probabilistic Probabilistic   |
    Equalizing   |            |       |'
  id: totrans-3782
  prefs: []
  type: TYPE_TB
  zh: '| 方法          | 先验        | 先验缩放 + 概率性取样                        | 平均化       |            |       |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-3783
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Scaling       | Post Scaling | Sampling                                      |
    Sampling +   | Membership |       |'
  id: totrans-3784
  prefs: []
  type: TYPE_TB
  zh: '| 缩放          | 后缩放      | 取样                                        | 取样 +       |
    归属      |       |'
- en: '|               | Post Scaling |                                               |              |            |       |'
  id: totrans-3785
  prefs: []
  type: TYPE_TB
  zh: '|               | 后缩放      |                                               |              |            |       |'
- en: '| Average MSSE  | 0.10         | 0.096                                         |
    0.099        | 0.089      | 0.195 |'
  id: totrans-3786
  prefs: []
  type: TYPE_TB
  zh: '| 平均 MSSE     | 0.10         | 0.096                                         |
    0.099        | 0.089      | 0.195 |'
- en: '| (for best cs) | (cs = 0.8)   | (cs = 0.6)                                    |
    (cs = 0.8)   | (cs = 0.3) |       |'
  id: totrans-3787
  prefs: []
  type: TYPE_TB
  zh: '| (最佳 cs)     | (cs = 0.8)   | (cs = 0.6)                                    |
    (cs = 0.8)   | (cs = 0.3) |       |'
- en: '| Average MSSE  | 0.19         | 0.10                                          |
    0.18         | 0.099      | 0.195 |'
  id: totrans-3788
  prefs: []
  type: TYPE_TB
  zh: '| 平均 MSSE     | 0.19         | 0.10                                          |
    0.18         | 0.099      | 0.195 |'
- en: '| (over all cs) |              |                                               |              |            |       |'
  id: totrans-3789
  prefs: []
  type: TYPE_TB
  zh: '| (针对所有 cs) |              |                                               |              |            |       |'
- en: 14.4 Explanation
  id: totrans-3790
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.4 说明
- en: This section discusses why the techniques presented can be useful, limitations
    of the techniques, and how they relate to the theoretical result that MLPs approximate
    Bayesian *a posteriori* probabilities under certain conditions.
  id: totrans-3791
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了所提出的技术为何有用、这些技术的局限性，以及它们与 MLP 在某些条件下近似贝叶斯 *后验* 概率的理论结果之间的关系。
- en: 14.4.1 Convergence And Representation Issues
  id: totrans-3792
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.4.1 收敛性和表示问题
- en: 'We first list four possible situations:'
  id: totrans-3793
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先列出四种可能的情况：
- en: 1. The proofs regarding estimation of Bayesian *a posteriori* probabilities
    assume networks with an infinite number of hidden nodes in order to obtain accurate
    approximation. For a given problem, it can be seen that a network which is too
    small will be unable to estimate the probabilities accurately due to limited resources.
  id: totrans-3794
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 关于贝叶斯 *后验* 概率估计的证明假设网络具有无限数量的隐藏节点，以便获得准确的近似。对于给定的问题，可以看出，过小的网络将无法由于资源有限而准确估计概率。
- en: '![298_image_0.png](298_image_0.png)'
  id: totrans-3795
  prefs: []
  type: TYPE_IMG
  zh: '![298_image_0.png](298_image_0.png)'
- en: Fig. 14.1. Box-whiskers plots (on the left in each case) along with the usual
    mean plus and minus one standard deviation plots (on the right in each case) showing
    the test set MSSE for probabilistic sampling and for probabilistic sampling plus
    post scaling. Each result is derived from 10 trials with different starting conditions.
    The probabilistic sampling plus post scaling case is offset by 0.03 to aid viewing.
    It can be seen that the selection of the scaling degree for the best performance
    is not as critical when using the combination of probabilistic sampling and post
    scaling.
  id: totrans-3796
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.1。箱线图（每种情况下左侧）以及通常的均值加减一个标准差图（每种情况下右侧），显示了概率采样和概率采样加后期缩放的测试集 MSSE。每个结果来自
    10 次不同起始条件的试验。概率采样加后期缩放的情况偏移 0.03 以便观察。可以看出，使用概率采样和后期缩放组合时，选择最佳性能的缩放程度并不是那么关键。
- en: 2. Training an MLP is NP-complete in general and it is well known that practical
    training algorithms used for MLPs often result in sub-optimal solutions
  id: totrans-3797
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个多层感知器（MLP）在一般情况下是 NP 完全的，众所周知，实际用于 MLP 的训练算法往往会导致次优解。
- en: (e.g. due to local minima). Often, a result of attaining a sub-optimal solution
    is that not all of the network resources are efficiently used. Experiments with
    a controlled task have indicated that the sub-optimal solutions often have smaller
    weights on average [17].
  id: totrans-3798
  prefs: []
  type: TYPE_NORMAL
  zh: （例如，由于局部极小值）。获得次优解的结果往往是网络资源没有得到有效利用。对受控任务的实验表明，次优解的平均权重往往较小[17]。
- en: 3. Weight decay [16] or weight elimination [30] are often used in MLP training
    and aim to minimize a cost function which penalizes large weights. These techniques
    tend to result in networks with smaller weights.
  id: totrans-3799
  prefs: []
  type: TYPE_NORMAL
  zh: 权重衰减[16]或权重消除[30]常用于 MLP 训练，旨在最小化惩罚大权重的成本函数。这些技术往往导致权重较小的网络。
- en: 4. A commonly recommended technique with MLP classification is to set the training
    targets away from the bounds of the activation function (e.g. (-0.8, 0.8) instead
    of (-1, 1) for the tanh activation function) [14].
  id: totrans-3800
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的建议技术是将 MLP 分类的训练目标设置在激活函数的边界之外（例如，tanh 激活函数的范围为 (-0.8, 0.8) 而不是 (-1, 1)）[14]。
- en: '![298_image_1.png](298_image_1.png)'
  id: totrans-3801
  prefs: []
  type: TYPE_IMG
  zh: '![298_image_1.png](298_image_1.png)'
- en: Fig. 14.2. Confusion matrices for the test set as the degree of prior scaling,
    cs, is varied from 0 (left) to 1 (right). The columns correspond to the predicted
    classes and the rows correspond to the desired classes. The classes are (left
    to right and top to bottom) N, PVC, S, F. For each desired class, the predicted
    classes are shaded in proportion to the number of examples which are labeled as
    the desired class. White indicates no predictions. A general trend can be observed
    where classes S & F are recognized as normal when cs = 0, and progressively more
    of the normal class examples are recognized as classes PVC, S, & F as cs approaches
    1.
  id: totrans-3802
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2。测试集的混淆矩阵，当先前缩放的程度 cs 从 0（左侧）变化到 1（右侧）。列对应于预测类别，行对应于期望类别。类别为（从左到右，从上到下）N、PVC、S、F。对于每个期望类别，预测类别的阴影深度与标记为期望类别的示例数量成比例。白色表示没有预测。可以观察到一个一般趋势，当
    cs = 0 时，类别 S 和 F 被识别为正常，随着 cs 接近 1，越来越多的正常类别示例被识别为类别 PVC、S 和 F。
- en: These four situations can all lead to a bias towards smaller weights, or
  id: totrans-3803
  prefs: []
  type: TYPE_NORMAL
  zh: 这四种情况都可能导致对较小权重的偏倚，或者
- en: '"smoother" models7. The possibility of such a bias is not taken into account
    by the proofs regarding posterior probabilities, i.e. the difference between theory
    and practice may, in part, be explained by violation of the assumption that sufficient
    convergence is obtained.'
  id: totrans-3804
  prefs: []
  type: TYPE_NORMAL
  zh: “更平滑”的模型。这种偏倚的可能性没有被后验概率的证明所考虑，即理论与实践之间的差异在某种程度上可能是由于违反了获得足够收敛的假设。
- en: When a network is biased towards a "smoother" solution, and accurate fitting
    of the optimal function is not possible, the result may be a tendency to "ignore"
    lower frequency classes8, e.g. if a network has the choice of fitting either a
    high frequency class or a low frequency class then it can provide a lower MSE
    by fitting the high frequency class9. We demonstrate by example.
  id: totrans-3805
  prefs: []
  type: TYPE_NORMAL
  zh: 当网络偏向于“更平滑”的解，并且无法准确拟合最优函数时，结果可能倾向于“忽略”低频类别，例如，如果网络必须选择拟合高频类别或低频类别，那么通过拟合高频类别可以提供更低的
    MSE。我们通过示例进行演示。
- en: 'We generated artificial training data using the following distributions: class
    1:'
  id: totrans-3806
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下分布生成了人工训练数据：类别 1：
- en: 'N(−5, 1, 2)+N(0, 1, 2)+N(5, 1, 2), class 2: N(−2.5, 0.25, 0.5)+N(2.5, 0.25,
    0.5),'
  id: totrans-3807
  prefs: []
  type: TYPE_NORMAL
  zh: 'N(−5, 1, 2)+N(0, 1, 2)+N(5, 1, 2)，类别 2: N(−2.5, 0.25, 0.5)+N(2.5, 0.25, 0.5)，'
- en: where N(*μ, σ, x*) is a normal distribution with mean μ, standard deviation
    σ, and is truncated to lie within (μ − *x, μ* + x). We generated 500 training
    and test examples from these distributions with the probability of selection for
    classes (1,2) being (0.9,0.1), i.e. the training and test sets have nine times
    as many samples of class 1 as they do of class 2. Note that there is no overlap
    between the classes. Figure 14.3 shows typical output probability plots for training
    an MLP with 10 hidden nodes10 with and without probabilistic sampling. 10 trials
    were performed in each case with very similar results (see table 14.3). It can
    be seen that the network "ignores" class two without the use of probabilistic
    sampling.
  id: totrans-3808
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 N(*μ, σ, x*) 是均值为 μ、标准差为 σ 的正态分布，并且截断在 (μ − *x, μ* + x) 之间。我们从这些分布中生成了 500
    个训练和测试样本，选择类别 (1,2) 的概率为 (0.9,0.1)，即训练集和测试集的类别 1 的样本数量是类别 2 的九倍。注意，两个类别之间没有重叠。图
    14.3 显示了训练具有 10 个隐含节点的 MLP 的典型输出概率图，分别采用了和未采用概率抽样的情况。每种情况进行了 10 次试验，结果非常相似（见表
    14.3）。可以看出，网络在不使用概率抽样的情况下“忽略”了类别二。
- en: It should be noted that using conjugate gradient training for this simple problem
    results in relatively accurate estimation of both classes with standard training
    (alternate parameters with backpropagation may also be successful).
  id: totrans-3809
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，对于这个简单问题，使用共轭梯度训练可以在标准训练下对两个类别进行相对准确的估计（使用反向传播的交替参数也可能成功）。
- en: Rather than arguing for either backpropagation or conjugate gradient here (neither
    training algorithm is expected to always find a global minimum in general),
  id: totrans-3810
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们并不是在争论反向传播或共轭梯度（这两种训练算法一般来说都不期望总是找到全局最小值），
- en: we simply note that our experience and the experience of others [7, 18, 19,
    27]
  id: totrans-3811
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是指出我们的经验以及其他人的经验 [7, 18, 19, 27]
- en: suggests that conjugate gradient is not superior for many problems - i.e. backpropagation
    works better on one class of problems and conjugate gradient works better on another
    class. Conjugate gradient resulted in significantly worse performance when tested
    on the ECG problem. It should be noted that there are
  id: totrans-3812
  prefs: []
  type: TYPE_NORMAL
  zh: 表明对于许多问题，共轭梯度并不优越——也就是说，反向传播在某类问题上效果更好，而共轭梯度在另一类问题上效果更好。在对 ECG 问题进行测试时，共轭梯度的性能显著较差。值得注意的是，
- en: 7 In general, smaller weights correspond to smoother functions, however this
    is not always true. For example, this is not the case when fitting the function
    *sech(x*)
  id: totrans-3813
  prefs: []
  type: TYPE_NORMAL
  zh: 7 一般而言，较小的权重对应于较平滑的函数，但这并不总是成立。例如，当拟合函数 *sech(x*) 时就不是这样。
- en: using two tanh sigmoids [8] (because *sech(x*) = limd→0(tanh(x + d) − tanh(x))/d,
    i.e. the weights become indefinitely large as the approximation improves). 8 In
    relation to the representational capacity (size of the network), Barnard and Botha
  id: totrans-3814
  prefs: []
  type: TYPE_NORMAL
  zh: 使用两个 tanh sigmoid [8]（因为 *sech(x*) = limd→0(tanh(x + d) − tanh(x))/d，即随着近似的改善，权重变得无限大）。8
    关于表示能力（网络的大小），Barnard 和 Botha
- en: '[3] have observed that MLP networks have a tendency to guess higher probability
    classes when a network is too small to approximate the decision boundaries reasonably
    well. 9 Lyon and Yaeger [20] find that their frequency balancing technique reduces
    the effect of the prior class probabilities on the network and effectively forces
    the network to allocate more resources to the lower frequency classes. 10 500,000
    stochastic training updates with backpropagation, initial learning rate 0.02 reduced
    linearly to zero.'
  id: totrans-3815
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 观察到，当网络过小而无法合理近似决策边界时，MLP 网络倾向于猜测更高概率的类别。9 Lyon 和 Yaeger [20] 发现他们的频率平衡技术减少了先前类别概率对网络的影响，并有效地迫使网络将更多资源分配给低频类别。10
    进行了 500,000 次带反向传播的随机训练更新，初始学习率为 0.02，线性减至零。'
- en: many options when implementing a conjugate gradient training algorithm and that
    poor performance may be attributed to the implementation used. We have used a
    modified implementation of the algorithm from Fletcher [9].
  id: totrans-3816
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现共轭梯度训练算法时有许多选项，而较差的性能可能归因于所使用的实现。我们使用了来自 Fletcher [9] 的算法修改实现。
- en: '![300_image_0.png](300_image_0.png)'
  id: totrans-3817
  prefs: []
  type: TYPE_IMG
  zh: '![300_image_0.png](300_image_0.png)'
- en: Fig. 14.3. Network outputs for the artificial problem with (below) and without
    (above)
  id: totrans-3818
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3. 人工问题的网络输出，下面是使用（上面是未使用）的情况。
- en: probabilistic sampling. It can be seen that the network "ignores" the lower
    frequency class without the use of probabilistic sampling. Note that the input
    has been normalized. Table 14.3. Mean and standard deviation of the classification
    error for the artificial problem both with and without the use of probabilistic
    sampling
  id: totrans-3819
  prefs: []
  type: TYPE_NORMAL
  zh: 概率采样。可以看出，网络在没有使用概率采样的情况下“忽略”了低频类别。请注意，输入已被归一化。表14.3。使用和不使用概率采样的人工问题分类错误的均值和标准差
- en: '| Classification Error        | Mean Standard Deviation   |       |'
  id: totrans-3820
  prefs: []
  type: TYPE_TB
  zh: '| 分类错误                   | 平均标准差               |       |'
- en: '| --- | --- | --- |'
  id: totrans-3821
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Standard Training           | 11.4                      | 0.02  |'
  id: totrans-3822
  prefs: []
  type: TYPE_TB
  zh: '| 标准训练                   | 11.4                      | 0.02  |'
- en: '| With Probabilistic Sampling | 0.8                       | 0.004 |'
  id: totrans-3823
  prefs: []
  type: TYPE_TB
  zh: '| 使用概率采样                | 0.8                       | 0.004 |'
- en: 14.4.2 Overlapping Distributions
  id: totrans-3824
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.4.2 重叠分布
- en: Consider figure 14.4. If classes 1 and 2 have distributions differing only by
    translation (c1 and c2) then the decision threshold between these classes should
    be chosen at x1. Equal percentages of each of these classes will be classified
    as the other class. Now, if the distribution of class 2 is as shown (c2) then
    the decision threshold between the classes should be chosen at x2. In this case,
    a higher percentage of class 2 will be classified as class 1 than the reverse.
    If it is desirable to maximize the class by class sensitivity then scaling such
    that the effective distribution of c2 is c2 might be appropriate. Similarly, class
    3 (c3) will be "ignored" without any scaling.
  id: totrans-3825
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑图14.4。如果类别1和类别2的分布仅通过平移（c1和c2）不同，则这两个类别之间的决策阈值应选择在x1处。这两个类别的相等比例将被分类为另一个类别。现在，如果类别2的分布如图所示（c2），则这两个类别之间的决策阈值应选择在x2处。在这种情况下，类别2被分类为类别1的比例将高于反向。如果希望最大化逐类敏感性，则可能需要缩放使得c2的有效分布为c2。类似地，类别3（c3）在没有任何缩放的情况下将被“忽略”。
- en: Scaling on a class by class basis may be desired when i) the distribution of
    samples in the training set does not match the true distribution (e.g. it may
    be
  id: totrans-3826
  prefs: []
  type: TYPE_NORMAL
  zh: 按类别缩放可能是必要的，当 i) 训练集中的样本分布与真实分布不匹配时（例如，它可能是
- en: '![301_image_0.png](301_image_0.png)'
  id: totrans-3827
  prefs: []
  type: TYPE_IMG
  zh: '![301_image_0.png](301_image_0.png)'
- en: more expensive to collect samples of a particular class)11, or ii) the distribution
    of the classes does not represent their relative importance, e.g. in a medical
    classification problem the cost of misclassifying a diseased case as normal may
    be much higher than the cost of classifying a normal case as a (possibly) diseased
    case [24]. The importance of each class may be independent of the class prior
    probabilities. Note that scaling such that lower frequency classes are made to
    be artificially more important can be useful when considering a higher level problem.
    For example, the training data from natural English words and phrases exhibit
    very non-uniform priors for different characters. Yaeger et al. [33] find that
    reducing the effect of these priors on the network using frequency balancing improves
    the performance of the higher level word recognition training.
  id: totrans-3828
  prefs: []
  type: TYPE_NORMAL
  zh: 收集特定类别样本的成本更高）11，或 ii) 类别的分布并不代表它们的相对重要性，例如在医疗分类问题中，将患病案例误分类为正常的成本可能远高于将正常案例误分类为（可能）患病案例的成本[24]。每个类别的重要性可能与类别的先验概率无关。请注意，缩放使得低频类别被人为地赋予更高重要性在考虑更高级问题时可能是有用的。例如，自然英语单词和短语的训练数据对于不同字符表现出非常不均匀的先验。Yaeger
    等人[33]发现，通过使用频率平衡减少这些先验对网络的影响，可以提高更高级别的单词识别训练的性能。
- en: Observations. a) There is no intrinsic problem if the distributions do not overlap.
    b) When distributions overlap, it is desirable to preprocess the data in a manner
    that results in reduced overlap. However, it is often not possible to obtain zero
    overlap (due to noise, for example).
  id: totrans-3829
  prefs: []
  type: TYPE_NORMAL
  zh: 观察。a) 如果分布不重叠，则没有内在问题。b) 当分布重叠时，预处理数据以减少重叠是可取的。然而，通常无法获得零重叠（例如，由于噪声）。
- en: Fig. 14.4. Overlapping distributions
  id: totrans-3830
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4。重叠分布
- en: 14.4.3 Limitations
  id: totrans-3831
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.4.3 限制
- en: 'We note a couple of limitations with the heuristics considered herein:'
  id: totrans-3832
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到此处考虑的启发式方法存在几个限制：
- en: 1. *Local issues.* The heuristics presented counteract biases in the network,
    training algorithm and/or training data. There is no reason for these biases to
    be constant throughout the input space, e.g. scaling may be helpful in one region
    but detrimental in another.
  id: totrans-3833
  prefs: []
  type: TYPE_NORMAL
  zh: 1. *局部问题.* 所提出的启发式方法抵消了网络、训练算法和/或训练数据中的偏差。这些偏差在输入空间中并不一定是恒定的，例如，在某一区域缩放可能是有帮助的，但在另一区域可能是有害的。
- en: 11 It may be possible to obtain more accurate estimates of class probabilities
    using data that has class labels without input information. For example, word
    frequency information can be obtained from text databases and the frequency of
    various diseases can be obtained from health statistics [23].
  id: totrans-3834
  prefs: []
  type: TYPE_NORMAL
  zh: 11 使用没有输入信息的类别标签数据可能能够获得更准确的类别概率估计。例如，文本数据库可以获得单词频率信息，各种疾病的频率可以从健康统计数据中获得[23]。
- en: 2. *Nonlinear calibration.* There is no reason for the linear scaling heuristics
    used here to be optimal (in the sense that they best counteract the biases).
  id: totrans-3835
  prefs: []
  type: TYPE_NORMAL
  zh: 2. *非线性校准.* 这里使用的线性缩放启发式方法并不一定是最佳的（在于它们能够最佳地抵消偏差的意义上）。
- en: 14.4.4 A Posteriori Proofs
  id: totrans-3836
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.4.4 后验证明
- en: Theoretically it is possible to show that the scaling techniques invalidate
    the a posteriori proofs - when performing scaling on a class by class basis the
    decision thresholds which are used to determine the winning class should be altered
    accordingly. This indicates another possible use of the prior scaling and probabilistic
    sampling techniques when the conditions given above do not exist. This use is
    related to the problem whereby lower frequency classes may be estimated less accurately
    than higher frequency classes (see section 14.2.3) - training may be performed
    with the heuristically altered problem (e.g. so that the class frequencies are
    effectively equal) and the outputs or decision thresholds can be altered accordingly.
  id: totrans-3837
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，可以证明缩放技术使后验证明失效——在逐类进行缩放时，用于确定获胜类别的决策阈值应相应改变。这表明，当上述条件不存在时，先前缩放和概率抽样技术可能的另一种用途。这种用途与低频类别可能比高频类别估计得不够准确的问题有关（见第14.2.3节）——可以对经过启发式改变的问题进行训练（例如，使类别频率有效相等），并相应改变输出或决策阈值。
- en: 14.5 Conclusions
  id: totrans-3838
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.5 结论
- en: In practice, training issues or characteristics of a given classification problem
    can mean that scaling the predicted class probabilities may improve performance
    in terms of overall classification error and/or in terms of an alternative criterion.
  id: totrans-3839
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，训练问题或特定分类问题的特征可能意味着缩放预测类别概率可能在整体分类错误和/或替代标准方面提高性能。
- en: We introduced algorithms which a) scale weight updates on a class by class basis
    according to the prior class probabilities, b) alter class frequencies probabilistically
    (very similar to the frequency balancing technique of Yaeger et al. [33]),
  id: totrans-3840
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了算法，a) 根据先前的类别概率按类别缩放权重更新，b) 以概率方式改变类别频率（非常类似于Yaeger等人[33]的频率平衡技术），
- en: and c) scale outputs after training in order to maximize a given performance
    criterion. For an electrocardiogram (ECG) classification problem, we found that
    the prior scaling, probabilistic sampling, and post scaling techniques provided
    better performance in comparison to a) no heuristics, and b) subsampling in order
    to equalize the number of cases in each class. The best performance for prior
    scaling and probabilistic sampling was obtained with a degree of scaling in between
    no scaling and scaling according to the prior probabilities. The optimal degree
    was difficult to determine *a priori*. However, it was found that using prior
    scaling or probabilistic sampling in combination with post scaling made the selection
    of the optimal degree far less critical.
  id: totrans-3841
  prefs: []
  type: TYPE_NORMAL
  zh: c) 在训练后缩放输出，以最大化给定的性能标准。对于心电图（ECG）分类问题，我们发现先前缩放、概率抽样和后缩放技术在性能方面优于a) 没有启发式方法和b)
    为了平衡每个类别的案例数量而进行的子抽样。先前缩放和概率抽样的最佳性能是在无缩放和根据先前概率进行缩放之间的缩放程度获得的。最佳程度很难*事先*确定。然而，发现将先前缩放或概率抽样与后缩放结合使用，使得选择最佳程度变得不那么关键。
- en: '[1] AAMI. Testing and reporting performance results of ventricular Arrhythmia
    detection algorithms. In: Association for the Advancement of Medical Instrumentation,
    ECAR 1987, Arlington, VA (1987)'
  id: totrans-3842
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] AAMI. 心室心律失常检测算法的测试和报告性能结果。载于：医学仪器协会，ECAR 1987，弗吉尼亚州阿灵顿（1987）'
- en: '[2] Anand, R., Mehrotra, K.G., Mohan, C.K., Ranka, S.: An improved algorithm
    for neural network classification of imbalanced training sets. IEEE Transactions
    on Neural Networks 4(6), 962–969 (1993)'
  id: totrans-3843
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Anand, R., Mehrotra, K.G., Mohan, C.K., Ranka, S.: 一种改进的算法用于神经网络对不平衡训练集的分类。《IEEE神经网络交易》4(6),
    962–969 (1993)'
- en: '[3] Barnard, E., Botha, E.C.: Back-propagation uses prior information efficiently.'
  id: totrans-3844
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Barnard, E., Botha, E.C.: 反向传播有效地利用先验信息。'
- en: IEEE Transactions on Neural Networks 4(5), 794–802 (1993)
  id: totrans-3845
  prefs: []
  type: TYPE_NORMAL
  zh: 《IEEE神经网络交易》4(5), 794–802 (1993)
- en: '[4] Barnard, E., Casasent, D.: A comparison between criterion functions for
    linear classifiers, with an application to neural nets. IEEE Transactions on Systems,
    Man, and Cybernetics 19(5), 1030–1041 (1989)'
  id: totrans-3846
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Barnard, E., Casasent, D.: 线性分类器的标准函数比较及其在神经网络中的应用。《IEEE系统、人与网络》19(5),
    1030–1041 (1989)'
- en: '[5] Barnard, E., Cole, R.A., Hou, L.: Location and classification of plosive
    constants using expert knowledge and neural-net classifiers. Journal of the Acoustical
    Society of America 84(suppl. 1), S60 (1988)'
  id: totrans-3847
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Barnard, E., Cole, R.A., Hou, L.: 使用专家知识和神经网络分类器进行爆破辅音的定位与分类。《美国声学学会杂志》84(补充1),
    S60 (1988)'
- en: '[6] Bourlard, H.A., Morgan, N.: Links between Markov models and multilayer
    perceptrons. In: Touretzky, D.S. (ed.) Advances in Neural Information Processing
    Systems, vol. 1, pp. 502–510. Morgan Kaufmann, San Mateo (1989)'
  id: totrans-3848
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Bourlard, H.A., Morgan, N.: 马尔可夫模型与多层感知器之间的联系。在：Touretzky, D.S. (编) 《神经信息处理系统进展》，第1卷，pp.
    502–510。摩根·考夫曼出版社，圣马特奥 (1989)'
- en: '[7] Bourlard, H.A., Morgan, N.: Connnectionist Speech Recognition: A Hybrid
    Approach. Kluwer Academic Publishers, Boston (1994)'
  id: totrans-3849
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Bourlard, H.A., Morgan, N.: 连接主义语音识别：一种混合方法。克鲁维尔学术出版社，波士顿 (1994)'
- en: '[8] Scott Cardell, N., Joerding, W., Li, Y.: Why some feedforward networks
    cannot learn some polynomials. Neural Computation 6(4), 761–766 (1994)'
  id: totrans-3850
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Scott Cardell, N., Joerding, W., Li, Y.: 为什么一些前馈网络无法学习某些多项式。《神经计算》6(4),
    761–766 (1994)'
- en: '[9] Fletcher, R.: Practical Methods of Optimization, Second Edition, 2nd edn.
    John Wiley & Sons (1987)'
  id: totrans-3851
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Fletcher, R.: 《实用优化方法》，第二版，2nd edn. 约翰·威利与儿子公司 (1987)'
- en: '[10] Geman, S., Bienenstock, E., Doursat, R.: Neural networks and the bias/variance
    dilemma. Neural Computation 4(1), 1–58 (1992)'
  id: totrans-3852
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Geman, S., Bienenstock, E., Doursat, R.: 神经网络与偏差/方差困境。《神经计算》4(1), 1–58
    (1992)'
- en: '[11] Gish, H.: A probabilistic approach to the understanding and training of
    neural network classifiers. In: Proceedings of the IEEE Conference on Acoustics,
    Speech and Signal Processing, pp. 1361–1364. IEEE Press (1990)'
  id: totrans-3853
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Gish, H.: 一种概率方法理解和训练神经网络分类器。在：IEEE声学、语音与信号处理会议论文集，pp. 1361–1364。IEEE出版社
    (1990)'
- en: '[12] Hampshire, J.B., Pearlmutter, B.: Equivalence proofs for multilayer perceptron
    classifiers and the Bayesian discriminant function. In: Touretzky, D.S., Elman,
    J.L., Sejnowski, T.J., Hinton, G.E. (eds.) Proceedings of the 1990 Connectionist
    Models Summer School, Morgan Kaufmann, San Mateo (1990)'
  id: totrans-3854
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Hampshire, J.B., Pearlmutter, B.: 多层感知器分类器与贝叶斯判别函数的等价性证明。在：Touretzky,
    D.S., Elman, J.L., Sejnowski, T.J., Hinton, G.E. (编) 1990年连接主义模型暑期学校论文集，摩根·考夫曼出版社，圣马特奥
    (1990)'
- en: '[13] Hampshire, J.B., Waibel, A.H.: A novel objective function for improved
    phoneme recognition using time delay neural networks. In: International Joint
    Conference on Neural Networks, Washington, DC, pp. 235–241 (June 1989)'
  id: totrans-3855
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Hampshire, J.B., Waibel, A.H.: 一种新颖的目标函数用于改进时间延迟神经网络的音素识别。在：国际神经网络联合会议，华盛顿特区，pp.
    235–241 (1989年6月)'
- en: '[14] Haykin, S.: Neural Networks, A Comprehensive Foundation. Macmillan, New
    York'
  id: totrans-3856
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Haykin, S.: 《神经网络：全面基础》。麦克米兰出版社，纽约'
- en: (1994)
  id: totrans-3857
  prefs: []
  type: TYPE_NORMAL
  zh: (1994)
- en: '[15] Kanaya, F., Miyake, S.: Bayes statistical behavior and valid generalization
    of pattern classifying neural networks. IEEE Transactions on Neural Networks 2(1),
    471'
  id: totrans-3858
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Kanaya, F., Miyake, S.: 贝叶斯统计行为与模式分类神经网络的有效泛化。《IEEE神经网络交易》2(1), 471'
- en: (1991)
  id: totrans-3859
  prefs: []
  type: TYPE_NORMAL
  zh: (1991)
- en: '[16] Krogh, A., Hertz, J.A.: A simple weight decay can improve generalization.
    In:'
  id: totrans-3860
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Krogh, A., Hertz, J.A.: 一种简单的权重衰减可以改善泛化。在：'
- en: Moody, J.E., Hanson, S.J., Lippmann, R.P. (eds.) Advances in Neural Information
    Processing Systems, vol. 4, pp. 950–957. Morgan Kaufmann, San Mateo (1992)
  id: totrans-3861
  prefs: []
  type: TYPE_NORMAL
  zh: Moody, J.E., Hanson, S.J., Lippmann, R.P. (编) 《神经信息处理系统进展》，第4卷，pp. 950–957。摩根·考夫曼出版社，圣马特奥
    (1992)
- en: '[17] Lawrence, S., Lee Giles, C., Tsoi, A.C.: Lessons in neural network training:
    Overfitting be harder than expected. In: Proceedings of the Fourteenth National
    Conference on Artificial Intelligence, AAAI 1997, pp. 540–545. AAAI Press, Menlo
    Park (1997)'
  id: totrans-3862
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Lawrence, S., Lee Giles, C., Tsoi, A.C.: 神经网络训练中的教训：过拟合比预期更难。 在：第十四届人工智能全国会议论文集，AAAI
    1997，pp. 540–545。AAAI出版社，门洛公园 (1997)'
- en: '[18] LeCun, Y.: Efficient learning and second order methods. In: Tutorial Presented
    at Neural Information Processing Systems, vol. 5 (1993)'
  id: totrans-3863
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] LeCun, Y.: 高效学习与二阶方法。见：在神经信息处理系统中的教程，第5卷 (1993)'
- en: '[19] LeCun, Y., Bengio, Y.: Pattern recognition. In: Arbib, M.A. (ed.) The
    Handbook of Brain Theory and Neural Networks, pp. 711–715. MIT Press (1995)'
  id: totrans-3864
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] LeCun, Y., Bengio, Y.: 模式识别。见：Arbib, M.A.（编）《脑理论与神经网络手册》，第711–715页。MIT出版社
    (1995)'
- en: '[20] Lyon, R., Yaeger, L.: On-line hand-printing recognition with neural networks.
    In:'
  id: totrans-3865
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Lyon, R., Yaeger, L.: 在线手写识别与神经网络。见：'
- en: Fifth International Conference on Microelectronics for Neural Networks and Fuzzy
    Systems, Lausanne, Switzerland. IEEE Computer Society Press (1996)
  id: totrans-3866
  prefs: []
  type: TYPE_NORMAL
  zh: 第五届微电子神经网络与模糊系统国际会议，瑞士洛桑。IEEE计算机学会出版社 (1996)
- en: '[21] MIT-BIH. MIT-BIH Arrhythmia database directory. Technical Report BMEC'
  id: totrans-3867
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] MIT-BIH. MIT-BIH心律不齐数据库目录。技术报告BMEC'
- en: TR010 (Revised), Massachusetts Institute of Technology and Beth Israel Hospital
    (1988)
  id: totrans-3868
  prefs: []
  type: TYPE_NORMAL
  zh: TR010（修订版），麻省理工学院与贝斯以色列医院 (1988)
- en: '[22] Murray, A.F., Edwards, P.J.: Enhanced MLP performance and fault tolerance
    resulting from synaptic weight noise during training. IEEE Transactions on Neural
    Networks 5(5), 792–802 (1994)'
  id: totrans-3869
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Murray, A.F., Edwards, P.J.: 在训练过程中，由于突触权重噪声导致的增强型多层感知器性能和容错能力。IEEE神经网络交易
    5(5), 792–802 (1994)'
- en: '[23] Richard, M.D., Lippmann, R.P.: Neural network classifiers estimate Bayesian
    a posteriori probabilities. Neural Computation 3(4), 461–483 (1991)'
  id: totrans-3870
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Richard, M.D., Lippmann, R.P.: 神经网络分类器估计贝叶斯后验概率。神经计算 3(4), 461–483 (1991)'
- en: '[24] Ripley, B.D.: Pattern Recognition and Neural Networks. Cambridge University
    Press, Cambridge (1996)'
  id: totrans-3871
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Ripley, B.D.: 《模式识别与神经网络》。剑桥大学出版社, 剑桥 (1996)'
- en: '[25] Rojas, R.: A short proof of the posterior probability property of classifier
    neural networks. Neural Computation 8, 41–43 (1996)'
  id: totrans-3872
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Rojas, R.: 分类器神经网络后验概率属性的简短证明。神经计算 8, 41–43 (1996)'
- en: '[26] Ruck, D.W., Rogers, S.K., Kabrisky, K., Oxley, M.E., Suter, B.W.: The
    multilayer perceptron as an approximation to an optimal Bayes estimator. IEEE
    Transactions on Neural Networks 1(4), 296–298 (1990)'
  id: totrans-3873
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Ruck, D.W., Rogers, S.K., Kabrisky, K., Oxley, M.E., Suter, B.W.: 多层感知器作为最佳贝叶斯估计器的近似。IEEE神经网络交易
    1(4), 296–298 (1990)'
- en: '[27] Schiffman, W., Joost, M., Werner, R.: Optimization of the backpropagation
    algorithm for training multilayer perceptrons. Technical report, University of
    Koblenz'
  id: totrans-3874
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Schiffman, W., Joost, M., Werner, R.: 多层感知器训练的反向传播算法优化。技术报告，科布伦茨大学'
- en: (1994)
  id: totrans-3875
  prefs: []
  type: TYPE_NORMAL
  zh: (1994)
- en: '[28] Shoemaker, P.A.: A note on least-squares learning procedures and classification
    by neural network models. IEEE Transactions on Neural Networks 2(1), 158–160 (1991)'
  id: totrans-3876
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] Shoemaker, P.A.: 关于最小二乘学习程序和神经网络模型分类的注释。IEEE神经网络交易 2(1), 158–160 (1991)'
- en: '[29] Wan, E.: Neural network classification: A Bayesian interpretation. IEEE
    Transactions on Neural Networks 1(4), 303–305 (1990) [30] Weigend, A.S., Rumelhart,
    D.E., Huberman, B.A.: Generalization by weightelimination with application to
    forecasting. In: Lippmann, R.P., Moody, J.E.,'
  id: totrans-3877
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] Wan, E.: 神经网络分类：贝叶斯解释。IEEE神经网络交易 1(4), 303–305 (1990) [30] Weigend, A.S.,
    Rumelhart, D.E., Huberman, B.A.: 通过权重消除进行泛化并应用于预测。见：Lippmann, R.P., Moody, J.E.,'
- en: Touretzky, D.S. (eds.) Advances in Neural Information Processing Systems, vol.
    3, pp. 875–882. Morgan Kaufmann, San Mateo (1991)
  id: totrans-3878
  prefs: []
  type: TYPE_NORMAL
  zh: Touretzky, D.S.（编）《神经信息处理系统进展》，第3卷，第875–882页。Morgan Kaufmann, San Mateo (1991)
- en: '[31] Weiss, N.A., Hassett, M.J.: Introductory Statistics, 2nd edn. Addison-Wesley,
    Reading (1987)'
  id: totrans-3879
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Weiss, N.A., Hassett, M.J.: 《初级统计学》，第2版。Addison-Wesley, Reading (1987)'
- en: '[32] White, H.: Learning in artificial neural networks: A statistical perspective.
    Neural Computation 1(4), 425–464 (1989)'
  id: totrans-3880
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] White, H.: 在人工神经网络中的学习：统计视角。神经计算 1(4), 425–464 (1989)'
- en: '[33] Yaeger, L., Lyon, R., Webb, B.: Effective training of a neural network
    character classifier for word recognition. In: Mozer, M.C., Jordan, M.I., Petsche,
    T. (eds.) Advances in Neural Information Processing Systems, vol. 9. MIT Press,
    Cambridge'
  id: totrans-3881
  prefs: []
  type: TYPE_NORMAL
  zh: '[33] Yaeger, L., Lyon, R., Webb, B.: 有效训练神经网络字符分类器以实现单词识别。见：Mozer, M.C., Jordan,
    M.I., Petsche, T.（编）《神经信息处理系统进展》，第9卷。MIT出版社, 剑桥'
- en: (1997)
  id: totrans-3882
  prefs: []
  type: TYPE_NORMAL
  zh: (1997)
- en: 15 Applying Divide And Conquer To Large Scale Pattern Recognition Tasks-
  id: totrans-3883
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15 将分治法应用于大规模模式识别任务-
- en: Jürgen Fritsch1 and Michael Finke2 1 Interactive Systems Laboratories University
    of Karlsruhe Am Fasanengarten 5 76128 Karlsruhe, Germany fritsch@ira.uka.de 2
    Interactive Systems Laboratories Carnegie Mellon University 5000 Forbes Avenue
    Pittsburgh, PA 15213, USA
  id: totrans-3884
  prefs: []
  type: TYPE_NORMAL
  zh: Jürgen Fritsch1 和 Michael Finke2 1 卡尔斯鲁厄大学交互系统实验室 Am Fasanengarten 5 76128 Karlsruhe,
    Germany fritsch@ira.uka.de 2 卡内基梅隆大学交互系统实验室 5000 Forbes Avenue Pittsburgh, PA
    15213, USA
- en: finkem@cs.cmu.edu http://www.cs.cmu.edu/˜finkem/
  id: totrans-3885
  prefs: []
  type: TYPE_NORMAL
  zh: finkem@cs.cmu.edu http://www.cs.cmu.edu/˜finkem/
- en: Abstract. Rather than presenting a specific trick, this paper aims at providing
    a methodology for large scale, real-world classification tasks involving thousands
    of classes and millions of training patterns. Such problems arise in speech recognition,
    handwriting recognition and speaker or writer identification, just to name a few.
    Given the typically very large number of classes to be distinguished, many approaches
    focus on parametric methods to independently estimate class conditional likelihoods.
  id: totrans-3886
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要。与其呈现特定的技巧，本文旨在提供一种针对大规模、现实世界分类任务的方法论，这些任务涉及数千个类别和数百万个训练模式。这类问题出现在语音识别、手写识别以及说话者或写作者识别等领域，仅举几例。考虑到通常需要区分的类别数量非常庞大，许多方法侧重于参数方法，以独立估计类别条件概率。
- en: In contrast, we demonstrate how the principles of modularity and hierarchy can
    be applied to directly estimate posterior class probabilities in a connectionist
    framework. Apart from offering better discrimination capability, we argue that
    a hierarchical classification scheme is crucial in tackling the above mentioned
    problems. Furthermore, we discuss training issues that have to be addressed when
    an almost infinite amount of training data is available.
  id: totrans-3887
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，我们展示了如何将模块化和层次结构的原则应用于在连接主义框架中直接估计后验类别概率。除了提供更好的区分能力外，我们认为层次分类方案在解决上述提到的问题时至关重要。此外，我们讨论了当可用训练数据几乎无限时需要解决的训练问题。
- en: 15.1 Introduction
  id: totrans-3888
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.1 引言
- en: The majority of contributions in the field of neural computation deal with relatively
    small datasets and, in case of classification tasks, with a relatively small number
    of classes to be distinguished. Representatives of such problems include the UCI
    machine learning database [16] and the Proben [20] benchmark set for learning
    algorithms. Research concentrates on aspects such as missing data, model selection,
    regularization, overfitting vs. generalization and the bias/variance trade-off.
    Over the years, many methods and 'tricks' have been developed to optimally learn
    and generalize when only a limited amount of data is available.
  id: totrans-3889
  prefs: []
  type: TYPE_NORMAL
  zh: 神经计算领域的大多数贡献处理相对较小的数据集，并且在分类任务中区分的类别数量相对较少。这类问题的代表包括UCI机器学习数据库[16]和Proben[20]学习算法基准集。研究集中于缺失数据、模型选择、正则化、过拟合与泛化以及偏差/方差权衡等方面。多年来，已经开发出许多方法和“技巧”，以便在数据量有限时进行最佳学习和泛化。
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-3890
  prefs: []
  type: TYPE_NORMAL
  zh: '- 先前发表于：Orr, G.B. 和 Müller, K.-R. (编)：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-3891
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0 (1998)。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    311–338, 2012.'
  id: totrans-3892
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon 等（编）：NN：行业技巧，第2版，LNCS 7700，第311–338页，2012。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-3893
  prefs: []
  type: TYPE_NORMAL
  zh: -c 施普林格出版社 柏林 海德堡 2012
- en: On the other hand, many problems in human computer interaction (HCI) such as
    speech and handwriting recognition, lipreading and speaker and writer identification
    require comparably large training databases and also often exhibit a large number
    of classes to be discriminated, such as (context-dependent) phones, letters and
    individual speakers or writers. For example, in state-of-the-art large vocabulary
    continuous speech recognition, we are typically faced with an inventory of several
    thousand basic acoustic units and training databases consisting of several millions
    of preprocessed speech patterns. There is only a limited amount of publications
    available on the sometimes very different problems concerning the choice of learning
    machines and training algorithms for such tasks and datasets.
  id: totrans-3894
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，人机交互（HCI）中的许多问题，如语音和手写识别、唇读以及说话者和写作者识别，需要相对较大的训练数据库，并且通常还需要区分大量类别，例如（上下文依赖）音素、字母以及个别说话者或写作者。例如，在最先进的大词汇量连续语音识别中，我们通常面临数千个基本声学单元的库存和包含数百万个预处理语音模式的训练数据库。关于这些任务和数据集的学习机器和训练算法选择的某些问题，现有文献相对较少。
- en: This article addresses exactly the latter kind of learning tasks and provides
    a principled approach to large scale classification problems, exemplifying it
    on the problem of connectionist speech recognition. Our approach is grounded on
    the powerful *divide and conquer* paradigm that traditionally has always been
    applied to problems of rather large size. We argue that a hierarchical approach
    that modularizes classification tasks is crucial in applying statistical estimators
    such as artificial neural networks. In that respect, this paper presents not just
    a single 'trick of the trade', it offers a methodology for large scale classification
    tasks. Such tasks have traditionally been addressed by building generative models
    rather than focusing on the prediction of posteriors without making strong assumptions
    on the distribution of the input.
  id: totrans-3895
  prefs: []
  type: TYPE_NORMAL
  zh: 本文正是针对后者的学习任务，提供了一种针对大规模分类问题的原则性方法，并在连接主义语音识别问题上进行了实例说明。我们的方法基于强大的*分而治之*范式，这一范式传统上一直应用于相对较大规模的问题。我们认为，模块化分类任务的层次化方法对于应用统计估计器（如人工神经网络）至关重要。在这方面，本文不仅提出了一种单一的“行业诀窍”，而是为大规模分类任务提供了一种方法论。这类任务传统上是通过构建生成模型来解决的，而不是专注于在对输入分布没有强假设的情况下预测后验。
- en: The remainder of the paper is organized as follows. Section 2 presents the general
    approach to soft hierarchical classification. Section 3 then discusses methods
    to design the topology of hierarchical classifiers - a task that is of increasing
    importance when dealing with large numbers of classes. Finally, section 4 demonstrates
    in detail the application of hierarchical classification to connectionist statistical
    speech recognition. Section 5 concludes this paper with a summary.
  id: totrans-3896
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：第 2 节介绍软层次分类的一般方法。第 3 节讨论设计层次分类器拓扑的方法——这一任务在处理大量类别时越来越重要。最后，第 4
    节详细演示了层次分类在连接主义统计语音识别中的应用。第 5 节总结了本文内容。
- en: 15.2 Hierarchical Classification
  id: totrans-3897
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.2 层次分类
- en: Consider the task of classifying patterns x as belonging to one of N classes
    ωk.
  id: totrans-3898
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑将模式 x 分类为 N 类 ωk 的任务。
- en: Given that we have access to the class conditional probability densities p(x|ωk),
  id: totrans-3899
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们可以获取类条件概率密度 p(x|ωk)，
- en: Bayes theory states that the optimal decision should be based on the a-posteriori
    probabilities
  id: totrans-3900
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯理论指出，最佳决策应基于后验概率。
- en: $$p(\omega_{k}|\mathbf{x})={\frac{p(\mathbf{x}|\omega_{k})p(\omega_{k})}{\sum_{i}p(\mathbf{x}|\omega_{i})p(\omega_{i})}}$$
  id: totrans-3901
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(\omega_{k}|\mathbf{x})={\frac{p(\mathbf{x}|\omega_{k})p(\omega_{k})}{\sum_{i}p(\mathbf{x}|\omega_{i})p(\omega_{i})}}$$
- en: .
  id: totrans-3902
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: Given that equal risks are associated with all possible misclassifications,
    the optimal decision is to choose the class with maximum a-posteriori probability
    given a specific pattern x. Two distinct approaches have to be considered when
    applying Bayes theory to a learning from examples task with generally unknown
    distributions. In the first approach, one tries to estimate class-conditional
    likelihoods p(x|ωk) and prior probabilities p(ωk) from a labeled dataset which
    are then used to calculate posterior probabilities according to Bayes rule. In
    principle, this approach can be applied to tasks with an arbitrary large number
    of classes since the class-conditional likelihoods can be estimated independently.
    However, such an approach focuses on the modeling of the class-conditional densities.
    For classification accuracy however, it is more important to model class boundaries.
  id: totrans-3903
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有可能的错误分类都关联着相同的风险，最佳决策是选择给定特定模式 x 的最大后验概率类。在将贝叶斯理论应用于一般未知分布的示例学习任务时，必须考虑两种不同的方法。在第一种方法中，试图从标记数据集中估计类条件似然
    p(x|ωk) 和先验概率 p(ωk)，然后根据贝叶斯规则计算后验概率。从原则上讲，该方法可以应用于任意大量类别的任务，因为类条件似然可以独立估计。然而，这种方法主要关注类条件密度的建模。对于分类准确性而言，更重要的是建模类边界。
- en: The second approach accommodates this perspective by directly estimating posterior
    class probabilities from datasets. It was shown (e. g. [6]) that a large class
    of artificial neural networks such as multi-layer perceptrons and recurrent neural
    networks can be trained to approximate posterior class probabilities.
  id: totrans-3904
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法通过直接从数据集中估计后验类概率来适应这一观点。研究表明（例如 [6]），多层感知器和递归神经网络等大量人工神经网络可以训练以近似后验类概率。
- en: The degree of accuracy of the approximation however depends on many factors,
    among them the plasticity of the network. Comparing the two approaches, the discriminative
    power of methods that estimate posterior probabilities directly is generally higher,
    resulting in better classification accuracy especially when the class-conditional
    distributions are very complex. This fact (among others) explains the success
    and popularity of neural network classifiers on many learning from examples tasks.
  id: totrans-3905
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，近似的准确度依赖于许多因素，其中包括网络的可塑性。比较这两种方法，直接估计后验概率的方法的判别能力通常更高，导致更好的分类准确度，尤其是在类别条件分布非常复杂时。这一事实（以及其他因素）解释了神经网络分类器在许多示例学习任务中的成功和受欢迎程度。
- en: 'However, when the number of classes to be distinguished increases to say several
    thousand, neural network estimators of posterior probabilities fail to provide
    good approximations mainly because of two reasons: First, real-world problems
    involving such a large number of classes often exhibit an extremely non-uniform
    distribution of priors, see chapter 14. Many learning algorithms for neural networks
    (especially stochastic on-line gradient descent) have difficulties with non-uniformly
    distributed classes. Particularly the distribution of posteriors of infrequent
    classes tend to be approximated poorly. Second, and more important, one of the
    prerequisites for training neural networks to estimate posteriors, the 1-out-of-N
    coding of training targets, implies that the number of output neurons matches
    the number of classes. It is unfeasible to train a neural network with thousands
    of output neurons. Also, with increasing number of classes, the complexity of
    the optimum discriminant functions also increases and the potential for conflicts
    between classes grows. Thus, from our point of view, typical monolithic neural
    network classifiers are not applicable because of their limitation to tasks with
    relatively few classes.'
  id: totrans-3906
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当需要区分的类别数量增加到数千时，神经网络后验概率的估计通常无法提供良好的近似，这主要有两个原因：首先，涉及如此大量类别的真实世界问题通常表现出极不均匀的先验分布，见第14章。许多神经网络学习算法（特别是随机在线梯度下降）在处理非均匀分布的类别时存在困难。特别是，稀有类别的后验分布往往被近似得很差。第二，更重要的是，训练神经网络以估计后验的一个先决条件是1-out-of-N编码的训练目标，这意味着输出神经元的数量必须与类别数量匹配。训练一个具有数千个输出神经元的神经网络是不可行的。此外，随着类别数量的增加，最佳判别函数的复杂性也增加，类别间冲突的潜力也随之增加。因此，从我们的角度来看，典型的单一神经网络分类器由于其对相对少数类别任务的限制而不适用。
- en: 15.2.1 Decomposition Of Posterior Probabilities
  id: totrans-3907
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.2.1 后验概率的分解
- en: Applying the principle of *divide and conquer*, we can break down the task of
    discriminating between thousands of classes into a hierarchical structure of many
    smaller classification tasks of controlled size. This idea underlies the approaches
    to decision tree architectures [5, 21, 23]. Decision trees classify input patterns
    by asking categorical questions at each internal node. Depending on the answer
    to these questions a single path is followed to one of the child nodes and the
    process repeats until a leaf node is reached and a (winner) class label is emitted.
    Therefore, decision tree classifiers can only supply us with hard decisions.
  id: totrans-3908
  prefs: []
  type: TYPE_NORMAL
  zh: 应用*分而治之*的原则，我们可以将区分成千上万类别的任务分解为多个受控规模的小分类任务的层次结构。这一思想是决策树架构方法的基础[5, 21, 23]。决策树通过在每个内部节点提出分类问题来对输入模式进行分类。根据这些问题的答案，沿着一条路径走向子节点，并重复该过程，直到达到叶节点并输出一个（胜者）类别标签。因此，决策树分类器只能为我们提供明确的决策。
- en: No information about the confusability of a specific input pattern is given
    to us. Rather, we are often interested in the posterior class probabilities because
    we wish to have a measure of the ambiguity of a decision. Furthermore, we are
    sometimes required to feed a measure of the degree of membership for all
  id: totrans-3909
  prefs: []
  type: TYPE_NORMAL
  zh: 关于特定输入模式的混淆性没有提供给我们任何信息。相反，我们通常对后验类别概率感兴趣，因为我们希望有一个决策模糊度的度量。此外，有时我们需要为所有类别提供一个成员资格程度的度量。
- en: '![308_image_0.png](308_image_0.png)'
  id: totrans-3910
  prefs: []
  type: TYPE_IMG
  zh: '![308_image_0.png](308_image_0.png)'
- en: Fig. 15.1. Hierarchical decomposition of posteriors
  id: totrans-3911
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.1. 后验的层次分解
- en: potential classes into a superior decision making process. As we will see in
    section 4, statistical speech recognition is a typical example for the latter
    scenario.
  id: totrans-3912
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在类别进入一个更优的决策过程。正如我们将在第4节中看到的，统计语音识别是后一种情况的典型例子。
- en: Adhering to the *divide and conquer* approach but generalizing the decision
    tree framework, the statistical method of factoring posteriors can be applied
    to design *soft* classification trees [24, 25]. For now, we assume, that optimal
    posterior probabilities are available. Let S be a (possibly large) set of classes
    ωk to be discriminated. Consider we have a method at our disposition which gives
    us a partitioning of S into M disjoint and non-empty subsets Si such that members
    of Si are almost never confused with members of Sj (∀j = i). A particular class
    ωk will now be a member of S and exactly one of the subsets Si. Therefore, we
    can rewrite the posterior probability of class ωk as a joint probability of the
    class and the corresponding subset Si and factor it according to
  id: totrans-3913
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循*分治*方法，但对决策树框架进行泛化，统计后验概率分解的方法可以用于设计*软*分类树 [24, 25]。目前，我们假设可以获得最佳后验概率。设 S
    为一组（可能很大）的待区分类别 ωk。假设我们有一种方法，可以将 S 划分为 M 个不相交且非空的子集 Si，使得 Si 的成员几乎从不与 Sj 的成员混淆（∀j
    = i）。特定类别 ωk 现在将是 S 的一个成员，并且正好属于一个子集 Si。因此，我们可以将类别 ωk 的后验概率重写为类别与相应子集 Si 的联合概率，并根据以下公式进行分解：
- en: $p(\omega_{k}|\mathbf{x})=p(\omega_{k},S_{i}|\mathbf{x})$ with $\omega_{k}\in
    S_{i}$, $=p(S_{i}|\mathbf{x})\ p(\omega_{k}|S_{i},\mathbf{x})$.
  id: totrans-3914
  prefs: []
  type: TYPE_NORMAL
  zh: $p(\omega_{k}|\mathbf{x})=p(\omega_{k},S_{i}|\mathbf{x})$，其中 $\omega_{k}\in
    S_{i}$，$=p(S_{i}|\mathbf{x})\ p(\omega_{k}|S_{i},\mathbf{x})$。
- en: Thus, the global task of discriminating between all the classes in S has been
    converted into (1) discriminating between subsets Si and (2) independently discriminating
    between the classes ωk remaining within each of the subsets Si.
  id: totrans-3915
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 S 中区分所有类别的全球任务已被转换为（1）区分子集 Si 和（2）独立区分每个子集 Si 中剩余的类别 ωk。
- en: Recursively repeating this process yields a hierarchical tree-organized structure
    (Fig. 15.1).
  id: totrans-3916
  prefs: []
  type: TYPE_NORMAL
  zh: 递归地重复这个过程会产生一个层次树状结构（图 15.1）。
- en: Note, that the number of subclasses Si of each node does not need to be constant
    throughout the classifier tree and might be subject to optimization during the
    tree design phase. In order to compute the posterior probability for a specific
    class, we have to follow the path from root node to the leaf corresponding to
    the class in question, multiplying all the conditional posteriors along the way.
    Both the design of the tree structure (*divide*) and the estimation and multiplication
    (*conquer*) of conditional posteriors at each node are important aspects in this
    architecture, that have to be considered thoroughly because in practice, only
    approximations to the conditional posteriors are available.
  id: totrans-3917
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个节点的子类 Si 的数量不需要在整个分类器树中保持不变，可能会在树设计阶段进行优化。为了计算特定类别的后验概率，我们必须沿着从根节点到与所询问类别相对应的叶子的路径，逐步相乘所有的条件后验概率。树结构的设计（*分治*）以及每个节点条件后验概率的估计和相乘（*征服*）都是该架构中重要的方面，必须彻底考虑，因为在实践中，只有条件后验的近似值可用。
- en: 15.2.2 Hierarchical Interpretation
  id: totrans-3918
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.2.2 层次解释
- en: The presented architecture can be interpreted as a probability mass distribution
    device. At the root node, an initial probability mass of 1 is fed into the architecture.
    At each node, the incoming probability mass is multiplied by the respective conditional
    posterior probabilities and fed into the child nodes. Eventually, the probability
    mass is distributed among all the leaves (classes) rendering their posterior probabilities.
    In contrast, classifier trees are mostly used as hard-switching devices, where
    only a single path from root node to one of the leaves is taken.
  id: totrans-3919
  prefs: []
  type: TYPE_NORMAL
  zh: 所呈现的架构可以解释为一种概率质量分配设备。在根节点，初始概率质量 1 被输入到架构中。在每个节点，传入的概率质量会乘以相应的条件后验概率，并传递到子节点。最终，概率质量在所有叶子（类别）之间分配，从而呈现它们的后验概率。相比之下，分类器树通常用作硬切换设备，从根节点到其中一个叶子只有一条路径。
- en: A hierarchical decomposition of posterior probabilities through a soft classification
    tree offers several advantages. If one of the nodes in the tree, for example the
    root node fails to provide good estimates of conditional posteriors, a hard decision
    tree will produce many classification errors. In a soft classification tree, such
    shortcomings will influence the decision process less dramatically. Also, recovery
    from errors is often possible through a superior decision process.
  id: totrans-3920
  prefs: []
  type: TYPE_NORMAL
  zh: 通过软分类树对后验概率进行的分层分解提供了几种优势。如果树中的一个节点，例如根节点，无法提供良好的条件后验估计，硬决策树将产生许多分类错误。在软分类树中，这种缺陷对决策过程的影响较小。此外，通常可以通过更优的决策过程来恢复错误。
- en: Another aspect of soft classification trees that can be exploited for various
    purposes is the sum-to-unity property observable in any horizontal cross-section
    at any level of the tree. The tree can be cut off at a certain level and still
    be used as a soft classification tree that computes posterior class probabilities.
    This is equivalent to creating a new (smaller) set of classes by clustering and
    merging the original classes according to the tree topology. In general, the resulting
    classification task will be easier to solve than the original one.
  id: totrans-3921
  prefs: []
  type: TYPE_NORMAL
  zh: 软分类树的另一个可以用于多种目的的方面是，在树的任何层次上观察到的和为一性质。树可以在某一层次上被截断，仍然可以作为软分类树使用，以计算后验类概率。这相当于通过根据树的拓扑聚类和合并原始类来创建一个新的（较小的）类集合。一般来说，结果分类任务将比原始任务更容易解决。
- en: Related to the sum-to-unity property of cross-sections is that the partial posteriors
    computed on a path from the root node to a leaf are decreasing monotonically.
    This in turn allows to close paths whenever a suitable threshold is reached, pruning
    whole subtrees with classes that would otherwise receive posteriors smaller than
    the threshold. This property yields the possibility to smoothly trade off classification
    accuracy against computational complexity. In the limit, when only a single path
    with highest conditional posterior is followed, the soft classification tree transmutes
    into a hard decision tree.
  id: totrans-3922
  prefs: []
  type: TYPE_NORMAL
  zh: 与交叉截面的和为一性质相关的是，从根节点到叶子的部分后验概率是单调递减的。这反过来允许在达到合适的阈值时关闭路径，从而修剪掉会得到低于阈值的后验概率的整个子树。这个性质使得可以在分类准确性与计算复杂性之间平滑地权衡。在极限情况下，当仅遵循一条具有最高条件后验概率的路径时，软分类树转变为硬决策树。
- en: 15.2.3 Estimation Of Conditional Node Posteriors
  id: totrans-3923
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.2.3 条件节点后验的估计
- en: Given a hierarchical decomposition of posterior class probabilities, it remains
    to instantiate the tree nodes with estimators for the required conditional posteriors.
  id: totrans-3924
  prefs: []
  type: TYPE_NORMAL
  zh: 给定后验类概率的分层分解，接下来需要用所需条件后验的估计器来实例化树节点。
- en: Conditioning a posterior on a subset of classes Si can be accomplished by restricting
    the training set of the corresponding learning device to the patterns with a class
    label from Si. According to this setting, the available training data in each
    node is distributed among all its child nodes according to the class partitioning.
  id: totrans-3925
  prefs: []
  type: TYPE_NORMAL
  zh: 对于类的子集 Si 进行后验条件化可以通过将相应学习设备的训练集限制为具有 Si 中类标签的模式来实现。根据这种设置，每个节点中可用的训练数据根据类划分分布在所有子节点之间。
- en: While the root node receives all available training data, nodes further down
    the tree receive less data than their predecessors. On the other hand, specialization
    increases from root node to leaves. This fact has important consequences on learning
    speed and model selection when training whole hierarchies.
  id: totrans-3926
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然根节点接收所有可用的训练数据，但树下方的节点接收的数据少于它们的前辈。另一方面，从根节点到叶子的专业化程度在增加。这一事实对在训练整个层次结构时的学习速度和模型选择有重要影响。
- en: One of the important issues in hierarchical decompositions of posterior probabilities
    are the unavoidable inaccuracies of practical estimators for the conditional posteriors
    that have to be provided in each tree node. Neural networks can only be trained
    to *approximate* the true distribution of posterior class probabilities and the
    degree of accuracy depends on both the inherent difficulty of the task as given
    by the training set and the network structure and training schedule being used.
    Inaccurate approximations to the true distribution of posteriors hurt most in
    the upper layers of a classification tree - a fact that has to be taken into account
    by tree design procedures, which we will discuss next.
  id: totrans-3927
  prefs: []
  type: TYPE_NORMAL
  zh: 在后验概率的分层分解中，一个重要问题是每个树节点中必须提供的条件后验的实际估计器不可避免的误差。神经网络只能被训练来*近似*后验类别概率的真实分布，准确度的高低依赖于训练集所给出的任务固有难度、网络结构以及所采用的训练方案。对后验真实分布的不准确近似在分类树的上层影响最大——这一点必须在树设计过程中考虑，我们将在接下来讨论。
- en: 15.3 Classifier Tree Design
  id: totrans-3928
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3 分类器树设计
- en: When it comes to the design of soft classifier trees, or equivalently to the
    design of hierarchical decompositions of class posteriors, the choice of algorithm
    depends mostly on the number of initial classes. We will first discuss optimal
    tree structures before we will turn to heuristic design algorithms necessary when
    dealing with the large number of classes that we have to deal with.
  id: totrans-3929
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计软分类树时，或等同于设计类别后验的分层分解时，算法的选择主要取决于初始类别的数量。我们将首先讨论最佳树结构，然后再转向在处理我们必须面对的大量类别时所需的启发式设计算法。
- en: 15.3.1 Optimality
  id: totrans-3930
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3.1 最优性
- en: The optimal soft classification tree for a given task and given type and structure
    of estimators for the conditional node posteriors is the one which results in
    minimum classification error in the Bayes setting. If all the node classifiers
    would compute the true conditional posteriors, the tree structure would have no
    influence on the classifier performance because any kind of factoring (through
    any kind of tree structure) yields an *exact* decomposition of the class posteriors.
    However, in practice, approximation errors of node classifiers render the choice
    of tree structure an important issue. For small numbers of classes, the optimal
    tree can in principle be found by exhaustively training and testing all possible
    partitionings for a particular node (starting with the root node) and chosing
    the one that gives the highest recognition accuracy. However, even if restricting
    the tree structure to binary branching nodes and balanced partitionings, the number
    K of partitionings that have to be examined at the root node
  id: totrans-3931
  prefs: []
  type: TYPE_NORMAL
  zh: 针对给定任务和条件节点后验的给定类型和结构的最佳软分类树是能够在贝叶斯设置中最小化分类错误的那一棵树。如果所有节点分类器都能够计算真实的条件后验，树的结构将不会影响分类器的性能，因为任何类型的因子化（通过任何树结构）都会导致类别后验的*精确*分解。然而，实际上，节点分类器的近似误差使得树结构的选择成为一个重要问题。对于少量类别，理论上可以通过彻底训练和测试某一特定节点（从根节点开始）所有可能的划分，找到最佳树，并选择能够提供最高识别准确度的那一个。然而，即使限制树结构为二叉分支节点和均衡划分，必须在根节点检查的划分数量K
- en: $$K={\frac{N!}{({\frac{N!}{2}}!)^{2}}}$$
  id: totrans-3932
  prefs: []
  type: TYPE_NORMAL
  zh: $$K={\frac{N!}{({\frac{N!}{2}}!)^{2}}}$$
- en: quickly brings this algorithm to its limits, even for a moderate number of classes
    N. Therefore, we have to consider heuristics to derive near optimal tree structures.
    For example, one valid possibility is to assume that the accuracy of achievable
    approximations to the true posteriors is related to the separability of the corresponding
    sets of classes.
  id: totrans-3933
  prefs: []
  type: TYPE_NORMAL
  zh: 很快就会使该算法达到极限，即使对于适度数量的类别N。因此，我们必须考虑启发式方法来推导近似最佳的树结构。例如，一种有效的可能性是假设可实现的后验真实近似的准确性与对应类别集合的可分离性相关。
- en: 15.3.2 Prior Knowledge
  id: totrans-3934
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3.2 先验知识
- en: Following the above mentioned guideline, prior knowledge about the task in question
    can often be applied to hierarchically partition the global set of classes into
    reasonable subsets. The goal is to partition the remaining set of classes in a
    way that intuitively maximizes the separability of the subsets. For example, given
    a set of phones in a speech recognizer, a reasonable first partitioning would
    be to build subsets consisting of voiced and unvoiced phones. In larger speech
    recognition systems where we have to distinguish among multi-state contextdependent
    phones, prior knowledge such as state and context identity can be used as splitting
    criterion. In tasks such as speaker or writer identification, features such as
    gender or age are potential candidates for splitting criteria.
  id: totrans-3935
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述指导原则，关于特定任务的先验知识通常可以应用于将全局类别集合分层划分为合理的子集。目标是以直观地最大化子集的分离性为方式来划分剩余的类别集合。例如，在语音识别器中，给定一组音素，合理的第一次划分将是构建由有声和无声音素组成的子集。在较大的语音识别系统中，我们必须区分多状态上下文相关音素时，诸如状态和上下文身份的先验知识可以作为划分标准。在说话者或书写者识别等任务中，性别或年龄等特征是潜在的划分标准候选者。
- en: The advantage of such knowledge driven decompositions is a fast tree design
    phase which is a clear superiority of this approach when dealing with large numbers
    of classes. However, this method for the design of hierarchical classifiers is
    subjective and error prone. Two experts in a specific field might disagree strongly
    on what constitutes a reasonable hierarchy. Furthermore, it is not always the
    case that *reasonable* partitionings yield good separability of subsets. Expert
    knowledge can be misleading.
  id: totrans-3936
  prefs: []
  type: TYPE_NORMAL
  zh: 这种知识驱动的分解的优点在于快速的树设计阶段，这是处理大量类别时该方法的明显优势。然而，这种设计层次分类器的方法是主观且容易出错的。两个特定领域的专家可能在什么构成合理层次上有强烈的分歧。此外，并非总是合理的划分会带来良好的子集分离性。专家知识可能具有误导性。
- en: 15.3.3 Confusion Matrices
  id: totrans-3937
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3.3 混淆矩阵
- en: In case the number of classes is small enough to allow the training of a single
    classifier, the design of a soft classifier tree can be based on the confusion
    matrix of the trained monolithic classifier. Indicating the confusability of each
    pair of classes, the confusion matrix yields relatively good measures of the separability
    of pairs of classes. This information can be exploited for designing a tree structure
    using a clustering algorithm. For instance, we can define the following
  id: totrans-3938
  prefs: []
  type: TYPE_NORMAL
  zh: 如果类别数量足够小，允许训练一个单一的分类器，则软分类器树的设计可以基于训练好的单体分类器的混淆矩阵。混淆矩阵指示每对类别的混淆性，提供了相对良好的类别对分离性的度量。这些信息可以被利用来使用聚类算法设计树结构。例如，我们可以定义以下内容。
- en: (symmetric) distance measure between two disjunct sets of classes Sk and Sl
  id: totrans-3939
  prefs: []
  type: TYPE_NORMAL
  zh: （对称）距离度量在两个不相交的类别集合Sk和Sl之间。
- en: $d(S_{k},S_{l})=-\sum_{\omega_{i}\in S_{k}}\sum_{\omega_{j}\in S_{l}}C(\omega_{i},\omega_{j}|T)+C(\omega_{j},\omega_{i}|T)$.
  id: totrans-3940
  prefs: []
  type: TYPE_NORMAL
  zh: $d(S_{k},S_{l})=-\sum_{\omega_{i}\in S_{k}}\sum_{\omega_{j}\in S_{l}}C(\omega_{i},\omega_{j}|T)+C(\omega_{j},\omega_{i}|T)$。
- en: where C(ωi, ωj|T ) denotes the number of times class ωi is confused with class
    ωj as measured on a set of labeled patterns T . The distance d(Sk, Sl) can now
    be used as a replacement for the usual Euclidean distance measure in a standard
    bottom-up clustering algorithm. Unfortunately, once the number of classes increases
    to several thousand, training of a monolithic classifier becomes increasingly
    difficult.
  id: totrans-3941
  prefs: []
  type: TYPE_NORMAL
  zh: C(ωi, ωj|T )表示在一组标记模式T上，类别ωi与类别ωj混淆的次数。距离d(Sk, Sl)现在可以用作标准自底向上的聚类算法中的通常欧几里得距离度量的替代。不幸的是，一旦类别数量增加到几千，训练一个单体分类器变得越来越困难。
- en: 15.3.4 Agglomerative Clustering
  id: totrans-3942
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3.4 聚合聚类
- en: Assuming that separability of classes correlates with approximation accuracy
    of estimators for the posterior class probabilities, we can go further and assume
    that separability of classes can be measured by a suitable distance between the
    class conditional distributions in feature space. We already introduced such a
    distance measure in form of the elements of a class confusion matrix. Other, computationally
    less expensive distance measures would be the Euclidean distance between class
    means or the Mahalanobis distance between the classes second order statistics.
    Irrespective of the chosen distance measure, the goal always is to group the set
    of classes in a way that results in maximum inter- and minimum intra-group distances.
    Solutions to this problem are known as agglomerative clustering algorithms and
    a large pool of variations of the basic algorithm is available in the literature
    [7].
  id: totrans-3943
  prefs: []
  type: TYPE_NORMAL
  zh: 假设类别的可分性与后验类别概率估计的近似精度相关，我们可以进一步假设类别的可分性可以通过特征空间中类别条件分布之间的适当距离来衡量。我们已经介绍了这种距离度量，作为类别混淆矩阵的元素。其他计算上更便宜的距离度量可以是类别均值之间的欧几里得距离或类别二阶统计量之间的马氏距离。不论选择哪种距离度量，目标始终是以最大化组间距离和最小化组内距离的方式对类别集合进行分组。对此问题的解决方案被称为聚合聚类算法，文献中提供了大量基本算法的变种
    [7]。
- en: 15.4 Application To Speech Recognition
  id: totrans-3944
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4 应用于语音识别。
- en: 'In this section, we will demonstrate the main ideas and benefits of the hierarchical
    classifier approach on the task of large vocabulary continuous speech recognition
    (LVCSR). More specifically, we will focus on acoustic modeling for statistical
    speech recognition using hidden Markov models (HMM) [27]. To give an impression
    of the complexity of such a task: training databases typically consist of tens
    of millions of speech patterns, the number of acoustic classes being distinguished
    ranges from ca. 50 (monophones) to over 20000 (context-dependent polyphones).'
  id: totrans-3945
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示层次分类器方法在大词汇量连续语音识别 (LVCSR) 任务中的主要思想和优势。更具体地，我们将重点关注使用隐马尔可夫模型 (HMM)
    的统计语音识别的声学建模 [27]。为了给出此类任务复杂性的印象：训练数据库通常由数千万个语音模式组成，所区分的声学类别数量从大约 50 个（单音节）到超过
    20000 个（上下文相关多音节）不等。
- en: 15.4.1 Statistical Speech Recognition
  id: totrans-3946
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4.1 统计语音识别。
- en: The basic statistical entity in HMM based speech recognition is the posterior
    probability of word sequences W1*,...,W*N given a sequence of acoustic observations
    X1*,...,* XM and a set of model parameters Θ
  id: totrans-3947
  prefs: []
  type: TYPE_NORMAL
  zh: HMM 基于语音识别的基本统计实体是给定声学观察序列 \(X_{1}^*,...,X_{M}^*\) 和一组模型参数 \(\Theta\) 的词序列 \(W_{1}^*,...,W_{N}^*\)
    的后验概率。
- en: $P(W_{1},\ldots,W_{N}|\bf{X_{1}},\ldots,\bf{X_{M}},\bf{\Theta})$
  id: totrans-3948
  prefs: []
  type: TYPE_NORMAL
  zh: \(P(W_{1},\ldots,W_{N}|\bf{X_{1}},\ldots,\bf{X_{M}},\bf{\Theta})\)
- en: During training, we are seeking parameters Θ that maximize this probability
    on the training data
  id: totrans-3949
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们寻找能够在训练数据上最大化此概率的参数 \(\Theta\)。
- en: $${\hat{\mathbf{\Theta}}}=\operatorname{arg\,max}_{\mathbf{\Theta}}\prod_{t=1}^{T}P(W_{1},\ldots,W_{N(t)}|\mathbf{X_{1}},\ldots,\mathbf{X_{M(t)}},\mathbf{\Theta})$$
  id: totrans-3950
  prefs: []
  type: TYPE_NORMAL
  zh: $${\hat{\mathbf{\Theta}}}=\operatorname{arg\,max}_{\mathbf{\Theta}}\prod_{t=1}^{T}P(W_{1},\ldots,W_{N(t)}|\mathbf{X_{1}},\ldots,\mathbf{X_{M(t)}},\mathbf{\Theta})$$
- en: and during recognition, we want to find the sequence of words that maximizes
    this probability for a given acoustic observation and fixed model parameters Θ
  id: totrans-3951
  prefs: []
  type: TYPE_NORMAL
  zh: 在识别过程中，我们希望找到最大化给定声学观察和固定模型参数 \(\Theta\) 的概率的词序列。
- en: $\hat{W}_{1},\ldots,\hat{W}_{N}=\arg\max_{W_{1},\ldots,W_{N}}P(W_{1},\ldots,W_{N}|\mathbf{X_{1}},\ldots,\mathbf{X_{M}},\mathbf{\Theta})$,
    $\mathbf{\Theta}$ is a linear differential operator for which $\mathbf{\Theta}$
    is the vector-valued additive
  id: totrans-3952
  prefs: []
  type: TYPE_NORMAL
  zh: \(\hat{W}_{1},\ldots,\hat{W}_{N}=\arg\max_{W_{1},\ldots,W_{N}}P(W_{1},\ldots,W_{N}|\mathbf{X_{1}},\ldots,\mathbf{X_{M}},\mathbf{\Theta})\)，\(\mathbf{\Theta}\)
    是线性微分算子，其中 \(\mathbf{\Theta}\) 是向量值的加性。
- en: In order to simplify the process of maximizing the posterior probability of
    word sequences, Bayes rule is usually applied
  id: totrans-3953
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化最大化词序列后验概率的过程，通常应用贝叶斯规则。
- en: $P(W_{1},\ldots,W_{N}|\mathbf{X_{1}},\ldots,\mathbf{X_{M}})=\frac{P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|W_{1},\ldots,W_{N})\;P(W_{1},\ldots,W_{N})}{P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}})}$.
  id: totrans-3954
  prefs: []
  type: TYPE_NORMAL
  zh: \(P(W_{1},\ldots,W_{N}|\mathbf{X_{1}},\ldots,\mathbf{X_{M}})=\frac{P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|W_{1},\ldots,W_{N})\;P(W_{1},\ldots,W_{N})}{P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}})}\)。
- en: This rule separates the estimation process into the so called *acoustic model
    (AM)*
  id: totrans-3955
  prefs: []
  type: TYPE_NORMAL
  zh: 这个规则将估计过程分为所谓的 *声学模型 (AM)*。
- en: consisting of terms that depend on the acoustic observations X1*,...,* XM and
    the *language model (LM)* consisting of terms that depend only on the sequence
    of words W1*,...,W*N . In this paper we will focus on acoustic modeling using
    connectionist estimators as a typical example of a task involving the discrimination
    of thousands of classes. For a review on other important aspects of LVCSR
  id: totrans-3956
  prefs: []
  type: TYPE_NORMAL
  zh: 由依赖于声学观察X1*,...,*XM的项和仅依赖于单词序列W1*,...,W*N的*语言模型（LM）*组成。在本文中，我们将重点关注使用连接主义估计器的声学建模，作为涉及成千上万类的任务的典型示例。有关LVCSR其他重要方面的综述
- en: such as pronunciation modeling, language modeling and decoding algorithms we
    refer the reader to [27].
  id: totrans-3957
  prefs: []
  type: TYPE_NORMAL
  zh: 例如发音建模、语言建模和解码算法，读者可参见[27]。
- en: The task of acoustic modeling (ignoring the denominator) is to estimate parameters
    ΘAM which maximize
  id: totrans-3958
  prefs: []
  type: TYPE_NORMAL
  zh: 声学建模的任务（忽略分母）是估计参数ΘAM，以最大化
- en: $$P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|W_{1},\ldots,W_{N},\mathbf{\Theta^{A
    M}}).$$
  id: totrans-3959
  prefs: []
  type: TYPE_NORMAL
  zh: $$P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|W_{1},\ldots,W_{N},\mathbf{\Theta^{A
    M}})$$。
- en: Words Wi are modeled as sequences (or graphs) of phone models. The mapping from
    words to phone models is usually accomplished by means of a pronunciation dictionary.
    Phone models in turn are usually modeled as m-state left-to-right hidden Markov
    models (HMM) to capture the temporal and acoustic variability of speech signals.
    The following figure shows the process of converting a sequence of words into
    (1) a pronunciation graph (possibly with pronunciation variants)
  id: totrans-3960
  prefs: []
  type: TYPE_NORMAL
  zh: 单词Wi被建模为音素模型的序列（或图）。单词到音素模型的映射通常通过发音词典完成。音素模型通常被建模为m状态的从左到右的隐马尔可夫模型（HMM），以捕捉语音信号的时间和声学变异性。下图显示了将单词序列转换为（1）发音图（可能包含发音变体）的过程。
- en: and (2) an HMM state graph.
  id: totrans-3961
  prefs: []
  type: TYPE_NORMAL
  zh: 以及（2）HMM状态图。
- en: '![313_image_0.png](313_image_0.png)'
  id: totrans-3962
  prefs: []
  type: TYPE_IMG
  zh: '![313_image_0.png](313_image_0.png)'
- en: '![313_image_1.png](313_image_1.png)'
  id: totrans-3963
  prefs: []
  type: TYPE_IMG
  zh: '![313_image_1.png](313_image_1.png)'
- en: Fig. 15.2. Typical hidden Markov model in speech recognition
  id: totrans-3964
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2. 语音识别中的典型隐马尔可夫模型
- en: In this framework, where word sequences are represented as directed acyclic
    graphs of HMM states, the likelihood of an acoustic observation can be rewritten
    as
  id: totrans-3965
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个框架中，单词序列被表示为HMM状态的有向无环图，声学观察的似然可以重写为
- en: $P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|W_{1},\ldots,W_{N})=\sum_{s_{1},\ldots,s_{M}}P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|s_{1},\ldots,s_{M})\
    p(s_{1},\ldots,s_{M})$
  id: totrans-3966
  prefs: []
  type: TYPE_NORMAL
  zh: $P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|W_{1},\ldots,W_{N})=\sum_{s_{1},\ldots,s_{M}}P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|s_{1},\ldots,s_{M})\
    p(s_{1},\ldots,s_{M})$。
- en: where the summation extends over all possible state sequences s1*,...,s*M in
    the HMM model for the word sequence W1*,...,W*N . In the Viterbi approximation,
    the above likelihood is approximated by the probability of the most likely state
    sequence
  id: totrans-3967
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，求和扩展到HMM模型中单词序列W1*,...,W*N的所有可能状态序列s1*,...,s*M。在维特比近似中，上述似然通过最可能状态序列的概率进行近似。
- en: $P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|W_{1},\ldots,W_{N})\approx\max_{s_{1},\ldots,s_{M}}P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|s_{1},\ldots,s_{M})\
    p(s_{1},\ldots,s_{M})$.
  id: totrans-3968
  prefs: []
  type: TYPE_NORMAL
  zh: $P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|W_{1},\ldots,W_{N})\approx\max_{s_{1},\ldots,s_{M}}P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|s_{1},\ldots,s_{M})\
    p(s_{1},\ldots,s_{M})$。
- en: Given a specific state sequence, the likelihood of the acoustic observations
    given that sequence can be factored as follows
  id: totrans-3969
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个特定的状态序列，给定该序列的声学观察的似然可以因式分解如下。
- en: $$P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|s_{1},\ldots,s_{M})\approx\prod_{i=1}^{M}p(\mathbf{X_{i}}|X_{1},\ldots,X_{i-1},s_{1},\ldots,s_{M})\
    p(s_{1},\ldots,s_{M}).$$
  id: totrans-3970
  prefs: []
  type: TYPE_NORMAL
  zh: $$P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|s_{1},\ldots,s_{M})\approx\prod_{i=1}^{M}p(\mathbf{X_{i}}|X_{1},\ldots,X_{i-1},s_{1},\ldots,s_{M})\
    p(s_{1},\ldots,s_{M})$$。
- en: 'In the application of first-order hidden Markov models to the estimation of
    such likelihoods one usually makes two simplifying assumptions:'
  id: totrans-3971
  prefs: []
  type: TYPE_NORMAL
  zh: 在将一阶隐马尔可夫模型应用于此类似然估计时，通常会做出两个简化假设：
- en: '- Independence of Observations:'
  id: totrans-3972
  prefs: []
  type: TYPE_NORMAL
  zh: '- 观察独立性：'
- en: $P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|s_{1},\ldots,s_{M})\approx\prod_{i=1}^{M}p(\mathbf{X_{i}}|s_{1},\ldots,s_{M})\
    p(s_{1},\ldots,s_{M})$
  id: totrans-3973
  prefs: []
  type: TYPE_NORMAL
  zh: $P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|s_{1},\ldots,s_{M})\approx\prod_{i=1}^{M}p(\mathbf{X_{i}}|s_{1},\ldots,s_{M})\
    p(s_{1},\ldots,s_{M})$。
- en: '- First-order Assumption:'
  id: totrans-3974
  prefs: []
  type: TYPE_NORMAL
  zh: '- 一阶假设：'
- en: $P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|s_{1},\ldots,s_{M})\approx\prod_{i=1}^{M}p(\mathbf{X_{i}}|s_{i})\
    p(s_{i}|s_{i-1})$
  id: totrans-3975
  prefs: []
  type: TYPE_NORMAL
  zh: $P(\mathbf{X_{1}},\ldots,\mathbf{X_{M}}|s_{1},\ldots,s_{M})\approx\prod_{i=1}^{M}p(\mathbf{X_{i}}|s_{i})\
    p(s_{i}|s_{i-1})$。
- en: 15.4.2 Emission And Transition Modeling
  id: totrans-3976
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4.2 发射和转移建模
- en: Mainstream LVCSR systems follow the above approach by modeling emission probability
    distributions p(Xi|si) and transition probabilities p(si|si−1) separately and
    independently. Emission probability distributions are usually modeled using mixture
    densities from the exponential family, such as the mixture of Gaussians
  id: totrans-3977
  prefs: []
  type: TYPE_NORMAL
  zh: 主流LVCSR系统遵循上述方法，通过分别和独立建模发射概率分布p(Xi|si)和转换概率p(si|si−1)。发射概率分布通常使用来自指数族的混合密度建模，例如高斯混合。
- en: $$p(\mathbf{X_{i}}|s_{i})=\sum_{k=1}^{n}\gamma_{k}N_{k}(\mathbf{X_{i}}|s_{i})$$
  id: totrans-3978
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(\mathbf{X_{i}}|s_{i})=\sum_{k=1}^{n}\gamma_{k}N_{k}(\mathbf{X_{i}}|s_{i})$$
- en: 'where the γk denote *mixture coefficients* and the Nk *mixture component densities*
    (here: normal distributions). Transition probabilities on the other hand are modeled
    by simple multinomial probabilities since they are conditioned on a discrete variable
    only (not on the input vector).'
  id: totrans-3979
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，γk表示*混合系数*，Nk表示*混合成分密度*（此处为：正态分布）。另一方面，转换概率则通过简单的多项式概率建模，因为它们仅依赖于离散变量（而非输入向量）。
- en: The advantage of this approach is a decoupled estimation process that separates
    temporal and acoustic modeling. As such, it allows to easily vary HMM
  id: totrans-3980
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点在于解耦的估计过程，分离了时间和声学建模。因此，它允许轻松变更HMM。
- en: state topologies after training in order to modify temporal behavior. For instance,
    state duplication is a popular technique to increase the minimum duration constraint
    in phone models. Having separated emission and transition probability estimation,
    state duplication consists of simply sharing acoustic models among multiple states
    and adapting the transition probabilities.
  id: totrans-3981
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后需要修改时间行为的状态拓扑。例如，状态复制是一种流行技术，用于增加电话模型中的最小持续时间约束。状态复制通过在多个状态之间共享声学模型并调整转换概率来实现，其包含了分离的发射和转换概率估计。
- en: However, the disadvantage of the above approach is a mismatch in the dynamic
    range of emission and transition probabilities. The reason is that transition
    probabilities are modeled separately as multinomial probabilities, constrained
    by the requirement to sum to one. This leads to a dominant role of emission probabilities
    with transition probabilities hardly influencing overall system performance.
  id: totrans-3982
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上述方法的缺点是发射和转换概率的动态范围不匹配。原因在于转换概率作为多项式概率被单独建模，并受到总和为一的约束。这导致发射概率占主导地位，转换概率几乎不影响整体系统性能。
- en: 15.4.3 Phonetic Context Modeling
  id: totrans-3983
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4.3 音位上下文建模
- en: So far we have assumed that only one HMM is required per modeled monophone
  id: totrans-3984
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设每个建模的单音位只需要一个HMM。
- en: (see Fig. 15.2). Since the English language can be modeled by approximately
    45 monophones, one might get the impression that only that number of HMM models
    need to be trained. In practice however, one observes an effect called coarticulation
    that causes large variations in the way specific monophones actually sound, depending
    on their phonetic context.
  id: totrans-3985
  prefs: []
  type: TYPE_NORMAL
  zh: （见图15.2）。由于英语可以通过大约45个单音位建模，人们可能会有这样的印象：只需训练该数量的HMM模型。然而，在实践中，人们观察到一种称为共发音的效应，这会导致特定单音位实际发音的巨大变化，具体取决于其音位上下文。
- en: Usually, explicit modeling of phones in phonetic context yields great gains
    in recognition accuracy. However, it is not immediately clear how to achieve robust
    context-dependent modeling. Consider, for example, so called *triphone* models.
    A
  id: totrans-3986
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，音位在音位上下文中的显式建模会带来识别准确性的巨大提升。然而，如何实现稳健的上下文依赖建模并不立即清晰。例如，考虑所谓的*三音位*模型。
- en: triphone essentially represents the realization of a specific monophone in a
    specific context spanning one phone to the left and right. Assuming an inventory
    of 45 monophones, the number of (theoretically) possible triphones is 453 = 91125.
  id: totrans-3987
  prefs: []
  type: TYPE_NORMAL
  zh: 三音位本质上代表特定单音位在特定上下文中的实现，跨越一个左侧和右侧的音位。假设有45个单音位，(理论上)可能的三音位数量为453 = 91125。
- en: Many of these triphones will occur rarely or never in actual speech due to the
    linguistic constraints in the language. Using triphones therefore results in a
    system which has too many parameters to train. To avoid this problem, one has
    to introduce a mechanism for sharing parameters across different triphone models.
  id: totrans-3988
  prefs: []
  type: TYPE_NORMAL
  zh: 由于语言中的语言学约束，许多三音位在实际语音中出现得很少或根本不出现。因此，使用三音位会导致系统参数过多，难以训练。为避免这个问题，必须引入跨不同三音位模型共享参数的机制。
- en: Typically, a CART like decision tree is adopted to cluster triphones into *generalized
    triphones* based on both their a-priori probability and their acoustic similarity.
    Such a top-down clustering requires the specification of viable attributes to
    be used as questions on phonetic context in order to split tree nodes.
  id: totrans-3989
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，采用类似 CART 的决策树，根据三音素的先验概率和声学相似性将其聚类为*广义三音素*。这种自上而下的聚类需要指定可行属性，以作为音素上下文问题来拆分树节点。
- en: Mostly, linguistic classes such as vowel, consonant, fricative, plosive, etc.
    are being employed. Furthermore, one can generalize triphones to *polyphones*
    by allowing dependence on a wider context (and not just the immediate left and
    right neighboring phones). Fig. 15.3 shows a typical decision tree for the clustering
    of polyphonic variations of a particular monophone state.
  id: totrans-3990
  prefs: []
  type: TYPE_NORMAL
  zh: 通常使用的语言类别包括元音、辅音、摩擦音、爆破音等。此外，可以通过允许依赖于更广泛的上下文（而不仅仅是紧邻的左右音素）将三音素推广为*多音素*。图 15.3
    显示了特定单音素状态的多音变体聚类的典型决策树。
- en: The collection of all leaf nodes of decision trees for each monophone state
    in a given system represents a robust and general set of context-dependent subphonetic
    units. Since each of these units corresponds to several triphone HMM
  id: totrans-3991
  prefs: []
  type: TYPE_NORMAL
  zh: 给定系统中每个单音素状态的决策树的所有叶节点集合代表了一组强大而通用的上下文相关亚音素单元。由于这些单元中的每一个都对应几个三音素 HMM。
- en: '![316_image_0.png](316_image_0.png)'
  id: totrans-3992
  prefs: []
  type: TYPE_IMG
  zh: '![316_image_0.png](316_image_0.png)'
- en: Fig. 15.3. Phonetic Context Modeling using Decision Trees. Shown is a decision
    tree modeling phonetic contexts of middle state (3-state HMM) of monophone /AX/.
  id: totrans-3993
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.3. 使用决策树的音素上下文建模。显示的是单音素 /AX/ 的中间状态（3 状态 HMM）的音素上下文决策树。
- en: states, they are often called *tied states*. Typically, a large vocabulary continuous
    speech recognizer models between 3000 and 24000 such tied states. Mainstream LVCSR
    systems scale to any number of context-dependent modeling units since emission
    and transition models are independently estimated for each tied state.
  id: totrans-3994
  prefs: []
  type: TYPE_NORMAL
  zh: 状态，通常称为*关联状态*。典型的大词汇量连续语音识别器模型介于 3000 到 24000 个这样的关联状态之间。主流 LVCSR 系统可以扩展到任意数量的上下文相关建模单元，因为每个关联状态的发射和转移模型是独立估计的。
- en: 15.4.4 Connectionist Acoustic Modeling
  id: totrans-3995
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4.4 连接主义声学建模
- en: Locally discriminant connectionist acoustic modeling is the most popular approach
    to integrate neural networks into an HMM framework [3, 4, 18]. It is based on
    converting estimates of local posterior class probabilities to scaled likelihoods
    using Bayes rule. These scaled likelihoods can then be used as observation probability
    estimates in standard HMMs. For a moderately small number N of HMM states, a neural
    network can be trained to jointly estimate posterior probabilities p(si|Xi) for
    each state si given an input vector Xi. Bayes rule yields the corresponding scaled
    1 class conditional likelihoods
  id: totrans-3996
  prefs: []
  type: TYPE_NORMAL
  zh: 局部判别连接主义声学建模是将神经网络整合到 HMM 框架中的最流行方法 [3, 4, 18]。它基于使用贝叶斯法则将局部后验类别概率的估计转换为缩放的似然值。这些缩放的似然值随后可以作为标准
    HMM 中的观测概率估计。对于适中的小数量 N 的 HMM 状态，可以训练一个神经网络共同估计每个状态 si 在给定输入向量 Xi 的情况下的后验概率 p(si|Xi)。贝叶斯法则得到相应的缩放
    1 类条件似然。
- en: $${\hat{p}}(\mathbf{X_{i}}|s_{i})={\frac{p(s_{i}|\mathbf{X_{i}})}{p(s_{i})}},$$
  id: totrans-3997
  prefs: []
  type: TYPE_NORMAL
  zh: $${\hat{p}}(\mathbf{X_{i}}|s_{i})={\frac{p(s_{i}|\mathbf{X_{i}})}{p(s_{i})}},$$
- en: 1 The missing additional term consisting of the probability of the input vector
    p(Xi)
  id: totrans-3998
  prefs: []
  type: TYPE_NORMAL
  zh: 1 缺失的额外项由输入向量的概率 p(Xi) 组成。
- en: is usually omitted because it is independent of the class/state identity and
    therefore does not influence a Viterbi style search for the most likely state
    sequence.
  id: totrans-3999
  prefs: []
  type: TYPE_NORMAL
  zh: 通常被省略，因为它与类别/状态身份无关，因此不会影响 Viterbi 风格搜索最可能的状态序列。
- en: '![317_image_0.png](317_image_0.png)'
  id: totrans-4000
  prefs: []
  type: TYPE_IMG
  zh: '![317_image_0.png](317_image_0.png)'
- en: Fig. 15.4. Topology of a Hierarchy of Neural Networks (HNN) to estimate contextdependent
    posteriors, factored based on a-priori phonetic knowledge
  id: totrans-4001
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.4. 用于估计上下文相关后验的神经网络（HNN）层次结构拓扑，基于先验音素知识进行因子分解。
- en: While p(si|Xi) is estimated using a neural network, prior probabilities p(si)
    can be estimated by relative frequencies as observed in the training data. Several
    researchers (e. g. [3, 14]) have reported improvements with connectionist acoustic
    modeling when the technique for the estimation of emission probabilities was the
    only difference in comparison. Since mainstream HMMs for speech recognizers are
    mostly trained in a maximum likelihood framework using the ExpectationMaximization
    (EM) algorithm, incorporation of discriminatively trained neural networks that
    focus on modeling of class boundaries instead of class distributions is often
    observed to be beneficial. Also, compared to mixtures of Gaussians based acoustic
    models, connectionist acoustic models are often reported to achieve the same accuracy
    with far less parameters.
  id: totrans-4002
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用神经网络估计p(si|Xi)时，先验概率p(si)可以通过训练数据中观察到的相对频率进行估计。一些研究者（例如[3, 14]）报告称，当发射概率估计技术是唯一的不同之处时，连接主义声学建模有所改善。由于主流的语音识别HMM大多是在最大似然框架下使用期望最大化（EM）算法进行训练，因此通常观察到，结合专注于建模类别边界而非类别分布的判别训练神经网络是有益的。此外，与高斯混合声学模型相比，连接主义声学模型通常在参数更少的情况下实现相同的准确性。
- en: However, when the number of HMM states is increased to model contextdependent
    polyphones (triphones,quintphones), a single neural network can no longer be applied
    to estimate posteriors. It becomes necessary to factor the posterior state probabilities
    [17] and modularize the process of estimating those posteriors. In most approaches,
    the posteriors are factored on phonetic context or monophone identity (e.g. [4,
    9, 15]). Viewing factoring as a hierarchical decomposition of posteriors, we generalized
    the approaches to context-dependent connectionist acoustic modeling by introducing
    a tree structured hierarchy of neural networks (HNN) [12, 13] corresponding to
    a multi-level factoring of posteriors based on a-priori knowledge such as broad
    sound classes (silence, noises, phones), phonetic context and HMM state identity.
    Fig. 15.4 shows the topology of such a structure.
  id: totrans-4003
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当HMM状态的数量增加以建模上下文相关的多音素（三音素、五音素）时，单一神经网络无法再用于估计后验概率。这就需要对后验状态概率进行分解[17]，并模块化估计这些后验的过程。在大多数方法中，后验是在音素上下文或单音素身份上进行分解的（例如[4,
    9, 15]）。将分解视为后验的层次分解，我们通过引入神经网络的树状结构层次（HNN）[12, 13]，将这些方法推广到上下文相关的连接主义声学建模，基于先验知识（例如广泛的声音类别（静音、噪音、音素）、音素上下文和HMM状态身份）对后验进行多层次分解。图15.4显示了这种结构的拓扑。
- en: At the top of this hierarchy, we discriminate silence, noise and speech sounds
    by means of two networks (SIL-Net, SPEECH-Net). The motivation for this specific
    partitioning comes from the observation that these three classes are easy to distinguish
    acoustically. The remainder of the tree structure decomposes the posterior of
    speech, conditioning on monophone, context and state identity as these are convenient
    sound classes modeled by any phone based HMM speech recognizer. The hierarchy
    of Fig. 15.4 can be decomposed even further, for instance by factoring conditional
    monophone posteriors (estimated by the MONO-Net) based on linguistic features
    (e.g. voiced/unvoiced, vowel/consonantal, fricative etc.). The motivation behind
    such a decomposition is twofold. First, it reduces the number of local classes
    in each node, improving approximation accuracy and second, it yields a decoupled
    and specialized set of expert networks having to handle a smaller amount of phonetic
    variation.
  id: totrans-4004
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个层次的顶端，我们通过两个网络（SIL-Net，SPEECH-Net）区分静音、噪音和语音声音。这种特定划分的动机来自于观察到这三类声音在声学上易于区分。树状结构的其余部分对语音的后验进行分解，条件为单音素、上下文和状态身份，因为这些是任何基于音素的HMM语音识别器建模的方便声音类别。图15.4的层次结构甚至可以进一步分解，例如，通过基于语言特征（例如浊音/清音、元音/辅音、摩擦音等）对条件单音素后验（由MONO-Net估计）进行分解。这样的分解背后的动机有两个方面。首先，它减少了每个节点中的局部类别数量，提高了近似精度；其次，它产生了一组解耦和专门的专家网络，能够处理更少的音素变异。
- en: However, as mentioned in section 3, the use of prior knowledge for the design
    of a hierarchy of neural networks does not take into account dissimilarity of
    the observed classes in feature space. We therefore developed an agglomerative
    clustering algorithm to automatically design such hierarchies for the estimation
    of posteriors for a large number of classes. We termed this framework ACID/HNN
    [11].
  id: totrans-4005
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如第3节所述，使用先验知识来设计神经网络层次结构并未考虑特征空间中观察类别的相似性。因此，我们开发了一种聚合聚类算法，自动设计这样的层次结构，以估计大量类别的后验。我们将这一框架称为ACID/HNN
    [11]。
- en: 15.4.5 Acid Clustering
  id: totrans-4006
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4.5 酸聚类
- en: ACID (Agglomerative Clustering based on Information Divergence) is a bottomup
    clustering algorithm for the design of tree-structured soft classifiers such as
    a hierarchy of neural networks (HNN) [10, 11]. Although developed for connectionist
    acoustic modeling, the algorithm can in principle be used for any kind of classification
    task. Starting from a typically very large set of initial classes, for example
    the set of decision tree clustered HMM states in a speech recognizer 2, the ACID
    algorithm constructs a binary hierarchy. The nodes in the resulting tree are then
    instantiated with estimators for the respective conditional posterior probabilities,
    for instance in form of an HNN. The clustering metric in the ACID algorithm is
    the symmetric information divergence [26]
  id: totrans-4007
  prefs: []
  type: TYPE_NORMAL
  zh: ACID（基于信息散度的聚类）是一种自下而上的聚类算法，用于设计树状软分类器，例如神经网络（HNN）的层次结构[10, 11]。虽然该算法是为了连接主义声学建模而开发的，但原则上可以用于任何类型的分类任务。从一个通常非常大的初始类别集合开始，例如语音识别器中的决策树聚类HMM状态集合2，ACID算法构建一个二叉层次结构。结果树中的节点随后用相应条件后验概率的估计量进行实例化，例如以HNN的形式。ACID算法中的聚类度量是对称信息散度[26]。
- en: $$d(s_{i},s_{j})=\int_{\mathbf{x}}\left(p\left(\mathbf{x}|s_{i}\right)-p\left(\mathbf{x}|s_{j}\right)\right)\log{\frac{p(\mathbf{x}|s_{i})}{p\left(\mathbf{x}|s_{j}\right)}}\
    d\mathbf{x}$$
  id: totrans-4008
  prefs: []
  type: TYPE_NORMAL
  zh: $$d(s_{i},s_{j})=\int_{\mathbf{x}}\left(p\left(\mathbf{x}|s_{i}\right)-p\left(\mathbf{x}|s_{j}\right)\right)\log{\frac{p(\mathbf{x}|s_{i})}{p\left(\mathbf{x}|s_{j}\right)}}\
    d\mathbf{x}$$
- en: between class conditional densities of clusters. In contrast to standard agglomerative
    clustering algorithms which mostly represent clusters by their means and employ
    the Euclidean distance metric, we chose to represent clusters by parametric mixture
    densities (mixtures of Gaussians) in the ACID algorithm.
  id: totrans-4009
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的类条件密度。与大多数通过均值表示聚类并采用欧几里得距离度量的标准聚合聚类算法相比，我们选择在ACID算法中通过参数混合密度（高斯混合）来表示聚类。
- en: 2 In our case, we experimented with up to 24000 initial classes.
  id: totrans-4010
  prefs: []
  type: TYPE_NORMAL
  zh: 2 在我们的实验中，我们尝试了多达24000个初始类别。
- en: Modeling clusters with mixture densities is much more adequate than just using
    the mean and it still allows to cluster large amounts of classes in a reasonable
    time. The symmetric information divergence (also called Kullback-Leibler distance)
    measures the dissimilarity of two distributions and was therefore chosen as the
    clustering metric. Typically, each initial class (state) is modeled using a single
    full covariance multivariate Gaussian density
  id: totrans-4011
  prefs: []
  type: TYPE_NORMAL
  zh: 使用混合密度建模聚类比仅使用均值要合适得多，并且仍然可以在合理的时间内聚类大量类别。对称信息散度（也称为Kullback-Leibler距离）衡量两个分布的相似性，因此被选择为聚类度量。通常，每个初始类别（状态）使用单个完全协方差的多变量高斯密度建模。
- en: $$p(\mathbf{x}|s_{i})={\frac{1}{\sqrt{(2\pi)^{n}|\Sigma_{i}|}}}\exp\{-{\frac{1}{2}}(\mathbf{x}-\mu_{i})^{t}\Sigma_{i}^{-1}(\mathbf{x}-\mu_{i})\}$$
  id: totrans-4012
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(\mathbf{x}|s_{i})={\frac{1}{\sqrt{(2\pi)^{n}|\Sigma_{i}|}}}\exp\{-{\frac{1}{2}}(\mathbf{x}-\mu_{i})^{t}\Sigma_{i}^{-1}(\mathbf{x}-\mu_{i})\}$$
- en: with mean vector μi and covariance matrix Σi. Clustering then continuously merges
    initial classes which corresponds to building mixture densities based on the Gaussians.
    The symmetric information divergence between two states si and sj with Gaussian
    distributions amounts to
  id: totrans-4013
  prefs: []
  type: TYPE_NORMAL
  zh: 具有均值向量μi和协方差矩阵Σi。聚类然后不断合并初始类别，这对应于基于高斯分布构建混合密度。具有高斯分布的两个状态si和sj之间的对称信息散度为
- en: $$\begin{array}{l}{{d(s_{i},s_{j})=\frac{1}{2}t r\{(\mathbf{\Sigma}_{i}-\mathbf{\Sigma}_{j})(\mathbf{\Sigma}_{j}^{-1}-\mathbf{\Sigma}_{i}^{-1})\}}}\\
    {{\qquad\qquad+\frac{1}{2}t r\{(\mathbf{\Sigma}_{i}^{-1}+\mathbf{\Sigma}_{j}^{-1})(\mu_{i}-\mu_{j})(\mu_{i}-\mu_{j})^{t}\}}}\end{array}$$
  id: totrans-4014
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{l}{{d(s_{i},s_{j})=\frac{1}{2}t r\{(\mathbf{\Sigma}_{i}-\mathbf{\Sigma}_{j})(\mathbf{\Sigma}_{j}^{-1}-\mathbf{\Sigma}_{i}^{-1})\}}}\\
    {{\qquad\qquad+\frac{1}{2}t r\{(\mathbf{\Sigma}_{i}^{-1}+\mathbf{\Sigma}_{j}^{-1})(\mu_{i}-\mu_{j})(\mu_{i}-\mu_{j})^{t}\}}}\end{array}$$
- en: The computation of this distance measure requires O(n2) multiplications and
    additions (assuming pre-computed inverse covariance matrices), where n is the
    dimensionality of the input feature space. To reduce the computational load of
    the ACID clustering algorithm, one can model the class conditional likelihoods
    with diagonal covariance matrices only. Feature space transformations such as
    principal component analysis and linear discriminant analysis can be used to approximate
    such distributions. When using diagonal covariance Gaussians, the symmetric information
    divergence simplifies to
  id: totrans-4015
  prefs: []
  type: TYPE_NORMAL
  zh: 计算该距离度量需要O(n²)次乘法和加法（假设预先计算了逆协方差矩阵），其中n是输入特征空间的维度。为了降低ACID聚类算法的计算负担，可以仅用对角协方差矩阵来建模类别条件似然。主成分分析和线性判别分析等特征空间变换可以用来近似这样的分布。当使用对角协方差高斯时，对称信息散度简化为...
- en: $$d(s_{i},s_{j})=\frac{1}{2}\sum_{k=1}^{n}\frac{(\sigma_{j k}^{2}-\sigma_{i
    k}^{2})^{2}+(\sigma_{i k}^{2}+\sigma_{j k}^{2})(\mu_{i k}-\mu_{j k})^{2}}{\sigma_{i
    k}^{2}\sigma_{j k}^{2}}$$
  id: totrans-4016
  prefs: []
  type: TYPE_NORMAL
  zh: $$d(s_{i},s_{j})=\frac{1}{2}\sum_{k=1}^{n}\frac{(\sigma_{j k}^{2}-\sigma_{i
    k}^{2})^{2}+(\sigma_{i k}^{2}+\sigma_{j k}^{2})(\mu_{i k}-\mu_{j k})^{2}}{\sigma_{i
    k}^{2}\sigma_{j k}^{2}}$$
- en: where σ2ik and μik denote the k-th coefficient of the variance and mean vectors
    of state si, respectively. The evaluation of the latter distance measure requires
    only O(n) multiplications and additions.
  id: totrans-4017
  prefs: []
  type: TYPE_NORMAL
  zh: 其中σ²ik和μik分别表示状态si的方差和均值向量的第k个系数。后者距离度量的评估仅需O(n)次乘法和加法。
- en: Making the simplifying assumption of linearity of information divergence, we
    can define the following distance measure between clusters of Gaussians Sk and
    Sl
  id: totrans-4018
  prefs: []
  type: TYPE_NORMAL
  zh: 在简化假设信息散度线性的情况下，我们可以定义以下高斯群集Sk和Sl之间的距离度量。
- en: $$D(S_{k},S_{l})=\sum_{s_{i}\in S_{k}}p(s_{i}|S_{k})\sum_{s_{j}\in S_{l}}p(s_{j}|S_{l})d(s_{i},s_{j})$$
  id: totrans-4019
  prefs: []
  type: TYPE_NORMAL
  zh: $$D(S_{k},S_{l})=\sum_{s_{i}\in S_{k}}p(s_{i}|S_{k})\sum_{s_{j}\in S_{l}}p(s_{j}|S_{l})d(s_{i},s_{j})$$
- en: 'This distance measure is used in the ACID clustering algorithm:'
  id: totrans-4020
  prefs: []
  type: TYPE_NORMAL
  zh: 此距离度量在ACID聚类算法中使用：
- en: 1. Initialize algorithm with N clusters Si, each containing
  id: totrans-4021
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 用N个簇Si初始化算法，每个簇包含...
- en: '![320_image_1.png](320_image_1.png)'
  id: totrans-4022
  prefs: []
  type: TYPE_IMG
  zh: '![320_image_1.png](320_image_1.png)'
- en: '![320_image_0.png](320_image_0.png)'
  id: totrans-4023
  prefs: []
  type: TYPE_IMG
  zh: '![320_image_0.png](320_image_0.png)'
- en: ACID Initialization. Initialization requires to estimate class conditional likelihoods
    for all (tied) states modeled by the recognizer. The number N of initial
  id: totrans-4024
  prefs: []
  type: TYPE_NORMAL
  zh: ACID初始化。初始化需要估计识别器建模的所有（绑定的）状态的类别条件似然。初始类别的数量N...
- en: '![320_image_2.png](320_image_2.png)'
  id: totrans-4025
  prefs: []
  type: TYPE_IMG
  zh: '![320_image_2.png](320_image_2.png)'
- en: '![320_image_3.png](320_image_3.png)'
  id: totrans-4026
  prefs: []
  type: TYPE_IMG
  zh: '![320_image_3.png](320_image_3.png)'
- en: classes therefore is determined by other parts of the speech recognizer, namely
    by the phonetic decision tree that is typically applied to cluster phonetic contexts,
    or equivalently to tie HMM states [27]. Initial class conditional densities for
    these classes can be computed from state alignments using either the Viterbi or
    the Forward-Backward algorithm on training data and corresponding HMM
  id: totrans-4027
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，类别的数量由语音识别器的其他部分确定，即通常应用于聚类音素上下文的语音决策树，或等效地绑定HMM状态[27]。这些类别的初始类别条件密度可以通过使用维特比算法或前向-后向算法在训练数据和相应HMM上进行状态对齐计算。
- en: state graphs generated from training transcriptions. Estimation of initial parametric
    models for the ACID algorithm therefore requires a single pass through the training
    data. After initial models have been estimated, the actual ACID
  id: totrans-4028
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练转录生成的状态图。因此，ACID算法的初始参数模型的估计只需对训练数据进行一次遍历。在初始模型估计后，实际的ACID...
- en: clustering does not require any further passes through the training data. Furthermore,
    note that this algorithm clusters HMM states without knowledge of their phonetic
    identity solemnly based on acoustic dissimilarity.
  id: totrans-4029
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类不需要对训练数据进行任何进一步的遍历。此外，请注意，该算法在没有语音身份知识的情况下，仅基于声学差异对HMM状态进行聚类。
- en: ACID Dendrograms. For illustration purposes, Fig. 15.5 shows a dendrogram of
    a typical ACID clustering run on a relatively small set of only 56 initial classes
    corresponding to the set of single-state monophone HMMs in a contextindependent
    speech recognizer. The set of classes consists of 44 standard English phones along
    with 7 noise sounds (marked with a plus), 4 phones modeling interjections (marked
    with an ampersand) and silence (SIL).
  id: totrans-4030
  prefs: []
  type: TYPE_NORMAL
  zh: ACID树状图。为了说明，图15.5显示了在相对较小的仅56个初始类别（对应于上下文独立语音识别器中的单状态单音HMM集合）上进行的典型ACID聚类运行的树状图。类别集合由44个标准英语音素和7个噪声音（用加号标记）、4个模拟感叹词的音素（用&符号标记）以及静音（SIL）组成。
- en: Already the top level split separates silence, breathing and noise sounds (lower
    subtree) almost perfectly from phonetic sounds (upper subtree). Furthermore, clusters
    of acoustically similar phones can be observed in the ACID tree, for instance
  id: totrans-4031
  prefs: []
  type: TYPE_NORMAL
  zh: 顶层划分几乎完美地区分了沉默、呼吸和噪音（下层子树）与语音声音（上层子树）。此外，在ACID树中可以观察到声学上相似的音素聚类，例如
- en: '- IX,IH,IY,Y - JH,CH,SH,ZH - Z,S,F - ER,AXR,R'
  id: totrans-4032
  prefs: []
  type: TYPE_NORMAL
  zh: '- IX, IH, IY, Y - JH, CH, SH, ZH - Z, S, F - ER, AXR, R'
- en: '![321_image_0.png](321_image_0.png)'
  id: totrans-4033
  prefs: []
  type: TYPE_IMG
  zh: '![321_image_0.png](321_image_0.png)'
- en: Fig. 15.5. Typical dendrogram of ACID clustering
  id: totrans-4034
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5. ACID聚类的典型树状图
- en: ACID clustering was found to be quite effective in generating a hierarchical
    decomposition of a classification task into subtasks of increasing difficulty
    (when traversing the tree from root node to leaves). In the case of connectionist
    acoustic modeling for speech recognition, we observed that nodes in the upper
    layers of an ACID clustered HNN tree distinguish between broad phonetic classes,
    whereas nodes further down the tree begin to distinguish the particular phones
    within a broad phonetic class. Thus, ACID clustering constitutes an effective
    algorithm for discovering inherent hierarchical structure and exploiting it for
    modular classification.
  id: totrans-4035
  prefs: []
  type: TYPE_NORMAL
  zh: 发现ACID聚类在将分类任务层次分解为逐渐增加难度的子任务方面非常有效（当从根节点遍历到叶子节点时）。在连接主义声学建模的语音识别中，我们观察到ACID聚类的HNN树上层节点区分广泛的语音类别，而树下层节点则开始区分特定音素。因此，ACID聚类构成了一种有效的算法，用于发现固有的层次结构并利用其进行模块化分类。
- en: Model Selection. The choice of model size and topology becomes very important
    in the application of hierarchical soft classifiers to tasks such as connectionist
    speech recognition. While the global tree topology is determined by the outcome
    of the ACID clustering (or any other tree design procedure), it remains to decide
    on local (node-internal) classifier topology. The task of a local classifier is
    to estimate conditional posterior probabilities based on the available training
    data. Since a particular local estimator is conditioned on all predecessor nodes
    in the tree, it only receives training data from all the classes (leaves) that
    can be reached from the respective node. This amounts to a gradually diminishing
    training set when going from root node to nodes further down the tree. Fig. 15.6
    shows this property of HNNs with a plot of the amount of available training patterns
    vs. node depth for a binary hierarchy with 6000 leaves. Note the logscale on the
    ordinate.
  id: totrans-4036
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择。模型大小和拓扑的选择在将层次软分类器应用于连接主义语音识别等任务时变得非常重要。尽管全局树的拓扑由ACID聚类的结果（或任何其他树设计过程）决定，但仍需决定局部（节点内部）分类器的拓扑。局部分类器的任务是根据可用的训练数据估计条件后验概率。由于特定的局部估计量以树中所有前驱节点为条件，因此它只能接收来自可以从相应节点到达的所有类别（叶子）的训练数据。这意味着从根节点到树中更深的节点，训练集逐渐减少。图15.6展示了这一HNN特性，绘制了可用训练模式数量与节点深度之间的关系，对于具有6000个叶子的二叉层级，请注意纵轴的对数刻度。
- en: '![322_image_0.png](322_image_0.png)'
  id: totrans-4037
  prefs: []
  type: TYPE_IMG
  zh: '![322_image_0.png](322_image_0.png)'
- en: Fig. 15.6. Available Training Data in Different Depths of HNN Tree
  id: totrans-4038
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.6. HNN树中不同深度的可用训练数据
- en: 'When deciding on the local model complexity, we consider tree nodes as lying
    in a continuum between the following two extrema:'
  id: totrans-4039
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定局部模型复杂性时，我们将树节点视为在以下两个极端之间的连续体：
- en: Top Of The Hierarchy
  id: totrans-4040
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层级顶部
- en: '- large amounts of training data available'
  id: totrans-4041
  prefs: []
  type: TYPE_NORMAL
  zh: '- 大量的训练数据可用'
- en: '- allows for large node classifiers - relatively easy, general classification
    tasks Bottom of the Hierarchy'
  id: totrans-4042
  prefs: []
  type: TYPE_NORMAL
  zh: '- 允许大型节点分类器 - 相对简单的通用分类任务 层级底部'
- en: '- only small amounts of training data available - requires relatively small
    node classifiers - comparably hard classification tasks'
  id: totrans-4043
  prefs: []
  type: TYPE_NORMAL
  zh: '- 仅有少量可用的训练数据 - 需要相对较小的节点分类器 - 相对困难的分类任务'
- en: '- high degree of specialization Ideally, the complexity of local node classifiers
    should be selected so as to maximize generalization ability of the complete hierarchy.
    Generalization, on the other hand, is influenced by three factors: (1) size and
    distribution of the training set, (2) model complexity and (3) classification
    complexity of the specific task at hand. Obviously, we can not alter the latter
    of these factors. Furthermore, in the context of our architecture, we assume that
    the size of the training set for each node is fixed by the tree topology, once
    the hierarchy has been designed.'
  id: totrans-4044
  prefs: []
  type: TYPE_NORMAL
  zh: '- 高度专业化 理想情况下，局部节点分类器的复杂度应选择以最大化完整层次结构的泛化能力。另一方面，泛化受到三个因素的影响：（1）训练集的大小和分布，（2）模型复杂度，以及（3）特定任务的分类复杂度。显然，我们无法改变后者的因素。此外，在我们架构的背景下，我们假设每个节点的训练集大小由树拓扑结构固定，一旦层次结构设计完成。'
- en: Therefore, we have to choose model complexity based on available training data
    and difficulty of classification task.
  id: totrans-4045
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们必须根据可用的训练数据和分类任务的难度选择模型复杂度。
- en: In our experiments in connectionist acoustic modeling, we typically use multi
    layer perceptron (MLP) nodes with a single hidden layer and control model complexity
    by varying the number of hidden units. We use standard projective kernels with
    tanh activations for the hidden units and a task dependent nonlinearity for the
    output units (sigmoid for binary and softmax for multiway classification). The
    overall number of weights in such a network depends linearly on the number of
    hidden units. According to [1] and with some approximations, a rule of thumb is
    to choose the number of hidden units M to satisfy
  id: totrans-4046
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的连接主义声学建模实验中，我们通常使用具有单个隐藏层的多层感知机（MLP）节点，并通过改变隐藏单元的数量来控制模型复杂度。我们使用标准的投影核，隐藏单元采用tanh激活，输出单元使用与任务相关的非线性（对二分类使用sigmoid，对多分类使用softmax）。该网络的整体权重数量线性依赖于隐藏单元的数量。根据[1]和一些近似法，经验法则是选择隐藏单元M，使其满足
- en: $$N>{\frac{M}{\epsilon}}$$
  id: totrans-4047
  prefs: []
  type: TYPE_NORMAL
  zh: $$N>{\frac{M}{\epsilon}}$$
- en: where N is the size of the training set and  is the expected error rate on the
    test set. In our case, the variation in the number of training patterns in the
    different nodes dominates the above formula. Therefore, we set the number of hidden
    units proportional to b−n, where b is the branching factor of the classification
    tree and n is the node depth. As long as the tree is approximately balanced in
    terms of the prior distribution of child nodes, this strategy leads to hidden
    layers with size proportional to the number of available training patterns. A
    more fundamental treatment of model complexity using multiple training runs and
    cross validation is desirable. However, in case of large-scale applications such
    as speech recognition, such a strategy is not realizable because of the high computational
    cost resulting from very large training databases. Less heuristic approaches to
    select model complexity still have to be explored.
  id: totrans-4048
  prefs: []
  type: TYPE_NORMAL
  zh: 其中N是训练集的大小， 是测试集上的预期错误率。在我们的案例中，不同节点中训练模式数量的变化主导了上述公式。因此，我们将隐藏单元的数量设为与b−n成比例，其中b是分类树的分支因子，n是节点深度。只要树在子节点的先验分布方面大致平衡，这一策略就会导致隐藏层的大小与可用训练模式的数量成比例。希望对模型复杂度的更基础的处理使用多个训练运行和交叉验证。然而，在大型应用如语音识别的情况下，由于来自非常大的训练数据库的高计算成本，这一策略是不可实现的。仍需探索选择模型复杂度的更少启发式的方法。
- en: 15.4.6 Training Hierarchies Of Neural Networks On Large Datasets
  id: totrans-4049
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4.6 在大型数据集上训练神经网络的层次结构
- en: For the demonstration of various aspects of training large and complex structures
    such as hierarchies of neural networks on typical datasets, we report on experiments
    on the Switchboard [19] speech recognition database. Switchboard is a large corpus
    of conversational American English dialogs, recorded in telephone quality all
    over the US. It consists of about 170 hours of speech which typically corresponds
    to about 60 million training samples. The corpus currently serves as a benchmark
    for the official evaluation of state-of-the-art speech recognition systems. Switchboard
    is a comparably hard task, current best systems achieve word error rates in the
    vicinity of 30-40%. Fig. 15.7 shows the structure of an HNN based connectionist
    acoustic model for an HMM based recognizer, in our case the Janus recognition
    toolkit (JanusRTk) [8].
  id: totrans-4050
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示在典型数据集上训练大型和复杂结构（如神经网络层次结构）的各种方面，我们报告了在Switchboard [19]语音识别数据库上的实验。Switchboard是一个大型的美国英语对话语料库，在全美国以电话质量录制。它包含大约170小时的语音，通常对应约6000万个训练样本。该语料库目前作为最先进语音识别系统的官方评估基准。Switchboard是一个相对困难的任务，目前最佳系统的词错误率在30-40%的范围内。图15.7显示了基于HNN的连接主义声学模型的结构，适用于基于HMM的识别器，在我们的案例中是Janus识别工具包（JanusRTk）[8]。
- en: '![324_image_0.png](324_image_0.png)'
  id: totrans-4051
  prefs: []
  type: TYPE_IMG
  zh: '![324_image_0.png](324_image_0.png)'
- en: 'Fig. 15.7. Hierarchy of Neural Networks for Connectionist Acoustic Modeling:
    The upper part shows an ACID clustered HNN after node merging. This architecture
    computes posterior probabilities for a set of generalized polyphones. To allow
    for integration into the HMM framework, these posteriors are converted to scaled
    likelihoods. The correspondence to actual HMM states is accomplished by means
    of phonetic decision trees.'
  id: totrans-4052
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.7. 用于连接主义声学建模的神经网络层次结构：上半部分显示了经过节点合并的ACID聚类HNN。这种架构计算一组广义多音素的后验概率。为了便于集成到HMM框架中，这些后验概率被转换为缩放似然度。与实际HMM状态的对应是通过音素决策树实现的。
- en: Due to the inherent variability and complexity of the task and the large amount
    of training data, typical speech recognition systems model several thousand distinct
    subphonetic units (HMM states) as base classes. This requires to train an estimator
    for posterior probabilities of thousands of distinct acoustic classes based on
    millions of training samples, in order to take advantage of the full modeling
    granularity of the speech recognizer.
  id: totrans-4053
  prefs: []
  type: TYPE_NORMAL
  zh: 由于任务固有的可变性和复杂性以及大量的训练数据，典型的语音识别系统将几千个不同的子音素单元（HMM状态）建模为基本类别。这需要训练一个估计器，以根据数百万个训练样本估算数千个不同声学类别的后验概率，以便充分利用语音识别器的建模粒度。
- en: In the following, we will discuss several aspects of training a hierarchical
    soft classifier on large datasets such as Switchboard. Due to the modular structure
    of the classifier, the size of the model inventory and the training database,
    the following discussion leads to rather unique problems and solutions. However,
    it is important to emphasize that these properties stem from the structure of
    the classifier and the size of the task - not from the specific task of acoustic
    modeling for speech recognition. Thus, they are transferable to comparably large
    tasks, e. g. handwriting, speaker or face recognition. Classifier Tree Topology.
    Depending on the number of classes to be modeled, tree design algorithm, branching
    factor and size and structure of local node classifiers have to be chosen. For
    Switchboard, we were experimenting with three systems consisting of 6000, 10000
    and 24000 distinct classes, respectively. We used the ACID clustering algorithm
    to design an initial tree structure from the set of base classes for the 6k and
    24k systems. As a second step of the tree design phase, we applied a greedy node
    merging algorithm on the ACID clustered hierarchy. Node merging decreases the
    number of internal nodes while increasing the average branching factor (arity)
    of the tree. Training of such hierarchies is less problematic than training of
    the original binary tree structure since the difference among nodes (in terms
    of the number of available training patterns) is somewhat extenuated and the overall
    number of networks is reduced. However, local classification tasks change from
    2-way (binomial) to more complex multi-way (multinomial) problems which might
    have an impact on the accuracy of estimating conditional posteriors. Therefore,
    we constrain the node merging algorithm to produce nodes with a maximum branching
    factor of 8-12. This value was found to improve training speed while not affecting
    overall classifier accuracy. Considerably larger branching factors are not reasonable
    in our case as we would gradually loose the advantage of the hierarchical structure
    by flattening the tree.
  id: totrans-4054
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论在大型数据集（如Switchboard）上训练层次软分类器的几个方面。由于分类器的模块化结构、模型库存和训练数据库的大小，以下讨论导致了一些独特的问题和解决方案。然而，重要的是要强调，这些属性源于分类器的结构和任务的规模——而不是特定的语音识别声学建模任务。因此，它们可以转移到相应的大型任务，例如手写、说话人或面部识别。分类器树拓扑。根据要建模的类别数量，需要选择树设计算法、分支因子以及局部节点分类器的大小和结构。对于Switchboard，我们实验了由6000、10000和24000个不同类别组成的三个系统。我们使用ACID聚类算法从6k和24k系统的基础类别集中设计了初始树结构。作为树设计阶段的第二步，我们在ACID聚类层次上应用了贪婪节点合并算法。节点合并减少了内部节点的数量，同时增加了树的平均分支因子（元数）。训练这样的层次结构比训练原始的二叉树结构问题更少，因为节点之间的差异（就可用训练模式的数量而言）有所减弱，网络的总数量减少。然而，局部分类任务从2向（二项式）变为更复杂的多向（多项式）问题，这可能会影响条件后验估计的准确性。因此，我们限制节点合并算法生成最大分支因子为8-12的节点。这个值被发现能够提高训练速度，同时不影响整体分类器的准确性。在我们的案例中，明显更大的分支因子是不合理的，因为我们将逐渐失去层次结构的优势，导致树的扁平化。
- en: For the 10k system, we were using the architecture of Fig. 15.4 that was designed
    by prior knowledge, not taking into account any measure of class similarity. This
    structure exhibits a larger average branching factor and less depth than the ACID
    clustered trees. Although we could decrease the branching factor at the MONO node
    by introducing linguistic classes as mentioned earlier, we still have large branching
    factors at the context nodes which are much harder to resolve with prior knowledge
    only.
  id: totrans-4055
  prefs: []
  type: TYPE_NORMAL
  zh: 对于10k系统，我们使用的是图15.4的架构，该架构是基于先前的知识设计的，并未考虑任何类别相似性的度量。该结构表现出更大的平均分支因子和比ACID聚类树更少的深度。虽然我们可以通过引入如前所述的语言类别来降低MONO节点的分支因子，但上下文节点的分支因子仍然很大，仅凭先前的知识很难解决。
- en: The resulting tree nodes were instantiated with MLPs of varying size of the
    (single) hidden layer. The local MLPs output layer were parameterized with the
    softmax non-linearity for two reasons. First, it complies to the property of the
    modeled probability distribution to sum up to one, and second, the softmax function
    implements the expected value of the multinomial probability density.
  id: totrans-4056
  prefs: []
  type: TYPE_NORMAL
  zh: 结果树节点由不同大小的单层隐藏层的MLP实例化。局部MLP的输出层使用softmax非线性参数化，原因有两个。首先，它符合建模概率分布的属性，总和为1；其次，softmax函数实现了多项式概率密度的期望值。
- en: Fig. 15.8 gives an overview of the structure of the ACID/HNN systems. Tree compactification
    reduced the number of internal nodes of the 24k system from 24k to about 4k by
    increasing the average number of local classes (average branching
  id: totrans-4057
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8概述了ACID/HNN系统的结构。树的压缩将24k系统的内部节点数量从24k减少到约4k，平均局部类别数（平均分支因子）从2增加到约8。尤其是在处理大量类别时，我们发现适度的树压缩提高了分类器性能。
- en: '| level   | # nodes =                | # hidden   |'
  id: totrans-4058
  prefs: []
  type: TYPE_TB
  zh: '| level   | # nodes =                | # hidden   |'
- en: '| --- | --- | --- |'
  id: totrans-4059
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|         | # networks units/network |            |'
  id: totrans-4060
  prefs: []
  type: TYPE_TB
  zh: '|         | # networks units/network |            |'
- en: '| 1       | 1                        | 128        |'
  id: totrans-4061
  prefs: []
  type: TYPE_TB
  zh: '| 1       | 1                        | 128        |'
- en: '| 2       | 10                       | 128        |'
  id: totrans-4062
  prefs: []
  type: TYPE_TB
  zh: '| 2       | 10                       | 128        |'
- en: '| 3       | 77                       | 64         |'
  id: totrans-4063
  prefs: []
  type: TYPE_TB
  zh: '| 3       | 77                       | 64         |'
- en: '| 4       | 524                      | 32         |'
  id: totrans-4064
  prefs: []
  type: TYPE_TB
  zh: '| 4       | 524                      | 32         |'
- en: '| 5       | 3434                     | 16         |'
  id: totrans-4065
  prefs: []
  type: TYPE_TB
  zh: '| 5       | 3434                     | 16         |'
- en: '| total   | 4046                     |            |'
  id: totrans-4066
  prefs: []
  type: TYPE_TB
  zh: '| total   | 4046                     |            |'
- en: '| level                    | # nodes =   | # hidden   |'
  id: totrans-4067
  prefs: []
  type: TYPE_TB
  zh: '| level                    | # nodes =   | # hidden   |'
- en: '| --- | --- | --- |'
  id: totrans-4068
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| # networks units/network |             |            |'
  id: totrans-4069
  prefs: []
  type: TYPE_TB
  zh: '| # networks units/network |             |            |'
- en: '| 1                        | 1           | 256        |'
  id: totrans-4070
  prefs: []
  type: TYPE_TB
  zh: '| 1                        | 1           | 256        |'
- en: '| 2                        | 1           | 256        |'
  id: totrans-4071
  prefs: []
  type: TYPE_TB
  zh: '| 2                        | 1           | 256        |'
- en: '| 3                        | 1           | 256        |'
  id: totrans-4072
  prefs: []
  type: TYPE_TB
  zh: '| 3                        | 1           | 256        |'
- en: '| 4                        | 3           | 192        |'
  id: totrans-4073
  prefs: []
  type: TYPE_TB
  zh: '| 4                        | 3           | 192        |'
- en: '| 5                        | 19          | 128        |'
  id: totrans-4074
  prefs: []
  type: TYPE_TB
  zh: '| 5                        | 19          | 128        |'
- en: '| 6                        | 121         | 64         |'
  id: totrans-4075
  prefs: []
  type: TYPE_TB
  zh: '| 6                        | 121         | 64         |'
- en: '| 7                        | 816         | 32         |'
  id: totrans-4076
  prefs: []
  type: TYPE_TB
  zh: '| 7                        | 816         | 32         |'
- en: '| total                    | 962         |            |'
  id: totrans-4077
  prefs: []
  type: TYPE_TB
  zh: '| total                    | 962         |            |'
- en: Fig. 15.8. Overview of ACID clustered HNNs for 6k (left) and 24k (right) classes
  id: totrans-4078
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8. ACID集群HNNs的概述，左侧为6k类，右侧为24k类
- en: factor) from 2 to about 8. Especially when dealing with large numbers of classes,
    we found that moderate tree compactification improved classifier performance.
  id: totrans-4079
  prefs: []
  type: TYPE_NORMAL
  zh: 因素)从2到约8。尤其是在处理大量类别时，我们发现适度的树压缩提高了分类器性能。
- en: The overall numbers of parameters of the tree classifiers were 2M for the 6k
    system, 2.4M for the 10k system and 3.1M for the 24k system. Training Algorithm
    and Parameters. Training a distributed, hierarchically organized collection of
    neural networks on different amounts of training data is a challenging task. Our
    training criterion is maximum likelihood, assuming a multinomial probability model
    (1-out-of-N) over all base classes. A target class label is associated with each
    training pattern, indicating the correct base class. All networks in nodes along
    the path from root node to the current target class' leaf receive the current
    pattern for training. Because of the large amount of training data, we use on-line
    (stochastic) gradient ascent in log-likelihood with small batches (10-100 patterns)
    to train the individual networks. More elaborate training algorithms which apply
    second order methods in optimizing the objective function are too costly in our
    scenario - a single epoch of training, processing all 60 million patterns in the
    training database takes 3-5 days on a Sparc Ultra workstation. A practical training
    algorithm therefore must not take longer than 1-4 epochs to converge. Furthermore,
    because of the large number of networks that have to be trained, a potential training
    algorithm can not be allowed to use large amounts of memory - which could be the
    case with second order methods. Of course, training of individual node classifiers
    is independent and can therefore easily be parallelized for shared memory multi-processors
    which alleviates the latter requirement.
  id: totrans-4080
  prefs: []
  type: TYPE_NORMAL
  zh: 树分类器的整体参数数量为6k系统的2M，10k系统的2.4M和24k系统的3.1M。训练算法和参数。在不同训练数据量上训练分布式、分层组织的神经网络集合是一项具有挑战性的任务。我们的训练标准是最大似然，假设所有基本类的多项式概率模型（1-out-of-N）。每个训练样本都有一个目标类标签，指示正确的基本类。沿着从根节点到当前目标类叶子的路径上的所有节点网络都接收当前样本进行训练。由于训练数据量大，我们使用在线（随机）梯度上升法，采用小批量（10-100个样本）训练各个网络。在我们的场景中，应用二阶方法优化目标函数的更复杂训练算法成本过高——单个训练周期处理训练数据库中所有6000万样本需要在Sparc
    Ultra工作站上花费3-5天。因此，实用的训练算法必须在1-4个周期内收敛。此外，由于需要训练的大量网络，潜在的训练算法不能占用大量内存——这可能是二阶方法的情况。当然，单个节点分类器的训练是独立的，因此可以轻松并行化以适应共享内存的多处理器，从而减轻了后者的要求。
- en: 'Since we are relying on *stochastic* gradient ascent in our training method,
    we additionally use a simple momentum term to smooth gradients. Also, we use local
    learning rates for each network that are initialized with a global learning rate
    and adapted individually to the specific learning task. The global learning rate
    is annealed in an exponentially decaying scheme:'
  id: totrans-4081
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在训练方法中依赖于*随机*梯度上升，因此我们还使用一个简单的动量项来平滑梯度。此外，我们为每个网络使用局部学习率，初始时使用全局学习率，并根据具体学习任务进行单独调整。全局学习率采用指数衰减方案：
- en: $$\eta_{G}^{n+1}=\eta_{G}^{n}*\gamma_{G}.$$
  id: totrans-4082
  prefs: []
  type: TYPE_NORMAL
  zh: $$\eta_{G}^{n+1}=\eta_{G}^{n}*\gamma_{G}.$$
- en: Typically, we use an initial global learning rate ηG between 0.001 and 0.01,
    a momentum constant of 0.5 and a global annealing factor γG of 0.999 ... 0.9999
    applied after each batch update.
  id: totrans-4083
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们使用初始全局学习率ηG在0.001到0.01之间，动量常数为0.5，全局退火因子γG在每次批量更新后为0.999 ... 0.9999。
- en: 'In order to accommodate the different learning speeds of the node classifiers
    due to the different amount of available training data, we control individual
    learning rates using the following measure of correlation between successive gradient
    vectors gn−1 and gn:'
  id: totrans-4084
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应由于可用训练数据量不同而导致的节点分类器的不同学习速度，我们使用以下连续梯度向量gn−1和gn之间的相关性度量来控制各自的学习率：
- en: $$\alpha_{n}=\operatorname{arccos}\left({\frac{g_{n}^{t}g_{n-1}}{||g_{n}||||g_{n-1}||}}\right)$$
  id: totrans-4085
  prefs: []
  type: TYPE_NORMAL
  zh: $$\alpha_{n}=\operatorname{arccos}\left({\frac{g_{n}^{t}g_{n-1}}{||g_{n}||||g_{n-1}||}}\right)$$
- en: αn measures the angle between the gradients gn−1 and gn. Small angles indicate
    high correlation and therefore steady movement in weight space. Therefore, we
    increase the learning rate linearly up to the current maximum (as determined by
    initial learning rate, annealing factor and number of updates performed) whenever
    αn < 90◦ for several batch updates M. Large angles, on the other hand, indicate
    random jumps in weight space. We therefore decrease the learning rate exponentially
    whenever αn > 90◦ for several batch updates M.
  id: totrans-4086
  prefs: []
  type: TYPE_NORMAL
  zh: αn 测量梯度 gn−1 和 gn 之间的角度。小角度表示高度相关，因此在权重空间中稳定移动。因此，每当 αn < 90◦ 并持续几个批次更新 M 时，我们会将学习速率线性提高到当前最大值（由初始学习速率、退火因子和执行的更新次数决定）。另一方面，大角度表示权重空间中的随机跳跃。因此，每当
    αn > 90◦ 并持续几个批次更新 M 时，我们会指数级降低学习速率。
- en: 'In summary, we obtain the following update rule for local learning rate ηi
    of network i:'
  id: totrans-4087
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们得到网络 i 的局部学习速率 ηi 的以下更新规则：
- en: $$\eta_{i}^{n+1}=\operatorname*{min}\left\{\eta_{G}^{n+1},\left\{\begin{array}{l}{{\eta_{i}^{n}+\delta}}\\
    {{\eta_{i}^{n}*\gamma}}\\ {{\eta_{i}^{n}}}\end{array}\right\}\right\}\quad\quad\mathrm{if}\quad\left\{\begin{array}{l
    l}{{{\frac{1}{M}\left(\sum_{k=0}^{M}\alpha_{n-k}\right)<90^{\circ}-\epsilon}}}\\
    {{{\frac{1}{M}\left(\sum_{k=0}^{M}\alpha_{n-k}\right)>90^{\circ}+\epsilon}}}\\
    {{\mathrm{else}}}\end{array}\right\}.$$
  id: totrans-4088
  prefs: []
  type: TYPE_NORMAL
  zh: $$\eta_{i}^{n+1}=\operatorname*{min}\left\{\eta_{G}^{n+1},\left\{\begin{array}{l}{{\eta_{i}^{n}+\delta}}\\
    {{\eta_{i}^{n}*\gamma}}\\ {{\eta_{i}^{n}}}\end{array}\right\}\right\}\quad\quad\mathrm{if}\quad\left\{\begin{array}{l
    l}{{{\frac{1}{M}\left(\sum_{k=0}^{M}\alpha_{n-k}\right)<90^{\circ}-\epsilon}}}\\
    {{{\frac{1}{M}\left(\sum_{k=0}^{M}\alpha_{n-k}\right)>90^{\circ}+\epsilon}}}\\
    {{\mathrm{else}}}\end{array}\right\}.$$
- en: with linear increase δ = 0.001 ... 0.01 and exponential annealing factor γ =
    0.5 ... 0.9. The number of batch updates M controls smoothing of α whereas controls
    the influence of the global learning rate. For  → 90◦, local learning rates are
    forced to follow the global learning rate, whereas low values of  allow local
    learning rates to develop individually. Typical values that have been used in
    our experiments are M = 10 and  = 20◦.
  id: totrans-4089
  prefs: []
  type: TYPE_NORMAL
  zh: 线性增加 δ = 0.001 ... 0.01 和指数退火因子 γ = 0.5 ... 0.9。批次更新次数 M 控制 α 的平滑程度，而 控制全局学习速率的影响。对于
    → 90◦，局部学习速率被迫跟随全局学习速率，而低值 允许局部学习速率独立发展。在我们的实验中使用的典型值为 M = 10 和 = 20◦。
- en: Adapting individual learning rates to the training speed is a critical issue
    in hierarchical classifiers. Networks at the top of the tree have to be trained
    on very large amounts of training data. Therefore, learning rates must be allowed
    to become relatively small in order to benefit from all the data and not reach
    the point of saturation too early. On the other hand, networks at the bottom of
    the tree have to be trained with comparably small amounts of data. In order to
    train these networks within the same small number of passes through the overall
    data, we have to apply comparably large learning rates to reach a maximum in local
    likelihood as fast as possible. However, unconstrained adaptation of learning
    rates with aggressive optimization of learning speed may result in failure to
    converge. In our experiments with global initialization of all networks using
    the same maximum learning rate, global annealing of the maximum learning rate
    and local adaptation of individual learning rates that are constrained to never
    become larger than the global learning rate gives best results.
  id: totrans-4090
  prefs: []
  type: TYPE_NORMAL
  zh: 适应个体学习速率以应对训练速度是分层分类器中的一个关键问题。树顶的网络需要在非常大量的训练数据上进行训练。因此，必须允许学习速率相对较小，以便充分利用所有数据，避免过早达到饱和点。另一方面，树底的网络需要在相对较小的数据量上进行训练。为了在整个数据上以相同的少量轮次训练这些网络，我们必须施加相对较大的学习速率，以尽快达到局部似然的最大值。然而，不受限制的学习速率适应与激进的学习速度优化可能导致无法收敛。在我们对所有网络使用相同最大学习速率的全局初始化实验中，最大学习速率的全局退火和局部学习速率的适应（被限制为绝不超过全局学习速率）取得了最佳结果。
- en: 'Generalization/Overfitting. Simply speaking, we did not observe any overfitting
    in our experiments. Taking a look at the training of a large hierarchy in terms
    of performance on an independent cross-validation set (Fig. 15.9), we can see
    that the likelihood on this data levels off, but never starts to decrease again,
    as is often observed on smaller tasks. In the plots of Fig. 15.9, the vertical
    lines indicate multiple epochs (passes) through the training data (consisting
    of 87000 utterances). Obviously, the large amount of available training data allows
    for excellent generalization, early stopping was not necessary. This behavior
    is surprising at first sight, because we did not use any kind of explicit regularization
    of the local MLPs. At second sight, however, we can identify several reasons for
    the good generalization of HNNs on this task:'
  id: totrans-4091
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化/过拟合。简单来说，我们在实验中没有观察到任何过拟合。查看大型层次结构在独立交叉验证集上的表现（图 15.9），我们可以看到该数据的似然性趋于平稳，但从未再次下降，这在较小任务中常常会观察到。在图
    15.9 的图表中，垂直线表示通过训练数据（包含 87000 个语句）的多个历时（遍数）。显然，大量可用的训练数据允许出色的泛化，早期停止并不是必要的。这种行为初看令人惊讶，因为我们没有对局部
    MLPs 使用任何形式的显式正则化。然而，仔细观察后，我们可以确定 HNNs 在此任务上良好泛化的几个原因：
- en: '- Training data can be considered very noisy in our case, since samples come
    from a large variety of different speakers and recording conditions. Training
    with noisy data is similar to regularization and therefore improves generalization
    [2].'
  id: totrans-4092
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在我们的案例中，训练数据可以被视为非常嘈杂，因为样本来自不同说话者和录音条件的多种来源。使用嘈杂数据进行训练类似于正则化，因此可以改善泛化[2]。'
- en: '- Consider the hierarchy for the 6k classes system (Fig. 15.8). Some of the
    816 networks at the bottom of the tree probably have not seen enough training
    patterns to generalize well to new data. Although all of these networks together
    constitute 85% of the total number of networks, they contribute just as one of
    7 networks to any particular posterior probability. The networks in the upper
    part of the hierarchy have the largest influence on the evaluation of posterior
    probabilities. For those networks, the amount of available training data can be
    considered so abundant that test set error approaches training set error rate.
    In other words, optimal generalization can be achieved.'
  id: totrans-4093
  prefs: []
  type: TYPE_NORMAL
  zh: '- 考虑 6k 类系统的层次结构（图 15.8）。树底部的 816 个网络可能没有见过足够的训练模式，无法很好地泛化到新数据。尽管这些网络总共构成了
    85% 的网络总数，但它们在任何特定后验概率中只贡献 7 个网络之一。层次结构上部的网络对后验概率的评估有最大的影响。对于这些网络，现有的训练数据量可以被视为非常丰富，以至于测试集误差接近训练集误差率。换句话说，可以实现**最佳泛化**。'
- en: Results. We evaluate the proposed hierarchical classifiers as connectionist
    acoustic models in a speech recognition system. Performance of speech recognizers
    is usually measured in terms of the word error rate on a reasonably large set
    of test utterances. In our case, we test the different acoustic classifiers with
    the Janus [8] recognizer on the first 30 seconds of each speaker in the official
    1996 Switchboard evaluation test set, consisting of 366 utterances not present
    in the training set.
  id: totrans-4094
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。我们将提出的层次分类器作为连接主义声学模型评估在语音识别系统中的表现。语音识别器的性能通常通过在一个合理大的测试语句集上的字错误率来衡量。在我们的案例中，我们在官方
    1996 Switchboard 评估测试集的每位说话者前 30 秒中测试不同的声学分类器，该测试集包含 366 个不在训练集中的语句。
- en: '| acoustic classifier   | # classes # parameters word error rate   |       |        |'
  id: totrans-4095
  prefs: []
  type: TYPE_TB
  zh: '| 声学分类器         | 类别数量 参数数量 字错误率             |       |        |'
- en: '| --- | --- | --- | --- |'
  id: totrans-4096
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| HNN                   | 10000                                    | 2.0 M
    | 37.3 % |'
  id: totrans-4097
  prefs: []
  type: TYPE_TB
  zh: '| HNN                   | 10000                                    | 2.0 M
    | 37.3 % |'
- en: '| ACID/HNN              | 6000                                     | 2.4 M
    | 36.7 % |'
  id: totrans-4098
  prefs: []
  type: TYPE_TB
  zh: '| ACID/HNN              | 6000                                     | 2.4 M
    | 36.7 % |'
- en: '| ACID/HNN              | 24000                                    | 3.1 M
    | 33.3 % |'
  id: totrans-4099
  prefs: []
  type: TYPE_TB
  zh: '| ACID/HNN              | 24000                                    | 3.1 M
    | 33.3 % |'
- en: '![329_image_0.png](329_image_0.png)'
  id: totrans-4100
  prefs: []
  type: TYPE_IMG
  zh: '![329_image_0.png](329_image_0.png)'
- en: Fig. 15.9. Cross-Validation during training of 24k ACID/HNN architecture
  id: totrans-4101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.9. 24k ACID/HNN 架构训练中的交叉验证
- en: The above results are competitive with those of state-of-the-art systems and
    indicate a clear advantage of the ACID clustered over the pre-determined hierarchical
    classifiers. We suspect, that the reason for the better performance of automatically
    clustered hierarchies of neural networks is the difference in tree topology. Automatically
    clustered HNNs such as the presented ACID/HNN trees exhibit small and comparably
    uniform average branching factors that allow to robustly train estimators of conditional
    posterior probabilities. In contrast, handcrafted hierarchies such as the 10k
    HNN tree contain nodes with rather large branching factors. Fig. 15.10 shows the
    branching factors for all the networks in the 10k tree structure.
  id: totrans-4102
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果与最先进系统的结果具有竞争力，并表明ACID聚类相较于预定层次分类器具有明显优势。我们怀疑，自动聚类的神经网络层次结构性能更佳的原因在于树的拓扑结构差异。自动聚类的HNN（如所展示的ACID/HNN树）表现出小而相对均匀的平均分支因子，这使得能够稳健地训练条件后验概率的估计器。相比之下，手工设计的层次结构如10k
    HNN树包含分支因子相对较大的节点。图 15.10 显示了10k树结构中所有网络的分支因子。
- en: '![330_image_0.png](330_image_0.png)'
  id: totrans-4103
  prefs: []
  type: TYPE_IMG
  zh: '![330_image_0.png](330_image_0.png)'
- en: Fig. 15.10. Branching Factors of Individual Nodes in 10k HNN
  id: totrans-4104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.10. 10k HNN中各个节点的分支因子
- en: The largest observed branching factor in this tree was 276. This requires the
    joint estimation of conditional posterior probabilities for as many as 276 classes
    which may result in rather poor approximations to the true posterior probabilities
    for some of the networks in the tree.
  id: totrans-4105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这棵树中观察到的最大分支因子为276。这需要对多达276个类别的条件后验概率进行联合估计，这可能导致对树中某些网络的真实后验概率的相当糟糕的逼近。
- en: Furthermore, the superior performance of both ACID/HNN classifiers over the
    hand-crafted 10k tree, demonstrates the full scalability of the hierarchical approach
    and justifies the increase in the number of parameters. Earlier attempts to train
    hand-crafted hierarchies for 24k classes failed to provide classifiers that could
    be used as acoustic models in a speech recognizer. Poor approximations to the
    real posterior probabilities led to instabilities in decoding when dividing by
    priors in this case. Apart from that, we do not know of any other non-parametric
    approach capable of directly and discriminatively estimating posterior probabilities
    for such a large amount of classes.
  id: totrans-4106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，ACID/HNN分类器相较于手工设计的10k树的优越性能，证明了层次方法的完全可扩展性，并为参数数量的增加提供了正当理由。早期尝试为24k类别训练手工设计的层次结构未能提供可用作语音识别器声学模型的分类器。对真实后验概率的粗略逼近导致在此情况下解码时因除以先验而产生不稳定性。除此之外，我们不知道其他任何非参数方法能够直接和有区分地估计如此大量类别的后验概率。
- en: 15.5 Conclusions
  id: totrans-4107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.5 结论
- en: We have presented and discussed a methodology for the estimation of posterior
    probabilities for large numbers of classes using a hierarchical connectionist
    framework. The aim of the paper is to demonstrate the necessity of hierarchical
    approaches to modularize classification tasks in large-scale application domains
    such as speech recognition, where thousands of classes have to be considered and
    millions of training samples are available. The *divide and conquer* approach
    proves to be a versatile tool in breaking down the complexity of the original
    problem into many smaller tasks. Furthermore, agglomerative clustering techniques
    can be applied to automatically impose a suitable hierarchical structure on a
    given set of classes, even in the case this set contains tens of thousands of
    classes. In contrast to the relatively small standard benchmarks for learning
    machines, aspects such as choice of training method, model selection and generalization
    ability appear in different light when tackling large-scale probability estimation
    problems.
  id: totrans-4108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出并讨论了一种利用层次连接主义框架对大量类别的后验概率进行估计的方法论。本文的目的是展示层次方法在大型应用领域（如语音识别）中对分类任务进行模块化的必要性，在这些领域中必须考虑成千上万的类别，并且有数百万的训练样本可用。*分而治之*的方法被证明是一种灵活的工具，可以将原始问题的复杂性分解为许多较小的任务。此外，聚合聚类技术可以自动对给定类别集施加合适的层次结构，即使该类别集包含数万个类别。在处理大规模概率估计问题时，与相对较小的标准学习机器基准相比，训练方法的选择、模型选择和泛化能力等方面显得格外重要。
- en: '[1] Baum, E.B., Haussler, D.: What Size Net Gives Valid Generalization? Neural
    Computation 1, 151–160 (1989)'
  id: totrans-4109
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Baum, E.B., Haussler, D.: 多大规模的网络能提供有效的泛化？神经计算 1, 151–160 (1989)'
- en: '[2] Bishop Training, C.M.: with Noise is Equivalent to Tikhonov Regularization.
    Neural Computation 7(1), 108–116 (1995)'
  id: totrans-4110
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Bishop Training, C.M.: 带噪声相当于Tikhonov正则化。神经计算7(1)，108–116（1995年）'
- en: '[3] Bourlard, H., Morgan, N.: Connectionist Speech Recognition - A Hybrid Approach.
    Kluwer Academic Press (1994)'
  id: totrans-4111
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Bourlard, H., Morgan, N.: 连接主义语音识别 - 一种混合方法。Kluwer学术出版社（1994年）'
- en: '[4] Bourlard, H., Morgan, N.: A Context Dependent Neural Network for Continuous
    Speech Recognition. In: IEEE Proc. Intl. Conf. on Acoustics, Speech and Signal
    Processing, San Francisco, CA, vol. 2, pp. 349–352 (1992)'
  id: totrans-4112
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Bourlard, H., Morgan, N.: 一种用于连续语音识别的上下文依赖神经网络。在：IEEE国际声学、语音和信号处理会议，旧金山，加州，卷2，页349–352（1992年）'
- en: '[5] Breiman, L., Friedman, J.H., Olshen, R.A., Stone, C.J.: Classification
    and Regression Trees. Wadsworth International Group, Belmont (1984)'
  id: totrans-4113
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Breiman, L., Friedman, J.H., Olshen, R.A., Stone, C.J.: 分类与回归树。Wadsworth国际集团，贝尔蒙特（1984年）'
- en: '[6] Bridle, J.: Probabilistic Interpretation of Feed Forward Classification
    Network Outputs, with Relationships to Statistical Pattern Recognition. In: FogelmanSoulie,
    F., Hérault, J. (eds.) Neurocomputing: Algorithms, Architectures, and Applications.
    Springer, New York (1990)'
  id: totrans-4114
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Bridle, J.: 前馈分类网络输出的概率解释及其与统计模式识别的关系。在：Fogelman-Soulie, F., Hérault, J.（主编）神经计算：算法、架构和应用。施普林格出版社，纽约（1990年）'
- en: '[7] Duda, R., Hart, P.: Pattern Classification and Scene Analysis. John Wiley
    & Sons, Inc. (1973)'
  id: totrans-4115
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Duda, R., Hart, P.: 模式分类与场景分析。约翰·威利与儿子公司（1973年）'
- en: '[8] Finke, M., Fritsch, J., Geutner, P., Ries, K., Zeppenfeld, T.: The JanusRTk
    Switchboard/Callhome 1997 Evaluation System. In: Proceedings of LVCSR Hub5e Workshop,
    Baltimore, Maryland, May 13-15 (1997)'
  id: totrans-4116
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Finke, M., Fritsch, J., Geutner, P., Ries, K., Zeppenfeld, T.: JanusRTk
    Switchboard/Callhome 1997评估系统。在：LVCSR Hub5e研讨会论文集，巴尔的摩，马里兰，5月13-15日（1997年）'
- en: '[9] Franco, H., Cohen, M., Morgan, N., Rumelhart, D., Abrash, V.: ContextDependent
    Connectionist Probability Estimation in a Hybrid Hidden Markov Model - Neural
    Net Speech Recognition System. Computer Speech and Language 8(3), 211–222 (1994)'
  id: totrans-4117
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Franco, H., Cohen, M., Morgan, N., Rumelhart, D., Abrash, V.: 在混合隐马尔可夫模型-神经网络语音识别系统中的上下文依赖连接主义概率估计。计算机语音与语言8(3)，211–222（1994年）'
- en: '[10] Fritsch, J., Finke, M.: ACID/HNN: Clustering Hierarchies of Neural Networks
    for Context-Dependent Connectionist Acoustic Modeling. In: Proceedings of International
    Conference on Acoustics, Speech and Signal Processing, Seattle, Wa (May 1998)'
  id: totrans-4118
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Fritsch, J., Finke, M.: ACID/HNN：用于上下文依赖连接主义声学建模的神经网络聚类层次。在：国际声学、语音和信号处理会议论文集，西雅图，华盛顿（1998年5月）'
- en: '[11] Fritsch, J.: ACID/HNN: A Framework for Hierarchical Connectionist Acoustic
    Modeling. In: Proceedings of IEEE Workshop on Automatic Speech Recognition and
    Understanding, Santa Barbara, Ca (December 1997)'
  id: totrans-4119
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Fritsch, J.: ACID/HNN：分层连接主义声学建模框架。在：IEEE自动语音识别与理解研讨会论文集，圣巴巴拉，加州（1997年12月）'
- en: '[12] Fritsch, J., Finke, M., Waibel, A.: Context-Dependent Hybrid HME/HMM Speech
    Recognition using Polyphone Clustering Decision Trees. In: Intl. Conf. on Acoustics,
    Speech and Signal Processing, Munich, Germany, vol. 3, p. 1759 (1997)'
  id: totrans-4120
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Fritsch, J., Finke, M., Waibel, A.: 使用多音聚类决策树的上下文依赖混合HME/HMM语音识别。在：国际声学、语音和信号处理会议，慕尼黑，德国，卷3，页1759（1997年）'
- en: '[13] Fritsch, J.: Modular Neural Networks for Speech Recognition, Tech. Report
    CMUCS-96-203, Carnegie Mellon University, Pittsburgh, PA (1996)'
  id: totrans-4121
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Fritsch, J.: 用于语音识别的模块化神经网络，技术报告CMUCS-96-203，卡内基梅隆大学，匹兹堡，PA（1996年）'
- en: '[14] Hochberg, M.M., Cook, G.D., Renals, S.J., Robinson, A.J., Schechtman,
    R.S.: The 1994 ABBOT Hybrid Connectionist-HMM Large-Vocabulary Recognition System.'
  id: totrans-4122
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Hochberg, M.M., Cook, G.D., Renals, S.J., Robinson, A.J., Schechtman,
    R.S.: 1994年ABBOT混合连接主义-HMM大词汇识别系统。'
- en: 'In: Spoken Language Systems Technology Workshop, pp. 170–176. ARPA (January
    1995)'
  id: totrans-4123
  prefs: []
  type: TYPE_NORMAL
  zh: 在：口语语言系统技术研讨会，页170–176。ARPA（1995年1月）
- en: '[15] Kershaw, D.J., Hochberg, M.M., Robinson, A.J.: Context-Dependent Classes
    in a Hybrid Recurrent Network-HMM Speech Recognition System, Tech.'
  id: totrans-4124
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Kershaw, D.J., Hochberg, M.M., Robinson, A.J.: 在混合递归网络-HMM语音识别系统中的上下文依赖类，技术报告。'
- en: Rep. CUED/F-INFENG/TR217, Cambridge University Engineering Department, Cambridge,
    England (1995)
  id: totrans-4125
  prefs: []
  type: TYPE_NORMAL
  zh: CUED/F-INFENG/TR217，剑桥大学工程系，剑桥，英格兰（1995年）
- en: '[16] Merz, C.J., Murphy, P.M.: UCI Repository of Machine Learning Databases,
    University of California, Department of Information and Computer Science (1996),'
  id: totrans-4126
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Merz, C.J., Murphy, P.M.: 加州大学UCI机器学习数据库存储库，信息与计算机科学系（1996年）'
- en: http://www.ics.uci.edu/~mlearn/MLRepository.html
  id: totrans-4127
  prefs: []
  type: TYPE_NORMAL
  zh: http://www.ics.uci.edu/~mlearn/MLRepository.html
- en: '[17] Morgan, N., Bourlard, H.: Factoring Networks by a Statistical Method.
    Neural Computation 4(6), 835–838 (1992)'
  id: totrans-4128
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] 摩根，N.，布尔拉德，H.：通过统计方法分解网络。神经计算 4(6)，835–838（1992）'
- en: '[18] Morgan, N., Bourlard, H.: An Introduction to Hybrid HMM/Connectionist
    Continuous Speech Recognition Signal Processing Magazine, 25–42 (May 1995) [19]
    NIST, Conversational Speech Recognition Workshop, DARPA Hub-5E Evaluation, May
    13-15, Baltimore, Maryland (1997)'
  id: totrans-4129
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] 摩根，N.，布尔拉德，H.：混合HMM/连接主义连续语音识别的介绍 信号处理杂志，25–42（1995年5月） [19] NIST，对话语音识别研讨会，DARPA
    Hub-5E评估，1997年5月13-15日，马里兰州巴尔的摩'
- en: '[20] Prechelt, L.: Proben1 - A Set of Neural Network Benchmark Problems and
    Benchmarking Rules. Technical Report 21/94, University of Karlsruhe, Germany (1994)'
  id: totrans-4130
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] 普雷切尔，L.：Proben1 - 一组神经网络基准问题和基准规则。技术报告 21/94，卡尔斯鲁厄大学，德国（1994）'
- en: '[21] Quinlan, J.R.: Induction of Decision Trees. Machine Learn. 1, 81–106 (1986)
    [22] Rabiner, L.R.: A Tutorial on Hidden Markov Models and Selected Applications
    in Speech Recognition. Proceedings of the IEEE 77, 257–285 (1989)'
  id: totrans-4131
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] 奎因兰，J.R.：决策树的归纳。机器学习 1, 81–106（1986） [22] 拉比纳，L.R.：隐马尔可夫模型及其在语音识别中的应用教程。IEEE会议论文
    77, 257–285（1989）'
- en: '[23] Safavian, S.R., Landgrebe, D.: A Survey of Decision Tree Classifier Methodology.'
  id: totrans-4132
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] 萨法维安，S.R.，兰德格雷贝，D.：决策树分类器方法的调查。'
- en: IEEE Transactions on Systems, Man and Cybernetics 21(3), 660–674 (1991)
  id: totrans-4133
  prefs: []
  type: TYPE_NORMAL
  zh: IEEE系统、人和控制论交易 21(3)，660–674（1991）
- en: '[24] Schürmann, J., Doster, W.: A Decision Theoretic Approach to Hierarchical
    Classifier Design. Pattern Recognition 17(3), 359–369 (1984)'
  id: totrans-4134
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] 舒尔曼，J.，多斯特，W.：一种基于决策的分层分类器设计方法。模式识别 17(3)，359–369（1984）'
- en: '[25] Schürmann, J.: Pattern Classification: A Unified View of Statistical and
    Neural Approaches. John Wiley & Sons, Inc., New York (1996)'
  id: totrans-4135
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] 舒尔曼，J.：模式分类：统计和神经方法的统一视角。约翰·威利父子公司，纽约（1996）'
- en: '[26] Tou, J.T., Ganzales, R.C.: Pattern Recognition Principles. Addison Wesley,
    Reading (1974)'
  id: totrans-4136
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] 图，J.T.，冈萨雷斯，R.C.：模式识别原理。阿迪森-韦斯利，阅读（1974）'
- en: '[27] Young, S.: Large Vocabulary Continuous Speech Recognition: a Review. CUED'
  id: totrans-4137
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] 杨，S.：大词汇连续语音识别：回顾。剑桥大学工程系'
- en: Technical Report, Cambridge University (1996)
  id: totrans-4138
  prefs: []
  type: TYPE_NORMAL
  zh: 技术报告，剑桥大学（1996）
- en: Tricks For Time Series-
  id: totrans-4139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列技巧-
- en: Preface
  id: totrans-4140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: In the last section we focus on tricks related to time series analysis and economic
    forecasting. In chapter 16, John Moody opens with a survey of the challenges of
    macroeconomic forecasting including problems such as noise, nonstationarities,
    nonlinearities, and the lack of good *a priori* models. Lest one be discouraged,
    descriptions of many possible neural network solutions are next presented including
    hyperparameter selection (e.g. for regularization, training window length),
  id: totrans-4141
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一部分，我们专注于与时间序列分析和经济预测相关的技巧。在第16章，约翰·穆迪首先调查宏观经济预测的挑战，包括噪声、非平稳性、非线性以及缺乏良好的*a
    priori*模型等问题。为了避免让人气馁，接下来介绍了许多可能的神经网络解决方案，包括超参数选择（例如用于正则化、训练窗口长度）。
- en: input variable selection, model selection (size and topology of network), better
    regularizers, committee forecasts, and model visualization.
  id: totrans-4142
  prefs: []
  type: TYPE_NORMAL
  zh: 输入变量选择、模型选择（网络的规模和拓扑）、更好的正则化器、委员会预测和模型可视化。
- en: The survey is followed by a more detailed description of smoothing regularizers,
    model selection methods (e.g. prediction risk, nonlinear crossvalidation (NCV)
    (p. 357)) and sensitivity-based pruning (SBP) (p. 359)
  id: totrans-4143
  prefs: []
  type: TYPE_NORMAL
  zh: 调查之后是对平滑正则化器、模型选择方法（例如预测风险、非线性交叉验证（NCV）（第357页））和基于灵敏度的剪枝（SBP）（第359页）的更详细描述。
- en: for input selection. The goal of using regularizers is to introduce bias into
    the model. But what is the "right bias"? Weight decay may be too *ad hoc* in that
    it does not consider the nature of the function being learned. As an alternative,
    the author presents several new smoothing regularizers for both feedforward and
    recurrent networks that empirically are found to work better.
  id: totrans-4144
  prefs: []
  type: TYPE_NORMAL
  zh: 用于输入选择。使用正则化器的目标是向模型引入偏差。但什么是“正确的偏差”？权重衰减可能过于*临时性*，因为它并未考虑被学习函数的性质。作为替代，作者提出几种新的平滑正则化器，适用于前馈和递归网络，实证结果显示效果更佳。
- en: 'In model selection, prediction risk is used as the criterion for determining
    "best fits". Several methods for estimating prediction risk are discussed and
    compared: generalized cross-validation (GCV), Akaike''s final prediction error
    (FPE), predicted squared error (PSE), and generalized prediction error (GPE)'
  id: totrans-4145
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型选择中，预测风险作为确定“最佳拟合”的标准。讨论并比较了几种估计预测风险的方法：广义交叉验证（GCV）、赤池信息量准则（FPE）、预测平方误差（PSE）和广义预测误差（GPE）。
- en: which can be expressed in terms of the effective number of parameters.
  id: totrans-4146
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用有效参数数量来表达。
- en: In cross validation, separate networks are trained on different subsets of the
    data to obtain estimates of the prediction risk. With nonlinear loss functions,
    however, each network may converge to distinct local minima making comparison
    difficult. NCV alleviates this problem by initializing each network to be trained
    on a CV subset in the same minimum w0 (obtained on the full training set). This
    way the CV errors computed on the different subsets estimate the prediction risk
    locally around this minimum w0 and not by using some remote local minima. SBP
    is used to select the "best subset" of input variables to use. Here a sensitivity
    measure (e.g. delta error, average gradient, average absolute gradient, RMS gradient)
    (p. 360) is used to measure the change in the training error that would result
    if a given input is removed. Input variables are ranked based on importance and,
    beginning with the least important, are pruned one at a time, retraining in between.
    Finally, John Moody shows how sensitivity measures can be examined visually over
    time to better understand the role of each input (p. 361). Throughout, empirical
    results are presented for forecasting the U.S. Index of Industrial Production.
  id: totrans-4147
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证中，对数据的不同子集训练独立的网络以获得预测风险的估计。然而，使用非线性损失函数时，每个网络可能会收敛到不同的局部最小值，使得比较变得困难。NCV通过将每个将在CV子集上训练的网络初始化到相同的最小值w0（在完整训练集上获得）来缓解这个问题。这样，在不同子集上计算的CV误差估计围绕这个最小值w0的预测风险，而不是使用一些远程局部最小值。SBP用于选择要使用的“最佳子集”输入变量。在这里，使用一种敏感性度量（例如，delta误差、平均梯度、平均绝对梯度、RMS梯度）（第360页）来测量如果去除给定输入将导致的训练误差变化。输入变量根据重要性排名，从最不重要的开始，逐个剪除，并在每次剪除之间进行重新训练。最后，John
    Moody展示了如何通过视觉手段随时间检查敏感性度量，以更好地理解每个输入的作用（第361页）。在整个过程中，提供了美国工业生产指数预测的实证结果。
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-4148
  prefs: []
  type: TYPE_NORMAL
  zh: '- 之前发表在：Orr, G.B. 和 Müller, K.-R.（编辑）：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-4149
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0（1998年）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    339–341, 2012.'
  id: totrans-4150
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon 等（编辑）：NN：实用技巧，第二版，LNCS 7700，第339–341页，2012年。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-4151
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: 'In chapter 17, Ralph Neuneier and Hans Georg Zimmermann describe in detail
    their impressive integrated system - with a lot of different tricks - for neural
    network training in the context of economic forecasting. The authors discuss all
    design steps of their system, e.g. input preprocessing, cost functions, handling
    of outliers, architecture, regularization techniques, learning techniques, robust
    estimation, estimation of error bars, pruning techniques. As in chapter 1, the
    tricks here are also highly interleaved, i.e. many tricks will not retain their
    full efficiency if they are used individually and not en bloc. Let us start with
    the preprocessing for which the authors use: squared inputs (see chapter 7), scaled
    relative differences, scaled forces in form of scaled curvatures or mean reverting
    that can characterize turning points in time series (p. 370). To limit the influence
    of outliers the previously mentioned inputs are transformed by'
  id: totrans-4152
  prefs: []
  type: TYPE_NORMAL
  zh: 在第17章中，Ralph Neuneier和Hans Georg Zimmermann详细描述了他们在经济预测背景下用于神经网络训练的令人印象深刻的综合系统——包括许多不同的技巧。作者讨论了他们系统的所有设计步骤，例如输入预处理、成本函数、异常值处理、架构、正则化技术、学习技术、稳健估计、误差条估计和剪枝技术。与第1章一样，这里的技巧也是高度交错的，即如果单独使用而不是整体使用，许多技巧将无法保持其全部效率。让我们从预处理开始，作者使用：平方输入（见第7章）、缩放相对差异、以缩放曲率或均值回归形式的缩放力，这可以表征时间序列的转折点（第370页）。为了限制异常值的影响，前面提到的输入被转换为
- en: X = Tanh(Wx),
  id: totrans-4153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: X = Tanh(Wx)，
- en: 'where the parameters w are learned as one layer in the training process (p.
    371). Subsequently, a bottleneck network reduces the number of inputs to the relevant
    ones (p. 372). Since networks for time series prediction are in general bottom
    heavy, i.e. the input dimension is large while the output dimension is very small,
    it is important to increase the number of output targets, so that more useful
    error signals can be backpropagated (see also [3, 2]). For this the authors introduce
    two output layers: (1) a point prediction layer, where not only the value to be
    predicted, i.e. yt+n, but also a number of neighboring values in time are used
    and (2) an interaction layer, where differences between these values, i.e. yt+n+1
    − yt+n, corresponding to curvature are employed (p. 375).'
  id: totrans-4154
  prefs: []
  type: TYPE_NORMAL
  zh: 其中参数w作为训练过程中的一层被学习（第371页）。随后，一个瓶颈网络将输入数量减少到相关的部分（第372页）。由于时间序列预测的网络通常是底重的，即输入维度大而输出维度非常小，因此增加输出目标的数量非常重要，以便更多有用的误差信号能够进行反向传播（另见[3,
    2]）。为此，作者引入了两个输出层：（1）点预测层，不仅使用待预测的值，即yt+n，还使用一系列时间上的邻近值；（2）交互层，其中使用这些值之间的差异，即yt+n+1
    − yt+n，来对应曲率（第375页）。
- en: Following this interesting trick, several point predictions are averaged to
    reduce the variance of the prediction, similar to bagging [1] (p. 377). This overall
    design gives rise to an 11-layer neural network architecture, where the available
    prior knowledge from financial forecasting is coded.
  id: totrans-4155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个有趣的技巧之后，将多个点预测进行平均，以减少预测的方差，类似于集成方法[1]（第377页）。这一整体设计催生了一个11层的神经网络架构，其中编码了来自金融预测的可用先验知识。
- en: For training this large - but rather constrained - architecture, cost functions
    are defined (p. 383) and a method based on the CDEN approach (p. 384) is proposed
    for estimating the error bars of a forecast.
  id: totrans-4156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练这种大型但相对受限的架构，定义了成本函数（第383页），并提出了一种基于CDEN方法（第384页）的估计预测误差条的方法。
- en: 'To train the network the so-called vario-η learning rule is introduced, which
    is essentially a stochastic approximation of a Quasi-Newton algorithm with individual
    learning rates for each weight (p. 392). The authors discuss how the simple pattern-by-pattern
    rule has structural consequences that improve generalization behavior; or to put
    it differently: the stochasticity of learning implicitly includes a curvature
    penalty on unreliable network parts.'
  id: totrans-4157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练网络，引入了所谓的变η学习规则，这本质上是具有每个权重个体学习率的准牛顿算法的随机近似（第392页）。作者讨论了简单的逐模式规则如何对结构产生影响，从而改善泛化行为；换句话说，学习的随机性隐含地对不可靠的网络部分施加了曲率惩罚。
- en: 'Then the authors raise the provoking question of the Observer-Observer dilemma:
    to create a model based on observed data while, at the same time, using this model
    to judge the correctness of new incoming data (p. 391). This leads to (1) clearning
    and (2) training with noise on input data. The rationale behind clearning is that
    a very noisy environment, such as financial data, will spoil a good prediction
    if the data is taken too seriously. That is, we are allowed to move the data point
    a little bit in input space in order to get smoother predictions (p. 395). Similarly,
    different additive noise levels for each input are chosen by an algorithm in order
    to adapt the noise level to the (estimated) importance of the input: a small noise
    level is used for perfectly described or unimportant inputs whereas a large noise
    level should be chosen for a poorly described (likely noise corrupted) input (p.
    397).'
  id: totrans-4158
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，作者提出了观察者-观察者困境的挑衅性问题：在基于观察数据创建模型的同时，使用该模型来判断新进入数据的正确性（第391页）。这导致了（1）清理和（2）对输入数据进行噪声训练。清理的基本原理是，一个非常嘈杂的环境，例如金融数据，如果数据过于严肃地被看待，会破坏良好的预测。也就是说，我们被允许在输入空间中稍微移动数据点，以便获得更平滑的预测（第395页）。类似地，通过算法为每个输入选择不同的附加噪声水平，以便将噪声水平适应于（估计的）输入重要性：对完美描述或不重要的输入使用较小的噪声水平，而对描述不佳（可能受到噪声干扰）的输入则应选择较大的噪声水平（第397页）。
- en: 'In the next step, pruning methods for optimizing the architecture are described.
    They are: (1) node-pruning (p. 401) and (2) several types of weightpruning: (a)
    stochastic pruning (p. 401), (b) early-brain-damage pruning'
  id: totrans-4159
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，描述了优化架构的剪枝方法。它们包括：（1）节点剪枝（第401页）和（2）几种类型的权重剪枝：（a）随机剪枝（第401页）、（b）早期脑损伤剪枝。
- en: (p. 402), (c) inverse-kurtosis pruning (p. 403), and (d) instability pruning
  id: totrans-4160
  prefs: []
  type: TYPE_NORMAL
  zh: （第402页）、（c）反峰度剪枝（第403页）以及（d）不稳定剪枝
- en: (p. 405). The authors use a combination of instability pruning and early-braindamage
    in their application; the first gives stable models and the latter generates very
    sparse networks.
  id: totrans-4161
  prefs: []
  type: TYPE_NORMAL
  zh: （第405页）。作者在他们的应用中使用了不稳定剪枝和早期脑损伤的组合；前者提供稳定模型，后者生成非常稀疏的网络。
- en: 'Finally, the whole set of tricks is combined into an integrated training process
    and monitored on a validation set: early/late stopping, pruning and weight decay
    regularization are alternated (p. 414) to obtain an excellent estimate of the
    German bond rate from June 1994 to May 1996 (p. 415).'
  id: totrans-4162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，整个技巧组合成一个综合的训练过程，并在验证集上进行监控：早停/晚停、剪枝和权重衰减正则化交替进行（第414页），以获得1994年6月至1996年5月德国债券利率的优秀估计（第415页）。
- en: Jenny & Klaus
  id: totrans-4163
  prefs: []
  type: TYPE_NORMAL
  zh: 珍妮与克劳斯
- en: '[1] Breiman, L.: Bagging predictors. Machine Learning 26(2), 123–140 (1996)
    [2] Caruana, R., de Sa, V.R.: Promoting poor features to supervisors: Some inputs
    work better as outputs. In: Mozer, M.C., Jordan, M.I., Petsche, T. (eds.) Advances
    in Neural Information Processing Systems, vol. 9, p. 389. The MIT Press (1997)'
  id: totrans-4164
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Breiman, L.: 装袋预测器。机器学习 26(2), 123–140 (1996) [2] Caruana, R., de Sa, V.R.:
    将不良特征提升为监督特征：某些输入作为输出效果更佳。在：Mozer, M.C., Jordan, M.I., Petsche, T. (编)。《神经信息处理系统进展》，第9卷，第389页。麻省理工学院出版社
    (1997)'
- en: '[3] Caruana, R., Pratt, L., Thrun, S.: Multitask learning. Machine Learning
    28, 41'
  id: totrans-4165
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Caruana, R., Pratt, L., Thrun, S.: 多任务学习。机器学习 28, 41'
- en: (1997)
  id: totrans-4166
  prefs: []
  type: TYPE_NORMAL
  zh: (1997)
- en: '16 Forecasting The Economy With Neural Nets: A Survey Of Challenges And Solutions-'
  id: totrans-4167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16 使用神经网络预测经济：挑战与解决方案的综述-
- en: John Moody Department of Computer Science & Engineering Oregon Graduate Institute
    of Science & Technology P.O. Box 91000, Portland, OR 97291, USA
  id: totrans-4168
  prefs: []
  type: TYPE_NORMAL
  zh: 约翰·穆迪 计算机科学与工程系 俄勒冈州立科技研究所 邮政信箱91000，波特兰，俄勒冈州97291，美国
- en: moody@cse.ogi.edu http://www.cse.ogi.edu/~moody/
  id: totrans-4169
  prefs: []
  type: TYPE_NORMAL
  zh: moody@cse.ogi.edu http://www.cse.ogi.edu/~moody/
- en: Abstract. Macroeconomic forecasting is a very difficult task due to the lack
    of an accurate, convincing model of the economy. The most accurate models for
    economic forecasting, "black box" time series models, assume little about the
    structure of the economy. Constructing reliable time series models is challenging
    due to short data series, high noise levels, nonstationarities, and nonlinear
    effects. This chapter describes these challenges and presents some neural network
    solutions to them. Important issues include balancing the *bias/variance tradeoff*
    and the noise/nonstationarity tradeoff. A brief survey of methods includes hyperparameter
    selection (regularization parameter and training window length), input variable
    selection and pruning, network architecture selection and pruning, new smoothing
    regularizers, committee forecasts and model visualization. Separate sections present
    more in-depth descriptions of smoothing regularizers, architecture selection via
    the *generalized* prediction error (GPE) and *nonlinear cross-validation (NCV)*,
    input selection via *sensitivity based pruning (SBP)*, and model interpretation
    and visualization. Throughout, empirical results are presented for forecasting
    the U.S. Index of Industrial Production. These demonstrate that, relative to conventional
    linear time series and regression methods, superior performance can be obtained
    using state-of-the-art neural network models.
  id: totrans-4170
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要。宏观经济预测是一项非常困难的任务，因为缺乏准确且令人信服的经济模型。用于经济预测的最准确模型，“黑箱”时间序列模型，对经济结构假设很少。构建可靠的时间序列模型面临挑战，因为数据序列短、噪声水平高、非平稳性和非线性效应。本章描述了这些挑战，并提出了一些神经网络解决方案。重要问题包括平衡*偏差/方差权衡*和噪声/非平稳性权衡。方法的简要调查包括超参数选择（正则化参数和训练窗口长度）、输入变量选择与剪枝、网络结构选择与剪枝、新平滑正则化器、委员会预测和模型可视化。单独的部分对平滑正则化器、通过*广义*预测误差（GPE）进行的结构选择、*非线性交叉验证（NCV）*、通过*基于敏感度的剪枝（SBP）*进行的输入选择，以及模型解释和可视化进行了更深入的描述。整个过程中，提供了预测美国工业生产指数的实证结果。这些结果表明，相较于传统的线性时间序列和回归方法，使用最先进的神经网络模型可以获得更优的表现。
- en: 16.1 Challenges Of Macroeconomic Forecasting
  id: totrans-4171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.1 宏观经济预测的挑战
- en: Of great interest to forecasters of the economy is predicting the "business
    cycle",
  id: totrans-4172
  prefs: []
  type: TYPE_NORMAL
  zh: 对经济预测者而言，预测“商业周期”是极为重要的，
- en: or the overall level of economic activity. The business cycle affects society
    as a whole by its fluctuations in economic quantities such as the unemployment
    rate (the misery index), corporate profits (which affect stock market prices),
    the demand for manufactured goods and new housing units, bankruptcy rates, investment
    in research and development, investment in capital equipment, savings
  id: totrans-4173
  prefs: []
  type: TYPE_NORMAL
  zh: 或整体经济活动水平。经济周期通过经济数量的波动影响整个社会，例如失业率（痛苦指数）、企业利润（影响股市价格）、对制造商品和新住房单位的需求、破产率、研发投资、资本设备投资、储蓄。
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-4174
  prefs: []
  type: TYPE_NORMAL
  zh: '- 以前出版于：Orr, G.B.和Müller, K.-R.（编）：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-4175
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0（1998）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    343–367, 2012.'
  id: totrans-4176
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon等（编）：NN：行业诀窍，第2版，LNCS 7700，第343-367页，2012年。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-4177
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: rates, and so on. The business cycle also affects important socio-political
    factors such as the general mood of the people and the outcomes of elections.
  id: totrans-4178
  prefs: []
  type: TYPE_NORMAL
  zh: 经济周期还会影响重要的社会政治因素，例如民众的整体情绪和选举结果。
- en: The standard measures of economic activity used by economists to track the business
    cycle include the Gross Domestic Product (GDP) and the Index of Industrial Production
    (IP). GDP is a broader measure of economic activity than is IP. However, GDP is
    computed by the U.S. Department of Commerce on only a quarterly basis, while Industrial
    Production is more timely, as it is computed and published monthly. IP exhibits
    stronger cycles than GDP, and is therefore more interesting and challenging to
    forecast. (See figure 16.1.) In this chapter, all empirical results presented
    are for forecasting the U.S. Index of Industrial Production.
  id: totrans-4179
  prefs: []
  type: TYPE_NORMAL
  zh: 经济学家用来追踪商业周期的经济活动标准测量包括国内生产总值（GDP）和工业生产指数（IP）。GDP是比IP更广泛的经济活动衡量标准。然而，GDP仅由美国商务部每季度计算，而工业生产则更及时，因为它是每月计算和发布的。IP的周期波动比GDP更强，因此在预测时更有趣且具有挑战性。（见图16.1。）在本章中，所有呈现的经验结果都是针对美国工业生产指数的预测。
- en: '![337_image_0.png](337_image_0.png)'
  id: totrans-4180
  prefs: []
  type: TYPE_IMG
  zh: '![337_image_0.png](337_image_0.png)'
- en: Fig. 16.1. U.S. Index of Industrial Production (IP) for the period 1967 to 1993.
    Shaded regions denote official recessions, while unshaded regions denote official
    expansions. The boundaries for recessions and expansions are determined by the
    National Bureau of Economic Research based on several macroeconomic series. As
    is evident for IP,
  id: totrans-4181
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1。1967年至1993年期间美国工业生产指数（IP）。阴影区域表示官方经济衰退，而非阴影区域表示官方扩张。衰退和扩张的边界由国家经济研究局根据几个宏观经济系列确定。显然，对于IP，
- en: business cycles are irregular in magnitude, duration, and structure.
  id: totrans-4182
  prefs: []
  type: TYPE_NORMAL
  zh: 商业周期在幅度、持续时间和结构上都是不规则的。
- en: 'Macroeconomic modeling and forecasting is challenging for several reasons:'
  id: totrans-4183
  prefs: []
  type: TYPE_NORMAL
  zh: 宏观经济建模和预测面临多个挑战：
- en: 'Non-experimental Science: Like evolutionary biology and cosmology, macroeconomics
    is largely a non-experimental science. There is only one instance of the world
    economy, and the economy of each country is not a closed system. Observing the
    state of an economy in aggregate is difficult, and it is generally not possible
    to do controlled experiments.'
  id: totrans-4184
  prefs: []
  type: TYPE_NORMAL
  zh: 非实验科学：像进化生物学和宇宙学一样，宏观经济学在很大程度上是一门非实验科学。世界经济只有一个实例，每个国家的经济并不是一个封闭系统。整体观察经济状态是困难的，通常无法进行控制实验。
- en: 'No *a priori* Models: A convincing and accurate scientific model of business
    cycle dynamics is not yet available due to the complexities of the economic system,
    the impossibility of doing controlled experiments on the economy, and non-quantifiable
    factors such as mass psychology and sociology that influence economic activity.
    There are two main approaches that economists have used to model the macroeconomy,
    econometric models and linear time series models:'
  id: totrans-4185
  prefs: []
  type: TYPE_NORMAL
  zh: 无*先验*模型：由于经济系统的复杂性、对经济进行控制实验的不可能性，以及影响经济活动的无法量化的因素（如大众心理和社会学），尚未出现令人信服和准确的商业周期动态科学模型。经济学家们主要使用两种方法来建模宏观经济，一种是计量经济模型，另一种是线性时间序列模型：
- en: 'Econometric Models: These models attempt to model the macroeconomy at a relatively
    fine scale and typically contain hundreds or thousands of equations and variables.
    The model structures are chosen by hand, but model parameters are estimated from
    the data. While econometric models are of some use in understanding the workings
    of the economy qualitatively, they are notoriously bad at making quantitative
    predictions.'
  id: totrans-4186
  prefs: []
  type: TYPE_NORMAL
  zh: 计量经济模型：这些模型试图以相对精细的尺度建模宏观经济，通常包含数百或数千个方程和变量。模型结构是手动选择的，但模型参数是从数据中估计的。虽然计量经济模型在定性理解经济运行方面有一定的用途，但它们在进行定量预测方面却
    notoriously 不佳。
- en: 'Linear Time Series Models: Given the poor forecasting performance of econometric
    models, many economists have resorted to analyzing and forecasting economic activity
    by using the empirical "black box" techniques of standard linear time series analysis.
    Such time series models typically have perhaps half a dozen to a dozen input series.
    The most reliable and popular of these models during the past decade or so have
    been bayesian vector autoregressive (BVAR) models [22]. As we have found in our
    own work, however, neural networks can often outperform standard linear time series
    models. The lack of an *a priori* model of the economy makes input variable selection,
    the selection of lag structures, and network model selection critical issues.'
  id: totrans-4187
  prefs: []
  type: TYPE_NORMAL
  zh: 线性时间序列模型：鉴于计量经济模型的预测性能较差，许多经济学家开始使用标准线性时间序列分析的经验“黑箱”技术来分析和预测经济活动。这类时间序列模型通常有大约六到十二个输入序列。在过去十年中，最可靠和受欢迎的模型是贝叶斯向量自回归（BVAR）模型[22]。然而，正如我们在自己的研究中发现的，神经网络往往能够超越标准线性时间序列模型。缺乏经济的*先验*模型使得输入变量选择、滞后结构选择和网络模型选择成为关键问题。
- en: 'Noise: Macroeconomic time series are intrinsically very noisy and generally
    have poor signal to noise ratios. (See figures 16.2 and 16.3.) The noise is due
    both to the many unobserved variables in the economy and to the survey techniques
    used to collect data for those variables that are measured. The noise distributions
    are typically heavy tailed and include outliers. The combination of short data
    series and significant noise levels makes controlling model variance, model complexity,
    and the *bias / variance tradeoff* important issues [9]. One measure of complexity
    for nonlinear models is Peff , the *effective number of parameters* [24, 25].
    Peff can be controlled to balance bias and variance by using regularization and
    model selection techniques. Nonstationarity: Due to the evolution of the world''s
    economies over time, macroeconomic series are intrinsically nonstationary. To
    confound matters, the definitions of many macroeconomic series are changed periodically
    as are the techniques employed in measuring them. Moreover, estimates of key series
    are periodically revised retroactively as better data are collected or definitions
    are changed. Not only do the underlying dynamics of the economy change with time,
    but the noise distributions for the measured series vary with time also. In many
    cases, such nonstationarity shortens the usable length of the data series, since
    training on older data will induce biases in predictions. The combination of noise
    and nonstationarity gives rise to a *noise / nonstationarity tradeoff* [23], where
    using a short training window results in too much model variance or estimation
    error due to noise in limited training data, while using a long training window
    results in too much model bias or *approximation error* due to nonstationarity.
    Nonlinearity: Traditional macroeconomic time series models are linear [12, 14].
    However, recent work by several investigators have suggested that nonlinearities
    can improve macroeconomic forecasting models in some cases [13, 27, 39, 35, 40].'
  id: totrans-4188
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声：宏观经济时间序列本质上非常嘈杂，通常信噪比低。（见图16.2和16.3。）噪声源于经济中许多未观察到的变量以及用于收集已测量变量数据的调查技术。噪声分布通常是重尾的，并包含异常值。短数据序列与显著噪声水平的结合，使得控制模型方差、模型复杂性和*偏差/方差权衡*成为重要问题[9]。非线性模型的复杂性度量之一是Peff，*有效参数数量*[24,
    25]。通过使用正则化和模型选择技术，可以控制Peff以平衡偏差和方差。非平稳性：由于世界经济随着时间的发展而演变，宏观经济系列本质上是非平稳的。更复杂的是，许多宏观经济系列的定义会定期更改，测量它们的技术也在变化。此外，关键系列的估计会定期向后修订，随着更好数据的收集或定义的更改而进行调整。经济的基本动态随着时间变化，而测量系列的噪声分布也随之变化。在许多情况下，这种非平稳性缩短了数据系列的可用长度，因为在较旧数据上训练会引入预测偏差。噪声与非平稳性的结合导致了*噪声/非平稳权衡*[23]，在短训练窗口下，由于有限训练数据中的噪声，模型方差或估计误差过大，而在长训练窗口下，则由于非平稳性导致模型偏差或*近似误差*过大。非线性：传统的宏观经济时间序列模型是线性的[12,
    14]。然而，最近几位研究者的工作表明，在某些情况下，非线性可以改善宏观经济预测模型[13, 27, 39, 35, 40]。
- en: '![339_image_0.png](339_image_0.png)'
  id: totrans-4189
  prefs: []
  type: TYPE_IMG
  zh: '![339_image_0.png](339_image_0.png)'
- en: Fig. 16.2. The U.S. Index of Industrial Production and five return series (rates
    of change measured as log differences) for time scales of 1, 3, 6, 9, and 12 months.
    These return series served as the prediction targets for the standard Jan 1950
    - Dec 1979 / Jan 1980 - Jan 1990 benchmark results reported in [27]. The difficulty
    of the prediction task is evidenced by the poor signal to noise ratios and erratic
    behavior of the target series. For the one month returns, the performance of our
    neural network predictor in table 1 suggests that the SNR is around 0.2. For all
    returns series, significant nonstationarities and deviations from normality of
    the noise distributions are present.
  id: totrans-4190
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2。美国工业生产指数和五个收益序列（变化率以对数差异测量），时间尺度为1、3、6、9和12个月。这些收益序列作为1979年12月/1980年1月到1990年1月基准结果的预测目标，详见[27]。预测任务的难度通过目标序列的信噪比差和不稳定行为得以体现。对于一个月的收益，表1中我们神经网络预测器的表现表明，信噪比约为0.2。所有收益序列中，存在显著的非平稳性和噪声分布的偏离正态性。
- en: (See table 16.1 and figures 16.2 and 16.3.) Based upon our own experience, the
    degree of nonlinearity captured by neural network models of macroeconomic series
    tends to be mild [27, 20, 38, 42, 28, 45]. Due to the high noise levels and limited
    data, simpler models are favored. This makes reliable estimation of nonlinearities
    more difficult.
  id: totrans-4191
  prefs: []
  type: TYPE_NORMAL
  zh: （参见表16.1和图16.2及16.3。）根据我们的经验，神经网络模型对宏观经济系列捕获的非线性程度往往较轻[27, 20, 38, 42, 28, 45]。由于高噪声水平和有限数据，更简单的模型更受青睐。这使得可靠估计非线性变得更加困难。
- en: 16.2 A Survey Of Neural Network Solutions
  id: totrans-4192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.2 神经网络解决方案调查
- en: We have been investigating a variety of algorithms for neural network model
    selection that go beyond the *vanilla* neural network approach.1 The goal of this
  id: totrans-4193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在研究多种神经网络模型选择算法，超越*香草*神经网络方法。1 本研究的目标是
- en: 1 We define a *vanilla* neural network to be a fully connected, two-layer sigmoidal
    network with a full set of input variables and a fixed number of hidden units
    that is trained on a data window of fixed length with backprop and early stopping
    using a validation set. No variable selection, pruning, regularization, or committee
    techniques are used.
  id: totrans-4194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将*香草*神经网络定义为一个完全连接的、两层的sigmoid网络，具有完整的输入变量集和固定数量的隐藏单元，使用固定长度的数据窗口进行训练，通过反向传播和早停法结合验证集进行训练。没有使用变量选择、剪枝、正则化或委员会技术。
- en: '![340_image_0.png](340_image_0.png)'
  id: totrans-4195
  prefs: []
  type: TYPE_IMG
  zh: '![340_image_0.png](340_image_0.png)'
- en: 'Fig. 16.3. The U.S. Index of Leading Indicators (DLEAD) and its 11 component
    series as currently defined. The Leading Index is a key tool for forecasting business
    cycles. The input variables for the IP forecasting models included transformed
    versions of DLEAD and several of its components [27]. The difficulty of macroeconomic
    forecasting is again evident, due to the high noise levels and erratic behaviors
    of DLEAD and its components. (Note that the component series included in DLEAD
    have been changed several times during the past 47 years. The labels for the various
    series are those defined in Citibase: HSBP denotes housing starts, FM2D82 is M2
    money supply, FSPCOM is the Standard & Poors 500 stock index, and so on.)'
  id: totrans-4196
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3. 美国领先指标（DLEAD）及其当前定义的11个组成系列。领先指数是预测商业周期的关键工具。IP预测模型的输入变量包括DLEAD及其几个组成部分的变换版本[27]。由于DLEAD及其组成部分的高噪声水平和不规则行为，宏观经济预测的困难再次显现。（注意，DLEAD中包含的组成系列在过去47年中发生了几次变化。各系列的标签是Citibase中定义的：HSBP表示新屋开工，FM2D82是M2货币供应量，FSPCOM是标准普尔500股票指数，等等。）
- en: work is to construct models with minimal prediction risk (expected test set
    error). The techniques that we are developing and testing are described below.
  id: totrans-4197
  prefs: []
  type: TYPE_NORMAL
  zh: 工作的目的是构建具有最小预测风险（预期测试集误差）的模型。我们正在开发和测试的技术如下所述。
- en: Given the brief nature of this survey, I have not attempted to provide an exhaustive
    list of the many relevant references in the literature.
  id: totrans-4198
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于本调查的简要性质，我没有尝试提供文献中众多相关参考文献的详尽列表。
- en: 'Hyperparameter Selection: Hyperparameters are parameters that appear in the
    training objective function, but not in the network itself. Examples include the
    regularization parameter, the training window length, and robust scale parameters.
    Examples of varying the regularization parameter and the training window length
    for a 12 month IP forecasting model are shown in figures 16.4 and 16.5. Varying
    the regularization parameter trades off bias and variance, while varying the training
    window length trades off noise and nonstationarity.'
  id: totrans-4199
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数选择：超参数是在训练目标函数中出现的参数，但不在网络本身中。示例包括正则化参数、训练窗口长度和稳健尺度参数。对于12个月的IP预测模型，图16.4和16.5展示了变化正则化参数和训练窗口长度的示例。变化正则化参数权衡了偏差和方差，而变化训练窗口长度则权衡了噪声和非平稳性。
- en: Table 16.1. Comparative summary of normalized prediction errors for rates of
    return on Industrial Production for the period January 1980 to January 1990 as
    presented in [27]. The four model types were trained on data from January 1950
    to December 1979. The neural network models significantly outperform the trivial
    predictors and linear models. For each forecast horizon, the normalization factor
    is the variance of the target variable for the training period. Nonstationarity
    in the IP series makes the test errors for the trivial predictors larger than
    1.0. In subsequent work, we have obtained substantially better results for the
    IP problem [20, 38, 42, 28, 45].
  id: totrans-4200
  prefs: []
  type: TYPE_NORMAL
  zh: 表16.1. 工业生产的收益率归一化预测误差比较总结，涵盖1980年1月至1990年1月的期间，如[27]所示。四种模型类型使用1950年1月至1979年12月的数据进行训练。神经网络模型显著优于简单预测器和线性模型。对于每个预测水平，归一化因子是训练期间目标变量的方差。IP系列中的非平稳性使得简单预测器的测试误差大于1.0。在后续工作中，我们在IP问题上获得了显著更好的结果
    [20, 38, 42, 28, 45]。
- en: '| Prediction   | Trivial                                   | Univariate   |
    Multivariate Sigmoidal Nets   |               |'
  id: totrans-4201
  prefs: []
  type: TYPE_TB
  zh: '| 预测        | 简单                                    | 单变量      | 多变量Sigmoidal网络        |               |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-4202
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Horizon      | (Average of                               | AR(14) Model |
    Linear Reg.                   | w/ PC Pruning |'
  id: totrans-4203
  prefs: []
  type: TYPE_TB
  zh: '| 水平        | （平均值                                  | AR(14)模型  | 线性回归                     |
    带PC剪枝    |'
- en: '| (Months)     | Training Set) Iterated Pred. Direct Pred. |              |                               |
    Direct Pred.  |'
  id: totrans-4204
  prefs: []
  type: TYPE_TB
  zh: '| （月份）     | 训练集）迭代预测 直接预测          |              |                               |
    直接预测    |'
- en: '| 1            | 1.04                                      | 0.90         |
    0.87                          | 0.81          |'
  id: totrans-4205
  prefs: []
  type: TYPE_TB
  zh: '| 1            | 1.04                                      | 0.90         |
    0.87                          | 0.81          |'
- en: '| 2            | 1.07                                      | 0.97         |
    0.85                          | 0.77          |'
  id: totrans-4206
  prefs: []
  type: TYPE_TB
  zh: '| 2            | 1.07                                      | 0.97         |
    0.85                          | 0.77          |'
- en: '| 3            | 1.09                                      | 1.07         |
    0.96                          | 0.75          |'
  id: totrans-4207
  prefs: []
  type: TYPE_TB
  zh: '| 3            | 1.09                                      | 1.07         |
    0.96                          | 0.75          |'
- en: '| 6            | 1.10                                      | 1.07         |
    1.38                          | 0.73          |'
  id: totrans-4208
  prefs: []
  type: TYPE_TB
  zh: '| 6            | 1.10                                      | 1.07         |
    1.38                          | 0.73          |'
- en: '| 9            | 1.10                                      | 0.96         |
    1.38                          | 0.67          |'
  id: totrans-4209
  prefs: []
  type: TYPE_TB
  zh: '| 9            | 1.10                                      | 0.96         |
    1.38                          | 0.67          |'
- en: '| 12           | 1.12                                      | 1.23         |
    1.20                          | 0.64          |'
  id: totrans-4210
  prefs: []
  type: TYPE_TB
  zh: '| 12           | 1.12                                      | 1.23         |
    1.20                          | 0.64          |'
- en: 'Input Variable Selection and Pruning: Selecting an informative set of input
    variables and an appropriate representation for them ("features") is critical
    to the solution of any forecasting problem. The variable selection and representation
    problem is part of the overall model selection problem. Variable selection procedures
    can be either model-independent or model-dependent. The Delta Test, a model independent
    procedure, is a nonparametric statistical algorithm that selects meaningful predictor
    variables by direct examination of the data set [36]. Other model-independent
    techniques make use of the mutual information'
  id: totrans-4211
  prefs: []
  type: TYPE_NORMAL
  zh: 输入变量选择与剪枝：选择一个信息丰富的输入变量集以及适当的表示（“特征”）对于任何预测问题的解决至关重要。变量选择和表示问题是整体模型选择问题的一部分。变量选择程序可以是模型无关或模型相关的。Delta测试，一种模型无关的程序，是一种非参数统计算法，通过直接检查数据集选择有意义的预测变量
    [36]。其他模型无关的技术利用互信息。
- en: '[4, 5, 46] or joint mutual information [46]. Sensitivity-based pruning (SBP)
    techniques are model-dependent algorithms that prune unnecessary or harmful input
    variables from a trained network [33, 30, 25, 42, 21]. Sensitivity based pruning
    methods are described in greater detail in section 16.4.6.'
  id: totrans-4212
  prefs: []
  type: TYPE_NORMAL
  zh: '[4, 5, 46] 或联合互信息 [46]。基于敏感度的剪枝（SBP）技术是依赖模型的算法，它们从训练过的网络中剪去不必要或有害的输入变量 [33,
    30, 25, 42, 21]。基于敏感度的剪枝方法在16.4.6节中有更详细的描述。'
- en: 'Model Selection and Pruning: A key technique for controlling the bias / variance
    tradeoff for noisy problems is to select the size and architecture of the network.
    For two-layer networks, this includes selecting the number of internal units,
    choosing a connectivity structure, and pruning unneeded nodes, weights, or weight
    matrix eigennodes. A constructive algorithm for selecting the number of internal
    units is sequential network construction (SNC) [2, 30, 25]. Techniques for pruning
    weights and internal nodes include sensitivity-based pruning methods like optimal
    brain damage (OBD) [19] and optimal brain surgeon (OBS) [15]. Our recentlyproposed
    supervised principal components pruning (PCP) method [20] prunes weight matrix
    eigennodes, rather than weights. Since PCP does not require training to a local
    minimum, it can be used with early stopping. It has computational advantages over
    OBS, and can outperform OBD when input variables or hidden node activities are
    noisy and correlated. Figure 16.6 shows reductions in prediction'
  id: totrans-4213
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择与剪枝：控制噪声问题的偏差/方差权衡的关键技术是选择网络的大小和架构。对于两层网络，这包括选择内部单元的数量、选择连接结构，以及剪枝不需要的节点、权重或权重矩阵特征节点。选择内部单元数量的一种构造算法是顺序网络构建（SNC）[2,
    30, 25]。剪枝权重和内部节点的技术包括基于敏感度的剪枝方法，如最佳大脑损伤（OBD）[19]和最佳大脑外科医生（OBS）[15]。我们最近提出的监督主成分剪枝（PCP）方法[20]剪枝权重矩阵特征节点，而不是权重。由于PCP不需要训练到局部最小值，它可以与提前停止一起使用。它在计算上优于OBS，并且在输入变量或隐藏节点活动嘈杂且相关时，可以超越OBD。图16.6显示了预测的减少。
- en: '![342_image_0.png](342_image_0.png)'
  id: totrans-4214
  prefs: []
  type: TYPE_IMG
  zh: '![342_image_0.png](342_image_0.png)'
- en: Fig. 16.4. Example of the *Noise / Nonstationary Tradeoff* and selection of
    the best training window, in this case 10 years [38, 28]. The longer training
    windows of 15 and 20 years yield higher test set error due to the model bias induced
    by nonstationarity. The shorter training windows of 5 and 7 years have significantly
    higher errors due to model variance resulting from noise in the data series and
    smaller data sets. The test errors correspond to models trained with the best
    regularization parameter 0.15 indicated in figure 16.5.
  id: totrans-4215
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4。*噪声/非平稳权衡*的示例及最佳训练窗口的选择，在这种情况下为10年[38, 28]。由于非平稳性引起的模型偏差，15年和20年的较长训练窗口导致更高的测试集错误。5年和7年的较短训练窗口由于数据序列中的噪声和较小的数据集导致显著更高的错误。测试错误对应于在图16.5中指示的最佳正则化参数0.15下训练的模型。
- en: errors obtained by using PCP on a set of IP forecasting models. Section 16.4
    describes the model selection problem and the use of estimates of prediction risk
    such as *nonlinear cross-validation* (NCV) to guide the selection process in greater
    detail.
  id: totrans-4216
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PCP在一组IP预测模型上获得的错误。第16.4节详细描述了模型选择问题以及使用*非线性交叉验证*（NCV）等预测风险估计来指导选择过程。
- en: 'Better Regularizers: Introducing biases in a model via regularization or pruning
    reduces model variance and can thus reduce prediction risk. Prediction risk can
    be best minimized by choosing appropriate biases. One such set of biases are smoothing
    constraints. We have proposed new classes of smoothing regularizers for both feedforward
    and recurrent networks [29, 45] that often yield better performance than the standard
    weight decay approach. These are described in greater detail in section 16.3.
    Committee Forecasts: Due to the extremely noisy nature of economic time series,
    the control of forecast variance is a critical issue. One approach for reducing
    forecast variance is to average the forecasts of a committee of models.'
  id: totrans-4217
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的正则化器：通过正则化或剪枝在模型中引入偏差可减少模型方差，从而降低预测风险。通过选择适当的偏差可以最好地最小化预测风险。其中一组偏差是平滑约束。我们为前馈和递归网络提出了新的平滑正则化器类[29,
    45]，其性能通常优于标准的权重衰减方法。这些在第16.3节中有更详细的描述。委员会预测：由于经济时间序列的极高噪声特性，控制预测方差是一个关键问题。减少预测方差的一种方法是对多个模型的预测进行平均。
- en: 'Researchers in economics have studied and used combined estimators for a long
    time, and generally find that they outperform their component estimators and that
    unweighted averages tend to outperform weighted averages, for a variety of weighting
    methods [12, 44, 6]. Reductions of prediction error variances obtained by unweighted
    committee averaging for a selection of different IP forecasting models are shown
    in figure 16.7. Model Interpretation and Visualization: It is important not only
    to be able to make accurate forecasts, but to also understand what factors influence
    the forecasts that are made. This can be accomplished via the sensitivity analyses
    described in sections 16.4.6 and 16.4.8 and the visualization tool presented in
    section 16.4.8.'
  id: totrans-4218
  prefs: []
  type: TYPE_NORMAL
  zh: 经济学研究人员长期以来一直研究并使用组合估计量，并普遍发现它们的表现优于各个组件估计量，而且在各种加权方法中，无权平均往往优于加权平均[12, 44,
    6]。不同 IP 预测模型的无权委员会平均所获得的预测误差方差降低情况如图 16.7 所示。模型解释与可视化：不仅能够做出准确的预测很重要，还要理解哪些因素影响了所做的预测。这可以通过第
    16.4.6 和 16.4.8 节中描述的敏感性分析以及第 16.4.8 节中介绍的可视化工具来实现。
- en: '![343_image_0.png](343_image_0.png)'
  id: totrans-4219
  prefs: []
  type: TYPE_IMG
  zh: '![343_image_0.png](343_image_0.png)'
- en: Fig. 16.5. Example of the effect of regularization (weight decay) parameter
    on test error [38, 28]. The five curves are for training windows of length 5,
    7, 10, 15, and 20 years. The *Bias / Variance Tradeoff* is clearly evident in
    all the curves; the minimum test set errors occur for weight decay parameters
    of order 0.1. Larger errors due to bias occur for larger weight decay coefficients,
    while larger errors due to model variance occur for smaller values of the coefficient.
  id: totrans-4220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.5. 正则化（权重衰减）参数对测试误差的影响示例[38, 28]。五条曲线代表长度为 5、7、10、15 和 20 年的训练窗口。所有曲线中都明显体现了
    *偏差/方差权衡*；最小的测试集误差发生在权重衰减参数约为 0.1 时。偏差导致的较大误差出现在较大的权重衰减系数下，而模型方差导致的较大误差则出现在较小的系数值下。
- en: 16.3 Smoothing Regularizers For Better Generalization
  id: totrans-4221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.3 平滑正则化器以获得更好的泛化能力
- en: Introducing biases in a model via regularization or pruning reduces model variance
    and can thus reduce prediction risk (see also chapters 2-6). Prediction risk can
    be best minimized by choosing appropriate biases. Quadratic weight decay [37,
    18, 17],
  id: totrans-4222
  prefs: []
  type: TYPE_NORMAL
  zh: 通过正则化或剪枝在模型中引入偏差可以减少模型方差，从而降低预测风险（另见第 2-6 章）。选择适当的偏差可以最佳化预测风险。二次权重衰减 [37, 18,
    17]，
- en: the standard approach to regularization used in the neural nets community, is
    an ad hoc function of the network weights. Weight decay is *ad hoc* in the sense
    that it imposes direct constraints on the weights independent of the nature of
    the function being learned or the parametrization of the network model. A more
    principled approach is to require that the function f(W, x) learned by the network
    be smooth.
  id: totrans-4223
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络社区中使用的标准正则化方法是网络权重的临时函数。权重衰减是 *临时的*，因为它对权重施加直接约束，而不考虑正在学习的函数的性质或网络模型的参数化。更有原则的方法是要求网络学习的函数
    f(W, x) 是光滑的。
- en: This can be accomplished by penalizing the mth order curvature of f(W, x). The
    regularization or penalty functional is then the smoothing integral
  id: totrans-4224
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过惩罚 f(W, x) 的 m 次导数的曲率来实现。正则化或惩罚泛函就是平滑积分。
- en: $$S(W,m)=\int d^{d}\mathbf{x}\Omega(\mathbf{x})\left\|\frac{d^{m}f(W,\mathbf{x})}{d\mathbf{x}^{m}}\right\|^{2}\,\tag{16.1}$$
  id: totrans-4225
  prefs: []
  type: TYPE_NORMAL
  zh: $$S(W,m)=\int d^{d}\mathbf{x}\Omega(\mathbf{x})\left\|\frac{d^{m}f(W,\mathbf{x})}{d\mathbf{x}^{m}}\right\|^{2}\,\tag{16.1}$$
- en: where Ω(x) is a weighting function and   denotes the Euclidean tensor norm.2
    Since numerical computation of (16.1) generally requires expensive Monte Carlo
    integrations and is therefore impractical during training, we have derived algebraically
    simple approximations and bounds to S(*W, m*) for feedforward
  id: totrans-4226
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Ω(x) 是一个加权函数，表示欧几里得张量范数。由于 (16.1) 的数值计算通常需要昂贵的蒙特卡洛积分，因此在训练过程中不切实际，我们推导出了对
    S(*W, m*) 的代数简单近似和界限，用于前馈网络。
- en: 2 The relation of this type of smoothing functional to radial basis functions
    has been studied by [10]. However, the approach developed in that work does not
    extend to standard feedforward sigmoidal networks, which are a special case of
    projective basis function networks (PBF's).
  id: totrans-4227
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的平滑泛函与径向基函数的关系已被研究[10]。然而，该研究中开发的方法并未扩展到标准的前馈 sigmoid 网络，这是一种投影基函数网络 (PBF)
    的特例。
- en: '![344_image_0.png](344_image_0.png)'
  id: totrans-4228
  prefs: []
  type: TYPE_IMG
  zh: '![344_image_0.png](344_image_0.png)'
- en: Fig. 16.6. Prediction errors for two sets of neural network models for 12 month
    returns for IP, with (dotted line) and without (solid line) Supervised Principal
    Components Pruning (PCP) [20]. Each data point is the mean error for 11 nets,
    while the error bars represent one standard deviation. Statistically significant
    improvements in prediction performance are obtained for the 6, 9, and 12 month
    prediction horizons by using the PCP algorithm to reduce the network complexities.
    While techniques like optimal brain damage and optimal brain surgeon prune weights
    from the network, PCP reduces network complexity and hence model variance by pruning
    eigennodes of the weight matrices. Unlike the unsupervised use of principal components,
    PCP removes those eigennodes that yield the greatest reduction in estimated prediction
    error.
  id: totrans-4229
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6. 对于IP的12个月收益，两组神经网络模型的预测误差，带（虚线）和不带（实线）监督主成分修剪（PCP）[20]。每个数据点是11个网络的平均误差，而误差条代表一个标准差。通过使用PCP算法来降低网络复杂性，在6、9和12个月的预测期内获得了统计显著的预测性能改善。尽管像最优脑损伤和最优脑外科医生这样的技术从网络中修剪权重，PCP通过修剪权重矩阵的特征节点来降低网络复杂性，从而减少模型方差。与无监督使用主成分不同，PCP去除了那些能最大限度减少估计预测误差的特征节点。
- en: networks that can be easily evaluated at each training step [29]. For these
    new classes of algebraically simple mth-order smoothing regularizers for networks
    of projective basis functions (PBF's) f(W, x) = Nj=1 ujg
  id: totrans-4230
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在每个训练步骤中轻松评估的网络[29]。对于这些新的代数上简单的m阶平滑正则化器，适用于投影基函数（PBF's）f(W, x) = Nj=1 ujg
- en: xT vj + vj0
  id: totrans-4231
  prefs: []
  type: TYPE_NORMAL
  zh: xT vj + vj0
- en: '+ u0, W = (*u, v*) with general transfer functions g[·], the regularizers are:'
  id: totrans-4232
  prefs: []
  type: TYPE_NORMAL
  zh: + u0，W = (*u, v*)，具有一般传递函数g[·]，正则化器为：
- en: $$R_{G}(W,m)=\sum_{j=1}^{N}u_{j}^{2}\|\mathbf{v}_{j}\|^{2m-1}\quad\quad\mathrm{Global\Form}$$
    $$R_{L}(W,m)=\sum_{j=1}^{N}u_{j}^{2}\|\mathbf{v}_{j}\|^{2m}\quad\quad\quad\mathrm{Local\
    Form}.$$
  id: totrans-4233
  prefs: []
  type: TYPE_NORMAL
  zh: $$R_{G}(W,m)=\sum_{j=1}^{N}u_{j}^{2}\|\mathbf{v}_{j}\|^{2m-1}\quad\quad\mathrm{Global\Form}$$
    $$R_{L}(W,m)=\sum_{j=1}^{N}u_{j}^{2}\|\mathbf{v}_{j}\|^{2m}\quad\quad\quad\mathrm{Local\
    Form}.$$
- en: Our empirical experience shows that these new smoothing regularizers typically
    yield better prediction accuracies than standard weight decay.
  id: totrans-4234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实证经验表明，这些新的平滑正则化器通常比标准的权重衰减产生更好的预测精度。
- en: In related work, we have derived an algebraically-simple regularizer for recurrent
    nets [45]. This regularizer can be viewed as a generalization of the first order
    Tikhonov stabilizer (the m = 1 *local form* above) to dynamic models. For two
    layer networks with recurrent connections described by
  id: totrans-4235
  prefs: []
  type: TYPE_NORMAL
  zh: 在相关工作中，我们为递归网络推导出了一种代数上简单的正则化器[45]。这个正则化器可以视为对动态模型的一阶Tikhonov稳定器（上面的m = 1*局部形式*）的推广。对于描述为
- en: $$Y(t)=\mathbf{g}\left(A Y(t-\tau)+V X(t)\right)\;,\;\hat{Z}(t)=U Y(t)\;,$$
  id: totrans-4236
  prefs: []
  type: TYPE_NORMAL
  zh: $$Y(t)=\mathbf{g}\left(A Y(t-\tau)+V X(t)\right)\;,\;\hat{Z}(t)=U Y(t)\;,$$
- en: '![345_image_0.png](345_image_0.png)'
  id: totrans-4237
  prefs: []
  type: TYPE_IMG
  zh: '![345_image_0.png](345_image_0.png)'
- en: Fig. 16.7. Reduction in error variance for prediction of the U.S.Index of Industrial
    Production by use of combining forecasts (or committees) [38, 28]. Abscissa points
    are various combinations of prediction horizon and test period. For example, "m12.80"
    denotes networks trained to make 12 month forecasts on the ten years prior to
    1979 and tested by making true *ex ante* forecasts on the year 1980. Performance
    metric is normalized mean square error (NMSE) computed over the particular year.
    All training sets have length 10 years. For each point, bars show range of values
    for either 1000 individual models, or 100 committees of 10. The individual networks
    each have three sigmoidal internal units, one linear output, and typically a dozen
    or so input variables selected by the δ-test from an initial set of 48 candidate
    variables.
  id: totrans-4238
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7. 使用组合预测（或委员会）对美国工业生产指数预测的误差方差减少[38, 28]。横坐标点是不同的预测期和测试期的组合。例如，“m12.80”表示在1979年前十年训练以进行12个月预测的网络，并通过对1980年的真实*事先*预测进行测试。性能指标是特定年份的归一化均方误差（NMSE）。所有训练集的长度为10年。对于每个点，条形显示1000个单独模型或100个由10个模型组成的委员会的值范围。每个单独网络通常有三个sigmoidal内部单元，一个线性输出，以及通过δ测试从48个候选变量的初始集合中选择的十几个输入变量。
- en: the training criterion with the regularizer is
  id: totrans-4239
  prefs: []
  type: TYPE_NORMAL
  zh: 训练标准的正则化器是
- en: $${\mathcal{E}}={\frac{1}{N}}\sum_{t=1}^{N}||Z(t)-{\hat{Z}}(W,I(t))||^{2}+\lambda{\rho_{\tau}}^{2}(W)\;,$$
  id: totrans-4240
  prefs: []
  type: TYPE_NORMAL
  zh: $${\mathcal{E}}={\frac{1}{N}}\sum_{t=1}^{N}||Z(t)-{\hat{Z}}(W,I(t))||^{2}+\lambda{\rho_{\tau}}^{2}(W)\;,$$
- en: 'where W = {*U, V, A*} is the network parameter set, Z(t) are the targets, I(t)
    = {X(s), s = 1, 2, ··· , t} represents the current and all historical input information,
    N is the size of the training data set, ρτ 2(W) is the regularizer, and λ is a
    regularization parameter. The closed-form expression for the regularizer for timelagged
    recurrent networks is:'
  id: totrans-4241
  prefs: []
  type: TYPE_NORMAL
  zh: 其中W = {*U, V, A*}是网络参数集，Z(t)是目标，I(t) = {X(s), s = 1, 2, ··· , t}表示当前和所有历史输入信息，N是训练数据集的大小，ρτ
    2(W)是正则化器，λ是正则化参数。时滞递归网络的正则化器的闭式表达为：
- en: $$\rho_{\tau}(W)=\frac{\gamma||U||||V||}{1-\gamma||A||}\left[1-e^{\frac{\gamma||A||-1}{\tau}}\right].$$
  id: totrans-4242
  prefs: []
  type: TYPE_NORMAL
  zh: $$\rho_{\tau}(W)=\frac{\gamma||U||||V||}{1-\gamma||A||}\left[1-e^{\frac{\gamma||A||-1}{\tau}}\right].$$
- en: where *|| ||* is the Euclidean matrix norm and γ is a factor which depends upon
    the maximal value of the first derivatives of the internal unit activations g(
    ). Simplifications of the regularizer can be obtained for simultaneous recurrent
    nets (τ → 0), two-layer feedforward nets, and one layer linear nets. We have
  id: totrans-4243
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*|| ||*是欧几里得矩阵范数，γ是一个因子，依赖于内部单元激活g( )的首导数的最大值。对于同时递归网络（τ → 0）、两层前馈网络和一层线性网络，可以得到正则化器的简化形式。我们有
- en: '![346_image_0.png](346_image_0.png)'
  id: totrans-4244
  prefs: []
  type: TYPE_IMG
  zh: '![346_image_0.png](346_image_0.png)'
- en: Fig. 16.8. Regularization parameter vs. normalized prediction errors for the
    task of predicting the one month rates of change of the U.S. Index of Industrial
    Production [45]. The example given is for a recurrent network trained with standard
    weight decay (left) or with the new recurrent smoothing regularizer (right). For
    standard weight decay, the optimal regularization parameter is 0.03 corresponding
    to a test error of 0.734. For the new smoothing regularizer, the optimal regularization
    parameter which leads to the least validation error is 0.8 corresponding to a
    test error of 0.646. The new recurrent regularizer thus yields a 12% reduction
    in test error relative to that obtained using quadratic weight decay.
  id: totrans-4245
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.8. 正则化参数与预测美国工业生产指数一个月变化率的标准化预测误差的关系[45]。给出的例子是使用标准权重衰减（左）或新递归平滑正则化器（右）训练的递归网络。对于标准权重衰减，最优正则化参数为0.03，对应测试误差为0.734。对于新的平滑正则化器，最优正则化参数为0.8，对应测试误差为0.646。因此，新递归正则化器相对于使用二次权重衰减获得的测试误差减少了12%。
- en: successfully tested this regularizer in a number of case studies and found that
    it performs better than standard quadratic weight decay. A comparison of this
    recurrent regularizer to quadratic weight decay for 1 month forecasts of IP is
    shown in figure 16.8.
  id: totrans-4246
  prefs: []
  type: TYPE_NORMAL
  zh: 在若干案例研究中成功测试了该正则化器，并发现其性能优于标准的二次权重衰减。图16.8展示了这种递归正则化器与二次权重衰减在预测工业生产的1个月预测中的比较。
- en: 16.4 Model Selection And Interpretation
  id: totrans-4247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.4 模型选择与解释
- en: In this section, we provide a more in-depth discussion of several issues and
    techniques for neural network model selection, including the problem of selecting
    inputs. We describe techniques for selecting architectures via estimates of the
    prediction risk, especially the *generalized prediction error (GPE)* and *nonlinear*
    cross-validation (NCV). We present *sensitivity-based pruning (SBP)* methods for
    selecting input variables, and demonstrate the use of these methods for predicting
    the U.S. Index of Industrial Production. Finally, we discuss some approaches to
    model interpretation and visualization that enable an understanding of economic
    relationships.
  id: totrans-4248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入讨论神经网络模型选择的几个问题和技术，包括选择输入的问题。我们描述了通过预测风险的估计选择架构的技术，特别是*广义预测误差（GPE）*和*非线性*交叉验证（NCV）。我们介绍了*基于敏感度的剪枝（SBP）*方法来选择输入变量，并展示了这些方法在预测美国工业生产指数中的应用。最后，我们讨论了一些模型解释和可视化的方法，以便理解经济关系。
- en: 16.4.1 Improving Forecasts Via Architecture And Input Selection
  id: totrans-4249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.4.1 通过架构和输入选择改善预测
- en: 'For the discussion of architecture selection in this paper, we focus on the
    most widely used neural network architecture, the two-layer *perceptron* (or *backpropagation*)
    network. The response function for such a network with architecture λ having Iλ
    input variables, Hλ internal (hidden) neurons, and a single output is:'
  id: totrans-4250
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中关于架构选择的讨论中，我们重点关注最广泛使用的神经网络架构——两层*感知器*（或*反向传播*）网络。对于具有Iλ输入变量、Hλ内部（隐藏）神经元和单个输出的架构λ，这种网络的响应函数为：
- en: $$\hat{\mu}_{\lambda}(\mathbf{x})=h(\,u_{0}+\sum_{j=1}^{H_{\lambda}}u_{j}\,g(v_{j0}+\sum_{i=1}^{I_{\lambda}}v_{j
    i}\,x_{i}))\ .$$
  id: totrans-4251
  prefs: []
  type: TYPE_NORMAL
  zh: $$\hat{\mu}_{\lambda}(\mathbf{x})=h(\,u_{0}+\sum_{j=1}^{H_{\lambda}}u_{j}\,g(v_{j0}+\sum_{i=1}^{I_{\lambda}}v_{j
    i}\,x_{i}))\ .$$
- en: $$(16.2)$$
  id: totrans-4252
  prefs: []
  type: TYPE_NORMAL
  zh: $$(16.2)$$
- en: Here, h and g are typically sigmoidal nonlinearities, the vji and vj0 are input
    weights and thresholds, the uj and u0 are the output weights and threshold, and
    the index λ is an abstract label for the specific two layer perceptron network
    architecture. While we consider for simplicity this restricted class of perceptron
    networks in this section, our approach can be easily generalized to networks with
    multiple outputs and multiple layers.
  id: totrans-4253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，h和g通常是sigmoid非线性函数，vji和vj0是输入权重和阈值，uj和u0是输出权重和阈值，而索引λ是特定双层感知器网络架构的抽象标签。虽然我们在本节中考虑的是这一限制的感知器网络类，但我们的方法可以轻松推广到具有多个输出和多层的网络。
- en: For two layer perceptrons, the *architecture selection problem* is to find a
    good, near-optimal architecture λ for modeling a given data set. The architecture
    λ is characterized by the number of hidden units Hλ, the subset of input variables
    Iλ, and the subset of weights uj and vji that are non-zero. If all of the uj and
    vji are non-zero, the network is referred to as *fully connected*. Since an exhaustive
    search over the space of possible architectures is impossible, the procedure for
    selecting this architecture requires a heuristic search. See Figure 16.9 for examples
    of heuristic search strategies and [25] and [30] for additional discussion.
  id: totrans-4254
  prefs: []
  type: TYPE_NORMAL
  zh: 对于双层感知器，*架构选择问题*是找到一个好的、近似最优的架构λ，以建模给定的数据集。架构λ的特征包括隐藏单元数Hλ、输入变量子集Iλ以及非零权重uj和vji的子集。如果所有的uj和vji都是非零的，则网络被称为*全连接*。由于对所有可能架构空间进行穷举搜索是不可能的，因此选择此架构的过程需要启发式搜索。有关启发式搜索策略的示例，请参见图16.9，并查阅[25]和[30]以获得更多讨论。
- en: '![347_image_0.png](347_image_0.png)'
  id: totrans-4255
  prefs: []
  type: TYPE_IMG
  zh: '![347_image_0.png](347_image_0.png)'
- en: 'Fig. 16.9. Heuristic Search Strategies: After selecting the number of hidden
    units Hλ, the input removal and weight elimination can be carried out in parallel
    (A) or sequentially (B). In (B), the selection of the number of hidden units and
    removal of inputs may be iterated (dashed line).'
  id: totrans-4256
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.9. 启发式搜索策略：在选择隐藏单元数量Hλ后，可以并行（A）或顺序（B）进行输入移除和权重消除。在（B）中，隐藏单元数量的选择和输入移除可能会迭代进行（虚线）。
- en: In this section, we focus on selecting the "best subset" of input variables
    for predicting the U.S. Index of Industrial Production. In order to avoid an exhaustive
    search over the exponentially-large space of architectures obtained by considering
    all possible combinations of inputs, we employ a directed search strategy using
    the sensitivity-based input pruning (SBP) algorithm (see section 16.4.6).
  id: totrans-4257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们专注于选择用于预测美国工业生产指数的“最佳子集”输入变量。为了避免在考虑所有可能输入组合时对指数级大的架构空间进行穷举搜索，我们采用了一种基于灵敏度的输入剪枝（SBP）算法的定向搜索策略（见第16.4.6节）。
- en: 16.4.2 Architecture Selection Via The Prediction Risk
  id: totrans-4258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.4.2 通过预测风险选择架构
- en: The notion of "best fits" can be captured via an objective criterion; such as
    *maximum* a posteriori *probability (MAP)*, minimum Bayesian information criterion
  id: totrans-4259
  prefs: []
  type: TYPE_NORMAL
  zh: “最佳拟合”的概念可以通过一个客观标准来捕捉，例如*最大*后验*概率（MAP）*、最小贝叶斯信息准则（BIC）。
- en: (BIC), *minimum description length (MDL)*, or *generalization ability*. The
    generalization ability can be defined precisely as the *prediction risk* Pλ, the
    expected performance of an estimator in predicting new observations. In this section,
    we use the prediction risk as our selection criterion for two reasons. First,
    it is straightforward to compute, and second, it provides more information than
    MAP, BIC, or MDL, since it tells us how much confidence to put in predictions
    produced by our best model.
  id: totrans-4260
  prefs: []
  type: TYPE_NORMAL
  zh: （BIC）、*最小描述长度（MDL）*或*泛化能力*。泛化能力可以精确定义为*预测风险* Pλ，即估计器在预测新观察值时的预期表现。在本节中，我们使用预测风险作为我们的选择标准有两个原因。首先，计算起来简单；其次，它提供比MAP、BIC或MDL更多的信息，因为它告诉我们对最佳模型产生的预测应该有多大的信心。
- en: Consider a set of observations D = {(xj , tj); j = 1 *...N*} that are assumed
    to be generated as tj = μ(xj ) + j where μ(x) is an unknown function, the inputs
    xj are drawn independently with an unknown stationary probability density function
    p(x), the j are independent random variables with zero mean (¯ = 0)
  id: totrans-4261
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一组观察值D = {(xj , tj); j = 1 *...N*}，假设生成方式为tj = μ(xj ) + j，其中μ(x)是一个未知函数，输入xj是独立抽取的，具有未知的平稳概率密度函数p(x)，j是均值为零的独立随机变量（¯
    = 0）。
- en: and variance σ2 , and the tj are the observed target values. The learning or
    regression problem is to find an estimate μˆλ(x; D) of μ(x) given the data set
    D from a class of predictors or models μλ(x) indexed by λ. In general, λ ∈ Λ =
  id: totrans-4262
  prefs: []
  type: TYPE_NORMAL
  zh: 以及方差 σ2 ，tj 是观察到的目标值。学习或回归问题是要找到一个关于数据集 D 的 μˆλ(x; D) 的估计，该数据集来自按 λ 索引的一类预测器或模型
    μλ(x)。一般来说，λ ∈ Λ =
- en: (*S, A, W*), where S ⊂ X denotes a chosen subset of the set of available input
    variables X, A is a selected architecture within a class of model architectures
    A,
  id: totrans-4263
  prefs: []
  type: TYPE_NORMAL
  zh: (*S, A, W*), 其中 S ⊂ X 表示所选的可用输入变量 X 的子集，A 是所选的模型架构类 A 中的一个架构，
- en: and W are the adjustable parameters (weights) of architecture A.
  id: totrans-4264
  prefs: []
  type: TYPE_NORMAL
  zh: W 是架构 A 的可调参数（权重）。
- en: The *prediction risk* Pλ (defined above) can be approximated by the expected
    performance on a finite test set. Pλ can be defined for a variety of loss functions.
  id: totrans-4265
  prefs: []
  type: TYPE_NORMAL
  zh: 上述定义的 *预测风险* Pλ 可以通过有限测试集的预期性能进行近似。Pλ 可以为多种损失函数定义。
- en: 'For the special case of squared error, it is:'
  id: totrans-4266
  prefs: []
  type: TYPE_NORMAL
  zh: 对于平方误差的特殊情况，它是：
- en: $$P_{\lambda}=\int dx\ p(\mathbf{x})[\mu(\mathbf{x})-\hat{\mu}(\mathbf{x})]^{2}+\sigma_{\epsilon}^{2}\tag{16.3}$$
    $$\approx\ E\{\frac{1}{N}\sum_{j=1}^{N}(t_{j}^{*}-\hat{\mu}_{\lambda}(\mathbf{x}_{j}^{*}))^{2}\}\tag{16.4}$$
  id: totrans-4267
  prefs: []
  type: TYPE_NORMAL
  zh: $$P_{\lambda}=\int dx\ p(\mathbf{x})[\mu(\mathbf{x})-\hat{\mu}(\mathbf{x})]^{2}+\sigma_{\epsilon}^{2}\tag{16.3}$$
    $$\approx\ E\{\frac{1}{N}\sum_{j=1}^{N}(t_{j}^{*}-\hat{\mu}_{\lambda}(\mathbf{x}_{j}^{*}))^{2}\}\tag{16.4}$$
- en: where (x∗j , t∗j ) are new observations that were not used in constructing μˆλ(x).
  id: totrans-4268
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 (x∗j , t∗j ) 是未用于构建 μˆλ(x) 的新观察值。
- en: In what follows, we shall use Pλ as a measure of the generalization ability
    of a model. Our strategy is to choose an architecture λ in the model space Λ which
    minimizes an estimate of the prediction risk Pλ.
  id: totrans-4269
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 Pλ 作为模型泛化能力的度量。我们的策略是选择模型空间 Λ 中的架构 λ，以最小化预测风险 Pλ 的估计。
- en: 16.4.3 Estimation Of Prediction Risk
  id: totrans-4270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.4.3 预测风险估计
- en: Since it is not possible to exactly calculate the prediction risk Pλ given only
    a finite sample of data, we have to estimate it. The restriction of limited data
    makes the model selection and prediction risk estimation problems more difficult.
    This is the typical situation in economic forecasting, where the time series are
    short.
  id: totrans-4271
  prefs: []
  type: TYPE_NORMAL
  zh: 由于仅根据有限样本数据无法准确计算预测风险 Pλ，因此我们必须对其进行估计。有限数据的限制使模型选择和预测风险估计问题更为复杂。这是经济预测中的典型情况，时间序列较短。
- en: A limited training set results in a more severe bias/variance (or underfitting
    vs overfitting) tradeoff (see e.g. [9]), so the model selection problem is both
    more challenging and more crucial. In particular, it is easier to overfit a small
    training set, so care must be taken to select a model that is not too large. Also,
    limited data sets make prediction risk estimation more difficult if there is not
    enough data available to hold out a sufficiently large independent test sample.
    In such situations, one must use alternative approaches which enable the estimation
    of prediction risk from the training data, such as data resampling and algebraic
    estimation techniques. Data resampling methods include nonlinear refinements of
    ν–
  id: totrans-4272
  prefs: []
  type: TYPE_NORMAL
  zh: 有限的训练集导致更严重的偏差/方差（或欠拟合与过拟合）权衡（参见 e.g. [9]），因此模型选择问题既更具挑战性又更为关键。尤其是，过拟合小型训练集的风险更高，因此必须谨慎选择一个不太大的模型。此外，如果没有足够的数据来保留足够大的独立测试样本，有限的数据集会使预测风险估计更加困难。在这种情况下，必须使用替代方法，通过训练数据估计预测风险，例如数据重采样和代数估计技术。数据重采样方法包括
    ν– 的非线性细化。
- en: fold cross–validation (NCV) and *bootstrap estimation*, while algebraic estimates
    (in the regression context) include Akaike's *final prediction error (FPE)* [1],
    for linear models, and the recently proposed *generalized prediction error (GPE)*
    for nonlinear models [31, 24, 25], which is identical to the independently-derived
    network information criterion [34]. For comprehensive discussions of prediction
    risk estimation, see [8, 16, 43, 25].
  id: totrans-4273
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证（NCV）和 *自助估计*，而代数估计（在回归上下文中）包括赤池信息量准则的 *最终预测误差 (FPE)* [1]，用于线性模型，以及最近提出的非线性模型的
    *广义预测误差 (GPE)* [31, 24, 25]，其与独立推导的网络信息准则相同 [34]。有关预测风险估计的全面讨论，请参见 [8, 16, 43,
    25]。
- en: 16.4.4 Algebraic Estimates Of Prediction Risk
  id: totrans-4274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.4.4 预测风险的代数估计
- en: Predicted Squared Error for Linear Models. For linear regression models with
    the squared error loss function, a number of useful algebraic estimates for the
    prediction risk have been derived. These include the well known *generalized*
    cross–validation (GCV) [7, 11] and Akaike's *final prediction error (FPE)* [1]
  id: totrans-4275
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型的预测平方误差。对于平方误差损失函数的线性回归模型，已推导出一些有用的预测风险代数估计。这些包括著名的*广义*交叉验证（GCV）[7, 11]和赤池的*最终预测误差（FPE）*[1]
- en: 'formulas:'
  id: totrans-4276
  prefs: []
  type: TYPE_NORMAL
  zh: 公式：
- en: $$G C V_{\lambda}=A S E_{\lambda}{\frac{1}{\left(1-{\frac{Q_{\lambda}}{N}}\right)^{2}}}\qquad\qquad
    F P E_{\lambda}=A S E_{\lambda}\left({\frac{1+{\frac{Q_{\lambda}}{N}}}{1-{\frac{Q_{\lambda}}{N}}}}\right)\
    .$$
  id: totrans-4277
  prefs: []
  type: TYPE_NORMAL
  zh: $$G C V_{\lambda}=A S E_{\lambda}{\frac{1}{\left(1-{\frac{Q_{\lambda}}{N}}\right)^{2}}}\qquad\qquad
    F P E_{\lambda}=A S E_{\lambda}\left({\frac{1+{\frac{Q_{\lambda}}{N}}}{1-{\frac{Q_{\lambda}}{N}}}}\right)\
    .$$
- en: . (16.5)
  id: totrans-4278
  prefs: []
  type: TYPE_NORMAL
  zh: . (16.5)
- en: 'Qλ denotes the number of weights of model λ (ASEλ denotes the average squared
    error). Note that although GCV and FPE are slightly different for small sample
    sizes, they are asymptotically equivalent for large N:'
  id: totrans-4279
  prefs: []
  type: TYPE_NORMAL
  zh: Qλ表示模型λ的权重数量（ASEλ表示平均平方误差）。注意，尽管对于小样本大小，GCV和FPE略有不同，但在大N下它们是渐近等价的：
- en: $$(16.5)$$
  id: totrans-4280
  prefs: []
  type: TYPE_NORMAL
  zh: $$(16.5)$$
- en: $$G C V_{\lambda}\approx F P E_{\lambda}\approx A S E_{\lambda}\left(1+2{\frac{Q_{\lambda}}{N}}\right)$$
  id: totrans-4281
  prefs: []
  type: TYPE_NORMAL
  zh: $$G C V_{\lambda}\approx F P E_{\lambda}\approx A S E_{\lambda}\left(1+2{\frac{Q_{\lambda}}{N}}\right)$$
- en: (16.6)
  id: totrans-4282
  prefs: []
  type: TYPE_NORMAL
  zh: (16.6)
- en: 'A more general expression of *predicted squared error (PSE)* is:'
  id: totrans-4283
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般的*预测平方误差 (PSE)* 表达式为：
- en: $$(16.6)$$
  id: totrans-4284
  prefs: []
  type: TYPE_NORMAL
  zh: $$(16.6)$$
- en: $$P S E_{\lambda}=A S E_{\lambda}+2\hat{\sigma}^{2}\frac{Q_{\lambda}}{N},$$
  id: totrans-4285
  prefs: []
  type: TYPE_NORMAL
  zh: $$P S E_{\lambda}=A S E_{\lambda}+2\hat{\sigma}^{2}\frac{Q_{\lambda}}{N},$$
- en: $$(16.7)$$
  id: totrans-4286
  prefs: []
  type: TYPE_NORMAL
  zh: $$(16.7)$$
- en: where σ%2 is an estimate of the noise variance in the data. Estimation strategies
    for
  id: totrans-4287
  prefs: []
  type: TYPE_NORMAL
  zh: 其中σ%2是数据中噪声方差的估计值。对
- en: (16.7) and its statistical properties have been analyzed by [3]. FPE is obtained
    as special case of PSE by setting σ%2 ≡ ASEλ/(N − Qλ). See [8, 16, 43] for tutorial
    treatments.
  id: totrans-4288
  prefs: []
  type: TYPE_NORMAL
  zh: (16.7)及其统计特性的估计策略已被分析[3]。通过设置σ%2 ≡ ASEλ/(N − Qλ)，可以将FPE作为PSE的特例获得。有关教程处理，请参见[8,
    16, 43]。
- en: 'It should be noted that PSE, FPE and GCV are asymptotically unbiased estimates
    of the prediction risk for the neural network models considered here under certain
    conditions. These are: (1) the noise j in the observed targets tj is independent
    and identically distributed, (2) the resulting model is unbiased, (3)'
  id: totrans-4289
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，在特定条件下，PSE、FPE和GCV是所考虑神经网络模型的预测风险的渐近无偏估计。这些条件为：(1) 观察目标tj中的噪声j是独立同分布的，(2)
    所得模型是无偏的，(3)
- en: weight decay is not used, and (4) the nonlinearity in the model can be neglected.
  id: totrans-4290
  prefs: []
  type: TYPE_NORMAL
  zh: 不使用权重衰减，且(4)模型中的非线性可以忽略。
- en: For PSE, we further require that an asymptotically unbiased estimate of σ%2
    is used. In practice, however, essentially all neural network fits to data will
    be biased and/or have significant nonlinearity.
  id: totrans-4291
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PSE，我们进一步要求使用渐近无偏的σ%2估计。然而，实际上，几乎所有神经网络对数据的拟合都是有偏的和/或具有显著的非线性。
- en: Although PSE, FPE and GCV are asymptotically unbiased only under the above assumptions,
    they are much cheaper to compute than NCV since no retraining is required.
  id: totrans-4292
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管PSE、FPE和GCV仅在上述假设下是渐近无偏的，但它们的计算成本远低于NCV，因为不需要重新训练。
- en: Generalized Prediction Error (GPE) for Nonlinear Models. The predicted squared
    error PSE, and therefore the final prediction error FPE, are special cases of
    the *generalized prediction error GPE* [31, 24, 25]. We present an abbreviated
    description here.
  id: totrans-4293
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性模型的广义预测误差（GPE）。预测平方误差PSE，因此最终预测误差FPE，是*广义预测误差GPE*的特例[31, 24, 25]。我们在此提供简要描述。
- en: GP E estimates the prediction risk for biased nonlinear models which may use
    general loss functions and include regularizers such as weight decay. The algebraic
    form is
  id: totrans-4294
  prefs: []
  type: TYPE_NORMAL
  zh: GP E估计了可能使用一般损失函数和包含正则化项（如权重衰减）的有偏非线性模型的预测风险。其代数形式为
- en: $$G P E_{\lambda}\equiv{\mathcal E}_{\lambda\mathrm{train}}+{\frac{2}{N}}\,\mathrm{tr}\,{\hat{V}}{\hat{G}}_{\lambda}\;\;,$$
  id: totrans-4295
  prefs: []
  type: TYPE_NORMAL
  zh: $$G P E_{\lambda}\equiv{\mathcal E}_{\lambda\mathrm{train}}+{\frac{2}{N}}\,\mathrm{tr}\,{\hat{V}}{\hat{G}}_{\lambda}\;\;,$$
- en: where Eλtrain is the training set error (average value of loss function on training
    set), V% is a nonlinear generalization of the estimated *noise covariance matrix*
    of the observed targets, and G%λ is the estimated *generalized influence matrix*,
    a nonlinear analog of the standard influence or hat matrix.
  id: totrans-4296
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Eλtrain是训练集误差（训练集上损失函数的平均值），V%是观察目标的*噪声协方差矩阵*的非线性推广，G%λ是估计的*广义影响矩阵*，它是标准影响矩阵或帽矩阵的非线性类似物。
- en: 'GPE can be expressed in an equivalent form as:'
  id: totrans-4297
  prefs: []
  type: TYPE_NORMAL
  zh: GPE可以用等效形式表示为：
- en: $$(16.8)$$
  id: totrans-4298
  prefs: []
  type: TYPE_NORMAL
  zh: $$(16.8)$$
- en: $$GPE_{\lambda}={\cal E}_{\lambda{\rm train}}+2\;\widehat{\sigma}_{\it eff}^{2}\;\frac{\widehat{Q}_{\lambda{\it
    eff}}}{N},\tag{16.9}$$
  id: totrans-4299
  prefs: []
  type: TYPE_NORMAL
  zh: $$GPE_{\lambda}={\cal E}_{\lambda{\rm train}}+2\;\widehat{\sigma}_{\it eff}^{2}\;\frac{\widehat{Q}_{\lambda{\it
    eff}}}{N},\tag{16.9}$$
- en: where Q%eff ≡ tr G% is the estimated *effective* number of model parameters,
    and σ%2eff ≡ (tr V%G%)/(tr G%) is the estimated effective noise variance in the
    data. For nonlinear and/or regularized models, Q%*λeff* is generally not equal
    to the number of weights Qλ.
  id: totrans-4300
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Q%eff ≡ tr G% 是模型参数的估计 *有效* 数量，σ%2eff ≡ (tr V%G%)/(tr G%) 是数据中噪声方差的估计有效值。对于非线性和/或正则化模型，Q%*λeff*
    通常不等于权重数量 Qλ。
- en: 'When the noise in the target variables is assumed to be independent with uniform
    variance and the squared error loss function is used, (16.9) simplifies to:'
  id: totrans-4301
  prefs: []
  type: TYPE_NORMAL
  zh: 当假定目标变量中的噪声是独立的且具有均匀方差，并且使用平方误差损失函数时，(16.9) 简化为：
- en: $$G P E_{\lambda}=A S E_{\lambda}+2\hat{\sigma}^{2}\frac{\hat{Q}_{\lambda e
    f f}}{N}\;\;.$$
  id: totrans-4302
  prefs: []
  type: TYPE_NORMAL
  zh: $$G P E_{\lambda}=A S E_{\lambda}+2\hat{\sigma}^{2}\frac{\hat{Q}_{\lambda e
    f f}}{N}\;\;.$$
- en: $$(16.10)$$
  id: totrans-4303
  prefs: []
  type: TYPE_NORMAL
  zh: $$(16.10)$$
- en: Note that replacing Q%*λeff* with Qλ gives the expression for PSE. Various other
    special cases of (16.8) and (16.10) have been derived by other authors and can
    be found in [8, 16, 43]. *GP E* was independently derived by [34], who called
    it the Network Information Criterion (NIC).
  id: totrans-4304
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，使用 Qλ 替代 Q%*λeff* 可以得到 PSE 的表达式。其他作者已推导出 (16.8) 和 (16.10) 的各种特殊情况，详见 [8,
    16, 43]。*GP E* 由 [34] 独立推导，并称其为网络信息准则（NIC）。
- en: '16.4.5 Ncv: Cross-Validation For Nonlinear Models'
  id: totrans-4305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.4.5 Ncv：非线性模型的交叉验证
- en: Cross-validation (CV) is a sample re–use method for estimating prediction risk;
    it makes maximally efficient use of the available data. A perturbative refinement
    of CV for nonlinear models is called *nonlinear cross-validation (NCV)* [25, 30].
  id: totrans-4306
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证（CV）是一种样本重用方法，用于估计预测风险；它最大限度地有效利用可用数据。对非线性模型的 CV 进行微扰性细化称为 *非线性交叉验证（NCV）*
    [25, 30]。
- en: '![351_image_0.png](351_image_0.png)'
  id: totrans-4307
  prefs: []
  type: TYPE_IMG
  zh: '![351_image_0.png](351_image_0.png)'
- en: Fig. 16.10. A nonlinear model can have many local minima in the error function.
    Each local minimum wi, wj and wk (solid curve) corresponds to a different set
    of parameters and thus to a different model. Training on a different finite sample
    of data or retraining on a subsample, as in nonlinear cross-validation, gives
    rise to a slightly different error curve (dashed) and perturbed minima wi, wj
    and wk. Variations due to data sampling in error curves and their minima are termed
    *model variance*.
  id: totrans-4308
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.10。非线性模型的误差函数可能具有许多局部最小值。每个局部最小值 wi、wj 和 wk（实线）对应一组不同的参数，从而对应不同的模型。在不同的有限数据样本上训练或在子样本上重新训练（如非线性交叉验证中）会导致稍有不同的误差曲线（虚线）和扰动的最小值
    wi、wj 和 wk。误差曲线及其最小值的采样变异被称为 *模型方差*。
- en: 'Let the data D be divided into ν randomly selected disjoint subsets Dj of roughly
    equal size: ∪νj=1Dj = D and ∀i = *j, D*i ∩ Dj = ∅. Let Nj denote the number of
    observations in subset Dj . Let μˆλ(Dj )(x) be an estimator trained on all data
    except for (x, t) ∈ Dj . Then, the cross-validation average squared error for
    subset j is defined as'
  id: totrans-4309
  prefs: []
  type: TYPE_NORMAL
  zh: 设数据 D 被分成 ν 个随机选择的大小大致相等的互斥子集 Dj：∪νj=1Dj = D 且 ∀i = *j, D*i ∩ Dj = ∅。设 Nj 表示子集
    Dj 中观测值的数量。设 μˆλ(Dj )(x) 为在除 (x, t) ∈ Dj 外的所有数据上训练的估计器。那么，子集 j 的交叉验证平均平方误差定义为
- en: $$CV_{D_{j}}(\lambda)=\frac{1}{N_{j}}\sum_{(\mathbf{x}_{k},t_{k})\in D_{j}}\left(t_{k}-\hat{\mu}_{\lambda(D_{j})}(\mathbf{x}_{k})\right)^{2}.\tag{16.11}$$
  id: totrans-4310
  prefs: []
  type: TYPE_NORMAL
  zh: $$CV_{D_{j}}(\lambda)=\frac{1}{N_{j}}\sum_{(\mathbf{x}_{k},t_{k})\in D_{j}}\left(t_{k}-\hat{\mu}_{\lambda(D_{j})}(\mathbf{x}_{k})\right)^{2}.\tag{16.11}$$
- en: 'These are averaged over j to obtain the ν-fold cross-validation estimate of
    prediction risk:'
  id: totrans-4311
  prefs: []
  type: TYPE_NORMAL
  zh: 这些平均化处理后，得到 ν 折交叉验证的预测风险估计：
- en: $$C V(\lambda)=\frac{1}{\nu}\sum_{j}C V_{D_{j}}(\lambda)\;\;.$$
  id: totrans-4312
  prefs: []
  type: TYPE_NORMAL
  zh: $$C V(\lambda)=\frac{1}{\nu}\sum_{j}C V_{D_{j}}(\lambda)\;\;.$$
- en: $$(16.12)$$
  id: totrans-4313
  prefs: []
  type: TYPE_NORMAL
  zh: $$(16.12)$$
- en: Typical choices for ν are 5 and 10. Leave–one–out CV is obtained in the limit
    ν = N. CV is a nonparametric estimate of the prediction risk that relies only
    on the available data.
  id: totrans-4314
  prefs: []
  type: TYPE_NORMAL
  zh: ν 的典型选择为 5 和 10。留一交叉验证在 ν = N 的极限下得到。CV 是一种非参数的预测风险估计，仅依赖于可用数据。
- en: The frequent occurrence of multiple minima in nonlinear models (see Figure 16.10),
    each of which represents a different predictor, requires a refinement of the cross-validation
    procedure. This refinement, nonlinear cross-validation
  id: totrans-4315
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性模型中经常出现多个最小值（见图 16.10），每个最小值代表一个不同的预测因子，这要求对交叉验证过程进行细化。这种细化称为非线性交叉验证。
- en: (NCV), is illustrated in Figure 16.11 for ν = 5.
  id: totrans-4316
  prefs: []
  type: TYPE_NORMAL
  zh: (NCV) 在图 16.11 中对 ν = 5 进行了说明。
- en: A network is trained on the entire data set D to obtain a model μˆλ(x) with
    weights W0. These weights are used as the starting point for the ν-fold cross–
  id: totrans-4317
  prefs: []
  type: TYPE_NORMAL
  zh: 网络在整个数据集 D 上进行训练，以获得具有权重 W0 的模型 μˆλ(x)。这些权重作为 ν 折交叉验证的起点使用。
- en: validation procedure. Each subset Dj is removed from the training data in turn.
    The network is re-trained using the remaining data starting at W0 (rather than
    using random initial weights). Under the assumption that deleting a subset from
    the training data does not lead to a large difference in the locally-optimal weights,
    the retraining from W0 "perturbs" the weights to obtain Wi, i = 1 *...ν*. The
    Cross-Validation error computed for the "perturbed models" μˆλ(Dj )(x) thus estimates
    the prediction risk for the model with locally-optimal weights W0 as desired,
    and not the performance of other predictors at other local minima.
  id: totrans-4318
  prefs: []
  type: TYPE_NORMAL
  zh: 验证程序。每个子集 Dj 依次从训练数据中移除。网络使用剩余数据重新训练，从 W0 开始（而不是使用随机初始权重）。在假设从训练数据中删除子集不会导致局部最优权重的较大差异的前提下，从
    W0 进行重新训练“扰动”权重以获得 Wi，i = 1 *...ν*。为“扰动模型”μˆλ(Dj )(x)计算的交叉验证误差因此估计了具有局部最优权重 W0
    的模型的预测风险，而不是其他局部最小值处其他预测器的表现。
- en: '![352_image_0.png](352_image_0.png)'
  id: totrans-4319
  prefs: []
  type: TYPE_IMG
  zh: '![352_image_0.png](352_image_0.png)'
- en: Fig. 16.11. Illustration of the computation of 5–fold nonlinear cross-validation
  id: totrans-4320
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.11。5 折非线性交叉验证计算的示意图
- en: (NCV). First, the network is trained on all data to obtain weights w0 which
    are used as starting point for the cross-validation. Each data subset Di, i =
    1 ... 5 is removed from the training data D in turn. The network is trained, starting
    at W0, using the remaining data. This "perturbs" the weights to obtain wi. The
    test error of the "perturbed model" wi is computed on the hold-out sample Di.
    The average of these errors is the 5-fold CV estimate of the prediction risk for
    the model with weights w0.
  id: totrans-4321
  prefs: []
  type: TYPE_NORMAL
  zh: (NCV)。首先，网络在所有数据上进行训练，以获得权重 w0，作为交叉验证的起点。每个数据子集 Di，i = 1 ... 5 依次从训练数据 D 中移除。网络从
    W0 开始，使用剩余数据进行训练。这“扰动”权重以获得 wi。“扰动模型”wi 的测试误差在保留样本 Di 上计算。这些误差的平均值是具有权重 w0 的模型的
    5 折交叉验证预测风险的估计。
- en: If the network would be trained from random initial weights for each subset,
    it could converge to a different minimum corresponding to Wi different from the
    one corresponding to W0. This would correspond to a different model. Thus, starting
    from W0 assures us that the cross-validation estimates the prediction risk for
    a particular model in question corresponding to W ≈ W0.
  id: totrans-4322
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网络对每个子集从随机初始权重进行训练，它可能会收敛到一个与 W0 不同的 Wi 对应的不同最小值。这将对应于不同的模型。因此，从 W0 开始确保交叉验证估计特定模型的预测风险，该模型对应于
    W ≈ W0。
- en: 16.4.6 Pruning Inputs Via Directed Search And Sensitivity Analysis
  id: totrans-4323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.4.6 通过定向搜索和敏感性分析修剪输入
- en: Selecting a "best subset" of input variables is a critical part of model selection
    for forecasting. This is especially true when the number of available input series
    is large, and exhaustive search through all combinations of variables is computationally
    infeasible. Inclusion of irrelevant variables not only does not help prediction,
    but can reduce forecast accuracy through added noise or systematic bias. A model-dependent
    method for input variable selection is *sensitivity-based* pruning (SBP) [41,
    32, 30, 25]. Extensions to this approach are presented in [21]. With this algorithm,
    candidate architectures are constructed by evaluating the effect of removing an
    input variable from the fully connected network. These are ranked in order of
    increasing training error. Inputs are then removed following a "Best First" strategy,
    i.e. selecting the input that, when removed, increases the training error least.
  id: totrans-4324
  prefs: []
  type: TYPE_NORMAL
  zh: 选择“最佳子集”输入变量是预测模型选择的关键部分。当可用输入序列数量较大时，尤其如此，通过所有变量组合进行穷举搜索在计算上不可行。纳入无关变量不仅无助于预测，还可能通过增加噪声或系统性偏差来降低预测准确性。基于模型的输入变量选择方法是
    *基于敏感性* 的修剪 (SBP) [41, 32, 30, 25]。该方法的扩展在 [21] 中提出。使用该算法，通过评估从完全连接的网络中移除输入变量的影响来构建候选架构。这些架构按训练误差递增的顺序进行排序。然后，按照“最佳优先”策略移除输入，即选择移除后训练误差增加最小的输入。
- en: The SBP algorithm computes a *sensitivity measure* Si to evaluate the change
    in training error that would result if input xi were removed from the network.
  id: totrans-4325
  prefs: []
  type: TYPE_NORMAL
  zh: SBP 算法计算 *敏感性度量* Si，以评估如果从网络中移除输入 xi 会导致的训练误差变化。
- en: 'One such sensitivity measure is the *delta error*, defined as:'
  id: totrans-4326
  prefs: []
  type: TYPE_NORMAL
  zh: 一种这样的敏感性度量是 *增量误差*，定义为：
- en: Delta Error (DE) $S_{i}=\frac{1}{N}\sum_{j=1}^{N}S_{ij}$ (16.13)
  id: totrans-4327
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Error (DE) $S_{i}=\frac{1}{N}\sum_{j=1}^{N}S_{ij}$ (16.13)
- en: $$(16.14)$$
  id: totrans-4328
  prefs: []
  type: TYPE_NORMAL
  zh: $$(16.14)$$
- en: 'where Sij is the sensitivity computed for exemplar xj . Since there are usually
    many fewer inputs than weights, a direct evaluation of Si is feasible:'
  id: totrans-4329
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Sij 是为样本 xj 计算的敏感度。由于通常输入的数量远少于权重，因此直接评估 Si 是可行的：
- en: Sij = SE(xi, wλ) − SE(xij , wλ) (16.14)
  id: totrans-4330
  prefs: []
  type: TYPE_NORMAL
  zh: Sij = SE(xi, wλ) − SE(xij , wλ) (16.14)
- en: $$S_{i j}=S E(\overline{{{x}}}_{i},w_{\lambda})-S E(x_{i j},w_{\lambda})$$ $$\overline{{{x}}}_{i}=\frac{1}{N}\sum_{j=1}^{N}x_{i
    j}$$
  id: totrans-4331
  prefs: []
  type: TYPE_NORMAL
  zh: $$S_{i j}=S E(\overline{{{x}}}_{i},w_{\lambda})-S E(x_{i j},w_{\lambda})$$ $$\overline{{{x}}}_{i}=\frac{1}{N}\sum_{j=1}^{N}x_{i
    j}$$
- en: Si measures the effect on the training squared error (SE) of replacing the i
    th input xi by its average xi for all exemplars (replacement of a variable by
    its average value removes its influence on the network output).
  id: totrans-4332
  prefs: []
  type: TYPE_NORMAL
  zh: Si 衡量将第 i 个输入 xi 替换为所有样本的平均值 xi 对训练平方误差(SE)的影响（将变量替换为其平均值会消除其对网络输出的影响）。
- en: Note that in computing Si, no retraining is done in evaluating SE(xi, wλ).
  id: totrans-4333
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在计算 Si 时，不会在评估 SE(xi, wλ) 时进行重新训练。
- en: Also note that it is not sufficient to just set xij = 0 ∀ j, because the value
    of the bias of each hidden unit was determined during training and would not be
    offset properly by setting the input arbitrarily to zero. Of course, if the inputs
    are normalized to have zero mean prior to training, then setting an input variable
    to zero is equivalent to replacing it by its mean.
  id: totrans-4334
  prefs: []
  type: TYPE_NORMAL
  zh: 还请注意，仅将 xij = 0 ∀ j 设置为零是不够的，因为每个隐藏单元的偏置值是在训练期间确定的，随意将输入设置为零无法正确抵消。当然，如果输入在训练前被归一化为零均值，那么将输入变量设置为零等同于用其均值替换它。
- en: 'If Si is large, the network error will be significantly increased by replacing
    the i th input variable with its mean. If Si is small, we need the help of other
    measures to decide whether the i th input variable is useful. Three additional
    sensitivity measures [21] can be computed based on perturbating an input or a
    hidden variable and monitoring network output variations:'
  id: totrans-4335
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Si 较大，则用其均值替换第 i 个输入变量会显著增加网络误差。如果 Si 较小，我们需要其他度量的帮助来决定第 i 个输入变量是否有用。可以根据扰动一个输入或隐藏变量并监测网络输出变化计算三种额外的敏感度度量
    [21]：
- en: $$\text{Average Gradient(AG)}\;S_i=\frac{1}{N}\sum_{j=1}^N\frac{\partial f^{(j)}}{\partial
    x_i}\;,$$ $$\text{AverageAbsolute Gradient(AAG)}\;S_i=\frac{1}{N}\sum_{j=1}^N|\frac{\partial
    f^{(j)}}{\partial x_i}|\;,$$ $$\text{RMSGradient(RMSG)}\;S_i=\sqrt{\frac{1}{N}\sum_{j=1}^N\left[\frac{\partial
    f^{(j)}}{\partial x_i}\right]^2}\;,$$
  id: totrans-4336
  prefs: []
  type: TYPE_NORMAL
  zh: $$\text{平均梯度(AG)}\;S_i=\frac{1}{N}\sum_{j=1}^N\frac{\partial f^{(j)}}{\partial
    x_i}\;,$$ $$\text{平均绝对梯度(AAG)}\;S_i=\frac{1}{N}\sum_{j=1}^N|\frac{\partial f^{(j)}}{\partial
    x_i}|\;,$$ $$\text{均方根梯度(RMSG)}\;S_i=\sqrt{\frac{1}{N}\sum_{j=1}^N\left[\frac{\partial
    f^{(j)}}{\partial x_i}\right]^2}\;,$$
- en: where for ease of notation we define f(x) ≡ μˆλ(x) and f(j) ≡ f(x1j , ... ,
    xij *, ... , x*dj )
  id: totrans-4337
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便记法，我们定义 f(x) ≡ μˆλ(x) 和 f(j) ≡ f(x1j , ... , xij *, ... , x*dj )
- en: is the network output given the jth input data pattern.
  id: totrans-4338
  prefs: []
  type: TYPE_NORMAL
  zh: 是给定第 j 个输入数据模式的网络输出。
- en: These three sensitivity measures together offer useful information. If SAG
  id: totrans-4339
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种敏感度度量一起提供有用的信息。如果 SAG
- en: i is positive and large, then on average the change of the direction of the
    network output f is the same as that of the i th input variable. If SAG
  id: totrans-4340
  prefs: []
  type: TYPE_NORMAL
  zh: i 为正且较大时，网络输出 f 的平均变化方向与第 i 个输入变量相同。如果 SAG
- en: i is negative and has large magnitude, the change of the direction of f on average
    is opposite to that of the i th input variable. When SAG
  id: totrans-4341
  prefs: []
  type: TYPE_NORMAL
  zh: i 为负且具有大幅度时，f 的平均变化方向与第 i 个输入变量相反。当 SAG
- en: i is close to zero, we can not get much information from this measure. If SAAG
  id: totrans-4342
  prefs: []
  type: TYPE_NORMAL
  zh: i 接近于零，我们无法从该度量中获得太多信息。如果 SAAG
- en: i is large, the output f is sensitive to the i th input variable; if SAAG
  id: totrans-4343
  prefs: []
  type: TYPE_NORMAL
  zh: i 较大时，输出 f 对第 i 个输入变量敏感；如果 SAAG
- en: i is small, f is not sensitive to the i th input variable.
  id: totrans-4344
  prefs: []
  type: TYPE_NORMAL
  zh: i 很小，f 对第 i 个输入变量不敏感。
- en: If SRMSG
  id: totrans-4345
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 SRMSG
- en: i is very different from SAAG
  id: totrans-4346
  prefs: []
  type: TYPE_NORMAL
  zh: i 与 SAAG 有很大不同
- en: i , the i th input series could be very noisy and have a lot of outliers.
  id: totrans-4347
  prefs: []
  type: TYPE_NORMAL
  zh: i，第 i 个输入序列可能非常嘈杂，并且有很多异常值。
- en: 16.4.7 Empirical Example
  id: totrans-4348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.4.7 实证示例
- en: As described in [42], we construct neural network models for predicting the
    rate of change of the U.S. Index of Industrial Production (IP). The prediction
    horizon for the IP results presented here is 12 months. Following previous work
    [27, 20], the results reported here use networks with three sigmoidal units and
    a single linear output unit.
  id: totrans-4349
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [42] 所述，我们构建神经网络模型以预测美国工业生产指数(IP)的变化率。这里呈现的IP预测范围为12个月。根据之前的工作 [27, 20]，这里报告的结果使用了三个
    sigmoid 单元和一个线性输出单元的网络。
- en: The data set consists of monthly observations of IP and other macroeconomic
    and financial series for the period from January 1950 to December 1989. The data
    set thus has a total of 480 exemplars. Input series are derived from around ten
    raw time series, including IP, the Index of Leading Indicators, the Standard
  id: totrans-4350
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集由1950年1月至1989年12月的工业生产（IP）及其他宏观经济和金融系列的每月观察数据组成。该数据集总共有480个样本。输入系列来自大约十个原始时间序列，包括IP、领先指标指数、标准普尔500指数等。
- en: '& Poors 500 Index, and so on. Both the "unfiltered" series and various "filtered"
    versions are considered for inclusion in the model, for a total of 48 possible
    input variables. The target series and all 48 candidate input series are normalized
    to zero mean and unit standard deviation. Figures 16.12 and 16.13 show the results
    of the sensitivity analysis for the case where the training-set consists of 360
    exemplars randomly chosen from the 40 year period; the remaining 120 monthly observations
    constitute the test-set.'
  id: totrans-4351
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑将“未过滤”系列和各种“过滤”版本纳入模型，共有48个可能的输入变量。目标系列和所有48个候选输入系列都被归一化为零均值和单位标准差。图16.12和16.13展示了训练集由从40年期间随机选择的360个样本组成时的敏感度分析结果；剩余的120个每月观察值构成测试集。
- en: Local optima for the number of inputs are found at 15 on the FPE curve and 13
    on the NCV curve. Due to the variability in the FPE and NCV estimates (readily
    apparent in figure 16.13 for NCV), we favor choosing the first good local minimum
    for these curves rather than a slightly better global minimum.
  id: totrans-4352
  prefs: []
  type: TYPE_NORMAL
  zh: 在FPE曲线上，输入数量的局部最优解为15，在NCV曲线上为13。由于FPE和NCV估计值的变动性（在图16.13中对于NCV很明显），我们更倾向于选择这些曲线的第一个良好的局部最小值，而不是稍微更好的全局最小值。
- en: This local minimum for NCV corresponds to a global minimum for the test error.
    Choosing it leads to a reduction of 35 (from 48 to 13) in the number of input
    series and a reduction in the number of network weights from 151 to 46. Inclusion
    of additional input variables, while decreasing the training error, does not improve
    the test-set performance.
  id: totrans-4353
  prefs: []
  type: TYPE_NORMAL
  zh: 这个NCV的局部最小值对应于测试误差的全局最小值。选择它导致输入系列数量从48减少到13，网络权重从151减少到46。虽然添加更多输入变量会降低训练误差，但并未改善测试集性能。
- en: This empirical example demonstrates the effectiveness of the *sensitivity-based*
    pruning (SBP) algorithm guided by an estimate of prediction risk, such as the
    nonlinear cross-validation (NCV) algorithm, for selecting a small subset of input
    variables from a large number of available inputs. The resulting network models
    exhibit better prediction performances, as measured by either estimates of prediction
    risk or errors on actual test sets, than models that make use of all 48 input
    series.
  id: totrans-4354
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实证例子展示了*基于敏感度*的修剪（SBP）算法在选择大量可用输入变量中的小子集时的有效性，该算法由预测风险的估计引导，例如非线性交叉验证（NCV）算法。所得到的网络模型在预测性能上表现更好，无论是通过预测风险的估计还是在实际测试集上的误差，相比之下，使用所有48个输入系列的模型效果较差。
- en: 16.4.8 Gaining Economic Understanding Through Model Visualization
  id: totrans-4355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.4.8 通过模型可视化获得经济理解
- en: Although this chapter has focussed on data-driven time series models for economic
    forecasting, it is possible to extract information from these models about
  id: totrans-4356
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章专注于用于经济预测的数据驱动时间序列模型，但从这些模型中提取信息是可能的。
- en: '![355_image_0.png](355_image_0.png)'
  id: totrans-4357
  prefs: []
  type: TYPE_IMG
  zh: '![355_image_0.png](355_image_0.png)'
- en: Fig. 16.12. Sensitivity-Based Pruning (SBP) method for selecting a subset of
    input variables for a neural net forecasting model [42]. The original network
    was trained on all 48 input variables to predict the 12 month percentage changes
    in Industrial Production
  id: totrans-4358
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.12. 用于选择神经网络预测模型输入变量子集的敏感度基于修剪（SBP）方法[42]。原始网络使用所有48个输入变量进行训练，以预测工业生产的12个月百分比变化。
- en: '(IP). The variables have been ranked in order of decreasing importance according
    to a sensitivity measure. The input variables are pruned one-by-one from the network;
    at each stage, the network is retrained. The figure shows four curves: the Training
    Error, Akaike Final Prediction Error (FPE), *Nonlinear Cross-Validation Error
    (NCV)* [30, 25], and the actual Test Error. NCV is used as a selection criterion
    and suggests that only 13 of the variables should be included. NCV predicts the
    actual test error quite well relative to FPE.'
  id: totrans-4359
  prefs: []
  type: TYPE_NORMAL
  zh: （IP）。这些变量根据敏感性测量按重要性降序排列。输入变量一个一个地从网络中修剪；在每个阶段，网络会重新训练。图中显示了四条曲线：训练误差、赤池最终预测误差（FPE）、*非线性交叉验证误差（NCV）*
    [30, 25]，以及实际测试误差。NCV作为选择标准，表明只有13个变量应被包括在内。与FPE相比，NCV对实际测试误差的预测相当准确。
- en: the structure of the economy. The sensitivity analyses presented above in section
    16.4.6 provide a global understanding about which inputs are important for predicting
    quantities of interest, such as the business cycle. Further information can be
    gained, however, by examining the evolution of sensitivities over time [21].
  id: totrans-4360
  prefs: []
  type: TYPE_NORMAL
  zh: 经济结构。上面在16.4.6节中提出的敏感性分析提供了关于哪些输入对预测感兴趣的数量（如商业周期）重要的整体理解。然而，通过检查敏感性随时间的演变，可以获得进一步的信息[21]。
- en: 'Sensitivity analysis performed for an individual exemplar provides information
    about which input features play an important role in producing the current prediction.
    Two sensitivity measures for individual exemplars can be defined as:'
  id: totrans-4361
  prefs: []
  type: TYPE_NORMAL
  zh: 对单个样本进行的敏感性分析提供了关于哪些输入特征在产生当前预测中起重要作用的信息。单个样本的两个敏感性测量可以定义为：
- en: '#### Delta Output (DO)  $\begin{array}{ll}S_{i}=\Delta f_{i}^{(j)}\\ &\\ \equiv
    f(x_{1j},\ \dots\ ,x_{ij},\ \dots\ ,x_{dj})\\ &-f(x_{1j},\ \dots\ ,\overline{x_{i}},\
    \dots\ ,x_{dj})\ ,\\ \text{Output Gradient(OG)}&S_{i}=\dfrac{\partial f^{(j)}}{\partial
    x_{i j}}\ ,\end{array}$'
  id: totrans-4362
  prefs: []
  type: TYPE_NORMAL
  zh: '#### Delta输出（DO）  $\begin{array}{ll}S_{i}=\Delta f_{i}^{(j)}\\ &\\ \equiv f(x_{1j},\
    \dots\ ,x_{ij},\ \dots\ ,x_{dj})\\ &-f(x_{1j},\ \dots\ ,\overline{x_{i}},\ \dots\
    ,x_{dj})\ ,\\ \text{输出梯度（OG）}&S_{i}=\dfrac{\partial f^{(j)}}{\partial x_{i j}}\
    ,\end{array}$'
- en: where as above we define f(j) ≡ f(x1j , ... , xij *, ... , x*dj ) . If SDO
  id: totrans-4363
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，我们定义f(j) ≡ f(x1j , ... , xij *, ... , x*dj )。如果SDO
- en: i or SOG
  id: totrans-4364
  prefs: []
  type: TYPE_NORMAL
  zh: i或SOG
- en: i is large, then the i th variable plays an important role in making the current
    prediction,
  id: totrans-4365
  prefs: []
  type: TYPE_NORMAL
  zh: i很大时，第i个变量在当前预测中起重要作用，
- en: '![356_image_0.png](356_image_0.png)'
  id: totrans-4366
  prefs: []
  type: TYPE_IMG
  zh: '![356_image_0.png](356_image_0.png)'
- en: Fig. 16.13. Sensitivity Input Pruning for IP (12 month prediction horizon).
    The figure illustrates the spread in test-set error for each of the 10 subsets
    used to calculate NCV
  id: totrans-4367
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.13。IP的敏感性输入修剪（12个月预测视角）。该图说明了用于计算NCV的10个子集的测试集误差的分布。
- en: (denoted by circles). The NCV error is the average of these test-set errors.
  id: totrans-4368
  prefs: []
  type: TYPE_NORMAL
  zh: （用圆圈表示）。NCV误差是这些测试集误差的平均值。
- en: and slightly changing the value of the variable may cause a large change in
    the network output. Figure 16.14 gives an example of a graphical display of the
    individual exemplar sensitivities.
  id: totrans-4369
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微改变变量的值可能会导致网络输出的巨大变化。图16.14给出了单个样本敏感性的图形显示示例。
- en: Using this graphical display, we can observe which input variables play important
    roles in producing the current forecast, or which input variables, when we change
    them, can significantly increase or decrease the forecast error. We can also observe
    how the roles of different input variables change through time. For example, in
    Figure 16.14, for exemplars with indices from 56 to 60, the third input variable
    has large negative sensitivity measures. Starting with the 65th exemplar, the
    sixth input variable starts to play an important role, and this lasts until the
    71th exemplar. This kind of display provides insight into the dynamics of the
    economy, as learned from the data by the trained neural network model.
  id: totrans-4370
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种图形显示，我们可以观察到哪些输入变量在产生当前预测中起着重要作用，或者哪些输入变量在改变时会显著增加或减少预测误差。我们还可以观察到不同输入变量的作用如何随时间变化。例如，在图16.14中，对于索引从56到60的样本，第三个输入变量具有较大的负敏感度测量。从第65个样本开始，第六个输入变量开始发挥重要作用，这种情况持续到第71个样本。这种显示方式提供了对经济动态的洞察，正如经过训练的神经网络模型从数据中学习的那样。
- en: 16.5 Discussion
  id: totrans-4371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.5 讨论
- en: In concluding this brief survey of the algorithms for improving forecast accuracy
    with neural networks, it is important to note that many other potentially useful
    techniques have been proposed (see also chapter 17). Also, the empirical results
    presented herein are intended to be illustrative, rather than definitive.
  id: totrans-4372
  prefs: []
  type: TYPE_NORMAL
  zh: 在总结这一简要的算法调查以提高神经网络预测准确性时，重要的是要注意，许多其他潜在有用的技术已被提出（另见第17章）。此外，本章呈现的经验结果旨在为说明性，而非决定性。
- en: Further work on both the algorithms and forecasting models may yield additional
    improvements. As a final comment, I would like to emphasize that given the difficulty
    of macroeconomic forecasting, no single technique for reducing prediction risk
    is sufficient for obtaining optimal performance. Rather, a combination of techniques
    is required.
  id: totrans-4373
  prefs: []
  type: TYPE_NORMAL
  zh: 对算法和预测模型的进一步研究可能会带来额外的改进。最后，我想强调的是，鉴于宏观经济预测的困难，单一的降低预测风险的技术不足以获得*最佳表现*。相反，需要结合多种技术。
- en: '![357_image_0.png](357_image_0.png)'
  id: totrans-4374
  prefs: []
  type: TYPE_IMG
  zh: '![357_image_0.png](357_image_0.png)'
- en: Fig. 16.14. Sensitivity analysis results for individual exemplars for a 10 input
    model for predicting the U.S. Index of Industrial Production [21]. Black and gray
    represent negative and positive respectively. The size of a rectangle represents
    the magnitude of a value. The monthly time index changes along the horizontal
    direction. The indices of input variables are plotted vertically. This type of
    graph shows which input variables are important for making forecasts at various
    points in time. This enables an understanding of economic relationships.
  id: totrans-4375
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.14。预测美国工业生产指数的10输入模型的个体示例的敏感性分析结果[21]。黑色和灰色分别表示负和正。矩形的大小表示值的大小。每月时间索引沿水平方向变化。输入变量的索引垂直绘制。此类型的图表显示了在不同时间点进行预测时哪些输入变量重要。这有助于理解经济关系。
- en: Acknowledgements. The author wishes to thank Todd Leen, Asriel Levin, Yuansong
    Liao, Hong Pi, Steve Rehfuss, Thorsteinn Rögnvaldsson, Matthew Saffell, Joachim
    Utans, Lizhong Wu and Howard Yang for their many contributions to this research.
    This chapter is an expanded version of [26]. This work was supported at OGI by
    ONR/ARPA grants N00014-92-J-4062 and N00014-94-10071, NSF grants CDA-9309728 and
    CDA-9503968, and at Nonlinear Prediction Systems by DARPA contracts DAAH01-92-CR361
    and DAAH01-96-CR026.
  id: totrans-4376
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢。作者感谢Todd Leen、Asriel Levin、Yuansong Liao、Hong Pi、Steve Rehfuss、Thorsteinn
    Rögnvaldsson、Matthew Saffell、Joachim Utans、Lizhong Wu和Howard Yang对本研究的诸多贡献。本章是[26]的扩展版本。本工作得到了OGI的ONR/ARPA资助，编号N00014-92-J-4062和N00014-94-10071，以及NSF资助CDA-9309728和CDA-9503968，非线性预测系统的DARPA合同DAAH01-92-CR361和DAAH01-96-CR026的支持。
- en: '[1] Akaike, H.: Statistical predictor identification. Ann. Inst. Statist. Math.
    22, 203–'
  id: totrans-4377
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Akaike, H.: 统计预测器识别。统计数学研究所年刊 22, 203–'
- en: 217 (1970)
  id: totrans-4378
  prefs: []
  type: TYPE_NORMAL
  zh: 217 (1970)
- en: '[2] Ash, T.: Dynamic node creation in backpropagation neural networks. Connection
    Science 1(4), 365–375 (1989)'
  id: totrans-4379
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Ash, T.: 反向传播神经网络中的动态节点创建。连接科学 1(4), 365–375 (1989)'
- en: '[3] Barron, A.: Predicted squared error: a criterion for automatic model selection.
    In:'
  id: totrans-4380
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Barron, A.: 预测平方误差：自动模型选择的标准。见：'
- en: Farlow, S. (ed.) Self–Organizing Methods in Modeling. Marcel Dekker, New York
    (1984)
  id: totrans-4381
  prefs: []
  type: TYPE_NORMAL
  zh: Farlow, S. (编) 自组织方法在建模中的应用。Marcel Dekker, 纽约 (1984)
- en: '[4] Battiti, R.: Using mutual information for selecting features in supervised
    neural net learning. IEEE Trans. on Neural Networks 5(4), 537–550 (1994)'
  id: totrans-4382
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Battiti, R.: 使用互信息选择监督神经网络学习中的特征。IEEE神经网络杂志 5(4), 537–550 (1994)'
- en: '[5] Bonnlander, B.: Nonparametric selection of input variables for connectionist
    learning. Technical report, PhD Thesis. Department of Computer Science, University
    of Colorado (1996)'
  id: totrans-4383
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Bonnlander, B.: 连接主义学习的输入变量的非参数选择。技术报告，博士论文。科罗拉多大学计算机科学系 (1996)'
- en: '[6] Clemen, R.T.: Combining forecasts: A review and annotated bibliography.
    International Journal of Forecasting (5), 559–583 (1989)'
  id: totrans-4384
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Clemen, R.T.: 组合预测：综述与注释书目。国际预测杂志 (5), 559–583 (1989)'
- en: '[7] Craven, P., Wahba, G.: Smoothing noisy data with spline functions: Estimating
    the correct degree of smoothing by the method of generalized cross-validation.
    Numer. Math. 31, 377–403 (1979)'
  id: totrans-4385
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Craven, P., Wahba, G.: 用样条函数平滑噪声数据：通过广义交叉验证方法估计正确的平滑程度。数值数学 31, 377–403
    (1979)'
- en: '[8] Eubank, R.L.: Spline Smoothing and Nonparametric Regression. Marcel Dekker,
    Inc. (1988)'
  id: totrans-4386
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Eubank, R.L.: 样条平滑和非参数回归。Marcel Dekker, Inc. (1988)'
- en: '[9] Geman, S., Bienenstock, E., Doursat, R.: Neural networks and the bias/variance
    dilemma. Neural Computation 4(1), 1–58 (1992)'
  id: totrans-4387
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Geman, S., Bienenstock, E., Doursat, R.: 神经网络与偏差/方差困境。神经计算 4(1), 1–58 (1992)'
- en: '[10] Girosi, F., Jones, M., Poggio, T.: Regularization theory and neural network
    architectures. Neural Computation 7, 219–269 (1995)'
  id: totrans-4388
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Girosi, F., Jones, M., Poggio, T.: 正则化理论与神经网络架构。神经计算 7, 219–269 (1995)'
- en: '[11] Golub, G., Heath, H., Wahba, G.: Generalized cross validation as a method
    for choosing a good ridge parameter. Technometrics 21, 215–224 (1979)'
  id: totrans-4389
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Golub, G., Heath, H., Wahba, G.: 广义交叉验证作为选择良好岭参数的一种方法。技术计量学 21, 215–224
    (1979)'
- en: '[12] Granger, C.W.J., Newbold, P.: Forecasting Economic Time Series, 2nd edn.
    Academic Press, San Diego (1986)'
  id: totrans-4390
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Granger, C.W.J., Newbold, P.: 经济时间序列预测, 第2版。学术出版社, 圣地亚哥 (1986)'
- en: '[13] Granger, C.W.J., Terasvirta, T.: Modelling Nonlinear Economic Relationships.'
  id: totrans-4391
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Granger, C.W.J., Terasvirta, T.: 建模非线性经济关系。'
- en: Oxford University Press (1993)
  id: totrans-4392
  prefs: []
  type: TYPE_NORMAL
  zh: 牛津大学出版社 (1993)
- en: '[14] Hamilton, J.D.: Time Series Analysis. Princeton University Press (1994)'
  id: totrans-4393
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Hamilton, J.D.: 时间序列分析。普林斯顿大学出版社 (1994)'
- en: '[15] Hassibi, B., Stork, D.G.: Second order derivatives for network pruning:
    Optimal brain surgeon. In: Hanson, S.J., Cowan, J.D., Giles, C.L. (eds.) Advances
    in Neural Information Processing Systems, vol. 5, pp. 164–171. Morgan Kaufmann
    Publishers, San Mateo (1993)'
  id: totrans-4394
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Hassibi, B., Stork, D.G.: 网络剪枝的二阶导数：最优脑外科医生。 In: Hanson, S.J., Cowan,
    J.D., Giles, C.L. (eds.) 神经信息处理系统进展, 第5卷, 第164–171页。摩根·考夫曼出版社, 圣马特奥 (1993)'
- en: '[16] Hastie, T.J., Tibshirani, R.J.: *Generalized Additive Models*. Monographs
    on Statistics and Applied Probability, vol. 43. Chapman and Hall (1990)'
  id: totrans-4395
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Hastie, T.J., Tibshirani, R.J.: *广义加法模型*。统计与应用概率专著, 第43卷。查普曼与霍尔 (1990)'
- en: '[17] Hoerl, A.E., Kennard, R.W.: Ridge regression: applications to nonorthogonal
    problems. Technometrics 12, 69–82 (1970)'
  id: totrans-4396
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Hoerl, A.E., Kennard, R.W.: 岭回归：非正交问题的应用。技术计量学 12, 69–82 (1970)'
- en: '[18] Hoerl, A.E., Kennard, R.W.: Ridge regression: biased estimation for nonorthogonal
    problems. Technometrics 12, 55–67 (1970)'
  id: totrans-4397
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Hoerl, A.E., Kennard, R.W.: 岭回归：非正交问题的偏倚估计。技术计量学 12, 55–67 (1970)'
- en: '[19] LeCun, Y., Denker, J.S., Solla, S.A.: Optimal brain damage. In: Touretzky,
    D.S.'
  id: totrans-4398
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] LeCun, Y., Denker, J.S., Solla, S.A.: 最优脑损伤。 In: Touretzky, D.S.'
- en: (ed.) Advances in Neural Information Processing Systems, vol. 2. Morgan Kaufmann
    Publishers (1990)
  id: totrans-4399
  prefs: []
  type: TYPE_NORMAL
  zh: (ed.) 神经信息处理系统进展, 第2卷。摩根·考夫曼出版社 (1990)
- en: '[20] Levin, A.U., Leen, T.K., Moody, J.E.: Fast pruning using principal components.'
  id: totrans-4400
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Levin, A.U., Leen, T.K., Moody, J.E.: 使用主成分进行快速剪枝。'
- en: 'In: Cowan, J., Tesauro, G., Alspector, J. (eds.) Advances in Neural Information
    Processing Systems, vol. 6. Morgan Kaufmann Publishers, San Francisco (1994)'
  id: totrans-4401
  prefs: []
  type: TYPE_NORMAL
  zh: 'In: Cowan, J., Tesauro, G., Alspector, J. (eds.) 神经信息处理系统进展, 第6卷。摩根·考夫曼出版社,
    旧金山 (1994)'
- en: '[21] Liao, Y., Moody, J.E.: A neural network visualization and sensitivity
    analysis toolkit. In: Amari, S.I., Xu, L., Chan, L.-W., King, I., Leung, K.-S.
    (eds.) Proceedings of the International Conference on Neural Information Processing,
    pp.'
  id: totrans-4402
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Liao, Y., Moody, J.E.: 一个神经网络可视化与灵敏度分析工具包。 In: Amari, S.I., Xu, L., Chan,
    L.-W., King, I., Leung, K.-S. (eds.) 国际神经信息处理会议论文集，第。'
- en: 1069–1074. Springer-Verlag Singapore Pte. Ltd. (1996)
  id: totrans-4403
  prefs: []
  type: TYPE_NORMAL
  zh: 1069–1074。施普林格-维尔哈吉新加坡私人有限公司 (1996)
- en: '[22] Litterman, R.B.: Forecasting with Bayesian vector autoregressions - five
    years of experience. Journal of Business and Economic Statistics 4(1), 25–38 (1986)'
  id: totrans-4404
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Litterman, R.B.: 使用贝叶斯向量自回归进行预测 - 五年的经验。商业与经济统计杂志 4(1), 25–38 (1986)'
- en: '[23] Moody, J.: Challenges of Economic Forecasting: Noise, Nonstationarity,
    and Nonlinearity. Invited talk presented at Machines that Learn., Snowbird Utah
    (April 1994)'
  id: totrans-4405
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Moody, J.: 经济预测的挑战：噪声、非平稳性和非线性。受邀演讲，机器学习会议，犹他州斯诺伯德 (1994年4月)'
- en: '[24] Moody, J.: The effective number of parameters: an analysis of generalization
    and regularization in nonlinear learning systems. In: Moody, J.E., Hanson, S.J.,
    Lippmann, R.P. (eds.) Advances in Neural Information Processing Systems, vol.
    4, pp.'
  id: totrans-4406
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Moody, J.: 有效参数数量：非线性学习系统中泛化与正则化的分析。 In: Moody, J.E., Hanson, S.J., Lippmann,
    R.P. (eds.) 神经信息处理系统进展, 第4卷，第。'
- en: 847–854. Morgan Kaufmann Publishers, San Mateo (1992)
  id: totrans-4407
  prefs: []
  type: TYPE_NORMAL
  zh: 847–854。摩根·考夫曼出版社, 圣马特奥 (1992)
- en: '[25] Moody, J.: Prediction risk and neural network architecture selection.
    In:'
  id: totrans-4408
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Moody, J.: 预测风险与神经网络架构选择。 In:'
- en: 'Cherkassky, V., Friedman, J.H., Wechsler, H. (eds.) From Statistics to Neural
    Networks: Theory and Pattern Recognition Applications. Springer (1994)'
  id: totrans-4409
  prefs: []
  type: TYPE_NORMAL
  zh: Cherkassky, V., Friedman, J.H., Wechsler, H. (eds.) 从统计到神经网络：理论与模式识别应用。施普林格
    (1994)
- en: '[26] Moody, J.: Macroeconomic Forecasting: Challenges and Neural Network Solutions.'
  id: totrans-4410
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Moody, J.：宏观经济预测：挑战与神经网络解决方案。'
- en: 'In: Proceedings of the International Symposium on Artificial Neural Networks,
    Hsinchu, Taiwan (1995) (Invited keynote address)'
  id: totrans-4411
  prefs: []
  type: TYPE_NORMAL
  zh: 见于：国际人工神经网络研讨会，台湾新竹（1995）（受邀主题演讲）
- en: '[27] Moody, J., Levin, A., Rehfuss, S.: Predicting the U.S. index of industrial
    production. Neural Network World 3(6), 791–794 (1993); Special Issue: Proceedings
    of Parallel Applications in Statistics and Economics 1993'
  id: totrans-4412
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Moody, J., Levin, A., Rehfuss, S.：预测美国工业生产指数。神经网络世界 3(6)，791–794（1993）；特刊：1993年统计与经济学平行应用会议的论文集'
- en: '[28] Moody, J., Rehfuss, S., Saffell, M.: Macroeconomic forecasting with neural
    networks (1999) (manuscript in preparation)'
  id: totrans-4413
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] Moody, J., Rehfuss, S., Saffell, M.：使用神经网络进行宏观经济预测（1999）（手稿准备中）'
- en: '[29] Moody, J., Rögnvaldsson, T.: Smoothing regularizers for projective basis
    function networks. In: Proceedings of Advances in Neural Information Processing
    Systems, NIPS 1996, vol. 9. MIT Press, Cambridge (1997)'
  id: totrans-4414
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] Moody, J., Rögnvaldsson, T.：投影基函数网络的平滑正则化器。见于：1996年神经信息处理系统进展会议，NIPS 1996，第9卷。麻省理工学院出版社，剑桥（1997）'
- en: '[30] Moody, J., Utans, J.: Architecture selection strategies for neural networks:
    Application to corporate bond rating prediction. In: Refenes, A.N. (ed.) Neural
    Networks in the Captial Markets. John Wiley & Sons (1994)'
  id: totrans-4415
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] Moody, J., Utans, J.：神经网络的架构选择策略：应用于企业债券评级预测。见于：Refenes, A.N.（编）《资本市场中的神经网络》。约翰·威利父子公司（1994）'
- en: '[31] Moody, J.E.: Note on generalization, regularization and architecture selection
    in nonlinear learning systems. In: Juang, B.H., Kung, S.Y., Kamm, C.A. (eds.)
    Neural Networks for Signal Processing, pp. 1–10. IEEE Signal Processing Society'
  id: totrans-4416
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Moody, J.E.：关于非线性学习系统中的泛化、正则化和架构选择的说明。见于：Juang, B.H., Kung, S.Y., Kamm,
    C.A.（编）《信号处理的神经网络》，第1–10页。IEEE信号处理学会'
- en: (1991)
  id: totrans-4417
  prefs: []
  type: TYPE_NORMAL
  zh: （1991）
- en: '[32] Moody, J.E., Utans, J.: Principled architecture selection for neural networks:
    Application to corporate bond rating prediction. In: Moody, J.E., Hanson, S.J.,
    Lippmann, R.P. (eds.) Advances in Neural Information Processing Systems, vol.
    4, pp.'
  id: totrans-4418
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] Moody, J.E., Utans, J.：神经网络的原则性架构选择：应用于企业债券评级预测。见于：Moody, J.E., Hanson,
    S.J., Lippmann, R.P.（编）《神经信息处理系统进展》，第4卷，第'
- en: 683–690. Morgan Kaufmann Publishers, San Mateo (1992)
  id: totrans-4419
  prefs: []
  type: TYPE_NORMAL
  zh: 683–690。摩根考夫曼出版社，圣马特奥（1992）
- en: '[33] Mozer, M.C., Smolensky, P.: Skeletonization: A technique for trimming
    the fat from a network via relevance assessment. In: Touretzky, D.S. (ed.) Advances
    in Neural Information Processing Systems, vol. 1. Morgan Kaufmann Publishers,
    San Mateo (1990)'
  id: totrans-4420
  prefs: []
  type: TYPE_NORMAL
  zh: '[33] Mozer, M.C., Smolensky, P.：骨架化：一种通过相关性评估修剪网络冗余的技术。见于：Touretzky, D.S.（编）《神经信息处理系统进展》，第1卷。摩根考夫曼出版社，圣马特奥（1990）'
- en: '[34] Murata, N., Yoshizawa, S., Amari, S.: Network information criterion -
    determining the number of hidden units for an artificial neural network model.
    IEEE Transactions on Neural Networks 5(6), 865–872 (1994)'
  id: totrans-4421
  prefs: []
  type: TYPE_NORMAL
  zh: '[34] Murata, N., Yoshizawa, S., Amari, S.：网络信息准则 - 确定人工神经网络模型的隐藏单元数量。IEEE神经网络汇刊
    5(6)，865–872（1994）'
- en: '[35] Natter, M., Haefke, C., Soni, T., Otruba, H.: Macroeconomic forecasting
    using neural networks. In: Neural Networks in the Capital Markets 1994 (1994)'
  id: totrans-4422
  prefs: []
  type: TYPE_NORMAL
  zh: '[35] Natter, M., Haefke, C., Soni, T., Otruba, H.：使用神经网络进行宏观经济预测。见于：1994年资本市场中的神经网络（1994）'
- en: '[36] Pi, H., Peterson, C.: Finding the embedding dimension and variable dependencies
    in time series. Neural Computation, 509–520 (1994)'
  id: totrans-4423
  prefs: []
  type: TYPE_NORMAL
  zh: '[36] Pi, H., Peterson, C.：寻找时间序列中的嵌入维数和变量依赖关系。神经计算，509–520（1994）'
- en: '[37] Plaut, D., Nowlan, S., Hinton, G.: Experiments on learning by back propagation.'
  id: totrans-4424
  prefs: []
  type: TYPE_NORMAL
  zh: '[37] Plaut, D., Nowlan, S., Hinton, G.：通过反向传播学习的实验。'
- en: Technical Report CMU-CS-86-126, Dept. of Computer Science, Carnegie-Mellon University,
    Pittsburgh, Pennsylvania (1986)
  id: totrans-4425
  prefs: []
  type: TYPE_NORMAL
  zh: 技术报告 CMU-CS-86-126，卡内基梅隆大学计算机科学系，宾夕法尼亚州匹兹堡（1986）
- en: '[38] Rehfuss, S.: Macroeconomic forecasting with neural networks (1994) (unpublished
    simulations)'
  id: totrans-4426
  prefs: []
  type: TYPE_NORMAL
  zh: '[38] Rehfuss, S.：使用神经网络进行宏观经济预测（1994）（未发表的模拟）'
- en: '[39] Rehkugler, H., Zimmermann, H.G. (eds.): Neuronale Netze in der Ökonomie.
    Verlag Vahlen (1994)'
  id: totrans-4427
  prefs: []
  type: TYPE_NORMAL
  zh: '[39] Rehkugler, H., Zimmermann, H.G.（编）：经济中的神经网络。Vahlen出版社（1994）'
- en: '[40] Swanson, N.R., White, H.: A model selection approach to real-time macroeconomic
    forecasting using linear models and artificial neural networks. Discussion paper,
    Department of Economics, Pennsylvania State University (1995)'
  id: totrans-4428
  prefs: []
  type: TYPE_NORMAL
  zh: '[40] Swanson, N.R., White, H.：一种使用线性模型和人工神经网络的实时宏观经济预测的模型选择方法。讨论论文，宾夕法尼亚州立大学经济系（1995）'
- en: '[41] Utans, J., Moody, J.: Selecting neural network architectures via the prediction
    risk: Application to corporate bond rating prediction. In: Proceedings of the
    First International Conference on Artificial Intelligence Applications on Wall
    Street. IEEE Computer Society Press, Los Alamitos (1991)'
  id: totrans-4429
  prefs: []
  type: TYPE_NORMAL
  zh: '[41] Utans, J., Moody, J.: 通过预测风险选择神经网络架构：应用于公司债券评级预测。载于：第一届华尔街人工智能应用国际会议论文集。IEEE计算机学会出版社，洛杉矶（1991）'
- en: '[42] Utans, J., Moody, J., Rehfuss, S.: Selecting input variables via sensitivity
    analysis:'
  id: totrans-4430
  prefs: []
  type: TYPE_NORMAL
  zh: '[42] Utans, J., Moody, J., Rehfuss, S.: 通过灵敏度分析选择输入变量：'
- en: 'Application to predicting the U.S. business cycle. In: Proceedings of Computational
    Intelligence in Financial Engineering. IEEE Press (1995)'
  id: totrans-4431
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于预测美国商业周期。载于：金融工程中的计算智能会议论文集。IEEE出版社（1995）
- en: '[43] Wahba, G.: Spline models for observational data. CBMS-NSF Regional Conference
    Series in Applied Mathematics (1990)'
  id: totrans-4432
  prefs: []
  type: TYPE_NORMAL
  zh: '[43] Wahba, G.: 观测数据的样条模型。CBMS-NSF区域会议系列应用数学（1990）'
- en: '[44] Winkler, R.L., Makridakis, S.: The combination of forecasts. Journal of
    Royal Statistical Society (146) (1983)'
  id: totrans-4433
  prefs: []
  type: TYPE_NORMAL
  zh: '[44] Winkler, R.L., Makridakis, S.: 预测的组合。皇家统计学会杂志（146）（1983）'
- en: '[45] Wu, L., Moody, J.: A smoothing regularizer for feedforward and recurrent
    networks. Neural Computation 8(2) (1996)'
  id: totrans-4434
  prefs: []
  type: TYPE_NORMAL
  zh: '[45] Wu, L., Moody, J.: 用于前馈和递归网络的平滑正则化器。神经计算 8(2)（1996）'
- en: '[46] Yang, H., Moody, J.: Input variable selection based on joing mutual information.'
  id: totrans-4435
  prefs: []
  type: TYPE_NORMAL
  zh: '[46] Yang, H., Moody, J.: 基于联合互信息的输入变量选择。'
- en: Technical report, Department of Computer Science, Oregon Graduate Institute
    (1998)
  id: totrans-4436
  prefs: []
  type: TYPE_NORMAL
  zh: 技术报告，计算机科学系，俄勒冈研究院（1998）
- en: 17 How To Train Neural Networks-
  id: totrans-4437
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 17 如何训练神经网络-
- en: Ralph Neuneier and Hans Georg Zimmermann Siemens AG, Corporate Technology, D-81730
    München, Germany
  id: totrans-4438
  prefs: []
  type: TYPE_NORMAL
  zh: Ralph Neuneier 和 Hans Georg Zimmermann 西门子公司，企业技术，D-81730 慕尼黑，德国
- en: '{Ralph.Neuneier,Georg.Zimmermann}@mchp.siemens.de http://w2.siemens.de/zfe_nn/homepage.html
    Abstract. The purpose of this paper is to give a guidance in neural network modeling.
    Starting with the preprocessing of the data, we discuss different types of network
    architecture and show how these can be combined effectively. We analyze several
    cost functions to avoid unstable learning due to outliers and heteroscedasticity.
    The Observer - Observation Dilemma is solved by forcing the network to construct
    smooth approximation functions. Furthermore, we propose some pruning algorithms
    to optimize the network architecture. All these features and techniques are linked
    up to a complete and consistent training procedure (see figure 17.25 for an overview),
    such that the synergy of the methods is maximized.'
  id: totrans-4439
  prefs: []
  type: TYPE_NORMAL
  zh: '{Ralph.Neuneier,Georg.Zimmermann}@mchp.siemens.de http://w2.siemens.de/zfe_nn/homepage.html
    摘要。本论文旨在为神经网络建模提供指导。从数据预处理开始，我们讨论不同类型的网络架构，并展示如何有效地组合这些架构。我们分析了几种成本函数，以避免由于异常值和异方差性导致的不稳定学习。通过强迫网络构建平滑的逼近函数，解决观察者-观察困境。此外，我们提出了一些剪枝算法以优化网络架构。所有这些特性和技术都与一个完整且一致的训练过程相结合（见图17.25以获取概述），从而最大化方法的协同效应。'
- en: 17.1 Introduction
  id: totrans-4440
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.1 引言
- en: The use of neural networks in system identification or regression tasks is often
    motivated by the theoretical result that in principle a three layer network can
    approximate any structure contained in a data set [14]. Consequently, the characteristics
    of the available data determine the quality of the resulting model. The authors
    believe that this is a misleading point of view, especially, if the amount of
    useful information that can be extracted from the data is small. This situation
    arises typically for problems with a low signal to noise ratio and a relative
    small training data set at hand. Neural networks are such a rich class of functions,
    that the control of the optimization process, i. e. the learning algorithm, pruning,
    architecture, cost functions and so forth, is a central part of the modeling process.
  id: totrans-4441
  prefs: []
  type: TYPE_NORMAL
  zh: 在系统识别或回归任务中使用神经网络，通常是因为理论结果表明，原则上一个三层网络可以逼近数据集中包含的任何结构[14]。因此，可用数据的特性决定了所得到模型的质量。作者认为，这是一种误导性的观点，尤其是在可从数据中提取的信息量较小时。这种情况通常出现在信噪比低且可用训练数据集相对较小的问题中。神经网络是一类丰富的函数，因此优化过程的控制，即学习算法、剪枝、架构、成本函数等，是建模过程的核心部分。
- en: The statement that "The neural network solution is not better than [any classical]
    method" has been used too often in order to describe the results of neural network
    modeling. At any rate, the assessment of this evaluation presupposes a precise
    knowledge of the procedure involved in the neural network solution that has been
    achieved. This is the case because a great variety of additional features and
    techniques can be applied at the different stages of the process to prevent all
    the known problems like overfitting and sensitivity to outliers concerning neural
    networks. Due to the lack of a general recipe, one can often find a statement
  id: totrans-4442
  prefs: []
  type: TYPE_NORMAL
  zh: “神经网络解决方案不比[任何经典]方法更好”的说法被过于频繁地用来描述神经网络建模的结果。无论如何，对这种评估的判断预设了对所实现的神经网络解决方案中涉及的过程的精确了解。这是因为在过程的不同阶段可以应用多种附加特性和技术，以防止所有已知问题，如过拟合和对异常值的敏感性。由于缺乏通用的配方，人们常常会找到一种声明
- en: '- Previously published in: Orr, G.B. and Müller, K.-R. (Eds.): LNCS 1524, ISBN'
  id: totrans-4443
  prefs: []
  type: TYPE_NORMAL
  zh: '- 之前发表在：Orr, G.B. 和 Müller, K.-R.（编）：LNCS 1524，ISBN'
- en: 978-3-540-65311-0 (1998).
  id: totrans-4444
  prefs: []
  type: TYPE_NORMAL
  zh: 978-3-540-65311-0（1998年）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    369–418, 2012.'
  id: totrans-4445
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon 等（编）：NN：行业技巧，第二版，LNCS 7700，第369–418页，2012年。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-4446
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: declaring that the quality of the neural network model depends strongly on the
    person who generated the model, which is usually perceived as negative. In contrast,
    we consider the additional features an outstanding advantage of neural networks
    compared to classical methods, which typically do not allow such a sophisticated
    control of their optimization. The aim of our article is to provide a set of techniques
    to efficiently exploit the capabilities of neural networks. More important, these
    features will be combined in such a way that we will achieve a maximal effect
    of synergy by their application.
  id: totrans-4447
  prefs: []
  type: TYPE_NORMAL
  zh: 声明神经网络模型的质量强烈依赖于生成模型的人，这通常被视为负面。相反，我们认为附加特性是神经网络相较于经典方法的一个显著优势，后者通常不允许如此复杂的优化控制。我们文章的目的是提供一套技术，以有效利用神经网络的能力。更重要的是，这些特性将以一种方式组合，以最大化其应用的协同效应。
- en: First, we begin with the preprocessing of the data and define a network architecture.
    Then, we analyze the interaction between the data and the architecture
  id: totrans-4448
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从数据预处理开始，并定义网络架构。然后，我们分析数据与架构之间的交互。
- en: (Observer - Observation Dilemma) and discuss several pruning techniques to optimize
    the network topology. Finally, we conclude by combining the proposed features
    into a unified training procedure (see fig. 17.25 in section 17.8).
  id: totrans-4449
  prefs: []
  type: TYPE_NORMAL
  zh: （观察者 - 观察困境），并讨论几种修剪技术以优化网络拓扑。最后，我们通过将所提特性整合成一个统一的训练程序来得出结论（见第17.8节中的图17.25）。
- en: Most of the paper is relevant to nonlinear regression in general. Some considerations
    are focussed on time series modeling and forecasting. All the proposed approaches
    have been tested on diverse tasks we have to solve for our clients, e. g. forecasting
    financial markets. The typical problem can be characterized by a relative small
    set of very noisy data and a high dimensional input vector to cover the complexity
    of the underlying dynamical system. The paper gives an overview of the unified
    training procedure we have developed to solve such problems.
  id: totrans-4450
  prefs: []
  type: TYPE_NORMAL
  zh: 本文大部分内容与非线性回归相关。某些考虑集中在时间序列建模和预测上。所有提出的方法都在我们为客户解决的多样任务上进行了测试，例如金融市场预测。典型问题可以通过相对较小的非常嘈杂的数据集和一个高维输入向量来表征，以涵盖基础动态系统的复杂性。本文概述了我们开发的统一训练程序，以解决此类问题。
- en: 17.2 Preprocessing
  id: totrans-4451
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.2 数据预处理
- en: Besides the obvious scaling of the data (in the following abbreviated by scale(·)),
  id: totrans-4452
  prefs: []
  type: TYPE_NORMAL
  zh: 除了明显的数据缩放（在下文中缩写为scale(·)），
- en: which transforms the different time series such that each series has a mean
    value of zero and a statistical variance of one, some authors have proposed complicated
    preprocessing functions. In the field of financial forecasting, these functions
    are often derived from technical analysis in order to capture some of the underlying
    dynamics of the financial markets (see [37] and [25] for some examples).
  id: totrans-4453
  prefs: []
  type: TYPE_NORMAL
  zh: 一些作者提出了复杂的预处理函数，这些函数将不同的时间序列转换为每个序列的均值为零，统计方差为一。在金融预测领域，这些函数通常源于技术分析，以捕捉金融市场的一些基础动态（见[37]和[25]的例子）。
- en: 'After many experiments with real data, we have settled with the following simple
    transformations. If the original time series which has been selected as an input,
    is changing very slowly with respect to the prediction horizon, i. e. there is
    no clearly identifiable mean reverting equilibrium, then an indicator for the
    inertia and an information of the driving force has been proven to be very informative.
    The inertia can be described by a momentum (relative change, eq. 17.1) and the
    force by the acceleration of the time series (normalized curvature, eq. 17.2).
    If we have a prediction horizon of n steps into the future the original time series
    xt is transformed in the following way:'
  id: totrans-4454
  prefs: []
  type: TYPE_NORMAL
  zh: 经过多次真实数据实验后，我们确定了以下简单的转换。如果选定的原始时间序列相对于预测视野变化非常缓慢，即没有明显可识别的均值回归均衡，则动量的指示器和驱动力的信息被证明非常有用。动量可以通过动量（相对变化，公式17.1）来描述，而力量可以通过时间序列的加速度（归一化曲率，公式17.2）来描述。如果我们有n步未来的预测视野，则原始时间序列xt以以下方式进行转换：
- en: 'momentum: $\tilde{x}_{t}=\mbox{scale}\left(\frac{x_{t}-x_{t-n}}{x_{t-n}}\right)$,
    (17.1)'
  id: totrans-4455
  prefs: []
  type: TYPE_NORMAL
  zh: 动量：$\tilde{x}_{t}=\mbox{scale}\left(\frac{x_{t}-x_{t-n}}{x_{t-n}}\right)$, (17.1)
- en: $$\mathrm{force:}\qquad{\hat{x}}_{t}=\mathrm{scale}\left({\frac{x_{t}-2x_{t-n}+x_{t-2n}}{x_{t-n}}}\right)\,.$$
  id: totrans-4456
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{force:}\qquad{\hat{x}}_{t}=\mathrm{scale}\left({\frac{x_{t}-2x_{t-n}+x_{t-2n}}{x_{t-n}}}\right)\,.$$
- en: $$(17.2)$$
  id: totrans-4457
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.2)$$
- en: . (17.2)
  id: totrans-4458
  prefs: []
  type: TYPE_NORMAL
  zh: . (17.2)
- en: In eq. 17.1, the relative difference is computed to eliminate exponential trends
    which, for example, may be caused by inflationary influences. Using only the preprocessing
    functions of eq. 17.1 typically leads to poor models which only follow obvious
    trends. The forces, i. e. the transformations by eq. 17.2, are important to characterize
    the turning points of the time series.
  id: totrans-4459
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式17.1中，计算相对差异以消除指数趋势，例如，这可能是由通货膨胀影响造成的。仅使用公式17.1的预处理函数通常会导致仅跟随明显趋势的糟糕模型。力量，即公式17.2所描述的转换，对于表征时间序列的转折点非常重要。
- en: A time series may be fast in returning to it's equilibrium state after new information
    has entered the market, as is the case for most prices of goods and stock rates.
    In this case, we substitute eq. 17.2 by a description of the forces which drive
    the price back to the estimated equilibrium. A simple way to estimate the underlying
    price equilibrium is to take the average over some past values of the time series.
  id: totrans-4460
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦有新信息进入市场，时间序列可能会快速恢复到其均衡状态，正如大多数商品价格和股票利率的情况一样。在这种情况下，我们用描述驱动价格回到估计均衡的力量来替代公式17.2。估计潜在价格均衡的一个简单方法是对时间序列的某些过去值进行平均。
- en: Instead of using the relative difference between the estimate and the current
    value, we look at the difference between the equilibrium and the past value, which
    lies in the middle of the averaging window. In our example, this is formulated
    as
  id: totrans-4461
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不使用估计值和当前值之间的相对差异，而是关注均衡值与位于平均窗口中间的过去值之间的差异。在我们的示例中，这被表述为
- en: $$\hat{x}_{t}=\text{scale}\left(\frac{x_{t-n}-\frac{1}{2n+1}\sum_{\tau=0}^{2n}x_{t-\tau}}{x_{t-n}}\right)\,.\tag{17.3}$$
  id: totrans-4462
  prefs: []
  type: TYPE_NORMAL
  zh: $$\hat{x}_{t}=\text{scale}\left(\frac{x_{t-n}-\frac{1}{2n+1}\sum_{\tau=0}^{2n}x_{t-\tau}}{x_{t-n}}\right)\,.\tag{17.3}$$
- en: Note that in eq. 17.3 we use xt−n instead of the present value xt leading to
    an estimation of the equilibrium centered around xt−n. Since, in fact, we are
    interested in measuring the tension between a price level and the underlying market
    equilibrium, this estimation offers a more appropriate description at the cost
    of ignoring the newest possible point information. This concept, known as mean
    reverting dynamics in economics, is analog to the behavior of a pendulum in physics.
  id: totrans-4463
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在公式17.3中，我们使用xt−n而不是当前值xt，从而导致围绕xt−n的均衡估计。实际上，我们关注的是测量价格水平与潜在市场均衡之间的紧张关系，这一估计在忽略最新可能的信息点的情况下提供了更合适的描述。这个概念在经济学中被称为均值回归动态，类似于物理学中摆的行为。
- en: 17.3 Architectures
  id: totrans-4464
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.3 架构
- en: We will present several separate architectural building blocks (figures 17.1
    to 17.7), which will finally be combined into a unified neural network architecture
    for time series analysis (figure 17.8). Most of the structural elements can be
    used in general regression.
  id: totrans-4465
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示几个独立的架构构建模块（图17.1至17.7），这些模块最终将组合成一个统一的时间序列分析神经网络架构（图17.8）。大多数结构元素可以用于一般回归。
- en: 17.3.1 Net Internal Preprocessing By A Diagonal Connector
  id: totrans-4466
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.3.1 通过对角连接器进行净内部预处理
- en: In order to have potentially very different inputs on a comparable scale, we
    standardize the input data to zero mean and unit variance. A problem with this
    common approach is that outliers in the input can have a large impact.
  id: totrans-4467
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使潜在上非常不同的输入在可比较的尺度上，我们将输入数据标准化为零均值和单位方差。这种常见方法的问题是，输入中的异常值可能会产生重大影响。
- en: This is particularly serious for data that contain large shocks, as in finance
    and economics. To avoid this problem, we propose an additional, *net internal*
    (short for *network internal*) preprocessing of the inputs by scaling them according
    to
  id: totrans-4468
  prefs: []
  type: TYPE_NORMAL
  zh: 对于包含大冲击的数据，如金融和经济学，这尤为严重。为避免此问题，我们提出通过根据比例缩放输入进行额外的*净内部*（*网络内部*的缩写）预处理。
- en: $$x^{\prime}=\operatorname{tanh}(w x)\,.$$
  id: totrans-4469
  prefs: []
  type: TYPE_NORMAL
  zh: $$x^{\prime}=\operatorname{tanh}(w x)\,.$$
- en: $$(17.4)$$
  id: totrans-4470
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.4)$$
- en: x = tanh(wx). (17.4)
  id: totrans-4471
  prefs: []
  type: TYPE_NORMAL
  zh: x = tanh(wx). (17.4)
- en: The implementation of this preprocessing layer is shown in fig. 17.1. This layer
    has the same number of hidden units as the input layer and uses standard tanh
    squashing functions. The particular weight matrix between the input layer and
    the preprocessing layer is only a square diagonal matrix.
  id: totrans-4472
  prefs: []
  type: TYPE_NORMAL
  zh: 此预处理层的实现如图17.1所示。该层的隐藏单元数与输入层相同，并使用标准的tanh压缩函数。输入层与预处理层之间的特定权重矩阵仅为方形对角矩阵。
- en: For short term forecasts (e. g. modeling daily returns of stock markets), we
    typically initialize the weights with the value of 0.1 to ensure that the tanh
    is in its linear range, which in turn ensures that the external inputs pass through
    essentially unchanged. If interested in long term models (e. g. six month forecast
    horizon), we prefer to start with an initial value of 1. The reason is that monthly
    data are typically more contaminated by "non-economical" effects like political
    shocks. The large initial weight values eliminate such outliers from the beginning.
  id: totrans-4473
  prefs: []
  type: TYPE_NORMAL
  zh: 对于短期预测（例如建模股市每日收益），我们通常将权重初始化为0.1，以确保tanh处于其线性范围，从而确保外部输入基本上不变。如果关注长期模型（例如六个月预测期），我们更倾向于从初始值1开始。原因是，月度数据通常受到“非经济”效应如政治冲击的更大污染。较大的初始权重值从一开始就消除了这些异常值。
- en: The weights in the diagonal connector are restricted to be positive to avoid
    fluctuations of the sign of x during training. This constraint keeps eq. 17.4
    as a monotonic transformation with the ability to limit outliers. No bias should
    be used for the preprocessing layer to prevent numerical ambiguities. We found
    that these additional constraints improve the training stability of the preprocessing
    layer.
  id: totrans-4474
  prefs: []
  type: TYPE_NORMAL
  zh: 对角连接器中的权重被限制为正值，以避免训练期间x的符号波动。该约束使得公式17.4保持单调变换，并具有限制异常值的能力。预处理层不应使用偏置，以防止数值模糊。我们发现这些额外的约束改善了预处理层的训练稳定性。
- en: These weights will be adapted during training in the same way as all the other
    weights within the network. In practice, we observe both growing and shrinking
    values for the weights. Growing values cause a larger proportion of the input
    range to be compressed by the squashing function of the tanh. Very small values
    of diagonal elements indicate the option to prune the corresponding input.
  id: totrans-4475
  prefs: []
  type: TYPE_NORMAL
  zh: 这些权重将在训练过程中以与网络中所有其他权重相同的方式进行调整。在实践中，我们观察到权重值有增长和缩小。增长的值导致输入范围的更大比例被tanh的压缩函数压缩。对角元素的非常小的值表明可以修剪相应的输入。
- en: '![364_image_0.png](364_image_0.png)'
  id: totrans-4476
  prefs: []
  type: TYPE_IMG
  zh: '![364_image_0.png](364_image_0.png)'
- en: Fig. 17.1. Net internal preprocessing to limit the influence of outliers and
    to eliminate unimportant inputs
  id: totrans-4477
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1. 净内部预处理，以限制异常值的影响并消除不重要的输入
- en: 17.3.2 Net Internal Preprocessing By A Bottleneck Network
  id: totrans-4478
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.3.2 通过瓶颈网络的净内部预处理
- en: It is an old idea to use a bottleneck network to shrink the dimensionality of
    the input vector. This technique was not only used to build encoders [1], but
    also to perform a principle component analysis [26]. Using a bottleneck as an
    internal preprocessing building block within a larger network offers the opportunity
    for us to compress the input information. In addition, the tanh squashing function
    of the bottleneck layer acts as a limiter of the outliers in the data.
  id: totrans-4479
  prefs: []
  type: TYPE_NORMAL
  zh: 使用瓶颈网络来缩小输入向量的维度是一个古老的想法。这种技术不仅用于构建编码器[1]，还用于进行主成分分析[26]。在更大网络中将瓶颈作为内部预处理构建模块，提供了压缩输入信息的机会。此外，瓶颈层的tanh压缩函数作为数据中异常值的限制器。
- en: In a first attempt, one may implement a bottleneck architecture as an inputhidden-output-layer
    sub-network using the inputs also as targets on the output level. An additional
    connector from the hidden layer is connected to the remaining parts of the network.
    This design allows the compression of input signals, but there are two major disadvantages
    implied. If we apply input pruning to the original inputs the decompression becomes
    disordered. Furthermore, adding noise to the inputs becomes more complex because
    the disturbed inputs have also to be used as targets at the output layer.
  id: totrans-4480
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次尝试中，可以将瓶颈架构实现为输入-隐藏-输出层的子网络，同时将输入作为输出层的目标。隐藏层的额外连接器与网络的其余部分相连。此设计允许对输入信号进行压缩，但暗含两个主要缺点。如果对原始输入进行输入剪枝，解压缩将变得无序。此外，向输入中添加噪声变得更加复杂，因为扰动的输入也必须作为输出层的目标使用。
- en: A distinct approach of the bottleneck sub-networks which avoids these difficulties
    is indicated in fig. 17.2. On the left side of the diagram 17.2 we have the typical
    compressor-decompressor network. The additional connection is a frozen identity
    matrix, which duplicates the input layer to the output layer. Since the target
    for this layer is set to zero, the output of the compressor-decompressor network
    will adopt the negative values of the inputs in order to compensate for the identical
    copy of the inputs. Even though the sign of the input signals has been reversed,
    this circumstance does not imply any consequence for the information flow through
    the bottleneck layer. This architecture allows an appropriate elimination of inputs
    as well as the disturbance of the input signals by artificial noise.
  id: totrans-4481
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.2 指出了一种避免这些困难的瓶颈子网络的独特方法。在图 17.2 的左侧，我们有典型的压缩-解压缩网络。额外的连接是一个冻结的单位矩阵，它将输入层复制到输出层。由于该层的目标被设置为零，压缩-解压缩网络的输出将采用输入的负值，以补偿输入的相同副本。尽管输入信号的符号被反转，但这一情况对信息通过瓶颈层的流动没有任何影响。该架构允许适当消除输入以及通过人工噪声干扰输入信号。
- en: '![365_image_0.png](365_image_0.png)'
  id: totrans-4482
  prefs: []
  type: TYPE_IMG
  zh: '![365_image_0.png](365_image_0.png)'
- en: Fig. 17.2. Net internal preprocessing cluster using a bottleneck
  id: totrans-4483
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.2. 使用瓶颈的网络内部预处理集群
- en: In our experiments, we observed that it is possible to train the bottleneck
    in parallel with the rest of the network. As a consequence, the bottleneck can
    be shrunk as long as the error on the training set remains at the same level as
    it would without a compression. As will be described in section 17.6 it is important
    to use a dynamically controlled input noise as a regularization method during
    learning. The architecture of fig. 17.2 allows the combination of noise and net
    internal preprocessing by data compression.
  id: totrans-4484
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们观察到可以与网络的其余部分并行训练瓶颈。因此，只要训练集上的误差保持在与没有压缩时相同的水平，瓶颈就可以缩小。如在 17.6 节中将描述的那样，在学习过程中使用动态控制的输入噪声作为正则化方法是很重要的。图
    17.2 的架构允许通过数据压缩将噪声和网络内部预处理结合起来。
- en: 17.3.3 Squared Inputs
  id: totrans-4485
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.3.3 平方输入
- en: Following the suggestion of G. Flake in [10], we also supply the network with
    the squared values of the inputs which leads to an integration of global and local
    decision making. The processing of the original inputs and the squared inputs
    within a tanh activation function enables the network to act as a combination
    of widely-known neural networks using sigmoidal activation function (MLP) and
    radial basis function networks (RBF). The output of a typical three-layer MLP
  id: totrans-4486
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 G. Flake 在[10]中的建议，我们还向网络提供输入的平方值，这导致全球和局部决策的整合。通过 tanh 激活函数处理原始输入和平方输入，使得网络能够作为使用
    sigmoid 激活函数（MLP）和径向基函数网络（RBF）广为人知的神经网络的组合。典型的三层 MLP 的输出为
- en: is
  id: totrans-4487
  prefs: []
  type: TYPE_NORMAL
  zh: 是
- en: $$y=\sum_{j}v_{j}\operatorname{tanh}\left(\sum_{i}w_{j i}x_{i}-\theta_{j}\right)\
    ,$$
  id: totrans-4488
  prefs: []
  type: TYPE_NORMAL
  zh: $$y=\sum_{j}v_{j}\operatorname{tanh}\left(\sum_{i}w_{j i}x_{i}-\theta_{j}\right)\
    ,$$
- en: whereas the output of a RBF computes to
  id: totrans-4489
  prefs: []
  type: TYPE_NORMAL
  zh: 而 RBF 的输出计算为
- en: $$(17.5)$$
  id: totrans-4490
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.5)$$
- en: $$y=\sum_{j}v_{j}\ e^{-\frac{1}{2}\sum_{i}\left(\frac{x_{i}-\mu_{j i}}{\sigma_{j
    i}}\right)^{2}}\ .$$
  id: totrans-4491
  prefs: []
  type: TYPE_NORMAL
  zh: $$y=\sum_{j}v_{j}\ e^{-\frac{1}{2}\sum_{i}\left(\frac{x_{i}-\mu_{j i}}{\sigma_{j
    i}}\right)^{2}}\ .$$
- en: $$(17.6)$$
  id: totrans-4492
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.6)$$
- en: Some research papers typically deal with the comparison of these two types of
    basis functions whereas Flake proposes the following combining approach
  id: totrans-4493
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究论文通常比较这两种基函数，而 Flake 提出了以下结合方法。
- en: $$y=\sum_{j}v_{j}\tanh\left(\sum_{i}w_{ji}x_{i}+u_{ji}x_{i}^{2}-\theta_{j}\right)\,\tag{17.7}$$
  id: totrans-4494
  prefs: []
  type: TYPE_NORMAL
  zh: $$y=\sum_{j}v_{j}\tanh\left(\sum_{i}w_{ji}x_{i}+u_{ji}x_{i}^{2}-\theta_{j}\right)\,\tag{17.7}$$
- en: which covers the MLP of eq. 17.5 and simultaneously approximates the RBF output
    eq. 17.6 to a sufficient level.
  id: totrans-4495
  prefs: []
  type: TYPE_NORMAL
  zh: 它涵盖了方程17.5的MLP，同时将RBF输出方程17.6近似到足够的水平。
- en: Nevertheless, we propose some minor changes to Flake's approach by taking the
    square of the internally preprocessed inputs instead of supplying the squared
    original inputs separately. That way, we take advantage of the preprocessing cluster
    because it limits the outliers. Furthermore, pruning inputs leads to a simultaneous
    elimination of its linear and squared transformation. A possible implementation
    is shown in fig. 17.3. The connector id is a fixed identity matrix.
  id: totrans-4496
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们通过取内部预处理输入的平方而不是单独提供平方原始输入，建议对Flake的方法做一些小修改。这样，我们利用了预处理集群，因为它限制了异常值。此外，修剪输入会同时消除其线性和平方变换。一个可能的实现示意图见图17.3。连接器ID是一个固定的单位矩阵。
- en: The cluster above this connector uses square as the nonlinear activation function.
    The next hidden cluster is able to create MLP and RBF structures with its tanh
    squashing function.
  id: totrans-4497
  prefs: []
  type: TYPE_NORMAL
  zh: 该连接器上方的集群使用平方作为非线性激活函数。下一个隐藏集群能够使用其tanh压缩函数创建MLP和RBF结构。
- en: One might think that the addition of a new connector between the square and
    the hidden layer with all its additional weights would boost the overfitting.
    Our experience indicates just the opposite. Furthermore, when optimizing a usual
    MLP,
  id: totrans-4498
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会认为，在平方和隐藏层之间添加一个新连接器及其所有额外权重会增加过拟合。我们的经验恰恰相反。此外，在优化普通MLP时，
- en: one typically stops training before the error on the training set gets too small
    (early stopping) in order to increase the probability of good results on the test
    set.We have observed, that the error on the training set can be very small while
    at the same time achieving good test results, even if one does not use weight
    pruning. Our experiences can be understood on the basis of the local modeling
    features of the architecture. That is, local artifacts in the input space can
    be encapsulated, so that they have no more global influence which may lead to
    bad generalization performance. This is especially true for high dimensional input
    spaces.
  id: totrans-4499
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在训练集上的误差变得过小时（提前停止）之前停止训练，以提高在测试集上获得好结果的概率。我们观察到，训练集上的误差可以非常小，同时获得良好的测试结果，即使不使用权重修剪。我们的经验可以在该架构的局部建模特征的基础上理解。也就是说，输入空间中的局部伪影可以被封装，因此它们不再对全局产生影响，可能导致糟糕的泛化性能。这对于高维输入空间尤其真实。
- en: '![367_image_0.png](367_image_0.png)'
  id: totrans-4500
  prefs: []
  type: TYPE_IMG
  zh: '![367_image_0.png](367_image_0.png)'
- en: Fig. 17.3. Net internal preprocessing cluster and the square cluster, which
    produces the input signals for following clusters. Note, that the connector from
    bias to "hidden" is required but suppressed for visual clarity.
  id: totrans-4501
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.3. 网络内部预处理集群和平方集群，产生后续集群的输入信号。请注意，偏置到“隐藏”的连接器是必需的，但为了视觉清晰被抑制。
- en: 17.3.4 Interaction Layer
  id: totrans-4502
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.3.4 交互层
- en: This section summarizes the articles of Weigend and Zimmermann in [36, 32].
  id: totrans-4503
  prefs: []
  type: TYPE_NORMAL
  zh: 本节总结了Weigend和Zimmermann在[36, 32]的文章。
- en: In most applications of neural networks in financial engineering the number
    of inputs is huge (of the order of a hundred), but only a single output is used.
    This situation can be viewed as a large "inverted" funnel; the hard problem is
    that the only information available for learning is that of a single output. This
    is one of the reasons for the "data-hungriness" of single-output neural networks,
    i. e. a large set of training data is often required in order to distinguish nonlinearities
    from noise. The flip-side of the coin is that small data sets need to be regularized,
    thus only allowing an identification of a simple model, which implies a bias towards
    linear models. This circumstance is not to be confused with the bias towards linear
    models which is a consequence of some techniques for avoiding overfitting such
    as early stopping or weight decay.
  id: totrans-4504
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融工程中，神经网络的大多数应用中，输入数量庞大（约为一百），但仅使用单个输出。这种情况可以视为一个大型的“倒漏斗”；棘手的问题在于，学习中唯一可用的信息就是单个输出。这是单输出神经网络“数据饥渴”的原因之一，即通常需要大量的训练数据以区分非线性与噪声。反过来说，小数据集需要正则化，因此只能识别简单模型，这意味着对线性模型的偏向。这种情况不应与由于某些避免过拟合的技术（如提前停止或权重衰减）而导致的线性模型偏向混淆。
- en: One approach for increasing the information flow from the output side to the
    input side is to increase the number of output units. In the simplest case, two
    output units can be used, one to predict the return, the other one to predict
    the sign of the return [34].
  id: totrans-4505
  prefs: []
  type: TYPE_NORMAL
  zh: 增加输出侧到输入侧的信息流的一种方法是增加输出单元的数量。在最简单的情况下，可以使用两个输出单元，一个预测收益，另一个预测收益的符号[34]。
- en: Since we have to model dynamical systems, we would like to provide enough information
    to characterize the state of the autonomous part of the dynamics on the output
    side, similar to Takens' theorem for the notion of state on the input side [11].
    The idea of multiple output units has recently been popularized in the connectionist
    community in a non-time series context by Caruana [6].
  id: totrans-4506
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们必须建模动态系统，我们希望提供足够的信息来表征输出侧动态的自主部分状态，类似于输入侧状态的塔肯斯定理[11]。多个输出单元的想法最近在连接主义社区中被卡鲁阿纳[6]在非时间序列上下文中普及。
- en: The present paper is concerned with time series, so the embedding of the output
    can be done analogously to the input side of a tapped "delay" line, indicated
    in fig. 17.4 as the *point prediction layer*.
  id: totrans-4507
  prefs: []
  type: TYPE_NORMAL
  zh: 本文关注时间序列，因此输出的嵌入可以类比于“延迟”线的输入侧，图17.4中标记为*点预测层*。
- en: '![368_image_0.png](368_image_0.png)'
  id: totrans-4508
  prefs: []
  type: TYPE_IMG
  zh: '![368_image_0.png](368_image_0.png)'
- en: Fig. 17.4. Point predictions followed by the interaction layer
  id: totrans-4509
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.4. 点预测后接交互层
- en: For the forecast we are interested in yt+n, the n-step ahead forecast of variable
    y. Additionally, we also want to predict yt+n−1 and yt+n+1. However, after experimenting
    with this architecture, we do not consider it to be much of a benefit to our aim,
    because there seems only very little interaction between the behavior of the outputs
    as reflected through the implicit transfer by sharing hidden units.
  id: totrans-4510
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们感兴趣的预测yt+n，即变量y的n步预测。此外，我们还希望预测yt+n−1和yt+n+1。然而，在实验这种架构后，我们认为对我们的目标并没有太大好处，因为输出行为之间似乎只有很小的交互，这通过共享隐藏单元的隐式传递反映出来。
- en: This prompted us to introduce an explicit second output layer, the *interaction
    layer.* The additional layer computes the next-neighbor derivatives (yt+n −
  id: totrans-4511
  prefs: []
  type: TYPE_NORMAL
  zh: 这促使我们引入一个显式的第二输出层，即*交互层*。额外的层计算下一邻居的导数(yt+n −
- en: yt+n−1) and (yt+n+1 −yt+n), as well as the curvature (yt+n+1 −2yt+n +yt+n−1).
  id: totrans-4512
  prefs: []
  type: TYPE_NORMAL
  zh: (yt+n−1)和(yt+n+1 −yt+n)，以及曲率(yt+n+1 −2yt+n +yt+n−1)。
- en: Differences between neighbors are encoded as fixed weights between point prediction
    and interaction layer, so they do not enlarge the number of parameters to be estimated.
    The overall cost function is the sum of all six contributions where all individual
    contributions are weighted evenly. If the target values are not properly scaled,
    it may be useful to give equal contribution to the error and to scale each error
    output by the average error of that output unit.
  id: totrans-4513
  prefs: []
  type: TYPE_NORMAL
  zh: 邻居之间的差异被编码为点预测和交互层之间的固定权重，因此它们不会增加待估计的参数数量。整体成本函数是所有六个贡献的总和，所有单独的贡献被均匀加权。如果目标值未正确缩放，给误差相同的贡献并通过该输出单元的平均误差缩放每个误差输出可能是有用的。
- en: If the point forecasts were perfect, the interaction layer would have no effect
    at all. To explain the effect of non-zero errors, consider fig. 17.5. Both predictions
    of the three points have the same pointwise errors at each of the three neighboring
    points. However, both the slopes and the curvature are correct in Model 1 (they
    don't give additional errors), but do add to the errors for Model 2.1
  id: totrans-4514
  prefs: []
  type: TYPE_NORMAL
  zh: 如果点预测完美，交互层将完全没有效果。为了解释非零误差的影响，请参考图17.5。三点的两个预测在每个相邻点的点状误差是相同的。然而，模型1的斜率和曲率是正确的（它们没有增加额外误差），但会对模型2.1的误差产生影响。
- en: '![369_image_0.png](369_image_0.png)'
  id: totrans-4515
  prefs: []
  type: TYPE_IMG
  zh: '![369_image_0.png](369_image_0.png)'
- en: Fig. 17.5. Geometric interpretation of the effect of the interaction layer on
    the cost function. Given are three curves connecting points at three adjacent
    steps in time.
  id: totrans-4516
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.5. 交互层对成本函数影响的几何解释。给出三条连接时间上三个相邻步骤的曲线。
- en: Whereas Model 1 (connected by dashed line) and Model 2 (connected by the dotted
    line) have identical pointwise errors to the observations (connected by the solid
    line), taking derivatives and curvatures into account favors Model 1.
  id: totrans-4517
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模型1（由虚线连接）和模型2（由点线连接）对观察值（由实线连接）的点状误差是相同的，但考虑导数和曲率更有利于模型1。
- en: The principle of the interaction layer can also be used to model further relationships
    on the output side. Let us take as an example a forecasting model of several exchange
    rates. Between all these forecasts we should guarantee that there is no arbitrage
    between the assets at the forecast horizon. In other words, at yt+n there is no
    way to change money in a closed loop and have a positive return. These intermarket
    relationships can be realized in form of an interaction layer.
  id: totrans-4518
  prefs: []
  type: TYPE_NORMAL
  zh: 交互层的原理也可以用来建模输出端的进一步关系。让我们以多个汇率的预测模型为例。在所有这些预测之间，我们应该保证在预测期内资产之间没有套利。换句话说，在yt+n处，无法以闭环的方式换钱并获得正收益。这些跨市场关系可以通过交互层实现。
- en: In general, the control of intermarket relationships becomes more important
    if we proceed from forecasting models to portfolio analysis models. In the latter
    the correct forecast of the interrelationships of the assets is more important
    than a perfect result on one of the titles.
  id: totrans-4519
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，控制跨市场关系变得更加重要，如果我们从预测模型转向投资组合分析模型。在后者中，资产间关系的正确预测比某个标题上的完美结果更为重要。
- en: 17.3.5 Averaging
  id: totrans-4520
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.3.5 平均
- en: Let us assume, that we have m sub-networks for the same learning task. Different
    solutions of the sub-networks may be caused by instabilities of the learning or
    by a different design of the sub-networks. It is a well known principle that averaging
    the output of several networks may give us a better and more stable result [24,
    4].
  id: totrans-4521
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有m个子网络用于相同的学习任务。子网络的不同解决方案可能是由于学习的不稳定性或子网络设计的不同。众所周知的原则是，平均多个网络的输出可能会给我们更好、更稳定的结果[24,
    4]。
- en: These advantages can be clearly seen if we define as average error function
  id: totrans-4522
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们定义平均误差函数，这些优势就会显而易见。
- en: $$E{\mathrm{average}}={\frac{1}{T}}\sum_{t=1}^{T}\left[\left({\frac{1}{m}}\sum_{i=1}^{m}y_{i,t+n}\right)-y_{t+n}^{d}\right]^{2}$$
  id: totrans-4523
  prefs: []
  type: TYPE_NORMAL
  zh: $$E{\mathrm{average}}={\frac{1}{T}}\sum_{t=1}^{T}\left[\left({\frac{1}{m}}\sum_{i=1}^{m}y_{i,t+n}\right)-y_{t+n}^{d}\right]^{2}$$
- en: $$(17.8)$$
  id: totrans-4524
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.8)$$
- en: 1 Note that in the case of a quadratic error function, the interaction layer
    can be substituted by a single output layer of point predictions, combined with
    a positive definite non-diagonal quadratic form as target function.
  id: totrans-4525
  prefs: []
  type: TYPE_NORMAL
  zh: 1 注意，在二次误差函数的情况下，交互层可以用单个输出层的点预测替代，结合正定非对角的二次形式作为目标函数。
- en: '![370_image_0.png](370_image_0.png)'
  id: totrans-4526
  prefs: []
  type: TYPE_IMG
  zh: '![370_image_0.png](370_image_0.png)'
- en: Fig. 17.6. Averaging of several point predictions for the same forecast horizon
  id: totrans-4527
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.6. 同一预测期的多个点预测的平均
- en: with ydt+n as the target pattern, yi,t+n as the output of sub-network i, m as
    the number of subnetworks, and T as the number of training patterns. Assuming
    that the errors of the sub-networks are uncorrelated,
  id: totrans-4528
  prefs: []
  type: TYPE_NORMAL
  zh: 以ydt+n作为目标模式，yi,t+n作为子网络i的输出，m为子网络的数量，T为训练模式的数量。假设子网络的误差是无关的，
- en: $$\frac{1}{T}\sum_{t=1}^{T}(y_{i,t+n}-y_{t+n}^{d})(y_{j,t+n}-y_{t+n}^{d})=0\;,\;\;\forall\;i\neq
    j\tag{17.9}$$
  id: totrans-4529
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{1}{T}\sum_{t=1}^{T}(y_{i,t+n}-y_{t+n}^{d})(y_{j,t+n}-y_{t+n}^{d})=0\;,\;\;\forall\;i\neq
    j\tag{17.9}$$
- en: leads to
  id: totrans-4530
  prefs: []
  type: TYPE_NORMAL
  zh: 导致
- en: $$E_{\rm average}=\frac{1}{m}\left(\frac{1}{m}\sum_{i=1}^{m}\left(\frac{1}{T}\sum_{t=1}^{T}(y_{i,t+n}-y_{t+n}^{d})^{2}\right)\right)\tag{17.10}$$
  id: totrans-4531
  prefs: []
  type: TYPE_NORMAL
  zh: $$E_{\rm average}=\frac{1}{m}\left(\frac{1}{m}\sum_{i=1}^{m}\left(\frac{1}{T}\sum_{t=1}^{T}(y_{i,t+n}-y_{t+n}^{d})^{2}\right)\right)\tag{17.10}$$
- en: $=\frac{1}{m}$ average ($E_{\mbox{sub-networks}}$). (17.11)
  id: totrans-4532
  prefs: []
  type: TYPE_NORMAL
  zh: $=\frac{1}{m}$ 平均 ($E_{\mbox{sub-networks}}$). (17.11)
- en: According to this argument, the error due to the uncertainty of the training
    can be reduced. Finally, it is to be noted that averaging adds no additional information
    about the specific application of our network.
  id: totrans-4533
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个论点，训练的不确定性造成的误差可以减少。最后需要指出的是，平均化并没有为我们网络的具体应用添加额外的信息。
- en: 17.3.6 Regularization By Random Targets
  id: totrans-4534
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.3.6 随机目标的正则化
- en: It is known in the neural network community that the addition of random targets
    can improve the learning behavior2. An architectural realization of this idea
    is shown fig. 17.7.
  id: totrans-4535
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络领域，增加随机目标可以改善学习行为2。这个想法的架构实现如图17.7所示。
- en: In economical applications we typically use large input vectors in order to
    capture all probably relevant indicators. The resulting large number of parameters
    if connecting the input layer to a hidden layer of an appropriate size is the
    source
  id: totrans-4536
  prefs: []
  type: TYPE_NORMAL
  zh: 在经济应用中，我们通常使用大输入向量以捕捉所有可能相关的指标。连接输入层到合适大小的隐藏层所产生的大量参数是来源。
- en: 2 The authors would appreciate any useful citation.
  id: totrans-4537
  prefs: []
  type: TYPE_NORMAL
  zh: 2 作者将感谢任何有用的引用。
- en: '![371_image_0.png](371_image_0.png)'
  id: totrans-4538
  prefs: []
  type: TYPE_IMG
  zh: '![371_image_0.png](371_image_0.png)'
- en: Fig. 17.7. A simple architecture using the original targets (upper left branch)
    and the random targets for regularization (upper right branch)
  id: totrans-4539
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.7. 使用原始目标（左上分支）和用于正则化的随机目标（右上分支）的简单架构
- en: of overfitting. To partly remedy this effect, one can extend the neural network
    with additional but random targets. As a decreasing error of this outputs can
    only be achieved by memorizing these random events, this technique absorbs a part
    of the overparametrization of the network. It should not be confused with artificial
    noise on the output side because the additional patterns are randomly selected
    according a probability distribution before the training and are held fixed during
    the learning.
  id: totrans-4540
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合的部分。为部分缓解这一影响，可以用额外但随机的目标扩展神经网络。由于仅通过记忆这些随机事件才能实现这些输出的误差下降，这一技术吸收了网络的部分过参数化。它不应与输出端的人工噪声混淆，因为额外模式是在训练前根据概率分布随机选择的，并在学习过程中保持不变。
- en: The question if enough or too many additional random targets are supplied can
    be answered by observing the learning behavior. During training the error with
    respect to these random targets is steadily decreasing until convergence. If the
    number of additional targets is too small one may observe overfitting effects
  id: totrans-4541
  prefs: []
  type: TYPE_NORMAL
  zh: 是否提供了足够或过多的随机目标的问题可以通过观察学习行为来回答。在训练过程中，相对于这些随机目标的误差在收敛之前稳步下降。如果额外目标的数量太少，可能会观察到过拟合效应。
- en: (e. g. using a validation set) on the real targets after this convergence. On
    the other hand, if the the number of additional targets is too large the learning
    slows down.
  id: totrans-4542
  prefs: []
  type: TYPE_NORMAL
  zh: （例如，在收敛后对真实目标使用验证集）。另一方面，如果额外目标的数量过多，学习会减缓。
- en: One may suspect the parameters focusing on the random targets may have a unpredictable
    effect on the generalization set but we could not observe such a behavior. If
    using squared inputs (see sec. 17.3.3), their local learning possibilities supports
    this by encapsulating local artifacts.
  id: totrans-4543
  prefs: []
  type: TYPE_NORMAL
  zh: 人们可能会怀疑专注于随机目标的参数对泛化集可能产生不可预测的影响，但我们并未观察到这种行为。如果使用平方输入（见第17.3.3节），它们的局部学习可能性通过封装局部伪影来支持这一点。
- en: 17.3.7 An Integrated Network Architecture For Forecasting Problems
  id: totrans-4544
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.3.7 针对预测问题的综合网络架构
- en: 'As a next step in our investigation, we suppose our task to be an economic
    forecasting model. In the following section we integrate the architectural building
    blocks discussed above into an eleven layer network specialized to fulfill our
    purpose (see fig. 17.8). As indicated in section 17.2, the net external preprocessing
    contains at least two inputs per original time series: the momentum in the form
    of the relative difference (eq. 17.1) and a force indicator in the form of a curvature
    or mean reverting description (eq. 17.2).'
  id: totrans-4545
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们调查的下一步，我们假设我们的任务是一个经济预测模型。在接下来的部分中，我们将上述讨论的建筑模块整合成一个专门为实现我们目的而设计的十一层网络（见图17.8）。如第17.2节所示，网络外部预处理至少包含每个原始时间序列两个输入：动量以相对差异的形式（方程17.1）和以曲率或均值回复描述的形式的力指示器（方程17.2）。
- en: The lowest part of the network shows the net internal preprocessing by a diagonal
    connector. Alternatively, we could also use the bottleneck network. By the square
    layer, we allow our network to cover the difference and similarity analysis of
    MLP- and RBF- networks. The signals from the internally preprocessed inputs and
    their squared values are used as inputs, weighted by the associated parameters,
    to the hidden embedding and hidden force layers (see fig. 17.8).
  id: totrans-4546
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的最底部通过对角连接显示了网络内部预处理。我们也可以使用瓶颈网络。通过平方层，我们允许网络覆盖MLP和RBF网络的差异和相似性分析。来自内部预处理输入及其平方值的信号作为输入使用，并通过相关参数加权，输入到隐藏的嵌入层和隐藏的力层（见图17.8）。
- en: In contrast to typical neural networks, the upper part of the net is organized
    in a new way. The underlying dynamical system is supposed to be characterized
    by an estimation of different features around the forecast horizon. We distinguish
    these indicators by two particular characteristics, embeddings and forces.
  id: totrans-4547
  prefs: []
  type: TYPE_NORMAL
  zh: 与典型的神经网络相比，网络的上半部分以一种新的方式组织。基础动态系统应通过对预测视野周围不同特征的估计来表征。我们通过两个特定特征区分这些指标，即嵌入和力。
- en: 'The forecast of these different features has been separated in two branches
    of the network in order to avoid interferences during training. Instead of directly
    forecasting our final target which has the form:'
  id: totrans-4548
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在训练过程中干扰，这些不同特征的预测已被分为网络的两个分支。我们没有直接预测形式如下的最终目标：
- en: $${\frac{y_{t+n}-y_{t}}{y_{t}}}\,,$$
  id: totrans-4549
  prefs: []
  type: TYPE_NORMAL
  zh: $${\frac{y_{t+n}-y_{t}}{y_{t}}}\,,$$
- en: 'we use the following indicators as targets in these two output layers:'
  id: totrans-4550
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这两个输出层中使用以下指标作为目标：
- en: 'embeddings:'
  id: totrans-4551
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入：
- en: $$u_{i}=\frac{y_{t+n+i}+y_{t+n}+y_{t+n-i}}{3y_{t}}-1\;i=1,\ldots m$$  $$v_{i}=\frac{\frac{1}{2i+1}\sum_{j=-i}^{i}y_{t+n+j}-y_{t}}{y_{t}}\qquad
    i=1,\ldots m$$
  id: totrans-4552
  prefs: []
  type: TYPE_NORMAL
  zh: $$u_{i}=\frac{y_{t+n+i}+y_{t+n}+y_{t+n-i}}{3y_{t}}-1\;i=1,\ldots m$$  $$v_{i}=\frac{\frac{1}{2i+1}\sum_{j=-i}^{i}y_{t+n+j}-y_{t}}{y_{t}}\qquad
    i=1,\ldots m$$
- en: $$(17.12)$$
  id: totrans-4553
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.12)$$
- en: $$(17.13)$$
  id: totrans-4554
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.13)$$
- en: 'forces:'
  id: totrans-4555
  prefs: []
  type: TYPE_NORMAL
  zh: 力：
- en: $r_{i}=\frac{-y_{t+n+i}+2y_{t+n}-y_{t+n-i}}{3y_{t}}\quad i=1,\ldots m$  $s_{i}=\frac{y_{t+n}-\frac{1}{2i+1}\sum_{j=-i}^{i}y_{t+n+j}}{y_{t}}\quad
    i=1,\ldots m$
  id: totrans-4556
  prefs: []
  type: TYPE_NORMAL
  zh: $r_{i}=\frac{-y_{t+n+i}+2y_{t+n}-y_{t+n-i}}{3y_{t}}\quad i=1,\ldots m$  $s_{i}=\frac{y_{t+n}-\frac{1}{2i+1}\sum_{j=-i}^{i}y_{t+n+j}}{y_{t}}\quad
    i=1,\ldots m$
- en: Target ui describes a normalized 3-point embedding with respect to our forecast
    horizon t + n while vi represents a complete average around t + n versus present
    time. The forces ri and si are formulated as curvatures or mean reverting (see
    also section 17.2). Similar to the embeddings they describe features of a dynamical
    system with an increasing width. The different widths in turn allow a complete
    characterization of the time series, analogous to the characterization by points
    in Takens' Theorem [29].
  id: totrans-4557
  prefs: []
  type: TYPE_NORMAL
  zh: 目标ui描述了相对于我们的预测范围t + n的标准化3点嵌入，而vi则表示t + n与当前时间的完整平均值。力ri和si被公式化为曲率或均值回归（另见第17.2节）。与嵌入相似，它们描述了一个宽度不断增加的动态系统特征。这些不同的宽度反过来允许对时间序列进行完整特征描述，类似于通过Takens定理[29]进行的点特征描述。
- en: The motivation of this design is contained in the step from the embedding and
    force output layers to the multiforecast output layer. We have
  id: totrans-4558
  prefs: []
  type: TYPE_NORMAL
  zh: 该设计的动机包含在从嵌入和力输出层到多重预测输出层的步骤中。我们有
- en: $\begin{array}{c}\underline{y_{t+n}-y_{t}}\\ \underline{y_{t}}\\ \underline{y_{t+n}-y_{t}}\\
    \underline{y_{t}}\end{array}=u_{i}+r_{i}\ i=1,\ldots m$.
  id: totrans-4559
  prefs: []
  type: TYPE_NORMAL
  zh: $\begin{array}{c}\underline{y_{t+n}-y_{t}}\\ \underline{y_{t}}\\ \underline{y_{t+n}-y_{t}}\\
    \underline{y_{t}}\end{array}=u_{i}+r_{i}\ i=1,\ldots m$。
- en: $$(17.14)$$
  id: totrans-4560
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.14)$$
- en: That means, we get 2m estimations of our final target by simply adding up the
    embeddings and their associated forces in pairs. In the network this is easy to
    realize by two identity connectors. After the multiforecast layer we can add an
    averaging connector to get our final output. This averaging can be done by fixed
    weights 1/2m or by learning after finishing the training of the lower network.
  id: totrans-4561
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，我们通过简单地将嵌入及其相关力成对相加，获得了我们的最终目标的2m个估计。在网络中，这可以通过两个恒等连接器轻松实现。在多重预测层之后，我们可以添加一个平均连接器以获得最终输出。这种平均可以通过固定权重1/2m或在完成下层网络训练后进行学习。
- en: The control-embedding and control-force clusters are motivated by the following
    observation. On the embedding / force output level the network has to estimate
    only slightly different features depending on the width parameter.
  id: totrans-4562
  prefs: []
  type: TYPE_NORMAL
  zh: 控制嵌入和控制力集群的动机源于以下观察。在嵌入/力输出层，网络必须根据宽度参数仅估计略有不同的特征。
- en: Neural networks have a tendency to estimate these features too similarly. To
    counteract this behavior we have to add two additional clusters which control
    the difference between the individual outputs inside the embedding and the force
    cluster. Thus, the network has not only to learn ui and ui+1 on the embedding
    output level but also the difference ui − ui+1 on the control embedding level.
    The same is valid for vi, ri, si.
  id: totrans-4563
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络倾向于对这些特征进行过于相似的估计。为了解决这种行为，我们必须增加两个附加集群，以控制嵌入和力集群内部各个输出之间的差异。因此，网络不仅需要在嵌入输出层学习ui和ui+1，还需要在控制嵌入层学习差异ui
    − ui+1。vi、ri和si同样适用。
- en: From a formal viewpoint the multiforecast cluster, the control-embedding cluster
    and control-force cluster are interaction layers supporting the identification
    of the underlying dynamical system in form of embeddings and forces. Keep in mind,
    that although the full network seems to be relatively complex, most of the connectors
    are fixed during training. Those are only used to produce the appropriate information
    flows whose design is the real focus of this section.
  id: totrans-4564
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式观点来看，多重预测集群、控制嵌入集群和控制力集群是支持识别底层动态系统（以嵌入和力的形式）的交互层。请记住，尽管整个网络看起来相对复杂，但在训练期间大多数连接是固定的。那些仅用于产生适当的信息流，其设计才是本节的真正重点。
- en: 'Our proposed network design allows for an intuitive evaluation of the dimension
    of the target indicators *embeddings* and *forces*: how to choose m in eq. 17.13?'
  id: totrans-4565
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的网络设计允许直观评估目标指标的 *嵌入* 和 *力* 的维度：如何在方程 17.13 中选择 \(m\)？
- en: Start with a relative large m and train the network to the minimal error on
    the training set as will be described in section 17.8. Then train only the weights
    of the up to now fixed connector between cluster multi-forecast and average forecast
    (fig. 17.8). If the dimension m has been chosen too large training may lead to
    such weights which suppress the long range embeddings and forces. Thus, it is
    possible to achieve an optimal dimension m of embedding and forces. For the case
    of a six month forecast horizon we were successful with a value of m = 6.
  id: totrans-4566
  prefs: []
  type: TYPE_NORMAL
  zh: 从相对较大的 \(m\) 开始，并训练网络以在训练集上达到最小误差，如第 17.8 节所述。然后仅训练集群多重预测与平均预测之间的固定连接的权重（图 17.8）。如果维度
    \(m\) 选择得过大，训练可能导致压制长范围嵌入和力的权重。因此，可能实现嵌入和力的最佳维度 \(m\)。在六个月的预测范围内，我们成功地使用了 \(m
    = 6\) 的值。
- en: The eleven layer network automatically integrates aspects of section 17.3.6
    concerning random targets. If the dimension m has been chosen too large, then
    the extreme target indicators act as random targets. On the other hand, if the
    forecast problem is characterized by high noise in the short term, the indicators
    for smaller m values generate random targets. Thus, choosing m too large does
    not harm our network design as discussed in section 17.3.6, but can improve generalization
    by partly adsorbing the overparametrization of the network.
  id: totrans-4567
  prefs: []
  type: TYPE_NORMAL
  zh: 十一层网络自动集成了关于随机目标的第 17.3.6 节的各个方面。如果选择的维度 \(m\) 过大，则极端目标指标表现为随机目标。另一方面，如果预测问题在短期内特征噪声较高，则较小
    \(m\) 值的指标会生成随机目标。因此，选择过大的 \(m\) 不会损害我们的网络设计，如第 17.3.6 节所述，但可以通过部分吸收网络的过参数化来提高泛化能力。
- en: '![374_image_0.png](374_image_0.png)'
  id: totrans-4568
  prefs: []
  type: TYPE_IMG
  zh: '![374_image_0.png](374_image_0.png)'
- en: Fig. 17.8. The integrating eleven layer architecture
  id: totrans-4569
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.8。集成的十一层架构
- en: The different forecasts oi of the multiforecast cluster in fig. 17.8 can be
    used to estimate structural instability s of the network model by
  id: totrans-4570
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.8 中的多重预测集群的不同预测 \(o_i\) 可用于通过
- en: $s=\frac{1}{2}$
  id: totrans-4571
  prefs: []
  type: TYPE_NORMAL
  zh: \(s=\frac{1}{2}\)
- en: s = -
  id: totrans-4572
  prefs: []
  type: TYPE_NORMAL
  zh: \(s = -\)
- en: $$\sum_{i=1}^{2m}|o_{i}-\overline{o}|\quad\mbox{with}\quad\overline{o}=\frac{1}{2m}\sum_{i=1}^{2m}o_{i}.\tag{17.15}$$
  id: totrans-4573
  prefs: []
  type: TYPE_NORMAL
  zh: $$\sum_{i=1}^{2m}|o_{i}-\overline{o}|\quad\mbox{其中}\quad\overline{o}=\frac{1}{2m}\sum_{i=1}^{2m}o_{i}.\tag{17.15}$$
- en: A subsequent decision support system using this network can interpret the measurements
    s as indicators how much one can trust the model. Note that these values of uncertainty
    must not be identified with error bars as described in section 17.5 because the
    s merely quantify the instability of the learning.
  id: totrans-4574
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该网络的后续决策支持系统可以将测量值 \(s\) 解读为对模型可信度的指标。请注意，这些不确定性值不能与第 17.5 节中描述的误差条识别，因为 \(s\)
    仅量化学习的不稳定性。
- en: 17.4 Cost Functions
  id: totrans-4575
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.4 成本函数
- en: Typical error functions can be written as a sum of individual terms over all
    T training patterns,
  id: totrans-4576
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的误差函数可以写成所有 \(T\) 个训练模式的个体项之和，
- en: $$E={\frac{1}{T}}\sum_{t=1}^{T}E_{t}\,,$$
  id: totrans-4577
  prefs: []
  type: TYPE_NORMAL
  zh: $$E={\frac{1}{T}}\sum_{t=1}^{T}E_{t}\,，$$
- en: $$(17.16)$$
  id: totrans-4578
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.16)$$
- en: $$(17.17)$$
  id: totrans-4579
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.17)$$
- en: with the individual error Et depending on the network output y(xt, w) and the
    given target data ydt . The often used square error,
  id: totrans-4580
  prefs: []
  type: TYPE_NORMAL
  zh: 个体误差 \(E_t\) 依赖于网络输出 \(y(x_t, w)\) 和给定的目标数据 \(y_d^t\)。常用的平方误差，
- en: $$E_{t}=\frac{1}{2}\left(y(x_{t},w)-y_{t}^{d}\right)^{2}\,,$$
  id: totrans-4581
  prefs: []
  type: TYPE_NORMAL
  zh: $$E_{t}=\frac{1}{2}\left(y(x_{t},w)-y_{t}^{d}\right)^{2}\,，$$
- en: 2 , (17.17)
  id: totrans-4582
  prefs: []
  type: TYPE_NORMAL
  zh: 2，(17.17)
- en: 'can be derived from the maximum-likelihood principle and a Gaussian noise model.
    Eq. 17.17 yields relatively simple error derivatives and results in asymptotically
    best estimators under certain distribution assumptions, i. e. homoscedasticity.
    In practical applications, however, several of these assumptions are commonly
    violated, which may dramatically reduce the prediction reliability of the neural
    network. A problem arising from this violation is the large impact outliers in
    the target data can have on the learning, which is also a result of scaling the
    original time series to zero mean and unit variance. This effect is particularly
    serious for data in finance and economics which contain large shocks. Another
    cause of difficulties in financial time series analysis is heteroscedasticity,
    i.e. situations where the variance of target variable changes over time. We will
    especially consider cases where the variance is input-dependent: σ2t = σ2(xt).'
  id: totrans-4583
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过最大似然原理和高斯噪声模型推导而得。公式17.17在某些分布假设下产生相对简单的误差导数，并在渐近上得到最佳估计器，即同方差性。然而，在实际应用中，这些假设常常被违反，这可能显著降低神经网络的预测可靠性。这种违反带来的一个问题是，目标数据中的异常值对学习的巨大影响，这也是将原始时间序列缩放到零均值和单位方差的结果。对于包含大冲击的金融和经济数据，这种影响尤为严重。金融时间序列分析中的另一个困难来源是异方差性，即目标变量的方差随时间变化的情况。我们将特别考虑方差依赖于输入的情况：σ2t
    = σ2(xt)。
- en: We propose two approaches to reduce the problems resulting from outliers and
    heteroscedasticity.
  id: totrans-4584
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出两种方法来减少由于异常值和异方差性造成的问题。
- en: 17.4.1 Robust Estimation With Lncosh
  id: totrans-4585
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.4.1 使用Lncosh的鲁棒估计
- en: Outliers are common in financial time series and are usually caused by "information
    shocks" such as announcements of government data or dividends paid by companies
    that are out of line with market expectations. These shocks appear as discontinuities
    in the trajectory of an affected asset. To be robust against such shocks, typical
    cost functions like
  id: totrans-4586
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值在金融时间序列中很常见，通常是由于“信息冲击”引起的，比如政府数据公告或公司派发的股息，这些通常与市场预期不符。这些冲击在受影响资产的轨迹中表现为不连续性。为了对这些冲击具有鲁棒性，典型的成本函数如
- en: $$E_{t}=\left|y(x_{t},w)-y_{t}^{d}\right|\tag{1}$$
  id: totrans-4587
  prefs: []
  type: TYPE_NORMAL
  zh: $$E_{t}=\left|y(x_{t},w)-y_{t}^{d}\right|\tag{1}$$
- en: (17.18)
  id: totrans-4588
  prefs: []
  type: TYPE_NORMAL
  zh: (17.18)
- en: are used which do not overweight large errors. A smoother version is given by
  id: totrans-4589
  prefs: []
  type: TYPE_NORMAL
  zh: 被使用，不会对大误差给予过重的权重。一个更平滑的版本由
- en: $$(17.18)$$
  id: totrans-4590
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.18)$$
- en: $E_{t}=\frac{1}{a}\log\cosh\left(a\left(y(x_{t},w)-y_{t}^{d}\right)\right)$,  $\frac{1}{a}$.
    We typically use $a\in[3,4]$. This function app
  id: totrans-4591
  prefs: []
  type: TYPE_NORMAL
  zh: $E_{t}=\frac{1}{a}\log\cosh\left(a\left(y(x_{t},w)-y_{t}^{d}\right)\right)$，$\frac{1}{a}$。我们通常使用$a\in[3,4]$。这个函数应用
- en: $$(17.19)$$
  id: totrans-4592
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.19)$$
- en: ', (17.19)'
  id: totrans-4593
  prefs: []
  type: TYPE_NORMAL
  zh: ', (17.19)'
- en: with parameter a > 1. We typically use a ∈ [3, 4]. This function approximates
    the parabola of the squared errors for small differences (Gaussian noise model),
    and is proportional to the absolute value of the difference for larger values
    of the difference (Laplacian noise model). The assumed noise model, the cost function
    17.19 and its derivative for the case a = 5 is shown in fig. 17.9. The function
  id: totrans-4594
  prefs: []
  type: TYPE_NORMAL
  zh: 参数a > 1。我们通常使用a ∈ [3, 4]。这个函数对小差异的平方误差的抛物线进行近似（高斯噪声模型），并且对于较大差异的绝对值成比例（拉普拉斯噪声模型）。假设的噪声模型、成本函数17.19及其在a
    = 5时的导数如图17.9所示。该函数
- en: '![376_image_0.png](376_image_0.png)'
  id: totrans-4595
  prefs: []
  type: TYPE_IMG
  zh: '![376_image_0.png](376_image_0.png)'
- en: Fig. 17.9. The Laplacian-like noise model, left, the log cosh error function
    17.19, middle, and its corresponding derivative, right
  id: totrans-4596
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.9。左侧是类拉普拉斯噪声模型，中间是对数cosh误差函数17.19，右侧是其相应的导数
- en: log cosh is motivated by the observation that the derivative of |x| is sign(x)
  id: totrans-4597
  prefs: []
  type: TYPE_NORMAL
  zh: log cosh的动机在于观察到|x|的导数是sign(x)。
- en: and tanh(ax) is a smooth approximation of this step function with the integral
  id: totrans-4598
  prefs: []
  type: TYPE_NORMAL
  zh: tanh(ax)是该阶跃函数的光滑近似，其积分为
- en: = tanh(az)dz = 1a log cosh (az).
  id: totrans-4599
  prefs: []
  type: TYPE_NORMAL
  zh: = tanh(az)dz = 1a log cosh (az)。
- en: 17.4.2 Robust Estimation With Cden
  id: totrans-4600
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.4.2 使用Cden的鲁棒估计
- en: This section describes a more general framework for robust estimation which
    is based on the theory of density estimation. The advantages are twofold. First,
    any parameters of the cost function (e. g. the a for eq. 17.19) can be determined
    by learning which avoids an artificial bias by setting those parameters to predefined
    values. Second, the proposed methods allow the modeling of heteroscedastic time
    series whose variance changes over time.
  id: totrans-4601
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了一种更通用的鲁棒估计框架，该框架基于密度估计理论。其优势有二。首先，成本函数的任何参数（例如公式17.19中的a）都可以通过学习来确定，避免了通过将这些参数设置为预定义值而产生的人工偏差。其次，所提出的方法允许建模方差随时间变化的异方差时间序列。
- en: Probability density estimating neural networks have recently gained major attention
    in the neural network community as a more adequate tool to describe probabilistic
    relations than common feed-forward networks (see [35], [22] and [20]). Interest
    has hereby focussed on exploiting the additional information which is inherent
    to the conditional density, for example the conditional variance as a measure
    of prediction reliability [28], the representation of multi-valued mappings in
    the form of multi-modal densities to approach inverse problems [3], or the use
    of the conditional densities for optimal portfolio construction [18]. In this
    paper, we will use the *Conditional Density Estimation Network (CDEN)*, which
    is, among various other density estimation methods, extensively discussed in [22]
    and [20].
  id: totrans-4602
  prefs: []
  type: TYPE_NORMAL
  zh: 概率密度估计神经网络最近在神经网络领域引起了重大关注，因为它比常见的前馈网络更适合描述概率关系（见[35]、[22]和[20]）。因此，兴趣集中在利用条件密度固有的额外信息上，例如条件方差作为预测可靠性的度量[28]，以多模态密度的形式表示多值映射来处理逆问题[3]，或使用条件密度进行最优投资组合构建[18]。在本文中，我们将使用*条件密度估计网络（CDEN）*，这是在[22]和[20]中广泛讨论的多种密度估计方法之一。
- en: A possible architecture of the CDEN is shown in fig. 17.10. It is assumed that
    p(y|x), the conditional density to be identified, may be formulated as the parametric
    density p(y|φ(x)). The condition on x is realized by the parameter vector φ which
    determines the form of the probability distribution p(y|φ(x)). Both φ(x) and p(y|·)
    may be implemented as neural networks with the output of the first determining
    the weights of the latter. Denoting the weights contained in the parameter prediction
    network φ(x) as w, we may thus write p(y|*x, w*) = p(y|φw(x)).
  id: totrans-4603
  prefs: []
  type: TYPE_NORMAL
  zh: CDEN的可能架构如图17.10所示。假设p(y|x)是要识别的条件密度，可以表示为参数密度p(y|φ(x))。关于x的条件由参数向量φ实现，该向量确定概率分布p(y|φ(x))的形式。φ(x)和p(y|·)均可以实现为神经网络，第一个的输出决定了后者的权重。将参数预测网络φ(x)中包含的权重表示为w，我们可以写成p(y|*x,
    w*) = p(y|φw(x))。
- en: Assuming independence such that p(y1, ··· , yT |·) = p(y1|·)··· p(yT |·) we
    minimize the negative Log-Likelihood error function.
  id: totrans-4604
  prefs: []
  type: TYPE_NORMAL
  zh: 假设独立性，使得p(y1, ··· , yT |·) = p(y1|·)··· p(yT |·)，我们最小化负对数似然误差函数。
- en: $$E=-\mathrm{log}p(y_{1},\cdots,y_{T}|\cdot)$$
  id: totrans-4605
  prefs: []
  type: TYPE_NORMAL
  zh: $$E=-\mathrm{log}p(y_{1},\cdots,y_{T}|\cdot)$$
- en: $$=-\log\prod_{t=1}^{T}p(y_{t}|x_{t},w)$$
  id: totrans-4606
  prefs: []
  type: TYPE_NORMAL
  zh: $$=-\log\prod_{t=1}^{T}p(y_{t}|x_{t},w)$$
- en: $$(17.20)\,$$.
  id: totrans-4607
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.20)\,$$。
- en: $$=-\sum_{t=1}^{T}\log p(y_{t}|x_{t},w)$$
  id: totrans-4608
  prefs: []
  type: TYPE_NORMAL
  zh: $$=-\sum_{t=1}^{T}\log p(y_{t}|x_{t},w)$$
- en: logp(yt|xt, w) (17.20)
  id: totrans-4609
  prefs: []
  type: TYPE_NORMAL
  zh: logp(yt|xt, w) (17.20)
- en: by performing gradient descent using a variant of the common backpropagation
    algorithm, we give way to a maximum likelihood estimate of the weights in the
    parameter prediction network [22, 20].
  id: totrans-4610
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用常见反向传播算法的变体进行梯度下降，我们为参数预测网络中的权重提供了最大似然估计[22, 20]。
- en: '![377_image_0.png](377_image_0.png)'
  id: totrans-4611
  prefs: []
  type: TYPE_IMG
  zh: '![377_image_0.png](377_image_0.png)'
- en: Fig. 17.10. Conditional Density Estimation Network, CDEN
  id: totrans-4612
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.10. 条件密度估计网络，CDEN
- en: Specific problems with the discussed model are first approached by determining
    an appropriate density family p(y|φ(x)). A powerful choice is a Gaussian mixture
  id: totrans-4613
  prefs: []
  type: TYPE_NORMAL
  zh: 具体问题通过确定适当的密度族p(y|φ(x))来首先解决。一个强有力的选择是高斯混合模型。
- en: $$p(y|\phi(x))=\sum_{i=1}^{n}P_{i}(x)\ p(y|\mu_{i}(x),\sigma_{i}(x)),\quad P_{i}(x)\geq0,\
    \sum_{i}^{n}P_{i}(x)=1\,,\tag{17.21}$$
  id: totrans-4614
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(y|\phi(x))=\sum_{i=1}^{n}P_{i}(x)\ p(y|\mu_{i}(x),\sigma_{i}(x)),\quad P_{i}(x)\geq0,\
    \sum_{i}^{n}P_{i}(x)=1\,,\tag{17.21}$$
- en: because they cover a wide range of probability models. For the univariate case
  id: totrans-4615
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它们覆盖了广泛的概率模型。对于单变量情况
- en: '(one output), the p(y|μi(x), σi(x)) are normal density functions:'
  id: totrans-4616
  prefs: []
  type: TYPE_NORMAL
  zh: （一个输出），p(y|μi(x), σi(x))是正态密度函数：
- en: $$p(y|\mu_{i}(x),\sigma_{i}(x))=\frac{1}{\sqrt{2\pi}\sigma_{i}(x)}e^{-\frac{1}{2}\left(\frac{y-\mu_{i}(x)}{\sigma_{i}(x)}\right)^{2}}.\tag{17.22}$$
  id: totrans-4617
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(y|\mu_{i}(x),\sigma_{i}(x))=\frac{1}{\sqrt{2\pi}\sigma_{i}(x)}e^{-\frac{1}{2}\left(\frac{y-\mu_{i}(x)}{\sigma_{i}(x)}\right)^{2}}.\tag{17.22}$$
- en: There are several ways to determine the individual density parameters contained
    in (Pi, μi, σi)ni=1. Either they are set as the output of the parameter prediction
    network, or they are trained as x-independent, adaptable weights of the density
    network, or some of them may be given by prior knowledge (e. g. clustering, neuro-fuzzy).
  id: totrans-4618
  prefs: []
  type: TYPE_NORMAL
  zh: 确定包含在(Pi, μi, σi)ni=1中的各个密度参数有几种方法。它们要么作为参数预测网络的输出设置，要么作为与x无关的可适应权重训练密度网络，或者其中一些可能由先验知识（例如聚类、神经模糊）给出。
- en: A probability model p(y|·) based on the CDEN architecture in fig. 17.11 which
    perceives the presence of outliers and thus leads to robust estimators is illustrated
    in the left part of figure 17.12. The CDEN consists of two Gaussians with identical
    estimation for their mean μ(x). While its narrow Gaussian represents the distribution
    of the non-outliers part of the data, the wider one expresses the assumption that
    some data are located at larger distances from the prediction.
  id: totrans-4619
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图17.11中的CDEN架构的概率模型p(y|·)，能够识别异常值的存在，从而导致鲁棒估计器，在图17.12的左侧部分进行了说明。CDEN由两个高斯分布组成，它们的均值μ(x)估计是相同的。其狭窄的高斯分布代表了数据中非异常值部分的分布，而较宽的高斯分布则表示某些数据位于预测值的更大距离处。
- en: 'The fact that outliers are basically exceptions may be reflected by an appropriate
    choice of the mixture weightings Pi, which may be regarded as prior probabilities
    for each Gaussian distribution. Using this probability model yields the following
    norm for the maximum-likelihood minimization:'
  id: totrans-4620
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值基本上是例外，这一事实可以通过适当选择混合权重Pi来反映，Pi可以视为每个高斯分布的先验概率。使用该概率模型可以得出最大似然最小化的以下范数：
- en: '![378_image_0.png](378_image_0.png)'
  id: totrans-4621
  prefs: []
  type: TYPE_IMG
  zh: '![378_image_0.png](378_image_0.png)'
- en: Fig. 17.11. A CDEN for limiting the influence of outliers
  id: totrans-4622
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.11。限制异常值影响的CDEN
- en: The qualitative behavior of Et in the one-dimensional case is illustrated in
    the middle and left part of figure 17.12 (compare also fig. 17.9). One may clearly
    see how the mixture limits the influence of outliers. Norms with influence-limiting
    properties are called M-estimators in regression [5].
  id: totrans-4623
  prefs: []
  type: TYPE_NORMAL
  zh: 在一维情况下，Et的定性行为在图17.12的中间和左侧部分得到了说明（也可比较图17.9）。可以清楚地看到，混合如何限制异常值的影响。具有影响限制特性的范数在回归中被称为M估计器[5]。
- en: The difficulty involved in using M-estimators is that they usually possess a
    set of parameters which have to be properly determined. In our case, the parameters
    are P1, P2, σ1 and σ2. In the framework of the CDEN architecture, they are considered
    as adaptable weights of the density network. The advantage is that unlike classical
    M-estimators, the parameters are determined by the data during training and thus
    do not bias the solution. One method which has proven to be successful in regression
    is to substitute eq. 17.23 by a mixture of a quadratic error function and a linear
    function of the absolute error to limit the influence of the outliers. An adaptive
    version of such an error measure may easily be implemented in the CDEN framework
    by substituting a mixture of Gaussian and Laplace distributions in equation 17.21.
    Other limiting error functions may be constructed alike.
  id: totrans-4624
  prefs: []
  type: TYPE_NORMAL
  zh: 使用M估计器的困难在于，它们通常具有一组需要适当确定的参数。在我们的案例中，参数是P1、P2、σ1和σ2。在CDEN架构的框架下，它们被视为密度网络的可适应权重。其优点是，与经典M估计器不同，这些参数是在训练过程中由数据确定的，因此不会偏倚解。已证明在回归中成功的方法是将方程17.23替换为二次误差函数和绝对误差的线性函数的混合，以限制异常值的影响。通过在方程17.21中替换高斯分布和拉普拉斯分布的混合，这种误差测度的自适应版本可以很容易地在CDEN框架中实现。其他限制误差函数也可以类似构建。
- en: '![379_image_0.png](379_image_0.png)'
  id: totrans-4625
  prefs: []
  type: TYPE_IMG
  zh: '![379_image_0.png](379_image_0.png)'
- en: Fig. 17.12. The Gaussian mixture noise model, left, the error function 17.23,
    middle, and its corresponding derivative, right
  id: totrans-4626
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.12。左侧为高斯混合噪声模型，中间为误差函数17.23，右侧为其对应的导数
- en: Heteroscedasticity may arise due to changes in the risk associated with an investment.
    In the stock market, for example, the variance of a stock's return is commonly
    related to the company's debt-equity ratio due to the well-known leverage effect
    of a varying income on its return on equity. Heteroscedasticity has been extensively
    studied in time series analysis and is commonly approached using the (G)ARCH methodology.
    While the latter explains the conditional variances based on past residuals, the
    CDEN particularly accounts for nonlinear dependencies of the variances on past
    observations.
  id: totrans-4627
  prefs: []
  type: TYPE_NORMAL
  zh: 异方差可能由于与投资相关的风险变化而产生。例如，在股票市场中，股票收益的方差通常与公司的债务股权比相关，这源于众所周知的杠杆效应，即收入的变化对其股本回报的影响。异方差在时间序列分析中得到了广泛研究，通常采用(G)ARCH方法进行处理。后者基于过去的残差解释条件方差，而CDEN特别考虑了方差对过去观察值的非线性依赖。
- en: If we assume a normally distributed noise model with zero mean and variance
    σ2, an appropriate representation is a single Gaussian with variable scale and
    location parameter. An implementation in the CDEN is straightforward. We use a
    parameter prediction network with two outputs, one for the conditional expectation
    and another one for the conditional variance. This special case of the CDEN has
    also been extensively investigated by Nix and Weigend [21]. During the training
    of the network, the weights are optimized with respect to
  id: totrans-4628
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设噪声模型服从正态分布，均值为零，方差为σ²，那么一个合适的表示是具有可变尺度和位置参数的单一高斯分布。在CDEN中的实现是直接的。我们使用一个具有两个输出的参数预测网络，一个用于条件期望，另一个用于条件方差。Nix和Weigend
    [21] 也对CDEN的这个特例进行了广泛研究。在网络训练期间，权重是针对
- en: $$E_{t}=\left[\log\sqrt{2\pi}\sigma_{t}(x_{t},w)+\frac{1}{2\sigma_{t}(x_{t},w)^{2}}\left(y(x_{t},w)-y_{t}^{d}\right)^{2}\right]\,.\tag{17.24}$$
  id: totrans-4629
  prefs: []
  type: TYPE_NORMAL
  zh: $$E_{t}=\left[\log\sqrt{2\pi}\sigma_{t}(x_{t},w)+\frac{1}{2\sigma_{t}(x_{t},w)^{2}}\left(y(x_{t},w)-y_{t}^{d}\right)^{2}\right]\,.\tag{17.24}$$
- en: Minimization according to eq. 17.17 and eq. 17.24 obviously only differs in
    that the individual errors (y(xt, w) − ydt ) are weighted by estimates of the
    inverse variances 1/σ2t (xt, w) in the CDEN. Training the CDEN thus corresponds
    to deriving a *generalized least square estimator (GLSE)*, except that we use
    an estimate σˆt(xt, w) instead of the unknown σt.
  id: totrans-4630
  prefs: []
  type: TYPE_NORMAL
  zh: 根据公式17.17和公式17.24的最小化显然只是在于CDEN中个体误差(y(xt, w) − ydt)被逆方差估计值1/σ²t (xt, w)加权。因此，训练CDEN相当于推导一个*广义最小二乘估计器（GLSE）*，除了我们使用一个估计σˆt(xt,
    w)而不是未知的σt。
- en: '3'
  id: totrans-4631
  prefs: []
  type: TYPE_NORMAL
  zh: '3'
- en: 3 Definition and properties of the GLSE are extensively discussed in [5]. The
    case where an estimate of σ is used during optimization is commonly denoted as
    a *two-stage* estimation.
  id: totrans-4632
  prefs: []
  type: TYPE_NORMAL
  zh: 3 GLSE的定义和性质在[5]中有广泛讨论。在优化过程中使用σ的估计通常称为*两阶段*估计。
- en: 17.5 Error Bar Estimation With Cden
  id: totrans-4633
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.5 使用CDEN的误差条估计
- en: In addition to a more robust learning, the CDEN approach can be used to estimate
    the uncertainty associated with the prediction of the expected value yt+n := y(xt,
    w). Here we assume a normally distributed error and have to optimize a likelihood
    function of the form
  id: totrans-4634
  prefs: []
  type: TYPE_NORMAL
  zh: 除了更稳健的学习外，CDEN方法还可以用来估计与期望值yt+n := y(xt, w)的预测相关的不确定性。在这里，我们假设误差服从正态分布，并需要优化如下形式的似然函数
- en: $$E=\frac{1}{T}\sum_{t=1}^{T}\left[\log\left(\sqrt{2\pi}\sigma(x_{t},w_{\sigma})\right)+\frac{(y(x_{t},w)-y_{t}^{d})^{2}}{2\sigma^{2}(x_{t},w_{\sigma})}\right]\,.$$
  id: totrans-4635
  prefs: []
  type: TYPE_NORMAL
  zh: $$E=\frac{1}{T}\sum_{t=1}^{T}\left[\log\left(\sqrt{2\pi}\sigma(x_{t},w_{\sigma})\right)+\frac{(y(x_{t},w)-y_{t}^{d})^{2}}{2\sigma^{2}(x_{t},w_{\sigma})}\right]\,.$$
- en: $$(17.25)$$
  id: totrans-4636
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.25)$$
- en: . (17.25)
  id: totrans-4637
  prefs: []
  type: TYPE_NORMAL
  zh: . (17.25)
- en: If we assume a Laplacian noise model, the cost function is
  id: totrans-4638
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设拉普拉斯噪声模型，代价函数为
- en: $$E=\frac{1}{T}\sum_{t=1}^{T}\left[\log\left(2\sigma(x_{t},w_{\sigma})\right)+\frac{|y(x_{t},w)-y_{t}^{d})|}{\sigma(x_{t},w_{\sigma})}\right]\,.$$
  id: totrans-4639
  prefs: []
  type: TYPE_NORMAL
  zh: $$E=\frac{1}{T}\sum_{t=1}^{T}\left[\log\left(2\sigma(x_{t},w_{\sigma})\right)+\frac{|y(x_{t},w)-y_{t}^{d}|}{\sigma(x_{t},w_{\sigma})}\right]\,.$$
- en: $$(17.26)$$
  id: totrans-4640
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.26)$$
- en: . (17.26)
  id: totrans-4641
  prefs: []
  type: TYPE_NORMAL
  zh: . (17.26)
- en: The log cosh-approximation of the Laplacian model has the form
  id: totrans-4642
  prefs: []
  type: TYPE_NORMAL
  zh: 拉普拉斯模型的对数cosh近似形式为
- en: $$E=\frac{1}{T}\sum_{t=1}^{T}\left[\log\left(\pi\sigma(x_{t},w_{\sigma})\right)+\log\cosh\left(\frac{y(x_{t},w)-y_{t}^{d}}{\sigma(x_{t},w_{\sigma})}\right)\right]\,.\tag{17.27}$$
  id: totrans-4643
  prefs: []
  type: TYPE_NORMAL
  zh: $$E=\frac{1}{T}\sum_{t=1}^{T}\left[\log\left(\pi\sigma(x_{t},w_{\sigma})\right)+\log\cosh\left(\frac{y(x_{t},w)-y_{t}^{d}}{\sigma(x_{t},w_{\sigma})}\right)\right]\,.\tag{17.27}$$
- en: In fig. 17.13 we show a network architecture which we found useful to apply
    such error bar estimations. The architecture combines net internal preprocessing
    with the squared input approach and estimates in two branches the expected value
    y(xt, w) and the standard derivation σ(xt, wσ). The CDEN cluster combines these
    pieces of information and computes the flow of the error.
  id: totrans-4644
  prefs: []
  type: TYPE_NORMAL
  zh: 在图17.13中，我们展示了一个网络架构，我们发现这种架构对应用误差条估计非常有用。该架构将网络内部预处理与平方输入方法结合，并在两个分支中估计期望值y(xt,
    w)和标准差σ(xt, wσ)。CDEN集群结合这些信息并计算误差流。
- en: We have found that the mixture of local and global analysis as shown in fig.
    17.13 harmonizes well with the aim of solving a forecasting problem including
    error bars. To assure positive values for σ(x) a suitable activation function
    e(·)
  id: totrans-4645
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，如图17.13所示，局部与全局分析的混合与解决包括误差条的预测问题的目标相得益彰。为确保σ(x)的正值，使用合适的激活函数e(·)
- en: is used in the σ cluster. By using a positive offset (bias) one can avoid the
    singularities in the likelihood target function which are caused by very small
    σ(x).
  id: totrans-4646
  prefs: []
  type: TYPE_NORMAL
  zh: 在σ集群中使用。通过使用正偏移（偏差），可以避免由于非常小的σ(x)而导致的似然目标函数中的奇点。
- en: 'In fig. 17.14 the CDEN with one Gauss-function is combined with the architecture
    of section 17.3.7. Here we assume that the estimated "forces" contain the necessary
    information to approximate the uncertainty of our averaged forcecast:'
  id: totrans-4647
  prefs: []
  type: TYPE_NORMAL
  zh: 在图17.14中，具有一个高斯函数的CDEN与第17.3.7节的架构相结合。我们假设估计的“力量”包含了近似我们平均强度预报的不确定性所需的信息：
- en: $$\sigma^{2}(y_{t+n}|x_{t})=\sigma^{2}(y_{t+n}|\mathrm{forces}(x_{t}))$$
  id: totrans-4648
  prefs: []
  type: TYPE_NORMAL
  zh: $$\sigma^{2}(y_{t+n}|x_{t})=\sigma^{2}(y_{t+n}|\mathrm{forces}(x_{t}))$$
- en: 'or, more specifically, using the acceleration and mean reverting forces of
    fig. 17.14:'
  id: totrans-4649
  prefs: []
  type: TYPE_NORMAL
  zh: 或者更具体地说，使用图17.14中的加速度和均值回归力：
- en: $$\sigma^{2}(y_{t+n}|x_{t})=\sum_{i=1}^{2m}w_{i}\cdot\mbox{force}_{i}^{2}(x_{t}).\tag{17.29}$$
  id: totrans-4650
  prefs: []
  type: TYPE_NORMAL
  zh: $$\sigma^{2}(y_{t+n}|x_{t})=\sum_{i=1}^{2m}w_{i}\cdot\mbox{force}_{i}^{2}(x_{t}).\tag{17.29}$$
- en: $$(17.28)$$
  id: totrans-4651
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.28)$$
- en: '![381_image_0.png](381_image_0.png)'
  id: totrans-4652
  prefs: []
  type: TYPE_IMG
  zh: '![381_image_0.png](381_image_0.png)'
- en: Fig. 17.13. Error bar estimation with CDEN
  id: totrans-4653
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.13. 使用CDEN进行误差条估计
- en: By a linear combination of the squared values of the forces, this architecture
    learns the input dependent variance σ(yt+n|xt) (see fig. 17.14 and eq. 17.13 for
    a description of forces). Thus, we are able to achieve a forecast as well as an
    error bar.
  id: totrans-4654
  prefs: []
  type: TYPE_NORMAL
  zh: '通过对力量的平方值进行线性组合，该架构学习输入相关的方差σ(yt+n|xt)（见图17.14和方程17.13描述力量）。因此，我们能够实现预测以及误差条。 '
- en: The interesting point in this combination is not only to be seen in the possibility
    of using the general frame as the means to identify dynamical systems.
  id: totrans-4655
  prefs: []
  type: TYPE_NORMAL
  zh: 这个组合中有趣的一点不仅在于使用一般框架来识别动态系统的可能性。
- en: In this environment we can even analyze the responsibility of the forces, long
    range or short range, with respect to the uncertainty of the forecast. In several
    monthly forecast models we have found a monotonic increase of importance from
    the short to the long range forces.
  id: totrans-4656
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个环境中，我们甚至可以分析力量的责任，无论是长程还是短程，针对预测的不确定性。在几个月的预测模型中，我们发现短程力量到长程力量的重要性单调增加。
- en: Error bars and variance estimates are an essential piece of information for
    the typically used mean-variance approach in portfolio management [7]. The aim
    is
  id: totrans-4657
  prefs: []
  type: TYPE_NORMAL
  zh: 误差条和方差估计是投资组合管理中通常使用的均值-方差方法的基本信息[7]。目标是
- en: '![382_image_0.png](382_image_0.png)'
  id: totrans-4658
  prefs: []
  type: TYPE_IMG
  zh: '![382_image_0.png](382_image_0.png)'
- en: Fig. 17.14. Error bar estimation with CDEN using the architecture of section
    17.3. The control-embedding and control-force clusters are suppressed for visual
    clarity.
  id: totrans-4659
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.14. 使用第17.3节架构的CDEN进行误差条估计。控制嵌入和控制力集群因视觉清晰度而被抑制。
- en: to compute efficient portfolios which allocate the investor's capital to several
    assets in order to maximize the return of the investment for a certain level of
    risk. The variance is often estimated by linear models or is given a priori by
    an expert. In contrast, the CDEN approximates σ(yt+n|xt) as a function of the
    current state of the financial market and of the forecast of the neural network.
  id: totrans-4660
  prefs: []
  type: TYPE_NORMAL
  zh: 以计算高效投资组合，将投资者的资本分配到多个资产中，以最大化某一风险水平下的投资回报。方差通常通过线性模型进行估计，或者由专家提前给出。相比之下，CDEN将σ(yt+n|xt)近似为金融市场当前状态和神经网络预测的函数。
- en: Covariances can also be estimated with CDEN using a multivariate Gaussian in
    eq. 17.22. Implementation details can be found in [17, 19]. If one approximates
    the conditional density p(y|φ(x)) with several Gaussians, the estimated expected
    mean, variance and covariance may be computed using typical moment generating
    transformations [23, 19].
  id: totrans-4661
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差也可以使用CDEN通过方程17.22中的多元高斯进行估计。实现细节可以在[17, 19]中找到。如果将条件密度p(y|φ(x))用多个高斯进行近似，则可以使用典型的矩生成变换[23,
    19]计算估计的期望均值、方差和协方差。
- en: 17.6 Data Meets Structure 17.6.1 The Observer-Observation Dilemma
  id: totrans-4662
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.6 数据与结构 17.6.1 观察者-观察困境
- en: Human beings believe that they are able to solve a psychological version of
    the Observer-Observation Dilemma. On the one hand, they use their observations
    to constitute an understanding of the laws of the world, on the other hand, they
    use this understanding to evaluate the correctness of the incoming pieces of information.
    Of course, as everybody knows, human beings are not free from making mistakes
    in this psychological dilemma. We encounter a similar situation when we try to
    build a mathematical model using data. Learning relationships from the data is
    only one part of the model building process. Overrating this part often leads
    to the phenomenon of overfitting in many applications (especially in economic
    forecasting). In practice, evaluation of the data is often done by external knowledge,
    i. e. by optimizing the model under constraints of smoothness and regularization
    [16]. If we assume that our model summarizes the best knowledge of the system
    to be identified, why shouldn't we use the model itself to evaluate the correctness
    of the data? One approach to do this is called Clearning [33]. In this paper,
    we present a unified approach of the interaction between the data and a neural
    network (see also [38]). It includes a new symmetric view on the optimization
    algorithms, here learning and cleaning, and their control by parameter and data
    noise.
  id: totrans-4663
  prefs: []
  type: TYPE_NORMAL
  zh: 人类相信他们能够解决观察者-观察对象悖论的心理版本。一方面，他们利用观察形成对世界法则的理解，另一方面，他们利用这种理解来评估传入信息的正确性。当然，正如众所周知，人类在这个心理悖论中并非没有犯错。当我们试图使用数据建立数学模型时，也会遇到类似的情况。从数据中学习关系仅是模型构建过程的一部分。过分重视这一部分常常导致在许多应用中出现过拟合现象（尤其是在经济预测中）。在实践中，数据的评估通常通过外部知识进行，即通过在平滑性和正则化的约束下优化模型[16]。如果我们假设我们的模型总结了要识别系统的最佳知识，为什么我们不利用模型本身来评估数据的正确性呢？一种实现这一点的方法称为
    Clearning [33]。在本文中，我们提出了数据与神经网络之间交互的统一方法（另见 [38]）。它包括对优化算法的新对称视角，这里学习和清理，并通过参数和数据噪声进行控制。
- en: 17.6.2 Learning Reviewed
  id: totrans-4664
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.6.2 学习回顾
- en: We are especially interested in using the output of a neural network y(*x, w*),
    given the input pattern, x, and the weight vector, w, as a forecast of financial
    time series. In the context of neural networks learning normally means the minimization
    of an error function E by changing the weight vector w in order to achieve good
    generalization performance. Again, we assume that the error function can be written
    as a sum of individual terms over all T training patterns, E = 1T
  id: totrans-4665
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尤其感兴趣的是在给定输入模式 x 和权重向量 w 的情况下，使用神经网络的输出 y(*x, w*) 作为金融时间序列的预测。在神经网络的背景下，学习通常意味着通过改变权重向量
    w 来最小化误差函数 E，以达到良好的泛化性能。再次假设误差函数可以写成对所有 T 个训练模式的各个项的总和，E = 1T
- en: 'Tt=1 Et. The often used sum-of-square error can be derived from the maximum-likelihood
    principle and a Gaussian noise model:'
  id: totrans-4666
  prefs: []
  type: TYPE_NORMAL
  zh: Tt=1 Et。常用的平方和误差可以从最大似然原则和高斯噪声模型推导而来：
- en: $$E_{t}=\frac{1}{2}\left(y(x,w)-y_{t}^{d}\right)^{2},\tag{17.30}$$
  id: totrans-4667
  prefs: []
  type: TYPE_NORMAL
  zh: $$E_{t}=\frac{1}{2}\left(y(x,w)-y_{t}^{d}\right)^{2},\tag{17.30}$$
- en: 'with ydt as the given target pattern. If the error function is a nonlinear
    function of the parameters, learning has to be done iteratively by a search through
    the weight space, changing the weights from step τ to τ + 1 according to:'
  id: totrans-4668
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ydt 是给定的目标模式。如果误差函数是参数的非线性函数，学习必须通过在权重空间中搜索来迭代进行，根据以下公式将权重从步骤 τ 变更到 τ + 1：
- en: $$w^{(\tau+1)}=w^{(\tau)}+\Delta w^{(\tau)}.$$
  id: totrans-4669
  prefs: []
  type: TYPE_NORMAL
  zh: $$w^{(\tau+1)}=w^{(\tau)}+\Delta w^{(\tau)}.$$
- en: $$(17.31)$$
  id: totrans-4670
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.31)$$
- en: w(τ+1) = w(τ) + Δw(τ). (17.31)
  id: totrans-4671
  prefs: []
  type: TYPE_NORMAL
  zh: w(τ+1) = w(τ) + Δw(τ)。 (17.31)
- en: There are several algorithms for choosing the weight increment Δw(τ), the easiest
    being *gradient descent*. After each presentation of an input pattern, the gradient
    gt := ∇Et|w of the error function with respect to the weights is computed. In
    the batch version of gradient descent the increments are based on all training
    patterns
  id: totrans-4672
  prefs: []
  type: TYPE_NORMAL
  zh: 选择权重增量 Δw(τ) 有几种算法，其中最简单的是 *梯度下降*。在每次输入模式呈现后，误差函数相对于权重的梯度 gt := ∇Et|w 被计算出来。在梯度下降的批处理版本中，增量基于所有训练模式
- en: $$\Delta w^{(\tau)}=-\eta g=-\eta\frac{1}{T}\sum_{t=1}^{T}g_{t},$$
  id: totrans-4673
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w^{(\tau)}=-\eta g=-\eta\frac{1}{T}\sum_{t=1}^{T}g_{t},$$
- en: $$(17.32)$$
  id: totrans-4674
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.32)$$
- en: $$(17.33)$$
  id: totrans-4675
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.33)$$
- en: 'whereas the pattern-by-pattern version changes the weights after each presentation
    of a pattern xt (often randomly chosen from the training set):'
  id: totrans-4676
  prefs: []
  type: TYPE_NORMAL
  zh: 而逐个模式版本在每次呈现模式 xt 后改变权重（通常是从训练集中随机选择）：
- en: $$\Delta w^{(\tau)}=-\eta g_{t}.$$
  id: totrans-4677
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w^{(\tau)}=-\eta g_{t}.$$
- en: Δw(τ) = −ηgt. (17.33)
  id: totrans-4678
  prefs: []
  type: TYPE_NORMAL
  zh: Δw(τ) = −ηgt. (17.33)
- en: The learning rate η is typically held constant or follows an annealing procedure
    during training to assure convergence.
  id: totrans-4679
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率η通常保持恒定或在训练期间遵循退火过程以确保收敛。
- en: 'Our experiments have shown that small batches are most useful, especially in
    combination with Vario-Eta, a stochastic approximation of a Quasi-Newton method
    [9]:'
  id: totrans-4680
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验表明，小批次是最有用的，特别是与Vario-Eta结合使用时，Vario-Eta是一种类牛顿方法的随机近似[9]：
- en: $$\Delta w^{(\tau)}=-\frac{\eta}{\sqrt{\frac{1}{T}\sum(g_{t}-g)^{2}}}\cdot\frac{1}{N}\sum_{t=1}^{N}g_{t},$$
  id: totrans-4681
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w^{(\tau)}=-\frac{\eta}{\sqrt{\frac{1}{T}\sum(g_{t}-g)^{2}}}\cdot\frac{1}{N}\sum_{t=1}^{N}g_{t},$$
- en: T
  id: totrans-4682
  prefs: []
  type: TYPE_NORMAL
  zh: T
- en: $$(17.34)$$
  id: totrans-4683
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.34)$$
- en: gt, (17.34)
  id: totrans-4684
  prefs: []
  type: TYPE_NORMAL
  zh: gt, (17.34)
- en: with N ≤ 20.
  id: totrans-4685
  prefs: []
  type: TYPE_NORMAL
  zh: 其中N ≤ 20。
- en: Let us assume, that the error function of a specific problem is characterized
    by a minimum in a narrow valley whose boundaries are parallel to the axes of the
    weight space. For the two dimensional case, such a situation is shown in fig.
    17.15.
  id: totrans-4686
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设，特定问题的误差函数在一个窄谷中的最小值特征，其边界与权重空间的轴平行。二维情况下，如图17.15所示。
- en: '![384_image_0.png](384_image_0.png)'
  id: totrans-4687
  prefs: []
  type: TYPE_IMG
  zh: '![384_image_0.png](384_image_0.png)'
- en: Fig. 17.15. Vario-Eta, a stochastic approximation of a Quasi-Newton method
  id: totrans-4688
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.15。Vario-Eta，类牛顿方法的随机近似
- en: A gradient approach would follow a "zigzagging" track and would approximate
    the minimum very slowly. With Vario-Eta, the zigzagging along w2 is damped and
    the drift along w1 is accelerated. This behavior is similar to the weight trajectories
    classical Newton methods show in such a situation. The actual implementation uses
    stochastic approximations to compute the standard deviation.
  id: totrans-4689
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度方法将遵循“之字形”的轨迹，且会非常缓慢地逼近最小值。使用Vario-Eta，沿w2的之字形运动被减弱，沿w1的漂移被加速。这种行为类似于经典牛顿方法在这种情况下所显示的权重轨迹。实际实施使用随机近似来计算标准差。
- en: The use of the standard deviation instead of the variance in Vario-Eta means
    an additional advantage in training large networks. Passing a long sequence of
    layers in a neural network, the error signals contain less and less information.
  id: totrans-4690
  prefs: []
  type: TYPE_NORMAL
  zh: 在Vario-Eta中使用标准差而不是方差意味着在训练大型网络时的额外优势。在神经网络中经过一系列层时，误差信号包含的信息越来越少。
- en: The normalization in eq. 17.34 rescales the learning information for every weight.
    If one designed Vario-Eta as close as possible to second order methods, it would
    be appropriate to use the variance instead the standard deviation in the denominator,
    but then we would also lose the scaling property. In section 17.6.3, we will provide
    a further advantage of using the standard deviation.
  id: totrans-4691
  prefs: []
  type: TYPE_NORMAL
  zh: 等式17.34中的归一化为每个权重重新缩放学习信息。如果将Vario-Eta设计得尽可能接近二阶方法，则在分母中使用方差而不是标准差是合适的，但那样我们也会失去缩放属性。在17.6.3节中，我们将提供使用标准差的进一步优势。
- en: 'Learning pattern-by-pattern or with small batches can be viewed as a stochastic
    search process because we can write the weight increments as:'
  id: totrans-4692
  prefs: []
  type: TYPE_NORMAL
  zh: 按模式或使用小批次进行学习可以被视为一种随机搜索过程，因为我们可以将权重增量写为：
- en: $$\Delta w^{(\tau)}=-\eta\left[g+\left(\frac{1}{N}\sum_{t=1}^{N}g_{t}-g\right)\right].\tag{17.35}$$
  id: totrans-4693
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w^{(\tau)}=-\eta\left[g+\left(\frac{1}{N}\sum_{t=1}^{N}g_{t}-g\right)\right].\tag{17.35}$$
- en: These increments consist of the terms g with a drift to a local minimum and
    of noise terms ( 1N
  id: totrans-4694
  prefs: []
  type: TYPE_NORMAL
  zh: 这些增量由朝向局部最小值的g项和噪声项(1N)组成。
- en: Nt=1 gt − g) disturbing this drift.
  id: totrans-4695
  prefs: []
  type: TYPE_NORMAL
  zh: Nt=1 gt − g) 扰动这个漂移。
- en: 17.6.3 Parameter Noise as an Implicit Penalty Function Consider the Taylor expansion
    of E(w) around some point w in the weight space
  id: totrans-4696
  prefs: []
  type: TYPE_NORMAL
  zh: 17.6.3 参数噪声作为隐式惩罚函数考虑在权重空间中某点w周围的E(w)的泰勒展开。
- en: $$E(w+\Delta w)=E(w)+\sum_{i}\frac{\partial E}{\partial w_{i}}\Delta w_{i}+\frac{1}{2}\sum_{i,j}\frac{\partial^{2}E}{\partial
    w_{i}\partial w_{j}}\Delta w_{i}\Delta w_{j}+\ldots.\tag{17.36}$$
  id: totrans-4697
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(w+\Delta w)=E(w)+\sum_{i}\frac{\partial E}{\partial w_{i}}\Delta w_{i}+\frac{1}{2}\sum_{i,j}\frac{\partial^{2}E}{\partial
    w_{i}\partial w_{j}}\Delta w_{i}\Delta w_{j}+\ldots.\tag{17.36}$$
- en: Assume a given sequence of T disturbance vectors Δwt, whose elements are uncorrelated
    over t with zero mean and variance (row-)vector var(Δwi). The expected value E(w)
    can then be approximated by
  id: totrans-4698
  prefs: []
  type: TYPE_NORMAL
  zh: 假设给定一个T个扰动向量Δwt的序列，其元素在t上不相关，均值为零，方差为（行）向量var(Δwi)。那么，期望值E(w)可以近似为
- en: $$\langle E(w)\rangle\approx{\frac{1}{T}}\sum_{t}E(w+\Delta w_{t})=E(w)+{\frac{1}{2}}\sum_{i}\operatorname{var}(\Delta
    w_{i}){\frac{\partial^{2}E}{\partial w_{i}^{2}}}$$
  id: totrans-4699
  prefs: []
  type: TYPE_NORMAL
  zh: $$\langle E(w)\rangle\approx{\frac{1}{T}}\sum_{t}E(w+\Delta w_{t})=E(w)+{\frac{1}{2}}\sum_{i}\operatorname{var}(\Delta
    w_{i}){\frac{\partial^{2}E}{\partial w_{i}^{2}}}$$
- en: $$(17.37)$$
  id: totrans-4700
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.37)$$
- en: assuming that the first and second derivatives of E are stable if we are close
    to a local minimum. In eq. 17.37, noise on the weights acts implicitly as a penalty
    term to the error function given by the second derivatives ∂2E
  id: totrans-4701
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在接近局部最小值时，E的一阶和二阶导数是稳定的。在公式17.37中，权重上的噪声隐含地作为误差函数的惩罚项，由二阶导数∂2E给出。
- en: ∂w2i
  id: totrans-4702
  prefs: []
  type: TYPE_NORMAL
  zh: ∂w2i
- en: . The noise variances var(Δwi) operate as penalty parameters. As a consequence,
    flat minima solutions which may be important for achieving good generalization
    performance are favored [13].
  id: totrans-4703
  prefs: []
  type: TYPE_NORMAL
  zh: . 噪声方差var(Δwi)作为惩罚参数操作。因此，平坦的最小值解可能对实现良好的泛化性能至关重要。[13]
- en: Learning pattern-by-pattern introduces automatically such noise in the training
    procedure i.e., Δwt = −η · gt. Close to convergence, we can assume that gt is
    i.i.d. with zero mean and variance vector var(gi) so that the expected value can
    be approximated by
  id: totrans-4704
  prefs: []
  type: TYPE_NORMAL
  zh: 逐样本学习自动引入了训练过程中的这种噪声，即Δwt = −η · gt。接近收敛时，我们可以假设gt是独立同分布的，均值为零，方差向量var(gi)，以便期望值可以近似为
- en: $$\langle E(w)\rangle\approx E(w)+\frac{\eta^{2}}{2}\sum_{i}\mathrm{var}(g_{i})\frac{\partial^{2}E}{\partial
    w_{i}^{2}}\;.$$
  id: totrans-4705
  prefs: []
  type: TYPE_NORMAL
  zh: $$\langle E(w)\rangle\approx E(w)+\frac{\eta^{2}}{2}\sum_{i}\mathrm{var}(g_{i})\frac{\partial^{2}E}{\partial
    w_{i}^{2}}\;.$$
- en: $$(17.38)$$
  id: totrans-4706
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.38)$$
- en: . (17.38)
  id: totrans-4707
  prefs: []
  type: TYPE_NORMAL
  zh: . (17.38)
- en: This type of learning introduces a local penalty parameter var(gi), characterizing
    the stability of the weights w = [wi]i=1*,...,k*. In a local minimum the sum of
    gradients for the weight wi is git = 0 whereas the variance var(gi) may be large.
    In this case the solution is very sensitive against resampling of the data and
    therefore unstable. To improve generalization the curvature of the error function
    around such weights with high variance should be strongly penalized.
  id: totrans-4708
  prefs: []
  type: TYPE_NORMAL
  zh: 这种学习类型引入了一个局部惩罚参数var(gi)，特征化权重w = [wi]i=1*,...,k*的稳定性。在局部最小值，权重wi的梯度总和为git =
    0，而方差var(gi)可能很大。在这种情况下，解对数据的重采样非常敏感，因此不稳定。为了提高泛化能力，应该对具有高方差的权重周围的误差函数的曲率进行强惩罚。
- en: This is automatically done by pattern-by-pattern learning.
  id: totrans-4709
  prefs: []
  type: TYPE_NORMAL
  zh: 这由逐样本学习自动完成。
- en: The noise effects due to Vario-Eta learning Δwt(i) = −√
  id: totrans-4710
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Vario-Eta学习引起的噪声Δwt(i) = −√
- en: η σ2i
  id: totrans-4711
  prefs: []
  type: TYPE_NORMAL
  zh: η σ2i
- en: · gti leads to an expected value
  id: totrans-4712
  prefs: []
  type: TYPE_NORMAL
  zh: · gti导致一个期望值
- en: $$\langle E(w)\rangle\approx E(w)+\frac{\eta^{2}}{2}\sum_{i}\frac{\partial^{2}E}{\partial
    w_{i}^{2}}\;.$$
  id: totrans-4713
  prefs: []
  type: TYPE_NORMAL
  zh: $$\langle E(w)\rangle\approx E(w)+\frac{\eta^{2}}{2}\sum_{i}\frac{\partial^{2}E}{\partial
    w_{i}^{2}}\;.$$
- en: $$(17.39)$$
  id: totrans-4714
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.39)$$
- en: . (17.39)
  id: totrans-4715
  prefs: []
  type: TYPE_NORMAL
  zh: . (17.39)
- en: By canceling the term var(gi) in eq. 17.38, Vario-Eta achieves a simplified
    uniform penalty parameter, which depends only on the learning rate η. Whereas
    pattern-by-pattern learning is a slow algorithm with a locally adjusted penalty
    control, Vario-Eta is fast only at the cost of a simplified uniform penalty term.
  id: totrans-4716
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在公式17.38中消去项var(gi)，Vario-Eta实现了一个简化的均匀惩罚参数，仅依赖于学习率η。虽然逐样本学习是一个慢算法，具有局部调整的惩罚控制，但Vario-Eta在简化的均匀惩罚项的代价下却速度较快。
- en: '| local   | learning rate (Vario-Eta)          | →   | global   | penalty   |'
  id: totrans-4717
  prefs: []
  type: TYPE_TB
  zh: '| 局部   | 学习率（Vario-Eta）          | →   | 全局   | 惩罚   |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-4718
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| global  | learning rate (pattern by pattern) | →   | local    | penalty   |'
  id: totrans-4719
  prefs: []
  type: TYPE_TB
  zh: '| 全局  | 学习率（逐样本） | →   | 局部    | 惩罚   |'
- en: Following these thoughts, we will now show that typical Newton methods should
    not be used in stochastic learning. Let us assume that we are close to a local
    minimum and that the Hessian H of the error function is not significantly changing
    anymore. On this supposition, the implied noise would be of the form Δwt = ηH−1gt.
    Because of the stable Hessian, the mean of the noise is zero so that the expected
    error function becomes
  id: totrans-4720
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些思路，我们将展示典型的牛顿方法不应在随机学习中使用。假设我们接近局部最小值，并且误差函数的海森矩阵H不再显著变化。在这个假设下，隐含的噪声将是Δwt
    = ηH−1gt。由于海森矩阵的稳定性，噪声的均值为零，因此期望的误差函数变为
- en: $$\langle E(w)\rangle\approx E(w)+\frac{\eta^{2}}{2}\sum_{i}\mbox{var}(g_{i})\left(\frac{\partial^{2}E}{\partial
    w_{i}^{2}}\right)^{-1}.\tag{17.40}$$
  id: totrans-4721
  prefs: []
  type: TYPE_NORMAL
  zh: $$\langle E(w)\rangle\approx E(w)+\frac{\eta^{2}}{2}\sum_{i}\mbox{var}(g_{i})\left(\frac{\partial^{2}E}{\partial
    w_{i}^{2}}\right)^{-1}.\tag{17.40}$$
- en: In this case, we have again a local control of the penalty parameter through
    the variance of the gradients var(gi), like in the pattern by pattern learning.
    But now, we are penalizing weights at points in the weight space where the inverse
    of the curvature is large. This means, we penalize flat minima solutions, which
    counters our goal of searching for stable solutions.
  id: totrans-4722
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们再次通过梯度的方差 var(gi) 来局部控制惩罚参数，就像模式逐个学习一样。但现在，我们在权重空间中，惩罚曲率的逆值较大的权重点。这意味着我们惩罚平坦的极小值解，这与我们寻找稳定解的目标相悖。
- en: Typically Newton methods are used as cumulative learning methods so that the
    previous arguments do not apply. Therefore, we conclude, that second order methods
    should not be used in stochastic search algorithms. To support global
  id: totrans-4723
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，牛顿方法被用作累积学习方法，因此之前的论点不适用。因此，我们得出结论，二阶方法不应在随机搜索算法中使用。以支持全局
- en: '| learning           | structure   | speed   |'
  id: totrans-4724
  prefs: []
  type: TYPE_TB
  zh: '| 学习           | 结构   | 速度   |'
- en: '| --- | --- | --- |'
  id: totrans-4725
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| pattern by pattern | +           | −       |'
  id: totrans-4726
  prefs: []
  type: TYPE_TB
  zh: '| 模式逐个 | +           | −       |'
- en: '| VarioEta           | −           | +       |'
  id: totrans-4727
  prefs: []
  type: TYPE_TB
  zh: '| VarioEta           | −           | +       |'
- en: Table 17.1. Structure-Speed-Dilemma
  id: totrans-4728
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17.1 结构-速度-困境
- en: learning and to exploit the bias to flat minima solution of such algorithms,
    we only use pattern-by-pattern learning or Vario-Eta in the following.
  id: totrans-4729
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用这些算法对平坦极小值解的偏置，我们在接下来只使用逐个模式学习或 Vario-Eta。
- en: 'We summarize this section by giving some advice on how to achieve flat minima
    solutions (see also table 17.1):'
  id: totrans-4730
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过提供一些如何实现平坦极小值解的建议来总结本节（另见表 17.1）：
- en: '- Train the network to a minimal training error solution with Vario-Eta, which
    is a stochastic approximation of a Newton method and therefore fast.'
  id: totrans-4731
  prefs: []
  type: TYPE_NORMAL
  zh: '- 使用 Vario-Eta 将网络训练到最小训练误差解，Vario-Eta 是牛顿方法的随机近似，因此速度较快。'
- en: '- Add a final phase of pattern-by-pattern learning with a uniform learning
    rate to fine tune the local curvature structure by the local penalty parameters'
  id: totrans-4732
  prefs: []
  type: TYPE_NORMAL
  zh: '- 添加一个最终的逐个模式学习阶段，使用均匀的学习率，通过局部惩罚参数来微调局部曲率结构。'
- en: (eq. 17.38). For networks with many layers, this step should be omitted because
    the gradients will vanish due to the long signal flows. Only Vario-Eta with its
    scaling capability can solve such optimization problems appropriately.
  id: totrans-4733
  prefs: []
  type: TYPE_NORMAL
  zh: (eq. 17.38)。对于具有多个层的网络，此步骤应被省略，因为由于信号流长，梯度会消失。只有 Vario-Eta 及其缩放能力能够恰当地解决此类优化问题。
- en: '- Use a learning rate η as high as possible to keep the penalty effective.
    The training error may vary a bit, but the inclusion of the implicit penalty is
    more important.'
  id: totrans-4734
  prefs: []
  type: TYPE_NORMAL
  zh: '- 使用尽可能高的学习率 η 来保持惩罚的有效性。训练误差可能会有所变化，但隐含惩罚的包含更为重要。'
- en: We want to point out that the decision of which learning algorithm to use not
    only influences the speed and global behavior of the learning, but also, for fixed
    learning rates, leads to different structural solutions. This structural consequence
    has also to be taken into account if one analyzes and compares stochastic learning
    algorithms.
  id: totrans-4735
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想指出，选择使用哪种学习算法不仅影响学习的速度和全局行为，而且对于固定学习率会导致不同的结构解。在分析和比较随机学习算法时，也必须考虑这一结构后果。
- en: 17.6.4 Cleaning Reviewed
  id: totrans-4736
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.6.4 清理回顾
- en: When training neural networks, one typically assumes that the input data is
    noise-free and one forces the network to fit the data exactly. Even the control
    procedures to minimize overfitting effects (i.e., pruning) consider the inputs
    as exact values. However, this assumption is often violated, especially in the
    field of financial analysis, and we are taught by the phenomenon of overfitting
    not to follow the data exactly. Clearning, as a combination of cleaning and learning,
    has been introduced in [33]. In the following, we focus on the cleaning aspects.
    The motivation was to minimize overfitting effects by considering the input data
    as being corrupted by noise whose distribution has to be learned also.
  id: totrans-4737
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，通常假设输入数据是无噪声的，并且强制网络精确拟合数据。即使是为了最小化过拟合影响的控制程序（即剪枝）也将输入视为精确值。然而，这一假设往往被违反，尤其是在金融分析领域，我们通过过拟合现象学到不能完全依赖数据。清洗，作为清理和学习的结合，已在[33]中引入。接下来，我们将重点放在清理方面。其动机是通过考虑输入数据被噪声污染来最小化过拟合影响，而噪声的分布也需要学习。
- en: '![388_image_0.png](388_image_0.png)'
  id: totrans-4738
  prefs: []
  type: TYPE_IMG
  zh: '![388_image_0.png](388_image_0.png)'
- en: $$(17.41)$$
  id: totrans-4739
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.41)$$
- en: Fig. 17.16. If the slope of the modeled function is large, then a small shift
    in the input data decreases the output error dramatically
  id: totrans-4740
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.16。如果模型函数的斜率很大，则输入数据的小变化会显著减少输出误差。
- en: The cleaning error function for the pattern t is given by the sum of two terms
    assuming same variance levels for input and output
  id: totrans-4741
  prefs: []
  type: TYPE_NORMAL
  zh: 模式 t 的清理误差函数由两项的和给出，假设输入和输出具有相同的方差水平。
- en: $$E_{t}^{y,x}={\frac{1}{2}}\left[\left(y_{t}-y_{t}^{d}\right)^{2}+\left(x_{t}-x_{t}^{d}\right)^{2}\right]=E_{t}^{y}+E_{t}^{x}$$
  id: totrans-4742
  prefs: []
  type: TYPE_NORMAL
  zh: $$E_{t}^{y,x}={\frac{1}{2}}\left[\left(y_{t}-y_{t}^{d}\right)^{2}+\left(x_{t}-x_{t}^{d}\right)^{2}\right]=E_{t}^{y}+E_{t}^{x}$$
- en: 2)= Eyt + Ext (17.41)
  id: totrans-4743
  prefs: []
  type: TYPE_NORMAL
  zh: 2)= Eyt + Ext (17.41)
- en: with xdt , ydt as the observed data point. In the pattern-by-pattern learning,
    the network output y(xt, w) determines the weight adaptation as usual,
  id: totrans-4744
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 xdt , ydt 作为观察数据点。在逐模式学习中，网络输出 y(xt, w) 确定权重的适应性，和往常一样，
- en: $$w^{(\tau+1)}=w^{(\tau)}-\eta{\frac{\partial E^{y}}{\partial w}}\ .$$
  id: totrans-4745
  prefs: []
  type: TYPE_NORMAL
  zh: $$w^{(\tau+1)}=w^{(\tau)}-\eta{\frac{\partial E^{y}}{\partial w}}\ .$$
- en: ∂w . (17.42)
  id: totrans-4746
  prefs: []
  type: TYPE_NORMAL
  zh: ∂w . (17.42)
- en: We also must memorize correction vectors Δxt for all input data of the training
    set in order to present the cleaned input xt to the network,
  id: totrans-4747
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须记住训练集所有输入数据的修正向量 Δxt，以便将清理后的输入 xt 提供给网络，
- en: $$(17.42)$$
  id: totrans-4748
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.42)$$
- en: $$x_{t}=x_{t}^{d}+\Delta x_{t}\ .$$
  id: totrans-4749
  prefs: []
  type: TYPE_NORMAL
  zh: $$x_{t}=x_{t}^{d}+\Delta x_{t}\ .$$
- en: $$(17.43)$$
  id: totrans-4750
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.43)$$
- en: $$(17.44)$$
  id: totrans-4751
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.44)$$
- en: xt = xdt + Δxt . (17.43)
  id: totrans-4752
  prefs: []
  type: TYPE_NORMAL
  zh: xt = xdt + Δxt . (17.43)
- en: The update rule for the corrections, initialized with Δx(0)
  id: totrans-4753
  prefs: []
  type: TYPE_NORMAL
  zh: 修正的更新规则，以 Δx(0) 初始化
- en: t = 0 can be derived from typical adaptation sequences x(τ+1)
  id: totrans-4754
  prefs: []
  type: TYPE_NORMAL
  zh: t = 0 可以从典型的适应序列 x(τ+1) 推导出来。
- en: t = x(τ)
  id: totrans-4755
  prefs: []
  type: TYPE_NORMAL
  zh: t = x(τ)
- en: t − η ∂Ey,x
  id: totrans-4756
  prefs: []
  type: TYPE_NORMAL
  zh: t − η ∂Ey,x
- en: ∂x leading to
  id: totrans-4757
  prefs: []
  type: TYPE_NORMAL
  zh: ∂x 导致
- en: $$\Delta x_{t}^{(\tau+1)}=(1-\eta)\Delta x_{t}^{(\tau)}-\eta(y_{t}-y_{t}^{d})\frac{\partial
    y}{\partial x}\ .$$
  id: totrans-4758
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta x_{t}^{(\tau+1)}=(1-\eta)\Delta x_{t}^{(\tau)}-\eta(y_{t}-y_{t}^{d})\frac{\partial
    y}{\partial x}\ .$$
- en: ∂x . (17.44)
  id: totrans-4759
  prefs: []
  type: TYPE_NORMAL
  zh: ∂x . (17.44)
- en: This is a nonlinear version of the error-in-variables concept in statistics
    [27] (see also fig. 17.16 for the two dimensional case).
  id: totrans-4760
  prefs: []
  type: TYPE_NORMAL
  zh: 这是统计学中误差变量概念的非线性版本 [27]（有关二维情况，请参见图 17.16）。
- en: All of the necessary quantities, i. e. (yt − ydt ) ∂y(x,w)
  id: totrans-4761
  prefs: []
  type: TYPE_NORMAL
  zh: 所有必要的量，即 (yt − ydt ) ∂y(x,w)
- en: ∂x are computed by typical back-propagation algorithms anyway. We found that
    the algorithms work well if the same learning rate η is used for both the weight
    and cleaning updates. For regression, cleaning forces the acceptance of a small
    error in x, which can in turn decrease the error in y dramatically, especially
    in the case of outliers. Successful applications of cleaning are reported in [33]
    and [30].
  id: totrans-4762
  prefs: []
  type: TYPE_NORMAL
  zh: ∂x 无论如何都是通过典型的反向传播算法计算的。我们发现，如果对权重和清理更新都使用相同的学习率 η，算法的效果会很好。对于回归，清理强制接受 x 中的小误差，这反过来可以显著减少
    y 的误差，尤其是在存在离群值的情况下。成功的清理应用在 [33] 和 [30] 中有报告。
- en: Although the network may learn an optimal model for the cleaned input data,
    there is no easy way to work with cleaned data on the test set because for this
    data we do not know the output target difference for computing eq. 17.44. As a
    consequence, the model is evaluated on a test set with a different noise characteristic
    compared to the training set. We will later propose a combination of learning
    with noise and cleaning to work around this serious disadvantage.
  id: totrans-4763
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然网络可能会为清理后的输入数据学习一个最佳模型，但在测试集上处理清理数据并不容易，因为对于这些数据，我们不知道输出目标差异以计算方程 17.44。因此，模型在具有不同噪声特征的测试集上进行评估，而与训练集不同。我们稍后将提出结合噪声学习和清理的方法，以解决这一严重缺陷。
- en: 17.6.5 Data Noise Reviewed
  id: totrans-4764
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.6.5 数据噪声回顾
- en: Artificial noise on the input data is often used during training because it
    creates an infinite number of training examples and expands the data to empty
    parts of the input space. As a result, the tendency of learning by heart may be
    limited because smoother regression functions are produced.
  id: totrans-4765
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，输入数据上常常使用人工噪声，因为它创建了无限数量的训练示例，并将数据扩展到输入空间的空白部分。因此，由于产生了更平滑的回归函数，死记硬背的学习倾向可能会受到限制。
- en: Now, we consider again the Taylor expansion, this time applied to E(x) around
    some point x in the input space. The expected value E(x) is approximated by
  id: totrans-4766
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们再次考虑泰勒展开，这次应用于输入空间中某个点 x 周围的 E(x)。期望值 E(x) 近似为
- en: $$\langle E(x)\rangle\approx{\frac{1}{T}}\sum_{t}E(x+\Delta x_{t})=E(x)+{\frac{1}{2}}\sum_{j}\operatorname{var}(\Delta
    x_{j}){\frac{\partial^{2}E}{\partial x_{j}^{2}}}$$
  id: totrans-4767
  prefs: []
  type: TYPE_NORMAL
  zh: $$\langle E(x)\rangle\approx{\frac{1}{T}}\sum_{t}E(x+\Delta x_{t})=E(x)+{\frac{1}{2}}\sum_{j}\operatorname{var}(\Delta
    x_{j}){\frac{\partial^{2}E}{\partial x_{j}^{2}}}$$
- en: $$(17.45)$$
  id: totrans-4768
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.45)$$
- en: where ∂2E
  id: totrans-4769
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ∂2E
- en: ∂x2j are the diagonal elements of the Hessian Hxx of the error function with
    respect to the inputs x. Again, in eq. 17.45, noise on the inputs acts implicitly
    as a penalty term to the error function with the noise variances var(Δxj )
  id: totrans-4770
  prefs: []
  type: TYPE_NORMAL
  zh: ∂x2j 是误差函数相对于输入 x 的 Hessian Hxx 的对角元素。同样，在公式 17.45 中，输入上的噪声隐含地作为误差函数的惩罚项与噪声方差
    var(Δxj ) 一起作用。
- en: operating as penalty parameters (compare eq. 17.37). Noise on the input improves
    generalization behavior by favoring smooth models [3].
  id: totrans-4771
  prefs: []
  type: TYPE_NORMAL
  zh: 作为惩罚参数（比较公式 17.37）。输入上的噪声通过偏向光滑模型改善泛化行为 [3]。
- en: 'The noise levels can be set to a constant value, e. g. given by a priori knowledge,
    or adaptive as described now. We will concentrate on a uniform or normal noise
    distribution. Then, the adaptive noise level ξj is estimated for each input j
    individually. Suppressing pattern indices, we define the average residual errors
    ξj and ξ2j as:'
  id: totrans-4772
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声水平可以设置为一个常数值，例如，由先验知识给出，或者如现在所描述的那样自适应。我们将集中于均匀或正态噪声分布。然后，自适应噪声水平 ξj 将针对每个输入
    j 单独估计。抑制模式索引后，我们定义平均残差误差 ξj 和 ξ2j 如下：
- en: $$\mathrm{uniform~residual~error}\colon\xi_{j}={\frac{1}{T}}\sum_{t}\left|{\frac{\partial
    E^{y}}{\partial x_{j}}}\right|\,,$$
  id: totrans-4773
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{均匀~残差~误差}\colon\xi_{j}={\frac{1}{T}}\sum_{t}\left|{\frac{\partial
    E^{y}}{\partial x_{j}}}\right|\,,$$
- en: $$(17.46)$$
  id: totrans-4774
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.46)$$
- en: ', (17.46)'
  id: totrans-4775
  prefs: []
  type: TYPE_NORMAL
  zh: ', (17.46)'
- en: $$\mathrm{Gaussian~residual~error}\colon\xi_{j}^{2}={\frac{1}{T}}\sum_{t}\left({\frac{\partial
    E^{y}}{\partial x_{j}}}\right)^{2}\,.$$
  id: totrans-4776
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{高斯~残差~误差}\colon\xi_{j}^{2}={\frac{1}{T}}\sum_{t}\left({\frac{\partial
    E^{y}}{\partial x_{j}}}\right)^{2}\,.$$
- en: $$(17.47)$$
  id: totrans-4777
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.47)$$
- en: 2. (17.47)
  id: totrans-4778
  prefs: []
  type: TYPE_NORMAL
  zh: 2. (17.47)
- en: Actual implementations use stochastic approximation, e. g. for the uniform residual
    error
  id: totrans-4779
  prefs: []
  type: TYPE_NORMAL
  zh: 实际实现使用随机逼近，例如，用于均匀残差误差
- en: $$\xi_{j}^{(\tau+1)}=(1-\frac{1}{T})\xi_{j}^{(\tau)}+\frac{1}{T}\left|\frac{\partial
    E^{y}}{\partial x_{j}}\right|\,.$$
  id: totrans-4780
  prefs: []
  type: TYPE_NORMAL
  zh: $$\xi_{j}^{(\tau+1)}=(1-\frac{1}{T})\xi_{j}^{(\tau)}+\frac{1}{T}\left|\frac{\partial
    E^{y}}{\partial x_{j}}\right|\,.$$
- en: $$(17.48)$$
  id: totrans-4781
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.48)$$
- en: . (17.48)
  id: totrans-4782
  prefs: []
  type: TYPE_NORMAL
  zh: . (17.48)
- en: 'The different residual error levels can be interpreted as follows (table 17.2):'
  id: totrans-4783
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的残差误差水平可以如下解释（表 17.2）：
- en: A small level ξj may indicate an unimportant input j or a perfect fit of the
  id: totrans-4784
  prefs: []
  type: TYPE_NORMAL
  zh: 较小的 ξj 可能表明输入 j 不重要或与
- en: network concerning this input j. In both cases, a small noise level is appropriate.
  id: totrans-4785
  prefs: []
  type: TYPE_NORMAL
  zh: 网络对该输入 j 的影响。在这两种情况下，较小的噪声水平都是合适的。
- en: On the other hand, a high value of ξj for an input j indicates an important
    but
  id: totrans-4786
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，输入 j 的高值 ξj 表示一个重要但
- en: imperfectly fitted input. In this case high noise levels are advisable. High
    values of
  id: totrans-4787
  prefs: []
  type: TYPE_NORMAL
  zh: 不完全适配的输入。在这种情况下，建议使用高噪声水平。高值的
- en: ξj lead to a stiffer regression model and may therefore increase the generalization
  id: totrans-4788
  prefs: []
  type: TYPE_NORMAL
  zh: ξj 会导致更僵硬的回归模型，因此可能会提高泛化能力。
- en: performance of the network. Therefore, we use ξj or ξ2j as parameter to control
  id: totrans-4789
  prefs: []
  type: TYPE_NORMAL
  zh: 网络性能的完美拟合。因此，我们使用 ξj 或 ξ2j 作为控制
- en: the level of noise for input j.
  id: totrans-4790
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 j 的噪声水平。
- en: Table 17.2. The interpretation of different levels of the residual errors ξ
  id: totrans-4791
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17.2. 残差误差 ξ 不同水平的解释
- en: '| observation   | of                              | advice         | for   |
    the   |'
  id: totrans-4792
  prefs: []
  type: TYPE_TB
  zh: '| 观察         | 的                              | 建议         | 对   | 该   |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-4793
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| the           | residual                        | noise control  |       |       |'
  id: totrans-4794
  prefs: []
  type: TYPE_TB
  zh: '| 预测的      | 残差                        | 噪声控制  |       |       |'
- en: '| error         | interpretation perfect fit      |                |       |       |'
  id: totrans-4795
  prefs: []
  type: TYPE_TB
  zh: '| 误差         | 解释 完美拟合      |                |       |       |'
- en: '| ξ small       | or                              | use low noise  |       |       |'
  id: totrans-4796
  prefs: []
  type: TYPE_TB
  zh: '| ξ 小       | 或                              | 使用低噪声  |       |       |'
- en: '|               | unimportant input imperfect fit |                |       |       |'
  id: totrans-4797
  prefs: []
  type: TYPE_TB
  zh: '|               | 不重要输入 不完美拟合 |                |       |       |'
- en: '| ξ large       | but                             | use high noise |       |       |'
  id: totrans-4798
  prefs: []
  type: TYPE_TB
  zh: '| ξ 大       | 但                             | 使用高噪声 |       |       |'
- en: '|               | important input                 |                |       |       |'
  id: totrans-4799
  prefs: []
  type: TYPE_TB
  zh: '|               | 重要输入                 |                |       |       |'
- en: 17.6.6 Cleaning With Noise
  id: totrans-4800
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.6.6 使用噪声清理
- en: 'Typically, training with noisy inputs involves taking a data point and adding
    a random variable drawn from a fixed or adaptive distribution. This new data point
    xt is used as an input to the network. If we assume, that the data is corrupted
    by outliers and other influences, it is preferable to add the noise term to the
    cleaned input. For the case of Gaussian noise the resulting new input is:'
  id: totrans-4801
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，带噪声输入的训练涉及获取一个数据点并添加一个从固定或自适应分布中抽取的随机变量。这个新的数据点 xt 用作网络的输入。如果我们假设数据被离群值和其他影响所污染，最好将噪声项添加到清理后的输入中。对于高斯噪声的情况，结果新输入为：
- en: $$x_{t}=x_{t}^{d}+\Delta x_{t}+\xi\phi,$$
  id: totrans-4802
  prefs: []
  type: TYPE_NORMAL
  zh: $$x_{t}=x_{t}^{d}+\Delta x_{t}+\xi\phi,$$
- en: $$(17.49)$$
  id: totrans-4803
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.49)$$
- en: xt = xdt + Δxt + ξφ, (17.49)
  id: totrans-4804
  prefs: []
  type: TYPE_NORMAL
  zh: xt = xdt + Δxt + ξφ, (17.49)
- en: with φ drawn from the normal distribution. The cleaning of the data leads to
    a corrected mean of the data and therefore to a more symmetric noise distribution,
    which also covers the observed data xt.
  id: totrans-4805
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 φ 取自正态分布。数据的清洗导致数据的均值被修正，从而使噪声分布更加对称，同时也覆盖了观察到的数据 xt。
- en: 'We propose a variant which allows more complicated and problem dependent noise
    distributions:'
  id: totrans-4806
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种变体，允许更复杂且依赖于问题的噪声分布：
- en: xt = xdt + Δxt − Δxk, (17.50)
  id: totrans-4807
  prefs: []
  type: TYPE_NORMAL
  zh: xt = xdt + Δxt − Δxk, (17.50)
- en: where k is a random number drawn from the indices of the memorized correction
    vectors [Δxt]t=1*,...,T* . By this, we exploit the distribution properties of
    the correction vectors in order to generate a possibly asymmetric and/or dependent
    noise distribution, which still covers xt = xdt if k = t.
  id: totrans-4808
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 k 是从记忆的修正向量 [Δxt]t=1*,...,T* 的索引中抽取的随机数。通过这一方式，我们利用修正向量的分布特性生成可能不对称和/或依赖的噪声分布，这样当
    k = t 时仍然覆盖 xt = xdt。
- en: One might wonder why we want to disturb the cleaned input xdt +Δxt with an additional
    noisy term Δxk. The reason for this is that we want to benefit from representing
    the whole input distribution to the network instead of only using one particular
    realization. This approach supplies a solution to the cleaning problem when switching
    from the training set to the test set as described in section 17.6.4.
  id: totrans-4809
  prefs: []
  type: TYPE_NORMAL
  zh: 人们可能会想，为什么我们要用额外的噪声项 Δxk 来干扰清洗后的输入 xdt + Δxt。原因在于我们希望将整个输入分布呈现给网络，而不仅仅使用某一特定的实现。这种方法为在训练集与测试集之间切换时的清洗问题提供了解决方案，如第
    17.6.4 节所述。
- en: '17.6.7 A Unifying Approach: The Separation Of Structure And Noise'
  id: totrans-4810
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.6.7 统一方法：结构与噪声的分离
- en: In the previous sections we explained how the data can be separated into a cleaned
    part and an unexplainable noisy part. Analogously, the neural network is described
    as a time invariant structure (otherwise no forecasting is possible) and a noisy
    part.
  id: totrans-4811
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们解释了数据如何被分离为清洗部分和无法解释的噪声部分。类似地，神经网络被描述为一种时间不变的结构（否则无法进行预测）和一个噪声部分。
- en: data → cleaned data + time invariant data noise neural network → time invariant
    parameters + parameter noise We propose using cleaning and adaptive noise to separate
    the data and using learning and stochastic search to separate the structure of
    the neural network.
  id: totrans-4812
  prefs: []
  type: TYPE_NORMAL
  zh: 数据 → 清洗数据 + 时间不变数据噪声 神经网络 → 时间不变参数 + 参数噪声 我们建议使用清洗和自适应噪声来分离数据，并使用学习和随机搜索来分离神经网络的结构。
- en: data ← cleaning (neural network) + adaptive noise (neural network)
  id: totrans-4813
  prefs: []
  type: TYPE_NORMAL
  zh: 数据 ← 清洗（神经网络） + 自适应噪声（神经网络）
- en: neural network ← learning (data) + stochastic search (data)
  id: totrans-4814
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络 ← 学习（数据） + 随机搜索（数据）
- en: The algorithms analyzing the data depend directly on the network whereas the
    methods searching for structure are directly related to the data. It should be
    clear that the model building process should combine both aspects in an alternating
    or simultaneous manner. We normally use learning and cleaning simultaneously.
    The interaction of the data analysis and network structure algorithms is a direct
    embodiment of the the concept of the Observer-Observation Dilemma.
  id: totrans-4815
  prefs: []
  type: TYPE_NORMAL
  zh: 分析数据的算法直接依赖于网络，而寻找结构的方法则直接与数据相关。应该明确的是，模型构建过程应以交替或同时的方式结合这两个方面。我们通常同时使用学习和清洗。数据分析与网络结构算法的相互作用直接体现了观察者-观察对象困境的概念。
- en: 'The aim of the unified approach can be described, exemplary assuming here a
    Gaussian noise model, as the minimization of the error due to both, the structure
    and the data:'
  id: totrans-4816
  prefs: []
  type: TYPE_NORMAL
  zh: 统一方法的目标可以描述为，在这里假设一个高斯噪声模型，即最小化由于结构和数据引起的误差：
- en: $$\frac{1}{2T}\sum_{t=1}^{T}\left[\left(y_{t}-y_{t}^{d}\right)^{2}+\left(x_{t}-x_{t}^{d}\right)^{2}\right]\rightarrow\min_{x_{t},w}\tag{17.51}$$
  id: totrans-4817
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{1}{2T}\sum_{t=1}^{T}\left[\left(y_{t}-y_{t}^{d}\right)^{2}+\left(x_{t}-x_{t}^{d}\right)^{2}\right]\rightarrow\min_{x_{t},w}\tag{17.51}$$
- en: Combining the algorithms and approximating the cumulative gradient g by g˜ using
    an exponential smoothing over patterns, we obtain
  id: totrans-4818
  prefs: []
  type: TYPE_NORMAL
  zh: 结合算法并使用指数平滑法近似累计梯度 g 为 g˜，我们得到
- en: $$\mathrm{data}$$
  id: totrans-4819
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{data}$$
- en: $$\begin{array}{r l}{{\frac{d a t a}{d x_{t}^{(\tau+1)}}=(1-\eta)\Delta x_{t}^{(\tau)}-\eta(y_{t}-y_{t}^{d}){\frac{\partial
    y}{\partial x}}}}\\ {{x_{t}=x_{t}^{d}+\underbrace{\Delta x_{t}^{(\tau)}}_{\mathrm{cleaning}}-\underbrace{\Delta
    x_{k}^{(\tau)}}_{\mathrm{noise}}}}\end{array}$$
  id: totrans-4820
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{r l}{{\frac{d a t a}{d x_{t}^{(\tau+1)}}=(1-\eta)\Delta x_{t}^{(\tau)}-\eta(y_{t}-y_{t}^{d}){\frac{\partial
    y}{\partial x}}}}\\ {{x_{t}=x_{t}^{d}+\underbrace{\Delta x_{t}^{(\tau)}}_{\mathrm{cleaning}}-\underbrace{\Delta
    x_{k}^{(\tau)}}_{\mathrm{noise}}}}\end{array}$$
- en: $$(17.52)$$
  id: totrans-4821
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.52)$$
- en: structure
  id: totrans-4822
  prefs: []
  type: TYPE_NORMAL
  zh: 结构
- en: $$\begin{array}{r l}{{}}&{{}{\tilde{g}^{(\tau+1)}=(1-\alpha)\tilde{g}^{(\tau)}+\alpha(y_{t}-y_{t}^{d})\frac{\partial
    y}{\partial w}}}\\ {{}}&{{}}\\ {{w^{(\tau+1)}=w^{(\tau)}-\underbrace{\eta\tilde{g}^{(\tau)}}_{\mathrm{learning}}-\underbrace{\eta(g_{t}-\tilde{g}^{(\tau)})}_{\mathrm{noise}}}}\end{array}$$
  id: totrans-4823
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{r l}{{}}&{{}{\tilde{g}^{(\tau+1)}=(1-\alpha)\tilde{g}^{(\tau)}+\alpha(y_{t}-y_{t}^{d})\frac{\partial
    y}{\partial w}}}\\ {{}}&{{}}\\ {{w^{(\tau+1)}=w^{(\tau)}-\underbrace{\eta\tilde{g}^{(\tau)}}_{\mathrm{learning}}-\underbrace{\eta(g_{t}-\tilde{g}^{(\tau)})}_{\mathrm{noise}}}}\end{array}$$
- en: The cleaning of the data by the network computes an individual correction term
    for each training pattern. The adaptive noise procedure according to eq. 17.50
    generates a potentially asymmetric and dependent noise distribution which also
    covers the observed data. The implied curvature penalty, whose strength depends
    on the individual liability of the input variables, can improve the generalization
    performance of the neural network.
  id: totrans-4824
  prefs: []
  type: TYPE_NORMAL
  zh: 网络对数据的清理为每个训练模式计算了一个单独的修正项。根据公式17.50的自适应噪声过程生成了一个潜在的不对称和相关的噪声分布，同时也涵盖了观察到的数据。隐含的曲率惩罚，其强度依赖于输入变量的个体负担，可以提高神经网络的泛化性能。
- en: The learning of the structure involves a search for time invariant parameters
    characterized by 1T
  id: totrans-4825
  prefs: []
  type: TYPE_NORMAL
  zh: 结构的学习涉及对特征为1T的时间不变参数的搜索。
- en: gt = 0. The parameter noise supports this exploration as a stochastic search
    to find better "global" minima. Additionally, the generalization performance may
    be further improved by the implied curvature penalty depending on the local liability
    of the parameters. Note that, although the description of the weight updates collapses
    to the simple form of eq. 17.33, we preferred the formula above to emphasize the
    analogy between the mechanism which handles the data and the structure.
  id: totrans-4826
  prefs: []
  type: TYPE_NORMAL
  zh: gt = 0。参数噪声支持这种探索，作为一种随机搜索，以寻找更好的“全局”极小值。此外，隐含的曲率惩罚可能会进一步改善泛化性能，具体取决于参数的局部负担。请注意，尽管权重更新的描述简化为公式17.33的简单形式，我们更倾向于使用上述公式，以强调处理数据和结构之间机制的类比。
- en: In searching for an optimal combination of data and parameters, we have needed
    to model in both. This is not an indication of our failure to build a perfect
    model but rather it is an important element for controlling the interaction of
    data and structure.
  id: totrans-4827
  prefs: []
  type: TYPE_NORMAL
  zh: 在寻找数据和参数的最佳组合时，我们需要对两者进行建模。这并不是我们未能构建完美模型的表现，而是控制数据与结构相互作用的重要元素。
- en: 17.7 Architectural Optimization
  id: totrans-4828
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.7 架构优化
- en: The initial network can only be a guess on the appropriate final architecture.
    One way to handle this inadequacy is to use a growing network. As a possibility
    in the opposite direction, pruning can be applied to shrink the network. From
    our experience the advantage of fast learning of growing networks is more than
    counterbalanced by the better learning properties of large architectures. At least
    in the first learning steps, large networks are not trapped as quickly in local
    minima. When applying the pruning methods according the *late stopping* concept,
    which trains the weights of the network until the error function converges (see
    section 17.8.1), the learning procedure had a sufficient number of adaptation
    steps to adjust the broad network structure. Another advantage of the pruning
    technique is the greater flexibility in generating sparse and possibly irregular
    network architectures.
  id: totrans-4829
  prefs: []
  type: TYPE_NORMAL
  zh: 初始网络只能是对合适最终架构的猜测。应对这种不足的一种方法是使用生长网络。相反的可能性是，可以应用剪枝来缩小网络。从我们的经验来看，生长网络的快速学习优势被大型架构的更好学习特性所抵消。至少在最初的学习步骤中，大型网络不会那么快被困在局部极小值中。根据*晚停止*概念应用剪枝方法时，该方法训练网络的权重，直到误差函数收敛（见第17.8.1节），学习过程有足够的适应步骤来调整广泛的网络结构。剪枝技术的另一个优势是更大灵活性，以生成稀疏和可能不规则的网络架构。
- en: 17.7.1 Node-Pruning
  id: totrans-4830
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.7.1 节点剪枝
- en: We evaluate the importance of input- or hidden nodes by using as a test value
  id: totrans-4831
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用测试值来评估输入节点或隐藏节点的重要性。
- en: $${\rm test}_{i}=E(x_{1},\ldots,x_{i}=\mu(x_{i}),\ldots,x_{n})-E(x_{1},\ldots,x_{n})\,,\tag{17.53}$$
  id: totrans-4832
  prefs: []
  type: TYPE_NORMAL
  zh: $${\rm test}_{i}=E(x_{1},\ldots,x_{i}=\mu(x_{i}),\ldots,x_{n})-E(x_{1},\ldots,x_{n})\,,\tag{17.53}$$
- en: with μ(xi) describing the mean value of the i-th input in the time series for
    the training set. This test value is a measure of the increase of the error if
    we omit one of the input series.
  id: totrans-4833
  prefs: []
  type: TYPE_NORMAL
  zh: 以μ(xi)描述训练集中第i个输入在时间序列中的平均值。这个测试值是一个度量，如果我们省略其中一个输入序列，误差的增加情况。
- en: The creation of the bottleneck structure in the net internal preprocessing
  id: totrans-4834
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络内部预处理中的瓶颈结构的创建
- en: (section 17.2) is best performed by applying this test for the hidden neurons
    in the bottleneck on the training set. Thereby the disturbance of the learning
    is reduced to the minimum. In one of the steps of the training procedure (section
    17.8) we will show that the pruning of the input neurons should be done on the
    validation set to improve the time stability of the forecasting.
  id: totrans-4835
  prefs: []
  type: TYPE_NORMAL
  zh: （第17.2节）最好通过对训练集中的瓶颈隐藏神经元应用此测试来执行。这样，学习的干扰减少到最低。在训练过程的一个步骤中（第17.8节），我们将展示输入神经元的剪枝应该在验证集上进行，以改善预测的时间稳定性。
- en: In addition to the deletion of nodes the ranking of the inputs can give us a
    deeper insight into the task (which inputs are the most relevant and so on).
  id: totrans-4836
  prefs: []
  type: TYPE_NORMAL
  zh: 除了删除节点，输入的排名可以使我们深入了解任务（哪些输入是最相关的，等等）。
- en: 17.7.2 Weight-Pruning
  id: totrans-4837
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.7.2 权重剪枝
- en: The neural network topology represents only a hypothesis of the true underlying
    class of functions. Due to possible mis-specification, we may have defects of
    the parameter noise distribution. Pruning algorithms not only limit the memory
    of the network, but they also appear to be useful for correcting the noise distribution
    in different ways.
  id: totrans-4838
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络拓扑仅代表真实基础函数类别的假设。由于可能的错误规格，我们可能会出现参数噪声分布的缺陷。剪枝算法不仅限制了网络的记忆，还在不同方式上对纠正噪声分布显得有用。
- en: Stochastic-Pruning
  id: totrans-4839
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机剪枝
- en: Stochastic-Pruning [8] is basically a t-test on the weights w,
  id: totrans-4840
  prefs: []
  type: TYPE_NORMAL
  zh: 随机剪枝 [8] 基本上是对权重w的t检验，
- en: $$\mathrm{test}_{w}={\frac{|w+g|}{\sqrt{{\frac{1}{T}}\sum(g_{t}-g)^{2}}}},$$
  id: totrans-4841
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{test}_{w}={\frac{|w+g|}{\sqrt{{\frac{1}{T}}\sum(g_{t}-g)^{2}}}},$$
- en: $$(17.54)$$
  id: totrans-4842
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.54)$$
- en: ', (17.54)'
  id: totrans-4843
  prefs: []
  type: TYPE_NORMAL
  zh: ', (17.54)'
- en: with g = 1T
  id: totrans-4844
  prefs: []
  type: TYPE_NORMAL
  zh: g = 1T
- en: t gt. Weights with low testw values constitute candidates for pruning. From
    the viewpoint of our approach, this pruning algorithm is equivalent to the cancellation
    of weights with low *liability* as measured by the size of the weight divided
    by the standard deviation of its fluctuations. By this, we get a stabilization
    of the learning against resampling of the training data.
  id: totrans-4845
  prefs: []
  type: TYPE_NORMAL
  zh: t gt。测试值低的权重构成剪枝的候选者。从我们的方法角度来看，这种剪枝算法等同于根据权重的大小除以其波动的标准差来取消低*责任*的权重。通过这样，我们可以实现对训练数据重采样的学习稳定化。
- en: This easy to compute method worked well in combination with the early stopping
    concept in contrast to insufficient results with the late stopping concept.
  id: totrans-4846
  prefs: []
  type: TYPE_NORMAL
  zh: 这种易于计算的方法在早期停止概念的结合下效果良好，而在晚期停止概念下则效果不足。
- en: For early stopping Stochastic-Pruning acts like a t-test, whereas for late stopping
    the gradients of larger weights do not fluctuate enough to give useful test values.
    Thus, the pruning procedure includes an artificial bias to nonlinear models. Furthermore,
    Stochastic-Pruning is also able to revive already pruned weights.
  id: totrans-4847
  prefs: []
  type: TYPE_NORMAL
  zh: 对于早期停止，随机剪枝类似于t检验，而对于晚期停止，较大权重的梯度波动不足，无法提供有用的测试值。因此，剪枝过程包括对非线性模型的人工偏差。此外，随机剪枝也能够恢复已经剪枝的权重。
- en: Early-Brain-Damage
  id: totrans-4848
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 早期脑损伤
- en: A further weight pruning method is EBD, *Early-Brain-Damage* [31], which is
    based on the often cited OBD pruning method [15]. In contrast to OBD, EBD allows
    its application before the training has reached a local minimum.
  id: totrans-4849
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种权重剪枝方法是EBD，*早期脑损伤* [31]，它基于经常引用的OBD剪枝方法 [15]。与OBD相比，EBD允许在训练达到局部最小值之前应用。
- en: For every weight, EBD computes as a test value an approximation of the difference
    between the error function for w = 0 versus the value of the error function for
    the best situation this weight can have
  id: totrans-4850
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个权重，EBD计算作为测试值的误差函数在w = 0与该权重可能拥有的最佳情况的误差函数之间的差值的近似值
- en: $$\mathrm{test}_{w}=E(0)-E(w_{\mathrm{min}})=-g w+\frac{1}{2}w^{\prime}H w+\frac{1}{2}g
    H^{-1}g^{\prime}~.$$
  id: totrans-4851
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{test}_{w}=E(0)-E(w_{\mathrm{min}})=-g w+\frac{1}{2}w^{\prime}H w+\frac{1}{2}g
    H^{-1}g^{\prime}~.$$
- en: The above approximation is motivated by a Taylor expansion of the error function.
    From
  id: totrans-4852
  prefs: []
  type: TYPE_NORMAL
  zh: 上述近似是由误差函数的泰勒展开所激励的。从
- en: $$E(\bar{w})=E(w)+g(\bar{w}-w)+\frac{1}{2}(\bar{w}-w)^{\prime}H(\bar{w}-w)$$
  id: totrans-4853
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(\bar{w})=E(w)+g(\bar{w}-w)+\frac{1}{2}(\bar{w}-w)^{\prime}H(\bar{w}-w)$$
- en: $${\mathrm{we~get}}$$
  id: totrans-4854
  prefs: []
  type: TYPE_NORMAL
  zh: $${\mathrm{we~get}}$$
- en: $$(17.55)$$
  id: totrans-4855
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.55)$$
- en: $$(17.56)$$
  id: totrans-4856
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.56)$$
- en: $$E(0)=E(w)-g w+{\frac{1}{2}}w^{\prime}H w$$
  id: totrans-4857
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(0)=E(w)-g w+{\frac{1}{2}}w^{\prime}H w$$
- en: $$(17.57)$$
  id: totrans-4858
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.57)$$
- en: and as a solution to the minimum problem E( ˜w) → min we have
  id: totrans-4859
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最小化问题的解 E( ˜w) → min，我们有
- en: $$w_{\rm min}=w-H^{-1}g^{\prime}\ {\rm together\ with}\ \ \ E(w_{\rm min})=E(w)-\frac{1}{2}gH^{-1}g^{\prime}\,.\tag{17.58}$$
  id: totrans-4860
  prefs: []
  type: TYPE_NORMAL
  zh: $$w_{\rm min}=w-H^{-1}g^{\prime}\ {\rm together\ with}\ \ \ E(w_{\rm min})=E(w)-\frac{1}{2}gH^{-1}g^{\prime}\,.\tag{17.58}$$
- en: The difference of these two error values is the proposed EBD test. The Hessian
    H in this approach is computed in the same way as in the original OBD calculus
  id: totrans-4861
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个误差值的差异是提出的 EBD 测试。在这种方法中，Hessian H 的计算方式与原始 OBD 计算相同。
- en: '[15].'
  id: totrans-4862
  prefs: []
  type: TYPE_NORMAL
  zh: '[15].'
- en: One of the advantages of EBD over OBD is the possibility of performing the testing
    while being slightly away from a local minimum. In our training procedure we propose
    using noise even in the final part of learning so that we are only near a local
    minimum. Furthermore, EBD is also able to revive already pruned weights.
  id: totrans-4863
  prefs: []
  type: TYPE_NORMAL
  zh: EBD 相较于 OBD 的一个优势是可以在离局部最小值稍远的情况下进行测试。在我们的训练过程中，我们建议在学习的最后阶段使用噪声，这样我们仅接近局部最小值。此外，EBD
    还能够复活已经剪枝的权重。
- en: Similar to Stochastic Pruning, EBD favors weights with a low rate of fluctuations.
    If a weight is pushed around by a high noise, the implicit curvature
  id: totrans-4864
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于随机剪枝，EBD 更倾向于低波动率的权重。如果一个权重受到高噪声的影响，则隐式曲率
- en: '![395_image_0.png](395_image_0.png)'
  id: totrans-4865
  prefs: []
  type: TYPE_IMG
  zh: '![395_image_0.png](395_image_0.png)'
- en: Fig. 17.17. EBD versus ODB weight pruning
  id: totrans-4866
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.17. EBD 与 ODB 权重剪枝
- en: penalty would favor a flat minimum around this weight leading to its elimination
    by EBD.
  id: totrans-4867
  prefs: []
  type: TYPE_NORMAL
  zh: 罚项将偏向于围绕该权重的平坦最小值，从而导致其被 EBD 消除。
- en: 'Our practical experience shows that EBD pruning allows the creation of extremely
    sparse networks. We had examples where we could prune an initial network with
    3000 weights down to a structure with around 50 weights. The first iterations
    of EBD pruning would typically give no improvement in generalization. This is
    due to the fact that EBD is testing the importance of a weight regarding the error
    function, i. e. the same information which is used by the backpropagation algorithm.
    To say it in another way: EBD cancels out only weights which are not disturbing
    the learning. Only at the end of the training procedure does the network have
    to give up a part of the coded structure which leads to an improvement in generalization.'
  id: totrans-4868
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实践经验表明，EBD 剪枝允许创建极为稀疏的网络。我们有例子显示，可以将初始拥有 3000 个权重的网络剪枝到大约 50 个权重的结构。EBD 剪枝的初始迭代通常不会改善泛化能力。这是因为
    EBD 测试权重在误差函数中的重要性，即与反向传播算法使用的信息相同。换句话说：EBD 仅消除那些不干扰学习的权重。训练过程的最后，网络才需要放弃部分编码结构，从而改善泛化能力。
- en: Inverse-Kurtosis
  id: totrans-4869
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逆峭度
- en: A third pruning method we want to discuss is a method which we call *InverseKurtosis*.
    The motivation follows an analysis of the following examples of possible distributions
    of gradient impulses forcing a weight shown in fig. 17.18.
  id: totrans-4870
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想讨论的第三种剪枝方法是我们称之为 *逆峭度* 的方法。其动机来源于对图 17.18 中强迫一个权重的梯度脉冲的可能分布的分析。
- en: If the network is trained to a local minimum the mean of all gradients by definition
    is equal to zero. Nevertheless the distribution of the gradients may differ.
  id: totrans-4871
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网络训练到局部最小值，则所有梯度的均值根据定义等于零。然而，梯度的分布可能会有所不同。
- en: Now we have to analyze the difference of a peaked or very broad distribution
    versus a normal distribution. It is our understanding that the peaked distribution
    indicates a weight which reacts only to a small number of training patterns.
  id: totrans-4872
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须分析峰态或非常宽广的分布与正态分布之间的差异。我们理解峰态分布表明权重仅对少量训练模式做出反应。
- en: A broader distribution, on the other hand, is a sign that many training patterns
  id: totrans-4873
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，更广泛的分布则表明许多训练模式。
- en: '![396_image_0.png](396_image_0.png)'
  id: totrans-4874
  prefs: []
  type: TYPE_IMG
  zh: '![396_image_0.png](396_image_0.png)'
- en: Fig. 17.18. Possible distributions of the gradients gt, if weights are fixed
  id: totrans-4875
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.18. 如果权重固定，梯度 gt 的可能分布
- en: 'focus on the optimization of this weight. In other words, a weight with a peaked
    distribution has learned by heart special events of the time series but it has
    not modeled a general underlying structure of the data. Therefore, Inverse-Kurtosis
    pruning harmonizes well with our network architecture which can encapsulate local
    structures with its squared layer. Furthermore, a weight with a very broad distribution
    of its gradients is pushed around by random events because it reacts to almost
    every pattern with a similar strength. A straight forward way to distinguish between
    the above distributions is based on the kurtosis to measure the difference to
    a normal distribution:'
  id: totrans-4876
  prefs: []
  type: TYPE_NORMAL
  zh: 重点优化该权重。换句话说，具有尖峰分布的权重已经记住了时间序列中特殊事件，但没有建模数据的一般潜在结构。因此，反峰度剪枝与我们的网络架构很好地协调，该架构可以通过其平方层封装局部结构。此外，梯度分布非常广泛的权重会因随机事件而被推来推去，因为它几乎以相似的强度对每个模式做出反应。区分上述分布的一种简单方法是基于峰度来测量与正态分布的差异：
- en: $${\rm distance}_{w}=\left(\frac{\frac{1}{T}\sum_{t=1}^{T}(g_{t}-g)^{4}}{\left(\frac{1}{T}\sum_{t=1}^{T}(g_{t}-g)^{2}\right)^{2}}-3\right)^{2}.\tag{17.59}$$
  id: totrans-4877
  prefs: []
  type: TYPE_NORMAL
  zh: $${\rm distance}_{w}=\left(\frac{\frac{1}{T}\sum_{t=1}^{T}(g_{t}-g)^{4}}{\left(\frac{1}{T}\sum_{t=1}^{T}(g_{t}-g)^{2}\right)^{2}}-3\right)^{2}.\tag{17.59}$$
- en: To rank importance of the network weights based on this difference to a normal
    distribution we define the test values as
  id: totrans-4878
  prefs: []
  type: TYPE_NORMAL
  zh: 为了根据与正态分布的差异对网络权重的重要性进行排序，我们定义测试值为
- en: $\text{test}_{w}=\frac{1}{\varepsilon+|\text{distance}_{w}|}$, (17.60)
  id: totrans-4879
  prefs: []
  type: TYPE_NORMAL
  zh: $\text{test}_{w}=\frac{1}{\varepsilon+|\text{distance}_{w}|}$, (17.60)
- en: with the small term ε ≈ 0.001 to avoid numerical problems.
  id: totrans-4880
  prefs: []
  type: TYPE_NORMAL
  zh: 使用小项ε ≈ 0.001以避免数值问题。
- en: Similar to the previously mentioned pruning techniques we have now large test
    values for weights we do not want to eliminate. Note that in this test, we have
    neglected the size of the gradients and only taken into account the form of the
    distribution. The weights themselves are not a part of the evaluation, and we
    have not observed that this method has much effect on the distribution of the
    weight sizes found by the learning. In contrast, Stochastic Pruning and EBD have
    a tendency to prune out small weights because they explicitly refer to the size
    of the weights. Basically, Inverse-Kurtosis computes low test values
  id: totrans-4881
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于之前提到的剪枝技术，我们现在对不想消除的权重有了较大的测试值。请注意，在此测试中，我们忽略了梯度的大小，仅考虑了分布的形式。权重本身并不是评估的一部分，我们没有观察到该方法对学习得到的权重大小分布有很大影响。相反，随机剪枝和EBD倾向于剪除小权重，因为它们明确提到了权重的大小。基本上，反峰度计算出低测试值。
- en: (i. e. probable pruning candidates) for weights which are encapsulating only
    local artifacts or random events in the data instead of modeling a general structure.
  id: totrans-4882
  prefs: []
  type: TYPE_NORMAL
  zh: （即可能的剪枝候选）针对仅封装数据中局部伪影或随机事件的权重，而不是建模一般结构。
- en: In our experience we found that this technique can give a strong improvement
    of generalization in the first several iterations of pruning.
  id: totrans-4883
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，我们发现这种技术可以在剪枝的前几次迭代中显著提高泛化能力。
- en: Instability-Pruning
  id: totrans-4884
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不稳定性剪枝
- en: The local consequence of our learning until the minimal training error is that
    the cumulative gradient g for every weight is zero on the training set,
  id: totrans-4885
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习到最小训练误差的局部结果是每个权重的累积梯度g在训练集上为零，
- en: $$g={\frac{1}{T}}\sum_{t\in T}g_{t}=0\,.$$
  id: totrans-4886
  prefs: []
  type: TYPE_NORMAL
  zh: $$g={\frac{1}{T}}\sum_{t\in T}g_{t}=0\,.$$
- en: $$(17.61)$$
  id: totrans-4887
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.61)$$
- en: 'If this condition was valid for the validation set our model would be perfectly
    stable:'
  id: totrans-4888
  prefs: []
  type: TYPE_NORMAL
  zh: 如果该条件在验证集上成立，我们的模型将是完美稳定的：
- en: $$g_{V}=\frac{1}{V}\sum_{t\in V}g_{t}=0\,.$$
  id: totrans-4889
  prefs: []
  type: TYPE_NORMAL
  zh: $$g_{V}=\frac{1}{V}\sum_{t\in V}g_{t}=0\,.$$
- en: $$(17.62)$$
  id: totrans-4890
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.62)$$
- en: $$(17.63)$$
  id: totrans-4891
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.63)$$
- en: 'Since gV typically is different from zero we have to check if the measured
    difference is significant enough to indicate instability. In the language of statistics
    this is a two sample test for the equality of the mean of two distributions. The
    following test value (Welch-test) measures this difference:'
  id: totrans-4892
  prefs: []
  type: TYPE_NORMAL
  zh: 由于gV通常与零不同，我们必须检查测量的差异是否足够显著以指示不稳定性。在统计学的语言中，这是两个分布均值相等的两样本检验。以下测试值（Welch检验）测量此差异：
- en: $$\mathrm{distance}_{w}={\frac{g v-g}{\sqrt{{\frac{\sigma_{V}^{2}}{V}}+{\frac{\sigma_{T}^{2}}{T}}}}},$$
  id: totrans-4893
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{distance}_{w}={\frac{g v-g}{\sqrt{{\frac{\sigma_{V}^{2}}{V}}+{\frac{\sigma_{T}^{2}}{T}}}}},$$
- en: ', (17.63)'
  id: totrans-4894
  prefs: []
  type: TYPE_NORMAL
  zh: ', (17.63)'
- en: which is approximately normally distributed with zero mean and unit variance.
    As a pruning test value we define
  id: totrans-4895
  prefs: []
  type: TYPE_NORMAL
  zh: 该值大致呈正态分布，均值为零，方差为一。作为剪枝测试值，我们定义
- en: $$\mathrm{test}_{w}={\frac{1}{\varepsilon+|\mathrm{distance}_{w}|}}\,.$$
  id: totrans-4896
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{test}_{w}={\frac{1}{\varepsilon+|\mathrm{distance}_{w}|}}\,.$$
- en: $$(17.64)$$
  id: totrans-4897
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.64)$$
- en: . (17.64)
  id: totrans-4898
  prefs: []
  type: TYPE_NORMAL
  zh: . (17.64)
- en: 'This test value can be used to construct a ranking of the weights in the following
    way: Train the network until weights are near a local optimum and compute the
    test values. Then, take out 5% of the most unstable weights as measured by this
    test criterion and redo the learning step. Eliminating more than 5% of the weights
    is not recommandable because Instability-Pruning is not referring to the cost
    function used for learning, and thus may have a large impact on the error.'
  id: totrans-4899
  prefs: []
  type: TYPE_NORMAL
  zh: 该测试值可用于构建权重排名，方法如下：训练网络直到权重接近局部最优，然后计算测试值。接着，按照该测试标准去除5%最不稳定的权重，并重新进行学习步骤。去除超过5%的权重是不推荐的，因为不稳定剪枝并不参考用于学习的成本函数，因此可能对误差产生重大影响。
- en: This test of stability allows the definition of an interesting criterion when
    to stop the model building process. After pruning weights until a given stability
    level, we can check if the model is still approximating well enough. This final
    stopping criterion is possible due to the asymptotic normal distribution of the
    test. For example, if we define weights with |distancew| < 1 as stable, then we
    prune weights with testw < 1.
  id: totrans-4900
  prefs: []
  type: TYPE_NORMAL
  zh: 这种稳定性测试允许定义一个有趣的标准，决定何时停止模型构建过程。在剪除权重直到达到给定稳定性水平后，我们可以检查模型是否仍然足够接近正确。这个最终停止标准得益于测试的渐近正态分布。例如，如果我们定义|distancew|
    < 1的权重为稳定，则我们剪除测试值testw < 1的权重。
- en: 'One may argue that the usage of the validation set in this approach is an implicit
    modeling of the validation set and therefore the validation set allows no error
    estimation on the generalization set. On the other hand, the typical need for
    an validation set is the estimation of a final stopping point in the pruning procedure.
    We have tried an alternative approach: Instead of using the validation set in
    terms of gV and σ2V in our comparison let us take a weighted measurement of the
    training set such that most recent data is given more importance:'
  id: totrans-4901
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会争辩说，这种方法中验证集的使用是一种隐式建模，因此验证集不允许对泛化集进行误差估计。另一方面，验证集的典型需求是估计剪枝过程中的最终停止点。我们尝试了一种替代方法：在比较中，不使用验证集的gV和σ2V，而是对训练集进行加权测量，使得最新数据被赋予更大的重要性：
- en: $$\tilde{g}=\frac{2}{T(T+1)}\sum_{t\in T}t g_{t}\,,$$
  id: totrans-4902
  prefs: []
  type: TYPE_NORMAL
  zh: $$\tilde{g}=\frac{2}{T(T+1)}\sum_{t\in T}t g_{t}\,,$$
- en: $$(17.65)$$
  id: totrans-4903
  prefs: []
  type: TYPE_NORMAL
  zh: $$(17.65)$$
- en: $$\tilde{\sigma}^{2}=\frac{2}{T(T+1)}\sum_{t\in T}t(g_{t}-\tilde{g})^{2}\,.\tag{17.66}$$
  id: totrans-4904
  prefs: []
  type: TYPE_NORMAL
  zh: $$\tilde{\sigma}^{2}=\frac{2}{T(T+1)}\sum_{t\in T}t(g_{t}-\tilde{g})^{2}\,.\tag{17.66}$$
- en: Checking the stability over the training set by substituting gv resp. σ2V with
    eq. 17.65 resp. 17.66 in eq. 17.63 avoids using the validation set. In our first
    experiences we observed that this version works as well as the previous one although
    the evaluation still needs more testing.
  id: totrans-4905
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在方程17.63中用等式17.65和17.66替代gv和σ2V来检查训练集的稳定性，避免了使用验证集。在我们的第一次经验中，我们观察到这种版本的效果与之前的版本一样，尽管评估仍需更多测试。
- en: 'In comparison to EBD, which eliminates weights with only a small effect on
    the error function, Instability-Pruning introduces a new feature in the process
    of model building: stability over time. Additional information about when to choose
    which pruning method will be given in the following section 17.8.4.'
  id: totrans-4906
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅消除对误差函数影响较小的权重的EBD相比，不稳定剪枝在模型构建过程中引入了一个新特征：时间上的稳定性。关于何时选择哪种剪枝方法的更多信息将在后面的第17.8.4节中提供。
- en: 17.8 The Training Procedure
  id: totrans-4907
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.8 训练过程
- en: The authors believe that complex real world problems should be attacked by a
    combination of various methods in order to handle the different types of difficulties
    which may arise during the optimization of neural networks. Thus, the aim of this
    section is to link all the previously described features to a complete and consistent
    training procedure.
  id: totrans-4908
  prefs: []
  type: TYPE_NORMAL
  zh: 作者认为，复杂的现实问题应该通过多种方法的结合来解决，以应对在优化神经网络过程中可能出现的不同类型的困难。因此，本节的目的是将所有之前描述的特征与完整且一致的训练过程联系起来。
- en: '17.8.1 Training Paradigms: Early Vs. Late Stopping'
  id: totrans-4909
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.8.1 训练范式：提前停止与延迟停止
- en: One of the most well known techniques for attacking the overfitting problem
    is the early stopping procedure. In this procedure, we start the learning with
    a network initialized by small weights. In its most simple version one uses a
    validation set to estimate the beginning of overfitting, at which point the network
    learning is stopped. A more sophisticated variant of the procedure is to learn
    up to the start of overfitting and then to prune a part of the network structure,
    by say 10% of the weights. Then, one restarts the smaller network and repeats
    the steps. This sequence, which is schematically shown in in fig. 17.19, has to
    be iterated until a stable structure with no signs of overfitting is achieved.
  id: totrans-4910
  prefs: []
  type: TYPE_NORMAL
  zh: 解决过拟合问题最著名的技术之一是早期停止程序。在这个程序中，我们用小权重初始化的网络开始学习。在最简单的版本中，使用验证集来估计过拟合的开始，此时停止网络学习。该程序的更复杂变体是学习到过拟合开始，然后修剪网络结构的一部分，例如权重的10%。然后，重新启动较小的网络并重复这些步骤。这个序列如图17.19所示，必须迭代，直到达到没有过拟合迹象的稳定结构。
- en: In principle this procedure may work and it will generate a model with good
    generalization performance, but in many cases it will fail to do so, as we have
    observed in our experiments during the last several years. Difficulties with early
    stopping arise because the stopping point turns up after a few learning epochs
    through the training data. The authors have worked on examples where the
  id: totrans-4911
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，这个过程可能有效，并会生成具有良好泛化性能的模型，但在许多情况下，它会失败，正如我们在过去几年的实验中观察到的。早期停止的困难在于，停止点在经过几次学习周期后出现。作者们在一些例子中研究了这一点。
- en: '![399_image_0.png](399_image_0.png)'
  id: totrans-4912
  prefs: []
  type: TYPE_IMG
  zh: '![399_image_0.png](399_image_0.png)'
- en: 'w1 Fig. 17.19. Early stopping: After initialization with small weights the
    network is trained until the error on the validation set starts to increase (search
    path 1 to stopping point 1). Then, some weights are pruned, the remaining weights
    are reinitialized and training starts again until stopping point 2. This procedure
    has to be iterated until a stable structure with no signs of overfitting is achieved.'
  id: totrans-4913
  prefs: []
  type: TYPE_NORMAL
  zh: w1 图17.19。早期停止：在小权重初始化后，网络被训练直到验证集上的误差开始增加（搜索路径1到停止点1）。然后，一些权重被修剪，剩余的权重被重新初始化，训练再次开始直到停止点2。该过程必须迭代，直到达到没有过拟合迹象的稳定结构。
- en: stopping point appeared as early as after the second epoch of learning. In this
    case, the solution is restricted to linear models, since the network has not been
    offered any chance to learn a complex nonlinearity from the data. A decreased
    learning rate does not mean a reduction of this bias, because it only slows down
    the movement away from the linear models. Using initial weights with larger values
    is also problematic for two reasons. The random initial (probably incorrect) specification
    of the network may lead to decreasing error curves due to shrinking weight values,
    but after a while overfitting will probably start again. Another important critique
    of this initialization is the intrinsic dependency of the solution on the starting
    point.
  id: totrans-4914
  prefs: []
  type: TYPE_NORMAL
  zh: 停止点出现在学习的第二个周期之后。在这种情况下，解决方案被限制为线性模型，因为网络没有机会从数据中学习复杂的非线性。降低学习率并不意味着减少这种偏见，因为它只是减缓了远离线性模型的过程。使用较大值的初始权重也存在两个问题。随机的初始（可能不正确）网络规格可能导致误差曲线下降，但过一段时间后，过拟合可能会再次开始。对这种初始化的另一个重要批评是解决方案对起始点的内在依赖性。
- en: The same argument is true for some of the penalty term approaches for regularizing
    the networks. Weight decay, for example, is a regularizer with a bias towards
    linear models. Most of the other penalty functions (like data independent smoothness)
    set additional constraints on the model building process. These restrictions are
    not necessarily related to the task to be solved (see [16] for smoothing regularizers
    for neural networks with sigmoidal activation function in the hidden layer).
  id: totrans-4915
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些正则化网络的惩罚项方法，同样的论点也适用。例如，权重衰减是一个偏向线性模型的正则化器。大多数其他惩罚函数（如与数据无关的平滑性）为模型构建过程设置了额外约束。这些限制并不一定与待解决的任务相关（请参见[16]，了解在隐藏层使用sigmoid激活函数的神经网络的平滑正则化器）。
- en: Following these thoughts, in this section we present a sequence of steps that
    follow a late stopping concept. We start with small weights to lower the influence
    of the random initialization. Then, we learn until the minimal error on the training
    data is achieved. Typically, the network shows clear overfitting at this point
    but we can be sure that the maximum nonlinearity which can be represented by the
    chosen network, has been learned from the data. At this point we must describe
    the way back to a solution with good generalization performance, as indicated
    in fig. 17.20.
  id: totrans-4916
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些思考，在本节中，我们提出一系列遵循延迟停止概念的步骤。我们从小权重开始，以降低随机初始化的影响。然后，我们学习直到在训练数据上达到最小误差。通常，此时网络会明显过拟合，但我们可以确定所选网络可以表示的最大非线性已从数据中学习到。此时，我们必须描述回归到具有良好泛化性能的解决方案的方式，如图17.20所示。
- en: '![400_image_0.png](400_image_0.png)'
  id: totrans-4917
  prefs: []
  type: TYPE_IMG
  zh: '![400_image_0.png](400_image_0.png)'
- en: w1
  id: totrans-4918
  prefs: []
  type: TYPE_NORMAL
  zh: w1
- en: 'Fig. 17.20. Late stopping: After initialization with small weights the network
    is trained until the minimal training error. Subsequent optimization of the network
    structure by pruning increases generalization performance. Then, the network is
    trained again until the minimal training error.'
  id: totrans-4919
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.20. 延迟停止：在以小权重初始化后，网络训练直到最小训练误差。随后通过修剪优化网络结构，提高泛化性能。然后，网络再次训练直到最小训练误差。
- en: Basically the two parts of the training, learning to the minimal training error
    and extracting an appropriate generalizing model, can be understood as a generation
    of a structural hypothesis followed by a falsification of the generated structure.
    These steps will be explained in detail in the next paragraphs.
  id: totrans-4920
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，训练的两个部分，即学习到最小训练误差和提取合适的泛化模型，可以理解为生成一个结构假设，然后对生成的结构进行伪造。这些步骤将在接下来的段落中详细解释。
- en: 17.8.2 Setup Steps
  id: totrans-4921
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.8.2 设置步骤
- en: Before starting the learning process we have to specify the network architecture.
    Let us assume that our aim is the identification of a dynamical system. Then,
    we propose using the network architecture (fig. 17.8) of section 17.3, may be
    with the CDEN extension (fig. 17.14) of section 17.5 in order to additionally
    estimate error bars.
  id: totrans-4922
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始学习过程之前，我们必须指定网络架构。假设我们的目标是识别一个动态系统。那么，我们建议使用第17.3节中的网络架构（图17.8），可能结合第17.5节中的CDEN扩展（图17.14），以额外估计误差条。
- en: A further setup decision is the separation of the data set into a training set,
    a validation set and a generalization set. Possible separations are shown in fig.
    17.21.
  id: totrans-4923
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的设置决策是将数据集分为训练集、验证集和泛化集。可能的分离如图17.21所示。
- en: Considering forecasting models of time series (at least in economic applications),
    the sets should be explicitly separated in the order indicated in fig. 17.21.1.
    Otherwise there is no chance to test the model stability over time. If we randomly
    took out the validation set of the training set (fig. 17.21.2), we would have
    no chance to test this stability because the validation patterns are always embedded
    in training patterns.
  id: totrans-4924
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑时间序列的预测模型（至少在经济应用中），集合应该按图17.21.1中所示的顺序明确分开。否则，就没有机会测试模型随时间的稳定性。如果我们随机抽取训练集中的验证集（图17.21.2），就没有机会测试这种稳定性，因为验证模式总是嵌入在训练模式中。
- en: In the proposed sequence we do not use the most recent data before the test
    set in the learning. As a consequence, one might tend to choose the validation
    patterns from the oldest part while using the most recent data in the training
    set (fig. 17.21.3). This leads to a better basis for the generation of our structural
    hypothesis by learning. Basically, the pruning methods are a falsification of
    the generated structure. Using the separation in fig. 17.21.3 with the validation
    set
  id: totrans-4925
  prefs: []
  type: TYPE_NORMAL
  zh: 在提议的序列中，我们不在学习中使用测试集之前的最新数据。因此，人们可能倾向于从最旧部分选择验证模式，同时在训练集中使用最新数据（图17.21.3）。这为通过学习生成我们的结构假设提供了更好的基础。基本上，修剪方法是对生成结构的伪造。使用图17.21.3中的分离和验证集
- en: '![401_image_0.png](401_image_0.png)'
  id: totrans-4926
  prefs: []
  type: TYPE_IMG
  zh: '![401_image_0.png](401_image_0.png)'
- en: Fig. 17.21. Separation of the data
  id: totrans-4927
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.21. 数据分离
- en: including the oldest data can make this falsification misleading. In a fast
    changing world, model stability over time is an important performance characteristic
    of a good network model. Thus, we prefer the separation in fig. 17.21.1.
  id: totrans-4928
  prefs: []
  type: TYPE_NORMAL
  zh: 包括最旧数据可能会使这种伪造误导。在快速变化的世界中，模型随时间的稳定性是一个良好网络模型的重要性能特征。因此，我们更倾向于图17.21.1中的分离。
- en: The several preprocessed time series which serve as inputs have to be checked
    for correlation because highly correlated inputs only serve to increase the amount
    of numerical instability in our model. To avoid this, we can introduce a bottleneck
    substructure to perform a principle component analysis. For our typical application
    with about 50 inputs, we use net internal preprocessing by the diagonal matrix.
    For models with a very large number of inputs (some hundreds)
  id: totrans-4929
  prefs: []
  type: TYPE_NORMAL
  zh: 作为输入的多个预处理时间序列必须检查相关性，因为高度相关的输入只会增加模型中的数值不稳定性。为避免这一点，我们可以引入瓶颈子结构来执行主成分分析。对于我们的典型应用，约有50个输入，我们使用对角矩阵的净内部预处理。对于具有大量输入（数百个）的模型，
- en: the bottleneck approach may be superior.
  id: totrans-4930
  prefs: []
  type: TYPE_NORMAL
  zh: 瓶颈方法可能是更优的选择。
- en: If using the net internal preprocessing by the diagonal matrix, it is necessary
    to check the pairwise correlation of the inputs. Following the common guideline
    of using lots of chart indicators in financial modeling it is typical that many
    of these indicators have a high correlation. Keeping them in the training set
    will cause the solutions of different optimization runs to give us different answers
    regarding the importance of the input factors.
  id: totrans-4931
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用对角矩阵的净内部预处理，则有必要检查输入的成对相关性。根据金融建模中使用大量图表指标的常见指导原则，这些指标中的许多通常具有高相关性。将它们保留在训练集中会导致不同优化运行的解决方案在输入因子重要性上给出不同的答案。
- en: Since we often start with a number of weights which is large relative to the
    number of training patterns, we also propose using a small weight decay during
    learning. The penalty parameter should be in the range of 10% of the learning
    rate because we are not interested in reducing the number of effective parameters.
    With this small value of the decay parameter, only those weights which simply
    learn nothing are pulled to zero. By this, we still achieve the minimal training
    error, but eliminate all the unnecessary weights which have an unpredictable effect
    on the test set.
  id: totrans-4932
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们通常从相对于训练模式数量较大的权重开始，我们还建议在学习过程中使用小的权重衰减。惩罚参数应在学习率的10%范围内，因为我们不想减少有效参数的数量。使用这个小的衰减参数值，只有那些根本没有学习的权重才会被拉至零。通过这样，我们仍然实现了最小的训练误差，但消除了所有对测试集有不可预测影响的不必要权重。
- en: '17.8.3 Learning: Generation Of Structural Hypothesis'
  id: totrans-4933
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.8.3 学习：生成结构假设
- en: If confronted with a large data set, such as in the task of forecasting daily
    returns, we propose training the weights with the VarioEta adaptation algorithm.
    As a stochastic approximation to a Newton method this is a fast learning technique.
  id: totrans-4934
  prefs: []
  type: TYPE_NORMAL
  zh: 如果面对一个大型数据集，例如预测每日收益的任务，我们建议使用VarioEta适应算法训练权重。作为牛顿方法的随机近似，这是一种快速学习技术。
- en: Obeying the arguments about the implicit penalty of section 17.6 we should let
    the VarioEta training be followed by a simple pattern by pattern learning with
    a constant learning rate to achieve structurally correct learning. As mentioned,
    it is valuable to hold the learning rate as high as possible to benefit from the
    implied penalty term. However, if interested in monthly forecasting models, one
    may use only the pattern-by-pattern learning because learning speed is not relevant
    due to the small data set. On the other hand, the implied curvature penalty is
    more important to generate good models (see section 17.6).
  id: totrans-4935
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循第17.6节关于隐式惩罚的论点，我们应该让VarioEta训练后进行简单的逐模式学习，保持恒定的学习率，以实现结构上正确的学习。如前所述，保持尽可能高的学习率是有价值的，以利用隐含的惩罚项。然而，如果对月度预测模型感兴趣，可以仅使用逐模式学习，因为由于数据集小，学习速度并不相关。另一方面，隐含的曲率惩罚对于生成良好模型更为重要（见第17.6节）。
- en: We propose to start the cleaning and the cleaning noise procedure of section
    17.6 from the beginning. In this way, one can observe the following interesting
    relationship and interaction between the stochastic learning and the cleaning
    noise which improves the learning behavior. Since the initial noise variance is
    set to zero, the noise to the input variables will start with a low level and
    will then increase rapidly during the first learning epochs. After several epochs,
    when the network has captured some of the structure in the data, the noise decays
    in parallel with the residual input error of the network. As a consequence, in
    the beginning of the training, the network can learn only the global structure
    in the data. Later in the training process, more and more detailed features of
    the data are extracted which leads simultaneously to lower cleaning noise levels
  id: totrans-4936
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议从头开始进行第17.6节的清理及清理噪声程序。这样，可以观察到随机学习与改善学习行为的清理噪声之间的有趣关系和相互作用。由于初始噪声方差设为零，输入变量的噪声将以低水平开始，并在前几个学习时期迅速增加。在经过几个时期后，当网络捕获到数据中的一些结构时，噪声与网络的残余输入误差并行衰减。因此，在训练开始时，网络只能学习数据中的全局结构。随着训练过程的推进，越来越多的数据细节被提取，导致清理噪声水平同时降低。
- en: (fig. 17.22). Cleaning4 improves the learning process by sequencing the data
    structure from global to increasingly specialized features. Furthermore, noise
    improves the stochastic search in the weight space and therefore reduces the problem
    of local minima.
  id: totrans-4937
  prefs: []
  type: TYPE_NORMAL
  zh: （图17.22）。清理4通过将数据结构从全局序列化为越来越专业化的特征来改善学习过程。此外，噪声改善了权重空间中的随机搜索，从而减少了局部最小值的问题。
- en: '![402_image_0.png](402_image_0.png)'
  id: totrans-4938
  prefs: []
  type: TYPE_IMG
  zh: '![402_image_0.png](402_image_0.png)'
- en: Fig. 17.22. Due to the large noise level in the beginning of the training phase,
    the network first learns global features. At later training steps, it is able
    to extract more and more details.
  id: totrans-4939
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.22。由于训练阶段开始时的噪声水平较大，网络首先学习全局特征。在后续训练步骤中，它能够提取越来越多的细节。
- en: Depending on the problem, we observed that the implicitly controlled noise level
    can go down close to zero or it can converge to an intermediate level. We finish
    the step of learning to a minimal training error when we observe a stable
  id: totrans-4940
  prefs: []
  type: TYPE_NORMAL
  zh: 根据问题的不同，我们观察到隐式控制的噪声水平可以降到接近零，也可以收敛到中间水平。当我们观察到稳定时，我们结束学习步骤以达到最小训练误差。
- en: 4 Even in an early stopping concept Cleaning may improve the learning.
  id: totrans-4941
  prefs: []
  type: TYPE_NORMAL
  zh: 4 即使在早期停止概念中，清理也可能改善学习。
- en: behavior of the error function on the training and validation set simultaneously
    with the stable behavior of the mean noise level for all inputs on the training
    set. The final model of this step is a structural hypothesis about the dynamical
    system we are analyzing.
  id: totrans-4942
  prefs: []
  type: TYPE_NORMAL
  zh: 错误函数在训练集和验证集上的行为，同时与训练集上所有输入的平均噪声水平的稳定行为一致。本步骤的最终模型是关于我们分析的动态系统的结构假设。
- en: '17.8.4 Pruning: Falsification Of The Generated Structure'
  id: totrans-4943
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.8.4 剪枝：生成结构的伪造
- en: Next, we have to test the stability of our model over time. Especially in economic
    modeling, it is not evident that the structure of the capital markets we are extracting
    from the seventies or eighties are still valid now. We can check this stability
    by testing the network behavior on the validation set which follows the training
    set on the time axis.
  id: totrans-4944
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须测试模型随时间的稳定性。特别是在经济建模中，从七十年代或八十年代提取的资本市场结构在现在是否仍然有效并不明显。我们可以通过在时间轴上跟随训练集的验证集上测试网络行为来检查这种稳定性。
- en: By using input pruning on the validation set (see section 17.7), we are able
    to identify input series that may be of high relevance in the training set but
    may also have no or even a counterproductive effect on the validation data. Note
    that, the pruning of non-relevant time series is not possible before achieving
    the minimal training error because there exists no well defined relationship between
    all the inputs and the target variables before. On the other hand, unimportant
    or inconsistent inputs with respect to the validation set should not be eliminated
    later in order to facilitate the construction of models with high generalization
    performance by subsequent learning.
  id: totrans-4945
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在验证集上进行输入修剪（见第17.7节），我们能够识别在训练集中可能高度相关但在验证数据上可能没有或甚至产生相反效果的输入序列。请注意，在达到最小训练误差之前，无法修剪不相关的时间序列，因为在此之前输入与目标变量之间并不存在明确的关系。另一方面，针对验证集的不重要或不一致的输入不应在后期被消除，以便通过后续学习促进具有高泛化性能的模型构建。
- en: An alternative way to check the stability of the model is to perform weight
    pruning on the validation set using the Instability-Pruning algorithm. If the
    data set is very large, weight pruning has a significant advantage because only
    one pass through the data set is necessary, in comparison to n passes to get the
    ranking of the input pruning with n as the number of inputs. This becomes very
    important in the field of data-mining where we have to deal with hundreds or thousands
    of megabytes of training data.
  id: totrans-4946
  prefs: []
  type: TYPE_NORMAL
  zh: 检查模型稳定性的另一种方法是使用不稳定修剪算法对验证集进行权重修剪。如果数据集非常庞大，权重修剪具有显著优势，因为只需对数据集进行一次遍历，而不是进行n次遍历来获得输入修剪的排名，其中n为输入数量。这在数据挖掘领域尤为重要，因为我们需要处理数百或数千兆字节的训练数据。
- en: Each time after pruning of weights or inputs, the network is trained again to
    achieve a stable behavior of the error curves on the training / validation set
    (fig. 17.24) and of the noise level (fig. 17.23). Here, by stable behavior of
    the error curve we mean that there is no significant downward trend (i. e. improving
    of the model) or upward trend (i. e. restructuring of the model). In the diagram
    shown in fig. 17.25, these stop conditions are marked with (∗).
  id: totrans-4947
  prefs: []
  type: TYPE_NORMAL
  zh: 每次进行权重或输入的修剪后，网络都会重新训练，以实现训练/验证集（图17.24）上误差曲线和噪声水平（图17.23）的稳定表现。这里，我们所说的误差曲线的稳定表现是指没有显著的下降趋势（即模型改进）或上升趋势（即模型重构）。在图17.25中，这些停止条件用（∗）标记。
- en: To eliminate local artifacts in the network structure at this stage, we can
    include some iterations of Inverse-Kurtosis pruning and subsequent retraining.
    The process of iteratively pruning and training ends if a *drastic* decay, i.
    e. 5%,
  id: totrans-4948
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在这一阶段消除网络结构中的局部伪影，我们可以包括一些逆峰度修剪的迭代和随后的再训练。如果*剧烈*下降，即5%这个标准不再可观察，那么迭代修剪和训练的过程结束。
- en: of this criterion is no longer observable. For several iterations of training
    and pruning, this strategy leads to a rapidly decreasing error on the validation
    set.
  id: totrans-4949
  prefs: []
  type: TYPE_NORMAL
  zh: 在几次训练和修剪迭代后，这一策略导致验证集上的误差迅速下降。
- en: Due to the fact that the shrinking number of network parameters leads to increasing
    gradients, pattern by pattern learning with a constant learning rate is most useful
    because the implicit local curvature penalty becomes increasingly important.
  id: totrans-4950
  prefs: []
  type: TYPE_NORMAL
  zh: 由于网络参数数量的减少导致梯度增加，使用恒定学习率的逐模式学习最为有效，因为隐式局部曲率惩罚变得愈加重要。
- en: One may criticize the proposed pruning policy because it is indeed an optimization
    on the validation set. Thus, the error on the validation set does not represent
    a good estimation of the generalization error anymore. We believe that there is
    no way to omit this pruning step since the test of the time stability is important
    for achieving models with high generalization performance.
  id: totrans-4951
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会批评所提议的修剪策略，因为它确实是在验证集上的一种优化。因此，验证集上的误差不再很好地代表泛化误差的估计。我们认为，没有办法省略这一步修剪，因为测试时间稳定性对于获得具有高泛化性能的模型至关重要。
- en: In the subsequent part of the network learning, we use "Occam's Razor" by weight
    pruning based on test values computed on the training set. This may be EBD or
    Instability-Pruning (see section 17.7.2). The performance of both methods based
    on hundreds of experiments is comparable. After pruning about 10% resp. 5% of
    the active weights, we train again until stable error curves and noise levels
    are obtained.
  id: totrans-4952
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络学习的后续部分，我们利用基于训练集上计算的测试值进行权重剪枝的“奥卡姆剃刀”。这可能是EBD或不稳定剪枝（见第17.7.2节）。基于数百次实验，两种方法的性能是可比的。在剪枝约10%或5%的活动权重后，我们再次训练，直到获得稳定的误差曲线和噪声水平。
- en: On can combine these pruning methods by first eliminating weights with Instability-Pruning
    and generating very sparse networks using EBD afterwards. Instability-Pruning
    favors model stability over time whereas EBD allows sparse models which still
    approximate well.
  id: totrans-4953
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过首先消除不稳定剪枝下的权重，然后使用EBD生成非常稀疏的网络来组合这些剪枝方法。不稳定剪枝在时间上更偏向于模型稳定性，而EBD则允许稀疏模型仍能很好地逼近。
- en: If using the eleven layer architecture of fig. 17.8, weight pruning should only
    be applied to the weights connecting the preprocessing or square layer to the
    hidden layers for two reasons. First, these connectors represent the majority
    of the weights in the neural network. Second, it is important to apply the pruning
    techniques only to such weights which have the same distance to the output neurons.
    Thus, the test values of the gradient based pruning algorithms are comparable
    which leads to a reliable ranking of the weights.5
  id: totrans-4954
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用图17.8的十一层架构，权重剪枝应仅应用于连接预处理层或平方层与隐藏层的权重，原因有二。首先，这些连接器代表了神经网络中大多数权重。其次，重要的是仅对与输出神经元距离相同的权重应用剪枝技术。因此，基于梯度的剪枝算法的测试值是可比较的，这导致权重的可靠排名。
- en: '![404_image_0.png](404_image_0.png)'
  id: totrans-4955
  prefs: []
  type: TYPE_IMG
  zh: '![404_image_0.png](404_image_0.png)'
- en: Fig. 17.23. Interaction between pruning and the adaptive noise level
  id: totrans-4956
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.23 剪枝与自适应噪声水平之间的交互
- en: 5 If one decides not to use the eleven layer architecture but a typical neural
    network extended by random targets, one has to consider the following consequences
    for input and weight pruning. The test values have to be computed only with respect
    to the correct targets. By this, weights which are modeling random targets will
    loose their importance. EBD pruning works very well in identifying such flat minima
    weights (section 17.7.2), and thus, is our favorite pruning method for such a
    modeling approach.
  id: totrans-4957
  prefs: []
  type: TYPE_NORMAL
  zh: 如果选择不使用十一层架构，而是扩展随机目标的典型神经网络，则必须考虑输入和权重剪枝的以下后果。测试值必须仅针对正确的目标进行计算。这样，建模随机目标的权重将失去其重要性。EBD剪枝在识别此类平坦极小值权重方面表现良好（第17.7.2节），因此，这是我们喜欢的剪枝方法。
- en: There is an interesting interaction between the pruning and the adaptive noise
    procedure (see fig. 17.23). At each pruning step, we eliminate some parameters
    which have memorized some of the structure in the data. Thereby, the residual
    input errors will increase, which leads to increased noise levels on the inputs.
    Consequently, after each pruning step the learning has to focus on more global
    structures of the data. Owing to the fact that by pruning the network memory gradually
    decreases, the network cannot rely on any particular features of the data but
    rather is forced to concentrate more and more on the general underlying structure.
    Interestingly, this part of the procedure can be viewed as being the opposite
    of learning until the minimal training error. There the network is forced by the
    cleaning noise to learn the global structures first before trying to extract specific
    characteristics from the data.
  id: totrans-4958
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝与自适应噪声过程之间存在有趣的相互作用（见图17.23）。在每个剪枝步骤中，我们会消除一些已经记住数据结构的参数。因此，残余输入误差将增加，从而导致输入上的噪声水平升高。因此，在每次剪枝后，学习必须关注数据的更全球结构。由于剪枝后网络记忆逐渐减少，网络无法依赖数据的特定特征，而是被迫越来越多地集中于一般的潜在结构。有趣的是，这部分过程可以被视为与学习直到最小训练误差的相反。在那里，网络被清洗噪声迫使先学习全球结构，然后再试图提取数据的具体特征。
- en: 17.8.5 Final Stopping Criteria Of The Training
  id: totrans-4959
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.8.5 训练的最终停止标准
- en: The last question we have to answer is the definition of the final stopping
    point.
  id: totrans-4960
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要回答的最后一个问题是最终停止点的定义。
- en: Obviously, after many weight pruning and training iterations, the error will
    increase for the training and the validation set. Thus, the minimal error on the
    validation set should give us an estimate of when to stop the optimization. In
    practice, the error curves during learning are not smooth and monotonic curves
    with only one minimum. The most simple advice is to prune until all weights are
    eliminated while keeping a trace to store the best intermediate solution on the
    validation set. Fig. 17.24 displays the error behavior in the different phases
    of the training procedure assuming the worst situation from the viewpoint of an
    early stopping method due to the immediately increasing error on the validation
    set.
  id: totrans-4961
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在多次权重剪枝和训练迭代后，训练集和验证集的错误会增加。因此，验证集上的最小错误应给我们一个停止优化的估计。在实践中，学习过程中的错误曲线并不是平滑且单调的曲线，只有一个最小值。最简单的建议是剪枝直到所有权重都被消除，同时保持一个痕迹以存储验证集上的最佳中间解。图17.24显示了在训练过程中不同阶段的错误行为，假设从早期停止方法的角度来看，验证集上的错误立即增加是最糟糕的情况。
- en: Instability-Pruning offers an alternative definition of a final stopping point.
    If we substitute EBD by this pruning method for applying Occam's Razor, each
  id: totrans-4962
  prefs: []
  type: TYPE_NORMAL
  zh: 不稳定剪枝提供了最终停止点的另一种定义。如果我们用这种剪枝方法替代EBD以应用奥卡姆剃刀，
- en: '![405_image_0.png](405_image_0.png)'
  id: totrans-4963
  prefs: []
  type: TYPE_IMG
  zh: '![405_image_0.png](405_image_0.png)'
- en: Fig. 17.24. The error behavior during the training. Region (1) describes the
    way to a minimal training error, (2) is the typical behavior during the input
    pruning on the validation set and (3) shows the consequence of Occam's razor in
    form of the pruning.
  id: totrans-4964
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.24. 训练过程中的错误行为。区域（1）描述通往最小训练误差的路径，（2）是验证集上输入剪枝的典型行为，（3）展示了以剪枝形式体现的奥卡姆剃刀的后果。
- en: pruning / retraining iteration will increase the stability of the model because
    Instability-Pruning deletes unstable weights (see section 17.7.2).
  id: totrans-4965
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝/重训练迭代将提高模型的稳定性，因为不稳定剪枝删除不稳定权重（见第17.7.2节）。
- en: 17.8.6 Diagram Of The Training Procedure
  id: totrans-4966
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.8.6 训练过程的示意图
- en: The following diagram in fig. 17.25 shows the training steps combined into a
    unified training procedure.
  id: totrans-4967
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.25中的下图展示了合并成统一训练过程的训练步骤。
- en: '![406_image_0.png](406_image_0.png)'
  id: totrans-4968
  prefs: []
  type: TYPE_IMG
  zh: '![406_image_0.png](406_image_0.png)'
- en: '![406_image_1.png](406_image_1.png)'
  id: totrans-4969
  prefs: []
  type: TYPE_IMG
  zh: '![406_image_1.png](406_image_1.png)'
- en: Activate a trace of the weights on the validation set.
  id: totrans-4970
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证集上激活权重的痕迹。
- en: 8.4 7.1, 7.2 Falsify the time stability of the model by input pruning (one input
    at each time step) or Retraining after each pruning using pattern by pattern learning
    to stop conditions *.
  id: totrans-4971
  prefs: []
  type: TYPE_NORMAL
  zh: 8.4 7.1, 7.2 通过输入剪枝（每个时间步一个输入）或在每次剪枝后使用模式逐一学习进行重训练来伪造模型的时间稳定性*。
- en: Instability Pruning (5% of the weights) as long as the validation set error
    drops at least for 5%.
  id: totrans-4972
  prefs: []
  type: TYPE_NORMAL
  zh: 只要验证集错误至少下降5%，就进行不稳定剪枝（占权重的5%）。
- en: Delete encapsulated artefacts in the network structure by Inverse-Kurtosis pruning.
  id: totrans-4973
  prefs: []
  type: TYPE_NORMAL
  zh: 通过逆峭度剪枝删除网络结构中的封装伪影。
- en: '![406_image_2.png](406_image_2.png)'
  id: totrans-4974
  prefs: []
  type: TYPE_IMG
  zh: '![406_image_2.png](406_image_2.png)'
- en: Retraining after each pruning using pattern by pattern learning to stop conditions
    *.
  id: totrans-4975
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次剪枝后使用模式逐一学习进行重训练，以满足停止条件*。
- en: Eliminate 10% (5%) of the effective weights per iteration.
  id: totrans-4976
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代消除10%（5%）的有效权重。
- en: 'Occam''s Razor: Decrease the memory by weight pruning EBD (or Instability-Pruning)
    on the training set.'
  id: totrans-4977
  prefs: []
  type: TYPE_NORMAL
  zh: 奥卡姆剃刀：通过在训练集上进行权重剪枝EBD（或不稳定剪枝）来减少内存。
- en: 7.2 8.4 8.5 Define the best intermediate solution on the validation set (stored
    in the trace) as the resulting model.
  id: totrans-4978
  prefs: []
  type: TYPE_NORMAL
  zh: 7.2 8.4 8.5 将验证集上的最佳中间解（存储在痕迹中）定义为结果模型。
- en: Fig. 17.25. The diagram of the training procedure. The marks (∗) indicates the
    stopping points (section 17.8.3 and 17.8.4). Box numbers correspond to sections.
  id: totrans-4979
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.25. 训练过程的示意图。标记（∗）表示停止点（第17.8.3节和17.8.4节）。框号对应于各个节。
- en: 17.9 Experiments
  id: totrans-4980
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.9 实验
- en: In a research project sponsored by the European Community we are using the proposed
    approach to estimate the returns of 3 financial markets for each of the G7 countries
    [12]. These estimations will be subsequently used in an asset allocation scheme
    to create a Markowitz-optimal portfolio. In this paper, we only report the results
    of the estimation of the German bond rate, which is one of the more difficult
    tasks due to the reunification of Germany and GDR. Here, we have to predict the
    return of the bond rate 6 months ahead. The inputs consist of 39 variables obtained
    by preprocessing 16 relevant financial time series6. The training set covers the
    time from April 1974 to December 1991, the validation set from January 1992 to
    May 1994. The results are collected using the out-of-sample data which runs from
    June 1994 to May 1996. To demonstrate the behavior of the algorithms, we compare
    our approach with a standard neural network with one hidden layer (20 neurons,
    tanh transfer function) and one linear output (eq. 17.30 as error function). First,
    we trained the neural network until convergence with pattern-by-pattern learning
    using a small batch size of 20 patterns. We refer to this network as a classical
    approach. Then, we trained the 11-layer network
  id: totrans-4981
  prefs: []
  type: TYPE_NORMAL
  zh: 在欧洲共同体赞助的研究项目中，我们使用所提议的方法估计G7各国3个金融市场的回报[12]。这些估计随后将用于资产配置方案，以创建马科维茨最优投资组合。在本文中，我们仅报告德国债券利率的估计结果，这是由于德国和东德的统一而较为困难的任务。在这里，我们必须预测债券利率提前6个月的回报。输入包括通过预处理16个相关金融时间序列获得的39个变量。训练集覆盖的时间为1974年4月至1991年12月，验证集为1992年1月至1994年5月。结果使用从1994年6月到1996年5月的样本外数据收集。为了展示算法的表现，我们将我们的方法与一个具有一个隐藏层（20个神经元，tanh传递函数）和一个线性输出（方程17.30作为误差函数）的标准神经网络进行比较。首先，我们使用小批量20个模式的逐模式学习训练神经网络，直到收敛。我们将这个网络称为经典方法。然后，我们训练11层网络。
- en: (fig. 17.8) using the unified approach as described in section 4.1. Due to small
    data sets we used pattern-by-pattern learning without VarioEta. The data were
    manipulated by the cleaning and noise method of eq. 17.50. We compare the resulting
    predictions of the networks on the basis of three performance measures (see tab.
    17.3). First, the hit rate counts how often the sign of the return of the bond
    has been correctly predicted. As for the other measures, the step from the forecast
    model to a trading system is here kept very simple. If the output is positive,
    we buy shares of the bond, otherwise we sell them. The potential realized is the
    ratio of the return to the maximum possible return over the training (test) set.
    The annualized return is the average yearly profit of the trading systems. Our
    approach turns out to be superior. For example, we almost doubled the annualized
    return from 4.5% to 8.5% on the test set. In fig. 17.26, we compare the accumulated
    return of the two approaches on the test set. The unified approach not only shows
    a higher profitability, but also has by far a less maximal draw down.
  id: totrans-4982
  prefs: []
  type: TYPE_NORMAL
  zh: （图17.8）使用第4.1节中描述的统一方法。由于数据集较小，我们采用无VarioEta的逐模式学习。数据通过方程17.50的清洗和噪声方法进行处理。我们基于三项性能指标（见表17.3）比较网络的预测结果。首先，命中率计算债券收益的符号被正确预测的次数。至于其他指标，预测模型到交易系统的步骤在这里保持非常简单。如果输出为正，我们购买债券的股份，否则我们卖出。实现的潜力是训练（测试）集上收益与最大可能收益的比率。年化回报是交易系统的平均年利润。我们的方法结果优越。例如，我们几乎将测试集的年化回报从4.5%翻倍至8.5%。在图17.26中，我们比较了两种方法在测试集上的累计回报。统一方法不仅显示出更高的盈利能力，而且最大回撤远低于其他方法。
- en: Table 17.3. Comparison of the hit rate, the realized potential and the annualized
    return of the two networks for the training (test) data
  id: totrans-4983
  prefs: []
  type: TYPE_NORMAL
  zh: 表17.3. 两个网络在训练（测试）数据上的命中率、实现潜力和年化回报的比较
- en: '| network            | hit rate   | realized potential   | annualized return   |'
  id: totrans-4984
  prefs: []
  type: TYPE_TB
  zh: '| 网络              | 命中率    | 实现潜力          | 年化回报         |'
- en: '| --- | --- | --- | --- |'
  id: totrans-4985
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| our approach       | 96% (81%)  | 100% (75%)           | 11.22% (8.5%)       |'
  id: totrans-4986
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法       | 96% (81%)  | 100% (75%)        | 11.22% (8.5%)    |'
- en: '| classical approach | 93% (66%)  | 96% (44%)            | 10.08% (4.5%)       |'
  id: totrans-4987
  prefs: []
  type: TYPE_TB
  zh: '| 经典方法        | 93% (66%)  | 96% (44%)         | 10.08% (4.5%)    |'
- en: 6 At the moment, we are not allowed to be more specific on the actual data we
    used.
  id: totrans-4988
  prefs: []
  type: TYPE_NORMAL
  zh: 6 目前，我们不允许对我们使用的实际数据进行更具体的说明。
- en: '![408_image_0.png](408_image_0.png)'
  id: totrans-4989
  prefs: []
  type: TYPE_IMG
  zh: '![408_image_0.png](408_image_0.png)'
- en: Fig. 17.26. Comparison of the accumulated profit&loss curve of the two approaches
  id: totrans-4990
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.26. 两种方法的累计盈亏曲线比较
- en: 17.10 Conclusion
  id: totrans-4991
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.10 结论
- en: 'A typical regression analysis by neural networks begins with the statement:
    "In principal a neural network can model everything". If the data is not very
    reliable due to noise or missing factors, learning is only one part of the model
    building procedure. Often, task-independent regularization is used as weapon to
    reduce the uncertainty introduced by the data. More generally, the model building
    process demands more information (prior knowledge) about the specific task.'
  id: totrans-4992
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的神经网络回归分析始于声明：“原则上，神经网络可以建模一切”。如果由于噪声或缺失因素数据不太可靠，学习仅是模型构建过程的一部分。通常，任务无关的正则化被用作减少数据引入的不确定性的手段。更一般地说，模型构建过程需要更多关于特定任务的信息（先验知识）。
- en: We have shown that there are a large number of techniques for including additional
    constraints which depend on the problem. Consequently, the resulting model is
    not only based on the training data but also on the additional constraints and
    also on the exact steps of the training procedure. From a Bayesian viewpoint,
    this can be described as an integration of priors in the model building process.
    Thus, this article can also be interpreted as a discussion of valuable priors
    and of how to combine them in order to achieve a maximal synergy.
  id: totrans-4993
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了有大量技术可以包含依赖于问题的额外约束。因此，得到的模型不仅基于训练数据，还基于额外约束以及训练程序的确切步骤。从贝叶斯的角度来看，这可以描述为在模型构建过程中对先验的整合。因此，本文也可以解读为对有价值的先验进行讨论，以及如何组合它们以实现最大的协同效应。
- en: The authors believe that the additional features are an outstanding advantage
    of neural networks and will lead to more robust and successful models. This paper
    gives an overview about some of the features we experienced as useful in our applications.
  id: totrans-4994
  prefs: []
  type: TYPE_NORMAL
  zh: 作者认为，额外特性是神经网络的显著优势，并将导致更鲁棒和成功的模型。本文概述了我们在应用中认为有用的一些特性。
- en: Finally, the unified training procedure is a recipe for building a model with
    neural networks by way of a relatively simple sequence of steps. An important
    aspect of this paper has been the study of the interactions between the different
    techniques. The described algorithms are integrated in the *Simulation Environment
    for Neural Networks*, SENN, a product of Siemens AG. More information can be found
    on the web page http://www.senn.sni.de.
  id: totrans-4995
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，统一的训练程序是通过相对简单的步骤序列构建神经网络模型的一个配方。本文的重要方面是研究不同技术之间的相互作用。所描述的算法集成在*神经网络模拟环境*（SENN）中，这是西门子公司的产品。更多信息可以在网页
    http://www.senn.sni.de 找到。
- en: '[1] Ackley, D.H., Hinton, G.E., Sejnowski, T.J.: A learning algorithm for Boltzmann
    machines. Cognitive Science 9, 147–169 (1985); Reprinted in [2]'
  id: totrans-4996
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 阿克利, D.H., 希顿, G.E., 塞尔诺夫斯基, T.J.: 一种用于玻尔兹曼机的学习算法。认知科学 9, 147–169 (1985);
    重新印刷于 [2]'
- en: '[2] Anderson, J.A., Rosenfeld, E. (eds.): Neurocomputing: Foundations of Research.'
  id: totrans-4997
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 安德森, J.A., 罗斯菲尔德, E. (主编): 神经计算：研究基础。'
- en: The MIT Press, Cambridge (1988)
  id: totrans-4998
  prefs: []
  type: TYPE_NORMAL
  zh: 麻省理工学院出版社，剑桥 (1988)
- en: '[3] Bishop, C.M.: Neural Networks for Pattern Recognition. Clarendon Press,
    Oxford'
  id: totrans-4999
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 毕肖普, C.M.: 用于模式识别的神经网络。克拉伦登出版社，牛津'
- en: (1995)
  id: totrans-5000
  prefs: []
  type: TYPE_NORMAL
  zh: (1995)
- en: '[4] Breiman, L.: Bagging predictors. Technical Report TR No. 421, Department
    of Statistics, University of California (1994)'
  id: totrans-5001
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 布雷曼, L.: 装袋预测器。技术报告 TR No. 421，加州大学统计系 (1994)'
- en: '[5] Bunke, H., Bunke, O.: Nonlinear Regression, Functional Analysis and Robust
    Methods, vol. 2. John Wiley and Sons (1989)'
  id: totrans-5002
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 本克, H., 本克, O.: 非线性回归、泛函分析与鲁棒方法，第2卷。约翰·威利与儿子公司 (1989)'
- en: '[6] Caruana, R.: Multitask learning. Machine Learning 28, 41 (1997) [7] Elton,
    E.J., Gruber, M.J.: Modern Portfolio Theory and Investment Analysis.'
  id: totrans-5003
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] 卡鲁阿纳, R.: 多任务学习。机器学习 28, 41 (1997) [7] 埃尔顿, E.J., 格鲁伯, M.J.: 现代投资组合理论与投资分析。'
- en: John Wiley & Sons (1995)
  id: totrans-5004
  prefs: []
  type: TYPE_NORMAL
  zh: 约翰·威利与儿子公司 (1995)
- en: '[8] Finnoff, W., Hergert, F., Zimmermann, H.G.: Improving generalization performance
    by nonconvergent model selection methods. In: Aleksander, I., Taylor, J.'
  id: totrans-5005
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] 菲诺夫, W., 赫尔盖特, F., 齐默尔曼, H.G.: 通过非收敛模型选择方法提高泛化性能。见：亚历山大, I., 泰勒, J.'
- en: (eds.) Proc. of the Inter. Conference on Artificial Neural Networks, ICANN 1992,
    vol. 2, pp. 233–236 (1992)
  id: totrans-5006
  prefs: []
  type: TYPE_NORMAL
  zh: (主编) 人工神经网络国际会议论文集，ICANN 1992，第2卷，第233–236页 (1992)
- en: '[9] Finnoff, W., Hergert, F., Zimmermann, H.G.: Neuronale Lernverfahren mit
    variabler Schrittweite, Tech. report, Siemens AG (1993)'
  id: totrans-5007
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] 菲诺夫, W., 赫尔盖特, F., 齐默尔曼, H.G.: 具有可变步长的神经学习方法，技术报告，西门子公司 (1993)'
- en: '[10] Flake, G.W.: Square Unit Augmented, Radially Extended, Multilayer Perceptrons.
    In: Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the Trade, 1st edn. LNCS,'
  id: totrans-5008
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Flake, G.W.: 方形单元增强的径向扩展多层感知器。在：Orr, G.B., Müller, K.-R. (编辑) 神经网络：行业技巧，第
    1 版。LNCS，'
- en: vol. 7700, pp. 143–161. Springer, Heidelberg (2012)
  id: totrans-5009
  prefs: []
  type: TYPE_NORMAL
  zh: 卷 7700，第 143–161 页。施普林格出版社，海德堡（2012）
- en: '[11] Gershenfeld, N.A.: An experimentalist''s introduction to the observation
    of dynamical systems. In: Hao, B.L. (ed.) Directions in Chaos, vol. 2, pp. 310–384.
    World Scientific, Singapore (1989)'
  id: totrans-5010
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Gershenfeld, N.A.: 实验者对动态系统观察的介绍。在：Hao, B.L. (编辑) 混沌方向，卷 2，第 310–384 页。世界科学出版社，新加坡（1989）'
- en: '[12] Herve, P., Naim, P., Zimmermann, H.G.: Advanced Adaptive Architectures
    for Asset Allocation: A Trial Application. In: Forecasting Financial Markets (1996)'
  id: totrans-5011
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Herve, P., Naim, P., Zimmermann, H.G.: 资产配置的高级自适应架构：一个试验应用。在：金融市场预测（1996）'
- en: '[13] Hochreiter, S., Schmidhuber, J.: Flat minima. Neural Computation 9(1),
    1–42'
  id: totrans-5012
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Hochreiter, S., Schmidhuber, J.: 平坦极小值。神经计算 9(1)，1–42'
- en: (1997)
  id: totrans-5013
  prefs: []
  type: TYPE_NORMAL
  zh: (1997)
- en: '[14] Hornik, K.: Approximation Capabilities of Multilayer Feedforward Networks.
    Neural Networks 4, 251–257 (1991)'
  id: totrans-5014
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Hornik, K.: 多层前馈网络的逼近能力。神经网络 4，251–257（1991）'
- en: '[15] le Cun, Y., Denker, J.S., Solla, S.A.: Optimal brain damage. In: Touretzky,
    D.S.'
  id: totrans-5015
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] le Cun, Y., Denker, J.S., Solla, S.A.: 最优脑损伤。在：Touretzky, D.S.'
- en: (ed.) Advances in Neural Information Processing Systems, NIPS 1989, vol. 2,
    pp. 598–605. Morgan Kaufmann, San Mateo (1990)
  id: totrans-5016
  prefs: []
  type: TYPE_NORMAL
  zh: (编辑) 神经信息处理系统进展，NIPS 1989，卷 2，第 598–605 页。摩根·考夫曼，圣马特奥（1990）
- en: '[16] Moody, J.E., Rögnvaldsson, T.S.: Smoothing regularizers for projective
    basis function networks. In: Mozer, M.C., Jordan, M.I., Petsche, T. (eds.) Advances
    in Neural Information Processing Systems, vol. 9, p. 585. The MIT Press (1997)'
  id: totrans-5017
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Moody, J.E., Rögnvaldsson, T.S.: 投影基函数网络的平滑正则化器。在：Mozer, M.C., Jordan,
    M.I., Petsche, T. (编辑) 神经信息处理系统进展，卷 9，第 585 页。麻省理工学院出版社（1997）'
- en: '[17] Williams, P.M.: Using Neural Networks to Model Conditional Multivariate
    Densities. Technical Report CSRP 371, School of Cognitive and Computing Sciences,
    Univ. of Sussex (February 1995)'
  id: totrans-5018
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Williams, P.M.: 使用神经网络建模条件多元密度。技术报告 CSRP 371，萨塞克斯大学认知与计算科学学院（1995年2月）'
- en: '[18] Neuneier, R.: Optimal asset allocation using adaptive dynamic programming.
    In:'
  id: totrans-5019
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Neuneier, R.: 使用自适应动态编程的最佳资产配置。在：'
- en: Advances in Neural Information Processing Systems, vol. 8. MIT Press (1996)
  id: totrans-5020
  prefs: []
  type: TYPE_NORMAL
  zh: 神经信息处理系统进展，卷 8。麻省理工学院出版社（1996）
- en: '[19] Neuneier, R.: Optimale Investitionsentscheidungen mit Neuronalen Netzen.
    PhD'
  id: totrans-5021
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Neuneier, R.: 使用神经网络的最佳投资决策。博士'
- en: thesis, Universität Kaiserslautern, Institut für Informatik (1998)
  id: totrans-5022
  prefs: []
  type: TYPE_NORMAL
  zh: 论文，凯瑟尔劳滕大学，计算机科学研究所（1998）
- en: '[20] Neuneier, R., Finnoff, W., Hergert, F., Ormoneit, D.: Estimation of Conditional
    Densities: A Comparison of Neural Network Approaches. In: Intern. Conf. on Artificial
    Neural Networks, ICANN, vol. 1, pp. 689–692. Springer (1994)'
  id: totrans-5023
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Neuneier, R., Finnoff, W., Hergert, F., Ormoneit, D.: 条件密度的估计：神经网络方法的比较。在：国际人工神经网络会议，ICANN，卷
    1，第 689–692 页。施普林格出版社（1994）'
- en: '[21] Nix, D.A., Weigend, A.S.: Estimating the mean and variance of the target
    probability distribution. In: World Congress of Neural Networks. Lawrence Erlbaum
    Associates (1994)'
  id: totrans-5024
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Nix, D.A., Weigend, A.S.: 估计目标概率分布的均值和方差。在：神经网络世界大会。劳伦斯·厄尔鲍姆协会（1994）'
- en: '[22] Ormoneit, D.: Estimation of Probability Densities using Neural Networks.
    Master''s thesis, Fakultät für Informatik, Technische Universität München (1993)'
  id: totrans-5025
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Ormoneit, D.: 使用神经网络估计概率密度。硕士论文，慕尼黑工业大学计算机科学系（1993）'
- en: '[23] Papoulis, A.: Probability, Random Variables, and Stochastic Processes,
    3rd edn.'
  id: totrans-5026
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Papoulis, A.: 概率、随机变量和随机过程，第 3 版。'
- en: McGraw Hill, Inc. (1991)
  id: totrans-5027
  prefs: []
  type: TYPE_NORMAL
  zh: 麦Graw Hill, Inc.（1991）
- en: '[24] Perrone, M.P.: Improving Regression Estimates: Averaging Methods for Variance
    Reduction with Extensions to General Convex Measure Optimization. PhD thesis,
    Brown University (1993)'
  id: totrans-5028
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Perrone, M.P.: 改进回归估计：方差减少的平均方法及其在一般凸度量优化中的扩展。博士论文，布朗大学（1993）'
- en: '[25] Refenes, A.P. (ed.): Neural Networks in the Capital Market. Wiley & Sons
    (1994)'
  id: totrans-5029
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Refenes, A.P. (编辑): 资本市场中的神经网络。威利父子公司（1994）'
- en: '[26] Sanger, T.D.: Optimal unsupervised learning in a single-layer linear feedforward
    network. Neural Networks 2, 459–473 (1989)'
  id: totrans-5030
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Sanger, T.D.: 单层线性前馈网络中的最佳无监督学习。神经网络 2，459–473（1989）'
- en: '[27] Seber, G.A.F., Wild, C.J.: Nonlinear Regression. John Wiley & Sons, New
    York'
  id: totrans-5031
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Seber, G.A.F., Wild, C.J.: 非线性回归。约翰·威利父子公司，纽约'
- en: (1989)
  id: totrans-5032
  prefs: []
  type: TYPE_NORMAL
  zh: (1989)
- en: '[28] Srivastava, A.N., Weigend, A.S.: Computing the probability density in
    connectionist regression. In: Marinaro, M., Morasso, P.G. (eds.) Proceedings of
    the International Conference on Artificial Neural Networks, Sorrento, Italy (ICANN
    1994),'
  id: totrans-5033
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] Srivastava, A.N., Weigend, A.S.: 计算连接主义回归中的概率密度。在：Marinaro, M., Morasso,
    P.G. (编辑) 《国际人工神经网络会议论文集》，意大利索伦托（ICANN 1994），'
- en: pp. 685–688. Springer (1994); Also in Proceedings of the IEEE International
    Conference on Neural Networks, Orlando, FL (IEEE–ICNN 1994), pp. 3786–3789.
  id: totrans-5034
  prefs: []
  type: TYPE_NORMAL
  zh: 第685–688页。施普林格（1994）；亦见于第IEEE国际神经网络会议论文集，佛罗里达州奥兰多（IEEE–ICNN 1994），第3786–3789页。
- en: IEEE Press (1994)
  id: totrans-5035
  prefs: []
  type: TYPE_NORMAL
  zh: IEEE出版社（1994）
- en: '[29] Takens, F.: Detecting Strange Attractors in Turbulence. In: Rand, D.A.,
    Young, L.S. (eds.) Dynamical Systems and Turbulence. Lecture Notes in Mathematics,
    vol. 898, pp. 366–381. Springer (1981)'
  id: totrans-5036
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] Takens, F.: 在湍流中检测奇异吸引子。在：Rand, D.A., Young, L.S.（编辑）《动力系统与湍流》。数学讲义，卷898，第366–381页。施普林格（1981）'
- en: '[30] Tang, B., Hsieh, W., Tangang, F.: Clearning neural networks with continuity
    constraints for prediction of noisy time series. In: Progres in Neural Information
    Processing (ICONIP 1996), pp. 722–725. Springer, Berlin (1996)'
  id: totrans-5037
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] Tang, B., Hsieh, W., Tangang, F.: 使用连续性约束的神经网络清理以预测噪声时间序列。在：神经信息处理进展（ICONIP
    1996），第722–725页。施普林格，柏林（1996）'
- en: '[31] Tresp, V., Neuneier, R., Zimmermann, H.G.: Early brain damage. In: Advances
    in Neural Information Processing Systems, vol. 9. MIT Press (1997)'
  id: totrans-5038
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Tresp, V., Neuneier, R., Zimmermann, H.G.: 早期脑损伤。在《神经信息处理系统进展》，第9卷。MIT出版社（1997）'
- en: '[32] Weigend, A.S., Zimmermann, H.G.: Exploiting local relations as soft constraints
    to improve forecasting. Computational Intelligence in Finance 6(1) (January 1998)'
  id: totrans-5039
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] Weigend, A.S., Zimmermann, H.G.: 利用局部关系作为软约束来改善预测。计算智能在金融中的应用 6(1) (1998年1月)'
- en: '[33] Weigend, A.S., Zimmermann, H.G., Neuneier, R.: The observer-observation
    dilemma in neuro-forecasting: Reliable models from unreliable data through clearning.
    In: Freedman, R. (ed.) AI Applications on Wall Street, pp. 308–317. Software Engineering
    Press, New York (1995)'
  id: totrans-5040
  prefs: []
  type: TYPE_NORMAL
  zh: '[33] Weigend, A.S., Zimmermann, H.G., Neuneier, R.: 神经预测中的观察者-观察问题：通过清理从不可靠数据中获得可靠模型。在：Freedman,
    R.（编辑）《华尔街的人工智能应用》，第308–317页。软件工程出版社，纽约（1995）'
- en: '[34] Weigend, A.S., Rumelhart, D.E., Huberman, B.A.: Generalization by weightelimination
    with application to forecasting. In: Lippmann, R.P., Moody, J.E.,'
  id: totrans-5041
  prefs: []
  type: TYPE_NORMAL
  zh: '[34] Weigend, A.S., Rumelhart, D.E., Huberman, B.A.: 通过权重消除实现泛化并应用于预测。在：Lippmann,
    R.P., Moody, J.E.,'
- en: Touretzky, D.S. (eds.) Advances in Neural Information Processing Systems, vol.
    3, pp. 875–882. Morgan Kaufmann, San Mateo (1991)
  id: totrans-5042
  prefs: []
  type: TYPE_NORMAL
  zh: Touretzky, D.S.（编辑）《神经信息处理系统进展》，第3卷，第875–882页。摩根·考夫曼，旧金山（1991）
- en: '[35] White, H.: Parametrical statistical estimation with artificial neural
    networks.'
  id: totrans-5043
  prefs: []
  type: TYPE_NORMAL
  zh: '[35] White, H.: 使用人工神经网络的参数统计估计。'
- en: Technical report, University of California, San Diego (1991)
  id: totrans-5044
  prefs: []
  type: TYPE_NORMAL
  zh: 技术报告，加州大学圣地亚哥分校（1991）
- en: '[36] Zimmermann, H.G., Weigend, A.S.: Representing dynamical systems in feedforward
    networks: A six layer architecture. In: Weigend, A.S., Abu-Mostafa, Y.,'
  id: totrans-5045
  prefs: []
  type: TYPE_NORMAL
  zh: '[36] Zimmermann, H.G., Weigend, A.S.: 在前馈网络中表示动态系统：一种六层架构。在：Weigend, A.S.,
    Abu-Mostafa, Y.,'
- en: 'Refenes, A.-P.N. (eds.) Decision Technologies for Financial Engineering: Proceedings
    of the Fourth International Conference on Neural Networks in the Capital Markets
    (NNCM 1996). World Scientific, Singapore (1997)'
  id: totrans-5046
  prefs: []
  type: TYPE_NORMAL
  zh: Refenes, A.-P.N.（编辑）《金融工程决策技术：第四届国际神经网络资本市场会议论文集（NNCM 1996）》。世界科学，新加坡（1997）
- en: '[37] Zimmermann, H.G.: Neuronale Netze als Entscheidungskalkül. In: Rehkugler,
    H.,'
  id: totrans-5047
  prefs: []
  type: TYPE_NORMAL
  zh: '[37] Zimmermann, H.G.: 作为决策计算的神经网络。在：Rehkugler, H.,'
- en: Zimmermann, H.G. (eds.) Neuronale Netze in der Ökonomie. Verlag Franz Vahlen
  id: totrans-5048
  prefs: []
  type: TYPE_NORMAL
  zh: Zimmermann, H.G.（编辑）《经济学中的神经网络》。弗朗茨·瓦伦出版社
- en: (1994)
  id: totrans-5049
  prefs: []
  type: TYPE_NORMAL
  zh: (1994)
- en: '[38] Zimmermann, H.G., Neuneier, R.: The observer-observation dilemma in neuroforecasting.
    In: Advances in Neural Information Processing Systems, vol. 10. MIT'
  id: totrans-5050
  prefs: []
  type: TYPE_NORMAL
  zh: '[38] Zimmermann, H.G., Neuneier, R.: 神经预测中的观察者-观察问题。在《神经信息处理系统进展》，第10卷。MIT'
- en: Press (1998)
  id: totrans-5051
  prefs: []
  type: TYPE_NORMAL
  zh: 出版社（1998）
- en: Big Learning And Deep Neural Networks
  id: totrans-5052
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大学习与深度神经网络
- en: Preface
  id: totrans-5053
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序言
- en: More data and compute resources opens the way to "big learning", that is, scaling
    up machine learning to large data sets and complex problems. In order to solve
    these new problems, we need to identify the complex dependencies that interrelate
    inputs and outputs [1]. Achieving this goal requires powerful algorithms in terms
    of both representational power and ability to absorb and distill information from
    large data flows. More and more it becomes apparent that neural networks provide
    an excellent toolset to scale learning. This holds true for simple linear systems
    as well as today's grand challenges where the underlying application problem requires
    high nonlinearity and complex structured representations.
  id: totrans-5054
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的数据和计算资源为“规模学习”开辟了道路，即将机器学习扩展到大型数据集和复杂问题。为了解决这些新问题，我们需要识别输入和输出之间复杂的依赖关系[1]。实现这一目标需要在表示能力和从大量数据流中吸收和提炼信息的能力方面都强大的算法。越来越明显的是，神经网络提供了一个出色的工具集来扩展学习。这对于简单的线性系统以及当今的重大挑战都适用，这些挑战的基础应用问题要求高非线性和复杂的结构化表示。
- en: The following four chapters introduce the basic toolbox for solving these complex
    learning problems. They include first and second-order optimization methods, the
    best practice of training neural networks and an introduction to Torch7, a neural
    network library for implementing large-scale learning problems.
  id: totrans-5055
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的四章介绍了解决这些复杂学习问题的基本工具箱。它们包括一阶和二阶优化方法、训练神经网络的最佳实践以及Torch7的介绍，这是一个用于实现大规模学习问题的神经网络库。
- en: A first challenge is to feed the model with enough data in order to properly
    identify the rich set of dependencies. Chapter 18 [2] presents the stochastic
    gradient descent algorithm (learning one example at the time), showing that it
    can minimize the objective function at a rate that is under certain conditions
    asymptotically optimal.
  id: totrans-5056
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个挑战是为模型提供足够的数据，以便正确识别丰富的依赖关系集合。第18章[2]介绍了随机梯度下降算法（一次学习一个例子），表明在某些条件下它可以以渐近最优的速度最小化目标函数。
- en: Neural networks are a useful framework to carry out such optimization.
  id: totrans-5057
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是执行此类优化的有效框架。
- en: The *correspondence principle for neural networks* that will be detailed later
    in Chapter 28 [7] states that we can associate to an equation or an optimization
    problem, a neural network architecture that reduces the learning problem to an
    exchange of local signals between neighboring units of the network. Therefore,
    the tractability of the optimization problem is much dependent on the neural network
    architecture and its multiple hyperparameters [3]. These hyperparameters and the
    methods to choose them are dissected in Chapter 19 [5].
  id: totrans-5058
  prefs: []
  type: TYPE_NORMAL
  zh: 在第28章[7]中详细介绍的*神经网络的对应原理*指出，我们可以将一个方程或优化问题与一种神经网络架构相关联，该架构将学习问题简化为网络中相邻单元之间局部信号的交换。因此，优化问题的可处理性在很大程度上依赖于神经网络架构及其多个超参数[3]。这些超参数及其选择方法在第19章[5]中进行了剖析。
- en: In some cases, the multiple hyperparameters of the neural network are insufficient
    to produce a well-conditioned optimization problem. Difficult optimization problems
    include, for example, recurrent neural networks or deep neural networks, both
    involving long-range dependencies. Exploiting second order information (i.e. the
    second derivatives of the objective function) can lead to faster convergence.
    Second-order methods are much more sensitive to noise than first order methods
    and require carefully tuned optimization parameters in order to achieve a good
    speed-stability tradeoff. Second-order methods for deep and recurrent neural networks
    are discussed in much depth in Chapter 20 [6].
  id: totrans-5059
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，神经网络的多个超参数不足以产生良好条件的优化问题。困难的优化问题包括，例如，递归神经网络或深度神经网络，它们都涉及长范围的依赖关系。利用二阶信息（即目标函数的二阶导数）可以实现更快的收敛。与一阶方法相比，二阶方法对噪声更加敏感，并且需要仔细调整的优化参数以实现良好的速度与稳定性的权衡。第20章[6]中深入讨论了深度和递归神经网络的二阶方法。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    419–420, 2012.'
  id: totrans-5060
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon等人（主编）：NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp. 419–420,
    2012。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-5061
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: Finally, algorithms must be translated into code that can be run by the machine.
    Chapter 21 [4] presents *Torch7*, an efficient and easy to use software for building
    and training neural networks. The software provides a large number of neural network
    primitives and is highly modular so that it can be applied to a wide range of
    problems including computer vision, natural language processing, speech recognition
    and many more.
  id: totrans-5062
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，算法必须被翻译成可以由机器运行的代码。第21章[4]介绍了*Torch7*，一种高效且易于使用的构建和训练神经网络的软件。该软件提供大量神经网络原语，具有高度模块化，适用于计算机视觉、自然语言处理、语音识别等广泛问题。
- en: Grégoire & Klaus
  id: totrans-5063
  prefs: []
  type: TYPE_NORMAL
  zh: 格雷戈尔与克劳斯
- en: '[1] Bengio, Y., LeCun, Y.: Scaling learning algorithms towards AI. In: Large
    Scale Kernel Machines (2007)'
  id: totrans-5064
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Bengio, Y., LeCun, Y.: 将学习算法扩展到人工智能。在：大型规模核机器（2007）'
- en: '[2] Bottou, L.: Stochastic Gradient Descent Tricks. In: Montavon, G., Orr,
    G.B.,'
  id: totrans-5065
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Bottou, L.: 随机梯度下降技巧。在：Montavon, G., Orr, G.B.,'
- en: 'Müller, K.-R. (eds.) NN: Tricks of the Trade, 2nd edn. LNCS, vol. 7700, pp.
    421– 436. Springer, Heidelberg (2012)'
  id: totrans-5066
  prefs: []
  type: TYPE_NORMAL
  zh: Müller, K.-R.（主编）《神经网络：行业窍门》，第2版。LNCS，第7700卷，第421–436页。施普林格，海德堡（2012）
- en: '[3] LeCun, Y., Bottou, L., Orr, G.B., Müller, K.-R.: Efficient BackProp. In:
    Orr, G.B.,'
  id: totrans-5067
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] LeCun, Y., Bottou, L., Orr, G.B., Müller, K.-R.: 高效反向传播。在：Orr, G.B.,'
- en: Müller, K.-R. (eds.) NIPS-WS 1996. LNCS, vol. 1524, pp. 9–50. Springer, Heidelberg
    (1998)
  id: totrans-5068
  prefs: []
  type: TYPE_NORMAL
  zh: Müller, K.-R.（主编）《NIPS-WS 1996》。LNCS，第1524卷，第9–50页。施普林格，海德堡（1998）
- en: '[4] Collobert, R., Kavukcuoglu, K., Farabet, C.: Implementing Neural Networks
    Efficiently. In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the
    Trade, 2nd edn. LNCS, vol. 7700, pp. 537–557. Springer, Heidelberg (2012)'
  id: totrans-5069
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Collobert, R., Kavukcuoglu, K., Farabet, C.: 高效实现神经网络。在：Montavon, G., Orr,
    G.B., Müller, K.-R.（主编）《神经网络：行业窍门》，第2版。LNCS，第7700卷，第537–557页。施普林格，海德堡（2012）'
- en: '[5] Bengio, Y.: Practical Recommendations for Gradient-based Training of Deep
    Architectures. In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of
    the Trade, 2nd edn. LNCS, vol. 7700, pp. 437–478. Springer, Heidelberg (2012)'
  id: totrans-5070
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Bengio, Y.: 深度架构基于梯度的训练的实用建议。在：Montavon, G., Orr, G.B., Müller, K.-R.（主编）《神经网络：行业窍门》，第2版。LNCS，第7700卷，第437–478页。施普林格，海德堡（2012）'
- en: '[6] Martens, J., Sutskever, I.: Training Deep and Recurrent Networks with Hessian-free
    Optimization. In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of
    the Trade, 2nd edn. LNCS, vol. 7700, pp. 479–535. Springer, Heidelberg (2012)'
  id: totrans-5071
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Martens, J., Sutskever, I.: 无海森优化训练深度和递归网络。在：Montavon, G., Orr, G.B., Müller,
    K.-R.（主编）《神经网络：行业窍门》，第2版。LNCS，第7700卷，第479–535页。施普林格，海德堡（2012）'
- en: '[7] Zimmermann, H.-G., Tietz, C., Grothmann, R.: Forecasting with Recurrent
    Neural Networks: 12 Tricks. In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.)
    NN: Tricks of the Trade, 2nd edn. LNCS, vol. 7700, pp. 687–707. Springer, Heidelberg
    (2012)'
  id: totrans-5072
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Zimmermann, H.-G., Tietz, C., Grothmann, R.: 使用递归神经网络的预测：12个技巧。在：Montavon,
    G., Orr, G.B., Müller, K.-R.（主编）《神经网络：行业窍门》，第2版。LNCS，第7700卷，第687–707页。施普林格，海德堡（2012）'
- en: 18 Stochastic Gradient Descent Tricks
  id: totrans-5073
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 18 随机梯度下降技巧
- en: Léon Bottou Microsoft Research, Redmond, WA
  id: totrans-5074
  prefs: []
  type: TYPE_NORMAL
  zh: 莱昂·博图，微软研究院，雷德蒙德，华盛顿州
- en: leon@bottou.org http://leon.bottou.org Abstract. Chapter 1 strongly advocates
    the *stochastic back-propagation* method to train neural networks. This is in
    fact an instance of a more general technique called *stochastic gradient descent*
    (SGD). This chapter provides background material, explains why SGD is a good learning
    algorithm when the training set is large, and provides useful recommendations.
  id: totrans-5075
  prefs: []
  type: TYPE_NORMAL
  zh: leon@bottou.org http://leon.bottou.org 摘要：第1章强烈倡导使用*随机反向传播*方法训练神经网络。这实际上是更通用技术*随机梯度下降*（SGD）的一个实例。本章提供背景材料，解释了为什么SGD在训练集较大时是一个好的学习算法，并提供有用的建议。
- en: 18.1 Introduction
  id: totrans-5076
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.1 引言
- en: Chapter 1 strongly advocates the *stochastic back-propagation* method to train
    neural networks. This is in fact an instance of a more general technique called
    stochastic gradient descent (SGD). This chapter provides background material,
    explains why SGD is a good learning algorithm when the training set is large,
    and provides useful recommendations.
  id: totrans-5077
  prefs: []
  type: TYPE_NORMAL
  zh: 第1章强烈倡导使用*随机反向传播*方法训练神经网络。这实际上是更通用技术随机梯度下降（SGD）的一个实例。本章提供背景材料，解释了为什么SGD在训练集较大时是一个好的学习算法，并提供有用的建议。
- en: 18.2 What Is Stochastic Gradient Descent?
  id: totrans-5078
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.2 什么是随机梯度下降？
- en: Let us first consider a simple supervised learning setup. Each example z is
    a pair (*x, y*) composed of an arbitrary input x and a scalar output y. We consider
    a *loss* function (ˆ*y, y*) that measures the cost of predicting yˆ when the actual
    answer is y, and we choose a family F of functions fw(x) parametrized by a weight
    vector w. We seek the function f ∈ F that minimizes the loss Q(z,w) = (fw(x),
    y)
  id: totrans-5079
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先考虑一个简单的监督学习设置。每个示例 z 是一对 (*x, y*)，由任意输入 x 和标量输出 y 组成。我们考虑一个 *损失* 函数 (ˆ*y,
    y*)，它衡量当实际答案为 y 时预测 yˆ 的代价，我们选择一个由权重向量 w 参数化的函数族 F。我们寻找函数 f ∈ F，使损失 Q(z,w) = (fw(x),
    y) 最小化。
- en: averaged on the examples. Although we would like to average over the unknown
    distribution dP(z) that embodies the Laws of Nature, we must often settle for
    computing the average on a sample z1 *...z*n.
  id: totrans-5080
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些示例上进行平均。尽管我们希望对体现自然法则的未知分布 dP(z) 进行平均，但我们通常不得不满足于在样本 z1 *...z*n 上计算平均。
- en: $$E(f)=\int\ell(f(x),y)\,dP(z)\qquad E_{n}(f)=\frac{1}{n}\sum_{i=1}^{n}\ell(f(x_{i}),y_{i})\tag{18.1}$$
  id: totrans-5081
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(f)=\int\ell(f(x),y)\,dP(z)\qquad E_{n}(f)=\frac{1}{n}\sum_{i=1}^{n}\ell(f(x_{i}),y_{i})\tag{18.1}$$
- en: The *empirical risk* En(f) measures the training set performance. The *expected*
    risk E(f) measures the generalization performance, that is, the expected performance
    on future examples. The statistical learning theory [25] justifies minimizing
    the empirical risk instead of the expected risk when the chosen family F is sufficiently
    restrictive.
  id: totrans-5082
  prefs: []
  type: TYPE_NORMAL
  zh: '*经验风险* En(f) 衡量训练集的表现。*期望*风险 E(f) 衡量泛化性能，即对未来示例的期望表现。统计学习理论[25]证明，当所选的函数族 F
    足够限制时，最小化经验风险而不是期望风险是合理的。'
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    421–436, 2012.'
  id: totrans-5083
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon 等 (Eds.): NN: Tricks of the Trade, 第2版, LNCS 7700, pp. 421–436,
    2012。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-5084
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: 18.2.1 Gradient Descent
  id: totrans-5085
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.2.1 梯度下降
- en: It has often been proposed (e.g., [18]) to minimize the empirical risk En(fw)
  id: totrans-5086
  prefs: []
  type: TYPE_NORMAL
  zh: 通常建议（例如，[18]）最小化经验风险 En(fw)
- en: using *gradient descent* (GD). Each iteration updates the weights w on the basis
    of the gradient of En(fw),
  id: totrans-5087
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *梯度下降* (GD)。每次迭代根据 En(fw) 的梯度更新权重 w，
- en: $$w_{t+1}=w_{t}-\gamma\frac{1}{n}\sum_{i=1}^{n}\nabla_{w}\,Q(z_{i},w_{t})\,,\tag{18.2}$$
  id: totrans-5088
  prefs: []
  type: TYPE_NORMAL
  zh: $$w_{t+1}=w_{t}-\gamma\frac{1}{n}\sum_{i=1}^{n}\nabla_{w}\,Q(z_{i},w_{t})\,,\tag{18.2}$$
- en: 'where γ is an adequately chosen learning rate. Under sufficient regularity
    assumptions, when the initial estimate w0 is close enough to the optimum, and
    when the learning rate γ is sufficiently small, this algorithm achieves *linear*
    convergence [6], that is, − log ρ ∼ t, where ρ represents the residual error.1
    Much better optimization algorithms can be designed by replacing the scalar learning
    rate γ by a positive definite matrix Γt that approaches the inverse of the Hessian
    of the cost at the optimum :'
  id: totrans-5089
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 γ 是适当选择的学习率。在足够的正则性假设下，当初始估计 w0 足够接近最优解时，以及学习率 γ 足够小时，该算法实现了 *线性* 收敛 [6]，即
    − log ρ ∼ t，其中 ρ 代表残差误差。1 通过用接近于成本在最优点的 Hessian 的逆的正定矩阵 Γt 替代标量学习率 γ，可以设计出更好的优化算法：
- en: $$w_{t+1}\ =\ w_{t}-\Gamma_{t}\frac{1}{n}\sum_{i=1}^{n}\nabla_{w}\,Q(z_{i},w_{t})\,.$$
  id: totrans-5090
  prefs: []
  type: TYPE_NORMAL
  zh: $$w_{t+1}\ =\ w_{t}-\Gamma_{t}\frac{1}{n}\sum_{i=1}^{n}\nabla_{w}\,Q(z_{i},w_{t})\,.$$
- en: $$(18.3)$$
  id: totrans-5091
  prefs: []
  type: TYPE_NORMAL
  zh: $$(18.3)$$
- en: This *second order gradient descent* (2GD) is a variant of the well known Newton
    algorithm. Under sufficiently optimistic regularity assumptions, and provided
    that w0 is sufficiently close to the optimum, second order gradient descent achieves
    *quadratic convergence*. When the cost is quadratic and the scaling matrix Γ is
    exact, the algorithm reaches the optimum after a single iteration.
  id: totrans-5092
  prefs: []
  type: TYPE_NORMAL
  zh: 这种 *二阶梯度下降* (2GD) 是著名的牛顿算法的一种变体。在足够乐观的正则性假设下，且在 w0 足够接近最优解的情况下，二阶梯度下降实现了 *二次收敛*。当代价是二次的且缩放矩阵
    Γ 是精确的时，该算法在一次迭代后达到最优解。
- en: Otherwise, assuming sufficient smoothness, we have − log log ρ ∼ t.
  id: totrans-5093
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，假设有足够的平滑性，我们有 − log log ρ ∼ t。
- en: 18.2.2 Stochastic Gradient Descent
  id: totrans-5094
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.2.2 随机梯度下降
- en: The *stochastic gradient descent* (SGD) algorithm is a drastic simplification.
  id: totrans-5095
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机梯度下降* (SGD) 算法是一个极大的简化。'
- en: 'Instead of computing the gradient of En(fw) exactly, each iteration estimates
    this gradient on the basis of a single randomly picked example zt :'
  id: totrans-5096
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代不精确计算 En(fw) 的梯度，而是基于单个随机选取的示例 zt 来估计该梯度：
- en: $$w_{t+1}\ =\ w_{t}-\gamma_{t}\nabla_{w}\,Q(z_{t},w_{t})\,.$$
  id: totrans-5097
  prefs: []
  type: TYPE_NORMAL
  zh: $$w_{t+1}\ =\ w_{t}-\gamma_{t}\nabla_{w}\,Q(z_{t},w_{t})\,.$$
- en: $$(18.4)$$
  id: totrans-5098
  prefs: []
  type: TYPE_NORMAL
  zh: $$(18.4)$$
- en: The stochastic process {wt, t= 1*,...* } depends on the examples randomly picked
    at each iteration. It is hoped that (18.4) behaves like its expectation (18.2)
  id: totrans-5099
  prefs: []
  type: TYPE_NORMAL
  zh: 随机过程 {wt, t= 1*,...* } 依赖于每次迭代随机挑选的示例。希望 (18.4) 的表现类似于它的期望 (18.2)
- en: despite the noise introduced by this simplified procedure.
  id: totrans-5100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这一简化过程引入了噪声。
- en: Since the stochastic algorithm does not need to remember which examples were
    visited during the previous iterations, it can process examples on the fly in
    a deployed system. In such a situation, the stochastic gradient descent directly
    optimizes the expected risk, since the examples are randomly drawn from the ground
    truth distribution.
  id: totrans-5101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机算法不需要记住上一次迭代中访问过的示例，因此它可以在部署的系统中即时处理示例。在这种情况下，随机梯度下降直接优化期望风险，因为示例是从真实分布中随机抽取的。
- en: 1 For mostly historical reasons, *linear convergence* means that the residual
    error asymptotically decreases exponentially, and *quadratic convergence* denotes
    an even faster asymptotic convergence. Both convergence rates are considerably
    faster than the SGD convergence rates discussed in section 18.2.3.
  id: totrans-5102
  prefs: []
  type: TYPE_NORMAL
  zh: 1 出于历史原因，*线性收敛*意味着残差误差呈指数级渐近减小，而*二次收敛*则表示更快的渐近收敛。这两种收敛速度均显著快于第18.2.3节讨论的SGD收敛速度。
- en: '| Table 18.1.               | Stochastic gradient algorithms for various learning
    systems                                                 |               |            |'
  id: totrans-5103
  prefs: []
  type: TYPE_TB
  zh: '| 表 18.1.               | 各种学习系统的随机梯度算法                                                 |               |            |'
- en: '| --- | --- | --- | --- |'
  id: totrans-5104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Loss                      | Stochastic gradient algorithm                                                                               |               |            |'
  id: totrans-5105
  prefs: []
  type: TYPE_TB
  zh: '| 损失                      | 随机梯度算法                                                                               |               |            |'
- en: '| Adaline [26]              |  yt − wΦ(xt)  Φ(xt)                                                                                                             |               |            |'
  id: totrans-5106
  prefs: []
  type: TYPE_TB
  zh: '| Adaline [26]              |  yt − wΦ(xt)  Φ(xt)                                                                                                             |               |            |'
- en: '| Qadaline = 1 2  y − wΦ(x) 2                           | w ← w + γt                                                                                                  |               |            |'
  id: totrans-5107
  prefs: []
  type: TYPE_TB
  zh: '| Qadaline = 1 2  y − wΦ(x) 2                           | w ← w + γt                                                                                                  |               |            |'
- en: '| Features Φ(x) ∈ Rd,       | Classes y = ±1                                                                                              |               |            |'
  id: totrans-5108
  prefs: []
  type: TYPE_TB
  zh: '| 特征 Φ(x) ∈ Rd,       | 类别 y = ±1                                                                                              |               |            |'
- en: '| Perceptron [17]           | yt Φ(xt) if yt wΦ(xt) ≤ 0                                                                                                             |               |            |'
  id: totrans-5109
  prefs: []
  type: TYPE_TB
  zh: '| 感知机 [17]           | yt Φ(xt) 如果 yt wΦ(xt) ≤ 0                                                                                                             |               |            |'
- en: '|                           | w ← w + γt                                                                                                  |               |            |'
  id: totrans-5110
  prefs: []
  type: TYPE_TB
  zh: '|                           | w ← w + γt                                                                                                  |               |            |'
- en: '| Qperceptron = max{0, −y wΦ(x)}                           | 0                                                                                                           |
    otherwise     |            |'
  id: totrans-5111
  prefs: []
  type: TYPE_TB
  zh: '| Qperceptron = max{0, −y wΦ(x)}                           | 0                                                                                                           |
    否则     |            |'
- en: '| Features Φ(x) ∈ Rd,       | Classes y = ±1                                                                                              |               |            |'
  id: totrans-5112
  prefs: []
  type: TYPE_TB
  zh: '| 特征 Φ(x) ∈ Rd,       | 类别 y = ±1                                                                                              |               |            |'
- en: '| K-Means [12] 1            | 2                                                                                                           |               |            |'
  id: totrans-5113
  prefs: []
  type: TYPE_TB
  zh: '| K-Means [12] 1            | 2                                                                                                           |               |            |'
- en: '| Qkmeans = mink 2 (z − wk) | 2                                                                                                           |               |            |'
  id: totrans-5114
  prefs: []
  type: TYPE_TB
  zh: '| Qkmeans = mink 2 (z − wk) | 2                                                                                                           |               |            |'
- en: '|                           | k∗ = arg mink(zt − wk) nk∗ ← nk∗ + 1 wk∗ ← wk∗
    + 1 nk∗ (zt − wk∗ ) (counts provide optimal learning rates!) |               |            |'
  id: totrans-5115
  prefs: []
  type: TYPE_TB
  zh: '|                           | k∗ = arg mink(zt − wk) nk∗ ← nk∗ + 1 wk∗ ← wk∗
    + 1 nk∗ (zt − wk∗ ) (计数提供最佳学习率！) |               |            |'
- en: '| Data z ∈ Rd Centroids w1 ...wk ∈ Rd Counts n1 ...nk ∈ N, initially 0 SVM
    [5] Qsvm = λw2 + max{0, 1 − y wΦ(x)} Features Φ(x) ∈ Rd, Classes y = ±1 Hyperparameter
    λ > 0                           | λw                                                                                                          |
    if yt wΦ(xt) > 1,               |            |'
  id: totrans-5116
  prefs: []
  type: TYPE_TB
  zh: '| 数据 z ∈ Rd 中心 w1 ...wk ∈ Rd 计数 n1 ...nk ∈ N，初始为 0 SVM [5] Qsvm = λw2 + max{0,
    1 − y wΦ(x)} 特征 Φ(x) ∈ Rd，类别 y = ±1 超参数 λ > 0                           | λw                                                                                                          |
    如果 yt wΦ(xt) > 1，               |            |'
- en: '|                           | w ← w − γt                                                                                                  |
    λw − yt Φ(xt) | otherwise. |'
  id: totrans-5117
  prefs: []
  type: TYPE_TB
  zh: '|                           | w ← w − γt                                                                                                  |
    λw − yt Φ(xt) | 否则。 |'
- en: "| Lasso [23]                | ui ← \f ui − γt  λ − (yt − wΦ(xt))Φi(xt)  + vi\
    \ ← \f vi − γt  λ + (yt − wΦ(xt))Φi(xt)  + with notation [x]+ = max{0, x}.   \
    \                                                                            \
    \                              |               |            |"
  id: totrans-5118
  prefs: []
  type: TYPE_TB
  zh: "| Lasso [23]                | ui ← \f ui − γt  λ − (yt − wΦ(xt))Φi(xt)  + vi\
    \ ← \f vi − γt  λ + (yt − wΦ(xt))Φi(xt)  + 记法 [x]+ = max{0, x}。              \
    \                                                                            \
    \                   |               |            |"
- en: '| Qlasso = λ|w|1 + 1 2  y − wΦ(x) 2 w = (u1 − v1,...,ud − vd) Features Φ(x)
    ∈ Rd, Classes y = ±1 Hyperparameter λ > 0                           |                                                                                                             |               |            |'
  id: totrans-5119
  prefs: []
  type: TYPE_TB
  zh: '| Qlasso = λ|w|1 + 1 2  y − wΦ(x) 2 w = (u1 − v1,...,ud − vd) 特征 Φ(x) ∈ Rd，类别
    y = ±1 超参数 λ > 0                           |                                                                                                             |               |            |'
- en: Table 18.1 illustrates stochastic gradient descent algorithms for a number of
    classic machine learning schemes. The stochastic gradient descent for the Perceptron,
    for the Adaline, and for k-Means match the algorithms proposed in the original
    papers. The SVM and the Lasso were first described with traditional optimization
    techniques. Both Qsvm and Qlasso include a regularization term controlled by the
    hyper-parameter λ. The K-means algorithm converges to a local minimum because
    Qkmeans is nonconvex. On the other hand, the proposed update rule uses second
    order learning rates that ensure a fast convergence. The proposed Lasso algorithm
    represents each weight as the difference of two positive variables. Applying the
    stochastic gradient rule to these variables and enforcing their positivity leads
    to sparser solutions.
  id: totrans-5120
  prefs: []
  type: TYPE_NORMAL
  zh: 表 18.1 展示了多种经典机器学习方案的随机梯度下降算法。感知器、Adaline 和 k-Means 的随机梯度下降与原始论文中提出的算法相匹配。支持向量机
    (SVM) 和 Lasso 最初是通过传统优化技术描述的。Qsvm 和 Qlasso 都包含一个由超参数 λ 控制的正则化项。由于 Qkmeans 是非凸的，K-means
    算法收敛到一个局部最小值。另一方面，所提出的更新规则使用二阶学习率，以确保快速收敛。所提出的 Lasso 算法将每个权重表示为两个正变量的差。将随机梯度规则应用于这些变量并强制其为正，导致更稀疏的解。
- en: 18.2.3 The Convergence Of Stochastic Gradient Descent
  id: totrans-5121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.2.3 随机梯度下降的收敛性
- en: The convergence of stochastic gradient descent has been studied extensively
    in the stochastic approximation literature. Convergence results usually require
    decreasing learning rates satisfying the conditions t γ2t < ∞ and t γt = ∞.
  id: totrans-5122
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降的收敛性在随机逼近文献中得到了广泛研究。收敛结果通常需要满足条件 t γ2t < ∞ 和 t γt = ∞ 的递减学习率。
- en: The Robbins-Siegmund theorem [16] provides the means to establish almost sure
    convergence under surprisingly mild conditions [3], including cases where the
    loss function is non smooth.
  id: totrans-5123
  prefs: []
  type: TYPE_NORMAL
  zh: Robbins-Siegmund 定理 [16] 提供了在令人惊讶的温和条件下建立几乎必然收敛的方法 [3]，包括损失函数非光滑的情况。
- en: The convergence speed of stochastic gradient descent is in fact limited by the
    noisy approximation of the true gradient. When the learning rates decrease too
    slowly, the variance of the parameter estimate wt decreases equally slowly.
  id: totrans-5124
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降的收敛速度实际上受到真实梯度的噪声近似的限制。当学习率下降得太慢时，参数估计 wt 的方差也会同样缓慢地下降。
- en: When the learning rates decrease too quickly, the expectation of the parameter
    estimate wt takes a very long time to approach the optimum.
  id: totrans-5125
  prefs: []
  type: TYPE_NORMAL
  zh: 当学习率下降得太快时，参数估计 wt 的期望需要很长时间才能接近最优值。
- en: '- When the Hessian matrix of the cost function at the optimum is strictly positive
    definite, the best convergence speed is achieved using learning rates γt ∼ t'
  id: totrans-5126
  prefs: []
  type: TYPE_NORMAL
  zh: '- 当成本函数在最优点的Hessian矩阵严格正定时，最佳收敛速度通过学习率γt ∼ t实现。'
- en: −1 (e.g. [14]). The expectation of the residual error then decreases with similar
    speed, that is, E(ρ) ∼ t−1. These theoretical convergence rates are frequently
    observed in practice.
  id: totrans-5127
  prefs: []
  type: TYPE_NORMAL
  zh: −1（例如，[14]）。此时残差误差的期望以类似速度下降，即E(ρ) ∼ t−1。这些理论收敛速度在实践中经常观察到。
- en: '- When we relax these regularity assumptions, the theory suggests slower asymptotic
    convergence rates, typically like E(ρ) ∼ t'
  id: totrans-5128
  prefs: []
  type: TYPE_NORMAL
  zh: '- 当我们放宽这些正则性假设时，理论表明渐近收敛速度通常更慢，如E(ρ) ∼ t'
- en: −1/2 (e.g., [28]). In practice, the convergence only slows down during the final
    stage of the optimization process. This may not matter in practice because one
    often stops the optimization before reaching this stage (see section 18.3.1.)
  id: totrans-5129
  prefs: []
  type: TYPE_NORMAL
  zh: −1/2（例如，[28]）。实际上，收敛速度仅在优化过程的最后阶段减缓。在实践中，这可能无关紧要，因为人们往往在达到此阶段之前就停止优化（见18.3.1节）。
- en: 'Second order stochastic gradient descent (2SGD) multiplies the gradients by
    a positive definite matrix Γt approaching the inverse of the Hessian :'
  id: totrans-5130
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶随机梯度下降（2SGD）将梯度乘以一个正定矩阵Γt，接近于Hessian的逆：
- en: $$w_{t+1}~=~w_{t}-\gamma_{t}\Gamma_{t}\nabla_{w}\,Q(z_{t},w_{t})\,.$$
  id: totrans-5131
  prefs: []
  type: TYPE_NORMAL
  zh: $$w_{t+1}~=~w_{t}-\gamma_{t}\Gamma_{t}\nabla_{w}\,Q(z_{t},w_{t})\,.$$
- en: $$(18.5)$$
  id: totrans-5132
  prefs: []
  type: TYPE_NORMAL
  zh: $$(18.5)$$
- en: wt+1 = wt − γtΓt∇w Q(zt, wt). (18.5)
  id: totrans-5133
  prefs: []
  type: TYPE_NORMAL
  zh: wt+1 = wt − γtΓt∇w Q(zt, wt). (18.5)
- en: Unfortunately, this modification does not reduce the stochastic noise and therefore
    does not significantly improve the variance of wt. Although constants are improved,
    the expectation of the residual error still decreases like t
  id: totrans-5134
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这一修改并不能减少随机噪声，因此并没有显著改善wt的方差。尽管常数有所改善，残差误差的期望仍然像t一样下降。
- en: −1, that is, E(ρ) ∼ t
  id: totrans-5135
  prefs: []
  type: TYPE_NORMAL
  zh: −1，也就是说，E(ρ) ∼ t
- en: −1 at best, (e.g. [1], appendix).
  id: totrans-5136
  prefs: []
  type: TYPE_NORMAL
  zh: −1为最佳，（例如，[1]，附录）。
- en: Therefore, as an optimization algorithm, stochastic gradient descent is asymptotically
    *much slower* than a typical batch algorithm. However, this is not the whole story.
    . .
  id: totrans-5137
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作为一种优化算法，随机梯度下降在渐近上*要比*典型的批量算法慢得多。然而，这并不是全部故事……
- en: 18.3 When To Use Stochastic Gradient Descent?
  id: totrans-5138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.3 什么时候使用随机梯度下降？
- en: During the last decade, the data sizes have grown faster than the speed of processors.
    In this context, the capabilities of statistical machine learning methods is limited
    by the computing time rather than the sample size. The analysis presented in this
    section shows that stochastic gradient descent performs very well in this context.
  id: totrans-5139
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，数据规模增长的速度超过了处理器的速度。在这种情况下，统计机器学习方法的能力受限于计算时间而不是样本大小。本节中呈现的分析表明，在这种情况下，随机梯度下降表现非常出色。
- en: Use stochastic gradient descent
  id: totrans-5140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机梯度下降
- en: '![416_image_0.png](416_image_0.png)'
  id: totrans-5141
  prefs: []
  type: TYPE_IMG
  zh: '![416_image_0.png](416_image_0.png)'
- en: '![416_image_1.png](416_image_1.png)'
  id: totrans-5142
  prefs: []
  type: TYPE_IMG
  zh: '![416_image_1.png](416_image_1.png)'
- en: when training time is the bottleneck.
  id: totrans-5143
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练时间成为瓶颈时。
- en: 18.3.1 The Trade-Offs Of Large Scale Learning
  id: totrans-5144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.3.1 大规模学习的权衡
- en: Let f ∗ = arg minf E(f) be the best possible prediction function. Since we seek
    the prediction function from a parametrized family of functions F, let f ∗F =
    arg minf∈F E(f) be the best function in this family. Since we optimize the empirical
    risk instead of the expected risk, let fn = arg minf∈F En(f)
  id: totrans-5145
  prefs: []
  type: TYPE_NORMAL
  zh: 设f ∗ = arg minf E(f)为最佳预测函数。由于我们希望从参数化的函数族F中寻找预测函数，设f ∗F = arg minf∈F E(f)为该族中最佳的函数。由于我们优化的是经验风险而不是期望风险，设fn
    = arg minf∈F En(f)
- en: "be the empirical optimum. Since this optimization can be costly, let us stop\
    \ the algorithm when it reaches a solution ˜fn that minimizes the objective function\
    \ with a predefined accuracy En( ˜fn) < En(fn) + ρ. The excess error E = E\fE(\
    \ ˜fn) − E(f ∗)can then be decomposed in three terms [2] :"
  id: totrans-5146
  prefs: []
  type: TYPE_NORMAL
  zh: 为经验最优。由于这一优化可能代价高昂，当算法达到以预定义精度En( ˜fn) < En(fn) + ρ的目标函数最小化解˜fn时，让我们停止算法。此时过剩误差E
    = E( ˜fn) − E(f ∗)可以被分解为三项[2]：
- en: $${\cal E}\ =\ \underbrace{\mathbb{E}\big{[}E(f_{\cal F}^{*})-E(f^{*})\big{]}}_{{\cal
    E}_{\rm NP}}\ \ +\ \underbrace{\mathbb{E}\big{[}E(f_{n})-E(f_{\cal F}^{*})\big{]}}_{{\cal
    E}_{\rm out}}\ \ +\ \underbrace{\mathbb{E}\big{[}E(\bar{f}_{n})-E(f_{n})\big{]}}_{{\cal
    E}_{\rm opt}}.\tag{18.6}$$
  id: totrans-5147
  prefs: []
  type: TYPE_NORMAL
  zh: $${\cal E}\ =\ \underbrace{\mathbb{E}\big{[}E(f_{\cal F}^{*})-E(f^{*})\big{]}}_{{\cal
    E}_{\rm NP}}\ \ +\ \underbrace{\mathbb{E}\big{[}E(f_{n})-E(f_{\cal F}^{*})\big{]}}_{{\cal
    E}_{\rm out}}\ \ +\ \underbrace{\mathbb{E}\big{[}E(\bar{f}_{n})-E(f_{n})\big{]}}_{{\cal
    E}_{\rm opt}}.\tag{18.6}$$
- en: "- The approximation error Eapp = E\fE(f ∗F ) − E(f ∗) measures how closely\
    \ functions in F can approximate the optimal solution f ∗. The approximation error\
    \ can be reduced by choosing a larger family of functions."
  id: totrans-5148
  prefs: []
  type: TYPE_NORMAL
  zh: '- 近似误差Eapp = E(f ∗F ) − E(f ∗)衡量的是F中的函数与最优解f ∗的接近程度。通过选择更大的函数族，可以减少近似误差。'
- en: '- The estimation error Eest = E'
  id: totrans-5149
  prefs: []
  type: TYPE_NORMAL
  zh: '- 估计误差Eest = E'
- en: E(fn) − E(f ∗F )
  id: totrans-5150
  prefs: []
  type: TYPE_NORMAL
  zh: E(fn) − E(f ∗F )
- en: measures the effect of minimizing the empirical risk En(f) instead of the expected
    risk E(f). The estimation error can be reduced by choosing a smaller family of
    functions or by increasing the size of the training set.
  id: totrans-5151
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量的是最小化经验风险En(f)而不是期望风险E(f)的影响。通过选择更小的函数族或增加训练集的大小，可以减少估计误差。
- en: "- The optimization error Eopt = E\fE( ˜fn) − E(fn)measures the impact of the\
    \ approximate optimization on the expected risk. The optimization error can be\
    \ reduced by running the optimizer longer. The additional computing time depends\
    \ of course on the family of function and on the size of the training set."
  id: totrans-5152
  prefs: []
  type: TYPE_NORMAL
  zh: '- 优化误差Eopt = E( ˜fn) − E(fn)衡量的是近似优化对期望风险的影响。通过更长时间运行优化器可以减少优化误差。当然，额外的计算时间依赖于函数族和训练集的大小。'
- en: Given constraints on the maximal computation time Tmax and the maximal training
    set size nmax, this decomposition outlines a trade-off involving the size of the
    family of functions F, the optimization accuracy ρ, and the number of examples
    n effectively processed by the optimization algorithm.
  id: totrans-5153
  prefs: []
  type: TYPE_NORMAL
  zh: 在最大计算时间Tmax和最大训练集大小nmax的约束下，这一分解概述了涉及函数族F的大小、优化精度ρ和优化算法有效处理的样本数n之间的权衡。
- en: $$\min_{\mathcal{F},\rho,n}\ \ \mathcal{E}=\mathcal{E}_{\rm app}+\mathcal{E}_{\rm
    est}+\mathcal{E}_{\rm opt}\ \ \ \ \text{subject to}\ \left\{\begin{matrix}n\leq
    n_{\max}\\ T(\mathcal{F},\rho,n)\leq T_{\max}\end{matrix}\right.\tag{18.7}$$
  id: totrans-5154
  prefs: []
  type: TYPE_NORMAL
  zh: $$\min_{\mathcal{F},\rho,n}\ \ \mathcal{E}=\mathcal{E}_{\rm app}+\mathcal{E}_{\rm
    est}+\mathcal{E}_{\rm opt}\ \ \ \ \text{subject to}\ \left\{\begin{matrix}n\leq
    n_{\max}\\ T(\mathcal{F},\rho,n)\leq T_{\max}\end{matrix}\right.\tag{18.7}$$
- en: 'Two cases should be distinguished:'
  id: totrans-5155
  prefs: []
  type: TYPE_NORMAL
  zh: 应该区分两种情况：
- en: '- *Small-scale learning problems* are first constrained by the maximal number
    of examples. Since the computing time is not an issue, we can reduce the optimization
    error Eopt to insignificant levels by choosing ρ arbitrarily small, and we can
    minimize the estimation error Eest by choosing n = nmax. We then recover the approximation-estimation
    trade-off that has been widely studied in statistics and in learning theory.'
  id: totrans-5156
  prefs: []
  type: TYPE_NORMAL
  zh: '- *小规模学习问题*首先受到最大样本数的限制。由于计算时间不是问题，我们可以通过选择ρ为任意小值来将优化误差Eopt减少到微不足道的水平，并通过选择n
    = nmax来最小化估计误差Eest。我们因此恢复了在统计学和学习理论中广泛研究的近似-估计权衡。'
- en: '- *Large-scale learning problems* are constrained by the maximal computing
    time, usually because the supply of training examples is very large. Approximate
    optimization can achieve better expected risk because more training examples can
    be processed during the allowed time. The specifics depend on the computational
    properties of the chosen optimization algorithm.'
  id: totrans-5157
  prefs: []
  type: TYPE_NORMAL
  zh: '- *大规模学习问题*受到最大计算时间的限制，通常是因为训练样本的供应非常庞大。近似优化可以实现更好的期望风险，因为可以在允许的时间内处理更多的训练样本。具体情况依赖于所选择的优化算法的计算属性。'
- en: 18.3.2 Asymptotic Analysis Of The Large-Scale Case
  id: totrans-5158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.3.2 大规模案例的渐进分析
- en: Solving (18.7) in the asymptotic regime amounts to ensuring that the terms of
    the decomposition (18.6) decrease at similar rates. Since the asymptotic convergence
    rate of the excess error (18.6) is the convergence rate of its slowest term, the
    computational effort required to make a term decrease faster would be wasted.
  id: totrans-5159
  prefs: []
  type: TYPE_NORMAL
  zh: 在渐进条件下求解(18.7)等同于确保分解(18.6)的各项以相似的速率下降。由于过剩误差(18.6)的渐进收敛速率是其最慢项的收敛速率，因此使某一项更快下降所需的计算努力将会浪费。
- en: For simplicity, we assume in this section that the Vapnik-Chervonenkis dimensions
    of the families of functions F are bounded by a common constant. We also assume
    that the optimization algorithms satisfy all the assumptions required to achieve
    the convergence rates discussed in section 18.2. Similar analyses can be carried
    out for specific algorithms under weaker assumptions (e.g. [22]).
  id: totrans-5160
  prefs: []
  type: TYPE_NORMAL
  zh: 为简化起见，我们在本节中假设函数族F的Vapnik-Chervonenkis维数由一个共同常数界定。我们还假设优化算法满足实现第18.2节中讨论的收敛速率所需的所有假设。针对特定算法在较弱假设下也可以进行类似分析（例如[22]）。
- en: A simple application of the uniform convergence results of [25] gives then the
    upper bound
  id: totrans-5161
  prefs: []
  type: TYPE_NORMAL
  zh: '[25]的均匀收敛结果的一个简单应用则给出了上界。'
- en: $${\mathcal{E}}\ =\ {\mathcal{E}}_{\mathrm{app}}+{\mathcal{E}}_{\mathrm{est}}+{\mathcal{E}}_{\mathrm{opt}}\
    =\ {\mathcal{E}}_{\mathrm{app}}\ +\ {\mathcal{O}}\left({\sqrt{\frac{\log n}{n}}}\
    +\rho\right).$$
  id: totrans-5162
  prefs: []
  type: TYPE_NORMAL
  zh: $${\mathcal{E}}\ =\ {\mathcal{E}}_{\mathrm{app}}+{\mathcal{E}}_{\mathrm{est}}+{\mathcal{E}}_{\mathrm{opt}}\
    =\ {\mathcal{E}}_{\mathrm{app}}\ +\ {\mathcal{O}}\left({\sqrt{\frac{\log n}{n}}}\
    +\rho\right).$$
- en: Unfortunately the convergence rate of this bound is too pessimistic. Faster
    convergence occurs when the loss function has strong convexity properties [9]
    or when the data distribution satisfies certain assumptions [24]. The equivalence
  id: totrans-5163
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个界限的收敛速率过于悲观。当损失函数具有强凸性属性时 [9]，或者当数据分布满足某些假设时 [24]，会发生更快的收敛。
- en: $${\cal E}\ =\ {\cal E}_{\rm app}+{\cal E}_{\rm est}+{\cal E}_{\rm opt}\ \sim\
    {\cal E}_{\rm app}+\left(\frac{\log n}{n}\right)^{\alpha}+\rho\,,\quad\mbox{for
    some}\alpha\in\left[\frac{1}{2},1\right]\,,\tag{18.8}$$
  id: totrans-5164
  prefs: []
  type: TYPE_NORMAL
  zh: $${\cal E}\ =\ {\cal E}_{\rm app}+{\cal E}_{\rm est}+{\cal E}_{\rm opt}\ \sim\
    {\cal E}_{\rm app}+\left(\frac{\log n}{n}\right)^{\alpha}+\rho\,,\quad\mbox{对于某些}\alpha\in\left[\frac{1}{2},1\right]\,,\tag{18.8}$$
- en: provides a more realistic view of the asymptotic behavior of the excess error
    (e.g. [13, 4]). Since the three components of the excess error should decrease
    at the same rate, the solution of the trade-off problem (18.7) must then obey
    the multiple asymptotic equivalences
  id: totrans-5165
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了关于超额错误渐近行为的更现实的观点（例如 [13, 4]）。由于超额错误的三个分量应该以相同的速率减少，因此权衡问题的解（18.7）必须遵循多个渐近等价关系。
- en: $${\cal E}\ \sim\ {\cal E}_{\rm app}\ \sim\ {\cal E}_{\rm est}\ \sim\ {\cal
    E}_{\rm opt}\ \sim\ \left(\frac{\log n}{n}\right)^{\alpha}\ \sim\ \rho.\tag{18.9}$$
  id: totrans-5166
  prefs: []
  type: TYPE_NORMAL
  zh: $${\cal E}\ \sim\ {\cal E}_{\rm app}\ \sim\ {\cal E}_{\rm est}\ \sim\ {\cal
    E}_{\rm opt}\ \sim\ \left(\frac{\log n}{n}\right)^{\alpha}\ \sim\ \rho.\tag{18.9}$$
- en: Table 18.2 summarizes the asymptotic behavior of the four gradient algorithms
    described in section 18.2. The first three rows list the computational cost of
    each iteration, the number of iterations required to reach an optimization accuracy
    ρ, and the corresponding computational cost. The last row provides a more interesting
    measure for large scale machine learning purposes. Assuming we operate at the
    optimum of the approximation-estimation-optimization tradeoff (18.7), this line
    indicates the computational cost necessary to reach a predefined value of the
    excess error, and therefore of the expected risk. This is computed by applying
    the equivalences (18.9) to eliminate the variables n and ρ from the third row
    results.2 Although the stochastic gradient algorithms, SGD and 2SGD, are clearly
    the worst optimization algorithms (third row), they need less time than the
  id: totrans-5167
  prefs: []
  type: TYPE_NORMAL
  zh: 表 18.2 总结了第 18.2 节中描述的四种梯度算法的渐近行为。前三行列出了每次迭代的计算成本、达到优化精度 ρ 所需的迭代次数以及相应的计算成本。最后一行提供了一个对于大规模机器学习目的更有趣的度量。假设我们在近似-估计-优化权衡的最优点（18.7）进行操作，这一行表示达到预定义的超额错误值及其期望风险所需的计算成本。这是通过应用等式
    (18.9) 消去第三行结果中的变量 n 和 ρ 计算得出的。虽然随机梯度算法 SGD 和 2SGD 显然是最差的优化算法（第三行），但它们所需的时间少于
- en: 2 Note that ε1/α ∼ log(n)/n implies both α−1 log ε ∼ log log(n) − log(n) ∼ −
    log(n)
  id: totrans-5168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 ε1/α ∼ log(n)/n 意味着 α−1 log ε ∼ log log(n) − log(n) ∼ − log(n)
- en: and n ∼ ε−1/α log n. Replacing log(n) in the latter gives n ∼ ε−1/α log(1/ε).
  id: totrans-5169
  prefs: []
  type: TYPE_NORMAL
  zh: 且 n ∼ ε−1/α log n。将后者中的 log(n) 替换为 n ∼ ε−1/α log(1/ε)。
- en: 'Table 18.2. Asymptotic equivalents for various optimization algorithms: gradient
    descent (GD, eq. 18.2), second order gradient descent (2GD, eq. 18.3), stochastic
    gradient descent (SGD, eq. 18.4), and second order stochastic gradient descent
    (2SGD, eq. 18.5). Although they are the worst optimization algorithms, SGD and
    2SGD achieve the fastest convergence speed on the expected risk. They differ only
    by constant factors not shown in this table, such as condition numbers and weight
    vector dimension.'
  id: totrans-5170
  prefs: []
  type: TYPE_NORMAL
  zh: 表 18.2 各种优化算法的渐近等价：梯度下降 (GD, eq. 18.2)、二阶梯度下降 (2GD, eq. 18.3)、随机梯度下降 (SGD, eq.
    18.4) 和二阶随机梯度下降 (2SGD, eq. 18.5)。尽管它们是最差的优化算法，SGD 和 2SGD 在期望风险上实现了最快的收敛速度。它们之间的差异仅在于未在此表中显示的常数因子，例如条件数和权重向量维度。
- en: '| GD                         | 2GD       | SGD           | 2SGD        |     |     |'
  id: totrans-5171
  prefs: []
  type: TYPE_TB
  zh: '| GD                         | 2GD       | SGD           | 2SGD        |     |     |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-5172
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Time per iteration :       | n         | n             | 1           | 1   |     |'
  id: totrans-5173
  prefs: []
  type: TYPE_TB
  zh: '| 每次迭代的时间 :       | n         | n             | 1           | 1   |     |'
- en: '| Iterations to accuracy ρ : | log 1 ρ   | log log 1 ρ   | 1/ρ         | 1/ρ
    |     |'
  id: totrans-5174
  prefs: []
  type: TYPE_TB
  zh: '| 达到精度 ρ 的迭代次数 : | log 1 ρ   | log log 1 ρ   | 1/ρ         | 1/ρ |     |'
- en: '| Time to accuracy ρ :       | n log 1 ρ | n log log 1 ρ | 1/ρ         | 1/ρ
    |     |'
  id: totrans-5175
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 ρ 的时间：       | n log 1 ρ | n log log 1 ρ | 1/ρ         | 1/ρ |     |'
- en: '| Time to excess error E :   | 1         |               |             |     |     |'
  id: totrans-5176
  prefs: []
  type: TYPE_TB
  zh: '| 过量误差 E 的时间：      | 1         |               |             |     |     |'
- en: '| E1/α                       | log2 1 E  | 1             |             |     |     |'
  id: totrans-5177
  prefs: []
  type: TYPE_TB
  zh: '| E1/α                       | log2 1 E  | 1             |             |     |     |'
- en: '|                            | E1/α      | log 1 E       | log log 1 E | 1/E
    | 1/E |'
  id: totrans-5178
  prefs: []
  type: TYPE_TB
  zh: '|                            | E1/α      | log 1 E       | log log 1 E | 1/E
    | 1/E |'
- en: other algorithms to reach a predefined expected risk (fourth row). Therefore,
    in the large scale setup, that is, when the limiting factor is the computing time
    rather than the number of examples, the stochastic learning algorithms performs
    asymptotically better !
  id: totrans-5179
  prefs: []
  type: TYPE_NORMAL
  zh: '其他算法达到预定义的期望风险（第四行）。因此，在大规模设置中，也就是说，当限制因素是计算时间而不是示例数量时，随机学习算法的表现渐近优越！ '
- en: 18.4 General Recommendations
  id: totrans-5180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.4 一般建议
- en: The rest of this contribution provides a series of recommendations for using
    stochastic gradient algorithms. Although some of these recommendations seem trivial,
    experience has shown again and again how easily they can be overlooked.
  id: totrans-5181
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分提供了一系列使用随机梯度算法的建议。尽管其中一些建议似乎微不足道，但经验一再表明它们很容易被忽视。
- en: 18.4.1 Preparing The Data Randomly Shuffle The Training Examples.
  id: totrans-5182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.4.1 准备数据 随机打乱训练示例。
- en: Although the theory calls for picking examples randomly, it is usually faster
    to zip sequentially through the training set. But this does not work if the examples
    are grouped by class or come in a particular order. Randomly shuffling the examples
    eliminates this source of problems. Section 1.4.2 provides an additional discussion.
  id: totrans-5183
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然理论上要求随机选择示例，但通常顺序遍历训练集会更快。不过，如果示例按类别分组或按特定顺序出现，这种方法就不适用了。随机打乱示例可以消除这一问题。第1.4.2节提供了更多讨论。
- en: Use Preconditioning Techniques.
  id: totrans-5184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预处理技术。
- en: Stochastic gradient descent is a first-order algorithm and therefore suffers
    dramatically when it reaches an area where the Hessian is ill-conditioned. Fortunately,
    many simple preprocessing techniques can vastly improve the situation. Sections
    1.4.3 and 1.5.3 provide many useful tips.
  id: totrans-5185
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降是一种一阶算法，因此在达到海森矩阵条件不良的区域时会遭受重大损失。幸运的是，许多简单的预处理技术可以大大改善这种情况。第1.4.3节和第1.5.3节提供了许多有用的建议。
- en: Monitor both the training cost and the validation error.
  id: totrans-5186
  prefs: []
  type: TYPE_NORMAL
  zh: 监控训练成本和验证误差。
- en: Since stochastic gradient descent is useful when the training time is the primary
    concern, we can spare some training examples to build a decent validation set.
    It is important to periodically evaluate the validation error during training
    because we can stop training when we observe that the validation error has not
    improved in a long time.
  id: totrans-5187
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机梯度下降在训练时间为主要关注点时非常有用，我们可以保留一些训练示例以建立一个合理的验证集。在训练过程中定期评估验证误差很重要，因为我们可以在观察到验证误差长时间没有改善时停止训练。
- en: It is also important to periodically compute the training cost because stochastic
    gradient descent is an iterative optimization algorithm. Since the training cost
    is exactly what the algorithm seeks to optimize, the training cost should be generally
    decreasing.
  id: totrans-5188
  prefs: []
  type: TYPE_NORMAL
  zh: 定期计算训练成本也很重要，因为随机梯度下降是一种迭代优化算法。由于训练成本正是算法寻求优化的目标，因此训练成本应该普遍下降。
- en: 'A good approach is to repeat the following operations:'
  id: totrans-5189
  prefs: []
  type: TYPE_NORMAL
  zh: 一种好的方法是重复以下操作：
- en: 1. Zip once through the shuffled training set and perform the stochastic gradient
    descent updates (18.4).
  id: totrans-5190
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 在打乱后的训练集上循环一次，并执行随机梯度下降更新（18.4）。
- en: 2. With an additional loop over the training set, compute the training cost.
  id: totrans-5191
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 通过对训练集的额外循环，计算训练成本。
- en: Training cost here means the criterion that the algorithm seeks to optimize.
    You can take advantage of the loop to compute other metrics, but the training
    cost is the one to watch 3. With an additional loop over the validation set, to
    compute the validation set error. Error here means the performance measure of
    interest, such as the classification error. You can also take advantage of this
    loop to cheaply compute other metrics.
  id: totrans-5192
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的训练成本指的是算法寻求优化的标准。你可以利用这个循环计算其他指标，但训练成本是需要关注的重点。3. 通过对验证集的额外循环，计算验证集误差。此处的误差指的是关注的性能指标，例如分类误差。你也可以利用这个循环便宜地计算其他指标。
- en: Computing the training cost and the validation error represent a significant
    computational effort because it requires additional passes over the training and
    validation data. But this beats running blind.
  id: totrans-5193
  prefs: []
  type: TYPE_NORMAL
  zh: 计算训练成本和验证误差需要大量的计算，因为这需要对训练和验证数据进行额外的遍历。但这总比盲目运行要好。
- en: Check The Gradients Using Finite Differences.
  id: totrans-5194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用有限差分检查梯度。
- en: When the computation of the gradients is slightly incorrect, stochastic gradient
    descent often works slowly and erratically. This has led many to believe that
    slow and erratic is the normal operation of the algorithm.
  id: totrans-5195
  prefs: []
  type: TYPE_NORMAL
  zh: 当梯度的计算稍有不准确时，随机梯度下降通常表现缓慢且不稳定。这使得许多人认为缓慢和不稳定是该算法的正常操作。
- en: During the last twenty years, I have often been approached for advice in setting
    the learning rates γt of some rebellious stochastic gradient descent program.
  id: totrans-5196
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的二十年里，我经常被咨询有关某些叛逆的随机梯度下降程序的学习率 γt 的设定建议。
- en: 'My advice is to forget about the learning rates and check that the gradients
    are computed correctly. This reply is biased because people who compute the gradients
    correctly quickly find that setting small enough learning rates is easy. Those
    who ask usually have incorrect gradients. Carefully checking each line of the
    gradient computation code is the wrong way to check the gradients. Use finite
    differences:'
  id: totrans-5197
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是忘记学习率，检查梯度是否计算正确。这个回答有偏见，因为那些正确计算梯度的人很快会发现设置足够小的学习率是容易的。询问的人通常梯度计算不正确。仔细检查梯度计算代码的每一行是检查梯度的错误方式。使用有限差分：
- en: 1. Pick an example z.
  id: totrans-5198
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 选择一个示例z。
- en: 2. Compute the loss Q(z,w) for the current w.
  id: totrans-5199
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 计算当前 w 的损失 Q(z,w)。
- en: 3. Compute the gradient g = ∇w Q(z,w).
  id: totrans-5200
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 计算梯度 g = ∇w Q(z,w)。
- en: 4. Apply a slight perturbation w = w +δ. For instance, change a single weight
    by a small increment, or use δ = −γg with γ small enough.
  id: totrans-5201
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 应用轻微的扰动 w = w + δ。例如，通过一个小增量改变单个权重，或者使用 δ = −γg，且 γ 足够小。
- en: 5. Compute the new loss Q(z,w) and verify that Q(z,w) ≈ Q(z,w) + δg .
  id: totrans-5202
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 计算新的损失 Q(z,w)，并验证 Q(z,w) ≈ Q(z,w) + δg。
- en: This process can be automated and should be repeated for many examples z, many
    perturbations δ, and many initial weights w. Flaws in the gradient computation
    tend to only appear when peculiar conditions are met. It is not uncommon to discover
    such bugs in SGD code that has been quietly used for years.
  id: totrans-5203
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可以自动化，并且应该在许多示例 z、许多扰动 δ 和许多初始权重 w 上重复进行。梯度计算中的缺陷往往只在特定条件下出现。在已经安静使用了多年的
    SGD 代码中发现此类错误并不罕见。
- en: Experiment with the learning rates γt using a small sample of the training set.
  id: totrans-5204
  prefs: []
  type: TYPE_NORMAL
  zh: 使用小样本的训练集来实验学习率 γt。
- en: The mathematics of stochastic gradient descent are amazingly independent of
    the training set size. In particular, the asymptotic SGD convergence rates [14]
    are independent from the sample size. Therefore, assuming the gradients are correct,
    the best way to determine the correct learning rates is to perform experiments
    using a small but representative sample of the training set. Because the sample
    is small, it is also possible to run traditional optimization algorithms on this
    same dataset in order to obtain reference point and set the training cost target.
  id: totrans-5205
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降的数学原理与训练集大小惊人地独立。特别是，渐近的 SGD 收敛速率与样本大小无关。因此，假设梯度是正确的，确定正确学习率的最佳方法是使用一个小而具有代表性的训练集样本进行实验。由于样本较小，因此也可以在同一数据集上运行传统优化算法，以获得参考点并设定训练成本目标。
- en: When the algorithm performs well on the training cost of the small dataset,
    keep the same learning rates, and let it soldier on the full training set. Expect
    the validation performance to plateau after a number of epochs roughly comparable
    to the number of epochs needed to reach this point on the small training set.
  id: totrans-5206
  prefs: []
  type: TYPE_NORMAL
  zh: 当算法在小数据集的训练成本上表现良好时，保持相同的学习率，并让其在完整训练集上继续进行。预计验证性能将在经过的 epochs 数量上达到一个平稳状态，这个数量大致与在小训练集上达到该点所需的
    epochs 数量相当。
- en: 18.5 Linear Models with L2 Regularization This section provides specific recommendations
    for training large linear models with L2 regularization. The training objective
    of such models has the form
  id: totrans-5207
  prefs: []
  type: TYPE_NORMAL
  zh: 18.5 具有 L2 正则化的线性模型 本节为训练大型线性模型提供了具体建议，这些模型具有 L2 正则化。此类模型的训练目标具有以下形式
- en: $$E_{n}(w)\ =\ {\frac{\lambda}{2}}\|w\|^{2}+{\frac{1}{n}}\sum_{i=1}^{n}\ell(y_{t}w
    x_{t})$$
  id: totrans-5208
  prefs: []
  type: TYPE_NORMAL
  zh: $$E_{n}(w)\ =\ {\frac{\lambda}{2}}\|w\|^{2}+{\frac{1}{n}}\sum_{i=1}^{n}\ell(y_{t}w
    x_{t})$$
- en: $$(18.10)$$
  id: totrans-5209
  prefs: []
  type: TYPE_NORMAL
  zh: $$(18.10)$$
- en: $$(18.11)$$
  id: totrans-5210
  prefs: []
  type: TYPE_NORMAL
  zh: $$(18.11)$$
- en: (ytwxt) (18.10)
  id: totrans-5211
  prefs: []
  type: TYPE_NORMAL
  zh: (ytwxt) (18.10)
- en: where yt = ±1, and where the function (m) is convex. The corresponding stochastic
    gradient update is then obtained by approximating the derivative of the sum by
    the derivative of the loss with respect to a single example
  id: totrans-5212
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 yt = ±1，并且函数 (m) 是凸的。相应的随机梯度更新是通过将和的导数近似为关于单个示例的损失的导数而获得的。
- en: $$w_{t+1}=(1-\gamma_{t}\lambda)w_{t}-\gamma_{t}y_{t}x_{t}\ell^{\prime}(y_{t}w_{t}x_{t})$$
  id: totrans-5213
  prefs: []
  type: TYPE_NORMAL
  zh: $$w_{t+1}=(1-\gamma_{t}\lambda)w_{t}-\gamma_{t}y_{t}x_{t}\ell^{\prime}(y_{t}w_{t}x_{t})$$
- en: wt+1 = (1 − γtλ)wt − γtytxt(ytwtxt) (18.11)
  id: totrans-5214
  prefs: []
  type: TYPE_NORMAL
  zh: wt+1 = (1 − γtλ)wt − γtytxt(ytwtxt) (18.11)
- en: 'Examples:'
  id: totrans-5215
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '- Support Vector Machines (SVM) use the non differentiable *hinge loss* [5]
    :'
  id: totrans-5216
  prefs: []
  type: TYPE_NORMAL
  zh: '- 支持向量机 (SVM) 使用不可微的 *铰链损失* [5]：'
- en: $$\ell(m)=\operatorname*{max}\{0,1-m\}\,.$$
  id: totrans-5217
  prefs: []
  type: TYPE_NORMAL
  zh: $$\ell(m)=\operatorname*{max}\{0,1-m\}\,.$$
- en: '- It is often more convenient in the linear case to use the *log-loss*:'
  id: totrans-5218
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在线性情况下，使用 *对数损失* 通常更方便：'
- en: $$\ell(m)=\log(1+e^{-m})\,.$$
  id: totrans-5219
  prefs: []
  type: TYPE_NORMAL
  zh: $$\ell(m)=\log(1+e^{-m})\,.$$
- en: 'The differentiable log-loss is more suitable for the gradient algorithms discussed
    here. This choice leads to a logistic regression algorithm: probability estimates
    can be derived using the logistic function:'
  id: totrans-5220
  prefs: []
  type: TYPE_NORMAL
  zh: 可微的对数损失更适合于这里讨论的梯度算法。这一选择导致了一个逻辑回归算法：概率估计可以通过逻辑函数得出：
- en: $$P(y=+1|x)\approx{\frac{1}{1+e^{-w x}}}$$
  id: totrans-5221
  prefs: []
  type: TYPE_NORMAL
  zh: $$P(y=+1|x)\approx{\frac{1}{1+e^{-w x}}}$$
- en: '- All statistical models with linear parametrization are in fact amenable to
    stochastic gradient descent, using the log-likelihood of the model as the loss
    function Q(z,w). For instance, results for Conditional Random Fields (CRF) [8]
    are reported in Sec. 18.5.4.'
  id: totrans-5222
  prefs: []
  type: TYPE_NORMAL
  zh: '- 所有具有线性参数化的统计模型实际上都适合使用随机梯度下降，使用模型的对数似然作为损失函数 Q(z,w)。例如，条件随机场 (CRF) [8] 的结果在第
    18.5.4 节中报告。'
- en: 18.5.1 Sparsity
  id: totrans-5223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.5.1 稀疏性
- en: Leverage the sparsity of the training examples {xt}.
  id: totrans-5224
  prefs: []
  type: TYPE_NORMAL
  zh: 利用训练样本 {xt} 的稀疏性。
- en: '- Represent wt as a product stWt where st ∈ IR.'
  id: totrans-5225
  prefs: []
  type: TYPE_NORMAL
  zh: 将 wt 表示为 stWt 的乘积，其中 st ∈ IR。
- en: The training examples often are very high dimensional vectors with only a few
    non zero coefficients. The stochastic gradient update (18.11)
  id: totrans-5226
  prefs: []
  type: TYPE_NORMAL
  zh: 训练样本通常是高维向量，只有少数非零系数。随机梯度更新 (18.11)
- en: $$w_{t+1}=(1-\gamma_{t}\lambda)w_{t}-\gamma_{t}y_{t}x_{t}\ell^{\prime}(y_{t}w_{t}x_{t})$$
  id: totrans-5227
  prefs: []
  type: TYPE_NORMAL
  zh: $$w_{t+1}=(1-\gamma_{t}\lambda)w_{t}-\gamma_{t}y_{t}x_{t}\ell^{\prime}(y_{t}w_{t}x_{t})$$
- en: is then inconvenient because it first rescales all coefficients of vector w
    by factor
  id: totrans-5228
  prefs: []
  type: TYPE_NORMAL
  zh: 这在最初缩放向量 w 的所有系数时是不方便的。
- en: (1−γtλ). In contrast, the rest of the update only involves the weight coefficients
    corresponding to a nonzero coefficient in the pattern xt.
  id: totrans-5229
  prefs: []
  type: TYPE_NORMAL
  zh: (1−γtλ)。相反，更新的其余部分仅涉及模式 xt 中对应于非零系数的权重系数。
- en: 'Expressing the vector wt as the product stWt, where s is a scalar, provides
    a workaround [21]. The stochastic gradient update (18.11) can then be divided
    into operations whose complexity scales with the number of nonzero terms in xt:'
  id: totrans-5230
  prefs: []
  type: TYPE_NORMAL
  zh: 将向量 wt 表示为乘积 stWt，其中 s 是标量，提供了一个解决方案 [21]。随机梯度更新 (18.11) 可以分解为复杂度与 xt 中非零项数量相关的操作：
- en: $$\begin{array}{c}{{g_{t}=\ell^{\prime}(y_{t}s_{t}W_{t}x_{t})\,,}}\\ {{s_{t+1}=(1-\gamma_{t}\lambda)s_{t}\,,}}\\
    {{W_{t+1}=W_{t}-\gamma_{t}y_{t}g_{t}x_{t}/s_{t+1}\,.}}\end{array}$$
  id: totrans-5231
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{c}{{g_{t}=\ell^{\prime}(y_{t}s_{t}W_{t}x_{t})\,,}}\\ {{s_{t+1}=(1-\gamma_{t}\lambda)s_{t}\,,}}\\
    {{W_{t+1}=W_{t}-\gamma_{t}y_{t}g_{t}x_{t}/s_{t+1}\,.}}\end{array}$$
- en: 18.5.2 Learning Rates
  id: totrans-5232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.5.2 学习率
- en: Use learning rates of the form γt = γ0 **(1 +** γ0λt)−1
  id: totrans-5233
  prefs: []
  type: TYPE_NORMAL
  zh: 使用形式为 γt = γ0 **(1 +** γ0λt)−1 的学习率。
- en: '- Determine the best γ0 using a small training data sample.'
  id: totrans-5234
  prefs: []
  type: TYPE_NORMAL
  zh: '- 使用小的训练数据样本确定最佳的 γ0。'
- en: When the Hessian matrix of the cost function at the optimum is strictly positive,
    the best convergence speed is achieved using learning rates of the form (λmint)−1
    where λmin is the smallest eigenvalue of the Hessian [14]. The theoretical analysis
    also shows that overestimating λmin by more than a factor two leads to very slow
    convergence. Although we do not know the exact value of λmin, the L2 regularization
    term in the training objective function means that λmin ≥ λ.
  id: totrans-5235
  prefs: []
  type: TYPE_NORMAL
  zh: 当成本函数的海森矩阵在最优点处严格正定时，使用形式为 (λmint)−1 的学习率可以实现最佳收敛速度，其中 λmin 是海森矩阵的最小特征值[14]。理论分析还表明，超过两倍的
    λmin 估计会导致非常缓慢的收敛。尽管我们不知道 λmin 的确切值，但训练目标函数中的 L2 正则化项意味着 λmin ≥ λ。
- en: Therefore we can safely use learning rates that asymptotically decrease like
  id: totrans-5236
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以安全地使用渐近下降的学习率。
- en: (λt)−1.
  id: totrans-5237
  prefs: []
  type: TYPE_NORMAL
  zh: (λt)−1。
- en: Unfortunately, simply using γt = (λt)−1 leads to very large learning rates in
    the beginning of the optimization. It is possible to use an additional projection
    step [21] to contain the damage until the learning rates reach reasonable values.
  id: totrans-5238
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，简单使用γt = (λt)−1会导致优化开始时学习率非常大。可以使用额外的投影步骤 [21] 来抑制损害，直到学习率达到合理值。
- en: However it is simply better to start with reasonable learning rates. The formula
    γt = γ0(1 + γ0λt)−1 ensures that the learning rates γt start from a predefined
    value γ0 and asymptotically decrease like (λt)−1.
  id: totrans-5239
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从合理的学习率开始更为简单。公式γt = γ0(1 + γ0λt)−1确保学习率γt从预定义值γ0开始，并渐近减少如(λt)−1。
- en: The most robust approach is to determine the best γ0 as explained earlier, using
    a small sample of the training set. This is justified because the asymptotic SGD
    convergence rates [14] are independent from the sample size. In order to make
    the method more robust, I often use a γ0 slightly smaller than the best value
    observed on the small training sample.
  id: totrans-5240
  prefs: []
  type: TYPE_NORMAL
  zh: 最稳健的方法是如前所述，使用小样本的训练集来确定最佳γ0。这是合理的，因为渐近SGD的收敛速度 [14] 与样本大小无关。为了使方法更稳健，我通常使用略小于在小训练样本上观察到的最佳值的γ0。
- en: Such learning rates have been found to be effective in situations that far exceed
    the scope of this particular analysis. For instance, they work well with nondifferentiable
    loss functions such as the hinge loss [21]. They also work well when one adds
    an unregularized bias term to the model. However it is then wise to use smaller
    learning rates for the bias term itself.
  id: totrans-5241
  prefs: []
  type: TYPE_NORMAL
  zh: 这种学习率在远超出本分析范围的情况下被发现是有效的。例如，它们在非可微损失函数（如铰链损失 [21]）中表现良好。当向模型添加未正则化的偏差项时，它们也能很好地工作。然而，此时对于偏差项本身使用较小的学习率是明智的。
- en: 18.5.3 Averaged Stochastic Gradient Descent
  id: totrans-5242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.5.3 平均随机梯度下降
- en: The *stochastic gradient descent!averaged SGD* (ASGD) algorithm [19] performs
    the normal stochastic gradient update (18.4) and computes the average
  id: totrans-5243
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机梯度下降！平均SGD*（ASGD）算法 [19] 执行正常的随机梯度更新（18.4）并计算平均值。'
- en: $$\bar{w}_{t}=\frac{1}{t-t_{0}}\ \sum_{i=t_{0}+1}^{t}w_{t}\,.$$
  id: totrans-5244
  prefs: []
  type: TYPE_NORMAL
  zh: $$\bar{w}_{t}=\frac{1}{t-t_{0}}\ \sum_{i=t_{0}+1}^{t}w_{t}\,.$$
- en: 'This average can be computed efficiently using a recursive formula. For instance,
    in the case of the L2 regularized training objective (18.10), the following weight
    updates implement the ASGD algorithm:'
  id: totrans-5245
  prefs: []
  type: TYPE_NORMAL
  zh: 这个平均值可以通过递归公式高效计算。例如，在L2正则化训练目标（18.10）的情况下，以下权重更新实现了ASGD算法：
- en: $$\begin{array}{l}{{w_{t+1}=(1-\gamma_{t}\lambda)w_{t}-\gamma_{t}y_{t}x_{t}\ell^{\prime}(y_{t}w_{t}x_{t})}}\\
    {{\bar{w}_{t+1}=\bar{w}_{t}+\mu_{t}(w_{t+1}-\bar{w}_{t})}}\end{array}$$
  id: totrans-5246
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{l}{{w_{t+1}=(1-\gamma_{t}\lambda)w_{t}-\gamma_{t}y_{t}x_{t}\ell^{\prime}(y_{t}w_{t}x_{t})}}\\
    {{\bar{w}_{t+1}=\bar{w}_{t}+\mu_{t}(w_{t+1}-\bar{w}_{t})}}\end{array}$$
- en: with the averaging rate
  id: totrans-5247
  prefs: []
  type: TYPE_NORMAL
  zh: 通过平均速率
- en: $$\mu_{t}=1/\mathrm{max}\{1,t-t_{0}\}\,.$$
  id: totrans-5248
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mu_{t}=1/\mathrm{max}\{1,t-t_{0}\}\,.$$
- en: When one uses learning rates γt that decrease slower than t
  id: totrans-5249
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用学习率γt时，如果其降低速度慢于t
- en: −1, the theoretical analysis of ASGD shows that the training error En( ¯wt)
    decreases like t
  id: totrans-5250
  prefs: []
  type: TYPE_NORMAL
  zh: −1，ASGD的理论分析显示训练误差En( ¯wt)减少速度与t成正比。
- en: −1 with the *optimal constant* [15]. This is as good as the second order stochastic
    gradient descent (2SGD) for a fraction of the computational cost of (18.5).
  id: totrans-5251
  prefs: []
  type: TYPE_NORMAL
  zh: −1 与*最优常数* [15]相当。这与二阶随机梯度下降（2SGD）的效果相当，计算成本却仅为（18.5）的一个小部分。
- en: Unfortunately, ASGD typically starts more slowly than the plain SGD and can
    take a long time to reach the optimal asymptotic convergence speed. Although an
    adequate choice of the learning rates helps [27], the problem worsens when the
    dimension d of the inputs xt increases. Unfortunately, there are no clear guidelines
    for selecting the time t0 that determines when we engage the averaging process.
  id: totrans-5252
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，ASGD通常启动时比简单的SGD更慢，并且可能需要很长时间才能达到最佳渐近收敛速度。尽管适当选择学习率有所帮助 [27]，但当输入xt的维度d增加时，问题会加重。不幸的是，关于选择时间t0以确定何时启动平均过程没有明确的指导方针。
- en: '![424_image_0.png](424_image_0.png)'
  id: totrans-5253
  prefs: []
  type: TYPE_IMG
  zh: '![424_image_0.png](424_image_0.png)'
- en: Similar to the trick explained in Sec. 18.5.1, there is an efficient method
    to implement averaged stochastic gradient descent for sparse training data. The
    idea is to represent the variables wt and w¯t as
  id: totrans-5254
  prefs: []
  type: TYPE_NORMAL
  zh: 与第18.5.1节中解释的技巧类似，有一种有效的方法可以对稀疏训练数据实施平均随机梯度下降。这个思路是将变量wt和w¯t表示为
- en: $$\begin{array}{l}{{w_{t}=s_{t}W_{t}}}\\ {{\bar{w}_{t}=(A_{t}+\alpha_{t}W_{t})/\beta_{t}}}\end{array}$$
  id: totrans-5255
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{l}{{w_{t}=s_{t}W_{t}}}\\ {{\bar{w}_{t}=(A_{t}+\alpha_{t}W_{t})/\beta_{t}}}\end{array}$$
- en: '![424_image_1.png](424_image_1.png)'
  id: totrans-5256
  prefs: []
  type: TYPE_IMG
  zh: '![424_image_1.png](424_image_1.png)'
- en: 'where ηt, αt and βt are scalars. The average stochastic gradient update equations
    can then be rewritten in the manner that only involve scalars or sparse operations
    [27] :'
  id: totrans-5257
  prefs: []
  type: TYPE_NORMAL
  zh: 其中ηt, αt和βt是标量。平均随机梯度更新方程可以重新写成只涉及标量或稀疏操作的形式[27]：
- en: $$g_{t}=\ell^{\prime}(y_{t}s_{t}W_{t}x_{t})\,,$$
  id: totrans-5258
  prefs: []
  type: TYPE_NORMAL
  zh: $$g_{t}=\ell^{\prime}(y_{t}s_{t}W_{t}x_{t})\,,$$
- en: st+1 = (1 − γtλ)st Wt+1 = Wt − γtytxtgt/st+1 At+1 = At + γtαtytxtgt/st+1 βt+1
    = βt/(1 − μt)
  id: totrans-5259
  prefs: []
  type: TYPE_NORMAL
  zh: st+1 = (1 − γtλ)st Wt+1 = Wt − γtytxtgt/st+1 At+1 = At + γtαtytxtgt/st+1 βt+1
    = βt/(1 − μt)
- en: αt+1 = αt + μtβt+1st+1
  id: totrans-5260
  prefs: []
  type: TYPE_NORMAL
  zh: αt+1 = αt + μtβt+1st+1
- en: 18.5.4 Experiments
  id: totrans-5261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.5.4 实验
- en: This section briefly reports experimental results illustrating the actual performance
    of SGD and ASGD on a variety of linear systems. The source code is available at
    http://leon.bottou.org/projects/sgd. All learning rates were determined as explained
    in section 18.5.2.
  id: totrans-5262
  prefs: []
  type: TYPE_NORMAL
  zh: 本节简要报告了SGD和ASGD在各种线性系统上的实际性能实验结果。源代码可在http://leon.bottou.org/projects/sgd获取。所有学习率均按照18.5.2节中解释的方式确定。
- en: Figure 18.1 reports results achieved using SGD for a linear SVM trained for
    the recognition of the CCAT category in the RCV1 dataset [10] using both the hinge
    loss and the log loss. The training set contains 781,265 documents represented
    by 47,152 relatively sparse TF/IDF features. SGD runs considerably faster than
    either the standard SVM solvers SVMLight and SVMPerf [7] or the super-linear optimization
    algorithm TRON [11].
  id: totrans-5263
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.1报告了使用SGD的结果，该线性SVM用于识别RCV1数据集中CCAT类别[10]，同时使用铰链损失和对数损失。训练集包含781,265份文档，表示为47,152个相对稀疏的TF/IDF特征。SGD运行速度显著快于标准SVM求解器SVMLight和SVMPerf[7]，或超线性优化算法TRON[11]。
- en: Figure 18.2 reports results achieved for a linear model trained on the ALPHA
  id: totrans-5264
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.2报告了在ALPHA任务上训练的线性模型的结果。
- en: task of the 2008 Pascal Large Scale Learning Challenge using the squared hinge
  id: totrans-5265
  prefs: []
  type: TYPE_NORMAL
  zh: 2008年Pascal大规模学习挑战赛的任务使用了平方铰链损失。
- en: '| Algorithm                 | Time      | Test Error   |'
  id: totrans-5266
  prefs: []
  type: TYPE_TB
  zh: '| 算法                     | 时间      | 测试误差   |'
- en: '| --- | --- | --- |'
  id: totrans-5267
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Hinge loss SVM, λ = 10−4. |           |              |'
  id: totrans-5268
  prefs: []
  type: TYPE_TB
  zh: '| 铰链损失 SVM, λ = 10−4. |           |              |'
- en: '| SVMLight                  | 23,642 s. | 6.02 %       |'
  id: totrans-5269
  prefs: []
  type: TYPE_TB
  zh: '| SVMLight                | 23,642 s. | 6.02 %       |'
- en: '| SVMPerf                   | 66 s.     | 6.03 %       |'
  id: totrans-5270
  prefs: []
  type: TYPE_TB
  zh: '| SVMPerf                 | 66 s.     | 6.03 %       |'
- en: '| SGD                       | 1.4 s.    | 6.02 %       |'
  id: totrans-5271
  prefs: []
  type: TYPE_TB
  zh: '| SGD                     | 1.4 s.    | 6.02 %       |'
- en: '| Log loss SVM, λ = 10−5.   |           |              |'
  id: totrans-5272
  prefs: []
  type: TYPE_TB
  zh: '| 对数损失 SVM, λ = 10−5. |           |              |'
- en: '| TRON (-e0.01)             | 30 s.     | 5.68 %       |'
  id: totrans-5273
  prefs: []
  type: TYPE_TB
  zh: '| TRON (-e0.01)           | 30 s.     | 5.68 %       |'
- en: '| TRON (-e0.001)            | 44 s.     | 5.70 %       |'
  id: totrans-5274
  prefs: []
  type: TYPE_TB
  zh: '| TRON (-e0.001)          | 44 s.     | 5.70 %       |'
- en: '| SGD                       | 2.3 s.    | 5.66 %       |'
  id: totrans-5275
  prefs: []
  type: TYPE_TB
  zh: '| SGD                     | 2.3 s.    | 5.66 %       |'
- en: '![425_image_0.png](425_image_0.png)'
  id: totrans-5276
  prefs: []
  type: TYPE_IMG
  zh: '![425_image_0.png](425_image_0.png)'
- en: Fig. 18.1. Results achieved with a L2 regularized linear model trained on the
    RCV1 task using both the hinge loss and the log loss. The lower half of the plot
    shows the time required by SGD and TRON to reach a predefined accuracy ρ on the
    log loss task. The upper half shows that the expected risk stops improving long
    before the super-linear optimization algorithm TRON overcomes SGD.
  id: totrans-5277
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.1. 使用铰链损失和对数损失在RCV1任务上训练的L2正则化线性模型所取得的结果。图的下半部分显示了SGD和TRON达到预定准确度ρ所需的时间。上半部分显示预期风险在超线性优化算法TRON克服SGD之前就停止改善。
- en: '![425_image_1.png](425_image_1.png)'
  id: totrans-5278
  prefs: []
  type: TYPE_IMG
  zh: '![425_image_1.png](425_image_1.png)'
- en: Fig. 18.2. Comparison of the test set performance of SGD, SGDQN, and ASGD for
    a L2 regularized linear model trained with the squared hinge loss on the ALPHA
    task of the 2008 Pascal Large Scale Learning Challenge. ASGD nearly reaches the
    optimal expected risk after a single pass.
  id: totrans-5279
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.2. 在2008年Pascal大规模学习挑战赛的ALPHA任务上，比较SGD、SGDQ和ASGD的测试集性能。ASGD在单次遍历后几乎达到了最优预期风险。
- en: loss (m) = max{0, 1 − m}2. For reference, we also provide the results achieved
    by the SGDQN algorithm [1] which was one of the winners of this competition, and
    works by adapting a separate learning rate for each weight. The training set contains
    100,000 patterns represented by 500 centered and normalized variables.
  id: totrans-5280
  prefs: []
  type: TYPE_NORMAL
  zh: 损失(m) = max{0, 1 − m}²。作为参考，我们还提供了SGDQ算法[1]的结果，该算法是本次比赛的获胜者之一，通过为每个权重适配单独的学习率来工作。训练集包含100,000个样本，由500个中心化和标准化的变量表示。
- en: Performances measured on a separate testing set are plotted against the number
    of passes over the training set. ASGD achieves near optimal results after one
    epoch only.
  id: totrans-5281
  prefs: []
  type: TYPE_NORMAL
  zh: 在单独测试集上测量的性能与训练集的遍历次数进行对比。ASGD在仅一个周期后便达到了近乎最优的结果。
- en: Figure 18.3 reports results achieved using SGD, SGDQN, and ASGD for a CRF [8]
    trained on the CONLL 2000 Chunking task [20]. The training set contains 8936 sentences
    for a 1.68 × 106 dimensional parameter space.
  id: totrans-5282
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.3 报告了使用 SGD、SGDQN 和 ASGD 在 CONLL 2000 Chunking 任务 [20] 上训练的 CRF [8] 所取得的结果。训练集包含
    8936 个句子，参数空间维度为 1.68 × 10^6。
- en: Performances measured on a separate testing set are plotted against the number
    of passes over the training set. SGDQN appears more attractive because ASGD
  id: totrans-5283
  prefs: []
  type: TYPE_NORMAL
  zh: 在单独的测试集上测量的性能与训练集的遍历次数绘制成图。SGDQN 看起来更具吸引力，因为 ASGD
- en: '![426_image_0.png](426_image_0.png)'
  id: totrans-5284
  prefs: []
  type: TYPE_IMG
  zh: '![426_image_0.png](426_image_0.png)'
- en: '![426_image_1.png](426_image_1.png)'
  id: totrans-5285
  prefs: []
  type: TYPE_IMG
  zh: '![426_image_1.png](426_image_1.png)'
- en: Fig. 18.3. Comparison of the test set performance of SGD, SGDQN, and ASGD on
    a L2 regularized CRF trained on the CONLL Chunking task. On this task, SGDQN appears
    more attractive because ASGD does not fully reach its asymptotic performance.
  id: totrans-5286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.3. 在 CONLL Chunking 任务上，对 L2 正则化的 CRF 进行 SGD、SGDQN 和 ASGD 的测试集性能比较。在该任务中，SGDQN
    看起来更具吸引力，因为 ASGD 并未完全达到其渐近性能。
- en: does not reach its asymptotic performance. All three algorithms reach the best
    test set performance in a couple minutes. The standard CRF L-BFGS optimizer takes
    72 minutes to compute an equivalent solution.
  id: totrans-5287
  prefs: []
  type: TYPE_NORMAL
  zh: 并未达到其渐近性能。这三种算法在几分钟内达到了最佳测试集性能。标准的 CRF L-BFGS 优化器需要 72 分钟计算出等效解。
- en: 18.6 Conclusion
  id: totrans-5288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.6 结论
- en: Stochastic gradient descent and its variants are versatile techniques that have
    proven invaluable as a learning algorithms for large datasets. The best advice
    for a successful application of these techniques is (i) to perform small-scale
    experiments with subsets of the training data, and (ii) to pay a ruthless attention
    to the correctness of the gradient computation.
  id: totrans-5289
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降及其变体是经过验证的灵活技术，作为大型数据集的学习算法非常宝贵。成功应用这些技术的最佳建议是 (i) 对训练数据的子集进行小规模实验，以及
    (ii) 严格关注梯度计算的正确性。
- en: '[1] Bordes, A., Bottou, L., Gallinari, P.: SGD-QN: Careful quasi-Newton stochastic
    gradient descent. Journal of Machine Learning Research 10, 1737–1754 (2009);'
  id: totrans-5290
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Bordes, A., Bottou, L., Gallinari, P.: SGD-QN: 小心的拟牛顿随机梯度下降。机器学习研究杂志 10,
    1737–1754 (2009);'
- en: with erratum, JMLR 11, 2229–2240 (2010)
  id: totrans-5291
  prefs: []
  type: TYPE_NORMAL
  zh: 带勘误，JMLR 11, 2229–2240 (2010)
- en: '[2] Bottou, L., Bousquet, O.: The tradeoffs of large scale learning. In: Platt,
    J., Koller, D., Singer, Y., Roweis, S. (eds.) Advances in Neural Information Processing
    Systems, vol. 20, pp. 161–168. NIPS Foundation (2008), http://books.nips.cc'
  id: totrans-5292
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Bottou, L., Bousquet, O.: 大规模学习的权衡。在：Platt, J., Koller, D., Singer, Y.,
    Roweis, S. (主编) 神经信息处理系统的进展，第 20 卷，第 161–168 页。NIPS 基金会 (2008), http://books.nips.cc'
- en: '[3] Bottou, L.: Online algorithms and stochastic approximations. In: Saad,
    D. (ed.)'
  id: totrans-5293
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Bottou, L.: 在线算法与随机逼近。在：Saad, D. (主编)'
- en: Online Learning and Neural Networks. Cambridge University Press, Cambridge (1998)
  id: totrans-5294
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习与神经网络。剑桥大学出版社，剑桥 (1998)
- en: '[4] Bousquet, O.: Concentration Inequalities and Empirical Processes Theory
    Applied to the Analysis of Learning Algorithms. Ph.D. thesis, Ecole Polytechnique,
    Palaiseau, France (2002)'
  id: totrans-5295
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Bousquet, O.: 集中不等式和应用于学习算法分析的经验过程理论。博士论文，法国帕莱索学院 (2002)'
- en: '[5] Cortes, C., Vapnik, V.: Support-vector network. Machine Learning 20(3),
    273–297'
  id: totrans-5296
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Cortes, C., Vapnik, V.: 支持向量网络。机器学习 20(3), 273–297'
- en: (1995)
  id: totrans-5297
  prefs: []
  type: TYPE_NORMAL
  zh: (1995)
- en: '[6] Dennis, J., Schnabel, R.B.: Numerical Methods For Unconstrained Optimization
    and Nonlinear Equations. Prentice-Hall, Inc., Englewood Cliffs (1983)'
  id: totrans-5298
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Dennis, J., Schnabel, R.B.: 无约束优化和非线性方程的数值方法。普伦蒂斯-霍尔公司，英格尔伍德悬崖 (1983)'
- en: '[7] Joachims, T.: Training linear SVMs in linear time. In: Proceedings of the
    12th ACM SIGKDD International Conference, New York (2006)'
  id: totrans-5299
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Joachims, T.: 以线性时间训练线性 SVM。在：第十二届 ACM SIGKDD 国际会议论文集，纽约 (2006)'
- en: '[8] Lafferty, J.D., McCallum, A., Pereira, F.C.N.: Conditional random fields:
    Probabilistic models for segmenting and labeling sequence data. In: Brodley, C.E.,'
  id: totrans-5300
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Lafferty, J.D., McCallum, A., Pereira, F.C.N.: 条件随机场：用于分割和标记序列数据的概率模型。在：Brodley,
    C.E.,'
- en: Danyluk, A.P. (eds.) Proceedings of the Eighteenth International Conference
    on Machine Learning (ICML), pp. 282–289. Morgan Kaufmann, Williams College
  id: totrans-5301
  prefs: []
  type: TYPE_NORMAL
  zh: Danyluk, A.P. (主编) 第十八届国际机器学习会议（ICML）论文集，第 282–289 页。摩根·考夫曼出版社，威廉姆斯学院
- en: (2001)
  id: totrans-5302
  prefs: []
  type: TYPE_NORMAL
  zh: (2001)
- en: '[9] Lee, W.S., Bartlett, P.L., Williamson, R.C.: The importance of convexity
    in learning with squared loss. IEEE Transactions on Information Theory 44(5),
    1974–1980'
  id: totrans-5303
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Lee, W.S., Bartlett, P.L., Williamson, R.C.: 凸性在平方损失学习中的重要性。IEEE 信息理论学报
    44(5), 1974–1980'
- en: (1998)
  id: totrans-5304
  prefs: []
  type: TYPE_NORMAL
  zh: (1998)
- en: '[10] Lewis, D.D., Yang, Y., Rose, T.G., Li, F.: RCV1: A new benchmark collection
    for text categorization research. Journal of Machine Learning Research 5, 361–397
    (2004)'
  id: totrans-5305
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Lewis, D.D., Yang, Y., Rose, T.G., Li, F.: RCV1：文本分类研究的新基准集合。机器学习研究杂志
    5, 361–397（2004）'
- en: '[11] Lin, C.J., Weng, R.C., Keerthi, S.S.: Trust region newton methods for
    large-scale logistic regression. In: Ghahramani, Z. (ed.) Proc. Twenty-Fourth
    International Conference on Machine Learning (ICML), pp. 561–568. ACM (2007)'
  id: totrans-5306
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Lin, C.J., Weng, R.C., Keerthi, S.S.: 大规模逻辑回归的信任区域牛顿方法。在：Ghahramani, Z.（编辑）第24届国际机器学习会议（ICML）会议记录，561–568页。ACM（2007）'
- en: '[12] MacQueen, J.: Some methods for classification and analysis of multivariate
    observations. In: LeCam, L.M., Neyman, J. (eds.) Proceedings of the Fifth Berkeley
    Symposium on Mathematics, Statistics, and Probabilities, vol. 1, pp. 281–297.'
  id: totrans-5307
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] MacQueen, J.: 一些多元观察分类与分析的方法。在：LeCam, L.M., Neyman, J.（编辑）第五届伯克利数学、统计和概率研讨会论文集，第1卷，281–297页。'
- en: University of California Press, Berkeley (1967)
  id: totrans-5308
  prefs: []
  type: TYPE_NORMAL
  zh: 加州大学出版社，伯克利（1967）
- en: '[13] Massart, P.: Some applications of concentration inequalities to statistics.
    Annales de la Faculté des Sciences de Toulouse series 6 9(2), 245–303 (2000)'
  id: totrans-5309
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Massart, P.: 集中不等式在统计学中的一些应用。图卢兹科学学院年报系列6 9(2), 245–303（2000）'
- en: '[14] Murata, N.: A statistical study of on-line learning. In: Saad, D. (ed.)
    Online Learning and Neural Networks. Cambridge University Press, Cambridge (1998)'
  id: totrans-5310
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Murata, N.: 在线学习的统计研究。在：Saad, D.（编辑）在线学习与神经网络。剑桥大学出版社，剑桥（1998）'
- en: '[15] Polyak, B.T., Juditsky, A.B.: Acceleration of stochastic approximation
    by averaging. SIAM J. Control Optim. 30(4), 838–855 (1992)'
  id: totrans-5311
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Polyak, B.T., Juditsky, A.B.: 通过平均加速随机逼近。SIAM控制优化杂志 30(4), 838–855（1992）'
- en: '[16] Robbins, H., Siegmund, D.: A convergence theorem for non negative almost
    supermartingales and some applications. In: Rustagi, J.S. (ed.) Optimizing Methods
    in Statistics. Academic Press (1971)'
  id: totrans-5312
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Robbins, H., Siegmund, D.: 非负几乎超马丁戈尔的收敛定理及其应用。在：Rustagi, J.S.（编辑）统计优化方法。学术出版社（1971）'
- en: '[17] Rosenblatt, F.: The perceptron: A perceiving and recognizing automaton.
    Tech.'
  id: totrans-5313
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Rosenblatt, F.: 感知器：一个感知和识别的自动机。技术。'
- en: Rep. 85-460-1, Project PARA, Cornell Aeronautical Lab (1957)
  id: totrans-5314
  prefs: []
  type: TYPE_NORMAL
  zh: 报告85-460-1，PARA项目，康奈尔航空实验室（1957）
- en: '[18] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning internal representations
    by error propagation. In: Parallel Distributed Processing: Explorations in the
    Microstructure of Cognition, vol. I, pp. 318–362. Bradford Books, Cambridge (1986)'
  id: totrans-5315
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: 通过误差传播学习内部表征。在：并行分布处理：认知微结构探索，第I卷，318–362页。Bradford
    Books，剑桥（1986）'
- en: '[19] Ruppert, D.: Efficient estimations from a slowly convergent robbins-monro
    process. Tech. Rep. 781, Cornell University Operations Research and Industrial
    Engineering (1988)'
  id: totrans-5316
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Ruppert, D.: 来自缓慢收敛的Robbins-Monro过程的高效估计。技术报告781，康奈尔大学运筹学与工业工程（1988）'
- en: '[20] Sang, E.F.T.K., Buchholz, S.: Introduction to the CoNLL-2000 shared task:'
  id: totrans-5317
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Sang, E.F.T.K., Buchholz, S.: CoNLL-2000共享任务简介：'
- en: 'Chunking. In: Cardie, C., Daelemans, W., Nedellec, C., Tjong Kim Sang, E.F.'
  id: totrans-5318
  prefs: []
  type: TYPE_NORMAL
  zh: 分块。在：Cardie, C., Daelemans, W., Nedellec, C., Tjong Kim Sang, E.F.
- en: (eds.) Proceedings of CoNLL 2000 and LLL 2000, Lisbon, Portugal, pp. 127–132
    (2000)
  id: totrans-5319
  prefs: []
  type: TYPE_NORMAL
  zh: （编辑）CoNLL 2000和LLL 2000会议论文集，葡萄牙里斯本，127–132页（2000）
- en: '[21] Shalev-Shwartz, S., Singer, Y., Srebro, N.: Pegasos: Primal estimated
    subgradient solver for SVM. In: Proc. 24th Intl. Conf. on Machine Learning (ICML
    2007), pp.'
  id: totrans-5320
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Shalev-Shwartz, S., Singer, Y., Srebro, N.: Pegasos：SVM的原始估计子梯度求解器。在：第24届国际机器学习会议（ICML
    2007）会议记录，页码。'
- en: 807–814. ACM (2007)
  id: totrans-5321
  prefs: []
  type: TYPE_NORMAL
  zh: 807–814。ACM（2007）
- en: '[22] Shalev-Shwartz, S., Srebro, N.: SVM optimization: inverse dependence on
    training set size. In: Proceedings of the 25th International Machine Learning
    Conference'
  id: totrans-5322
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Shalev-Shwartz, S., Srebro, N.: SVM优化：对训练集大小的逆依赖。在：第25届国际机器学习会议论文集'
- en: (ICML 2008), pp. 928–935. ACM (2008)
  id: totrans-5323
  prefs: []
  type: TYPE_NORMAL
  zh: （ICML 2008），928–935页。ACM（2008）
- en: '[23] Tibshirani, R.: Regression shrinkage and selection via the lasso. Journal
    of the Royal Statistical Society (Series B) 58, 267–288 (1996)'
  id: totrans-5324
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Tibshirani, R.: 通过lasso进行回归收缩和选择。皇家统计学会杂志（B系列）58, 267–288（1996）'
- en: '[24] Tsybakov, A.B.: Optimal aggregation of classifiers in statistical learning.
    Annals of Statististics 32(1) (2004)'
  id: totrans-5325
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Tsybakov, A.B.: 在统计学习中分类器的最优聚合。统计年鉴 32(1)（2004）'
- en: '[25] Vapnik, V.N., Chervonenkis, A.Y.: On the uniform convergence of relative
    frequencies of events to their probabilities. Theory of Probability and its Applications
    16(2), 264–280 (1971)'
  id: totrans-5326
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Vapnik, V.N., Chervonenkis, A.Y.: 关于事件相对频率对其概率的均匀收敛。概率论及其应用 16(2), 264–280（1971）'
- en: '[26] Widrow, B., Hoff, M.E.: Adaptive switching circuits. In: IRE WESCON Conv.'
  id: totrans-5327
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Widrow, B., Hoff, M.E.: 自适应开关电路。在：IRE WESCON大会。'
- en: Record, Part 4, pp. 96–104 (1960)
  id: totrans-5328
  prefs: []
  type: TYPE_NORMAL
  zh: 记录，第4部分，第96–104页（1960）
- en: '[27] Xu, W.: Towards optimal one pass large scale learning with averaged stochastic
    gradient descent (2011), http://arxiv.org/abs/1107.2490'
  id: totrans-5329
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Xu, W.: 朝着最佳一次性大规模学习，采用平均随机梯度下降法（2011），http://arxiv.org/abs/1107.2490'
- en: '[28] Zinkevich, M.: Online convex programming and generalized infinitesimal
    gradient ascent. In: Proc. Twentieth International Conference on Machine Learning
    (2003)'
  id: totrans-5330
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] Zinkevich, M.: 在线凸编程与广义无穷小梯度上升。在：第二十届国际机器学习会议论文集（2003）'
- en: 19 Practical Recommendations For Gradient-Based Training Of Deep Architectures
  id: totrans-5331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 19条针对基于梯度的深度架构训练的实用建议
- en: Yoshua Bengio Université de Montréal Abstract. Learning algorithms related to
    artificial neural networks and in particular for Deep Learning may seem to involve
    many bells and whistles, called hyper-parameters. This chapter is meant as a practical
    guide with recommendations for some of the most commonly used hyperparameters,
    in particular in the context of learning algorithms based on back-propagated gradient
    and gradient-based optimization. It also discusses how to deal with the fact that
    more interesting results can be obtained when allowing one to adjust many hyper-parameters.
    Overall, it describes elements of the practice used to successfully and efficiently
    train and debug large-scale and often deep multi-layer neural networks. It closes
    with open questions about the training difficulties observed with deeper architectures.
  id: totrans-5332
  prefs: []
  type: TYPE_NORMAL
  zh: Yoshua Bengio 蒙特利尔大学 摘要。与人工神经网络相关的学习算法，尤其是深度学习，似乎涉及许多复杂的元素，称为超参数。本章旨在作为一个实用指南，提供一些最常用超参数的建议，特别是在基于反向传播梯度和基于梯度优化的学习算法上下文中。它还讨论了如何应对允许调整多个超参数时可能获得更有趣结果的事实。总体而言，它描述了成功和高效训练及调试大规模且通常是深层多层神经网络的实践元素。最后提出了关于深层架构训练难点的开放性问题。
- en: 19.1 Introduction
  id: totrans-5333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.1 引言
- en: Following a decade of lower activity, research in artificial neural networks
    was revived after a 2006 breakthrough [61, 14, 95] in the area of *Deep Learning*,
    based on greedy layer-wise unsupervised pre-training of each layer of features.
    See [7] for a review. Many of the practical recommendations that justified the
    previous edition of this book are still valid, and new elements were added, while
    some survived longer by virtue of the practical advantages they provided. The
    panorama presented in this chapter regards some of these surviving or novel elements
    of practice, focusing on learning algorithms aiming at training deep neural networks,
    but leaving most of the material specific to the Boltzmann machine family to another
    chapter [60].
  id: totrans-5334
  prefs: []
  type: TYPE_NORMAL
  zh: 在经历了十年的低活动期后，人工神经网络的研究在2006年的突破后复苏，基于贪婪的逐层无监督预训练方法，使得*深度学习*领域得以发展。详见[7]的综述。许多为本书之前版本提供依据的实用建议依然有效，新的元素也被添加，而某些建议因其提供的实用优势而得以延续。本章展示的全景图涉及这些持续存在或新出现的实践元素，重点讨论旨在训练深度神经网络的学习算法，而将与玻尔兹曼机家族相关的大部分材料留给另一章[60]。
- en: Although such recommendations come out of a living practice that emerged from
    years of experimentation and to some extent mathematical justification, they should
    be challenged. They constitute a good starting point for the experimenter and
    user of learning algorithms but very often have not been formally validated, leaving
    open many questions that can be answered either by theoretical analysis or by
    solid comparative experimental work (ideally by both). A
  id: totrans-5335
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些建议源自于多年的实验和在某种程度上的数学验证所形成的活跃实践，但它们应当受到挑战。它们构成了实验者和学习算法用户的良好起点，但往往没有经过正式验证，留下了许多可以通过理论分析或扎实的比较实验工作（理想情况下两者结合）来解答的问题。
- en: good indication of the need for such validation is that different researchers
    and research groups do not always agree on the practice of training neural networks.
  id: totrans-5336
  prefs: []
  type: TYPE_NORMAL
  zh: 这种验证需求的良好迹象是，不同研究人员和研究小组在神经网络训练的实践上并不总是一致。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    437–478, 2012.'
  id: totrans-5337
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon 等（编）：神经网络：实用技巧，第2版，LNCS 7700，第437–478页，2012年。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-5338
  prefs: []
  type: TYPE_NORMAL
  zh: -c 施普林格-瓦尔拉赫 柏林 海德堡 2012年
- en: Several of the recommendations presented here can be found implemented in the
    *Deep Learning Tutorials*1 and in the related *Pylearn2* library2, all based on
    the Theano library (discussed below) written in the *Python* programming language.
  id: totrans-5339
  prefs: []
  type: TYPE_NORMAL
  zh: 此处提出的若干建议可以在*深度学习教程*1和相关的*Pylearn2*库2中找到实现，所有这些都基于用*Python*编程语言编写的Theano库（见下文）。
- en: The 2006 Deep Learning breakthrough [61, 14, 95] centered on the use of unsupervised
    representation learning to help learning *internal representations*3 by providing
    a *local training signal* at each level of a hierarchy of features4.
  id: totrans-5340
  prefs: []
  type: TYPE_NORMAL
  zh: 2006年深度学习的突破[61, 14, 95]集中在利用无监督表示学习来帮助学习*内部表示*3，通过在特征层次的每个级别提供*局部训练信号*4。
- en: Unsupervised representation learning algorithms can be applied several times
    to learn different layers of a deep model. Several unsupervised representation
    learning algorithms have been proposed since then. Those covered in this chapter
    (such as auto-encoder variants) retain many of the properties of artificial multi-layer
    neural networks, relying on the back-propagation algorithm to estimate stochastic
    gradients. Deep Learning algorithms such as those based on the Boltzmann machine
    and those based on auto-encoder or sparse coding variants often include a supervised
    fine-tuning stage. This supervised fine-tuning as well as the gradient descent
    performed with auto-encoder variants also involves the back-propagation algorithm,
    just as like when training deterministic feedforward or recurrent artificial neural
    networks. Hence this chapter also includes recommendations for training ordinary
    supervised deterministic neural networks or more generally, most machine learning
    algorithms relying on iterative gradientbased optimization of a parametrized learner
    with respect to an explicit training criterion.
  id: totrans-5341
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督表示学习算法可以多次应用，以学习深度模型的不同层。自那时以来，提出了几种无监督表示学习算法。本章涵盖的算法（如自编码器变体）保留了人工多层神经网络的许多特性，依赖反向传播算法来估计随机梯度。基于玻尔兹曼机和自编码器或稀疏编码变体的深度学习算法通常包括一个监督微调阶段。这个监督微调以及与自编码器变体一起进行的梯度下降也涉及反向传播算法，就像训练确定性前馈或递归人工神经网络时一样。因此，本章还包括关于训练普通监督确定性神经网络或更一般而言，大多数依赖于对带参数学习者进行迭代基于梯度优化的机器学习算法的建议。
- en: This chapter assumes that the reader already understands the standard algorithms
    for training supervised multi-layer neural networks, with the loss gradient computed
    thanks to the back-propagation algorithm [103]. It starts by explaining basic
    concepts behind Deep Learning and the greedy layer-wise pretraining strategy (Section
    19.1.1), and recent unsupervised pre-training algorithms (denoising and contractive
    auto-encoders) that are closely related in the way they are trained to standard
    multi-layer neural networks (Section 19.1.2). It then reviews in Section 19.2
    basic concepts in iterative gradient-based optimization and in particular the
    stochastic gradient method, gradient computation with a flow graph, automatic
    differentation. The main section of this chapter is Section 19.3, which explains
    hyper-parameters in general, their optimization, and specifically covers the main
    hyper-parameters of neural networks. Section 19.4 briefly describes simple ideas
    and methods to debug and visualize neural networks, while Section 19.5 covers
    parallelism, sparse high-dimensional inputs, symbolic inputs
  id: totrans-5342
  prefs: []
  type: TYPE_NORMAL
  zh: 本章假设读者已经了解用于训练监督多层神经网络的标准算法，损失梯度是通过反向传播算法计算的[103]。本章首先解释深度学习背后的基本概念和贪婪逐层预训练策略（第19.1.1节），以及与标准多层神经网络训练方式密切相关的最近无监督预训练算法（去噪和收缩自编码器）（第19.1.2节）。然后在第19.2节回顾迭代基于梯度的优化中的基本概念，特别是随机梯度法、流图的梯度计算和自动微分。本章的主要部分是第19.3节，解释了一般的超参数及其优化，特别涵盖神经网络的主要超参数。第19.4节简要描述了调试和可视化神经网络的简单想法和方法，而第19.5节涉及并行性、稀疏高维输入和符号输入。
- en: 1 http://deeplearning.net/tutorial/ 2 http://deeplearning.net/software/pylearn2
    3 A neural network computes a sequence of data transformations, each step encoding
    the raw input into an intermediate or internal representation, in principle to
    make the prediction or modeling task of interest easier. 4 In standard multi-layer
    neural networks trained using back-propagated gradients, the only signal that
    drives parameter updates is provided at the output of the network (and then propagated
    backwards). Some unsupervised learning algorithms provide a local source of guidance
    for the parameter update in each layer, based only on the inputs and outputs of
    that layer.
  id: totrans-5343
  prefs: []
  type: TYPE_NORMAL
  zh: 1 http://deeplearning.net/tutorial/ 2 http://deeplearning.net/software/pylearn2
    3 神经网络计算一系列数据转换，每一步将原始输入编码为中间或内部表示，原则上旨在简化所关注的预测或建模任务。4 在标准的多层神经网络中，使用反向传播梯度训练，驱动参数更新的唯一信号是在网络的输出处提供的（然后向后传播）。一些无监督学习算法为每层的参数更新提供局部指导，仅基于该层的输入和输出。
- en: and embeddings, and multi-relational learning. The chapter closes (Section 19.6)
  id: totrans-5344
  prefs: []
  type: TYPE_NORMAL
  zh: 以及嵌入和多关系学习。本章结束于（第19.6节）
- en: with open questions on the difficulty of training deep architectures and improving
    the optimization methods for neural networks.
  id: totrans-5345
  prefs: []
  type: TYPE_NORMAL
  zh: 关于训练深度架构的难度和改善神经网络优化方法的问题依然存在开放性。
- en: 19.1.1 Deep Learning And Greedy Layer-Wise Pretraining
  id: totrans-5346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.1.1 深度学习与贪婪的逐层预训练
- en: 'The notion of *reuse*, which explains the power of distributed representations
    [7], is also at the heart of the theoretical advantages behind *Deep Learning*.
    Complexity theory of circuits, e.g. [54, 55], (which include neural networks as
    special cases) has much preceded the recent research on deep learning. The depth
    of a circuit is the length of the longest path from an input node of the circuit
    to an output node of the circuit. Formally, one can change the depth of a given
    circuit by changing the definition of what each node can compute, but only by
    a constant factor [7]. The typical computations we allow in each node include:
    weighted sum, product, artificial neuron model (such as a monotone non-linearity
    on top of an affine transformation), computation of a kernel, or logic gates.
    Theoretical results [54, 55, 13, 10, 9] clearly identify families of functions
    where a deep representation can be exponentially more efficient than one that
    is insufficiently deep. If the same set of functions can be represented from within
    a family of architectures associated with a smaller VC-dimension (e.g. less hidden
    units5), learning theory would suggest that it can be learned with fewer examples,
    yielding improvements in both *computational* efficiency and *statistical* efficiency.'
  id: totrans-5347
  prefs: []
  type: TYPE_NORMAL
  zh: '*重用*的概念解释了分布式表示的力量[7]，这也是*深度学习*背后理论优势的核心。电路的复杂性理论，例如[54, 55]（其中包括神经网络作为特殊情况），早于最近的深度学习研究。电路的深度是从电路的输入节点到输出节点的最长路径的长度。形式上，可以通过改变每个节点的计算定义来改变给定电路的深度，但仅限于一个常数因子[7]。我们在每个节点中允许的典型计算包括：加权和、乘积、人工神经元模型（例如在仿射变换之上的单调非线性）、核计算或逻辑门。理论结果[54,
    55, 13, 10, 9]清晰地识别出深层表示在效率上可以比不够深的表示高出指数倍的函数族。如果同一组函数可以通过与较小VC维度（例如较少的隐含单元5）相关的架构家族表示，学习理论将表明，它可以用更少的示例进行学习，从而提高*计算*效率和*统计*效率。'
- en: Another important motivation for feature learning and Deep Learning is that
    they can be done with *unlabeled examples*, so long as the factors (unobserved
    random variables explaining the data) relevant to the questions we will ask later
    (e.g. classes to be predicted) are somehow salient in the input distribution itself.
    This is true under the *manifold hypothesis*, which states that natural classes
    and other high-level concepts in which humans are interested are associated with
    low-dimensional regions in input space (manifolds) near which the distribution
    concentrates, and that different class manifolds are well-separated by regions
    of very low density. It means that a small semantic change around a particular
    example can be captured by changing only a few numbers in a high-level abstract
    representation space. As a consequence, feature learning and Deep Learning are
    intimately related to principles of *unsupervised learning*, and they can work
    in the *semi-supervised setting* (where only a few examples are labeled), as well
    as in the *transfer learning* and *multi-task* settings (where we aim to generalize
    to new classes or tasks). The underlying hypothesis is that many of the underlying
    factors are *shared* across classes or tasks. Since representation learning aims
    to extract and isolate these factors, representations can be *shared* across classes
    and tasks.
  id: totrans-5348
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的特征学习和深度学习动机是它们可以使用*未标记的示例*进行，只要与我们稍后要问的问题（例如要预测的类别）相关的因素（未观察到的随机变量）在输入分布中以某种方式显著。这在*流形假设*下是成立的，该假设指出，自然类别和人类感兴趣的其他高层次概念与输入空间中的低维区域（流形）相关联，在这些区域附近分布集中，并且不同的类别流形被非常低密度的区域很好地分隔。这意味着对特定示例的小语义变化可以通过仅改变高层次抽象表示空间中的几个数字来捕获。因此，特征学习和深度学习与*无监督学习*的原则密切相关，并且它们可以在*半监督设置*（仅有少量示例被标记）中工作，也可以在*迁移学习*和*多任务*设置中工作（我们旨在推广到新的类别或任务）。其基本假设是，许多潜在因素在类别或任务之间是*共享的*。由于表示学习旨在提取和隔离这些因素，表示可以在类别和任务之间*共享*。
- en: One of the most commonly used approaches for training deep neural networks is
    based on *greedy layer-wise pre-training* [14]. The idea, first introduced in
    Hinton *et al.* [61], is to train one layer of a deep architecture at a time using
  id: totrans-5349
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度神经网络最常用的方法之一是基于*贪婪分层预训练* [14]。这个想法最早由Hinton *等* [61]提出，即一次训练深度架构的一层。
- en: 5 Note that in our experiments, deep architectures tend to generalize very well
    even when they have quite large numbers of parameters.
  id: totrans-5350
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在我们的实验中，深度架构即使在参数数量相当大的情况下也往往能够很好地推广。
- en: unsupervised representation learning. Each level takes as input the representation
    learned at the previous level and learns a new representation. The learned representation(s)
    can then be used as input to predict variables of interest, for example to classify
    objects. After unsupervised pre-training, one can also perform supervised fine-tuning
    of the whole system6, i.e., optimize not just the classifier but also the lower
    levels of the feature hierarchy with respect to some objective of interest. Combining
    unsupervised pre-training and supervised finetuning usually gives better generalization
    than pure supervised learning from a purely random initialization. The unsupervised
    representation learning algorithms for pre-training proposed in 2006 were the
    Restricted Boltzmann Machine or RBM [61], the auto-encoder [14] and a sparsifying
    form of auto-encoder similar to sparse coding [95].
  id: totrans-5351
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督表示学习。每一层以前一层学习到的表示为输入，并学习一个新的表示。学习到的表示可以用作预测感兴趣变量的输入，例如对物体进行分类。在无监督预训练之后，还可以对整个系统进行监督微调6，即，不仅优化分类器，还优化特征层次的较低层次，以符合某个感兴趣的目标。结合无监督预训练和监督微调通常比从完全随机初始化的纯监督学习具有更好的泛化能力。2006年提出的用于预训练的无监督表示学习算法包括限制玻尔兹曼机（RBM）[61]、自编码器[14]以及类似稀疏编码的稀疏自编码器形式[95]。
- en: 19.1.2 Denoising And Contractive Auto-Encoders
  id: totrans-5352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.1.2 去噪和收缩自编码器
- en: 'An auto-encoder has two parts: an encoder function f that maps the input x
    to a representation h = f(x), and a decoder function g that maps h back in the
    space of x in order to reconstruct x. In the regular auto-encoder the reconstruction
    function r(·) = g(f(·)) is trained to minimize the average value of a reconstruction
    loss on the training examples. Note that reconstruction loss should be high for
    most other input configurations7. The regularization mechanism makes sure that
    reconstruction cannot be perfect everywhere, while minimizing the reconstruction
    loss at training examples digs a hole in reconstruction error where the density
    of training examples is large. Examples of reconstruction loss functions include
    ||x − r(x)||2'
  id: totrans-5353
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器有两个部分：一个编码函数f，将输入x映射到表示h = f(x)，以及一个解码函数g，将h映射回x的空间，以重建x。在常规自编码器中，重建函数r(·)
    = g(f(·))被训练以最小化训练样本上的重建损失的平均值。请注意，对于大多数其他输入配置，重建损失应该是高的7。正则化机制确保重建无法在每个地方都是完美的，而在训练样本上最小化重建损失会在重建误差中挖一个坑，训练样本的密度很大。重建损失函数的例子包括||x
    − r(x)||2
- en: (for real-valued inputs) and −i xi log ri(x) + (1 − xi) log(1 − ri(x)) (when
    interpreting xi as a bit or a probability of a binary event). Auto-encoders capture
    the input distribution by learning to *better reconstruct more likely input* configurations.
    The difference between the reconstruction vector and the input vector can be shown
    to be related to the log-density gradient as estimated by the learner [114, 16]
    and the Jacobian matrix of the reconstruction with respect to the input gives
    information about the second derivative of the density, i.e., in which direction
    the density remains high when you are on a high-density manifold [99, 16]. In
    the Denoising Auto-Encoder (DAE) and the Contractive Auto-Encoder (CAE), the training
    procedure also introduces *robustness* (insensitivity to small variations), respectively
    in the reconstruction r(x) or in the representation f(x). In the DAE [115, 116],
    this is achieved by training with stochastically corrupted inputs, but trying
    to reconstruct the uncorrupted inputs. In the CAE [99], this is achieved by adding
    an explicit regularizing term in the training criterion, proportional to the norm
    of the Jacobian of the encoder, || ∂f(x)
  id: totrans-5354
  prefs: []
  type: TYPE_NORMAL
  zh: （对于实值输入）和−i xi log ri(x) + (1 − xi) log(1 − ri(x))（当将xi解释为二进制事件的位或概率时）。自编码器通过学习*更好地重建更可能的输入*配置来捕捉输入分布。重建向量与输入向量之间的差异可以显示为与学习者估计的对数密度梯度相关[114,
    16]，重建相对于输入的雅可比矩阵提供关于密度的二阶导数的信息，即在高密度流形上，密度保持高的方向[99, 16]。在去噪自编码器（DAE）和收缩自编码器（CAE）中，训练过程还分别引入了*鲁棒性*（对小变化的不敏感性），无论是在重建r(x)还是在表示f(x)中。在DAE
    [115, 116]中，这是通过训练具有随机损坏输入来实现的，但尝试重建未损坏的输入。在CAE [99]中，这是通过在训练标准中添加一个显式的正则化项来实现的，该项与编码器的雅可比范数成正比，||
    ∂f(x)
- en: '∂x ||2. But the CAE and the DAE are very related [16]: when the 6 The whole
    system composes the computation of the representation with computation of the
    predictor''s output.'
  id: totrans-5355
  prefs: []
  type: TYPE_NORMAL
  zh: ∂x ||2。但CAE和DAE是非常相关的[16]：当6整个系统由表示的计算与预测器输出的计算组成。
- en: '7 Different regularization mechanisms have been proposed to push reconstruction
    error up in low density areas: denoising criterion, contractive criterion, and
    code sparsity. It has been argued that such constraints play a role similar to
    the partition function for Boltzmann machines [96].'
  id: totrans-5356
  prefs: []
  type: TYPE_NORMAL
  zh: 7 提出了不同的正则化机制，以在低密度区域推高重建误差：去噪标准、收缩标准和代码稀疏性。有观点认为，这些约束在作用上类似于玻尔兹曼机的分区函数[96]。
- en: 'noise is Gaussian and small, the denoising error minimized by the DAE is equivalent
    to minimizing the norm of the Jacobian of the reconstruction function r(·) = g(f(·)),
    whereas the CAE minimizes the norm of the Jacobian of the encoder f(·). Besides
    Gaussian noise, another interesting form of corruption has been very successful
    with DAEs: it is called the *masking corruption* and consists in randomly zeroing
    out a large fraction (like 20% or even 50%) of the inputs, where the zeroed out
    subset is randomly selected for each example. In addition to the contractive effect,
    it forces the learned encoder to be able to rely only on an arbitrary subset of
    the input features.'
  id: totrans-5357
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声是高斯且较小，DAE所最小化的去噪误差等同于最小化重构函数r(·) = g(f(·))的雅可比矩阵的范数，而CAE最小化的是编码器f(·)的雅可比矩阵的范数。除了高斯噪声外，另一种有趣的损坏形式在DAE中取得了很大成功：它被称为*掩蔽损坏*，其内容是随机将大部分输入（例如20%甚至50%）置为零，其中被置为零的子集是针对每个示例随机选择的。除了收缩效应外，它还迫使学习的编码器能够仅依赖于输入特征的任意子集。
- en: Another way to prevent the auto-encoder from perfectly reconstructing everywhere
    is to introduce a sparsity penalty on h, discussed below (Section 19.3.1).
  id: totrans-5358
  prefs: []
  type: TYPE_NORMAL
  zh: 防止自编码器在每个地方完美重建的另一种方法是对h引入稀疏惩罚，具体内容在下文（第19.3.1节）讨论。
- en: 19.1.3 Online Learning And Optimization Of Generalization Error
  id: totrans-5359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.1.3 在线学习与泛化误差的优化
- en: 'The objective of learning is not to minimize training error or even the training
    criterion. The latter is a surrogate for generalization error, i.e., performance
    on new (out-of-sample) examples, and there are no hard guarantees that minimizing
    the training criterion will yield good generalization error: it depends on the
    appropriateness of the parametrization and training criterion (with the corresponding
    prior they imply) for the task at hand.'
  id: totrans-5360
  prefs: []
  type: TYPE_NORMAL
  zh: 学习的目标并不是最小化训练误差或甚至训练标准。后者是泛化误差的替代，即在新的（样本外）示例上的表现，并没有严格保证最小化训练标准会产生良好的泛化误差：这取决于参数化和训练标准（以及它们所暗示的相应先验）对当前任务的适用性。
- en: Many learning tasks of interest will require huge quantities of data (most of
    which will be unlabeled) and as the number of examples increases, so long as capacity
    is limited (the number of parameters is small compared to the number of examples),
    training error and generalization approach each other. In the regime of such large
    datasets, we can consider that the learner sees an unending stream of examples
    (e.g., think about a process that harvests text and images from the web and feeds
    it to a machine learning algorithm). In that context, it is most efficient to
    simply update the parameters of the model after each example or few examples,
    as they arrive. This is the ideal *online learning* scenario, and in a simplified
    setting, we can even consider each new example z as being sampled i.i.d. from
    an unknown generating distribution with probability density p(z). More realistically,
    examples in online learning do not arrive i.i.d. but instead from an unknown stochastic
    process which exhibits serial correlation and other temporal dependencies. Many
    learning algorithms rely on gradient-based numerical optimization of a training
    criterion. Let L(z,θ) be the loss incurred on example z when the parameter vector
    takes value θ. The gradient vector for the loss associated with a single example
    is ∂L(z,θ)
  id: totrans-5361
  prefs: []
  type: TYPE_NORMAL
  zh: 许多感兴趣的学习任务将需要大量的数据（其中大部分将是未标记的），随着示例数量的增加，只要容量有限（参数的数量相对于示例的数量较小），训练误差和泛化误差会相互接近。在如此大数据集的情况下，我们可以认为学习者看到的是一个不断涌现的示例流（例如，想象一个从网络收集文本和图像并将其输入机器学习算法的过程）。在这种情况下，最有效的做法是简单地在每个示例或少量示例到达后更新模型的参数。这是理想的*在线学习*场景，在一个简化的环境中，我们甚至可以认为每个新的示例z都是从未知生成分布中以独立同分布的方式采样，具有概率密度p(z)。更现实的是，在线学习中的示例并不是独立同分布到达的，而是来自一个未知的随机过程，该过程表现出序列相关性和其他时间依赖性。许多学习算法依赖于基于梯度的数值优化训练标准。设L(z,θ)为参数向量取值为θ时示例z所产生的损失。与单个示例相关的损失的梯度向量为∂L(z,θ)。
- en: ∂θ .
  id: totrans-5362
  prefs: []
  type: TYPE_NORMAL
  zh: ∂θ。
- en: 'If we consider the simplified case of i.i.d. data, there is an interesting
    observation to be made: *the online learner is performing stochastic gradient
    descent* on its generalization error. Indeed, the generalization error C of a
    learner with parameters θ and loss function L is'
  id: totrans-5363
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑独立同分布数据的简化情况，有一个有趣的观察：*在线学习者正在对其泛化误差执行随机梯度下降*。实际上，具有参数θ和损失函数L的学习者的泛化误差C是
- en: $$C=E[L(z,\theta)]=\int p(z)L(z,\theta)d z$$
  id: totrans-5364
  prefs: []
  type: TYPE_NORMAL
  zh: $$C=E[L(z,\theta)]=\int p(z)L(z,\theta)d z$$
- en: while the stochastic gradient from sample z is
  id: totrans-5365
  prefs: []
  type: TYPE_NORMAL
  zh: 而从样本 z 得到的随机梯度是。
- en: $$\hat{g}=\frac{\partial L(z,\theta)}{\partial\theta}$$
  id: totrans-5366
  prefs: []
  type: TYPE_NORMAL
  zh: $$\hat{g}=\frac{\partial L(z,\theta)}{\partial\theta}$$
- en: with z a random variable sampled from p. The gradient of generalization error
  id: totrans-5367
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 z 是从 p 中采样的随机变量。泛化误差的梯度。
- en: is∂C
  id: totrans-5368
  prefs: []
  type: TYPE_NORMAL
  zh: 是 ∂C。
- en: $ \frac{\partial C}{\partial\theta}=\frac{\partial}{\partial\theta}\int p(z)L(z,\theta)dz=\int
    p(z)\frac{\partial L(z,\theta)}{\partial\theta}dz=E[\hat{g}]$  with radius equal
    to $\hat{g}$ is equal to the distance of the $\theta$.
  id: totrans-5369
  prefs: []
  type: TYPE_NORMAL
  zh: $ \frac{\partial C}{\partial\theta}=\frac{\partial}{\partial\theta}\int p(z)L(z,\theta)dz=\int
    p(z)\frac{\partial L(z,\theta)}{\partial\theta}dz=E[\hat{g}]$，半径等于 $\hat{g}$，等于
    $\theta$ 的距离。
- en: Showing That The Online Gradient Gˆ Is An Unbiased Estimator Of The Generalization
  id: totrans-5370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 显示在线梯度 Gˆ 是泛化的无偏估计量。
- en: Error Gradient ∂C
  id: totrans-5371
  prefs: []
  type: TYPE_NORMAL
  zh: 误差梯度 ∂C。
- en: ∂Θ . It Means That Online Learners, When Given A Stream Of Nonrepetitive Training
    Data, Really Optimize (Maybe Not In The Optimal Way, I.E., Using
  id: totrans-5372
  prefs: []
  type: TYPE_NORMAL
  zh: ∂Θ。这意味着在线学习者在接收到一系列非重复的训练数据时，确实在优化（也许不是以最优方式，即使用）。
- en: 'A First-Order Gradient Technique) What We Really Care About: Generalization
    Error. 19.2 Gradients 19.2.1 Gradient Descent And Learning Rate'
  id: totrans-5373
  prefs: []
  type: TYPE_NORMAL
  zh: 一阶梯度技术）我们真正关心的：泛化误差。19.2 梯度 19.2.1 梯度下降与学习率。
- en: The gradient or an estimator of the gradient is used as the core part the computation
    of parameter updates for gradient-based numerical optimization algorithms.
  id: totrans-5374
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度或梯度的估计量作为基于梯度的数值优化算法中参数更新计算的核心部分。
- en: For example, simple online (or stochastic) gradient descent [102, 28] updates
    the parameters after each example is seen, according to
  id: totrans-5375
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，简单的在线（或随机）梯度下降[102，28]在每次看到示例后更新参数，按照。
- en: $$\theta^{(t)}\leftarrow\theta^{(t-1)}-\epsilon_{t}\frac{\partial L(z_{t},\theta)}{\partial\theta}$$
  id: totrans-5376
  prefs: []
  type: TYPE_NORMAL
  zh: $$\theta^{(t)}\leftarrow\theta^{(t-1)}-\epsilon_{t}\frac{\partial L(z_{t},\theta)}{\partial\theta}$$
- en: where zt is an example sampled at iteration t and where t is a *hyper-parameter*
    that is called the *learning rate* and whose choice is crucial. If the learning
    rate is too large8, the average loss will increase. The optimal learning rate
    is usually close to (by a factor of 2) the largest learning rate that does not
    cause divergence of the training criterion, an observation that can guide heuristics
    for setting the learning rate [8], e.g., start with a large learning rate and
    if the training criterion diverges, try again with 3 times smaller learning rate,
    etc., until no divergence is observed.
  id: totrans-5377
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 zt 是在迭代 t 时采样的一个示例，而 t 是一个称为*学习率*的*超参数*，其选择至关重要。如果学习率过大，平均损失将增加。最佳学习率通常接近（以
    2 倍的因子）不导致训练标准发散的最大学习率，这一观察可以指导设置学习率的启发式方法[8]，例如，从较大的学习率开始，如果训练标准发散，则尝试 3 倍较小的学习率，等等，直到没有观察到发散。
- en: See [26] for a deeper treatment of stochastic gradient descent, including suggestions
    to set learning rate schedule and improve the asymptotic convergence through averaging.
  id: totrans-5378
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见[26]，以深入探讨随机梯度下降，包括设置学习率计划的建议和通过平均提高渐近收敛性。
- en: 'In practice, we use *mini-batch* updates based on an *average* of the gradients9
    inside each block of B examples:'
  id: totrans-5379
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们使用基于每个 B 示例块内梯度的*平均*的*迷你批*更新：
- en: $$\theta^{(t)}\leftarrow\theta^{(t-1)}-\epsilon_{t}\frac{1}{B}\sum_{t^{\prime}=B
    t+1}^{B(t+1)}\frac{\partial L(z_{t^{\prime}},\theta)}{\partial\theta}.$$
  id: totrans-5380
  prefs: []
  type: TYPE_NORMAL
  zh: $$\theta^{(t)}\leftarrow\theta^{(t-1)}-\epsilon_{t}\frac{1}{B}\sum_{t^{\prime}=B
    t+1}^{B(t+1)}\frac{\partial L(z_{t^{\prime}},\theta)}{\partial\theta}.$$
- en: $$(19.1)$$
  id: totrans-5381
  prefs: []
  type: TYPE_NORMAL
  zh: $$(19.1)$$
- en: 8 Above a value which is approximately 2 times the largest eigenvalue of the
    average loss Hessian matrix. 9 Compared to a sum, an average makes a small change
    in B have only a small effect on the optimal learning rate, with an increase in
    B generally allowing a small increase in the learning rate because of the reduced
    variance of the gradient.
  id: totrans-5382
  prefs: []
  type: TYPE_NORMAL
  zh: 8 超过约为平均损失 Hessian 矩阵最大特征值 2 倍的值。9 与总和相比，平均值使得 B 的小变化对最佳学习率仅有小影响，通常 B 的增加会允许学习率的轻微增加，因为梯度的方差降低。
- en: With B = 1 we are back to ordinary online gradient descent, while with B equal
    to the training set size, this is standard (also called "batch") gradient descent.
    With intermediate values of B there is generally a sweet spot. When B increases
    we can get more multiply-add operations per second by taking advantage of parallelism
    or efficient matrix-matrix multiplications (instead of separate matrixvector multiplications),
    often gaining a factor of 2 in practice in overall training time. On the other
    hand, as B increases, the number of updates per computation done decreases, which
    slows down convergence (in terms of error vs number of multiply-add operations
    performed) because less updates can be done in the same computing time. Combining
    these two opposing effects yields a typical U-curve with a sweet spot at an intermediate
    value of B.
  id: totrans-5383
  prefs: []
  type: TYPE_NORMAL
  zh: 当B = 1时，我们回到了普通的在线梯度下降，而当B等于训练集大小时，这就是标准的（也称为“批量”）梯度下降。对于中间值B，通常会有一个最佳点。当B增加时，我们可以通过利用并行性或高效的矩阵乘法（而不是单独的矩阵向量乘法）每秒获得更多的乘加操作，通常在整体训练时间上实际获得约2倍的收益。另一方面，随着B的增加，每次计算的更新次数减少，这会减慢收敛速度（以误差与所执行的乘加操作数量为准），因为在相同的计算时间内可以进行的更新次数减少。结合这两种对立效应，通常会出现一个典型的U型曲线，其中在B的中间值处有一个最佳点。
- en: Keep in mind that even the true gradient direction (averaging over the whole
    training set) is only the steepest descent direction locally but may not point
    in the right direction when considering larger steps. In particular, because the
    training criterion is not quadratic in the parameters, as one moves in parameter
    space the optimal descent direction keeps changing. Because the gradient direction
    is not quite the right direction of descent, there is no point in spending a lot
    of computation to estimate it precisely for gradient descent. Instead, doing more
    updates more frequently helps to explore more and faster, especially with large
    learning rates. In addition, smaller values of B may benefit from more exploration
    in parameter space and a form of regularization both due to the "noise" injected
    in the gradient estimator, which may explain the better test results sometimes
    observed with smaller B.
  id: totrans-5384
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，即使是真实的梯度方向（对整个训练集进行平均）也只是局部的最陡下降方向，但在考虑更大步伐时可能并不指向正确的方向。特别是因为训练标准在参数中不是二次的，随着在参数空间中的移动，最佳下降方向会不断变化。由于梯度方向并不是准确的下降方向，因此在梯度下降中花费大量计算来精确估计它是没有意义的。相反，更频繁地进行更多更新有助于更快更广泛地探索，特别是在学习率较大时。此外，较小的B值可能由于梯度估计中注入的“噪声”，受益于参数空间的更多探索和一种正则化形式，这可能解释了有时较小B值带来的更好测试结果。
- en: When the training set is finite, training proceeds by sweeps through the training
    set called an *epoch*, and full training usually requires many epochs (iterations
    through the training set). Note that stochastic gradient (either one example at
    a time or with mini-batches) is different from ordinary *gradient descent*, sometimes
    called "batch gradient descent", which corresponds to the case where B equals
    the training set size, i.e., there is one parameter update per epoch). The great
    advantage of stochastic gradient descent and other online or minibatch update
    methods is that their convergence does not depend on the size of the training
    set, only on the number of updates and the richness of the training distribution.
    In the limit of a large or infinite training set, a batch method (which updates
    only after seeing all the examples) is hopeless. In fact, even for ordinary datasets
    of tens or hundreds of thousands of examples (or more!), stochastic gradient descent
    converges much faster than ordinary (batch) gradient descent, and beyond some
    dataset sizes the speed-up is almost linear (i.e., doubling the size almost doubles
    the gain)10. It is really important to use the stochastic version in order to
    get reasonable clock-time convergence speeds.
  id: totrans-5385
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练集是有限时，训练通过称为*周期*的训练集遍历进行，完整训练通常需要多个周期（对训练集的迭代）。注意，随机梯度（一次一个示例或小批量）与普通的*梯度下降*不同，有时称为“批量梯度下降”，它对应于B等于训练集大小的情况，即每个周期更新一个参数）。随机梯度下降及其他在线或小批量更新方法的最大优势在于，它们的收敛不依赖于训练集的大小，仅依赖于更新次数和训练分布的丰富性。在大或无限训练集的极限情况下，批量方法（在看到所有示例后才更新）是无望的。事实上，即使对于数万或数十万示例（或更多！）的普通数据集，随机梯度下降的收敛速度也远快于普通（批量）梯度下降，并且在某些数据集大小之后，速度提升几乎是线性的（即，大小翻倍几乎将收益翻倍）。使用随机版本以获得合理的时钟时间收敛速度是非常重要的。
- en: As for any stochastic gradient descent method (including the mini-batch case),
  id: totrans-5386
  prefs: []
  type: TYPE_NORMAL
  zh: 至于任何随机梯度下降方法（包括小批量情况），
- en: it is important for efficiency of the estimator that each example or mini-batch
    be sampled approximately independently. Because random access to memory (or even
    worse, to disk) is expensive, a good approximation, called incremental
  id: totrans-5387
  prefs: []
  type: TYPE_NORMAL
  zh: 对估计器的效率而言，每个示例或小批量被近似独立地抽样是很重要的。因为随机访问内存（或更糟的是，访问磁盘）是昂贵的，所以一种称为增量的良好近似方法是必需的。
- en: 10 On the other hand, batch methods can be parallelized easily, which becomes
    an important advantage with currently available forms of computing power.
  id: totrans-5388
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，批量方法可以轻松并行化，这在当前可用的计算能力中成为一个重要优势。
- en: gradient [21], is to visit the examples (or mini-batches) in a fixed order corresponding
    to their order in memory or disk (repeating the examples in the same order on
    a second epoch, if we are not in the pure online case where each example is visited
    only once). In this context, it is safer if the examples or mini-batches are first
    put in a random order (to make sure this is the case, it could be useful to first
    shuffle the examples). Faster convergence has been observed if the order in which
    the mini-batches are visited is changed for each epoch, which can be reasonably
    efficient if the training set holds in computer memory.
  id: totrans-5389
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度[21]是以固定顺序访问示例（或小批量），对应于它们在内存或磁盘中的顺序（如果我们不是在纯在线情况下，即每个示例只访问一次，则在第二个周期重复以相同顺序访问示例）。在这种情况下，首先将示例或小批量随机排列更为安全（为了确保这一点，首先打乱示例可能很有用）。观察到，如果在每个周期中更改小批量访问的顺序，收敛速度会更快，如果训练集能够保存在计算机内存中，这样做是相当高效的。
- en: 19.2.2 Gradient Computation And Automatic Differentiation
  id: totrans-5390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.2.2 梯度计算与自动微分
- en: 'The gradient can be either computed manually or through automatic differentiation.
    Either way, it helps to structure this computation as a *flow graph*, in order
    to prevent mathematical mistakes and make sure an implementation is computationally
    efficient. The computation of the loss L(z,θ) as a function of θ is laid out in
    a graph whose nodes correspond to elementary operations such as addition, multiplication,
    and non-linear operations such as the neural networks activation function (e.g.,
    sigmoid or hyperbolic tangent), possibly at the level of vectors, matrices or
    tensors. The flow graph is directed and acyclic and has three types of nodes:
    input nodes, internal nodes, and output nodes. Each of its nodes is associated
    with a numerical output which is the result of the application of that computation
    (none in the case of input nodes), taking as input the output of previous nodes
    in a directed acyclic graph. Example z and parameter vector θ (or their elements)
    are the input nodes of the graph (i.e., they do not have inputs themselves) and
    L(z,θ) is a scalar output of the graph. Note that here, in the supervised case,
    z can include an input part x (e.g. an image) and a target part y (e.g. a target
    class associated with an object in the image). In the unsupervised case z = x.
    In a semi-supervised case, there is a mix of labeled and unlabeled examples, and
    z includes y on the labeled examples but not on the unlabeled ones.'
  id: totrans-5391
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度可以手动计算或通过自动微分计算。无论哪种方式，将此计算结构化为*流图*都有助于防止数学错误，并确保实现计算上高效。损失L(z,θ)作为θ的函数，其计算在一个图中展开，该图的节点对应于基本操作，如加法、乘法，以及非线性操作，如神经网络激活函数（例如，sigmoid或双曲正切），可能在向量、矩阵或张量的层面上。流图是有向无环的，并具有三种类型的节点：输入节点、内部节点和输出节点。每个节点都与一个数值输出相关联，该输出是该计算应用的结果（在输入节点的情况下没有输出），以有向无环图中前一节点的输出作为输入。示例z和参数向量θ（或其元素）是图的输入节点（即，它们没有输入），而L(z,θ)是图的标量输出。请注意，在监督情况下，z可以包括输入部分x（例如，一张图像）和目标部分y（例如，与图像中对象相关的目标类）。在无监督情况下，z
    = x。在半监督情况下，存在标签和未标签示例的混合，z在有标签示例中包括y，但在无标签示例中不包括。
- en: In addition to associating a numerical output oa to each node a of the flow
    graph, we can associate a gradient ga = ∂L(z,θ)
  id: totrans-5392
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将数值输出oa与流图的每个节点a相关联外，我们还可以将梯度ga = ∂L(z,θ)相关联
- en: ∂oa . The gradient will be defined and computed recursively in the graph, in
    the opposite direction of the computation of the nodes' outputs, i.e., whereas
    oa is computed using outputs op of *predecessor* nodes p of a, ga will be computed
    using the gradients gs of *successor* nodes s of a. More precisely, the chain
    rule dictates
  id: totrans-5393
  prefs: []
  type: TYPE_NORMAL
  zh: ∂oa。梯度将在图中递归定义和计算，方向与节点输出的计算相反，即，虽然oa是通过*前驱*节点p的输出op计算的，ga将通过*a*的*后继*节点s的梯度gs计算。更准确地说，链式法则规定
- en: $$g_{a}=\sum_{s}g_{s}{\frac{\partial o_{s}}{\partial o_{a}}}$$
  id: totrans-5394
  prefs: []
  type: TYPE_NORMAL
  zh: $$g_{a}=\sum_{s}g_{s}{\frac{\partial o_{s}}{\partial o_{a}}}$$
- en: where the sum is over immediate successors of a. Only output nodes have no successor,
    and in particular for the output node that computes L, the gradient is set to
    1 since ∂L
  id: totrans-5395
  prefs: []
  type: TYPE_NORMAL
  zh: 其中求和是针对a的直接后继节点。只有输出节点没有后继，特别是对于计算L的输出节点，梯度设置为1，因为∂L
- en: ∂L = 1, thus initializing the recursion. Manual or automatic differentiation
    then only requires to define the partial derivative associated with each type
    of operation performed by any node of the graph. When implementing gradient descent
    algorithms with manual differentiation the result tends to be verbose, brittle
    code that lacks modularity - all bad things in terms of software engineering.
    A better approach is to express the flow graph in terms of objects that modularize
    how to compute outputs from inputs as well as how to compute the partial derivatives
    necessary for gradient descent. One can pre-define the operations of these objects
    (in a "forward propagation" or fprop method) and their partial derivatives (in
    a "backward propagation" or bprop method) and encapsulate these computations in
    an object that knows how to compute its output given its inputs, and how to compute
    the gradient with respect to its inputs given the gradient with respect to its
    output. This is the strategy adopted in the Theano library11 with its Op objects
    [18], as well as in libraries such as Torch12 [37] and Lush13.
  id: totrans-5396
  prefs: []
  type: TYPE_NORMAL
  zh: ∂L = 1，从而初始化递归。手动或自动微分则只需定义与图中任何节点执行的每种操作相关的偏导数。当使用手动微分实现梯度下降算法时，结果往往是冗长、脆弱的代码，缺乏模块性——在软件工程方面都是坏事。一个更好的方法是将流程图表示为模块化如何从输入计算输出以及如何计算梯度下降所需的偏导数的对象。可以预定义这些对象的操作（在“前向传播”或
    fprop 方法中）及其偏导数（在“后向传播”或 bprop 方法中），并将这些计算封装在一个对象中，该对象知道如何根据其输入计算输出，以及如何在给定相对于其输出的梯度的情况下计算相对于其输入的梯度。这是
    Theano 库采用的策略，使用其 Op 对象，以及在 Torch 和 Lush 等库中。
- en: 'Compared to Torch and Lush, Theano adds an interesting ingredient which makes
    it a full-fledged automatic differentiation tool: symbolic computation. The flow
    graph itself (without the numerical values attached) can be viewed as a symbolic
    representation (in a data structure) of a numerical computation. In Theano, the
    gradient computation is first performed symbolically, i.e., each Op object knows
    how to create other Ops corresponding to the computation of the partial derivatives
    associated with that Op. Hence the *symbolic differentiation* of the output of
    a flow graph with respect to any or all of its input nodes can be performed easily
    in most cases, yielding another flow graph which specifies how to compute these
    gradients, given the input of the original graph. Since the gradient graph typically
    contains the original graph (mapping parameters to loss) as a sub-graph, in order
    to make computations efficient it is important to automate (as done in Theano)
    a number of *simplifications* which are graph transformations preserving the semantics
    of the output (given the input) but yielding smaller (or more numerically stable
    or more efficiently computed) graphs (e.g.,'
  id: totrans-5397
  prefs: []
  type: TYPE_NORMAL
  zh: '与 Torch 和 Lush 相比，Theano 添加了一个有趣的成分，使其成为一个功能齐全的自动微分工具：符号计算。流程图本身（不带数值）可以被视为数值计算的符号表示（在数据结构中）。在
    Theano 中，梯度计算首先是以符号方式进行的，即每个 Op 对象知道如何创建与该 Op 相关的偏导数计算的其他 Ops。因此，对于流程图的输出相对于其任何或所有输入节点的*符号微分*在大多数情况下可以轻松执行，生成另一个流程图，该图指定如何在给定原始图输入的情况下计算这些梯度。由于梯度图通常将原始图（将参数映射到损失）作为子图，因此为了提高计算效率，自动化（如
    Theano 中所做的）一些*简化*是重要的，这些简化是保持输出语义的图变换（给定输入），但生成更小（或数值上更稳定或计算效率更高）的图（例如，  '
- en: removing redundant computations). To take advantage of the fact that computing
    the loss gradient includes as a first step computing the loss itself, it is advantageous
    to structure the code so that both the loss and its gradient are computed at once,
    with a single graph having multiple outputs. The advantages of performing gradient
    computations symbolically are numerous. First of all, one can readily compute
    gradients over gradients, i.e., second derivatives, which are useful for some
    learning algorithms. Second, one can define algorithms or training criteria involving
    gradients themselves, as required for example in the Contractive Auto-Encoder
    (which uses the norm of a Jacobian matrix in its training criterion, i.e., really
    requires second derivatives, which here are cheap to compute). Third, it makes
    it easy to implement other useful graph transformations such as graph simplifications
    or numerical optimizations and transformations that help making the numerical
    results more robust and more efficient (such as working in the domain of logarithms
    of probabilities rather than in the domain of probabilities directly). Other potential
    beneficial applications of such symbolic manipulations include parallelization
    and additional differential operators
  id: totrans-5398
  prefs: []
  type: TYPE_NORMAL
  zh: 去除冗余计算）。为了利用计算损失梯度的事实，首先需要计算损失本身，最好将代码结构化，使得损失及其梯度一次性计算，使用一个具有多个输出的单一图形。以符号方式执行梯度计算的优点是多方面的。首先，可以方便地计算梯度的梯度，即二阶导数，这对某些学习算法非常有用。其次，可以定义涉及梯度本身的算法或训练标准，例如在收缩自编码器中（该算法在其训练标准中使用雅可比矩阵的范数，即真正需要二阶导数，而在这里计算成本很低）。第三，这使得实现其他有用的图形转换变得容易，例如图形简化或数值优化和转换，这有助于使数值结果更健壮和高效（例如，在概率的对数域中工作，而不是直接在概率域中）。其他潜在的有益应用包括并行化和额外的微分运算符。
- en: (such as the R-operator, recently implemented in Theano, which is very useful
    to
  id: totrans-5399
  prefs: []
  type: TYPE_NORMAL
  zh: （例如，最近在Theano中实现的R算子，非常有用）
- en: 11 http://deeplearning.net/software/theano/ 12 http://www.torch.ch 13 http://lush.sourceforge.net
  id: totrans-5400
  prefs: []
  type: TYPE_NORMAL
  zh: 11 http://deeplearning.net/software/theano/ 12 http://www.torch.ch 13 http://lush.sourceforge.net
- en: compute the product of a Jacobian matrix ∂f(x)
  id: totrans-5401
  prefs: []
  type: TYPE_NORMAL
  zh: 计算雅可比矩阵的乘积 ∂f(x)
- en: ∂x or Hessian matrix ∂2L(x,θ)
  id: totrans-5402
  prefs: []
  type: TYPE_NORMAL
  zh: ∂x 或海森矩阵 ∂2L(x,θ)
- en: ∂θ2 with a vector without ever having to actually compute and store the matrix
    itself [90]).
  id: totrans-5403
  prefs: []
  type: TYPE_NORMAL
  zh: ∂θ2与一个向量结合，而不必实际计算和存储矩阵本身[90]。
- en: 19.3 Hyper-Parameters
  id: totrans-5404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.3 超参数
- en: A pure learning algorithm can be seen as a function taking training data as
    input and producing as output a function (e.g. a predictor) or model (i.e. a bunch
    of functions). However, in practice, many learning algorithms involve hyperparameters,
    i.e., annoying knobs to be adjusted. In many algorithms such as Deep Learning
    algorithms the number of hyper-parameters (ten or more!) can make the idea of
    having to adjust all of them unappealing. In addition, it has been shown that
    the use of computer clusters for hyper-parameter selection can have an important
    effect on results [91]. Choosing hyper-parameter values is formally equivalent
    to the question of *model selection*, i.e., given a family or set of learning
    algorithms, how to pick the most appropriate one inside the set? We define a hyper-parameter
    for a learning algorithm A as a variable to be set prior to the actual application
    of A to the data, one that is not directly selected by the learning algorithm
    itself. It is basically an outside control knob. It can be discrete (as in model
    selection) or continuous (such as the learning rate discussed above).
  id: totrans-5405
  prefs: []
  type: TYPE_NORMAL
  zh: 纯学习算法可以视为一个函数，它以训练数据为输入，输出一个函数（例如，一个预测器）或模型（即一组函数）。然而，在实践中，许多学习算法涉及超参数，即需要调整的烦人旋钮。在许多算法中，例如深度学习算法，超参数的数量（十个或更多！）使得调整所有参数的想法变得不吸引人。此外，已经证明使用计算机集群进行超参数选择对结果有重要影响[91]。选择超参数值在形式上等同于*模型选择*的问题，即在给定一组学习算法的情况下，如何从中选择最合适的一个？我们将学习算法A的超参数定义为在实际应用A于数据之前需要设置的变量，而这个变量并不是由学习算法本身直接选择的。它基本上是一个外部控制旋钮。它可以是离散的（如模型选择中）或连续的（例如，上述讨论的学习率）。
- en: Of course, one can hide these hyper-parameters by wrapping another learning
    algorithm, say B, around A, to selects A's hyper-parameters (e.g. to minimize
    validation set error). We can then call B a hyper-learner, and if B has no hyperparameters
    itself then the composition of B over A could be a "pure" learning algorithm,
    with no hyper-parameter. In the end, to apply a learner to training data, one
    has to have a pure learning algorithm. The hyper-parameters can be fixed by hand
    or tuned by an algorithm, but their value has to be selected. The value of some
    hyper-parameters can be selected based on the performance of A on its training
    data, but most cannot. For any hyper-parameter that has an impact on the effective
    capacity of a learner, it makes more sense to select its value based on out-of-sample
    data (outside the training set), e.g., a validation set performance, online error,
    or cross-validation error. Note that some learning algorithms (in particular unsupervised
    learning algorithms such as algorithms for training RBMs by approximate maximum
    likelihood) are problematic in this respect because we cannot directly measure
    the quantity that is to be optimized (e.g. the likelihood) because it is intractable.
    On the other hand, the expected denoising reconstruction error is easy to estimate
    (by just averaging the denoising error over a validation set).
  id: totrans-5406
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，可以通过将另一个学习算法（例如B）包装在A周围来隐藏这些超参数，从而选择A的超参数（例如，最小化验证集误差）。我们可以称B为超学习者，如果B本身没有超参数，那么B与A的组合可以是一个“纯”学习算法，没有超参数。最终，要将学习算法应用于训练数据，必须有一个纯学习算法。超参数可以手动固定或通过算法调整，但其值必须被选定。某些超参数的值可以根据A在其训练数据上的表现来选择，但大多数不能。对于任何影响学习者有效容量的超参数，基于样本外数据（训练集外）选择其值更为合理，例如基于验证集表现、在线错误或交叉验证误差。请注意，一些学习算法（尤其是无监督学习算法，例如通过近似最大似然训练RBM的算法）在这方面是有问题的，因为我们无法直接测量要优化的量（例如似然性），因为它是不可解的。另一方面，预期去噪重建误差是容易估计的（只需对验证集上的去噪误差进行平均）。
- en: Once some out-of-sample data has been used for selecting hyper-parameter values,
    it cannot be used anymore to obtain an unbiased estimator of generalization performance,
    so one typically uses a test set (or double cross-validation14, in
  id: totrans-5407
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦一些样本外数据被用于选择超参数值，就不能再用于获得无偏的泛化性能估计，因此通常使用测试集（或双重交叉验证14，）。
- en: 14 Double cross-validation applies recursively the idea of cross-validation,
    using an outer loop cross-validation to evaluate generalization error and then
    applying an inner loop cross-validation inside each outer loop split's training
    subset (i.e., splitting it again into training and validation folds) in order
    to select hyper-parameters for that split.
  id: totrans-5408
  prefs: []
  type: TYPE_NORMAL
  zh: 14 双重交叉验证递归应用交叉验证的思想，使用外层循环交叉验证来评估泛化误差，然后在每个外层循环分割的训练子集内部应用内层循环交叉验证（即再次将其拆分为训练和验证折）以选择该分割的超参数。
- en: the case of small datasets) to estimate generalization error of the pure learning
    algorithm (with hyper-parameter selection hidden inside).
  id: totrans-5409
  prefs: []
  type: TYPE_NORMAL
  zh: （小数据集的情况）来估计纯学习算法的泛化误差（将超参数选择隐藏在内部）。
- en: 19.3.1 Neural Network Hyper-Parameters
  id: totrans-5410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.3.1 神经网络超参数
- en: 'Different learning algorithms involve different sets of hyper-parameters, and
    it is useful to get a sense of the kinds of choices that practitioners have to
    make in choosing their values. We focus here mostly on those relevant to neural
    networks and Deep Learning algorithms. Hyper-parameters of the Approximate Optimization.
    First of all, several learning algorithms can be viewed as the combination of
    two elements: a training criterion and a model (e.g., a family of functions, a
    parametrization) on the one hand, and on the other hand, a particular procedure
    for approximately optimizing this criterion. Correspondingly, one should distinguish
    hyper-parameters associated with the optimizer from hyper-parameters associated
    with the model itself, i.e., typically the function class, regularizer and loss
    function. We have already mentioned above some of the hyper-parameters typically
    associated with gradient-based optimization. Here is a more extensive descriptive
    list, focusing on those used in stochastic (mini-batch) gradient descent (although
    number of training iterations is used for all iterative optimization algorithms).'
  id: totrans-5411
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的学习算法涉及不同的超参数集，了解从业者在选择其值时所需做出的选择是有益的。我们主要关注与神经网络和深度学习算法相关的那些超参数。近似优化的超参数。首先，几种学习算法可以视为两个元素的组合：一方面是训练标准和模型（例如，一组函数，一个参数化），另一方面是近似优化该标准的特定程序。因此，应区分与优化器相关的超参数与与模型本身相关的超参数，即，通常是函数类、正则化器和损失函数。我们在上文中已经提到了一些与基于梯度的优化相关的超参数。这里是一个更全面的描述列表，重点关注那些在随机（小批量）梯度下降中使用的超参数（尽管训练迭代次数适用于所有迭代优化算法）。
- en: '- The initial learning rate (0 below, Eq.(19.2)). This is often the single
    most important hyper-parameter and one should always make sure that it has been
    tuned (up to approximately a factor of 2). Typical values for a neural network
    with standardized inputs (or inputs mapped to the (0,1)'
  id: totrans-5412
  prefs: []
  type: TYPE_NORMAL
  zh: '- 初始学习率（下文为0，公式(19.2)）。这通常是最重要的超参数之一，应该始终确保它已被调整（大约调整到2倍）。对于标准化输入的神经网络（或映射到(0,1)的输入），典型值为'
- en: interval) are less than 1 and greater than 10−6 but these should not be taken
    as strict ranges and greatly depend on the parametrization of the model. A default
    value of 0.01 typically works for standard multi-layer neural networks but it
    would be foolish to rely exclusively on this default value. If there is only time
    to optimize one hyper-parameter and one uses stochastic gradient descent, then
    this is the hyper-parameter that is worth tuning.
  id: totrans-5413
  prefs: []
  type: TYPE_NORMAL
  zh: 间隔) 小于1且大于10−6，但这些不应被视为严格范围，且极大依赖于模型的参数化。对于标准的多层神经网络，默认值0.01通常有效，但仅依赖于此默认值是愚蠢的。如果只有时间来优化一个超参数，并且使用随机梯度下降，那么这个超参数值得调整。
- en: '- The choice of strategy for decreasing or adapting the learning rate schedule
    (with hyper-parameters such as the time constant τ in Eq. (19.2) below).'
  id: totrans-5414
  prefs: []
  type: TYPE_NORMAL
  zh: '- 选择降低或调整学习率调度的策略（带有超参数，例如公式(19.2)中的时间常数τ）。'
- en: The default value of τ → ∞ means that the learning rate is constant over training
    iterations. In many cases the benefit of choosing other than this default value
    is small. An example of O(1/t) learning rate schedule, used in Bergstra and Bengio
    [17] is
  id: totrans-5415
  prefs: []
  type: TYPE_NORMAL
  zh: 默认值τ → ∞意味着学习率在训练迭代中是恒定的。在许多情况下，选择其他非默认值的好处很小。Bergstra和Bengio [17]中使用的O(1/t)学习率调度的一个例子是
- en: $$\epsilon_{t}={\frac{\epsilon_{0}\tau}{\operatorname*{max}(t,\tau)}}$$
  id: totrans-5416
  prefs: []
  type: TYPE_NORMAL
  zh: $$\epsilon_{t}={\frac{\epsilon_{0}\tau}{\operatorname*{max}(t,\tau)}}$$
- en: $$(19.2)$$
  id: totrans-5417
  prefs: []
  type: TYPE_NORMAL
  zh: $$(19.2)$$
- en: max(*t, τ*) (19.2)
  id: totrans-5418
  prefs: []
  type: TYPE_NORMAL
  zh: max(*t, τ*) (19.2)
- en: 'which keeps the learning rate constant for the first τ steps and then decreases
    it in O(1/tα), with traditional recommendations (based on asymptotic analysis
    of the convex case) suggesting α = 1. See Bach and Moulines [2] for a recent analysis
    of the rate of convergence for the general case of α ≤ 1, suggesting that smaller
    values of α should be used in the non-convex case, especially when using a gradient
    averaging or momentum technique (see below). An adaptive and heuristic way of
    automatically setting τ above is to keep t constant until the training criterion
    stops decreasing significantly (by more than some relative improvement threshold)
    from epoch to epoch. That threshold is a less sensitive hyper-parameter than τ
    itself. An alternative to a fixed schedule with a couple of (global) free hyper-parameters
    like in the above formula is the use of an *adaptive* learning rate heuristic,
    e.g., the simple procedure proposed in [26]: at regular intervals during training,
    using a fixed small subset of the training set (what matters is only the number
    of examples used, not what fraction of the whole training set it represents),
    continue training with N different choices of learning rate (all in parallel),'
  id: totrans-5419
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法在前τ步保持学习率恒定，然后以O(1/tα)降低，传统建议（基于凸情况下的渐近分析）建议α = 1。有关α ≤ 1的一般情况的收敛速度的最近分析，请参见Bach和Moulines
    [2]，该分析建议在非凸情况下使用较小的α值，尤其是在使用梯度平均或动量技术时（见下文）。自动设置上述τ的自适应和启发式方法是，在训练标准显著不再减少（超过某个相对改进阈值）之前保持t不变。这一阈值是一个不如τ敏感的超参数。与上述公式中的几个（全局）自由超参数的固定计划相比，使用*自适应*学习率启发式，例如在[26]中提出的简单过程：在训练期间定期使用固定的小训练集子集（重要的是使用的样本数量，而不是它占整个训练集的比例），继续进行N种不同学习率选择（全部并行），
- en: and keep the value that gave the best results until the next re-estimation of
    the optimal learning rate. Other examples of adaptive learning rate strategies
    are discussed below (Sec. 19.6.2).
  id: totrans-5420
  prefs: []
  type: TYPE_NORMAL
  zh: 并保持提供最佳结果的值，直到下次重新评估最佳学习率。下文讨论了其他自适应学习率策略的例子（见第19.6.2节）。
- en: '- The mini-batch size (B in Eq. (19.1)) is typically chosen between 1 and a
    few hundreds, e.g. B = 32 is a good default value, with values above 10 taking
    advantage of the speed-up of matrix-matrix products over matrix-vector products.
    The impact of B is mostly computational, i.e., larger B yield faster computation
    (with appropriate implementations) but requires visiting more examples in order
    to reach the same error, since there are less updates per epoch. In theory, this
    hyper-parameter should impact *training time* and not so much *test performance*,
    so it can be optimized separately of the other hyper-parameters, by comparing
    training curves (training and validation error vs amount of training time), after
    the other hyper-parameters (except learning rate) have been selected. B and 0
    may slightly interact with other hyper-parameters so both should be re-optimized
    at the end. Once B is selected, it can generally be fixed while the other hyper-parameters
    can be further optimized (except for a *momentum* hyper-parameter, if one is used).'
  id: totrans-5421
  prefs: []
  type: TYPE_NORMAL
  zh: '- 小批量大小（在公式(19.1)中为B）通常选择在1到几百之间，例如B = 32是一个良好的默认值，超过10的值可以利用矩阵乘法相对于矩阵-向量乘法的加速。B的影响主要是计算上的，即较大的B可以加快计算（在适当的实现下），但需要访问更多样本以达到相同的错误，因为每个epoch的更新较少。从理论上讲，这个超参数应该影响*训练时间*，而不太影响*测试性能*，因此可以在选择其他超参数后，通过比较训练曲线（训练和验证误差与训练时间）来单独优化。B和0可能与其他超参数略有交互，因此两者在最后应重新优化。一旦选择了B，它通常可以固定，而其他超参数可以进一步优化（如果使用动量超参数，则除外）。'
- en: '- Number of training iterations T (measured in mini-batch updates). This hyper-parameter
    is particular in that it can be optimized almost for free using the principle
    of *early stopping*: by keeping track of the out-of-sample error (as for example
    estimated on a validation set) as training progresses (every N updates), one can
    decide how long to train for any given setting of all the other hyper-parameters.
    Early stopping is an inexpensive way to avoid strong overfitting, i.e., even if
    the other hyper-parameters would yield to overfitting, early stopping will considerably
    reduce the overfitting damage that would otherwise ensue. It also means that it
    hides the overfitting effect of other hyper-parameters, possibly obscuring the
    analysis that one may want to do when trying to figure out the effect of individual
    hyper-parameters, i.e., it tends to even out the performance obtained by many
    otherwise overfitting configurations of hyper-parameters by compensating a too
    large capacity with a smaller training time. For this reason, it might be useful
    to turn early-stopping off when analyzing the effect of individual hyper-parameters.
    Now let us turn to implementation details. Practically, one needs to continue
    training beyond the selected number of training iterations Tˆ (which should be
    the point of lowest validation error in the training run) in order to ascertain
    that validation error is unlikely to go lower than at the selected point. A'
  id: totrans-5422
  prefs: []
  type: TYPE_NORMAL
  zh: 训练迭代次数 T（以小批量更新为单位）。这个超参数的特殊之处在于，可以几乎免费地使用*早停*原则进行优化：通过在训练过程中（每 N 次更新）跟踪样本外误差（例如在验证集上估计），可以决定在所有其他超参数的设置下训练多长时间。早停是一种廉价的方式，可以避免强烈的过拟合，即使其他超参数会导致过拟合，早停也会显著减少否则可能出现的过拟合损害。这也意味着它掩盖了其他超参数的过拟合效应，可能会模糊在尝试弄清楚单个超参数的影响时所需的分析，即它倾向于平衡许多其他过拟合超参数配置所获得的性能，通过用较小的训练时间补偿过大的容量。因此，在分析单个超参数的影响时，关闭早停可能会很有用。现在让我们转向实施细节。实际上，需要继续训练超过所选的训练迭代次数
    Tˆ（应为训练运行中最低验证误差的点），以确保验证误差不太可能低于所选点。
- en: heuristic introduced in the *Deep Learning Tutorials*15 is based on the idea
    of *patience* (set initially to 10000 examples in the MLP tutorial), which is
    a minimum number of training examples to see after the candidate selected point
    Tˆ before deciding to stop training (i.e. before accepting this candidate as the
    final answer). As training proceeds and new candidate selected points Tˆ (new
    minima of the validation error) are observed, the patience parameter is increased,
    either multiplicatively or additively on top of the last Tˆ found.
  id: totrans-5423
  prefs: []
  type: TYPE_NORMAL
  zh: 在*深度学习教程*中引入的启发式方法基于*耐心*的概念（在 MLP 教程中初始设置为 10000 个样本），这是在决定停止训练之前（即在接受该候选作为最终答案之前）需要查看的最小训练样本数，时间点
    Tˆ 选定后。如果在训练过程中观察到新的候选时间点 Tˆ（验证误差的新最小值），则耐心参数会增加，无论是乘法还是加法叠加在最后找到的 Tˆ 上。
- en: Hence, if we find a new minimum16 at t, we save the current best model, update
    Tˆ ← t and we increase our patience up to t+constant or t× constant.
  id: totrans-5424
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们在 t 时找到新的最小值，我们保存当前最佳模型，更新 Tˆ ← t，并将我们的耐心增加到 t+常数或 t×常数。
- en: Note that validation error should not be estimated after each training update
  id: totrans-5425
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，验证误差不应在每次训练更新后进行估计。
- en: (that would be really wasteful) but after every N examples, where N is at least
    as large as the validation set (ideally several times larger so that the early
    stopping overhead remains small)17.
  id: totrans-5426
  prefs: []
  type: TYPE_NORMAL
  zh: （这会非常浪费），但在每 N 个样本后，其中 N 至少与验证集一样大（理想情况下大几倍，以保持早停开销较小）。
- en: '- Momentum β. It has long been advocated [56, 59] to temporally smooth out
    the stochastic gradient samples obtained during the stochastic gradient descent.
    For example, a moving average of the past gradients can be computed with g¯ ←
    (1 − β)¯g + βg, where g is the instantaneous gradient'
  id: totrans-5427
  prefs: []
  type: TYPE_NORMAL
  zh: '- 动量 β。长期以来，人们一直主张[56, 59]在随机梯度下降过程中对获得的随机梯度样本进行时间平滑。例如，可以计算过去梯度的移动平均：g¯ ←
    (1 − β)¯g + βg，其中 g 是瞬时梯度。'
- en: ∂L(zt,θ)
  id: totrans-5428
  prefs: []
  type: TYPE_NORMAL
  zh: ∂L(zt,θ)
- en: ∂θ or a minibatch average, and β is a small positive coefficient that controls
    how fast the old examples get downweighted in the moving average. The simplest
    momentum trick is to make the updates proportional to this smoothed gradient estimator
    g¯ instead of the instantaneous gradient g. The idea is that it removes some of
    the noise and oscillations that gradient descent has, in particular in the directions
    of high curvature of the loss function18. A default value of β = 1 (no momentum)
    works well in many cases but in some cases momentum seems to make a positive difference.
    Polyak averaging [93] is a related form of parameter averaging19 that has theoretical
    advantages and has been advocated and shown to bring improvements on some unsupervised
    learning procedures such as RBMs [110]. More recently, several mathematically
    motivated algorithms [88, 75] have been proposed that incorporate some form of
    momentum and that also ensure much faster convergence (linear rather than sublinear)
    compared to stochastic gradient
  id: totrans-5429
  prefs: []
  type: TYPE_NORMAL
  zh: ∂θ或小批量平均，β是一个控制旧示例在移动平均中降低权重速度的小正系数。最简单的动量技巧是使更新与这个平滑的梯度估计g¯成比例，而不是瞬时梯度g。这个想法是去除一些随机梯度下降所带来的噪声和振荡，特别是在损失函数的高曲率方向上18。默认值β
    = 1（无动量）在许多情况下效果很好，但在某些情况下，动量似乎带来了积极的效果。Polyak平均[93]是一种相关的参数平均形式，具有理论优势，已被提倡并在某些无监督学习程序（如RBM）中证明能带来改进[110]。最近，提出了几种数学动机的算法[88,
    75]，它们结合某种形式的动量，并且确保比随机梯度下降更快的收敛（线性而非子线性）。
- en: 15 http://deeplearning.net/tutorial/ 16 Ideally, we should use a statistical
    test of significance and accept a new minimum
  id: totrans-5430
  prefs: []
  type: TYPE_NORMAL
  zh: 15 [http://deeplearning.net/tutorial/](http://deeplearning.net/tutorial/) 16
    理想情况下，我们应该使用显著性统计检验并接受新的最小值
- en: (over a longer training period) only if the improvement is statistically significant,
    based on the size and variance estimates one can compute for the validation set.
    17 When an extra processor on the same machine is available, validation error
    can conveniently be recomputed by a processor different from the one performing
    the training updates, allowing more frequent computation of validation error.
    18 Think about a ball coming down a valley. Since it has not started from the
    bottom of the valley it will oscillate between its sides as it settles deeper,
    forcing the learning rate to be small to avoid large oscillations that would kick
    it out of the valley. Averaging out the local gradients along the way will cancel
    the opposing forces from each side of the valley. 19 Polyak averaging uses for
    predictions a moving average of the parameters found in the trajectory of stochastic
    gradient descent.
  id: totrans-5431
  prefs: []
  type: TYPE_NORMAL
  zh: （在更长的训练期间）仅当改善在统计上显著时，基于可以为验证集计算的大小和方差估计。17 当同一机器上的额外处理器可用时，可以通过与执行训练更新不同的处理器方便地重新计算验证误差，从而允许更频繁地计算验证误差。18
    想象一个球在谷底下落。由于它并未从谷底开始，它将在两侧之间振荡，随着它的深入，迫使学习率保持小以避免大振荡将其踢出谷底。在这个过程中平均局部梯度将抵消来自谷底两侧的相反力量。19
    Polyak平均利用随机梯度下降轨迹中找到的参数的移动平均进行预测。
- en: descent, at least for convex optimization problems. See also [26] for an example
    of averaged SGD with successful empirical speedups in the convex case.
  id: totrans-5432
  prefs: []
  type: TYPE_NORMAL
  zh: 下降，至少对于凸优化问题而言。有关成功经验加速的平均SGD示例，请参见[26]。
- en: Note however that in the pure online case (stream of examples) and under some
    assumptions, the sublinear rate of convergence of stochastic gradient descent
    with O(1/t) decrease of learning rate is an optimal rate, at least for convex
    problems [87]. That would suggest that for really large training sets it may not
    be possible to obtain better rates than ordinary stochastic gradient descent,
    albeit the constants in front (which depend on the condition number of the Hessian)
    may still be greatly reduced by using second-order information online [28, 27].
  id: totrans-5433
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，在纯在线情况下（示例流）并在某些假设下，随机梯度下降的子线性收敛速度，以O(1/t)的学习率减少，是一种**最优速率**，至少对于凸问题而言[87]。这表明，对于非常大的训练集，可能无法获得比普通随机梯度下降更好的速率，尽管前面的常数（依赖于Hessian的条件数）仍然可以通过使用在线的二阶信息大大减少[28,
    27]。
- en: '- Layer-specific optimization hyper-parameters: although rarely done, it is
    possible to use different values of optimization hyper-parameters (such as the
    learning rate) on different layers of a multi-layer network. This is especially
    appropriate (and easier to do) in the context of layer-wise unsupervised pre-training,
    since each layer is trained separately (while the layers below are kept fixed).
    This would be particularly useful when the number of units per layer varies a
    lot from layer to layer. See the paragraph below entitled Layer-wise optimization
    of hyper-parameters (Sec. 19.3.3). Some researchers also advocate the use of different
    learning rates for the different types of parameters one finds in the model, such
    as biases and weights in the standard multi-layer network, but the issue becomes
    more important when parameters such as precision or variance are included in the
    lot [38].'
  id: totrans-5434
  prefs: []
  type: TYPE_NORMAL
  zh: '- 层特定的优化超参数：尽管很少这样做，但在多层网络的不同层上使用不同的优化超参数（例如学习率）是可能的。这在层级无监督预训练的上下文中尤其合适（且更容易实现），因为每一层是单独训练的（同时保持下面的层固定）。当每层的单元数量差异较大时，这将特别有用。请参见下文标题为“超参数的层级优化”（第
    19.3.3 节）。一些研究人员还主张对模型中不同类型的参数（如标准多层网络中的偏置和权重）使用不同的学习率，但当像精度或方差这样的参数包含在内时，这个问题就显得更为重要。'
- en: Up to now we have only discussed the hyper-parameters in the setup where one
    trains a neural network by stochastic gradient descent. With other optimization
    algorithms, some hyper-parameters are typically different. For example, Conjugate
    Gradient (CG) algorithms typically have a number of line search steps (which is
    a hyper-parameter) and a tolerance for stopping each line search
  id: totrans-5435
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们仅讨论了在通过随机梯度下降训练神经网络时的超参数。在其他优化算法中，一些超参数通常会有所不同。例如，共轭梯度（CG）算法通常有多个线搜索步骤（这是一个超参数）和停止每次线搜索的容忍度（另一个超参数）。
- en: (another hyper-parameter). An optimization algorithm like L-BFGS (limitedmemory
    Broyden-Fletcher-Goldfarb-Shanno) also has a hyper-parameter controlling the memory
    usage of the algorithm, the rank of the Hessian approximation kept in memory,
    which also has an influence on the efficiency of each step.
  id: totrans-5436
  prefs: []
  type: TYPE_NORMAL
  zh: 像 L-BFGS（有限记忆 Broyden-Fletcher-Goldfarb-Shanno）这样的优化算法也有一个控制算法内存使用的超参数，即在内存中保持的海森矩阵近似的秩，这也会影响每一步的效率。
- en: Both CG and L-BFGS are iterative (e.g., one line search per iteration), and
    the number of iterations can be optimized as described above for stochastic gradient
    descent, with early stopping.
  id: totrans-5437
  prefs: []
  type: TYPE_NORMAL
  zh: CG 和 L-BFGS 都是迭代算法（例如，每次迭代进行一次线搜索），迭代次数可以如上所述通过随机梯度下降进行优化，并且可使用提前停止。
- en: 19.3.2 Hyper-Parameters Of The Model And Training Criterion
  id: totrans-5438
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.3.2 模型和训练标准的超参数
- en: Let us now turn to "model" and "criterion" hyper-parameters typically found
    in neural networks, especially deep neural networks.
  id: totrans-5439
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向神经网络中通常发现的“模型”和“标准”超参数，特别是深度神经网络。
- en: '- Number of hidden units nh. Each layer in a multi-layer neural network typically
    has a size that we are free to set and that controls capacity. Because of early
    stopping and possibly other regularizers (e.g., weight decay, discussed below),
    it is mostly important to choose nh large enough. Larger than optimal values typically
    do not hurt generalization performance much, but of course they require proportionally
    more computation (in O(n2h) if scaling all the layers at the same time in a fully
    connected architecture). Like for many other hyper-parameters, there is the option
    of allowing a different value of nh for each hidden layer20 of a deep architecture.
    See the paragraph below entitled Layer-wise optimization of hyper-parameters (Sec.
    19.3.3). In a large comparative study [70], we found that using the same size
    for all layers worked generally better or the same as using a decreasing size'
  id: totrans-5440
  prefs: []
  type: TYPE_NORMAL
  zh: '- 隐藏单元的数量 nh。多层神经网络中的每一层通常有一个可以自由设置的大小，这个大小控制着容量。由于提前停止和可能的其他正则化方法（例如，权重衰减，下文讨论），选择足够大的
    nh 是非常重要的。通常，过大的值对泛化性能影响不大，但当然会需要成比例更多的计算（在完全连接的架构中，如果同时扩展所有层，复杂度为 O(n²h)）。与许多其他超参数一样，深层架构中的每个隐藏层也可以允许不同的
    nh 值。请参见下文标题为“超参数的层级优化”（第 19.3.3 节）。在一项大型比较研究中，我们发现为所有层使用相同的大小通常效果更好或与使用递减大小相同。'
- en: (pyramid-like) or increasing size (upside down pyramid), but of course this
    may be data-dependent. For most tasks that we worked on, we find that an overcomplete21
    first hidden layer works better than an undercomplete one.
  id: totrans-5441
  prefs: []
  type: TYPE_NORMAL
  zh: （金字塔状）或增大的大小（倒金字塔），但当然这可能依赖于数据。对于我们处理的大多数任务，我们发现过完备的第一隐藏层效果优于欠完备的层。
- en: Another even more often validated empirical observation is that the optimal
    nh is much larger when using unsupervised pre-training in a supervised neural
    network, e.g., going from hundreds of units to thousands of units. A plausible
    explanation is that after unsupervised pre-training many of the hidden units are
    carrying information that is irrelevant to the specific supervised task of interest.
    In order to make sure that the information relevant to the task is captured, larger
    hidden layers are therefore necessary when using unsupervised pre-training.
  id: totrans-5442
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个更常验证的经验观察是，在监督神经网络中使用无监督预训练时，最佳的nh要大得多，例如，从数百个单元增加到数千个单元。一个合理的解释是，在无监督预训练后，许多隐藏单元携带的信息与特定的监督任务无关。为了确保捕捉到与任务相关的信息，因此在使用无监督预训练时需要更大的隐藏层。
- en: '- Weight decay regularization coefficient λ. A way to reduce overfitting is
    to add a *regularization term* to the training criterion, which limits the capacity
    of the learner. The parameters of machine learning models can be regularized by
    pushing them towards a prior value, which is typically 0.'
  id: totrans-5443
  prefs: []
  type: TYPE_NORMAL
  zh: '- 权重衰减正则化系数λ。减少过拟合的一种方法是向训练标准添加一个*正则化项*，限制学习者的能力。机器学习模型的参数可以通过将它们推向一个先验值（通常为0）来进行正则化。'
- en: L2 regularization adds a term λi θ2i to the training criterion, while L1 regularization
    adds a term λi |θi|. Both types of terms can be included.
  id: totrans-5444
  prefs: []
  type: TYPE_NORMAL
  zh: L2正则化在训练标准中添加一个项λi θ2i，而L1正则化添加一个项λi |θi|。这两种类型的项都可以包含。
- en: 'There is a clean Bayesian justification for such a regularization term: it
    is the negative log-prior − log P(θ) on the parameters θ. The training criterion
    then corresponds to the negative joint likelihood of data and parameters,'
  id: totrans-5445
  prefs: []
  type: TYPE_NORMAL
  zh: 这种正则化项有一个干净的贝叶斯理由：它是对参数θ的负对数先验− log P(θ)。训练标准对应于数据和参数的负联合似然。
- en: − log P(*data, θ*) = − log P(*data*|θ) − log P(θ), with the loss function L(z,θ)
  id: totrans-5446
  prefs: []
  type: TYPE_NORMAL
  zh: − log P(*data, θ*) = − log P(*data*|θ) − log P(θ)，损失函数为L(z,θ)。
- en: being interpreted as − log P(z|θ) and − log P(*data*|θ) = −Tt=1 L(zt, θ) if
    the *data* consists of T i.i.d. examples zt. This detail is important to note
    because when one is doing stochastic gradient-based learning, it makes sense to
    use an unbiased estimator of the gradient of the total training criterion
  id: totrans-5447
  prefs: []
  type: TYPE_NORMAL
  zh: 被解释为− log P(z|θ)和− log P(*data*|θ) = −Tt=1 L(zt, θ)，如果*data*包含T个独立同分布的示例zt。这个细节很重要，因为在进行随机梯度学习时，使用总训练标准的无偏估计器是合理的。
- en: (including both the total loss and the regularizer), but one only considers
    a single mini-batch or example at a time. How should the regularizer be weighted
    in this sum, which is different from the sum of the regularizer and the total
    loss on all examples? On each mini-batch update, the gradient of the regularization
    penalty should be multiplied not just by λ but also by BT ,
  id: totrans-5448
  prefs: []
  type: TYPE_NORMAL
  zh: （包括总损失和正则化器），但一次只考虑单个小批量或示例。如何在这个总和中对正则化器进行加权，这与所有示例的正则化器和总损失之和不同？在每次小批量更新中，正则化惩罚的梯度应不仅乘以λ，还应乘以BT。
- en: i.e., one over the number of updates needed to go once through the training
    set. When the training set size is not a multiple of B, the last mini-batch will
    have size B < B and the contribution of the regularizer to the minibatch gradient
    should therefore be modified accordingly (i.e. scaled by B-B
  id: totrans-5449
  prefs: []
  type: TYPE_NORMAL
  zh: 即，每次更新所需的次数。当训练集大小不是B的倍数时，最后一个小批量的大小将是B < B，因此正则化器对小批量梯度的贡献应相应调整（即按B-B缩放）。
- en: compared to other mini-batches). In the pure online setting (there is no fixed
    ahead training set size nor iterating again on the examples), it would then
  id: totrans-5450
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他小批量相比。在纯在线设置中（没有固定的提前训练集大小，也不再对示例进行迭代），那么
- en: 20 A hidden layer is a group of units that is neither an input layer nor an
    output layer. 21 Larger than the input vector.
  id: totrans-5451
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层是既不是输入层也不是输出层的一组单元。比输入向量更大。
- en: make sense to use Bt at example t, or one over the number of updates to date.
    L2 regularization penalizes large values more strongly and corresponds to a Gaussian
    prior ∝ exp(−12 ||θ||2 σ2 ) with prior variance σ2 = 1/(2λ). Note that there is
    a connection between early stopping (see above, choosing the number of training
    iterations) and L2 regularization [34], with one basically playing the same role
    as the other (but early stopping allowing a much more efficient selection of the
    hyper-parameter value, which suggests dropping L2 regularization altogether when
    early-stopping is used). However, L1 regularization behaves differently and can
    sometimes be useful, acting as a form of feature selection. L1 regularization
    makes sure that parameters that are not really very useful are driven to zero
    (i.e. encouraging sparsity of the parameter values), and corresponds to a Laplace
    density prior ∝ e− |θ| s with scale parameter s = 1λ. L1 regularization often
    helps to make the input filters22 cleaner (more spatially localized) and easier
    to interpret. Stochastic gradient descent will not yield actual zeros but values
    hovering around zero. If both L1 and L2 regularization are used, a different coefficient
    (i.e. a different hyper-parameter) should be considered for each, and one may
    also use a different coefficient for different layers. In particular, the input
    weights and output weights may be treated differently.
  id: totrans-5452
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例 t 上使用 Bt 是有意义的，或者是迄今为止更新次数的倒数。L2 正则化对大值的惩罚更强，并对应于高斯先验 ∝ exp(−12 ||θ||2 σ2
    )，其中先验方差 σ2 = 1/(2λ)。注意，早停（见上文，选择训练迭代次数）与 L2 正则化之间存在联系 [34]，两者基本上扮演着相同的角色（但早停允许更有效地选择超参数值，这表明在使用早停时可以完全放弃
    L2 正则化）。然而，L1 正则化表现不同，有时是有用的，充当特征选择的一种形式。L1 正则化确保那些并不真正有用的参数被压缩为零（即，鼓励参数值的稀疏性），并对应于拉普拉斯密度先验
    ∝ e− |θ| s，其中尺度参数 s = 1λ。L1 正则化通常有助于使输入过滤器更干净（空间上更局部）且更易于解释。随机梯度下降不会产生真正的零，而是值在零附近波动。如果同时使用
    L1 和 L2 正则化，则每个正则化应考虑不同的系数（即不同的超参数），也可以对不同层使用不同的系数。特别是，输入权重和输出权重可以有不同的处理方式。
- en: 'One reason for treating output weights differently (i.e., not relying only
    on early stopping) is that we know that it is sufficient to regularize only the
    output weights in order to constrain capacity: in the limit case of the number
    of hidden units going to infinity, L2 regularization corresponds to Support Vector
    Machines (SVM) while L1 regularization corresponds to boosting [12]. Another reason
    for treating inputs and outputs differently from hidden units is because they
    may be sparse. For example, some input features may be 0 most of the time while
    others are non-zero frequently. In that case, there are fewer examples that inform
    the model about that rarely active input feature, and the corresponding parameters
    (weights outgoing from the corresponding input units) should be more regularized
    than the parameters associated with frequently observed inputs. A similar situation
    may occur with target variables that are sparse (e.g., trying to predict rarely
    observed events). In both cases, the effective number of meaningful updates seen
    by these parameters is less than the actual number of updates. This suggests to
    scale the regularization coefficient of these parameters by one over the effective
    number of updates seen by the parameter. A related formula turns up in Bayesian
    probit regression applied to sparse inputs [53]. Some practitioners also choose
    to penalize only the weights w and not the biases b associated with the hidden
    unit activations wz+b for a unit taking the vector of values z as input. This
    guarantees that even with strong regularization, the predictor would converge
    to the optimal constant predictor, rather than the one corresponding to 0 activation.
    For example, with the mean-square loss and the cross-entropy loss, the optimal
    constant predictor is the output average.'
  id: totrans-5453
  prefs: []
  type: TYPE_NORMAL
  zh: 将输出权重处理得不同的一个原因（即，不仅仅依赖于早停法）是因为我们知道，仅对输出权重进行正则化就足以约束容量：在隐含单元数趋于无穷大的极限情况下，L2
    正则化对应于支持向量机（SVM），而 L1 正则化对应于提升 [12]。另一个将输入和输出与隐含单元区分对待的原因是它们可能是稀疏的。例如，一些输入特征在大多数时间可能为
    0，而其他特征则经常非零。在这种情况下，提供有关该稀疏输入特征的信息的示例较少，相应的参数（从对应输入单元发出的权重）应该比与经常观察到的输入相关的参数更受到正则化。稀疏的目标变量也可能出现类似情况（例如，试图预测罕见事件）。在这两种情况下，这些参数所看到的有效有意义的更新次数少于实际的更新次数。这表明应通过参数所见的有效更新次数的倒数来缩放这些参数的正则化系数。在应用于稀疏输入的贝叶斯
    probit 回归中出现了一个相关公式 [53]。一些从业者还选择仅惩罚权重 w，而不惩罚与隐含单元激活 wz+b 相关的偏置 b，这对于以值向量 z 为输入的单元是有效的。这确保了即使在强正则化下，预测器也会收敛到最佳常量预测器，而不是对应于
    0 激活的预测器。例如，在均方损失和交叉熵损失下，最佳常量预测器是输出平均值。
- en: 22 The input weights of a 1st layer neuron are often called "filters" because
    of analogies with signal processing techniques such as convolutions.
  id: totrans-5454
  prefs: []
  type: TYPE_NORMAL
  zh: 22 第一层神经元的输入权重通常被称为“滤波器”，因为它们与信号处理技术（例如卷积）有类比关系。
- en: '- Sparsity of activation regularization coefficient α. A common practice in
    the Deep Learning literature [95, 97, 81, 82, 3, 49, 33, 52] consists in adding
    a penalty term to the training criterion that encourages the hidden units to be
    sparse, i.e., with values at or near 0. Although the L1 penalty (discussed above
    in the case of weights) can also be applied to hidden units activations, this
    is mathematically very different from the L1 regularization term on parameters.
    Whereas the latter corresponds to a prior on the parameters, the former does not
    because it involves the training distribution (since we are looking at data-dependent
    hidden units outputs). Although we will not discuss this much here, the inspiration
    for a sparse representation in Deep Learning comes from the earlier work on *sparse
    coding* [89]. As discussed in Goodfellow *et al.* [51] sparse representations
    may be advantageous because they encourage representations that *disentangle*
    the underlying factors of representation. A sparsity-inducing penalty is also
    a way to regularize (in the sense of reducing the number of examples that the
    learner can learn by heart) [97], which means that the sparsity coefficient is
    likely to interact with the many other hyper-parameters which influence capacity.
    In general, increased sparsity can be compensated by a larger number of hidden
    units.'
  id: totrans-5455
  prefs: []
  type: TYPE_NORMAL
  zh: 激活稀疏性正则化系数α。在深度学习文献中，一种常见做法是向训练标准中添加惩罚项，鼓励隐藏单元稀疏，即值接近或等于0。虽然L1惩罚（在权重案例中讨论过）也可以应用于隐藏单元的激活，但这在数学上与参数上的L1正则化项非常不同。后者对应于参数的先验，而前者则不，因为它涉及训练分布（因为我们正在查看与数据相关的隐藏单元输出）。尽管我们在这里不会多加讨论，深度学习中稀疏表示的灵感来源于早期的*稀疏编码*研究。如Goodfellow
    *et al.* [51]所述，稀疏表示可能是有利的，因为它们鼓励表示*解耦*底层表示因素。引入稀疏性的惩罚也是一种正则化方式（在减少学习者记忆示例的数量的意义上）[97]，这意味着稀疏系数可能与许多影响能力的其他超参数相互作用。一般来说，增加的稀疏性可以通过增加隐藏单元的数量来补偿。
- en: Several approaches have been proposed to induce a sparse representation (or
    with more hidden units whose activation is closer to 0). One approach [97, 72,
    120] is simply to penalize the L1 norm of the representation or another function
    of the hidden units' activation (such as the student-t log-prior). This typically
    makes sense for non-linearities such as the sigmoid which have a saturating output
    around 0, but not for the hyperbolic tangent non-linearity (whose saturation is
    near the -1 and 1 interval borders rather than near the origin). Another option
    is to penalize the biases of the hidden units, to make them more negative [95,
    81, 51, 69]. Note that penalizing the bias runs the danger that the weights could
    compensate for the bias23, which could hurt the numerical optimization of parameters.
    When directly penalizing the hidden unit outputs, several variants can be found
    in the literature, but no clear comparative analysis has been published to evaluate
    which one works better. Although the L1 penalty (i.e., simply α times the sum
    of output elements hj in the case of sigmoid non-linearity) would seem the most
    natural (because of its use in sparse coding), it is used in few papers involving
    sparse auto-encoders. A close cousin of the L1 penalty is the Student-t penalty
    (log(1 + h2j )), originally proposed for sparse coding [89]. Several researchers
    penalize the *average* output h¯j (e.g. over a mini-batch), and instead of pushing
    it to 0, encourage it to approach a fixed target ρ. This can be done through a
    mean-square error penalty such as j (ρ − h¯j )2, or maybe more sensibly (because
    hj behaves like a probability), a KullbackLiebler divergence with respect to the
    binomial distribution with probability ρ, −ρ log h¯j − (1 − ρ) log(1 − ¯hj)+constant,
    e.g., with ρ = 0.05, as in [59]. In addition to the regularization penalty itself,
    the choice of activation function
  id: totrans-5456
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出几种方法来诱导稀疏表示（或激活更接近0的更多隐藏单元）。一种方法[97, 72, 120]是简单地惩罚表示的L1范数或隐藏单元激活的其他函数（如学生-t对数先验）。这通常对具有饱和输出的非线性（如sigmoid）有意义，但对双曲正切非线性（其饱和接近-1和1的边界而不是接近原点）则不然。另一种选择是惩罚隐藏单元的偏置，使其更负[95,
    81, 51, 69]。请注意，惩罚偏置的危险在于权重可能会补偿偏置，这可能会损害参数的数值优化。当直接惩罚隐藏单元输出时，文献中可以找到几种变体，但没有明确的比较分析来评估哪种效果更好。虽然L1惩罚（即在sigmoid非线性的情况下，简单地为α乘以输出元素hj的总和）似乎最为自然（因为它在稀疏编码中的使用），但在涉及稀疏自编码器的论文中使用得并不多。L1惩罚的近亲是学生-t惩罚（log(1
    + h²j)），最初是为稀疏编码提出的[89]。一些研究者惩罚*平均*输出h¯j（例如，跨小批量），而不是将其推向0，而是鼓励它接近一个固定目标ρ。这可以通过均方误差惩罚来实现，例如j
    (ρ − h¯j)²，或者可能更合理（因为hj表现得像概率），与概率为ρ的二项分布的Kullback-Leibler散度，−ρ log h¯j − (1 −
    ρ) log(1 − ¯hj)+常数，例如ρ = 0.05，如[59]所示。除了正则化惩罚本身，激活函数的选择
- en: 23 Because the input to the layer generally has a non-zero average, that when
    multiplied by the weights acts like a bias.
  id: totrans-5457
  prefs: []
  type: TYPE_NORMAL
  zh: 由于层的输入通常具有非零平均值，当乘以权重时，它的作用类似于偏置。
- en: can have a strong impact on the sparsity obtained. In particular, rectifying
    non-linearities (such as max(0, x), instead of a sigmoid) have been very successful
    in several instances [64, 86, 49, 84, 50]. The rectifier also relates to the hard
    tanh [35], whose derivatives are also 0 or 1. In sparse coding and sparse predictive
    coding [65] the activations are directly optimized and actual zeros are the expected
    result of the optimization. In that case, ordinary stochastic gradient is not
    guaranteed to find these zeros (it will oscillate around) and other methods such
    as proximal gradient are more appropriate [21].
  id: totrans-5458
  prefs: []
  type: TYPE_NORMAL
  zh: 这对获得的稀疏性可能有很大影响。特别是，整流非线性（例如max(0, x)，而不是sigmoid）在多个实例中非常成功[64, 86, 49, 84,
    50]。整流器也与硬tanh相关[35]，其导数也是0或1。在稀疏编码和稀疏预测编码[65]中，激活直接被优化，实际的零是优化的预期结果。在这种情况下，普通的随机梯度不一定能找到这些零（它会在周围振荡），而其他方法如近端梯度更为合适[21]。
- en: '- Neuron non-linearity. The typical neuron output is s(a) = s(wx + b),'
  id: totrans-5459
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元非线性。典型的神经元输出为s(a) = s(wx + b)，
- en: where x is the vector of inputs into the neuron, w the vector of weights and
    b the offset or bias parameter, while s is a scalar non-linear function. Several
    non-linearities have been proposed and some choices of non-linearities have been
    shown to be more successful [64, 48, 49]. The most commonly used by the author,
    for hidden units, are the sigmoid 1/(1 + e−a), the hyperbolic tangent ea−e−a ea+e−a
    , the rectifier max(0, a) and the hard tanh [35]. Note that the sigmoid was shown
    to yield serious optimization difficulties when used as the top hidden layer of
    a deep supervised network [48] without unsupervised pre-training, but works well
    for auto-encoder variants24. For output (or reconstruction) units, hard neuron
    non-linearities like the rectifier do not make sense because when the unit is
    saturated (e.g. a < 0 for the rectifier) and associated with a loss, no gradient
    is propagated inside the network, i.e., there is no chance to correct the error25.
    In the case of hidden layers the gradient manages to go through a subset of the
    hidden units, even if the others are saturated. For output units a good trick
    is to obtain the output non-linearity and the loss by considering the associated
    negative log-likelihood and choosing an appropriate (conditional) output probability
    model, usually in the exponential family. For example, one can typically take
    squared error and linear outputs to correspond to a Gaussian output model, cross-entropy
    and sigmoids to correspond to a binomial output model, and
  id: totrans-5460
  prefs: []
  type: TYPE_NORMAL
  zh: 其中x是输入到神经元的向量，w是权重向量，b是偏移或偏置参数，而s是一个标量非线性函数。已提出几种非线性，某些非线性的选择已被证明更成功[64, 48,
    49]。作者最常用的非线性，对于隐藏单元，分别是sigmoid 1/(1 + e−a)、双曲正切ea−e−a ea+e−a、修正线性max(0, a)和硬tanh
    [35]。注意，sigmoid在作为深度监督网络的顶层隐藏层时，未经过无监督预训练会导致严重的优化困难[48]，但在自编码器变体中表现良好24。对于输出（或重构）单元，硬神经元非线性如修正线性没有意义，因为当单元饱和（例如，对于修正线性a
    < 0）并与损失相关时，网络内部不传播梯度，即没有机会修正错误25。在隐藏层的情况下，梯度能够通过一部分隐藏单元，即使其他单元饱和。对于输出单元，一个好的技巧是通过考虑相关的负对数似然并选择适当的（条件）输出概率模型，通常在指数族中，来获取输出非线性和损失。例如，通常可以将平方误差和线性输出对应于高斯输出模型，将交叉熵和sigmoid对应于二项输出模型，和
- en: − log output[target class] with softmax outputs to correspond to multinomial
    output variables. For reasons yet to be elucidated, having a sigmoidal non-linearity
    on the output (reconstruction) units (along with target inputs normalized in the
    (0,1) interval) seems to be helpful when training the contractive auto-encoder.
  id: totrans-5461
  prefs: []
  type: TYPE_NORMAL
  zh: − 对应于多项式输出变量的softmax输出，记录输出[target class]。尚未阐明的原因是，输出（重构）单元上的sigmoid非线性（以及目标输入在(0,1)区间内的归一化）在训练收缩自编码器时似乎是有帮助的。
- en: '- Weights initialization scaling coefficient. Biases can generally be initialized
    to zero but weights need to be initialized carefully to break the'
  id: totrans-5462
  prefs: []
  type: TYPE_NORMAL
  zh: '- 权重初始化缩放系数。偏差通常可以初始化为零，但权重需要谨慎初始化以打破'
- en: 24 The author hypothesizes that this discrepency is due to the fact that the
    weight matrix W of an auto-encoder of the form r(x) = WT sigmoid(W x) is pulled
    towards being orthonormal since this would make the auto-encoder closer to the
    identity function, because WTW x ≈ x when W is orthonormal and x is in the span
    of the rows of W. 25 A hard non-linearity for the output units non-linearity is
    very different from a hard non-linearity in the loss function, such as the hinge
    loss. In the latter case the derivative is 0 only when there is no error.
  id: totrans-5463
  prefs: []
  type: TYPE_NORMAL
  zh: 24 作者假设这种差异是由于自编码器的权重矩阵W的形式为r(x) = WT sigmoid(W x)，因为W被拉向正交规范化，这会使自编码器更接近于恒等函数，因为当W是正交规范化时，WTW
    x ≈ x，并且x位于W的行的跨度内。25 输出单元的硬非线性与损失函数中的硬非线性（如铰链损失）非常不同。在后者情况下，只有在没有错误时导数才为0。
- en: symmetry between hidden units of the same layer26. Because different output
    units receive different gradient signals, this symmetry breaking issue does not
    concern the output weights (into the output units), which can therefore also be
    set to zero. Although several tricks [79, 48] for initializing the weights into
    hidden layers have been proposed (i.e. a hyper-parameter is the discrete choice
    between them), Bergstra and Bengio [17] also inserted as an extra hyper-parameter
    a scaling coefficient for the initialization range.
  id: totrans-5464
  prefs: []
  type: TYPE_NORMAL
  zh: 同一层隐藏单元之间存在对称性。这种对称性破裂问题并不影响输出权重（进入输出单元），因此输出权重也可以设为零。虽然已经提出了几种初始化隐藏层权重的技巧（即超参数是它们之间的离散选择），Bergstra
    和 Bengio 还额外插入了初始化范围的缩放系数作为超参数。
- en: These tricks are based on the idea that units with more inputs (the *fanin*
    of the unit) should have smaller weights. Both LeCun *et al.* [79] and Glorot
    and Bengio [48] recommend scaling by the inverse of the *square* root of the fan-in,
    although Glorot and Bengio [48] and the Deep Learning Tutorials use a combination
    of the fan-in and fan-out, e.g., sample a Uniform(−*r, r*) with r = &6/(fan-in
    + fan-out) for hyperbolic tangent units and r = 4&6/(fan-in + fan-out) for sigmoid
    units. We have found that we could avoid any hyper-parameter related to initialization
    using these formulas (and the derivation in Glorot and Bengio [48] can be used
    to derive the formula for other settings). Note however that in the case of RBMs,
    a zeromean Gaussian with a small standard deviation around 0.1 or 0.01 works well
    [59] to initialize the weights, while visible biases are typically set to their
    optimal value if the weights were 0, i.e., log(¯x/(1 − x¯)) in the case of a binomial
    visible unit whose corresponding binary input feature has empirical mean x¯ in
    the training set.
  id: totrans-5465
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技巧基于这样的观点：输入更多的单元（单元的*fan-in*）应具有更小的权重。LeCun *等人*和 Glorot 与 Bengio 推荐按*fan-in*的平方根的倒数进行缩放，尽管
    Glorot 和 Bengio 以及深度学习教程使用了*fan-in*和*fan-out*的组合，例如，对于双曲正切单元，样本为 Uniform(−*r,
    r*)，其中 r = 6/(fan-in + fan-out)，对于 sigmoid 单元，r = 4*6/(fan-in + fan-out)。我们发现使用这些公式可以避免与初始化相关的任何超参数（Glorot
    和 Bengio 的推导可用于推导其他设置的公式）。然而，请注意，在 RBM 的情况下，均值为零的小标准差高斯分布（约为 0.1 或 0.01）非常有效地初始化权重，而可见偏差通常设置为其最佳值，如果权重为
    0，即当对应的二元输入特征在训练集中的经验均值为 x¯ 时，使用 log(¯x/(1 − x¯))。
- en: An important choice is whether one should use *unsupervised pre-training*
  id: totrans-5466
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的选择是是否使用*无监督预训练*。
- en: (and which unsupervised feature learning algorithm to use) in order to initialize
    parameters. In most settings we have found unsupervised pre-training to help and
    very rarely to hurt, but of course that implies additional training time and additional
    hyper-parameters.
  id: totrans-5467
  prefs: []
  type: TYPE_NORMAL
  zh: （以及使用哪种无监督特征学习算法）以初始化参数。在大多数情况下，我们发现无监督预训练有助于学习，很少会产生负面影响，但当然这意味着额外的训练时间和额外的超参数。
- en: '- Random seeds. There are often several sources of randomness in the training
    of neural networks and deep learners (such as for random initialization, sampling
    examples, sampling hidden units in stochastic models such as RBMs, or sampling
    corruption noise in denoising auto-encoders). Some random seeds could therefore
    yield better results than others. Because of the presence of local minima in the
    training criterion of neural networks (except in the linear case or with fixed
    lower layers), parameter initialization matters. See Erhan *et al.* [44] for an
    example of histograms of test errors for hundreds of different random seeds. Typically,
    the choice of random seed only has a slight effect on the result and can mostly
    be ignored in general or for most of the hyper-parameter search process. If computing
    power is available, then a final set of jobs with different random seeds (5 to
    10) for a small set of best choices of hyper-parameter values can squeeze a bit
    more performance. Another way to exploit computing power to push performance a
    bit is model averaging, as in Bagging [29] and Bayesian methods. After training
    them,'
  id: totrans-5468
  prefs: []
  type: TYPE_NORMAL
  zh: '- 随机种子。神经网络和深度学习中的训练通常有多个随机性来源（例如随机初始化、示例采样、随机模型中的隐藏单元采样，如RBMs，或去噪自编码器中的噪声采样）。因此，某些随机种子可能会比其他种子产生更好的结果。由于神经网络的训练标准存在局部最小值（除了线性情况或固定下层），参数初始化至关重要。有关不同随机种子的测试误差直方图的示例，请参见Erhan
    *et al.* [44]。通常，随机种子的选择对结果的影响较小，通常可以忽略，或在大多数超参数搜索过程中也可以忽略。如果有计算能力，那么对少量最佳超参数值的最终作业集使用不同的随机种子（5到10）可以进一步提升性能。另一种利用计算能力提高性能的方法是模型平均，如Bagging
    [29]和贝叶斯方法。在训练它们之后，'
- en: 26 By symmetry, if hidden units of the same layer share the same input and output
    weights, they will compute the same output and receive the same gradient, hence
    performing the same update and remaining identical, thus wasting capacity.
  id: totrans-5469
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对称性，如果同一层的隐藏单元共享相同的输入和输出权重，它们将计算相同的输出并接收相同的梯度，从而执行相同的更新并保持相同，从而浪费了容量。
- en: the outputs of different networks (or in general different learning algorithms)
  id: totrans-5470
  prefs: []
  type: TYPE_NORMAL
  zh: 不同网络的输出（或一般的不同学习算法）
- en: can be averaged. For example, the difference between the neural networks being
    averaged into a committee may come from the different seeds used for parameter
    initialization, or the use of different subsets of input variables, or different
    subsets of training examples (the latter being called Bagging).
  id: totrans-5471
  prefs: []
  type: TYPE_NORMAL
  zh: 可以进行平均。例如，平均成一个委员会的神经网络之间的差异可能来自于用于参数初始化的不同种子，或使用不同的输入变量子集，或不同的训练示例子集（后者称为Bagging）。
- en: '- Preprocessing. Many preprocessing steps have been proposed to massage raw
    data into appropriate inputs for neural networks and model selection must also
    choose among them. In addition to element-wise standardization (subtract mean
    and divide by standard deviation), Principal Components Analysis (PCA) has often
    been advocated [79, 17] and also allows dimensionality reduction, at the price
    of an extra hyper-parameter (the number of principal components retained, or the
    proportion of variance explained). A'
  id: totrans-5472
  prefs: []
  type: TYPE_NORMAL
  zh: '- 预处理。许多预处理步骤被提出，以将原始数据转化为神经网络的适当输入，模型选择也必须在其中进行选择。除了逐元素标准化（减去均值并除以标准差），主成分分析（PCA）常被推荐
    [79, 17]，并允许进行降维，但代价是增加了一个超参数（保留的主成分数量或解释的方差比例）。'
- en: convenient non-linear preprocessing is the *uniformization* [84] of each feature
  id: totrans-5473
  prefs: []
  type: TYPE_NORMAL
  zh: 方便的非线性预处理是每个特征的*均匀化* [84]。
- en: (which estimates its cumulative distribution Fi and then transforms each feature
    xi by its quantile F −1 i (xi), i.e., returns an approximate normalized rank or
    quantile for the value xi). A simpler to compute transform that may help reduce
    the tails of input features is a non-linearity such as the logarithm or the square
    root, in an attempt to make them more Gaussian-like.
  id: totrans-5474
  prefs: []
  type: TYPE_NORMAL
  zh: （其估计累积分布Fi，然后通过其分位数F −1 i (xi)转化每个特征xi，即返回值xi的近似归一化等级或分位数）。一种更简单的计算变换，可能有助于减少输入特征的尾部，是使用对数或平方根等非线性，以使其更接近高斯分布。
- en: 'In addition to the above somewhat generic choices, more choices arise with
    different architectures and learning algorithms. For example, the denoising autoencoder
    has a hyper-parameter scaling the amount of input corruption and the contractive
    auto-encoder has as hyper-parameter a coefficient scaling the norm of the Jacobian
    of the encoder, i.e., controlling the importance of the contraction penalty. The
    latter seems to be a rather sensitive hyper-parameter that must be tuned carefully.
    The contractive auto-encoder''s success also seems sensitive to the weight tying
    constraint used in many auto-encoder architectures: the decoder''s weight matrix
    is equal to the transpose of the encoder''s weight matrix. The specific architecture
    used in the contractive auto-encoder (with tied weights, sigmoid non-linearies
    on hidden and reconstruction units, along with squared loss or cross-entropy loss)
    works quite well but other related variants do not always train well, for reasons
    that remain to be understood.'
  id: totrans-5475
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述一些通用选择外，随着不同架构和学习算法的出现，还会产生更多选择。例如，去噪自编码器有一个超参数来调整输入损坏的程度，而收缩自编码器则有一个超参数来缩放编码器雅可比矩阵的范数，即控制收缩惩罚的重要性。后者似乎是一个相当敏感的超参数，必须仔细调整。收缩自编码器的成功似乎也对在许多自编码器架构中使用的权重绑定约束敏感：解码器的权重矩阵等于编码器的权重矩阵的转置。在收缩自编码器中使用的具体架构（具有绑定权重、隐藏和重建单元上的sigmoid非线性，以及平方损失或交叉熵损失）表现相当良好，但其他相关变体并不总是能很好地训练，原因仍有待理解。
- en: There are also many architectural choices that are relevant in the case of convolutional
    architectures (e.g. for modeling images, time-series or sound) [78, 80, 71] in
    which hidden units have local receptive fields.
  id: totrans-5476
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积架构（例如用于建模图像、时间序列或声音）中，也存在许多相关的架构选择[78, 80, 71]，其中隐藏单元具有局部感受野。
- en: 19.3.3 Manual Search And Grid Search
  id: totrans-5477
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.3.3 手动搜索和网格搜索
- en: Many of the hyper-parameters or model choices described above can be ignored
    by picking a standard trick suggested here or in some other paper. Still, one
    remains with a substantial number of choices to be made, which may give the impression
    of neural network training as an art. With modern computing facilities based on
    large computer clusters, it is however possible to make the optimization of hyper-parameters
    a more reproducible and automated process, using techniques such as grid search
    or better, random search, or even hyperparameter optimization, discussed below.
  id: totrans-5478
  prefs: []
  type: TYPE_NORMAL
  zh: 上述许多超参数或模型选择可以通过选择这里或其他论文中建议的标准技巧来忽略。然而，仍然需要做出相当数量的选择，这可能给人一种神经网络训练是一门艺术的印象。借助基于大型计算集群的现代计算设施，可以使超参数的优化过程变得更加可重复和自动化，采用网格搜索、随机搜索或更先进的超参数优化等技术，下面将讨论这些技术。
- en: 'General Guidance for the Exploration of Hyper-parameters. First of all, let
    us consider recommendations for exploring hyper-parameter settings, whether with
    manual search, with an automated procedure, or with a combination of both. We
    call a *numerical hyper-parameter* one that involves choosing a real number or
    an integer (where order matters), as opposed to making a discrete symbolic choice
    from an unordered set. Examples of numerical hyperparameters are regularization
    coefficients, number of hidden units, number of training iterations, etc. One
    has to think of hyper-parameter selection as *a difficult form of learning*: there
    is both an optimization problem (looking for hyperparameter configurations that
    yield low validation error) and a generalization problem: there is uncertainty
    about the expected generalization after optimizing validation performance, and
    it is possible to overfit the validation error and get optimistically biased estimators
    of performance when comparing many hyperparameter configurations. The training
    criterion for this learning is typically the validation set error, which is a
    proxy for generalization error. Unfortunately, the relation between hyper-parameters
    and validation error can be complicated.'
  id: totrans-5479
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数探索的一般指导。首先，让我们考虑有关探索超参数设置的建议，无论是手动搜索、自动化程序，还是两者的结合。我们将*数值超参数*定义为涉及选择一个实数或整数（顺序很重要）的超参数，而不是从无序集合中进行离散符号选择。数值超参数的例子包括正则化系数、隐藏单元数量、训练迭代次数等。我们必须将超参数选择视为*一种困难的学习形式*：它既有优化问题（寻找导致低验证错误的超参数配置），也有泛化问题：优化验证性能后，关于预期泛化存在不确定性，并且在比较多个超参数配置时，可能会对验证错误过拟合，从而得到性能的乐观偏倚估计。此学习的训练标准通常是验证集错误，它是泛化错误的代理。不幸的是，超参数与验证错误之间的关系可能很复杂。
- en: Although to first approximation we expect a kind of U-shaped curve (when considering
    only a single hyper-parameter, the others being fixed), this curve can also have
    noisy variations, in part due to the use of finite data sets.
  id: totrans-5480
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在初步近似中我们期望得到一种 U 形曲线（在只考虑单个超参数的情况下，其余参数保持固定），但由于使用有限数据集，这条曲线也可能会有噪声变化。
- en: '- Best value on the border. When considering the validation error obtained
    for different values of a numerical hyper-parameter one should pay attention as
    to whether or not the best value found is near the border of the investigated
    interval. If it is near the border, then this suggests that better values can
    be found with values beyond the border: it is recommended in that case to explore
    further, beyond that border. Because the relation between a hyper-parameter and
    validation error can be noisy, it is generally not enough to try very few values.
    For instance, trying only 3 values for a numerical hyper-parameter is insufficient,
    even if the best value found is the middle one.'
  id: totrans-5481
  prefs: []
  type: TYPE_NORMAL
  zh: '- 边界上的最佳值。当考虑不同数值超参数值所获得的验证错误时，应该注意找到的最佳值是否接近所调查区间的边界。如果它接近边界，那么这表明可以通过超出边界的值找到更好的值：在这种情况下，建议进一步探索超出该边界。由于超参数与验证错误之间的关系可能会有噪声，通常仅尝试很少的值是不够的。例如，仅尝试
    3 个数值超参数的值是不够的，即使找到的最佳值是中间那个。'
- en: '- Scale of values considered. Exploring values of a numerical hyper-parameter
    entails choosing a *starting interval* to be searched, which is therefore a kind
    of hyper-hyper-parameter. By choosing the interval large enough to start with,
    but based on previous experience with this hyperparameter, we ensure that we do
    not get completely wrong results. Now instead of choosing the intermediate values
    *linearly* in the chosen interval, it often makes much more sense to consider
    a linear or uniform sampling in the *log-domain* (in the space of the logarithm
    of the hyper-parameter). For example, the results obtained with a learning rate
    of 0.01 are likely to be very similar to the results with 0.011 while results
    with 0.001 could be quite different from results with 0.002 even though the absolute
    difference is the same in both cases. The *ratio* between different values is
    often a better guide of the expected impact of the change. That is why exploring
    uniformly or regularly-spaced values in the space of the *logarithm* of the numerical
    hyper-parameter is typically preferred for positive-valued numerical hyperparameters.'
  id: totrans-5482
  prefs: []
  type: TYPE_NORMAL
  zh: '- 考虑的值的范围。探索数值超参数的值需要选择一个*起始区间*进行搜索，这实际上是一种超超参数。通过选择足够大的区间，但基于之前对该超参数的经验，我们确保不会得到完全错误的结果。现在，与其在所选区间内*线性*选择中间值，考虑在*对数域*（超参数的对数空间）中进行线性或均匀采样通常更有意义。例如，学习率为0.01时获得的结果可能与0.011的结果非常相似，而0.001的结果与0.002的结果可能会有很大不同，即使在两种情况下绝对差异是相同的。不同值之间的*比例*往往更好地指示变化的预期影响。这就是为什么在正值数值超参数的*对数*空间中均匀或定间隔探索值通常更受青睐。'
- en: '- Computational considerations. Validation error is actually not the only measure
    to consider in selecting hyper-parameters. Often, one has to consider computational
    cost, either of training or prediction. Computing resources for training and prediction
    are limited and generally condition the choice of intervals of considered values:
    for example increasing the number of hidden units or number of training iterations
    also scales up computation.'
  id: totrans-5483
  prefs: []
  type: TYPE_NORMAL
  zh: '- 计算考虑。验证误差实际上并不是选择超参数时唯一需要考虑的指标。通常，还需要考虑训练或预测的计算成本。训练和预测的计算资源是有限的，通常会影响所考虑值的区间选择：例如，增加隐藏单元数量或训练迭代次数也会增加计算量。'
- en: An interesting idea is to use *computationally cheap estimators* of validation
    error to select some hyper-parameters. For example, Saxe *et al.* [105] showed
    that the architecture hyper-parameters of convolutional networks could be selected
    using *random weights* in the lower layers of the network (filters of the convolution).
    While this yields a noisy and biased (pessimistic) estimator of the validation
    error which would otherwise be obtained with full training, this cheap estimator
    appears to be correlated with the expensive validation error. Hence this cheap
    estimator is enough for selecting some hyper-parameters (or for keeping under
    consideration for further and more expensive evaluation only the few best choices
    found). Even without cheap estimators of generalization error, high-throughput
    computing (e.g., on clusters, GPUs, or clusters of GPUs) can be exploited to run
    not just hundreds but thousands of training jobs, something not conceivable only
    a few years ago, with each job taking on the order of hours or days for larger
    datasets. With computationally cheap surrogates, some researchers have run on
    the order of ten thousands trials, and we can expect future advances in parallelized
    computing power to boost these numbers.
  id: totrans-5484
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的想法是使用*计算便宜的估计器*来选择一些超参数。例如，Saxe *等* [105] 证明了卷积网络的架构超参数可以使用网络低层的*随机权重*进行选择（卷积的滤波器）。虽然这会产生一个嘈杂且偏倚（悲观）的验证误差估计器，该估计器与完整训练获得的验证误差不同，但这个便宜的估计器似乎与昂贵的验证误差相关。因此，这个便宜的估计器足以用于选择一些超参数（或仅在考虑进一步更昂贵的评估时保留找到的少数最佳选择）。即使没有通用化误差的便宜估计器，高吞吐量计算（例如，在集群、GPU或GPU集群上）可以被利用来运行不仅是数百而是成千上万的训练作业，这在几年前是不可想象的，每个作业对较大数据集的耗时在几个小时到几天之间。使用计算便宜的替代品，一些研究人员进行了数万次试验，我们可以期待未来在并行计算能力上的进步将提升这些数字。
- en: 'Coordinate Descent and Multi-resolution Search. When performing a manual search
    and with access to only a single computer, a reasonable strategy is *coordinate
    descent*: change only one hyper-parameter at a time, always making a change from
    the best configuration of hyper-parameters found up to now. Instead of a standard
    coordinate descent (which systematically cycles through all the variables to be
    optimized) one can make sure to regularly fine-tune the most sensitive variables,
    such as the learning rate.'
  id: totrans-5485
  prefs: []
  type: TYPE_NORMAL
  zh: 坐标下降法和多分辨率搜索。当进行手动搜索且仅能使用一台计算机时，一个合理的策略是*坐标下降法*：一次只更改一个超参数，总是从迄今找到的最佳超参数配置开始更改。与标准的坐标下降法（系统性地循环遍历所有待优化变量）不同，可以确保定期微调最敏感的变量，如学习率。
- en: Another important idea is that there is no point in exploring the effect of
    fine changes before one or more reasonably good settings have been found. The
    idea of *multi-resolution search* is to start the search by considering only a
    few values of the numerical hyper-parameters (over a large range), or considering
    large changes each time a new value is tried. One can then start from the one
    or few best configurations found and explore more locally around them with smaller
    variations around these values.
  id: totrans-5486
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的观点是，在找到一个或多个合理的好设置之前，探索微小变化的效果没有意义。*多分辨率搜索*的理念是开始搜索时仅考虑少量数值超参数的值（覆盖大范围），或者每次尝试新值时考虑大的变化。然后，可以从找到的一个或几个最佳配置开始，并围绕这些值以较小的变化进行更局部的探索。
- en: Automated and Semi-automated Grid Search. Once some interval or set of values
    has been selected for each hyper-parameter (thus defining a search space), a simple
    strategy that exploits parallel computing is the grid search. One first needs
    to convert the numerical intervals into lists of values (e.g., K regularly-spaced
    values in the log-domain of the hyper-parameter). The grid search is simply an
    exhaustive search through all the combinations of these values. The cross-product
    of these lists contains a number of elements that is unfortunately exponential
    in the number of hyper-parameters (e.g., with 5 hyper-parameters, each allowed
    to take 6 different values, one gets 65 = 7776 configurations). In section 19.3.4
    below we consider an approach that works more efficiently than the grid search
    when the number of hyper-parameters increases beyond 2 or 3.
  id: totrans-5487
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化和半自动化网格搜索。一旦为每个超参数选择了一些区间或值集合（从而定义了搜索空间），一种利用并行计算的简单策略是网格搜索。首先，需要将数值区间转换为值列表（例如，在超参数的对数域中均匀分布的K个值）。网格搜索就是通过这些值的所有组合进行穷举搜索。这些列表的笛卡尔积包含的元素数量在超参数的数量上是指数级的（例如，若有5个超参数，每个允许6个不同的值，则得到65
    = 7776种配置）。在下面的19.3.4节中，我们考虑一种在超参数数量超过2或3时比网格搜索更高效的方法。
- en: The advantage of the grid search, compared to many other optimization strategies
    (such as coordinate descent), is that it is fully parallelizable. If a large computer
    cluster is available, it is tempting to choose a model selection strategy that
    can take advantage of parallelization. One practical disadvantage of grid search
    (especially against random search, Sec. 19.3.4), with a parallelized set of jobs
    on a cluster, is that if only one of the jobs fails27 then one has to launch another
    volley of jobs to complete the grid (and yet a third one if any of these fails,
    etc.), thus multiplying the overall computing time.
  id: totrans-5488
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多其他优化策略（如坐标下降法）相比，网格搜索的优势在于它是完全可并行化的。如果有一个大型计算机集群可用，选择一种能够利用并行化的模型选择策略是很诱人的。网格搜索的一个实际缺点（尤其是与随机搜索相比，见第19.3.4节）是，如果集群上的工作中只有一个失败，则必须重新发起另一轮工作以完成网格（如果其中任何一个失败，则还需第三轮，依此类推），从而增加整体计算时间。
- en: Typically, a single grid search is not enough and practitioners tend to proceed
    with a sequence of grid searches, each time adjusting the ranges of values considered
    based on the previous results obtained. Although this can be done manually, this
    procedure can also be automated by considering the idea of multiresolution search
    to guide this outer loop. Different, more local, grid searches can be launched
    in the neighborhood of the best solutions found previously. In addition, the idea
    of coordinate descent can also be thrown in, by making each grid search focus
    on only a few of the hyper-parameters. For example, it is common practice to start
    by exploring the initial learning rate while keeping fixed
  id: totrans-5489
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，单次网格搜索是不够的，实践者往往会进行一系列的网格搜索，每次根据之前的结果调整考虑的值范围。虽然这可以手动完成，但也可以通过考虑多分辨率搜索的理念来自动化这一外部循环。在之前找到的最佳解决方案附近可以启动不同的、更局部的网格搜索。此外，还可以引入坐标下降的概念，使每次网格搜索仅关注少数几个超参数。例如，通常的做法是首先探索初始学习率，同时保持固定
- en: (and initially constant) the learning rate descent schedule. Once the shape
    of the schedule has been chosen, it may be possible to further refine the learning
    rate, but in a smaller interval around the best value found.
  id: totrans-5490
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了计划的形状，可能会进一步细化学习率，但仅限于围绕找到的最佳值的小范围内。
- en: Humans can get very good at performing hyper-parameter search, and having a
    human in the loop also has the advantage that it can help detect bugs or unwanted
    or unexpected behavior of a learning algorithm. However, for the sake of reproducibility,
    machine learning researchers should strive to use procedures that do not involve
    human decisions in the middle, only at the outset (e.g.,
  id: totrans-5491
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在执行超参数搜索方面可以变得非常优秀，而且将人类纳入过程中的优势在于可以帮助检测学习算法的错误或不希望的行为。然而，为了可重复性，机器学习研究人员应努力使用不涉及人类决策的程序，仅在一开始时涉及（例如，
- en: setting hyper-parameter ranges, which can be specified in a paper describing
    the experiments).
  id: totrans-5492
  prefs: []
  type: TYPE_NORMAL
  zh: 设置超参数范围，这些范围可以在描述实验的论文中指定。
- en: Layer-Wise Optimization of Hyper-parameters. In the case of Deep Learning with
    unsupervised pre-training there is an opportunity for combining coordinate descent
    and cheap relative validation set performance evaluation associated with some
    hyper-parameter choices. The idea, described by Mesnil *et al.*
  id: totrans-5493
  prefs: []
  type: TYPE_NORMAL
  zh: 层级优化超参数。在无监督预训练的深度学习中，有机会结合坐标下降和与某些超参数选择相关的低成本相对验证集性能评估。这个想法由Mesnil等人描述。
- en: '[84], Bengio [8], is to perform greedy choices for the hyper-parameters associated
    with lower layers (near the input) before training the higher layers. One first
    trains (unsupervised) the first layer with different hyper-parameter values and
    somehow estimates the relative validation error that would be obtained from these
    different configurations if the final network only had this single layer as internal
    representation. In the common case where the ultimate task is supervised, it means
    training a simple supervised predictor (e.g. a linear classifier) on'
  id: totrans-5494
  prefs: []
  type: TYPE_NORMAL
  zh: '[84]，Bengio [8]的观点是，在训练更高层之前，对与底层（接近输入）相关的超参数进行贪婪选择。首先使用不同的超参数值训练（无监督）第一层，并以某种方式估计如果最终网络只有这一层作为内部表示，所获得的相对验证误差。在最终任务为监督的常见情况下，这意味着在（初始时为常数的）学习率下降计划上训练一个简单的监督预测器（例如线性分类器）。'
- en: 27 For all kinds of hardware and software reasons, a job failing is very common.
  id: totrans-5495
  prefs: []
  type: TYPE_NORMAL
  zh: 出于各种硬件和软件原因，作业失败是非常常见的。
- en: top of the learned representation. In the case of a linear predictor (e.g. regression
    or logistic regression) this can even be done on the fly while unsupervised training
    of the representation progresses (i.e. can be used for early stopping as well),
    as in [70]. Once a set of apparently good (according to this greedy evaluation)
    hyper-parameters values has been found (or possibly using only the best one found),
    these good values can be used as starting point to train (and hyperoptimize) a
    second layer in the same way, etc. The completely greedy approach is to keep only
    the best configuration up to now (for the lower layers), but keeping the K best
    configurations overall only multiplies computational costs of hyperparameter selection
    by K for layers beyond the first one, because we would still keep only the best
    K configurations from all the 1st layer and 2nd layer hyper-parameters as starting
    points for exploring 3rd layer hyper-parameters, etc. This procedure is formalized
    in the Algorithm 19.1 below. Since greedy layer-wise pre-training does not modify
    the lower layers when pre-training the upper layers, this is also very efficient
    computationally. This procedure allows one to set the hyper-parameters associated
    with the unsupervised pre-training stage, and then there remains hyper-parameters
    to be selected for the supervised fine-tuning stage, if one is desired. A final
    supervised fine-tuning stage is strongly suggested, especially when there are
    many labeled examples [67].
  id: totrans-5496
  prefs: []
  type: TYPE_NORMAL
  zh: 学习表示的顶部。在线性预测器（例如回归或逻辑回归）的情况下，这甚至可以在无监督训练表示进展的同时动态进行（即也可以用于早期停止），如文献[70]所示。一旦找到一组明显良好的（根据这种贪婪评估）超参数值（或可能仅使用找到的最佳值），这些良好的值可以作为起点，以同样的方式训练（和超优化）第二层，等等。完全贪婪的方法是仅保留到目前为止的最佳配置（对于较低层），但整体上仅保留K个最佳配置只是将超参数选择的计算成本乘以K，针对第一层之后的层，因为我们仍然只会保留来自所有第一层和第二层超参数的最佳K个配置作为探索第三层超参数的起点，等等。该过程在下面的算法19.1中进行了形式化。由于贪婪的逐层预训练在预训练上层时不会修改下层，因此在计算上也非常高效。该过程允许设置与无监督预训练阶段相关的超参数，然后如果需要，还剩下需要为有监督微调阶段选择的超参数。强烈建议进行最终的有监督微调阶段，特别是在有许多标记示例的情况下[67]。
- en: 19.3.4 Random Sampling Of Hyper-Parameters
  id: totrans-5497
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.3.4 超参数的随机采样
- en: A serious problem with the grid search approach to find good hyper-parameter
    configurations is that it scales exponentially badly with the number of hyperparameters
    considered. In the above sections we have discussed numerous hyperparameters and
    if all of them were to be explored at the same time it would be impossible to
    use only a grid search to do so.
  id: totrans-5498
  prefs: []
  type: TYPE_NORMAL
  zh: 使用网格搜索方法寻找良好超参数配置的一个严重问题是，随着考虑的超参数数量的增加，其扩展性呈指数级恶化。在上述部分中，我们讨论了众多超参数，如果所有超参数同时进行探索，仅使用网格搜索是不可能的。
- en: One may think that there are no other options simply because this is an instance
    of the curse of dimensionality. But like we have found in our work on Deep Learning
    [7], if there is some structure in a target function we are trying to discover,
    then there is a chance to find good solutions without paying an exponential price.
    It turns out that in many practical cases we have encountered, there is a kind
    of structure that *random sampling* can exploit [17]. The idea of random sampling
    is to replace the regular grid by a random (typically uniform) sampling. Each
    tested hyper-parameter configuration is selected by independently sampling each
    hyper-parameter from a prior distribution (typically uniform in the log-domain,
    inside the interval of interest). For a discrete hyper-parameter, a multinomial
    distribution can be defined according to our prior beliefs on the likely good
    values. At worse, i.e., with no prior preference at all, this would be a uniform
    distribution across the allowed values. In fact, we can use our prior knowledge
    to make this prior distribution quite sophisticated. For example, we can readily
    include knowledge that some values of some hyper-parameters only make sense in
    the context of other particular values of hyper-parameters. This is a practical
    consideration for example when considering layer-specific hyperparameters when
    the number of layers itself is a hyper-parameter.
  id: totrans-5499
  prefs: []
  type: TYPE_NORMAL
  zh: 人们可能会认为没有其他选择，仅仅因为这是维度诅咒的一个实例。但正如我们在深度学习的研究中发现的那样 [7]，如果目标函数中存在某种结构，那么就有机会在不支付指数代价的情况下找到好的解决方案。实际上，在我们遇到的许多实际案例中，存在一种
    *随机采样* 可以利用的结构 [17]。随机采样的想法是用随机（通常是均匀的）采样替代常规网格。每个测试的超参数配置都是通过从先验分布中独立抽样每个超参数选择的（通常在对数域中是均匀的，在感兴趣的区间内）。对于离散超参数，可以根据我们对可能好值的先验信念定义多项分布。在最坏情况下，即没有任何先验偏好时，这将是允许值之间的均匀分布。实际上，我们可以利用先前知识使这一先验分布相当复杂。例如，我们可以直接包括一些超参数的某些值仅在其他特定超参数值的上下文中才有意义的知识。当考虑层特定超参数时，层数本身是一个超参数时，这是一个实际考虑。
- en: 19. Recommendations for Training Deep Architectures 461 Algorithm 19.1 Greedy
    layer-wise hyper-parameter optimization.
  id: totrans-5500
  prefs: []
  type: TYPE_NORMAL
  zh: 19. 对深度架构的训练建议 461 算法 19.1 贪心层级超参数优化。
- en: 'input K: number of best configurations to keep at each level. input *NLEV ELS*:
    number of levels of the deep architecture input *LEV ELSET T INGS*: list of hyper-parameter
    settings to be considered for unsupervised pre-training of a level input *SF T
    SET T INGS*: list of hyper-parameter settings to be considered for supervised
    fine-tuning Initialize set of best configurations S = ∅'
  id: totrans-5501
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 K：在每个层级保留的最佳配置数量。输入 *NLEV ELS*：深度架构的层数输入 *LEV ELSET T INGS*：待考虑用于无监督预训练的超参数设置列表输入
    *SF T SET T INGS*：待考虑用于监督微调的超参数设置列表 初始化最佳配置集合 S = ∅
- en: for L = 1 to *NLEV ELS* do for C in *LEV ELSET T INGS* do for H in (S or {∅})
    do
  id: totrans-5502
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 L = 1 到 *NLEV ELS*，对 *LEV ELSET T INGS* 中的 C 进行迭代，对 (S 或 {∅}) 中的 H 进行迭代。
- en: '* Pretrain level L using hyper-parameter setting C for level L and the parameters
    obtained with setting H for lower levels.'
  id: totrans-5503
  prefs: []
  type: TYPE_NORMAL
  zh: '* 使用超参数设置 C 预训练层级 L，并使用设置 H 获得的参数进行下层级的训练。'
- en: '* Evaluate target task performance L using this depth-L pre-trained architecture
    (e.g. train a linear classifier on top of these layers and estimate validation
    error).'
  id: totrans-5504
  prefs: []
  type: TYPE_NORMAL
  zh: '* 使用此深度 L 预训练架构评估目标任务性能 L（例如，在这些层上训练线性分类器并估算验证误差）。'
- en: '* Push the pair (C∪H,L) into S if it is among the K best performing of S.'
  id: totrans-5505
  prefs: []
  type: TYPE_NORMAL
  zh: '* 如果 (C∪H,L) 在 S 的 K 个表现最佳中，则将该对推入 S。'
- en: end for end for end for for C in *SF T SET T INGS* do for H in S do
  id: totrans-5506
  prefs: []
  type: TYPE_NORMAL
  zh: 结束循环 结束循环 结束循环 对于 *SF T SET T INGS* 中的 C，针对 S 中的 H 进行迭代。
- en: '* Supervised fine-tuning of the pre-trained architecture associated with H,
    using supervised fine-tuning hyper-parameter setting C.'
  id: totrans-5507
  prefs: []
  type: TYPE_NORMAL
  zh: '* 监督性微调与 H 相关的预训练架构，使用监督性微调超参数设置 C。'
- en: '* Evaluate target task performance L of this fine-tuned predictor (e.g.'
  id: totrans-5508
  prefs: []
  type: TYPE_NORMAL
  zh: '* 评估该微调预测器的目标任务性能 L（例如。'
- en: validation error).
  id: totrans-5509
  prefs: []
  type: TYPE_NORMAL
  zh: 验证误差）。
- en: '* Push the pair (C ∪ H,L) into S if it is among the K best performing of S.'
  id: totrans-5510
  prefs: []
  type: TYPE_NORMAL
  zh: '* 如果 (C ∪ H,L) 在 S 的 K 个表现最佳中，则将该对推入 S。'
- en: end for end for output S the set of K best-performing models with their settings
    and validation performance.
  id: totrans-5511
  prefs: []
  type: TYPE_NORMAL
  zh: 结束循环 结束循环 输出 S 为 K 个表现最佳的模型及其设置和验证性能的集合。
- en: The experiments performed [17] show that random sampling can be many times more
    efficient than grid search as soon as the number of hyper-parameters goes beyond
    the 2 or 3 typically seen with SVMs and vanilla neural networks.
  id: totrans-5512
  prefs: []
  type: TYPE_NORMAL
  zh: 进行的实验[17]表明，当超参数数量超过SVM和普通神经网络通常看到的2或3时，随机采样的效率可能是网格搜索的多倍。
- en: The main reason why faster convergence is observed is because it allows one
    to explore more values for each hyper-parameter, whereas in grid search, the same
    value of a hyper-parameter is repeated in exponentially many configurations (of
    all the other hyper-parameters). In particular, if only a small subset of the
    hyperparameters really matters, then this procedure can be shown to be exponentially
    more efficient. What we found is that for different datasets and architectures,
    the subset of hyper-parameters that mattered most was different, but it was often
    the case that a few hyper-parameters made a big difference (and the learning rate
    is always one of them!). When marginalizing (by averaging or minimizing) the validation
    performance to visualize the effect of one or two hyper-parameters, we get a more
    noisy picture using a random search compared to a grid search, because of the
    random variations of the other hyper-parameters but one with much more resolution,
    because so many more different values have been considered. Practically, one can
    plot the curves of best validation error as the number of random trials28 is increased
    (with mean and standard deviation, obtained by considering, for each choice of
    number of trials, all possible same-size subsets of trials), and this curve tells
    us that we are approaching a plateau, i.e., it tells us whether it is worth it
    or not to continue launching jobs, i.e., we can perform a kind of early stopping
    in the outer optimization over hyper-parameters.
  id: totrans-5513
  prefs: []
  type: TYPE_NORMAL
  zh: 更快收敛的主要原因是，它允许探索每个超参数的更多值，而在网格搜索中，同一个超参数的值在所有其他超参数的指数配置中重复。特别是，如果只有少量超参数真正重要，那么可以证明该过程在效率上是指数级的。我们的发现是，对于不同的数据集和架构，最重要的超参数子集是不同的，但通常情况下，少数超参数会带来很大差异（学习率总是其中之一！）。在对验证性能进行边际化（通过平均或最小化）以可视化一两个超参数的效果时，使用随机搜索得到的结果相较于网格搜索会更嘈杂，因为其他超参数的随机变动，但分辨率更高，因为考虑了更多不同的值。实际上，可以绘制最佳验证误差曲线，随着随机试验数量的增加（通过考虑每次试验数量的所有可能相同大小的试验子集，得到均值和标准差），这条曲线告诉我们是否接近一个平台，即它告诉我们继续发起作业是否值得，即我们可以在超参数的外部优化中进行某种早期停止。
- en: Note that one should distinguish the curve of the "best trial in first N trials"
    with the curve of the mean (and standard deviation) of the "best in a subset of
    size N". The latter is a better statistical representative of the improvements
    we should expect if we increase the number of trials. Even if the former has a
    plateau, the latter may still be on the increase, pointing for the need to more
    hyper-parameter configuration samples, i.e., more trials [17]. Comparing these
    curves with the equivalent obtained from grid search we see faster convergence
    with random search. On the other hand, note that one advantage of grid search
    compared to random sampling is that the qualitative analysis of results is easier
    because one can consider variations of a single hyper-parameter with all the other
    hyper-parameters being fixed. It may remain a valid option to do a small grid
    search around the best solutions found by random search, considering only the
    hyper-parameters that were found to matter or which concern a scientific question
    of interest29.
  id: totrans-5514
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，应区分“前N次试验中最佳试验”的曲线与“大小为N的子集中的最佳均值（和标准差）”的曲线。后者是对我们在增加试验数量时应期望的改进的更好统计代表。即使前者已经达到平台，后者可能仍在上升，这表明需要更多的超参数配置样本，即更多的试验[17]。将这些曲线与从网格搜索获得的等效曲线进行比较，我们看到随机搜索的收敛更快。另一方面，请注意，与随机采样相比，网格搜索的一个优势是结果的定性分析更容易，因为可以考虑单个超参数的变化，同时其他超参数保持固定。在随机搜索找到的最佳解决方案附近进行小范围网格搜索，考虑仅与重要超参数或相关科学问题有关的超参数，仍然可能是一个有效的选择29。
- en: Random search maintains the advantage of easy parallelization provided by grid
    search and improves on it. Indeed, a practical advantage of random search compared
    to grid search is that if one of the jobs fails then there is no need to re-launch
    that job. It also means that if one has launched 100 random search
  id: totrans-5515
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索保持了网格搜索所提供的易于并行化的优势，并在此基础上加以改进。实际上，随机搜索相比网格搜索的一个实际优势是，如果其中一个作业失败，则无需重新启动该作业。这也意味着，如果启动了100次随机搜索
- en: 28 Each random trial corresponding to a training job with a particular choice
    of hyperparameter values. 29 This is often the case in machine learning research,
    e.g., does depth of architecture matter? then we need to control accurately for
    the effect of depth, with all other hyper-parameters optimized for each value
    of depth.
  id: totrans-5516
  prefs: []
  type: TYPE_NORMAL
  zh: 每次随机试验对应于具有特定超参数值选择的训练作业。这在机器学习研究中通常是如此，例如，架构的深度是否重要？那么我们需要准确控制深度的影响，同时为每个深度值优化所有其他超参数。
- en: jobs, and finds that the convergence curve still has an interesting slope, one
    can launch another 50 or 100 without wasting the first 100. It is not that simple
    to combine the results of two grid searches because they are not always compatible
  id: totrans-5517
  prefs: []
  type: TYPE_NORMAL
  zh: 作业，发现收敛曲线仍然具有有趣的斜率，可以在不浪费前100次的情况下，再启动另外50或100次。将两个网格搜索的结果结合起来并不简单，因为它们并不总是兼容。
- en: (i.e., one is not a subset of the other).
  id: totrans-5518
  prefs: []
  type: TYPE_NORMAL
  zh: （即，一个并不是另一个的子集）。
- en: Finally, although random search is a useful addition to the toolbox of the practitioner,
    semi-automatic exploration is still helpful and one will often iterate between
    launching a new volley of jobs and analysis of the results obtained with the previous
    volley in order to guide model design and research. What we need is more, and
    more efficient, automation of hyper-parameter optimization. There are some interesting
    steps in this direction [62, 19, 63, 109] but much more needs to done.
  id: totrans-5519
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，尽管随机搜索是实践者工具箱中有用的补充，半自动探索仍然是有帮助的，通常会在启动新一轮作业和分析上一个轮次获得的结果之间反复进行，以指导模型设计和研究。我们需要的是更高效的超参数优化自动化。在这方面有一些有趣的进展[62,
    19, 63, 109]，但还有很多工作要做。
- en: 19.4 Debugging And Analysis 19.4.1 Gradient Checking And Controlled Overfitting
  id: totrans-5520
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.4 调试与分析 19.4.1 梯度检查与控制过拟合
- en: A very useful debugging step consists in verifying that the implementation of
    the gradient ∂L
  id: totrans-5521
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常有用的调试步骤是验证梯度∂L的实现。
- en: ∂θ is compatible with the computation of L as a function of θ. If the analytically
    computed gradient does not match the one obtained by a finite difference approximation,
    this signals that a bug is probably present somewhere.
  id: totrans-5522
  prefs: []
  type: TYPE_NORMAL
  zh: ∂θ与L作为θ的函数的计算是兼容的。如果解析计算的梯度与通过有限差分近似获得的梯度不匹配，这就表明可能存在某个地方的bug。
- en: First of all, looking at for which i one gets important relative change between
    ∂L
  id: totrans-5523
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，查看对于哪个i，∂L之间的重要相对变化
- en: ∂θi and its finite difference approximation, we can get hints as to where the
    problem may be. An error in sign is particularly troubling, of course. A good
    next step is then to verify in the same way intermediate gradients ∂L
  id: totrans-5524
  prefs: []
  type: TYPE_NORMAL
  zh: 通过∂θi及其有限差分近似，我们可以获得问题可能出现的线索。当然，符号上的错误尤其令人困扰。接下来的一个好步骤是以同样的方式验证中间梯度∂L。
- en: ∂a with a some quantities that depend on the faulty θ, such as intervening neuron
    activations.
  id: totrans-5525
  prefs: []
  type: TYPE_NORMAL
  zh: ∂a与一些依赖于故障θ的量有关，例如中介神经元激活。
- en: 'As many researchers know, the gradient can be approximated by a finite difference
    approximation obtained from the first-order Taylor expansion of a scalar function
    f with respect to a scalar argument x:'
  id: totrans-5526
  prefs: []
  type: TYPE_NORMAL
  zh: 正如许多研究人员所知，梯度可以通过对标量函数f相对于标量变量x的一阶泰勒展开进行有限差分近似：
- en: $${\frac{\partial f(x)}{\partial x}}={\frac{f(x+\varepsilon)-f(x)}{\varepsilon}}+o(\varepsilon)$$
  id: totrans-5527
  prefs: []
  type: TYPE_NORMAL
  zh: $${\frac{\partial f(x)}{\partial x}}={\frac{f(x+\varepsilon)-f(x)}{\varepsilon}}+o(\varepsilon)$$
- en: 'But a less known fact is that a second order approximation can be achieved
    by considering the following alternative formula:'
  id: totrans-5528
  prefs: []
  type: TYPE_NORMAL
  zh: 但一个较不为人知的事实是，可以通过考虑以下替代公式来实现二阶近似：
- en: $${\frac{\partial f(x)}{\partial x}}\approx{\frac{f(x+\varepsilon)-f(x-\varepsilon)}{2\varepsilon}}+o(\varepsilon^{2}).$$
  id: totrans-5529
  prefs: []
  type: TYPE_NORMAL
  zh: $${\frac{\partial f(x)}{\partial x}}\approx{\frac{f(x+\varepsilon)-f(x-\varepsilon)}{2\varepsilon}}+o(\varepsilon^{2})$$。
- en: The second order terms of the Taylor expansion of f(x + ε) and f(x − ε) cancel
    each other because they are even, leaving only 3rd or higher order terms, i.e.,
  id: totrans-5530
  prefs: []
  type: TYPE_NORMAL
  zh: 泰勒展开中f(x + ε)和f(x − ε)的二阶项相互抵消，因为它们是偶数项，仅留下三阶或更高阶项，即，
- en: o(ε2) error after dividing the difference by ε. Hence this formula is twice
    more expensive (not a big deal while debugging) but provides quadratically more
    precision.
  id: totrans-5531
  prefs: []
  type: TYPE_NORMAL
  zh: o(ε2)误差在用ε划分差异后。因此，这个公式的成本是两倍（在调试时并不算大事），但提供了二次更多的精度。
- en: Note that because of finite precision in the computation, there will be a difference
    between the analytic (even correct) and finite difference gradient. Contrary to
    naive expectations, the relative difference may *grow* if we choose an ε that
    is too small, i.e., the error should first decrease as ε is decreased, and then
    may worsen when numerical precision kicks in, due to non-linearities. We have
    often used a value of ε = 10−4 in neural networks, a value that is sufficiently
    small to detect most bugs.
  id: totrans-5532
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于计算中的有限精度，解析（即使正确的）和有限差分梯度之间会存在差异。与天真的期望相反，相对差异可能会*增加*，如果我们选择的ε过小，即，误差应该在ε减小时先减少，然后可能因数值精度的影响而恶化，因非线性效应。我们在神经网络中常常使用ε
    = 10−4的值，这个值足够小以检测大多数错误。
- en: Once the gradient is known to be well computed, another sanity check is that
    gradient descent (or any other gradient-based optimization) should be able to
    overfit on a small training set30. In particular, to factor out effects of SGD
    hyperparameters, a good sanity check for the code (and the other hyper-parameters)
  id: totrans-5533
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦梯度被正确计算，另一个合理的检查是，梯度下降（或任何其他基于梯度的优化）应该能够在小训练集上过拟合。特别地，为了排除SGD超参数的影响，代码（及其他超参数）的一个良好合理性检查。
- en: is to verify that one can overfit on a small training set using a powerful second
    order method such as L-BFGS. For any optimizer, though, as the number of examples
    is increased, the degradation of training error should be gradual while validation
    error should improve. And one typically sees the advantages of SGD
  id: totrans-5534
  prefs: []
  type: TYPE_NORMAL
  zh: 验证是否可以使用强大的二阶方法如L-BFGS在小训练集上过拟合。尽管对于任何优化器，随着示例数量的增加，训练误差的恶化应该是渐进的，而验证误差应该改善。而且通常会看到SGD的优势。
- en: over batch second-order methods like L-BFGS increase as the training set size
    increases. The break-even point may depend on the task, parallelization (multicore
    or GPU, see Sec.19.5 below), and architecture (number of computations compared
    to number of parameters, per example).
  id: totrans-5535
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练集大小的增加，批量二阶方法如L-BFGS的优势也随之增强。盈亏平衡点可能取决于任务、并行化（多核或GPU，见下文Sec.19.5）以及架构（每个示例中计算次数与参数数量的比较）。
- en: Of course, the real goal of learning is to achieve good generalization error,
    and the latter can be estimated by measuring performance on an independent test
    set. When test error is considered too high, the first question to ask is whether
    it is because of a difficulty in optimizing the training criterion or because
    of overfitting. Comparing training error and test error (and how they change as
    we change hyper-parameters that influence capacity, such as the number of training
    iterations) helps to answer that question. Depending on the answer, of course,
    the appropriate ways to improve test error are different. Optimization difficulties
    can be fixed by looking for bugs in the training code, inappropriate values of
    optimization hyper-parameters, or simply insufficient capacity (e.g.
  id: totrans-5536
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，学习的真正目标是实现良好的泛化误差，而后者可以通过在独立测试集上的性能测量来估计。当测试误差被认为过高时，首先要问的问题是：这是否由于优化训练标准的困难，还是由于过拟合。比较训练误差和测试误差（以及当我们更改影响容量的超参数时它们的变化，如训练迭代次数）有助于回答这个问题。根据答案，当然，改善测试误差的适当方法是不同的。优化困难可以通过查找训练代码中的错误、不适当的优化超参数值，或简单地容量不足来解决（例如。
- en: not enough degrees of freedom, hidden units, embedding sizes, etc.). Overfitting
    difficulties can be addressed by collecting more training data, introducing more
    or better regularization terms, multi-task training, unsupervised pre-training,
    unsupervised term in the training criterion, or considering different function
    families (or neural network architectures). In a multi-layer neural network, both
    problems can be simultaneously present. For example, as discussed in Bengio et
    al. [14], Bengio [7], it is possible to have zero training error with a large
    toplevel hidden layer that allows the output layer to overfit, while the lower
    layer are not doing a good job of extracting useful features because they were
    not properly optimized.
  id: totrans-5537
  prefs: []
  type: TYPE_NORMAL
  zh: 自由度不足、隐藏单元、嵌入大小等）。过拟合问题可以通过收集更多训练数据、引入更多或更好的正则化项、多任务训练、无监督预训练、训练标准中的无监督项或考虑不同的函数族（或神经网络架构）来解决。在多层神经网络中，这两种问题可能会同时存在。例如，如Bengio等人
    [14]、Bengio [7] 所讨论的，存在一种可能性，即通过一个较大的顶层隐藏层实现零训练误差，从而使输出层过拟合，而下层则未能很好地提取有用特征，因为它们没有得到适当优化。
- en: Unless using a framework such as Theano which automatically handles the efficient
    allocation of buffers for intermediate results, it is important to pay attention
    to such buffers in the design of the code. The first objective is to avoid memory
    allocation in the middle of the training loop, i.e., all memory buffers should
    be allocated once and for all. Careless reuse of the same memory
  id: totrans-5538
  prefs: []
  type: TYPE_NORMAL
  zh: 除非使用像Theano这样的框架，它自动处理中间结果的高效缓冲分配，否则在代码设计中注意这些缓冲区是很重要的。首要目标是在训练循环中避免内存分配，即所有内存缓冲区应一次性分配完成。粗心的重用相同的内存
- en: 30 In principle, bad local minima could prevent that, but in the overfitting
    regime, e.g., with more hidden units than examples, the global minimum of the
    training error can generally be reached almost surely from random initialization,
    presumably because the training criterion becomes convex in the parameters that
    suffice to get the training error to zero [12], i.e., the output weights of the
    neural network.
  id: totrans-5539
  prefs: []
  type: TYPE_NORMAL
  zh: 从原则上讲，糟糕的局部极小值可能会阻止这一点，但在过拟合状态下，例如，当隐藏单元数量超过样本数量时，训练误差的全局最小值通常几乎可以从随机初始化中几乎肯定地达到，这可能是因为训练标准在足以将训练误差降至零的参数中变得凸性
    [12]，即神经网络的输出权重。
- en: buffers for different uses can however lead to bugs, which can be checked, in
    the debugging phase, by initializing buffers to the NaN (Not-A-Number) value,
    which propagates into downstream computation (making it easy to detect that uninitialized
    values were used)31.
  id: totrans-5540
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，不同用途的缓冲区重用可能导致错误，这可以在调试阶段通过将缓冲区初始化为NaN（非数字）值进行检查，该值会传播到下游计算中（使得容易检测未初始化的值被使用）31。
- en: 19.4.2 Visualizations And Statistics
  id: totrans-5541
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.4.2 可视化与统计
- en: The most basic statistics that should be measured during training are error
    statistics. The *average loss* on the training set and the validation set and
    their evolution during training are very useful to monitor progress and differentiate
    overfitting from poor optimization. To make comparisons easier, it may be useful
    to compare neural networks during training in terms of their "age" (number of
    updates made times mini-batch size B, i.e., number of examples visited) rather
    than in terms of number of epochs (which is very sensitive to the training set
    size).
  id: totrans-5542
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中应测量的最基本统计数据是误差统计。训练集和验证集上的*平均损失*及其在训练过程中的演变对于监测进展以及区分过拟合与优化不良非常有用。为了便于比较，比较神经网络在训练过程中的“年龄”（已进行的更新次数乘以小批量大小B，即访问的样本数量）可能比根据轮数（这对训练集大小非常敏感）进行比较更有用。
- en: When using unsupervised training to learn the first few layers of a deep architecture,
    a very common debugging and analysis tool is the visualization of filters, i.e.,
    of the weight vectors associated with individual hidden units. This is simplest
    in the case of the first layer and where the inputs are images (or image patches),
    time-series, or spectrograms (all of which are visually interpretable). Several
    recipes have been proposed to extend this idea to visualize the preferred input
    of hidden units in layers that follow the first one [81, 43]. In the case of the
    first layer, since one often obtains Gabor filters, a parametric fit of these
    filters to the weight vector can be done so as to visualize the distribution of
    orientations, positions and scales of the learned filters. An interesting special
    case of visualizing first-layer weights is the visualization of *word embeddings*
    (see Section 19.5.3 below) using a dimensionality reduction technique such as
    t-SNE [113].
  id: totrans-5543
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用无监督训练学习深度架构的前几层时，一种非常常见的调试和分析工具是滤波器的可视化，即与各个隐藏单元关联的权重向量的可视化。这在第一层且输入为图像（或图像块）、时间序列或声谱图（这些都是可视化可解释的）时最为简单。已经提出了几种方案，将这一理念扩展到可视化第一层之后层中隐藏单元的优选输入[81,
    43]。在第一层的情况下，由于通常获得Gabor滤波器，因此可以对这些滤波器与权重向量进行参数拟合，从而可视化学习到的滤波器的方向、位置和尺度的分布。可视化第一层权重的一个有趣特殊情况是使用降维技术（如t-SNE）可视化*词嵌入*（见下文第19.5.3节）。
- en: An extension of the idea of visualizing filters (which can apply to non-linear
    or deeper features) is that of visualizing local (arount the given test point)
    leading tangent vectors, i.e., the main directions in input space to which the
    representation (at a given layer) is most sensitive to [100].
  id: totrans-5544
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化滤波器（可以应用于非线性或更深特征）这一理念的延伸是可视化局部（围绕给定测试点）主切向量，即输入空间中表示（在给定层次）对其最敏感的主要方向[100]。
- en: In the case where the inputs are not images or easily visualizable, or to get
    a sense of the weight values in different hidden units, Hinton diagrams [58] are
    also very useful, using small squares whose color (black or white) indicates a
    weight's sign and whose area represents its magnitude.
  id: totrans-5545
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入不是图像或不易可视化的情况下，或者为了感知不同隐藏单元中的权重值，Hinton 图[58]也非常有用，它使用小方块，其颜色（黑色或白色）表示权重的符号，面积表示其大小。
- en: Another way to visualize what has been learned by an unsupervised (or joint
    label-input) model is to look at samples from the model. Sampling procedures have
    been defined at the outset for RBMs, Deep Belief Nets, and Deep Boltzmann Machines,
    for example based on Gibbs sampling. When weights become larger, mixing between
    modes can become very slow with Gibbs sampling. An interesting alternative is
    rates-FPCD [112, 30] which appears to be more robust to this problem and generally
    mixes faster, but at the cost of losing theoretical guarantees.
  id: totrans-5546
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可视化无监督（或联合标签-输入）模型学习结果的方法是查看模型的样本。采样程序在RBM、深度信念网络和深度玻尔兹曼机等模型中已被定义，例如基于吉布斯采样。当权重变得更大时，吉布斯采样中模式之间的混合可能会变得非常缓慢。一个有趣的替代方案是rates-FPCD[112,
    30]，它似乎对这个问题更具鲁棒性，通常混合得更快，但以失去理论保证为代价。
- en: 31 Personal communication from David Warde-Farley, who learned this trick from
    Sam Roweis.
  id: totrans-5547
  prefs: []
  type: TYPE_NORMAL
  zh: 31来自大卫·沃德-法利的个人通信，他从萨姆·罗维斯那里学到了这个技巧。
- en: In the case of auto-encoder variants, it was not clear until recently whether
    they were really capturing the underlying density (since they are not optimized
    with respect to the maximum likelihood principle or an approximation of it). It
    was therefore even less clear if there existed appropriate sampling algorithms
    for auto-encoders, but a recent proposal for sampling from contractive auto-encoders
    appears to be working very well [101], based on arguments about the geometric
    interpretation of the first derivative of the encoder [16], showing that denoising
    and contractive auto-encoders capture local moments (first and second) of the
    training density.
  id: totrans-5548
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自编码器变体，直到最近才明确它们是否真的捕捉到了基础密度（因为它们并未根据最大似然原则或其近似进行优化）。因此，更不清楚是否存在适用于自编码器的适当采样算法，但最近提出的一种从收缩自编码器进行采样的方案似乎效果很好[101]，基于关于编码器一阶导数的几何解释[16]，显示去噪和收缩自编码器捕捉到了训练密度的局部矩（第一和第二）。
- en: To get a sense of what individual hidden units represent, it has also been proposed
    to vary only one unit while keeping the others fixed, e.g., to the value obtained
    by finding the hidden units representation associated with a particular input
    example.
  id: totrans-5549
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解单个隐藏单元所代表的内容，还建议仅改变一个单元，同时保持其他单元不变，例如，将其固定为与特定输入示例关联的隐藏单元表示所获得的值。
- en: Another interesting technique is the *visualization of the learning trajectory
    in* function space [44]. The idea is to associate the function (as opposed to
    simply the parameters) computed by a neural network with a low-dimensional (2-D
    or 3-D) representation, e.g., with the t-SNE [113] or Isomap [111] algorithms,
    and then plot the evolution of this function during training, or the population
    of such trajectories for different initializations. This provides visualization
    of effective local minima32 and shows that no two different random initializations
    ended up in the same effective local minimum.
  id: totrans-5550
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的技术是*在*函数空间中可视化学习轨迹[44]。这个想法是将神经网络计算的函数（与仅仅是参数相对）与低维（2D或3D）表示关联，例如，使用t-SNE[113]或Isomap[111]算法，然后绘制该函数在训练过程中的演变，或不同初始化下的轨迹群体。这提供了有效局部最小值的可视化，并显示出没有两个不同的随机初始化最终落在同一个有效局部最小值上。
- en: Finally, another useful type of visualization is to display statistics (e.g.,
    histogram, mean and standard deviation) of activations (inputs and outputs of
    the non-linearities at each layer), activation gradients, parameters and parameter
    gradients, by groups (e.g. different layers, biases vs weights) and across training
    iterations. See Glorot and Bengio [48] for a practical example. A particularly
    interesting quantity to monitor is the discriminative ability of the representations
    learnt at each layer, as discussed in [85], and ultimately leading to an analysis
    of the disentangled factors captured by the different layers as we consider deeper
    architectures.
  id: totrans-5551
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，另一种有用的可视化类型是通过组（例如不同层、偏置与权重）以及跨训练迭代显示激活（每层非线性输入和输出）、激活梯度、参数和参数梯度的统计数据（例如直方图、均值和标准差）。有关实际示例，请参见Glorot和Bengio
    [48]。一个特别有趣的监测量是各层学习的表示的判别能力，如[85]中讨论的，最终导致对不同层捕捉的解缠因子的分析，尤其是在考虑更深的架构时。
- en: 19.5 Other Recommendations 19.5.1 Multi-Core Machines, Blas And Gpus
  id: totrans-5552
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.5 其他建议 19.5.1 多核机器、BLAS与GPU
- en: Matrix operations are the most time-consuming in efficient implementations of
    many machine learning algorithms and this is particularly true of neural networks
    and deep architectures. The basic operations are matrix-vector products
  id: totrans-5553
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵运算在许多机器学习算法的高效实现中是最耗时的，尤其是在神经网络和深度架构中。基本操作是矩阵-向量乘法。
- en: '(forward propagation and back-propagation) and vector times vector outer products
    (resulting in a matrix of weight gradients). Matrix-matrix multiplications can
    be done substantially faster than the equivalent sequence of matrix-vector products
    for two reasons: by smart caching mechanisms such as implemented in the BLAS library
    (which is called from many higher-level environments such as'
  id: totrans-5554
  prefs: []
  type: TYPE_NORMAL
  zh: （前向传播和反向传播）以及向量与向量外积（产生权重梯度的矩阵）。矩阵-矩阵乘法可以比相应的矩阵-向量乘法序列更快地完成，原因有两个：通过智能缓存机制，例如在BLAS库中实现的（该库被许多更高级别的环境调用）。
- en: 32 It is difficult to know for sure if it is a true local minima or if it appears
    like one because the optimization algorithm is stuck.
  id: totrans-5555
  prefs: []
  type: TYPE_NORMAL
  zh: 32 确定它是否是真正的局部最小值还是因为优化算法停滞而看起来像一个局部最小值是很困难的。
- en: python's numpy and Theano, Matlab, Torch or Lush), and thanks to parallelism.
  id: totrans-5556
  prefs: []
  type: TYPE_NORMAL
  zh: python的numpy和Theano、Matlab、Torch或Lush）以及得益于并行性。
- en: Appropriate versions of BLAS can take advantage of multi-core machines to distribute
    these computations on multi-core machines. The speed-up is however generally a
    fraction of the total speedup one can hope for (e.g. 4× on a 4-core machine),
    because of communication overheads and because not all computation is parallelized.
    Parallelism becomes more efficient when the sizes of these matrices is increased,
    which is why mini-batch updates can be computationally advantageous, and more
    so when more cores are present.
  id: totrans-5557
  prefs: []
  type: TYPE_NORMAL
  zh: 适当版本的BLAS可以利用多核机器将这些计算分布到多核机器上。然而，由于通信开销以及并非所有计算都是并行化的，因此加速通常只是总加速的一部分（例如，在4核机器上为4倍）。当这些矩阵的大小增加时，平行计算变得更有效，这就是为什么小批量更新在计算上具有优势，尤其是在有更多核心时。
- en: The extreme multi-core machines are the GPUs (Graphics Processing Units),
  id: totrans-5558
  prefs: []
  type: TYPE_NORMAL
  zh: 极端的多核机器是GPU（图形处理单元），
- en: with hundreds of cores. Unfortunately, they also come with constraints and specialized
    compilers which make it more difficult to fully take advantage of their potential.
    On 512-core machines, we are routinely able to get speed-ups of 4×
  id: totrans-5559
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有数百个核心。不幸的是，它们也伴随着限制和专用编译器，使得充分利用其潜力变得更加困难。在512核机器上，我们通常能够获得4倍的加速。
- en: to 40× for large neural networks. To make the use of GPUs practical, it really
    helps to use existing libraries that efficiently implement computations on GPUs.
    See Bergstra *et al.* [18] for a comparative study of the Theano library
  id: totrans-5560
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型神经网络，可以达到40倍的加速。为了使GPU的使用变得实用，利用现有库来高效实现GPU上的计算确实非常有帮助。请参见Bergstra *et al.*
    [18]，了解Theano库的比较研究。
- en: (which compiles numpy-like code for GPUs). One practical issue is that only
    the GPU-compiled operations will typically be done on the GPU, and that transfers
    between the GPU and CPU considerably slow things down. It is important to use
    a profiler to find out what is done on the GPU and how efficient these operations
    are in order to quickly invest one's time where needed to make an implementation
    GPU-efficient and keep most operations on the GPU card.
  id: totrans-5561
  prefs: []
  type: TYPE_NORMAL
  zh: （它编译适用于GPU的类似numpy的代码）。一个实际问题是，通常只有GPU编译的操作会在GPU上执行，而GPU和CPU之间的传输会显著降低速度。使用分析工具来了解在GPU上完成了什么以及这些操作的效率是很重要的，以便快速将时间投资到使实现高效的GPU，并保持大多数操作在GPU卡上。
- en: 19.5.2 Sparse High-Dimensional Inputs
  id: totrans-5562
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.5.2 稀疏高维输入
- en: Sparse high-dimensional inputs can be efficiently handled by traditional supervised
    neural networks by using a sparse matrix multiplication. Typically, the input
    is a sparse vector while the weights are in a dense matrix, and one should use
    an efficient implementation made for just this case in order to optimally take
    advantage of sparsity. There is still going to be an overhead on the order of
    2×
  id: totrans-5563
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的监督神经网络可以通过使用稀疏矩阵乘法有效处理稀疏高维输入。通常，输入是一个稀疏向量，而权重则是在一个稠密矩阵中，应该使用专为此情况制作的高效实现，以最优利用稀疏性。仍然会有大约2倍的开销。
- en: or more (on the multiply-add operations, not the others) compared to a dense
    implementation of the matrix-vector product.
  id: totrans-5564
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于稠密实现的矩阵-向量乘积，在乘法-加法操作上可以获得4倍或更多的加速。
- en: For many unsupervised learning algorithms there is unfortunately a difficulty.
  id: totrans-5565
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多无监督学习算法，不幸的是存在一个困难。
- en: The computation for these learning algorithms usually involves some kind of
    reconstruction of the input (like for all auto-encoder variants, but also for
    RBMs and sparse coding variants), as if the inputs were in the output space of
    the learner. Two exceptions to this problem are semi-supervised embedding [117]
    and Slow Feature Analysis [119, 20]. The former pulls the representation of nearby
    examples near each other and pushes dissimilar points apart, while also tuning
    the representation for a supervised learning task. The latter maximizes the learned
    features' variance while minimizing their covariance and maximizing their temporal
    auto-correlation.
  id: totrans-5566
  prefs: []
  type: TYPE_NORMAL
  zh: 这些学习算法的计算通常涉及某种形式的输入重构（例如所有自编码器变体，以及RBM和稀疏编码变体），就好像输入处于学习者的输出空间一样。对此问题的两个例外是半监督嵌入
    [117] 和慢特征分析 [119, 20]。前者将相近示例的表示拉近，而将不相似的点推远，同时为监督学习任务调整表示。后者最大化所学特征的方差，同时最小化其协方差，并最大化其时间自相关。
- en: For algorithms that do need a form of input reconstruction, an efficient approach
    based on *sampled reconstruction* [39] has been proposed, successfully implemented
    and evaluated for the case of auto-encoders and denoising autoencoders. The first
    idea is that on each example (or mini-batch), one samples a subset of the elements
    of the reconstruction vector, along with the associated reconstruction loss. One
    only needs to compute the reconstruction and the loss associated with these sampled
    elements (or features), as well as the associated back-propagation operations
    into hidden units and reconstruction weights. That alone would multiplicatively
    reduce the computational cost by the amount of sparsity but make the gradient
    much more noisy and possibly biased as well, if the sampling distribution was
    chosen not uniform. To reduce the variance of that estimator, the idea is to guess
    for which features the reconstruction loss will be larger and to sample with higher
    probability these features (and their loss). In particular, the authors always
    sample the features with a non-zero in the input (or the corrupted input, in the
    denoising case), and uniformly sample an equal number of those with a zero in
    the input and corrupted input. To make the estimator unbiased now requires introducing
    a weight on the reconstruction loss associated with each sampled feature, inversely
    proportional to the probability of sampling it, i.e., this is an importance sampling
    scheme. The experiments show that the speed-up increases linearly with the amount
    of sparsity while the average loss is optimized as well as in the deterministic
    full-computation case.
  id: totrans-5567
  prefs: []
  type: TYPE_NORMAL
  zh: 对于确实需要某种输入重建形式的算法，已提出一种基于*采样重建*的方法[39]，并成功实施和评估了自编码器和去噪自编码器的案例。第一个想法是在每个示例（或小批量）中，从重建向量的元素中采样一个子集，以及相关的重建损失。只需计算这些采样元素（或特征）的重建及其相关损失，以及反向传播操作到隐藏单元和重建权重。单凭这一点就能按稀疏程度乘法减少计算成本，但如果采样分布选择不均匀，则会使梯度噪声更大，可能还会有偏差。为了降低该估计器的方差，思路是猜测哪些特征的重建损失会更大，并以更高的概率对这些特征（及其损失）进行采样。特别是，作者总是对输入中非零的特征（或去噪情况下的损坏输入）进行采样，同时均匀采样相同数量的零特征和损坏输入特征。要使估计器无偏，现在需要对与每个采样特征相关的重建损失引入一个权重，该权重与采样概率成反比，即这是一种重要性采样方案。实验表明，加速随着稀疏程度的增加线性增长，同时平均损失也得到了优化，类似于确定性全计算情况。
- en: 19.5.3 Symbolic Variables, Embeddings, Multi-Task Learning And Multi-Relational
    Learning
  id: totrans-5568
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.5.3 符号变量、嵌入、多任务学习和多关系学习
- en: 'Parameter sharing [68, 77, 68, 31, 4, 5] is an old neural network technique
    for increasing statistical power: if a parameter is used in N times more contexts
    (different tasks, different parts of the input, etc.) then it may be as if we
    had N times more training examples for tuning its value. More examples to estimate
    a parameter reduces its variance (with respect to sampling of training examples),
    which is directly influencing generalization error: for example the generalization
    mean squared error can be decomposed as the sum of a bias term and a variance
    term [46]. The reuse idea was first exploited by applying the same parameter to
    different parts of the input, as in convolutional neural networks [68, 77]. Reuse
    was also exploited by sharing the lower layers of a network (and the representation
    of the input that they capture) across multiple tasks associated with different
    outputs of the network [31, 4, 5]. This idea is also one of the key motivations
    behind Deep Learning [7] because one can think of the intermediate features computed
    in higher (deeper) layers as different tasks that can share the sub-features computed
    in lower layers (nearer the input). This very basic notion of reuse is key to
    improving generalization in many settings, guiding the design of neural network
    architectures in practical applications as well.'
  id: totrans-5569
  prefs: []
  type: TYPE_NORMAL
  zh: 参数共享[68, 77, 68, 31, 4, 5]是一种古老的神经网络技术，用于提高统计能力：如果一个参数在N倍更多的上下文中使用（不同的任务、输入的不同部分等），那么就好像我们有N倍更多的训练示例来调整其值。更多示例用于估计参数可以减少其方差（相对于训练示例的抽样），这直接影响到泛化误差：例如，泛化均方误差可以分解为偏差项和方差项的总和[46]。重用的思想最初是通过将相同的参数应用于输入的不同部分来利用的，如同卷积神经网络[68,
    77]。重用也通过在多个与网络不同输出相关的任务中共享网络的低层（及其捕获的输入表示）来实现[31, 4, 5]。这个思想也是深度学习背后的关键动机之一[7]，因为人们可以将更高（更深）层中计算的中间特征视为可以共享低层（靠近输入）计算的子特征的不同任务。这种非常基本的重用概念对于在许多场景中提高泛化能力至关重要，并指导了实际应用中神经网络架构的设计。
- en: An interesting special case of these ideas is in the context of learning with
    symbolic data. If some input variables are symbolic, taking value in a finite
    alphabet, they can be represented as neural network inputs by a one-hot subvector
    of the input vector (with a 0 everywhere except at the position associated with
    the particular symbol). Now, sometimes different input variables refer to different
    instances of the same *type* of symbol. A patent example is with neural language
    models [11, 6], where the input is a sequence of words. In these models, the same
    input layer weights are reused for words at different positions in the input sequence
    (as in convolutional networks). The product of a one-hot sub-vector with this
    shared weight matrix is a generally dense vector, and this associates each symbol
    in the alphabet with a point in a vector space33, which we call its *embedding*.
    The idea of vector space representations for words and symbols is older [40] and
    is a particular case of the notion of *distributed representation* [57, 58] central
    to the connectionist approaches. Learned embeddings of symbols (or other objects)
    can be conveniently visualized using a dimensionality reduction algorithm such
    as t-SNE [113].
  id: totrans-5570
  prefs: []
  type: TYPE_NORMAL
  zh: 这些思想的一个有趣特殊案例是在使用符号数据学习的背景下。如果某些输入变量是符号性的，取值于有限字母表，它们可以通过输入向量的一个热编码子向量表示为神经网络输入（在除了与特定符号关联的位置外，其他位置均为0）。现在，有时不同的输入变量指代同一种*类型*的符号。一个明显的例子是神经语言模型[11,
    6]，其输入是一系列单词。在这些模型中，相同的输入层权重被重复用于输入序列中不同位置的单词（如同卷积网络）。一个热编码子向量与该共享权重矩阵的乘积是一个通常较为稠密的向量，这将字母表中的每个符号与一个向量空间中的点关联起来，这个点我们称之为它的*嵌入*。用于单词和符号的向量空间表示的思想较早[40]，并且是*分布式表示*[57,
    58]这一与连接主义方法密切相关的概念的特定案例。符号（或其他对象）的学习嵌入可以方便地使用降维算法如t-SNE[113]进行可视化。
- en: In addition to sharing the embedding parameters across positions of words in
    an input sentence, Collobert *et al.* [36] share them across natural language
    processing tasks such as Part-Of-Speech tagging, chunking and semantic role labeling.
    Parameter sharing is a key idea behind convolutional nets, recurrent neural networks
    and dynamic Bayes nets, in which the same parameters are used for different temporal
    or spatial slices of the data. This idea has been generalized from sequences and
    2-D images to arbitrary graphs with recursive neural networks or recursive graphical
    models [92, 45, 25, 108], Markov Logic Networks [98]
  id: totrans-5571
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在输入句子的词汇位置之间共享嵌入参数，Collobert *等人* [36] 还在自然语言处理任务中共享这些参数，例如词性标注、分块和语义角色标注。参数共享是卷积网络、递归神经网络和动态贝叶斯网络背后的关键思想，其中相同的参数用于数据的不同时间或空间切片。这个理念已经从序列和二维图像推广到任意图形，使用递归神经网络或递归图模型
    [92, 45, 25, 108]，以及马尔可夫逻辑网络 [98]。
- en: and relational learning [47]. A relational database can be seen as a set of
    objects
  id: totrans-5572
  prefs: []
  type: TYPE_NORMAL
  zh: 关系学习和关系数据库可以视为一组对象。
- en: (or typed values) and relations between them, of the form (object1, relationtype,
    object2). The same global set of parameters can be shared to characterize such
    relations, across relations (which can be seen as tasks) and objects. Objectspecific
    parameters are the parameters specifying the embedding of a particular discrete
    object. One can think of the elements of each embedding vector as *implicit learned
    attributes*. Different tasks may demand different attributes, so that objects
    which share some underlying characteristics and behavior should end up having
    similar values of some of their attributes. For example, words appearing in semantically
    and syntactically similar contexts end up getting a very close embedding [36].
    If the same attributes can be useful for several tasks, then statistical power
    is gained through parameter sharing, and transfer of information between tasks
    can happen, making the data of some task informative for generalizing properly
    on another task.
  id: totrans-5573
  prefs: []
  type: TYPE_NORMAL
  zh: （或类型值）和它们之间的关系，形式为（对象1，关系类型，对象2）。可以共享同一组全局参数来表征这种关系，跨越关系（可以视为任务）和对象。对象特定参数是指定特定离散对象嵌入的参数。可以将每个嵌入向量的元素视为*隐式学习属性*。不同的任务可能要求不同的属性，因此共享某些基本特征和行为的对象最终应在某些属性上具有相似的值。例如，出现在语义和句法相似上下文中的单词最终会获得非常接近的嵌入
    [36]。如果相同的属性对多个任务有用，则通过参数共享获得统计效力，任务之间的信息传递可以发生，使得某个任务的数据对在另一个任务上的正确泛化具有信息价值。
- en: 'The idea proposed in Bordes *et al.* [23, 24] is to learn an energy function
    that is lower for positive (valid) relations present in the training set, and
    parametrized in two parts: on the one hand the symbol embeddings and on the other
    hand the rest of the neural network that maps them to a scalar energy. In addition,
    by considering relation types themselves as particular symbolic objects, the model
    can reason about relations themselves and have relations between relation types.
    For example, ''To be'' can act as a relation type (in subject-attribute relations)
    but in the statement " ''To be'' is a verb" it appears both as a relation type
    and as an object of the relation.'
  id: totrans-5574
  prefs: []
  type: TYPE_NORMAL
  zh: Bordes *等人* [23, 24] 提出的理念是学习一种能量函数，对于训练集中存在的正（有效）关系，其能量值较低，并分为两个部分：一方面是符号嵌入，另一方面是将它们映射为标量能量的神经网络。此外，通过将关系类型本身视为特定的符号对象，模型能够推理关系本身并在关系类型之间建立联系。例如，“存在”可以作为一种关系类型（在主语-属性关系中），但在“‘存在’是一个动词”的陈述中，它既作为一种关系类型又作为关系的对象出现。
- en: Such multi-relational learning opens the door to the application of neural networks
    outside of their traditional applications, which was based on a single homogeneous
    source of data, often seen as a matrix with one row per example and one column
    (or group of columns) per random variable. Instead, one often has multiple heterogeneous
    sources of data (typically providing examples seen
  id: totrans-5575
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多关系学习为神经网络的应用打开了新的大门，超越了基于单一同质数据源的传统应用，通常被视为每个示例一行、每个随机变量一列（或一组列）的矩阵。相反，通常存在多个异构数据源（通常提供的示例是
- en: 33 The result of the matrix multiplication, which equals one of the columns
    of the matrix.
  id: totrans-5576
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法的结果，等于矩阵的其中一列。
- en: as a tuple of values), each involving different random variables. So long as
    these different sources *share some variables*, then the above multi-relational
    multitask learning approaches can be applied. Each variable can be associated
    with its embedding function (that maps the value of a variable to a generic representation
    space that is valid *across tasks and data sources*). This framework can be applied
    not only to symbolic data but to mixed symbolic/numeric data if the mapping from
    object to embedding is generalized from a table look-up to a parametrized function
    (the simplest being a linear mapping) from its raw attributes (e.g., image features)
    to its embedding. This has been exploited successfully to design image search
    systems in which images and queries are mapped to the same semantic space [118].
  id: totrans-5577
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一组值的元组，每个元组涉及不同的随机变量。只要这些不同的来源*共享一些变量*，那么上述多关系多任务学习方法就可以应用。每个变量可以与其嵌入函数相关联（该函数将变量的值映射到一个有效的通用表示空间，该空间在*任务和数据源之间有效*）。这个框架不仅可以应用于符号数据，也可以应用于混合符号/数值数据，如果从对象到嵌入的映射从表查找推广到从原始属性（例如，图像特征）到嵌入的参数化函数（最简单的是线性映射）。这一点已成功用于设计图像搜索系统，其中图像和查询被映射到同一语义空间
    [118]。
- en: 19.6 Open Questions 19.6.1 On The Added Difficulty Of Training Deeper Architectures
  id: totrans-5578
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.6 未解的问题 19.6.1 关于训练更深架构的额外难度
- en: There are experimental results which provide some evidence that, at least in
    some circumstances, deeper neural networks are more difficult to train than shallow
    ones, in the sense that there is a greater chance of missing out on better minima
    when starting from random initialization. This is borne out by all the experiments
    where we find that some initialization scheme can drastically improve performance.
    In the Deep Learning literature this has been shown with the use of unsupervised
    pre-training (supervised or not), both applied to supervised tasks - training
    a neural network for classification [61, 14, 95] - and unsupervised tasks - training
    a Deep Boltzmann Machine to model the data distribution [104].
  id: totrans-5579
  prefs: []
  type: TYPE_NORMAL
  zh: 有实验结果提供了一些证据，表明至少在某些情况下，深层神经网络比浅层神经网络更难训练，因为随机初始化时更有可能错过更好的最小值。所有实验都表明，某些初始化方案可以显著提高性能。在深度学习文献中，这通过使用无监督预训练（无论是有监督还是无监督）得到验证，应用于有监督任务——训练用于分类的神经网络
    [61, 14, 95]——以及无监督任务——训练深度玻尔兹曼机以建模数据分布 [104]。
- en: The learning trajectories visualizations of Erhan *et al.* [44] have shown that
    even when starting from nearby configurations in function space, different initializations
    seem to always fall in a different effective local minimum. Furthermore, the same
    study showed that the minima found when using unsupervised pretraining were far
    in function space from those found from random initialization, in addition to
    giving better generalization error. Both of these findings highlight the importance
    of initialization, hence of local minima effects, in deep networks.
  id: totrans-5580
  prefs: []
  type: TYPE_NORMAL
  zh: Erhan *et al.* [44]的学习轨迹可视化表明，即使从函数空间中的相邻配置开始，不同的初始化似乎总是落入不同的有效局部最小值。此外，同一研究显示，使用无监督预训练时找到的最小值在函数空间中远离随机初始化找到的最小值，并且提供了更好的泛化误差。这两个发现都突显了初始化的重要性，从而强调了局部最小值效应在深度网络中的重要性。
- en: Finally, it has been shown that these effects were both increased when considering
    deeper architectures [44].
  id: totrans-5581
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，已经显示，当考虑更深的架构时，这些影响会增加 [44]。
- en: There are also results showing that specific ways of setting the initial distribution
    and ordering of examples ("curriculum learning") can yield better solutions [42,
    15, 66]. This also suggest that very particular ways of initializing parameters,
    very different from uniformly sampled, can have a strong impact on the solutions
    found by gradient descent. The hypothesis proposed in [15] is that curriculum
    learning can act similarly to a *continuation method*, i.e., starting from an
    easier optimization task (e.g. convex) and tracking the local minimum as the learning
    task is gradually made more difficult and closer to the real task of interest.
  id: totrans-5582
  prefs: []
  type: TYPE_NORMAL
  zh: 还有研究结果表明，特定的初始分布设置和样本排序方式（“课程学习”）可以产生更好的解决方案 [42, 15, 66]。这也表明，初始化参数的特定方式，与均匀采样有很大不同，可能对梯度下降找到的解决方案有强烈影响。[15]中提出的假设是，课程学习可以类似于*延续方法*，即从更简单的优化任务（例如凸优化）开始，并随着学习任务逐渐变得更加困难，更接近实际感兴趣的任务时，跟踪局部最小值。
- en: Why would training deeper networks be more difficult? This is clearly still
    an open question. A plausible partial answer is that deeper networks are also
    more non-linear (since each layer composes more non-linearity on top of the previous
    ones), making gradient-based methods less efficient. It may also be that the number
    and structure of local minima both change qualitatively as we increase depth.
    Theoretical arguments support a potentially exponential gain in expressive power
    of deeper architectures [7, 9] and it would be plausible that with this added
    expressive power coming from the combinatorics of composed reuse of sub-functions
    could come a corresponding increase in the number (and possibly quality) of local
    minima. But the best ones could then also be more difficult to find.
  id: totrans-5583
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么训练更深的网络会更困难？这个问题显然仍然没有答案。一个合理的部分答案是，更深的网络也更具非线性（因为每一层在前一层之上叠加了更多的非线性），使得基于梯度的方法效率降低。随着深度增加，局部最小值的数量和结构可能会在质量上发生变化。理论论证支持更深架构在表达能力上的潜在指数增长[7,
    9]，并且可以合理地认为，随着来自组合重用子函数的表达能力的增加，局部最小值的数量（和可能的质量）也会相应增加。但最好的局部最小值可能也更难找到。
- en: 'On the practical side, several experimental results point to factors that may
    help training deep architectures:'
  id: totrans-5584
  prefs: []
  type: TYPE_NORMAL
  zh: 从实践角度来看，一些实验结果指出可能有助于训练深度架构的因素：
- en: '- A local training signal. What many successful procedures for training deep
    networks have in common is that they involve a local training signal that helps
    each layer decide what to do without requiring the back-propagation of gradients
    through many non-linearities. This includes of course the many variants of greedy
    layer-wise pre-training but also the less well-known semisupervised embedding
    algorithm [117].'
  id: totrans-5585
  prefs: []
  type: TYPE_NORMAL
  zh: '- 本地训练信号。许多成功的深度网络训练程序的共同点在于，它们涉及一个本地训练信号，帮助每一层决定该做什么，而不需要通过许多非线性层进行梯度反向传播。当然，这也包括多种贪婪层次的预训练变体，以及不那么知名的半监督嵌入算法[117]。'
- en: '- Initialization in the right range. Based on the idea that both activations
    and gradients should be able to flow well through a deep architecture without
    significant reduction in variance, Glorot and Bengio [48] proposed setting up
    the initial weights to make the Jacobian of each layer have singular values near
    1 (or preserve variance in both directions). In their experiments this clearly
    helped greatly reducing the gap between purely supervised and pretrained deep
    networks.'
  id: totrans-5586
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在合适范围内初始化。基于激活和梯度应能在深度架构中良好流动而不会显著降低方差的理念，Glorot和Bengio[48]提出设置初始权重，使得每一层的雅可比矩阵的奇异值接近1（或在两个方向上保持方差）。在他们的实验中，这显著帮助减少了纯监督和预训练深度网络之间的差距。'
- en: '- Choice of non-linearities. In the same study [48] and a follow-up [49]'
  id: totrans-5587
  prefs: []
  type: TYPE_NORMAL
  zh: '- 非线性的选择。在同一研究[48]及后续研究[49]中'
- en: it was shown that the choice of hidden layer non-linearities interacted with
    depth. In particular, without unsupervised pre-training, a deep neural network
    with sigmoids in the top hidden layer would get stuck for a long time on a plateau
    and generally produce inferior results, due to the special role of 0 and of the
    initial gradients from the output units. Symmetric nonlinearities like the hyperbolic
    tangent did not suffer from that problem, while softer non-linearities (without
    exponential tails) such as the *softsign* function s(a) = a 1+|a| worked even
    better. In Glorot *et al.* [49] it was shown that an asymmetric but hard-limiting
    non-linearity such as the rectifier (s(a) = max(0, a), see also [86]) actually
    worked very well (but should not be used for output units), in spite of the prior
    belief that the fact that when hidden units are saturated, gradients would not
    flow well into lower layers. In fact gradients flow very well, but on selected
    paths, possibly making the credit assignment (which parameters should change to
    handle the current error) sharper and the Hessian condition number better. A recent
    heuristic that is related to the difficulty of gradient propagation through neural
    net non-linearities is the idea of "centering" the non-linear operation such that
    each hidden unit has zero average output and zero average slope [107, 94].
  id: totrans-5588
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，隐藏层非线性的选择与深度有关。特别是，如果没有无监督的预训练，具有sigmoid激活函数的深度神经网络在顶部隐藏层会长时间陷入平稳状态，并且通常产生较差的结果，这与0的特殊角色以及输出单元的初始梯度有关。对称非线性如双曲正切没有遇到这个问题，而较软的非线性（没有指数尾部）如*softsign*函数
    s(a) = a / (1 + |a|) 的效果甚至更好。在Glorot *等* [49] 的研究中表明，像ReLU（s(a) = max(0, a)，另见
    [86]）这样的不对称但硬限制的非线性实际上表现得很好（但不应用于输出单元），尽管之前认为隐藏单元饱和时，梯度无法良好传播到下层。实际上，梯度在选定路径上流动非常好，这可能使得信用分配（哪些参数应更改以处理当前错误）更为明确，并改善Hessian条件数。最近与神经网络非线性传播困难相关的启发式方法是“中心化”非线性操作的想法，使每个隐藏单元的平均输出和平均斜率均为零
    [107, 94]。
- en: 19.6.2 Adaptive Learning Rates And Second-Order Methods
  id: totrans-5589
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.6.2 自适应学习率与二阶方法
- en: To improve convergence and remove learning rates from the list of hyper-parameters,
    many authors have advocated exploring adaptive learning rate methods, either for
    a global learning rate [32], a layer-wise learning rate, a neuron-wise learning
    rate, or a parameter-wise learning rate [22] (which then starts to look like a
    diagonal Newton method). LeCun [76], LeCun *et al.* [79]
  id: totrans-5590
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改善收敛性并将学习率从超参数列表中移除，许多作者提倡探索自适应学习率方法，无论是全局学习率 [32]、层级学习率、神经元级学习率，还是参数级学习率
    [22]（这开始看起来像是对角牛顿方法）。LeCun [76]，LeCun *等* [79]
- en: advocate the use of a second-order diagonal Newton (always positive) approximation,
    with one learning rate per parameter (associated with the approximated inverse
    second derivative of the loss with respect to the parameter). Hinton [59] proposes
    scaling learning rates so that the average weight update is on the order of 1/1000th
    of the weight magnitude. LeCun *et al.* [79] also propose a simple power method
    in order to estimate the largest eigenvalue of the Hessian (which would be the
    optimal learning rate). An interesting alternative to variants of Newton's method
    are variants of the *natural gradient* method [1], but like the basic Newton method
    it is computationally too expensive, requiring operations on a too large square
    matrix (number of parameters by number of parameters). Diagonal and low-rank online
    approximations of natural gradient [73, 74]
  id: totrans-5591
  prefs: []
  type: TYPE_NORMAL
  zh: 倡导使用二阶对角牛顿（始终为正）近似，每个参数使用一个学习率（与相对于参数的损失的近似逆二阶导数相关）。Hinton [59] 提出了调整学习率的方案，以便平均权重更新约为权重大小的1/1000。LeCun
    *等* [79] 还提出了一种简单的幂法，以估计Hessian矩阵的最大特征值（这将是最佳学习率）。牛顿方法的变种有一个有趣的替代方案，即*自然梯度*方法
    [1]，但与基本牛顿方法一样，它的计算成本过高，需要对过大的方阵（参数数目乘以参数数目）进行操作。自然梯度的对角和低秩在线近似 [73, 74]
- en: have been proposed and shown to speed-up training in some contexts. Several
    adaptive learning rate procedures have been proposed recently and merit more attention
    and evaluations in the neural network context, such as *adagrad* [41] and the
    adaptive learning rate method from Schaul *et al.* [106] which claims to remove
    completely the need for a learning rate hyper-parameter.
  id: totrans-5592
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出并显示在某些情况下可以加速训练。最近提出了几种自适应学习率程序，在神经网络的背景下值得更多关注和评估，如*adagrad* [41]和Schaul
    *et al.* [106]提出的自适应学习率方法，声称完全消除了对学习率超参数的需求。
- en: Whereas stochastic gradient descent converges very quickly initially it is generally
    slower than second-order methods for the final convergence, and this may be important
    in some applications. As a consequence, batch training algorithms (performing
    only one update after seeing the whole training set) such as the Conjugate Gradient
    method (a second order method) have dominated stochastic gradient descent for
    not too large datasets (e.g. less than thousands or tens of thousands of examples).
    Furthermore, it has recently been proposed and successfully applied to use second-order
    methods over *large mini-batches* [72, 83].
  id: totrans-5593
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随机梯度下降最初收敛非常快，但在最终收敛时通常比二阶方法慢，这在某些应用中可能很重要。因此，批量训练算法（在看到整个训练集后仅进行一次更新），如共轭梯度法（二阶方法），在不太大的数据集（例如，少于几千或几万个样本）中主导了随机梯度下降。此外，最近提出并成功应用于使用二阶方法处理*大小批量*
    [72, 83]。
- en: The idea is to do just a few iterations of the second-order methods on each
    minibatch and then move on to the next mini-batch, starting from the best previous
    point found. A useful twist is to start training with one or more epoch of SGD,
  id: totrans-5594
  prefs: []
  type: TYPE_NORMAL
  zh: 其想法是在每个小批量上仅进行少量的二阶方法迭代，然后转到下一个小批量，从找到的最佳前一点开始。一个有用的变化是用一个或多个SGD的周期开始训练。
- en: since SGD remains the fastest optimizer early on in training.
  id: totrans-5595
  prefs: []
  type: TYPE_NORMAL
  zh: 因为SGD在训练早期仍然是最快的优化器。
- en: At this point in time however, although the second-order and natural gradient
    methods are appealing conceptually, have demonstrably helped in the studied cases
    and may in the end prove to be very important, they have not yet become a standard
    for neural networks optimization and need to be validated and maybe improved by
    other researchers, before displacing simple (mini-batch) stochastic gradient descent
    variants.
  id: totrans-5596
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前尽管二阶和自然梯度方法在概念上令人信服，并在研究案例中有明显的帮助，并且最终可能被证明非常重要，但它们尚未成为神经网络优化的标准，仍需其他研究人员进行验证和改进，才能取代简单的（小批量）随机梯度下降变体。
- en: 19.7 Conclusion
  id: totrans-5597
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.7 结论
- en: In spite of decades of experimental and theoretical work on artificial neural
    networks, and with all the impressive progress made since the first edition of
    this book, in particular in the area of Deep Learning, there is still much to
    be done to better train neural networks and better understand the underlying issues
    that can make the training task difficult. As stated in the introduction, the
    wisdom distilled here should be taken as a guideline, to be tried and challenged,
    not as a practice set in stone. The practice summarized here, coupled with the
    increase in available computing power, now allows researchers to train neural
    networks on a scale that is far beyond what was possible at the time of the first
    edition of this book, helping to move us closer to artificial intelligence. Acknowledgements.
    The author is grateful for the comments and feedback provided by Nicolas Le Roux,
    Ian Goodfellow, James Bergstra, Guillaume Desjardins, Razvan Pascanu, David Warde-Farley,
    Eric Larsen, Frederic Bastien, and Sina Honari, as well as for the financial support
    of NSERC, FQRNT, CIFAR, and the Canada Research Chairs.
  id: totrans-5598
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在人工神经网络方面经过数十年的实验和理论工作，并且自本书第一版以来，尤其是在深度学习领域取得了令人印象深刻的进展，但在更好地训练神经网络和理解导致训练任务困难的潜在问题上，仍有许多工作要做。如引言中所述，这里提炼的智慧应被视为指导方针，值得尝试和挑战，而非刻在石头上的实践。这里总结的实践，加上可用计算能力的增加，现在允许研究人员以远超本书第一版时可能的规模训练神经网络，帮助我们更接近人工智能。致谢。作者感谢Nicolas
    Le Roux、Ian Goodfellow、James Bergstra、Guillaume Desjardins、Razvan Pascanu、David
    Warde-Farley、Eric Larsen、Frederic Bastien和Sina Honari提供的评论和反馈，以及NSERC、FQRNT、CIFAR和加拿大研究主席的财政支持。
- en: '[1] Amari, S.: Natural gradient works efficiently in learning. Neural Computation
    10(2), 251–276 (1998) [2] Bach, F., Moulines, E.: Non-asymptotic analysis of stochastic
    approximation algorithms. In: NIPS 2011 (2011)'
  id: totrans-5599
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Amari, S.：自然梯度在学习中高效工作。神经计算10(2)，251–276（1998） [2] Bach, F., Moulines,
    E.：随机近似算法的非渐近分析。在：NIPS 2011（2011）'
- en: '[3] Bagnell, J.A., Bradley, D.M.: Differentiable sparse coding. In: NIPS 2009,
    pp.'
  id: totrans-5600
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Bagnell, J.A., Bradley, D.M.：可微分稀疏编码。在：NIPS 2009，第'
- en: 113–120 (2009)
  id: totrans-5601
  prefs: []
  type: TYPE_NORMAL
  zh: 113–120（2009）
- en: '[4] Baxter, J.: Learning internal representations. In: COLT 1995, pp. 311–320
    (1995)'
  id: totrans-5602
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Baxter, J.：学习内部表示。在：COLT 1995，第311–320页（1995）'
- en: '[5] Baxter, J.: A Bayesian/information theoretic model of learning via multiple
    task sampling. Machine Learning 28, 7–40 (1997)'
  id: totrans-5603
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Baxter, J.：通过多任务采样的贝叶斯/信息理论学习模型。机器学习28，7–40（1997）'
- en: '[6] Bengio, Y.: Neural net language models. Scholarpedia 3(1), 3881 (2008)'
  id: totrans-5604
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Bengio, Y.：神经网络语言模型。Scholarpedia 3(1)，3881（2008）'
- en: '[7] Bengio, Y.: Learning deep architectures for AI. Now Publishers (2009)'
  id: totrans-5605
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Bengio, Y.：为人工智能学习深度架构。Now Publishers（2009）'
- en: '[8] Bengio, Y.: Deep learning of representations for unsupervised and transfer
    learning. In: JMLR W&CP: Proc. Unsupervised and Transfer Learning (2011)'
  id: totrans-5606
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Bengio, Y.：无监督和迁移学习的深度表示学习。在：JMLR W&CP：无监督和迁移学习会议论文集（2011）'
- en: '[9] Bengio, Y., Delalleau, O.: On the Expressive Power of Deep Architectures.
    In:'
  id: totrans-5607
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Bengio, Y., Delalleau, O.：关于深度架构的表达能力。在：'
- en: Kivinen, J., Szepesvári, C., Ukkonen, E., Zeugmann, T. (eds.) ALT 2011. LNCS,
  id: totrans-5608
  prefs: []
  type: TYPE_NORMAL
  zh: Kivinen, J., Szepesvári, C., Ukkonen, E., Zeugmann, T.（编辑）ALT 2011。LNCS，
- en: vol. 6925, pp. 18–36. Springer, Heidelberg (2011)
  id: totrans-5609
  prefs: []
  type: TYPE_NORMAL
  zh: 卷6925，第18–36页。施普林格，海德堡（2011）
- en: '[10] Bengio, Y., LeCun, Y.: Scaling learning algorithms towards AI. In: Large
    Scale Kernel Machines (2007)'
  id: totrans-5610
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Bengio, Y., LeCun, Y.：将学习算法扩展至人工智能。在：大规模核机器（2007）'
- en: '[11] Bengio, Y., Ducharme, R., Vincent, P., Jauvin, C.: A neural probabilistic
    language model. JMLR 3, 1137–1155 (2003)'
  id: totrans-5611
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Bengio, Y., Ducharme, R., Vincent, P., Jauvin, C.：一个神经概率语言模型。JMLR 3，1137–1155（2003）'
- en: '[12] Bengio, Y., Le Roux, N., Vincent, P., Delalleau, O., Marcotte, P.: Convex
    neural networks. In: NIPS 2005, pp. 123–130 (2006a)'
  id: totrans-5612
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Bengio, Y., Le Roux, N., Vincent, P., Delalleau, O., Marcotte, P.：凸神经网络。在：NIPS
    2005，第123–130页（2006a）'
- en: '[13] Bengio, Y., Delalleau, O., Le Roux, N.: The curse of highly variable functions
    for local kernel machines. In: NIPS 2005, pp. 107–114 (2006b)'
  id: totrans-5613
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Bengio, Y., Delalleau, O., Le Roux, N.：高变函数对局部核机器的诅咒。在：NIPS 2005，第107–114页（2006b）'
- en: '[14] Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H.: Greedy layer-wise
    training of deep networks. In: NIPS 2006 (2007)'
  id: totrans-5614
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H.：贪婪层次训练深度网络。在：NIPS
    2006（2007）'
- en: '[15] Bengio, Y., Louradour, J., Collobert, R., Weston, J.: Curriculum learning.
    In:'
  id: totrans-5615
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Bengio, Y., Louradour, J., Collobert, R., Weston, J.：课程学习。在：'
- en: ICML 2009 (2009)
  id: totrans-5616
  prefs: []
  type: TYPE_NORMAL
  zh: ICML 2009（2009）
- en: '[16] Bengio, Y., Alain, G., Rifai, S.: Implicit density estimation by local
    moment matching to sample from auto-encoders. Technical report, arXiv:1207.0057
    (2012)'
  id: totrans-5617
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Bengio, Y., Alain, G., Rifai, S.：通过局部矩匹配进行隐式密度估计以从自编码器采样。技术报告，arXiv:1207.0057（2012）'
- en: '[17] Bergstra, J., Bengio, Y.: Random search for hyper-parameter optimization.
    J.'
  id: totrans-5618
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Bergstra, J., Bengio, Y.：超参数优化的随机搜索。J.'
- en: Machine Learning Res. 13, 281–305 (2012)
  id: totrans-5619
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习研究。13，281–305（2012）
- en: '[18] Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins,
    G., Turian, J., Warde-Farley, D., Bengio, Y.: Theano: a CPU and GPU math expression
    compiler. In: Proc. Python for Scientific Comp. Conf. (SciPy) (2010)'
  id: totrans-5620
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins,
    G., Turian, J., Warde-Farley, D., Bengio, Y.：Theano：一个CPU和GPU数学表达编译器。在：Proc. Python
    for Scientific Comp. Conf. (SciPy)（2010）'
- en: '[19] Bergstra, J., Bardenet, R., Bengio, Y., Kégl, B.: Algorithms for hyper-parameter
    optimization. In: NIPS 2011 (2011)'
  id: totrans-5621
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Bergstra, J., Bardenet, R., Bengio, Y., Kégl, B.：超参数优化算法。在：NIPS 2011（2011）'
- en: '[20] Berkes, P., Wiskott, L.: Applying Slow Feature Analysis to Image Sequences
    Yields a Rich Repertoire of Complex Cell Properties. In: Dorronsoro, J.R. (ed.)'
  id: totrans-5622
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Berkes, P., Wiskott, L.：将慢特征分析应用于图像序列产生丰富的复杂细胞特性。在：Dorronsoro, J.R.（编辑）'
- en: ICANN 2002. LNCS, vol. 2415, pp. 81–86. Springer, Heidelberg (2002)
  id: totrans-5623
  prefs: []
  type: TYPE_NORMAL
  zh: ICANN 2002。LNCS，卷2415，第81–86页。施普林格，海德堡（2002）
- en: '[21] Bertsekas, D.P.: Incremental gradient, subgradient, and proximal methods
    for convex optimization: a survey. Technical Report 2848, LIDS (2010)'
  id: totrans-5624
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Bertsekas, D.P.：用于凸优化的增量梯度、次梯度和近端方法：一项综述。技术报告2848，LIDS（2010）'
- en: '[22] Bordes, A., Bottou, L., Gallinari, P.: Sgd-qn: Careful quasi-newton stochastic
    gradient descent. Journal of Machine Learning Research 10, 1737–1754 (2009)'
  id: totrans-5625
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Bordes, A., Bottou, L., Gallinari, P.：Sgd-qn：仔细的拟牛顿随机梯度下降。机器学习研究杂志10，1737–1754（2009）'
- en: '[23] Bordes, A., Weston, J., Collobert, R., Bengio, Y. (2011). Learning structured
    embeddings of knowledge bases. In: AAAI (2011)'
  id: totrans-5626
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Bordes, A., Weston, J., Collobert, R., Bengio, Y. (2011)。学习知识库的结构嵌入。发表于：AAAI
    (2011)'
- en: '[24] Bordes, A., Glorot, X., Weston, J., Bengio, Y.: Joint learning of words
    and meaning representations for open-text semantic parsing. In: AISTATS 2012 (2012)'
  id: totrans-5627
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Bordes, A., Glorot, X., Weston, J., Bengio, Y.：联合学习单词和语义表示以进行开放文本语义解析。发表于：AISTATS
    2012 (2012)'
- en: '[25] Bottou, L.: From machine learning to machine reasoning. Technical report,
    arXiv.1102 (2011)'
  id: totrans-5628
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Bottou, L.：从机器学习到机器推理。技术报告，arXiv.1102 (2011)'
- en: '[26] Bottou, L.: Stochastic Gradient Descent Tricks. In: Montavon, G., Orr,
    G.B.,'
  id: totrans-5629
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Bottou, L.：随机梯度下降技巧。发表于：Montavon, G., Orr, G.B.,'
- en: 'Müller, K.-R. (eds.) NN: Tricks of the Trade, 2nd edn. LNCS, vol. 7700, pp.
    421–436. Springer, Heidelberg (2012)'
  id: totrans-5630
  prefs: []
  type: TYPE_NORMAL
  zh: Müller, K.-R. (eds.) NN：交易技巧，第2版。LNCS，第7700卷，pp. 421–436。斯普林格，海德堡 (2012)
- en: '[27] Bottou, L., Bousquet, O.: The tradeoffs of large scale learning. In: NIPS
    2008'
  id: totrans-5631
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Bottou, L., Bousquet, O.：大规模学习的权衡。发表于：NIPS 2008'
- en: (2008)
  id: totrans-5632
  prefs: []
  type: TYPE_NORMAL
  zh: (2008)
- en: '[28] Bottou, L., LeCun, Y.: Large-scale on-line learning. In: NIPS 2003 (2004)'
  id: totrans-5633
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] Bottou, L., LeCun, Y.：大规模在线学习。发表于：NIPS 2003 (2004)'
- en: '[29] Breiman, L.: Bagging predictors. Machine Learning 24(2), 123–140 (1994)'
  id: totrans-5634
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] Breiman, L.：自助法预测器。机器学习 24(2), 123–140 (1994)'
- en: '[30] Breuleux, O., Bengio, Y., Vincent, P.: Quickly generating representative
    samples from an rbm-derived process. Neural Computation 23(8), 2053–2073 (2011)'
  id: totrans-5635
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] Breuleux, O., Bengio, Y., Vincent, P.：快速生成代表性样本来自RBM派生过程。神经计算 23(8), 2053–2073
    (2011)'
- en: '[31] Caruana, R.: Multitask connectionist learning. In: Proceedings of the
    1993 Connectionist Models Summer School, pp. 372–379 (1993)'
  id: totrans-5636
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Caruana, R.：多任务连接主义学习。发表于：1993年连接主义模型夏季学校会议，pp. 372–379 (1993)'
- en: '[32] Cho, K., Raiko, T., Ilin, A.: Enhanced gradient and adaptive learning
    rate for training restricted boltzmann machines. In: ICML 2011, pp. 105–112 (2011)'
  id: totrans-5637
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] Cho, K., Raiko, T., Ilin, A.：用于训练限制玻尔兹曼机的增强梯度和自适应学习率。发表于：ICML 2011，pp.
    105–112 (2011)'
- en: '[33] Coates, A., Ng, A.Y.: The importance of encoding versus training with
    sparse coding and vector quantization. In: ICML 2011 (2011)'
  id: totrans-5638
  prefs: []
  type: TYPE_NORMAL
  zh: '[33] Coates, A., Ng, A.Y.：编码与稀疏编码和向量量化训练的重要性。发表于：ICML 2011 (2011)'
- en: '[34] Collobert, R., Bengio, S.: Links between perceptrons, MLPs and SVMs. In:
    ICML'
  id: totrans-5639
  prefs: []
  type: TYPE_NORMAL
  zh: '[34] Collobert, R., Bengio, S.：感知器、MLP和SVM之间的联系。发表于：ICML'
- en: 2004 (2004a)
  id: totrans-5640
  prefs: []
  type: TYPE_NORMAL
  zh: 2004 (2004a)
- en: '[35] Collobert, R., Bengio, S.: Links between perceptrons, MLPs and SVMs. In:
    International Conference on Machine Learning, ICML (2004b)'
  id: totrans-5641
  prefs: []
  type: TYPE_NORMAL
  zh: '[35] Collobert, R., Bengio, S.：感知器、MLP和SVM之间的联系。发表于：国际机器学习会议，ICML (2004b)'
- en: '[36] Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa,
    P.:'
  id: totrans-5642
  prefs: []
  type: TYPE_NORMAL
  zh: '[36] Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa,
    P.：'
- en: Natural language processing (almost) from scratch. Journal of Machine Learning
    Research 12, 2493–2537 (2011a)
  id: totrans-5643
  prefs: []
  type: TYPE_NORMAL
  zh: 准备几乎完全从头开始的自然语言处理。机器学习研究杂志 12, 2493–2537 (2011a)
- en: '[37] Collobert, R., Kavukcuoglu, K., Farabet, C.: Torch7: A matlab-like environment
    for machine learning. In: BigLearn, NIPS Workshop (2011b)'
  id: totrans-5644
  prefs: []
  type: TYPE_NORMAL
  zh: '[37] Collobert, R., Kavukcuoglu, K., Farabet, C.：Torch7：一个类似于Matlab的机器学习环境。发表于：BigLearn，NIPS研讨会
    (2011b)'
- en: '[38] Courville, A., Bergstra, J., Bengio, Y.: Unsupervised models of images
    by spikeand-slab RBMs. In: ICML 2011 (2011)'
  id: totrans-5645
  prefs: []
  type: TYPE_NORMAL
  zh: '[38] Courville, A., Bergstra, J., Bengio, Y.：通过脉冲和板RBM的无监督图像模型。发表于：ICML 2011
    (2011)'
- en: '[39] Dauphin, Y., Glorot, X., Bengio, Y.: Sampled reconstruction for large-scale
    learning of embeddings. In: Proc. ICML 2011 (2011)'
  id: totrans-5646
  prefs: []
  type: TYPE_NORMAL
  zh: '[39] Dauphin, Y., Glorot, X., Bengio, Y.：用于大规模嵌入学习的采样重建。发表于：Proc. ICML 2011
    (2011)'
- en: '[40] Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K., Harshman,
    R.:'
  id: totrans-5647
  prefs: []
  type: TYPE_NORMAL
  zh: '[40] Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K., Harshman,
    R.：'
- en: Indexing by latent semantic analysis. J. Am. Soc. Information Science 41(6),
  id: totrans-5648
  prefs: []
  type: TYPE_NORMAL
  zh: 通过潜在语义分析进行索引。美国信息科学学会杂志 41(6)，
- en: 391–407 (1990)
  id: totrans-5649
  prefs: []
  type: TYPE_NORMAL
  zh: 391–407 (1990)
- en: '[41] Duchi, J., Hazan, E., Singer, Y.: Adaptive subgradient methods for online
    learning and stochastic optimization. Journal of Machine Learning Research (2011)'
  id: totrans-5650
  prefs: []
  type: TYPE_NORMAL
  zh: '[41] Duchi, J., Hazan, E., Singer, Y.：用于在线学习和随机优化的自适应子梯度方法。机器学习研究杂志 (2011)'
- en: '[42] Elman, J.L.: Learning and development in neural networks: The importance
    of starting small. Cognition 48, 781–799 (1993)'
  id: totrans-5651
  prefs: []
  type: TYPE_NORMAL
  zh: '[42] Elman, J.L.：神经网络中的学习与发展：从小开始的重要性。认知 48, 781–799 (1993)'
- en: '[43] Erhan, D., Courville, A., Bengio, Y.: Understanding representations learned
    in deep architectures. Technical Report 1355, Université de Montréal/DIRO (2010a)'
  id: totrans-5652
  prefs: []
  type: TYPE_NORMAL
  zh: '[43] Erhan, D., Courville, A., Bengio, Y.：理解深度架构中学习到的表示。技术报告 1355，蒙特利尔大学/DIRO
    (2010a)'
- en: '[44] Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P., Bengio,
    S.:'
  id: totrans-5653
  prefs: []
  type: TYPE_NORMAL
  zh: '[44] Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P., Bengio,
    S.：'
- en: Why does unsupervised pre-training help deep learning? J. Machine Learning Res.
    11, 625–660 (2010b)
  id: totrans-5654
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么无监督预训练有助于深度学习？《机器学习研究》11，页625–660（2010b）
- en: '[45] Frasconi, P., Gori, M., Sperduti, A.: A general framework for adaptive
    processing of data structures. IEEE Transactions on Neural Networks 9(5), 768–786
    (1998)'
  id: totrans-5655
  prefs: []
  type: TYPE_NORMAL
  zh: '[45] Frasconi, P., Gori, M., Sperduti, A.: 自适应处理数据结构的一般框架。《IEEE神经网络学报》9(5)，页768–786（1998）'
- en: '[46] Geman, S., Bienenstock, E., Doursat, R.: Neural networks and the bias/variance
    dilemma. Neural Computation 4(1), 1–58 (1992)'
  id: totrans-5656
  prefs: []
  type: TYPE_NORMAL
  zh: '[46] Geman, S., Bienenstock, E., Doursat, R.: 神经网络与偏差/方差困境。《神经计算》4(1)，页1–58（1992）'
- en: '[47] Getoor, L., Taskar, B.: Introduction to Statistical Relational Learning.
    MIT Press'
  id: totrans-5657
  prefs: []
  type: TYPE_NORMAL
  zh: '[47] Getoor, L., Taskar, B.: 统计关系学习导论。麻省理工学院出版社'
- en: (2006)
  id: totrans-5658
  prefs: []
  type: TYPE_NORMAL
  zh: （2006）
- en: '[48] Glorot, X., Bengio, Y.: Understanding the difficulty of training deep
    feedforward neural networks. In: AISTATS 2010, pp. 249–256 (2010)'
  id: totrans-5659
  prefs: []
  type: TYPE_NORMAL
  zh: '[48] Glorot, X., Bengio, Y.: 理解训练深度前馈神经网络的难度。见：AISTATS 2010，页249–256（2010）'
- en: '[49] Glorot, X., Bordes, A., Bengio, Y. (2011a). Deep sparse rectifier neural
    networks.'
  id: totrans-5660
  prefs: []
  type: TYPE_NORMAL
  zh: '[49] Glorot, X., Bordes, A., Bengio, Y. (2011a). 深度稀疏整流神经网络。'
- en: 'In: AISTATS 2011 (2011)'
  id: totrans-5661
  prefs: []
  type: TYPE_NORMAL
  zh: 见：AISTATS 2011（2011）
- en: '[50] Glorot, X., Bordes, A., Bengio, Y.: Domain adaptation for large-scale
    sentiment classification: A deep learning approach. In: ICML 2011 (2011b)'
  id: totrans-5662
  prefs: []
  type: TYPE_NORMAL
  zh: '[50] Glorot, X., Bordes, A., Bengio, Y.: 大规模情感分类的领域适应：一种深度学习方法。在：ICML 2011（2011b）'
- en: '[51] Goodfellow, I., Le, Q., Saxe, A., Ng, A.: Measuring invariances in deep
    networks.'
  id: totrans-5663
  prefs: []
  type: TYPE_NORMAL
  zh: '[51] Goodfellow, I., Le, Q., Saxe, A., Ng, A.: 测量深度网络中的不变性。'
- en: 'In: NIPS 2009, pp. 646–654 (2009)'
  id: totrans-5664
  prefs: []
  type: TYPE_NORMAL
  zh: 见：NIPS 2009，页646–654（2009）
- en: '[52] Goodfellow, I., Courville, A., Bengio, Y.: Spike-and-slab sparse coding
    for unsupervised feature discovery. In: NIPS Workshop on Challenges in Learning
    Hierarchical Models (2011)'
  id: totrans-5665
  prefs: []
  type: TYPE_NORMAL
  zh: '[52] Goodfellow, I., Courville, A., Bengio, Y.: 用于无监督特征发现的尖峰-板条稀疏编码。见：NIPS关于学习层次模型的挑战研讨会（2011）'
- en: '[53] Graepel, T., Candela, J.Q., Borchert, T., Herbrich, R.: Web-scale Bayesian
    clickthrough rate prediction for sponsored search advertising in microsoft''s
    bing search engine. In: ICML (2010)'
  id: totrans-5666
  prefs: []
  type: TYPE_NORMAL
  zh: '[53] Graepel, T., Candela, J.Q., Borchert, T., Herbrich, R.: 面向微软必应搜索引擎的网络规模贝叶斯点击率预测。在：ICML（2010）'
- en: '[54] Håstad, J.: Almost optimal lower bounds for small depth circuits. In:
    STOC 1986, pp. 6–20 (1986)'
  id: totrans-5667
  prefs: []
  type: TYPE_NORMAL
  zh: '[54] Håstad, J.: 小深度电路的几乎最优下界。在：STOC 1986，页6–20（1986）'
- en: '[55] Håstad, J., Goldmann, M.: On the power of small-depth threshold circuits.
    Computational Complexity 1, 113–129 (1991)'
  id: totrans-5668
  prefs: []
  type: TYPE_NORMAL
  zh: '[55] Håstad, J., Goldmann, M.: 小深度阈值电路的能力。《计算复杂性》1，页113–129（1991）'
- en: '[56] Hinton, G.E.: Relaxation and its role in vision. Ph.D. thesis, University
    of Edinburgh (1978)'
  id: totrans-5669
  prefs: []
  type: TYPE_NORMAL
  zh: '[56] Hinton, G.E.: 松弛及其在视觉中的作用。博士论文，爱丁堡大学（1978）'
- en: '[57] Hinton, G.E.: Learning distributed representations of concepts. In: Proc.
    8th Annual Conf. Cog. Sc. Society, pp. 1–12 (1986)'
  id: totrans-5670
  prefs: []
  type: TYPE_NORMAL
  zh: '[57] Hinton, G.E.: 学习概念的分布式表示。见：第8届认知科学协会年会论文集，页1–12（1986）'
- en: '[58] Hinton, G.E.: Connectionist learning procedures. Artificial Intelligence
    40, 185–'
  id: totrans-5671
  prefs: []
  type: TYPE_NORMAL
  zh: '[58] Hinton, G.E.: 连接主义学习程序。《人工智能》40，185–'
- en: 234 (1989)
  id: totrans-5672
  prefs: []
  type: TYPE_NORMAL
  zh: 234（1989）
- en: '[59] Hinton, G.E.: A practical guide to training restricted Boltzmann machines.
    Technical Report UTML TR 2010-003, Department of Computer Science, University
    of Toronto (2010)'
  id: totrans-5673
  prefs: []
  type: TYPE_NORMAL
  zh: '[59] Hinton, G.E.: 限制玻尔兹曼机训练的实用指南。技术报告 UTML TR 2010-003，多伦多大学计算机科学系（2010）'
- en: '[60] Hinton, G.E.: A Practical Guide to Training Restricted Boltzmann Machines.
    In:'
  id: totrans-5674
  prefs: []
  type: TYPE_NORMAL
  zh: '[60] Hinton, G.E.: 限制玻尔兹曼机训练的实用指南。见：'
- en: 'Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the Trade, 2nd
    edn.'
  id: totrans-5675
  prefs: []
  type: TYPE_NORMAL
  zh: Montavon, G., Orr, G.B., Müller, K.-R. (编辑)《神经网络：行业技巧》，第二版。
- en: LNCS, vol. 7700, pp. 599–619. Springer, Heidelberg (2012)
  id: totrans-5676
  prefs: []
  type: TYPE_NORMAL
  zh: LNCS，第7700卷，页599–619。施普林格，海德堡（2012）
- en: '[61] Hinton, G.E., Osindero, S., Teh, Y.-W.: A fast learning algorithm for
    deep belief nets. Neural Computation 18, 1527–1554 (2006)'
  id: totrans-5677
  prefs: []
  type: TYPE_NORMAL
  zh: '[61] Hinton, G.E., Osindero, S., Teh, Y.-W.: 深度信念网络的快速学习算法。《神经计算》18，页1527–1554（2006）'
- en: '[62] Hutter, F.: Automated Configuration of Algorithms for Solving Hard Computational
    Problems. Ph.D. thesis, University of British Columbia (2009)'
  id: totrans-5678
  prefs: []
  type: TYPE_NORMAL
  zh: '[62] Hutter, F.: 针对难解计算问题的算法自动配置。博士论文，不列颠哥伦比亚大学（2009）'
- en: '[63] Hutter, F., Hoos, H.H., Leyton-Brown, K.: Sequential model-based optimization
    for general algorithm configuration. In: Coello Coello, C.A. (ed.) LION 5. LNCS,'
  id: totrans-5679
  prefs: []
  type: TYPE_NORMAL
  zh: '[63] Hutter, F., Hoos, H.H., Leyton-Brown, K.: 用于一般算法配置的基于模型的序列优化。见：Coello
    Coello, C.A.（编）《LION 5》。LNCS，'
- en: vol. 6683, pp. 507–523. Springer, Heidelberg (2011)
  id: totrans-5680
  prefs: []
  type: TYPE_NORMAL
  zh: 第6683卷，页507–523。施普林格，海德堡（2011）
- en: '[64] Jarrett, K., Kavukcuoglu, K., Ranzato, M., LeCun, Y.: What is the best
    multistage architecture for object recognition? In: ICCV (2009)'
  id: totrans-5681
  prefs: []
  type: TYPE_NORMAL
  zh: '[64] Jarrett, K., Kavukcuoglu, K., Ranzato, M., LeCun, Y.: 什么是目标识别的最佳多阶段架构？见：ICCV（2009）'
- en: '[65] Kavukcuoglu, K., Ranzato, M.-A., Fergus, R., LeCun, Y.: Learning invariant
    features through topographic filter maps. In: CVPR 2009 (2009)'
  id: totrans-5682
  prefs: []
  type: TYPE_NORMAL
  zh: '[65] Kavukcuoglu, K., Ranzato, M.-A., Fergus, R., LeCun, Y.: 通过地形滤波器图学习不变特征.
    在：CVPR 2009 (2009)'
- en: '[66] Krueger, K.A., Dayan, P.: Flexible shaping: how learning in small steps
    helps.'
  id: totrans-5683
  prefs: []
  type: TYPE_NORMAL
  zh: '[66] Krueger, K.A., Dayan, P.: 灵活塑造：如何通过小步学习帮助.'
- en: Cognition 110, 380–394 (2009)
  id: totrans-5684
  prefs: []
  type: TYPE_NORMAL
  zh: Cognition 110, 380–394 (2009)
- en: '[67] Lamblin, P., Bengio, Y.: Important gains from supervised fine-tuning of
    deep architectures on large labeled sets. In: NIPS 2010 Deep Learning and Unsupervised
    Feature Learning Workshop (2010)'
  id: totrans-5685
  prefs: []
  type: TYPE_NORMAL
  zh: '[67] Lamblin, P., Bengio, Y.: 通过监督微调深度架构在大型标记集上的重要收益. 在：NIPS 2010 深度学习与无监督特征学习研讨会
    (2010)'
- en: '[68] Lang, K.J., Hinton, G.E.: The development of the time-delay neural network
    architecture for speech recognition. Technical Report CMU-CS-88-152, CarnegieMellon
    University (1988)'
  id: totrans-5686
  prefs: []
  type: TYPE_NORMAL
  zh: '[68] Lang, K.J., Hinton, G.E.: 时延神经网络架构的发展用于语音识别. 技术报告 CMU-CS-88-152, 卡内基梅隆大学
    (1988)'
- en: '[69] Larochelle, H., Bengio, Y.: Classification using discriminative restricted
    Boltzmann machines. In: ICML 2008 (2008)'
  id: totrans-5687
  prefs: []
  type: TYPE_NORMAL
  zh: '[69] Larochelle, H., Bengio, Y.: 使用判别限制玻尔兹曼机进行分类. 在：ICML 2008 (2008)'
- en: '[70] Larochelle, H., Bengio, Y., Louradour, J., Lamblin, P.: Exploring strategies
    for training deep neural networks. J. Machine Learning Res. 10, 1–40 (2009)'
  id: totrans-5688
  prefs: []
  type: TYPE_NORMAL
  zh: '[70] Larochelle, H., Bengio, Y., Louradour, J., Lamblin, P.: 探索训练深度神经网络的策略.
    J. Machine Learning Res. 10, 1–40 (2009)'
- en: '[71] Le, Q., Ngiam, J., Chen, Z., Hao Chia, D.J., Koh, P.W., Ng, A.: Tiled
    convolutional neural networks. In: NIPS 2010 (2010)'
  id: totrans-5689
  prefs: []
  type: TYPE_NORMAL
  zh: '[71] Le, Q., Ngiam, J., Chen, Z., Hao Chia, D.J., Koh, P.W., Ng, A.: 瓦片卷积神经网络.
    在：NIPS 2010 (2010)'
- en: '[72] Le, Q., Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., Ng, A.: On optimization
    methods for deep learning. In: ICML 2011 (2011)'
  id: totrans-5690
  prefs: []
  type: TYPE_NORMAL
  zh: '[72] Le, Q., Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., Ng, A.: 关于深度学习的优化方法.
    在：ICML 2011 (2011)'
- en: '[73] Le Roux, N., Manzagol, P.-A., Bengio, Y.: Topmoumoute online natural gradient
    algorithm. In: NIPS 2007 (2008)'
  id: totrans-5691
  prefs: []
  type: TYPE_NORMAL
  zh: '[73] Le Roux, N., Manzagol, P.-A., Bengio, Y.: Topmoumoute 在线自然梯度算法. 在：NIPS
    2007 (2008)'
- en: '[74] Le Roux, N., Bengio, Y., Fitzgibbon, A.: Improving first and second-order
    methods by modeling uncertainty. In: Optimization for Machine Learning. MIT Press'
  id: totrans-5692
  prefs: []
  type: TYPE_NORMAL
  zh: '[74] Le Roux, N., Bengio, Y., Fitzgibbon, A.: 通过建模不确定性改善一阶和二阶方法. 在：机器学习优化.
    MIT出版社'
- en: (2011)
  id: totrans-5693
  prefs: []
  type: TYPE_NORMAL
  zh: (2011)
- en: '[75] Le Roux, N., Schmidt, M., Bach, F.: A stochastic gradient method with
    an exponential convergence rate for strongly-convex optimization with finite training
    sets. Technical report, arXiv:1202.6258 (2012)'
  id: totrans-5694
  prefs: []
  type: TYPE_NORMAL
  zh: '[75] Le Roux, N., Schmidt, M., Bach, F.: 一种具有指数收敛率的随机梯度方法，用于有限训练集的强凸优化. 技术报告,
    arXiv:1202.6258 (2012)'
- en: '[76] LeCun, Y.: Modèles connexionistes de l''apprentissage. Ph.D. thesis, Université
    de Paris VI (1987)'
  id: totrans-5695
  prefs: []
  type: TYPE_NORMAL
  zh: '[76] LeCun, Y.: 连接主义学习模型. 博士论文, 巴黎六大学 (1987)'
- en: '[77] LeCun, Y.: Generalization and network design strategies. Technical Report
    CRGTR-89-4, University of Toronto (1989)'
  id: totrans-5696
  prefs: []
  type: TYPE_NORMAL
  zh: '[77] LeCun, Y.: 泛化和网络设计策略. 技术报告 CRGTR-89-4, 多伦多大学 (1989)'
- en: '[78] LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard,
    W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition.'
  id: totrans-5697
  prefs: []
  type: TYPE_NORMAL
  zh: '[78] LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard,
    W., Jackel, L.D.: 反向传播应用于手写邮政编码识别.'
- en: Neural Computation 1(4), 541–551 (1989)
  id: totrans-5698
  prefs: []
  type: TYPE_NORMAL
  zh: 神经计算 1(4), 541–551 (1989)
- en: '[79] LeCun, Y.A., Bottou, L., Orr, G.B., Müller, K.-R.: Efficient BackProp.
    In: Orr, G.B., Müller, K.-R. (eds.) NIPS-WS 1996. LNCS, vol. 1524, pp. 9–50. Springer,
    Heidelberg (1998a)'
  id: totrans-5699
  prefs: []
  type: TYPE_NORMAL
  zh: '[79] LeCun, Y.A., Bottou, L., Orr, G.B., Müller, K.-R.: 高效反向传播. 在：Orr, G.B.,
    Müller, K.-R. (编辑) NIPS-WS 1996. LNCS, vol. 1524, pp. 9–50. Springer, Heidelberg
    (1998a)'
- en: '[80] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient based learning
    applied to document recognition. IEEE 86(11), 2278–2324 (1998b)'
  id: totrans-5700
  prefs: []
  type: TYPE_NORMAL
  zh: '[80] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: 基于梯度的学习应用于文档识别. IEEE 86(11),
    2278–2324 (1998b)'
- en: '[81] Lee, H., Ekanadham, C., Ng, A. (2008). Sparse deep belief net model for
    visual area V2. In: NIPS 2007 (2007)'
  id: totrans-5701
  prefs: []
  type: TYPE_NORMAL
  zh: '[81] Lee, H., Ekanadham, C., Ng, A. (2008). 稀疏深度信念网络模型用于视觉区域 V2. 在：NIPS 2007
    (2007)'
- en: '[82] Lee, H., Grosse, R., Ranganath, R., Ng, A.Y.: Convolutional deep belief
    networks for scalable unsupervised learning of hierarchical representations. In:
    ICML 2009'
  id: totrans-5702
  prefs: []
  type: TYPE_NORMAL
  zh: '[82] Lee, H., Grosse, R., Ranganath, R., Ng, A.Y.: 卷积深度信念网络用于可扩展的无监督学习层次表示.
    在：ICML 2009'
- en: (2009)
  id: totrans-5703
  prefs: []
  type: TYPE_NORMAL
  zh: (2009)
- en: '[83] Martens, J.: Deep learning via Hessian-free optimization. In: ICML 2010,
    pp.'
  id: totrans-5704
  prefs: []
  type: TYPE_NORMAL
  zh: '[83] Martens, J.: 通过无海森优化进行深度学习. 在：ICML 2010, pp.'
- en: 735–742 (2010)
  id: totrans-5705
  prefs: []
  type: TYPE_NORMAL
  zh: 735–742 (2010)
- en: '[84] Mesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow,
    I., Lavoie, E., Muller, X., Desjardins, G., Warde-Farley, D., Vincent, P., Courville,
    A.,'
  id: totrans-5706
  prefs: []
  type: TYPE_NORMAL
  zh: '[84] Mesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow,
    I., Lavoie, E., Muller, X., Desjardins, G., Warde-Farley, D., Vincent, P., Courville,
    A.,'
- en: 'Bergstra, J.: Unsupervised and transfer learning challenge: a deep learning
    approach. In: Proc. Unsupervised and Transfer Learning, JMLR W&CP, vol. 7'
  id: totrans-5707
  prefs: []
  type: TYPE_NORMAL
  zh: 'Bergstra, J.: 无监督与迁移学习挑战：一种深度学习方法。在：无监督与迁移学习会议论文集，JMLR W&CP，第7卷'
- en: (2011)
  id: totrans-5708
  prefs: []
  type: TYPE_NORMAL
  zh: (2011)
- en: '[85] Montavon, G., Braun, M.L., Müller, K.-R.: Deep Boltzmann machines as feedforward
    hierarchies. In: AISTATS 2012 (2012) [86] Nair, V., Hinton, G.E.: Rectified linear
    units improve restricted Boltzmann machines. In: ICML 2010 (2010) [87] Nemirovski,
    A., Yudin, D.: Problem complexity and method efficiency in optimization. Wiley
    (1983)'
  id: totrans-5709
  prefs: []
  type: TYPE_NORMAL
  zh: '[85] Montavon, G., Braun, M.L., Müller, K.-R.: 深度玻尔兹曼机作为前馈层次结构。在：AISTATS 2012
    (2012) [86] Nair, V., Hinton, G.E.: 经过修正的线性单元改善限制玻尔兹曼机。在：ICML 2010 (2010) [87]
    Nemirovski, A., Yudin, D.: 优化中的问题复杂性与方法效率。Wiley (1983)'
- en: '[88] Nesterov, Y.: Primal-dual subgradient methods for convex problems. Mathematical
    Programming 120(1), 221–259 (2009)'
  id: totrans-5710
  prefs: []
  type: TYPE_NORMAL
  zh: '[88] Nesterov, Y.: 针对凸问题的原始-对偶次梯度方法。数学规划 120(1), 221–259 (2009)'
- en: '[89] Olshausen, B.A., Field, D.J.: Sparse coding with an overcomplete basis
    set: a strategy employed by V1? Vision Research 37, 3311–3325 (1997)'
  id: totrans-5711
  prefs: []
  type: TYPE_NORMAL
  zh: '[89] Olshausen, B.A., Field, D.J.: 用过完备基集进行稀疏编码：V1采用的策略？视觉研究 37, 3311–3325
    (1997)'
- en: '[90] Pearlmutter, B.: Fast exact multiplication by the Hessian. Neural Computation
    6(1), 147–160 (1994)'
  id: totrans-5712
  prefs: []
  type: TYPE_NORMAL
  zh: '[90] Pearlmutter, B.: 通过海森矩阵进行快速精确的乘法。神经计算 6(1), 147–160 (1994)'
- en: '[91] Pinto, N., Doukhan, D., DiCarlo, J.J., Cox, D.D.: A high-throughput screening
    approach to discovering good forms of biologically inspired visual representation.'
  id: totrans-5713
  prefs: []
  type: TYPE_NORMAL
  zh: '[91] Pinto, N., Doukhan, D., DiCarlo, J.J., Cox, D.D.: 一种高通量筛选方法，以发现良好的生物启发式视觉表示形式。'
- en: PLoS Comput. Biol. 5(11), e1000579 (2009)
  id: totrans-5714
  prefs: []
  type: TYPE_NORMAL
  zh: PLoS Comput. Biol. 5(11), e1000579 (2009)
- en: '[92] Pollack, J.B.: Recursive distributed representations. Artificial Intelligence
    46(1),'
  id: totrans-5715
  prefs: []
  type: TYPE_NORMAL
  zh: '[92] Pollack, J.B.: 递归分布式表示。人工智能 46(1),'
- en: 77–105 (1990)
  id: totrans-5716
  prefs: []
  type: TYPE_NORMAL
  zh: 77–105 (1990)
- en: '[93] Polyak, B., Juditsky, A.: Acceleration of stochastic approximation by
    averaging.'
  id: totrans-5717
  prefs: []
  type: TYPE_NORMAL
  zh: '[93] Polyak, B., Juditsky, A.: 通过平均加速随机近似。'
- en: SIAM J. Control and Optimization 30(4), 838–855 (1992)
  id: totrans-5718
  prefs: []
  type: TYPE_NORMAL
  zh: SIAM J. 控制与优化 30(4), 838–855 (1992)
- en: '[94] Raiko, T., Valpola, H., LeCun, Y. (2012). Deep learning made easier by
    linear transformations in perceptrons. In: AISTATS 2012 (2012)'
  id: totrans-5719
  prefs: []
  type: TYPE_NORMAL
  zh: '[94] Raiko, T., Valpola, H., LeCun, Y. (2012). 通过感知机中的线性变换简化深度学习。在：AISTATS
    2012 (2012)'
- en: '[95] Ranzato, M., Poultney, C., Chopra, S., LeCun, Y.: Efficient learning of
    sparse representations with an energy-based model. In: NIPS 2006 (2007)'
  id: totrans-5720
  prefs: []
  type: TYPE_NORMAL
  zh: '[95] Ranzato, M., Poultney, C., Chopra, S., LeCun, Y.: 使用基于能量的模型有效学习稀疏表示。在：NIPS
    2006 (2007)'
- en: '[96] Ranzato, M., Boureau, Y.-L., LeCun, Y.: Sparse feature learning for deep
    belief networks. In: Platt, J., Koller, D., Singer, Y., Roweis, S. (eds.) Advances
    in Neural Information Processing Systems (NIPS 2007), vol. 20, pp. 1185–1192.
    MIT Press, Cambridge (2008a)'
  id: totrans-5721
  prefs: []
  type: TYPE_NORMAL
  zh: '[96] Ranzato, M., Boureau, Y.-L., LeCun, Y.: 深度信念网络的稀疏特征学习。在：Platt, J., Koller,
    D., Singer, Y., Roweis, S. (编辑) 《神经信息处理系统进展》（NIPS 2007），第20卷，1185–1192页。麻省理工学院出版社，剑桥
    (2008a)'
- en: '[97] Ranzato, M., Boureau, Y., LeCun, Y.: Sparse feature learning for deep
    belief networks. In: NIPS 2007 (2008b)'
  id: totrans-5722
  prefs: []
  type: TYPE_NORMAL
  zh: '[97] Ranzato, M., Boureau, Y., LeCun, Y.: 深度信念网络的稀疏特征学习。在：NIPS 2007 (2008b)'
- en: '[98] Richardson, M., Domingos, P.: Markov logic networks. Machine Learning
    62, 107–136 (2006)'
  id: totrans-5723
  prefs: []
  type: TYPE_NORMAL
  zh: '[98] Richardson, M., Domingos, P.: 马尔可夫逻辑网络。机器学习 62, 107–136 (2006)'
- en: '[99] Rifai, S., Vincent, P., Muller, X., Glorot, X., Bengio, Y.: Contracting
    autoencoders: Explicit invariance during feature extraction. In: ICML 2011 (2011a)'
  id: totrans-5724
  prefs: []
  type: TYPE_NORMAL
  zh: '[99] Rifai, S., Vincent, P., Muller, X., Glorot, X., Bengio, Y.: 收缩自编码器：特征提取过程中的显式不变性。在：ICML
    2011 (2011a)'
- en: '[100] Rifai, S., Dauphin, Y., Vincent, P., Bengio, Y., Muller, X.: The manifold
    tangent classifier. In: NIPS 2011 (2011b)'
  id: totrans-5725
  prefs: []
  type: TYPE_NORMAL
  zh: '[100] Rifai, S., Dauphin, Y., Vincent, P., Bengio, Y., Muller, X.: 流形切线分类器。在：NIPS
    2011 (2011b)'
- en: '[101] Rifai, S., Bengio, Y., Dauphin, Y., Vincent, P.: A generative process
    for sampling contractive auto-encoders. In: ICML 2012 (2012)'
  id: totrans-5726
  prefs: []
  type: TYPE_NORMAL
  zh: '[101] Rifai, S., Bengio, Y., Dauphin, Y., Vincent, P.: 用于采样收缩自编码器的生成过程。在：ICML
    2012 (2012)'
- en: '[102] Robbins, H., Monro, S.: A stochastic approximation method. Annals of
    Mathematical Statistics 22, 400–407 (1951) [103] Rumelhart, D.E., Hinton, G.E.,
    Williams, R.J.: Learning representations by backpropagating errors. Nature 323,
    533–536 (1986)'
  id: totrans-5727
  prefs: []
  type: TYPE_NORMAL
  zh: '[102] Robbins, H., Monro, S.: 一种随机近似方法。数学统计年鉴 22, 400–407 (1951) [103] Rumelhart,
    D.E., Hinton, G.E., Williams, R.J.: 通过反向传播学习表示。自然 323, 533–536 (1986)'
- en: '[104] Salakhutdinov, R., Hinton, G.: Deep Boltzmann machines. In: AISTATS 2009'
  id: totrans-5728
  prefs: []
  type: TYPE_NORMAL
  zh: '[104] Salakhutdinov, R., Hinton, G.: 深度玻尔兹曼机。在：AISTATS 2009'
- en: (2009)
  id: totrans-5729
  prefs: []
  type: TYPE_NORMAL
  zh: (2009)
- en: '[105] Saxe, A.M., Koh, P.W., Chen, Z., Bhand, M., Suresh, B., Ng, A.: On random
    weights and unsupervised feature learning. In: ICML 2011 (2011)'
  id: totrans-5730
  prefs: []
  type: TYPE_NORMAL
  zh: '[105] Saxe, A.M., Koh, P.W., Chen, Z., Bhand, M., Suresh, B., Ng, A.: 关于随机权重和无监督特征学习。在：ICML
    2011 (2011)'
- en: '[106] Schaul, T., Zhang, S., LeCun, Y.: No More Pesky Learning Rates. Technical
    report (2012)'
  id: totrans-5731
  prefs: []
  type: TYPE_NORMAL
  zh: '[106] Schaul, T., Zhang, S., LeCun, Y.: 不再烦恼学习率。技术报告（2012）'
- en: '[107] Schraudolph, N.N.: Centering Neural Network Gradient Factors. In: Orr,
    G.B.,'
  id: totrans-5732
  prefs: []
  type: TYPE_NORMAL
  zh: '[107] Schraudolph, N.N.: 中心化神经网络梯度因子。在：Orr, G.B.，'
- en: Müller, K.-R. (eds.) NIPS-WS 1996. LNCS, vol. 1524, pp. 207–548. Springer, Heidelberg
    (1998)
  id: totrans-5733
  prefs: []
  type: TYPE_NORMAL
  zh: Müller, K.-R. (编) NIPS-WS 1996。LNCS，第1524卷，第207–548页。施普林格，海德堡（1998）
- en: '[108] Socher, R., Manning, C., Ng, A.Y.: Parsing natural scenes and natural
    language with recursive neural networks. In: ICML 2011 (2011)'
  id: totrans-5734
  prefs: []
  type: TYPE_NORMAL
  zh: '[108] Socher, R., Manning, C., Ng, A.Y.: 使用递归神经网络解析自然场景和自然语言。在：ICML 2011（2011）'
- en: '[109] Srinivasan, A., Ramakrishnan, G.: Parameter screening and optimisation
    for ILP'
  id: totrans-5735
  prefs: []
  type: TYPE_NORMAL
  zh: '[109] Srinivasan, A., Ramakrishnan, G.: ILP 的参数筛选和优化'
- en: using designed experiments. Journal of Machine Learning Research 12, 627–662
  id: totrans-5736
  prefs: []
  type: TYPE_NORMAL
  zh: 使用设计实验。机器学习研究期刊 12, 627–662
- en: (2011)
  id: totrans-5737
  prefs: []
  type: TYPE_NORMAL
  zh: (2011)
- en: '[110] Swersky, K., Chen, B., Marlin, B., de Freitas, N.: A tutorial on stochastic
    approximation algorithms for training restricted boltzmann machines and deep belief
    nets. In: Information Theory and Applications Workshop (2010)'
  id: totrans-5738
  prefs: []
  type: TYPE_NORMAL
  zh: '[110] Swersky, K., Chen, B., Marlin, B., de Freitas, N.: 受限玻尔兹曼机和深度信念网络训练的随机逼近算法教程。在：信息理论与应用研讨会（2010）'
- en: '[111] Tenenbaum, J., de Silva, V., Langford, J.C.: A global geometric framework
    for nonlinear dimensionality reduction. Science 290(5500), 2319–2323 (2000)'
  id: totrans-5739
  prefs: []
  type: TYPE_NORMAL
  zh: '[111] Tenenbaum, J., de Silva, V., Langford, J.C.: 非线性维度约简的全局几何框架。科学 290(5500),
    2319–2323（2000）'
- en: '[112] Tieleman, T., Hinton, G.: Using fast weights to improve persistent contrastive
    divergence. In: ICML 2009 (2009)'
  id: totrans-5740
  prefs: []
  type: TYPE_NORMAL
  zh: '[112] Tieleman, T., Hinton, G.: 使用快速权重改善持续对比散度。在：ICML 2009（2009）'
- en: '[113] van der Maaten, L., Hinton, G.E.: Visualizing data using t-sne. J. Machine
    Learning Res. 9 (2008)'
  id: totrans-5741
  prefs: []
  type: TYPE_NORMAL
  zh: '[113] van der Maaten, L., Hinton, G.E.: 使用 t-SNE 可视化数据。机器学习研究期刊 9（2008）'
- en: '[114] Vincent, P.: A connection between score matching and denoising autoencoders.'
  id: totrans-5742
  prefs: []
  type: TYPE_NORMAL
  zh: '[114] Vincent, P.: 得分匹配与去噪自编码器之间的联系。'
- en: Neural Computation 23(7) (2011)
  id: totrans-5743
  prefs: []
  type: TYPE_NORMAL
  zh: 神经计算 23(7)（2011）
- en: '[115] Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.-A.: Extracting
    and composing robust features with denoising autoencoders. In: ICML 2008 (2008)'
  id: totrans-5744
  prefs: []
  type: TYPE_NORMAL
  zh: '[115] Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.-A.: 使用去噪自编码器提取和组合鲁棒特征。在：ICML
    2008（2008）'
- en: '[116] Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.-A.:
    Stacked denoising autoencoders: Learning useful representations in a deep network
    with a local denoising criterion. J. Machine Learning Res. 11 (2010)'
  id: totrans-5745
  prefs: []
  type: TYPE_NORMAL
  zh: '[116] Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.-A.:
    堆叠去噪自编码器：在具有局部去噪标准的深度网络中学习有用表示。机器学习研究期刊 11（2010）'
- en: '[117] Weston, J., Ratle, F., Collobert, R.: Deep learning via semi-supervised
    embedding. In: ICML 2008 (2008)'
  id: totrans-5746
  prefs: []
  type: TYPE_NORMAL
  zh: '[117] Weston, J., Ratle, F., Collobert, R.: 通过半监督嵌入进行深度学习。在：ICML 2008（2008）'
- en: '[118] Weston, J., Bengio, S., Usunier, N.: Wsabie: Scaling up to large vocabulary
    image annotation. In: Proceedings of the International Joint Conference on Artificial
    Intelligence, IJCAI (2011)'
  id: totrans-5747
  prefs: []
  type: TYPE_NORMAL
  zh: '[118] Weston, J., Bengio, S., Usunier, N.: Wsabie：扩展到大型词汇图像注释。在：国际人工智能联合会议论文集，IJCAI（2011）'
- en: '[119] Wiskott, L., Sejnowski, T.J.: Slow feature analysis: Unsupervised learning
    of invariances. Neural Computation 14(4), 715–770 (2002)'
  id: totrans-5748
  prefs: []
  type: TYPE_NORMAL
  zh: '[119] Wiskott, L., Sejnowski, T.J.: 慢特征分析：无监督学习不变性。神经计算 14(4), 715–770（2002）'
- en: '[120] Zou, W.Y., Ng, A.Y., Yu, K.: Unsupervised learning of visual invariance
    with temporal coherence. In: NIPS 2011 Workshop on Deep Learning and Unsupervised
    Feature Learning (2011)'
  id: totrans-5749
  prefs: []
  type: TYPE_NORMAL
  zh: '[120] Zou, W.Y., Ng, A.Y., Yu, K.: 具有时间一致性的视觉不变性无监督学习。在：NIPS 2011 深度学习与无监督特征学习研讨会（2011）'
- en: 20 Training Deep And Recurrent Networks With Hessian-Free Optimization
  id: totrans-5750
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20 使用无海森优化训练深度和递归网络
- en: James Martens and Ilya Sutskever University of Toronto, Department of Computer
    Science
  id: totrans-5751
  prefs: []
  type: TYPE_NORMAL
  zh: James Martens 和 Ilya Sutskever 多伦多大学计算机科学系
- en: '{jmartens,ilya}@cs.utoronto.ca Abstract. In this chapter we will first describe
    the basic HF approach, and then examine well-known performance-improving techniques
    such as preconditioning which we have found to be beneficial for neural network
    training, as well as others of a more heuristic nature which are harder to justify,
    but which we have found to work well in practice. We will also provide practical
    tips for creating efficient and bug-free implementations and discuss various pitfalls
    which may arise when designing and using an HF-type approach in a particular application.'
  id: totrans-5752
  prefs: []
  type: TYPE_NORMAL
  zh: '{jmartens,ilya}@cs.utoronto.ca 摘要。本章我们将首先描述基本的HF方法，然后考察一些著名的性能提升技术，例如我们发现对神经网络训练有益的预处理技术，以及其他一些更具启发性的技术，虽然这些技术更难以证明，但我们发现它们在实践中效果良好。我们还将提供创建高效且无错误实现的实用技巧，并讨论在特定应用中设计和使用HF类型方法时可能出现的各种陷阱。'
- en: 20.1 Introduction
  id: totrans-5753
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.1 引言
- en: Hessian-Free optimization (HF) is an approach for unconstrained minimization
    of real-valued smooth objective functions. Like standard Newton's method, it uses
    local quadratic approximations to generate update proposals. It belongs to the
    broad class of approximate Newton methods that are practical for problems of very
    high dimensionality, such as the training objectives of large neural networks.
    Different algorithms that use many of the same key principles have appeared in
    the literatures of various communities under different names such as Newton-CG,
    CG-Steihaug, Newton-Lanczos, and Truncated Newton [27, 28], but applications to
    machine learning and especially neural networks, have been limited or non-existent
    until recently. With the work of Martens [22] and later Martens and Sutskever
    [23] it has been demonstrated that such an approach, if carefully designed and
    implemented, can work very well for optimizing nonconvex functions such as the
    training objective for deep neural networks and recurrent neural networks (RNNs),
    given sensible random initializations. This was significant because gradient descent
    methods have been observed to be very slow and sometimes completely ineffective
    [17, 4, 18] on these problems, unless special non-random initializations schemes
    like layer-wise pre-training [17, 16, 3] are used. HF, which is a general optimization-based
    approach, can be used in conjunction with or as an alternative to existing pre-training
    methods and is more widely applicable, since it relies on fewer assumptions about
    the specific structure of the network.
  id: totrans-5754
  prefs: []
  type: TYPE_NORMAL
  zh: 无赫斯优化（HF）是一种用于无约束最小化实值平滑目标函数的方法。与标准牛顿法类似，它使用局部二次近似生成更新建议。它属于广泛的近似牛顿方法类，这些方法适用于非常高维的问题，例如大规模神经网络的训练目标。不同的算法虽然使用了许多相同的关键原则，但在不同的社区文献中以不同的名称出现，如牛顿-CG、CG-Steihaug、牛顿-Lanczos和截断牛顿[27,
    28]，但在机器学习，尤其是神经网络方面的应用直到最近一直有限或不存在。随着Martens[22]及后来的Martens和Sutskever[23]的研究，已证明如果仔细设计和实现，这种方法可以非常有效地优化非凸函数，如深度神经网络和递归神经网络（RNN）的训练目标，前提是合理的随机初始化。这一点非常重要，因为在这些问题上，观察到梯度下降方法非常缓慢，有时完全无效[17,
    4, 18]，除非使用特殊的非随机初始化方案，如层次预训练[17, 16, 3]。HF是一种通用的基于优化的方法，可以与现有的预训练方法结合使用或作为替代，因其对网络特定结构的假设更少而更具广泛适用性。
- en: In this report we will first describe the basic HF approach, and then examine
    well-known general purpose performance-improving techniques as well as others
  id: totrans-5755
  prefs: []
  type: TYPE_NORMAL
  zh: 在本报告中，我们将首先描述基本的HF方法，然后考察一些著名的通用性能提升技术以及其他技术。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    479–535, 2012.'
  id: totrans-5756
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon 等（编辑）：NN: 贸易技巧，第二版，LNCS 7700，第479–535页，2012年。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-5757
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: that are specific to HF (versus other Truncated-Newton type approaches) or to
    neural networks. We will also provide practical tips for creating efficient and
    correct implementations, and discuss the pitfalls which may arise when designing
    and using an HF-based approach in a particular application.
  id: totrans-5758
  prefs: []
  type: TYPE_NORMAL
  zh: 这些陷阱是特定于HF（与其他截断牛顿方法相比）或神经网络的。我们还将提供创建高效且正确实现的实用技巧，并讨论在特定应用中设计和使用基于HF的方法时可能出现的陷阱。
- en: Table 20.1. A summary of the notation used. Note we will occasionally use some
    of these symbols to describe certain concepts that are local to a given sub-section.
    The subscripts "k" and "k−1", will often be dropped for compactness where they
    are implied from the context.
  id: totrans-5759
  prefs: []
  type: TYPE_NORMAL
  zh: 表 20.1. 使用的符号摘要。请注意，我们将偶尔使用这些符号来描述某些局部概念。下标 "k" 和 "k−1" 将通常被省略以提高简洁性，只要它们在上下文中隐含即可。
- en: '| from the context. Notation Description [x]i The i-th entry of a vector x
    [A]i,j The (i, j)-th entry a matrix A 1m A vector of length m whose entries are
    1 sq(·) The element-wise square of a vector or a matrix vec(A) The vectorization
    of a matrix A f The objective function fi The objective function on case i k the
    current iteration of HF θk The parameter setting at the k-th HF iteration n The
    dimension of θ δk The variable being optimized by CG at the k-th HF iteration
    Mk−1 A local quadratic approximation of f at θk−1 Mˆ k−1 A "damped" version of
    the above Bk−1 The curvature matrix of Mk−1 Bˆ k−1 The curvature matrix of Mˆk−1
    h , ∇h The gradient of a scalar function h h, ∇2h The Hessian of a scalar function
    h L(·) The loss function ρ The reduction ratio f(θk) − f(θk−1) Mk−1(δk) F(θ) A
    function that maps parameters to predictions on all training case   |                          |
    s   |'
  id: totrans-5760
  prefs: []
  type: TYPE_TB
  zh: '| 上下文中的符号。 符号 描述 [x]i 向量 x 的第 i 项 [A]i,j 矩阵 A 的 (i, j) 项 1m 长度为 m 的向量，其元素均为
    1 sq(·) 向量或矩阵的逐元素平方 vec(A) 矩阵 A 的向量化 f 目标函数 fi 案例 i 上的目标函数 k HF 的当前迭代 θk 第 k 次
    HF 迭代时的参数设置 n θ 的维度 δk 在第 k 次 HF 迭代中被 CG 优化的变量 Mk−1 在 θk−1 处的 f 的局部二次近似 Mˆ k−1
    上述的“阻尼”版本 Bk−1 Mk−1 的曲率矩阵 Bˆ k−1 Mˆk−1 的曲率矩阵 h , ∇h 标量函数 h 的梯度 h, ∇2h 标量函数 h 的
    Hessian L(·) 损失函数 ρ 减少比率 f(θk) − f(θk−1) Mk−1(δk) F(θ) 一个将参数映射到所有训练案例预测的函数   |                          |
    s   |'
- en: '| --- | --- | --- |'
  id: totrans-5761
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| D | A damping matrix         |     |'
  id: totrans-5762
  prefs: []
  type: TYPE_TB
  zh: '| D | 一个阻尼矩阵         |     |'
- en: '| P | A preconditioning matrix |     |'
  id: totrans-5763
  prefs: []
  type: TYPE_TB
  zh: '| P | 一个预处理矩阵 |     |'
- en: '| Ki(A, r0) The subspace span{r0, Ar0,...,Ai−1r0}  The number of layers of
    a feedforward net z The output of the network m The dimension of z T The number
    of time-steps of an RNN λ Strength constant for penalty damping terms λj j-th
    eigenvalue of curvature matrix diag(A) A vector consisting of the diagonal of
    the matrix A diag(v) A diagonal matrix A satisfying [A]i,i = [v]i   |                          |     |'
  id: totrans-5764
  prefs: []
  type: TYPE_TB
  zh: '| Ki(A, r0) 子空间 span{r0, Ar0,...,Ai−1r0}  前馈网络的层数 z 网络的输出 m z 的维度 T RNN 的时间步数
    λ 惩罚衰减项的强度常数 λj j-th 曲率矩阵的特征值 diag(A) 一个由矩阵 A 的对角线元素组成的向量 diag(v) 一个对角矩阵 A，满足
    [A]i,i = [v]i   |                          |     |'
- en: 20.2 Feedforward Neural Networks
  id: totrans-5765
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.2 前馈神经网络
- en: We now formalize feedforward neural networks (FNNs). Given an input x and setting
    of the parameters θ that determine weight matrices and the biases
  id: totrans-5766
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将形式化前馈神经网络（FNN）。给定输入 x 和确定权重矩阵和偏置的参数 θ 的设置。
- en: '(W1*,...,W*−1, b1*,...,b*−1), the FNN computes its output y by the following
    recurrence:'
  id: totrans-5767
  prefs: []
  type: TYPE_NORMAL
  zh: (W1*,...,W*−1, b1*,...,b*−1)，FNN通过以下递归计算其输出 y：
- en: yi+1 = si(Wiyi + bi)
  id: totrans-5768
  prefs: []
  type: TYPE_NORMAL
  zh: yi+1 = si(Wiyi + bi)
- en: where y1 = x. The vectors yi are the activations of the neural network, and
    the activation functions si(·) are some nonlinear functions, typically sigmoid
    or a tanh functions applied coordinate-wise.
  id: totrans-5769
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 y1 = x。向量 yi 是神经网络的激活，激活函数 si(·) 是某些非线性函数，通常是 sigmoid 或 tanh 函数，逐坐标应用。
- en: Given a matching target t, the FNN's training objective on a single case f(θ;
    (*x, t*)) is given by
  id: totrans-5770
  prefs: []
  type: TYPE_NORMAL
  zh: 给定匹配的目标 t，FNN 在单个案例上的训练目标 f(θ; (*x, t*)) 为：
- en: $$f(\theta;(x,t))=L(y_{\ell};t)$$
  id: totrans-5771
  prefs: []
  type: TYPE_NORMAL
  zh: $$f(\theta;(x,t))=L(y_{\ell};t)$$
- en: where L(z;t) is a loss function which quantifies how bad z is at predicting
    the target t. Note that L may not compare z directly to t, but instead may transform
    it first into some prediction vector p.
  id: totrans-5772
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 L(z;t) 是一个损失函数，用于量化 z 在预测目标 t 时的表现有多糟。注意，L 可能不会直接将 z 与 t 进行比较，而是可能会先将其转换为某个预测向量
    p。
- en: 'Finally, the training error, which is the objective of interest for learning,
    is obtained by averaging the losses f(θ; (*x, t*)) over a set S of input-output
    pairs (aka training cases):'
  id: totrans-5773
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，训练误差是学习的主要目标，通过对一组输入-输出对（即训练案例）上的损失 f(θ; (*x, t*)) 进行平均来获得：
- en: $$f(\theta)={\frac{1}{|S|}}\sum_{(x,t)\in S}f(\theta;(x,t))$$
  id: totrans-5774
  prefs: []
  type: TYPE_NORMAL
  zh: $$f(\theta)={\frac{1}{|S|}}\sum_{(x,t)\in S}f(\theta;(x,t))$$
- en: 'Algorithm 20.1 . An algorithm for computing the gradient of a feedforward neural
    network input: y0; θ mapped to (W1,...,W−1, b1*,...,b*−1).'
  id: totrans-5775
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 20.1 . 计算前馈神经网络梯度的算法，输入：y0; θ 映射到 (W1,...,W−1, b1*,...,b*−1)。
- en: for all i from 1 to  − 1 do xi+1 ← Wiyi + bi yi+1 ← si+1(xi+1)
  id: totrans-5776
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 i 从 1 到 − 1，执行 xi+1 ← Wiyi + bi，yi+1 ← si+1(xi+1)
- en: end for dy ← ∂L(y;t)/∂y (t is the target)
  id: totrans-5777
  prefs: []
  type: TYPE_NORMAL
  zh: 结束循环 dy ← ∂L(y;t)/∂y（t 是目标）
- en: 'for all i from  − 1 downto 1 do dxi+1 ← dyi+1si+1(xi+1) dWi ← dxi+1yi dbi ←
    dxi+1 dyi ← Wi dxi+1 end for output: ∇f(θ) as mapped from (dW1, . . . , dW−1,
    db1*, . . . , db*−1).'
  id: totrans-5778
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有的 i 从 −1 倒数到 1，执行 dxi+1 ← dyi+1si+1(xi+1) dWi ← dxi+1yi dbi ← dxi+1 dyi
    ← Wi dxi+1，结束循环，输出：∇f(θ) 映射自 (dW1, . . . , dW−1, db1*, . . . , db*−1)。
- en: 20.3 Recurrent Neural Networks
  id: totrans-5779
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.3 循环神经网络
- en: Recurrent Neural Networks (RNNs) are the time-series analog of feed-forward
    neural networks. RNNs model the mapping from an input sequence to an output sequence,
    and possess feedback connections in their hidden units that allow them to use
    information about past inputs to inform the predictions of future outputs.
  id: totrans-5780
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）是前馈神经网络的时间序列类比。RNN 建模从输入序列到输出序列的映射，并在其隐藏单元中具有反馈连接，使它们能够利用过去输入的信息来指导未来输出的预测。
- en: They may also be viewed as a special kind of feed-forward net with a "layer"
    for each time-step of the sequence. But unlike in a feed-forward network where
    each layer has its own parameters, the "layers" of an RNN share their parameters.
  id: totrans-5781
  prefs: []
  type: TYPE_NORMAL
  zh: 它们也可以视为一种特殊的前馈网络，每个时间步都有一个“层”。但与每层都有其自身参数的前馈网络不同，RNN 的“层”共享其参数。
- en: Their high-dimensional hidden state and nonlinear dynamics allow RNNs to learn
    very general and versatile representations, and to express highly complex sequential
    relationships. This representational power makes it possible, in principle, for
    RNNs to learn compact solutions for very difficult sequence modeling and labeling
    tasks. But despite their attractive qualities, RNNs did not enjoy widespread adoption
    after their initial discovery due to the perception that they were too difficult
    to properly train. The vanishing gradients problem [4, 18], where the derivative
    terms can exponentially decay to zero or explode during back-propagation through
    time is cited as one of the main reasons for this difficultly. In the case of
    decay, important back-propagated error signals from the output at future time-steps
    may decay nearly to zero by the time they have been back-propagated far enough
    to reach the relevant inputs.This makes the unmodified gradient a poor direction
    to follow if the RNN is to learn to exploit long-range input-output dependencies
    in the certain datasets.
  id: totrans-5782
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的高维隐藏状态和非线性动态使 RNN 能够学习非常一般和多样化的表示，并表达高度复杂的序列关系。这种表示能力使得 RNN 原则上能够学习非常困难的序列建模和标记任务的紧凑解决方案。但尽管它们的优点，RNN
    在最初被发现后并没有广泛采用，因为人们普遍认为它们难以正确训练。消失梯度问题 [4, 18] 被认为是造成这种困难的主要原因之一，在时间反向传播过程中，导数项可能会指数衰减到零或爆炸。在衰减的情况下，来自未来时间步的输出的重要反向传播误差信号可能在反向传播到足够远以到达相关输入之前几乎衰减到零。这使得如果
    RNN 要学习利用某些数据集中的长范围输入输出依赖关系，未修改的梯度是一个很差的方向。
- en: Recent work by Martens & Sutskever [23] has demonstrated that HF is a viable
    method for optimizing RNNs on datasets that exhibit pathological long range dependencies
    that were believed difficult or impossible to learn with gradient descent. These
    problems were first examined by Hochreiter & Schmidhuber [19] where the proposed
    solution was to modify the RNN architecture with special memory units.
  id: totrans-5783
  prefs: []
  type: TYPE_NORMAL
  zh: Martens 和 Sutskever 的最新研究 [23] 表明，HF 是一种可行的方法，用于在存在病态长范围依赖的数据集上优化 RNN，这些数据集被认为难以或不可能用梯度下降法学习。这些问题最初由
    Hochreiter 和 Schmidhuber [19] 进行研究，提出的解决方案是用特殊的记忆单元修改 RNN 架构。
- en: 'Basic RNNs are parameterized by three matrices and a special initial hidden
    state vector, so that θ ≡ (Wxh, Whh, Wzh, h0), where Wxh are the connections from
    the inputs to the hidden units, Whh are the recurrent connections, and Wzh are
    the hidden-to-output connections. Given a sequence of vector-valued inputs x =
    (x1*,...,x*T ) and vector-valued target outputs t = (t1*,...,t*T ), the RNN computes
    a sequence of hidden states and predictions according to:'
  id: totrans-5784
  prefs: []
  type: TYPE_NORMAL
  zh: 基本 RNN 由三个矩阵和一个特殊的初始隐藏状态向量参数化，因此 θ ≡ (Wxh, Whh, Wzh, h0)，其中 Wxh 是输入到隐藏单元的连接，Whh
    是递归连接，Wzh 是隐藏到输出的连接。给定一个向量值输入序列 x = (x1*,...,x*T) 和向量值目标输出 t = (t1*,...,t*T)，RNN
    根据以下公式计算一系列隐藏状态和预测：
- en: $$\begin{array}{l}{{h_{\tau}=s(W_{x h}x_{\tau}+W_{h h}h_{\tau-1})}}\\ {{z_{\tau}=W_{z
    h}h_{\tau}}}\end{array}$$
  id: totrans-5785
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{l}{{h_{\tau}=s(W_{x h}x_{\tau}+W_{h h}h_{\tau-1})}}\\ {{z_{\tau}=W_{z
    h}h_{\tau}}}\end{array}$$
- en: where h0 is a special parameter vector of the initial state and s(·) is a nonlinear
    activation function (typically evaluated coordinate-wise).
  id: totrans-5786
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 h0 是初始状态的特殊参数向量，s(·) 是非线性激活函数（通常按坐标逐一计算）。
- en: The RNN learning objective for a single input-output pair of sequences (*x,
    t*)
  id: totrans-5787
  prefs: []
  type: TYPE_NORMAL
  zh: RNN学习目标针对单个输入-输出对序列(*x, t*)。
- en: 'is given by:'
  id: totrans-5788
  prefs: []
  type: TYPE_NORMAL
  zh: 给出为：
- en: $$f(\theta;(x,t))=L(z;t)\equiv\sum_{\tau=1}^{T}L_{\tau}(z_{\tau};t_{\tau})$$
  id: totrans-5789
  prefs: []
  type: TYPE_NORMAL
  zh: $$f(\theta;(x,t))=L(z;t)\equiv\sum_{\tau=1}^{T}L_{\tau}(z_{\tau};t_{\tau})$$
- en: 'where Lτ is a loss function as in the previous section. As with FNNs, the objective
    function is obtained by averaging the loss over the training cases:'
  id: totrans-5790
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Lτ是前一节中的损失函数。与FNN相同，目标函数通过对训练案例的损失进行平均获得：
- en: $$f(\theta)={\frac{1}{|S|}}\sum_{(x,t)\in S}f(\theta;(x,t))$$
  id: totrans-5791
  prefs: []
  type: TYPE_NORMAL
  zh: $$f(\theta)={\frac{1}{|S|}}\sum_{(x,t)\in S}f(\theta;(x,t))$$
- en: 20.4 Hessian-Free Optimization Basics
  id: totrans-5792
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.4 无Hessian优化基础
- en: 'We consider the setting of unconstrained minimization of a twice-differentiable
    objective function f : Rn → R w.r.t. to a vector of real-valued parameters θ ∈
    Rn. 2nd-order optimizers such as HF are derived from the classical Newton''s method
    (a.k.a. the Newton-Raphson method), an approach based on the idea of iteratively
    optimizing a sequence of local quadratic models/approximations of the objective
    function in order to produce updates to θ. In the simplest situation, given the
    previous setting of the parameters θk−1, iteration k produces a new iterate θk
    by minimizing a local quadratic model Mk−1(δ) of the objective f(θk−1 + δ), which
    is formed using gradient and curvature information local to θk−1. More precisely,
    we define'
  id: totrans-5793
  prefs: []
  type: TYPE_NORMAL
  zh: '我们考虑无约束最小化的设置，目标函数为二次可微的f : Rn → R，关于实值参数向量θ ∈ Rn。二阶优化器如HF来源于经典的牛顿法（也称牛顿-拉夫森法），这种方法基于迭代优化目标函数的局部二次模型/近似序列的思想，以生成对θ的更新。在最简单的情况下，给定参数θk−1的先前设置，迭代k通过最小化局部二次模型Mk−1(δ)生成新的迭代θk，该模型是使用与θk−1相关的梯度和曲率信息形成的。更精确地说，我们定义'
- en: $$M_{k-1}(\delta)=f(\theta_{k-1})+\nabla f(\theta_{k-1})^{\top}\delta+\frac{1}{2}\delta^{\top}{\rm
    B}_{k-1}\delta\tag{20.1}$$
  id: totrans-5794
  prefs: []
  type: TYPE_NORMAL
  zh: $$M_{k-1}(\delta)=f(\theta_{k-1})+\nabla f(\theta_{k-1})^{\top}\delta+\frac{1}{2}\delta^{\top}{\rm
    B}_{k-1}\delta\tag{20.1}$$
- en: where Bk−1 is the "curvature matrix", and is chosen to be the Hessian H(θk−1)
    of f at θk−1 in the case of standard Newton's method. The new iterate θk is computed
    as θk−1 + αkδk where δ∗k is the minimizer of 20.1, and αk ∈ [0, 1]
  id: totrans-5795
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Bk−1是“曲率矩阵”，在标准牛顿法的情况下选择为f在θk−1处的Hessian H(θk−1)。新迭代θk的计算为θk−1 + αkδk，其中δ∗k是20.1的最小化器，αk
    ∈ [0, 1]。
- en: is chosen typically chosen via a line-search, with a preference for αk = 1.
    A
  id: totrans-5796
  prefs: []
  type: TYPE_NORMAL
  zh: 通常通过线搜索选择，并偏好αk = 1。
- en: standard efficient method for performing this kind of line search will be briefly
    discussed in section 20.8.8. The multiplication of δk by αk can be viewed as a
    crude instance of a general technique called "update damping", which we will introduce
    next, and later discuss in depth in section 20.8.
  id: totrans-5797
  prefs: []
  type: TYPE_NORMAL
  zh: 在20.8.8节中将简要讨论执行此类线搜索的标准高效方法。δk乘以αk可以视为一种称为“更新阻尼”的一般技术的粗略实例，我们将在接下来的部分介绍，并在20.8节中深入讨论。
- en: When Bk−1 is positive definite (PD), M(δk) will be bounded below and so its
    minimizer will exist, and will be given by δ∗k = xk − B−1 k−1∇f(θk−1), which is
    the standard Newton step. Unfortunately, for many good choices of Bk−1, such as
    the Hessian at θk−1, even computing the entire n×n curvature matrix Bk−1, let
    alone inverting it/solving the system Bk−1δk = −f(θk−1) (at a cost of O(n3)),
  id: totrans-5798
  prefs: []
  type: TYPE_NORMAL
  zh: 当Bk−1为正定（PD）时，M(δk)将有下界，因此其最小化器将存在，且给出为δ∗k = xk − B−1 k−1∇f(θk−1)，这就是标准的牛顿步。然而，对于许多良好的Bk−1选择，例如在θk−1处的Hessian，甚至计算整个n×n曲率矩阵Bk−1，更不用说反转它/求解系统Bk−1δk
    = −f(θk−1)（代价为O(n3)），
- en: will be impractical for all but very small neural networks.
  id: totrans-5799
  prefs: []
  type: TYPE_NORMAL
  zh: 对于除非常小的神经网络之外，这将是不切实际的。
- en: The main idea in Truncated-Newton methods such as HF is to avoid this costly
    inversion by partially optimizing the quadratic function M using the linear conjugate
    gradient algorithm (CG) [15], and using the resulting approximate minimizer δk
    to update θ. CG is a specialized optimizer created specifically for quadratic
    objectives of the form q(x) = 12 xAx−bx where A ∈ Rn×n is positive semi-definite
    (PSD), and b ∈ Rn. CG works by constructing the update from a sequence of vectors
    which have the property that they are "A-conjugate" and can thus be optimized
    independently in sequence. To apply CG to eqn. 20.1 we take x = δ, A = Bk−1 and
    b = ∇f(θk−1), noting that the constant term f(θk−1)
  id: totrans-5800
  prefs: []
  type: TYPE_NORMAL
  zh: 截断牛顿方法如HF的主要思想是通过使用线性共轭梯度算法（CG）部分优化二次函数M，以避免这一高成本的逆运算[15]，并利用得到的近似最小化器δk来更新θ。CG是专门为形式为q(x)
    = 12 xAx−bx的二次目标创建的优化器，其中A ∈ Rn×n是正半定（PSD），b ∈ Rn。CG通过构造一个具有“A-共轭”属性的向量序列来进行更新，因此可以独立顺序优化。要将CG应用于方程20.1，我们取x
    = δ，A = Bk−1和b = ∇f(θk−1)，注意常数项f(θk−1)。
- en: can be ignored.
  id: totrans-5801
  prefs: []
  type: TYPE_NORMAL
  zh: 可以忽略。
- en: 'Note: From this point forward, we will abbreviate Mk−1 with M'
  id: totrans-5802
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：从此时起，我们将Mk−1简写为M。
- en: and Bk−1 with B when the subscript is implied by the context.
  id: totrans-5803
  prefs: []
  type: TYPE_NORMAL
  zh: 当上下文暗示时，Bk−1用B表示。
- en: CG has the nice property that it only requires access to matrix-vectors products
    with the curvature matrix B (which can be computed *much* more efficiently than
    the entire matrix in many cases, as we will discuss in section 20.5), and it has
    a fixed-size storage overhead of a few n-dimensional vectors. Moreover, CG is
    a very powerful algorithm, which after i iterations, will find the provably optimal
    solution of any convex quadratic function q(x) over the Krylov subspace Ki(*A,
    r*0) ≡ span{r0, Ar0, A2r0*, ..., A*i−1r0}, where r0 = Ax0 − b and x0 is the initial
    solution [32]. Any other gradient based method applied directly to a quadratic
    function like M, even a very powerful one like Nesterov's accelerated gradient
    descent [29], can also be shown to produce solutions which lie in the Krylov subspace,
    and thus will always be strictly outperformed by CG given the same number of iterations1.
  id: totrans-5804
  prefs: []
  type: TYPE_NORMAL
  zh: CG具有一个优点，即它只需要访问与曲率矩阵B的矩阵-向量乘积（在许多情况下，这比计算整个矩阵*要*高效得多，正如我们将在20.5节中讨论的），并且它的存储开销固定为几个n维向量。此外，CG是一个非常强大的算法，在i次迭代后，将找到任何凸二次函数q(x)在Krylov子空间Ki(*A,
    r*0) ≡ span{r0, Ar0, A2r0*, ..., A*i−1r0}中的可证明最优解，其中r0 = Ax0 − b，x0是初始解[32]。任何其他直接应用于类似M的二次函数的基于梯度的方法，即使是像Nesterov加速梯度下降[29]这样非常强大的方法，也可以证明其解位于Krylov子空间内，因此在相同迭代次数下，CG将始终优于它们。
- en: Fortunately, in addition to these strong optimality properties, CG works extremely
    well in practice and may often converge in a number of iterations i n, depending
    on the structure of B. But even when it does not converge it tends to make very
    good partial progress.
  id: totrans-5805
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，除了这些强大的最优性特性外，CG在实践中也表现极佳，通常在迭代次数n内收敛，具体取决于B的结构。但即使不收敛，它也倾向于取得很好的部分进展。
- en: The preconditioned CG algorithm is given in alg. 20.2. Note that Api only needs
    to be computed once in each iteration of the main loop, and the quadratic objective
    q(xi) can be cheaply computed as q(xi) = 12
  id: totrans-5806
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理CG算法在算法20.2中给出。注意，Api只需要在主循环的每次迭代中计算一次，二次目标q(xi)可以便宜地计算为q(xi) = 12。
- en: (ri − b)
  id: totrans-5807
  prefs: []
  type: TYPE_NORMAL
  zh: （ri − b）
- en: xi. Also note that any notation such as αi or yi should not be confused with
    the other uses of these symbols that occur elsewhere in this report. The preconditioning
    matrix P allows CG to operate within a transformed coordinate system and a good
    choice of P can substantially accelerate the method. This is possible despite
    the previously claimed optimality of CG because P induces a transformed Krylov
    subspace. Preconditioning, methods for implementing it, its role within HF, and
    its subtle interaction with other parts of the HF approach, will be discussed
    in section 20.11.
  id: totrans-5808
  prefs: []
  type: TYPE_NORMAL
  zh: xi。还需注意，任何符号如αi或yi不应与本报告中其他地方出现的这些符号混淆。预处理矩阵P使得CG可以在变换坐标系中操作，P的良好选择可以显著加速该方法。尽管之前声称CG的最优性，但这是可能的，因为P引入了一个变换的Krylov子空间。预处理、实现它的方法、它在HF中的作用及其与HF方法其他部分的微妙交互将在20.11节中讨论。
- en: 'With practicality in mind, one can terminate CG according to various criteria,
    balancing the quality of the solution with the number of iterations required to
    obtain it (and hence number of matrix vector products - the main computational
    expense of the method). The approach taken by Martens [22] was to terminate CG
    based on a measure of relative progress optimizing M, computed as:'
  id: totrans-5809
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到实际应用，可以根据各种标准终止CG，在解决方案的质量与获得所需迭代次数（即矩阵向量乘法的次数——该方法的主要计算开销）之间进行平衡。Martens
    [22]采用的方法是根据优化M的相对进展度量来终止CG，该度量计算为：
- en: $$s_{j}={\frac{M(x_{j})-M(x_{j-k})}{M(x_{j})}}$$
  id: totrans-5810
  prefs: []
  type: TYPE_NORMAL
  zh: $$s_{j}={\frac{M(x_{j})-M(x_{j-k})}{M(x_{j})}}$$
- en: 1 This being said, it is possible to construct quadratic optimization problems
    where CG will perform essentially no better than accelerated gradient descent.
    Although it is also possible to construct ones where CG converge in only a few
    iteration while accelerated gradient descent will take much longer.
  id: totrans-5811
  prefs: []
  type: TYPE_NORMAL
  zh: 1 话虽如此，确实可以构造二次优化问题，使得CG的性能基本不比加速梯度下降好。尽管也可以构造出CG在仅几次迭代中收敛，而加速梯度下降则需要更长时间的情况。
- en: Algorithm 20.2 . Preconditioned conjugate gradient algorithm (PCG)
  id: totrans-5812
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 20.2. 预条件共轭梯度算法（PCG）
- en: 'inputs: b, A, x0, P r0 ← Ax0 − b y0 ← solution of P y = r0 p0 ← −y0 i ← 0 while
    termination conditions do not apply do αi ←'
  id: totrans-5813
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：b, A, x0, P r0 ← Ax0 − b y0 ← P y = r0 的解 p0 ← −y0 i ← 0 当终止条件不适用时 do αi
    ←
- en: ri yi pi Api xi+1 ← xi + αipi ri+1 ← ri + αiApi yi+1 ← solution of P y = ri+1
    βi+1 ←
  id: totrans-5814
  prefs: []
  type: TYPE_NORMAL
  zh: ri yi pi Api xi+1 ← xi + αipi ri+1 ← ri + αiApi yi+1 ← P y = ri+1的解 βi+1 ←
- en: 'ri+1yi+1 ri yi pi+1 ← −yi+1 + βi+1pi i ← i + 1 end while output: xi where xj
    is the j-th iterate of CG and k is the size of the window over which the average
    is computed, which should be increased with j. A reasonable choice that works
    well in practice is k = max(10*, j/*10). CG can be terminated at iteration j when
    sj < 0.0001 (20.2)'
  id: totrans-5815
  prefs: []
  type: TYPE_NORMAL
  zh: ri+1yi+1 ri yi pi+1 ← −yi+1 + βi+1pi i ← i + 1 结束当输出：xi，其中xj是CG的第j次迭代，k是计算平均值的窗口大小，应随j增加而增加。实践中有效的合理选择是k
    = max(10*, j/*10)。当sj < 0.0001时，可以在迭代j时终止CG（20.2）。
- en: or some other such constant. Depending on the situation it may make more sense
    to truncate earlier to find a more economical trade-off between relative progress
    and computation.
  id: totrans-5816
  prefs: []
  type: TYPE_NORMAL
  zh: 或其他类似常数。根据具体情况，早期截断可能更为合理，以便在相对进展和计算之间找到更经济的权衡。
- en: However, deciding when to terminate CG turns out to be a much more complex and
    subtle issue than implied by the above discussion, and in section 20.8.7 of this
    paper we will discuss additional reasons to terminate CG that have nothing directly
    to do with the value of M. In particular, earlier truncations may sometimes have
    a beneficial damping effect, producing updates that give a better improvement
    in f than would be obtained by a fully converged solution (or equivalently, one
    produced by exact inversion of the curvature matrix).
  id: totrans-5817
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，决定何时终止CG实际上是一个比上述讨论所暗示的更复杂、更微妙的问题，在本文的20.8.7节中，我们将讨论与M值无直接关系的额外终止CG的理由。特别是，早期的截断有时可能具有有益的阻尼效果，产生的更新在f上比完全收敛的解（或等效的，由曲率矩阵精确反演产生的解）更能带来改善。
- en: When f is non-convex (as it is with neural networks), B will sometimes be indefinite,
    and so the minimizer of M may not exist. In particular, progressively larger δ's
    may produce arbitrarily low values of M, leading to nonsensical or undefined updates.
    This issue can be viewed as an extreme example of the general problem that the
    quadratic model M is only a crude local approximation to f, and so its minimizer
    (assuming it even exists), might lie in a region of Rn where the approximation
    breaks down, sometimes catastrophically. While the aforementioned line-search
    can remedy this problem to some degree, this is a general problem with 2nd-order
    optimization that must be carefully addressed. Ways to do this are sometimes called
    "damping methods", a term which we shall use here, and include such techniques
    as restriction of the optimization over M(·) to a "trust-region", and the augmentation
    of M by penalty terms which are designed to encourage the minimizer of M to be
    somewhere in Rn where M
  id: totrans-5818
  prefs: []
  type: TYPE_NORMAL
  zh: 当 f 是非凸时（如神经网络情况），B 有时会是不定的，因此 M 的最小化器可能不存在。尤其是，逐渐增大的 δ 可能会产生任意低的 M 值，导致不合理或未定义的更新。这个问题可以看作是二次模型
    M 仅是 f 的粗略局部近似的极端例子，因此其最小化器（假设它存在）可能位于 Rn 的一个区域，那里近似可能会崩溃，有时甚至是灾难性的。尽管上述线搜索可以在某种程度上缓解这一问题，但这是二阶优化中的一个普遍问题，必须谨慎处理。解决这个问题的方法有时被称为“阻尼方法”，我们将在这里使用这个术语，包括将
    M(·) 的优化限制在“信任域”内，以及通过惩罚项增强 M 的技术，这些技术旨在鼓励 M 的最小化器位于 Rn 中的某个地方。
- en: remains a good approximation to f. Such approaches must be used with care, since
    restricting/penalizing the optimization of M too much will result in very reliable
    updates which are nonetheless useless due to being too "small". In section 20.8
    we will discuss various general damping methods in 2nd-order optimization, and
    some which are more specific to HF.
  id: totrans-5819
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然是 f 的良好近似。这些方法必须谨慎使用，因为对 M 的优化限制/惩罚过多会导致非常可靠的更新，但由于“太小”而变得无用。在第 20.8 节中，我们将讨论第二阶优化中的各种一般阻尼方法，以及一些更特定于
    HF 的方法。
- en: While the damping methods such as those mentioned above allow one to optimize
    M even when B is indefinite, there is another way to deal with the indefiniteness
    problem directly. The classical Gauss-Newton algorithm for non-linear least squares
    uses a positive semi-definite curvature matrix which is viewed as an approximation
    to the Hessian, and Schraudolph [11] was able to generalize this idea to cover
    a much larger class of objective functions that include most neural network training
    objectives. This "generalized Gauss-Newton matrix" (GGN), is also guaranteed to
    be positive semi-definite, and tends to work much better than the Hessian in practice
    as a curvature matrix when optimizing non-convex objectives. While using the GGN
    matrix will not eliminate the need for damping, Martens [22] nonetheless found
    that it was easier to use than the Hessian, producing better updates and requiring
    less damping. The computational and theoretical aspects of the GGN matrix and
    its use within HF will be discussed in detail in section 20.6.
  id: totrans-5820
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上述提到的阻尼方法允许在 B 不定的情况下优化 M，但还有另一种直接处理不定性问题的方法。经典的高斯-牛顿算法用于非线性最小二乘法，采用正半定的曲率矩阵，视为海森矩阵的近似，Schraudolph
    [11] 能够将这一思路推广到涵盖大多数神经网络训练目标的更大类目标函数。这种“广义高斯-牛顿矩阵”（GGN）也被保证为正半定，并且在实践中优化非凸目标时，作为曲率矩阵的效果往往优于海森矩阵。尽管使用
    GGN 矩阵并不能消除对阻尼的需求，Martens [22] 发现其使用起来仍然比海森矩阵更容易，能产生更好的更新并减少阻尼需求。GGN 矩阵及其在 HF
    中的使用的计算和理论方面将在第 20.6 节中详细讨论。
- en: 'Objective functions f(θ) that appear in machine learning are almost always
    defined as arithmetic averages over a training set S, and thus so can the gradient
    and the curvature-matrix vector products:'
  id: totrans-5821
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中出现的目标函数 f(θ) 几乎总是定义为训练集 S 的算术平均，因此梯度和曲率矩阵向量积也是如此：
- en: $$f(\theta)={\frac{1}{|S|}}\sum_{(x,t)\in S}f(\theta;(x,t))$$ $$\nabla f(\theta)={\frac{1}{|S|}}\sum_{(x,t)\in
    S}\nabla f(\theta;(x,t))$$ $$\mathrm{B}(\theta)v={\frac{1}{|S|}}\sum_{(x,t)\in
    S}\mathrm{B}(\theta;(x,t))v$$
  id: totrans-5822
  prefs: []
  type: TYPE_NORMAL
  zh: $$f(\theta)={\frac{1}{|S|}}\sum_{(x,t)\in S}f(\theta;(x,t))$$ $$\nabla f(\theta)={\frac{1}{|S|}}\sum_{(x,t)\in
    S}\nabla f(\theta;(x,t))$$ $$\mathrm{B}(\theta)v={\frac{1}{|S|}}\sum_{(x,t)\in
    S}\mathrm{B}(\theta;(x,t))v$$
- en: where f(θ; (*x, t*)) is the objective and B(θ; (*x, t*)) the curvature matrix
    associated with the training pair (*x, t*).
  id: totrans-5823
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 f(θ; (*x, t*)) 是目标函数，而 B(θ; (*x, t*)) 是与训练对 (*x, t*) 相关的曲率矩阵。
- en: In order to make HF practical for large datasets it is necessary to estimate
    the gradient and curvature matrix-vector products using subsets of the training
    data, called "minibatches." And while it may seem natural to compute the matrix-vector
    products required by CG using a newly sampled minibatch at each iteration of alg.
    20.2, CG is unfortunately not designed to handle this kind of "stochasticity"
    and its theory depends very much on a stable definition of B for concepts like
    B-conjugacy to even make sense. And in practice, we have found that such an approach
    does not seem to work very well, and results in CG itself diverging in some cases.
    The solution advocated by Martens [22] and independently by Byrd et al. [8] is
    to fix the minibatch used to define B for the entire run of CG. Minibatches and
    the practical issues which arise when using them will be discussed in more depth
    in section 20.12.
  id: totrans-5824
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使 HF 在大数据集上实用，有必要使用称为“迷你批次”的训练数据子集来估计梯度和曲率矩阵-向量乘积。虽然在算法 20.2 的每次迭代中使用新采样的迷你批次来计算
    CG 所需的矩阵-向量乘积似乎很自然，但不幸的是，CG 并不是为处理这种“随机性”而设计的，其理论在很大程度上依赖于 B 的稳定定义，以使 B-共轭等概念有意义。在实践中，我们发现这种方法似乎效果不佳，并且在某些情况下导致
    CG 本身发散。Martens [22] 和 Byrd 等人 [8] 独立提出的解决方案是固定用于定义 B 的迷你批次，以供整个 CG 运行期间使用。使用迷你批次时出现的实际问题将在第
    20.12 节中更深入地讨论。
- en: '| Algorithm 20.3 . High-level outline for the basic Hessian-free approach.
    Various details have been purposefully left unstated, and some aspects will be
    subject to change throughout this report. inputs: θ0, λ Set δ0 ← 0 k ← 1 while
    solution is not satisfactory do Select a set of points S for the gradient sec.
    20.12 b ← −∇f(θk−1) on S sec. 20.2 Select a set of points S for the curvature
    sec. 20.12 Compute a preconditioner P at θk sec. 20.11 Compute a damping matrix
    Dk sec. 20.8 Define A(v) ≡ G(θk−1)v + λDkv on S sec. 20.6 Choose a decay constant
    ζ ∈ [0, 1] sec. 20.10 δk ← PCG(b, A, ζδk−1, P) alg. 20.2 Update λ with the Levenberg-Marquardt
    method sec. 20.8.5 Choose/compute a step-size α sec. 20.8.8 θk ← θk−1 + αδk k
    ← k + 1 end while   |'
  id: totrans-5825
  prefs: []
  type: TYPE_TB
  zh: '| 算法 20.3 . 基本无赫斯特方法的高级概述。各个细节故意未予说明，某些方面将在本报告中有所变化。 输入：θ0, λ 设置 δ0 ← 0 k ←
    1 当解不满意时 do 选择一组点 S 以获得梯度 sec. 20.12 b ← −∇f(θk−1) 在 S sec. 20.2 选择一组点 S 以获得曲率
    sec. 20.12 在 θk 计算预条件器 P sec. 20.11 计算阻尼矩阵 Dk sec. 20.8 定义 A(v) ≡ G(θk−1)v + λDkv
    在 S sec. 20.6 选择衰减常数 ζ ∈ [0, 1] sec. 20.10 δk ← PCG(b, A, ζδk−1, P) 算法 20.2 用
    Levenberg-Marquardt 方法更新 λ sec. 20.8.5 选择/计算步长 α sec. 20.8.8 θk ← θk−1 + αδk k
    ← k + 1 结束 while   |'
- en: '| --- |'
  id: totrans-5826
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: 20.5 Exact Multiplication By The Hessian
  id: totrans-5827
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.5 通过 Hessian 进行精确乘法
- en: To use the Hessian H of f as the curvature matrix B within HF we need an algorithm
    to efficiently compute matrix-vector products with arbitrary vectors v ∈ Rn. Noting
    that the Hessian is the Jacobian of the gradient, we have that the Hessian-vector
    product H(θ)v is the directional derivative of the gradient
  id: totrans-5828
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 HF 中将 f 的 Hessian H 用作曲率矩阵 B，我们需要一个算法来有效计算与任意向量 v ∈ Rn 的矩阵-向量乘积。注意到 Hessian
    是梯度的雅可比，我们有 Hessian-向量乘积 H(θ)v 是梯度的方向导数。
- en: ∇f(θ) in the direction v, and so by the definition of directions derivatives,
  id: totrans-5829
  prefs: []
  type: TYPE_NORMAL
  zh: 在方向v上计算∇f(θ)，因此根据方向导数的定义，
- en: $$H(\theta)v=\operatorname*{lim}_{\varepsilon\to0}{\frac{\nabla f(\theta+\varepsilon
    v)-\nabla f(\theta)}{\varepsilon}}$$
  id: totrans-5830
  prefs: []
  type: TYPE_NORMAL
  zh: $$H(\theta)v=\operatorname*{lim}_{\varepsilon\to0}{\frac{\nabla f(\theta+\varepsilon
    v)-\nabla f(\theta)}{\varepsilon}}$$
- en: This equation implies a finite-differences algorithm for computing Hv at the
    cost of a single extra gradient evaluation. But in practice, and in particular
    when dealing with highly nonlinear functions like neural network training objectives,
    methods that use finite differences suffer from significant numerical issues,
    which can make them generally undesirable and perhaps even unusable in some situations.
  id: totrans-5831
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程暗示了一种有限差分算法，通过单次额外的梯度评估来计算 Hv。但在实践中，尤其是在处理高度非线性的函数，如神经网络训练目标时，使用有限差分的方法会遭遇显著的数值问题，这使得它们在一般情况下不太理想，甚至在某些情况下不可用。
- en: Fortunately, there is a method for computing the sought-after directional derivative
    in a numerically stable way that does not resort to finite differences. In the
    optimization theory literature, the method is known as "forwarddifferentiation"
    [34, 30], although we follow the exposition of Pearlmutter [31],
  id: totrans-5832
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一种方法可以以数值稳定的方式计算所需的方向导数，而不依赖于有限差分。在优化理论文献中，这种方法被称为“前向微分”[34, 30]，尽管我们遵循
    Pearlmutter [31] 的阐述，
- en: who rediscovered it for neural networks and other related models. The idea is
    to make repeated use of the chain rule, much like in the backpropagation algorithm,
    to differentiate the value of every node in the computational graph of the gradient.
    We formalize this notion by introducing the Rv-notation. Let RvX
  id: totrans-5833
  prefs: []
  type: TYPE_NORMAL
  zh: 谁为神经网络和其他相关模型重新发现了它。这个想法是反复使用链式法则，就像在反向传播算法中一样，以区分计算图中每个节点的值。我们通过引入Rv符号形式化这一概念。设RvX
- en: 'denote the directional derivative of X in direction v:'
  id: totrans-5834
  prefs: []
  type: TYPE_NORMAL
  zh: 表示X在方向v上的方向导数：
- en: $$\mathrm{R}_{v}X=\operatorname*{lim}_{\varepsilon\to0}{\frac{X(\theta+\varepsilon
    v)-X(\theta)}{\varepsilon}}={\frac{\partial X}{\partial\theta}}v$$
  id: totrans-5835
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{R}_{v}X=\operatorname*{lim}_{\varepsilon\to0}{\frac{X(\theta+\varepsilon
    v)-X(\theta)}{\varepsilon}}={\frac{\partial X}{\partial\theta}}v$$
- en: $$(20.3)$$
  id: totrans-5836
  prefs: []
  type: TYPE_NORMAL
  zh: $$(20.3)$$
- en: $\left(20.4\right)^{2}$
  id: totrans-5837
  prefs: []
  type: TYPE_NORMAL
  zh: $\left(20.4\right)^{2}$
- en: 'Being a derivative, the Rv(·) operator obeys the usual rules of differentiation:'
  id: totrans-5838
  prefs: []
  type: TYPE_NORMAL
  zh: 作为导数，Rv(·)算子遵循常规的微分规则：
- en: $$\begin{array}{r}{\operatorname{R}_{v}(X+Y)=\operatorname{R}_{v}X+\operatorname{R}_{v}Y}\\
    {\quad\operatorname{R}_{v}(X Y)=(\operatorname{R}_{v}X)Y+X\operatorname{R}_{v}Y}\\
    {\quad\operatorname{R}_{v}(h(X))=(\operatorname{R}_{v}X)h^{\prime}(X)}\end{array}$$
  id: totrans-5839
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{r}{\operatorname{R}_{v}(X+Y)=\operatorname{R}_{v}X+\operatorname{R}_{v}Y}\\
    {\quad\operatorname{R}_{v}(X Y)=(\operatorname{R}_{v}X)Y+X\operatorname{R}_{v}Y}\\
    {\quad\operatorname{R}_{v}(h(X))=(\operatorname{R}_{v}X)h^{\prime}(X)}\end{array}$$
- en: Rv(X + Y )=RvX + RvY linearity (20.4)
  id: totrans-5840
  prefs: []
  type: TYPE_NORMAL
  zh: Rv(X + Y )=RvX + RvY 线性法则 (20.4)
- en: Rv(XY ) = (RvX)Y + XRvY product rule (20.5)
  id: totrans-5841
  prefs: []
  type: TYPE_NORMAL
  zh: Rv(XY ) = (RvX)Y + XRvY 乘积法则 (20.5)
- en: Rv(h(X)) = (RvX)h(X) chain rule (20.6)
  id: totrans-5842
  prefs: []
  type: TYPE_NORMAL
  zh: Rv(h(X)) = (RvX)h(X) 链式法则 (20.6)
- en: where h denotes the Jacobian of h. From this point on we will abbreviate Rv
    as simply "R" to keep the notation compact.
  id: totrans-5843
  prefs: []
  type: TYPE_NORMAL
  zh: 其中h表示h的雅可比矩阵。从此时起我们将简化Rv为"R"，以保持符号简洁。
- en: Noting that Hv = R{∇f(θ)}, computing the Hessian-vector product amounts to computing
    R{∇f(θ)} by applying these rules recursively to the computational graph for ∇f(θ),
    in a way analogous to back-propagation (but operating forward instead of backwards).
  id: totrans-5844
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到Hv = R{∇f(θ)}，计算Hessian-向量积相当于通过递归地将这些规则应用于∇f(θ)的计算图来计算R{∇f(θ)}，这类似于反向传播（但向前操作而不是向后）。
- en: To make this precise, we will formalize the notion of a computational graph
    for an arbitrary vector-valued function h(θ), which can be thought of as a special
    kind of graph which implements the computation of a given function by breaking
    it down as a collection of simpler operations, represented by M nodes, with various
    input-output dependencies between the nodes indicated by directed edges. The nodes
    of the computational graph are vector valued, and each node i computes an arbitrary
    differentiable functions ai = γi(zi) of their input zi. Each input vector zi is
    formally the concatenation the output of each of its parent nodes aj ∈ Pi. The
    input θ is distributed over a set of input nodes I⊂{1*, ..., M*}
  id: totrans-5845
  prefs: []
  type: TYPE_NORMAL
  zh: 为了精确化这一点，我们将为任意向量值函数h(θ)形式化计算图的概念，可以认为它是一种特殊类型的图，通过将给定函数分解为一系列简单的操作（由M个节点表示），并通过有向边表示节点之间的各种输入输出依赖关系。计算图的节点是向量值的，每个节点i计算其输入zi的任意可微函数ai
    = γi(zi)。每个输入向量zi正式上是其所有父节点的输出aj ∈ Pi的串联。输入θ分布在输入节点I⊂{1*, ..., M*}上。
- en: and the outputs are computed at output nodes O⊂{1*, ..., M*}.
  id: totrans-5846
  prefs: []
  type: TYPE_NORMAL
  zh: 输出在输出节点O⊂{1*, ..., M*}处计算。
- en: 'In summary, the function h(θ) is computed according to the following procedure:'
  id: totrans-5847
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，函数h(θ)是根据以下步骤计算的：
- en: '1. For each i ∈ I set ai according to entries of θ 2. For i from 1 to M such
    that i ∈ I:'
  id: totrans-5848
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 对于每个i ∈ I，根据θ的条目设置ai 2. 对于i从1到M，使得i ∈ I：
- en: $$z_{i}={\mathrm{concat}}_{j\in P_{i}}a_{j}$$ $$a_{i}=\gamma_{i}(z_{i})$$
  id: totrans-5849
  prefs: []
  type: TYPE_NORMAL
  zh: $$z_{i}={\mathrm{concat}}_{j\in P_{i}}a_{j}$$ $$a_{i}=\gamma_{i}(z_{i})$$
- en: 3. Output h(θ) according to the values in {ai}i∈O
  id: totrans-5850
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 根据{ai}i∈O中的值输出h(θ)
- en: where Pi is the set of parents of node i.
  id: totrans-5851
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Pi是节点i的父节点集合。
- en: 'The advantage of the computational graph formalism is that it allows the application
    of the R-operator to be performed in a fool-proof and mechanical way that can
    be automated. In particular, our function R(h(θ)) can be computed as follows:'
  id: totrans-5852
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图形式的优势在于它允许以一种防错和机械的方式进行R算子的应用，这可以被自动化。特别是，我们的函数R(h(θ))可以如下计算：
- en: 1. For each i ∈ I set Rai according to entries of v
  id: totrans-5853
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 对于每个i ∈ I，根据v的条目设置Rai
- en: (which correspond to entries of θ)
  id: totrans-5854
  prefs: []
  type: TYPE_NORMAL
  zh: （与θ的条目对应）
- en: '2. For i from 1 to M such that i ∈ I:'
  id: totrans-5855
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 对于i从1到M，使得i ∈ I：
- en: $$\begin{array}{l}{{\mathrm{R}z_{i}=\mathrm{concat}_{j\in P_{i}}\mathrm{R}a_{j}}}\\
    {{\mathrm{R}a_{i}=\gamma_{i}^{\prime}(z_{i})\mathrm{R}z_{i}}}\end{array}$$
  id: totrans-5856
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{l}{{\mathrm{R}z_{i}=\mathrm{concat}_{j\in P_{i}}\mathrm{R}a_{j}}}\\
    {{\mathrm{R}a_{i}=\gamma_{i}^{\prime}(z_{i})\mathrm{R}z_{i}}}\end{array}$$
- en: 3. Set output R(h(θ)) according to the values in
  id: totrans-5857
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 根据中的值设置输出 R(h(θ))
- en: '{Rzi}i∈O'
  id: totrans-5858
  prefs: []
  type: TYPE_NORMAL
  zh: '{Rzi}i∈O'
- en: where γi(zi) is the Jacobian of γi.
  id: totrans-5859
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 γi(zi) 是 γi 的雅可比矩阵。
- en: In general, computing γi(zi) (or more simply multiplying it by a vector) is
    simple2 and is of comparable cost to computing γi(zi), which makes computing the
    Hessian-vector product using this method comparable to the cost of the gradient.
    Notice however that we need to have each zi available in order to evaluate γi(zi)
    in general, so all of the zi's (or equivalently all of the ai's) must either be
    computed in tandem with the Rai's and Rzi's (making the cost of the Hessian-vector
    product roughly comparable to the cost of two evaluations of the gradient), or
    be precomputed and cached (e.g. during the initial computation of the gradient).
  id: totrans-5860
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，计算 γi(zi)（或者更简单地与一个向量相乘）是简单的，并且与计算 γi(zi) 的成本相当，这使得使用这种方法计算 Hessian-向量积的成本大致与梯度的成本相当。然而，请注意，我们需要有每个
    zi 才能评估 γi(zi)，因此所有的 zi（或者等效地所有的 ai）必须与 Rai 和 Rzi 一起计算（使得 Hessian-向量积的成本大致相当于两次梯度评估的成本），或者预计算并缓存（例如在梯度的初始计算过程中）。
- en: When using an iterative algorithm like CG that requires multiple Hessianvector
    products for the same θ, caching can save considerable computation, but as discussed
    in section 20.7 may require considerable extra storage when computing matrix-vector
    products over large minibatches.
  id: totrans-5861
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用像 CG 这样的迭代算法时，它需要对相同的 θ 进行多次 Hessian-向量积，缓存可以节省相当的计算，但如第 20.7 节所述，在对大小批量计算矩阵-向量积时可能需要大量额外存储。
- en: Algorithm 20.5 gives the pseudo-code for computing the Hessianvector product
    associated with the feedforward neural network defined in section 20.2. The parameter
    vector θ defines the weight matrices and the biases (W1,...,W−1, b1*,...,b*−1)
    and v maps analogously to (RW1,..., RW−1, Rb1*,...,* Rb−1). This algorithm was
    derived by applying the rules 20.4–20.6 to each line of alg. 20.2, where various
    required quantities such as yi are assumed to be available either because they
    are cached, or by running the corresponding lines of alg. 20.2 in tandem.
  id: totrans-5862
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 20.5 提供了计算与第 20.2 节定义的前馈神经网络相关的 Hessian-向量积的伪代码。参数向量 θ 定义了权重矩阵和偏置（W1,...,W−1,
    b1*,...,b*−1），而 v 类似地映射到 (RW1,..., RW−1, Rb1*,...,* Rb−1)。该算法通过将规则 20.4–20.6 应用于算法
    20.2 的每一行而得出，其中各种所需量如 yi 假定可以使用，原因是它们被缓存，或者通过同时运行算法 20.2 的相应行。
- en: $$(20.7)$$
  id: totrans-5863
  prefs: []
  type: TYPE_NORMAL
  zh: $$(20.7)$$
- en: '![481_image_0.png](481_image_0.png)'
  id: totrans-5864
  prefs: []
  type: TYPE_IMG
  zh: '![481_image_0.png](481_image_0.png)'
- en: $$(20.8)^{\frac{1}{2}}$$
  id: totrans-5865
  prefs: []
  type: TYPE_NORMAL
  zh: $$(20.8)^{\frac{1}{2}}$$
- en: Fig. 20.1. An example of a computational graph of the loss of a neural network
    objective. The weights are considered the inputs here.
  id: totrans-5866
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.1。神经网络目标损失的计算图示例。这里权重被视为输入。
- en: 20.6 The Generalized Gauss-Newton Matrix
  id: totrans-5867
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.6 广义高斯-牛顿矩阵
- en: The indefiniteness of the Hessian is problematic for 2nd-order optimization
    of non-convex functions because an indefinite curvature matrix B may result in
    a 2 If this is not the case then node i should be split into several simpler operations.
  id: totrans-5868
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian 的不确定性对非凸函数的二阶优化是一个问题，因为不确定的曲率矩阵 B 可能导致 2 如果不是这种情况，则节点 i 应该被拆分成几个更简单的操作。
- en: Algorithm 20.4 . An algorithm for computing H(θ)v in feedforward neural networks.
  id: totrans-5869
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 20.4。用于在前馈神经网络中计算 H(θ)v 的算法。
- en: 'input: v mapped to (RW1,..., RW−1, Rb1*,...,* Rb−1)'
  id: totrans-5870
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：v 映射到 (RW1,..., RW−1, Rb1*,...,* Rb−1)
- en: Ry0 ← 0 (since y0 is not a function of the parameters)
  id: totrans-5871
  prefs: []
  type: TYPE_NORMAL
  zh: Ry0 ← 0（因为 y0 不是参数的函数）
- en: for all i from 1 to  − 1 do Rxi+1 ← RWiyi + WiRyi + Rbi (product rule)
  id: totrans-5872
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 i 从 1 到 − 1 执行 Rxi+1 ← RWiyi + WiRyi + Rbi（乘积法则）
- en: Ryi+1 ← Rxi+1si+1(xi+1) (chain rule)
  id: totrans-5873
  prefs: []
  type: TYPE_NORMAL
  zh: Ryi+1 ← Rxi+1si+1(xi+1)（链式法则）
- en: end for Rdy ← R
  id: totrans-5874
  prefs: []
  type: TYPE_NORMAL
  zh: 结束 Rdy ← R
- en: ∂L(y;t)
  id: totrans-5875
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ∂L(y;t)
- en: ∂y
  id: totrans-5876
  prefs: []
  type: TYPE_NORMAL
  zh: ∂y
- en: = ∂{∂L(y;t)/∂y}
  id: totrans-5877
  prefs: []
  type: TYPE_NORMAL
  zh: = ∂{∂L(y;t)/∂y}
- en: ∂y Ry = ∂2L(y;t)
  id: totrans-5878
  prefs: []
  type: TYPE_NORMAL
  zh: ∂y Ry = ∂2L(y;t)
- en: ∂y2 Ry for all i from  − 1 downto 1 do Rdxi+1 ← Rdyi+1si+1(xi+1) + dyi+1R si+1(xi+1)
    (product rule)
  id: totrans-5879
  prefs: []
  type: TYPE_NORMAL
  zh: ∂y2 Ry 对于所有 i 从 − 1 到 1 执行 Rdxi+1 ← Rdyi+1si+1(xi+1) + dyi+1R si+1(xi+1)（乘积法则）
- en: = dyi+1si+1(xi+1)Rxi+1 (chain rule)
  id: totrans-5880
  prefs: []
  type: TYPE_NORMAL
  zh: = dyi+1si+1(xi+1)Rxi+1（链式法则）
- en: RdWi ← Rdxi+1yi + dxi+1Ryi (product rule)
  id: totrans-5881
  prefs: []
  type: TYPE_NORMAL
  zh: RdWi ← Rdxi+1yi + dxi+1Ryi（乘积法则）
- en: Rdbi ← Rdyi Rdyi ← RWi dxi+1 + Wi Rdxi+1 (product rule)
  id: totrans-5882
  prefs: []
  type: TYPE_NORMAL
  zh: Rdbi ← Rdyi Rdyi ← RWi dxi+1 + Wi Rdxi+1（乘积法则）
- en: 'end for output: H(θ)v as mapped from (RdW1,..., RdW−1, Rdb1*, . . . , Rdb*−1).'
  id: totrans-5883
  prefs: []
  type: TYPE_NORMAL
  zh: 结束输出：H(θ)v 作为从 (RdW1,..., RdW−1, Rdb1*, . . . , Rdb*−1) 映射得到。
- en: quadratic M which is not bounded below and thus does not have a minimizer to
    use as the update δ. This problem can be addressed in a multitude of ways. For
    example, imposing a trust-region (sec. 20.8.6) will constrain the optimization,
    or a penalty-based damping method (sec. 20.8.1) will effectively add a positive
    semi-definite (PSD) contribution to B which may render it positive definite (PD).
  id: totrans-5884
  prefs: []
  type: TYPE_NORMAL
  zh: 二次 M 是不下界的，因此没有可以用作更新 δ 的最小化器。可以通过多种方式解决这个问题。例如，施加信任域（见 20.8.6 节）将限制优化，或者基于惩罚的阻尼方法（见
    20.8.1 节）将有效地为 B 添加一个正半定（PSD）贡献，这可能使其变为正定（PD）。
- en: Another solution specific to truncated Newton methods is to truncate CG as soon
    as it generates a conjugate direction with negative curvature (i.e., when pi Api
    < 0 in alg. 20.2), a solution which may be useful in some applications but which
    we have not found to be particularly effective for neural network training.
  id: totrans-5885
  prefs: []
  type: TYPE_NORMAL
  zh: 针对截断牛顿方法的另一种特定解决方案是，尽快在生成具有负曲率的共轭方向时截断 CG（即，当 pi Api < 0 时，在算法 20.2 中），这种解决方案在某些应用中可能有用，但我们发现它对于神经网络训练并不是特别有效。
- en: Based on our experience, the best solution to the indefiniteness problem is
    to instead use the generalized Gauss-Newton (GGN) matrix proposed by Schraudolph
    [11], which is a provably positive semidefinite curvature matrix that can be viewed
    as an approximation to the Hessian. We will denote this matrix as G.
  id: totrans-5886
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，解决不定性问题的最佳方案是使用 Schraudolph [11] 提出的广义高斯-牛顿（GGN）矩阵，这是一种可以被视为 Hessian
    近似的可证明的正半定曲率矩阵。我们将该矩阵表示为 G。
- en: The generalized Gauss-Newton matrix can be derived in at least two ways, and
    both require that the objective f(θ) be expressed as the composition of two functions
    as f(θ) = L(F(θ)) where L is convex. In a neural network setting, F maps the parameters
    θ to a m-dimensional vector of the neural network's outputs z ≡ F(θ), and L(z)
    is a convex "loss function" which typically measures the difference between the
    network's outputs (which may be further transformed within L to produce "predictions"
    p) and the targets. For RNNs, z will be a vector of the outputs from all the time-steps
    and L computes the sum over losses at each one of them.
  id: totrans-5887
  prefs: []
  type: TYPE_NORMAL
  zh: 广义高斯-牛顿矩阵可以通过至少两种方式推导，且两者都要求目标函数 f(θ) 表达为两个函数的组合，即 f(θ) = L(F(θ))，其中 L 是凸的。在神经网络设置中，F
    将参数 θ 映射为神经网络输出的 m 维向量 z ≡ F(θ)，而 L(z) 是一个凸的“损失函数”，通常用于衡量网络输出（这些输出可能在 L 内进一步转换为“预测”
    p）与目标之间的差异。对于 RNN，z 将是所有时间步的输出向量，而 L 计算每一个时间步的损失总和。
- en: 'One way to view the GGN matrix is as an approximation of H where we drop certain
    terms that involve the 2nd-derivatives of F. Applying the chain rule to compute
    the Hessian of f (at θk−1), we get:'
  id: totrans-5888
  prefs: []
  type: TYPE_NORMAL
  zh: 一种看待 GGN 矩阵的方法是将其视为 H 的近似，其中我们舍弃与 F 的二阶导数相关的某些项。应用链式法则计算 f 在 θk−1 处的 Hessian，我们得到：
- en: $$f=L(F(\theta))$$ $$\nabla f(\theta)=J^{\top}\nabla L$$
  id: totrans-5889
  prefs: []
  type: TYPE_NORMAL
  zh: $$f=L(F(\theta))$$ $$\nabla f(\theta)=J^{\top}\nabla L$$
- en: $$\nabla f(\theta)=J^{\top}\nabla L$$ $$f^{\prime\prime}(\theta)=J^{\top}L^{\prime\prime}J+\sum_{i=1}^{m}[\nabla
    L]_{i}([F]_{i})^{\prime\prime}$$
  id: totrans-5890
  prefs: []
  type: TYPE_NORMAL
  zh: $$\nabla f(\theta)=J^{\top}\nabla L$$ $$f^{\prime\prime}(\theta)=J^{\top}L^{\prime\prime}J+\sum_{i=1}^{m}[\nabla
    L]_{i}([F]_{i})^{\prime\prime}$$
- en: where J denotes the Jacobian of F, ∇L is the gradient of L(z) w.r.t. z, and
    all 1st and 2nd derivatives are evaluated at θk−1. The first term JLJ is a positive
    definite matrix whenever L(z) is convex in z, and is defined as the GGN
  id: totrans-5891
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 J 表示 F 的雅可比，∇L 是 L(z) 相对于 z 的梯度，所有一阶和二阶导数都在 θk−1 处评估。第一项 JLJ 是正定矩阵，前提是 L(z)
    在 z 中是凸的，并被定义为 GGN。
- en: matrix. Note that in the special case where L(z) = 12 z2 (so that L = I)
  id: totrans-5892
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵。注意，在特殊情况下，当 L(z) = 12 z²（因此 L = I）
- en: we recover the standard Gauss-Newton matrix usually seen in the context of non-linear
    least squares optimization and the Levenberg-Marquardt algorithm [25].
  id: totrans-5893
  prefs: []
  type: TYPE_NORMAL
  zh: 我们恢复了通常在非线性最小二乘优化和 Levenberg-Marquardt 算法 [25] 的背景下看到的标准高斯-牛顿矩阵。
- en: 'Martens and Sutskever [23] showed that the GGN matrix can also be viewed as
    the Hessian of a particular approximation of f constructed by replacing F with
    its 1st-order approximation. Consider a local convex approximation ˆf to f at
    θk−1 that is obtained by taking the first-order approximation F(θ) ≈ F(θk−1) +
    Jδ (where δ = θ − θk−1):'
  id: totrans-5894
  prefs: []
  type: TYPE_NORMAL
  zh: Martens 和 Sutskever [23] 表明，GGN 矩阵也可以视为通过用 F 的一阶近似替换 F 而构造的 f 的特定近似的 Hessian。考虑在
    θk−1 处获得的对 f 的局部凸近似 ˆf，它是通过取一阶近似 F(θ) ≈ F(θk−1) + Jδ（其中 δ = θ − θk−1）获得的：
- en: $${\hat{f}}(\delta)=L(F(\theta_{k-1})+J\delta)$$
  id: totrans-5895
  prefs: []
  type: TYPE_NORMAL
  zh: $${\hat{f}}(\delta)=L(F(\theta_{k-1})+J\delta)$$
- en: $$(20.9)$$
  id: totrans-5896
  prefs: []
  type: TYPE_NORMAL
  zh: $$(20.9)$$
- en: ˆf(δ) = L(F(θk−1) + Jδ) (20.9)
  id: totrans-5897
  prefs: []
  type: TYPE_NORMAL
  zh: ˆf(δ) = L(F(θk−1) + Jδ) (20.9)
- en: The approximation ˆf is convex because it is a composition of a convex function
    and an affine function. It is easy to see that ˆf and f have the same derivative
    when δ = 0, because
  id: totrans-5898
  prefs: []
  type: TYPE_NORMAL
  zh: 近似值ˆf是凸的，因为它是一个凸函数和一个仿射函数的组合。很容易看出，当δ = 0时，ˆf和f具有相同的导数，因为
- en: ∇ ˆf = J∇L = J∇L
  id: totrans-5899
  prefs: []
  type: TYPE_NORMAL
  zh: ∇ ˆf = J∇L = J∇L
- en: 'which is precisely the derivative of f at θk−1. And the Hessian of ˆf at δ
    = 0 is precisely the GGN matrix:'
  id: totrans-5900
  prefs: []
  type: TYPE_NORMAL
  zh: 这恰好是f在θk−1处的导数。并且当δ = 0时，ˆf的海森矩阵恰好是GGN矩阵：
- en: $${\hat{\tau}}=J^{\top}\nabla.$$
  id: totrans-5901
  prefs: []
  type: TYPE_NORMAL
  zh: $${\hat{\tau}}=J^{\top}\nabla.$$
- en: $${\bar{\iota}}=J$$
  id: totrans-5902
  prefs: []
  type: TYPE_NORMAL
  zh: $${\bar{\iota}}=J$$
- en: $\nabla_{\mathcal{L}}$
  id: totrans-5903
  prefs: []
  type: TYPE_NORMAL
  zh: $\nabla_{\mathcal{L}}$
- en: $${\hat{f}}^{\prime\prime}=J^{\top}L^{\prime\prime}J=\mathbf{G}$$
  id: totrans-5904
  prefs: []
  type: TYPE_NORMAL
  zh: $${\hat{f}}^{\prime\prime}=J^{\top}L^{\prime\prime}J=\mathbf{G}$$
- en: Note that it may be possible to represent a function f with multiple distinct
    compositions of the form L(F(θ)), and each of these will give rise to a slightly
    different GGN matrix. For neural networks, a natural choice for the output vector
    z is often just to identify it as the output of the final layer (i.e., y), however
    this may not always result in a convex L.As a rule of thumb, it is best to define
    L and F in way that L performs "as much of the computation of f as possible" (but
    this is a problematic concept due to the existence of multiple distinct sequences
    of operations for computing f). For the case of neural networks with a softmax
    output layer and cross-entropy error, it is best to define L so that it performs
    both the softmax and then the cross-entropy, while F computes only the inputs
    to the soft-max function. This is also the recommendation made by Schraudolph
    [11]. A possible reason that this choice works best is due to the fact that F
  id: totrans-5905
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，可能存在多种不同的形式L(F(θ))来表示函数f，每种形式都会产生稍微不同的GGN矩阵。对于神经网络，输出向量z的自然选择通常只是将其识别为最终层的输出（即y），然而这不一定总是导致凸L。作为经验法则，最好以“尽可能多地执行f的计算”的方式定义L和F（但由于存在多种不同的操作序列来计算f，这一概念是有问题的）。对于具有softmax输出层和交叉熵误差的神经网络，最好定义L，使其既执行softmax，又执行交叉熵，而F只计算softmax函数的输入。这也是Schraudolph
    [11]的建议。之所以选择这样做的一个可能原因是，F
- en: is being replaced with its first-order approximation whose range is unbounded.
  id: totrans-5906
  prefs: []
  type: TYPE_NORMAL
  zh: 被替换为其一阶近似，且该近似的范围是无界的。
- en: Hence the GGN matrix makes sense only when L's input domain is Rm (as opposed
    to [0, 1]m for the cross-entropy error), since this is the range of the 1st-order
    approximation of F.
  id: totrans-5907
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当L的输入域是Rm时，GGN矩阵才有意义（与交叉熵误差的[0, 1]m相对），因为这是F的一阶近似的范围。
- en: 20.6.1 Multiplying By The Gauss-Newton Matrix
  id: totrans-5908
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.6.1 乘以高斯-牛顿矩阵
- en: For the GGN matrix to be useful in the context of HF, we need an efficient algorithm
    for computing the Gv products. Methods for multiplying by the classical Gauss-Newton
    matrix are well-known in the optimization literature [30], and these methods were
    generalized by Schraudolph [11] for the GGN matrix, using an approach which we
    will now describe.
  id: totrans-5909
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使GGN矩阵在HF的上下文中有用，我们需要一个有效的算法来计算Gv的乘积。文献[30]中对经典高斯-牛顿矩阵的乘法方法是众所周知的，而这些方法被Schraudolph
    [11]推广到GGN矩阵，使用的方法我们将现在描述。
- en: 'We know from the previous section that the GGN matrix can be expressed as the
    product of three matrices: Gv = JLJv. Thus multiplication of a vector v by the
    GGN matrix amounts to the sequential multiplication of that vector by these 3
    matrices. First, the product Jv is a Jacobian times vector and is therefore precisely
    equal to the directional derivative Rv{F(θ)}, and thus can be efficiently computed
    with the R-method as in section 20.5. Next, given that the loss function L is
    usually simple, multiplication of Jv by L is also simple'
  id: totrans-5910
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从前一节知道，GGN矩阵可以表示为三个矩阵的乘积：Gv = JLJv。因此，将向量v乘以GGN矩阵相当于依次将该向量乘以这三个矩阵。首先，乘积Jv是雅可比矩阵乘以向量，因此恰好等于方向导数Rv{F(θ)}，因此可以使用第20.5节中的R方法有效计算。接下来，由于损失函数L通常很简单，Jv乘以L也很简单。
- en: (sec. 20.6.2). Finally, we multiply the vector LJv by the matrix J using the
    backpropagation algorithm. Note that the backpropagation algorithm takes the derivatives
    w.r.t. the predictions (∇L) as inputs, and returns the derivative w.r.t. the parameters,
    namely J∇L, but we can replace ∇L with any vector we want. Algorithm 20.5 . An
    algorithm for computing Gv of a feedforward neural network.
  id: totrans-5911
  prefs: []
  type: TYPE_NORMAL
  zh: （第20.6.2节）。最后，我们使用反向传播算法将向量LJv乘以矩阵J。注意，反向传播算法以预测的导数（∇L）作为输入，并返回相对于参数的导数，即J∇L，但我们可以用任何我们想要的向量替换∇L。算法20.5：用于计算前馈神经网络的Gv的算法。
- en: 'input: RW1,..., RW−1, Rb1*,...,* Rb−1.'
  id: totrans-5912
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：RW1,..., RW−1, Rb1*,...,* Rb−1。
- en: Ry0 ← 0 (y0 is not a function of the parameters)
  id: totrans-5913
  prefs: []
  type: TYPE_NORMAL
  zh: Ry0 ← 0（y0 与参数无关）
- en: for all i from 1 to  − 1 do Rxi+1 ← RWiyi + WiRyi + Rbi (product rule)
  id: totrans-5914
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有 i 从 1 到 − 1，执行 Rxi+1 ← RWiyi + WiRyi + Rbi（乘积法则）
- en: Ryi+1 ← Rxi+1si+1(xi+1)
  id: totrans-5915
  prefs: []
  type: TYPE_NORMAL
  zh: Ryi+1 ← Rxi+1si+1(xi+1)
- en: end for Rdy ←
  id: totrans-5916
  prefs: []
  type: TYPE_NORMAL
  zh: 结束于 Rdy ←
- en: ∂2L(y;t)
  id: totrans-5917
  prefs: []
  type: TYPE_NORMAL
  zh: ∂2L(y;t)
- en: ∂y2 Ry for all i from  − 1 downto 1 do Rdxi+1 ← Rdyi+1si+1(xi+1)
  id: totrans-5918
  prefs: []
  type: TYPE_NORMAL
  zh: ∂y2 Ry 对所有 i 从 − 1 倒数到 1，执行 Rdxi+1 ← Rdyi+1si+1(xi+1)
- en: 'RdWi ← Rdxi+1yi Rdbi ← Rdxi+1 Rdyi ← RWi dxi+1 end for output: (RdW1,..., RdW−1,
    Rb1*,...,* Rb−1).'
  id: totrans-5919
  prefs: []
  type: TYPE_NORMAL
  zh: RdWi ← Rdxi+1yi Rdbi ← Rdxi+1 Rdyi ← RWi dxi+1 结束于输出：（RdW1,..., RdW−1, Rb1*,...,*
    Rb−1）。
- en: As observed by Martens and Sutskever [23], the second interpretation of the
    GGN matrix given in the previous section immediately implies an alternative method
    for computing Gv products. In particular, we can use the R method from sec. 20.5
    to efficiently multiply by the Hessian of ˆf, given a computational graph for
    ∇ ˆf. While doing this would require one to replace the part of the forward pass
    corresponding to F with a multiplication by the analogous Jacobian evaluated at
    θk−1 (which can be done using the R operator method applied to f), a simpler approach
    is just to modify the algorithm for computing ∇f so that all derivative terms
    involving intermediate quantities in the back-propagation through F are treated
    as "constants", which while they are computed from θk−1, are formally independent
    of θ. This version will only compute ˆf properly for θ = θk−1, but this is fine
    for our purposes since this is the point at which we wish to evaluate the GGN
    matrix.
  id: totrans-5920
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 Martens 和 Sutskever [23] 所观察到的，上一节中 GG  矩阵的第二种解释立即暗示了一种替代的方法来计算 Gv 乘积。特别是，我们可以使用第
    20.5 节中的 R 方法高效地乘以 ˆf 的海森矩阵，给定 ∇ ˆf 的计算图。虽然这样做需要将正向传播中与 F 相对应的部分替换为乘以在 θk−1 处评估的类似雅可比矩阵（这可以使用应用于
    f 的 R 运算符方法完成），但更简单的方法是修改计算 ∇f 的算法，使所有涉及通过 F 进行反向传播的中间量的导数项被视为“常量”，尽管它们是从 θk−1
    计算出来的，但在形式上与 θ 独立。该版本只会在 θ = θk−1 时正确计算 ˆf，但这对我们的目的来说是可以的，因为这是我们希望评估 GGN 矩阵的点。
- en: Algorithm 20.5 multiplies by the GGN matrix for the special case of a feedforward
    neural network and is derived using this second technique.
  id: totrans-5921
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 20.5 针对前馈神经网络的特殊情况乘以 GGN 矩阵，并采用了这种第二种技术。
- en: 20.6.2 Typical Losses
  id: totrans-5922
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.6.2 典型损失
- en: In this section we present a number of typical loss functions and their Hessians
  id: totrans-5923
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们介绍若干典型的损失函数及其海森矩阵
- en: (table 20.2). The function p(z) computes the predictions p from the network
    outputs z. These losses are convex and it is easy to multiply by their Hessians
    Table 20.2. Typical losses with their derivatives and Hessians. The loss L and
    the nonlinearity p(z) are "matching", which means that the Hessian is independent
    of the target t and is PSD.
  id: totrans-5924
  prefs: []
  type: TYPE_NORMAL
  zh: （表 20.2）。函数 p(z) 从网络输出 z 计算预测 p。这些损失是凸的，且乘以其海森矩阵很简单。表 20.2. 典型损失及其导数和海森矩阵。损失
    L 和非线性 p(z) 是“匹配”的，这意味着海森矩阵与目标 t 独立，并且是 PSD。
- en: '| Name                            | L(z;t)                                                              |
    ∇L(z;t)   | L(z;t)   | p     |'
  id: totrans-5925
  prefs: []
  type: TYPE_TB
  zh: '| 名称                          | L(z;t)                                                              |
    ∇L(z;t)   | L(z;t)   | p     |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-5926
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Squared error                   | 1 2 p − t2                                                                     |
    −(p − t)  | I | p = z |'
  id: totrans-5927
  prefs: []
  type: TYPE_TB
  zh: '| 平方误差                     | 1 2 p − t2                                                                     |
    −(p − t)  | I | p = z |'
- en: '| Cross-entropy error             | −tlog p − (1 − t) log(1 − p) −(p − t) diag(p(1
    − p)) p = Sigmoid(z) |           |   |       |'
  id: totrans-5928
  prefs: []
  type: TYPE_TB
  zh: '| 交叉熵误差                     | −tlog p − (1 − t) log(1 − p) −(p − t) diag(p(1
    − p)) p = Sigmoid(z) |           |   |       |'
- en: '| Cross-entropy error (multi-dim) | − i[t]i log[p]i                                                                     |
    −(p − t) diag(p) − pp p = Softmax(z)           |   |       |'
  id: totrans-5929
  prefs: []
  type: TYPE_TB
  zh: '| 交叉熵误差（多维）         | − i[t]i log[p]i                                                                     |
    −(p − t) diag(p) − pp p = Softmax(z)           |   |       |'
- en: without explicitly forming the matrix, since they are each either diagonal or
    the sum of a diagonal and a rank-1 term.
  id: totrans-5930
  prefs: []
  type: TYPE_NORMAL
  zh: 无需显式构造矩阵，因为它们每个都是对角线的或对角线和秩为1的项的和。
- en: When applying this formulation to FNNs, tote that because it formally includes
    the computation of the predictions p from the network outputs z (assumed to lie
    anywhere in Rm) in the loss function itself (instead of in the activation function
    s at the output layer), s should be set to the identity function.
  id: totrans-5931
  prefs: []
  type: TYPE_NORMAL
  zh: 在将此公式应用于FNN时，请注意，因为它正式包含了从网络输出 z 计算预测 p 的过程（假设位于 Rm 中的任意位置），所以损失函数本身（而不是输出层的激活函数
    s）应设为恒等函数。
- en: 20.6.3 Dealing With Non-Convex Losses
  id: totrans-5932
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.6.3 处理非凸损失
- en: We may sometimes want to have a non-convex loss function. The generalized Gauss-Newton
    matrix construction will not produce a positive definite matrix in this case because
    the GGN matrix JLJ will usually be PSD only when L is, which is a problem that
    can be addressed in one of several ways. For example, if our loss is L(y;t) =  tanh(y)
    − t2/2, which is non-convex, we could formally treat the tanh nonlinearity as
    being part of F (replacing F with tanh ◦F), and redefine the loss L as y − t2/2.
    Another trick which may work would be to approximate the loss-Hessian L with a
    positive definite matrix, which could be done, say, by adding a scaled multiple
    of the diagonal to L, or by taking the eigendecomposition of L and discarding
    the eigenvectors that have negative eigenvalues.
  id: totrans-5933
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有时可能想要使用非凸损失函数。在这种情况下，广义高斯-牛顿矩阵构造不会生成正定矩阵，因为GGN矩阵JLJ通常仅在L为PSD时才会成立，这个问题可以通过几种方式来解决。例如，如果我们的损失是L(y;t)
    = tanh(y) − t²/2，这是非凸的，我们可以正式将tanh非线性视为F的一部分（用tanh ◦F替换F），并重新定义损失L为y − t²/2。另一个可能有效的技巧是用正定矩阵来近似损失-海森矩阵L，这可以通过给L添加对角线的缩放倍数，或通过进行L的特征分解并丢弃具有负特征值的特征向量来完成。
- en: 20.7 Implementation Details 20.7.1 Efficiency Via Parallelism
  id: totrans-5934
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.7 实现细节 20.7.1 通过并行化提高效率
- en: A good implementation of HF can make fruitful use of parallelization in two
    ways.
  id: totrans-5935
  prefs: []
  type: TYPE_NORMAL
  zh: HF的良好实现可以通过两种方式充分利用并行化。
- en: First, it can benefit from model parallelism, which is the ability to perform
    the input and output computations associated with each neuron in a given layer
    parallel. Although model parallelism accelerates any optimization algorithm that
    is applied to neural networks, current hardware is incapable of fully taking advantage
    of it, mostly because weights are stored in a centralized memory with very limited
    bandwidth.
  id: totrans-5936
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它可以受益于模型并行性，即能够并行执行与给定层中每个神经元相关的输入和输出计算。尽管模型并行性加速了对神经网络应用的任何优化算法，但当前硬件无法充分利用它，主要是因为权重存储在带宽非常有限的集中式内存中。
- en: Second, an implementation of HF can benefit from data parallelism, where the
    computation of the gradient or curvature matrix vector products is performed independently
    and in parallel across the training cases in the current minibatch. Data parallelism
    is much easier to exploit in current hardware because it requires minimal communication,
    in stark contrast to model parallelism, which requires frequent and rapid communication
    of unit activations. The potential speedup offered by data parallelism is limited
    by the gains that can be derived from using larger minibatches to compute updates
    in HF, as well as the sheer amount of parallel computing power available.
  id: totrans-5937
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，HF的实现可以受益于数据并行性，其中当前小批次中的梯度或曲率矩阵向量积的计算是独立并行地进行的。由于数据并行性只需最小的通信，因此在当前硬件上更容易利用，这与模型并行性形成鲜明对比，后者需要频繁快速地传输单元激活值。数据并行性提供的潜在加速受限于使用更大小批次计算HF更新所能获得的收益，以及可用的并行计算能力的总量。
- en: HF tends to benefit from using relatively large minibatches, especially compared
    to first-order methods like stochastic gradient descent, and so exploiting data
    parallelism may bring significant reductions in computation time. Nonetheless,
    there is a point of diminishing returns after which making the minibatch larger
    provides limited or no benefit in terms of the quality of the update proposals
    (as measured by how much they reduce f).
  id: totrans-5938
  prefs: []
  type: TYPE_NORMAL
  zh: HF通常受益于使用相对较大的小批次，尤其是与随机梯度下降等一阶方法相比，因此利用数据并行性可能会显著减少计算时间。然而，存在收益递减的临界点，超过此点后，增大小批次对更新提案质量的提高（以减少f的程度来衡量）带来的好处有限或没有。
- en: Data parallelism is typically implemented using vectorization, which is a way
    of specifying a single computational process that is independently performed on
    every element of a vector. Since most implementations that use vectorization
  id: totrans-5939
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行性通常通过向量化来实现，这是一种指定单个计算过程的方法，该过程在向量的每个元素上独立执行。由于大多数使用向量化的实现
- en: (e.g. GPU code) become more efficient per case as the size of the minibatch
    increases, there is a distinct benefit to using larger minibatches (up until the
    aforementioned point of diminishing returns, or the point where the implementations
    parallel computing resources are fully utilized).
  id: totrans-5940
  prefs: []
  type: TYPE_NORMAL
  zh: （例如，GPU 代码）在每个案例中变得更加高效，随着小批量大小的增加，使用更大的小批量有明显的好处（直到上述收益递减的点，或者实现的并行计算资源完全利用的点）。
- en: Most of the computation performed by HF consists of computing the GGN
  id: totrans-5941
  prefs: []
  type: TYPE_NORMAL
  zh: HF 执行的大部分计算由计算 GGN 组成。
- en: vector products and fortunately it is possible to obtain a 50% speedup over
    a naive implementation of the GGN vector products using activity caching. Recall
    that a multiplication by the GGN matrix consists of a multiplication by J which
    is followed by a multiplication by J, both of which require the neural network's
    unit activations (the yi's in FNNs or yτ 's in RNNs). However, given that the
    network's activations are a function of only θ, and that CG multiplies different
    vectors by the same GGN matrix (so its setting of θ is fixed), it is possible
    to cache the network's activations yi and to reuse them for all the GGN-vector
    products made during an entire run of CG.
  id: totrans-5942
  prefs: []
  type: TYPE_NORMAL
  zh: 向量乘积，并且幸运的是，通过活动缓存，可以在天真的 GGN 向量乘积实现上获得 50% 的加速。回想一下，对 GGN 矩阵的乘法由先乘以 J，然后再乘以
    J，这两者都需要神经网络的单元激活（FNN 中的 yi 或 RNN 中的 yτ）。然而，由于网络的激活仅是 θ 的函数，并且 CG 对不同向量乘以相同的 GGN
    矩阵（因此它的 θ 设置是固定的），可以缓存网络的激活 yi，并在 CG 整个运行过程中重用它们用于所有 GGN 向量乘积。
- en: When a model is very large, which is the case for a large RNN with a large number
    of time-steps T , the unit activations produced by even a modestly-sized minibatch
    may become too numerous to fit in memory. This is especially a problem for GPU
    implementations since GPUs typically have much less memory available than CPUs.
    This has two undesirable consequences. First, activity caching becomes impossible,
    and second, it necessitates the splitting of a large minibatch into many smaller
    "computational minibatches" (the results from which will be summed up after each
    has been processed), which can greatly reduce the cost-effectiveness of vectorization.
  id: totrans-5943
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型非常大时，例如具有大量时间步长 T 的大型 RNN，即使是适度大小的小批量产生的单元激活也可能变得太多而无法在内存中容纳。这对于 GPU 实现尤其是一个问题，因为
    GPU 通常可用的内存比 CPU 少得多。这会带来两个不理想的后果。首先，活动缓存变得不可能；其次，它需要将大型小批量分割成许多较小的“计算小批量”（每个小批量处理后将结果相加），这会大大降低向量化的成本效益。
- en: The problem can be addressed in at least 2 ways. One is to cache the activations
    in a larger but slower memory storage (e.g. the CPU memory), and to retrieve them
    as needed. This is often faster than the use of many smaller minibatches.
  id: totrans-5944
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可以通过至少两种方法解决。一种是将激活缓存到更大但更慢的存储器中（例如 CPU 内存），并根据需要检索它们。这通常比使用多个较小的小批量更快。
- en: Another way involves reducing the storage requirements at the cost of performing
    re-computation of some of the states. In particular, we store the hidden states
    at every multiple of √T time-steps (thus reducing the storage requirement by a
    factor of √T), and recompute sequences of √T between these "check-points" as they
    become needed, discarding them immediately after use. Due to the way the forward
    and backwards passes involved in computing gradients and matrixvector products
    go through the time-steps in linear order, the state at each time-step needs to
    be recomputed at most once in the case of the gradient, and twice in the case
    of the matrix-vector product.
  id: totrans-5945
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是以执行某些状态的重新计算为代价，减少存储需求。特别地，我们在每个 √T 的时间步长处存储隐藏状态（从而将存储需求降低 √T 倍），并在需要时重新计算这些“检查点”之间的
    √T 序列，使用后立即丢弃。由于计算梯度和矩阵向量乘积的前向和反向传播以线性顺序遍历时间步长，梯度情况下每个时间步的状态最多需要重新计算一次，而矩阵向量乘积情况下需要重新计算两次。
- en: Fig. 20.2. An illustration of the method for conserving the memory of the RNN.
    Each
  id: totrans-5946
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.2. 一种节省 RNN 内存的方法的示意图。每个
- en: '![487_image_0.png](487_image_0.png)'
  id: totrans-5947
  prefs: []
  type: TYPE_IMG
  zh: '![487_image_0.png](487_image_0.png)'
- en: column represents a hidden state of an RNN, and only the highlighted columns
    reside in memory at any given time.
  id: totrans-5948
  prefs: []
  type: TYPE_NORMAL
  zh: 列表示 RNN 的一个隐藏状态，且在任何给定时间仅高亮的列驻留在内存中。
- en: 20.7.2 Verifying The Correctness Of G Products
  id: totrans-5949
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.7.2 验证 G 产品的正确性
- en: A well-known pitfall for neural networks practitioners is an incorrect implementation
    for computing the gradient, which is hard to diagnose without having a correct
    implementation to compare against. The usual procedure is to re-implement the
    gradient computation using finite differences and verify that the two implementations
    agree, up to some reasonable precision.
  id: totrans-5950
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络从业者的一个著名陷阱是计算梯度的不正确实现，这在没有正确实现进行比较的情况下很难诊断。通常的程序是使用有限差分重新实现梯度计算，并验证两个实现的一致性，直到某种合理的精度。
- en: To verify the correctness of an implementation of Truncated Newton optimizer
    like HF, as we must also verify the correctness of the curvature-matrix vector
    products. When B = H, there are well-known black-box finite differentiation implementations
    available which can be used for this purpose. Thus we will concentrate on how
    to verify the correctness of the Gv products.
  id: totrans-5951
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证像 HF 这样的截断牛顿优化器的实现正确性，我们还必须验证曲率矩阵向量乘积的正确性。当 B = H 时，有许多著名的黑箱有限差分实现可以用于此目的。因此，我们将专注于如何验证
    Gv 乘积的正确性。
- en: Given that G = JLJ so Gv = J(L(Jv)), computing the G-vector products via finite
    differences reduces to doing this for Jw, Lw and Jw for arbitrary vectors w of
    appropriate dimension (not necessarily the same for each).
  id: totrans-5952
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 G = JLJ，因此 Gv = J(L(Jv))，通过有限差分计算 G 向量乘积就简化为对 Jw、Lw 和 Jw 的处理，适用于任意维度的向量 w（不一定每个相同）。
- en: 1. For Jw we compute (F(θ + εw) − F(θ − εw))/(2ε) for a small ε.
  id: totrans-5953
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 对于 Jw，我们计算 (F(θ + εw) − F(θ − εw))/(2ε)，其中 ε 很小。
- en: 2. For Lw we can simply approximate L using one of the aforementioned finite-differences
    implementations that are available for approximating Hessians.
  id: totrans-5954
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 对于 Lw，我们可以简单地使用前面提到的有限差分实现之一来近似 L，这些实现可用于近似 Hessian。
- en: 3. For Jw we exploit [J]
  id: totrans-5955
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 对于 Jw，我们利用 [J]。
- en: j,i = [Jej ]i where ej is the j-th standard basis vector, and use the method
    in point 1 to approximate Jej To be especially thorough, one should probably test
    that Gej agrees with its finite differences version for each j, effectively constructing
    the whole matrix G.
  id: totrans-5956
  prefs: []
  type: TYPE_NORMAL
  zh: j,i = [Jej ]i，其中 ej 是第 j 个标准基向量，并使用第 1 点中的方法来近似 Jej。为了特别彻底，应该测试 Gej 是否与其有限差分版本一致，对每个
    j 有效地构造整个矩阵 G。
- en: For this kind of finite-differences numerical differentiation to be practical
    it is important to use small toy versions of the target networks, with much fewer
    units in each layer, and smaller values for the depth  or sequence length T (such
    as 4). In most situations, a good value of ε is often around 10−4, and it is possible
    to achieve a relative estimation error from the finite differences approximation
    of around 10−6, assuming a high-precision floating point implementation (i.e.
  id: totrans-5957
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这种有限差分数值微分具有实用性，重要的是使用小型玩具版本的目标网络，每层中的单元数要少得多，深度或序列长度 T 也要较小（例如 4）。在大多数情况下，ε
    的良好值通常约为 10−4，假设有高精度的浮点实现，从有限差分近似中可以实现约 10−6 的相对估计误差。
- en: float64 rather than float32).
  id: totrans-5958
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 float64 而不是 float32。
- en: It is also important to use random θ's that are of a reasonable scale. Parameters
    that are too small will fail to engage the nonlinearities, leaving them in their
    "linear regions" and making them behave like linear functions, while parameters
    that are too large may cause "saturation" of the units of the network, making
    them behave like step-functions (the opposite extreme). In either case, a proposed
    implementation of some exact derivative computation could match the finite differences
    versions to high precision despite being incorrect, as the local derivatives of
    the activation functions may be constant or even zero.
  id: totrans-5959
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是使用合理规模的随机 θ。参数过小会导致无法激发非线性，使其停留在“线性区域”，表现得像线性函数，而参数过大可能导致网络单元“饱和”，使其表现得像阶跃函数（相反的极端）。在任何情况下，某些精确导数计算的提议实现可能与有限差分版本高度匹配，尽管它不正确，因为激活函数的局部导数可能是常数甚至为零。
- en: Another option to consider when implementing complex gradient/matrix computations
    is to use an automatic differentiation system package such as Theano
  id: totrans-5960
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现复杂的梯度/矩阵计算时，另一个可考虑的选项是使用自动微分系统包，例如 Theano。
- en: '[5]. This approach has the advantage of being mostly fool proof, at the possible
    cost of customization and efficiency (e.g. it may be hard to cache the activities
    using previously discussed techniques).'
  id: totrans-5961
  prefs: []
  type: TYPE_NORMAL
  zh: '[5]. 这种方法的优点在于基本上是防错的，可能会以定制性和效率为代价（例如，使用之前讨论的技术可能很难缓存活动）。'
- en: 20.8 Damping
  id: totrans-5962
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.8 阻尼
- en: While unmodified Newton's method may work well for certain objectives, it tends
    to do very poorly if applied directly to highly nonlinear objective functions,
    such as those which arise when training neural networks. The reason for this failure
    has to do with the fact that the minimizer δ∗ of the quadratic approximation M
    may be very large and "aggressive" in the early and the intermediate stages of
    the optimization, in the sense that it is often located far beyond the region
    where the quadratic approximation is reasonably trust-worthy.
  id: totrans-5963
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管未经修改的牛顿法可能对某些目标有效，但如果直接应用于高度非线性的目标函数（如训练神经网络时出现的函数），它的效果往往很差。这种失败的原因与二次近似
    M 的极小值 δ∗ 在优化的早期和中期阶段可能非常大且“激进”有关，这意味着它通常位于二次近似可信区域之外。
- en: The convergence theory for non-convex smooth optimization problems (which include
    neural net training objectives) describes what happens only when the optimization
    process gets close enough to a local minimum so that the steps taken are small
    compared to the change in curvature (e.g. as measured by the Lipschitz constant
    of the Hessian). In such a situation, the quadratic model will always be highly
    accurate at δ∗, and so one can fully optimize M and generate a sequence of updates
    which will converge "quadratically" to the local minimum of f. And for some very
    simply optimization problems which can arise in practice it may even be possible
    to apply unmodified Newton's method without any trouble, ignoring the theoretical
    requirement of proximity to a local minimum. However, for neural network training
    objectives, and in particular deep feed-forward networks and RNNs, the necessity
    of these proximity assumptions quickly becomes clear after basic experiments,
    where such naive 2nd-order optimization tends to diverge rapidly from most sensible
    random initializations of θ.
  id: totrans-5964
  prefs: []
  type: TYPE_NORMAL
  zh: 非凸平滑优化问题的收敛理论（包括神经网络训练目标）仅描述当优化过程接近局部最小值时所发生的情况，此时所采取的步骤相较于曲率变化（例如，利用海森矩阵的利普希茨常数来测量）是小的。在这种情况下，二次模型在
    δ∗ 处总是非常准确，因此可以完全优化 M 并生成一系列将“二次收敛”到 f 的局部最小值的更新。而对于一些在实践中可能出现的非常简单的优化问题，甚至可以在不考虑接近局部最小值的理论要求的情况下应用未经修改的牛顿法。然而，对于神经网络训练目标，特别是深度前馈网络和递归神经网络，经过基本实验后，这些接近假设的必要性迅速变得明显，因为这种幼稚的二阶优化往往从大多数合理的随机初始化
    θ 中快速发散。
- en: The solution that is sometimes advocated for this problem is to use a more stable
    and reliable method, like gradient-descent for the beginning of optimization,
    and then switch later to 2nd-order methods for "fine convergence". Optimization
    theory guarantees that as long as the learning rate constant is sufficiently small,
    gradient descent will converge from any starting point. But precise convergence
    is often not necessary, or even undesirable (due to issues of overfitting). Instead,
    if we believe that making use of curvature information can be beneficial in constructing
    updates long before the "fine convergence" regime described by local convergence
    theory sets in, it may be worthwhile to consider how to make more careful and
    conservative use of curvature information in order to construct large but still
    sensible update proposals, instead of defaulting to 1st-order ones out of necessity.
  id: totrans-5965
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题，有时提倡的解决方案是使用更稳定可靠的方法，如在优化初期使用梯度下降，然后再切换到二阶方法以实现“精细收敛”。优化理论保证，只要学习率常数足够小，梯度下降将从任何起点收敛。但精确收敛往往不是必要的，甚至是不可取的（由于过拟合问题）。相反，如果我们认为利用曲率信息在“精细收敛”阶段之前构建更新是有益的，那么考虑如何更加谨慎和保守地使用曲率信息，以构建大而合理的更新提案，而不是出于必要而默认使用一阶方法，可能是值得的。
- en: '"Damping", a term used mostly in the engineering literature, and one which
    we will adopt here, refers to methods which modify M or constrain the optimization
    over it in order to make it more likely that the resulting update δ will lie in
    a region where M remains a reasonable approximation to f and hence yield a substantial
    reduction. The key difficulty with damping methods is that if they are overused
    or improperly calibrated, the resulting updates will be "reliable" but also be
    too small and insignificant (as measured by the reduction in f).'
  id: totrans-5966
  prefs: []
  type: TYPE_NORMAL
  zh: “阻尼”是一个主要在工程文献中使用的术语，我们在这里也将采用它，指的是修改M或约束其优化的方法，以使得结果更新δ更可能位于M仍然是f的合理近似的区域，从而带来实质性的减小。阻尼方法的关键困难在于，如果它们被过度使用或校准不当，所产生的更新将是“可靠的”，但也会过小且微不足道（以f的减少量来衡量）。
- en: An effective damping method is of critical importance to the performance of
    a 2nd-order method, and obtaining the best results will likely require the use
    of a variety of different techniques, whose usefulness depends both on the particular
    application and the underlying 2nd-order method. In this section we will discuss
    some generic damping methods that can be used in 2nd-order optimizers and how
    to apply them in HF (either separately or in some combination) along with methods
    which are specific to neural networks and HF.
  id: totrans-5967
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的阻尼方法对二阶方法的性能至关重要，获得最佳结果可能需要使用多种不同的技术，其有效性取决于特定应用和底层二阶方法。在本节中，我们将讨论一些可以在二阶优化器中使用的通用阻尼方法，以及如何在HF中应用它们（单独或以某种组合形式），以及针对神经网络和HF的特定方法。
- en: One thing to keep in mind when reading this section is that while the immediate
    goal of damping methods is to increase the quality of the parameter update produced
    by optimizing M (as measured by the immediate improvement in the objective f),
    damping methods can and will have an important influence on the global optimization
    performance of 2nd-order optimizers when applied to multimodal objectives functions,
    in ways that are sometimes difficult to predict or explain, and will be problem
    dependent. For example, we have observed empirically that on difficult neural-net
    training objectives, damping schemes which tend to produce updates that give the
    best reductions in f in the short term, may not always yield the best global optimization
    performance in the long term.
  id: totrans-5968
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本节时要记住的一件事是，虽然阻尼方法的直接目标是通过优化M来提高参数更新的质量（以目标函数f的直接改进为衡量标准），但阻尼方法在应用于多模态目标函数时，确实会对二阶优化器的全局优化性能产生重要影响，这种影响有时难以预测或解释，并且会因问题而异。例如，我们从经验上观察到，在困难的神经网络训练目标上，倾向于在短期内产生最佳f值减小的更新的阻尼方案，可能并不总是能在长期内带来最佳的全局优化性能。
- en: 20.8.1 Tikhonov Damping
  id: totrans-5969
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.8.1 提赫诺夫阻尼
- en: '"Tikhonov regularization" or Tikhonov damping 3 is arguably the most wellknown
    damping method, and works by penalizing the squared magnitude δ2 of the update
    δ by introducing an additional quadratic penalty term into the quadratic model
    M. Thus, instead of minimizing M, we minimize a "damped" quadratic'
  id: totrans-5970
  prefs: []
  type: TYPE_NORMAL
  zh: “提赫诺夫正则化”或提赫诺夫阻尼可以说是最知名的阻尼方法，其通过在二次模型M中引入额外的二次惩罚项来惩罚更新δ的平方大小δ²。因此，我们不再最小化M，而是最小化一个“阻尼”二次函数。
- en: $$\hat{M}(\delta)\equiv M(\delta)+\frac{\lambda}{2}\delta^{\top}\delta=f(\theta)+\nabla
    f(\theta)^{\top}\delta+\frac{1}{2}\delta^{\top}\hat{\mathrm{B}}\delta$$
  id: totrans-5971
  prefs: []
  type: TYPE_NORMAL
  zh: $$\hat{M}(\delta)\equiv M(\delta)+\frac{\lambda}{2}\delta^{\top}\delta=f(\theta)+\nabla
    f(\theta)^{\top}\delta+\frac{1}{2}\delta^{\top}\hat{\mathrm{B}}\delta$$
- en: where B = B+ ˆ λI, where λ ≥ 0 is a scalar parameter determining the "strength"
    of the damping. Computing the matrix-vector product with Bˆ is straightforward
    since Bˆv = (B + λI)v = Bv + λv.
  id: totrans-5972
  prefs: []
  type: TYPE_NORMAL
  zh: 其中B = B+ ˆ λI，λ ≥ 0是一个标量参数，决定了阻尼的“强度”。计算Bˆ与向量的乘积是直接的，因为Bˆv = (B + λI)v = Bv
    + λv。
- en: As λ → ∞, the damped curvature matrix Bˆ tends to a multiple of the identity
    and the minimizer δ∗ has the property that δ∗ → ∇f(θ)/λ, meaning the overall optimization
    process reduces to gradient descent with a particular learning rate.
  id: totrans-5973
  prefs: []
  type: TYPE_NORMAL
  zh: 当λ → ∞时，阻尼曲率矩阵Bˆ趋向于单位矩阵的一个倍数，最小化器δ∗具有δ∗ → ∇f(θ)/λ的特性，这意味着整体优化过程简化为具有特定学习率的梯度下降。
- en: To better understand the effect of the Tikhonov damping, note that the addition
    of a scalar multiple of the identity matrix to B has the effect of increasing
    each of the eigenvalues by precisely λ. This can be seen by noting that if B =
    VΣV  where V = [v1|v2| ... |vn] are eigenvectors of B (which are orthonormal since
    B is symmetric), and Σ ≡ diag(λ1, λ2*,...,λ*n) the diagonal matrix of eigenvalues,
    then B = ˆ VΣV  + λI = VΣV  + *λV V*  = V (Σ + λI)V . Thus the curvature associated
    with each eigenvector vj in the damped matrix is given by vj Bˆvj = λj + λ.
  id: totrans-5974
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解Tikhonov阻尼的影响，注意到将一个标量倍数的单位矩阵添加到B中，效果是每个特征值都会恰好增加λ。这可以通过注意到如果B = VΣV，其中V
    = [v1|v2| ... |vn]是B的特征向量（由于B是对称的，因此是正交归一的），而Σ ≡ diag(λ1, λ2*,...,λ*n)是特征值的对角矩阵，因此B
    = ˆ VΣV + λI = VΣV + *λV V* = V (Σ + λI)V。因此，与每个特征向量vj相关的曲率在阻尼矩阵中由vj Bˆvj = λj
    + λ给出。
- en: This modulation of the curvature has profound effect on the inverse of Bˆ since
    B = ˆ V (Σ + λI)−1V , where (Σ + λI)−1 =
  id: totrans-5975
  prefs: []
  type: TYPE_NORMAL
  zh: 曲率的这种调制对Bˆ的逆有深远的影响，因为B = ˆ V (Σ + λI)−1V，其中(Σ + λI)−1 =
- en: diag (λ1 + λ)−1,(λ2 + λ)−1*,...,*(λn + λ)−1 and this will be particularly significant
    for λj 's that are small compared to λ, since (λj + λ)−1 will generally be much
    smaller than λ−1 j in such cases.
  id: totrans-5976
  prefs: []
  type: TYPE_NORMAL
  zh: diag (λ1 + λ)−1,(λ2 + λ)−1*,...,*(λn + λ)−1，这在λj相对于λ较小时尤为重要，因为在这种情况下，(λj + λ)−1通常会比λ−1
    j小得多。
- en: 3 A name which we will use to avoid confusion with the other meaning of term
    regularization in the learning context.
  id: totrans-5977
  prefs: []
  type: TYPE_NORMAL
  zh: 3 这是我们将用来避免与学习背景中术语正则化的其他含义混淆的名称。
- en: The effect on the minimizer δ∗ of Mˆ can be seen by noting that
  id: totrans-5978
  prefs: []
  type: TYPE_NORMAL
  zh: Mˆ对最小化器δ∗的影响可以通过注意到看出
- en: $$\delta^{*}=-\sum_{j}\frac{v_{j}^{\top}\nabla f(\theta_{k-1})}{\lambda_{j}+\lambda}v_{j}$$
  id: totrans-5979
  prefs: []
  type: TYPE_NORMAL
  zh: $$\delta^{*}=-\sum_{j}\frac{v_{j}^{\top}\nabla f(\theta_{k-1})}{\lambda_{j}+\lambda}v_{j}$$
- en: so the distance vj δ∗ that δ∗ moves θ in the direction vj will be effectively
    multiplied by λj λj + λ. Thus, Tikhonov damping should be appropriate when the
    quadratic model is most untrustworthy along directions of very low-curvature
  id: totrans-5980
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，δ∗在方向vj上移动的距离vj δ∗将有效地被λj λj + λ所放大。因此，当二次模型在非常低曲率的方向上最不可靠时，Tikhonov阻尼应是合适的。
- en: (along which δ∗ will tend to travel very far in the absence of damping).
  id: totrans-5981
  prefs: []
  type: TYPE_NORMAL
  zh: （在没有阻尼的情况下，δ∗将倾向于向远处移动）。
- en: Picking a good value of λ is critical to the success of a Tikhonov damping approach.
    Too high, and the update will resemble gradient descent with a very small learning
    rate and most of the power of 2nd-order optimization will be lost, with the low-curvature
    directions particularly affected. Conversely, if λ is too small, the quadratic
    model Mˆ will be too aggressively optimized by CG, resulting in a very large parameter
    update (particular in directions of low curvature) which may cause an increase
    in f instead of a decrease. Unfortunately, determining a good value of λ is a
    nontrivial problem, which is sensitive to the overall scale of the objective function
    (i.e. using λ = 1 for f gives the same update as λ = 2 would for 2f), and other
    more subtle properties of f, many of which will vary over the parameter space.
    It is in fact very rarely the case that a single value of λ will be appropriate
    at all θ's.
  id: totrans-5982
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个合适的λ值对Tikhonov阻尼方法的成功至关重要。如果λ值过高，更新将类似于具有非常小学习率的梯度下降，第二阶优化的大部分能力将会丧失，尤其是低曲率方向受到的影响较大。相反，如果λ值过小，二次模型Mˆ将被CG过于激进地优化，导致参数更新非常大（特别是在低曲率方向），这可能会导致f的增加而不是减少。不幸的是，确定一个合适的λ值是一个复杂的问题，敏感于目标函数的整体规模（即，对于f使用λ
    = 1与对于2f使用λ = 2将产生相同的更新），以及f的其他更微妙的属性，其中许多将在参数空间中变化。实际上，很少有单一的λ值能在所有θ上都适用。
- en: A method for dynamically adapting λ during optimization, which we have found
    works reasonably well in practice, will be discussed in section 20.8.5. Note that
    Tikhonov damping is the method used by LeCun et al. [21], where the constant "μ"
    (which is not adapted) plays the role of λ.
  id: totrans-5983
  prefs: []
  type: TYPE_NORMAL
  zh: 一种在优化过程中动态调整λ的方法，我们发现这种方法在实践中效果相当不错，将在第20.8.5节中讨论。请注意，Tikhonov阻尼是LeCun等人使用的方法[21]，其中常数"μ"（未进行调整）充当λ的角色。
- en: It is worth noting that Vinyals and Povey [33] have recently developed an alternative
    approach to Tikhonov damping, based on the idea of directly optimizing f over
    a K-dimensional Krylov basis generated by CG (or equivalently a Lanczos iteration).
    Because the Krylov subspace generated using a B=B+ ˆ λI doesn't depend on λ (assuming
    a CG initialization of x0 = 0), this method searches over a space of solutions
    that contain all those which would be found by optimizing a Tikhonov-damped Mˆ
    for some λ. Because of this it can find solutions which will give more reduction
    in f than CG could obtain for any value of λ. The downsides of the approach are
    that the searching must be performed using a general-purpose 2nd-order optimizer
    like BFGS, which will require extra gradient and function evaluations, that a
    basis for the entire Krylov subspace must be stored in memory (which may not always
    be practical when n is large), and finally that CG initializations cannot influence
    the construction of the Krylov subspace.
  id: totrans-5984
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，Vinyals 和 Povey [33] 最近开发了一种替代的 Tikhonov 阻尼方法，基于直接优化 f 在由 CG 生成的 K 维
    Krylov 基上的思想（或等效的 Lanczos 迭代）。因为使用 B=B+ ˆ λI 生成的 Krylov 子空间不依赖于 λ（假设 CG 初始化为 x0
    = 0），该方法在解空间中搜索，包含通过优化某些 λ 的 Tikhonov 阻尼 Mˆ 所能找到的所有解。由于这个原因，它可以找到比 CG 在任何 λ 值下获得的更大减少
    f 的解。该方法的缺点是，搜索必须使用通用的二阶优化器，如 BFGS，这将需要额外的梯度和函数评估，并且整个 Krylov 子空间的基必须存储在内存中（当
    n 较大时，这可能并不总是实用），最后 CG 初始化不能影响 Krylov 子空间的构建。
- en: 20.8.2 Problems With Tikhonov Damping
  id: totrans-5985
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.8.2 Tikhonov 阻尼的问题
- en: For standard parameterizations of neural networks, where entries of the weightmatrices
    and bias vectors are precisely the entries of θ, and the regularization is the
    standard spherical L2 penalty βθ2, Tikhonov damping appears to be a natural choice,
    and works pretty well in practice. This is because for certain nicely behaved
    and also useful areas of the parameter space, the effective scale at which each
    parameter operates is (very) roughly equal. But imagine a simple reparameterization
    of a FNN so that at some particular layer j, θ parameterizes 104Wj instead of
    Wj . Now the objective function is 104 times more sensitive than it was before
    to changes in the parameters associated with layer j and only layer j, and imposing
    a Tikhonov damping penalty consisting of an equally weighted sum of squared changes
    over all entries of θ (given by λ/2δ2 = λ/2ni=1 δ2i )
  id: totrans-5986
  prefs: []
  type: TYPE_NORMAL
  zh: 对于神经网络的标准参数化，其中权重矩阵和偏置向量的条目恰好是 θ 的条目，正则化是标准的球形 L2 惩罚 βθ2，Tikhonov 阻尼似乎是一个自然的选择，并且在实践中效果很好。这是因为对于参数空间中某些表现良好且有用的区域，每个参数操作的有效尺度是（非常）粗略相等的。但想象一下对
    FNN 的简单重参数化，以便在某一特定层 j，θ 参数化为 104Wj 而不是 Wj。现在目标函数对与层 j 相关的参数变化的敏感性是之前的 104 倍，并且施加
    Tikhonov 阻尼惩罚，由 θ 的所有条目的平方变化的等权重和给出（由 λ/2δ2 = λ/2ni=1 δ2i）。
- en: no longer seems like a good idea.
  id: totrans-5987
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎不再是一个好主意。
- en: For an even more extreme example, consider the case where we would like to constrain
    some of the weights of the network to be positive, and do this by a simple reparameterization
    via the exp function, so that for each component
  id: totrans-5988
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个更极端的例子，考虑我们希望约束网络的一些权重为正，并通过简单的重参数化 via exp 函数来实现，因此对于每个分量
- en: '[θ]i of θ corresponding to one of these weights w, we have w = exp([θ]i) instead
    w = [θ]i. By applying the chain rule we see that in the new parameterization,
    the i-th component of the gradient, and the i-th row and column of the GGN'
  id: totrans-5989
  prefs: []
  type: TYPE_NORMAL
  zh: '[θ]i 对应于这些权重中的一个 w，我们有 w = exp([θ]i)，而不是 w = [θ]i。通过应用链式法则，我们看到在新的参数化中，梯度的第
    i 个分量，以及 GGN 的第 i 行和第 i 列'
- en: matrix are both effectively multiplied by exp([θ]i), resulting in the update
    δ∗
  id: totrans-5990
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵实际上都被 exp([θ]i) 乘以，从而导致更新 δ∗
- en: changing by a factor exp([θ]i)−1 in entry i.
  id: totrans-5991
  prefs: []
  type: TYPE_NORMAL
  zh: 在条目 i 中变化的因子为 exp([θ]i)−1。
- en: 'More formally, if we define C ∈ Rn×n to be Jacobian of the function φ which
    maps the new parameters back to the default ones, then the gradient and GGN matrix
    in the new parameterization can be expressed in terms of those from the original
    parameterization as C∇f and CGC respectively4. The optimal update thus becomes:'
  id: totrans-5992
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，如果我们定义 C ∈ Rn×n 为函数 φ 的雅可比，它将新参数映射回默认参数，那么在新的参数化中的梯度和 GGN 矩阵可以用原始参数化中的梯度和
    GGN 矩阵来表示为 C∇f 和 CGC，分别4。因此，最优更新变为：
- en: δ∗ = (CBC)
  id: totrans-5993
  prefs: []
  type: TYPE_NORMAL
  zh: δ∗ = (CBC)
- en: −1C∇f = C−B−1∇f For our particular example, C is a diagonal matrix satisfying
    [C]i,i = exp([θ]i) for reparameterized entries of θ, and [C]i,i = 1 for the rest.
  id: totrans-5994
  prefs: []
  type: TYPE_NORMAL
  zh: −1C∇f = C−B−1∇f 在我们的特定示例中，C是一个对角矩阵，满足重新参数化的θ条目中[C]i,i = exp([θ]i)，其余条目中[C]i,i
    = 1。
- en: Assuming that the original 2nd-order update was a reasonable one in the original
    parameterization, the 2nd-order update as computed in the new parameterization
    should also reasonable (when taken in the new parameterization). In particular,
    a reparameterized weight w (and hence f) will become exponentially more sensitive
    to changes in [θ]i as [θ]i itself grows, and exponentially less sensitive as it
    shrinks, so an extra multiplicative factor of exp([θ]i)−1 compensates for this
    nicely. This should be contrasted with gradient descent, where the update will
    change in exactly the opposite way (being multiplied by exp([θ]i)) thus further
    compounding the sensitivity problem.
  id: totrans-5995
  prefs: []
  type: TYPE_NORMAL
  zh: 假设原始的二阶更新在原参数化中是合理的，在新的参数化中计算的二阶更新也应该是合理的（当在新的参数化中考虑时）。特别地，重新参数化的权重w（因此是f）在[θ]i增大时对[θ]i的变化变得指数级敏感，而在[θ]i缩小时则变得指数级不敏感，因此exp([θ]i)−1的额外乘法因子很好地补偿了这一点。这与梯度下降形成对比，后者的更新将以完全相反的方式变化（乘以exp([θ]i)），从而进一步加剧敏感性问题。
- en: Unfortunately, if we use standard Tikhonov damping directly in the reparameterized
    space, the assumption that all parameters operate at similar scales will be strongly
    violated, and we will lose the nice self-rescaling property of our update. For
    example, the curvature associated with [θ]i, which is equal to the curvature for
    w multiplied by exp([θ]i)2, may be completely overwhelmed by the addition of λ
    to the diagonal of G when [θ]i is below zero, resulting in an update which will
    fail to make a substantial change in [θ]i. Conversely, if [θ]i is large
  id: totrans-5996
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，如果我们在重新参数化空间中直接使用标准的Tikhonov阻尼，所有参数在类似尺度上运行的假设将被严重违反，我们将失去更新的良好自我缩放特性。例如，与[θ]i相关的曲率等于w的曲率乘以exp([θ]i)²，当[θ]i小于零时，λ对G的对角线的添加可能会完全压倒这部分曲率，从而导致一个更新未能对[θ]i产生实质性变化。相反，如果[θ]i很大，
- en: 4 Note that this result holds for smooth and invertible φ, as long as we use
    the GGN
  id: totrans-5997
  prefs: []
  type: TYPE_NORMAL
  zh: 4 注意，这个结果适用于平滑且可逆的φ，只要我们使用GGN
- en: matrix. If we use the Hessian, it holds only if φ is affine.
  id: totrans-5998
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵。如果我们使用Hessian，仅在φ是仿射的情况下才成立。
- en: then the Tikhonov damping contribution won't properly penalize large changes
    to [θ]i which may lead to a very large and untrustworthy update.
  id: totrans-5999
  prefs: []
  type: TYPE_NORMAL
  zh: 那么Tikhonov阻尼贡献将不会正确惩罚对[θ]i的大幅变化，这可能导致一个非常大且不可靠的更新。
- en: We could hope that a sensible scheme for adapting λ would compensate by adjusting
    λ in proportion with exp([θ]i), but the issue is that there are many other components
    of θ, such as other exp-reparameterized weights, and these may easily be small
    or larger than [θ]i, and thus operate at vastly different scales. In practice,
    what will mostly likely happen is that any sensible scheme for dynamically adjusting
    λ will cause it to increase until it matches the scale of the largest of these
    reparameterized weights, resulting in updates which make virtually no changes
    to the other weights of the network.
  id: totrans-6000
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以希望一个合理的λ调整方案通过与exp([θ]i)成比例地调整λ来进行补偿，但问题在于，θ还有许多其他组件，例如其他exp重新参数化的权重，这些权重可能很小或大于[θ]i，从而在完全不同的尺度上运行。实际上，最有可能发生的情况是，任何合理的动态调整λ的方案将导致λ增加，直到其与这些重新参数化权重中的最大值匹配，从而导致的更新几乎不会对网络的其他权重产生影响。
- en: In general, Tikhonov and any of the other quadratic penalty based damping methods
    we will discuss in the following sections, can all be made arbitrarily strong
    through the choice of λ, thus constraining the optimization of Mˆ to a degree
    sufficient to ensure that the update will not leave the region where M is a sensible
    approximation. What differentiates good approaches from bad ones is how they weigh
    different directions relative to each other. Schemes that tend to assign more
    weight to directions associated with more serious violations of the approximation
    quality of M will get away with using smaller values of λ, thus allowing the sub-optimization
    of Mˆ to be less constrained and thus produce larger and more useful updates to
    θ.
  id: totrans-6001
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，Tikhonov 及我们将在接下来的章节中讨论的任何其他基于二次惩罚的阻尼方法，都可以通过选择 λ 变得任意强，从而在一定程度上约束 Mˆ 的优化，以确保更新不会离开
    M 是合理近似的区域。区分好的方法和坏的方法在于它们如何相对权衡不同方向。倾向于给与与 M 的近似质量更严重违反相关方向赋予更多权重的方案，可以使用更小的
    λ 值，从而使 Mˆ 的子优化约束更少，从而产生更大且更有用的 θ 更新。
- en: 20.8.3 Scale-Sensitive Damping
  id: totrans-6002
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.8.3 尺度敏感阻尼
- en: 'The scale sensitivity of the Tikhonov damping is similar to the scale sensitivity
    that plagues 1st-order methods, and is precisely the type of issue we would like
    to avoid by moving to 2nd-order methods. Tikhonov damping makes the same implicit
    assumptions about scale that are made by first-order methods: that the default
    norm · on Rn is a reasonable way to measure change in θ and a reasonable quantity
    to penalize when searching for a suitable update to θ. 1storder methods can even
    be viewed as a special case of 2nd-order methods where the curvature term is given
    entirely by a Tikhonov-type damping penalty, so that B =ˆ λI and δ∗ = −1/λ∇f(θ).'
  id: totrans-6003
  prefs: []
  type: TYPE_NORMAL
  zh: Tikhonov 阻尼的尺度敏感性类似于困扰一阶方法的尺度敏感性，正是我们希望通过转向二阶方法来避免的问题。Tikhonov 阻尼对尺度做出了与一阶方法相同的隐含假设：即
    Rn 上的默认范数 · 是测量 θ 变化的合理方式，也是搜索 θ 合适更新时合理的惩罚量。一阶方法甚至可以视为二阶方法的特例，其中曲率项完全由 Tikhonov
    类型的阻尼惩罚给出，因此 B =ˆ λI 和 δ∗ = −1/λ∇f(θ)。
- en: One solution to this problem is to only use parameterizations which exhibit
    approximately uniform sensitivity properties, but this is limiting and it may
    be hard to tell at-a-glance if such a property holds for a particular network
    and associated parameterization.
  id: totrans-6004
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是仅使用具有大致均匀敏感性特性的参数化，但这有限制，并且可能很难一眼看出这种特性是否适用于特定的网络及其相关参数化。
- en: A potential way to address this problem is to use a quadratic penalty function
    which depends on the current position in parameter space (θk−1) and is designed
    to better respect the local scale properties of f at θk−1. In particular, instead
    of adding the penalty term λ/2δ2 to M we may instead add λ/2δ2Dk−1 =
  id: totrans-6005
  prefs: []
  type: TYPE_NORMAL
  zh: 解决此问题的一种潜在方法是使用依赖于参数空间当前位点 (θk−1) 的二次惩罚函数，并旨在更好地尊重 θk−1 处 f 的局部尺度特性。具体而言，我们可以选择不向
    M 添加惩罚项 λ/2δ2，而是添加 λ/2δ2Dk−1 =
- en: λ/2δDk−1δ, where Dk−1 is some symmetric positive definite (PD) matrix that depends
    on θk−1. Such a term may provide a more meaningful measure of change in θ, by
    accounting for the sensitivity properties of f more precisely. We call the matrix
    Dk−1, the damping matrix, and will drop the subscript k − 1 for brevity.
  id: totrans-6006
  prefs: []
  type: TYPE_NORMAL
  zh: λ/2δDk−1δ，其中 Dk−1 是一个依赖于 θk−1 的对称正定 (PD) 矩阵。这样的项可能通过更精确地考虑 f 的敏感性特性，提供对 θ 变化更有意义的度量。我们将矩阵
    Dk−1 称为阻尼矩阵，并为了简洁省略下标 k − 1。
- en: Scale sensitive damping is implemented in HF by working with a "damped" curvature
    matrix given B=B+ ˆ λD, where the required matrix-vector products can be computed
    using as Bˆv = Bv + λDv, assuming an efficient algorithm for computing matrix-vector
    products with D.
  id: totrans-6007
  prefs: []
  type: TYPE_NORMAL
  zh: 尺度敏感阻尼在 HF 中通过使用“阻尼”曲率矩阵 B=B+ ˆ λD 实现，其中所需的矩阵-向量乘积可以使用 Bˆv = Bv + λDv 计算，假设有一个有效的算法来计算与
    D 的矩阵-向量乘积。
- en: A specific damping matrix which may work well in the case of the expreparameterized
    network discussed in the previous sub-section would be D =
  id: totrans-6008
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一小节中讨论的表达参数化网络的特定阻尼矩阵可能是 D =
- en: 'CC (for a definition of C, see the previous sub-section). With such a choice
    we find that the update δ∗ produced by fully optimizing Mˆ is equal to C−1 times
    the update which would have been obtained with the original parameterization and
    standard Tikhonov damping with strength λ. Similarly to the undamped case, this
    is true because:'
  id: totrans-6009
  prefs: []
  type: TYPE_NORMAL
  zh: CC（有关C的定义，请参见前一小节）。通过这种选择，我们发现，通过完全优化Mˆ生成的更新δ∗等于C−1乘以使用原始参数化和强度为λ的标准Tikhonov阻尼所获得的更新。与无阻尼情况类似，这是因为：
- en: $$(C^{\top}\mathrm{B}C+\lambda C^{\top}C)^{-1}C^{\top}g=C^{-1}(\mathrm{B}+\lambda
    I)^{-1}C^{-\top}C^{\top}g=C^{-1}(\mathrm{B}+\lambda I)^{-1}g$$
  id: totrans-6010
  prefs: []
  type: TYPE_NORMAL
  zh: $$(C^{\top}\mathrm{B}C+\lambda C^{\top}C)^{-1}C^{\top}g=C^{-1}(\mathrm{B}+\lambda
    I)^{-1}C^{-\top}C^{\top}g=C^{-1}(\mathrm{B}+\lambda I)^{-1}g$$
- en: It should also be noted that this choice of damping matrix corresponds to a
    penalty function 12 δ2CC which is precisely the Gauss-Newton approximation of
    λ2 θ†2 = λ2 φ(θ)2 w.r.t. to the new parameters θ, where θ† = φ(θ) are the default/original
    parameters. The interpretation is that we are penalizing change in the original
    parameters (which are assumed to have a very roughly uniform scale), despite performing
    optimization w.r.t. the new ones.
  id: totrans-6011
  prefs: []
  type: TYPE_NORMAL
  zh: 还应该注意，这种阻尼矩阵的选择对应于惩罚函数12 δ2CC，正是λ2 θ†2 = λ2 φ(θ)2在新参数θ下的高斯-牛顿近似，其中θ† = φ(θ)是默认/原始参数。其解释是，我们对原始参数的变化进行惩罚（假设这些参数具有大致均匀的尺度），尽管是在新参数下进行优化。
- en: While we were able to design a sensible custom scheme in this example, exploiting
    the fact that the default parameterization of a neural network gives parameters
    which tend to operate at approximately similar scales (in most areas of the parameter
    space anyway), it would be nice to have a more generic and self-adaptive approach
    in the cases where we do not have such a property.
  id: totrans-6012
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们能够在这个例子中设计出合理的自定义方案，利用神经网络的默认参数化提供的参数在大多数参数空间区域内大致以相似的尺度操作的事实，但在没有这种属性的情况下，拥有更通用和自适应的方法会更好。
- en: One possible approach is to set D to be the diagonal matrix formed by taking
    the diagonal of B (i.e. D = diag(diag(B))), a choice made in the classical Levenberg-Marquardt
    algorithm. With this choice, the update δ†∗ produced by fully optimizing the damped
    quadratic Mˆ will be invariant to diagonal linear reparameterizations of θ.
  id: totrans-6013
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的方法是将D设为通过取B的对角线形成的对角矩阵（即D = diag(diag(B)）），这是在经典的Levenberg-Marquardt算法中所做的选择。选择此D时，通过完全优化阻尼二次Mˆ生成的更新δ†∗将对θ的对角线性重新参数化不变。
- en: Another nice property of this choice of D is that it produces an update which
    is invariant to rescaling of f (i.e. optimizing βf instead of f for some β > 0).
    By contrast, a pure Tikhonov damping scheme would rely on the careful adjustment
    of λ to achieve such an invariance.
  id: totrans-6014
  prefs: []
  type: TYPE_NORMAL
  zh: 选择D的另一个良好特性是，它产生的更新对f的重缩放不变（即优化βf而不是f，β > 0）。相比之下，纯Tikhonov阻尼方案将依赖于λ的精细调整以实现这种不变性。
- en: One obvious way to overcome the deficiencies of a damping approach based on
    a diagonal matrix would be to use a non-diagonal one, such as the original curvature
    matrix B itself. Such a choice for D produces updates that share all of the desirable
    invariance properties associated with a pure undamped Newton approach (assuming
    full optimization of Mˆ ), such as invariance to *arbitrary* linear reparameterizations,
    and rescalings of f. This is because with this choice, the damping-modified curvature
    matrix Bˆ is simply (1 + λ)B, and if we assume either full optimization of Mˆ
    , or partial optimization via a run of CG initialized from 0, this type of damping
    has the effect of simply rescaling the update δ by a factor of 1/(1 + δ). In section
    20.8.8 we will discuss line-search methods which effectively accomplish the same
    type of rescaling.
  id: totrans-6015
  prefs: []
  type: TYPE_NORMAL
  zh: 克服基于对角矩阵的阻尼方法缺陷的一种明显方式是使用非对角矩阵，例如原始曲率矩阵B本身。选择D的这种方式产生的更新具有与纯无阻尼牛顿方法相关的所有理想不变性特征（假设Mˆ的完全优化），如对*任意*线性重新参数化和f的重缩放不变。这是因为，在这种选择下，阻尼修正的曲率矩阵Bˆ仅为(1
    + λ)B，如果我们假设Mˆ的完全优化，或者通过从0初始化的CG运行进行部分优化，这种类型的阻尼的效果只是将更新δ按1/(1 + δ)的因子重新缩放。在20.8.8节中，我们将讨论有效实现同类型重缩放的线搜索方法。
- en: Despite the nice scale invariance properties associated with these choices,
    there are good reasons not to use either of them in practice, or at least to use
    them only with certain modifications, and in conjunction with other approaches.
  id: totrans-6016
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些选择具有良好的尺度不变性特性，但在实践中有充分理由不使用其中任何一种，或者至少只在某些修改下使用，并与其他方法结合。
- en: While the Tikhonov approach arguably makes too few assumptions about the local
    properties of f, damping approaches based on D = B or its diagonal may make too
    many. In particular, they make the same modeling assumptions as the original undamped
    quadratic approximation M itself. For example, B may not even be full-rank, and
    in such a situation it may be the case that M will predict unbounded improvement
    along some direction in B's nullspace, a problem which will not be handled by
    damping with D = B for any value of λ, no matter how big. Even if B is full-rank,
    there may be directions of near-zero curvature which can cause a less extreme
    version of the same problem. Since the diagonal B will usually be full-rank even
    when B isn't, or just better conditioned in general, using it instead may give
    some limited immunity to these kinds of problems, but it is far from an ideal
    solution, as demonstrated in fig. 20.8.3.
  id: totrans-6017
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Tikhonov 方法可以说对 f 的局部性质做了过少的假设，但基于 D = B 或其对角线的阻尼方法可能做了过多的假设。特别是，它们与原始未阻尼的二次近似
    M 自身做了相同的建模假设。例如，B 可能甚至不是满秩的，在这种情况下，M 可能会预测在 B 的零空间沿某个方向的无界改进，而对于任何值的 λ，使用 D =
    B 的阻尼将无法处理这个问题，无论 λ 多大。即使 B 是满秩的，也可能存在接近零曲率的方向，这会导致相同问题的较轻版本。由于对角线 B 通常即使在 B 不是满秩时也会是满秩的，或一般来说条件更好，因此使用它可能对这些问题提供有限的免疫，但这远不是理想的解决方案，如图
    20.8.3 所示。
- en: '![495_image_0.png](495_image_0.png)'
  id: totrans-6018
  prefs: []
  type: TYPE_IMG
  zh: '![495_image_0.png](495_image_0.png)'
- en: Fig. 20.3. A 2D toy example of how using D = diag(B) results in an overly restricted
    update. Let u = [−1, 1] and v = [1, 1], and let B be uu +avv where a is large
    (e.g.
  id: totrans-6019
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.3. 一个二维玩具示例，展示使用 D = diag(B) 会导致更新过于受限。设 u = [−1, 1]，v = [1, 1]，并且设 B 为
    uu + avv，其中 a 很大（例如矩阵）。
- en: 104, although we take a = 15 for display purposes). This matrix is full rank,
    and its diagonal entries are given by [a + 1, a + 1], representing the fact that
    the quadratic is highly sensitive to independent changes to the 2 parameters.
    The small circular region is where the update will be effectively restricted to
    when we make λ large enough.
  id: totrans-6020
  prefs: []
  type: TYPE_NORMAL
  zh: 104，尽管我们为显示目的设 a = 15）。这个矩阵是满秩的，其对角元素为 [a + 1, a + 1]，表示二次形式对两个参数的独立变化高度敏感。小圆形区域是当我们将
    λ 设得足够大时，更新将有效限制在此区域内。
- en: In order to explain such degeneracies and understand why choices like D = B
  id: totrans-6021
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释这些简并现象并理解为什么选择 D = B。
- en: can be bad, it is useful to more closely examine and critique our original choices
    for making them. The quadratic approximation breaks down due to higher-order effects
    (and even certain unmodelled 2nd-order effects in the case of the GGN
  id: totrans-6022
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可能是有害的，但仔细审视和批判我们最初的选择是有用的。二次近似由于高阶效应而失效（在 GGN 的情况下甚至某些未建模的二阶效应）。
- en: matrix) and it is the goal of damping to help compensate for this. By taking
    D = B we are penalizing directions according to their curvature, and so are in
    some sense assuming that the relative strength of the contribution to f from the
    high-order terms (and thus the untrustworthiness of M) along two different directions
    can be predicted reasonably well by looking at the ratio of their respective curvatures.
    And while there is a tendency for this to be true for certain objective functions,
    making this assumption too strongly may be dangerous.
  id: totrans-6023
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是通过阻尼来补偿这个问题。通过取 D = B，我们根据其曲率对方向施加惩罚，因此在某种意义上假设高阶项对 f 的贡献相对强度（以及 M 的不可信度）沿两个不同方向的比率可以被合理预测。虽然对某些目标函数来说，这种假设可能是正确的，但过于强烈地做出这种假设可能是危险的。
- en: Unfortunately, in the absence of any other information about the semi-local
    behavior of the function f, it may not always be clear what kind of assumption
    we should fall back on. To move towards the uniform scale assumption implied by
    the Tikhonov approach by choosing D to be some interpolation between the diagonal
    of B and a multiple of the identity (e.g. using the methods discussed in 20.11.2)
    seems like an arbitrary choice, since in general there may not be anything particularly
    special or natural about whatever default parameterization of f that we are given
    to work with. Despite this, such a strategy can be reasonably effective in some
    situations, and a reasonable compromise between Tikhonov damping and damping with
    B.
  id: totrans-6024
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在缺乏关于函数 f 半局部行为的其他信息的情况下，可能并不总是清楚我们应回归于什么样的假设。为了朝着 Tikhonov 方法所暗示的均匀尺度假设前进，选择
    D 为 B 的对角线和单位矩阵的某种插值（例如，使用 20.11.2 中讨论的方法）似乎是一个任意的选择，因为一般来说，对于我们给定的 f 的默认参数化，可能没有特别特殊或自然的东西。尽管如此，这种策略在某些情况下可以相当有效，并在
    Tikhonov 阻尼与 B 的阻尼之间取得合理的折中。
- en: A conceivably better approach would be to collect information about higherorder
    derivatives, or to use information collected and aggregated from previous iterations
    of the optimization process to build a simple model of the coarse geometric structure
    of f. Or perhaps some useful information could be gleaned from examining the structure
    of the computational graph of f. Unfortunately we are unaware of the existence
    of general methods for building D based on such ideas, and so this remains a direction
    for future research.
  id: totrans-6025
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能更好的方法是收集高阶导数的信息，或者利用从优化过程之前的迭代中收集和汇总的信息，建立一个简单的函数 f 粗几何结构模型。或者，也许可以通过检查函数
    f 的计算图结构获得一些有用的信息。不幸的是，我们不知道基于这些想法构建 D 的通用方法，因此这仍然是未来研究的方向。
- en: In the next section we discuss a method called "structural damping" which constructs
    D using knowledge of the particular structure of deep and recurrent neural networks,
    in order to construct damping matrices which may be better at selectively penalizing
    directions for the purposes of damping.
  id: totrans-6026
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们讨论了一种称为“结构阻尼”的方法，该方法利用深度和递归神经网络的特定结构构建 D，以构造可能更好地选择性惩罚阻尼方向的阻尼矩阵。
- en: 20.8.4 Structural Damping
  id: totrans-6027
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.8.4 结构阻尼
- en: Recurrent Neural Networks are known to be difficult to train with gradient descent,
    so it is conceivable that problematic variations in scale and curvature are responsible.
    Indeed, a direct application of the implementation of HF presented by Martens
    [22] to RNNs can yield reasonable results, performing well on a family of synthetic
    pathological problems [19, 23] designed to have very longrange temporal dependencies
    of up to 100 time-steps. However Martens and Sutskever [23] found that performance
    could be made substantially better and more robust using an idea called "structural
    damping".
  id: totrans-6028
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络被认为难以通过梯度下降训练，因此可以想象规模和曲率的有问题变化是导致这种情况的原因。实际上，将 Martens [22] 提出的 HF 实现直接应用于
    RNNs 可以得到合理的结果，在一类设计用于具有长达 100 时间步的长期时间依赖的合成病态问题上表现良好 [19, 23]。然而，Martens 和 Sutskever
    [23] 发现，使用一种称为“结构阻尼”的想法可以显著改善性能并提高鲁棒性。
- en: Martens and Sutskever [23] found that a basic Tikhonov damping approach performed
    poorly when applied to training RNNs. In particular, in order to avoid very large
    and untrustworthy update proposals, they found λ needed to be very high, and this
    in turn would lead to much slower optimization. This need for a large λ can be
    explained by the extreme sensitivity of the RNN's long sequence of hidden states
    to changes in the parameters and in particular the hidden dynamics matrix Whh.
    While these sorts of problems exist with deep feed-forward neural networks like
    the autoencoders considered in Hinton and Salakhutdinov [17] and Martens [22],
    the situation with RNNs is much more extreme, since they have many more effective
    "layers", and their parameters are applied repeatedly at every time-step and can
    thus have a dramatic effect on the entire hidden state sequence [4, 18]. Due to
    this extreme and highly non-linear sensitivity, local quadratic approximations
    are likely to be highly inaccurate in certain directions in parameter space, even
    over very small distances. A Tikhonov damping approach can only compensate for
    this by imposing a strict penalty against changes in all directions, since it
    lacks any mechanism to be more selective.
  id: totrans-6029
  prefs: []
  type: TYPE_NORMAL
  zh: Martens 和 Sutskever [23] 发现基本的 Tikhonov 阻尼方法在训练 RNN 时表现不佳。特别是，为了避免非常大且不可靠的更新提议，他们发现
    λ 需要非常高，这反过来会导致优化速度大幅减慢。对大 λ 的需求可以通过 RNN 的长序列隐藏状态对参数变化的极端敏感性来解释，尤其是隐藏动态矩阵 Whh。虽然这些问题在
    Hinton 和 Salakhutdinov [17] 以及 Martens [22] 考虑的自编码器等深度前馈神经网络中也存在，但 RNN 的情况更加极端，因为它们具有更多的有效“层”，其参数在每个时间步都反复应用，因此可以对整个隐藏状态序列产生重大影响[4,
    18]。由于这种极端且高度非线性的敏感性，局部二次近似在参数空间的某些方向上可能在非常小的距离内也是高度不准确的。Tikhonov 阻尼方法只能通过对所有方向的变化施加严格惩罚来弥补这一点，因为它缺乏更具选择性的机制。
- en: Structural damping addresses this problem by imposing a quadratic penalty not
    just to changes in parameters, but also to cetain intermediate quantities that
    appear in the evaluation of f, such as the hidden state activities of an RNN.
    This allows us to be more selective in the way we penalize directions of change
    in parameter space, focusing on those that are more likely to lead to the large
    changes in the hidden state sequence, which due to their highly nonlinear nature,
    tend to correspond to catastrophic breakdowns in the accuracy of the quadratic
    approximation.
  id: totrans-6030
  prefs: []
  type: TYPE_NORMAL
  zh: 结构阻尼通过对参数变化施加二次惩罚，不仅限于参数的变化，还包括在评估 f 时出现的某些中间量，例如 RNN 的隐藏状态活动。这使我们能够在惩罚参数空间中变化的方向时更具选择性，集中于那些更可能导致隐藏状态序列大变化的方向，这些变化由于其高度非线性，往往对应于二次近似的准确性发生灾难性崩溃。
- en: Speculatively, structural damping may have another more subtle benefit for RNN
    learning. It is known [20, 23] that good random initializations give rise to nontrivial
    hidden state dynamics that can carry useful information about the past inputs
    even before any learning has taken place. So if an RNN is initialized carefully
    to contain such random dynamics, the inclusion of structural damping may encourage
    the updates to preserve them at least until the hidden-to-output weights have
    had some time to be adapted to the point where the long-range information contained
    in the hidden activities actually gets used to inform future predictions. After
    such a point, a locally greedy optimizer like HF will have more obvious reasons
    to preserve the dynamics.
  id: totrans-6031
  prefs: []
  type: TYPE_NORMAL
  zh: 从推测来看，结构阻尼可能对 RNN 学习有另一个更微妙的好处。已知[20, 23]良好的随机初始化会产生非平凡的隐藏状态动态，即使在没有进行任何学习之前，也能携带有关过去输入的有用信息。因此，如果
    RNN 被仔细初始化以包含这种随机动态，结构阻尼的引入可能会促使更新保留这些动态，至少在隐藏到输出的权重有一些时间适应到能够使用隐藏活动中的长程信息来影响未来预测的程度之后。在这样的时刻，像
    HF 这样的局部贪婪优化器会更明显地有理由保留这些动态。
- en: To formalize structural damping we first re-express the nonlinear objective
    f(θ) as a composition of functions L(z(h(θ), θ)), where h(θ) computes the hidden
    states (whose change we wish to penalize), z(*h, θ*) computes the outputs, and
    L(z) computes the loss.
  id: totrans-6032
  prefs: []
  type: TYPE_NORMAL
  zh: 为了规范结构阻尼，我们首先将非线性目标 f(θ) 重新表示为函数 L(z(h(θ), θ)) 的组合，其中 h(θ) 计算隐藏状态（我们希望惩罚其变化），z(*h,
    θ*) 计算输出，而 L(z) 计算损失。
- en: Given the current parameter setting θk−1, the local (undamped) quadratic approximation
    is given by M(δ) and its curvature is B. We prevent large changes in the hidden
    state by penalizing the distance between h(θk−1 + δ) and h(θk−1)
  id: totrans-6033
  prefs: []
  type: TYPE_NORMAL
  zh: 给定当前的参数设置θk−1，局部（无阻尼）二次近似由M(δ)给出，其曲率为B。我们通过惩罚h(θk−1 + δ)与h(θk−1)之间的距离来防止隐藏状态的大幅变化。
- en: according to the penalty function S(δ) = d(h(θk−1 + δ); h(θk−1))
  id: totrans-6034
  prefs: []
  type: TYPE_NORMAL
  zh: 根据惩罚函数S(δ) = d(h(θk−1 + δ); h(θk−1))
- en: where d is a distance function or loss function such as a squared error or the
    cross-entropy5.
  id: totrans-6035
  prefs: []
  type: TYPE_NORMAL
  zh: 其中d是距离函数或损失函数，如平方误差或交叉熵。
- en: 'Ideally, we would define the damped local objective as:'
  id: totrans-6036
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们将定义阻尼局部目标为：
- en: Mˆ †(δ) = M(δ) + μS(δ) + λI
  id: totrans-6037
  prefs: []
  type: TYPE_NORMAL
  zh: Mˆ †(δ) = M(δ) + μS(δ) + λI
- en: where μ is a strength constant similar to λ. But since we cannot minimize a
    non-quadratic objective with CG, we must resort to using a local quadratic approximation
    to S(δ). This will be given by δDk−1δ/2 where Dk−1 is the Gauss-Newton matrix
    of the penalty function S(δ) at δ = 0. Note that the quadratic approximation to
    S(δ) does not have a linear term because δ = 0 is a minimum of S.
  id: totrans-6038
  prefs: []
  type: TYPE_NORMAL
  zh: 其中μ是一个与λ相似的强度常数。但由于我们无法用CG最小化非二次目标，我们必须依赖于使用S(δ)的局部二次近似。这将由δDk−1δ/2给出，其中Dk−1是在δ
    = 0时惩罚函数S(δ)的高斯-牛顿矩阵。请注意，S(δ)的二次近似没有线性项，因为δ = 0是S的一个极小值。
- en: 'Fortunately, it is straightforward to multiply by the generalized Gauss-Newton
    matrix of S using the techniques outlined in section 20.6. Thus we could compute
    5 The cross-entropy is suitable when the hidden units use a logistic sigmoid nonlinearity
    the products Bv and μDk−1v using two separate Gauss-Newton matrix-vector products,
    adding together the results, approximately doubling the computational burden.
    In order to avoid this, we can instead compute the sum (Bk−1+μDk−1)v directly
    by exploiting the fact that S(δ) can be computed much more efficiently by reusing
    the h(θk + δ) which gets computed as an intermediate quantity for f(θk + δ). Indeed,
    consider a neural network whose output units include the hidden state as well
    as the predictions:'
  id: totrans-6039
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，使用第20.6节中概述的技术，将S的广义高斯-牛顿矩阵相乘是相对简单的。因此，我们可以计算5 交叉熵适合于隐藏单元使用逻辑 sigmoid 非线性，乘积Bv和μDk−1v可以通过两个单独的高斯-牛顿矩阵-向量乘积来计算，将结果相加，近似地加倍计算负担。为了避免这种情况，我们可以直接计算(Bk−1+μDk−1)v，利用S(δ)可以通过重用h(θk
    + δ)高效计算的事实，h(θk + δ)作为f(θk + δ)的中间量被计算。实际上，考虑一个神经网络，其输出单元包括隐藏状态以及预测：
- en: $$c(\theta)\equiv[h(\theta),z(h(\theta),\theta)]$$
  id: totrans-6040
  prefs: []
  type: TYPE_NORMAL
  zh: $$c(\theta)\equiv[h(\theta),z(h(\theta),\theta)]$$
- en: and whose loss function is given by Lμ(*h, y*) = μd(h; h(θk−1) + L(y), so that
    we have f(θ) + μS(θ) = Lμ(c(θ)). This "new" neural network includes structural
    damping in its loss, and any automatic routines computing the required Jacobian-vector
    products for c will be no more expensive than the analogous routines in the original
    network. Multiplication by the Hessian of the loss
  id: totrans-6041
  prefs: []
  type: TYPE_NORMAL
  zh: 其损失函数为Lμ(*h, y*) = μd(h; h(θk−1) + L(y)，因此我们有f(θ) + μS(θ) = Lμ(c(θ))。这个“新”神经网络在其损失中包括结构阻尼，任何计算c所需的雅可比-向量乘积的自动程序的成本都不会高于原始网络中的类似程序。通过损失的海森矩阵进行乘法
- en: Lμ =
  id: totrans-6042
  prefs: []
  type: TYPE_NORMAL
  zh: Lμ =
- en: L(y) 0 0 μd(h; h(θk−1)) is also easy and can be done block-wise.
  id: totrans-6043
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: L(y) 0 0 μd(h; h(θk−1))也很简单，可以分块进行。
- en: 20.8.5 The Levenberg-Marquardt Heuristic
  id: totrans-6044
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.8.5 勒文伯格-马夸特启发式
- en: For the penalty-based damping methods such as those described above to work
    well, λ (and perhaps also μ, as defined in the previous sub-section) must be constantly
    adapted to keep up with the changing local curvature properties of f.
  id: totrans-6045
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使上述惩罚基阻尼方法有效，λ（也许还有μ，如前一小节中定义的）必须不断适应，以跟上f的局部曲率特性的变化。
- en: Fortunately, we have found that the well-known Levenberg-Marquardt (LM)
  id: totrans-6046
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们发现著名的勒文伯格-马夸特（LM）方法
- en: heuristic, which is usually used in the context of the LM method [25] to be
    effective at adapting λ in a sensible way even in the context of the truncated
    CG runs that are used in HF.
  id: totrans-6047
  prefs: []
  type: TYPE_NORMAL
  zh: 启发式，通常用于LM方法的上下文中[25]，以便在HF中使用截断CG运行的背景下有效地以合理的方式调整λ。
- en: The key quantity behind the LM heuristic is the "reduction ratio", denoted by
    ρ, which is given by
  id: totrans-6048
  prefs: []
  type: TYPE_NORMAL
  zh: LM启发式背后的关键量是“减少比”，用ρ表示，给出如下
- en: $$\rho\equiv\frac{f(\theta_{k-1}+\delta_{k})-f(\theta_{k-1})}{M_{k-1}(\delta_{k})}$$
  id: totrans-6049
  prefs: []
  type: TYPE_NORMAL
  zh: $$\rho\equiv\frac{f(\theta_{k-1}+\delta_{k})-f(\theta_{k-1})}{M_{k-1}(\delta_{k})}$$
- en: $$(20.10)$$
  id: totrans-6050
  prefs: []
  type: TYPE_NORMAL
  zh: $$(20.10)$$
- en: The reduction ratio measures the ratio of the reduction in the objective f(θk−1+
    δk) − f(θk−1) produced by the update δk, to the amount of reduction predicted
    by the quadratic model. When ρ is much smaller than 1, the quadratic model overestimates
    the amount of reduction and so λ should be increased, encouraging future updates
    to be more conservative and thus lie somewhere that the quadratic model more accurately
    predicts the reduction. In contrast, when ρ is close to 1, the quadratic approximation
    is likely to be fairly accurate near δ∗, and so we can afford to reduce λ, thus
    relaxing the constraints on δk and allowing for "larger" and more substantial
    updates.
  id: totrans-6051
  prefs: []
  type: TYPE_NORMAL
  zh: 减少比率衡量由更新δk产生的目标f(θk−1+ δk) − f(θk−1)的减少量与二次模型预测的减少量之间的比率。当ρ远小于1时，二次模型会高估减少量，因此应该增加λ，鼓励未来的更新更加保守，从而使其处于二次模型更准确预测减少的区域。相反，当ρ接近1时，二次近似在δ∗附近可能相当准确，因此我们可以降低λ，从而放宽对δk的约束，允许“更大”和更实质的更新。
- en: 'The Levenberg-Marquardt heuristic is given by:'
  id: totrans-6052
  prefs: []
  type: TYPE_NORMAL
  zh: Levenberg-Marquardt启发式如下：
- en: $$\begin{array}{l l}{{1.}}&{{\mathrm{~If~}\rho>3/4}}\\ {{2.}}&{{\mathrm{~If~}\rho<1/4}}\end{array}$$
  id: totrans-6053
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{l l}{{1.}}&{{\mathrm{~如果~}\rho>3/4}}\\ {{2.}}&{{\mathrm{~如果~}\rho<1/4}}\end{array}$$
- en: 'If $\rho>3/4$ then $\lambda\gets\lambda2/3$ :   If $\rho<1/4$ then $\lambda\gets\lambda3/2$
    .'
  id: totrans-6054
  prefs: []
  type: TYPE_NORMAL
  zh: 如果$\rho>3/4$，那么$\lambda\gets\lambda2/3$；如果$\rho<1/4$，那么$\lambda\gets\lambda3/2$。
- en: Although the constants in the above description of the LM are somewhat arbitrary,
    we found them to work well in our experiments.
  id: totrans-6055
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述LM描述中的常数有些任意，但我们在实验中发现它们效果良好。
- en: Note that the above heuristic is valid in the situation where ρ < 0, which can
    arise in one of two ways. The first way is where Mk−1 < 0 and f(θk−1 + δk) − f(θk−1)
    > 0, which means that the quadratic approximation is very inaccurate around δ∗
    and doesn't even get the sign of the change right. The other possibility is that
    Mk−1 > 0 and f(θk−1 + δk) − f(θk−1) < 0, which can only occur if CG is initialized
    from a nonzero previous solution and doesn't make sufficient progress from that
    point to obtain a negative value Mk−1 before being terminated/truncated. When
    this occurs one should either allow CG to use more iterations or possibly initialize
    the next run of CG from 0 (as this will guarantee that Mk−1 < 0, since Mk−1(0)
    = 0 and CG decreases Mk−1 monotonically).
  id: totrans-6056
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，上述启发式在ρ < 0的情况下是有效的，这可以通过两种方式产生。第一种方式是Mk−1 < 0且f(θk−1 + δk) − f(θk−1) > 0，这意味着二次近似在δ∗附近非常不准确，甚至无法正确判断变化的符号。另一种可能是Mk−1
    > 0且f(θk−1 + δk) − f(θk−1) < 0，这只能在CG从非零的前一个解初始化并且从该点未能取得足够进展以获得负值Mk−1的情况下发生。在这种情况下，应允许CG使用更多迭代，或者可能从0初始化下一次CG运行（因为这将保证Mk−1
    < 0，因为Mk−1(0) = 0且CG单调减少Mk−1）。
- en: While the definition of ρ in eqn. 20.10 uses the undamped quadratic in the denominator,
    the damped quadratic approximation Mˆk−1 can also be used, and in our experience
    will give similar results, favoring only slightly lower values of λ.
  id: totrans-6057
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管方程20.10中ρ的定义使用了无阻尼二次方程作为分母，阻尼二次近似Mˆk−1也可以使用，并且根据我们的经验，会给出类似的结果，仅略微偏向较低的λ值。
- en: Because of this tendency for M to lose accuracy as CG iterates (see subsection
    20.8.7), the value of ρ tends to decrease as well (sometimes after an initial
    up-swing caused by using a non-zero initialization as in section 20.10). If CG
    were to run to convergence, the Levenberg-Marquardt heuristic would work just
    as it does in the classical Levenberg-Marquardt algorithm, which is to say, very
    effectively and giving provable strong local convergence guarantees. But the situation
    becomes more complicated when this heuristic is used in conjunction with updates
    produced by unconverged runs of CG, because the "optimal" value of λ, which the
    LM heuristic is trying to find, will be a function of how much progress CG tends
    to make when optimizing M.
  id: totrans-6058
  prefs: []
  type: TYPE_NORMAL
  zh: 由于M在CG迭代过程中失去准确性的倾向（见20.8.7小节），ρ的值也往往会下降（有时在使用非零初始化时会出现初始的上升，如20.10节所述）。如果CG运行到收敛，Levenberg-Marquardt启发式方法的效果与经典Levenberg-Marquardt算法相同，也就是说，非常有效，并提供可证明的强局部收敛保证。但当此启发式与由未收敛的CG运行产生的更新结合使用时，情况变得更加复杂，因为LM启发式试图找到的“最优”λ值将取决于CG在优化M时的进展程度。
- en: Fortunately, as long as the local properties of f change slowly enough, terminating
    CG according to a fixed maximum number of steps should result in a relatively
    stable and well-chosen value of λ.
  id: totrans-6059
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，只要f的局部特性变化足够缓慢，根据固定的最大步数终止CG应该会导致相对稳定且选择良好的λ值。
- en: But unfortunately, well intentioned methods which attempt to be smarter and
    terminate CG based on the value of f, for example, can run into problems caused
    by this dependency of the optimal λ on the performance of CG. In particular, this
    "smart" decision of when to stop CG will have an affect on ρ, which will affect
    the choice of λ via the LM heuristic, which will affect the damping and hence
    how the value of f evolves as CG iterates (at the next HF iteration), which will
    finally affect the decision of when to stop CG, bringing us full circle. It is
    this kind of feed-back which may result in unexpected and undesirable behaviors
    when using the LM heuristic, such as λ and the number of the length of the CG
  id: totrans-6060
  prefs: []
  type: TYPE_NORMAL
  zh: 但不幸的是，善意的方法试图更智能，并基于f的值终止CG，例如，可能会遇到由最优λ依赖于CG性能而引发的问题。特别是，何时停止CG的这种“聪明”决定将影响ρ，这将通过LM启发式影响λ的选择，进而影响阻尼，因此影响f的值如何在CG迭代中演变（在下一次HF迭代时），最终影响何时停止CG的决定，形成闭环。正是这种反馈可能导致使用LM启发式时出现意外和不良行为，例如λ和CG的长度。
- en: runs both going to zero as HF iterates, or both quantities creeping upwards
    to inappropriately large values.
  id: totrans-6061
  prefs: []
  type: TYPE_NORMAL
  zh: 在HF迭代过程中，两个量均趋近于零，或者两个量逐渐增加到不适当的大值。
- en: 20.8.6 Trust-Region Methods
  id: totrans-6062
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.8.6 信任域方法
- en: In contrast to damping methods that are based on penalty terms designed to encourage
    updates to be "smaller" according to some measure, trust region methods impose
    an explicit constraint on the optimization of the quadratic model M. Instead of
    performing unconstrained optimization on the (possibly damped)
  id: totrans-6063
  prefs: []
  type: TYPE_NORMAL
  zh: '与基于惩罚项的阻尼方法不同，这些方法旨在根据某种度量促使更新变得“更小”，信任域方法在优化二次模型M时施加了明确的约束。它并不是在（可能被阻尼的） '
- en: 'quadratic Mˆ , a constrained optimization is performed over M. This is referred
    to as the trust-region sub-problem, and is defined by:'
  id: totrans-6064
  prefs: []
  type: TYPE_NORMAL
  zh: 在二次函数Mˆ上，执行约束优化。这被称为信任域子问题，定义为：
- en: Δ∗R = Argminδ:Δ∈Rm(Δ)
  id: totrans-6065
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Δ∗R = Argminδ:Δ∈Rm(Δ)
- en: where R ⊆ Rn is some region localized around δ = 0 called the "trust-region".
  id: totrans-6066
  prefs: []
  type: TYPE_NORMAL
  zh: 其中R ⊆ Rn是围绕δ = 0局部化的某个区域，称为“信任域”。
- en: Ideally, R has the property that M remains a reasonable approximation to f for
    any δ ∈ R, without being overly restrictive. Or perhaps more weakly (and practically),
    that whatever update δ∗R is produced by solving the trust-region sub-problem will
    produce a significant reduction in f. Commonly, R is chosen to be a ball of radius
    r centered at 0, although it could just as easily be an ellipsoid or something
    more exotic, as long as the required constrained optimization can be performed
    efficiently enough.
  id: totrans-6067
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，R具有这样的特性，即M对于任何δ ∈ R仍然是f的合理近似，而不至于过于严格。或者可能更弱（在实际中），即通过解决信任域子问题产生的任何更新δ∗R将显著减少f。通常，R被选择为以0为中心，半径为r的球体，尽管它同样可以是椭球体或其他更特殊的形状，只要所需的约束优化能够高效地执行。
- en: 'There is a formal connection between trust region methods and penalty-based
    damping methods such as Tikhonov damping, which states that when R is an elliptical
    ball around 0 of radius r given R = {x : xQ ≤ r} for some positive definite matrix
    Q, then for each quadratic function M(δ) there exists a λ such that'
  id: totrans-6068
  prefs: []
  type: TYPE_NORMAL
  zh: '信任域方法与基于惩罚的阻尼方法（如Tikhonov阻尼）之间存在正式的联系，指出当R是以0为中心，半径为r的椭球体时，给定R = {x : xQ ≤
    r}，对于每个二次函数M(δ)，存在一个λ，使得'
- en: $$\operatorname{argmin}_{\delta:\delta\in R}M(\delta)=\operatorname{argmin}_{\delta}M(\delta)+{\frac{\lambda}{2}}\|\delta\|_{Q}^{2}$$
  id: totrans-6069
  prefs: []
  type: TYPE_NORMAL
  zh: $$\operatorname{argmin}_{\delta:\delta\in R}M(\delta)=\operatorname{argmin}_{\delta}M(\delta)+{\frac{\lambda}{2}}\|\delta\|_{Q}^{2}$$
- en: This result is valid for all quadratic functions M, even when B is indefinite,
    and can be proved using Lagrange multipliers.6 Trust-region methods have some
    appeal over the previously discussed penaltybased damping methods, because it
    may be easier to reason intuitively, and perhaps also mathematically, about the
    effect of an explicit trust-region (with a particular radius r) on the update
    than a quadratic penalty. Indeed, the trust region R is invariant to changes in
    the scale of the objective7, which may make it easier to tune, either manually
    or by some automatic method.
  id: totrans-6070
  prefs: []
  type: TYPE_NORMAL
  zh: 该结果适用于所有二次函数M，即使B是不定的，并且可以使用拉格朗日乘数法证明。信任域方法相对于前面讨论的基于惩罚的阻尼方法具有一定的吸引力，因为在明确的信任域（具有特定半径r）对更新的影响上，直观推理和数学推导可能更容易。实际上，信任域R对目标的尺度变化是不变的，这可能使得手动或通过某种自动方法调整变得更容易。
- en: However, the trust region optimization problem is much more difficult than the
    unconstrained quadratic optimization of Mˆ . It cannot be directly solved either
    by CG or by matrix inversion. Even in the case where a spherical trustregion with
    radius r is used, the previously discussed result is non-constructive and merely
    guarantees the existence of an appropriate λ that makes the exact minimizer of
    the Tikhonov damped Mˆ equal to the solution of the trust region sub-problem.
    Finding such a λ is a hard problem, and while there are algorithms that do this
    [26], they involve expensive operations such as full decompositions of
  id: totrans-6071
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，信任区域优化问题比无约束的Mˆ二次优化要困难得多。它不能通过CG或矩阵求逆直接解决。即使在使用半径为r的球形信任区域的情况下，前面讨论的结果也是非构造性的，仅仅保证存在一个合适的λ，使得Tikhonov阻尼的Mˆ的确切最小化器等于信任区域子问题的解。找到这样的λ是一个难题，尽管有一些算法能够做到这一点[26]，但它们涉及昂贵的操作，例如完全分解。
- en: '6 For completeness, we present an outline of the proof here: Consider the Lagrangian
    L(δ, λ) = M(δ)+(r − 12 δ*Qδ)λ*. It is known that there exists a λ∗ such that the
    minimizer of the trust region problem δ∗ satisfies ∂L(δ∗, λ∗)/∂(*δ, λ*)=0. For
    M(δ) ='
  id: totrans-6072
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们在这里给出证明的大纲：考虑拉格朗日函数L(δ, λ) = M(δ)+(r − 12 δ*Qδ)λ*。已知存在一个λ∗使得信任区域问题的最小化器δ∗满足∂L(δ∗,
    λ∗)/∂(*δ, λ*)=0。对于M(δ)=
- en: gδ−δBδ/2, this is equivalent to g−Bδ∗−λ∗Qδ∗ = 0, and thus (B+λ∗Q)δ∗ = g. If
    the matrix B+λ∗Q is positive definite, then δ∗ can be expressed as the unconstrained
    minimization of gδ − δ(B + λ∗I)δ/2. 7 If the objective is multiplied by 2, then
    λ also needs to be multiplied by 2 to achieve the same effect when using a penalty
    method. By contrast, the trust region R is unaffected by such a scale change.
  id: totrans-6073
  prefs: []
  type: TYPE_NORMAL
  zh: gδ−δBδ/2，这相当于g−Bδ∗−λ∗Qδ∗ = 0，因此(B+λ∗Q)δ∗ = g。如果矩阵B+λ∗Q是正定的，则δ∗可以表示为gδ − δ(B
    + λ∗I)δ/2的无约束最小化。如果目标乘以2，则λ也需要乘以2，以在使用惩罚方法时达到相同效果。相比之下，信任区域R不受这种尺度变化的影响。
- en: the B matrix, or finding the solutions of multiple Tikhonov-damped quadratics
    of the form Mˆ . When CG is used to perform partial optimization of the quadratic
    model, there are also good approximation algorithms [13] based on examining the
    tridiagonal decomposition of B (restricted to the Krylov subspace), but these
    require either storing a basis for the entire Krylov subspace (which may be impractical
    when n is large), or will require a separate run of CG once λ has been found via
    the tridiagonal decomposition, effectively doubling the amount of matrix-vector
    products that must be computed.
  id: totrans-6074
  prefs: []
  type: TYPE_NORMAL
  zh: B矩阵，或求解多个形式为Mˆ的Tikhonov阻尼二次型的解。当使用CG对二次模型进行部分优化时，也有一些基于检查B的三对角分解（限制在Krylov子空间）的良好近似算法[13]，但这些要么需要存储整个Krylov子空间的基（当n较大时可能不切实际），要么在通过三对角分解找到λ后需要单独运行CG，从而有效地使必须计算的矩阵-向量乘积数量加倍。
- en: Even if we can easily solve (or partially optimize) the trust-region subproblem,
    we are still left with the problem of adjusting r. And while it is likely that
    the "optimal" r will remain a more stable quantity than the "optimal" λ over the
    parameter space, it still needs to be adjusted using some heuristic. Indeed, one
    heuristic which is advocated for adjusting r [30] is precisely the LevenbergMarquardt
    heuristic discussed in section 20.8.5 which is already quite effective
  id: totrans-6075
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们能够轻松解决（或部分优化）信任区域子问题，我们仍然面临调整r的问题。虽然“最优”r在参数空间中很可能保持比“最优”λ更稳定，但仍需使用一些启发式方法进行调整。事实上，调整r的一个启发式方法[30]正是第20.8.5节讨论的Levenberg-Marquardt启发式，这已相当有效。
- en: '(in our experience) at adjusting λ directly. This leaves us with the question:
    why not just adjust λ directly and avoid the extra work required to compute λ
    by way of r?'
  id: totrans-6076
  prefs: []
  type: TYPE_NORMAL
  zh: （根据我们的经验）直接调整λ。这让我们产生了一个问题：为什么不直接调整λ，以避免通过r计算λ所需的额外工作？
- en: One method which is effective at finding reasonable *approximate* solutions
    of the trust-region sub-problem, for the case where R is a ball of radius r, is
    to run CG (initialized at zero) until the norm of the solution exceeds r (i.e.
    the solution leaves the trust-region) and truncate CG at that iteration, with
    a possible modification to the α multiplier for the final conjugate direction
    to ensure a solution of norm exactly r. This is known as the Steihaug-Toint method,
    and it can be shown [13] that, provided CG is initialized from a zero starting
    solution and no preconditioning is used, the norm of the solution will increase
    monotonically and that if CG is truncated in the manner described above, the resultant
    solution δ
  id: totrans-6077
  prefs: []
  type: TYPE_NORMAL
  zh: 一种有效找到合理 *近似* 解决方案的信任区域子问题的方法，在 R 为半径为 r 的球的情况下，是运行 CG（初始化为零），直到解的范数超过 r（即解离开信任区域），并在该迭代处截断
    CG，同时可能需要对最终共轭方向的 α 乘子进行修改，以确保解的范数恰好为 r。这被称为 Steihaug-Toint 方法，并且可以证明 [13]，只要
    CG 是从零起始解初始化且不使用预处理，解的范数将单调增加，如果以上述方式截断 CG，得到的解 δ
- en: †
  id: totrans-6078
  prefs: []
  type: TYPE_NORMAL
  zh: †
- en: k will satisfy M(δ
  id: totrans-6079
  prefs: []
  type: TYPE_NORMAL
  zh: k 将满足 M(δ
- en: †
  id: totrans-6080
  prefs: []
  type: TYPE_NORMAL
  zh: †
- en: k) ≤ 12M(δ∗k) where δ∗k is the optimal solution to the trust-region sub-problem.
    This seems like a good compromise, and it is more economical than approaches that
    try to solve the trust region problem exactly
  id: totrans-6081
  prefs: []
  type: TYPE_NORMAL
  zh: k) ≤ 12M(δ∗k)，其中 δ∗k 是信任区域子问题的最优解。这似乎是一个不错的折中，而且比试图精确解决信任区域问题的方法更经济。
- en: (or using better approximations, as discussed above). Unfortunately, the restriction
    that CG must be initialized from zero cannot be easily removed, and in our experience
    such initializations turn out to be very beneficial, as we will discuss in section
    20.10.
  id: totrans-6082
  prefs: []
  type: TYPE_NORMAL
  zh: （或使用更好的近似，如上所讨论）。不幸的是，CG 必须从零初始化的限制无法轻易去除，而根据我们的经验，这种初始化被证明是非常有益的，正如我们将在第 20.10
    节中讨论的那样。
- en: Another argument against using the Steihaug-Toint method is that if the trust-region
    is left after only a few steps of CG, it will likely be the case that few, if
    any, of the low-curvature directions have converged to a significant degree (see
    section 20.9). And while we know that this will not affect the optimized value
    of M by more than a factor of 2, it will nonetheless produce a qualitatively different
    type of update than one produced using a penalty method, which will have a possibly
    negative effect on the overall optimization trajectory.
  id: totrans-6083
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个反对使用 Steihaug-Toint 方法的理由是，如果在 CG 的几步之后信任区域仍然存在，那么很可能几乎没有低曲率方向收敛到显著程度（见第
    20.9 节）。尽管我们知道这对 M 的优化值不会产生超过 2 倍的影响，但它仍将产生一种与使用惩罚方法所产生的更新在质上不同的类型，这可能对整体优化轨迹产生负面影响。
- en: 'Another possible argument against using the Steihaug-Toint method is that it
    cannot used with *preconditioned* CG. However, as long as we are willing to enforce
    an elliptical trust-region of the form {x : xP < r} where P is the preconditioning
    matrix, instead of a spherical one, the method still works and its theory remains
    valid. And depending on the situation, this kind of elliptical trust-region may
    actually be a very natural choice.'
  id: totrans-6084
  prefs: []
  type: TYPE_NORMAL
  zh: '另一个可能反对使用 Steihaug-Toint 方法的理由是它不能与 *预处理* CG 一起使用。然而，只要我们愿意强制执行形如 {x : xP <
    r} 的椭圆信任区域，其中 P 是预处理矩阵，而不是球形信任区域，该方法仍然有效，其理论保持有效。根据具体情况，这种椭圆信任区域实际上可能是一个非常自然的选择。'
- en: 20.8.7 Cg Truncation As Damping
  id: totrans-6085
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.8.7 Cg 截断作为阻尼
- en: 'Within HF, the main reason for terminating CG before it has converged is one
    of a practical nature: the matrix-vector products are expensive and additional
    iterations will eventually provide diminishing returns as far as optimizing the
    quadratic model M. But there is another more subtle reason we may want to truncate
    early: CG truncation may be viewed as special type of damping which may be used
    in conjunction with (or as an alternative to) the various other damping methods
    discussed in this section.'
  id: totrans-6086
  prefs: []
  type: TYPE_NORMAL
  zh: 在 HF 中，提前终止 CG 而未收敛的主要原因是实用性的：矩阵-向量乘积很昂贵，额外的迭代最终在优化二次模型 M 时将提供递减的收益。但还有另一个更微妙的原因，我们可能希望提前截断：CG
    截断可以被视为一种特殊类型的阻尼，可以与本节中讨论的各种其他阻尼方法一起使用（或作为替代）。
- en: As CG iterates, the accuracy of M(δ) tends to go down, even while f(θk−1+δ)
  id: totrans-6087
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 CG 迭代，M(δ) 的准确性趋向下降，即使 f(θk−1+δ)
- en: may still be improving. One way to explain this, assuming a zero initialization,
    is that the norm of δ will increase monotonically with each step, and thus be
    more likely to leave the implicit region around δ = 0 where M is a reasonable
    approximation of f. There is also theory which suggests that CG will converge
    to δ∗ first along directions of mostly higher curvature, and only later along
    directions of mostly low curvature (see section 20.9). While pursuing low curvature
    directions seems to be important for optimization of deep networks and RNNs, it
    also tends to be associated with large changes in θ which can lead to more serious
    breakdowns in the accuracy of the local quadratic model.
  id: totrans-6088
  prefs: []
  type: TYPE_NORMAL
  zh: 可能仍在改善中。一种解释方式是，假设零初始化，δ 的范数将随着每一步单调增加，因此更有可能离开 δ = 0 附近的隐含区域，在该区域 M 是 f 的合理近似。此外，还有理论表明，CG
    会首先沿着大多数高曲率的方向收敛到 δ∗，而仅在之后沿着大多数低曲率的方向收敛（见第 20.9 节）。虽然追求低曲率方向似乎对深度网络和 RNN 的优化很重要，但这也往往与
    θ 的大变化相关，可能导致局部二次模型准确性更严重的崩溃。
- en: The Steihaug-Toint method described in section 20.8.6 already makes use of this
    phenomenon and relies exclusively on truncating the solution early to enforce
    trust-region constraints. And it is well-known that truncating CG, which effectively
    limits the size of the Krylov subspace, has certain regularizing properties in
    the context of solving noisy and ill-posed systems of linear equations
  id: totrans-6089
  prefs: []
  type: TYPE_NORMAL
  zh: 第 20.8.6 节中描述的 Steihaug-Toint 方法已经利用了这一现象，并完全依赖于提前截断解来强制执行信任区域约束。而且众所周知，截断 CG
    有效地限制了 Krylov 子空间的大小，在解决噪声和病态线性方程组的过程中具有一定的正则化特性。
- en: '[14].'
  id: totrans-6090
  prefs: []
  type: TYPE_NORMAL
  zh: '[14].'
- en: Although it wasn't emphasize this in the paper, Martens [22] supplemented the
    progress-based CG termination criteria with a maximum limit of 250 steps. In practice,
    we have found that this maximum is consistently reached after the first 100 (or
    so) iterations of HF when the approach is applied to problems like the "CURVES"
    auto-encoder from Hinton and Salakhutdinov [17], and that it plays an important
    role which is not limited to saving computation time.
  id: totrans-6091
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管论文中没有强调这一点，Martens [22] 为基于进展的 CG 终止标准补充了最大限制为 250 步。在实践中，我们发现，当该方法应用于 Hinton
    和 Salakhutdinov [17] 的“CURVES”自编码器等问题时，这一最大值在 HF 的前 100 次（左右）迭代后始终会被达到，并且它发挥了重要作用，不仅限于节省计算时间。
- en: Instead, we can observe that the improvement in the value of the objective f
    is non-monotonic in the number of CG steps, and may peak long before condition
    20.2 is satisfied (see fig. 20.4).
  id: totrans-6092
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以观察到目标函数 f 的改进在 CG 步数上是非单调的，并且可能在满足条件 20.2 之前就达到峰值（见图 20.4）。
- en: Martens [22] proposed "CG-backtracking" as a method to take advantage of this
    tendency for earlier iterates to be more favorable as updates, by selecting the
    "best" iterate among some manageable subset, as measured by the associated reduction
    in f. One possible approach which is reasonably efficient is to compute the objective
    f only on a small subset of the current minibatch or training set, and only at
    every multiple of c steps for some fixed c, or every power of ν steps (rounding
    to the nearest integer) for some fixed ν, and then terminate once it appears that
    there will be no further possible improvement in the objective.
  id: totrans-6093
  prefs: []
  type: TYPE_NORMAL
  zh: Martens [22] 提出了“CG 回溯”作为一种方法，利用早期迭代作为更新的更有利的趋势，通过在一些可管理的子集中选择“最佳”迭代，来衡量与 f
    相关的减少量。一种合理有效的方法是，仅在当前小批量或训练集的一个小子集上计算目标 f，并且仅在每 c 步的倍数上，或每 ν 步的幂（四舍五入到最接近的整数）上计算，最后在似乎不再有进一步改进的可能时终止。
- en: '![503_image_0.png](503_image_0.png)'
  id: totrans-6094
  prefs: []
  type: TYPE_IMG
  zh: '![503_image_0.png](503_image_0.png)'
- en: Fig. 20.4. A plot of the objective function (solid) versus the quadratic approximation
    (dotted) as a function of the number of CG steps. This was selected as a typical
    example of a single run of CG performed by HF on a typical deep auto-encoder training
    task.
  id: totrans-6095
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.4. 目标函数（实线）与二次近似（虚线）相对于 CG 步数的图示。这被选为 HF 在典型深度自编码器训练任务中执行 CG 的单次运行的典型示例。
- en: An important thing to keep in mind is that the damping effect of CG truncation
    will depend on both the preconditioning scheme used within CG (as discussed in
    section 20.11), and on the particular manner in which CG is initialized
  id: totrans-6096
  prefs: []
  type: TYPE_NORMAL
  zh: 需要牢记的一点是，CG 截断的阻尼效应将依赖于 CG 中使用的预处理方案（如第 20.11 节所讨论的），以及 CG 初始化的特定方式。
- en: (as discussed in sec. 20.10). In the extreme case where P = B, the eigenvalues
    will all be equal and CG will converge in one step, rendering CG truncation trivial/useless.
    And for "stronger" preconditioners that let CG converge faster it can be argued
    that truncation at a particular iteration i will have a smaller effect than with
    "weaker" preconditioners. But this perspective oversimplies a very complex issue.
  id: totrans-6097
  prefs: []
  type: TYPE_NORMAL
  zh: （在 20.10 节中讨论）。在极端情况下，当 P = B 时，特征值将全部相等，CG 将在一步内收敛，从而使得 CG 截断变得微不足道/无用。而对于让
    CG 更快收敛的“更强”预处理器，可以认为在某个特定迭代 i 处的截断效果会比“较弱”预处理器小。但这种看法简化了一个非常复杂的问题。
- en: Preconditioning is effectively reparameterizing the quadratic optimization problem,
    which has the effect of creating new eigenvectors with new eigenvalues and of
    changing the Krylov subspace (as discussed further in section 20.11.1). This in
    turn affects the "order" in which CG tends to prioritize convergence along different
    directions (see section 20.9). Thus, when CG is terminated long before convergence,
    preconditioning will have an important effect on the nature of the implicit damping
    and thus on the quality of the update.
  id: totrans-6098
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理实际上是对二次优化问题进行重新参数化，这将产生新的特征向量和新的特征值，并改变 Krylov 子空间（在 20.11.1 节中进一步讨论）。这反过来影响
    CG 在不同方向上优先收敛的“顺序”（见 20.9 节）。因此，当 CG 在收敛之前很早就终止时，预处理将对隐式阻尼的性质以及更新的质量产生重要影响。
- en: From the perspective of the Steihaug-Toint method and trust-regions, CG
  id: totrans-6099
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Steihaug-Toint 方法和信赖域的角度来看，CG
- en: 'preconditioning can be thought of as determining the shape of the trust-region
    that is being implicitly enforced through the use of truncation. In particular,
    the trust region will be given by R = {x : xP ≤ r}, for some r, where P is the
    preconditioning matrix. Similarly, initializing CG from some non-zero point x0
    can be thought of as shifting the center of said trust-region away from 0. In
    both cases, the guarantee remains true that the solution found by truncated CG
    will be at least half as good (in terms of the value of M), subject to the trust-region
    radius implied by the current iterate, as long as we the modify definition of
    the trust region appropriately.'
  id: totrans-6100
  prefs: []
  type: TYPE_NORMAL
  zh: '预处理可以看作是确定通过截断隐式施加的信赖域的形状。特别地，信赖域将由 R = {x : xP ≤ r} 给出，其中 r 为某个值，P 是预处理矩阵。同样，从某个非零点
    x0 初始化 CG 可以看作是将该信赖域的中心从 0 移开。在这两种情况下，只要我们适当地修改信赖域的定义，截断 CG 所找到的解至少在 M 的值上也会有一半的质量，这一点是有保证的。'
- en: 20.8.8 Line Searching
  id: totrans-6101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.8.8 线搜索
- en: The most basic form of damping, which is present in almost every 2nd-order optimization
    algorithm, is standard line searching applied to the update proposal δk. In particular,
    we select a scalar αk to multiply the update δk before adding it to θ so that
    θk = θk−1 +αδk. Usually α is set to 1 as long as certain conditions hold (see
    Nocedal et al. [30]), and decreased only as necessary until they do. Doing this
    guarantees that certain local convergence theorems apply.
  id: totrans-6102
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的阻尼形式几乎出现在每个二阶优化算法中，即对更新提议 δk 应用的标准线搜索。特别地，我们选择一个标量 αk 来乘以更新 δk，然后将其加到 θ
    上，使得 θk = θk−1 +αδk。通常，只要某些条件成立（见 Nocedal 等人 [30]），α 会设为 1，只有在必要时才会降低。这样做保证了某些局部收敛定理的适用。
- en: 'Provided that δk is a descent direction (δk ∇f(θk−1) < 0) we know that for
    a sufficiently small (but non-zero) value of α, we will have:'
  id: totrans-6103
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 δk 是一个下降方向（δk ∇f(θk−1) < 0），我们知道，对于一个足够小（但非零）的 α 值，我们将有：
- en: $$f(\theta_{k-1})>f(\theta_{k})=f(\theta_{k-1}+\alpha\delta_{k})$$
  id: totrans-6104
  prefs: []
  type: TYPE_NORMAL
  zh: $$f(\theta_{k-1})>f(\theta_{k})=f(\theta_{k-1}+\alpha\delta_{k})$$
- en: $$(20.11)$$
  id: totrans-6105
  prefs: []
  type: TYPE_NORMAL
  zh: $$(20.11)$$
- en: f(θk−1) > f(θk) = f(θk−1 + αδk) (20.11)
  id: totrans-6106
  prefs: []
  type: TYPE_NORMAL
  zh: f(θk−1) > f(θk) = f(θk−1 + αδk) (20.11)
- en: δk will be descent direction as long as Bˆk−1 is positive definite, ∇f is computed
    on the entire training set, and Mˆ is optimized either exactly, or partially with
    CG (provided that we achieve M(δ) < 0). Thus, in many practical scenarios, a line
    search will ensure that the update results in a decrease in f, although it may
    be very small. And if we are content with the weaker condition that only the terms
    of f corresponding to the minibatches used to estimate ∇f(θk−1) must decrease,
    then we can drop the requirement that ∇f be computed using the whole training
    set.
  id: totrans-6107
  prefs: []
  type: TYPE_NORMAL
  zh: 只要Bˆk−1是正定的，δk将是下降方向，∇f是在整个训练集上计算的，并且Mˆ是通过CG方法进行优化（前提是我们实现了M(δ) < 0）。因此，在许多实际场景中，线搜索将确保更新导致f的减少，尽管减少的量可能非常小。如果我们满足于仅要求用于估计∇f(θk−1)的minibatch对应的f项减少，那么我们可以放弃要求∇f使用整个训练集进行计算。
- en: One easy way to implement line searching is via the back-tracking approach,
    which starts at α = 1 and repeatedly multiplies this by some constant β ∈ (0,
    1)
  id: totrans-6108
  prefs: []
  type: TYPE_NORMAL
  zh: 实现线搜索的一种简单方法是通过回溯方法，该方法从α = 1开始，并不断将其乘以某个常数β ∈ (0, 1)。
- en: until the "sufficient-decrease condition" applies. This is given by
  id: totrans-6109
  prefs: []
  type: TYPE_NORMAL
  zh: 直到“充分减少条件”适用。这由以下给出。
- en: $$f(\theta_{k-1}+\alpha\delta_{k})\leq f(\theta_{k-1})+c\alpha\nabla f(\theta_{k-1})^{\top}\delta_{k}$$
  id: totrans-6110
  prefs: []
  type: TYPE_NORMAL
  zh: $$f(\theta_{k-1}+\alpha\delta_{k})\leq f(\theta_{k-1})+c\alpha\nabla f(\theta_{k-1})^{\top}\delta_{k}$$
- en: where c is some small constant like 10−2. It can be shown that this following
    approach will produce an update which has strong convergence guarantees 8.
  id: totrans-6111
  prefs: []
  type: TYPE_NORMAL
  zh: 其中c是一些小常数，如10−2。可以证明，以下方法将产生具有强收敛保证的更新。
- en: Unlike 1st-order methods where the total number of updates to θ can often be
    on the order of 104 − 106 for a large neural network, powerful approximate Newton
    methods like HF may only require on the order of 102 − 103, so the expense of
    a line-search is much easier to justify.
  id: totrans-6112
  prefs: []
  type: TYPE_NORMAL
  zh: 与一阶方法相比，对于大型神经网络，θ的更新总次数通常在104到106之间，而强大的近似牛顿方法（如HF）可能仅需102到103，因此线搜索的成本更容易证明合理。
- en: Note also that it is also possible to view line searching as a special type
    of a penalty based damping method, where we use a penalty term of the form 12α
    B.
  id: totrans-6113
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，线搜索也可以被视为一种基于惩罚的阻尼方法的特殊类型，我们使用形式为12α B的惩罚项。
- en: In other words, we simply increase the scale of curvature matrix B by 1/α. This
    interpretation is valid as long as we solve Mˆ exactly, or partially by CG as
    long as it is initialized from 0.
  id: totrans-6114
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们只是将曲率矩阵B的规模增加1/α。只要我们精确地解决Mˆ，或通过CG部分解决（前提是初始化为0），这一解释是有效的。
- en: The line search is best thought of as a last line of defense to compensate for
    occasional and temporary maladjustment of the various non-static parameters of
    the other damping methods, such as λ or r, and not as a replacement for 8 But
    note that other possible ways of choosing an α that satisfies this condition may
    make α too small.
  id: totrans-6115
  prefs: []
  type: TYPE_NORMAL
  zh: 线搜索最好被视为一种最后的防线，用于弥补其他阻尼方法（如λ或r）各种非静态参数的偶尔和临时的失调，而不是替代方案。但请注意，选择满足此条件的其他可能方式可能会使α过小。
- en: these methods. If the line search becomes very active (i.e., very often chooses
    an α strictly less than 1) there are two probable causes, which should be addressed
    directly instead of by relying on the line-search. The first is poorly designed/inappropriate
    damping methods and/or poor heuristics for adjusting their non-static meta-parameters.
    The second probable cause is that the data used to estimate the curvature and/or
    gradient is insufficient and the update has effectively "overfitted" the current
    minibatch.
  id: totrans-6116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果线搜索变得非常活跃（即非常频繁地选择一个严格小于1的α），可能有两个原因，应直接解决，而不是依赖线搜索。第一个是设计不良/不适当的阻尼方法和/或调整其非静态元参数的启发式方法不佳。第二个可能原因是用于估计曲率和/或梯度的数据不足，更新实际上“过拟合”了当前的minibatch。
- en: The argument for not relying on line searches to fix whatever problems might
    exist with the updates produced by optimizing M is that they work by rescaling
    each eigen-component (or conjugate direction) of the update *equally* by the multiplicative
    factor α, which is a very limited and inflexible approach. It turns out that in
    many practical situations, the type of scaling modification performed by a penalty
    method like Tikhonov damping is highly preferable, such as when B is very ill-conditioned.
    In such a situation, minimizing M results in an update proposal δk which is scaled
    much too strongly in certain, possibly spurious, lowcurvature eigen-directions,
    by a factor of possibly 103 or more, and a line-search will have to divide *all
    components* by this factor in order to make the update viable, which results in
    an extremely small update. Meanwhile, a Tikhonov approach, because it effectively
    modifies the curvatures by the additive constant λ (as discussed in section 20.8.1)
    can easily suppress these very low-curvature directions while leaving the higher
    curvature directions relatively untouched, which makes the update much bigger
    and more useful as a result.
  id: totrans-6117
  prefs: []
  type: TYPE_NORMAL
  zh: 不依赖于线搜索来解决通过优化 M 产生的更新可能存在的问题的原因在于，它们通过乘法因子 α *同等* 重新缩放更新的每个特征分量（或共轭方向），这是一种非常有限且不灵活的方法。事实证明，在许多实际情况中，像
    Tikhonov 阻尼这样的惩罚方法所进行的缩放修改是高度可取的，尤其是在 B 的条件非常差的情况下。在这种情况下，最小化 M 导致的更新提议 δk 在某些可能虚假的低曲率特征方向上缩放过强，可能达到了
    103 或更高的因子，而线搜索必须将 *所有分量* 除以这个因子，以使更新可行，这会导致更新极其微小。与此同时，Tikhonov 方法由于通过加性常数 λ
    有效地修改曲率（如第 20.8.1 节所讨论）可以轻松抑制这些非常低曲率的方向，同时使高曲率方向相对保持不变，从而使更新变得更大且更有用。
- en: 20.9 Convergence Of Cg
  id: totrans-6118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.9 CG 的收敛性
- en: In this section we will examine the theoretical convergence properties of CG
    and provide justifications for various statements regarding its convergence made
    throughout this report, such as how δ converges along different "directions" in
    parameter space, and how CG prioritizes these directions according to their "associated
    curvature". This analysis has particularly important implications for preconditioning
    (see section 20.11) and CG truncation damping (see section 20.8.7).
  id: totrans-6119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将考察 CG 的理论收敛性质，并为报告中关于其收敛性的各种陈述提供依据，例如 δ 如何沿着参数空间中的不同“方向”收敛，以及 CG 如何根据它们的“相关曲率”优先考虑这些方向。这一分析对预条件（见第
    20.11 节）和 CG 截断阻尼（见第 20.8.7 节）具有特别重要的意义。
- en: Before we begin it should be noted that due to inexact computer arithmetic,
    in practice the conclusions of this analysis (which implicitly assume exact arithmetic)
    will only hold approximately. Indeed, CG, unlike many other numerical algorithms,
    is highly sensitive to numerical issues and after only 5−10 iterations on a high-precision
    machine will produce iterates that can differ significantly from those which would
    be produced by a hypothetical exact implementation.
  id: totrans-6120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，应该注意到，由于计算机算术的不精确性，实际上该分析的结论（隐含假设精确算术）仅能近似成立。实际上，CG 与许多其他数值算法不同，对数值问题非常敏感，在高精度机器上经过仅
    5−10 次迭代后，产生的迭代结果可能与假设的精确实现产生的结果有显著差异。
- en: We will analyze CG applied to a general quadratic objective of the form q(x)
    = 12xAx−bx for a symmetric positive definite matrix A ∈ Rn×n, b ∈ Rn.
  id: totrans-6121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分析应用于一般二次目标的 CG，形式为 q(x) = 12xAx−bx，其中 A ∈ Rn×n 为对称正定矩阵，b ∈ Rn。
- en: This can be related by to HF by taking A = B (or A = Bˆ in the case of a damped
    quadratic approximation) and b = −∇f.
  id: totrans-6122
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过取 A = B（或在阻尼二次近似情况下取 A = Bˆ）和 b = −∇f 来与 HF 相关联。
- en: Note that x as defined here is an n-dimensional vector and should not be confused
    with its use elsewhere in this chapter as the input to a neuron.
  id: totrans-6123
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里定义的 x 是一个 n 维向量，不应与本章其他地方作为神经元输入的用法混淆。
- en: Let {λj}nj=1 be the eigenvalues of A and {vj}nj=1 the corresponding (unit)
  id: totrans-6124
  prefs: []
  type: TYPE_NORMAL
  zh: 设 {λj}nj=1 为 A 的特征值，{vj}nj=1 为相应的（单位）
- en: eigenvectors, x0 be the initialization of CG, and x∗ the minimizer of q. Since
    the vj 's are an orthonormal basis for Rn (because A is symmetric and invertible)
    for any x we can express x0 − x∗ in terms of the vj 's, giving x0 − x∗ = nj=1
    ξjvj for ξj = vj (x0 − x∗).
  id: totrans-6125
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量，x0 为 CG 的初始化，x∗ 为 q 的最小化器。由于 vj 是 Rn 的正交归一基（因为 A 是对称且可逆的），对于任何 x，我们可以用
    vj 来表示 x0 − x∗，即 x0 − x∗ = nj=1 ξjvj，其中 ξj = vj (x0 − x∗)。
- en: 'It can be shown [30, 32] that:'
  id: totrans-6126
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明 [30, 32]：
- en: $$\|x_{i}-x^{*}\|_{A}^{2}=\operatorname*{min}_{p}\sum_{j=1}^{n}\lambda_{j}p(\lambda_{j})^{2}\xi_{j}^{2}$$
  id: totrans-6127
  prefs: []
  type: TYPE_NORMAL
  zh: $$\|x_{i}-x^{*}\|_{A}^{2}=\operatorname*{min}_{p}\sum_{j=1}^{n}\lambda_{j}p(\lambda_{j})^{2}\xi_{j}^{2}$$
- en: $$(20.12)$$
  id: totrans-6128
  prefs: []
  type: TYPE_NORMAL
  zh: $$(20.12)$$
- en: 2ξ2j (20.12)
  id: totrans-6129
  prefs: []
  type: TYPE_NORMAL
  zh: 2ξ2j (20.12)
- en: where z2A ≡ 12 zAz and where the minimum is taken over all polynomials of degree
    i with constant term 1. This result can be used to prove various convergence theorems
    for CG [30]. For example, CG will always converge to x∗ after a number of iterations
    less than or equal to the number m of distinct eigenvalues of A, since it is easy
    to design a polynomial of degree m with constant term 1 that satisfies p(λj )=0
    for all j.
  id: totrans-6130
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 z2A ≡ 12 zAz，并且最小值是在所有常数项为 1 的 i 次多项式中取的。这个结果可以用来证明 CG 的各种收敛定理 [30]。例如，CG
    将始终在小于或等于 A 的不同特征值数量 m 的迭代次数内收敛到 x∗，因为很容易设计出一个常数项为 1 的 m 次多项式，使得 p(λj )=0 对于所有
    j 成立。
- en: 'To gain more insight into eqn. 20.12 we will re-derive and re-express it in
    a way that implies an intuitive interpretation for each term. It is easy to show
    that:'
  id: totrans-6131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地理解方程 20.12，我们将重新推导并重新表达它，以便为每个项提供直观的解释。容易证明：
- en: $$q(z+x_{0})-q(x_{0})=\frac{1}{2}z^{\top}A z+r_{0}^{\top}z$$
  id: totrans-6132
  prefs: []
  type: TYPE_NORMAL
  zh: $$q(z+x_{0})-q(x_{0})=\frac{1}{2}z^{\top}A z+r_{0}^{\top}z$$
- en: where r0 = Ax0 − b (i.e. the initial residual). And so q(z + x0) − q(x0) is
    a quadratic in z with constant term equal to 0, and linear term r0.
  id: totrans-6133
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 r0 = Ax0 − b（即初始残差）。因此，q(z + x0) − q(x0) 是 z 的二次函数，其常数项为 0，线性项为 r0。
- en: Defining ηj = r0 vj to be the size of eigenvector vj in direction r0 = Ax0 −
    b
  id: totrans-6134
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 ηj = r0 vj 为特征向量 vj 在方向 r0 = Ax0 − b 上的大小
- en: '(which is the initial residual), and observing that vj Avj = λjvj vj = λj ,
    we have for any α ∈ R:'
  id: totrans-6135
  prefs: []
  type: TYPE_NORMAL
  zh: （这就是初始残差），并观察到 vj Avj = λj vj = λj，我们对于任何 α ∈ R 有：
- en: $$q(\alpha v_{j}+x_{0})-q(x_{0})=\frac{1}{2}\alpha^{2}v_{j}^{\top}A v_{j}+\alpha
    r_{0}^{\top}v_{j}=\frac{1}{2}\alpha^{2}\lambda_{j}+\alpha\eta_{j}$$
  id: totrans-6136
  prefs: []
  type: TYPE_NORMAL
  zh: $$q(\alpha v_{j}+x_{0})-q(x_{0})=\frac{1}{2}\alpha^{2}v_{j}^{\top}A v_{j}+\alpha
    r_{0}^{\top}v_{j}=\frac{1}{2}\alpha^{2}\lambda_{j}+\alpha\eta_{j}$$
- en: Since the vj 's are an orthonormal basis for Rn (because A is symmetric and
    invertible), we can express x − x0 (for any x) in terms of the vj 's, giving x
    =
  id: totrans-6137
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 vj 是 Rn 的正交归一基（因为 A 是对称且可逆的），我们可以用 vj 来表示 x − x0（对于任何 x），得到 x =
- en: x0 + j βjvj where βj = vj (x − x0). Note that the vj 's are also mutually conjugate
    (which follows from them being both orthonormal and eigenvectors)
  id: totrans-6138
  prefs: []
  type: TYPE_NORMAL
  zh: x0 + j βjvj，其中 βj = vj (x − x0)。注意，vj 之间也是相互共轭的（这是由于它们都是正交归一和特征向量）。
- en: 'and that since q(z + x0) − q(x0) is quadratic in z with constant term 0 we
    have that for any two conjugate vectors u and w:'
  id: totrans-6139
  prefs: []
  type: TYPE_NORMAL
  zh: 并且由于 q(z + x0) − q(x0) 在 z 中是二次函数，常数项为 0，因此对于任何两个共轭向量 u 和 w：
- en: $$q(u+w+x_{0})-q(x_{0})=(q(u+x_{0})-q(x_{0}))+(q(w+x_{0})-q(x_{0}))$$
  id: totrans-6140
  prefs: []
  type: TYPE_NORMAL
  zh: $$q(u+w+x_{0})-q(x_{0})=(q(u+x_{0})-q(x_{0}))+(q(w+x_{0})-q(x_{0}))$$
- en: 'which is straightforward to show. Thus we have:'
  id: totrans-6141
  prefs: []
  type: TYPE_NORMAL
  zh: 这很容易证明。因此我们有：
- en: $$q(x)-q(x_{0})=q((x-x_{0})+x_{0})-q(x_{0})$$ $$=\sum_{j=1}^{n}(q(\beta_{j}v_{j}+x_{0})-q(x_{0}))=\sum_{j=1}^{n}\left(\frac{1}{2}\beta_{j}^{2}\lambda_{j}+\beta_{j}\eta_{j}\right).$$
  id: totrans-6142
  prefs: []
  type: TYPE_NORMAL
  zh: $$q(x)-q(x_{0})=q((x-x_{0})+x_{0})-q(x_{0})$$ $$=\sum_{j=1}^{n}(q(\beta_{j}v_{j}+x_{0})-q(x_{0}))=\sum_{j=1}^{n}\left(\frac{1}{2}\beta_{j}^{2}\lambda_{j}+\beta_{j}\eta_{j}\right).$$
- en: What this says is that the size βj of the contribution of each eigenvector/eigendirection
    to x, have an independent influence on the value of q(x)−q(x0), and so we can
    meaningfully talk about how each one of them independently "converges" as CG iterates.
  id: totrans-6143
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，每个特征向量/特征方向对 x 的贡献大小 βj 独立影响 q(x)−q(x0) 的值，因此我们可以有意义地讨论它们在 CG 迭代过程中如何独立地“收敛”。
- en: In a sense, each βj is being optimized by CG, with the ultimate goal of minimizing
    the corresponding 1-D quadratic q(βjvj + x0) − q(x0), whose minimizer is β∗j =
    − ηj λj
  id: totrans-6144
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，每个 βj 都在通过 CG 进行优化，其最终目标是最小化相应的一维二次函数 q(βjvj + x0) − q(x0)，其最小化值为 β∗j
    = − ηj λj。
- en: '= vj (x∗ − x0) with associated minimum value:'
  id: totrans-6145
  prefs: []
  type: TYPE_NORMAL
  zh: = vj (x∗ − x0)，其相关的最小值为：
- en: $$q(\beta_{j}^{*}v_{j}+x_{0})-q(x_{0})=-\frac{1}{2}\frac{\eta_{j}^{2}}{\lambda_{j}}=-\omega_{j}$$
  id: totrans-6146
  prefs: []
  type: TYPE_NORMAL
  zh: $$q(\beta_{j}^{*}v_{j}+x_{0})-q(x_{0})=-\frac{1}{2}\frac{\eta_{j}^{2}}{\lambda_{j}}=-\omega_{j}$$
- en: where we define ωj ≡ η2j λj . The difference between the current value of q(βjvj+x0)
  id: totrans-6147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义 ωj ≡ η2j λj 。当前值 q(βjvj+x0) 与
- en: 'and its minimum has a particularly nice form:'
  id: totrans-6148
  prefs: []
  type: TYPE_NORMAL
  zh: 其最小值有一个特别好的形式：
- en: $$q(\beta_{j}v_{j}+x_{0})-q(\beta_{j}^{*}v_{j}+x_{0})=\frac{1}{2}\beta_{j}^{2}\lambda_{j}+\beta_{j}\eta_{j}+\omega_{j}=\omega_{j}\left(\frac{\lambda_{j}}{\eta_{j}}\beta_{j}+1\right)^{2}$$
  id: totrans-6149
  prefs: []
  type: TYPE_NORMAL
  zh: $$q(\beta_{j}v_{j}+x_{0})-q(\beta_{j}^{*}v_{j}+x_{0})=\frac{1}{2}\beta_{j}^{2}\lambda_{j}+\beta_{j}\eta_{j}+\omega_{j}=\omega_{j}\left(\frac{\lambda_{j}}{\eta_{j}}\beta_{j}+1\right)^{2}$$
- en: Now suppose that x − x0 ∈ Ki(*A, r*0) so that there exists some (i − 1)-degree
    polynomial s s.t. x − x0 = s(A)r0 and note that s(A)vj = s(λj )vj . Then,
  id: totrans-6150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设 x − x0 ∈ Ki(*A, r*0)，以便存在某个(i − 1)次多项式 s，使得 x − x0 = s(A)r0，并注意到 s(A)vj
    = s(λj)vj。那么，
- en: $$\beta_{j}=v_{j}^{\top}(x-x_{0})=v_{j}^{\top}s(A)r_{0}=(s(A)v_{j})^{\top}r_{0}=(s(\lambda_{j})v_{j})^{\top}r_{0}=s(\lambda_{j})\eta_{j}$$
  id: totrans-6151
  prefs: []
  type: TYPE_NORMAL
  zh: $$\beta_{j}=v_{j}^{\top}(x-x_{0})=v_{j}^{\top}s(A)r_{0}=(s(A)v_{j})^{\top}r_{0}=(s(\lambda_{j})v_{j})^{\top}r_{0}=s(\lambda_{j})\eta_{j}$$
- en: 'so that:'
  id: totrans-6152
  prefs: []
  type: TYPE_NORMAL
  zh: 这样：
- en: $$q(\beta_{j}v_{j}+x_{0})-q(\beta_{j}^{*}v_{j}+x_{0})=\omega_{j}\left(\frac{\lambda_{j}}{\eta_{j}}s(\lambda_{j})\eta_{j}+1\right)^{2}$$
    $$=\omega_{j}(\lambda_{j}s(\lambda_{j})+1)^{2}=\omega_{j}p(\lambda_{j})^{2}$$
  id: totrans-6153
  prefs: []
  type: TYPE_NORMAL
  zh: $$q(\beta_{j}v_{j}+x_{0})-q(\beta_{j}^{*}v_{j}+x_{0})=\omega_{j}\left(\frac{\lambda_{j}}{\eta_{j}}s(\lambda_{j})\eta_{j}+1\right)^{2}$$
    $$=\omega_{j}(\lambda_{j}s(\lambda_{j})+1)^{2}=\omega_{j}p(\lambda_{j})^{2}$$
- en: where we define p(z) = zs(z)+1 (which is a general polynomial of degree i with
    constant coefficient 1).
  id: totrans-6154
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义 p(z) = zs(z)+1（这是一个度数为 i 的一般多项式，常数项为 1）。
- en: 'Summarizing, we have:'
  id: totrans-6155
  prefs: []
  type: TYPE_NORMAL
  zh: 总结如下：
- en: $$q(x)-q(x^{*})=(q(x)-q(x_{0}))-(q(x^{*})-q(x_{0}))\tag{20.13}$$ $$=\sum_{j=1}^{n}(q(\beta_{j}v_{j}+x_{0})-q(\beta_{j}^{*}v_{j}+x_{0}))=\sum_{j=1}^{n}\omega_{j}p(\lambda_{j})^{2}$$
  id: totrans-6156
  prefs: []
  type: TYPE_NORMAL
  zh: $$q(x)-q(x^{*})=(q(x)-q(x_{0}))-(q(x^{*})-q(x_{0}))\tag{20.13}$$ $$=\sum_{j=1}^{n}(q(\beta_{j}v_{j}+x_{0})-q(\beta_{j}^{*}v_{j}+x_{0}))=\sum_{j=1}^{n}\omega_{j}p(\lambda_{j})^{2}$$
- en: 'We now apply this results to CG. We know that after i steps, CG applied to
    q with initialization x0, finds the iterate xi which minimizes q(xi) subject to
    restriction xi −x0 ∈ Ki(*A, r*0). This is equivalent to minimizing over all possible
    p with the requirement that p has degree i with constant coefficient 1, or in
    other words:'
  id: totrans-6157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将这些结果应用于 CG。我们知道，在经过 i 步之后，CG 应用于 q，初始化为 x0，找到迭代 xi，以最小化 q(xi)，并且限制 xi −x0
    ∈ Ki(*A, r*0)。这相当于在所有可能的 p 上进行最小化，并要求 p 的度数为 i，常数项为 1，换句话说：
- en: $$q(x_{i})-q(x^{*})=\min_{p}\sum_{j=1}^{n}\omega_{j}p(\lambda_{j})^{2}\tag{20.14}$$
  id: totrans-6158
  prefs: []
  type: TYPE_NORMAL
  zh: $$q(x_{i})-q(x^{*})=\min_{p}\sum_{j=1}^{n}\omega_{j}p(\lambda_{j})^{2}\tag{20.14}$$
- en: Thus we see that CG is effectively picking a polynomial p to minimize the weighted
    sum of p2 evaluated at the different eigenvalues.
  id: totrans-6159
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们看到，CG 实际上是选择一个多项式 p 来最小化在不同特征值处评估的 p² 的加权和。
- en: '![508_image_0.png](508_image_0.png)'
  id: totrans-6160
  prefs: []
  type: TYPE_IMG
  zh: '![508_image_0.png](508_image_0.png)'
- en: Fig. 20.5. This figure demonstrates geometrically how the contribution to the
    polynomial p(z) of an additional root ν or ν in the vicinity of a small eigenvalue
    λ1 or a large eigenvalue λ2 (resp.) affects the loss term associated with the
    other eigenvalue.
  id: totrans-6161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.5. 该图在几何上演示了在小特征值 λ1 或大特征值 λ2（分别）附近，一个额外根 ν 或 ν 对多项式 p(z) 的贡献如何影响与其他特征值相关的损失项。
- en: In particular, the distance of the lines above or below the horizontal axis
    is equal to the factor whose square effectively multiplies the loss term associated
    with the given eigenvalue.
  id: totrans-6162
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，水平轴上方或下方的线的距离等于一个因子，其平方有效地乘以与给定特征值相关的损失项。
- en: As mentioned before, there are various results that make use of expressions
    similar to this one in order to prove results about how the distribution of the
    eigenvalues of A determine how quickly CG can make progress optimizing q. One
    particularly interesting result states that if the eigenvalues cluster into m
    groups, then since we can easily design a degree-m polynomial p which is relatively
    small in the vicinity of each cluster by placing a root of p at each cluster center,
    the error will be quite low by the m-th iteration [30].
  id: totrans-6163
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，有各种结果利用类似的表达式来证明关于 A 的特征值分布如何决定 CG 优化 q 的进展速度的结果。一个特别有趣的结果表明，如果特征值聚集成 m
    组，那么由于我们可以轻松设计一个在每个聚类附近相对较小的 m 次多项式 p，通过将 p 的根放在每个聚类中心，误差将在第 m 次迭代时非常低 [30]。
- en: However, the particular form of eqn. 20.14 and its derivation allow us to paint
    a more intuitive picture of how CG operates. Each of the terms in the sum 20.13
    correspond to a direction-restricted objective q(βjvj + x0) − q(β∗j vj + x0) =
  id: totrans-6164
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，公式 20.14 的特定形式及其推导使我们能够对 CG 的操作提供一个更直观的描述。和式 20.13 中的每一项对应于方向限制的目标 q(βjvj
    + x0) − q(β∗j vj + x0) =
- en: ωjp(λj )2 which are indirectly optimized by CG w.r.t. the βj 's. The size each
    of these "loss" terms negatively correlates with how much progress CG has made
    in optimizing x along the corresponding eigen-directions, and by examining the
    form of these terms, we can talk about how CG will "prioritize" these different
    terms (and hence the optimization of their associated eigen-directions) through
    its choice of an optimal polynomial p.
  id: totrans-6165
  prefs: []
  type: TYPE_NORMAL
  zh: ωjp(λj )2是CG间接优化相对于βj的。这些“损失”项的大小与CG在沿着相应特征方向优化x时取得的进展负相关，通过检查这些项的形式，我们可以讨论CG如何通过选择一个最佳多项式p来“优先考虑”这些不同的项（因此优化其相关的特征方向）。
- en: Firstly, consider the "weights" ωj = −(q(β∗j vj + x0) − q(x0)) = 12 η2j λj
  id: totrans-6166
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，考虑“权重”ωj = −(q(β∗j vj + x0) − q(x0)) = 12 η2j λj
- en: ', which measure the total decrease in q that can be obtained by fully optimizing
    along the associated direction vj . Their effect is thus to shift the focus of
    CG towards those eigen-directions which will give the most reduction. They are
    inversely proportional to the curvature λj , and proportional to η2j = (vj r0)2,
    which is the square of the size of the contribution of the eigen-direction within
    the initial residual (which in HF will be the gradient of f when x0 = 0), and
    this makes the ωj ''s "scale-invariant", in the sense that any linear reparameterization
    which preserves the eigenvectors of A, while possibly rescaling the eigenvalues,
    will have no effect on the ωj''s.'
  id: totrans-6167
  prefs: []
  type: TYPE_NORMAL
  zh: ，这衡量了通过沿着相关方向vj 完全优化所能获得的q的总减少。它们的效果因此是将CG的重点转向那些将提供最大减少的特征方向。它们与曲率λj 成反比，与η2j
    = (vj r0)2成正比，后者是特征方向在初始残差中的贡献的平方（在HF中，当x0 = 0时，将是f的梯度），这使得ωj是“尺度不变”的，意味着任何保持A的特征向量的线性重参数化，可能会重新缩放特征值，但对ωj没有影响。
- en: Secondly, we note the effect of the size of the λj 's, or in other words, the
    curvatures associated with vj 's. If it weren't for the requirement that p must
    have a constant term of 1, the λj 's probably wouldn't have any influence on CG's
    prioritizing of directions (beyond for how they modulate the weights ωj).
  id: totrans-6168
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们注意到λj的大小，换句话说，与vj相关的曲率的影响。如果不是因为p必须有一个常数项为1的要求，λj可能不会影响CG对方向的优先级（超出它们如何调节权重ωj）。
- en: 'But note that general polynomials of degree i with constant coefficient 1 must
    have the form:'
  id: totrans-6169
  prefs: []
  type: TYPE_NORMAL
  zh: 但请注意，常数项为1的一般多项式的形式必须是：
- en: $$p(z)=\prod_{k=1}^{i}\left(1-{\frac{z}{\nu_{k}}}\right)$$
  id: totrans-6170
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(z)=\prod_{k=1}^{i}\left(1-{\frac{z}{\nu_{k}}}\right)$$
- en: for νk ∈ C. We will argue by illustrative example that this fact implies that
    CG
  id: totrans-6171
  prefs: []
  type: TYPE_NORMAL
  zh: 对于νk ∈ C。我们将通过示例论证这一事实意味着CG
- en: will favor high-curvature directions, everything else being equal. Suppose there
    are two tight clusters of eigenvalues of A, a low-magnitude one located close
    to zero and a large-magnitude one located further away. Suppose also that they
    have equal total loss as measured by the sum of the associated ωjp(λj )2's (for
    the current p). Placing an additional root νk close to the large-magnitude cluster
    will greatly reduce the associated ωjp(λj )2 loss terms in that cluster, by effectively
    multiplying each by \#1 − λj νk
  id: totrans-6172
  prefs: []
  type: TYPE_NORMAL
  zh: 将偏好高曲率方向，其他条件相同。假设A的特征值有两个紧密的簇，一个是接近零的小幅度簇，另一个是更远的大幅度簇。假设它们的总损失通过相关的ωjp(λj )2的总和（对于当前的p）来衡量是相等的。在大幅度簇附近放置一个额外的根νk将大大减少该簇中的相关ωjp(λj
    )2损失项，实际上是将每个乘以\#1 − λj νk
- en: $2which will be small due to the closeness of νk to each λj . Meanwhile, for
    the λj 's in the small-magnitude cluster, the associated loss terms will be multiplied
    by \#1 − λj νk
  id: totrans-6173
  prefs: []
  type: TYPE_NORMAL
  zh: $2这将因为νk与每个λj的接近而变得很小。与此同时，对于小幅度簇中的λj，相关的损失项将乘以\#1 − λj νk
- en: $2which won't be greater than 1 since 0 < λj < νk, implying that these loss
    terms will not increase (in fact, they will very slightly decrease).
  id: totrans-6174
  prefs: []
  type: TYPE_NORMAL
  zh: $2不会大于1，因为0 < λj < νk，这意味着这些损失项不会增加（实际上，它们会非常轻微地减少）。
- en: Now contrast this with what would happen if we placed a root νk close to the
    small magnitude cluster. As before, the loss terms associated with that cluster
    will be greatly reduced. However, because λj ! νk for λj 's in the largemagnitude
    cluster, we will have \#1 − λj νk
  id: totrans-6175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在对比一下，如果我们将根νk放置在小幅度簇附近会发生什么。如前所述，与该簇相关的损失项将大大减少。然而，因为在大幅度簇中的λj ! νk，我们将有\#1
    − λj νk
- en: $2! 1 for such λj 's, and so the associated loss terms will greatly increase,
    possibly even resulting in a net increase in q. Thus CG, being optimal, will place
    the root near to the large-magnitude cluster in this situation, versus the small
    magnitude-one, despite convergence of either one yielding the same improvement
    in q.
  id: totrans-6176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样的 λj，相关的损失项将大幅增加，可能甚至导致 q 的净增加。因此，CG 作为最优方法，在这种情况下会将根放置在大幅度簇附近，而不是小幅度的簇，尽管两者的收敛都能带来
    q 的相同改善。
- en: 20.10 Initializing Cg
  id: totrans-6177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.10 初始化 Cg
- en: As in the previous section we will use the generic notation q(x) = 12xAx − bx
    to refer to the quadratic objective being optimized by CG.
  id: totrans-6178
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，我们将使用通用符号 q(x) = 12xAx − bx 来指代 CG 正在优化的二次目标。
- en: A useful property of CG is that it is able to make use of arbitrary initial
    guesses x0 for x. This choice can have a strong effect on the performance of CG,
    which is not surprising since xi depends strongly on the Krylov subspace Ki(*A,
    r*0) (where r0 = Ax0 − b), which in turn depends strongly on x0. From the perspective
    of the previous section, an initial x0 may be "more converged" than x = 0 along
    some eigen-directions and less converged along others, thus affecting the corresponding
    weights ωj , which measure the total reduction that can be obtained by fully optimizing
    eigen-direction vj (versus leaving it as it is in x0). This "redistribution" of
    weights caused by taking a different x0 may make the quadratic optimization easier
    or harder for CG to optimize, depending on how the eigenvalues and associated
    weights are distributed.
  id: totrans-6179
  prefs: []
  type: TYPE_NORMAL
  zh: CG 的一个有用特性是能够利用任意初始猜测 x0 进行 x 的优化。这个选择对 CG 的性能可能有很大影响，这并不令人惊讶，因为 xi 很大程度上依赖于
    Krylov 子空间 Ki(*A, r*0)（其中 r0 = Ax0 − b），而这又强烈依赖于 x0。从前一节的角度来看，初始 x0 可能在某些特征方向上“更收敛”，而在其他方向上则收敛较少，从而影响相应的权重
    ωj，这些权重衡量通过完全优化特征方向 vj 所能获得的总减少量（与保持 x0 不变相比）。由于选择不同的 x0 而导致的权重“重新分配”可能使 CG 的二次优化变得更容易或更困难，这取决于特征值和相关权重的分布。
- en: Since the local geometry of the error surface of f (and hence the local damped
    quadratic model q = Mˆ ) changes relatively slowly between updates (at least along
    some eigen-directions), this suggests using the previous update δk−1 as the starting
    solution x0 for CG, as was done by Martens [22].
  id: totrans-6180
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 f 的误差面局部几何（因此局部阻尼二次模型 q = Mˆ ）在更新之间变化相对缓慢（至少沿某些特征方向），这表明可以使用之前的更新 δk−1 作为
    CG 的起始解 x0，正如 Martens 所做的 [22]。
- en: In practice, this choice can result in an initial value of q which is higher
    than zero, and thus seemingly worse than just using x0 = 0, which satisfies q(x0)=0.
    x0 may not even be a descent direction, implying that q(x0) > 0 for all  > 0.
  id: totrans-6181
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这个选择可能导致初始 q 值高于零，因此看起来比使用满足 q(x0)=0 的 x0 = 0 更糟。x0 甚至可能不是一个下降方向，意味着对于所有
    > 0，q(x0) > 0。
- en: But these objections are based on the naive notion that the value of q tells
    us everything there is to know about the quality of potential initialization.
    What we observe in practice is that while CG runs initialized with x0 = δk−1 "start
    slow" (as measured by the value of q(x)), they eventually catch up and then surpass
    runs started from x0 = 0.
  id: totrans-6182
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些反对意见是基于一种幼稚的观点，即 q 的值告诉我们关于潜在初始化质量的所有信息。我们在实践中观察到，尽管使用 x0 = δk−1 初始化的 CG
    运行“开始较慢”（以 q(x) 的值来衡量），但它们最终会赶上并超过从 x0 = 0 开始的运行。
- en: To make sense of this finding, we first note it is easy to design initializations
    which will have arbitrarily high values of q, but which require only one CG step
    to reach the minimum. To do this, we simply take the minimizer of q and add a
    large multiple of one of the eigenvectors of A to it. This corresponds to the
    situation where only one eigenvalue λj has non-zero weight ωj , so that to make
    q(x) − q(x∗)=0 CG can simply select the degree-1 polynomial which places a root
    at λj .
  id: totrans-6183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一发现，我们首先注意到，设计出初始值任意高的初始化是很简单的，但只需一步 CG 就能达到最小值。为此，我们只需取 q 的最小化器并向其添加 A
    的一个特征向量的大倍数。这对应于只有一个特征值 λj 具有非零权重 ωj 的情况，因此要使 q(x) − q(x∗)=0，CG 可以简单选择一个在 λj 处有根的一次多项式。
- en: More generally, x0 may be more converged than 0 along eigen-directions which
    are more numerous, or which have have small and spread-out eigenvalues (i.e. curvatures),
    and meanwhile less converged than 0 (perhaps severely so) only along eigen-directions
    which are fewer in number, or have larger or more tightly clustered eigenvalues.
    If the later group has a larger total weight (given by the sum of the ωj 's as
    defined in the previous section) this will cause q(x0) > 0.
  id: totrans-6184
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般而言，x0在数量更多或特征值（即曲率）小且分散的特征方向上可能更接近0，而在数量较少或特征值更大或更紧密聚集的特征方向上则可能不那么接近0（甚至严重不接近）。如果后者的总权重较大（由上一节定义的ωj之和给出），这将导致q(x0)
    > 0。
- en: But since the former directions will be easier for CG to optimize than the latter,
    this implies that the given x0 will still be a highly preferable initialization
    over
  id: totrans-6185
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，由于前者的方向比后者更容易被CG优化，这意味着给定的x0仍然会是一个高度可取的初始化。
- en: '"safer" choice of 0, as long as CG is given enough iterations to properly optimize
    the later group of badly initialized but "easy-to-fix" eigen-directions.'
  id: totrans-6186
  prefs: []
  type: TYPE_NORMAL
  zh: “更安全”的0选择，只要CG给出足够的迭代来正确优化后者那组初始化不良但“易于修复”的特征方向。
- en: We surmise that the choice x0 = δk−1 fits into the situation described above,
    where the later group of eigen-directions correspond to the slowly optimized low-curvature
    directions that tend to remain descent-directions across many HF
  id: totrans-6187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们推测，选择x0 = δk−1符合上述情况，其中后者特征方向对应于缓慢优化的低曲率方向，往往在许多HF步骤中保持下降方向。
- en: iterations. Consistent with this theory, is our observation that the number
    of CG steps required to achieve q(x) < 0 from the initialization x0 = δk−1 tends
    to grow linearly with the number of CG steps used at the previous HF iteration9.
  id: totrans-6188
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代。与此理论一致的是，我们观察到，从初始化x0 = δk−1开始，实现q(x) < 0所需的CG步骤数量倾向于与上一个HF迭代中使用的CG步骤数量线性增长。
- en: 9 This is, incidentally, one reason why it is good to use a fixed maximum number
    of CG iterations at each HF step.
  id: totrans-6189
  prefs: []
  type: TYPE_NORMAL
  zh: 9 这恰好是为什么在每个HF步骤中使用固定的最大CG迭代次数是好的一个原因。
- en: Analogously to how the current update vector is "decayed" by a scalar constant
    when using gradient descent with momentum, we have found that it is helpful to
    slightly decay the initialization, taking x0 = ζδk−1 for some constant 0 ≤ ζ ≤
    1, such as 0.95.
  id: totrans-6190
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于在使用动量的梯度下降时，当前更新向量被一个标量常数“衰减”，我们发现轻微衰减初始化是有帮助的，取x0 = ζδk−1，其中某个常数0 ≤ ζ ≤
    1，例如0.95。
- en: Choosing this decay factor for HF carefully is not nearly as important as it
    can be for momentum methods. This is because while momentum methods modify their
    current update vectors by a single gradient-descent step, HF uses an entire run
    of CG, which can make much more significant changes. This allows HF to more easily
    scale back x0 along eigen-directions, which while they may have been helpful at
    the previous θ, are no longer appropriate to follow from the current θ.
  id: totrans-6191
  prefs: []
  type: TYPE_NORMAL
  zh: 精心选择HF的衰减因子并不像动量方法那样重要。这是因为动量方法通过单个梯度下降步骤来修改当前的更新向量，而HF使用整个CG运行，这可以产生更显著的变化。这使得HF能够更轻松地沿特征方向缩减x0，虽然在之前的θ中可能有帮助，但从当前θ出发则不再合适。
- en: In particular, x0 will be quickly "corrected" along turbulent directions of
    highcurvature, reducing (but not completely eliminating) the need for a decay
    to help "clean up" these directions.
  id: totrans-6192
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，x0将在高曲率的湍流方向上迅速“纠正”，减少（但并未完全消除）衰减的需要，以帮助“清理”这些方向。
- en: Our experience suggests that properly tuning the decay constant becomes more
    important as aggressive CG truncation, or other factors like weak preconditioning,
    limit CG's ability either to modify the update from its initial value x0, or to
    make good progress along important low-curvature directions. While the former
    problem calls for lowering ζ, the later calls for raising it. The optimal value
    will likely depend on the amount of truncation, the type of preconditioning, and
    the local geometry of the objective being optimized. ζ = 0.95 seems to be a good
    default value, but it may help to reduce it when using an approach which truncates
    CG very early. It may also be beneficial to increase it in the later stages of
    optimization where CG struggles much harder to optimize q along low curvature
    directions.
  id: totrans-6193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的经验表明，适当调整衰减常数变得越来越重要，因为激进的CG截断或其他因素（如弱预处理）限制了CG修改其初始值x0的能力，或在重要的低曲率方向上取得良好进展。前一个问题需要降低ζ，而后一个问题则需要提高它。最佳值可能依赖于截断的程度、预处理的类型以及被优化目标的局部几何形状。ζ
    = 0.95似乎是一个不错的默认值，但在使用早期截断CG的方法时，可能有助于将其降低。在优化的后期阶段，当CG在低曲率方向上优化q时，增加ζ也可能是有益的。
- en: 20.11 Preconditioning
  id: totrans-6194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.11 预处理
- en: As powerful as CG is, there are quadratics optimization problems which can be
    easily solved using more direct methods, that CG will struggle with. For example,
    if the curvature matrix is diagonal, CG will in general require i iterations to
    converge (where i is the number of distinct values on the diagonal) using a total
    of O(in) time. Meanwhile, we could easily solve the entire system by straightforward
    inversion of the diagonal curvature matrix in time O(n). CG is, in a sense, unaware
    of this special structure and unable to exploit it.
  id: totrans-6195
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CG功能强大，但仍有一些二次优化问题可以使用更直接的方法轻松解决，而CG则会遇到困难。例如，如果曲率矩阵是对角的，CG通常需要i次迭代才能收敛（其中i是对角线上不同值的数量），总时间为O(in)。与此同时，我们可以通过对角曲率矩阵的简单求逆在O(n)时间内轻松解决整个系统。CG在某种程度上对这种特殊结构并不敏感，也无法利用它。
- en: While the curvature matrix will in general not be diagonal or have any other
    special form that makes it easy to invert, there may nevertheless be cheap operations
    which can exploit information about the course structure of the curvature matrix
    to do some of the work in optimizing q, reducing the burden on CG.
  id: totrans-6196
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管曲率矩阵通常不会是对角形或具有任何其他使其易于求逆的特殊形式，但仍然可能存在廉价的操作，可以利用关于曲率矩阵结构的信息来优化q，从而减轻CG的负担。
- en: In the context of HF, preconditioning refers to the reparameterization of Mˆ
    10 according to some linear transformation relatively easy to invert, with the
    idea that CG will make more rapid progress per iteration optimizing w.r.t. the
    new parameterization.
  id: totrans-6197
  prefs: []
  type: TYPE_NORMAL
  zh: 在HF的上下文中，预处理指的是根据一些相对容易求逆的线性变换重新参数化Mˆ 10，目的是使CG在新参数化下每次迭代能更快速地取得进展。
- en: 10 We will use theˆ. notation for the damped quadratic and associated damped
    curvature matrix Bˆ since this is what CG will actually optimize when used within
    HF.
  id: totrans-6198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用ˆ.符号表示阻尼二次函数及相关的阻尼曲率矩阵Bˆ，因为这是CG在HF中实际优化的内容。
- en: Formally, given some invertible transformation defined by a matrix C, we transform
    the quadratic objective Mˆ (δ) by a change of coordinates δ = C−1γ and optimize
    w.r.t. γ instead of δ.
  id: totrans-6199
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，给定由矩阵C定义的一些可逆变换，我们通过坐标变换δ = C−1γ来转换二次目标Mˆ (δ)，并相对于γ进行优化，而不是δ。
- en: $$\hat{M}(C^{-1}\gamma)=\frac{1}{2}\gamma^{\top}C^{-\top}\hat{B}C^{-1}\gamma+\nabla
    f^{\top}C^{-1}\gamma$$
  id: totrans-6200
  prefs: []
  type: TYPE_NORMAL
  zh: $$\hat{M}(C^{-1}\gamma)=\frac{1}{2}\gamma^{\top}C^{-\top}\hat{B}C^{-1}\gamma+\nabla
    f^{\top}C^{-1}\gamma$$
- en: Applying preconditioning to CG is very easy and amounts to computing transformed
    residual vectors yi at each iteration, by solving P yi = ri, where P = CC
  id: totrans-6201
  prefs: []
  type: TYPE_NORMAL
  zh: 将预处理应用于CG非常简单，只需在每次迭代中计算变换后的残差向量yi，通过解P yi = ri，其中P = CC。
- en: (see alg. 20.2). This can be accomplished, say, by multiplication of ri by P
    −1, which for many common choices of P (such as diagonal approximations of Bˆ)
    is a cheap operation.
  id: totrans-6202
  prefs: []
  type: TYPE_NORMAL
  zh: （见算法20.2）。这可以通过将ri乘以P −1来实现，对于许多常见的P选择（如Bˆ的对角近似），这是一项廉价的操作。
- en: Preconditioning can be applied to other optimization methods, such as gradient
    descent, where it corresponds to a non-static linear reparameterization of the
    objective f that typically varies with each iteration, and amounts simply to multiplication
    of the gradient update by P −1. In fact, one way to view 2nd-order optimization
    is as a particular non-static preconditioning approach for gradient descent, where
    P is given by the curvature matrix B (or some approximation or Krylov subspace
    restriction of it).
  id: totrans-6203
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理也可以应用于其他优化方法，例如梯度下降，在这种情况下，它对应于目标 f 的非静态线性重参数化，通常随着每次迭代而变化，并简单地相当于将梯度更新乘以
    P −1。事实上，第二阶优化的一个观点是将其视为梯度下降的一种特定非静态预处理方法，其中 P 由曲率矩阵 B（或其某种近似或 Krylov 子空间限制）给出。
- en: 20.11.1 The Effects Of Preconditioning
  id: totrans-6204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.11.1 预处理的影响
- en: In section 20.9, we saw how the eigenvalues of the curvature matrix and the
    corresponding sizes of the contributions to the initial residual of each eigendirection
    effectively determine the convergence characteristics of CG, in terms of the "order"
    in which the eigen-directions tend to converge, and how quickly. It was found
    that each eigen-direction has an effective "weight" ωj (corresponding to the total
    decrease in q which can be obtained by completely optimizing it), and that CG
    prioritizes convergence along the eigen-directions both according to their weights
    and their associated curvature/eigenvalue, preferring larger values of both. Because
    CG is optimal, it will tend to make faster progress along directions whose the
    eigenvalues are close proximity to many other ones that are associated with directions
    of high-weight (due to its ability to make progress on many such directions at
    once when their eigenvalues are closely packed).
  id: totrans-6205
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 20.9 节中，我们看到曲率矩阵的特征值及其对应的特征方向对初始残差的贡献的大小有效地决定了共轭梯度法的收敛特性，具体体现在特征方向收敛的“顺序”以及速度上。研究发现，每个特征方向都有一个有效的“权重”
    ωj（对应于通过完全优化它可以获得的 q 的总减少），共轭梯度法优先沿特征方向收敛，这取决于它们的权重和相关的曲率/特征值，偏好更大的值。由于共轭梯度法是最优的，它会在特征值与许多与高权重方向相关的其他特征值接近的方向上更快地取得进展（因为它能够在这些特征值紧密分布时同时在许多这样的方向上取得进展）。
- en: Thus to understand how a potential preconditioning scheme affects the convergence
    of CG we can look at the eigen-distribution of the transformed curvature matrix
    C−BˆC−1, and the associated weights, which depend on the transformed initial residual
    C−(Bˆx0 − ∇f). Choices for C (or equivalently P) which yield tight clusters of
    eigenvalues should lead to overall faster convergence, at least along the directions
    which are contained in such clusters.
  id: totrans-6206
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了理解潜在的预处理方案如何影响共轭梯度法的收敛性，我们可以查看变换后的曲率矩阵 C−BˆC−1 的特征值分布及其相关权重，这些权重依赖于变换后的初始残差
    C−(Bˆx0 − ∇f)。选择能够产生紧凑特征值簇的 C（或等价的 P）应能导致整体更快的收敛，至少在包含这些簇的方向上。
- en: But as discussed in section 20.8.7, the eigenvectors and corresponding eigenvalue
    distribution will affect the order in which various directions converge, and this
    will interact in a non-trivial way with CG truncation damping. In particular,
    certain directions which would otherwise never be touched by CG in the original
    parameterization, either because their eigenvalues are located far away from any
    high-weight eigenvalue clusters, or because they have very low curvature (i.e.,
    low eigenvalue), could, within the reparameterization, become part of eigen-directions
    with the opposite properties, and thus be partially optimized by CG even when
    it is aggressively truncated.
  id: totrans-6207
  prefs: []
  type: TYPE_NORMAL
  zh: 但正如在第 20.8.7 节中讨论的，特征向量和相应的特征值分布将影响各种方向收敛的顺序，这将以非平凡的方式与共轭梯度法的截断阻尼相互作用。特别是，某些方向在原始参数化中本不会被共轭梯度法触及，可能是因为它们的特征值远离任何高权重特征值簇，或者因为它们的曲率非常低（即，特征值低），在重参数化过程中可能变成具有相反属性的特征方向，从而即使在积极截断的情况下也能被共轭梯度法部分优化。
- en: This is a potential problem, since it is our experience that certain very low
    curvature directions tend to be highly non-trustworthy for neural network training
    objectives (in the default parameterization). In particular, they often tend to
    correspond to degeneracies in the quadratic model, such as those introduced by
    using different sets of data to compute the gradient and curvature-matrix vector
    products (see section 20.12.1), or to directions which yield small reductions
    in q for the current minibatch but large increases on other training data (an
    issue called "minibatch overfitting", which is discussed in section 20.12.2).
  id: totrans-6208
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个潜在问题，因为我们的经验表明，某些非常低曲率方向在神经网络训练目标上往往不可靠（在默认参数化下）。特别是，它们往往对应于二次模型中的退化现象，例如通过使用不同的数据集来计算梯度和曲率矩阵向量积所引入的（见20.12.1节），或者对应于在当前小批量中导致q的小幅降低但在其他训练数据上造成大幅增加的方向（这被称为“小批量过拟合”，在20.12.2节中讨论）。
- en: 20.11.2 Designing A Good Preconditioner
  id: totrans-6209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.11.2 设计一个好的预条件器
- en: Designing a good preconditioner is an application specific art, especially for
    HF, and it is unlikely that any one preconditioning scheme will be the best in
    all situations. There will often be a trade-off between the computational efficiency
    of implementing the preconditioner and its effectiveness, both in terms of how
    it speeds of convergence of CG, and how it may reduce the effectiveness of CG
    truncation damping.
  id: totrans-6210
  prefs: []
  type: TYPE_NORMAL
  zh: 设计一个好的预条件器是一种特定于应用的艺术，尤其是在HF中，任何一种预条件方案不太可能在所有情况下都是最佳的。通常，在实现预条件器的计算效率与其有效性之间会存在权衡，包括其对CG收敛速度的加速效果，以及可能降低CG截断阻尼的有效性。
- en: While the previous section describes how a preconditioner can help in theory,
    in practice it is not obvious how to design one based directly on insights about
    eigen-directions and their prioritization.
  id: totrans-6211
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前一节描述了预条件器如何在理论上提供帮助，但在实践中，直接根据特征方向及其优先级的洞察来设计预条件器并不明显。
- en: An approach which is popular and often very effective in various domains where
    CG is used is to design P to be some kind of easily inverted approximation of
    the curvature matrix (in our case, Bˆ). While the ultimate purpose of preconditioning
    is to help CG optimize more effectively, which may conceivably be accomplished
    by less obvious choices for P, approximating Bˆ may be an easier goal to approach
    directly. Justifying this idea is the fact that when P = Bˆ, the preconditioned
    matrix is I, so CG will converge in one step.
  id: totrans-6212
  prefs: []
  type: TYPE_NORMAL
  zh: 一种在CG使用的各个领域中广受欢迎且通常非常有效的方法是设计P，使其成为某种容易逆的曲率矩阵（在我们这个案例中是Bˆ）的逼近。尽管预条件的最终目的是帮助CG更有效地优化，而这可能通过对P的非明显选择来实现，但直接逼近Bˆ可能是一个更容易达到的目标。支持这一观点的是，当P
    = Bˆ时，预条件矩阵为I，因此CG将一步收敛。
- en: Adopting the perspective that P should approximate Bˆ, the task of designing
    a good preconditioner becomes one of balancing approximation quality with practical
    concerns, such as the cost of multiplying by P −1.
  id: totrans-6213
  prefs: []
  type: TYPE_NORMAL
  zh: 采用P应近似Bˆ的观点，设计一个好的预条件器的任务变成了在逼近质量与实际问题（如乘以P −1的成本）之间取得平衡。
- en: Of course, "approximation quality" is a problematic concept, since the various
    ways we might want to define it precisely, such as via various matrix norms, may
    not correlate well with the effectiveness of P as a preconditioner. Indeed, CG
    is invariant to the overall scale of the preconditioner, and so while βBˆ would
    be an optimal preconditioner for any β > 0, it could be considered an arbitrarily
    poor approximation to Bˆ as β grows, depending on how we measure this.
  id: totrans-6214
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，“逼近质量”是一个有问题的概念，因为我们可能希望通过各种矩阵范数等方式来精确定义它，但这些方式可能与P作为预条件器的有效性相关性不高。实际上，CG对预条件器的整体尺度是不变的，因此虽然βBˆ对于任何β
    > 0都是一个最佳预条件器，但随着β的增加，它可能被视为对Bˆ的任意糟糕的逼近，这取决于我们如何衡量这一点。
- en: Diagonal P's are a very convenient choice due to the many nice properties they
    naturally possess, such as being full rank, easy to invert, and easy to store.
    They also tend to be quite effective for optimizing deep feed-forward neural networks,
    due to how the scale of the gradient and curvature w.r.t. the hidden activities
    grows or shrinks exponentially as we proceed backwards through the layers [4,
    18], and how each parameter is associated with a single layer. Without compensating
    for this with diagonal preconditioning, the eigenvalues of the effective curvature
    matrix will likely be much more "spread out" and thus harder for CG to deal with.
    By contrast, RNN optimization does not seem to benefit as much from diagonal preconditioning,
    as reported by Martens and Sutskever [23]. Despite how RNNs can possess per-timestep
    scale variations analogous to the per-layer scale variations sometimes seen with
    feed-forward nets, these won't manifest as differences in scales between any particular
    parameters (i.e. diagonal scale differences), due to the way each parameter is
    used at *every* time-step.
  id: totrans-6215
  prefs: []
  type: TYPE_NORMAL
  zh: 对角P是一个非常方便的选择，因为它们自然具备许多良好的属性，例如满秩、易于反转和易于存储。由于梯度和曲率相对于隐藏活动的规模在层间反向传播时呈指数增长或缩小，它们在优化深度前馈神经网络方面也通常非常有效，且每个参数与单个层相关联。如果不通过对角预条件进行补偿，有效曲率矩阵的特征值可能会“分散”得更广，因此更难以处理。相比之下，正如Martens和Sutskever
    [23]所报道的，RNN优化似乎并不太受益于对角预条件，尽管RNN可以具有类似于前馈网络中有时看到的逐层规模变化的逐时间步规模变化，但这些不会表现为任何特定参数之间的规模差异（即对角规模差异），因为每个参数在*每个*时间步都被使用。
- en: Many obvious ways of constructing non-diagonal preconditioners end up resulting
    in P's which are expensive and cumbersome to use when n is large. For example,
    if P or P −1 is the sum of a k-rank matrix and a diagonal, it will require O((k
    + 1)n) storage, which for very large n will be a problem (unless k is very small).
  id: totrans-6216
  prefs: []
  type: TYPE_NORMAL
  zh: 构建非对角预条件器的许多显而易见的方法最终导致P在n较大时使用昂贵且繁琐。例如，如果P或P −1是一个k秩矩阵和一个对角矩阵的和，它将需要O((k +
    1)n)的存储，对于非常大的n将是一个问题（除非k非常小）。
- en: A well-designed diagonal preconditioner P should represent a conservative estimate
    of the overall scale of each parameter, and while the diagonal of the curvature
    matrix is a natural choice in many situations, such as when the curvature matrix
    is diagonally dominant, it is seriously flawed for curvature matrices with a strong
    non-diagonal component. Nonetheless, building a diagonal preconditioner based
    on d = diag(B) ˆ (or an approximation of this) is a sensible idea, and forms the
    basis of the approaches taken by Martens [22] and Chapelle and Erhan [10]. However,
    it may be beneficial, as Martens [22] found, not to use d directly, but to choose
    P to be somewhere between diag(d) and a scalar multiple of the identity matrix.
    This has the effect of making it more gentle and conservative, and it works considerably
    better in practice. One way to accomplish this is by raising each entry of d (or
    equivalently, the whole matrix P) to some power 0 <ξ< 1, which will make P tend
    to the identity as ξ approaches 0.
  id: totrans-6217
  prefs: []
  type: TYPE_NORMAL
  zh: 一个设计良好的对角预条件器P应该代表每个参数整体规模的保守估计，尽管在许多情况下，曲率矩阵的对角线是一个自然的选择，比如当曲率矩阵是对角主导时，但对于具有强非对角成分的曲率矩阵，这种选择严重缺陷。然而，基于d
    = diag(B)（或其近似）构建对角预条件器是一个合理的想法，并且形成了Martens [22]以及Chapelle和Erhan [10]所采用的方法的基础。然而，正如Martens
    [22]发现的那样，可能更有利于不直接使用d，而是选择P介于diag(d)和单位矩阵的标量倍数之间。这使得它更温和和保守，并且在实践中效果显著更好。实现这一点的一种方法是将d的每个条目（或等效地，整个矩阵P）提升到某个0
    < ξ < 1的幂，这将使得当ξ接近0时，P趋向于单位矩阵。
- en: 'In situations where diagonal damping penalty terms like the Tikhonov term are
    weak or absent, it may also be beneficial to include an additional additive constant
    κ, which also has the effective of making P tend to a scalar multiple of the identity
    as κ grows so that we have:'
  id: totrans-6218
  prefs: []
  type: TYPE_NORMAL
  zh: 在像Tikhonov项这样的对角阻尼惩罚项薄弱或缺失的情况下，加入额外的加性常数κ也可能是有益的，这样也会使得P趋向于单位矩阵的标量倍数，随着κ的增长，公式为：
- en: P = (diag(d) + κI)
  id: totrans-6219
  prefs: []
  type: TYPE_NORMAL
  zh: P = (diag(d) + κI)
- en: ξ If there is information available about the coarse relative scale of the parameters,
    in the form of some vector s ∈ Rn, such as the reparameterized neural network
    example discussed in sec. 20.8.2, it may better to use κdiag(s) instead of κI.
  id: totrans-6220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有关于参数粗略相对规模的信息，以某个向量s ∈ Rn的形式，例如在第20.8.2节讨论的重新参数化神经网络示例，使用κdiag(s)可能比κI更好。
- en: It is important to emphasize that d should approximate diag(B) ˆ and not diag(B),
    since it is the latter curvature matrix which is used in the quadratic which CG
    actually optimizes. When D is a diagonal matrix, one should take
  id: totrans-6221
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是强调d应近似diag(B) ˆ而不是diag(B)，因为后者的曲率矩阵在CG实际优化的二次方程中使用。当D是对角矩阵时，应取
- en: D = Diag(B) + Λd
  id: totrans-6222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D = Diag(B) + Λd
- en: where the latter contribution can be computed independently and exactly (and
    not via the methods for approximating diag(B) which we will discuss next). Meanwhile,
    if the damping matrix D is non-diagonal, then one should take d = diag(B + λD)
  id: totrans-6223
  prefs: []
  type: TYPE_NORMAL
  zh: 后者的贡献可以独立且精确地计算（而不是通过我们接下来将讨论的近似diag(B)的方法）。同时，如果阻尼矩阵D是非对角的，则应取d = diag(B +
    λD)
- en: where we might in fact use the aforementioned methods in order to approximate
    the diagonal of B + λD together.
  id: totrans-6224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可能实际上会使用上述方法来近似计算B + λD的对角线。
- en: So far the discussion ignored the cost of obtaining the diagonal of a curvature
    matrix. Although it is easy to compute Hessian-vector products of arbitrary functions,
    there exists no efficient exact algorithm for computing the diagonal of the Hessian
    of a general nonlinear function (Martens et al. [24, sec. 4]), so approximations
    must be used. Lecun et al. [2] report an efficient method for computing the diagonal
    of the Gauss-Newton matrix, but close examination reveals that it is mathematically
    unsound (although it can still be viewed as a heuristic approximation).
  id: totrans-6225
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，讨论忽略了获得曲率矩阵对角线的成本。尽管计算任意函数的Hessian-向量乘积很容易，但没有有效的精确算法用于计算一般非线性函数的Hessian的对角线（Martens等人
    [24，sec. 4]），因此必须使用近似。Lecun等人[2]报告了一种有效的方法来计算Gauss-Newton矩阵的对角线，但仔细检查表明它在数学上是不合理的（尽管它仍然可以被视为启发式近似）。
- en: In case of the Gauss-Newton matrix, it is possible to obtain the exact diagonal
    at the cost of k runs of backpropagation, where k is the number of output units
    [6]. This approach can be generalized in the obvious way to compute the diagonal
    of the generalized Gauss-Newton matrix, and is feasible for classification problems
    with small numbers of classes, although not feasible for problems such as deep
    autoencoders or RNNs which have high-dimensional outputs. In the next sections,
    we describe two practical methods for approximating the diagonal of the GGN matrix
    regardless of the dimension of the output.
  id: totrans-6226
  prefs: []
  type: TYPE_NORMAL
  zh: 在Gauss-Newton矩阵的情况下，可以通过k次反向传播获得确切的对角线，其中k是输出单元的数量[6]。这种方法可以以明显的方式推广，以计算广义Gauss-Newton矩阵的对角线，并且对于类别数较少的分类问题是可行的，但对于具有高维输出的深度自编码器或RNN等问题则不可行。在接下来的部分中，我们将描述两种实用方法，用于近似GGN矩阵的对角线，而不考虑输出的维度。
- en: 20.11.3 The Empirical Fisher Diagonal
  id: totrans-6227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.11.3 经验费舍尔对角线
- en: One approach to approximating the diagonal of the GGN matrix G is to instead
    compute the diagonal of a related matrix for which exact computation of the diagonal
    is easier. For this purpose Martens [22] selected the Empirical Fisher Information
    matrix F, which is an approximation to the well-known Fisher information matrix
    [1] (which is itself related to the generalized Gauss-Newton matrix). The empirical
    Fisher Information matrix is given by
  id: totrans-6228
  prefs: []
  type: TYPE_NORMAL
  zh: 一种近似GGN矩阵G的对角线的方法是计算一个相关矩阵的对角线，该矩阵的对角线精确计算更容易。为此，Martens [22]选择了经验费舍尔信息矩阵F，它是著名的费舍尔信息矩阵[1]的近似（该矩阵与广义Gauss-Newton矩阵相关）。经验费舍尔信息矩阵表示为
- en: $$\mathrm{F}\equiv\sum_{i}\nabla f_{i}\nabla f_{i}^{\top}$$
  id: totrans-6229
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{F}\equiv\sum_{i}\nabla f_{i}\nabla f_{i}^{\top}$$
- en: 'where ∇fi is the gradient on case i. However, because of its special low-rank
    form, its diagonal is readily computable as:'
  id: totrans-6230
  prefs: []
  type: TYPE_NORMAL
  zh: 其中∇fi是案例i的梯度。然而，由于其特殊的低秩形式，其对角线可以很容易地计算为：
- en: $$\operatorname{diag}(\mathbf{F})=\sum_{i}\operatorname{sq}(\nabla f_{i})$$
  id: totrans-6231
  prefs: []
  type: TYPE_NORMAL
  zh: $$\operatorname{diag}(\mathbf{F})=\sum_{i}\operatorname{sq}(\nabla f_{i})$$
- en: where sq(x) denotes coordinate-wise square.
  id: totrans-6232
  prefs: []
  type: TYPE_NORMAL
  zh: 其中sq(x)表示坐标平方。
- en: Because the ∇fi's are available from the gradient computation ∇f = i ∇fi over
    the minibatch, additionally computing diag(F) over the same minibatch in parallel
    incurs no extra cost, save for the possible requirement of storing the
  id: totrans-6233
  prefs: []
  type: TYPE_NORMAL
  zh: 由于∇fi可以通过对小批量的梯度计算∇f = i ∇fi获得，因此在同一小批量上额外并行计算diag(F)不会产生额外的成本，除了可能需要存储的
- en: ∇fi's, which can be avoided for feed-forward networks but not RNNs. Algorithm
    20.11.3 computes the diagonal of the Empirical Fisher Information matrix without
    the extra storage. In the algorithm, each yi is a matrix with B columns which
    represent the activations of a minibatch with B cases, and sq(·) is the coordinate-wise
    square. It differs from algorithm 20.2 only in lines 9 and 10.
  id: totrans-6234
  prefs: []
  type: TYPE_NORMAL
  zh: ∇fi 的，这对于前馈网络可以避免，但对于 RNN 不可避免。算法 20.11.3 在没有额外存储的情况下计算经验费舍尔信息矩阵的对角线。在算法中，每个
    yi 是一个具有 B 列的矩阵，表示一个包含 B 个案例的微型批次的激活，而 sq(·) 是坐标方向上的平方。它与算法 20.2 的不同之处仅在于第 9 和第
    10 行。
- en: In general, it is possible to compute the sum of squares of gradients in a minibatch
    in parallel without storing the squares of the individual gradients (which is
    often prohibitively expensive) whenever the computational graph of Algorithm 20.6
    . An algorithm for computing the diagonal diag(F) of the Empirical Fisher Information
    matrix of a feedforward neural network (includes the standard forward pass)
  id: totrans-6235
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，可以在不存储单个梯度平方（这通常成本过高）的情况下并行计算微型批次中梯度的平方和，只要算法 20.6 的计算图可用。一个用于计算前馈神经网络的经验费舍尔信息矩阵的对角线
    diag(F) 的算法（包括标准的前向传播）
- en: '1: input: y0, θ mapped to (W1,...,W−1, b1*,...,b*−1)'
  id: totrans-6236
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入: y0, θ 映射到 (W1,...,W−1, b1*,...,b*−1)'
- en: '2: for all i from 1 to  − 1 do 3: xi+1 ← Wiyi + bi 4: yi+1 ← si+1(xi+1)'
  id: totrans-6237
  prefs: []
  type: TYPE_NORMAL
  zh: '2: 对于所有 i 从 1 到 − 1 循环 3: xi+1 ← Wiyi + bi 4: yi+1 ← si+1(xi+1)'
- en: '5: end for 6: dy ← ∂L(y;t)/∂y (t is the target)'
  id: totrans-6238
  prefs: []
  type: TYPE_NORMAL
  zh: '5: 结束循环 6: dy ← ∂L(y;t)/∂y (t 是目标)'
- en: '7: for i from  − 1 downto 1 do 8: dxi+1 ← dyi+1si+1(xi+1)'
  id: totrans-6239
  prefs: []
  type: TYPE_NORMAL
  zh: '7: 从  − 1 循环到 1 进行 8: dxi+1 ← dyi+1si+1(xi+1)'
- en: '9: Set entries of diag([F]) corresponding to Wi to be sq(dxi+1)sq(yi)'
  id: totrans-6240
  prefs: []
  type: TYPE_NORMAL
  zh: '9: 将 diag([F]) 中与 Wi 对应的条目设置为 sq(dxi+1)sq(yi)'
- en: '10: Set entries of diag([F]) corresponding to bi to be sq(dxi+1)1B'
  id: totrans-6241
  prefs: []
  type: TYPE_NORMAL
  zh: '10: 将 diag([F]) 中与 bi 对应的条目设置为 sq(dxi+1)1B'
- en: '11: dyi ← Wi dxi+1 12: end for 13: output: diag(F)'
  id: totrans-6242
  prefs: []
  type: TYPE_NORMAL
  zh: '11: dyi ← Wi dxi+1 12: 结束循环 13: 输出: diag(F)'
- en: the gradient makes precisely one additive contribution to every parameter for
    each case. In this case, it possible to add [∇fi]
  id: totrans-6243
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度对每个参数的贡献在每个案例中恰好是一次加性贡献。在这种情况下，可以添加 [∇fi]
- en: 2 j to the appropriate entry of diag(F) as soon as it is computed, so we need
    not allocate temporary storage for [∇fi]j for each i and j (rather, only each
    j).
  id: totrans-6244
  prefs: []
  type: TYPE_NORMAL
  zh: 2 j 一旦计算出来，就立即填入 diag(F) 的适当条目，因此我们无需为每个 i 和 j 分配临时存储 [∇fi]j（而只需为每个 j 分配）。
- en: However, when the computational graph of the derivative (for a given case i)
  id: totrans-6245
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当导数的计算图（对于给定的案例 i）
- en: makes multiple additive contributions to each [∇fi]j , it is necessary to allocate
    temporary storage for this quantity since we must square its total contribution
    before summing over the cases. Interestingly, this issue does not occur for the
    RNN's gradient computation, since without the per-i squaring, each contribution
    to [∇fi]j can be stored in a single vector for all the i's.
  id: totrans-6246
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个 [∇fi]j 的多次加性贡献，必须为该数量分配临时存储，因为我们必须在对所有案例求和之前平方其总贡献。有趣的是，对于 RNN 的梯度计算，这个问题并不存在，因为在没有逐个平方的情况下，每个对
    [∇fi]j 的贡献可以在所有 i 的单个向量中存储。
- en: 20.11.4 An Unbiased Estimator For The Diagonal Of G
  id: totrans-6247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.11.4 G 的对角线的无偏估计量
- en: Chapelle and Erhan [10] give a randomized algorithm for computing an unbiased
    estimate of the diagonal of the generalized Gauss-Newton matrix, which requires
    the same amount of work as computing the gradient. And just as with any unbiased
    estimate, this approach can be repeatedly applied, and the results averaged, to
    achieve more precise estimates.
  id: totrans-6248
  prefs: []
  type: TYPE_NORMAL
  zh: Chapelle 和 Erhan [10] 提供了一种随机算法，用于计算广义高斯-牛顿矩阵的对角线的无偏估计，该算法所需的工作量与计算梯度的工作量相同。就像任何无偏估计一样，这种方法可以反复应用，并对结果取平均，以获得更精确的估计。
- en: 'The method of Chapelle and Erhan is described in algorithm 20.7 below:'
  id: totrans-6249
  prefs: []
  type: TYPE_NORMAL
  zh: Chapelle 和 Erhan 的方法在下面的算法 20.7 中描述：
- en: 'Algorithm 20.7 . Computing an unbiased estimate of the diagonal of the GGN
    matrix 1: Sample v ∈ Rm from a distribution satisfying E[vv] = I'
  id: totrans-6250
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 20.7 . 计算 GGN 矩阵对角线的无偏估计 1: 从满足 E[vv] = I 的分布中抽样 v ∈ Rm'
- en: '2: output sq \#JL1/2v'
  id: totrans-6251
  prefs: []
  type: TYPE_NORMAL
  zh: '2: 输出 sq \#JL1/2v'
- en: $
  id: totrans-6252
  prefs: []
  type: TYPE_NORMAL
  zh: $
- en: 'As discussed in section 20.6, multiplication of an arbitrary v ∈ Rm by the
    Jacobian J of F can be performed efficiently by the usual back-propagation algorithm.
    The correctness of algorithm 20.7 is easy to prove:'
  id: totrans-6253
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在第 20.6 节中讨论的那样，任意 v ∈ Rm 乘以 F 的雅可比 J 可以通过常规的反向传播算法高效执行。算法 20.7 的正确性很容易证明：
- en: $$\mathbb{E}\left[\operatorname{sq}\left(J^{\top}L^{\prime\,1/2}{}^{\top}v\right)\right]=\mathbb{E}\left[\operatorname{diag}\left((J^{\top}L^{\prime\,1/2}{}^{\top}v)(J^{\top}L^{\prime\,1/2}{}^{\top}v)^{\top}\right)\right]$$
    $$=\operatorname{diag}\left(J^{\top}L^{\prime\,1/2}{}^{\top}\mathbb{E}[vv^{\top}]L^{\prime\,1/2}J\right)$$
    $$=\operatorname{diag}\left(J^{\top}L^{\prime\,1/2}{}^{\top}L^{\prime\,1/2}J\right)\quad(\operatorname{as}\mathbb{E}[vv^{\top}]=I)$$
    $$=\operatorname{diag}(J^{\top}L^{\prime\,\prime}J)$$ $$=\operatorname{diag}(\operatorname{G})$$
  id: totrans-6254
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbb{E}\left[\operatorname{sq}\left(J^{\top}L^{\prime\,1/2}{}^{\top}v\right)\right]=\mathbb{E}\left[\operatorname{diag}\left((J^{\top}L^{\prime\,1/2}{}^{\top}v)(J^{\top}L^{\prime\,1/2}{}^{\top}v)^{\top}\right)\right]$$
    $$=\operatorname{diag}\left(J^{\top}L^{\prime\,1/2}{}^{\top}\mathbb{E}[vv^{\top}]L^{\prime\,1/2}J\right)$$
    $$=\operatorname{diag}\left(J^{\top}L^{\prime\,1/2}{}^{\top}L^{\prime\,1/2}J\right)\quad(\operatorname{as}\mathbb{E}[vv^{\top}]=I)$$
    $$=\operatorname{diag}(J^{\top}L^{\prime\,\prime}J)$$ $$=\operatorname{diag}(\operatorname{G})$$
- en: where we have used the identity sq(x) = diag(xx).
  id: totrans-6255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们使用了恒等式 sq(x) = diag(xx)。
- en: Martens et al. [24], following the work of Chapelle and Erhan [10], introduced
    an efficient unbiased approximation method for estimating the entire Hessian or
    GGN matrix of a given function (or just their diagonals, if this is desired) with
    a cost also comparable to computing the gradient. In the special case of estimating
    the diagonal of the GGN matrix, the two methods are equivalent. Of practical import,
    Martens et el. [24] also proved that sampling the components of v uniformly from
    −1, 1 will produce lower variance estimates than will be obtained by sampling
    them from N(0, 1).
  id: totrans-6256
  prefs: []
  type: TYPE_NORMAL
  zh: Martens 等人 [24] 在 Chapelle 和 Erhan [10] 的工作基础上，引入了一种高效的无偏估计方法，用于估计给定函数的整个 Hessian
    或 GGN 矩阵（如果需要，也可以仅估计其对角线），其成本与计算梯度相当。在估计 GGN 矩阵的对角线的特殊情况下，这两种方法是等价的。实际重要的是，Martens
    等人 [24] 还证明了从 −1, 1 中均匀采样 v 的分量将产生比从 N(0, 1) 中采样获得的更低方差的估计。
- en: Computationally, the method of Chapelle and Erhan is very similar to the method
    for computing the diagonal of the Empirical Fisher Information that was described
    in the previous section. Indeed, while the latter compute sq J∇L,
  id: totrans-6257
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算上，Chapelle 和 Erhan 的方法与上一节中描述的计算经验 Fisher 信息对角线的方法非常相似。实际上，后者计算 sq J∇L，
- en: this method computes sq \#JL1/2v
  id: totrans-6258
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法计算 sq \#JL1/2v
- en: $for random v's, and so the methods have similar implementations. In particular,
    both estimates can be computed in parallel over cases in the minibatch, and they
    share the issue with temporary stored discussed in the previous section, which
    can be overcome in for feed-forward networks but not RNNs.
  id: totrans-6259
  prefs: []
  type: TYPE_NORMAL
  zh: $对于随机的 v，因此这两种方法的实现类似。特别是，这两种估计可以在小批量中的案例上并行计算，并且它们共享前一节讨论的临时存储问题，这在前馈网络中可以克服，但在
    RNN 中则不能。
- en: In our experience, both methods tend to produce preconditioners with similar
    properties and performance characteristics, although Chapelle and Erhan [10] found
    that in certain situations this unbiased estimate gave better results. One clear
    advantage of this method is that it can correctly account for structural damping,
    which is not done by using the empirical Fisher matrix, as the gradients of the
    structural damping objective are equal to zero. The disadvantage of this method
    is that due to the stochastic nature of the curvature estimates, there could be
    parameters with non-zero gradients whose diagonal estimates could nonetheless
    be very small or even zero (which will never happen with the diagonal of the Fisher
    matrix). The diagonal of the Fisher matrix also has the additional advantage that
    it can be computed in tandem with the gradient at virtually no extra cost.
  id: totrans-6260
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，这两种方法往往产生具有相似特性和性能特征的预条件器，尽管 Chapelle 和 Erhan [10] 发现，在某些情况下，这种无偏估计给出了更好的结果。这种方法的一个明显优势是，它能够正确考虑结构阻尼，而使用经验
    Fisher 矩阵则无法做到这一点，因为结构阻尼目标的梯度等于零。这种方法的缺点是，由于曲率估计的随机性质，可能会存在具有非零梯度的参数，其对角线估计可能非常小甚至为零（而
    Fisher 矩阵的对角线不会出现这种情况）。Fisher 矩阵的对角线还有一个额外的优势，即几乎没有额外成本的情况下，可以与梯度同时计算。
- en: 20.12 Minibatching
  id: totrans-6261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.12 小批量处理
- en: In modern machine learning applications, training sets can be very large, and
    a learning algorithm which processes all the examples in the training set to compute
    each parameter update (called "batch processing") will likely be very slow or
    even totally impractical [7]. Some training datasets may even be infinite and
    so it may not even make any sense to talk about an algorithm operating in batch-mode
    at all.
  id: totrans-6262
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代机器学习应用中，训练集可能非常大，处理训练集中所有示例以计算每个参数更新的学习算法（称为"批处理"）可能非常缓慢，甚至完全不切实际。一些训练数据集甚至可能是无限的，因此讨论一个算法在批处理模式下操作可能毫无意义。
- en: '"Online" or "stochastic" gradient algorithms like stochastic gradient descent'
  id: totrans-6263
  prefs: []
  type: TYPE_NORMAL
  zh: '"在线"或"随机"梯度算法如随机梯度下降'
- en: (SGD) can theoretically use gradient information computed on arbitrarily small
    subsets of the training set, called "minibatches", as long as the learning rate
    is sufficiently small. HF, on the other hand, uses minibatches to estimate the
    curvature matrix, which is used in a very strong way to produce large and sometimes
    aggressive parameter updates. These curvature matrix estimates may become increasingly
    low-rank and degenerate as the minibatch size shrinks (assuming no contribution
    from damping or weight-decay), which in some cases (see subsection 20.12.1) may
    lead to unbounded and nonsensical updates, although the damping mechanisms discussed
    in sec. 20.8 can compensate for this to some extent.
  id: totrans-6264
  prefs: []
  type: TYPE_NORMAL
  zh: （SGD）理论上可以在任意小的训练集子集上使用计算出的梯度信息，称为"小批量"，只要学习率足够小。另一方面，HF使用小批量来估计曲率矩阵，这在生成大且有时激进的参数更新时非常有效。随着小批量大小的缩小，这些曲率矩阵估计可能变得越来越低秩和退化（假设没有阻尼或权重衰减的贡献），在某些情况下（见第20.12.1小节）可能导致不受限和不合理的更新，尽管第20.8节讨论的阻尼机制在某种程度上可以对此进行补偿。
- en: But even without these more obvious degeneracy issues, it can be argued that,
    intuitively, the matrix B captures "soft-constraints" about how far we can go
    in any one direction before making things worse, and if the constraints relevant
    to a particular training case are not well approximated in the curvature estimated
    from the minibatch, the update δ obtained from optimizing M could easily make
    the objective f worse on such a case, perhaps severely so.
  id: totrans-6265
  prefs: []
  type: TYPE_NORMAL
  zh: 但即使没有这些更明显的退化问题，可以说，直观上，矩阵B捕捉了关于在某个方向上能走多远的"软约束"，如果与特定训练案例相关的约束在从小批量估计的曲率中没有得到良好近似，优化M所获得的更新δ可能会使该目标f在这种情况下变得更糟，甚至可能严重恶化。
- en: Thus 2nd-order methods like HF which must estimate the curvature only from the
    current minibatch, may not work nearly as well with very small minibatches. And
    while there are several strategies to deal with minibatches that are "too small",
    (as we will discuss in subsection 20.12.2), the benefits of using a 2ndorder method
    like HF may be diminished by their use.
  id: totrans-6266
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，像HF这样的二阶方法只能从当前的小批量估计曲率，可能在非常小的小批量上表现得不那么好。尽管有几种策略来处理"过小"的小批量（如我们将在第20.12.2小节讨论），但使用二阶方法如HF的好处可能会因其使用而减少。
- en: Fortunately, in the case of neural networks, there are elegant and natural implementations
    which exploit data-parallelism and vectorization to efficiently compute gradients
    and curvature matrix-vector products over minibatches (see section 20.7). For
    highly parallel architectures like GPUs, these tend to give an increase in computational
    cost which remains sublinear (as a function of minibatch size) up to and beyond
    minibatch sizes which are useful in HF.
  id: totrans-6267
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在神经网络的情况下，有优雅而自然的实现利用数据并行和向量化来高效计算梯度和曲率矩阵-向量乘积，适用于小批量（见第20.7节）。对于像GPU这样的高度并行架构，这些通常会导致计算成本的增加，但在小批量大小的函数中保持亚线性，直到超过HF中有用的小批量大小。
- en: 20.12.1 Higher Quality Gradient Estimates
  id: totrans-6268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.12.1 更高质量的梯度估计
- en: Unlike 1st order optimization schemes like SGD where the number of iterations
    required to approach a good solution can reach as high as 105 −107, the number
    of iterations required by a strong 2nd-order optimizer like HF is in our experience
    orders of magnitude smaller, and usually around 102−103. While the linear term
    b = −∇f(θk−1) passed to CG needs to be computed only once for each update, CG
    may require on the order of 102 − 103 matrix-vector products with the curvature
    matrix A = B to produce each update. These matrix-vector products are by far the
    most computationally costly part of any truncated Newton approach.
  id: totrans-6269
  prefs: []
  type: TYPE_NORMAL
  zh: 与像SGD这样的1阶优化方案不同，接近良好解决方案所需的迭代次数可高达105 −107，而强大的2阶优化器如HF所需的迭代次数在我们经验中则低了几个数量级，通常在102−103左右。虽然线性项b
    = −∇f(θk−1)在每次更新时仅需计算一次，但CG可能需要在102 − 103的范围内进行矩阵-向量乘法，以生成每次更新。这些矩阵-向量乘法是任何截断牛顿方法中计算成本最高的部分。
- en: It may therefore be cost-effective to compute the gradient on a much larger
    minibatch than is used to compute the matrix-vector products. Martens [22] recommended
    using this technique (as does Byrd et al. [8]), and in our experience it can often
    improve optimization speed if used carefully and in the right contexts.
  id: totrans-6270
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在比用于计算矩阵-向量乘法的小批量更大的小批量上计算梯度可能是成本效益高的。Martens [22]建议使用此技术（Byrd等人 [8] 也建议如此），在我们经验中，如果在正确的上下文中谨慎使用，它往往可以提高优化速度。
- en: But despite this, there are several good theoretical reasons why it might be
    better, at least in some situations, to use the same minibatch to compute gradient
    and curvature matrix-vector products. These have been corroborated by our practical
    experience.
  id: totrans-6271
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，有几个理论理由说明，在某些情况下，使用相同的小批量计算梯度和曲率矩阵-向量乘法可能更好。这些已通过我们的实践经验得到证实。
- en: We will refer to the minibatches used to compute the gradient and curvature
    matrix-vector products as the "gradient minibatch" and "curvature minibatch",
    respectively.
  id: totrans-6272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于计算梯度和曲率矩阵-向量乘法的小批量称为“梯度小批量”和“曲率小批量”。
- en: 'When the gradient and curvature minibatches are equal, and the GGN curvature
    matrix is used, the quadratic model M maintains its interpretation as the local
    Taylor series approximation of a convex function, which will simply be the approximation
    of f obtained by linearizing F (see eqn. 20.9), but restricted to the data in
    the current minibatch. In such a situation, with some additional reasonable assumptions
    about the convex function L (strong convexity would be sufficient, but is more
    than what is needed), the quadratic model M can be written as:'
  id: totrans-6273
  prefs: []
  type: TYPE_NORMAL
  zh: 当梯度和曲率的小批量相等，并且使用GGN曲率矩阵时，二次模型M保持其作为凸函数局部泰勒级数近似的解释，这将简单地是通过线性化F（见公式20.9）获得的f的近似，但仅限于当前小批量中的数据。在这种情况下，结合对凸函数L的一些额外合理假设（强凸性就足够，但超出了需要），二次模型M可以写为：
- en: $$M(\delta)=\frac{1}{2}\delta^{\top}\mathrm{B}\delta+\nabla f_{k-1}^{\top}\delta+f(\theta_{k-1})=\frac{1}{2}\delta^{\top}J^{\top}L^{\prime\prime}J\delta+\delta^{\top}(J^{\top}\nabla
    L)+f(\theta_{k-1})$$ $$=\frac{1}{2}(J\delta)^{\top}L^{\prime\prime}(J\delta)+(J\delta)^{\top}L^{\prime\prime}L^{\prime\prime-1}\nabla
    L+\nabla L^{\top}L^{\prime\prime-1}L^{\prime\prime}L^{\prime\prime-1}\nabla L$$
    $$-\nabla L^{\top}L^{\prime\prime-1}L^{\prime\prime}L^{\prime\prime-1}\nabla L+f(\theta_{k-1})$$  1.
  id: totrans-6274
  prefs: []
  type: TYPE_NORMAL
  zh: $$M(\delta)=\frac{1}{2}\delta^{\top}\mathrm{B}\delta+\nabla f_{k-1}^{\top}\delta+f(\theta_{k-1})=\frac{1}{2}\delta^{\top}J^{\top}L^{\prime\prime}J\delta+\delta^{\top}(J^{\top}\nabla
    L)+f(\theta_{k-1})$$ $$=\frac{1}{2}(J\delta)^{\top}L^{\prime\prime}(J\delta)+(J\delta)^{\top}L^{\prime\prime}L^{\prime\prime-1}\nabla
    L+\nabla L^{\top}L^{\prime\prime-1}L^{\prime\prime}L^{\prime\prime-1}\nabla L$$
    $$-\nabla L^{\top}L^{\prime\prime-1}L^{\prime\prime}L^{\prime\prime-1}\nabla L+f(\theta_{k-1})$$  1.
- en: $$\begin{array}{r l}{{}}&{{}-{\mathrm{~v}}}\\ {}&{{}}\\ {={\frac{1}{2}}{\left(J\delta+\nabla
    L\right)}^{\top}L^{\prime\prime}{\left(J\delta+\nabla L\right)}^{\top}+c}\\ {}&{{}}\\
    {={\frac{1}{2}}{\|J\delta+L^{\prime\prime-1}\nabla L\|}_{L^{\prime\prime}}^{2}+c}\end{array}$$
  id: totrans-6275
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{r l}{{}}&{{}-{\mathrm{~v}}}\\ {}&{{}}\\ {={\frac{1}{2}}{\left(J\delta+\nabla
    L\right)}^{\top}L^{\prime\prime}{\left(J\delta+\nabla L\right)}^{\top}+c}\\ {}&{{}}\\
    {={\frac{1}{2}}{\|J\delta+L^{\prime\prime-1}\nabla L\|}_{L^{\prime\prime}}^{2}+c}\end{array}$$
- en: where c = f(θk−1) − ∇LL−1∇L and all quantities are computed only on the current
    minibatch. Here we have used the fact that L is invertible, which follows from
    the fact that L is strongly convex.
  id: totrans-6276
  prefs: []
  type: TYPE_NORMAL
  zh: 其中c = f(θk−1) − ∇LL−1∇L，所有量仅在当前小批量上计算。在这里，我们利用了L是可逆的这一事实，这来自于L是强凸的事实。
- en: 'This result is interesting because it applies only when B is the generalized
    Gauss-Newton matrix (instead of the Hessian), and it establishes a bound on the
    maximum improvement in f that the quadratic model M can ever predict:'
  id: totrans-6277
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果很有趣，因为它仅在 B 是广义高斯-牛顿矩阵（而不是 Hessian）时适用，并且它为二次模型 M 预测的 f 的最大改进建立了界限：
- en: ∇LL−1∇L, a quantity which does not depend on the properties of the network,
    only on its current predictions and the associated convex loss function.
  id: totrans-6278
  prefs: []
  type: TYPE_NORMAL
  zh: ∇LL−1∇L，这是一个不依赖于网络属性的量，仅依赖于其当前预测和相关的凸损失函数。
- en: Such a boundedness result does not exist whenever M is estimated using different
    minibatches for the gradient and curvature. In this case, the estimated gradient
    may easily lie outside the column space of the estimated curvature matrix in the
    sense that there may exist directions d s.t. gd < 0, d = 1, but dBd = 0. In such
    a case it is easy to see that the quadratic model is unbounded and M(αd) → −∞
    as α → ∞. While boundedness can be guaranteed with the inclusion of damping penalty
    terms which ensure that the damped curvature matrix Bˆ k is positive definite,
    and will also be guaranteed when the curvature matrix B is full rank, it may be
    the case that the boundedness is "weak" in the sense that dBd may be non-zero
    but extremely small, leading to a nearly degenerate update δ. For example, when
    using Tikhonov damping then we know that dBˆkd ≥ λ, but in order for this to sufficiently
    constrain the update along direction d, λ may have to be large enough that it
    would impose unreasonably high constraints on optimization in all directions.
  id: totrans-6279
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的有界性结果在使用不同的小批量来估计梯度和曲率时并不存在。在这种情况下，估计的梯度可能很容易落在估计的曲率矩阵的列空间之外，意味着可能存在方向 d，使得
    gd < 0，且 d = 1，但 dBd = 0。在这种情况下，可以很容易看出二次模型是无界的，并且 M(αd) → −∞ 当 α → ∞。虽然通过包含阻尼惩罚项可以保证有界性，这确保了阻尼曲率矩阵
    Bˆ k 是正定的，而且当曲率矩阵 B 是满秩时也会得到保证，但有可能出现“弱”有界性的情况，即 dBd 可能非零但极其小，导致近乎退化的更新 δ。例如，当使用
    Tikhonov 阻尼时，我们知道 dBˆkd ≥ λ，但为了使其在方向 d 上足够限制更新，λ 可能需要大到对所有方向的优化施加不合理的高约束。
- en: More intuitively, the gradient represents a linear reward for movement in certain
    directions d (the strength of which is given by gd) while the curvature matrix
    represents a quadratic penalty. If we include the linear rewards associated with
    a subset of cases without also including the corresponding quadratic penalties,
    then there is a chance that this will lead to a degenerate situation where some
    directions will have lots of reward (predicted linear reduction) without any corresponding
    penalty (curvature). This can result in an update which makes f worse even on
    cases contained in both the gradient and curvature minibatches, for reasons that
    have nothing directly to do with a breakdown in the reliability of the quadratic
    approximation to f. On the other hand, if the curvature and gradient minibatches
    are equal and the quadratic approximation to f is otherwise reliable (or properly
    damped), then using equal gradient and curvature minibatches provides a minimal
    guarantee that f will improve on the current minibatch after the proposed update
    is applied.
  id: totrans-6280
  prefs: []
  type: TYPE_NORMAL
  zh: 更直观地说，梯度代表了在某些方向 d 上移动的线性奖励（其强度由 gd 给出），而曲率矩阵代表了二次惩罚。如果我们在不同时考虑对应的二次惩罚的情况下包含与某些情况相关的线性奖励，那么这可能导致一种退化情况，其中某些方向会有大量奖励（预测的线性减少），而没有任何对应的惩罚（曲率）。这可能导致更新使
    f 更糟，即使在包含梯度和曲率小批量的情况下，原因与二次近似 f 的可靠性失效没有直接关系。另一方面，如果曲率和梯度小批量相等，并且对 f 的二次近似在其他方面是可靠的（或适当阻尼），那么使用相等的梯度和曲率小批量提供了一个最低限度的保证，确保在应用提议的更新后
    f 会在当前小批量上有所改善。
- en: Another more subtle way in which using a smaller-sized curvature minibatch than
    gradient minibatch could be counterproductive, is that in addition to causing
    a dangerous underestimation of the curvature associated with the left-out cases,
    it may also lead to an *over*estimation of the curvature for the cases actually
    in the curvature-minibatch. This is because when we compute estimates of the curvature
    by averaging, we must divide by the number of cases in the minibatch, and since
    this number will be smaller for the curvature estimate than for the gradient,
    the gradient contributions from these cases will be smaller in proportion to the
    corresponding curvature terms.
  id: totrans-6281
  prefs: []
  type: TYPE_NORMAL
  zh: 使用比梯度小的曲率小批量的另一种更微妙的反效果是，除了导致对被排除案例的曲率产生危险的低估外，它还可能导致对实际在曲率小批量中的案例的*过*高估。这是因为在通过平均计算曲率估计时，我们必须除以小批量中的案例数量，而由于这个数字对于曲率估计会比对于梯度小，因此这些案例的梯度贡献与相应的曲率项相比会显得较小。
- en: Byrd et al. [8] showed that if the eigenvalues of the curvature matrices (estimated
    using any method) are uniformly bounded from below in the sense that there exists
    μ > 0 s.t. Bˆ − μI is PSD for all possible Bˆ's which we might produce, then assuming
    the use of a basic line-search and other mild conditions, an truncated Newton
    algorithm like HF which estimates the gradient on the full training set will converge
    in the sense that the gradients will go to zero. But this result makes no use
    of the particular form of B and is as a result very weak, saying nothing about
    the rate of convergence, or how small the updates will have to be. As far as theorem
    is concerned, B can be any λ dependent matrix with the required boundedness property,
    and need not have anything to do with local quadratic models of f at all.
  id: totrans-6282
  prefs: []
  type: TYPE_NORMAL
  zh: Byrd等人[8]表明，如果曲率矩阵的特征值（使用任何方法估计）在下方是均匀有界的，即存在μ > 0，使得对于我们可能产生的所有Bˆ，Bˆ − μI是PSD，那么假设使用基本的线搜索和其他温和条件，像HF这样的截断牛顿算法（它在整个训练集上估计梯度）将会收敛，即梯度将趋近于零。但这个结果没有利用B的特定形式，因此非常薄弱，并没有说明收敛的速率或更新需要多小。就定理而言，B可以是任何具有所需有界性的λ相关矩阵，根本不需要与f的局部二次模型有关。
- en: Despite all of these objections, the higher quality estimates of the gradient
    may nonetheless provide superior convergence properties in some situations. The
    best trade-off between these various factors is likely to be highly dependent
    on the particular problem, the stage of the optimization (early versus late),
    and the damping mechanisms being used. Our experience is that penalty and CGtruncation
    damping become more active when there is a significant qualitative mismatch between
    the gradient and curvature estimates, which is more likely to happen when the
    training dataset, or the network's responses to it, are particular
  id: totrans-6283
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在所有这些反对意见，梯度的高质量估计在某些情况下仍可能提供更优的收敛特性。这些各种因素之间的最佳权衡很可能高度依赖于特定问题、优化的阶段（早期与晚期）以及使用的阻尼机制。我们的经验是，当梯度与曲率估计之间存在显著的定性不匹配时，惩罚和CG截断阻尼会变得更加活跃，这种情况在训练数据集或网络对其的响应特别时更容易发生。
- en: '"diverse".'
  id: totrans-6284
  prefs: []
  type: TYPE_NORMAL
  zh: “多样化”。
- en: 20.12.2 Minibatch Overfitting And Methods To Combat It
  id: totrans-6285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.12.2 小批量过拟合及其对策。
- en: As mentioned in the previous section, the updates produced by HF may be effectively
    "overfit" to the current minibatch of training data. While a single update of
    SGD has the same problem, this is less of an issue because the updates are extremely
    cheap and numerous. HF, by contrast, performs a run of CG with anywhere between
    10 to 300+ iterations, which is a long and expensive process, and must be performed
    using the same fixed estimates of the gradient and curvature from a single minibatch.
    Ideally, we could use a stochastic algorithm when optimizing the local quadratic
    models which would be able to see much more data at no extra cost. Unfortunately
    we are not aware of any batch methods which possess the same strongly optimal
    performance for optimizing quadratics as CG does, while also working well as a
    stochastic method.
  id: totrans-6286
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，HF产生的更新可能有效地“过拟合”当前小批量的训练数据。虽然单次SGD更新也存在同样的问题，但由于更新非常便宜且数量众多，因此问题较小。相比之下，HF执行的CG运行有10到300+次迭代，这是一个漫长且昂贵的过程，并且必须使用来自单个小批量的相同固定梯度和曲率估计。理想情况下，我们可以在优化局部二次模型时使用随机算法，这样可以以零额外成本看到更多的数据。不幸的是，我们不知道任何批处理方法在优化二次函数时具有与CG相同的强最优性能，同时在随机方法中也表现良好。
- en: The simplest solution to dealing with the minibatch overfitting problem is to
    increase the size of the minibatches, thus providing CG with more accurate estimates
    of the gradient and curvature. When optimizing neural networks, we have observed
    that the minibatch overfitting problem becomes gradually worse as optimization
    proceeds, and so implementing this solution will require continually growing the
    minibatches, possibly without bound. Fortunately, there are other ways of dealing
    with this problem.
  id: totrans-6287
  prefs: []
  type: TYPE_NORMAL
  zh: 处理小批量过拟合问题的最简单解决方案是增加小批量的大小，从而为CG提供更准确的梯度和曲率估计。在优化神经网络时，我们观察到小批量过拟合问题在优化进行时逐渐恶化，因此实施此解决方案将需要不断增加小批量的大小，可能没有上限。幸运的是，还有其他处理该问题的方法。
- en: The damping methods discussed in section 20.8 were developed to compensate for
    untrustworthiness of the local quadratic approximations M being made to f, which
    exists due to the simple fact that f is not actually a convex quadratic, and so
    M may fail to be a sensible approximation to f at its minimum δ∗. These methods
    work by imposing various soft or hard constraints on the update δ in order to
    keep it "closer" to 0 (where M trustworthy by construction), according to some
    metric.
  id: totrans-6288
  prefs: []
  type: TYPE_NORMAL
  zh: 在20.8节中讨论的阻尼方法是为了补偿对局部二次逼近M的不可靠性而开发的，这种不可靠性源于简单的事实：f实际上并不是一个凸二次函数，因此M在其最小值δ∗处可能无法合理地逼近f。这些方法通过对更新δ施加各种软约束或硬约束，以便根据某种度量使其“更接近”0（在此处，M是通过构造而可靠的）。
- en: Using minibatches to compute the gradient and curvature imposes a different
    kind of untrustworthiness on the quadratic model, arising from the fact that the
    function being approximated is not actually the true objective but rather just
    an unbiased sample-based approximation of it. But despite the differing nature
    of the source of this untrustworthiness, the previously developed damping methods
    turn out to be well suited to the task of compensating for it, in our experience.
  id: totrans-6289
  prefs: []
  type: TYPE_NORMAL
  zh: 使用小批量计算梯度和曲率对二次模型施加了另一种不可靠性，源于被逼近的函数实际上并不是真正的目标，而仅仅是对其的无偏样本逼近。尽管这种不可靠性的来源性质不同，但根据我们的经验，之前开发的阻尼方法非常适合补偿这一点。
- en: Of these, decreasing the maximum number of CG steps, using larger minbatches
    for the gradient, and decreasing the default learning rate (which is equivalent
    to damping by adding multiples of B, as discussed in section 20.8.3) according
    to some SGD-like schedule, seem to be the most effective approaches in practice.
    If standard Tikhonov damping is used and its strength λ is increased to compensate
    for minibatch overfitting, this will make HF behave asymptotically like SGD with
    a dynamically adjusted learning rate.
  id: totrans-6290
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些方法中，减少CG步骤的最大数量、使用更大的小批量进行梯度计算，以及根据某种类似SGD的计划降低默认学习率（这等同于通过添加B的倍数进行阻尼，如20.8.3节中讨论的那样）似乎是实践中最有效的方法。如果使用标准的Tikhonov阻尼并且其强度λ被增加以补偿小批量过拟合，这将使HF表现得渐近于具有动态调整学习率的SGD。
- en: There is a compelling analogy between the minibatch overfitting which occurs
    when optimizing δ with CG, and the general overfitting of a non-linear optimizer
    applied to a conventional learning problem. And some of the potential solutions
    to both of these problems turn out to be analogous as well. Tikhonov damping,
    for example, is analogous to an L2 prior or "weight-decay" penalty (but centered
    at δ = 0), and CG truncation is analogous to "early-stopping".
  id: totrans-6291
  prefs: []
  type: TYPE_NORMAL
  zh: 在用CG优化δ时，发生的小批量过拟合与应用于传统学习问题的非线性优化器的一般过拟合之间有着令人信服的类比。这两个问题的一些潜在解决方案也发现是类似的。例如，Tikhonov阻尼类似于L2先验或“权重衰减”惩罚（但以δ
    = 0为中心），而CG截断则类似于“早停”。
- en: Recall that damping approaches are justified as a method for dealing with quadratic
    approximation error because as the "size" of the update shrinks (according to
    any reasonable metric), it will eventually lie inside a region where this source
    of error must necessarily be negligible. This is due to the simple fact that any
    super-linear terms in the Taylor series expansion of f, which are unmodeled by
    M, will approach zero much more rapidly than the size of the update. It is important
    to keep in mind that a similar justification *does not* apply to the handling
    of minibatch-related estimation errors with update damping methods. Indeed, the
    negative gradient computed on a given minibatch may not even be a descent direction
    for the total objective (as computed on the complete training set), and even an
    infinitesimally small update computed from a given minibatch may actually make
    the total objective worse.
  id: totrans-6292
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，阻尼方法被证明是一种处理二次逼近误差的有效方法，因为随着更新的“大小”缩小（根据任何合理的度量），它最终将落入一个该误差源必然微不足道的区域。这是因为f的泰勒级数展开中的任何超线性项，这些项未被M建模，将比更新的大小更快接近于零。重要的是要记住，类似的理由*并不*适用于用更新阻尼方法处理与小批量相关的估计误差。实际上，在给定小批量上计算的负梯度甚至可能不是总目标（如在完整训练集上计算）的下降方向，甚至从给定小批量计算的微不足道的更新可能实际上会使总目标变得更糟。
- en: Thus, when tuning damping mechanisms to handle minibatch overfitting (either
    by hand, or dynamically using an automatic heuristic), one shouldn't aim for obtaining
    a reduction in the total f that is a fixed multiple of that which is predicted
    by the minibatch-computed M (as is done in section 20.8.5), but rather to simply
    obtain a more modest reduction which is proportional to the relative contribution
    of the current minibatch to the entire training dataset.
  id: totrans-6293
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在调整阻尼机制以处理小批量过拟合时（无论是手动还是动态使用自动启发式方法），不应目标为获得一个固定倍数的总f减少，这种减少是由小批量计算的M所预测的（如第20.8.5节所做），而应简单地获得一个与当前小批量对整个训练数据集的相对贡献成比例的更适度的减少。
- en: It is also worthwhile noting that the practice of initializing CG from the update
    computed at the previous iteration of HF (as discussed in section 20.10)
  id: totrans-6294
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得注意的是，从HF的上一个迭代计算的更新初始化CG的做法（如第20.10节所讨论）。
- en: seems to bias CG towards finding solutions that generalize better to data outside
    of the current minibatch. We don't have a good understanding for why this happens,
    but one possible explanation is that by carrying δ over to each new run of CG
    and performing an incomplete optimization on it using new data, δ is allowed to
    slowly grow (as HF iterates) along low-curvature directions11 that by necessity,
    must generalize across lots of training data. The reasoning is that if such slowly
    optimized directions didn't generalize well, then they would inevitably be detected
    as high-curvature ascent directions for some new minibatch and quickly zeroed
    out by CG before ever having a chance grow large in δ.
  id: totrans-6295
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎使CG偏向于找到对当前小批量外的数据更好泛化的解决方案。我们对为什么会发生这种情况并没有很好的理解，但一个可能的解释是，通过将δ带入每次新的CG运行，并使用新数据对其进行不完全优化，允许δ沿着低曲率方向缓慢增长（随着HF迭代），这些方向必然需要在大量训练数据上进行泛化。推理是，如果这样的缓慢优化方向泛化得不好，那么它们必然会被检测为某个新小批量的高曲率上升方向，并在有机会使δ增大之前被CG迅速归零。
- en: Finally, Byrd et al. [9] has developed methods to deal with the minibatch overfitting
    problem, which are based on heuristics that increase the minibatch size and also
    terminate CG early, according to estimates of the variance of the gradient and
    curvature-matrix vector products. While this is a potentially effective approach
    (which we don't have experience with), there are several problems with it, in
    theory. First, variance is measured according to highly
  id: totrans-6296
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Byrd 等人 [9] 开发了一些应对迷你批次过拟合问题的方法，这些方法基于根据梯度和曲率矩阵向量乘积的方差估计来增加迷你批次大小并提前终止 CG
    的启发式。虽然这是一种潜在有效的方法（我们没有这方面的经验），但从理论上讲，它存在几个问题。首先，方差是根据高度
- en: 11 Which get optimized much more slowly by CG than high-curvature directions,
    as shown in section 20.9
  id: totrans-6297
  prefs: []
  type: TYPE_NORMAL
  zh: 11 在第 20.9 节中显示，CG 优化的速度远低于高曲率方向
- en: parameterization-dependent metrics which are not particularly meaningful. Second,
    increasing the size of the minibatch, which is only one method to deal with minibatch
    overfitting, is not a strategy which will remain practical for very long.
  id: totrans-6298
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖参数化的指标并不特别有意义。其次，增加迷你批次的大小，这只是处理迷你批次过拟合的一种方法，实际上并不是一个能够长期保持实用的策略。
- en: Thirdly, aggressive early termination heuristics for CG, similar to this one,
    tend to interact badly with non-zero CG initializations12 and other forms of damping.
  id: totrans-6299
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，对于 CG 的激进提前终止启发式方法，类似于这种方法，往往与非零 CG 初始化和其他形式的阻尼相互作用不佳。
- en: And finally, there are other more direct ways of measuring how well updates
    will generalize, such as simply monitoring f on some training data outside of
    the current minibatch.
  id: totrans-6300
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有其他更直接的方法来衡量更新的泛化能力，比如简单地监控某些训练数据上的 f，而这些数据不在当前的迷你批次中。
- en: 20.13 Tricks And Recipes
  id: totrans-6301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.13 技巧与配方
- en: There are many things to keep in mind when designing an HF-style optimization
    algorithm for a particular application and it can be somewhat daunting even to
    those of us that have lots of experience with the method. So in order to make
    things easier in this regard, in this section we relate some of our experience
    in designing effective methods, and describe several particular setups that seem
    to work particularly well for certain deep neural network learning problems, assuming
    the use of a standard parameterization.
  id: totrans-6302
  prefs: []
  type: TYPE_NORMAL
  zh: 在为特定应用设计 HF 风格的优化算法时，有许多事情需要考虑，即使是对这种方法经验丰富的我们来说，这也可能显得有些令人生畏。因此，为了在这方面简化问题，在本节中我们分享一些设计有效方法的经验，并描述几种特别适合某些深度神经网络学习问题的具体设置，假设使用标准参数化。
- en: 'Common elements to all successful approaches we have tried are:'
  id: totrans-6303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试的所有成功方法的共同元素包括：
- en: '- use of the GGN matrix instead of the Hessian'
  id: totrans-6304
  prefs: []
  type: TYPE_NORMAL
  zh: '- 使用 GGN 矩阵而不是海森矩阵'
- en: '- the CG initialization technique described in section 20.10 - a well-designed
    preconditioner. When using Tikhonov damping, a reasonable choice is an estimate
    of the diagonal of the GGN matrix, modified using the technique described in section
    20.11.2 with κ = 0 and ψ = 0.75. When using one of the scale-sensitive methods
    described in section 20.8.3, it may be necessary to increase κ to something like
    10−2 times the mean of the diagonal entries of D'
  id: totrans-6305
  prefs: []
  type: TYPE_NORMAL
  zh: '- 第 20.10 节中描述的 CG 初始化技术 - 一个设计良好的预条件器。在使用 Tikhonov 阻尼时，一个合理的选择是 GGN 矩阵对角线的估计，使用第
    20.11.2 节中描述的技术进行修改，κ = 0，ψ = 0.75。当使用第 20.8.3 节中描述的尺度敏感方法之一时，可能需要将 κ 增加到大约 10^-2
    倍的 D 的对角线条目的均值'
- en: '- the use of one of diagonal damping methods, possibly in conjunction with
    structure damping for certain RNN learning problems. For feedforward network learning
    problems under the default parameterization, Tikhonov damping often works well,
    and usually so does using a modified estimate of the diagonal of the GGN matrix,
    provided that κ is large enough (as in the previous point)'
  id: totrans-6306
  prefs: []
  type: TYPE_NORMAL
  zh: '- 使用对角阻尼方法之一，可能与某些 RNN 学习问题的结构阻尼结合使用。对于默认参数化下的前馈网络学习问题，Tikhonov 阻尼通常效果良好，通常使用修改过的
    GGN 矩阵对角线的估计也同样有效，只要 κ 足够大（如前一点所述）'
- en: '- the use of the progress-based termination criterion for CG described in section
    20.4 in addition to some other condition which may stop CG sooner, such as a fixed
    iteration limit'
  id: totrans-6307
  prefs: []
  type: TYPE_NORMAL
  zh: '- 除了一些可能更早终止 CG 的其他条件外，使用第 20.4 节中描述的基于进度的 CG 终止标准，例如固定迭代限制'
- en: '- dynamic adjustment of damping constants (e.g. λ) according to the LM'
  id: totrans-6308
  prefs: []
  type: TYPE_NORMAL
  zh: '- 根据 LM 动态调整阻尼常数（例如 λ）'
- en: heuristic or a similar method Now, we describe the particulars of each of these
    successful methods.
  id: totrans-6309
  prefs: []
  type: TYPE_NORMAL
  zh: 启发式或类似方法来测量的，现在我们描述每种成功方法的具体情况。
- en: First, there is the original algorithm described in [22], which works pretty
    well. Here, the "CG-backtracking" approach is used to select an iterate based
    on the objective function value (see section 20.8.7), the gradient is computed
  id: totrans-6310
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，有原始算法在 [22] 中描述，它的效果相当不错。在这里，使用 "CG-backtracking" 方法根据目标函数值选择迭代（见第 20.8.7
    节），计算梯度。
- en: 12 Because termination of CG may be forced before Mˆ (δ) < 0 is achieved.
  id: totrans-6311
  prefs: []
  type: TYPE_NORMAL
  zh: 12 因为 CG 的终止可能在 Mˆ (δ) < 0 达成之前被强制。
- en: on a larger subset of the training data than the curvature, and CG is always
    terminated before reaching a fixed maximum number of steps (around 50 − 250, depending
    on the problem).
  id: totrans-6312
  prefs: []
  type: TYPE_NORMAL
  zh: 在比曲率更大的训练数据子集上，CG 总是在达到固定的最大步数（大约 50 到 250，具体取决于问题）之前终止。
- en: Second, there is a subtle but powerful variation on the above method which differs
    only in how CG is terminated, how the iterate used for the update δ is selected,
    and how CG is initialized at the next iteration of HF. In particular, CG is terminated
    as soon as the objective function, as evaluated on the data in the "curvature
    minibatch" (see section 20.12.1) gets significantly worse than its value from
    some number of steps ago (e.g. 10). The iterate used as the parameter update δ
    is selected to minimize M (or perhaps f) as evaluated on some data which is not
    contained in the curvature minibatch. Lastly, CG is initialized at the next iteration
    k + 1, not from the previous update δk, but instead from the CG iterate which
    gave the highest objective function value on the curvature minibatch (which will
    be close to the last). In practice, the quantities used to determine when to terminate
    CG, and how to select the best iterate, do not need to be computed at every step
    of CG, and can also be computed on much smaller (but representative) subsets of
    data.
  id: totrans-6313
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，有一种微妙但强大的变体，其与上述方法的不同仅在于 CG 的终止方式、用于更新的迭代 δ 的选择方式，以及 HF 下一次迭代的 CG 初始化方式。具体而言，CG
    会在目标函数（在“曲率小批量”数据上评估，见第 20.12.1 节）的值显著低于几步之前的值（例如 10）时立即终止。用于参数更新的迭代 δ 被选为在不包含在曲率小批量中的某些数据上最小化
    M（或可能是 f）。最后，CG 在下一次迭代 k + 1 中初始化，而不是从上一次更新 δk，而是从在曲率小批量中给出最高目标函数值的 CG 迭代（该值接近最后一次）。实际上，用于确定何时终止
    CG 以及如何选择最佳迭代的量，不需要在 CG 的每一步都计算，也可以在更小（但具有代表性）数据子集上计算。
- en: Finally, an approach which has emerged recently as perhaps the most efficient
    and effective, but also the most difficult to use, involves modifying HF to behaving
    more like a tradition momentum method, thus making stronger use of the CG initializations
    (see section 20.10) to better distribute work involved in optimizing the local
    quadratics across many iterations. To do this, we use a smaller maximum number
    of CG steps (around 25 to 50), smaller minibatches of training-data, and we also
    pay more attention to the CG initialization decayconstant ζ, which usually means
    increasing it towards the end of optimization.
  id: totrans-6314
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最近出现的一种方法可能是最有效和有效的，但也最难使用，涉及修改 HF，使其更像传统的动量方法，从而更好地利用 CG 初始化（见第 20.10 节），以更好地分配在许多迭代中优化局部二次函数所涉及的工作。为此，我们使用较小的
    CG 最大步数（大约 25 到 50），较小的训练数据小批量，同时我们也更加关注 CG 初始化衰减常数 ζ，这通常意味着在优化结束时增加它。
- en: Using shorter runs of CG helps with minibatch overfitting, and makes it feasible
    to use smaller minibatches and also compute the gradient and curvature using the
    same data. And as discussed in section 20.12.1, computing the gradient on the
    same data as the curvature has numerous theoretical advantages, and in practice
    seems to result in a reduced need for extra damping, thus resulting in a λ that
    shrinks reliably towards 0 when adjusted by the LM heuristic. However, this method
    tends to produce "noisy" updates, which while arguably beneficial from the standpoint
    of global optimization, make it more difficult to obtain finer convergence on
    some problems. So when nearing the end of optimization, we adopt some of the methods
    described in section 20.12.2, such as lowering the learning rate, using shorter
    CG runs, increasing the minibatch size, and/or switching back to using larger
    minibatches to compute gradients (making sure to raise the damping constant λ
    to compensate for this) in order to achieve fine convergence.
  id: totrans-6315
  prefs: []
  type: TYPE_NORMAL
  zh: 使用较短的 CG 运行有助于小批量过拟合，使得使用更小的小批量成为可能，并且也可以使用相同数据计算梯度和曲率。正如在 20.12.1 节中讨论的那样，在相同数据上计算梯度与曲率具有众多理论优势，实践中似乎减少了额外阻尼的需求，从而使
    λ 在 LM 启发式调整时可靠地收缩至 0。然而，这种方法往往产生“嘈杂”的更新，尽管从全局优化的角度来看是有益的，但在某些问题上却使得获得更细致的收敛变得更加困难。因此，在优化接近尾声时，我们采用
    20.12.2 节中描述的一些方法，例如降低学习率、使用更短的 CG 运行、增加小批量大小，和/或切换回使用较大小批量来计算梯度（确保提高阻尼常数 λ 以补偿这一点），以实现细致的收敛。
- en: One more piece of general advice we have is that using small amounts of weight-decay
    regularization can be highly beneficial from the standpoint of global optimization.
    In particular, to get the lowest training error possible, we have observed that
    it helps to use such regularization at the beginning of optimization only to disable
    it near the end. Also, using a good initialization is extremely important in regards
    to global optimization, and methods which work well for deep networks include
    the sparse initialization scheme advocated in [22], and the method of Glorot &
    Bengio [12], and of course the pre-training techniques pioneered in [17].
  id: totrans-6316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一条通用建议是，使用少量的权重衰减正则化从全局优化的角度来看是非常有益的。特别是，为了获得尽可能低的训练误差，我们观察到在优化开始时使用这种正则化有帮助，而在结束时则禁用。此外，良好的初始化在全局优化中极为重要，对于深度网络有效的方法包括
    [22] 中提倡的稀疏初始化方案，以及 Glorot 和 Bengio [12] 的方法，当然还有 [17] 中开创的预训练技术。
- en: 20.14 Summary
  id: totrans-6317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.14 摘要
- en: 'We described various components of the Hessian-free optimization, how they
    can interact non-trivially, and how their effectiveness (or possibly harmfulness)
    is situation dependent. The main points to keep in mind are:'
  id: totrans-6318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述了无 Hessian 优化的各种组成部分，它们如何以非平凡的方式相互作用，以及它们的有效性（或可能的有害性）取决于具体情况。需要记住的要点是：
- en: '- for non-convex optimizations it is usually preferable to use the generalized
    Gauss-Newton matrix which is guaranteed to be PSD'
  id: totrans-6319
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对于非凸优化，通常更倾向于使用保证为 PSD 的广义高斯-牛顿矩阵。'
- en: '- updates must be "damped" due to the untrustworthiness of the quadratic model'
  id: totrans-6320
  prefs: []
  type: TYPE_NORMAL
  zh: '- 更新必须“阻尼”，因为二次模型的不可信性。'
- en: '- there are various damping techniques that can be used, and their effectiveness
    depends highly on f and how it is parameterized'
  id: totrans-6321
  prefs: []
  type: TYPE_NORMAL
  zh: '- 可以使用多种阻尼技术，它们的有效性高度依赖于 f 及其参数化方式。'
- en: '- truncating CG before convergence, in addition to making HF practical, can
    also provide a beneficial damping effect'
  id: totrans-6322
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在收敛之前截断 CG 除了使 HF 实用外，还可以提供有益的阻尼效果。'
- en: '- the strategy for terminating CG is usually a combination of progress-based
    heuristic and a hard-limit anywhere between 10 and 300+ (which should be considered
    an important meta-parameter)'
  id: totrans-6323
  prefs: []
  type: TYPE_NORMAL
  zh: '- 终止 CG 的策略通常是基于进展的启发式和一个介于 10 到 300+ 之间的硬限制的组合（这应被视为一个重要的元参数）。'
- en: '- preconditioning can sometimes enable CG to make more rapid progress per step,
    but only if used correctly'
  id: totrans-6324
  prefs: []
  type: TYPE_NORMAL
  zh: '- 预处理有时可以使 CG 每步取得更快的进展，但前提是正确使用。'
- en: '- simple diagonal preconditioning methods tend to work well for feed-forward
    nets but not for RNNs'
  id: totrans-6325
  prefs: []
  type: TYPE_NORMAL
  zh: '- 简单的对角预处理方法在前馈网络中效果较好，但在 RNN 中效果不佳。'
- en: '- preconditioning interacts non-trivially with certain forms of damping such
    as CG truncation, which must be kept in mind'
  id: totrans-6326
  prefs: []
  type: TYPE_NORMAL
  zh: '- 预处理与某些形式的阻尼（如 CG 截断）以非平凡的方式相互作用，这一点必须牢记。'
- en: '- initializing CG from the update computed by the previous run of CG can have
    a beneficial "momentum-type effect"'
  id: totrans-6327
  prefs: []
  type: TYPE_NORMAL
  zh: '- 从上一次 CG 运行计算的更新初始化 CG 可以产生有益的“动量型效应”。'
- en: '- HF tends to require much larger minibatches than are used in SGD'
  id: totrans-6328
  prefs: []
  type: TYPE_NORMAL
  zh: '- HF倾向于需要比SGD更大的小批量'
- en: '- minibatch-overfitting can cause HF''s update proposals to be poor even for
    δ''s where the quadratic model is trustworthy'
  id: totrans-6329
  prefs: []
  type: TYPE_NORMAL
  zh: '- 小批量过拟合可能导致HF的更新提议在二次模型可靠的情况下表现不佳'
- en: '- using more data to compute the gradients than the curvature matrix-vector
    products is a low-cost method of potentially increasing the quality of the updates,
    but it can sometimes do more harm than good'
  id: totrans-6330
  prefs: []
  type: TYPE_NORMAL
  zh: '- 使用更多数据来计算梯度而非曲率矩阵-向量产品是一种低成本的方法，可能提高更新质量，但有时可能弊大于利'
- en: '- minibatch-overfitting can also be combated using some of the standard damping
    methods, along with simply increasing the minibatch size'
  id: totrans-6331
  prefs: []
  type: TYPE_NORMAL
  zh: '- 小批量过拟合也可以通过一些标准的阻尼方法进行对抗，并简单地增加小批量的大小'
- en: '- structural damping works well for training RNNs, particularly on problems
    with pathological long-range dependencies'
  id: totrans-6332
  prefs: []
  type: TYPE_NORMAL
  zh: '- 结构阻尼在训练RNN时效果良好，特别是在具有病态长程依赖的问题上'
- en: '- exploiting data-parallelism is very important for producing an efficient
    implementation - correctness of curvature matrix-vector products should be checked
    using finite difference methods'
  id: totrans-6333
  prefs: []
  type: TYPE_NORMAL
  zh: '- 利用数据并行性对于产生高效实现非常重要 - 应使用有限差分方法检查曲率矩阵-向量产品的正确性'
- en: '- the extra memory costs associated with the parallel computation of gradients
    and curvature matrix-vector products can be mitigated The difficulty of customizing
    an HF approach for particular application will no doubt depend on the specifics
    of the model and the dataset. While in many cases a generic approach can be used
    to good effect, some more difficult problems like RNNs or feed-forward networks
    with non-standard parameterizations may require additional care. And even on "easier"
    problems, a better designed approach may allow one to surpass performance barriers
    that may have previously been mistaken for convergence.'
  id: totrans-6334
  prefs: []
  type: TYPE_NORMAL
  zh: '- 与梯度和曲率矩阵-向量产品的并行计算相关的额外内存成本可以得到缓解。定制HF方法以适应特定应用的难度无疑取决于模型和数据集的具体情况。虽然在许多情况下，可以使用通用方法获得良好效果，但一些更复杂的问题，如具有非标准参数化的RNN或前馈网络，可能需要额外的关注。即使在“更简单”的问题上，更好设计的方法也可能让人超越以前被误认为是收敛的性能障碍。'
- en: This report has described many of the tricks and ideas, along with their theoretical
    justifications, which may useful in this regard. While we can try to predict what
    combination of ideas will work best for a given problem, based on previous experience
    and/or mathematical/intuitive reasoning, the only way to be sure is with careful
    experimentation. Unfortunately, optimization theory has a long way to go before
    being able to predict the performance of a method like HF applied to the highly
    non-convex objectives functions associated with neural networks.
  id: totrans-6335
  prefs: []
  type: TYPE_NORMAL
  zh: 本报告描述了许多技巧和思路，以及它们的理论依据，这在这方面可能是有用的。虽然我们可以尝试根据以往经验和/或数学/直观推理预测出最适合特定问题的思路组合，但确保有效的方法只有通过仔细的实验。不幸的是，优化理论在能够预测HF等方法在与神经网络相关的高度非凸目标函数的性能之前，还有很长的路要走。
- en: '[1] Amari, S.I.: Natural gradient works efficiently in learning. Neural Computation
    10(2), 251–276 (1998)'
  id: totrans-6336
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Amari, S.I.: 自然梯度在学习中有效。神经计算 10(2), 251–276（1998）'
- en: '[2] Becker, S., Le Cun, Y.: Improving the convergence of back-propagation learning
    with second order methods. In: Proceedings of the 1988 Connectionist Models Summer
    School, pp. 29–37. Morgan Kaufmann, San Matteo (1988)'
  id: totrans-6337
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Becker, S., Le Cun, Y.: 使用二阶方法改善反向传播学习的收敛性。在：1988年联结主义模型暑期学校会议录，第29–37页。摩根·考夫曼，圣马泰奥（1988）'
- en: '[3] Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H.: Greedy layer-wise
    training of deep networks. In: Advances in Neural Information Processing Systems,
    vol. 19, p. 153 (2007)'
  id: totrans-6338
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H.: 深度网络的贪婪分层训练。在：神经信息处理系统进展，卷19，页153（2007）'
- en: '[4] Bengio, Y., Simard, P., Frasconi, P.: Learning long-term dependencies with
    gradient descent is difficult. IEEE Transactions on Neural Networks 5(2), 157–166'
  id: totrans-6339
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Bengio, Y., Simard, P., Frasconi, P.: 使用梯度下降学习长期依赖关系是困难的。IEEE神经网络学报 5(2),
    157–166'
- en: (1994)
  id: totrans-6340
  prefs: []
  type: TYPE_NORMAL
  zh: (1994)
- en: '[5] Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins,
    G.,'
  id: totrans-6341
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins,
    G.,'
- en: 'Turian, J., Warde-Farley, D., Bengio, Y.: Theano: a cpu and gpu math expression
    compiler. In: Proceedings of the Python for Scientific Computing Conference'
  id: totrans-6342
  prefs: []
  type: TYPE_NORMAL
  zh: 'Turian, J., Warde-Farley, D., Bengio, Y.: Theano：一个CPU和GPU数学表达编译器。在：科学计算会议的Python会议录'
- en: (SciPy), vol. 4 (2010)
  id: totrans-6343
  prefs: []
  type: TYPE_NORMAL
  zh: (SciPy)，卷4（2010）
- en: '[6] Bishop, C.: Exact calculation of the hessian matrix for the multilayer
    perceptron.'
  id: totrans-6344
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Bishop, C.: 多层感知器的海森矩阵的精确计算。'
- en: Neural Computation 4(4), 494–501 (1992)
  id: totrans-6345
  prefs: []
  type: TYPE_NORMAL
  zh: 神经计算 4(4), 494–501 (1992)
- en: '[7] Bottou, L., Bousquet, O.: The tradeoffs of large scale learning. In: Advances
    in Neural Information Processing Systems, vol. 20, pp. 161–168 (2008)'
  id: totrans-6346
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Bottou, L., Bousquet, O.: 大规模学习的权衡。在：神经信息处理系统进展，卷20, pp. 161–168 (2008)'
- en: '[8] Byrd, R.H., Chin, G.M., Neveitt, W., Nocedal, J.: On the use of stochastic
    hessian information in optimization methods for machine learning. SIAM Journal
    on Optimization 21, 977 (2011)'
  id: totrans-6347
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Byrd, R.H., Chin, G.M., Neveitt, W., Nocedal, J.: 在机器学习优化方法中使用随机海森信息。SIAM优化杂志
    21, 977 (2011)'
- en: '[9] Byrd, R.H., Chin, G.M., Nocedal, J., Wu, Y.: Sample size selection in optimization
    methods for machine learning. Mathematical Programming, 1–29 (2012)'
  id: totrans-6348
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Byrd, R.H., Chin, G.M., Nocedal, J., Wu, Y.: 机器学习优化方法中的样本大小选择。数学规划, 1–29
    (2012)'
- en: '[10] Chapelle, O., Erhan, D.: Improved preconditioner for hessian free optimization.'
  id: totrans-6349
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Chapelle, O., Erhan, D.: 改进的海森矩阵自由优化的预条件器。'
- en: 'In: NIPS Workshop on Deep Learning and Unsupervised Feature Learning (2011)'
  id: totrans-6350
  prefs: []
  type: TYPE_NORMAL
  zh: 在：NIPS深度学习与无监督特征学习研讨会 (2011)
- en: '[11] Schraudolph, N.: Fast Curvature Matrix-Vector Products for Second-Order
    Gradient Descent. Neural Computation 14 (2002)'
  id: totrans-6351
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Schraudolph, N.: 二阶梯度下降的快速曲率矩阵-向量乘积。神经计算 14 (2002)'
- en: '[12] Glorot, X., Bengio, Y.: Understanding the difficulty of training deep
    feedforward neural networks. In: Proceedings of the International Conference on
    Artificial Intelligence and Statistics (AISTATS 2010). Society for Artificial
    Intelligence and Statistics (2010)'
  id: totrans-6352
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Glorot, X., Bengio, Y.: 理解深度前馈神经网络训练的难度。在：人工智能与统计国际会议论文集 (AISTATS 2010)。人工智能与统计学会
    (2010)'
- en: '[13] Gould, N.I.M., Lucidi, S., Roma, M., Toint, P.L.: Solving the trust-region
    subproblem using the lanczos method. SIAM Journal on Optimization 9(2), 504–525'
  id: totrans-6353
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Gould, N.I.M., Lucidi, S., Roma, M., Toint, P.L.: 使用Lanczos方法求解信任域子问题。SIAM优化杂志
    9(2), 504–525'
- en: (1999)
  id: totrans-6354
  prefs: []
  type: TYPE_NORMAL
  zh: (1999)
- en: '[14] Hansen, P.C., O''Leary, D.P.: The use of the l-curve in the regularization
    of discrete ill-posed problems. SIAM Journal on Scientific Computing 14, 1487
    (1993)'
  id: totrans-6355
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Hansen, P.C., O''Leary, D.P.: 在离散病态问题正则化中使用l曲线。SIAM科学计算杂志 14, 1487 (1993)'
- en: '[15] Hestenes, M.R., Stiefel, E.: Methods of conjugate gradients for solving
    linear systems (1952)'
  id: totrans-6356
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Hestenes, M.R., Stiefel, E.: 用于求解线性系统的共轭梯度法 (1952)'
- en: '[16] Hinton, G.E., Osindero, S., Teh, Y.W.: A fast learning algorithm for deep
    belief nets. Neural Computation 18(7), 1527–1554 (2006)'
  id: totrans-6357
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Hinton, G.E., Osindero, S., Teh, Y.W.: 一种快速学习深度信念网络的算法。神经计算 18(7), 1527–1554
    (2006)'
- en: '[17] Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data
    with neural networks. Science 313(5786), 504–507 (2006)'
  id: totrans-6358
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Hinton, G.E., Salakhutdinov, R.R.: 使用神经网络降低数据的维度。科学 313(5786), 504–507
    (2006)'
- en: '[18] Hochreiter, S.: Untersuchungen zu dynamischen neuronalen netzen. diploma
    thesis, Institut für Informatik, Lehrstuhl Prof. Brauer, Technische Universität
    München (1991)'
  id: totrans-6359
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Hochreiter, S.: 动态神经网络的研究。学位论文，慕尼黑工业大学信息学研究所，Brauer教授教席 (1991)'
- en: '[19] Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation
    9(8), 1735–1780 (1997)'
  id: totrans-6360
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Hochreiter, S., Schmidhuber, J.: 长短期记忆。神经计算 9(8), 1735–1780 (1997)'
- en: '[20] Jaeger, H., Haas, H.: Harnessing nonlinearity: Predicting chaotic systems
    and saving energy in wireless communication. Science 304(5667), 78–80 (2004)'
  id: totrans-6361
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Jaeger, H., Haas, H.: 利用非线性：预测混沌系统并在无线通信中节能。科学 304(5667), 78–80 (2004)'
- en: '[21] LeCun, Y., Bottou, L., Orr, G.B., Müller, K.-R.: Efficient BackProp. In:
    Orr, G.B., Müller, K.-R. (eds.) NIPS-WS 1996. LNCS, vol. 1524, pp. 9–50. Springer,
    Heidelberg (1998)'
  id: totrans-6362
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] LeCun, Y., Bottou, L., Orr, G.B., Müller, K.-R.: 高效反向传播。在：Orr, G.B., Müller,
    K.-R. (编辑) NIPS-WS 1996. LNCS, 卷1524, pp. 9–50. Springer, Heidelberg (1998)'
- en: '[22] Martens, J.: Deep learning via hessian-free optimization. In: Proceedings
    of the 27th International Conference on Machine Learning (ICML), vol. 951 (2010)'
  id: totrans-6363
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Martens, J.: 通过无海森优化进行深度学习。在：第27届国际机器学习会议（ICML）论文集，卷951 (2010)'
- en: '[23] Martens, J., Sutskever, I.: Learning recurrent neural networks with hessian-free
    optimization. In: Proc. ICML (2011)'
  id: totrans-6364
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Martens, J., Sutskever, I.: 通过无海森优化学习递归神经网络。在：Proc. ICML (2011)'
- en: '[24] Martens, J., Sutskever, I., Swersky, K.: Estimating the hessian by backpropagating
    curvature. In: Proc. ICML (2012)'
  id: totrans-6365
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Martens, J., Sutskever, I., Swersky, K.: 通过反向传播曲率来估计海森矩阵。在：Proc. ICML
    (2012)'
- en: '[25] Moré, J.J.: The levenberg-marquardt algorithm: implementation and theory.
    Numerical Analysis, 105–116 (1978)'
  id: totrans-6366
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Moré, J.J.: Levenberg-Marquardt算法：实现与理论。数值分析, 105–116 (1978)'
- en: '[26] Moré, J.J., Sorensen, D.C.: Computing a trust region step. SIAM Journal
    on Scientific and Statistical Computing 4, 553 (1983)'
  id: totrans-6367
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Moré, J.J., Sorensen, D.C.: 计算信任域步长. SIAM 科学与统计计算杂志 4, 553 (1983)'
- en: '[27] Nash, S.G.: Newton-type minimization via the lanczos method. SIAM Journal
    on Numerical Analysis, 770–788 (1984)'
  id: totrans-6368
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Nash, S.G.: 通过 Lanczos 方法进行牛顿类型最小化. SIAM 数值分析杂志, 770–788 (1984)'
- en: '[28] Nash, S.G.: A survey of truncated-newton methods. Journal of Computational
    and Applied Mathematics 124(1), 45–59 (2000)'
  id: totrans-6369
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] Nash, S.G.: 截断牛顿方法的调查. 计算与应用数学杂志 124(1), 45–59 (2000)'
- en: '[29] Nesterov, Y.: A method for unconstrained convex minimization problem with
    the rate of convergence o (1/k2). In: Doklady AN SSSR, vol. 269, pp. 543–547 (1983)'
  id: totrans-6370
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] Nesterov, Y.: 一种无约束凸最小化问题的收敛率为 o (1/k²) 的方法. 在: Doklady AN SSSR, vol.
    269, pp. 543–547 (1983)'
- en: '[30] Nocedal, J., Wright, S.J.: Numerical optimization. Springer (1999)'
  id: totrans-6371
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] Nocedal, J., Wright, S.J.: 数值优化. Springer (1999)'
- en: '[31] Pearlmutter, B.A.: Fast exact multiplication by the hessian. Neural Computation
    6(1), 147–160 (1994)'
  id: totrans-6372
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Pearlmutter, B.A.: 通过 Hessian 进行快速精确乘法. 神经计算 6(1), 147–160 (1994)'
- en: '[32] Shewchuk, J.R.: An introduction to the conjugate gradient method without
    the agonizing pain (1994)'
  id: totrans-6373
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] Shewchuk, J.R.: 没有痛苦的共轭梯度法入门 (1994)'
- en: '[33] Vinyals, O., Povey, D.: Krylov subspace descent for deep learning. arXiv
    preprint arXiv:1111.4259 (2011)'
  id: totrans-6374
  prefs: []
  type: TYPE_NORMAL
  zh: '[33] Vinyals, O., Povey, D.: 深度学习的 Krylov 子空间下降. arXiv 预印本 arXiv:1111.4259
    (2011)'
- en: '[34] Wengert, R.E.: A simple automatic derivative evaluation program. Communications
    of the ACM 7(8), 463–464 (1964)'
  id: totrans-6375
  prefs: []
  type: TYPE_NORMAL
  zh: '[34] Wengert, R.E.: 一个简单的自动导数评估程序. ACM 通讯 7(8), 463–464 (1964)'
- en: 21 Implementing Neural Networks Efficiently
  id: totrans-6376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21 高效实现神经网络
- en: Ronan Collobert1, Koray Kavukcuoglu2, and Clément Farabet3,4 1 Idiap Research
    Institute Martigny, Switzerland 2 NEC Laboratories America Princeton, NJ, USA
    3 Courant Institute of Mathematical Sciences New York University, New York, NY,
    USA 4 Université Paris-Est Équipe A3SI - ESIEE Paris, France Abstract. Neural
    networks and machine learning algorithms in general require a flexible environment
    where new algorithm prototypes and experiments can be set up as quickly as possible
    with best possible computational performance. To that end, we provide a new framework
    called Torch7, that is especially suited to achieve both of these competing goals.
  id: totrans-6377
  prefs: []
  type: TYPE_NORMAL
  zh: Ronan Collobert1, Koray Kavukcuoglu2, 和 Clément Farabet3,4 1 Idiap 研究所，瑞士马提尼
    2 NEC 实验室，美国新泽西普林斯顿 3 纽约大学数学科学学院，美国纽约 4 巴黎东大学 A3SI 团队 - ESIEE 巴黎，法国 摘要：神经网络和机器学习算法通常需要一个灵活的环境，以便能够尽快设置新的算法原型和实验，同时达到最佳的计算性能。为此，我们提供了一个名为
    Torch7 的新框架，特别适合实现这两个相互竞争的目标。
- en: Torch7 is a versatile numeric computing framework and machine learning library
    that extends a very lightweight and powerful programming language Lua. Its goal
    is to provide a flexible environment to design, train and deploy learning machines.
    Flexibility is obtained via Lua, an extremely lightweight scripting language.
    High performance is obtained via efficient OpenMP/SSE and CUDA implementations
    of low-level numeric routines. *Torch7* can also easily be interfaced to third-party
    software thanks to Lua's light C interface.
  id: totrans-6378
  prefs: []
  type: TYPE_NORMAL
  zh: Torch7 是一个多功能数值计算框架和机器学习库，扩展了一个非常轻量且强大的编程语言 Lua。其目标是提供一个灵活的环境来设计、训练和部署学习机器。灵活性通过
    Lua 这一极轻量的脚本语言获得。高性能则通过高效的 OpenMP/SSE 和 CUDA 实现的低级数值例程获得。*Torch7* 也可以轻松地与第三方软件接口，得益于
    Lua 的轻量 C 接口。
- en: Runtime efficiency is probably perceived as the most important topic when considering
    an efficient neural network implementation. One should however not under-estimate
    the time spent in designing the right neural network for a given task, or even
    the amount of work put into feeding data to the neural network properly. *Designing*
    the right network for a given task in a short amount of time requires a flexible
    development environment and a properly designed neural network toolbox.
  id: totrans-6379
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑高效神经网络实现时，运行时效率可能被视为最重要的话题。然而，不应低估在设计适合特定任务的正确神经网络上所花费的时间，甚至在适当喂入数据给神经网络上所投入的工作量。*在短时间内设计*适合特定任务的正确网络需要一个灵活的开发环境和一个合理设计的神经网络工具箱。
- en: 'Several efficient (in terms of runtime execution) neural network libraries
    for very specific needs are freely available. QuickNet1 is a good example in the
    speech recognition community: it implements most commonly used algorithms, that
    is multi-layer perceptrons with few layers. However, flexible libraries are quite
    rare. It is not a trivial task to implement a library supporting a wide range
    of complex networks (such as convolutional networks for images, text or speech...),
    any type of connectivity (full connectivity, shared weights, order in layers...),
    or several type of training algorithms (stochastic gradient, batch, second order
    like 1 http://www.icsi.berkeley.edu/Speech/qn.html'
  id: totrans-6380
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个针对特定需求的高效（在运行时执行方面）神经网络库可供免费使用。QuickNet1是语音识别领域的一个好例子：它实现了最常用的算法，即少层的多层感知器。然而，灵活的库非常稀缺。实现一个支持广泛复杂网络的库（如图像、文本或语音的卷积网络……）、各种连接类型（完全连接、共享权重、层次顺序……）或几种训练算法类型（随机梯度、批量、二阶等）并非易事。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    537–557, 2012.'
  id: totrans-6381
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon 等（编）：NN: Tricks of the Trade, 第2版，LNCS 7700，第537–557页，2012年。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-6382
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: LBFGS...). It is even more difficult to find a unified environment where one
    can easily read, prepare, feed properly the data to the network, or debug the
    internals of the architecture (for example when the network is not training properly).
  id: totrans-6383
  prefs: []
  type: TYPE_NORMAL
  zh: 找到一个统一的环境以便轻松阅读、准备、正确输入数据到网络或调试架构内部（例如，当网络训练不当时），这甚至更加困难。
- en: In Section 21.1, we will consider efficient neural network implementation in
    terms of *efficient environment*. We will then focus on the *runtime efficiency*
    and analyze different state-of-the-art approaches to speed-up the network training
    and testing phases in section 21.2. In this work, our analysis is built on the
    experience we acquired with our own neural network implementation, *Torch*2, and
    more particularly the last version *Torch7*.
  id: totrans-6384
  prefs: []
  type: TYPE_NORMAL
  zh: 在第21.1节中，我们将考虑*高效环境*下的神经网络实现。接着，我们将重点关注*运行效率*，并在第21.2节分析不同的最先进方法以加速网络的训练和测试阶段。在这项工作中，我们的分析基于我们自己神经网络实现的经验，*Torch*2，尤其是最新版本*Torch7*。
- en: 21.1 Efficient Environment
  id: totrans-6385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.1 高效环境
- en: An efficient environment for implementing neural networks should not be only
    limited to neural networks themselves. It should provide all necessary tools for
    efficient development of new numerical algorithms in general. Often one needs
    various numerical algorithms to transform the data before feeding it to the neural
    network. Algorithms will strongly vary from one research domain to the other.
    Moreover, in the last few years, the research activity on neural networks started
    to intersect with many other domains like optimization, linear algebra, parallel
    processing to name a few. A successful framework should provide necessary tools
    to cope with the variability in the development process. Only in that case the
    framework would allow to not only easily investigate new types of models or new
    training algorithms, but also to easily compare or combine neural networks with
    other machine learning algorithms.
  id: totrans-6386
  prefs: []
  type: TYPE_NORMAL
  zh: 实现神经网络的高效环境不应仅限于神经网络本身。它应提供所有必要工具，以便在一般情况下高效开发新的数值算法。通常，在将数据输入神经网络之前，需要各种数值算法来转换数据。不同研究领域的算法差异很大。此外，在过去几年中，神经网络的研究活动开始与许多其他领域交叉，如优化、线性代数、并行处理等。成功的框架应提供必要工具，以应对开发过程中的多样性。只有在这种情况下，框架才能不仅轻松研究新类型模型或新训练算法，还能轻松比较或结合神经网络与其他机器学习算法。
- en: In order for a framework to provide necessary environment for development of
    new numerical algorithms, its *extension capabilities* should be very advanced.
    Machine learning researchers face many problems where there is need for using
    existing libraries. As we will see in Section 21.2, this includes interfacing
    efficient linear algebra libraries or even the neural network implementation itself.
  id: totrans-6387
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使框架提供必要的环境以开发新的数值算法，其*扩展能力*应非常先进。机器学习研究人员面临许多问题，需要使用现有库。正如我们在第21.2节中看到的，这包括与高效线性代数库或神经网络实现本身的接口。
- en: The ability to interface these existing libraries with as little runtime and
    code development overhead as possible is crucial for an efficient toolbox.
  id: totrans-6388
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些现有库接口与尽可能少的运行时和代码开发开销连接起来，对于一个高效的工具箱至关重要。
- en: Finally, the *neural network toolbox implementation itself should be modular*
    enough to allow for the creation of any kind of new neural network models or implementation
    on different modalities of data, leaving the choice of the architecture as much
    as possible to the user.
  id: totrans-6389
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*神经网络工具箱的实现本身应足够模块化*，以允许创建任何新类型的神经网络模型或在不同数据模态上实现，尽可能将架构的选择留给用户。
- en: 'In this section, we will cover the following three important points: efficiency
    of development environment, extension capabilities and modular neural network
    toolbox. The modular structure of *Torch7* that fuses advantages of high-level
    and low-level libraries is shown in Figure 21.1.'
  id: totrans-6390
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖以下三个重要点：开发环境的效率、扩展能力和模块化神经网络工具箱。图 21.1 显示了融合了高级和低级库优势的 *Torch7* 模块化结构。
- en: 21.1.1 Scripting Language
  id: totrans-6391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.1.1 脚本语言
- en: A scripting (or interpreted) language is the most convenient solution for fast
    prototyping and development of new algorithms. At the same time, it is crucial
    2 http://www.torch.ch
  id: totrans-6392
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本（或解释）语言是快速原型设计和新算法开发的最方便解决方案。同时，这也至关重要。
- en: '![530_image_0.png](530_image_0.png)'
  id: totrans-6393
  prefs: []
  type: TYPE_IMG
  zh: '![530_image_0.png](530_image_0.png)'
- en: Fig. 21.1. Modular Structure of *Torch7*. Low level numerical libraries are
    interfaced with TH to provide a unified tensor library. luaT provides essential
    data structures for object/class manipulation in Lua. The core Torch package uses
    TH and luaT to provide a numerical computing environment purely in Lua. All other
    packages can use either Torch interface from inside Lua scripting environment
    or can interface low-level C interfaces for increased performance optimizations.
  id: totrans-6394
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21.1. *Torch7* 的模块化结构。低级数值库与 TH 接口，以提供统一的张量库。luaT 为 Lua 中的对象/类操作提供了基本数据结构。核心
    Torch 包使用 TH 和 luaT，完全在 Lua 中提供数值计算环境。所有其他包可以使用来自 Lua 脚本环境的 Torch 接口，或可以使用低级 C
    接口以增加性能优化。
- en: for the interpreted language to have a lightweight C API, both in terms of simplicity
    and efficiency. Simplicity in the C API encourages easier interfacing to existing
    external libraries and efficiency is the single most important criterion for large-scale
    applications.
  id: totrans-6395
  prefs: []
  type: TYPE_NORMAL
  zh: 解释语言应具有轻量级 C API，无论在简单性还是效率上。C API 的简单性鼓励更容易与现有外部库接口，而效率是大规模应用中最重要的标准。
- en: In a complex development environment, the scripting language becomes the
  id: totrans-6396
  prefs: []
  type: TYPE_NORMAL
  zh: 在复杂的开发环境中，脚本语言成为关键。
- en: '"glue" between heterogeneous components: different structures of the same concept
    (coming from different libraries) can be merged together using a high-level language,
    while keeping all the functionalities that are exposed from all the different
    libraries.'
  id: totrans-6397
  prefs: []
  type: TYPE_NORMAL
  zh: 在异构组件之间的“胶水”：相同概念的不同结构（来自不同库）可以通过高级语言合并，同时保持所有不同库所暴露的功能。
- en: Lua. Among existing scripting languages3 finding the ones that satisfy runtime
    efficiency severely restricts the number of possibilities. In our machine learning
    framework *Torch7*, we chose Lua, the fastest interpreted language (with also
    the fastest Just In Time-JIT compiler4) we could find. Lua has also the advantage
    that it is designed to be easily *embedded* in a C application, and provides a
    very clean C API, based on a virtual stack to pass values and carry out function
    evaluation from C. This unifies the interface to C/C++ and minimizes the effort
    required for wrapping third party libraries.
  id: totrans-6398
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有脚本语言中，满足运行时效率的语言大大限制了选择的可能性。在我们的机器学习框架 *Torch7* 中，我们选择了 Lua，这是我们能找到的最快的解释语言（还有最快的即时编译器）。Lua
    还有一个优势，即它被设计为能够轻松地嵌入 C 应用程序，并提供基于虚拟堆栈的非常干净的 C API，以传递值并从 C 执行函数评估。这统一了 C/C++ 的接口，并最小化了封装第三方库所需的工作量。
- en: Lua is intended to be used as a powerful, light-weight scripting language for
    any program that needs one. It is implemented as a library, written in pure C
  id: totrans-6399
  prefs: []
  type: TYPE_NORMAL
  zh: Lua 旨在作为一种强大、轻量级的脚本语言供任何需要的程序使用。它作为库实现，使用纯 C 编写。
- en: in the common subset of ANSI C and C++. Quoting Lua webpage5, Lua combines simple
    procedural syntax with powerful data description constructs based on associative
    arrays and extensible semantics. Lua is dynamically typed, runs by interpreting
    bytecode for a register-based 3 E.g. on http://shootout.alioth.debian.org 4 http://luajit.org/
    5 http://www.lua.org virtual machine, and has automatic memory management with
    incremental garbage collection, making it ideal for configuration, scripting,
    and rapid prototyping.
  id: totrans-6400
  prefs: []
  type: TYPE_NORMAL
  zh: 在ANSI C和C++的公共子集中。引用Lua网页5，Lua结合了简单的过程语法与基于关联数组和可扩展语义的强大数据描述构造。Lua是动态类型的，通过解释字节码在基于寄存器的虚拟机上运行，并具有增量垃圾回收的自动内存管理，使其非常适合配置、脚本和快速原型开发。
- en: Lua offers good support for object-oriented programming, functional programming,
    and data-driven programming. As shown in Figure 21.2, it handles numerical computations
    very efficiently (compared to C). This is a great asset for rapid implementation
    of new numerical algorithms. Lua's main type is table, which implements associative
    arrays in a very efficient manner (see Figure 21.2). An associative array is an
    array that can be indexed not only with numbers, but also with strings or any
    other value of the language. Tables have no fixed size, can be resized dynamically,
    and can be used as "virtual tables" over another table, to simulate various object-oriented
    paradigms. Tables are the only data structuring mechanism in Lua, yet a powerful
    one. One can use tables to represent ordinary arrays, symbol tables, sets, records,
    queues, and other data structures, in a simple, uniform, and efficient way. Lua
    uses tables to represent packages as well. In addition, functions are first class
    citizens of the language. A
  id: totrans-6401
  prefs: []
  type: TYPE_NORMAL
  zh: Lua提供良好的支持对象导向编程、函数式编程和数据驱动编程。如图21.2所示，它在数值计算方面处理非常高效（与C相比）。这对快速实现新的数值算法来说是一个巨大的资产。Lua的主要类型是表，以非常高效的方式实现关联数组（见图21.2）。关联数组是可以不仅用数字索引，还可以用字符串或语言中的任何其他值索引的数组。表没有固定大小，可以动态调整大小，并且可以用作另一个表上的“虚拟表”，以模拟各种面向对象的范式。表是Lua中唯一的数据结构机制，但它是一个强大的机制。可以使用表以简单、统一和高效的方式表示普通数组、符号表、集合、记录、队列和其他数据结构。Lua也使用表来表示包。此外，函数是该语言的第一类公民。A
- en: function, just like any other variable can be passed as a variable to or returned
    from a function. Last, but not the least, Lua supports closures. Combined with
    tables, closures provide a very powerful and efficient syntax for data handling
    and programming complicated algorithms.
  id: totrans-6402
  prefs: []
  type: TYPE_NORMAL
  zh: 函数，就像任何其他变量一样，可以作为变量传递给或从函数返回。最后但同样重要的是，Lua支持闭包。结合表，闭包为数据处理和编程复杂算法提供了一种非常强大和高效的语法。
- en: '![531_image_0.png](531_image_0.png)'
  id: totrans-6403
  prefs: []
  type: TYPE_IMG
  zh: '![531_image_0.png](531_image_0.png)'
- en: Fig. 21.2. Comparison of runtime efficiency of the C language (with gcc 4.4),
    Lua 5.1.4 and Python 2.7. Lua and Python JIT implementations were LuaJIT and PyPy,
    respectively. The Mandelbrot and Binary Trees benchmarks are taken from "The Computer
    Language Benchmarks Game". All benchmarks were run using a single CPU on a highend
    12 cores Xeon server. The Mandelbrot benchmark makes a heavy use of numbers, while
    the Binary Trees benchmark makes a heavy use of data structures (struct in C,
  id: totrans-6404
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21.2. C语言（使用gcc 4.4）、Lua 5.1.4 和 Python 2.7 的运行效率比较。Lua和Python的JIT实现分别为LuaJIT和PyPy。Mandelbrot和二叉树基准测试来自《计算机语言基准游戏》。所有基准测试均在高端12核Xeon服务器的单个CPU上运行。Mandelbrot基准测试重度使用数字，而二叉树基准测试重度使用数据结构（C中的结构）。
- en: tables in Python and Lua). The execution time is reported (on a log-scale axis)
    for each language.
  id: totrans-6405
  prefs: []
  type: TYPE_NORMAL
  zh: Python和Lua中的表。每种语言的执行时间在对数尺度轴上报告。
- en: 'Why Not Python? It is hard to talk about a programming language without starting
    a flame war. While Lua is well known in the gaming programmer community, mostly
    due to its speed advantage and great embedding capabilities, Python is more popular
    in more general public. With no doubt, Python ships with more libraries and one
    can find support about almost any problem easily in many different contexts. However,
    Lua offers at least two important advantages over Python:'
  id: totrans-6406
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不选Python？谈论一种编程语言时很难不引发争论。虽然Lua在游戏编程社区中因其速度优势和出色的嵌入能力而广为人知，但Python在更广泛的公众中更受欢迎。毫无疑问，Python附带了更多的库，人们可以在许多不同的上下文中轻松找到几乎任何问题的支持。然而，Lua相对于Python至少提供了两个重要优势：
- en: '- First and foremost, the simplicity of integrating existing C/C++ libraries
    is very important. Many efficient numerical algorithms are implemented in specialized
    packages in BLAS, LAPACK, FFTW and similar libraries. A'
  id: totrans-6407
  prefs: []
  type: TYPE_NORMAL
  zh: '- 首先，集成现有C/C++库的简单性非常重要。许多高效的数值算法在BLAS、LAPACK、FFTW等专用包中实现。'
- en: lightweight interface to existing code is crucial for achieving a high performance
    environment. In section 21.2.8 we show quantitative results on efficiency of Lua
    compared to Python when wrapping BLAS function calls.
  id: totrans-6408
  prefs: []
  type: TYPE_NORMAL
  zh: 轻量级接口对现有代码至关重要，以实现高性能环境。在第21.2.8节中，我们展示了Lua与Python在包装BLAS函数调用时的效率定量结果。
- en: '- Second, since Lua is embeddable in C/C++, any prototyped application can
    be turned into a final system/product with very little extra effort. Since Lua
    is written in pure C and does not have dependency to any external library, it
    can be easily used in embedded applications like, Android, iOS 6, FPGAs 7 and
    DSPs.'
  id: totrans-6409
  prefs: []
  type: TYPE_NORMAL
  zh: '- 其次，由于Lua可以嵌入C/C++，任何原型应用都可以以极少的额外努力转变为最终系统/产品。由于Lua是用纯C编写的，且不依赖任何外部库，它可以轻松用于嵌入式应用，如Android、iOS
    6、FPGA 7和DSP。'
- en: There are also alternatives to writing a custom interface between interpreted
    language and C/C++, like Simplified Wrapper and Interface Generator (SWIG) 8.
  id: totrans-6410
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他替代方案可以在解释语言和C/C++之间编写自定义接口，如简化包装器和接口生成器（SWIG）8。
- en: Although these might provide a simplified interface at first, writing a tensor
    library with several linear algebra backends requires a very fine-grained control
    and we found it is harder to manage this interface rather than using the native
    Lua API.
  id: totrans-6411
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些可能最初提供了简化的接口，但编写一个具有多个线性代数后端的张量库需要非常细致的控制，我们发现管理这个接口比使用原生的Lua API更困难。
- en: In addition to its performance advantage on number of operations (see Figure
    21.2), Lua also provides other unique advantages - rarely found simultaneously
    in existing programming languages - for implementing a large-scale machine learning
    framework. In the following section we will show how we extended Lua's basic numerical
    capabilities to a rather complete framework for developing complex numerical algorithms.
  id: totrans-6412
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在操作数量上的性能优势（见图21.2），Lua还提供其他独特的优势——这些优势在现有编程语言中很少同时存在——适合实施大规模机器学习框架。在接下来的部分中，我们将展示如何将Lua的基本数值能力扩展到一个相当完整的框架，以开发复杂的数值算法。
- en: 21.1.2 Multi-Purpose Efficient N-Dimensional Tensor Object
  id: totrans-6413
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.1.2 多用途高效N维张量对象
- en: 'Torch7 provides a generic Tensor library (called TH) that is written in pure
    C. This library is interfaced in Lua, providing new efficient multi-dimensional
    array types in the scripting language. Most packages in *Torch7* (or third-party
    packages that depend on *Torch7*) rely on this Tensor class to represent signals,
    images, videos..., allowing Lua to nicely "glue" most libraries together. Fast
    prototyping and creation of new packages is made possible, as the library is available
    directly from both Lua and C. Interfacing or extending existing libraries is very
    efficient. The following code demonstrates a few standard Tensor-based operations,
    from the Lua side:'
  id: totrans-6414
  prefs: []
  type: TYPE_NORMAL
  zh: Torch7提供了一个通用的张量库（称为TH），它是用纯C编写的。这个库在Lua中进行接口，提供新的高效多维数组类型。*Torch7*中的大多数包（或依赖于*Torch7*的第三方包）依赖于这个张量类来表示信号、图像、视频等，从而使Lua能够很好地“连接”大多数库。快速原型设计和新包的创建成为可能，因为这个库可以直接从Lua和C中使用。接口或扩展现有库的效率很高。以下代码展示了几种标准的基于张量的操作，从Lua一侧：
- en: 6 https://github.com/clementfarabet/torch-ios 7 http://www.neuflow.org 8 www.swig.org
  id: totrans-6415
  prefs: []
  type: TYPE_NORMAL
  zh: 6 https://github.com/clementfarabet/torch-ios 7 http://www.neuflow.org 8 www.swig.org
- en: 1 -- create a tensor of single-precision floats 2 t = torch.FloatTensor(100,100)
  id: totrans-6416
  prefs: []
  type: TYPE_NORMAL
  zh: 1 -- 创建一个单精度浮点张量 2 t = torch.FloatTensor(100,100)
- en: '3 4 -- randomized: sampled from a normal distribution 5 l = torch.randn(100,100)'
  id: totrans-6417
  prefs: []
  type: TYPE_NORMAL
  zh: 3 4 -- 随机化：从正态分布中采样 5 l = torch.randn(100,100)
- en: 6 7 -- basic operators 8 r = t + l/2 9 10 -- in-place operators 11 r:add(0.5,
    t) 12 13 -- common operators 14 r = torch.log(torch.exp(-r)+10)
  id: totrans-6418
  prefs: []
  type: TYPE_NORMAL
  zh: 6 7 -- 基本操作符 8 r = t + l/2 9 10 -- 原地操作符 11 r:add(0.5, t) 12 13 -- 常见操作符 14
    r = torch.log(torch.exp(-r)+10)
- en: 'As in Matlab, multiple types can co-exist in *Torch7*, and it is easy to cast
    from one to the other:'
  id: totrans-6419
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Torch7*中，多个类型可以共存，且从一种类型转换为另一种类型非常简单：
- en: 1 -- a single-precision tensor 2 tfloat = torch.FloatTensor(100)
  id: totrans-6420
  prefs: []
  type: TYPE_NORMAL
  zh: 1 -- 单精度张量 2 tfloat = torch.FloatTensor(100)
- en: 3 4 -- converted to double-precision 5 tdouble = tfloat:double() 6 7 r = torch.FloatTensor(tdouble:size())
  id: totrans-6421
  prefs: []
  type: TYPE_NORMAL
  zh: 3 4 -- 转换为双精度 5 tdouble = tfloat:double() 6 7 r = torch.FloatTensor(tdouble:size())
- en: 8 9 -- automatically casts from double->float 10 r:copy(tdouble)
  id: totrans-6422
  prefs: []
  type: TYPE_NORMAL
  zh: 8 9 -- 自动从双精度转为浮点 10 r:copy(tdouble)
- en: A sample matrix, matrix multiplication operation is done as in the following
    example.
  id: totrans-6423
  prefs: []
  type: TYPE_NORMAL
  zh: 示例矩阵，矩阵乘法操作如下所示。
- en: 1 x = torch.Tensor(1000,5000) 2 y = torch.Tensor(5000,3000) 3 z = torch.mm(x,y)
  id: totrans-6424
  prefs: []
  type: TYPE_NORMAL
  zh: 1 x = torch.Tensor(1000,5000) 2 y = torch.Tensor(5000,3000) 3 z = torch.mm(x,y)
- en: 4 print(z:size())
  id: totrans-6425
  prefs: []
  type: TYPE_NORMAL
  zh: 4 print(z:size())
- en: 5 6 1000 7 3000 8 [torch.LongStorage of size 2]
  id: totrans-6426
  prefs: []
  type: TYPE_NORMAL
  zh: 5 6 1000 7 3000 8 [torch.LongStorage of size 2]
- en: The *Torch7* Tensor library provides a lot of classic operations (including
    linear algebra operations), efficiently implemented in C, leveraging SSE instructions
    on Intel's platforms and optionally binding linear algebra operations to existing
    efficient BLAS/Lapack implementations (like Intel MKL, OpenBLAS or ATLAS).
  id: totrans-6427
  prefs: []
  type: TYPE_NORMAL
  zh: '*Torch7* Tensor库提供了许多经典操作（包括线性代数操作），这些操作在C中高效实现，利用Intel平台上的SSE指令，并可选择将线性代数操作绑定到现有高效的BLAS/Lapack实现（如Intel
    MKL、OpenBLAS或ATLAS）。'
- en: As we will see in the next section, we also support OpenMP instructions and
    CUDA GPU computing for certain subset of operations where these platforms offer
    unique performance advantages. Related Approaches. Our Tensor library implementation
    got mostly inspired from SN [3] and Lush [5] toolboxes, which were one of the
    first to introduce the concept (in a LISP language framework). Matlab also supports
    N-dimension arrays (even though early releases only supported 2D matrices). Compared
    to Matlab, we put big emphasis on *memory allocation control*, as we will see
    in Section 21.2.2. Numpy9 is another popular alternative, but only available for
    the Python language. As mentioned before, Lua offers unique advantages for a machine
    learning framework because of its speed and the simpler C interface.
  id: totrans-6428
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在下一节中看到的，我们还支持OpenMP指令和CUDA GPU计算，以针对某些操作的子集提供这些平台独特的性能优势。相关方法。我们的Tensor库实现主要受到SN
    [3] 和 Lush [5] 工具箱的启发，这些工具箱是最早引入这一概念（在LISP语言框架中）的之一。Matlab也支持N维数组（尽管早期版本仅支持2D矩阵）。与Matlab相比，我们非常重视*内存分配控制*，正如我们将在21.2.2节中看到的。Numpy9是另一个流行的替代方案，但仅适用于Python语言。如前所述，Lua在机器学习框架中具有独特优势，因为其速度和更简单的C接口。
- en: 21.1.3 Modular Neural Networks
  id: totrans-6429
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.1.3 模块化神经网络
- en: 'Following [6], we view a neural network as a set of modules connected together
    in a particular graph. In *Torch7*, the "nn" package provides a set of standard
    neural network modules, as well as a set of container modules that can be used
    to define arbitrary directed (acyclic or not) graphs. By explicitly describing
    the graph''s architecture, using pluggable modules, we avoid the complexity of
    a graph parser, or any other middle-ware compiler. In practice, most networks
    are either sequential, or have simple branching patterns and recursions. The following
    example shows how to describe a multi-layer perceptron:'
  id: totrans-6430
  prefs: []
  type: TYPE_NORMAL
  zh: 按照[6]，我们将神经网络视为连接在一起的模块集，形成特定图形。在*Torch7*中，“nn”包提供了一组标准神经网络模块，以及一组容器模块，可用于定义任意有向（无环或有环）图形。通过明确描述图的架构，使用可插拔模块，我们避免了图解析器的复杂性或任何其他中间件编译器。在实践中，大多数网络都是顺序的，或具有简单的分支模式和递归。以下示例展示了如何描述多层感知器：
- en: 1 mlp = nn.Sequential()
  id: totrans-6431
  prefs: []
  type: TYPE_NORMAL
  zh: 1 mlp = nn.Sequential()
- en: 2 mlp:add(nn.Linear(100,1000))
  id: totrans-6432
  prefs: []
  type: TYPE_NORMAL
  zh: 2 mlp:add(nn.Linear(100,1000))
- en: 3 mlp:add(nn.Tanh()) 4 mlp:add(nn.Linear(1000,10)) 5 mlp:add(nn.SoftMax())
  id: totrans-6433
  prefs: []
  type: TYPE_NORMAL
  zh: 3 mlp:add(nn.Tanh()) 4 mlp:add(nn.Linear(1000,10)) 5 mlp:add(nn.SoftMax())
- en: Each module (or container) provides standard functions to compute its output
    state, and back-propagate derivatives to its inputs, and to its internal parameters.
    Given the previous network, an input X, and the gradient of some error E
  id: totrans-6434
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模块（或容器）提供标准函数来计算其输出状态，并将导数反向传播到其输入和内部参数。给定前面的网络，一个输入X，以及某个误差E的梯度
- en: 'with respect to the output Y —*dE/dY* —these three functions can be called
    like this:'
  id: totrans-6435
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于输出Y —*dE/dY* —这三个函数可以如下调用：
- en: 1 -- compute the activations Y = f(X)
  id: totrans-6436
  prefs: []
  type: TYPE_NORMAL
  zh: 1 -- 计算激活 Y = f(X)
- en: 2 Y = mlp:updateOutput(X)
  id: totrans-6437
  prefs: []
  type: TYPE_NORMAL
  zh: 2 Y = mlp:updateOutput(X)
- en: 3 4 -- compute some loss E = l(Y,T) 5 E = loss:updateOutput(Y,T)
  id: totrans-6438
  prefs: []
  type: TYPE_NORMAL
  zh: 3 4 -- 计算某些损失 E = l(Y,T) 5 E = loss:updateOutput(Y,T)
- en: 6 7 -- compute the gradient dE/dY = dl(Y,T)/dY
  id: totrans-6439
  prefs: []
  type: TYPE_NORMAL
  zh: 6 7 -- 计算梯度 dE/dY = dl(Y,T)/dY
- en: 8 dE_dY = loss:updateGradInput(Y,T)
  id: totrans-6440
  prefs: []
  type: TYPE_NORMAL
  zh: 8 dE_dY = loss:updateGradInput(Y,T)
- en: 9 10 -- back-propagate the gradients, down to dE/dX
  id: totrans-6441
  prefs: []
  type: TYPE_NORMAL
  zh: 9 10 -- 反向传播梯度，直至 dE/dX
- en: 11 dE_dX = mlp:updateGradInput(X,dE_dY)
  id: totrans-6442
  prefs: []
  type: TYPE_NORMAL
  zh: 11 dE_dX = mlp:updateGradInput(X,dE_dY)
- en: '12 13 -- compute the gradients wrt the weights: dE/dW'
  id: totrans-6443
  prefs: []
  type: TYPE_NORMAL
  zh: 12 13 -- 计算相对于权重的梯度：dE/dW
- en: 14 mlp:accGradParameters(X,dE_dY)
  id: totrans-6444
  prefs: []
  type: TYPE_NORMAL
  zh: 14 mlp:accGradParameters(X,dE_dY)
- en: The "nn" package in *Torch7* provides about 80 different neural network modules,
    allowing the user to implement most existing neural networks with minimal effort
    in pure Lua.
  id: totrans-6445
  prefs: []
  type: TYPE_NORMAL
  zh: '*Torch7*中的“nn”包提供约80种不同的神经网络模块，使用户能够用纯Lua实现大多数现有的神经网络，几乎不费力。'
- en: 9 http://numpy.scipy.org Leveraging the TH Library. Neural network modules in
    *Torch7* use Tensors provided by the TH library (see Section 21.1.2) to represent
    their own input data, output or internal states. Most modules are simply written
    in Lua, using the Torch package for intensive numerical operations. Only packages
    which require very specific operations have a dedicated C back-end. And, even
    in this case many of them use the TH library interface from C. In any case, Tensors
    are used as data containers to interact seamlessly with the rest of the library.
  id: totrans-6446
  prefs: []
  type: TYPE_NORMAL
  zh: 9 http://numpy.scipy.org 利用TH库。*Torch7*中的神经网络模块使用TH库提供的张量（见第21.1.2节）来表示它们自己的输入数据、输出或内部状态。大多数模块简单地用Lua编写，使用Torch包进行密集的数值运算。只有在需要非常特定的操作时，才会有专门的C后端。即使在这种情况下，许多模块也使用来自C的TH库接口。在任何情况下，张量被用作数据容器，与库的其余部分无缝交互。
- en: Training Algorithms. In *Torch7*, every neural network module, given the partial
    derivatives with respect to its outputs, is able to compute the partial derivative
    with respect to its parameters and its inputs. Thus, any complicated network structure
    can be trained using gradient-based optimization methods. Batch, mini-batch and
    stochastic gradient descent algorithms are supported.
  id: totrans-6447
  prefs: []
  type: TYPE_NORMAL
  zh: 训练算法。在*Torch7*中，给定相对于输出的偏导数，每个神经网络模块能够计算相对于其参数和输入的偏导数。因此，任何复杂的网络结构都可以使用基于梯度的优化方法进行训练。支持批量、小批量和随机梯度下降算法。
- en: More advanced algorithms, such as second-order gradient descent algorithms like
    conjugate gradient or LBFGS are also possible, thanks to a numerical package called
    "optim". While this optimization package is designed to be used standalone, it
    also provides second-order optimization capabilities for neural networks when
    used with the "nn" package.
  id: totrans-6448
  prefs: []
  type: TYPE_NORMAL
  zh: 更先进的算法，如共轭梯度或LBFGS等二阶梯度下降算法也是可能的，这要归功于一个名为“optim”的数值包。虽然该优化包设计为独立使用，但在与“nn”包结合使用时，它还为神经网络提供二阶优化能力。
- en: 21.1.4 Additional Torch7 Packages
  id: totrans-6449
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.1.4 附加Torch7包
- en: 'Torch7 comes with many built-in and third-party packages. In order to encourage
    collaborations and redistribution of machine learning algorithms, a built-in package
    manager is provided. It can easily download, compile and install additional *Torch7*
    packages from any package repository, when needed. At this time, the most interesting
    packages related to numerical computation or numerical analysis are:'
  id: totrans-6450
  prefs: []
  type: TYPE_NORMAL
  zh: Torch7附带许多内置和第三方包。为了鼓励机器学习算法的协作与再分配，提供了一个内置的包管理器。当需要时，它可以轻松下载、编译和安装来自任何包库的附加*Torch7*包。目前，与数值计算或数值分析相关的最有趣的包包括：
- en: '- torch: *Torch7*''s main package: provides Tensors, easy serialization and
    other basic functionalities. This package provides, Matlab-like functions to create,
    transform and use Tensors.'
  id: totrans-6451
  prefs: []
  type: TYPE_NORMAL
  zh: '- torch: *Torch7*的主包：提供张量、简单序列化和其他基本功能。此包提供类似于Matlab的函数来创建、转换和使用张量。'
- en: '- gnuplot: This package provides plotting interface to Gnuplot using Tensors.
    - image: An image processing package. It provides all the standard image processing
    functions such as loading and saving images, rescaling, rotating, converting color
    spaces, filtering operations, . . .'
  id: totrans-6452
  prefs: []
  type: TYPE_NORMAL
  zh: '- gnuplot: 此包提供使用张量的Gnuplot绘图接口。 - image: 一个图像处理包。提供所有标准的图像处理功能，如加载和保存图像、重新缩放、旋转、转换色彩空间、滤波操作，等等。'
- en: '- optim: A compact package providing steepest descent, conjugate gradient and
    limited memory BFGS implementations.'
  id: totrans-6453
  prefs: []
  type: TYPE_NORMAL
  zh: '- optim: 一个紧凑的包，提供最速下降法、共轭梯度法和有限记忆BFGS实现。'
- en: '- qt: Full bindings between Qt and Lua10, with transparent conversion of Torch7
    Tensors from/to QImages. Great for quickly developing interactive demos with advanced
    GUIs (running natively on Linux, Mac or Windows platforms).'
  id: totrans-6454
  prefs: []
  type: TYPE_NORMAL
  zh: '- qt: Qt与Lua10之间的完整绑定，能够透明地将Torch7张量转换为QImages，非常适合快速开发具有先进GUI的交互式演示（在Linux、Mac或Windows平台上原生运行）。'
- en: 'The list of available packages is constantly growing, as Lua makes it easy
    to interface any C library. Third-party packages include: *unsup*, which contains
    several unsupervised learning algorithms like sparse coding and auto encoders.'
  id: totrans-6455
  prefs: []
  type: TYPE_NORMAL
  zh: 可用包的列表不断增长，因为Lua使得与任何C库的接口变得容易。第三方包包括：*unsup*，它包含几种无监督学习算法，如稀疏编码和自编码器。
- en: 10 Thanks to Léon Bottou for this huge piece of work.
  id: totrans-6456
  prefs: []
  type: TYPE_NORMAL
  zh: 10 感谢 Léon Bottou 贡献了这项巨大工作。
- en: mattorch, which provides a two-way interface between Matlab's matrix format
    and Torch's tensor; *parallel*, which provides simple routines to fork and execute
    Lua code on local or remote machines, and exchange data using *Torch7*'s serialization
    mechanism; *camera*, a simple wrapper to camera/webcam drivers on Linux and MacOSX;
    *imgraph*, a package that provides lots of routines to create edge-weighted graphs
    on images, and manipulate these graphs.
  id: totrans-6457
  prefs: []
  type: TYPE_NORMAL
  zh: mattorch，提供了 Matlab 矩阵格式与 Torch 张量之间的双向接口；*parallel*，提供简单的例程来在本地或远程机器上分叉和执行
    Lua 代码，并使用 *Torch7* 的序列化机制交换数据；*camera*，是对 Linux 和 MacOSX 上摄像头/网络摄像头驱动程序的简单封装；*imgraph*，一个提供大量例程以在图像上创建边加权图并操作这些图的包。
- en: 21.2 Efficient Runtime Execution
  id: totrans-6458
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.2 高效的运行时执行
- en: 'Torch7 has been designed with efficiency in mind, leveraging SSE when possible
    and supporting two ways of parallelization: OpenMP and CUDA. The Tensor library
    (which is interfaced to Lua using the "torch" package) makes heavy use of these
    techniques. From the user viewpoint, enabling CUDA and OpenMP can lead to great
    speedups in any "Lua" script, at zero implementation cost (because most packages
    rely on the Tensor library). Other packages (like the "nn" package)'
  id: totrans-6459
  prefs: []
  type: TYPE_NORMAL
  zh: Torch7 设计时考虑了效率，尽可能利用 SSE，并支持两种并行化方式：OpenMP 和 CUDA。张量库（通过“torch”包与 Lua 进行接口）大量使用这些技术。从用户的角度来看，启用
    CUDA 和 OpenMP 可以在任何“Lua”脚本中实现显著的加速，且实现成本为零（因为大多数包依赖于张量库）。其他包（如“nn”包）
- en: also leverage OpenMP and CUDA for more specific usages not covered by the Tensor
    library. In the following we explain specific advantages of *Torch7* for achieving
    an excellent runtime performance.
  id: totrans-6460
  prefs: []
  type: TYPE_NORMAL
  zh: 还利用 OpenMP 和 CUDA 进行更具体的使用，弥补张量库未覆盖的领域。接下来，我们将解释 *Torch7* 在实现卓越运行时性能方面的具体优势。
- en: 21.2.1 Float Or Double Representations
  id: totrans-6461
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.2.1 浮点或双精度表示
- en: One of the major computational bottlenecks of modern computers is their memory
    bandwidth. When implementing any numerical algorithm, the number of memory accesses
    should be always reduced by all means. This has an impact not only on the coding
    style, but also on the floating point type we will choose when implementing neural
    networks. A C "double" takes usually 8 bytes in memory, while C "float" takes
    only 4. Given that high precision is rarely required in neural networks, one might
    consider using floating point precision in most cases.
  id: totrans-6462
  prefs: []
  type: TYPE_NORMAL
  zh: 现代计算机的主要计算瓶颈之一是其内存带宽。在实现任何数值算法时，应始终尽量减少内存访问的次数。这不仅影响编码风格，也影响我们在实现神经网络时选择的浮点类型。C
    的“double”通常占用 8 字节内存，而 C 的“float”只占用 4 字节。考虑到神经网络中高精度的需求很少，使用浮点精度在大多数情况下可能是个不错的选择。
- en: On a simple matrix-matrix operation, speedups of ×2 are common when using floats
    instead of doubles. In practice similar speedups are also observed in neural networks
    using floating point precision. In that respect, in *Torch7*, the user can easily
    choose (at runtime) the default floating point type.
  id: totrans-6463
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单的矩阵-矩阵操作中，使用浮点数而不是双精度数的加速通常达到 ×2。在实际应用中，使用浮点精度的神经网络也观察到了类似的加速。在这方面，在 *Torch7*
    中，用户可以轻松选择（在运行时）默认的浮点类型。
- en: 21.2.2 Memory Allocation Control
  id: totrans-6464
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.2.2 内存分配控制
- en: One of the main complaints about using high level interfaces (such as Matlab)
    for numerical programming is the loss of control over memory allocation. The high-level
    abstraction makes it very hard for the researcher to know when a copy of a tensor
    is created. Although this is not a major problem for smallscale applications,
    as the data size grows, repeated copy and memory allocation might become problematic
    and even a bottleneck for the algorithm. To avoid such problems, *Torch7* tensor
    library is designed to support complete control over new memory allocation only
    when the user wants to use it. To better demonstrate this point, we repeat the
    matrix multiplication example.
  id: totrans-6465
  prefs: []
  type: TYPE_NORMAL
  zh: 使用高级接口（如 Matlab）进行数值编程的主要抱怨之一是对内存分配的控制丧失。高层抽象使研究人员很难知道何时创建张量的副本。虽然对于小规模应用这并不是主要问题，但随着数据大小的增长，重复的复制和内存分配可能变得成问题，甚至成为算法的瓶颈。为避免此类问题，*Torch7*
    张量库旨在仅在用户希望使用时支持完全控制新的内存分配。为了更好地说明这一点，我们重复矩阵乘法示例。
- en: 1 x = torch.Tensor(1000,5000)
  id: totrans-6466
  prefs: []
  type: TYPE_NORMAL
  zh: 1 x = torch.Tensor(1000,5000)
- en: 2 y = torch.Tensor(5000,3000)
  id: totrans-6467
  prefs: []
  type: TYPE_NORMAL
  zh: 2 y = torch.Tensor(5000,3000)
- en: 3 z = torch.mm(x,y) print(z:size()) 4 5 1000 6 3000 7 [torch.LongStorage of
    size 2]
  id: totrans-6468
  prefs: []
  type: TYPE_NORMAL
  zh: 3 z = torch.mm(x,y) print(z:size()) 4 5 1000 6 3000 7 [torch.LongStorage of
    size 2]
- en: One can see that the tensor z, which did not exist before, is newly allocated
    in this context. One can imagine that these series of operations are done repeatedly
    inside a loop. In this case, *Torch7* allows the following intuitive syntax.
  id: totrans-6469
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，张量z在此上下文中是新分配的，之前并不存在。可以想象，这一系列操作是在循环内反复执行的。在这种情况下，*Torch7*允许以下直观的语法。
- en: 1 x = torch.Tensor(1000,5000)
  id: totrans-6470
  prefs: []
  type: TYPE_NORMAL
  zh: 1 x = torch.Tensor(1000,5000)
- en: 2 y = torch.Tensor(5000,3000)
  id: totrans-6471
  prefs: []
  type: TYPE_NORMAL
  zh: 2 y = torch.Tensor(5000,3000)
- en: 3 z = torch.Tensor(1000,3000) 4 torch.mm(z,x,y)
  id: totrans-6472
  prefs: []
  type: TYPE_NORMAL
  zh: 3 z = torch.Tensor(1000,3000) 4 torch.mm(z,x,y)
- en: As it can be seen from the example, the *torch.mm* function also can take three
    arguments, in which case the first argument becomes the result of the operation.
    This syntax is implemented for all operations in the Tensor library consistently,
    so that for every single operation, the user has the choice of passing in the
    target Tensor or allocating a new one without any overhead and heavy syntax. For
    example the following element-wise Sin operation can be represented in two different
    ways.
  id: totrans-6473
  prefs: []
  type: TYPE_NORMAL
  zh: 从示例中可以看出，*torch.mm*函数也可以接受三个参数，在这种情况下，第一个参数成为操作的结果。该语法在张量库中的所有操作中一致地实现，因此对于每个操作，用户可以选择传入目标张量或分配一个新张量，而无需任何额外的开销和复杂的语法。例如，以下逐元素的正弦操作可以用两种不同的方式表示。
- en: 1 x = torch.rand(1000)
  id: totrans-6474
  prefs: []
  type: TYPE_NORMAL
  zh: 1 x = torch.rand(1000)
- en: 2 3 -- a new tensor is created 4 tsin = torch.sin(x) 5 6 -- a scalar one is
    added to tensor x (x is reused)
  id: totrans-6475
  prefs: []
  type: TYPE_NORMAL
  zh: 2 3 -- 新张量被创建 4 tsin = torch.sin(x) 5 6 -- 一个标量被加到张量x（x被重用）
- en: 7 x:add(1) 8 9 -- tsin is reused 10 torch.sin(tsin,x)
  id: totrans-6476
  prefs: []
  type: TYPE_NORMAL
  zh: 7 x:add(1) 8 9 -- tsin被重用 10 torch.sin(tsin,x)
- en: In this example, both scalar addition to tensor x and calculating the Sin of
    resulting tensor did not allocate any new memory. In the above example, we also
    hinted another use of tensor library, where one can make method calls on a tensor
    object, as in any object oriented language. This syntax makes it explicit that
    the operation is directly applied on the object that the method call is done.
  id: totrans-6477
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，对张量x进行的标量加法和计算结果张量的正弦并没有分配任何新的内存。在上述例子中，我们还暗示了张量库的另一种用途，即可以在张量对象上进行方法调用，正如在任何面向对象的语言中一样。该语法明确表明操作是直接应用于进行方法调用的对象。
- en: 21.2.3 Blas/Lapack Interfaces
  id: totrans-6478
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.2.3 Blas/Lapack接口
- en: The key to a successful numerical computation framework is to have efficient
    implementations of linear algebra operations. This requires highly sophisticated
    algorithms with very precise implementations. In order to be able to provide the
    best experience, the C tensor library (TH) that is included in *Torch7* interfaces
    BLAS and LAPACK libraries.11 All the major Level 1, 2 and 3 BLAS
  id: totrans-6479
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的数值计算框架的关键在于高效实现线性代数操作。这需要非常复杂的算法和非常精确的实现。为了提供最佳体验，包含在*Torch7*中的C张量库（TH）接口了BLAS和LAPACK库。所有主要的1、2和3级BLAS
- en: 11 http://www.netlib.org.
  id: totrans-6480
  prefs: []
  type: TYPE_NORMAL
  zh: 11 http://www.netlib.org.
- en: operations like matrix-vector products, matrix-matrix products and most major
    linear algebra routines like singular value decomposition, matrix inverse, least
    square solutions are interfaced to BLAS and LAPACK libraries respectively. This
    interface provides the user with a rich experience of building block operations
    where higher level algorithms can easily be implemented.
  id: totrans-6481
  prefs: []
  type: TYPE_NORMAL
  zh: 操作，如矩阵-向量乘积、矩阵-矩阵乘积以及大多数主要线性代数例程，如奇异值分解、矩阵逆和最小二乘解，分别与BLAS和LAPACK库进行了接口。这种接口为用户提供了丰富的构建块操作体验，使得高层算法可以轻松实现。
- en: 21.2.4 Simd Instructions
  id: totrans-6482
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.2.4 Simd指令
- en: 'Most computations involved in a neural network consist in applying the same
    type of operations over (possibly large) vectors or matrices. Several CPU architectures,
    such as PowerPC, Intel or ARM support SIMD (Single Instruction, Multiple Data)
    operations which are perfectly suited for this kind of task: for example with
    SSE (Streaming SIMD Extensions) on Intel processors one might perform 4 additions
    over a vector with a unique instruction. Calling these instructions instead of
    regular CPU instructions might lead to great speedup. This type of optimization
    is unfortunately CPU-specific. Fortunately, in many cases one can rely on BLAS/LAPACK
    implementations specialized for a given platform, which leverage SIMD instructions.
    For other neural network specific cases, such as convolutions, one must implement
    specialized routines for each architecture of choice. In *Torch7*, we try to leverage
    SSE (on Intel architectures) and NEON (on ARM architectures) whenever possible.
    Compared to a non-SSE implementation 1.5× speedup are common, as shown in Figure
    21.3 in the case of convolutional neural networks.'
  id: totrans-6483
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的大多数计算涉及对（可能很大的）向量或矩阵应用相同类型的操作。许多CPU架构，例如PowerPC、Intel或ARM，支持SIMD（单指令、多数据）操作，这非常适合这种任务：例如，在Intel处理器上，使用SSE（流式SIMD扩展），可以通过单条指令对一个向量进行4次加法。调用这些指令而不是常规CPU指令可能会导致显著加速。不幸的是，这种类型的优化是CPU特定的。幸运的是，在许多情况下，可以依赖于为特定平台优化的BLAS/LAPACK实现，这些实现利用了SIMD指令。对于其他特定于神经网络的情况，例如卷积，必须为每个选择的架构实现专门的例程。在*Torch7*中，我们尽可能利用SSE（在Intel架构上）和NEON（在ARM架构上）。与非SSE实现相比，通常可以获得1.5倍的加速，如图21.3所示，尤其是在卷积神经网络的情况下。
- en: '![538_image_0.png](538_image_0.png)'
  id: totrans-6484
  prefs: []
  type: TYPE_IMG
  zh: '![538_image_0.png](538_image_0.png)'
- en: Fig. 21.3. Comparison of several convolutional neural network implementations
    (without SSE, with SSE or with BLAS). Tests were conducted using one core on a
    Intel biXeon X5690 server. Performance is given in number of examples processed
    by second
  id: totrans-6485
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.3。几种卷积神经网络实现的比较（无SSE、有SSE或使用BLAS）。测试是在Intel biXeon X5690服务器上使用一个核心进行的。性能以每秒处理的样本数量表示。
- en: (higher is better). Three different architectures were tested, with input image
    sizes of 32x32, 96x96 and 256x256 respectively.
  id: totrans-6486
  prefs: []
  type: TYPE_NORMAL
  zh: （越高越好）。测试了三种不同的架构，输入图像大小分别为32x32、96x96和256x256。
- en: 21.2.5 Ordering Memory Accesses
  id: totrans-6487
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.2.5 排序内存访问
- en: As already mentioned in Section 21.2.1 and Section 21.2.2, memory accesses are
    one of the main bottleneck on today's computers. In fact, not only the number
    of accesses is important, but also the *order* of these accesses. For example,
    operations with tensors not contiguous in memory (say, with extra jumps between
    each element of the tensor) should always be avoided. In many cases, it is better
    to organize the tensor in a contiguous memory block (possibly at the cost of the
    copy), before performing any intensive computations. A striking example for neural
    networks is convolutions. When performing a convolution over an image, successive
    dot products are done between the kernel and all possible patches of the image.
    One can create a copy of all these patches beforehand (the drawback being a huge
    memory cost for large convolutions or large images) and then apply a matrix-matrix
    operation (using BLAS) to compute all dot products. The memory consumption increases
    proportional to the number of pixels of convolutional kernel. As shown in Figure
    21.3, this leads to unbeatable runtime performance, even though the initial memory
    copy is quite large. *Torch7* provides this implementation as part of the neural
    net package too. Whenever there is sufficient memory available, it is advantageous
    to use this implementation which uses an innovative design that takes advantage
    of multi-core CPU architectures.
  id: totrans-6488
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在第21.2.1节和第21.2.2节中提到的，内存访问是当今计算机的主要瓶颈之一。事实上，不仅访问次数重要，这些访问的*顺序*也很重要。例如，应该始终避免对内存中不连续的张量进行操作（例如，张量的每个元素之间有额外跳转）。在许多情况下，最好先将张量组织在一个连续的内存块中（可能需要付出复制的代价），然后再进行任何密集计算。对于神经网络，一个突出的例子就是卷积。当对图像执行卷积时，会在核和图像的所有可能补丁之间进行连续的点积。可以提前创建所有这些补丁的副本（缺点是对于大卷积或大图像会产生巨大的内存成本），然后应用矩阵-矩阵操作（使用BLAS）来计算所有点积。内存消耗与卷积核的像素数量成正比。如图21.3所示，这导致了无与伦比的运行时性能，尽管初始内存复制相当大。*Torch7*也将此实现作为神经网络包的一部分提供。每当有足够的内存可用时，使用这种利用多核CPU架构优势的创新设计的实现是有利的。
- en: 21.2.6 Openmp Support
  id: totrans-6489
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.2.6 Openmp支持
- en: 'Open Multi-Processing (OpenMP) provides a shared memory CPU parallelization
    framework on C/C++ and Fortran languages on almost every operating system and
    compiler toolset. It generally requires minimal modification for integrating into
    an existing project. *Torch7* is designed and developed to use OpenMP directives
    for various operations in its tensor library and neural network package. Although
    the details of the OpenMP specification is beyond the scope of this work, below
    we show one of the most commonly used OpenMP directive, parallelization over for-loops:'
  id: totrans-6490
  prefs: []
  type: TYPE_NORMAL
  zh: '开放多处理（OpenMP）在几乎所有操作系统和编译器工具集上，为 C/C++ 和 Fortran 语言提供共享内存的 CPU 并行化框架。通常只需进行最小修改即可集成到现有项目中。*Torch7*
    旨在利用 OpenMP 指令进行其张量库和神经网络包中的各种操作。尽管 OpenMP 规范的细节超出了本工作的范围，下面展示了最常用的 OpenMP 指令之一，即对
    for 循环的并行化： '
- en: 1 // private makes a copy for each thread 2 \#pragma omp parallel for private(i)
  id: totrans-6491
  prefs: []
  type: TYPE_NORMAL
  zh: '1 // private 为每个线程创建一个副本 2 \#pragma omp parallel for private(i) '
- en: 3 for (i=0; i<N; i++) 4 {
  id: totrans-6492
  prefs: []
  type: TYPE_NORMAL
  zh: 3 for (i=0; i<N; i++) 4 {
- en: 5 a[i] = i*i; 6 }
  id: totrans-6493
  prefs: []
  type: TYPE_NORMAL
  zh: 5 a[i] = i*i; 6 }
- en: Without the omp parallel for directive at line 2, this piece of code will run
    to completion using a single thread. However, since each loop iteration is independent
    from each other, it becomes a trivial single line addition to existing code that
    parallelizes this computation over many cores.
  id: totrans-6494
  prefs: []
  type: TYPE_NORMAL
  zh: '如果没有在第 2 行使用 omp parallel for 指令，这段代码将使用单线程完成。然而，由于每个循环迭代相互独立，因此这仅需在现有代码中添加一行，即可将计算并行化到多个核心上。 '
- en: Torch7 automatically detects if the compiler supports OpenMP directives and
    compiles a high level package that adds multi-threaded tensor operations, convolutions
    and several neural network classes. The switch from single threaded code to multi-threaded
    code is completely transparent to the user and it only requires *-l openmp* argument
    to be passed to torch executable. With this option, *Torch7* by default uses the
    OpenMP enabled function calls when available. The number of threads to be used
    can be specified by either setting the
  id: totrans-6495
  prefs: []
  type: TYPE_NORMAL
  zh: 'Torch7 会自动检测编译器是否支持 OpenMP 指令，并编译一个高级包，添加多线程张量操作、卷积和多个神经网络类。从单线程代码切换到多线程代码对用户完全透明，只需将
    *-l openmp* 参数传递给 torch 可执行文件。使用此选项时，*Torch7* 默认在可用时使用启用 OpenMP 的函数调用。可以通过设置 '
- en: '"OMP_NUM_THREADS" environment variable to desired number:'
  id: totrans-6496
  prefs: []
  type: TYPE_NORMAL
  zh: '“OMP_NUM_THREADS” 环境变量设置为所需的数量： '
- en: 1 bash\# export OMP_NUM_THREADS=4 or from inside lua by 1 torch.setNumThread(4)
  id: totrans-6497
  prefs: []
  type: TYPE_NORMAL
  zh: '1 bash\# export OMP_NUM_THREADS=4 或在 lua 内部通过 1 torch.setNumThread(4) '
- en: function. Moreover, openmp can even be temporarily enabled or disabled using
    the following function calls.
  id: totrans-6498
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，openmp 甚至可以通过以下函数调用暂时启用或禁用。 '
- en: 1 torch.setNumThreads(1) 2 torch.setNumThreads(N)
  id: totrans-6499
  prefs: []
  type: TYPE_NORMAL
  zh: '1 torch.setNumThreads(1) 2 torch.setNumThreads(N) '
- en: Multi-threading of BLAS operations rely on the specific BLAS library that Torch7
    is linked against. For example Intel's MKL library also uses OpenMP for parallelizing
    Level 3 BLAS operations. In the neural network package nn, the convolutional layers,
    most common non-linearity functions like tanh and sigmoid, pooling operations
    like average, sum and max pooling and various other primitive operations like
    sum, square modules are all parallelized. For all the models that apply element-wise
    operations, the parallelization is almost as trivial as shown in the example above.
    For more complicated modules like convolutional layers with multiple input output
    feature maps, the function evaluation pass is parallelized over output feature
    maps so that every output feature is calculated in parallel. For calculating the
    gradient wrt kernels, operations are parallelized over kernels and over input
    feature maps for gradient wrt inputs. Using this strategy the convolutional network
    architecture can be sped up almost linearly.
  id: totrans-6500
  prefs: []
  type: TYPE_NORMAL
  zh: 'BLAS 操作的多线程依赖于 Torch7 关联的特定 BLAS 库。例如，英特尔的 MKL 库也使用 OpenMP 来并行化 Level 3 BLAS
    操作。在神经网络包 nn 中，卷积层、最常见的非线性函数（如 tanh 和 sigmoid）、池化操作（如平均池化、求和池化和最大池化）以及各种其他基本操作（如求和、平方模块）都被并行化。对于应用逐元素操作的所有模型，几乎可以像上面的示例那样轻松并行化。对于更复杂的模块，如具有多个输入输出特征图的卷积层，函数评估通过输出特征图并行化，以便每个输出特征都能并行计算。计算与核的梯度时，操作在核和输入特征图上并行化，以计算与输入的梯度。采用此策略，卷积网络架构几乎可以线性加速。 '
- en: 21.2.7 Cuda Support
  id: totrans-6501
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.2.7 Cuda 支持
- en: CUDA (Compute Unified Device Architecture) is nVidia's framework for programming
    their graphics processors to perform general purpose computations.
  id: totrans-6502
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA（统一计算设备架构）是nVidia为编程其图形处理器以执行通用计算而提供的框架。
- en: CUDA exposes the hierarchy of memories available to the graphics processor,
    the two main ones being the external (large, high-latency) DRAM and the internal
    shared memory (a couple of kB, low-latency). It also exposes the hierarchy of
    compute cores, and how they interact with each other, and with the different types
    of memory.
  id: totrans-6503
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA暴露了可供图形处理器使用的内存层次结构，主要有外部（大，高延迟）DRAM和内部共享内存（几千字节，低延迟）。它还暴露了计算核心的层次结构，以及它们如何相互作用，以及与不同类型内存的互动。
- en: 'Contrary to common belief, we found that writing CUDA code (kernels) can be
    significantly simplified. It is very easy to obtain decent performance, and the
    simplest kernels already yield satisfying speedups over regular C. The only three
    things to know, and carefully handle are: understanding the interaction between
    shared memory and threads; understand memory coalescing, to maximize bandwidth
    to/from external DRAM; understand the hierarchy of processing units, to efficiently
    divide the workload between blocks and threads. Once understood, these concepts
    were sufficient to allow us to write our own 2D convolutions, which are computed
    at about 200GFLOP/s on a GTX580, for large enough inputs. For smaller inputs,
    our OpenMP+SSE implementation remains more efficient.'
  id: totrans-6504
  prefs: []
  type: TYPE_NORMAL
  zh: 与普遍信念相反，我们发现编写CUDA代码（内核）可以大大简化。获得不错的性能非常容易，最简单的内核在常规C上已经能实现令人满意的加速。需要了解并谨慎处理的三件事是：理解共享内存与线程之间的交互；理解内存合并，以最大化与外部DRAM之间的带宽；理解处理单元的层次结构，以高效分配块和线程之间的工作负载。一旦理解了这些概念，我们能够编写自己的2D卷积，对于足够大的输入，GTX580上的计算速度约为200GFLOP/s。对于较小的输入，我们的OpenMP+SSE实现仍然更高效。
- en: It is worth mentioning that *Torch7* employs an efficient, yet general method
    for implementing a wide variety of CUDA kernels. As it is shown in the upcoming
    sections, this strategy results in the best performance in most cases. However,
    it is also possible to achieve superior performance by developing CUDA kernels
    under specific assumptions, like particular input or operator shape and sizes.
    Despite the performance advantage, these cases generally require significant development
    effort and produce modules that can not be reused, thus they are not suitable
    for a general machine learning library.
  id: totrans-6505
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，*Torch7*采用了一种高效但通用的方法来实现各种CUDA内核。正如接下来的章节所示，这种策略在大多数情况下能够实现最佳性能。然而，通过在特定假设下开发CUDA内核，如特定输入或运算符形状和大小，也可以实现更高的性能。尽管有性能优势，但这些情况通常需要大量的开发工作，并生成不能重用的模块，因此不适合通用机器学习库。
- en: 'Once built with CUDA, *software!Torch7* provides a new Tensor type: torch.'
  id: totrans-6506
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦用CUDA构建，*software!Torch7*提供了一种新的Tensor类型：torch。
- en: 'CudaTensor. Tensors with this particular type lives in the GPU''s DRAM memory.
    All operators defined on standard Tensors are also defined on CudaTensors, which
    completely abstracts the use of the graphics processor. Here is a small illustrative
    example, that demonstrates the simplicity of the interface:'
  id: totrans-6507
  prefs: []
  type: TYPE_NORMAL
  zh: CudaTensor。具有这种特定类型的张量生活在GPU的DRAM内存中。所有在标准张量上定义的运算符也在CudaTensors上定义，这完全抽象了图形处理器的使用。以下是一个小的示例，展示了接口的简单性：
- en: 1 -- lives in the CPU's DRAM
  id: totrans-6508
  prefs: []
  type: TYPE_NORMAL
  zh: 1 -- 生活在CPU的DRAM中。
- en: 2 tf = torch.FloatTensor(4,100,100) 3 4 -- lives in the GPU's DRAM
  id: totrans-6509
  prefs: []
  type: TYPE_NORMAL
  zh: 2 tf = torch.FloatTensor(4,100,100) 3 4 -- 生活在GPU的DRAM中。
- en: 5 tc = tf:cuda()
  id: totrans-6510
  prefs: []
  type: TYPE_NORMAL
  zh: 5 tc = tf:cuda()
- en: 6 7 -- performed by the GPU 8 tc:mul(3)
  id: totrans-6511
  prefs: []
  type: TYPE_NORMAL
  zh: 6 7 -- 由GPU执行 8 tc:mul(3)
- en: 9 10 -- res lives in the CPU's DRAM 11 res = tc:float()
  id: totrans-6512
  prefs: []
  type: TYPE_NORMAL
  zh: 9 10 -- res 生活在CPU的DRAM中 11 res = tc:float()
- en: On top of the Tensors' main operators, all the matrix-based operators are available,
    as well as most standard convolution routines.
  id: totrans-6513
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Tensors的主要运算符外，所有基于矩阵的运算符和大多数标准卷积例程也可用。
- en: 21.2.8 Benchmarks
  id: totrans-6514
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.2.8 基准测试
- en: 'In this section we analyze the efficiency of *Torch7* in two different setups:
    first in the framework of a matrix-matrix multiplication benchmark, then when
    training various neural networks. To that effect, we compare *Torch7* with *Numpy*
    and Theano. We chose *Numpy* as a reference because it is a widely-used numerical
    library for Python, the latter being itself a widely-used scripting language.'
  id: totrans-6515
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了*Torch7*在两种不同设置下的效率：首先在矩阵乘法基准测试框架中，然后在训练各种神经网络时。为此，我们将*Torch7*与*Numpy*和Theano进行了比较。我们选择*Numpy*作为参考，因为它是一个广泛使用的Python数值库，而Python本身也是一种广泛使用的脚本语言。
- en: Theano [1] is a recent compiler for mathematical expressions, built upon Python
    and Numpy, and which has been shown as over-performing many neural network implementations,
    which makes it a very relevant baseline. In our experiments we chose the latest
    version of each software, that is *Theano* 0.5, *Numpy* 1.6.1 and Scipy 0.10.1.
    Measuring the Overhead of Interpreted Languages. The majority of the computation
    for neural networks and many numerical algorithms is spent in BLAS calls for performing
    linear algebra operations. To that end, we demonstrate the efficiency of the *Torch7*
    and also the underlying C library TH.
  id: totrans-6516
  prefs: []
  type: TYPE_NORMAL
  zh: Theano [1] 是一个最近的数学表达式编译器，基于 Python 和 Numpy，已经被证明在许多神经网络实现中表现优异，这使得它成为一个非常相关的基准。在我们的实验中，我们选择了每个软件的最新版本，即
    *Theano* 0.5、*Numpy* 1.6.1 和 Scipy 0.10.1。测量解释性语言的开销。神经网络和许多数值算法的大部分计算都花费在 BLAS
    调用上，以执行线性代数运算。为此，我们展示了 *Torch7* 的效率以及底层 C 库 TH 的效率。
- en: The *Torch7* numerical routines follow a simple design that contains layers.
  id: totrans-6517
  prefs: []
  type: TYPE_NORMAL
  zh: '*Torch7* 的数值例程遵循简单的设计，包含多个层。'
- en: The first layer is an efficient C library that provides a high level tensor
    package
  id: totrans-6518
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层是一个高效的 C 库，提供高级张量包。
- en: (TH). TH library provides a templated design that enables the choice of different
    precisions. Available types are, Byte (unsigned char), Char (char), Short
  id: totrans-6519
  prefs: []
  type: TYPE_NORMAL
  zh: （TH）。TH 库提供模板化设计，能够选择不同的精度。可用类型包括，字节（无符号字符）、字符（字符）、短整型。
- en: '![542_image_0.png](542_image_0.png)'
  id: totrans-6520
  prefs: []
  type: TYPE_IMG
  zh: '![542_image_0.png](542_image_0.png)'
- en: Fig. 21.4. Benchmarks of matrix multiplication performance using C, *Torch7*
    torch package, nn package in *Torch7*, Numpy and Theano. Tests were conducted
    on a machine with two Intel Xeon X5690 CPUs with 6 computational cores in each
    CPU. Hyperthreading was disabled. We considered multi-thread computation using
    1, 2, 4, 8 and 12 CPU cores. Performance is given in seconds of time spent for
    processing, therefore smaller is better.
  id: totrans-6521
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21.4。使用 C、*Torch7* torch 包、*Torch7* 中的 nn 包、Numpy 和 Theano 的矩阵乘法性能基准测试。测试是在一台配备两颗
    Intel Xeon X5690 CPU（每个 CPU 有 6 个计算核心）的机器上进行的。超线程功能已禁用。我们考虑使用 1、2、4、8 和 12 个 CPU
    核心进行多线程计算。性能以处理所花费的时间（秒）表示，因此时间越小越好。
- en: (16 bit integer), Integer (32 bit integer), Long (64 bit integer), Float (32
    bit floating point) and Double (64 bit floating precision). TH library does not
    have any dependencies to Lua or any other language other than standard C libraries,
    therefore it is also suitable to be used by other projects that rely on efficient
    numerical routines. The choice of C language was a careful choice as with Lua.
    Since TH uses only C, it can be compiled in almost any programming environment
    like cellphones, DSP, embedded systems, etc. TH provides interface to many BLAS
    operations, but also contains hand-coded operations for all functions in case
    no BLAS library is available. It also provides and interface to several most widely
    used LAPACK routines for linear algebra. The second layer on top of TH is the
    torch package that integrates TH into Lua. All of the TH mathematical operations
    are interfaced from the Lua language in the torch packages. Finally, the nn package
    uses the *torch* package to provide a modular, yet fast and efficient, neural
    network library.
  id: totrans-6522
  prefs: []
  type: TYPE_NORMAL
  zh: （16 位整数）、整数（32 位整数）、长整型（64 位整数）、浮点数（32 位浮点）和双精度（64 位浮点精度）。TH 库不依赖于 Lua 或任何其他语言，除了标准
    C 库，因此也适合用于依赖高效数值例程的其他项目。选择 C 语言是经过深思熟虑的选择，就像 Lua 一样。由于 TH 仅使用 C，它可以在几乎任何编程环境中编译，如手机、DSP、嵌入式系统等。TH
    提供对许多 BLAS 操作的接口，同时还包含所有函数的手工编码操作，以防没有 BLAS 库可用。它还提供了对几个最常用的 LAPACK 例程的线性代数接口。位于
    TH 之上的第二层是 torch 包，它将 TH 集成到 Lua 中。所有 TH 数学操作都通过 torch 包中的 Lua 语言接口进行。最后，nn 包使用
    *torch* 包提供一个模块化的、高速且高效的神经网络库。
- en: One might argue that such a layered approach would introduce quite a bit of
    overhead. In order to quantify the overhead coming from each layer, we selected
    matrix-matrix multiplication as our test case since it is one of the most widely
    used operations in linear algebra and ran tests using different sizes of matrices
    and different layers of programming. We used 100×100 and 1000×1000, matrices and
    benchmarked using a C only program that directly uses TH library, using torch
    library, using linear layer (with no bias) from nn package in Torch7. We also
    included tests using *Numpy* package and finally *Theano*. We compiled all packages
    using Intel MKL library to be able achieve the best possible performance and maximize
    the advantages of using CPU threading. As it can be seen from the results given
    in Figure 21.4, the overhead coming from TH, *Torch7* or nn libraries is minimal,
    even for small size matrices. Even though Python gets a bit more overhead, for
    larger matrices the overhead is minimal in all configurations.
  id: totrans-6523
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会认为这样的分层方法会引入相当大的开销。为了量化每一层带来的开销，我们选择了矩阵乘法作为测试案例，因为这是线性代数中最广泛使用的操作之一，并使用不同大小的矩阵和不同层的编程进行了测试。我们使用了100×100和1000×1000的矩阵，并用一个仅使用C语言的程序进行了基准测试，该程序直接使用TH库、torch库以及Torch7中的nn包的线性层（无偏差）。我们还包括了使用*Numpy*包和最后的*Theano*的测试。我们使用Intel
    MKL库编译了所有包，以便实现最佳性能并最大化CPU线程的优势。从图21.4给出的结果可以看出，来自TH、*Torch7*或nn库的开销是微不足道的，即使对于小尺寸矩阵。尽管Python的开销稍大，但对于较大的矩阵，在所有配置中开销都很小。
- en: Comparing Machine Learning Packages. In a recent paper [1], the authors introduced
    a new compiler for mathematical expressions, built upon Python and Numpy. As for
    *Torch7*, Theano is (at this time) mainly used in a neural network framework.
    Theano can be either run on a CPU or a GPU. The authors of Theano showed benchmarks
    (involving the training of various neural networks architectures) comparing with
    other alternative implementations (when running Theano over a GPU), including
    Torch5, Matlab with GPUmat (running over a GPU) or EBLearn 12. Below, we reproduce
    these exact benchmarks, limiting ourselves to *Torch7* versus Theano, as Theano
    appears already faster than any existing implementation.
  id: totrans-6524
  prefs: []
  type: TYPE_NORMAL
  zh: 比较机器学习包。在最近的一篇论文[1]中，作者介绍了一种基于Python和Numpy的新数学表达式编译器。至于*Torch7*，Theano（此时）主要用于神经网络框架。Theano可以在CPU或GPU上运行。Theano的作者展示了基准测试（涉及多种神经网络架构的训练），与其他替代实现（在GPU上运行Theano时）进行比较，包括Torch5、带有GPUmat的Matlab（在GPU上运行）或EBLearn
    12。下面，我们复现了这些精确的基准测试，将自己限制在*Torch7*与Theano的比较上，因为Theano已显得比任何现有实现更快。
- en: '|                 | 32 × 32 96 × 96 256 × 256 # F. Maps   |       |       |    |'
  id: totrans-6525
  prefs: []
  type: TYPE_TB
  zh: '|                 | 32 × 32 96 × 96 256 × 256 # 特征图   |       |       |    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-6526
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1.c Convolution | 5 × 5                                 | 7 × 7 | 7 × 7 |
    6  |'
  id: totrans-6527
  prefs: []
  type: TYPE_TB
  zh: '| 1.c 卷积 | 5 × 5                                 | 7 × 7 | 7 × 7 | 6  |'
- en: '| 1.p Max-pooling | 2 × 2                                 | 3 × 3 | 5 × 5 |
    6  |'
  id: totrans-6528
  prefs: []
  type: TYPE_TB
  zh: '| 1.p 最大池化 | 2 × 2                                 | 3 × 3 | 5 × 5 | 6  |'
- en: '| 2.c Convolution | 5 × 5                                 | 7 × 7 | 7 × 7 |
    16 |'
  id: totrans-6529
  prefs: []
  type: TYPE_TB
  zh: '| 2.c 卷积 | 5 × 5                                 | 7 × 7 | 7 × 7 | 16 |'
- en: '| 2.p Max-pooling | 2 × 2                                 | 3 × 3 | 4 × 4 |
    16 |'
  id: totrans-6530
  prefs: []
  type: TYPE_TB
  zh: '| 2.p 最大池化 | 2 × 2                                 | 3 × 3 | 4 × 4 | 16 |'
- en: '| 3.l Linear      | 120 output features                   |       |       |    |'
  id: totrans-6531
  prefs: []
  type: TYPE_TB
  zh: '| 3.l 线性      | 120个输出特征                   |       |       |    |'
- en: '| 4.o Linear      | 10 output features                    |       |       |    |'
  id: totrans-6532
  prefs: []
  type: TYPE_TB
  zh: '| 4.o 线性      | 10个输出特征                    |       |       |    |'
- en: Table 21.1. Convolutional Network Architectures used in the benchmark study
  id: totrans-6533
  prefs: []
  type: TYPE_NORMAL
  zh: 表 21.1. 基准研究中使用的卷积网络架构
- en: For a fair comparison, we compiled both Numpy and SciPy (on which Theano relies)
    and *Torch7* against MKL Intel library. Latest versions of Theano also support
    direct link against MKL for certain operations (without passing by Numpy),
  id: totrans-6534
  prefs: []
  type: TYPE_NORMAL
  zh: 为了公平比较，我们将Numpy和SciPy（Theano依赖的库）与*Torch7*编译为MKL Intel库。Theano的最新版本也支持对MKL的直接链接用于某些操作（无需经过Numpy），
- en: which we setup carefully. We ran the experiments on a Intel Xeon X5690 with
    12 cores. We optionally used a nVidia Tesla M2090 GPU. Following [1] benchmark
    suite, we considered the training of three kinds of multi-layer Perceptrons. 1.
    784 inputs, 10 classes, cross-entropy cost, and respectively no-hidden layer.
    2.
  id: totrans-6535
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了仔细的设置。我们在一个12核的Intel Xeon X5690上进行了实验。我们可选地使用了nVidia Tesla M2090 GPU。按照[1]的基准套件，我们考虑了三种多层感知器的训练。1.
    784个输入，10个类别，交叉熵成本，以及没有隐藏层。
- en: One hidden layer of size 500. 3. Three hidden layers of size 1000. We also considered
    the training of three kinds of convolutional neural networks (as shown in Table
    21.1) on 32 × 32, 96 × 96, and 256 × 256 input images, following exactly the architectures
    given in [1]. The optimization algorithms we used were pure stochastic gradient
    descent (SGD) and SGD with a mini-batch of 60 examples. We compare all architectures
    running on a single CPU core, over multiple cores using OpenMP, or on the GPU.
    Note that Theano does not support OpenMP.
  id: totrans-6536
  prefs: []
  type: TYPE_NORMAL
  zh: 一个隐藏层，大小为 500。三个隐藏层，大小为 1000。我们还考虑了对三种卷积神经网络的训练（如表 21.1 所示），输入图像为 32 × 32、96
    × 96 和 256 × 256，完全按照文献 [1] 中给出的架构进行。我们使用的优化算法是纯随机梯度下降 (SGD) 和使用 60 个样本的迷你批次 SGD。我们比较了在单个
    CPU 核心、多核心使用 OpenMP 或在 GPU 上运行的所有架构。请注意，Theano 不支持 OpenMP。
- en: However, it gets a speedup (on the multi-layer Perceptron benchmarks), since
    the Intel MKL library (called through Numpy) supports multiple threads using OpenMP.
  id: totrans-6537
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在多层感知器基准测试中，它获得了加速，因为 Intel MKL 库（通过 Numpy 调用）支持使用 OpenMP 的多线程。
- en: As shown in Figure 21.5, *Torch7* is faster than Theano on most benchmarks.
  id: totrans-6538
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 21.5 所示，*Torch7* 在大多数基准测试中比 Theano 更快。
- en: Interestingly, Theano underperforms for small architectures using pure SGD
  id: totrans-6539
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，Theano 在使用纯 SGD 时在小型架构上表现不佳。
- en: training (left column in Figure 21.5), which might be explained by a Python
    overhead, as mentioned in the previous section. Another interesting comment is
  id: totrans-6540
  prefs: []
  type: TYPE_NORMAL
  zh: 训练（如图 21.5 左列所示），这可能是由于前一节提到的 Python 开销。另一个有趣的评论是
- en: 12 http://www.eblearn.sf.net
  id: totrans-6541
  prefs: []
  type: TYPE_NORMAL
  zh: 12 http://www.eblearn.sf.net
- en: the surprising performance of OpenMP implementations compared to the GPU
  id: totrans-6542
  prefs: []
  type: TYPE_NORMAL
  zh: OpenMP 实现的表现与 GPU 相比令人惊讶。
- en: implementation. As it can be seen from the graphs only largest network architectures
    will benefit from using the GPU. It is also worth mentioning that for CNN
  id: totrans-6543
  prefs: []
  type: TYPE_NORMAL
  zh: 实现。从图表中可以看出，只有最大的网络架构才会受益于使用 GPU。还值得一提的是，对于 CNN
- en: with 32 × 32 inputs using batch training, Theano's GPU implementation is superior
    than *Torch 7*. Under certain conditions, GPU optimizations might pay off by providing
    significant speed-ups, however they also require significant development effort
    for covering a small input domain. For CNN experiments a second Torch7 benchmark,
    TorchMM is included. In this case matrix-matrix product operations for performing
    convolutions as explained in section 21.2.5 are used. It can be seen that this
    implementation significantly outperforms other models from *Theano* and *Torch7*,
    including GPU implementations.
  id: totrans-6544
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用批量训练的 32 × 32 输入下，Theano 的 GPU 实现优于 *Torch 7*。在某些条件下，GPU 优化可能会通过提供显著的加速而获得回报，但它们也需要付出显著的开发努力以覆盖小输入域。对于
    CNN 实验，第二个 Torch7 基准 TorchMM 被纳入。在这种情况下，使用了第 21.2.5 节中解释的进行卷积的矩阵乘法操作。可以看到，这种实现显著优于
    *Theano* 和 *Torch7* 的其他模型，包括 GPU 实现。
- en: 21.3 Efficient Optimization Heuristics
  id: totrans-6545
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.3 高效优化启发式
- en: As pointed in Chapter 18, the size of datasets have grown faster than the speed
    of processors in the last couple of years. When estimating the parameters of a
    neural network, it is then crucial to use an optimization procedure that can scale
    accordingly. Recently, research on optimization methods for neural networks has
    become an important topic [2, 4, 8, 7]. *Torch 7* provides a flexible framework
    designed particularly to make it easy for developing optimization algorithms on
    neural networks.
  id: totrans-6546
  prefs: []
  type: TYPE_NORMAL
  zh: 正如第 18 章所指出的，数据集的大小在过去几年中增长速度快于处理器的速度。在估计神经网络的参数时，使用能够相应扩展的优化过程至关重要。最近，关于神经网络优化方法的研究已成为一个重要话题
    [2, 4, 8, 7]。*Torch 7* 提供了一个灵活的框架，专门设计用于简化神经网络上优化算法的开发。
- en: 'Let us consider the case of supervised learning, when one has a training set
    of N examples (xn, yn), with xn an observed input vector and yn an output target
    vector that we wish to predict. We consider a loss function l(ˆyn, yn) that measures
    the cost of predicting yˆn when the actual answer is yn. We also consider a predictor
    fw(xn), with trainable parameters w. The task of learning can be defined as finding
    the vector w that minimizes the loss function L over the entire training set:'
  id: totrans-6547
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑监督学习的情况，当训练集包含 N 个样本 (xn, yn) 时，其中 xn 是观察到的输入向量，yn 是我们希望预测的输出目标向量。我们考虑一个损失函数
    l(ˆyn, yn)，它衡量当实际答案为 yn 时预测 yˆn 的代价。我们还考虑一个预测器 fw(xn)，其可训练参数为 w。学习的任务可以定义为寻找向量
    w，使其在整个训练集上最小化损失函数 L：
- en: $$L({\bf w})=\frac{1}{N}\sum_{n=1}^{N}l(f_{\bf w}(x_{n}),y_{n}),$$ $${\bf w}^{*}=\mathop{\rm
    argmin}_{\bf w}L({\bf w}).$$
  id: totrans-6548
  prefs: []
  type: TYPE_NORMAL
  zh: $$L({\bf w})=\frac{1}{N}\sum_{n=1}^{N}l(f_{\bf w}(x_{n}),y_{n}),$$ $${\bf w}^{*}=\mathop{\rm
    argmin}_{\bf w}L({\bf w}).$$
- en: $$(21.1)$$
  id: totrans-6549
  prefs: []
  type: TYPE_NORMAL
  zh: $$(21.1)$$
- en: $$(21.2)$$
  id: totrans-6550
  prefs: []
  type: TYPE_NORMAL
  zh: $$(21.2)$$
- en: 'This general form of loss minimization can be easily carried out using one
    of a variety of optimization methods like Conjugate Gradient Descent (CG), BFGS
    or Limited Memory BFGS, Levenberg-Marquardt methods or simple SGD. In Torch7,
    these heuristics and methods can be carried out simply, using one unifying idea:
    decoupling the form of the function fw from the optimization procedure.'
  id: totrans-6551
  prefs: []
  type: TYPE_NORMAL
  zh: 这种损失最小化的一般形式可以通过多种优化方法轻松实现，例如共轭梯度下降(CG)、BFGS或有限记忆BFGS、莱文贝格-马夸特方法或简单的SGD。在Torch7中，这些启发式方法和方法可以简单实现，使用一个统一的思想：将函数f_w的形式与优化过程解耦。
- en: By grouping all the trainable parameters into a single parameter vector and
    using a vector of gradients of the same size for gradients, the type and shape
    of the neural network is completely abstracted from the developer. Combined with
    powerful closure mechanism of Lua, one can develop optimization algorithms
  id: totrans-6552
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将所有可训练参数组合成一个单一参数向量，并使用相同大小的梯度向量，神经网络的类型和形状完全与开发者抽象分离。结合Lua强大的闭包机制，可以开发优化算法。
- en: '![545_image_0.png](545_image_0.png)'
  id: totrans-6553
  prefs: []
  type: TYPE_IMG
  zh: '![545_image_0.png](545_image_0.png)'
- en: Fig. 21.5. Benchmarks of *Torch7* versus Theano, while training various neural
    networks architectures with SGD algorithm. Tests were conducted on a machine with
    two Intel Xeon X5690 CPUs and Nvidia M2090 GPU. We considered multi-thread computation
    using 1 to 12 CPU cores using OpenMP and GPU with Nvidia CUDA interface.
  id: totrans-6554
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.5。*Torch7*与Theano的基准测试，同时使用SGD算法训练各种神经网络架构。测试在一台配备两颗Intel Xeon X5690 CPU和Nvidia
    M2090 GPU的机器上进行。我们考虑使用1到12个CPU核心的多线程计算，利用OpenMP和Nvidia CUDA接口进行GPU计算。
- en: Performance is given in number of examples processed by second (higher is better).
    "batch" means 60 examples at a time were fed when training with SGD. *TorchMM*
    uses the convolutional neural network layer implementation introduced in Section
    21.2.5.
  id: totrans-6555
  prefs: []
  type: TYPE_NORMAL
  zh: 性能以每秒处理的示例数量表示（越高越好）。“batch”表示训练时每次喂入60个示例。*TorchMM*使用第21.2.5节中介绍的卷积神经网络层实现。
- en: 'for most complicated neural network models as easy for the simplest ones. The
    following code shows how this is done:'
  id: totrans-6556
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最复杂的神经网络模型，操作起来和最简单的模型一样简单。以下代码展示了这是如何实现的：
- en: '1 -- create an arbitrary model: 2 model = nn.Sequential() 3 model:add( nn.Linear(100,1000)
    ) 4 model:add( nn.Tanh() ) 5 model:add( nn.Linear(1000,10) )'
  id: totrans-6557
  prefs: []
  type: TYPE_NORMAL
  zh: 1 -- 创建一个任意模型：2 model = nn.Sequential() 3 model:add( nn.Linear(100,1000) ) 4
    model:add( nn.Tanh() ) 5 model:add( nn.Linear(1000,10) )
- en: '6 7 -- and a loss function: 8 loss = nn.MSECriterion() 9 10 -- extract the
    parameters, and the gradient holder 11 w,dloss_dw = model:getParameters()'
  id: totrans-6558
  prefs: []
  type: TYPE_NORMAL
  zh: 6 7 -- 和一个损失函数：8 loss = nn.MSECriterion() 9 10 -- 提取参数和梯度持有者 11 w,dloss_dw =
    model:getParameters()
- en: 12 13 -- w and dl_dw are two vectors of the same size Once the trainable parameter
    vector has been extracted, arbitrary, external optimization procedures can be
    used. *Torch7* provides a few standard methods
  id: totrans-6559
  prefs: []
  type: TYPE_NORMAL
  zh: 12 13 -- w和dl_dw是两个相同大小的向量。一旦提取了可训练参数向量，就可以使用任意外部优化程序。*Torch7*提供了一些标准方法。
- en: '(LBFGS, CG, SGD, ASGD) which simply require: (1) a function that computes Lw
    and dL'
  id: totrans-6560
  prefs: []
  type: TYPE_NORMAL
  zh: (LBFGS, CG, SGD, ASGD)只需要： (1)一个计算Lw和dL的函数
- en: dw and (2) the parameter vectors w and dL/dw. Of course, Lw can be either the
    true loss, or any approximation of it. The function that is defined is responsible
    for sampling from the training dataset, and estimating these approximations.
  id: totrans-6561
  prefs: []
  type: TYPE_NORMAL
  zh: dw和(2)参数向量w和dL/dw。当然，Lw可以是真实损失或其任何近似值。定义的函数负责从训练数据集中采样，并估计这些近似值。
- en: With these two concepts in mind, one can easily define a loop over a training
    dataset, and define a closure at each iteration, which computes Lw and dL
  id: totrans-6562
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这两个概念，可以轻松定义一个训练数据集的循环，并在每次迭代中定义一个闭包，以计算Lw和dL。
- en: dw .
  id: totrans-6563
  prefs: []
  type: TYPE_NORMAL
  zh: dw .
- en: 'The following listing shows an example of such a loop, assuming a pre-shuffled
    training dataset in which each entry is a tuple (xn, yn):'
  id: totrans-6564
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了这样一个循环的示例，假设一个预洗牌的训练数据集，其中每个条目是一个元组(x_n, y_n)：
- en: '1 -- assuming a training dataset ''trainset'', and the model 2 -- defined above:
    ''model'', ''w'' and ''dL_dw'':'
  id: totrans-6565
  prefs: []
  type: TYPE_NORMAL
  zh: 1 -- 假设训练数据集为'trainset'，模型为2 -- 如上定义的'model'、'w'和'dL_dw'：
- en: '3 for e = 1,nepochs do 4 for i,sample in ipairs(trainset) do 5 -- next training
    pair:'
  id: totrans-6566
  prefs: []
  type: TYPE_NORMAL
  zh: 3 for e = 1,nepochs do 4 for i,sample in ipairs(trainset) do 5 -- 下一个训练对：
- en: 6 x_n = sample[1]
  id: totrans-6567
  prefs: []
  type: TYPE_NORMAL
  zh: 6 x_n = sample[1]
- en: 7 y_n = sample[2]
  id: totrans-6568
  prefs: []
  type: TYPE_NORMAL
  zh: 7 y_n = sample[2]
- en: 8 9 -- create closure that estimates y_n_hat = f_w(x_n),
  id: totrans-6569
  prefs: []
  type: TYPE_NORMAL
  zh: 8 9 -- 创建闭包来估计y_n_hat = f_w(x_n)，
- en: '10 -- stochastically 11 feval = function() 12 -- estimate loss:'
  id: totrans-6570
  prefs: []
  type: TYPE_NORMAL
  zh: 10 -- 随机地 11 feval = function() 12 -- 估计损失：
- en: 13 y_n_hat = model:forward(x_n) 14 f = loss:forward(y_n_hat, y_n)
  id: totrans-6571
  prefs: []
  type: TYPE_NORMAL
  zh: 13 y_n_hat = model:forward(x_n) 14 f = loss:forward(y_n_hat, y_n)
- en: '15 16 -- estimate gradients:'
  id: totrans-6572
  prefs: []
  type: TYPE_NORMAL
  zh: 15 16 -- 估计梯度：
- en: 17 dloss_dw:zero()
  id: totrans-6573
  prefs: []
  type: TYPE_NORMAL
  zh: 17 dloss_dw:zero()
- en: 18 dloss_dy_n_hat = loss:backward(y_n_hat, y_n)
  id: totrans-6574
  prefs: []
  type: TYPE_NORMAL
  zh: 18 dloss_dy_n_hat = loss:backward(y_n_hat, y_n)
- en: 19 model:backward(x_n, dloss_dy_n_hat)
  id: totrans-6575
  prefs: []
  type: TYPE_NORMAL
  zh: 19 model:backward(x_n, dloss_dy_n_hat)
- en: 20 21 -- return loss, and gradients
  id: totrans-6576
  prefs: []
  type: TYPE_NORMAL
  zh: 20 21 -- 返回损失和梯度
- en: 22 return f,dloss_dw
  id: totrans-6577
  prefs: []
  type: TYPE_NORMAL
  zh: 22 返回 f,dloss_dw
- en: 23 end
  id: totrans-6578
  prefs: []
  type: TYPE_NORMAL
  zh: 23 结束
- en: '24 25 -- now that the closure is defined, pass it to an 26 -- optimization
    algorithm:'
  id: totrans-6579
  prefs: []
  type: TYPE_NORMAL
  zh: 24 25 -- 既然闭包已定义，将其传递给 26 -- 优化算法：
- en: 27 w,fs = optim.sgd(feval,w)
  id: totrans-6580
  prefs: []
  type: TYPE_NORMAL
  zh: 27 w,fs = optim.sgd(feval,w)
- en: 28 29 -- + the new w is returned, but as computations are 30 -- done in place,
    it is typically not necessary to
  id: totrans-6581
  prefs: []
  type: TYPE_NORMAL
  zh: 28 29 -- + 返回新的 w，但由于计算在 30 -- 原地进行，通常不需要
- en: 31 -- store it (the old w contains the new value)
  id: totrans-6582
  prefs: []
  type: TYPE_NORMAL
  zh: 31 -- 存储它（旧的 w 包含新的值）
- en: 32 -- + fs is a list of all the function (loss) 33 -- evaluations that were
    done during optimization. 34 -- SGD only returns one value, as it does not 35
    -- perform any line search. 36 end
  id: totrans-6583
  prefs: []
  type: TYPE_NORMAL
  zh: 32 -- + fs 是在优化过程中完成的所有函数（损失） 33 -- 评估的列表。 34 -- SGD 只返回一个值，因为它不 35 -- 执行任何线搜索。
    36 结束
- en: In the listing above, one can see that the loss and gradient estimation can
    be easily changed at runtime, and estimated over arbitrary batch sizes. To use
    a batch size different than 1 (as done above), one simply needs to create a list
    of training pairs, and the *feval* function needs to loop over these training
    pairs to estimate the approximate loss and gradients.
  id: totrans-6584
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述列表中，可以看到损失和梯度估计可以在运行时轻松更改，并且可以在任意批量大小上进行估计。要使用不同于 1 的批量大小（如上所示），只需创建一个训练对的列表，*feval*
    函数需要遍历这些训练对来估计近似损失和梯度。
- en: 21.4 Conclusion
  id: totrans-6585
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.4 结论
- en: 'Compared to the early days of neural network training, the challenges towards
    an efficient implementation did not change a lot, however the means changed slightly.
    Already in the late 80''s, the SN [3] toolbox was providing a scripting language
    (LISP) for building neural networks in a modular way. At the time, memory bandwidth
    and processor speed were about the same order of magnitude. Nowadays, we have
    to pay much more attention on memory accesses, counting number of instructions
    for optimizing the code is not sufficient anymore. Specific vectorized instructions
    can be easily integrated, but will not give order of magnitude speedups. In the
    end, what brings most advantage is parallelization. As computers become more and
    more parallel, it becomes crucial to leverage parallelization frameworks properly,
    such as OpenMP. On a more extreme side, GPUs (e.g. running with CUDA) are not
    as attractive as some could have expected: GPU-specific implementations require
    heavy extra work for a speedup (see Figure 21.5) which can be quite disappointing
    compared to what one can get with few extra lines of code with OpenMP.'
  id: totrans-6586
  prefs: []
  type: TYPE_NORMAL
  zh: 与神经网络训练的早期相比，朝着高效实现的挑战变化不大，但手段稍有不同。早在 80 年代末，SN [3] 工具箱就提供了一种脚本语言（LISP）用于以模块化方式构建神经网络。当时，内存带宽和处理器速度大致处于同一数量级。如今，我们必须更加关注内存访问，计算优化代码的指令数量已不足够。特定的向量化指令可以轻松集成，但不会带来数量级的速度提升。最终，带来最大优势的是并行化。随着计算机越来越多地并行化，正确利用并行化框架变得至关重要，例如
    OpenMP。在更极端的情况下，GPU（例如，运行 CUDA）并不像一些人所期待的那样吸引人：GPU 特定实现需要大量额外工作才能获得加速（见图 21.5），这相比于通过
    OpenMP 仅增加几行代码所能获得的效果可能令人失望。
- en: Acknowledgments. *Torch7* is the official successor to Torch313 and Torch5.14
    Over the years many distinguished machine learning researchers have contributed
    to *Torch*, including Léon Bottou (we thank him in particular for providing the
    Qt library interface), Jason Weston, Iain Melvin, Samy Bengio and Johnny Mariéthoz.
  id: totrans-6587
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢。*Torch7* 是 Torch313 和 Torch5.14 的官方继任者。多年来，许多杰出的机器学习研究人员为 *Torch* 做出了贡献，包括
    Léon Bottou（我们特别感谢他提供的 Qt 库接口）、Jason Weston、Iain Melvin、Samy Bengio 和 Johnny Mariéthoz。
- en: 13 http://www.torch.ch/torch3 14 http://torch5.sourceforge.net/
  id: totrans-6588
  prefs: []
  type: TYPE_NORMAL
  zh: 13 http://www.torch.ch/torch3 14 http://torch5.sourceforge.net/
- en: We would also like to thank Yann LeCun and Léon Bottou for sharing their work
    in SN, Lush, and extending their support and advices.
  id: totrans-6589
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还要感谢 Yann LeCun 和 Léon Bottou 共享他们在 SN、Lush 的工作，并扩展他们的支持和建议。
- en: Finally, we thank James Bergstra for making his benchmark code available.15
  id: totrans-6590
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们感谢 James Bergstra 提供基准代码。15
- en: '[1] Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins,
    G.,'
  id: totrans-6591
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins,
    G.,'
- en: 'Turian, J., Bengio, Y.: Theano: a CPU and GPU math expression compiler. In:
    Proceedings of the Python for Scientific Computing Conference, SciPy (2010)'
  id: totrans-6592
  prefs: []
  type: TYPE_NORMAL
  zh: 'Turian, J., Bengio, Y.: Theano：一个CPU和GPU数学表达式编译器。在：科学计算Python会议，SciPy（2010）'
- en: '[2] Bottou, L.: Large-scale machine learning with stochastic gradient descent.
    In:'
  id: totrans-6593
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Bottou, L.: 大规模机器学习与随机梯度下降。在：'
- en: Lechevallier, Y., Saporta, G. (eds.) Proceedings of the 19th International Conference
    on Computational Statistics (COMPSTAT 2010), Paris, France, pp. 177–187.
  id: totrans-6594
  prefs: []
  type: TYPE_NORMAL
  zh: Lechevallier, Y., Saporta, G.（编辑）第19届国际计算统计会议（COMPSTAT 2010）论文集，法国巴黎，第177–187页。
- en: Springer (August 2010)
  id: totrans-6595
  prefs: []
  type: TYPE_NORMAL
  zh: 施普林格（2010年8月）
- en: '[3] Bottou, L., LeCun, Y.: SN: A simulator for connectionist models. In: Proceedings
    of NeuroNimes 1988, Nimes, France (1988)'
  id: totrans-6596
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Bottou, L., LeCun, Y.: SN：连接主义模型的模拟器。在：1988年神经网络会议论文集，法国尼姆（1988）'
- en: '[4] Le, Q.V., Coates, A., Prochnow, B., Ng, A.Y.: On optimization methods for
    deep learning. Learning, 265–272 (2011)'
  id: totrans-6597
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Le, Q.V., Coates, A., Prochnow, B., Ng, A.Y.: 深度学习的优化方法。学习，第265–272页（2011）'
- en: '[5] LeCun, Y., Bottou, L.: Lush reference manual. Technical report (2002),
    code, http://lush.sourceforge.net'
  id: totrans-6598
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] LeCun, Y., Bottou, L.: Lush参考手册。技术报告（2002），代码，http://lush.sourceforge.net'
- en: '[6] LeCun, Y., Bottou, L., Orr, G.B., Müller, K.-R.: Efficient BackProp. In:
    Orr, G.B.,'
  id: totrans-6599
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] LeCun, Y., Bottou, L., Orr, G.B., Müller, K.-R.: 高效反向传播。在：Orr, G.B.,'
- en: Müller, K.-R. (eds.) NIPS-WS 1996. LNCS, vol. 1524, pp. 9–50. Springer, Heidelberg
    (1998)
  id: totrans-6600
  prefs: []
  type: TYPE_NORMAL
  zh: Müller, K.-R.（编辑）NIPS-WS 1996。LNCS，第1524卷，第9–50页。施普林格，海德堡（1998）
- en: '[7] Martens, J.: Deep learning via hessian-free optimization. In: Proceedings
    of the 27th International Conference on Machine Learning (ICML), vol. 951 (2010)'
  id: totrans-6601
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Martens, J.: 通过无海森优化进行深度学习。在：第27届国际机器学习会议（ICML）论文集，第951卷（2010）'
- en: '[8] Vinyals, O., Povey, D.: Krylov subspace descent for deep learning. Arxiv
    preprint arXiv:1111.4259 (2011)'
  id: totrans-6602
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Vinyals, O., Povey, D.: 用于深度学习的克里洛夫子空间下降。Arxiv预印本arXiv:1111.4259（2011）'
- en: 15 http://www.github.com/jaberg/DeepLearningBenchmarks
  id: totrans-6603
  prefs: []
  type: TYPE_NORMAL
  zh: 15 http://www.github.com/jaberg/DeepLearningBenchmarks
- en: 'Better Representations: Invariant, Disentangled And Reusable'
  id: totrans-6604
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更好的表征：不变、解耦和可重用
- en: Preface
  id: totrans-6605
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: In many cases, the amount of labeled data is limited and does not allow for
    fully identifying the function that needs to be learned. When labeled data is
    scarce, the learning algorithm is exposed to simultaneous underfitting and overfitting.
    The learning algorithm starts to "invent" nonexistent regularities (overfitting)
  id: totrans-6606
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，标记数据量有限，无法完全识别需要学习的函数。当标记数据稀缺时，学习算法同时面临欠拟合和过拟合的情况。学习算法开始“发明”不存在的规律（过拟合）。
- en: while at the same time not being able to model the true ones (underfitting).
    In the extreme case, this amounts to perfectly memorizing training data and not
    being able to generalize at all to new data.
  id: totrans-6607
  prefs: []
  type: TYPE_NORMAL
  zh: 同时无法建模真实情况（欠拟合）。在极端情况下，这意味着完全记住训练数据，而无法对新数据进行泛化。
- en: The following five chapters present various tricks to solve the underfitting/overfitting
    problem. Theseinclude approaches to forceinvarianceinto themodelin order to increase
    the signal-to-noise ratio, pretraining methods to disentangle the factors of variation,
    and how to use auxiliary tasks to learn a shared representation.
  id: totrans-6608
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的五章展示了各种解决欠拟合/过拟合问题的技巧。这些包括强制模型中的不变性以提高信噪比的方式、解耦变化因素的预训练方法，以及如何利用辅助任务学习共享表征。
- en: In image recognition, we know a priori that a good classifier should be translation
    invariant, rotation invariant, scale invariant, etc. Convolutional neural networks
    [7] are special networks composed of *convolution* and *pooling* layers that explicitly
    implement the translation invariance at multiple scales. They can be trained using
    standard backpropagation. One can also construct the convolutional network [9]
    one layer at the time in an unsupervised fashion. Chapter 22 [3] shows that, in
    this context, k-means is a particularly efficient method to learn the convolution
    filters.
  id: totrans-6609
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像识别中，我们先验知道一个好的分类器应该具有平移不变性、旋转不变性、尺度不变性等。卷积神经网络[7]是由*卷积*和*池化*层组成的特殊网络，它们在多个尺度上明确实现了平移不变性。可以使用标准反向传播进行训练。也可以以无监督的方式逐层构建卷积网络[9]。第22章[3]显示，在这种情况下，k-means是一种特别有效的学习卷积滤波器的方法。
- en: 'Alternatively, the desired invariance can be injected in the model with artificial
    training samples that are translated, rotated or distorted versions of the original
    samples. The idea is presented in Chapter 23 [2]. A well-engineered set of transformations
    can produce a very large number of artificial samples and considerably improve
    the generalization of the neural network. Things can become complicated as invariance
    is not always absolute: For example, the handwritten digit "1" is rotation-invariant
    only up to a certain angle, beyond which it can be confused with the handwritten
    digit "7".'
  id: totrans-6610
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，可以通过将期望的不变性注入模型中，使用经过翻译、旋转或扭曲的原始样本的人工训练样本。这个想法在第23章[2]中提出。一组设计良好的变换可以产生大量的人工样本，并显著提高神经网络的泛化能力。事情可能会变得复杂，因为不变性并非总是绝对的：例如，手写数字“1”只有在特定角度内是旋转不变的，超出此角度可能会与手写数字“7”混淆。
- en: Both convolutional networks and generating artificial samples yield excellent
    results on problems such as handwritten digit recognition and classification of
    small images. Combination of both techniques brings even higher performance. Unfortunately,
    these methods are only applicable when invariance is known. In the most general
    case, the invariance is unknown and other tricks are necessary in order to improve
    the learned representation.
  id: totrans-6611
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络和生成人工样本在手写数字识别和小图像分类等问题上都取得了优秀的结果。这两种技术的结合带来了更高的性能。不幸的是，这些方法仅在已知不变性时适用。在最一般的情况下，不变性是未知的，需要其他技巧来改善学习到的表示。
- en: The representation can be improved by learning an unsupervised model as a first
    step [5], using large amounts of unlabeled data. Unsupervised pretraining aims
    to construct a network of disentangled factors of variation. As a second step,
    the supervised learner only needs to choose in the set of disentangled factors,
    those that best predict the task of interest. Unsupervised pretraining transfers
    the burden of complex nonlinear modeling to unlabeled data which is generally
    available in much larger amounts. This two-steps approach was shown
  id: totrans-6612
  prefs: []
  type: TYPE_NORMAL
  zh: 表示可以通过将无监督模型作为第一步进行学习来改善[5]，使用大量未标记数据。无监督预训练旨在构建一个解耦变异因子的网络。作为第二步，监督学习者只需在解耦因子集合中选择那些最好预测感兴趣任务的因子。无监督预训练将复杂的非线性建模的负担转移到通常可用的更大量的未标记数据上。这种两步法已被证明有效。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    559–560, 2012.'
  id: totrans-6613
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon 等编著：《NN: Tricks of the Trade》，第2版，LNCS 7700，第559-560页，2012年。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-6614
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: to significantly reduce the underfitting/overfitting problem [5, 4] and improve
    handwritten digit and phoneme recognition performance.
  id: totrans-6615
  prefs: []
  type: TYPE_NORMAL
  zh: 显著减少欠拟合/过拟合问题[5, 4]并提高手写数字和音素识别性能。
- en: A well-established algorithm for learning the unsupervised representation is
    the restricted Boltzmann machine. Chapter 24 [6] explains step by step how to
    train it successfully and how to choose the multiple hyperparameters. Chapter
    25 [8] shows how keeping the model centered throughout training facilitates learning
    in the more sophisticated deep Boltzmann machine.
  id: totrans-6616
  prefs: []
  type: TYPE_NORMAL
  zh: 一种成熟的无监督表示学习算法是限制玻尔兹曼机。第24章[6]逐步解释了如何成功训练它以及如何选择多个超参数。第25章[8]展示了在整个训练过程中保持模型集中如何促进更复杂的深度玻尔兹曼机的学习。
- en: The series of five chapters terminates with the question of using auxiliary
    tasks to improve the solution learned by a neural network. As it was shown in
    Chapter 8 [1], this can be achieved by sharing an internal representation (and
    its associated parameters) across multiple related tasks and training the resulting
    multi-task network with backpropagation. A variant of multitask learning is introduced
    in Chapter 26 [10] where the auxiliary task is not defined by a set of labels
    but by a set of pairwise similarities between samples. This extension considerably
    widens the domain of applicability of multi-task learning.
  id: totrans-6617
  prefs: []
  type: TYPE_NORMAL
  zh: 五章系列以使用辅助任务来改善神经网络学习的解决方案的问题结束。如第8章[1]所示，这可以通过在多个相关任务之间共享内部表示（及其相关参数）并用反向传播训练结果多任务网络来实现。第26章[10]引入了一种多任务学习的变体，其中辅助任务不是由一组标签定义，而是由样本之间的一组成对相似性定义。这一扩展显著拓宽了多任务学习的适用范围。
- en: Grégoire & Klaus
  id: totrans-6618
  prefs: []
  type: TYPE_NORMAL
  zh: Grégoire & Klaus
- en: '[1] Caruana, R.: A Dozen Tricks with Multitask Learning. In: Orr, G.B., Müller,
    K.-R. (eds.) NIPS-WS 1996. LNCS, vol. 1524, pp. 163–189. Springer, Heidelberg
    (1998)'
  id: totrans-6619
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Caruana, R.：多任务学习的十二个技巧。载于：Orr, G.B., Müller, K.-R.（主编）NIPS-WS 1996。LNCS,
    第1524卷，163–189页。施普林格，海德堡（1998年）'
- en: '[2] Ciresan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Deep Big
    Multilayer Perceptrons for Digit Recognition. In: Montavon, G., Orr, G.B., Müller,
    K.-R. (eds.) NN: Tricks of the Trade, 2nd edn. LNCS, vol. 7700, pp. 581–598. Springer,
    Heidelberg (2012)'
  id: totrans-6620
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Ciresan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.：用于数字识别的深度大规模多层感知机。载于：Montavon,
    G., Orr, G.B., Müller, K.-R.（主编）NN：行业秘籍，第二版。LNCS, 第7700卷，581–598页。施普林格，海德堡（2012年）'
- en: '[3] Coates, A., Ng, A.Y.: Learning Feature Representations with k-means. In:
    Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the Trade, 2nd edn.
    LNCS,'
  id: totrans-6621
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Coates, A., Ng, A.Y.：使用K均值学习特征表示。载于：Montavon, G., Orr, G.B., Müller, K.-R.（主编）NN：行业秘籍，第二版。LNCS，'
- en: vol. 7700, pp. 561–580. Springer, Heidelberg (2012)
  id: totrans-6622
  prefs: []
  type: TYPE_NORMAL
  zh: 第7700卷，561–580页。施普林格，海德堡（2012年）
- en: '[4] Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P., Bengio,
    S.: Why does unsupervised pre-training help deep learning? J. Machine Learning
    Res. 11, 625–660 (2010)'
  id: totrans-6623
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P., Bengio,
    S.：为什么无监督预训练有助于深度学习？机器学习研究杂志 11, 625–660（2010年）'
- en: '[5] Hinton, G.E., Osindero, S., Teh, Y.-W.: A fast learning algorithm for deep
    belief nets. Neural Computation 18, 1527–1554 (2006)'
  id: totrans-6624
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Hinton, G.E., Osindero, S., Teh, Y.-W.：深度信念网络的快速学习算法。神经计算 18, 1527–1554（2006年）'
- en: '[6] Hinton, G.E.: A Practical Guide to Training Restricted Boltzmann Machines.
    In:'
  id: totrans-6625
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Hinton, G.E.：限制玻尔兹曼机训练的实用指南。载于：'
- en: 'Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the Trade, 2nd
    edn. LNCS, vol. 7700, pp. 437–478. Springer, Heidelberg (2012)'
  id: totrans-6626
  prefs: []
  type: TYPE_NORMAL
  zh: Montavon, G., Orr, G.B., Müller, K.-R.（主编）NN：行业秘籍，第二版。LNCS, 第7700卷，437–478页。施普林格，海德堡（2012年）
- en: '[7] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient based learning
    applied to document recognition. IEEE 86(11), 2278–2324 (1998)'
  id: totrans-6627
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.：应用于文档识别的基于梯度的学习。IEEE 86(11),
    2278–2324（1998年）'
- en: '[8] Montavon, G., Müller, K.-R.: Deep Boltzmann Machines and the Centering
    Trick.'
  id: totrans-6628
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Montavon, G., Müller, K.-R.：深度玻尔兹曼机及其中心化技巧。'
- en: 'In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the Trade,
    2nd edn. LNCS, vol. 7700, pp. 621–637. Springer, Heidelberg (2012)'
  id: totrans-6629
  prefs: []
  type: TYPE_NORMAL
  zh: 载于：Montavon, G., Orr, G.B., Müller, K.-R.（主编）NN：行业秘籍，第二版。LNCS, 第7700卷，621–637页。施普林格，海德堡（2012年）
- en: '[9] Serre, T., Wolf, L., Poggio, T.: Object recognition with features inspired
    by visual cortex. In: Computer Vision and Pattern Recognition Conference, pp.
    994–1000. IEEE Press (2005)'
  id: totrans-6630
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Serre, T., Wolf, L., Poggio, T.：受视觉皮层启发的特征进行对象识别。载于：计算机视觉与模式识别会议，994–1000页。IEEE出版社（2005年）'
- en: '[10] Weston, J., Ratle, F., Collobert, R.: Deep Learning via Semi-Supervised
    Embedding. In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the
    Trade, 2nd edn. LNCS, vol. 7700, pp. 639–655. Springer, Heidelberg (2012)'
  id: totrans-6631
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Weston, J., Ratle, F., Collobert, R.：通过半监督嵌入进行深度学习。载于：Montavon, G., Orr,
    G.B., Müller, K.-R.（主编）NN：行业秘籍，第二版。LNCS, 第7700卷，639–655页。施普林格，海德堡（2012年）'
- en: 22 Learning Feature Representations With K-Means
  id: totrans-6632
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 22 使用K均值学习特征表示
- en: Adam Coates and Andrew Y. Ng Stanford University, Stanford CA 94306, USA
  id: totrans-6633
  prefs: []
  type: TYPE_NORMAL
  zh: Adam Coates和Andrew Y. Ng 斯坦福大学，斯坦福CA 94306，美国
- en: 'Abstract. Many algorithms are available to learn deep hierarchies of features
    from unlabeled data, especially images. In many cases, these algorithms involve
    multi-layered networks of features (e.g., neural networks) that are sometimes
    tricky to train and tune and are difficult to scale up to many machines effectively.
    Recently, it has been found that K-means clustering can be used as a fast alternative
    training method. The main advantage of this approach is that it is very fast and
    easily implemented at large scale. On the other hand, employing this method in
    practice is not completely trivial: K-means has several limitations, and care
    must be taken to combine the right ingredients to get the system to work well.
    This chapter will summarize recent results and technical tricks that are needed
    to make effective use of K-means clustering for learning large-scale representations
    of images. We will also connect these results to other well-known algorithms to
    make clear when K-means can be most useful and convey intuitions about its behavior
    that are useful for debugging and engineering new systems.'
  id: totrans-6634
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要。许多算法可以从未标记的数据中学习深层特征层次，尤其是图像。在许多情况下，这些算法涉及多层特征网络（如神经网络），这些网络有时难以训练和调整，并且难以有效地扩展到多个机器上。最近发现，K均值聚类可以作为一种快速的替代训练方法。这种方法的主要优势在于它非常快速，并且易于在大规模上实施。另一方面，在实践中使用这种方法并非完全简单：K均值有若干限制，必须小心结合合适的要素以使系统良好运作。本章将总结最近的结果和技术技巧，这些技巧对于有效利用K均值聚类学习大规模图像表示是必要的。我们还将把这些结果与其他著名算法联系起来，以明确K均值何时最有用，并传达有关其行为的直觉，这对于调试和工程新系统是有用的。
- en: 22.1 Introduction
  id: totrans-6635
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.1 引言
- en: A major goal in machine learning is to learn deep hierarchies of features for
    other tasks. For instance, given a set of unlabeled images, many current algorithms
    seek to greedily learn successive layers of features that will make subsequent
    classification tasks (e.g., object recognition) easier to accomplish. A typical
    approach taken in the literature is to use an unsupervised learning algorithm
    to train a model of the unlabeled data and then use the results to extract interesting
    features from the data [35, 21, 31]. Depending on the choice of unsupervised learning
    scheme, it is sometimes difficult to make these systems work well. There can be
    many hyper-parameters and not much intuition for how to tune them. More recently,
    we have found that using K-means clustering as the unsupervised learning module
    in these types of "feature learning" pipelines can lead to excellent results,
    often rivaling state-of-the-art systems [11]. In this chapter, we will review
    some of this work with added notes on useful tricks and observations that are
    helpful for building large-scale feature learning systems.
  id: totrans-6636
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的一个主要目标是为其他任务学习深层特征层次。例如，给定一组未标记的图像，许多当前算法试图贪婪地学习连续的特征层，以便使后续分类任务（如对象识别）更容易完成。文献中采用的典型方法是使用无监督学习算法来训练未标记数据的模型，然后利用结果从数据中提取有趣的特征[35,
    21, 31]。根据所选择的无监督学习方案，有时很难使这些系统良好运作。可能有许多超参数，而对如何调整它们的直觉并不多。最近，我们发现将K均值聚类用作这些“特征学习”管道中的无监督学习模块可以带来优异的结果，常常与最先进的系统相媲美[11]。在本章中，我们将回顾这些工作，并附上有助于构建大规模特征学习系统的有用技巧和观察。
- en: K-means has already been identified as a successful method to learn features
    from images by computer vision researchers. The popular "bag of features" model
    [13, 28] from the computer vision community is very similar to the pipeline that
    we will use in this chapter, and many conclusions here are similar to those
  id: totrans-6637
  prefs: []
  type: TYPE_NORMAL
  zh: K均值已经被计算机视觉研究人员确定为从图像中学习特征的成功方法。计算机视觉领域流行的“特征袋”模型[13, 28]与我们在本章中使用的管道非常相似，这里得出的许多结论也是相似的。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    561–580, 2012.'
  id: totrans-6638
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon 等（编）：NN：行业技巧，第2版，LNCS 7700，第561–580页，2012年。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-6639
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: 'identified by vision researchers [18, 1]. In this chapter, however, we will
    focus on the ingredients needed to make K-means work well in a setting similar
    to that used by other deep learning and feature learning systems: learning directly
    from raw inputs (pixel intensities) and building multi-layered hierarchies, as
    well as connecting K-means to other well-known feature learning systems.'
  id: totrans-6640
  prefs: []
  type: TYPE_NORMAL
  zh: 由视觉研究者识别 [18, 1]。然而，在本章中，我们将重点关注在类似于其他深度学习和特征学习系统的设置中使 K-means 有效所需的要素：直接从原始输入（像素强度）学习并构建多层次结构，同时将
    K-means 与其他知名的特征学习系统连接起来。
- en: 'The classic K-means clustering algorithm finds cluster centroids that minimize
    the distance between data points and the nearest centroid. Also called "vector
    quantization", K-means can be viewed as a way of constructing a "dictionary" D∈Rn×k
    of k vectors so that a data vector x(i) ∈ Rn, i = 1*,...,m* can be mapped to a
    code vector that minimizes the error in reconstruction. In this chapter, we will
    use a modified version of K-means (sometimes called "gain shape" vector quantization
    [41], or "spherical K-means" [14]) that finds D according to:'
  id: totrans-6641
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的 K-means 聚类算法找到的聚类中心使数据点与最近中心之间的距离最小化。K-means 也被称为“向量量化”，可以看作是构建一个包含 k 个向量的字典
    D∈Rn×k 的一种方式，使得数据向量 x(i) ∈ Rn, i = 1*,...,m* 可以映射到一个最小化重构误差的代码向量。在这一章中，我们将使用 K-means
    的修改版（有时称为“增益形状”向量量化 [41]，或“球形 K-means” [14]），根据以下方式找到 D：
- en: $$\begin{array}{c}{{\operatorname*{minimize}_{\mathcal{D},s}\sum_{i}||\mathcal{D}s^{(i)}-x^{(i)}||_{2}^{2}}}\\
    {{\mathrm{subject~to~}||s^{(i)}||_{0}\leq1,\forall i}}\\ {{\mathrm{~and~}||\mathcal{D}^{(j)}||_{2}=1,\forall
    j}}\end{array}$$
  id: totrans-6642
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{c}{{\operatorname*{最小化}_{\mathcal{D},s}\sum_{i}||\mathcal{D}s^{(i)}-x^{(i)}||_{2}^{2}}}\\
    {{\mathrm{满足}~||s^{(i)}||_{0}\leq1,\forall i}}\\ {{\mathrm{~和~}||\mathcal{D}^{(j)}||_{2}=1,\forall
    j}}\end{array}$$
- en: where s(i) is a "code vector" associated with the input x(i), and D(j) is the
    j'th column of the dictionary D. The goal here is to find a dictionary D and a
    new representation, s(i), of each example x(i) that satisfies several criteria.
    First, given s(i) and D, we should be able to reconstruct the original x(i) well;
    in particular, we aim to minimize the squared difference between x(i) and its
    corresponding reconstruction Ds(i). This goal is optimized under two constraints.
    The first constraint, ||s(i)||0 ≤ 1, means that each s(i) is constrained to have
    at most one non-zero entry. Thus we are searching not only for a new representation
    of x(i)
  id: totrans-6643
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 s(i) 是与输入 x(i) 关联的“代码向量”，D(j) 是字典 D 的第 j 列。这里的目标是找到一个字典 D 和每个示例 x(i) 的新表示
    s(i)，以满足几个标准。首先，给定 s(i) 和 D，我们应该能够很好地重构原始 x(i)；特别是，我们的目标是最小化 x(i) 与其对应重构 Ds(i)
    之间的平方差。这个目标在两个约束下进行优化。第一个约束 ||s(i)||0 ≤ 1，意味着每个 s(i) 被限制为最多只有一个非零条目。因此，我们不仅在寻找
    x(i) 的新表示。
- en: that preserves it as well as possible, but also for a very simple or parsimonious
    representation. The second constraint requires that each dictionary column have
    unit length, preventing them from becoming arbitrarily large or small. Otherwise
    we could arbitrarily rescale D(j) and the corresponding s
  id: totrans-6644
  prefs: []
  type: TYPE_NORMAL
  zh: 这保留了尽可能多的信息，但也要求有一个非常简单或节约的表示。第二个约束要求每个字典列具有单位长度，防止它们变得过大或过小。否则我们就可以任意缩放 D(j)
    和相应的 s。
- en: (i)
  id: totrans-6645
  prefs: []
  type: TYPE_NORMAL
  zh: (i)
- en: j without effect.
  id: totrans-6646
  prefs: []
  type: TYPE_NORMAL
  zh: j 不产生影响。
- en: 'This algorithm is very similar in spirit to other algorithms for learning efficient
    coding schemes, such as sparse coding [34, 17]:'
  id: totrans-6647
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法在精神上与其他学习高效编码方案的算法非常相似，如稀疏编码 [34, 17]：
- en: $ \underset{\mathcal{D},s}{\text{minimize}}\sum_{i}||\mathcal{D}s^{(i)}-x^{(i)}||_2^2+\lambda||s^{(i)}||_1$  subject
    to $||D^{(j)}||_2=1,\forall j$.
  id: totrans-6648
  prefs: []
  type: TYPE_NORMAL
  zh: $ \underset{\mathcal{D},s}{\text{最小化}}\sum_{i}||\mathcal{D}s^{(i)}-x^{(i)}||_2^2+\lambda||s^{(i)}||_1$，并且满足
    $||D^{(j)}||_2=1,\forall j$。
- en: Sparse coding optimizes the same type of reconstruction objective, but constrains
    the complexity of s(i) by adding a penalty λ||s(i)||1 that encourages s(i) to
    be sparse. This is similar to the constraint used by K-means (||s(i)||0 ≤ 1),
    but allows more than one non-zero entry in each s(i), enabling a much more accurate
    representation of each x(i) while still requiring each s(i) to be simple.
  id: totrans-6649
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏编码优化了相同类型的重构目标，但通过添加惩罚项 λ||s(i)||1 来约束 s(i) 的复杂性，促使 s(i) 变得稀疏。这与 K-means 使用的约束类似
    (||s(i)||0 ≤ 1)，但允许每个 s(i) 有多个非零条目，从而能够更准确地表示每个 x(i)，同时仍要求每个 s(i) 简单。
- en: 'From their descriptions above, it is no surprise that K-means and more sophisticated
    dictionary-learning schemes like sparse coding are often interchangeable—differing
    in their optimization objectives, but producing code vectors s(i) and dictionaries
    D that accomplish similar goals. Empirically though, sparse coding appears to
    be a better performer in many applications. For instance, replacing K-means with
    sparse coding in the classic bag-of-features model has been shown to significantly
    improve image recognition results [39]. Despite its simplicity, however, K-means
    is still a very useful algorithm for learning features due to its speed and scalability.
    Sparse coding requires us to solve a convex optimization problem [36, 15, 32]
    for every s(i) repeatedly during the learning procedure and thus is very expensive
    to deploy at large scale. For K-means, by contrast, the optimal s(i) used in the
    algorithm above is simply:'
  id: totrans-6650
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述描述来看，K-means和更复杂的字典学习方案如稀疏编码常常是可以互换的——它们在优化目标上有所不同，但产生的代码向量s(i)和字典D实现了类似的目标。然而，经验表明，稀疏编码在许多应用中似乎表现更佳。例如，将K-means替换为稀疏编码的经典特征袋模型已被证明显著改善图像识别结果[39]。尽管其简单性，K-means仍然是一个非常有用的特征学习算法，因为其速度和可扩展性。相比之下，稀疏编码要求我们在学习过程中对每个s(i)重复求解一个凸优化问题[36,
    15, 32]，因此在大规模部署时非常昂贵。对于K-means，算法中使用的最优s(i)仅为：
- en: $$s_{j}^{(i)}=\begin{cases}\mathcal{D}^{(j)\top}x^{(i)}&{\mathrm{~if~}}j==\operatorname*{arg\,max}_{l}|\mathcal{D}^{(l)\top}x^{(i)}|\\
    0&{\mathrm{~otherwise.}}\end{cases}$$
  id: totrans-6651
  prefs: []
  type: TYPE_NORMAL
  zh: $$s_{j}^{(i)}=\begin{cases}\mathcal{D}^{(j)\top}x^{(i)}&{\mathrm{~if~}}j==\operatorname*{arg\,max}_{l}|\mathcal{D}^{(l)\top}x^{(i)}|\\
    0&{\mathrm{~otherwise.}}\end{cases}$$
- en: $$(22.1)$$
  id: totrans-6652
  prefs: []
  type: TYPE_NORMAL
  zh: $$(22.1)$$
- en: Because this can be done very quickly (and solving for D given s is also easy),
    we can train very large dictionaries rapidly by alternating optimization of D
    and s.
  id: totrans-6653
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这可以非常快速地完成（给定s求解D也很简单），我们可以通过交替优化D和s快速训练非常大的字典。
- en: As well, K-means does not have any parameters requiring tuning other than k,
    the number of centroids, making it relatively easy to get working. The surprise
    is that large dictionaries learned by K-means often work very well in practice
    provided we mix in a few other ingredients that are less commonly documented in
    other works. This chapter is about what these ingredients are as well as some
    intuition about why they are needed and how they affect results. For most of this
    work, we will use images (or image patches) as input data to the algorithm, but
    the basic principles are applicable to other types of data as well.
  id: totrans-6654
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，K-means除了k（质心的数量）外，没有其他需要调整的参数，使得其相对容易上手。令人惊讶的是，由K-means学习的大型字典在实际应用中通常表现非常好，只要我们混合一些在其他文献中不常见的成分。本章将讨论这些成分是什么，以及它们为何需要、如何影响结果的直觉。对于这项工作的绝大多数部分，我们将使用图像（或图像块）作为算法的输入数据，但基本原理同样适用于其他类型的数据。
- en: 22.2 Data, Pre-Processing And Initialization
  id: totrans-6655
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.2 数据、预处理和初始化
- en: We will begin with a dataset composed of small image patches. For concreteness,
    we will work with 16-by-16 pixel grayscale patches represented as a vector of
    256 pixel intensities (i.e., x(i) ∈ R256), but color patches can also be used
    similarly.
  id: totrans-6656
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个由小图像块组成的数据集开始。为确保具体性，我们将使用16×16像素的灰度图块，表示为一个256像素强度的向量（即，x(i) ∈ R256），但也可以类似地使用彩色图块。
- en: These patches can be collected from unlabeled imagery by cropping out random
    16-by-16 chunks. In order to build a "complete" dictionary (i.e., a dictionary
    with at least 256 centroids), we should ensure that there will be enough patches
    so that each cluster can claim a reasonable number of inputs. For 16-by-16 gray
    patches, m = 100, 000 patches is enough. In practice, we will often need *more*
    data to train a K-means dictionary than is necessary for other algorithms (e.g.,
    sparse coding), since each data point contributes to just 1 centroid in the end.
    Usually the added expense is easily offset by the speed of training. For notational
    convenience, we will assume that our data points are packed into the columns of
    a matrix X ∈ Rn×m. (Similarly, we will denote by S the matrix whose columns are
    the code vectors s(i) from Eq. (22.1).)
  id: totrans-6657
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像块可以通过随机裁剪 16x16 的块从未标记的图像中收集。为了构建一个“完整”的字典（即至少包含 256 个质心的字典），我们应该确保有足够的图像块，以便每个聚类能够获得合理数量的输入。对于
    16x16 的灰度图像块，m = 100,000 个图像块就足够了。实际上，我们通常需要 *更多* 的数据来训练 K-means 字典，而不是其他算法（例如稀疏编码），因为每个数据点最终只贡献给
    1 个质心。通常，增加的成本很容易通过训练速度来抵消。为了方便记号，我们将假设我们的数据点被打包到矩阵 X ∈ Rn×m 的列中。（类似地，我们将用 S 表示矩阵，其列是来自公式
    (22.1) 的代码向量 s(i)。）
- en: 22.2.1 Pre-Processing
  id: totrans-6658
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.2.1 预处理
- en: Before running a learning algorithm on our input data points x(i), it is useful
    to normalize the brightness and contrast of the patches. That is, for each x(i)
    we subtract out the mean of the intensities and divide by the standard deviation.
    A
  id: totrans-6659
  prefs: []
  type: TYPE_NORMAL
  zh: 在对输入数据点 x(i) 运行学习算法之前，规范化图像块的亮度和对比度是有益的。也就是说，对于每个 x(i)，我们减去强度的均值并除以标准差。
- en: 'small value is added to the variance before division to avoid divide by zero
    and also suppress noise. For pixel intensities in the range [0, 255], adding 10
    to the variance is often a good starting point:'
  id: totrans-6660
  prefs: []
  type: TYPE_NORMAL
  zh: 在除法之前向方差中添加小值，以避免除以零并抑制噪声。对于像素强度范围 [0, 255]，向方差中添加 10 通常是一个不错的起点：
- en: $$x^{(i)}=\frac{\tilde{x}^{(i)}-\operatorname{mean}(\tilde{x}^{(i)})}{\sqrt{\operatorname{var}(\tilde{x}^{(i)})+10}}$$
  id: totrans-6661
  prefs: []
  type: TYPE_NORMAL
  zh: $$x^{(i)}=\frac{\tilde{x}^{(i)}-\operatorname{mean}(\tilde{x}^{(i)})}{\sqrt{\operatorname{var}(\tilde{x}^{(i)})+10}}$$
- en: where x˜(i) are unnormalized patches and "mean" and "var" are the mean and variance
    of the elements of x˜(i).
  id: totrans-6662
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 x˜(i) 是未规范化的图像块，“mean”和“var”分别是 x˜(i) 元素的均值和方差。
- en: '![554_image_0.png](554_image_0.png)'
  id: totrans-6663
  prefs: []
  type: TYPE_IMG
  zh: '![554_image_0.png](554_image_0.png)'
- en: 'Fig. 22.1. (a) Centroids learned by K-means from natural images without whitening.
    (b) A cartoon depicting the effect of whitening on the K-means solution. Left:'
  id: totrans-6664
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22.1.（a）从自然图像中通过 K-means 学习到的质心，没有去白化。（b）一个卡通图描绘了去白化对 K-means 解决方案的影响。左侧：
- en: 'unwhitened data, where the centroids tend to be biased by the correlated data.
    Right: whitened data, where centroids are more orthogonal. (c) Centroids learned
    from whitened image patches.'
  id: totrans-6665
  prefs: []
  type: TYPE_NORMAL
  zh: 未去白化的数据，其中质心往往受到相关数据的偏见影响。右：去白化的数据，质心更加正交。（c）从去白化图像块中学习到的质心。
- en: After normalization, we can try to run K-means on the new input patches. The
    centroids that are obtained (i.e., the columns of the dictionary D) are visualized
    as patches in Figure 22.1a. It can be seen that K-means tends to learn lowfrequency
    edge-like centroids. This result has been reproduced many times in the past [16,
    37, 2]. Unfortunately, it turns out that these centroids tend to work poorly in
    recognition tasks [11]. One explanation for this result is that the correlations
    between nearby pixels (i.e., low-frequency variations in the images) tend to be
    very strong even after brightness and contrast normalization. In the presence
    of these correlations, K-means tends to generate many highly correlated centroids
    rather than spreading the centroids out to span the data more evenly. A cartoon
    depicting this problem is shown on the left of Figure 22.1b. To remedy this situation,
    one should use whitening (also called "sphering") to rescale the input data to
    remove these correlations [22]. This tends to cause K-means to allocate more centroids
    in the orthogonal directions, as shown on the right of Figure 22.1b.
  id: totrans-6666
  prefs: []
  type: TYPE_NORMAL
  zh: 在归一化后，我们可以尝试在新的输入补丁上运行K均值。得到的质心（即字典D的列）在图22.1a中以补丁的形式可视化。可以看到，K均值往往学习到低频边缘状似的质心。这个结果在过去已被多次重复[16,
    37, 2]。不幸的是，这些质心在识别任务中表现不佳[11]。对此结果的一个解释是，附近像素之间的相关性（即图像中的低频变化）即使在亮度和对比度归一化后也往往非常强。由于存在这些相关性，K均值往往生成许多高度相关的质心，而不是将质心分散以更均匀地覆盖数据。图22.1b左侧展示了这个问题的卡通图。为了解决这个问题，应使用白化（也称为“球面化”）来重新缩放输入数据，以消除这些相关性[22]。这通常会导致K均值在正交方向上分配更多的质心，如图22.1b右侧所示。
- en: A simple choice of whitening transform is the ZCA whitening transform. If V
    DV  = cov(x) is the eigenvalue decomposition of the covariance of the data points
    x, then the whitened points are computed as V (D+zcaI)−1/2V x, where zca is a
    small constant. For contrast-normalized data, setting zca to 0.01 for 16-by-16
    pixel patches, or 0.1 for 8-by-8 pixel patches is a good starting point. Note
    that setting this number too small can cause high-frequency noise to be amplified
    and make learning more difficult. Since rotating the data does not alter the behavior
    of K-means, one can also use other whitening transforms such as PCA whitening
    (which differ from ZCA only by a rotation).
  id: totrans-6667
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的白化变换选择是ZCA白化变换。如果V DV = cov(x)是数据点x的协方差的特征值分解，那么白化后的点计算为V (D+zcaI)−1/2V
    x，其中zca是一个小常数。对于对比度归一化的数据，16×16像素补丁将zca设置为0.01，或8×8像素补丁设置为0.1是一个不错的起点。注意，设置这个数值过小可能会导致高频噪声被放大，从而使学习变得更加困难。由于旋转数据不会改变K均值的行为，因此也可以使用其他白化变换，例如PCA白化（与ZCA仅在旋转上有所不同）。
- en: Running K-means on whitened image patches yields sharper edge features similar
    to those discovered by sparse coding, ICA, and others as seen in Figure 22.1c.
    This procedure of normalization, whitening, and K-means clustering is an effective
    "off the shelf" unsupervised learning module that can serve in many feature-learning
    roles. From this point forward, we will assume that whenever we apply K-means
    to new data that they are normalized and whitened as described here. But keep
    in mind that proper choices of the  parameters for normalization and whitening
    can sometimes require adjustment for new data sources. Though these are likely
    best set by cross validation, they can often be tuned visually (e.g., to yield
    image patches with high contrast, not too much noise, and not too much low-frequency
    undulation).
  id: totrans-6668
  prefs: []
  type: TYPE_NORMAL
  zh: 在白化后的图像补丁上运行K均值会产生更锐利的边缘特征，类似于稀疏编码、ICA等方法发现的特征，如图22.1c所示。这个归一化、白化和K均值聚类的过程是一个有效的“现成”无监督学习模块，可以在许多特征学习角色中发挥作用。从此以后，我们将假设每当我们将K均值应用于新数据时，它们都是按此处所述进行归一化和白化的。但请记住，归一化和白化参数的适当选择有时需要针对新数据源进行调整。尽管这些参数最好通过交叉验证来设置，但通常可以通过视觉调优（例如，得到高对比度、不太多噪声、以及不太多低频波动的图像补丁）来进行调整。
- en: 22.2.2 Initialization
  id: totrans-6669
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.2.2 初始化
- en: The usual K-means clustering algorithm is known to require a number of small
    tweaks to avoid common problems like empty clusters. One important consideration
    is the initialization of the centroids. Though it is common to initialize K-means
    to randomly-chosen examples drawn from the data, this has been found to be a poor
    choice. It is possible that images tend to group too densely in some areas, and
    thus initializing K-means with randomly chosen patches leads to a large number
    of centroids starting close together. Many of these centroids ultimately end up
    becoming near-empty clusters, as a single cluster accumulates all of the data
    points located within a dense area. Instead, it is better to randomly initialize
    the centroids from a Normal distribution and then normalize them to unit length.
    Note that because of the whitening stage, we expect that the important components
    of our data have already been rescaled to a more or less spherical distribution,
    so initializing to random vectors on a sphere is not a terrible starting point.
  id: totrans-6670
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的 K-means 聚类算法需要进行一些小调整，以避免空簇等常见问题。一个重要的考虑是中心点的初始化。虽然常见的做法是将 K-means 初始化为从数据中随机选择的示例，但这被发现是一个不好的选择。图像可能在某些区域过于密集，因此用随机选择的图像块初始化
    K-means 会导致大量中心点聚集在一起。许多这些中心点最终会变成几乎空的簇，因为单个簇会积累位于密集区域内的所有数据点。相反，从正态分布中随机初始化中心点并将其归一化到单位长度会更好。请注意，由于白化阶段，我们预计数据的重要成分已经重新缩放到近乎球形的分布，因此从球面上的随机向量初始化并不是一个糟糕的起点。
- en: Other well-known heuristics for improving the behavior of K-means can be useful.
    For instance, heuristics for reinitializing empty clusters are commonly used in
    other implementations. In practice, the initialization scheme above works relatively
    well for image data. When empty clusters do occur, reinitializing the centroids
    with random examples is usually sufficient, but this is rarely necessary.1 In
    fact, for a sufficiently scalable implementation, we can often train many centroids
    and simply throw away clusters that have too few data points.
  id: totrans-6671
  prefs: []
  type: TYPE_NORMAL
  zh: 改善 K-means 行为的其他知名启发式方法也很有用。例如，重新初始化空簇的启发式在其他实现中被广泛使用。实际上，上述初始化方案对于图像数据的效果相对较好。当空簇确实出现时，通常用随机示例重新初始化中心点就足够了，但这很少是必要的。实际上，对于一个足够可扩展的实现，我们通常可以训练许多中心点，并简单地丢弃数据点过少的簇。
- en: 1 Often, a large number of empty clusters indicates that the whitening or normalization
    parameters are improperly tuned, or the data is too high-dimensional for K-means
    to be successful.
  id: totrans-6672
  prefs: []
  type: TYPE_NORMAL
  zh: 1 通常，大量空簇表明白化或归一化参数调节不当，或者数据的维度过高，导致 K-means 无法成功。
- en: 'Another minor tweak that improves behavior is to use damped updates of the
    centroids. Specifically, at each iteration we compute new centroids according
    to:'
  id: totrans-6673
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个改善行为的小调整是使用阻尼更新中心点。具体来说，在每次迭代中，我们根据以下公式计算新中心点：
- en: $$\begin{array}{r l}{{\mathcal{D}}_{\mathrm{new}}:=\arg\min\vert\vert{\mathcal{D}}S-X\vert\vert_{2}^{2}+\vert\vert{\mathcal{D}}-{\mathcal{D}}_{\mathrm{old}}\vert\vert_{2}^{2}}}\\
    {{}}&{{}=(S S^{\top}+I)^{-1}(X S^{\top}+{\mathcal{D}}_{\mathrm{old}})}\\ {{}}&{{}\propto
    X S^{\top}+{\mathcal{D}}_{\mathrm{old}}}\\ {{\mathcal{D}}_{\mathrm{new}}:=\mathrm{normalize}({\mathcal{D}}_{\mathrm{new}}).}\end{array}$$
  id: totrans-6674
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{r l}{{\mathcal{D}}_{\mathrm{new}}:=\arg\min\vert\vert{\mathcal{D}}S-X\vert\vert_{2}^{2}+\vert\vert{\mathcal{D}}-{\mathcal{D}}_{\mathrm{old}}\vert\vert_{2}^{2}}}\\
    {{}}&{{}=(S S^{\top}+I)^{-1}(X S^{\top}+{\mathcal{D}}_{\mathrm{old}})}\\ {{}}&{{}\propto
    X S^{\top}+{\mathcal{D}}_{\mathrm{old}}}\\ {{\mathcal{D}}_{\mathrm{new}}:=\mathrm{normalize}({\mathcal{D}}_{\mathrm{new}}).}\end{array}$$
- en: Note that this form of damping does not affect "big" clusters very much (the
    j'th column of XS will be large compared to D(j)
  id: totrans-6675
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种阻尼形式对“大的”簇几乎没有影响（XS 的 j 列与 D(j) 相比会很大）。
- en: old) and only serves to prevent small clusters from being pulled too far in
    a single iteration.
  id: totrans-6676
  prefs: []
  type: TYPE_NORMAL
  zh: 旧的) 仅用于防止小簇在一次迭代中被拉得太远。
- en: 'Including the initialization and pre-processing, the full K-means training
    routine presented above is summarized here:'
  id: totrans-6677
  prefs: []
  type: TYPE_NORMAL
  zh: 包括初始化和预处理，上述完整的 K-means 训练流程在这里总结：
- en: '1. Normalize inputs:'
  id: totrans-6678
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 归一化输入：
- en: '![556_image_0.png](556_image_0.png)'
  id: totrans-6679
  prefs: []
  type: TYPE_IMG
  zh: '![556_image_0.png](556_image_0.png)'
- en: $$x^{(i)}:={\frac{x^{(i)}-\operatorname{mean}(x^{(i)})}{\sqrt{\operatorname{var}(x^{(i)})+\epsilon_{\operatorname{norm}}}}},\forall
    i$$
  id: totrans-6680
  prefs: []
  type: TYPE_NORMAL
  zh: $$x^{(i)}:={\frac{x^{(i)}-\operatorname{mean}(x^{(i)})}{\sqrt{\operatorname{var}(x^{(i)})+\epsilon_{\operatorname{norm}}}}},\forall
    i$$
- en: '2. Whiten inputs:'
  id: totrans-6681
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 对输入进行白化：
- en: $$\begin{array}{r l}{{}}&{{}[V,D]:=\mathrm{{eig}}(\mathrm{cov}(x));\ //\ \mathrm{{So}}\
    V D V^{\top}=\mathrm{{cov}}(x)}\\ {}&{{}}\\ {x^{(i)}:=V(D+\epsilon_{\mathrm{{zca}}}I)^{-1/2}V^{\top}x^{(i)},\forall
    i}\end{array}$$
  id: totrans-6682
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{r l}{{}}&{{}[V,D]:=\mathrm{{eig}}(\mathrm{cov}(x));\ //\ \mathrm{{所以}}\
    V D V^{\top}=\mathrm{{cov}}(x)}\\ {}&{{}}\\ {x^{(i)}:=V(D+\epsilon_{\mathrm{{zca}}}I)^{-1/2}V^{\top}x^{(i)},\forall
    i}\end{array}$$
- en: $\text{3.Loop until convergence(typically10iterations is enough):}$.
  id: totrans-6683
  prefs: []
  type: TYPE_NORMAL
  zh: $\text{3.循环直到收敛（通常10次迭代足够）：}$。
- en: $s_{j}^{(i)}:=\begin{cases}\mathcal{D}^{(j)\top}x^{(i)}&\text{if}j==\operatorname*{arg\,max}_{l}|\mathcal{D}^{(l)\top}x^{(i)}|\\
    0&\text{otherwise.}\end{cases}\forall j,i,j$  $\mathcal{D}:=XS^{\top}+\mathcal{D}$  $\mathcal{D}^{(j)}:=\mathcal{D}^{(j)}/||D^{(j)}||_{2}\forall
    j$
  id: totrans-6684
  prefs: []
  type: TYPE_NORMAL
  zh: $s_{j}^{(i)}:=\begin{cases}\mathcal{D}^{(j)\top}x^{(i)}&\text{如果}j==\operatorname*{arg\,max}_{l}|\mathcal{D}^{(l)\top}x^{(i)}|\\
    0&\text{否则.}\end{cases}\forall j,i,j$  $\mathcal{D}:=XS^{\top}+\mathcal{D}$  $\mathcal{D}^{(j)}:=\mathcal{D}^{(j)}/||D^{(j)}||_{2}\forall
    j$
- en: 22.3 Comparison To Sparse Feature Learning
  id: totrans-6685
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.3 与稀疏特征学习的比较
- en: As was shown above, K-means learns oriented edge-like features when applied
    to natural images, much like ICA [23] or sparse coding [34]. An important practical
    question is whether this is accidental (e.g., because edges are so common that
    learning "exemplars" from images is enough to find them) or whether this implies
    that K-means can perform a type of sparse decomposition similar to ICA. When we
    attempt to apply K-means to other types of data such as audio or features
  id: totrans-6686
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，当K-means应用于自然图像时，学习到的特征类似于ICA [23]或稀疏编码 [34]所学习到的边缘状特征。一个重要的实际问题是，这是否是偶然的（例如，由于边缘如此普遍，以至于从图像中学习“样本”就足以找到它们）或者这是否暗示K-means能够执行类似于ICA的稀疏分解。当我们尝试将K-means应用于其他类型的数据，例如音频或特征时，
- en: '![557_image_0.png](557_image_0.png)'
  id: totrans-6687
  prefs: []
  type: TYPE_IMG
  zh: '![557_image_0.png](557_image_0.png)'
- en: Fig. 22.2. The result of running spherical K-means on points sampled from a
    heavytailed distribution. The K-means "centroids" tend to point in the directions
    of the tails.
  id: totrans-6688
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.2. 在从重尾分布中抽样的点上运行球形K-means的结果。K-means的“质心”倾向于指向尾部的方向。
- en: computed by lower layers in a deep network, it is important to understand to
    what extent this clustering algorithm mimics well-known projection methods like
    ICA and what the limitations are. It is clear that because each s(i) is allowed
    to contain only a single non-zero entry, K-means tries to learn centroids that
    singlehandedly explain an entire input image. It is thus not guaranteed that the
    learned centroids will always be like the filters produced by sparse coding or
    ICA. These algorithms learn genuine "distributed" representations where a single
    image can be explained jointly by multiple columns of the dictionary instead of
    just one.
  id: totrans-6689
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度网络的低层计算的情况下，理解该聚类算法在多大程度上模仿了像ICA这样知名的投影方法以及其局限性是很重要的。显然，由于每个s(i)仅允许包含一个非零条目，K-means试图学习能够单独解释整个输入图像的质心。因此，学习到的质心不一定总会像稀疏编码或ICA生成的滤波器那样。这些算法学习到真正的“分布式”表示，其中单个图像可以由字典的多个列共同解释，而不仅仅是由一个。
- en: Nevertheless, empirically it turns out that K-means *does* tend to discover
    sparse projections of the data under the right conditions. Because of this property,
    we can often use the learned dictionary in a manner similar to the dictionaries
    or filters learned by other algorithms that explicitly search for sparse, distributed
    representations.
  id: totrans-6690
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，经验表明，在适当条件下，K-means*确实*倾向于发现数据的稀疏投影。由于这一特性，我们常常可以以类似于其他算法学习到的字典或滤波器的方式使用学习到的字典，这些算法明确寻求稀疏的分布式表示。
- en: One intuition for why this tends to occur can be seen in a simple lowdimensional
    example. Consider the case where our data is sampled from two independent, heavy-tailed
    (sparse) random variables. After normalization, the data will be most dense near
    the coordinate axes, and less dense in the quadrants between them. As a result,
    while K-means will tend to represent many points very badly, training 2 centroids
    on this distribution will tend to yield a dictionary of basis vectors pointing
    in the direction of the tails (i.e., in the sparse directions). This result is
    illustrated in Figure 22.2.
  id: totrans-6691
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这种情况倾向于发生的一个直觉可以在一个简单的低维示例中看到。考虑我们的数据是从两个独立的重尾（稀疏）随机变量中抽样的情况。经过归一化后，数据在坐标轴附近最为密集，而在其间的象限则密度较小。因此，虽然K-means在表示许多点时往往表现不佳，但在该分布上训练2个质心将倾向于产生指向尾部方向的基向量字典（即，指向稀疏方向）。这一结果在图22.2中得到了说明。
- en: If the data dimensionality is not too high (e.g., a hundred or so) this "tailseeking"
    phenomenon also shows up in more complicated scenarios. Figure 22.3 shows examples
    of three sets of images generated from sparse sources. On the left (at top), are
    16-by-16 images with pixels sampled from independent Laplace distributions (sparse
    pixel intensities). In the middle (top) are images composed of a sparse combination
    of gratings and at right (top) a sparse combination of non-orthogonal gabor filters.
    The bottom row shows the result of learning 256 centroids with K-means from 500000
    examples drawn from each distribution. It can be seen that K-means does, in fact,
    roughly recover the sparse projections we
  id: totrans-6692
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据维度不是太高（例如，大约一百），这种“尾部寻求”现象也会出现在更复杂的场景中。图22.3展示了从稀疏源生成的三组图像的例子。左侧（顶部）是16×16的图像，像素来自独立的拉普拉斯分布（稀疏的像素强度）。中间（顶部）是由稀疏组合的条纹构成的图像，右侧（顶部）是由非正交的Gabor滤波器的稀疏组合。底部行显示了从每个分布中提取的500000个样本中使用K均值学习256个中心点的结果。可以看出，K均值确实大致恢复了稀疏投影。
- en: '![558_image_0.png](558_image_0.png)'
  id: totrans-6693
  prefs: []
  type: TYPE_IMG
  zh: '![558_image_0.png](558_image_0.png)'
- en: 'Fig. 22.3. Top: Three different sparse distributions of images. Bottom: Filters
    (centroids) identified by K-means with a complete (256-centroid) dictionary.'
  id: totrans-6694
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.3。顶部：三种不同的稀疏图像分布。底部：由K均值算法识别的滤波器（中心点），使用了完整的（256中心点）字典。
- en: '![558_image_1.png](558_image_1.png)'
  id: totrans-6695
  prefs: []
  type: TYPE_IMG
  zh: '![558_image_1.png](558_image_1.png)'
- en: '![558_image_2.png](558_image_2.png)'
  id: totrans-6696
  prefs: []
  type: TYPE_IMG
  zh: '![558_image_2.png](558_image_2.png)'
- en: would expect. A similar experiment appears in [34] to demonstrate the sourceseparation
    ability of sparse coding—yet K-means tends to recover the same filters even though
    these filters are clearly not especially similar to individual images. That is,
    K-means can potentially do more than merely recover "exemplar" images from the
    input distribution.
  id: totrans-6697
  prefs: []
  type: TYPE_NORMAL
  zh: 会期待。类似的实验出现在[34]中，以展示稀疏编码的源分离能力——然而，K均值倾向于恢复相同的滤波器，即使这些滤波器显然与单个图像并不特别相似。也就是说，K均值可能做的远不止从输入分布中恢复“示例”图像。
- en: 'When applied to natural images, it is evident that the learned centroids D(j)
    (as in Figure 22.1c) are relatively sparse projections of the data. Figure 22.4a
    shows a histogram of responses resulting from projecting randomly chosen (whitened)
    image patches onto 4 different filters. The 4 filters used are:'
  id: totrans-6698
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用于自然图像时，很明显学习到的中心点D(j)（如图22.1c所示）是数据的相对稀疏投影。图22.4a显示了从随机选择的（去白化）图像补丁投影到4个不同滤波器上所得到的响应直方图。使用的4个滤波器为：
- en: (i) a centroid learned by K-means, (ii) a basis vector trained via sparse coding,
    (iii) a randomly selected image patch, and (iv) a randomly initialized filter.
    In the figure, it can be seen that using the K-means-trained centroid as a linear
    filter gives us a very sparse projection of the data. Thus, it appears that relative
    to other simple choices K-means does tend to seek out very sparse projections
    of the data, even though its formulation as a clustering algorithm does not aim
    to do this explicitly.
  id: totrans-6699
  prefs: []
  type: TYPE_NORMAL
  zh: (i) K均值学习到的中心点，(ii) 通过稀疏编码训练的基向量，(iii) 随机选择的图像补丁，以及(iv) 随机初始化的滤波器。在图中可以看到，使用K均值训练的中心点作为线性滤波器可以提供数据的非常稀疏投影。因此，相较于其他简单选择，K均值确实倾向于寻找数据的非常稀疏投影，尽管其作为聚类算法的形式并不明确旨在做到这一点。
- en: 'Despite this empirical similarity to ICA and sparse coding, K-means does have
    a major drawback: it turns out that its ability to discover sparse directions
    in the data depends heavily on the dimensionality of the input and the quantity
    of data. In particular, as the dimensionality increases, we need increasingly
    large quantities of data to get clean results. For instance, to obtain the results
    above we had to use a very large number of examples. We can obtain similar results
    easily with sparse coding with just 10000 examples. For very large patches, we
    must use even more. Figure 22.4b shows the results of running K-means on 500000
    64-by-64 patches—note that while we can capture a few edges, most of the clusters
    are composed of a single patch from the dataset. At this scale, empty or near-empty
    clusters become far more common and extremely large amounts of data are needed
    to make K-means work well. This tradeoff is the main driver in deciding when and
    how to employ K-means: depending on the dimensionality of the input, a certain
    amount of data will be required (typically much more than is needed for similar
    results from sparse coding). For modest dimensionalities (e.g.,'
  id: totrans-6700
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在经验上与ICA和稀疏编码有相似之处，但K均值确实有一个主要缺陷：它发现数据中稀疏方向的能力在很大程度上依赖于输入的维度和数据量。特别是，随着维度的增加，我们需要越来越大量的数据来获得干净的结果。例如，为了获得上述结果，我们不得不使用非常大量的例子。我们只需10000个例子即可轻松通过稀疏编码获得类似的结果。对于非常大的补丁，我们必须使用更多。图22.4b显示了对500000个64x64补丁运行K均值的结果——注意，虽然我们可以捕捉到一些边缘，但大多数簇由数据集中的单个补丁组成。在这个尺度上，空或近空的簇变得更为常见，需要极大的数据量才能使K均值有效工作。这个权衡是决定何时以及如何使用K均值的主要驱动因素：根据输入的维度，某种量的数据将是必需的（通常远远超过稀疏编码所需的相似结果）。对于适中的维度（例如，
- en: '![559_image_0.png](559_image_0.png)'
  id: totrans-6701
  prefs: []
  type: TYPE_IMG
  zh: '![559_image_0.png](559_image_0.png)'
- en: Fig. 22.4. (a) A comparison of the distribution of linear filter responses when
    filters obtained by 4 different methods are applied to natural image patches.
    K-means tends to learn filters with very sparse response characteristics similar
    to sparse coding—much more sparse than randomly chosen patches or randomly initialized
    filters. (b) "Centroids" learned from 64-by-64 pixel patches. At this scale, K-means
    becomes difficult to apply as many clusters become empty or singletons.
  id: totrans-6702
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.4。（a）当通过4种不同方法获得的滤波器应用于自然图像补丁时，线性滤波器响应的分布比较。K均值倾向于学习具有非常稀疏响应特征的滤波器，这些特征类似于稀疏编码——比随机选择的补丁或随机初始化的滤波器稀疏得多。（b）从64x64像素补丁学习到的“中心点”。在这个尺度上，K均值的应用变得困难，因为许多簇变得空或仅为单例。
- en: hundreds of inputs), this tradeoff can be advantageous because the additional
    data requirements do not outweigh the very large constant-factor speedup that
    is gained by training with K-means. For very high dimensionalities, however, it
    may well be the case that another algorithm like sparse coding works better or
    even faster.
  id: totrans-6703
  prefs: []
  type: TYPE_NORMAL
  zh: 数百个输入），这种权衡可能是有利的，因为额外的数据需求并不会超过通过K均值训练所获得的非常大的常数因子加速。然而，对于非常高的维度，可能另一个算法，如稀疏编码，会更好或更快。
- en: 22.4 Application To Image Recognition
  id: totrans-6704
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.4 应用于图像识别
- en: 'The above discussion has provided the basic ingredients needed to turn K-means
    into a simple feature learning method. Given a batch of unlabeled data, we can
    now learn a dictionary D whose columns yield more or less sparse projections of
    the data points. Just as with sparse coding, we can use the learned dictionary
    (centroids) to define features for a supervised learning task [35]. A typical
    pipeline used for image recognition applications (that is easy to use with learned
    features) is based on the classic spatial pyramid model developed in the computer
    vision literature [13, 28, 39, 11]. It is similar in many ways to a single-layered
    convolutional neural network [29, 30]. The main components of this pipeline are:
    (i) the unsupervised training algorithm (in this case, K-means), which generates
    a bank of filters D, (ii) a function f : Rn → Rk that maps an input image patch
    x ∈ Rn to a feature vector y ∈ Rk given the dictionary of k filters. For instance,
    we could choose f(x; D) = g(Dx) for an element-wise nonlinear function g(·).'
  id: totrans-6705
  prefs: []
  type: TYPE_NORMAL
  zh: '上述讨论提供了将 K-means 转变为简单特征学习方法所需的基本元素。给定一批无标签数据，我们现在可以学习一个字典 D，其列产生的数据点的稀疏投影。与稀疏编码一样，我们可以使用学习到的字典（中心点）为监督学习任务定义特征
    [35]。用于图像识别应用的典型管道（易于使用学习到的特征）基于计算机视觉文献中开发的经典空间金字塔模型 [13, 28, 39, 11]。在许多方面，它与单层卷积神经网络
    [29, 30] 相似。该管道的主要组件是：（i）无监督训练算法（在此情况下为 K-means），它生成过滤器 D 的库；（ii）一个函数 f : Rn →
    Rk，它将输入图像补丁 x ∈ Rn 映射到给定 k 个过滤器字典的特征向量 y ∈ Rk。例如，我们可以选择 f(x; D) = g(Dx)，其中 g(·)
    为逐元素非线性函数。'
- en: Using the learned feature extractor f(x; D), given any p-by-p pixel image patch,
    we can now compute a representation y ∈ Rk for that patch. We can thus
  id: totrans-6706
  prefs: []
  type: TYPE_NORMAL
  zh: 使用学习到的特征提取器 f(x; D)，给定任何 p-by-p 像素图像补丁，我们现在可以计算该补丁的表示 y ∈ Rk。因此，我们可以
- en: '![560_image_0.png](560_image_0.png)'
  id: totrans-6707
  prefs: []
  type: TYPE_IMG
  zh: '![560_image_0.png](560_image_0.png)'
- en: Fig. 22.5. A standard image recognition pipeline used in conjunction with K-means
    dictionary learning
  id: totrans-6708
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22.5. 与 K-means 字典学习结合使用的标准图像识别管道
- en: define a (single layer) representation of the entire image by applying the function
    f to many sub-patches. Specifically, given an image of w-by-w pixels, we define
    a
  id: totrans-6709
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将函数 f 应用到许多子补丁，定义整个图像的（单层）表示。具体来说，给定一个 w-by-w 像素的图像，我们定义一个
- en: (w−p+ 1)-by-(w−p+ 1)-by-k array of features by computing the representation
    y for every p-by-p "sub-patch" of the input image. For computational efficiency,
    we may also "step" our p-by-p feature extractor across the image with some step-size
    (or "stride") greater than 1. This is illustrated in Figure 22.5.
  id: totrans-6710
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算输入图像每个 p-by-p "子补丁" 的表示 y，我们得到一个 (w−p+ 1)-by-(w−p+ 1)-by-k 特征数组。为了计算效率，我们也可以以大于
    1 的步幅“步进”我们的 p-by-p 特征提取器，这在图 22.5 中有所示。
- en: Before classification, we typically reduce the dimensionality of the image representation
    by pooling. For a step size of 1 pixel, our feature extractor produces a (w−p+
    1)-by-(w−p+ 1)-by-k representation. We can reduce this by summing
  id: totrans-6711
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类之前，我们通常通过汇聚来降低图像表示的维度。对于 1 像素的步幅，我们的特征提取器生成一个 (w−p+ 1)-by-(w−p+ 1)-by-k 的表示。我们可以通过求和来减少这一点。
- en: (or applying some other reduction, e.g., max) over local regions of the feature
    responses. Once we have the pooled responses, we could attempt to learn higherlevel
    features by applying K-means again (this is the approach pursued by [1]), or just
    use all of the features directly with a standard linear classification algorithm
    (e.g., SVM).
  id: totrans-6712
  prefs: []
  type: TYPE_NORMAL
  zh: （或对特征响应的局部区域应用其他降维方法，例如最大值）。一旦我们获得了汇聚的响应，我们可以尝试通过再次应用 K-means 来学习更高层次的特征（这是
    [1] 中采用的方法），或者直接使用所有特征与标准线性分类算法（例如 SVM）。
- en: 22.4.1 Parameters
  id: totrans-6713
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.4.1 参数
- en: The processing pipeline above has a large number of tunable parameters, such
    as the patch size p, the choice of f(x; D), and the step size. It turns out that
    getting these parameters set correctly can make a major difference in performance
    for practical applications. In fact, these parameters often have a bigger influence
    on classification performance than the training algorithm itself. When we are
    unhappy with performance, searching for better choices of these parameters can
    often be more beneficial than trying to modify our learning algorithm [11, 18].
    Here we will briefly summarize current advice on how to choose these parameters.
  id: totrans-6714
  prefs: []
  type: TYPE_NORMAL
  zh: 上述处理流程有大量可调参数，如补丁大小p、f(x; D)的选择和步长。事实证明，正确设置这些参数对实际应用的性能有重大影响。实际上，这些参数对分类性能的影响通常大于训练算法本身。当我们对性能不满意时，寻找这些参数的更好选择往往比尝试修改我们的学习算法更有益[11,
    18]。在这里，我们将简要总结如何选择这些参数的当前建议。
- en: First, when we use K-means to train the filter bank D, we noted previously that
    the input dimensionality can significantly influence the data requirements and
    success of training. Thus, in addition to other effects it may have on classification
    performance, it is important to choose the patch size p wisely. If p is too large
    (e.g., 64 pixels, as in Figure 22.4b), then K-means may yield poor results. Though
    this situation can be debugged visually for applications to image data, it is
    much more difficult to know when K-means is doing well when it is applied to other
    kinds of data such as when training a multi-layered network where the higher-level
    features are hard to visualize. For this reason, it is recommended that p be chosen
    by cross validation or it should be set so that the dimensionality of the data
    passed to K-means is kept small (typically not more than a few hundred, depending
    on the amount of data used). For image patches, 6-by6 or 8-by-8 pixel (color or
    gray) patches work consistently well in the pipeline outlined above.
  id: totrans-6715
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，当我们使用K均值训练滤波器组D时，之前提到输入维度可以显著影响数据需求和训练成功。因此，除了它对分类性能的其他影响外，明智地选择补丁大小p是重要的。如果p太大（例如，图22.4b中的64像素），那么K均值可能会产生不良结果。虽然在应用于图像数据时，可以通过视觉调试来解决此问题，但当K均值应用于其他类型的数据（例如训练多层网络时）时，就很难知道它是否表现良好，因为高层特征很难可视化。因此，建议通过交叉验证选择p，或者将传递给K均值的数据维度保持较小（通常不超过几百，具体取决于使用的数据量）。对于图像补丁，6x6或8x8像素（彩色或灰度）补丁在上述流程中表现
    consistently well。
- en: Depending on the choice of pooling method, the step size and pooling regions
    may need to be chosen differently. There is a significant body of work covering
    these areas [6, 5, 4, 12, 24, 18]. In our own experience, for single layers of
    features, average pooling over a few equally-sized regions (e.g., a 2-by-2 or
    3-by-3 grid) can work very well in practice and is a good "first try" for image
    recognition.
  id: totrans-6716
  prefs: []
  type: TYPE_NORMAL
  zh: 根据选择的池化方法，步长和池化区域可能需要不同的选择。关于这些领域有大量的研究[6, 5, 4, 12, 24, 18]。根据我们的经验，对于单层特征，平均池化几个大小相同的区域（例如2x2或3x3网格）在实践中效果非常好，是图像识别的一个良好“首次尝试”。
- en: Finally, the number of features k learned by the algorithm has a significant
    influence on the results. It has been observed several times [18, 11] that learning
    large numbers of features can substantially improve supervised classification
    results. Indeed, it is frequently best to set k as large as compute resources
    will allow, considering data requirements. Though performance typically asymptotes
    as k becomes extremely large, increasing the size of k is a very effective way
    to squeeze out a bit of extra performance from an already-built system. This trend,
    combined with the fact that K-means is especially effective for building very
    large dictionaries, is the main advantage of the system presented above.
  id: totrans-6717
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，算法学习的特征数量k对结果有显著影响。有研究多次观察到[18, 11]，学习大量特征可以显著提高监督分类结果。实际上，通常将k设置为尽可能大是最佳选择，考虑到数据需求。尽管当k变得非常大时，性能通常会趋于平稳，但增加k的大小是一种非常有效的方法，可以从已经构建的系统中挤出额外的性能。这一趋势，再加上K均值在构建非常大字典时特别有效，是上述系统的主要优势。
- en: 22.4.2 Encoders
  id: totrans-6718
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.4.2 编码器
- en: The choice of the feature "encoding" function f(x; D) also has a major impact
    on recognition performance. For instance, consider the "hard assignment" encoding
    where we take f(x; D) = s, with s the standard "one-hot" code used during K-means
    training (Eq. (22.1)). It is well-known that this scheme performs very poorly
    compared to other choices [18, 1]. Thus, once we have run K-means to train our
    filters, one should certainly make an effort to choose a better encoder f. There
    are many encoding schemes present in the literature [34, 38, 40, 42, 19, 20, 7]
    and, while they often include their own training algorithms, one can choose to
    use K-means-trained dictionaries in conjunction with many of them.
  id: totrans-6719
  prefs: []
  type: TYPE_NORMAL
  zh: 特征“编码”函数 f(x; D) 的选择对识别性能也有重大影响。例如，考虑“硬分配”编码，我们取 f(x; D) = s，s 是在 K-means 训练期间使用的标准“独热”编码（方程（22.1））。众所周知，这种方案的表现与其他选择相比非常糟糕
    [18, 1]。因此，一旦我们运行 K-means 训练我们的滤波器，就应该努力选择一个更好的编码器 f。文献中存在许多编码方案 [34, 38, 40,
    42, 19, 20, 7]，尽管它们通常包含自己的训练算法，但也可以选择将 K-means 训练的字典与其中许多方案结合使用。
- en: Unfortunately, many encoding schemes proposed for computer vision tasks are
    potentially very expensive to compute. Many require us to solve a difficult optimization
    problem in order to compute f(x; D) [34, 40, 20, 7]. On the other hand, some encoders
    are simple nonlinear functions of the filter responses Dx that can be computed
    extremely quickly. In previous work it has appeared that algorithms like sparse
    coding are generally the best performers in benchmarks [39, 4]. However, in some
    cases we can manage to get away with much simpler encodings. Specifically, when
    we use the single-layer architecture outlined above, it turns out that algorithms
    like sparse coding and more sophisticated variants (e.g., spike-slab sparse coding
    [20]) are difficult to top when we have very little labeled data. But as can be
    seen in Figure 22.6, with much more labeled data the disparity in performance
    between sparse coding and a very simple nonlinear
  id: totrans-6720
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，许多为计算机视觉任务提出的编码方案在计算上可能非常昂贵。许多方案要求我们解决一个困难的优化问题以计算 f(x; D) [34, 40, 20,
    7]。另一方面，一些编码器是滤波器响应 Dx 的简单非线性函数，可以非常快速地计算。在以前的研究中，稀疏编码等算法通常在基准测试中表现最好 [39, 4]。然而，在某些情况下，我们可以用更简单的编码方案来应对。具体而言，当我们使用上述单层架构时，稀疏编码和更复杂的变体（例如，脉冲-板稀疏编码
    [20]）在标记数据非常少的情况下很难超越。但如图 22.6 所示，随着标记数据的增加，稀疏编码与非常简单的非线性之间的性能差距也随之增加。
- en: '![562_image_0.png](562_image_0.png)'
  id: totrans-6721
  prefs: []
  type: TYPE_IMG
  zh: '![562_image_0.png](562_image_0.png)'
- en: Fig. 22.6. A comparison of the performance between two encoders on the CIFAR10
    [25] benchmark as a function of the number of labeled examples. When labeled data
    is scarce, an expensive encoder can be worthwhile. If labeled data is plentiful,
    a fast, simple encoder such as a soft-threshold nonlinearity is sufficient.
  id: totrans-6722
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22.6. 两种编码器在 CIFAR10 [25] 基准上性能的比较，作为标记示例数量的函数。当标记数据稀缺时，昂贵的编码器可能是值得的。如果标记数据丰富，像软阈值非线性这样的快速简单编码器就足够了。
- en: encoder decreases significantly. We have found, not surprisingly, that as we
    use increasing quantities of labeled data the supervised learning stage takes
    over and works equally well with most reasonable encoders.
  id: totrans-6723
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的性能显著下降。我们发现，毫不奇怪，随着使用的标记数据量的增加，监督学习阶段占据主导地位，并且与大多数合理的编码器一样有效。
- en: As a result of this observation, application designers should consider the quantity
    of available labeled data. If labeled data is abundant, a fast feed-forward encoder
    works well (and is the easiest to use on large datasets). If labeled data is scarce,
    however, it can be worthwhile to use a more expensive encoding scheme. In the
    large-scale case we have found that soft-threshold nonlinearities
  id: totrans-6724
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一观察，应用设计者应考虑可用的标记数据量。如果标记数据丰富，快速前馈编码器表现良好（并且在大数据集上使用最为简单）。然而，如果标记数据稀缺，使用更昂贵的编码方案可能会更有价值。在大规模案例中，我们发现软阈值非线性函数
- en: (f(x; D, α) = max{0, Dx − α}, where α is a tunable constant) work very well.
  id: totrans-6725
  prefs: []
  type: TYPE_NORMAL
  zh: （f(x; D, α) = max{0, Dx − α}，其中 α 是一个可调常数）表现得非常好。
- en: By contrast, sigmoid nonlinearities (e.g., f(x; D, b) = (1 + exp(−Dx + b))−1)
  id: totrans-6726
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，sigmoid 非线性（例如，f(x; D, b) = (1 + exp(−Dx + b))−1）
- en: appear to work significantly less well [33, 12] in similar instances.
  id: totrans-6727
  prefs: []
  type: TYPE_NORMAL
  zh: 在类似情况下表现显著不佳 [33, 12]。
- en: 22.5 Local Receptive Fields And Multiple Layers
  id: totrans-6728
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.5 局部感受野与多层
- en: Several times we have referred to the difficulties involved in applying K-means
    to high-dimensional data. In Section 22.4.1 it was explained that we should choose
    the image patch size p ("receptive field" size) carefully to avoid exceeding the
    modeling capacity of K-means (as in Figure 22.4b). If we have a very large image
    it is generally not effective to apply K-means to the entire input at once.2 Instead,
    applying K-means to p-by-p pixel sub-patches is a reasonable substitute, since
    we expect that most learned features will be localized to a small region. This
    "trick" allows us to keep the input size to K-means relatively small (e.g.,
  id: totrans-6729
  prefs: []
  type: TYPE_NORMAL
  zh: 我们多次提到在高维数据上应用K均值所面临的困难。在22.4.1节中解释了我们应该仔细选择图像补丁大小p（“感受野”大小），以避免超过K均值的建模能力（如图22.4b所示）。如果我们有一张非常大的图像，通常不有效地将K均值应用于整个输入。相反，将K均值应用于p-by-p像素子补丁是一个合理的替代方案，因为我们预计大多数学习到的特征将局部化到一个小区域。这个“技巧”使我们能够将K均值的输入大小保持相对较小（例如，
- en: just p2 input dimensions for grayscale patches), but still use the resulting
    filters
  id: totrans-6730
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用灰度补丁的p2输入维度，但仍然使用得到的滤波器。
- en: 2 Note that for very large inputs it becomes impractical to perform whitening,
    which requires solving a very large optimization problem (e.g., eigenvalue decomposition).
  id: totrans-6731
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于非常大的输入，执行白化变得不切实际，这需要解决一个非常大的优化问题（例如，特征值分解）。
- en: In the authors' experience K-means starts giving poor results before this computational
    bottleneck is reached.
  id: totrans-6732
  prefs: []
  type: TYPE_NORMAL
  zh: 根据作者的经验，K均值在达到这个计算瓶颈之前就开始出现不良结果。
- en: on a much larger image by either reusing the filters for every p-by-p pixel
    subpatch of the image, or even by re-running K-means independently on each p-by-p
    region (if, for some reason, the features present in other parts of the image
    differ significantly). Though this approach is well-known from computer vision
    applications the same trick works more generally and in some cases is indispensable
    for building working systems.
  id: totrans-6733
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个更大的图像上，要么为图像的每个p-by-p像素子补丁重用滤波器，要么在每个p-by-p区域独立重新运行K均值（如果出于某种原因，图像其他部分的特征显著不同）。尽管这种方法在计算机视觉应用中广为人知，但同样的技巧在更一般的情况下有效，并且在某些情况下对于构建可用系统是不可或缺的。
- en: '![563_image_0.png](563_image_0.png)'
  id: totrans-6734
  prefs: []
  type: TYPE_IMG
  zh: '![563_image_0.png](563_image_0.png)'
- en: Fig. 22.7. (a) A dataset composed of concatenated pairs of independently sampled
    image patches. (b) Centroids trained from pairs of image patches. Note that, as
    expected, K-means learns filters that only involve half of the input at a time.
    Due to the increased dimensionality, significantly more data is needed compared
    to training on each image separately. (c) A dataset composed of image-depth pairs.
    The left half of each example is a whitened grayscale patch. The right half is
    a "depth image" [27]
  id: totrans-6735
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.7。(a) 由独立采样的图像补丁对串联组成的数据集。(b) 从图像补丁对训练得到的质心。注意，正如预期的那样，K均值学习的滤波器每次仅涉及一半输入。由于维度增加，与分别对每张图像训练相比，需要显著更多的数据。(c)
    由图像深度对组成的数据集。每个示例的左半部分是经过白化的灰度补丁，右半部分是“深度图像”[27]。
- en: where pixel intensity represents the distance of a surface from the camera.
    (d) Centroids learned jointly from image-depth pairs learn only a few weak features
    that use both modalities for similar reasons as in (b).
  id: totrans-6736
  prefs: []
  type: TYPE_NORMAL
  zh: 像素强度表示表面与相机的距离。(d) 从图像深度对共同学习的质心仅学习到一些弱特征，这些特征出于与(b)相似的原因同时使用两种模态。
- en: 'Consider a situation where our input is, in fact, a concatenation of two independent
    signals. Concretely, let us take two image patches drawn at random from a larger
    dataset and concatenate them side-by-side as in Figure 22.7a. When we run K-means
    on this type of data we end up with the centroids in Figure 22.7b where individual
    centroids tend to model just one of the two independent components, much like
    we would expect from ICA. Unfortunately, as observed previously, to achieve this
    result we actually need more data than if we had tried to model the two patches
    separately. Hence, whenever we can determine *a priori* that our input variables
    can be split into independent chunks, we should try to split them up immediately
    and run K-means on each chunk separately. Note that the contrived example here
    occurs in real applications, such as learning features from RGB-Depth data [27]:
    Figure 22.7c shows examples of image intensity concatenated with depth patches
    and Figure 22.7d shows centroids learned from them. Since at this scale depth
    tends to be only weakly related to raw pixel intensity, it might be better to
    run K-means separately on each modality of the data.'
  id: totrans-6737
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一种情况，我们的输入实际上是两个独立信号的连接。具体来说，让我们从更大的数据集中随机抽取两个图像块，并将它们并排连接，如图22.7a所示。当我们在这种类型的数据上运行K均值时，我们得到图22.7b中的质心，单个质心往往只建模两个独立成分中的一个，这与我们对ICA的预期相符。不幸的是，正如前面观察到的，为了实现这一结果，我们实际上需要比分别建模两个块时更多的数据。因此，每当我们可以*事先*判断输入变量可以分成独立块时，我们应该立即尝试将它们拆分，并分别在每个块上运行K均值。请注意，这里构造的例子在实际应用中确实存在，例如从RGB-深度数据中学习特征[27]：图22.7c显示了图像强度与深度块连接的示例，图22.7d显示了从中学习的质心。由于在这个尺度上，深度与原始像素强度的关系往往很弱，因此在每种数据模态上分别运行K均值可能更好。
- en: 22.5.1 Deep Networks
  id: totrans-6738
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.5.1 深度网络
- en: 'In Section 22.4 we presented a simple pipeline that enabled us to extract a
    single layer of features from an image by taking a dictionary learned from small
    patches and using it to extract features over a larger image (see Figure 22.5).
    We then used a pooling stage to reduce the number of features before applying
    a supervised learning algorithm. It would be nice if we could learn higher layers
    of features by taking the resulting single-layer representation and passing it
    back through our feature learning pipeline. For instance, one simple way we might
    try to accomplish this is to compute the pooled feature values for all of the
    examples in our unlabeled dataset X to give us a new dataset Z, then apply the
    exact same learning pipeline to Z to learn new features. This simple approach
    has been applied in [1], but it turns out that this straight-forward strategy
    can hit a major snag: the inputs Z used for the second layer of features will
    often be very high dimensional if, as is common, we use very large dictionaries
    of features (e.g., k = 10000 or more).'
  id: totrans-6739
  prefs: []
  type: TYPE_NORMAL
  zh: 在第22.4节中，我们介绍了一个简单的管道，使我们能够通过从小块学习到的字典提取图像的单层特征，并利用它提取更大图像的特征（见图22.5）。然后，我们使用一个池化阶段来减少特征的数量，之后应用监督学习算法。如果我们能通过将结果的单层表示传回特征学习管道来学习更高层次的特征，那就太好了。例如，我们可以尝试通过计算无标签数据集X中所有样本的池化特征值来生成一个新的数据集Z，然后将相同的学习管道应用于Z以学习新特征。这种简单的方法已在[1]中应用，但事实证明，这种直接策略可能会遇到一个重大障碍：用于第二层特征的输入Z通常会非常高维，如果我们使用非常大的特征字典（例如，k
    = 10000或更多），这在常见情况下是这样。
- en: Concretely, let's consider a simple example.3 Suppose that our goal is to learn
    features for a 20-by-20 pixel image patch. With the approach of Section 22.4 we
    train k = 10000 16-by-16 pixel filters with K-means from 16-by-16 patches cropped
    out of our dataset. We then take the learned dictionary D and extract feature
    responses with a step size of 4 pixels f(x; D) from the 20-by-20 pixel images,
    yielding a 2-by-2-by-10000 image representation. Finally, we sum up the responses
    over each 2-by-2 region (i.e., all responses produced by each filter) to yield
    a 1-by-1-by-10000 "pooled representation" which we will take as Z. We can think
    of each feature value zj as being a slightly translation-invariant version of
    the feature detector associated with the filter D(j). Note that each vector in
    Z now has 10000 dimensions to represent the original 400-dimensional patch.
  id: totrans-6740
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，让我们考虑一个简单的例子。3 假设我们的目标是为一个20×20像素的图像块学习特征。使用第22.4节的方法，我们从数据集中裁剪出的16×16图像块中训练k
    = 10000个16×16像素的滤波器。然后我们取学习到的字典D，并以步长为4像素的方式提取20×20像素图像的特征响应f(x; D)，得到一个2×2×10000的图像表示。最后，我们在每个2×2区域（即，每个滤波器产生的所有响应）上求和，得到一个1×1×10000的“池化表示”，我们将其视为Z。我们可以认为每个特征值zj是与滤波器D(j)相关的特征检测器的一个稍微平移不变的版本。请注意，Z中的每个向量现在有10000维来表示原始400维的图像块。
- en: 'At this scale, even learning a "complete" representation of 10000 features
    from the 10000-dimensional inputs Z becomes challenging. Regrettably, there is
    no obvious choice of local receptive field that can be made here: the 10000 features
    are unorganized and we have no way to split them up by hand.'
  id: totrans-6741
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个规模下，从10000维输入Z学习“完整”表示10000个特征变得具有挑战性。遗憾的是，这里没有明显的局部感受野选择：这10000个特征是无序的，我们没有办法手动将它们拆分。
- en: One proposed solution to this problem is to use a simple form of pair-wise
  id: totrans-6742
  prefs: []
  type: TYPE_NORMAL
  zh: 解决此问题的一个建议方案是使用简单的成对形式。
- en: '"dependency test" to help identify groups of dependent input variables in an
    automated way. If we can do this, then we can break up the input vector coordinates
    into small groups suitable for input to K-means instead of picking groups by hand.
    This tool is most valuable for building multiple layers of features with K-means.'
  id: totrans-6743
  prefs: []
  type: TYPE_NORMAL
  zh: “依赖性测试”有助于以自动化的方式识别相关输入变量组。如果我们能做到这一点，就可以将输入向量坐标分解成适合K均值算法的小组，而不是手动挑选小组。这个工具在构建多层特征与K均值时最为有价值。
- en: As an example, we can use a type of dependency called "energy correlation".
  id: totrans-6744
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，我们可以使用一种称为“能量相关性”的依赖类型。
- en: Given two whitened inputs (i.e., two inputs zj and zk that have no linear correlation)
    their energy correlation is just the correlation between their squared
  id: totrans-6745
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个经过白化处理的输入（即，没有线性相关性的两个输入zj和zk），它们的能量相关性仅是它们平方的相关性。
- en: 3 The numbers used in this example are chosen to illustrate the problem and
    its solution. For a more detailed and realistic setup, see [10].
  id: totrans-6746
  prefs: []
  type: TYPE_NORMAL
  zh: 3 本例中使用的数字旨在说明问题及其解决方案。有关更详细和现实的设置，请参见[10]。
- en: "responses. In particular, if we have E[z]=0 and E\fzz= I, then we will define\
    \ the dependency between inputs zj and zk as:"
  id: totrans-6747
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，如果我们有E[z]=0和E[zz]= I，那么我们将定义输入zj和zk之间的依赖性为：
- en: $$d[z_{j},z_{k}]=\mathrm{corr}(z_{j}^{2},z_{k}^{2})=\mathbb{E}\left[z_{j}^{2}z_{k}^{2}-1\right]/\sqrt{\mathbb{E}\left[z_{j}^{4}-1\right]\mathbb{E}\left[z_{k}^{4}-1\right]}.$$
  id: totrans-6748
  prefs: []
  type: TYPE_NORMAL
  zh: $$d[z_{j},z_{k}]=\mathrm{corr}(z_{j}^{2},z_{k}^{2})=\mathbb{E}\left[z_{j}^{2}z_{k}^{2}-1\right]/\sqrt{\mathbb{E}\left[z_{j}^{4}-1\right]\mathbb{E}\left[z_{k}^{4}-1\right]}.$$
- en: This metric is easy to compute by first whitening the input data Z with ZCA
  id: totrans-6749
  prefs: []
  type: TYPE_NORMAL
  zh: 这个度量很容易计算，首先用ZCA对输入数据Z进行白化。
- en: 'whitening [3], then computing the pairwise similarities between all of the
    features:'
  id: totrans-6750
  prefs: []
  type: TYPE_NORMAL
  zh: 白化[3]，然后计算所有特征之间的成对相似性：
- en: $$d(j,k;Z)\equiv d[z_j,z_k]\equiv\frac{\sum_i z_j^{(i)^2}z_k^{(i)^2}-1}{\sqrt{\sum_i(z_j^{(i)^4}-1)\sum_i(z_k^{(i)^4}-1)}}.$$tation
    is practical for fewer than 10000 input features, i.
  id: totrans-6751
  prefs: []
  type: TYPE_NORMAL
  zh: $$d(j,k;Z)\equiv d[z_j,z_k]\equiv\frac{\sum_i z_j^{(i)^2}z_k^{(i)^2}-1}{\sqrt{\sum_i(z_j^{(i)^4}-1)\sum_i(z_k^{(i)^4}-1)}}.$$对于少于10000个输入特征，tation是实用的。
- en: '![565_image_1.png](565_image_1.png)'
  id: totrans-6752
  prefs: []
  type: TYPE_IMG
  zh: '![565_image_1.png](565_image_1.png)'
- en: This computation is practical for fewer than 10000 input features. It can still
    be
  id: totrans-6753
  prefs: []
  type: TYPE_NORMAL
  zh: 该计算在少于10000个输入特征的情况下是实用的。它仍然可以。
- en: '![565_image_0.png](565_image_0.png) computed approximately for hundreds of
    thousands of features if necessary [10].'
  id: totrans-6754
  prefs: []
  type: TYPE_NORMAL
  zh: '![565_image_0.png](565_image_0.png) 如果需要，可以大约为数十万特征计算[10]。'
- en: Thus, we now have a function d(*j, k*;Z) that can provide a measure of the dependency
    between features zj and zk observed in a given dataset Z.
  id: totrans-6755
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在有一个函数d(*j, k*;Z)，它可以提供在给定数据集Z中观察到的特征zj和zk之间的依赖性度量。
- en: Fig. 22.8. Groups of features selected by an automated dependency test. The
    features corresponding to each group of filters would be processed separately
    by K-means to build a second layer of features.
  id: totrans-6756
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22.8. 通过自动依赖测试选择的特征组。每组滤波器对应的特征将由 K-means 单独处理，以构建第二层特征。
- en: 'Now we would like to try to learn some "higher level" features on top of the
    Z representation. Using our dependency test we can find reasonable choices of
    receptive fields in a relatively simple way: we pick one feature, say z0, and
    then use the dependency test to find the R features with the strongest dependence
    on z0 according to the test (i.e., find the indices j so that d(0, j;Z) is large).
    We then run K-means using only these R features as input. If we pick R small enough
    (e.g., 100 or 200) the usual normalization, whitening and K-means training steps
    can be applied easily and require virtually no tuning to work well. Because of
    the smaller input dimension we only need to train a few hundred centroids and
    thus we can use much less data than would be required to run K-means on the original
    10000-dimensional dataset. This procedure can be repeated for many choices of
    the "seed" feature (z0 above) until we have trained dictionaries from receptive
    fields covering all of the input variables in z. Figure 22.8 shows the first-layer
    filters from D corresponding to some of these groups of features (i.e.,'
  id: totrans-6757
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想尝试在 Z 表示的基础上学习一些“更高层次”的特征。使用我们的依赖测试，我们可以以相对简单的方式找到合理的感受野选择：我们选择一个特征，比如
    z0，然后使用依赖测试找到与 z0 具有最强依赖关系的 R 个特征（即，找到 j 的索引，使得 d(0, j;Z) 较大）。然后我们仅使用这些 R 个特征作为输入运行
    K-means。如果我们选择 R 足够小（例如，100 或 200），通常的归一化、白化和 K-means 训练步骤可以轻松应用，并几乎无需调优即可良好运行。由于输入维度较小，我们只需要训练几百个质心，因此我们可以使用比在原始
    10000 维数据集上运行 K-means 所需的数据少得多。这个过程可以对许多“种子”特征（上述的 z0）进行重复，直到我们训练出覆盖 z 中所有输入变量的感受野字典。图
    22.8 显示了 D 中对应于这些特征组的一层滤波器（即，
- en: these are the D(j) whose pooled responses zj have high dependence according
    to the energy correlation test).
  id: totrans-6758
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是根据能量相关性测试，池化响应 zj 具有高度依赖的 D(j)。
- en: Table 22.1. Results on CIFAR-10
  id: totrans-6759
  prefs: []
  type: TYPE_NORMAL
  zh: 表 22.1. CIFAR-10 的结果
- en: (full)
  id: totrans-6760
  prefs: []
  type: TYPE_NORMAL
  zh: (完整)
- en: Table 22.2. Results on CIFAR-10 (400 ex. per class)
  id: totrans-6761
  prefs: []
  type: TYPE_NORMAL
  zh: 表 22.2. CIFAR-10 的结果（每类 400 个样本）
- en: '|                                 | Architecture                                                               |
    Accuracy (%)   |'
  id: totrans-6762
  prefs: []
  type: TYPE_TB
  zh: '|                                 | 架构                                                                     |
    准确率 (%)     |'
- en: '| --- | --- | --- |'
  id: totrans-6763
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|                                 | 1 Layer                                                                    |
    64.6% (±0.8%)  |'
  id: totrans-6764
  prefs: []
  type: TYPE_TB
  zh: '|                                 | 1层                                                                    |
    64.6% (±0.8%)  |'
- en: '|                                 | 1 Layer (4800 maps)                                                        |
    63.7% (±0.7%)  |'
  id: totrans-6765
  prefs: []
  type: TYPE_TB
  zh: '|                                 | 1层（4800个映射）                                                          |
    63.7% (±0.7%)  |'
- en: '|                                 | 2 Layers (Single RF)                                                       |
    65.8% (±0.3%)  |'
  id: totrans-6766
  prefs: []
  type: TYPE_TB
  zh: '|                                 | 2层（单一RF）                                                             |
    65.8% (±0.3%)  |'
- en: '|                                 | 2 Layers (Random RF)                                                       |
    65.8% (±0.9%)  |'
  id: totrans-6767
  prefs: []
  type: TYPE_TB
  zh: '|                                 | 2层（随机RF）                                                               |
    65.8% (±0.9%)  |'
- en: '|                                 | 2 Layers (Learned RF)                                                      |
    69.2% (±0.7%)  |'
  id: totrans-6768
  prefs: []
  type: TYPE_TB
  zh: '|                                 | 2层（学习RF）                                                              |
    69.2% (±0.7%)  |'
- en: '|                                 | 3 Layers (Learned RF)                                                      |
    70.7% (±0.7%)  |'
  id: totrans-6769
  prefs: []
  type: TYPE_TB
  zh: '|                                 | 3层（学习RF）                                                            |
    70.7% (±0.7%)  |'
- en: '|                                 | Sparse coding (1 layer) [12] 66.4% (±0.8%)
    VQ (1 layer) [12] 64.4% (±1.0%) |                |'
  id: totrans-6770
  prefs: []
  type: TYPE_TB
  zh: '|                                 | 稀疏编码（1层）[12] 66.4% (±0.8%) VQ（1层）[12] 64.4%
    (±1.0%) |                |'
- en: '| Architecture                    | Accuracy (%)                                                               |                |'
  id: totrans-6771
  prefs: []
  type: TYPE_TB
  zh: '| 架构                           | 准确率 (%)                                                                |                |'
- en: '| 1 Layer                         | 78.3%                                                                      |                |'
  id: totrans-6772
  prefs: []
  type: TYPE_TB
  zh: '| 1层                           | 78.3%                                                                      |                |'
- en: '| 1 Layer (4800 maps)             | 80.6%                                                                      |                |'
  id: totrans-6773
  prefs: []
  type: TYPE_TB
  zh: '| 1层（4800个映射）             | 80.6%                                                                      |                |'
- en: '| 2 Layers (Single RF)            | 77.4%                                                                      |                |'
  id: totrans-6774
  prefs: []
  type: TYPE_TB
  zh: '| 2层（单一感受野）            | 77.4%                                                                      |                |'
- en: '| 2 Layers (Random RF)            | 77.6%                                                                      |                |'
  id: totrans-6775
  prefs: []
  type: TYPE_TB
  zh: '| 2层（随机感受野）              | 77.6%                                                                      |                |'
- en: '| 2 Layers (Learned RF)           | 81.2%                                                                      |                |'
  id: totrans-6776
  prefs: []
  type: TYPE_TB
  zh: '| 2层（学习的感受野）            | 81.2%                                                                      |                |'
- en: '| 3 Layers (Learned RF)           | 82.0%                                                                      |                |'
  id: totrans-6777
  prefs: []
  type: TYPE_TB
  zh: '| 3层（学习的感受野）            | 82.0%                                                                      |                |'
- en: '| VQ (6000 maps) [12]             | 81.5%                                                                      |                |'
  id: totrans-6778
  prefs: []
  type: TYPE_TB
  zh: '| VQ（6000个映射）[12]          | 81.5%                                                                      |                |'
- en: '| Conv. DBN [26]                  | 78.9%                                                                      |                |'
  id: totrans-6779
  prefs: []
  type: TYPE_TB
  zh: '| 卷积深度信念网络 [26]           | 78.9%                                                                      |                |'
- en: '| Deep NN [8]                     | 80.49%                                                                     |                |'
  id: totrans-6780
  prefs: []
  type: TYPE_TB
  zh: '| 深度神经网络 [8]               | 80.49%                                                                     |                |'
- en: '| Multi-column Deep NN [9] 88.79% |                                                                            |                |'
  id: totrans-6781
  prefs: []
  type: TYPE_TB
  zh: '| 多列深度神经网络 [9] 88.79%    |                                                                            |                |'
- en: '|                                                                            |
    Architecture   | Accuracy (%)   |'
  id: totrans-6782
  prefs: []
  type: TYPE_TB
  zh: '|                                                                            |
    架构         | 准确率 (%)     |'
- en: '| --- | --- | --- |'
  id: totrans-6783
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 Layer                                                                    |
    54.5% (±0.8%)  |                |'
  id: totrans-6784
  prefs: []
  type: TYPE_TB
  zh: '| 1层                                                                       |
    54.5% (±0.8%)  |                |'
- en: '| 1 Layer (4800 maps)                                                        |
    53.8% (±1.6%)  |                |'
  id: totrans-6785
  prefs: []
  type: TYPE_TB
  zh: '| 1层（4800个映射）                                                     | 53.8% (±1.6%)  |                |'
- en: '| 2 Layers (Single RF)                                                       |
    55.0% (±0.8%)  |                |'
  id: totrans-6786
  prefs: []
  type: TYPE_TB
  zh: '| 2层（单一感受野）                                                      | 55.0% (±0.8%)  |                |'
- en: '| 2 Layers (Random RF)                                                       |
    54.4% (±1.2%)  |                |'
  id: totrans-6787
  prefs: []
  type: TYPE_TB
  zh: '| 2层（随机感受野）                                                        | 54.4%
    (±1.2%)  |                |'
- en: '| 2 Layers (Learned RF)                                                      |
    58.9% (±1.1%)  |                |'
  id: totrans-6788
  prefs: []
  type: TYPE_TB
  zh: '| 2层（学习的感受野）                                                  | 58.9% (±1.1%)  |                |'
- en: '| 3 Layers (Learned RF)                                                      |
    60.1% (±1.0%)  |                |'
  id: totrans-6789
  prefs: []
  type: TYPE_TB
  zh: '| 3层（学习的感受野）            | 60.1% (±1.0%)  |                |'
- en: '| Sparse coding (1 layer) [12] 59.0% (±0.8%) VQ (1 layer) [12] 54.9% (±0.4%)
    |                |                |'
  id: totrans-6790
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏编码（1层）[12] 59.0% (±0.8%) VQ（1层）[12] 54.9% (±0.4%) |                |                |'
- en: Table 22.3. Classification Results on STL-10
  id: totrans-6791
  prefs: []
  type: TYPE_NORMAL
  zh: 表22.3. STL-10分类结果
- en: The effectiveness of this sort of approach combined with K-means has been shown
    in previous work [10]. Table 22.1 details results obtained on the full CIFAR dataset
    with various settings and comparisons to other contemporary methods.
  id: totrans-6792
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结合K-means的方法的有效性在之前的工作中已经得到证明[10]。表22.1详细列出了在完整CIFAR数据集上获得的各种设置和与其他当代方法的比较结果。
- en: 'First, we can see in these results that learning 2 layers of features is essentially
    fruitless when using naive choices of receptive fields: a single receptive field
    that includes all of the inputs, or receptive fields that connect to random inputs.
    Indeed, the results for 2 layer networks are *worse* (77.4% and 77.6%)'
  id: totrans-6793
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以从这些结果中看到，使用简单的感受野选择学习2层特征在本质上是无效的：一个包含所有输入的单一感受野，或者连接到随机输入的感受野。实际上，2层网络的结果是*更差的*（77.4%
    和 77.6%）。
- en: 'than obtained using a single layer alone (78.3%). This should be expected:
    using a single receptive field, K-means is unable to build good features due to
    the high-dimensional input (like Figure 22.4b), yet using a random receptive field
    wastes representational power modeling unrelated inputs (like Figure 22.7). By
    contrast, results obtained with the receptive field learning scheme above (with
    2nd-order dependency measure) are significantly better: achieving a significant
    improvement over the baseline single-layer results, and even out-performing a
    much larger single-layer network. With 3 layers, this system improves further
    to 82.0% accuracy. Achieving the best possible results (as reported in [9]) may
    require supervised training of the entire network, but this result demonstrates
    very clearly the importance of controlling the *connectivity* of features in order
    for K-means to work well in deep networks (where we typically use only unlabeled
    data to construct features).'
  id: totrans-6794
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单层获得的结果（78.3%）相比，使用两层的结果应该是预期的：单一感受野由于高维输入（如图22.4b）无法构建良好的特征，而使用随机感受野会浪费建模无关输入的表现力（如图22.7）。相比之下，使用上述感受野学习方案（带有二阶依赖度量）获得的结果显著更好：显著改善了基准单层结果，甚至超越了更大的单层网络。通过3层，该系统进一步提高到82.0%的准确率。实现最佳可能结果（如[9]中报告的）可能需要对整个网络进行监督训练，但这一结果非常清楚地展示了控制特征的*连接性*的重要性，以便K-means在深度网络中良好运作（通常我们只使用未标记数据来构建特征）。
- en: 'Training only from unlabeled data is much more useful in a scenario where we
    have limited labeled training data. Tables 22.2 and 22.3 show results obtained
    from similar experiments on the CIFAR-10 dataset when using only 400 labeled examples
    per class, and the STL-10 [11] dataset (where only 100 labels are available per
    class). The results are very similar, even though we have less supervision: poor
    choices of receptive fields almost entirely negate the benefits of training multiple
    layers of features, but using the simple receptive field selection technique above
    allows us to successfully build up to 3 layers of useful features with K-means.'
  id: totrans-6795
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记训练数据有限的情况下，仅从未标记数据中训练是非常有用的。表22.2和22.3展示了在CIFAR-10数据集上仅使用每类400个标记示例进行类似实验时获得的结果，以及STL-10
    [11] 数据集（每类仅提供100个标签）。尽管监督较少，结果非常相似：不当的感受野选择几乎完全抵消了训练多个特征层的好处，但使用上述简单的感受野选择技术使我们成功构建了最多3层有用的特征与K-means。
- en: 22.6 Conclusion
  id: totrans-6796
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.6 结论
- en: 'In this chapter we have reviewed many results, observations and tricks that
    are useful for building feature-learning systems with K-means as a scalable unsupervised
    learning module. The major considerations that we have covered, which practitioners
    should keep in mind before embarking on a new application, are summarized as:'
  id: totrans-6797
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了许多有助于构建以K-means为可扩展无监督学习模块的特征学习系统的结果、观察和技巧。我们总结了实践者在开始新应用之前应该牢记的主要考虑因素：
- en: 1. Mean and contrast normalize inputs.
  id: totrans-6798
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 均值和对比度归一化输入。
- en: 2. Use whitening to "sphere" the data, taking care to set the  parameter appropriately.
    If whitening cannot be performed due to input dimensionality, one should split
    up the input variables.
  id: totrans-6799
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 使用白化将数据“球化”，注意适当设置参数。如果由于输入维度无法进行白化，应拆分输入变量。
- en: 3. Initialize K-means centroids randomly from Gaussian noise and normalize.
    4. Use damped updates to help avoid empty clusters and improve stability.
  id: totrans-6800
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 随机从高斯噪声中初始化K-means中心点并进行归一化。4. 使用阻尼更新以帮助避免空聚类并提高稳定性。
- en: 5. Be mindful of the impact of dimensionality and sparsity on K-means. Kmeans
    tends to find sparse projections of the input data by seeking out
  id: totrans-6801
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 注意维度和稀疏性对K-means的影响。K-means倾向于通过寻求“重尾”方向找到输入数据的稀疏投影。
- en: '"heavy-tailed" directions. Yet when the data is not properly whitened, the
    input dimensionality is very high, or there is insufficient data, it may perform
    poorly.'
  id: totrans-6802
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当数据没有得到适当的白化、输入维度非常高或数据不足时，K-means可能表现不佳。
- en: 6. With higher dimensionalities, K-means will require significantly increased
    amounts of data, possibly negating its speed advantage.
  id: totrans-6803
  prefs: []
  type: TYPE_NORMAL
  zh: 6. 随着维度的增加，K-means将需要显著增加的数据量，可能会抵消其速度优势。
- en: 7. Exogenous parameters in the system (pooling, encoding methods, etc.) can
    have a bigger impact on final performance than the learning algorithm itself.
  id: totrans-6804
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 系统中的外生参数（池化、编码方法等）对最终性能的影响可能超过学习算法本身。
- en: Consider spending compute resources on more cross-validation for parameters
    before concluding that a more expensive learning scheme is required.
  id: totrans-6805
  prefs: []
  type: TYPE_NORMAL
  zh: 在得出需要更昂贵的学习方案之前，考虑在参数上花费更多的计算资源进行交叉验证。
- en: 8. Using more centroids almost always helps when using the image recognition
    pipeline described in this chapter, provided we have enough training data. Indeed,
    whenever more compute resources become available, this is the first thing to try.
  id: totrans-6806
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更多的中心点几乎总是有助于本章所描述的图像识别管道，前提是我们有足够的训练数据。实际上，每当可用的计算资源增加时，这都是第一个要尝试的事项。
- en: 9. When labeled data is abundant, find a cheap encoder and let a supervised
    learning system do most of the work. If labeled data is limited (e.g., hundreds
    of examples per class), an expensive encoder may work better.
  id: totrans-6807
  prefs: []
  type: TYPE_NORMAL
  zh: 当标记数据丰富时，寻找一个便宜的编码器，让监督学习系统做大部分工作。如果标记数据有限（例如，每类几百个例子），昂贵的编码器可能效果更好。
- en: 10. Use local receptive fields wherever possible. Input data dimensionality
    is the main bottleneck to the success of K-means and should be kept as low as
    possible. If local receptive fields cannot be chosen by hand, try an automated
    dependency test to help cut up your data into (overlapping) groups of inputs with
    lower dimensionality. This is likely a necessity for deep networks!
  id: totrans-6808
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，尽量使用局部感受野。输入数据的维度是K-means成功的主要瓶颈，应尽量保持在较低水平。如果无法手动选择局部感受野，可以尝试自动依赖性测试，帮助将数据划分为（重叠的）低维度输入组。这对于深度网络来说可能是必要的！
- en: The above recommendations cover essentially all of the tools, tricks and insights
    that underlie recent feature-learning results based on K-means. Though it is unclear
    how far K-means can be pushed in comparison to more expressive algorithms, the
    tips above are enough to know when K-means is appropriate and to get it working
    in many challenging scenarios.
  id: totrans-6809
  prefs: []
  type: TYPE_NORMAL
  zh: 上述建议基本涵盖了基于K-means的最新特征学习结果所依赖的所有工具、技巧和见解。尽管目前尚不清楚K-means相比于更具表现力的算法能推得多远，但以上技巧足以帮助判断何时适合使用K-means，并在许多具有挑战性的场景中使其正常工作。
- en: '[1] Agarwal, A., Triggs, B.: Hyperfeatures - Multilevel Local Coding for Visual
    Recognition. In: Leonardis, A., Bischof, H., Pinz, A. (eds.) ECCV 2006, Part I.
    LNCS,'
  id: totrans-6810
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Agarwal, A., Triggs, B.: 超特征 - 多层局部编码用于视觉识别。在：Leonardis, A., Bischof, H.,
    Pinz, A.（编辑）ECCV 2006, 第I部分。LNCS，'
- en: vol. 3951, pp. 30–43. Springer, Heidelberg (2006)
  id: totrans-6811
  prefs: []
  type: TYPE_NORMAL
  zh: vol. 3951, 页码30–43。施普林格，海德堡 (2006)
- en: '[2] Aharon, M., Elad, M., Bruckstein, A.: K-SVD: An algorithm for designing
    overcomplete dictionaries for sparse representation. IEEE Transactions on Signal
    Processing 54(11), 4311–4322 (2006)'
  id: totrans-6812
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Aharon, M., Elad, M., Bruckstein, A.: K-SVD：一种用于设计过完备字典以实现稀疏表示的算法。《IEEE信号处理学报》54(11),
    4311–4322 (2006)'
- en: '[3] Bell, A., Sejnowski, T.J.: The ''independent components'' of natural scenes
    are edge filters. Vision Research 37(23), 3327–3338 (1997)'
  id: totrans-6813
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Bell, A., Sejnowski, T.J.: 自然场景的“独立成分”是边缘滤波器。《视觉研究》37(23), 3327–3338 (1997)'
- en: '[4] Boureau, Y., Bach, F., LeCun, Y., Ponce, J.: Learning mid-level features
    for recognition. In: 23rd Conference on Computer Vision and Pattern Recognition,
    pp.'
  id: totrans-6814
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Boureau, Y., Bach, F., LeCun, Y., Ponce, J.: 学习中级特征以进行识别。在：第23届计算机视觉与模式识别会议，页码。'
- en: 2559–2566 (2010)
  id: totrans-6815
  prefs: []
  type: TYPE_NORMAL
  zh: 2559–2566 (2010)
- en: '[5] Boureau, Y., Ponce, J., LeCun, Y.: A theoretical analysis of feature pooling
    in visual recognition. In: 27th International Conference on Machine Learning,
    pp. 111–118 (2010)'
  id: totrans-6816
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Boureau, Y., Ponce, J., LeCun, Y.: 视觉识别中特征池化的理论分析。在：第27届国际机器学习会议，页码111–118
    (2010)'
- en: '[6] Boureau, Y., Roux, N.L., Bach, F., Ponce, J., LeCun, Y.: Ask the locals:
    multiway local pooling for image recognition. In: 13th International Conference
    on Computer Vision, pp. 2651–2658 (2011)'
  id: totrans-6817
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Boureau, Y., Roux, N.L., Bach, F., Ponce, J., LeCun, Y.: 请教本地：图像识别的多路局部池化。在：第13届国际计算机视觉会议，页码2651–2658
    (2011)'
- en: '[7] Bradley, D.M., Bagnell, J.A.: Differentiable sparse coding. In: Advances
    in Neural Information Processing Systems, vol. 22, pp. 113–120 (2008)'
  id: totrans-6818
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Bradley, D.M., Bagnell, J.A.: 可微分稀疏编码。在：神经信息处理系统的进展，vol. 22, 页码113–120
    (2008)'
- en: '[8] Ciresan, D.C., Meier, U., Masci, J., Gambardella, L.M., Schmidhuber, J.:
    Flexible, high performance convolutional neural networks for image classification.
    In:'
  id: totrans-6819
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Ciresan, D.C., Meier, U., Masci, J., Gambardella, L.M., Schmidhuber, J.:
    灵活的高性能卷积神经网络用于图像分类。在：'
- en: International Joint Conference on Artificial Intelligence, pp. 1237–1242 (2011)
  id: totrans-6820
  prefs: []
  type: TYPE_NORMAL
  zh: 国际人工智能联合会议，页码1237–1242 (2011)
- en: '[9] Ciresan, D.C., Meier, U., Schmidhuber, J.: Multi-column deep neural networks
    for image classification. In: Computer Vision and Pattern Recognition, pp. 3642–3649
    (2012)'
  id: totrans-6821
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Ciresan, D.C., Meier, U., Schmidhuber, J.: 多列深度神经网络用于图像分类。见：计算机视觉与模式识别，页3642–3649（2012）'
- en: '[10] Coates, A., Ng, A.Y.: Selecting receptive fields in deep networks. In:
    Advances in Neural Information Processing Systems, vol. 24, pp. 2528–2536 (2011)'
  id: totrans-6822
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Coates, A., Ng, A.Y.: 在深度网络中选择感受野。见：神经信息处理系统进展，第24卷，页2528–2536（2011）'
- en: '[11] Coates, A., Lee, H., Ng, A.Y.: An analysis of single-layer networks in
    unsupervised feature learning. In: 14th International Conference on AI and Statistics,
    pp. 215– 223 (2011)'
  id: totrans-6823
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Coates, A., Lee, H., Ng, A.Y.: 对无监督特征学习中的单层网络进行分析。见：第14届人工智能与统计国际会议，页215–223（2011）'
- en: '[12] Coates, A., Ng, A.Y.: The importance of encoding versus training with
    sparse coding and vector quantization. In: 28th International Conference on Machine
    Learning, pp. 921–928 (2011)'
  id: totrans-6824
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Coates, A., Ng, A.Y.: 编码与稀疏编码及向量量化训练的重要性。见：第28届机器学习国际会议，页921–928（2011）'
- en: '[13] Csurka, G., Dance, C., Fan, L., Willamowski, J., Bray, C.: Visual categorization
    with bags of keypoints. In: ECCV Workshop on Statistical Learning in Computer
    Vision, pp. 59–74 (2004)'
  id: totrans-6825
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Csurka, G., Dance, C., Fan, L., Willamowski, J., Bray, C.: 通过关键点袋进行视觉分类。见：ECCV计算机视觉统计学习研讨会，页59–74（2004）'
- en: '[14] Dhillon, I.S., Modha, D.M.: Concept decompositions for large sparse text
    data using clustering. Machine Learning 42(1), 143–175 (2001)'
  id: totrans-6826
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Dhillon, I.S., Modha, D.M.: 使用聚类对大型稀疏文本数据进行概念分解。《机器学习》42(1)，143–175（2001）'
- en: '[15] Efron, B., Hastie, T., Johnstone, I., Tibshirani, R.: Least angle regression.
    The Annals of statistics 32(2), 407–499 (2004)'
  id: totrans-6827
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Efron, B., Hastie, T., Johnstone, I., Tibshirani, R.: 最小角回归。《统计年鉴》32(2)，407–499（2004）'
- en: '[16] Fei-Fei, L., Perona, P.: A Bayesian hierarchical model for learning natural
    scene categories. In: Computer Vision and Pattern Recognition, vol. 2, pp. 524–531
    (2005)'
  id: totrans-6828
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Fei-Fei, L., Perona, P.: 用于学习自然场景类别的贝叶斯层次模型。见：计算机视觉与模式识别，第2卷，页524–531（2005）'
- en: '[17] Garrigues, P., Olshausen, B.: Group sparse coding with a laplacian scale
    mixture prior. In: Advances in Neural Information Processing Systems, vol. 23,
    pp. 676–684 (2010)'
  id: totrans-6829
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Garrigues, P., Olshausen, B.: 带有拉普拉斯尺度混合先验的群稀疏编码。见：神经信息处理系统进展，第23卷，页676–684（2010）'
- en: '[18] van Gemert, J.C., Geusebroek, J.-M., Veenman, C.J., Smeulders, A.W.M.:
    Kernel Codebooks for Scene Categorization. In: Forsyth, D., Torr, P., Zisserman,
    A. (eds.)'
  id: totrans-6830
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] van Gemert, J.C., Geusebroek, J.-M., Veenman, C.J., Smeulders, A.W.M.:
    用于场景分类的核代码簿。见：Forsyth, D., Torr, P., Zisserman, A.（主编）'
- en: ECCV 2008, Part III. LNCS, vol. 5304, pp. 696–709. Springer, Heidelberg (2008)
  id: totrans-6831
  prefs: []
  type: TYPE_NORMAL
  zh: ECCV 2008，第三部分。LNCS，第5304卷，页696–709。施普林格，海德堡（2008）
- en: '[19] Glorot, X., Bordes, A., Bengio, Y.: Deep sparse rectifier neural networks.
    In:'
  id: totrans-6832
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Glorot, X., Bordes, A., Bengio, Y.: 深度稀疏修正神经网络。见：'
- en: 14th International Conference on Artificial Intelligence and Statistics, pp.
    315– 323 (2011)
  id: totrans-6833
  prefs: []
  type: TYPE_NORMAL
  zh: 第14届人工智能与统计国际会议，页315–323（2011）
- en: '[20] Goodfellow, I., Courville, A., Bengio, Y.: Spike-and-slab sparse coding
    for unsupervised feature discovery. In: NIPS Workshop on Deep Learning and Unsupervised
    Feature Learning (2011)'
  id: totrans-6834
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Goodfellow, I., Courville, A., Bengio, Y.: 用于无监督特征发现的尖峰-平板稀疏编码。见：NIPS深度学习与无监督特征学习研讨会（2011）'
- en: '[21] Hinton, G., Osindero, S., Teh, Y.: A fast learning algorithm for deep
    belief nets.'
  id: totrans-6835
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Hinton, G., Osindero, S., Teh, Y.: 一种快速学习深度信念网络的算法。'
- en: Neural Computation 18(7), 1527–1554 (2006)
  id: totrans-6836
  prefs: []
  type: TYPE_NORMAL
  zh: 神经计算 18(7)，1527–1554（2006）
- en: '[22] Hyvärinen, A., Hurri, J., Hoyer, P.: Natural Image Statistics. Springer
    (2009)'
  id: totrans-6837
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Hyvärinen, A., Hurri, J., Hoyer, P.: 自然图像统计。施普林格（2009）'
- en: '[23] Hyvärinen, A., Oja, E.: Independent component analysis: algorithms and
    applications. Neural Networks 13(4-5), 411–430 (2000)'
  id: totrans-6838
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Hyvärinen, A., Oja, E.: 独立成分分析：算法与应用。《神经网络》13(4-5)，411–430（2000）'
- en: '[24] Jarrett, K., Kavukcuoglu, K., Ranzato, M., LeCun, Y.: What is the best
    multistage architecture for object recognition? In: 12th International Conference
    on Computer Vision, pp. 2146–2153 (2009)'
  id: totrans-6839
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Jarrett, K., Kavukcuoglu, K., Ranzato, M., LeCun, Y.: 什么是物体识别的最佳多阶段架构？见：第12届国际计算机视觉大会，页2146–2153（2009）'
- en: '[25] Krizhevsky, A.: Learning multiple layers of features from Tiny Images.
    Master''s thesis, Dept. of Comp. Sci., University of Toronto (2009)'
  id: totrans-6840
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Krizhevsky, A.: 从Tiny Images学习多个特征层。硕士论文，多伦多大学计算机科学系（2009）'
- en: '[26] Krizhevsky, A.: Convolutional Deep Belief Networks on CIFAR-10 (2010)
    (unpublished manuscript)'
  id: totrans-6841
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Krizhevsky, A.: CIFAR-10上的卷积深度信念网络（2010）（未出版手稿）'
- en: '[27] Lai, K., Bo, L., Ren, X., Fox, D.: A large-scale hierarchical multi-view
    RGB-D'
  id: totrans-6842
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Lai, K., Bo, L., Ren, X., Fox, D.: 大规模层次多视角RGB-D'
- en: 'object dataset. In: International Conference on Robotics and Automation, pp.'
  id: totrans-6843
  prefs: []
  type: TYPE_NORMAL
  zh: 对象数据集。在：国际机器人与自动化会议，页码。
- en: 1817–1824 (2011)
  id: totrans-6844
  prefs: []
  type: TYPE_NORMAL
  zh: 1817–1824 (2011)
- en: '[28] Lazebnik, S., Schmid, C., Ponce, J.: Beyond bags of features: Spatial
    pyramid matching for recognizing natural scene categories. In: Computer Vision
    and Pattern Recognition (2006)'
  id: totrans-6845
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] Lazebnik, S., Schmid, C., Ponce, J.: 超越特征袋：用于识别自然场景类别的空间金字塔匹配。在：计算机视觉与模式识别
    (2006)'
- en: '[29] LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard,
    W.,'
  id: totrans-6846
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard,
    W.,'
- en: 'Jackel, L.D.: Backpropagation applied to handwritten zip code recognition.
    Neural Computation 1, 541–551 (1989)'
  id: totrans-6847
  prefs: []
  type: TYPE_NORMAL
  zh: 'Jackel, L.D.: 反向传播应用于手写邮政编码识别。神经计算 1, 541–551 (1989)'
- en: '[30] LeCun, Y., Huang, F.J., Bottou, L.: Learning methods for generic object
    recognition with invariance to pose and lighting. In: Computer Vision and Pattern
    Recognition, vol. 2, pp. 97–104 (2004)'
  id: totrans-6848
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] LeCun, Y., Huang, F.J., Bottou, L.: 用于通用物体识别的学习方法，对姿态和光照具有不变性。在：计算机视觉与模式识别，卷2，页码97–104
    (2004)'
- en: '[31] Lee, H., Grosse, R., Ranganath, R., Ng, A.Y.: Convolutional deep belief
    networks for scalable unsupervised learning of hierarchical representations. In:
    26th International Conference on Machine Learning, pp. 609–616 (2009)'
  id: totrans-6849
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Lee, H., Grosse, R., Ranganath, R., Ng, A.Y.: 卷积深度信念网络用于可扩展的无监督学习层次表示。在：第26届国际机器学习会议，页码609–616
    (2009)'
- en: '[32] Mairal, J., Bach, F., Ponce, J., Sapiro, G.: Online learning for matrix
    factorization and sparse coding. Journal of Machine Learning Research 11, 19–60
    (2010)'
  id: totrans-6850
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] Mairal, J., Bach, F., Ponce, J., Sapiro, G.: 用于矩阵分解和稀疏编码的在线学习。机器学习研究杂志
    11, 19–60 (2010)'
- en: '[33] Nair, V., Hinton, G.E.: Rectified linear units improve restricted boltzmann
    machines. In: 27th International Conference on Machine Learning, pp. 807–814 (2010)'
  id: totrans-6851
  prefs: []
  type: TYPE_NORMAL
  zh: '[33] Nair, V., Hinton, G.E.: 修正线性单元改善受限玻尔兹曼机。在：第27届国际机器学习会议，页码807–814 (2010)'
- en: '[34] Olshausen, B.A., Field, D.J.: Emergence of simple-cell receptive field
    properties by learning a sparse code for natural images. Nature 381(6583), 607–609
    (1996)'
  id: totrans-6852
  prefs: []
  type: TYPE_NORMAL
  zh: '[34] Olshausen, B.A., Field, D.J.: 通过学习自然图像的稀疏编码而出现的简单细胞感受野特性。自然 381(6583),
    607–609 (1996)'
- en: '[35] Raina, R., Battle, A., Lee, H., Packer, B., Ng, A.: Self-taught learning:
    transfer learning from unlabeled data. In: 24th International Conference on Machine
    learning, pp. 759–766 (2007)'
  id: totrans-6853
  prefs: []
  type: TYPE_NORMAL
  zh: '[35] Raina, R., Battle, A., Lee, H., Packer, B., Ng, A.: 自学学习：从未标记数据的迁移学习。在：第24届国际机器学习会议，页码759–766
    (2007)'
- en: '[36] Tibshirani, R.: Regression shrinkage and selection via the lasso. Journal
    of the Royal Statistical Society. Series B (Methodological), 267–288 (1996)'
  id: totrans-6854
  prefs: []
  type: TYPE_NORMAL
  zh: '[36] Tibshirani, R.: 通过套索进行回归收缩和选择。皇家统计学会杂志，B系列（方法论），267–288 (1996)'
- en: '[37] Varma, M., Zisserman, A.: A statistical approach to material classification
    using image patch exemplars. Transactions on Pattern Analysis and Machine Intelligence
    31(11), 2032–2047 (2009)'
  id: totrans-6855
  prefs: []
  type: TYPE_NORMAL
  zh: '[37] Varma, M., Zisserman, A.: 使用图像补丁示例的材料分类统计方法。模式分析与机器智能交易 31(11), 2032–2047
    (2009)'
- en: '[38] Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., Gong, Y.: Locality-constrained
    linear coding for image classification. In: Computer Vision and Pattern Recognition,
    pp. 3360–3367 (2010)'
  id: totrans-6856
  prefs: []
  type: TYPE_NORMAL
  zh: '[38] Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., Gong, Y.: 用于图像分类的局部约束线性编码。在：计算机视觉与模式识别，页码3360–3367
    (2010)'
- en: '[39] Yang, J., Yu, K., Gong, Y., Huang, T.S.: Linear spatial pyramid matching
    using sparse coding for image classification. In: Computer Vision and Pattern
    Recognition, pp. 1794–1801 (2009)'
  id: totrans-6857
  prefs: []
  type: TYPE_NORMAL
  zh: '[39] Yang, J., Yu, K., Gong, Y., Huang, T.S.: 使用稀疏编码进行图像分类的线性空间金字塔匹配。在：计算机视觉与模式识别，页码1794–1801
    (2009)'
- en: '[40] Yu, K., Zhang, T., Gong, Y.: Nonlinear learning using local coordinate
    coding.'
  id: totrans-6858
  prefs: []
  type: TYPE_NORMAL
  zh: '[40] Yu, K., Zhang, T., Gong, Y.: 使用局部坐标编码的非线性学习。'
- en: 'In: Advances in Neural Information Processing Systems, vol. 22, pp. 2223–2231'
  id: totrans-6859
  prefs: []
  type: TYPE_NORMAL
  zh: 在：神经信息处理系统进展，卷22，页码2223–2231
- en: (2009)
  id: totrans-6860
  prefs: []
  type: TYPE_NORMAL
  zh: (2009)
- en: '[41] Zetzsche, C., Krieger, G., Wegmann, B.: The atoms of vision: Cartesian
    or polar?'
  id: totrans-6861
  prefs: []
  type: TYPE_NORMAL
  zh: '[41] Zetzsche, C., Krieger, G., Wegmann, B.: 视觉的原子：笛卡尔还是极坐标？'
- en: Journal of the Optical Society of America 16(7), 1554–1565 (1999)
  id: totrans-6862
  prefs: []
  type: TYPE_NORMAL
  zh: 美国光学学会杂志 16(7), 1554–1565 (1999)
- en: '[42] Zhou, X., Yu, K., Zhang, T., Huang, T.S.: Image Classification Using Super-Vector
    Coding of Local Image Descriptors. In: Daniilidis, K., Maragos, P., Paragios,
    N. (eds.) ECCV 2010, Part V. LNCS, vol. 6315, pp. 141–154. Springer, Heidelberg
    (2010)'
  id: totrans-6863
  prefs: []
  type: TYPE_NORMAL
  zh: '[42] Zhou, X., Yu, K., Zhang, T., Huang, T.S.: 使用局部图像描述符的超向量编码进行图像分类。在：Daniilidis,
    K., Maragos, P., Paragios, N. (主编) ECCV 2010，第五部分。LNCS, 卷6315，页码141–154。施普林格，海德堡
    (2010)'
- en: 23 Deep Big Multilayer Perceptrons For Digit Recognition
  id: totrans-6864
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 23 深度大多层感知器用于数字识别
- en: Dan Claudiu Cireşan1,2, Ueli Meier1,2, Luca Maria Gambardella1,2, and Jürgen
    Schmidhuber1,2 1 IDSIA, Galleria 2, 6928 Manno-Lugano, Switzerland 2 University
    of Lugano & SUPSI, Switzerland Abstract. The competitive MNIST handwritten digit
    recognition benchmark has a long history of broken records since 1998. The most
    recent advancement by others dates back 8 years (error rate 0.4%). Good old on-line
    back-propagation for plain multi-layer perceptrons yields a very low 0.35% error
    rate on the MNIST handwritten digits benchmark with a single MLP and 0.31% with
    a committee of seven MLP. All we need to achieve this until 2011 best result are
    many hidden layers, many neurons per layer, numerous deformed training images
    to avoid overfitting, and graphics cards to greatly speed up learning.
  id: totrans-6865
  prefs: []
  type: TYPE_NORMAL
  zh: Dan Claudiu Cireşan1,2, Ueli Meier1,2, Luca Maria Gambardella1,2, 和 Jürgen Schmidhuber1,2
    1 IDSIA, Galleria 2, 6928 Manno-Lugano, Switzerland 2 卢加诺大学与SUPSI，瑞士 摘要：竞争性的MNIST手写数字识别基准自1998年以来有着悠久的破纪录历史。其他人的最新进展可追溯到8年前（错误率0.4%）。经典的在线反向传播对于普通多层感知器在MNIST手写数字基准上取得了非常低的0.35%错误率，单个MLP为0.31%，而七个MLP的委员会。要在2011年之前达到最佳结果，我们只需许多隐藏层、每层许多神经元、大量变形训练图像以避免过拟合，以及图形卡以大大加速学习。
- en: 'Keywords: NN (Neural Network), MLP (Multilayer Perceptron), GPU'
  id: totrans-6866
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词：NN（神经网络）、MLP（多层感知器）、GPU
- en: (Graphics Processing Unit), training set deformations, MNIST1, committee, BP
    (back-propagation).
  id: totrans-6867
  prefs: []
  type: TYPE_NORMAL
  zh: （图形处理单元）、训练集变形、MNIST1、委员会、BP（反向传播）。
- en: 'Note: This work combines three previously published papers [6, 7, 22].'
  id: totrans-6868
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这项工作结合了三篇之前发表的论文[6, 7, 22]。
- en: 23.1 Introduction
  id: totrans-6869
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 23.1 引言
- en: Automatic handwriting recognition is of academic and commercial interest. Current
    algorithms are already pretty good at learning to recognize handwritten digits.
    Post offices use them to sort letters; banks use them to read personal checks.
    MNIST [21] is the most widely used benchmark for isolated handwritten digit recognition.
    More than a decade ago, artificial neural networks called Multilayer Perceptrons
    or MLPs [35, 20, 29] were among the first classifiers tested on MNIST. Most had
    few layers or few artificial neurons (units) per layer [21], but apparently back
    then they were the biggest feasible MLPs, trained when CPU cores were at least
    20 times slower than today. A more recent MLP with a single hidden layer of 800
    units achieved 0.70% error [33]. However, more complex methods listed on the MNIST
    web page always seemed to outperform MLPs, and the general trend went towards
    more and more complex variants of Support Vector
  id: totrans-6870
  prefs: []
  type: TYPE_NORMAL
  zh: 自动手写识别在学术和商业上都引起了兴趣。当前的算法已经相当擅长学习识别手写数字。邮局使用它们来分类信件；银行用它们来读取个人支票。MNIST[21]是孤立手写数字识别中最广泛使用的基准。十多年前，称为多层感知器或MLP的人工神经网络[35,
    20, 29]是首次在MNIST上测试的分类器之一。大多数在每层中只有少数层或少量人工神经元（单元）[21]，但显然在当时它们是最大的可行MLP，训练时CPU内核的速度至少比今天慢20倍。一个较新的单隐层800单元的MLP达到了0.70%的错误率[33]。然而，MNIST网页上列出的更复杂的方法似乎总是超越MLP，而整体趋势则趋向于越来越复杂的支持向量变体。
- en: 1 http://yann.lecun.com/exdb/mnist/
  id: totrans-6871
  prefs: []
  type: TYPE_NORMAL
  zh: 1 http://yann.lecun.com/exdb/mnist/
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    581–598, 2012.'
  id: totrans-6872
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon等（编）：NN：行业技巧，第2版，LNCS 7700，第581–598页，2012年。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-6873
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: Machines or SVMs [13] and combinations of NNs and SVMs [19] etc. Convolutional
    neural networks (CNNs) achieved a record-breaking 0.40% error rate [33],
  id: totrans-6874
  prefs: []
  type: TYPE_NORMAL
  zh: 机器或支持向量机[13]以及神经网络和支持向量机的组合[19]等。卷积神经网络（CNN）达到了创纪录的0.40%错误率[33]。
- en: using novel elastic training image deformations. Recent methods pre-train each
    hidden CNN layer one by one in an unsupervised fashion (this seems promising especially
    for small training sets), then use supervised learning to achieve 0.39% error
    rate [26, 27]. The biggest MLP so far [31] also was pre-trained without supervision,
    then piped its output into another classifier to achieve an error of 1% without
    domain-specific knowledge. Deep MLPs initialized by unsupervised pretraining were
    also successfully applied to speech recognition [23].
  id: totrans-6875
  prefs: []
  type: TYPE_NORMAL
  zh: 使用新颖的弹性训练图像变形。最近的方法以无监督的方式逐层预训练每个隐藏CNN层（这对于小型训练集尤其有希望），然后使用监督学习实现0.39%的错误率[26,
    27]。迄今为止最大的MLP[31]也没有经过监督的预训练，然后将其输出输入到另一个分类器中，以实现1%的错误率，而不需要领域特定知识。经过无监督预训练初始化的深度MLP也成功应用于语音识别[23]。
- en: Are all these complexifications of plain MLPs really necessary? Can't one simply
    train really big plain MLPs on MNIST? One reason is that at first glance deep
    MLPs do not seem to work better than shallow networks [1]. Training them is hard
    as back-propagated gradients quickly vanish exponentially in the number of layers
    [16, 18, 15], just like in the first recurrent neural networks [17]. Indeed, previous
    deep networks successfully trained with back-propagation (BP) either had few free
    parameters due to weight-sharing [21, 33] or used unsupervised, layer-wise pre-training
    [14, 1, 26]. But is it really true that deep BP-MLPs do not work at all, or do
    they just need more training time? How to test this? Unfortunately, on-line BP
    for hundreds/thousands of epochs on large MLPs may take weeks or months on standard
    serial computers. But can't one parallelize it? Well, on computer clusters this
    is hard due to communication latencies between individual computers. Parallelization
    across training cases and weight updates for mini-batches [24] might alleviate
    this problem, but still leaves the task of parallelizing fully online-BP. Only
    GPUs are capable of such fine grained parallelism. Multi-threading on a multi-core
    processor is not easy either. We may speed up BP using SSE (Streaming Single Instruction,
    Multiple Data Extensions), either manually, or by setting appropriate compiler
    flags. The maximum theoretical speedup under single precision floating-point,
    however, is four, which is not enough. And MNIST is large - its 60,000 images
    take almost 50MB, too much to fit in the L2/L3 cache of any current processor.
    This requires to continually access data in considerably slower RAM. To summarize,
    currently it is next to impossible to train big MLPs on CPUs.
  id: totrans-6876
  prefs: []
  type: TYPE_NORMAL
  zh: 这些对普通多层感知器（MLP）的复杂化真的有必要吗？难道不能简单地在 MNIST 上训练真正大的普通 MLP 吗？一个原因是，从表面上看，深度 MLP
    的表现似乎并不比浅层网络好[1]。训练它们很困难，因为反向传播的梯度在层数增加时会迅速以指数形式消失[16, 18, 15]，就像在最早的递归神经网络中一样[17]。实际上，之前成功使用反向传播（BP）训练的深度网络，要么由于权重共享而只有少量自由参数[21,
    33]，要么使用了无监督的逐层预训练[14, 1, 26]。但是，深度 BP-MLP 确实完全无效，还是说它们只是需要更多的训练时间？该如何测试这一点？不幸的是，在大型
    MLP 上进行数百或数千个周期的在线 BP 可能需要数周或数月的时间，在标准串行计算机上。然而，难道不能进行并行化吗？在计算机集群上，由于各个计算机之间的通信延迟，这很困难。跨训练案例和小批量的权重更新进行并行化[24]可能缓解这个问题，但仍然需要在线
    BP 的完全并行化。只有 GPU 才能实现如此细粒度的并行处理。在多核处理器上进行多线程操作也并不简单。我们可以通过手动或设置适当的编译器标志，利用 SSE（流式单指令多数据扩展）加速
    BP。然而，单精度浮点下的最大理论加速比是四倍，这还不够。而 MNIST 数据集很大——其 60,000 张图像几乎占用 50MB，远超过任何当前处理器的
    L2/L3 缓存。这就要求不断访问速度较慢的 RAM 数据。总之，目前在 CPU 上训练大型 MLP 几乎是不可能的。
- en: We showed how to overcome all these problems by training large, deep MLPs on
    graphics cards [6] and obtained an error rate of 0.35% with a deep MLP.
  id: totrans-6877
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了如何通过在图形卡上训练大型深度 MLP 来克服所有这些问题[6]，并获得了 0.35% 的错误率。
- en: Deformations proved essential to prevent MLPs with up to 12 million free parameters
    from overfitting. To let the deformation process keep up with network training
    speed we had to port it onto the GPU as well.
  id: totrans-6878
  prefs: []
  type: TYPE_NORMAL
  zh: 变形被证明对防止拥有多达 1200 万自由参数的 MLP 过拟合至关重要。为了让变形过程跟上网络训练的速度，我们还必须将其移植到 GPU 上。
- en: At some stage in the classifier design process one usually has collected a set
    of possible classifiers. Often one of them yields best performance. Intriguingly,
    however, the sets of patterns misclassified by the different classifiers do not
    necessarily overlap. This information could be harnessed in a committee. In the
    context of handwritten recognition it was already shown [4] how a combination
    of various classifiers can be trained more quickly than a single classifier yielding
    the same error rate. Here we focus on improving recognition rate using a committee
    of MLP. Our goal is to produce a group of classifiers whose errors on various
    parts of the training set differ (are uncorrelated) as much as possible [2].
  id: totrans-6879
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类器设计过程中的某个阶段，通常会收集一组可能的分类器。通常，其中一个会表现出最佳性能。然而，令人感兴趣的是，不同分类器错误分类的模式集合并不一定重叠。这些信息可以在委员会中加以利用。在手写识别的背景下，已有研究显示[4]，多种分类器的组合训练速度比单一分类器快，同时保持相同的错误率。这里，我们专注于使用
    MLP 委员会提高识别率。我们的目标是产生一组分类器，使其在训练集的不同部分的错误尽可能不同（不相关）[2]。
- en: We show that for handwritten digit recognition this can be achieved by training
    identical classifiers on data normalized in different ways prior to training.
  id: totrans-6880
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示，对于手写数字识别，可以通过在训练前以不同方式规范化数据来训练相同的分类器来实现。
- en: 23.2 Data
  id: totrans-6881
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 23.2 数据
- en: MNIST consists of two datasets, one for training (60,000 images) and one for
    testing (10,000 images). Many studies divide the training set into two sets consisting
    of 50,000 images for training and 10,000 for validation. Our network is trained
    on slightly deformed images, continually generated in on-line fashion; hence we
    may use the whole un-deformed training set for validation, without wasting training
    images. Pixel intensities of the original gray scale images range from 0 (background)
    to 255 (max foreground intensity). 28×28 = 784 pixels per image get mapped to
    real values *pixel intensity* 127.5 − 1.0 in [−1.0, 1.0], and are fed into the
    NN input layer.
  id: totrans-6882
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST由两个数据集组成，一个用于训练（60,000张图像）和一个用于测试（10,000张图像）。许多研究将训练集分为两个集合，分别由50,000张图像用于训练，10,000张图像用于验证。我们的网络在稍微变形的图像上进行训练，这些图像以在线方式持续生成；因此，我们可以将整个未变形的训练集用于验证，而不浪费训练图像。原始灰度图像的像素强度范围从0（背景）到255（最大前景强度）。每张图像的28×28
    = 784个像素被映射到实值*像素强度* 127.5 − 1.0在[−1.0, 1.0]之间，并输入到神经网络输入层。
- en: 23.3 Architectures
  id: totrans-6883
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 23.3 架构
- en: We train 5 MLPs with 2 to 9 hidden layers and varying numbers of hidden units.
  id: totrans-6884
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练5个多层感知机（MLP），具有2到9个隐藏层和不同数量的隐藏单元。
- en: Mostly but not always the number of hidden units per layer decreases towards
    the output layer (Table 23.3). There are 1.34 to 12.11 million free parameters
    (or weights, or synapses).
  id: totrans-6885
  prefs: []
  type: TYPE_NORMAL
  zh: 通常但并非总是，每层的隐藏单元数量向输出层减少（表23.3）。有134万到1211万自由参数（或权重，或突触）。
- en: 'We use standard on-line BP [30] , without momentum, but with a variable learning
    rate that shrinks by a multiplicative constant after each epoch, from 10−3 down
    to 10−6. Weights are initialized with a uniform random distribution in [−0.05,
    0.05]. Each neuron''s activation function is a scaled hyperbolic tangent:'
  id: totrans-6886
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用标准的在线BP [30]，没有动量，但使用一个在每个周期后以乘法常数缩小的可变学习率，从10−3降到10−6。权重初始化为均匀随机分布在[−0.05,
    0.05]之间。每个神经元的激活函数是一个缩放的双曲正切：
- en: y(a) = A tanh Ba, where A = 1.7159 and B = 0.6666 [21], and a softmax output
    layer is used. Weight initialization and annealing rate are not overly important
    as long as sensible choices are made.
  id: totrans-6887
  prefs: []
  type: TYPE_NORMAL
  zh: y(a) = A tanh Ba，其中A = 1.7159，B = 0.6666 [21]，并使用softmax输出层。权重初始化和退火率不是特别重要，只要做出合理的选择即可。
- en: 23.4 Deforming Images To Get More Training Instances
  id: totrans-6888
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 23.4 变形图像以获得更多训练实例
- en: 'So far, the best results on MNIST were obtained by deforming training images
    [33], thus greatly increasing their number. This allows for training networks
    with many weights without overfitting. We combine affine (rotation, scaling and
    horizontal shearing) and elastic deformations (Figure 23.1), characterized by
    the following real-valued parameters:'
  id: totrans-6889
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，MNIST上获得的最佳结果是通过变形训练图像得到的[33]，这大大增加了它们的数量。这使得可以在不发生过拟合的情况下训练具有许多权重的网络。我们结合了仿射变换（旋转、缩放和水平剪切）和弹性变形（图23.1），其特征由以下实值参数定义：
- en: '- σ and α: for elastic distortions emulating uncontrolled oscillations of hand
    muscles [33];'
  id: totrans-6890
  prefs: []
  type: TYPE_NORMAL
  zh: '- σ和α：用于模拟手部肌肉不受控制的震动的弹性扭曲[33]；'
- en: '- β: a random angle from [−β, +β] describes either rotation or horizontal shearing.
    In case of shearing, tan β defines the ratio between horizontal displacement and
    image height;'
  id: totrans-6891
  prefs: []
  type: TYPE_NORMAL
  zh: '- β：随机角度从[−β, +β]描述旋转或水平剪切。在剪切的情况下，tan β定义水平位移与图像高度之间的比率；'
- en: '- γx, γy: for horizontal and vertical scaling, randomly selected from [1 −'
  id: totrans-6892
  prefs: []
  type: TYPE_NORMAL
  zh: '- γx，γy：用于水平和垂直缩放，随机选择自[1 −'
- en: γ/100, 1 + γ/100].
  id: totrans-6893
  prefs: []
  type: TYPE_NORMAL
  zh: γ/100，1 + γ/100].
- en: '![574_image_0.png](574_image_0.png)'
  id: totrans-6894
  prefs: []
  type: TYPE_IMG
  zh: '![574_image_0.png](574_image_0.png)'
- en: Fig. 23.1. Original digit (top) and distorted digits (bottom). The digit was
    distorted with four different displacement fields shown in the middle.
  id: totrans-6895
  prefs: []
  type: TYPE_NORMAL
  zh: 图23.1. 原始数字（上）和扭曲数字（下）。数字被四个不同的位移场扭曲，显示在中间。
- en: 'Each affine deformation is fully defined by the corresponding real-valued parameter
    that is randomly drawn from a uniform interval. Building the elastic deformation
    field on the other hand consists of three parts: 1) create an initial random distortion
    vector field, 2) smooth the random distortion field by convolving it with a Gaussian
    kernel defined by a standard deviation σ, and 3) scale the smoothed deformation
    field with α, the elastic scaling parameter.'
  id: totrans-6896
  prefs: []
  type: TYPE_NORMAL
  zh: 每个仿射变形由从均匀区间随机抽取的对应实值参数完全定义。另一方面，构建弹性变形场由三个部分组成：1）创建初始随机扭曲向量场，2）通过与标准差σ定义的高斯核卷积来平滑随机扭曲场，3）用弹性缩放参数α缩放平滑的变形场。
- en: 'At the beginning of every epoch the entire original MNIST training set gets
    deformed. Initial experiments with small networks suggested the following deformation
    parameters: σ = 5.0 − 6.0, α = 36.0 − 38.0, γ = 15 − 20. Since digits 1 and 7
    are similar they get rotated/sheared less (β = 7.5◦) than other digits'
  id: totrans-6897
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个周期开始时，整个原始MNIST训练集会被变形。对小型网络的初步实验建议了以下变形参数：σ = 5.0 − 6.0，α = 36.0 − 38.0，γ
    = 15 − 20。由于数字1和7相似，它们的旋转/剪切少于其他数字（β = 7.5◦）。
- en: (β = 15.0◦).
  id: totrans-6898
  prefs: []
  type: TYPE_NORMAL
  zh: （β = 15.0◦）。
- en: It takes 83 CPU seconds to deform the 60,000 MNIST training images, most of
    them (75 seconds) for elastic distortions. Only the most time-consuming part of
    the latter—convolution with a Gaussian kernel—is ported to the GPU. The MNIST
    training set is split into 600 sequentially processed minibatches of 100 samples
    each. MNIST digits are scaled from the original 28×28 pixels to 29×29 pixels,
    to get a proper center, which simplifies convolution. Each batch grid
  id: totrans-6899
  prefs: []
  type: TYPE_NORMAL
  zh: 对60,000个MNIST训练图像进行变形需要83个CPU秒，其中大部分（75秒）用于弹性扭曲。只有后者中最耗时的部分——与高斯核的卷积——被移植到GPU。MNIST训练集被分成600个连续处理的小批次，每个批次100个样本。MNIST数字从原始的28×28像素缩放到29×29像素，以获得适当的中心，从而简化卷积。每个批次网格
- en: (10 × 10 images) has 290 × 290 cells, zero-padded to 310 × 310, thus avoiding
    margin effects when applying a Gaussian convolution kernel of size 21 × 21.
  id: totrans-6900
  prefs: []
  type: TYPE_NORMAL
  zh: （10 × 10图像）具有290 × 290个单元，零填充到310 × 310，从而避免在应用大小为21 × 21的高斯卷积核时出现边缘效应。
- en: The GPU program groups many threads into a block, where they share the same
  id: totrans-6901
  prefs: []
  type: TYPE_NORMAL
  zh: GPU程序将许多线程分组到一个块中，在这里它们共享相同的
- en: '![575_image_0.png](575_image_0.png)'
  id: totrans-6902
  prefs: []
  type: TYPE_IMG
  zh: '![575_image_0.png](575_image_0.png)'
- en: Fig. 23.2. Mapping the thread grid of convolution onto the distortion field
  id: totrans-6903
  prefs: []
  type: TYPE_NORMAL
  zh: 图23.2。将卷积的线程网格映射到扭曲场上
- en: Gaussian kernel and parts of the random field. All 29 × 290 blocks contain 21
    (the kernel size) ×10 threads, each computing a vertical strip of the convolution
  id: totrans-6904
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯核和随机场的部分。所有29 × 290的块包含21（核大小）×10个线程，每个线程计算卷积的一个垂直条带。
- en: (Figure 23.2). Generating the elastic displacement field takes only 1.5 seconds.
  id: totrans-6905
  prefs: []
  type: TYPE_NORMAL
  zh: （图23.2）。生成弹性位移场只需1.5秒。
- en: Deforming the whole training set is more than 10 times faster, taking 6.5 instead
    of the original 83 seconds. Further optimizations would be possible by porting
    all deformations onto GPU, and by using the hardware's interpolation capabilities
    to perform the final bilinear interpolation. We omitted these since deformations
    are already pretty fast (deforming all images of one epoch takes only 3-10 % of
    total computation time, depending on MLP size).
  id: totrans-6906
  prefs: []
  type: TYPE_NORMAL
  zh: 变形整个训练集的速度超过原来的10倍，仅需6.5秒，而不是83秒。通过将所有变形移植到GPU上，并利用硬件的插值能力进行最终的双线性插值，可以实现进一步优化。我们省略了这些，因为变形已经相当快速（一个周期中所有图像的变形仅占总计算时间的3-10％，具体取决于MLP大小）。
- en: 23.5 Forming A Committee
  id: totrans-6907
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 23.5 组建委员会
- en: The training procedure of a single network of the committee is summarized in
    Figure 23.3. Each network is trained separately on normalized or original data.
  id: totrans-6908
  prefs: []
  type: TYPE_NORMAL
  zh: 委员会中单个网络的训练过程如图23.3所示。每个网络分别在标准化或原始数据上进行训练。
- en: The normalization is done for all digits in the training set prior to training
  id: totrans-6909
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，所有训练集中数字的归一化都已完成。
- en: (normalization stage). For the network trained on original MNIST data the normalization
    step is omitted. Normalization of the original MNIST data is mainly motivated
    by practical experience. MNIST digits are already normalized such that the width
    or height of the bounding box equals 20 pixels. The variation of the aspect ratio
    for various digits is quite large, and we normalize the width of the bounding
    box to range from 10 to 20 pixels with a step-size of 2 pixels prior to training
    for all digits except ones. Normalizing the original MNIST training data results
    in 6 normalized training sets. In total we perform experiments with seven different
    data sets (6 normalized and the original MNIST).
  id: totrans-6910
  prefs: []
  type: TYPE_NORMAL
  zh: （归一化阶段）。对于在原始 MNIST 数据上训练的网络，省略了归一化步骤。原始 MNIST 数据的归一化主要是出于实践经验。MNIST 数字已归一化，使得边界框的宽度或高度等于
    20 像素。不同数字的宽高比变化较大，我们将边界框的宽度归一化为从 10 到 20 像素，步长为 2 像素，除了一的数字。在原始 MNIST 训练数据归一化后，得到了
    6 个归一化训练集。总共我们用七种不同的数据集进行了实验（6 个归一化数据集和原始 MNIST）。
- en: We perform six experiments to analyze performance improvements due to committees.
    Each committee consists of seven randomly initialized one-hiddenlayer MLPs with
    800 hidden units, trained with the same algorithm on randomly
  id: totrans-6911
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了六个实验，以分析委员会带来的性能改进。每个委员会由七个随机初始化的单隐层 MLP 组成，具有 800 个隐层单元，使用相同的算法在随机选定的批次上训练。
- en: '![576_image_0.png](576_image_0.png)'
  id: totrans-6912
  prefs: []
  type: TYPE_IMG
  zh: '![576_image_0.png](576_image_0.png)'
- en: Fig. 23.3. Training a committee member. Original MNIST training data (left digit)
  id: totrans-6913
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23.3. 训练一个委员会成员。原始 MNIST 训练数据（左侧数字）
- en: '![576_image_1.png](576_image_1.png)'
  id: totrans-6914
  prefs: []
  type: TYPE_IMG
  zh: '![576_image_1.png](576_image_1.png)'
- en: is normalized (W10) prior to training (middle digit). The normalized data is
    distorted
  id: totrans-6915
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，数据被归一化（W10）（中间数字）。归一化的数据被扭曲
- en: (D) for each training epoch and used as input (right digit) to the network (NN).
    Each depicted digit represents the whole training set. Fig. 23.4. Testing with
    a committee. If required, the input digits are width-normalized
  id: totrans-6916
  prefs: []
  type: TYPE_NORMAL
  zh: （D）在每个训练周期使用并作为输入（右侧数字）输入到网络（NN）。每个描绘的数字代表整个训练集。图 23.4. 使用委员会进行测试。如有需要，输入数字被宽度归一化。
- en: (W blocks) and then processed by the corresponding MLP. The committee is formed
    by averaging the outputs of all MLPs.
  id: totrans-6917
  prefs: []
  type: TYPE_NORMAL
  zh: （W 块）然后由相应的 MLP 处理。委员会通过平均所有 MLP 的输出形成。
- en: selected batches. The six committees differ only in how the data are normalized
    (or not) prior to training and on how the data are deformed during training.
  id: totrans-6918
  prefs: []
  type: TYPE_NORMAL
  zh: 选定的批次。这六个委员会仅在训练前数据的归一化方式（或不归一化）以及训练期间数据的变形方式上有所不同。
- en: The committees are formed by simply averaging the corresponding outputs as shown
    in Figure 23.4.
  id: totrans-6919
  prefs: []
  type: TYPE_NORMAL
  zh: 委员会是通过简单地平均对应的输出形成的，如图 23.4 所示。
- en: The first two experiments are performed on undeformed original MNIST images.
    We train a committee of seven MLPs on original MNIST and we also form a committee
    of MLPs trained on normalized data. In Table 23.1 the error rates are listed for
    each of the individual nets and the committees. The improvement of the committee
    with respect to the individual nets is marginal for the first experiment. Adding
    normalization, the individual experts as well as the corresponding committee of
    the second experiment achieve substantially better recognition rates.
  id: totrans-6920
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个实验是在未变形的原始 MNIST 图像上进行的。我们在原始 MNIST 上训练了七个 MLP 的委员会，同时也形成了一个在归一化数据上训练的 MLP
    委员会。在表 23.1 中列出了每个单独网络和委员会的错误率。对于第一个实验，委员会相对于单独网络的改进微乎其微。添加归一化后，单独专家以及第二个实验对应的委员会实现了显著更好的识别率。
- en: To study the combined effect of normalization and deformation, we perform four
    additional experiments on deformed MNIST (Tab. 23.2). Unless stated
  id: totrans-6921
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究归一化和变形的结合效果，我们在变形的 MNIST 上进行了四个额外的实验（表 23.2）。除非另有说明。
- en: 'Table 23.1. Error rates of individual nets and of the two resulting committees.
    For experiment 1 seven randomly initialized nets are trained on the original MNIST,
    whereas for experiment 2 seven randomly initialized nets are trained on width-normalized
    data: WN x - Width Normalization of the bounding box to be x pixels wide; ORIG
    - original MNIST.'
  id: totrans-6922
  prefs: []
  type: TYPE_NORMAL
  zh: 表 23.1. 单个网络和两个结果委员会的错误率。对于实验 1，七个随机初始化的网络在原始 MNIST 上训练，而对于实验 2，七个随机初始化的网络在宽度归一化数据上训练：WN
    x - 将边界框归一化到 x 像素宽；ORIG - 原始 MNIST。
- en: '|          |                          | Error rate [%]   |        |'
  id: totrans-6923
  prefs: []
  type: TYPE_TB
  zh: '|          |                          | 错误率 [%]       |        |'
- en: '| --- | --- | --- | --- |'
  id: totrans-6924
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|          |                          | Exp. 1           | Exp. 2 |'
  id: totrans-6925
  prefs: []
  type: TYPE_TB
  zh: '|          |                          | 实验 1           | 实验 2 |'
- en: '| Net 1:   | init 1: 1.79 WN 10: 1.62 |                  |        |'
  id: totrans-6926
  prefs: []
  type: TYPE_TB
  zh: '| 网络 1:   | 初始化 1: 1.79 WN 10: 1.62 |                  |        |'
- en: '| Net 2:   | init 2: 1.80 WN 12: 1.37 |                  |        |'
  id: totrans-6927
  prefs: []
  type: TYPE_TB
  zh: '| 网络 2:   | 初始化 2: 1.80 WN 12: 1.37 |                  |        |'
- en: '| Net 3:   | init 3: 1.77 WN 14: 1.48 |                  |        |'
  id: totrans-6928
  prefs: []
  type: TYPE_TB
  zh: '| 网络 3:   | 初始化 3: 1.77 WN 14: 1.48 |                  |        |'
- en: '| Net 4:   | init 4: 1.72 WN 16: 1.53 |                  |        |'
  id: totrans-6929
  prefs: []
  type: TYPE_TB
  zh: '| 网络 4:   | 初始化 4: 1.72 WN 16: 1.53 |                  |        |'
- en: '| Net 5:   | init 5: 1.91 WN 18: 1.56 |                  |        |'
  id: totrans-6930
  prefs: []
  type: TYPE_TB
  zh: '| 网络 5:   | 初始化 5: 1.91 WN 18: 1.56 |                  |        |'
- en: '| Net 6:   | init 6: 1.86 WN 20: 1.49 |                  |        |'
  id: totrans-6931
  prefs: []
  type: TYPE_TB
  zh: '| 网络 6:   | 初始化 6: 1.86 WN 20: 1.49 |                  |        |'
- en: '| Net 7:   | init 7: 1.75 ORIG:       | 1.79             |        |'
  id: totrans-6932
  prefs: []
  type: TYPE_TB
  zh: '| 网络 7:   | 初始化 7: 1.75 ORIG:       | 1.79             |        |'
- en: '| Average: |                          | 1.70             | 1.31   |'
  id: totrans-6933
  prefs: []
  type: TYPE_TB
  zh: '| 平均: |                          | 1.70             | 1.31   |'
- en: otherwise, default elastic deformation parameters σ = 6 and α = 36 are used.
  id: totrans-6934
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，使用默认的弹性变形参数 σ = 6 和 α = 36。
- en: All experiments with deformed images use independent horizontal and vertical
    scaling of maximum 12.5% and a maximum rotation of ±12.5◦. Experiment 3 is similar
    to Experiment 1, except that the data are continually deformed. Error rates of
    the individual experts are much lower than without deformation (Tab. 23.1). In
    experiment 4 we randomly reselect training and validation sets for each of the
    individual experts, simulating in this way the bootstrap aggregation technique
    [3]. The resulting committee performs slightly better than that of experiment
    3. In experiment 5 we vary deformations for each individual network.
  id: totrans-6935
  prefs: []
  type: TYPE_NORMAL
  zh: 所有使用变形图像的实验都采用独立的水平和垂直缩放，最大 12.5% 和最大旋转 ±12.5◦。实验 3 类似于实验 1，不同之处在于数据持续变形。各个专家的错误率远低于没有变形的情况（表
    23.1）。在实验 4 中，我们随机重新选择每个个别专家的训练和验证集，以这种方式模拟引导聚合技术 [3]。结果委员会的表现稍好于实验 3。在实验 5 中，我们对每个个体网络的变形进行变化。
- en: Error rates of some of the nets are bigger than in experiments 3 and 4, but
    the resulting committee has a lower error rate. In the last experiment we train
    seven MLPs on width-normalized images that are also continually deformed during
    training. The error rate of the committee (0.43 %) is the best result ever reported
    for such a simple architecture. We conclude that width-normalization is essential
    for good committee performance, i.e. it is not enough to form a committee from
    trained nets with different initializations (experiment 3) or trained on subsets
    of the original dataset (experiment 4).
  id: totrans-6936
  prefs: []
  type: TYPE_NORMAL
  zh: 一些网络的错误率大于实验 3 和 4，但最终形成的委员会具有更低的错误率。在最后的实验中，我们在宽度归一化的图像上训练七个 MLP，并且这些图像在训练期间也不断变形。委员会的错误率（0.43%）是迄今为止对于如此简单架构报告的最佳结果。我们得出结论，宽度归一化对于良好的委员会性能至关重要，即仅从不同初始化的训练网络（实验
    3）或在原始数据集子集上训练的网络（实验 4）形成委员会是不够的。
- en: 23.6 Using The Gpu To Train Deep Mlps
  id: totrans-6937
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 23.6 使用 GPU 训练深度 MLPs
- en: Using simple tricks, such as creating a virtually infinite amount of training
    data through random distortions at the beginning of every epoch and forming a
    committee of experts trained on differently preprocessed data, state-of-the art
    results are obtained on MNIST with a relatively small (800 hidden units) single
    hidden layer MLP. Here we report results using deep MLPs, with as many as 5 hidden
    layers and up to 12 millions of free parameters, that are prohibitive to train
    on
  id: totrans-6938
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一些简单的技巧，比如在每个时期开始时通过随机扭曲创建几乎无限的训练数据，并形成一个在不同预处理数据上训练的专家委员会，MNIST 上获得了最先进的结果，使用相对较小（800
    个隐藏单元）的单隐藏层 MLP。这里我们报告了使用深度 MLP 的结果，最多有 5 个隐藏层和高达 1200 万个自由参数，这在训练上是不可行的。
- en: Table 23.2. Error rates of the individual nets and of the resulting committees.
    In experiments 3 and 4 seven randomly initialized nets are trained on deformed
    (σ = 6, α = 36) MNIST, whereas in experiment 4 training and validation sets are
    reselected. In experiment 5 seven randomly initialized nets are trained on deformed
    (different σ, α) MNIST, and in experiment 6 seven randomly initialized nets are
    trained on widthnormalized, deformed (σ = 6, α = 36) MNIST. WN x - Width Normalization
    of the bounding box to be x pixels wide; ORIG - original MNIST.
  id: totrans-6939
  prefs: []
  type: TYPE_NORMAL
  zh: 表 23.2. 各个网络及其结果委员会的错误率。在实验 3 和 4 中，七个随机初始化的网络在变形（σ = 6, α = 36）MNIST 上进行训练，而在实验
    4 中，训练和验证集被重新选择。在实验 5 中，七个随机初始化的网络在变形（不同的 σ, α）MNIST 上进行训练，而在实验 6 中，七个随机初始化的网络在宽度归一化、变形（σ
    = 6, α = 36）MNIST 上进行训练。WN x - 将边界框宽度归一化为 x 像素；ORIG - 原始 MNIST。
- en: '|               |         | Error rate [%]   |        |                                  |      |'
  id: totrans-6940
  prefs: []
  type: TYPE_TB
  zh: '|               |         | 错误率 [%]       |        |                                  |      |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-6941
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Exp. 3 Exp. 4 |         | Exp. 5           | Exp. 6 |                                  |      |'
  id: totrans-6942
  prefs: []
  type: TYPE_TB
  zh: '| 实验 3 实验 4 |         | 实验 5           | 实验 6 |                                  |      |'
- en: '| Net 1:        | init 1: | 0.72             | 0.68   | σ = 4.5 α = 36: 0.69
    WN 10: 0.64 |      |'
  id: totrans-6943
  prefs: []
  type: TYPE_TB
  zh: '| 网络 1:        | 初始化 1: | 0.72             | 0.68   | σ = 4.5 α = 36: 0.69
    WN 10: 0.64 |      |'
- en: '| Net 2:        | init 2: | 0.71             | 0.82   | σ = 4.5 α = 42: 0.94
    WN 12: 0.78 |      |'
  id: totrans-6944
  prefs: []
  type: TYPE_TB
  zh: '| 网络 2:        | 初始化 2: | 0.71             | 0.82   | σ = 4.5 α = 42: 0.94
    WN 12: 0.78 |      |'
- en: '| Net 3:        | init 3: | 0.72             | 0.73   | σ = 6.0 α = 30: 0.55
    WN 14: 0.70 |      |'
  id: totrans-6945
  prefs: []
  type: TYPE_TB
  zh: '| 网络 3:        | 初始化 3: | 0.72             | 0.73   | σ = 6.0 α = 30: 0.55
    WN 14: 0.70 |      |'
- en: '| Net 4:        | init 4: | 0.71             | 0.69   | σ = 6.0 α = 36: 0.72
    WN 16: 0.60 |      |'
  id: totrans-6946
  prefs: []
  type: TYPE_TB
  zh: '| 网络 4:        | 初始化 4: | 0.71             | 0.69   | σ = 6.0 α = 36: 0.72
    WN 16: 0.60 |      |'
- en: '| Net 5:        | init 5: | 0.62             | 0.71   | σ = 6.0 α = 42: 0.60
    WN 18: 0.59 |      |'
  id: totrans-6947
  prefs: []
  type: TYPE_TB
  zh: '| 网络 5:        | 初始化 5: | 0.62             | 0.71   | σ = 6.0 α = 42: 0.60
    WN 18: 0.59 |      |'
- en: '| Net 6:        | init 6: | 0.65             | 0.70   | σ = 7.5 α = 30: 0.86
    WN 20: 0.70 |      |'
  id: totrans-6948
  prefs: []
  type: TYPE_TB
  zh: '| 网络 6:        | 初始化 6: | 0.65             | 0.70   | σ = 7.5 α = 30: 0.86
    WN 20: 0.70 |      |'
- en: '| Net 7:        | init 7: | 0.69             | 0.75   | σ = 7.5 α = 36: 0.79
    ORIG:       | 0.71 |'
  id: totrans-6949
  prefs: []
  type: TYPE_TB
  zh: '| 网络 7:        | 初始化 7: | 0.69             | 0.75   | σ = 7.5 α = 36: 0.79
    原始:       | 0.71 |'
- en: '| Average:      | 0.56    | 0.53             | 0.49   | 0.43                             |      |'
  id: totrans-6950
  prefs: []
  type: TYPE_TB
  zh: '| 平均:          | 0.56    | 0.53             | 0.49   | 0.43                             |      |'
- en: current CPUs but can successfully be trained on GPUs in a few days. All simulations
    were performed on a computer with a Core i7 920 2.66GHz processor, 12GB of RAM,
    and a GTX 480 graphics card. The GPU accelerates the deformation routine by a
    factor of 10 (only elastic deformations are GPU-optimized);
  id: totrans-6951
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的 CPU 但可以在几天内成功在 GPU 上训练。所有模拟均在配备 Core i7 920 2.66GHz 处理器、12GB RAM 和 GTX 480
    显卡的计算机上进行。GPU 将变形例程的速度提高了 10 倍（仅弹性变形经过 GPU 优化）；
- en: the forward propagation (FP) and BP routines are sped up by a factor of 50.
    We pick the trained MLP with the lowest validation error, and evaluate it on the
    MNIST test set.
  id: totrans-6952
  prefs: []
  type: TYPE_NORMAL
  zh: 正向传播（FP）和反向传播（BP）例程的速度提高了 50 倍。我们选择验证错误最低的训练 MLP，并在 MNIST 测试集上评估它。
- en: 23.6.1 Single Mlp
  id: totrans-6953
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 23.6.1 单个 Mlp
- en: 'We train various MLP and summarize the results in Table 23.3. Training starts
    with a learning rate of 10−3 multiplied with 0.997 after every epoch until it
    reaches 10−6, thus resulting in more than 2000 epochs, which can be computed in
    a few days even for the biggest net. The best network has an error rate of only
    0.35% (35 out of 10,000 digits). This is better than the best previously published
    results, namely, 0.39% [26] and 0.40% [33], both obtained by more complex methods.
    The 35 misclassified digits are shown in Figure 23.5a. Many of them are ambiguous
    and/or uncharacteristic, with obviously missing parts or strange strokes etc.
    Interestingly, the second guess of the network is correct for 30 out of the 35
    misclassified digits. The best test error of this MLP is even lower (0.32%) and
    may be viewed as the maximum capacity of the network, i.e. what it can learn if
    we do not get the result for the lowest error on validation set. Performance clearly
    profits from adding hidden layers and more units per Table 23.3. Error rates on
    MNIST test set. Architecture: 841 input neurons, hidden layers containing 2500,
    2000, 1500, 1000 and 500 neurons, and 10 outputs. TEfBV - test error for best
    validation, BTE - best test error.'
  id: totrans-6954
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练了多种 MLP，并在表 23.3 中总结了结果。训练以学习率 10−3 开始，每个纪元后乘以 0.997，直到达到 10−6，因此总共进行了超过
    2000 个纪元，即使是最大的网络，也可以在几天内计算完成。最佳网络的错误率仅为 0.35%（10,000 个数字中有 35 个错误分类）。这优于之前发布的最佳结果，即
    0.39% [26] 和 0.40% [33]，这两者均通过更复杂的方法获得。35 个错误分类的数字如图 23.5a 所示。其中许多数字模糊和/或不具特征，明显缺少部分或笔画奇怪等。有趣的是，网络对
    35 个错误分类数字的第二次猜测中，有 30 个是正确的。该 MLP 的最佳测试错误甚至更低（0.32%），可视为网络的最大容量，即在未获得最低验证集错误结果的情况下它能学到的内容。性能显然从增加隐藏层和每层更多单元中受益，见表
    23.3。MNIST 测试集的错误率。架构：841 输入神经元，包含 2500、2000、1500、1000 和 500 个神经元的隐藏层，以及 10 个输出。TEfBV
    - 最佳验证的测试错误，BTE - 最佳测试错误。
- en: '| ID   | architecture                      | TEfBV BTE simulation   | weights   |
    test error [%]   |                          |      |'
  id: totrans-6955
  prefs: []
  type: TYPE_TB
  zh: '| ID   | 架构                          | TEfBV BTE 模拟   | 权重   | 测试错误 [%]   |                          |      |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-6956
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '|      | (number of neurons in each layer) | [%]                    | [%]       |
    time [h]         | [millions] no distortion |      |'
  id: totrans-6957
  prefs: []
  type: TYPE_TB
  zh: '|      | （每层神经元数量） | [%]                    | [%]       | 时间 [h]         |
    [百万] 无失真 |      |'
- en: '| 1    | 1000, 500, 10                     | 0.49                   | 0.44      |
    23.4             | 1.34                     | 1.78 |'
  id: totrans-6958
  prefs: []
  type: TYPE_TB
  zh: '| 1    | 1000, 500, 10                     | 0.49                   | 0.44      |
    23.4             | 1.34                     | 1.78 |'
- en: '| 2    | 1500, 1000, 500, 10               | 0.46                   | 0.40      |
    44.2             | 3.26                     | 1.85 |'
  id: totrans-6959
  prefs: []
  type: TYPE_TB
  zh: '| 2    | 1500, 1000, 500, 10               | 0.46                   | 0.40      |
    44.2             | 3.26                     | 1.85 |'
- en: '| 3    | 2000, 1500, 1000, 500, 10         | 0.41                   | 0.39      |
    66.7             | 6.69                     | 1.73 |'
  id: totrans-6960
  prefs: []
  type: TYPE_TB
  zh: '| 3    | 2000, 1500, 1000, 500, 10         | 0.41                   | 0.39      |
    66.7             | 6.69                     | 1.73 |'
- en: '| 4    | 2500, 2000, 1500, 1000, 500, 10   | 0.35                   | 0.32      |
    114.5            | 12.11                    | 1.71 |'
  id: totrans-6961
  prefs: []
  type: TYPE_TB
  zh: '| 4    | 2500, 2000, 1500, 1000, 500, 10   | 0.35                   | 0.32      |
    114.5            | 12.11                    | 1.71 |'
- en: '| 5    | 9 × 1000, 10                      | 0.44                   | 0.43      |
    107.7            | 8.86                     | 1.81 |'
  id: totrans-6962
  prefs: []
  type: TYPE_TB
  zh: '| 5    | 9 × 1000, 10                      | 0.44                   | 0.43      |
    107.7            | 8.86                     | 1.81 |'
- en: layer. For example, network 5 has more but smaller hidden layers than network
  id: totrans-6963
  prefs: []
  type: TYPE_NORMAL
  zh: 层。例如，网络5的隐藏层数量更多但每层神经元较小，与网络
- en: '![579_image_0.png](579_image_0.png) 4 (Table 23.3).'
  id: totrans-6964
  prefs: []
  type: TYPE_NORMAL
  zh: '![579_image_0.png](579_image_0.png) 4（表23.3）。'
- en: 'Networks with up to 12 million weights can successfully be trained by plain
    gradient descent to achieve test errors below 1% after 20-30 epochs in less than
    2 hours of training. How can networks with so many parameters generalize well
    on the unseen test set? Answer: the continual deformations of the training set
    generate a virtually infinite supply of training examples, and the network rarely
    sees any training image twice. Without any distortions, the error for all networks
    is around 1.7-1.8% (last column in Table 23.3).'
  id: totrans-6965
  prefs: []
  type: TYPE_NORMAL
  zh: 最多拥有1200万权重的网络可以通过普通的梯度下降成功训练，以在20-30个周期内实现低于1%的测试错误，训练时间少于2小时。如此多参数的网络如何在未见过的测试集上良好泛化？答案是：训练集的持续变形生成了几乎无限的训练示例，网络很少看到任何训练图像两次。没有任何失真时，所有网络的错误率约为1.7-1.8%（表23.3的最后一列）。
- en: (a) (b)
  id: totrans-6966
  prefs: []
  type: TYPE_NORMAL
  zh: (a) (b)
- en: 'Fig. 23.5. The misclassified digits, together with the two most likely predictions
    (bottom, from left to right) and the correct label according to MNIST (top, right):
    (a) the best network from Table 23.3. (b) the committee from Table 23.4.'
  id: totrans-6967
  prefs: []
  type: TYPE_NORMAL
  zh: 图23.5. 错误分类的数字，以及两个最可能的预测（底部，从左到右）和根据MNIST的正确标签（顶部，右）：(a) 表23.3中的最佳网络。(b) 表23.4中的委员会。
- en: 23.6.2 Committee Of Mlp
  id: totrans-6968
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 23.6.2 多层感知器委员会
- en: 'Here we list results of a committee of MLP with the architecture that obtained
    0.35% error rate on MNIST (841 neurons in the input layer, five hidden layers
    Table 23.4. Error rates of the individual nets and of the resulting committee.
    Architecture: 841 input neurons, hidden layers containing 2500, 2000, 1500, 1000
    and 500 neurons, and 10 outputs. WN x—Width Normalization of the bounding box
    to be x pixels wide.'
  id: totrans-6969
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们列出了一个多层感知器（MLP）委员会的结果，该架构在MNIST上获得了0.35%的错误率（输入层有841个神经元，五个隐藏层）。表23.4. 各个网络及其结果委员会的错误率。架构：841个输入神经元，隐藏层包含2500、2000、1500、1000和500个神经元，以及10个输出。WN
    x—边界框的宽度归一化到宽度为x像素。
- en: '| WN                  | 10                            | 12   | 14   | 16   |
    18   | 20   | ORIGINAL MNIST   |'
  id: totrans-6970
  prefs: []
  type: TYPE_TB
  zh: '| WN                  | 10                            | 12   | 14   | 16   |
    18   | 20   | 原始MNIST   |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-6971
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| test error [%]      | 0.52 0.45 0.44 0.49 0.36 0.38 | 0.35 |      |      |      |      |                  |'
  id: totrans-6972
  prefs: []
  type: TYPE_TB
  zh: '| 测试错误 [%]      | 0.52 0.45 0.44 0.49 0.36 0.38 | 0.35 |      |      |      |      |                  |'
- en: '| committee error [%] | 0.31                          |      |      |      |      |      |                  |'
  id: totrans-6973
  prefs: []
  type: TYPE_TB
  zh: '| 委员会错误 [%] | 0.31                          |      |      |      |      |      |                  |'
- en: containing 2500, 2000, 1500, 1000 and 500 neurons, and 10 outputs). We train
    six additional nets with the same architecture on normalized data (the width of
    the digits is normalized prior to training) and form a committee by averaging
    the predictions of the individual nets (Table 23.4). The width-normalization is
    essential for good committee performance as shown in Section 23.5. All committee
    members distort their width-normalized training dataset before each epoch.
  id: totrans-6974
  prefs: []
  type: TYPE_NORMAL
  zh: 包含2500、2000、1500、1000和500个神经元，以及10个输出）。我们在归一化数据上训练六个具有相同结构的额外网络（在训练前对数字的宽度进行归一化），并通过平均各个网络的预测形成一个委员会（表23.4）。如23.5节所示，宽度归一化对良好的委员会表现至关重要。所有委员会成员在每个训练周期之前都会扭曲其宽度归一化的训练数据集。
- en: Interestingly, the error of the extremely simple committee (0.31%) is lower
    than those of the individual nets. This is the best result ever reported on MNIST
  id: totrans-6975
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，极其简单的委员会的错误率（0.31%）低于单个网络的错误率。这是在MNIST上报告的最佳结果。
- en: using MLP. Many of the 31 misclassified digits (Figure 23.5b) are ambiguous
    and/or uncharacteristic, with obviously missing parts or strange strokes etc.
    Remarkably, the committee's second guess is correct for 29 of the 31.
  id: totrans-6976
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MLP。许多31个误分类数字（图23.5b）都是模糊和/或不典型的，显然缺少部分或有奇怪的笔画等。值得注意的是，委员会的第二次猜测在31个中有29个是正确的。
- en: 23.7 Discussion
  id: totrans-6977
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 23.7 讨论
- en: In recent decades the amount of raw computing power per Euro has grown by a
    factor of 100-1000 per decade. Our results show that this ongoing hardware progress
    may be more important than advances in algorithms and software (although the future
    will belong to methods combining the best of both worlds). Current graphics cards
    (GPUs) are already more than 50 times faster than standard microprocessors when
    it comes to training big and deep neural networks by the ancient algorithm, on-line
    back-propagation (weight update rate up to 7.5×109/s, and more than 1015 per trained
    network). On the competitive MNIST
  id: totrans-6978
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近几十年，每欧元的原始计算能力增长了100-1000倍。我们的结果表明，这一持续的硬件进步可能比算法和软件的进步更为重要（尽管未来属于将两者最佳结合的方法）。当前的图形处理单元（GPU）在通过古老算法（在线反向传播）训练大型深度神经网络时，速度已经比标准微处理器快50倍以上（权重更新速率高达7.5×10^9/s，且每个训练的网络超过10^15）。在竞争激烈的MNIST上。
- en: handwriting benchmark, single precision floating-point GPU-based neural nets
    surpass all previously reported results, including those obtained by much more
    complex methods involving specialized architectures, unsupervised pre-training,
    combinations of machine learning classifiers etc. Training sets of sufficient
    size to avoid overfitting are obtained by appropriately deforming images. Of course,
    the approach is not limited to handwriting, and obviously holds great promise
    for many visual and other pattern recognition problems.
  id: totrans-6979
  prefs: []
  type: TYPE_NORMAL
  zh: 在手写基准测试中，基于单精度浮点的GPU神经网络超越了所有之前报告的结果，包括那些涉及专业架构、无监督预训练、机器学习分类器组合等更复杂方法所获得的结果。通过适当地变形图像获得足够大的训练集以避免过拟合。当然，这种方法不仅限于手写，并且显然在许多视觉和其他模式识别问题上具有巨大的潜力。
- en: Although big deep MLP are very powerful general classifiers when combined with
    an appropriate distortion algorithm to enhance the training set, they cannot compete
    with dedicated architectures such as max-pooling convolutional neural networks
    on complex image classification problems. For tasks more difficult than handwritten
    digit recognition MLP are not competitive anymore, both in classification performance
    and required training time. We have recently shown [11]
  id: totrans-6980
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型深度多层感知器（MLP）在与合适的扭曲算法结合以增强训练集时是非常强大的通用分类器，但在复杂的图像分类问题上，它们无法与专门的架构（如最大池化卷积神经网络）竞争。对于比手写数字识别更困难的任务，MLP的竞争力下降，无论是分类性能还是所需的训练时间。我们最近展示了[11]。
- en: that large convolutional neural networks combined with max-pooling [32] improve
    the state-of-the-art by 30-80% for a plethora of benchmarks like Latin letters
    [8], Chinese characters [11], traffic signs [9, 12], stereo projection of 3D
  id: totrans-6981
  prefs: []
  type: TYPE_NORMAL
  zh: 大型卷积神经网络与最大池化[32]结合，对众多基准（如拉丁字母[8]、汉字[11]、交通标志[9, 12]、3D立体投影）提高了30-80%的最先进水平。
- en: 'models [10, 11] and even small natural images [11]. Acknowledgments. Part of
    this work got started when Dan Cireşan was a PhD student at University "Politehnica"
    of Timişoara. He would like to thank his PhD advisor, Ştefan Holban, for his guidance,
    and Răzvan Moşincat for providing a CPU framework for MNIST. This work was partially
    supported by the Swiss Commission for Technology and Innovation (CTI), Project
    n. 9688.1 IFF: Intelligent Fill in Form., and by a FP7-ICT-2009-6 EU Grant, Project
    Code 270247: A Neuro-dynamic Framework for Cognitive Robotics: Scene Representations,
    Behavioral Sequences, and Learning.'
  id: totrans-6982
  prefs: []
  type: TYPE_NORMAL
  zh: 模型[10, 11]，甚至小型自然图像[11]。致谢。此项工作的部分起始于Dan Cireşan在“波利特尼卡”大学的博士生涯。他想感谢他的博士导师Ştefan
    Holban的指导，以及Răzvan Moşincat提供的MNIST CPU框架。此项工作部分得到了瑞士技术与创新委员会（CTI）的支持，项目编号9688.1
    IFF：智能表单填写，以及FP7-ICT-2009-6欧盟拨款，项目代码270247：用于认知机器人学的神经动态框架：场景表示、行为序列和学习。
- en: Appendix - Gpu Implementation Graphics Processing Unit
  id: totrans-6983
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 - GPU实现图形处理单元
- en: Until 2007 the only way to program a GPU was to translate the problem-solving
    algorithm into a set of graphical operations. Despite being hard to code and difficult
    to debug, several GPU-based NN implementations were developed when GPUs became
    faster than CPUs. Two layer MLPs [34] and CNNs [5] have been previously implemented
    on GPUs. Although speedups were relatively modest, these studies showed how GPUs
    can be used for machine learning. More recent GPU-based CNNs trained in batch
    mode are two orders of magnitude faster than CPU-based CNNs [32].
  id: totrans-6984
  prefs: []
  type: TYPE_NORMAL
  zh: 直到2007年，编程GPU的唯一方法是将解决问题的算法转换为一组图形操作。尽管编码困难且调试困难，但随着GPU比CPU更快，开发了几个基于GPU的神经网络实现。之前在GPU上实现了两层MLP[34]和CNN[5]。尽管加速相对温和，但这些研究展示了GPU在机器学习中的应用。最近基于GPU的CNN以批处理模式训练，速度比基于CPU的CNN快两个数量级[32]。
- en: The GPU code is written using CUDA (Compute Unified Device Architecture), a
    C-like general programming language. GPU speed and memory bandwidth are vastly
    superior to those of CPUs, and crucial for fast MLP implementations. To fully
    understand our algorithm in terms of GPU / CUDA, please visit the NVIDIA website
    [25]. According to CUDA terminology, the CPU is called *host* and the graphics
    card device or GPU.
  id: totrans-6985
  prefs: []
  type: TYPE_NORMAL
  zh: GPU代码使用CUDA（计算统一设备架构）编写，这是一种类似C的通用编程语言。GPU的速度和内存带宽远远优于CPU，这对快速的MLP实现至关重要。要全面理解我们关于GPU/CUDA的算法，请访问NVIDIA网站[25]。根据CUDA术语，CPU称为*主机*，图形卡设备或GPU称为*设备*。
- en: Deformations
  id: totrans-6986
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变形
- en: Only the most time-consuming part of the latter - convolution with a gaussian
    kernel [33] - is ported to the GPU. The MNIST training set is split into 600 sequentially
    processed batches. MNIST digits are scaled from the original 28×28 pixels to 29
    × 29 pixels, to get a proper center, which simplifies convolution. An image grid
    has 290 × 290 cells, zero-padded to 300 × 300, thus avoiding margin effects when
    applying a Gaussian convolution kernel of size 21 × 21.
  id: totrans-6987
  prefs: []
  type: TYPE_NORMAL
  zh: 仅将后者中最耗时的部分 - 与高斯核的卷积[33] - 移植到GPU。MNIST训练集被分成600个顺序处理的批次。MNIST数字从原始的28×28像素缩放到29×29像素，以获得合适的中心，从而简化卷积。图像网格有290
    × 290个单元，零填充到300 × 300，从而避免在应用大小为21 × 21的高斯卷积核时出现边缘效应。
- en: Our GPU program groups many threads into a block, where they share the same
    gaussian kernel and parts of the random field. The blocks contain 21 (the kernel
    size) ×10 threads, each computing a vertical strip of the convolution operation
    (Listing 23.1).
  id: totrans-6988
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的GPU程序将多个线程分组到一个块中，线程共享相同的高斯核和随机场的部分。每个块包含21（核大小）×10个线程，每个线程计算卷积操作的一个垂直条带（见列表23.1）。
- en: Listing 23.1. Convolution Kernel for elastic distortion 1 __global__ void ConvolveField(float
    ∗randomfield, int width, int height, float ∗kernel, float ∗outputfield, float
    elasticScale){
  id: totrans-6989
  prefs: []
  type: TYPE_NORMAL
  zh: 列表23.1。弹性变形的卷积核 1 __global__ void ConvolveField(float ∗randomfield, int width,
    int height, float ∗kernel, float ∗outputfield, float elasticScale){
- en: 2 float sum=0; 3 const int stride_k=GET_STRIDE(GAUSSIAN_FIELD_SIZE,pitch_x
  id: totrans-6990
  prefs: []
  type: TYPE_NORMAL
  zh: 2 float sum=0; 3 const int stride_k=GET_STRIDE(GAUSSIAN_FIELD_SIZE,pitch_x
- en: '>>2); *//stride for gaussian kernel* 4 __shared__ float K[GAUSSIAN_FIELD_SIZE][stride_k];
    *//kernel (21 x* 32 values)'
  id: totrans-6991
  prefs: []
  type: TYPE_NORMAL
  zh: '>>2); *//高斯核的步幅* 4 __shared__ float K[GAUSSIAN_FIELD_SIZE][stride_k]; *//核（21
    x* 32个值)'
- en: 5 __shared__ float R[GAUSSIAN_FIELD_SIZE+9][
  id: totrans-6992
  prefs: []
  type: TYPE_NORMAL
  zh: 5 __shared__ float R[GAUSSIAN_FIELD_SIZE+9][
- en: GAUSSIAN_FIELD_SIZE]; *//random field (30 x 21 values)*
  id: totrans-6993
  prefs: []
  type: TYPE_NORMAL
  zh: GAUSSIAN_FIELD_SIZE]; *//随机场（30 x 21个值）*
- en: 6 __shared__ float s[10][GAUSSIAN_FIELD_SIZE]; //partial sums (10 x 21 values)
  id: totrans-6994
  prefs: []
  type: TYPE_NORMAL
  zh: 6 __shared__ float s[10][GAUSSIAN_FIELD_SIZE]; //部分和（10 x 21个值）
- en: 7 int stride_in=GET_STRIDE(width,pitch_x>>2); //random field stride as a multiple
    of 32 8 int stride_out=GET_STRIDE(width−GAUSSIAN_FIELD_SIZE+1, pitch_x>>2); *//output
    stride as a multiple of 32* 9 10 *//loading gaussian kernel into K (21 x 21 values)*
  id: totrans-6995
  prefs: []
  type: TYPE_NORMAL
  zh: 7 int stride_in=GET_STRIDE(width,pitch_x>>2); //随机场步幅为 32 的倍数 8 int stride_out=GET_STRIDE(width−GAUSSIAN_FIELD_SIZE+1,
    pitch_x>>2); *//输出步幅为 32 的倍数* 9 10 *//将高斯内核加载到 K（21 x 21 值）*
- en: 11 K[ 0+threadIdx.y][threadIdx.x] = kernel[( 0+threadIdx.y)∗stride_k +
  id: totrans-6996
  prefs: []
  type: TYPE_NORMAL
  zh: 11 K[ 0+threadIdx.y][threadIdx.x] = kernel[( 0+threadIdx.y)∗stride_k +
- en: threadIdx.x];*//rows 0..9* 12 K[10+threadIdx.y][threadIdx.x] = kernel[(10+threadIdx.y)∗stride_k
    +
  id: totrans-6997
  prefs: []
  type: TYPE_NORMAL
  zh: threadIdx.x];*//行 0..9* 12 K[10+threadIdx.y][threadIdx.x] = kernel[(10+threadIdx.y)∗stride_k
    +
- en: threadIdx.x];*//rows 10..19* 13 if(threadIdx.y==0)
  id: totrans-6998
  prefs: []
  type: TYPE_NORMAL
  zh: threadIdx.x];*//行 10..19* 13 if(threadIdx.y==0)
- en: 14 K[20+threadIdx.y][threadIdx.x] = kernel[(20+threadIdx.y)∗stride_k +
  id: totrans-6999
  prefs: []
  type: TYPE_NORMAL
  zh: 14 K[20+threadIdx.y][threadIdx.x] = kernel[(20+threadIdx.y)∗stride_k +
- en: threadIdx.x];*//row 20* 15 16 *//loading randomfield into R*
  id: totrans-7000
  prefs: []
  type: TYPE_NORMAL
  zh: threadIdx.x];*//行 20* 15 16 *//将随机场加载到 R*
- en: 17 *//0..9 x 21 values* 18 R[ 0+threadIdx.y][threadIdx.x] = randomfield[(10∗blockIdx.y+
    0+
  id: totrans-7001
  prefs: []
  type: TYPE_NORMAL
  zh: 17 *//0..9 x 21 值* 18 R[ 0+threadIdx.y][threadIdx.x] = randomfield[(10∗blockIdx.y+
    0+
- en: threadIdx.y)∗stride_in + blockIdx.x + threadIdx.x];
  id: totrans-7002
  prefs: []
  type: TYPE_NORMAL
  zh: threadIdx.y)∗stride_in + blockIdx.x + threadIdx.x];
- en: 19 *//10..19 x 21 values* 20 R[10+threadIdx.y][threadIdx.x] = randomfield[(10∗blockIdx.y+10+
  id: totrans-7003
  prefs: []
  type: TYPE_NORMAL
  zh: 19 *//10..19 x 21 值* 20 R[10+threadIdx.y][threadIdx.x] = randomfield[(10∗blockIdx.y+10+
- en: threadIdx.y)∗stride_in + blockIdx.x + threadIdx.x];
  id: totrans-7004
  prefs: []
  type: TYPE_NORMAL
  zh: threadIdx.y)∗stride_in + blockIdx.x + threadIdx.x];
- en: 21 *//20..29 x 21 values* 22 R[20+threadIdx.y][threadIdx.x] = randomfield[(10∗blockIdx.y+20+
  id: totrans-7005
  prefs: []
  type: TYPE_NORMAL
  zh: 21 *//20..29 x 21 值* 22 R[20+threadIdx.y][threadIdx.x] = randomfield[(10∗blockIdx.y+20+
- en: threadIdx.y)∗stride_in + blockIdx.x + threadIdx.x];
  id: totrans-7006
  prefs: []
  type: TYPE_NORMAL
  zh: threadIdx.y)∗stride_in + blockIdx.x + threadIdx.x];
- en: 23 __syncthreads(); *//wait until everything is read into shared memory* 24
    25 *//computing partial sums* 26 \#pragma unroll 21 *//GAUSSIAN_FIELD_SIZE* 27
    for(int i=0;i<GAUSSIAN_FIELD_SIZE;i++)
  id: totrans-7007
  prefs: []
  type: TYPE_NORMAL
  zh: 23 __syncthreads(); *//等待所有内容读取到共享内存中* 24 25 *//计算部分和* 26 \#pragma unroll 21
    *//高斯场大小* 27 for(int i=0;i<GAUSSIAN_FIELD_SIZE;i++)
- en: 28 sum += R[threadIdx.y + i][threadIdx.x] ∗ K[i][threadIdx.x];
  id: totrans-7008
  prefs: []
  type: TYPE_NORMAL
  zh: 28 sum += R[threadIdx.y + i][threadIdx.x] ∗ K[i][threadIdx.x];
- en: 29 s[threadIdx.y][threadIdx.x]=sum; 30 __syncthreads(); 31 32 if(threadIdx.x==0){
    *//the first column of threads computes the final values* of the convolutions
    33 \#pragma unroll 20*//GAUSSIAN_FIELD_SIZE*−1 34 for(int i=1;i<GAUSSIAN_FIELD_SIZE;i++)
    sum+=s[threadIdx.y][i
  id: totrans-7009
  prefs: []
  type: TYPE_NORMAL
  zh: 29 s[threadIdx.y][threadIdx.x]=sum; 30 __syncthreads(); 31 32 if(threadIdx.x==0){
    *//第一列线程计算卷积的最终值* 33 \#pragma unroll 20*//高斯场大小*−1 34 for(int i=1;i<GAUSSIAN_FIELD_SIZE;i++)
    sum+=s[threadIdx.y][i
- en: '];'
  id: totrans-7010
  prefs: []
  type: TYPE_NORMAL
  zh: '];'
- en: 35 outputfield[(blockIdx.y∗10+threadIdx.y)∗stride_out + blockIdx.x] =
  id: totrans-7011
  prefs: []
  type: TYPE_NORMAL
  zh: 35 outputfield[(blockIdx.y∗10+threadIdx.y)∗stride_out + blockIdx.x] =
- en: sum ∗ elasticScale; 36 }
  id: totrans-7012
  prefs: []
  type: TYPE_NORMAL
  zh: sum ∗ elasticScale; 36 }
- en: 37 }
  id: totrans-7013
  prefs: []
  type: TYPE_NORMAL
  zh: 37 }
- en: Training Algorithm
  id: totrans-7014
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练算法
- en: We closely follow the standard BP algorithm [30], except that BP of deltas and
    weight updates are disentangled and performed sequentially. This allows for more
    parallelism within each routine.
  id: totrans-7015
  prefs: []
  type: TYPE_NORMAL
  zh: 我们紧密遵循标准 BP 算法 [30]，只是 BP 的增量和权重更新是分开的并且按顺序执行。这允许每个例程中有更多的并行性。
- en: Forward Propagation
  id: totrans-7016
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前向传播
- en: The algorithm is divided into two kernels. The weight matrix W is partitioned
  id: totrans-7017
  prefs: []
  type: TYPE_NORMAL
  zh: 算法分为两个内核。权重矩阵 W 被分区
- en: '![583_image_0.png](583_image_0.png) as illustrated in Figure 23.6.'
  id: totrans-7018
  prefs: []
  type: TYPE_NORMAL
  zh: '![583_image_0.png](583_image_0.png) 如图 23.6 所示。'
- en: 'Fig. 23.6. Forward propagation: a) mapping of kernel 1 grid onto the padded
    weight matrix; b) mapping the kernel 2 grid onto the partial dot products matrix;
    c) output of forward propagation'
  id: totrans-7019
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23.6. 前向传播：a) 将内核 1 网格映射到填充的权重矩阵；b) 将内核 2 网格映射到部分点积矩阵；c) 前向传播的输出
- en: 'Kernel 1. Each block has 256 threads (Figure 23.6a), each computing a partial
    dot product of 32 component vectors. The dot products are stored in a temporary
    matrix (Figure 23.6b). This kernel has a very high throughput: average memory
    bandwidth is 115GB/s. This is possible because many relatively small blocks keep
    the GPU busy. Each block uses shared memory for storing the previous layer activations,
    which are simultaneously read by the first 32 threads of each block and then used
    by all 256 threads. After thread synchronization, the partial dot products are
    computed in parallel (Listing 23.2). The number of instructions is kept to a minimum
    by pre-computing all common index parts.'
  id: totrans-7020
  prefs: []
  type: TYPE_NORMAL
  zh: 内核 1。每个块有 256 个线程（图 23.6a），每个线程计算 32 个组件向量的部分点积。点积存储在一个临时矩阵中（图 23.6b）。这个内核具有非常高的吞吐量：平均内存带宽为
    115GB/s。这是可能的，因为许多相对较小的块使 GPU 始终处于繁忙状态。每个块使用共享内存存储前一层的激活值，前 32 个线程同时读取这些值，然后由所有
    256 个线程使用。在线程同步后，部分点积并行计算（清单 23.2）。通过预计算所有公共索引部分，指令数量保持在最低限度。
- en: Kernel 2. The thread grid (Figure 23.6b) has only one row of blocks consisting
    of *warp* threads, since each thread has to compute a complete dot product (Figure
    23.6c) and then pipe it into the activation function. This kernel (Listing 23.2)
  id: totrans-7021
  prefs: []
  type: TYPE_NORMAL
  zh: 核心 2. 线程网格（图 23.6b）只有一行块，由*warp*线程组成，因为每个线程必须计算一个完整的点积（图 23.6c），然后将其输入到激活函数中。这个核心（列表
    23.2）
- en: is inefficient for layers with fewer than 1024 incoming connections per neuron,
    especially for the last layer which has only ten neurons, one for each digit.
    That is, its grid will have only one block, occupying only 6% of the GTX 480 GPU.
  id: totrans-7022
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个神经元输入连接少于1024的层，这种方式效率低下，特别是对于最后一层，只有十个神经元，每个数字一个。也就是说，它的网格只有一个块，仅占用GTX
    480 GPU的6%。
- en: Listing 23.2. Forward propagation kernels 1 __global__ void MLP_FP_reduction_Kernel1(float
    ∗prevLN, float ∗W,
  id: totrans-7023
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 23.2. 前向传播核心 1 __global__ void MLP_FP_reduction_Kernel1(float ∗prevLN, float
    ∗W,
- en: float ∗partialsum, unsigned int neurons, unsigned int prevneurons){
  id: totrans-7024
  prefs: []
  type: TYPE_NORMAL
  zh: float ∗partialsum, 无符号整数 neurons, 无符号整数 prevneurons){
- en: 2 const int threads=256; 3 const int stride=GET_STRIDE(neurons,pitch_x>>2);
    //horizontal stride of W matrix 4 int X=blockIdx.x∗threads + threadIdx.x; *//precomputing
    expressions* 5 int Y=X+stride∗blockIdx.y; 6 int Z=blockIdx.y∗pitch_y∗stride +
    X;
  id: totrans-7025
  prefs: []
  type: TYPE_NORMAL
  zh: 2 const int threads=256; 3 const int stride=GET_STRIDE(neurons,pitch_x>>2);
    //W矩阵的水平步幅 4 int X=blockIdx.x∗threads + threadIdx.x; *//预计算表达式* 5 int Y=X+stride∗blockIdx.y;
    6 int Z=blockIdx.y∗pitch_y∗stride + X;
- en: 7 float sum=0.0f; 8 __shared__ float output[pitch_y]; 9 if(blockIdx.y==0)
  id: totrans-7026
  prefs: []
  type: TYPE_NORMAL
  zh: 7 float sum=0.0f; 8 __shared__ float output[pitch_y]; 9 如果(blockIdx.y==0)
- en: 10 if(threadIdx.x==0) output[0]=1.0f; 11 else if(threadIdx.x<pitch_y) *//there
    are only 32 values to read and* 128 threads 12 output[threadIdx.x] = threadIdx.x−1<prevneurons
    ? prevLN[
  id: totrans-7027
  prefs: []
  type: TYPE_NORMAL
  zh: 10 如果(threadIdx.x==0) output[0]=1.0f; 11 否则如果(threadIdx.x<pitch_y) *//只有32个值可以读取和*
    128线程 12 output[threadIdx.x] = threadIdx.x−1<prevneurons ? prevLN[
- en: 'threadIdx.x−1] : 0.0f; 13 else; 14 else if(threadIdx.x<pitch_y) *//there are
    only 32 values to read and 128* threads 15 output[threadIdx.x] = blockIdx.y∗pitch_y+threadIdx.x−1<'
  id: totrans-7028
  prefs: []
  type: TYPE_NORMAL
  zh: 'threadIdx.x−1] : 0.0f; 13 否则; 14 否则如果(threadIdx.x<pitch_y) *//只有32个值可以读取和128个线程*
    15 output[threadIdx.x] = blockIdx.y∗pitch_y+threadIdx.x−1<'
- en: prevneurons ?
  id: totrans-7029
  prefs: []
  type: TYPE_NORMAL
  zh: prevneurons ?
- en: '16 prevLN[blockIdx.y∗pitch_y+threadIdx.x−1] : 0.0f; 17 else; 18 __syncthreads();
    19 if(X<neurons){*//compute partial sums* 20 *//\#pragma unroll 32* 21 int size=0;
    22 if((blockIdx.y+1)∗pitch_y>=prevneurons+1) 23 size = prevneurons + 1 − blockIdx.y∗pitch_y;
    24 else size=pitch_y; 25 for (int ic=0; ic<size; ic++){'
  id: totrans-7030
  prefs: []
  type: TYPE_NORMAL
  zh: '16 prevLN[blockIdx.y∗pitch_y+threadIdx.x−1] : 0.0f; 17 否则; 18 __syncthreads();
    19 如果(X<neurons){*//计算部分和* 20 *//\#pragma unroll 32* 21 int size=0; 22 如果((blockIdx.y+1)∗pitch_y>=prevneurons+1)
    23 size = prevneurons + 1 − blockIdx.y∗pitch_y; 24 否则 size=pitch_y; 25 for (int
    ic=0; ic<size; ic++){'
- en: 26 sum += output[ic] ∗ W[Z];
  id: totrans-7031
  prefs: []
  type: TYPE_NORMAL
  zh: 26 sum += output[ic] ∗ W[Z];
- en: 27 Z+=stride; 28 }
  id: totrans-7032
  prefs: []
  type: TYPE_NORMAL
  zh: 27 Z+=stride; 28 }
- en: 29 partialsum[Y]=sum; 30 }
  id: totrans-7033
  prefs: []
  type: TYPE_NORMAL
  zh: 29 partialsum[Y]=sum; 30 }
- en: 31 } 32 33 __global__ void MLP_FP_reduction_Kernel2(float ∗currLN, float ∗
  id: totrans-7034
  prefs: []
  type: TYPE_NORMAL
  zh: 31 } 32 33 __global__ void MLP_FP_reduction_Kernel2(float ∗currLN, float ∗
- en: partialsum, unsigned int neurons, unsigned int size){
  id: totrans-7035
  prefs: []
  type: TYPE_NORMAL
  zh: partialsum, 无符号整数 neurons, 无符号整数 size){
- en: 34 float sum=0.0f; 35 int idx = blockIdx.x∗(pitch_x>>2) + threadIdx.x; *//precomputed
    index* 36 unsigned int stride = GET_STRIDE(neurons,pitch_x>>2); *//stride for*
    partialsum matrix 37 38 if(idx>=neurons)return; *//is this thread computing a
    true neuron?*
  id: totrans-7036
  prefs: []
  type: TYPE_NORMAL
  zh: 34 float sum=0.0f; 35 int idx = blockIdx.x∗(pitch_x>>2) + threadIdx.x; *//预计算索引*
    36 无符号整数 stride = GET_STRIDE(neurons,pitch_x>>2); *//partialsum矩阵的步幅* 37 38 如果(idx>=neurons)return;
    *//这个线程在计算真正的神经元吗？*
- en: 39 for (int i=0; i<size; i++) sum += partialsum[i∗stride+idx]; *//computing*
    the final dot product 40 currLN[idx] = SIGMOIDF(sum); *//applying activation*
    41 }
  id: totrans-7037
  prefs: []
  type: TYPE_NORMAL
  zh: 39 for (int i=0; i<size; i++) sum += partialsum[i∗stride+idx]; *//计算* 最终点积 40
    currLN[idx] = SIGMOIDF(sum); *//应用激活* 41 }
- en: Backward Propagation
  id: totrans-7038
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播
- en: This is similar to FP, but we need WT for coalesced access. Instead of transposing
    the matrix, the computations are performed on patches of data read from device
    memory into shared memory, similar to the optimized matrix transposition algorithm
    of [28]. Shared memory access is much faster, without coalescing restrictions.
    Because we have to cope with layers of thousands of neurons, back-propagating
    deltas uses a reduction method implemented in two kernels communicating partial
    results via global memory (Listing 23.3).
  id: totrans-7039
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于FP，但我们需要WT以便进行合并访问。不是转置矩阵，而是对从设备内存读取到共享内存的数据块进行计算，类似于优化的矩阵转置算法[28]。共享内存访问速度更快，没有合并限制。由于我们必须处理数千个神经元的层，反向传播的增量使用两种核心实现的减少方法，通过全局内存传递部分结果（列表
    23.3）。
- en: Listing 23.3. Backpropagating Deltas Kernels
  id: totrans-7040
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列表 23.3. 反向传播增量核心
- en: 1 2 __global__ void backPropagateDeltasFC_A(float ∗indelta, float ∗weights,
    unsigned int ncon, unsigned int nrneur, float ∗partial){
  id: totrans-7041
  prefs: []
  type: TYPE_NORMAL
  zh: 1 2 __global__ void backPropagateDeltasFC_A(float ∗indelta, float ∗weights,
    unsigned int ncon, unsigned int nrneur, float ∗partial){
- en: 3 const int px = pitch_x>>2; 4 unsigned int stride_x = GET_STRIDE(nrneur,px);
    5 unsigned int stride_y = GET_STRIDE(ncon,pitch_y); 6 float outd = 0.0; 7 int
    idx = blockIdx.x∗px+threadIdx.x; 8 int X = blockIdx.y∗pitch_y∗stride_x + idx;
    9 int Y = threadIdx.x; 10 __shared__ float w[32∗33]; *//pitch_y and px should
    be equal ! +1 to* avoid bank conflict!
  id: totrans-7042
  prefs: []
  type: TYPE_NORMAL
  zh: 3 const int px = pitch_x>>2; 4 unsigned int stride_x = GET_STRIDE(nrneur,px);
    5 unsigned int stride_y = GET_STRIDE(ncon,pitch_y); 6 float outd = 0.0; 7 int
    idx = blockIdx.x∗px+threadIdx.x; 8 int X = blockIdx.y∗pitch_y∗stride_x + idx;
    9 int Y = threadIdx.x; 10 __shared__ float w[32∗33]; *//pitch_y 和 px 应该相等！ +1
    以* 避免银行冲突！
- en: 11 __shared__ float id[px]; *//input delta* 12 \#pragma unroll 32 *//read the
    weight patch in shared memory* 13 for(int i=0;i<pitch_y;i++){w[Y]=weights[X];
    X+=stride_x; Y+=33;} 14 *//read the input delta patch in shared memory* 15 if(idx>=nrneur)
    id[threadIdx.x]=0; //a fake input delta for inexistent indelta 16 else id[threadIdx.x]=indelta[idx];
    17 __syncthreads(); *//not needed for block with warp number of threads:*
  id: totrans-7043
  prefs: []
  type: TYPE_NORMAL
  zh: 11 __shared__ float id[px]; *//输入 delta* 12 \#pragma unroll 32 *//在共享内存中读取权重块*
    13 for(int i=0;i<pitch_y;i++){w[Y]=weights[X]; X+=stride_x; Y+=33;} 14 *//在共享内存中读取输入
    delta 块* 15 if(idx>=nrneur) id[threadIdx.x]=0; //不存在的 indelta 的假输入 delta 16 else
    id[threadIdx.x]=indelta[idx]; 17 __syncthreads(); *//对于具有 warp 数量线程的块不需要：*
- en: implicit synchronization 18 \#pragma unroll 32 *//compute partial results* 19
    for(int i=0;i<px;i++) outd+=w[threadIdx.x∗33+i]∗id[i];
  id: totrans-7044
  prefs: []
  type: TYPE_NORMAL
  zh: 隐式同步 18 \#pragma unroll 32 *//计算部分结果* 19 for(int i=0;i<px;i++) outd+=w[threadIdx.x∗33+i]∗id[i];
- en: 20 *//write out the partial results* 21 partial[blockIdx.x∗stride_y + blockIdx.y∗pitch_y
    + threadIdx.x] = outd; 22 }
  id: totrans-7045
  prefs: []
  type: TYPE_NORMAL
  zh: 20 *//写出部分结果* 21 partial[blockIdx.x∗stride_y + blockIdx.y∗pitch_y + threadIdx.x]
    = outd; 22 }
- en: 23 __global__ void backPropagateDeltasFC_B(float ∗outdelta,float ∗instates,
    unsigned int ncon, unsigned int nrneur, float ∗partial){
  id: totrans-7046
  prefs: []
  type: TYPE_NORMAL
  zh: 23 __global__ void backPropagateDeltasFC_B(float ∗outdelta,float ∗instates,
    unsigned int ncon, unsigned int nrneur, float ∗partial){
- en: 24 int px=pitch_x>>2; 25 unsigned int stride_x = GET_STRIDE(nrneur,px);
  id: totrans-7047
  prefs: []
  type: TYPE_NORMAL
  zh: 24 int px=pitch_x>>2; 25 unsigned int stride_x = GET_STRIDE(nrneur,px);
- en: 26 unsigned int stride_y = GET_STRIDE(ncon,pitch_y); 27 float outd = 0.0; 28
    int size=stride_x/px; 29 int idx=blockIdx.x∗pitch_y+threadIdx.x; 30 if(idx==0);
    *//true only for block and thread 0* 31 else{ 32 for(int i=0;i<size;i++)
  id: totrans-7048
  prefs: []
  type: TYPE_NORMAL
  zh: 26 unsigned int stride_y = GET_STRIDE(ncon,pitch_y); 27 float outd = 0.0; 28
    int size=stride_x/px; 29 int idx=blockIdx.x∗pitch_y+threadIdx.x; 30 if(idx==0);
    *//仅对块和线程 0 为真* 31 else{ 32 for(int i=0;i<size;i++)
- en: 33 outd+=partial[i∗stride_y + idx];
  id: totrans-7049
  prefs: []
  type: TYPE_NORMAL
  zh: 33 outd+=partial[i∗stride_y + idx];
- en: 34 outdelta[idx−1] = outd ∗ DSIGMOIDF(instates[idx−1]); //−*1 BIAS ...*
  id: totrans-7050
  prefs: []
  type: TYPE_NORMAL
  zh: 34 outdelta[idx−1] = outd ∗ DSIGMOIDF(instates[idx−1]); //−*1 偏置 ...*
- en: 35 }
  id: totrans-7051
  prefs: []
  type: TYPE_NORMAL
  zh: 35 }
- en: 36 }
  id: totrans-7052
  prefs: []
  type: TYPE_NORMAL
  zh: 36 }
- en: Kernel 1. The bi-dimensional grid is divided into blocks of *warp* (32) threads.
  id: totrans-7053
  prefs: []
  type: TYPE_NORMAL
  zh: 内核 1。二维网格被划分为 *warp*（32）线程的块。
- en: The kernel starts by reading a patch of 32 × 32 values from W. The stride of
    the shared memory block is 33 (*warp* + 1), thus avoiding all bank conflicts and
    significantly improving speed. Next, 32 input delta values are read and all memory
    locations that do not correspond to real neurons (because of vertical striding)
    are zero-padded to avoid branching in subsequent computations. The number of elements
    is fixed to *warp* size, and the computing loop is unrolled for further speedups.
    Before finishing, each thread writes its own partial dot product to global memory.
  id: totrans-7054
  prefs: []
  type: TYPE_NORMAL
  zh: 内核开始时从 W 中读取一个 32 × 32 的值块。共享内存块的步幅为 33 (*warp* + 1)，从而避免所有银行冲突并显著提高速度。接下来，读取
    32 个输入 delta 值，所有不对应于真实神经元的内存位置（由于垂直步幅）被填充为零，以避免后续计算中的分支。元素数量固定为 *warp* 大小，计算循环展开以进一步加速。在结束之前，每个线程将自己的部分点积写入全局内存。
- en: Kernel 2. This kernel completes BP of deltas by summing up partial deltas computed
    by the previous kernel. It multiplies the final result by the derivative of the
    activation function applied to the current neuron's state, and writes the new
    delta to global memory.
  id: totrans-7055
  prefs: []
  type: TYPE_NORMAL
  zh: 内核 2。此内核通过对前一个内核计算的部分 delta 进行求和，完成 delta 的反向传播。它将最终结果乘以应用于当前神经元状态的激活函数的导数，并将新
    delta 写入全局内存。
- en: Weight Updating
  id: totrans-7056
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重更新
- en: The algorithm (Listing 23.4) starts by reading the appropriate delta, and precomputes
    all repetitive expressions. Then the first 16 threads read the states from global
    memory into shared memory. The "bias neuron" with constant activation 1.0 is dealt
    with by conditional statements, which could be avoided through expressions containing
    the conditions. Once threads are synchronized, each single thread updates 16 weights
    in a fixed unrolled loop.
  id: totrans-7057
  prefs: []
  type: TYPE_NORMAL
  zh: 算法（列表 23.4）开始读取适当的 delta，并预计算所有重复表达式。然后前 16 个线程从全局内存读取状态到共享内存。常量激活为 1.0 的“偏置神经元”通过条件语句处理，可以通过包含条件的表达式避免。一旦线程同步，每个线程在固定展开循环中更新
    16 个权重。
- en: Listing 23.4. Weights adjustment kernel 1 __global__ void adjustWeightsFC(float
    ∗states,float ∗deltas, float ∗weights, float eta, unsigned int ncon, unsigned
    int nrneur){
  id: totrans-7058
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 23.4。权重调整内核 1 __global__ void adjustWeightsFC(float ∗states,float ∗deltas,
    float ∗weights, float eta, unsigned int ncon, unsigned int nrneur){
- en: 2 const int pitch_y=16; 3 const int threads=256; 4 unsigned int px = pitch_x
    >> 2; 5 unsigned int stride_x = GET_STRIDE(nrneur,px); 6 float etadeltak = eta∗deltas[blockIdx.x∗threads+threadIdx.x],t;
    7 int b=blockIdx.y∗stride_x∗pitch_y + threads∗blockIdx.x + threadIdx.x; 8 __shared__
    float st[pitch_y]; *//for states* 9 int cond1 = blockIdx.y || threadIdx.x; 10
    int cond2 = (blockIdx.y+1)∗pitch_y<=ncon; 11 int size = cond2 ∗ pitch_y + !cond2
    ∗ (ncon%pitch_y);
  id: totrans-7059
  prefs: []
  type: TYPE_NORMAL
  zh: 2 const int pitch_y=16; 3 const int threads=256; 4 unsigned int px = pitch_x
    >> 2; 5 unsigned int stride_x = GET_STRIDE(nrneur,px); 6 float etadeltak = eta∗deltas[blockIdx.x∗threads+threadIdx.x],t;
    7 int b=blockIdx.y∗stride_x∗pitch_y + threads∗blockIdx.x + threadIdx.x; 8 __shared__
    float st[pitch_y]; *//用于状态* 9 int cond1 = blockIdx.y || threadIdx.x; 10 int cond2
    = (blockIdx.y+1)∗pitch_y<=ncon; 11 int size = cond2 ∗ pitch_y + !cond2 ∗ (ncon%pitch_y);
- en: 12 if(threadIdx.x<pitch_y) st[threadIdx.x] = cond1 ∗ states[blockIdx.y∗
  id: totrans-7060
  prefs: []
  type: TYPE_NORMAL
  zh: 12 if(threadIdx.x<pitch_y) st[threadIdx.x] = cond1 ∗ states[blockIdx.y∗
- en: pitch_y + threadIdx.x − 1] + !cond1; 13 __syncthreads(); 14 15 if (blockIdx.x∗threads
    + threadIdx.x < nrneur){
  id: totrans-7061
  prefs: []
  type: TYPE_NORMAL
  zh: pitch_y + threadIdx.x − 1] + !cond1; 13 __syncthreads(); 14 15 if (blockIdx.x∗threads
    + threadIdx.x < nrneur){
- en: 16 \#pragma unroll 16 17 for (int j=0; j<16; j++){
  id: totrans-7062
  prefs: []
  type: TYPE_NORMAL
  zh: 16 \#pragma unroll 16 17 for (int j=0; j<16; j++){
- en: 18 t=weights[b];
  id: totrans-7063
  prefs: []
  type: TYPE_NORMAL
  zh: 18 t=weights[b];
- en: 19 t−= etadeltak ∗ st[j];
  id: totrans-7064
  prefs: []
  type: TYPE_NORMAL
  zh: 19 t−= etadeltak ∗ st[j];
- en: 20 weights[b]=t; 21 b+=stride_x;}} 22 }
  id: totrans-7065
  prefs: []
  type: TYPE_NORMAL
  zh: 20 weights[b]=t; 21 b+=stride_x;}} 22 }
- en: '[1] Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H.: Greedy layer-wise
    training of deep networks. In: Neural Information Processing Systems (2006)'
  id: totrans-7066
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H.: 贪婪的分层训练深度网络。在：神经信息处理系统
    (2006)'
- en: '[2] Bishop, C.M.: Pattern Recognition and Machine Learning. Springer (2006)'
  id: totrans-7067
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Bishop, C.M.: 模式识别与机器学习。施普林格 (2006)'
- en: '[3] Breiman, L.: Bagging predictors. Machine Learning 24, 123–140 (1996) [4]
    Chellapilla, K., Shilman, M., Simard, P.: Combining Multiple Classifiers for Faster
    Optical Character Recognition. In: Bunke, H., Spitz, A.L. (eds.) DAS 2006. LNCS,
    vol. 3872, pp. 358–367. Springer, Heidelberg (2006)'
  id: totrans-7068
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Breiman, L.: 装袋预测器。机器学习 24, 123–140 (1996) [4] Chellapilla, K., Shilman,
    M., Simard, P.: 组合多个分类器以加快光学字符识别。在：Bunke, H., Spitz, A.L. (eds.) DAS 2006。LNCS,
    vol. 3872, pp. 358–367。施普林格，海德堡 (2006)'
- en: '[5] Chellapilla, K., Puri, S., Simard, P.: High performance convolutional neural
    networks for document processing. In: International Workshop on Frontiers in Handwriting
    Recognition (2006)'
  id: totrans-7069
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Chellapilla, K., Puri, S., Simard, P.: 用于文档处理的高性能卷积神经网络。在：国际手写识别前沿研讨会 (2006)'
- en: '[6] Ciresan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Deep, big,
    simple neural nets for handwritten digit recognition. Neural Computation 22(12),
    3207– 3220 (2010)'
  id: totrans-7070
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Ciresan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: 用于手写数字识别的深度、大型、简单神经网络。神经计算
    22(12), 3207–3220 (2010)'
- en: '[7] Ciresan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Handwritten
    Digit Recognition with a Committee of Deep Neural Nets on GPUs. Technical Report
    IDSIA-03-11, Istituto Dalle Molle di Studi sull''Intelligenza Artificiale, IDSIA'
  id: totrans-7071
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Ciresan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: 使用深度神经网络委员会在
    GPU 上进行手写数字识别。技术报告 IDSIA-03-11，意大利 Dalle Molle 智能研究所，IDSIA'
- en: (2011)
  id: totrans-7072
  prefs: []
  type: TYPE_NORMAL
  zh: (2011)
- en: '[8] Ciresan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional
    neural network committees for handwritten character recognition. In: International
    Conference on Document Analysis and Recognition, pp. 1135–1139 (2011)'
  id: totrans-7073
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Ciresan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: 用于手写字符识别的卷积神经网络委员会。在：国际文档分析与识别会议，pp.
    1135–1139 (2011)'
- en: '[9] Ciresan, D.C., Meier, U., Masci, J., Schmidhuber, J.: A committee of neural
    networks for traffic sign classification. In: International Joint Conference on
    Neural Networks, pp. 1918–1921 (2011)'
  id: totrans-7074
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Ciresan, D.C., Meier, U., Masci, J., Schmidhuber, J.: 用于交通标志分类的神经网络委员会。在：国际神经网络联合会议，pp.
    1918–1921 (2011)'
- en: '[10] Ciresan, D.C., Meier, U., Masci, J., Gambardella, L.M., Schmidhuber, J.:
    Flexible, high performance convolutional neural networks for image classification.
    In:'
  id: totrans-7075
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Ciresan, D.C., Meier, U., Masci, J., Gambardella, L.M., Schmidhuber, J.:
    灵活的高性能卷积神经网络用于图像分类。在：'
- en: International Joint Conference on Artificial Intelligence, pp. 1237–1242 (2011)
  id: totrans-7076
  prefs: []
  type: TYPE_NORMAL
  zh: 国际人工智能联合会议，第1237–1242页（2011）
- en: '[11] Ciresan, D.C., Meier, U., Schmidhuber, J.: Multi-column deep neural networks
    for image classification. In: Computer Vision and Pattern Recognition, pp. 3642–3649
    (2012)'
  id: totrans-7077
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Ciresan, D.C., Meier, U., Schmidhuber, J.: 用于图像分类的多列深度神经网络。在：计算机视觉与模式识别，第3642–3649页（2012）'
- en: '[12] Ciresan, D.C., Meier, U., Masci, J., Schmidhuber, J.: Multi-column deep
    neural network for traffic sign classification. Neural Networks 32, 333–338 (2012)'
  id: totrans-7078
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Ciresan, D.C., Meier, U., Masci, J., Schmidhuber, J.: 用于交通标志分类的多列深度神经网络。神经网络
    32，333–338（2012）'
- en: '[13] Decoste, D., Scholkopf, B.: Training invariant support vector machines.
    Machine Learning (46), 161–190 (2002)'
  id: totrans-7079
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Decoste, D., Scholkopf, B.: 训练不变支持向量机。机器学习（46），161–190（2002）'
- en: '[14] Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data
    with neural networks. Science 313 (2006)'
  id: totrans-7080
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Hinton, G.E., Salakhutdinov, R.R.: 使用神经网络减少数据的维度。科学 313（2006）'
- en: '[15] Hinton, G.E.: To recognize shapes, first learn to generate images. Computational
    Neuroscience: Theoretical Insights into Brain Function (2007)'
  id: totrans-7081
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Hinton, G.E.: 识别形状，首先学习生成图像。计算神经科学：对大脑功能的理论洞察（2007）'
- en: '[16] Hochreiter, S.: Untersuchungen zu dynamischen neuronalen Netzen. Diploma
    thesis, Institut für Informatik, Lehrstuhl Prof. Brauer, Technische Universität
    München (1991), http://www7.informatik.tu-muenchen.de/~hochreit; advisor: J. Schmidhuber'
  id: totrans-7082
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Hochreiter, S.: 动态神经网络的研究。硕士论文，慕尼黑工业大学计算机科学系，Brauer教授讲座（1991），http://www7.informatik.tu-muenchen.de/~hochreit；导师：J.
    Schmidhuber'
- en: '[17] Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation
    9, 1735–1780 (1997)'
  id: totrans-7083
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Hochreiter, S., Schmidhuber, J.: 长短期记忆。神经计算 9，1735–1780（1997）'
- en: '[18] Hochreiter, S., Bengio, Y., Frasconi, P., Schmidhuber, J.: Gradient flow
    in recurrent nets: the difficulty of learning long-term dependencies. In: Kremer,
    S.C.,'
  id: totrans-7084
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Hochreiter, S., Bengio, Y., Frasconi, P., Schmidhuber, J.: 循环网络中的梯度流：学习长期依赖的难度。在：Kremer,
    S.C.，'
- en: Kolen, J.F. (eds.) A Field Guide to Dynamical Recurrent Neural Networks. IEEE
    Press (2001)
  id: totrans-7085
  prefs: []
  type: TYPE_NORMAL
  zh: Kolen, J.F.（主编）动态递归神经网络实用指南。IEEE出版社（2001）
- en: '[19] Lauer, F., Suen, C., Bloch, G.: A trainable feature extractor for handwritten
    digit recognition. Pattern Recognition (40), 1816–1824 (2007)'
  id: totrans-7086
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Lauer, F., Suen, C., Bloch, G.: 一种可训练的特征提取器用于手写数字识别。模式识别（40），1816–1824（2007）'
- en: '[20] LeCun, Y.: Une procédure d''apprentissage pour réseau a seuil asymmetrique
    (a learning scheme for asymmetric threshold networks). In: Proceedings of Cognitiva
    1985, Paris, France, pp. 599–604 (1985)'
  id: totrans-7087
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] LeCun, Y.: 不对称阈值网络的学习程序。在：1985年巴黎认知会议论文集，第599–604页（1985）'
- en: '[21] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning
    applied to document recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)'
  id: totrans-7088
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: 基于梯度的学习在文档识别中的应用。IEEE
    会议录 86(11)，2278–2324（1998）'
- en: '[22] Meier, U., Ciresan, D.C., Gambardella, L.M., Schmidhuber, J.: Better digit
    recognition with a committee of simple neural nets. In: ICDAR, pp. 1135–1139 (2011)'
  id: totrans-7089
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Meier, U., Ciresan, D.C., Gambardella, L.M., Schmidhuber, J.: 用简单神经网络的委员会实现更好的数字识别。在：ICDAR，第1135–1139页（2011）'
- en: '[23] Mohamed, A., Dahl, G., Hinton, G.E.: Deep belief networks for phone recognition.'
  id: totrans-7090
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Mohamed, A., Dahl, G., Hinton, G.E.: 用于语音识别的深度信念网络。'
- en: 'In: Proc. of NIPS 2009 Workshop on Deep Learning for Speech Recognition and
    Related Applications (2009)'
  id: totrans-7091
  prefs: []
  type: TYPE_NORMAL
  zh: 在：NIPS 2009 深度学习研讨会关于语音识别及相关应用的论文集（2009）
- en: '[24] Nair, V., Hinton, G.E.: 3D object recognition with deep belief nets. In:
    Advances in Neural Information Processing Systems (2009)'
  id: totrans-7092
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Nair, V., Hinton, G.E.: 使用深度信念网络的3D物体识别。在：神经信息处理系统进展（2009）'
- en: '[25] NVIDIA: NVIDIA CUDA. Reference Manual, vol. 2.3. NVIDIA (2009) [26] Ranzato,
    M., Poultney, C., Chopra, S., LeCun, Y.: Efficient learning of sparse representations
    with an energy-based model. In: Platt, J., et al. (eds.) Advances in Neural Information
    Processing Systems (NIPS 2006). MIT Press (2006)'
  id: totrans-7093
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] NVIDIA: NVIDIA CUDA。参考手册，第2.3卷。NVIDIA（2009） [26] Ranzato, M., Poultney,
    C., Chopra, S., LeCun, Y.: 使用基于能量模型的稀疏表示高效学习。在：Platt, J. 等（主编）神经信息处理系统进展（NIPS
    2006）。MIT出版社（2006）'
- en: '[27] Ranzato, M.: Fu Jie Huang, Y.L.B., LeCun, Y.: Unsupervised learning of
    invariant feature hierarchies with applications to object recognition. In: Proc.
    of Computer Vision and Pattern Recognition Conference (2007)'
  id: totrans-7094
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Ranzato, M.: Fu Jie Huang, Y.L.B., LeCun, Y.: 无监督学习不变特征层次及其在物体识别中的应用。在：计算机视觉与模式识别会议论文集（2007）'
- en: '[28] Ruetsch, G., Micikevicius, P.: Optimizing matrix transpose in cuda. In:
    NVIDIA'
  id: totrans-7095
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] Ruetsch, G., Micikevicius, P.: 在 CUDA 中优化矩阵转置。在：NVIDIA'
- en: GPU Computing SDK, pp. 1–2 (2009)
  id: totrans-7096
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 计算 SDK，第1–2页（2009）
- en: '[29] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning internal representations
    by error propagation. In: Parallel Distributed Processing: Explorations in the
    Microstructure of Cognition, vol. 1: Foundations, pp. 318–362. MIT Press, Cambridge'
  id: totrans-7097
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: 通过误差传播学习内部表示。在：并行分布处理：认知微结构探索，第1卷：基础，pp.
    318–362。麻省理工学院出版社，剑桥'
- en: (1986)
  id: totrans-7098
  prefs: []
  type: TYPE_NORMAL
  zh: (1986)
- en: '[30] Russell, S., Norvig, P.: Artificial Intelligence: A Modern Approach, 2nd
    edn.'
  id: totrans-7099
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] Russell, S., Norvig, P.: 人工智能：现代方法，第2版。'
- en: Prentice-Hall, Englewood Cliffs (2003)
  id: totrans-7100
  prefs: []
  type: TYPE_NORMAL
  zh: Prentice-Hall, Englewood Cliffs（2003）
- en: '[31] Salakhutdinov, R., Hinton, G.: Learning a nonlinear embedding by preserving
    class neighborhood structure. In: Proc. of the International Conference on Artificial
    Intelligence and Statistics, vol. 11 (2007)'
  id: totrans-7101
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Salakhutdinov, R., Hinton, G.: 通过保留类别邻域结构学习非线性嵌入。在：国际人工智能与统计会议论文集，第11卷（2007）'
- en: '[32] Scherer, D., Behnke, S.: Accelerating large-scale convolutional neural
    networks with parallel graphics multiprocessors. In: Proc. of NIPS 2009 Workshop
    on LargeScale Machine Learning: Parallelism and Massive Datasets (2009)'
  id: totrans-7102
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] Scherer, D., Behnke, S.: 使用并行图形多处理器加速大规模卷积神经网络。在：NIPS 2009 大规模机器学习研讨会：并行和海量数据集的论文集（2009）'
- en: '[33] Simard, P., Steinkraus, D., Platt, J.C.: Best practices for convolutional
    neural networks applied to visual document analysis. In: Seventh International
    Conference on Document Analysis and Recognition, pp. 958–963 (2003)'
  id: totrans-7103
  prefs: []
  type: TYPE_NORMAL
  zh: '[33] Simard, P., Steinkraus, D., Platt, J.C.: 应用于视觉文档分析的卷积神经网络最佳实践。在：第七届国际文档分析与识别会议，pp.
    958–963（2003）'
- en: '[34] Steinkraus, D., Simard, P.Y.: Gpus for machine learning algorithms. In:
    International Conference on Document Analysis and Recognition, pp. 1115–1120 (2005)'
  id: totrans-7104
  prefs: []
  type: TYPE_NORMAL
  zh: '[34] Steinkraus, D., Simard, P.Y.: 用于机器学习算法的GPU。在：国际文档分析与识别会议，pp. 1115–1120（2005）'
- en: '[35] Werbos, P.J.: Beyond Regression: New Tools for Prediction and Analysis
    in the Behavioral Sciences. PhD thesis, Harvard University (1974)'
  id: totrans-7105
  prefs: []
  type: TYPE_NORMAL
  zh: '[35] Werbos, P.J.: 超越回归：行为科学中预测和分析的新工具。博士论文，哈佛大学（1974）'
- en: 24 A Practical Guide To Training Restricted Boltzmann Machines
  id: totrans-7106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 24 受限玻尔兹曼机训练的实用指南
- en: Geoffrey E. Hinton University of Toronto, Toronto, Ontario M5S 3G4 Department
    of Computer Science hinton@cs.toronto.edu Abstract. Restricted Boltzmann machines
    (RBMs) have been used as generative models of many different types of data. RBMs
    are usually trained using the contrastive divergence learning procedure. This
    requires a certain amount of practical experience to decide how to set the values
    of numerical meta-parameters. Over the last few years, the machine learning group
    at the University of Toronto has acquired considerable expertise at training RBMs
    and this guide is an attempt to share this expertise with other machine learning
    researchers.
  id: totrans-7107
  prefs: []
  type: TYPE_NORMAL
  zh: Geoffrey E. Hinton 多伦多大学，安大略省多伦多 M5S 3G4 计算机科学系 hinton@cs.toronto.edu 摘要。受限玻尔兹曼机（RBMs）已被用作多种不同类型数据的生成模型。RBMs
    通常使用对比散度学习程序进行训练。这需要一定的实践经验来决定如何设置数值超参数的值。在过去几年中，多伦多大学的机器学习小组在训练 RBMs 方面积累了相当的专业知识，本指南旨在与其他机器学习研究人员分享这一专业知识。
- en: 24.1 Introduction
  id: totrans-7108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.1 引言
- en: Restricted Boltzmann machines (RBMs) have been used as generative models of
    many different types of data including labeled or unlabeled images [7], windows
    of mel-cepstral coefficients that represent speech [12], bags of words that represent
    documents [15], and user ratings of movies [17]. In their conditional form they
    can be used to model high-dimensional temporal sequences such as video or motion
    capture data [20] or speech [11]. Their most important use is as learning modules
    that are composed to form deep belief nets [7].
  id: totrans-7109
  prefs: []
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机（RBMs）已被用作多种不同类型数据的生成模型，包括标记或未标记的图像 [7]，表示语音的梅尔倒谱系数窗口 [12]，表示文档的词袋 [15]，以及用户对电影的评分
    [17]。在其条件形式中，它们可以用于建模高维时间序列，例如视频或动作捕捉数据 [20] 或语音 [11]。它们最重要的用途是作为学习模块，组成深度信念网络
    [7]。
- en: RBMs are usually trained using the contrastive divergence learning procedure
  id: totrans-7110
  prefs: []
  type: TYPE_NORMAL
  zh: 限制玻尔兹曼机通常使用对比散度学习过程进行训练。
- en: '[5]. This requires a certain amount of practical experience to decide how to
    set the values of numerical meta-parameters such as the learning rate, the momentum,
    the weight-cost, the sparsity target, the initial values of the weights, the number
    of hidden units and the size of each mini-batch. There are also decisions to be
    made about what types of units to use, whether to update their states stochastically
    or deterministically, how many times to update the states of the hidden units
    for each training case, and whether to start each sequence of state updates at
    a data-vector. In addition, it is useful to know how to monitor the progress of
    learning and when to terminate the training.'
  id: totrans-7111
  prefs: []
  type: TYPE_NORMAL
  zh: '[5]。这需要一定的实践经验，以决定如何设置数值元参数的值，如学习率、动量、权重成本、稀疏性目标、权重的初始值、隐藏单元的数量以及每个小批量的大小。还需要对使用何种类型的单元、是随机更新它们的状态还是确定性更新、对每个训练样本更新隐藏单元的状态次数，以及是否从数据向量开始每个状态更新序列做出决策。此外，了解如何监控学习进展以及何时终止训练也很有用。'
- en: For any particular application, the code that was used gives a complete specification
    of all of these decisions, but it does not explain why the decisions were made
    or how minor changes will affect performance. More significantly, it does not
    provide a novice user with any guidance about how to make good decisions for a
    new application. This requires some sensible heuristics and the ability to relate
    failures of the learning to the decisions that caused those failures.
  id: totrans-7112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何特定应用，所使用的代码提供了所有这些决策的完整规范，但并没有解释为什么做出这些决策，也没有说明小的变化将如何影响性能。更重要的是，它没有为新手用户提供关于如何为新应用做出良好决策的任何指导。这需要一些合理的启发式方法，以及将学习失败与导致这些失败的决策联系起来的能力。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    599–619, 2012.'
  id: totrans-7113
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon 等编：NN：行业诀窍，第2版，LNCS 7700，第599–619页，2012年。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-7114
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: Over the last few years, the machine learning group at the University of Toronto
    has acquired considerable expertise at training RBMs and this guide is an attempt
    to share this expertise with other machine learning researchers.
  id: totrans-7115
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，多伦多大学的机器学习小组在训练限制玻尔兹曼机方面积累了相当多的专业知识，而本指南试图与其他机器学习研究人员分享这种专业知识。
- en: 24.2 An Overview Of Restricted Boltzmann Machines And Contrastive Divergence
  id: totrans-7116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.2 限制玻尔兹曼机和对比散度概述
- en: 'Skip this section if you already know about RBMs Consider a training set of
    binary vectors which we will assume are binary images for the purposes of explanation.
    The training set can be modeled using a two-layer network called a "Restricted
    Boltzmann Machine" [18, 2, 5] in which stochastic, binary pixels are connected
    to stochastic, binary feature detectors using symmetrically weighted connections.
    The pixels correspond to "visible" units of the RBM because their states are observed;
    the feature detectors correspond to "hidden" units. A joint configuration, (v,
    h) of the visible and hidden units has an energy [9] given by:'
  id: totrans-7117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经了解限制玻尔兹曼机，请跳过此部分。考虑一个二进制向量的训练集，为了说明，我们假设这些是二进制图像。该训练集可以使用称为“限制玻尔兹曼机”的两层网络建模[18,
    2, 5]，在该网络中，随机的二进制像素与使用对称加权连接的随机二进制特征检测器相连。像素对应于RBM的“可见”单元，因为它们的状态是可观察的；特征检测器对应于“隐藏”单元。可见和隐藏单元的联合配置(v,
    h)的能量[9]给出为：
- en: $$E(\mathbf{v},\mathbf{h})=-\sum_{i\in\mathrm{visible}}a_{i}v_{i}-\sum_{j\in\mathrm{hidden}}b_{j}h_{j}-\sum_{i,j}v_{i}h_{j}w_{i
    j}$$
  id: totrans-7118
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(\mathbf{v},\mathbf{h})=-\sum_{i\in\mathrm{visible}}a_{i}v_{i}-\sum_{j\in\mathrm{hidden}}b_{j}h_{j}-\sum_{i,j}v_{i}h_{j}w_{i
    j}$$
- en: vihjwij (24.1)
  id: totrans-7119
  prefs: []
  type: TYPE_NORMAL
  zh: vihjwij (24.1)
- en: where vi, hj are the binary states of visible unit i and hidden unit j, ai,
    bj are their biases and wij is the weight between them. The network assigns a
  id: totrans-7120
  prefs: []
  type: TYPE_NORMAL
  zh: 其中vi, hj是可见单元i和隐藏单元j的二进制状态，ai, bj是它们的偏置，wij是它们之间的权重。网络分配一个
- en: 'probability to every possible pair of a visible and a hidden vector via this
    energy function:'
  id: totrans-7121
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个能量函数，给每一对可见向量和隐藏向量的概率：
- en: $$p(\mathbf{v},\mathbf{h})={\frac{1}{Z}}\;e^{-E(\mathbf{v},\mathbf{h})}$$
  id: totrans-7122
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(\mathbf{v},\mathbf{h})={\frac{1}{Z}}\;e^{-E(\mathbf{v},\mathbf{h})}$$
- en: 'where the "partition function", Z, is given by summing over all possible pairs
    of visible and hidden vectors:'
  id: totrans-7123
  prefs: []
  type: TYPE_NORMAL
  zh: 其中“分区函数”Z通过对所有可能的可见向量和隐藏向量对进行求和来给出：
- en: '$ Z$, is given by summing over all possible :  $ Z=\sum_{\mathbf{v,h}}e^{-E(\mathbf{v,h})}$  where
    $ \mathbf{v,h}$ are invertible matrices.'
  id: totrans-7124
  prefs: []
  type: TYPE_NORMAL
  zh: $Z$由所有可能的求和给出：$Z=\sum_{\mathbf{v,h}}e^{-E(\mathbf{v,h})}$，其中$\mathbf{v,h}$是可逆矩阵。
- en: 'The probability that the network assigns to a visible vector, v, is given by
    summing over all possible hidden vectors:'
  id: totrans-7125
  prefs: []
  type: TYPE_NORMAL
  zh: 网络为可见向量$v$分配的概率由所有可能的隐藏向量的总和给出：
- en: $$p(\mathbf{v})={\frac{1}{Z}}\sum_{\mathbf{h}}e^{-E(\mathbf{v},\mathbf{h})}$$
  id: totrans-7126
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(\mathbf{v})={\frac{1}{Z}}\sum_{\mathbf{h}}e^{-E(\mathbf{v},\mathbf{h})}$$
- en: $$(24.1)$$
  id: totrans-7127
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.1)$$
- en: $$(24.2)$$
  id: totrans-7128
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.2)$$
- en: $$\mathrm{{\;\;airs\;of\;\;}}$$ $$\mathrm{(24.3)}$$
  id: totrans-7129
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{{\;\;对\;\;}}$$ $$\mathrm{(24.3)}$$
- en: $$(24.4)$$
  id: totrans-7130
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.4)$$
- en: $$(24.5)$$
  id: totrans-7131
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.5)$$
- en: The probability that the network assigns to a training image can be raised by
    adjusting the weights and biases to lower the energy of that image and to raise
    the energy of other images, especially those that have low energies and therefore
    make a big contribution to the partition function. The derivative of the log probability
    of a training vector with respect to a weight is surprisingly simple.
  id: totrans-7132
  prefs: []
  type: TYPE_NORMAL
  zh: 网络为训练图像分配的概率可以通过调整权重和偏差来提高，从而降低该图像的能量，并提高其他图像的能量，尤其是那些能量低、对分区函数贡献大的图像。训练向量的对数概率关于权重的导数出奇的简单。
- en: $${\frac{\partial\log p(\mathbf{v})}{\partial w_{i j}}}=\langle v_{i}h_{j}\rangle_{d
    a t a}-\langle v_{i}h_{j}\rangle_{m o d e l}$$
  id: totrans-7133
  prefs: []
  type: TYPE_NORMAL
  zh: $${\frac{\partial\log p(\mathbf{v})}{\partial w_{i j}}}=\langle v_{i}h_{j}\rangle_{d
    a t a}-\langle v_{i}h_{j}\rangle_{m o d e l}$$
- en: $$(24.6)$$
  id: totrans-7134
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.6)$$
- en: $$(24.7)$$
  id: totrans-7135
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.7)$$
- en: where the angle brackets are used to denote expectations under the distribution
  id: totrans-7136
  prefs: []
  type: TYPE_NORMAL
  zh: 其中尖括号用于表示在该分布下的期望。
- en: specified by the subscript that follows. This leads to a very simple learning
    rule for performing stochastic steepest ascent in the log probability of the training
  id: totrans-7137
  prefs: []
  type: TYPE_NORMAL
  zh: 由后续下标指定。这导致了一个非常简单的学习规则，用于在训练的对数概率中执行随机最陡上升。
- en: 'data:'
  id: totrans-7138
  prefs: []
  type: TYPE_NORMAL
  zh: 数据：
- en: $$\Delta w_{i j}=\epsilon(\langle v_{i}h_{j}\rangle_{d a t a}-\langle v_{i}h_{j}\rangle_{m
    o d e l})$$
  id: totrans-7139
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w_{i j}=\epsilon(\langle v_{i}h_{j}\rangle_{d a t a}-\langle v_{i}h_{j}\rangle_{m
    o d e l})$$
- en: where  is a learning rate.
  id: totrans-7140
  prefs: []
  type: TYPE_NORMAL
  zh: 其中是学习率。
- en: Because there are no direct connections between hidden units in an RBM, it
  id: totrans-7141
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在RBM中隐藏单元之间没有直接连接，
- en: is very easy to get an unbiased sample of vihj *data*. Given a randomly selected
  id: totrans-7142
  prefs: []
  type: TYPE_NORMAL
  zh: 获取$ v_{i}h_{j} *数据*的无偏样本非常简单。给定一个随机选择的
- en: training image, v, the binary state, hj , of each hidden unit, j, is set to
    1 with
  id: totrans-7143
  prefs: []
  type: TYPE_NORMAL
  zh: 训练图像$v$，每个隐藏单元$j$的二进制状态$h_j$被设置为1，
- en: probability
  id: totrans-7144
  prefs: []
  type: TYPE_NORMAL
  zh: 概率
- en: viwij ) (24.7)
  id: totrans-7145
  prefs: []
  type: TYPE_NORMAL
  zh: viwij ) (24.7)
- en: $$p(h_{j}=1\mid\mathbf{v})=\sigma(b_{j}+\sum_{i}v_{i}w_{i j})$$
  id: totrans-7146
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(h_{j}=1\mid\mathbf{v})=\sigma(b_{j}+\sum_{i}v_{i}w_{i j})$$
- en: where σ(x) is the logistic sigmoid function 1/(1 + exp(−x)). vihj is then an
  id: totrans-7147
  prefs: []
  type: TYPE_NORMAL
  zh: 其中σ(x)是逻辑Sigmoid函数1/(1 + exp(−x))。$v_{i}h_{j}$然后是
- en: unbiased sample.
  id: totrans-7148
  prefs: []
  type: TYPE_NORMAL
  zh: 无偏样本。
- en: Because there are no direct connections between visible units in an RBM, it
  id: totrans-7149
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在RBM中可见单元之间没有直接连接，
- en: is also very easy to get an unbiased sample of the state of a visible unit,
    *given a*
  id: totrans-7150
  prefs: []
  type: TYPE_NORMAL
  zh: 获取可见单元状态的无偏样本也很容易，*给定一个*
- en: hidden vector
  id: totrans-7151
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏向量生成“重构”。
- en: $$p(v_{i}=1\mid\mathbf{h})=\sigma(a_{i}+\sum_{j}h_{j}w_{i j})$$
  id: totrans-7152
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(v_{i}=1\mid\mathbf{h})=\sigma(a_{i}+\sum_{j}h_{j}w_{i j})$$
- en: hjwij ) (24.8)
  id: totrans-7153
  prefs: []
  type: TYPE_NORMAL
  zh: hjwij ) (24.8)
- en: $$(24.8)$$
  id: totrans-7154
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.8)$$
- en: Getting an unbiased sample of vihj *model*, however, is much more difficult.
    It
  id: totrans-7155
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，获取$ v_{i}h_{j} *模型*的无偏样本要困难得多。它
- en: can be done by starting at any random state of the visible units and performing
    alternating Gibbs sampling for a very long time. One iteration of alternating
    Gibbs sampling consists of updating all of the hidden units in parallel using
    equation 24.7 followed by updating all of the visible units in parallel using
    equation
  id: totrans-7156
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过从可见单元的任意随机状态开始，并进行很长时间的交替吉布斯采样来完成。一轮交替吉布斯采样包括使用方程24.7并行更新所有隐藏单元，然后使用方程
- en: 24.8.
  id: totrans-7157
  prefs: []
  type: TYPE_NORMAL
  zh: 24.8.
- en: A much faster learning procedure was proposed in [5]. This starts by setting
  id: totrans-7158
  prefs: []
  type: TYPE_NORMAL
  zh: 在[5]中提出了一种更快的学习过程。这开始于设置
- en: the states of the visible units to a training vector. Then the binary states
    of the
  id: totrans-7159
  prefs: []
  type: TYPE_NORMAL
  zh: 将可见单元的状态设置为训练向量。然后，隐藏单元的二进制状态
- en: hidden units are all computed in parallel using equation 24.7. Once binary states
    have been chosen for the hidden units, a "reconstruction" is produced by setting
  id: totrans-7160
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏单元都是使用方程24.7并行计算的。一旦为隐藏单元选择了二进制状态，就会通过设置
- en: each vi to 1 with a probability given by equation 24.8. The change in a weight
  id: totrans-7161
  prefs: []
  type: TYPE_NORMAL
  zh: 每个$v_{i}$以方程24.8给定的概率设置为1。权重的变化
- en: is then given by
  id: totrans-7162
  prefs: []
  type: TYPE_NORMAL
  zh: 随后由以下公式给出：
- en: $$\Delta w_{i j}=\epsilon(\langle v_{i}h_{j}\rangle_{d a t a}-\langle v_{i}h_{j}\rangle_{r
    e c o n})$$
  id: totrans-7163
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w_{i j}=\epsilon(\langle v_{i}h_{j}\rangle_{d a t a}-\langle v_{i}h_{j}\rangle_{r
    e c o n})$$
- en: $$(24.9)$$
  id: totrans-7164
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.9)$$
- en: A simplified version of the same learning rule that uses the states of individual
    units instead of pairwise products is used for the biases.
  id: totrans-7165
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简化版本的学习规则使用单个单元的状态而不是成对乘积来处理偏置。
- en: The learning works well even though it is only crudely approximating the
  id: totrans-7166
  prefs: []
  type: TYPE_NORMAL
  zh: 学习效果很好，尽管它只是粗略地逼近
- en: gradient of the log probability of the training data [5]. The learning rule
    is much
  id: totrans-7167
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据的对数概率的梯度[5]。学习规则更加
- en: more closely approximating the gradient of another objective function called
    the
  id: totrans-7168
  prefs: []
  type: TYPE_NORMAL
  zh: 更接近另一个目标函数的梯度，这个目标函数称为
- en: Contrastive Divergence [5] which is the difference between two Kullback-Leibler
    divergences, but it ignores one tricky term in this objective function so it is
    not even following that gradient. Indeed, Sutskever and Tieleman have shown that
  id: totrans-7169
  prefs: []
  type: TYPE_NORMAL
  zh: 对比散度[5]是两个Kullback-Leibler散度之间的差异，但它忽略了这个目标函数中的一个棘手项，因此它甚至没有遵循那个梯度。确实，Sutskever和Tieleman已经证明
- en: it is not following the gradient of any function [19]. Nevertheless, it works
    well enough to achieve success in many significant applications.
  id: totrans-7170
  prefs: []
  type: TYPE_NORMAL
  zh: 它并没有遵循任何函数的梯度[19]。尽管如此，它在许多重要应用中效果足够好。
- en: RBMs typically learn better models if more steps of alternating Gibbs sampling
    are used before collecting the statistics for the second term in the learning
    rule, which will be called the negative statistics. CDn will be used to denote
    learning using n full steps of alternating Gibbs sampling.
  id: totrans-7171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在收集学习规则中第二项的统计数据之前使用更多步交替Gibbs采样，RBMs通常会学习到更好的模型，这将称为负统计量。CDn将用于表示使用n步完整交替Gibbs采样进行学习。
- en: 24.3 How To Collect Statistics When Using Contrastive Divergence
  id: totrans-7172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.3 如何在使用对比散度时收集统计数据
- en: To begin with, we shall assume that all of the visible and hidden units are
    binary.
  id: totrans-7173
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们假设所有可见和隐藏单元都是二进制的。
- en: Other types of units will be discussed in section 24.13. We shall also assume
    that the purpose of the learning is to create a good generative model of the set
    of training vectors. When using RBMs to learn Deep Belief Nets (see the article
    on Deep Belief Networks at www.scholarpedia.org) that will subsequently be finetuned
    using backpropagation, the generative model is not the ultimate objective and
    it may be possible to save time by underfitting it, but we will ignore that here.
  id: totrans-7174
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类型的单元将在24.13节中讨论。我们还将假设学习的目的是创建一组训练向量的良好生成模型。当使用RBMs学习深度信念网络（请参见www.scholarpedia.org上的深度信念网络文章）并随后使用反向传播进行微调时，生成模型并不是最终目标，可能通过欠拟合来节省时间，但我们在这里将忽略这一点。
- en: 24.3.1 Updating The Hidden States
  id: totrans-7175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.3.1 更新隐藏状态
- en: 'Assuming that the hidden units are binary and that you are using CD1, the hidden
    units should have stochastic binary states when they are being driven by a data-vector.
    The probability of turning on a hidden unit, j, is computed by applying the logistic
    function σ(x)=1/(1 + exp(−x)) to its "total input":'
  id: totrans-7176
  prefs: []
  type: TYPE_NORMAL
  zh: 假设隐藏单元是二进制的，并且你正在使用CD1，当它们被数据向量驱动时，隐藏单元应该具有随机二进制状态。激活隐藏单元j的概率是通过将逻辑函数σ(x)=1/(1
    + exp(−x))应用于其“总输入”计算的：
- en: $$p(h_{j}=1)=\sigma(b_{j}+\sum_{i}v_{i}w_{i j})$$
  id: totrans-7177
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(h_{j}=1)=\sigma(b_{j}+\sum_{i}v_{i}w_{i j})$$
- en: $$(24.10)$$
  id: totrans-7178
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.10)$$
- en: and the hidden unit turns on if this probability is greater than a random number
    uniformly distributed between 0 and 1.
  id: totrans-7179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个概率大于在0和1之间均匀分布的随机数，隐藏单元就会被激活。
- en: It is very important to make these hidden states binary, rather than using the
    probabilities themselves. If the probabilities are used, each hidden unit can
    communicate a real-value to the visible units during the reconstruction. This
    seriously violates the information bottleneck created by the fact that a hidden
    unit can convey at most one bit (on average). This information bottleneck acts
    as a strong regularizer.
  id: totrans-7180
  prefs: []
  type: TYPE_NORMAL
  zh: 使这些隐藏状态为二进制形式非常重要，而不是使用概率本身。如果使用概率，每个隐藏单元在重建过程中可以向可见单元传达实值。这严重违反了隐藏单元至多只能传达一位（平均而言）所创建的信息瓶颈。这个信息瓶颈充当了强有力的正则化器。
- en: For the last update of the hidden units, it is silly to use stochastic binary
    states because nothing depends on which state is chosen. So use the probability
    itself to avoid unnecessary sampling noise. When using CDn, only the final update
    of the hidden units should use the probability.
  id: totrans-7181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于隐藏单元的最后一次更新，使用随机二进制状态是愚蠢的，因为没有任何东西依赖于选择哪个状态。因此使用概率本身以避免不必要的采样噪声。在使用CDn时，只有隐藏单元的最终更新应该使用概率。
- en: 24.3.2 Updating The Visible States
  id: totrans-7182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.3.2 更新可见状态
- en: 'Assuming that the visible units are binary, the correct way to update the visible
    states when generating a reconstruction is to stochastically pick a 1 or 0 with
    a probability determined by the total top-down input:'
  id: totrans-7183
  prefs: []
  type: TYPE_NORMAL
  zh: 假设可见单元为二元，当生成重构时，更新可见状态的正确方法是随机选择1或0，其概率由总的自上而下输入决定：
- en: $p_{i}=p(v_{i}=1)=\sigma(a_{i}+\sum_{j}h_{j}w_{ij})$ (24.11)
  id: totrans-7184
  prefs: []
  type: TYPE_NORMAL
  zh: $p_{i}=p(v_{i}=1)=\sigma(a_{i}+\sum_{j}h_{j}w_{ij})$ (24.11)
- en: However, it is common to use the probability, pi, instead of sampling a binary
    value. This is not nearly as problematic as using probabilities for the data-driven
    hidden states and it reduces sampling noise thus allowing faster learning. There
    is some evidence that it leads to slightly worse density models (Tijmen Tieleman,
    personal communication, 2008). This probably does not matter when using an RBM
    to pretrain a layer of hidden features for use in a deep belief net.
  id: totrans-7185
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通常使用概率pi，而不是抽样二元值。这并不像对数据驱动的隐藏状态使用概率那样成问题，且减少了抽样噪声，从而允许更快的学习。有证据表明这会导致略差的密度模型（Tijmen
    Tieleman, 个人通信, 2008）。在使用RBM对深度信念网络的隐藏特征层进行预训练时，这可能并不重要。
- en: 24.3.3 Collecting The Statistics Needed For Learning
  id: totrans-7186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.3.3 收集学习所需的统计信息
- en: 'Assuming that the visible units are using real-valued probabilities instead
    of stochastic binary values, there are two sensible ways to collect the positive
    statistics for the connection between visible unit i and hidden unit j:'
  id: totrans-7187
  prefs: []
  type: TYPE_NORMAL
  zh: 假设可见单元使用实值概率而不是随机二元值，收集可见单元i与隐藏单元j之间正统计的两种合理方法是：
- en: $$\langle p_{i}h_{j}\rangle_{\mathrm{data}}\quad\quad\mathrm{or}\quad\quad\langle
    p_{i}p_{j}\rangle_{\mathrm{data}}$$
  id: totrans-7188
  prefs: []
  type: TYPE_NORMAL
  zh: $$\langle p_{i}h_{j}\rangle_{\mathrm{data}}\quad\quad\mathrm{or}\quad\quad\langle
    p_{i}p_{j}\rangle_{\mathrm{data}}$$
- en: where pj is a probability and hj is a binary state that takes value 1 with probability
    pj . Using hj is closer to the mathematical model of an RBM, but using pj usually
    has less sampling noise which allows slightly faster learning1.
  id: totrans-7189
  prefs: []
  type: TYPE_NORMAL
  zh: 其中pj是一个概率，hj是一个二元状态，其取值为1的概率为pj。使用hj更接近RBM的数学模型，但使用pj通常噪声更小，从而允许略快的学习。
- en: 24.3.4 A Recipe For Getting The Learning Signal For Cd1
  id: totrans-7190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.3.4 获取Cd1学习信号的食谱
- en: When the hidden units are being driven by data, *always* use stochastic binary
    states. When they are being driven by reconstructions, always use probabilities
    without sampling.
  id: totrans-7191
  prefs: []
  type: TYPE_NORMAL
  zh: 当隐藏单元被数据驱动时，*始终*使用随机二元状态。当它们被重构驱动时，始终使用无抽样的概率。
- en: Assuming the visible units use the logistic function, use real-valued probabilities
    for both the data and the reconstructions2.
  id: totrans-7192
  prefs: []
  type: TYPE_NORMAL
  zh: 假设可见单元使用逻辑函数，使用实值概率来处理数据和重构。
- en: When collecting the pairwise statistics for learning weights or the individual
    statistics for learning biases, use the probabilities, not the binary states,
    and make sure the weights have random initial values to break symmetry.
  id: totrans-7193
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集学习权重的成对统计或学习偏置的单独统计时，使用概率而不是二元状态，并确保权重具有随机初始值以打破对称性。
- en: 24.4 The Size Of A Mini-Batch
  id: totrans-7194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.4 小批量的大小
- en: It is possible to update the weights after estimating the gradient on a single
    training case, but it is often more efficient to divide the training set into
    small
  id: totrans-7195
  prefs: []
  type: TYPE_NORMAL
  zh: 在对单个训练样本估计梯度后，可以更新权重，但将训练集划分为小“迷你批量”往往更有效。
- en: '"mini-batches" of 10 to 100 cases3. This allows matrix-matrix multiplies to
    be used which is very advantageous on GPU boards or in Matlab.'
  id: totrans-7196
  prefs: []
  type: TYPE_NORMAL
  zh: “迷你批量”包含10到100个样本。这允许使用矩阵乘法，这在GPU板或Matlab中非常有利。
- en: 1 Using hj always creates more noise in the positive statistics than using pj
    but it can actually create less noise in the *difference* of the positive and
    negative statistics because the negative statistics depend on the binary decision
    for the state of j that is used for creating the reconstruction. The probability
    of j when driven by the reconstruction is highly correlated with the binary decision
    that was made for j when it was driven by the data.
  id: totrans-7197
  prefs: []
  type: TYPE_NORMAL
  zh: 1 使用hj总会在正统计中产生比使用pj更多的噪声，但在正负统计的*差异*中可能产生更少的噪声，因为负统计依赖于用于生成重构的j的状态的二元决策。当重构驱动j时，j的概率与当数据驱动j时做出的二元决策高度相关。
- en: 2 So there is nothing random about the generation of the reconstructions given
    the binary states of the hidden units. 3 The word "batch" is confusing and will
    be avoided because when it is used to contrast with "on-line" it usually means
    the entire training set.
  id: totrans-7198
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，给定隐藏单元的二进制状态，重建的生成没有任何随机性。3 “批次”这个词令人困惑，并且会被避免，因为当它与“在线”对比时，它通常意味着整个训练集。
- en: To avoid having to change the learning rate when the size of a mini-batch is
    changed, it is helpful to divide the total gradient computed on a mini-batch by
    the size of the mini-batch, so when talking about learning rates we will assume
    that they multiply the average, per-case gradient computed on a mini-batch, not
    the total gradient for the mini-batch.
  id: totrans-7199
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在更改小批次大小时需要更改学习率，将在小批次上计算的总梯度除以小批次的大小是有帮助的，因此在谈论学习率时，我们假设它们是乘以在小批次上计算的平均每个样本梯度，而不是小批次的总梯度。
- en: It is a serious mistake to make the mini-batches too large when using stochastic
    gradient descent (see chapter 1 and 18 for more details). Increasing the minibatch
    size by a factor of N leads to a more reliable gradient estimate but it does not
    increase the maximum stable learning rate by a factor of N, so the net effect
    is that the weight updates are smaller *per gradient evaluation*4.
  id: totrans-7200
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用随机梯度下降时，将小批次做得过大是一个严重的错误（有关更多细节，请参见第1章和第18章）。将小批次大小增加N倍会导致更可靠的梯度估计，但并不会使最大稳定学习率增加N倍，因此净效果是每次梯度评估时权重更新变得更小。
- en: 24.4.1 A Recipe For Dividing The Training Set Into Mini-Batches
  id: totrans-7201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.4.1 将训练集划分为小批次的方案
- en: For datasets that contain a small number of equiprobable classes, the ideal
    minibatch size is often equal to the number of classes and each mini-batch should
    contain one example of each class to reduce the sampling error when estimating
    the gradient for the whole training set from a single mini-batch. For other datasets,
    first randomize the order of the training examples then use minibatches of size
    about 10.
  id: totrans-7202
  prefs: []
  type: TYPE_NORMAL
  zh: 对于包含少数等概率类别的数据集，理想的小批次大小通常等于类别的数量，每个小批次应包含每个类别的一个示例，以减少从单个小批次估计整个训练集梯度时的采样误差。对于其他数据集，首先随机化训练示例的顺序，然后使用约10的大小的小批次。
- en: 24.5 Monitoring The Progress Of Learning
  id: totrans-7203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.5 监控学习进度
- en: It is easy to compute the squared error between the data and the reconstructions,
    so this quantity is often printed out during learning. The reconstruction error
    on the entire training set should fall rapidly and consistently at the start of
    learning and then more slowly. Due to the noise in the gradient estimates, the
    reconstruction error on the individual mini-batches will fluctuate gently after
    the initial rapid descent. It may also oscillate gently with a period of a few
    mini-batches when using high momentum (see section 24.9).
  id: totrans-7204
  prefs: []
  type: TYPE_NORMAL
  zh: 计算数据与重建之间的平方误差是很简单的，因此这个量在学习过程中经常被打印出来。整个训练集上的重建误差应该在学习开始时迅速且稳定地下降，然后逐渐减缓。由于梯度估计中的噪声，个别小批次的重建误差在最初的快速下降后会轻微波动。在使用高动量时，它也可能在几个小批次的周期内轻微震荡（见第24.9节）。
- en: Although it is convenient, the reconstruction error is actually a very poor
    measure of the progress of learning. It is not the function that CDn learning
    is approximately optimizing, especially for *n >>* 1, and it systematically confounds
    two different quantities that are changing during the learning. The first is the
    difference between the empirical distribution of the training data and the equilibrium
    distribution of the RBM. The second is the mixing rate of the alternating Gibbs
    Markov chain. If the mixing rate is very low, the reconstruction error will be
    very small even when the distributions of the data and the model are very different.
    As the weights increase the mixing rate falls, so decreases 4 The easy way to
    parallelize the learning on a cluster is to divide each mini-batch into sub-mini-batches
    and to use different cores to compute the gradients on each sub-mini-batch. The
    gradients computed by different cores must then be combined. To minimize the ratio
    of communication to computation, it is tempting to make the sub-mini-batches large.
    This usually makes the learning much less efficient, thus wiping out much of the
    gain achieved by using multiple cores (Vinod Nair, personal communication, 2007).
  id: totrans-7205
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管方便，重建误差实际上是衡量学习进展的一个非常差的指标。它不是CDn学习大致优化的函数，尤其是当*n >>* 1时，并且它系统性地混淆了学习过程中变化的两个不同量。第一个是训练数据的经验分布与RBM的平衡分布之间的差异。第二个是交替吉布斯马尔可夫链的混合率。如果混合率非常低，即使数据和模型的分布非常不同，重建误差也会非常小。随着权重的增加，混合率下降，因此降低了。并行化学习的简单方法是在集群中将每个小批次划分为子小批次，并使用不同的核心计算每个子小批次的梯度。由不同核心计算的梯度必须进行合并。为了最小化通信与计算的比率，诱惑是使子小批次较大。这通常会使学习效率大大降低，从而消除使用多个核心所获得的大部分收益（Vinod
    Nair，个人通信，2007年）。
- en: in reconstruction error do not necessarily mean that the model is improving
    and, conversely, small increases do not necessarily mean the model is getting
    worse. Large increases, however, are a bad sign except when they are temporary
    and caused by changes in the learning rate, momentum, weight-cost or sparsity
    meta-parameters.
  id: totrans-7206
  prefs: []
  type: TYPE_NORMAL
  zh: 重建误差的增大并不一定意味着模型在改善，相反，轻微的增幅也不一定意味着模型在变差。然而，显著的增幅是个坏兆头，除非它们是暂时性的，并且是由学习率、动量、权重成本或稀疏性元参数的变化引起的。
- en: 24.5.1 A Recipe For Using The Reconstruction Error
  id: totrans-7207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.5.1 使用重建误差的配方
- en: Use it but don't trust it. If you really want to know what is going on during
    the learning, use multiple histograms and graphic displays as described in section
    24.15. Also consider using Annealed Importance Sampling [16] to estimate the density
    on held out data. If you are learning a joint density model of labelled data (see
    section 24.16), consider monitoring the discriminative performance on the training
    data and on a held out validation set.
  id: totrans-7208
  prefs: []
  type: TYPE_NORMAL
  zh: 使用它，但不要完全信任。如果你真的想知道学习过程中发生了什么，请使用多个直方图和图形显示，如第24.15节所述。还可以考虑使用退火重要性采样[16]来估计保留数据的密度。如果你在学习标记数据的联合密度模型（见第24.16节），请考虑监测训练数据和保留验证集上的判别性能。
- en: 24.6 Monitoring The Overfitting
  id: totrans-7209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.6 监测过拟合
- en: When learning a generative model, the obvious quantity to monitor is the probability
    that the current model assigns to a datapoint. When this probability starts to
    decrease for held out validation data, it is time to stop learning. Unfortunately,
    for large RBMs, it is very difficult to compute this probability because it requires
    knowledge of the partition function. Nevertheless, it is possible to directly
    monitor the overfitting by comparing the free energies of training data and held
    out validation data. In this comparison, the partition function cancels out. The
    free energy of a data vector can be computed in a time that is linear in the number
    of hidden units (see section 24.16.1). If the model is not overfitting at all,
    the average free energy should be about the same on training and validation data.
    As the model starts to overfit the average free energy of the validation data
    will rise relative to the average free energy of the training data and this gap
    represents the amount of overfitting5.
  id: totrans-7210
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习生成模型时，明显需要监测的量是当前模型对数据点分配的概率。当这个概率开始下降时，就是停止学习的时机。不幸的是，对于大型RBM，这个概率的计算非常困难，因为它需要了解配分函数。然而，可以通过比较训练数据和持出验证数据的自由能来直接监测过拟合。在这个比较中，配分函数会抵消。数据向量的自由能可以在与隐含单元数量线性相关的时间内计算（见24.16.1节）。如果模型没有过拟合，训练数据和验证数据的平均自由能应该大致相同。随着模型开始过拟合，验证数据的平均自由能会相对于训练数据的平均自由能上升，这个差距表示过拟合的程度。
- en: 24.6.1 A Recipe For Monitoring The Overfitting
  id: totrans-7211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.6.1 监测过拟合的方案
- en: After every few epochs, compute the average free energy of a representative
    subset of the training data and compare it with the average free energy of a validation
    set. Always use the same subset of the training data. If the gap starts growing,
    the model is overfitting, though the probability of the training data may be growing
    even faster than the gap, so the probability of the validation data may still
    be improving. Make sure that the same weights are used when computing the two
    averages that you wish to compare.
  id: totrans-7212
  prefs: []
  type: TYPE_NORMAL
  zh: 每经过几轮，计算一组代表性训练数据的平均自由能，并将其与验证集的平均自由能进行比较。始终使用相同的训练数据子集。如果差距开始增大，模型就是在过拟合，尽管训练数据的概率可能增长得比差距还快，因此验证数据的概率可能仍在改善。确保在计算你希望比较的两个平均值时使用相同的权重。
- en: 5 The average free energies often change by large amounts during learning and
    this means very little because the log partition function also changes by large
    amounts. It is only *differences* in free energies that are easy to interpret
    without knowing the partition function.
  id: totrans-7213
  prefs: []
  type: TYPE_NORMAL
  zh: 5 平均自由能在学习过程中通常会发生较大变化，这意味着其意义不大，因为对数配分函数也会发生较大变化。只有自由能的*差异*在不需要知道配分函数的情况下才容易解释。
- en: 24.7 The Learning Rate
  id: totrans-7214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.7 学习率
- en: If the learning rate is much too large, the reconstruction error usually increases
    dramatically and the weights may explode.
  id: totrans-7215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果学习率过大，重构误差通常会急剧增加，权重可能会爆炸。
- en: If the learning rate is reduced while the network is learning normally, the
    reconstruction error will usually fall significiantly. This is not necessarily
    a good thing. It is due, in part, to the smaller noise level in the stochastic
    weight updates and it is generally accompanied by slower learning in the long
    term. Towards the end of learning, however, it typically pays to decrease the
    learning rate. Averaging the weights across several updates is an alternative
    way to remove some of the noise from the final weights.
  id: totrans-7216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在网络正常学习时降低学习率，重构误差通常会显著下降。这不一定是好事。这部分是由于随机权重更新中的噪声水平较小，通常伴随着长期学习的速度减慢。然而，在学习结束时，通常减少学习率是有利的。对几个更新的权重进行平均是去除最终权重噪声的另一种方法。
- en: 24.7.1 A Recipe For Setting The Learning Rates For Weights And Biases
  id: totrans-7217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.7.1 设置权重和偏置学习率的方案
- en: A good rule of thumb for setting the learning rate (Max Welling, personal communication,
    2002) is to look at a histogram of the weight updates and a histogram of the weights.
    The updates should be about 10−3 times the weights (to within about an order of
    magnitude). When a unit has a very large fan-in, the updates should be smaller
    since many small changes in the same direction can easily reverse the sign of
    the gradient. Conversely, for biases, the updates can be bigger.
  id: totrans-7218
  prefs: []
  type: TYPE_NORMAL
  zh: 一个设定学习率的好经验法则（Max Welling, 个人通讯, 2002）是查看权重更新的直方图和权重的直方图。更新量应大约是权重的10−3倍（在数量级范围内）。当某个单元的输入量非常大时，更新量应更小，因为同一方向的许多小变化可能会轻易反转梯度的符号。相反，对于偏置，更新量可以更大。
- en: 24.8 The Initial Values Of The Weights And Biases
  id: totrans-7219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.8 权重和偏置的初始值
- en: The weights are typically initialized to small random values chosen from a zeromean
    Gaussian with a standard deviation of about 0.01. Using larger random values can
    speed the initial learning, but it may lead to a slightly worse final model. Care
    should be taken to ensure that the initial weight values do not allow typical
    visible vectors to drive the hidden unit probabilities very close to 1 or 0 as
    this significantly slows the learning. If the statistics used for learning are
    stochastic, the initial weights can all be zero since the noise in the statistics
    will make the hidden units become different from one another even if they all
    have identical connectivities.
  id: totrans-7220
  prefs: []
  type: TYPE_NORMAL
  zh: 权重通常初始化为从均值为零、标准差约为0.01的高斯分布中选择的小随机值。使用较大的随机值可以加速初始学习，但可能导致最终模型略差。应确保初始权重值不使得典型的可见向量将隐藏单元的概率驱动得过于接近1或0，因为这会显著减缓学习。如果用于学习的统计是随机的，初始权重可以全部为零，因为统计中的噪声会使隐藏单元彼此不同，即使它们具有相同的连接性。
- en: It is usually helpful to initialize the bias of visible unit i to log[pi/(1
    − pi)]
  id: totrans-7221
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，将可见单元 i 的偏置初始化为 log[pi/(1 − pi)] 是有帮助的。
- en: where pi is the proportion of training vectors in which unit i is on. If this
    is not done, the early stage of learning will use the hidden units to make i turn
    on with a probability of approximately pi.
  id: totrans-7222
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 pi 是单元 i 开启的训练向量的比例。如果不这样做，学习的早期阶段将使用隐藏单元使 i 以大约 pi 的概率开启。
- en: When using a sparsity target probability of t (see section 24.11), it makes
    sense to initialize the hidden biases to be log[t/(1−t)]. Otherwise, initial hidden
    biases of 0 are usually fine. It is also possible to start the hidden units with
    quite large negative biases of about −4 as a crude way of encouraging sparsity.
  id: totrans-7223
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用稀疏目标概率 t（见24.11节）时，将隐藏偏置初始化为 log[t/(1−t)] 是有意义的。否则，初始隐藏偏置为0通常是可以的。也可以用约−4的较大负偏置来粗略鼓励稀疏性。
- en: 24.8.1 A Recipe For Setting The Initial Values Of The Weights And Biases
  id: totrans-7224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.8.1 设置权重和偏置初始值的配方
- en: Use small random values for the weights chosen from a zero-mean Gaussian with
    a standard deviation of 0.01. Set the hidden biases to 0. Set the visible biases
    to log[pi/(1 − pi)] where pi is the proportion of training vectors in which unit
    i is on. Look at the activities of the hidden units occasionally to check that
    they are not always on or off.
  id: totrans-7225
  prefs: []
  type: TYPE_NORMAL
  zh: 使用从均值为零、标准差为0.01的高斯分布中选择的小随机值作为权重。将隐藏偏置设为0。将可见偏置设为 log[pi/(1 − pi)]，其中 pi 是单元
    i 开启的训练向量的比例。偶尔查看隐藏单元的活动，以检查它们是否总是开启或关闭。
- en: 24.9 Momentum
  id: totrans-7226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.9 动量
- en: Momentum is a simple method for increasing the speed of learning when the objective
    function contains long, narrow and fairly straight ravines with a gentle but consistent
    gradient along the floor of the ravine and much steeper gradients up the sides
    of the ravine. The momentum method simulates a heavy ball rolling down a surface.
    The ball builds up velocity along the floor of the ravine, but not across the
    ravine because the opposing gradients on opposite sides of the ravine cancel each
    other out over time. Instead of using the estimated gradient times the learning
    rate to increment the *values* of the parameters, the momentum method uses this
    quantity to increment the *velocity*, v, of the parameters and the current velocity
    is then used as the parameter increment.
  id: totrans-7227
  prefs: []
  type: TYPE_NORMAL
  zh: 动量法是一个简单的方法，可以加快学习速度，特别是当目标函数包含长而窄且相当直的沟壑，沟壑底部的梯度平缓但一致，沟壑两侧的梯度则更陡峭。动量法模拟了一个重球在表面上滚动。球在沟壑的底部积累速度，但在沟壑横跨的地方没有，因为沟壑两侧的相反梯度会随着时间的推移相互抵消。动量法不是用估计的梯度乘以学习率来增加参数的*值*，而是用这个量来增加参数的*速度*，v，然后当前的速度用作参数增量。
- en: 'The velocity of the ball is assumed to decay with time and the "momentum" meta-parmeter,
    α is the fraction of the previous velocity that remains after computing the gradient
    on a new mini-batch:'
  id: totrans-7228
  prefs: []
  type: TYPE_NORMAL
  zh: 假设球的速度会随着时间衰减，而“动量”超参数α是计算新小批量梯度后，保留的前一速度的比例：
- en: $$\Delta\theta_{i}(t)=v_{i}(t)=\alpha v_{i}(t-1)-\epsilon\frac{dE}{d\theta_{i}}(t)\tag{24.12}$$
  id: totrans-7229
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta\theta_{i}(t)=v_{i}(t)=\alpha v_{i}(t-1)-\epsilon\frac{dE}{d\theta_{i}}(t)\tag{24.12}$$
- en: If the gradient remains constant, the terminal velocity will exceed *dE/dθ*i
    by a factor of 1/(1 − α). This is a factor of 10 for a momentum of 0.9 which is
    a typical setting of this meta-parameter. The temporal smoothing in the momentum
    method avoids the divergent oscillations across the ravine that would be caused
    by simply increasing the learning rate by a factor of 1/(1 − α).
  id: totrans-7230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果梯度保持不变，终端速度将超过*dE/dθ*i的因子为1/(1 − α)。对于动量0.9，这个因子为10，这是该超参数的典型设置。动量法中的时间平滑避免了简单增加学习率所导致的跨沟壑的发散振荡，因子为1/(1
    − α)。
- en: The momentum method causes the parameters to move in a direction that is not
    the direction of steepest descent, so it bears some resemblance to methods like
    conjugate gradient, but the way it uses the previous gradients is much simpler.
    Unlike methods that use different learning rates for each parameter, momentum
    works just as well when the ravines are not aligned with the parameter axes.
  id: totrans-7231
  prefs: []
  type: TYPE_NORMAL
  zh: 动量法使参数沿着一个不是最陡下降方向的方向移动，因此与共轭梯度等方法有一些相似之处，但它使用先前梯度的方式要简单得多。与为每个参数使用不同学习率的方法不同，动量法在沟壑与参数轴不对齐时同样有效。
- en: 'An alternative way of viewing the momentum method (Tijmen Tieleman, personal
    communication, 2008) is as follows: It is equivalent to increasing the learning
    rate by a factor of 1/(1−α) but delaying the full effect of each gradient estimate
    by dividing the full increment into a series of exponentially decaying installments.
    This gives the system time to respond to the early installments by moving to a
    region of parameter space that has opposing gradients before it feels the full
    effect of the increment. This, in turn, allows the learning rate to be larger
    without causing unstable oscillations.'
  id: totrans-7232
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种看待动量法的方法（Tijmen Tieleman，个人通讯，2008）如下：它等同于将学习率增加一个因子1/(1−α)，但通过将完整增量分成一系列指数衰减的分期来延迟每次梯度估计的完全效果。这使系统有时间对早期分期作出反应，移动到具有相反梯度的参数空间区域，然后再感受到增量的全部效果。反过来，这允许学习率更大，而不会引起不稳定的振荡。
- en: At the start of learning, the random initial parameter values may create very
    large gradients and the system is unlikely to be in the floor of a ravine, so
    it is usually best to start with a low momentum of 0.5 for a number of parameter
    updates. This very conservative momentum typically makes the learning more stable
    than no momentum at all by damping oscillations across ravines [4].
  id: totrans-7233
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习开始时，随机初始化的参数值可能会产生非常大的梯度，系统不太可能处于沟壑的底部，因此通常最好在多个参数更新中以0.5的低动量开始。这种非常保守的动量通常使学习比没有动量时更稳定，通过减弱沟壑之间的振荡[4]。
- en: 24.9.1 A Recipe For Using Momentum
  id: totrans-7234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.9.1 动量法的使用方法
- en: Start with a momentum of 0.5. Once the large initial progress in the reduction
    of the reconstruction error has settled down to gentle progress, increase the
    momentum to 0.9. This shock may cause a transient increase in the reconstruction
    error. If this causes a more lasting instability, keep reducing the learning rate
    by factors of 2 until the instability disappears.
  id: totrans-7235
  prefs: []
  type: TYPE_NORMAL
  zh: 从0.5的动量开始。当重建误差的大幅初始下降稳定到缓慢进展时，将动量增加到0.9。这种冲击可能会导致重建误差的暂时增加。如果这导致更持久的不稳定，请将学习率按2的因子逐步降低，直到不稳定消失。
- en: 24.10 Weight-Decay
  id: totrans-7236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.10 权重衰减
- en: Weight-decay works by adding an extra term to the normal gradient. The extra
    term is the derivative of a function that penalizes large weights. The simplest
    penalty function, called "L2", is half of the sum of the squared weights times
    a coefficient which will be called the weight-cost.
  id: totrans-7237
  prefs: []
  type: TYPE_NORMAL
  zh: 权重衰减通过向正常梯度添加一个额外项来工作。这个额外项是惩罚大权重的函数的导数。最简单的惩罚函数称为"L2"，是平方权重总和的一半乘以一个称为权重成本的系数。
- en: It is important to multiply the derivative of the penalty term by the learning
    rate. Otherwise, changes in the learning rate change the function that is being
    optimized rather than just changing the optimization procedure.
  id: totrans-7238
  prefs: []
  type: TYPE_NORMAL
  zh: 将惩罚项的导数乘以学习率是很重要的。否则，学习率的变化会改变被优化的函数，而不仅仅是改变优化过程。
- en: There are four different reasons for using weight-decay in an RBM. The first
    is to improve generalization to new data by reducing overfitting to the training
    data6. The second is to make the receptive fields of the hidden units smoother
    and more interpretable by shrinking useless weights. The third is to "unstick"
    hidden units that have developed very large weights early in the training and
    are either always firmly on or always firmly off. A better way to allow such units
    to become useful again is to use a "sparsity" target as described in section 24.11.
  id: totrans-7239
  prefs: []
  type: TYPE_NORMAL
  zh: 在RBM中使用权重衰减有四个不同的原因。第一个是通过减少对训练数据的过拟合来提高对新数据的泛化能力。第二个是通过缩小无用权重，使隐藏单元的感受野更加平滑和可解释。第三个是“解除”在训练早期已经发展出非常大权重的隐藏单元，这些单元要么总是牢牢开启，要么总是牢牢关闭。允许这些单元再次变得有用的更好方法是使用在第24.11节中描述的“稀疏性”目标。
- en: 'The fourth reason is to improve the mixing rate of the alternating Gibbs Markov
    chain. With small weights, the Markov chain mixes more rapidly7. The CD learning
    procedure is based on ignoring derivatives that come from later steps in the Markov
    chain (Hinton, Osindero and Teh, 2006), so it tends to approximate maximum likelihood
    learning better when the mixing is fast. The ignored derivatives are then small
    for the following reason: When a Markov chain is very close to its stationary
    distribution, the best parameters for modeling samples from the chain are very
    close to its current parameters.'
  id: totrans-7240
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个原因是提高交替吉布斯马尔可夫链的混合速率。使用小权重时，马尔可夫链的混合速度更快。CD学习过程基于忽略来自马尔可夫链后续步骤的导数（Hinton,
    Osindero和Teh, 2006），因此当混合速度快时，它更倾向于更好地逼近最大似然学习。被忽略的导数小的原因是：当马尔可夫链非常接近其平稳分布时，建模链中样本的最佳参数非常接近其当前参数。
- en: 6 Since the penalty is applied on every mini-batch, Bayesians really ought to
    divide the weight-cost by the size of the training set. They can then interpret
    weight-decay as the effect of a Gaussian weight prior whose variance is independent
    of the size of the training set. This division is typically not done. Instead,
    larger weight-costs are used for smaller training sets. 7 With all zero weights,
    it reaches its rather boring stationary distribution in a single full step.
  id: totrans-7241
  prefs: []
  type: TYPE_NORMAL
  zh: 由于惩罚应用于每个小批量，贝叶斯方法实际上应该将权重成本除以训练集的大小。这样，他们可以将权重衰减解释为高斯权重先验的影响，其方差与训练集大小无关。通常不会进行这种除法。相反，对于较小的训练集，使用更大的权重成本。具有全零权重时，它在一个完整的步骤中达到相当无聊的平稳分布。
- en: A different form of weight-decay called "L1" is to use the derivative of the
    sum of the absolute values of the weights. This often causes many of the weights
    to become exactly zero whilst allowing a few of the weights to grow quite large.
    This can make it easier to interpret the weights. When learning features for images,
    for example, L1 weight-decay often leads to strongly localized receptive fields.
  id: totrans-7242
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种称为"L1"的权重衰减形式是使用绝对值权重之和的导数。这通常会导致许多权重变为零，同时允许少数权重变得相当大。这可以使解释权重变得更加容易。例如，在为图像学习特征时，L1权重衰减通常会导致强局部化的感受野。
- en: An alternative way to control the size of the weights is to impose a maximum
    allowed value on the sum of the squares or absolute values of the incoming weights
    for each unit. After each weight update, the weights are rescaled if they exceed
    this maximum value. This helps to avoid hidden units getting stuck with extremely
    small weights, but a sparsity target is probably a better way to avoid this problem.
  id: totrans-7243
  prefs: []
  type: TYPE_NORMAL
  zh: 控制权重大小的另一种方法是对每个单元的输入权重平方和或绝对值的最大允许值施加限制。在每次权重更新后，如果权重超过此最大值，则对其进行重新缩放。这有助于避免隐藏单元陷入极小的权重，但稀疏目标可能是避免这个问题的更好方法。
- en: 24.10.1 A Recipe For Using Weight-Decay
  id: totrans-7244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.10.1 使用权重衰减的食谱
- en: For an RBM, sensible values for the weight-cost coefficient for L2 weight-decay
    typically range from 0.01 to 0.00001. Weight-cost is typically not applied to
    the hidden and visible biases because there are far fewer of these so they are
    less likely to cause overfitting. Also, the biases sometimes need to be quite
    large.
  id: totrans-7245
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RBM，L2权重衰减的权重成本系数的合理值通常在0.01到0.00001之间。权重成本通常不应用于隐藏和可见偏差，因为这些数量较少，因此不太可能导致过拟合。此外，偏差有时需要相当大。
- en: Try an initial weight-cost of 0.0001. If you are using Annealed Importance Sampling
    [16] to estimate the density on a held-out validation set, try adjusting the weight-cost
    by factors of 2 to optimize density. Small differences in weightcost are unlikely
    to cause big differences in performance. If you are training a joint density model
    that allows you to test discriminative performance on a validation set this can
    be used in place of the density for optimizing the weightcost. However, in either
    case, remember that weight-decay does more than just preventing overfitting. It
    also increases the mixing rate which makes CD learning a better approximation
    to maximum likelihood. So even if overfitting is not a problem because the supply
    of training data is infinite, weight-decay can still be helpful.
  id: totrans-7246
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试初始权重成本为0.0001。如果您使用退火重要性采样[16]来估计保留验证集上的密度，请尝试通过2的倍数调整权重成本以优化密度。权重成本的小差异不太可能导致性能的大差异。如果您正在训练一个联合密度模型，可以在验证集上测试判别性能，这可以用来替代密度以优化权重成本。然而，在任何情况下，请记住，权重衰减不仅仅是防止过拟合。它还增加了混合速率，使CD学习更接近最大似然。因此，即使过拟合不是问题，因为训练数据的供应是无限的，权重衰减仍然是有益的。
- en: 24.11 Encouraging Sparse Hidden Activities
  id: totrans-7247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.11 鼓励稀疏的隐藏活动
- en: Hidden units that are only rarely active are typically easier to interpret than
    those that are active about half of the time. Also, discriminative performance
    is sometimes improved by using features that are only rarely active [13].
  id: totrans-7248
  prefs: []
  type: TYPE_NORMAL
  zh: 只有偶尔活跃的隐藏单元通常比那些大约一半时间活跃的单元更容易解释。此外，使用只有偶尔活跃的特征有时会改善判别性能[13]。
- en: 'Sparse activities of the binary hidden units can be achieved by specifying
    a "sparsity target" which is the desired probability of being active, *p <<* 1.
    An additional penalty term is then used to encourage the actual probability of
    being active, q, to be close to p. q is estimated by using an exponentially decaying
    average of the mean probability that a unit is active in each mini-batch:'
  id: totrans-7249
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定一个“稀疏目标”可以实现二元隐藏单元的稀疏活动，该目标是希望的活动概率，*p <<* 1。然后使用一个额外的惩罚项来鼓励实际的活动概率q接近p。q通过使用每个小批量中单元活动的平均概率的指数衰减平均值来估计：
- en: $$(24.13)$$
  id: totrans-7250
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.13)$$
- en: $$q_{n e w}=\lambda q_{o l d}+(1-\lambda)q_{c u r r e n t}$$
  id: totrans-7251
  prefs: []
  type: TYPE_NORMAL
  zh: $$q_{n e w}=\lambda q_{o l d}+(1-\lambda)q_{c u r r e n t}$$
- en: qnew = λqold + (1 − λ)q*current* (24.13)
  id: totrans-7252
  prefs: []
  type: TYPE_NORMAL
  zh: qnew = λqold + (1 − λ)q*current* (24.13)
- en: where q*current* is the mean activation probability of the hidden unit on the
    current mini-batch.
  id: totrans-7253
  prefs: []
  type: TYPE_NORMAL
  zh: 其中q*current*是当前小批量中隐藏单元的平均激活概率。
- en: 'The natural penalty measure to use is the cross entropy between the desired
    and actual distributions:'
  id: totrans-7254
  prefs: []
  type: TYPE_NORMAL
  zh: 自然的惩罚度量是期望分布与实际分布之间的交叉熵：
- en: $$\mathrm{{Sparisty~penalty}}\propto-p\log q-(1-p)\log(1-q)$$
  id: totrans-7255
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{{Sparisty~penalty}}\propto-p\log q-(1-p)\log(1-q)$$
- en: $$(24.14)$$
  id: totrans-7256
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.14)$$
- en: For logistic units this has a simple derivative of q − p with respect to the
    total input to a unit. This derivative, scaled by a meta-parameter called "sparsitycost",
    is used to adjust both the bias and the incoming weights of each hidden unit.
    It is important to apply the same derivative to both. If the derivative is only
    applied to the bias, for example, the bias will typically keep becoming more negative
    to ensure the hidden unit is rarely on, but the weights will keep becoming more
    positive to make the unit more useful.
  id: totrans-7257
  prefs: []
  type: TYPE_NORMAL
  zh: 对于逻辑单元，这具有相对于单元总输入的简单导数 q − p。这个导数乘以一个称为“稀疏成本”的元参数，用于调整每个隐藏单元的偏置和输入权重。将相同的导数应用于两者是很重要的。例如，如果导数仅应用于偏置，偏置通常会变得更加负，以确保隐藏单元很少激活，但权重会变得更积极，使单元更有用。
- en: 24.11.1 A Recipe For Sparsity
  id: totrans-7258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.11.1 稀疏性的配方
- en: Set the sparsity target to between 0.01 and 0.18. Set the decay-rate, λ, of
    the estimated value of q to be between 0.9 and 0.99. Histogram the mean activities
    of the hidden units and set the sparsity-cost so that the hidden units have mean
    probabilities in the vicinity of the target. If the probabilities are tightly
    clustered around the target value, reduce the sparsity-cost so that it interferes
    less with the main objective of the learning.
  id: totrans-7259
  prefs: []
  type: TYPE_NORMAL
  zh: 将稀疏目标设置在0.01到0.18之间。将估计的q值的衰减率λ设置在0.9到0.99之间。对隐藏单元的平均活动进行直方图处理，并设置稀疏成本，使得隐藏单元的平均概率接近目标值。如果概率紧密聚集在目标值附近，减少稀疏成本，以便其对学习的主要目标的干扰较少。
- en: 24.12 The Number Of Hidden Units
  id: totrans-7260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.12 隐藏单元的数量
- en: Intuitions derived from discriminative machine learning are a bad guide for
    determining a sensible number of hidden units. In discriminative learning, the
    amount of constraint that a training case imposes on the parameters is equal to
    the number of bits that it takes to specify the label. Labels usually contain
    very few bits of information, so using more parameters than training cases will
    typically cause severe overfitting. When learning generative models of high-dimensional
    data, however, it is the number of bits that it takes to specify a data vector
    that determines how much constraint each training case imposes on the parameters
    of the model. This can be several orders of magnitude greater than number of bits
    required to specify a label. So it may be quite reasonable to fit a million parameters
    to 10,000 training images if each image contains 1,000 pixels. This would allow
    1000 globally connected hidden units. If the hidden units are locally connected
    or if they use weight-sharing, many more can be used.
  id: totrans-7261
  prefs: []
  type: TYPE_NORMAL
  zh: 从判别机器学习中得出的直觉对于确定合理的隐藏单元数量是个糟糕的参考。在判别学习中，训练案例对参数施加的约束量等于指定标签所需的比特数。标签通常只包含很少的信息比特，因此使用比训练案例更多的参数通常会导致严重的过拟合。然而，在学习高维数据的生成模型时，决定每个训练案例对模型参数施加多少约束的是指定数据向量所需的比特数。这可能比指定标签所需的比特数高几个数量级。因此，如果每个图像包含1000个像素，那么将一百万个参数拟合到10,000个训练图像是非常合理的。这将允许1000个全连接的隐藏单元。如果隐藏单元是局部连接的，或者如果它们使用权重共享，则可以使用更多单元。
- en: 24.12.1 A Recipe For Choosing The Number Of Hidden Units
  id: totrans-7262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.12.1 选择隐藏单元数量的配方
- en: Assuming that the main issue is overfitting rather than the amount of computation
    at training or test time, estimate how many bits it would take to describe
  id: totrans-7263
  prefs: []
  type: TYPE_NORMAL
  zh: 假设主要问题是过拟合，而不是训练或测试时的计算量，估计描述所需的比特数。
- en: 8 If you are only using the sparsity target to revive hidden units that are
    never active and suppress hidden units that are always active, a target value
    of 0.5 makes sense
  id: totrans-7264
  prefs: []
  type: TYPE_NORMAL
  zh: 8 如果你仅使用稀疏目标来复活从未激活的隐藏单元并抑制始终激活的隐藏单元，0.5的目标值是有意义的。
- en: (even though it makes nonsense of the name).
  id: totrans-7265
  prefs: []
  type: TYPE_NORMAL
  zh: （尽管这使得这个名称变得毫无意义）。
- en: each data-vector if you were using a good model (*i.e.* estimate the typical
    negative log2 probability of a datavector under a good model). Then multiply that
    estimate by the number of training cases and use a number of parameters that is
    about an order of magnitude smaller. If you are using a sparsity target that is
    very small, you may be able to use more hidden units. If the training cases are
    highly redundant, as they typically will be for very big training sets, you need
    to use fewer parameters.
  id: totrans-7266
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用一个好的模型（*即* 估计在良好模型下数据向量的典型负对数概率），然后将该估计乘以训练案例的数量，并使用大约小一个数量级的参数数量。如果你使用的稀疏目标非常小，你可能可以使用更多的隐藏单元。如果训练案例高度冗余（对于非常大的训练集通常是这样的），则需要使用更少的参数。
- en: 24.13 Different Types Of Unit
  id: totrans-7267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.13 不同类型的单元
- en: RBM's were developed using binary visible and hidden units, but many other types
    of unit can also be used. A general treatment for units in the exponential family
    is given in [24]. The main use of other types of unit is for dealing with data
    that is not well-modeled by binary (or logistic) visible units.
  id: totrans-7268
  prefs: []
  type: TYPE_NORMAL
  zh: RBM 使用二元可见单元和隐藏单元开发，但许多其他类型的单元也可以使用。关于指数族单元的一般处理可参考[24]。其他类型单元的主要用途是处理不适合用二元（或逻辑）可见单元建模的数据。
- en: 24.13.1 Softmax And Multinomial Units
  id: totrans-7269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.13.1 Softmax 和多项式单元
- en: For a binary unit, the probability of turning on is given by the logistic sigmoid
    function of its total input, x.
  id: totrans-7270
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元单元，开启的概率由其总输入 x 的逻辑 sigmoid 函数给出。
- en: $$p=\sigma(x)={\frac{1}{1+e^{-x}}}={\frac{e^{x}}{e^{x}+e^{0}}}$$
  id: totrans-7271
  prefs: []
  type: TYPE_NORMAL
  zh: $$p=\sigma(x)={\frac{1}{1+e^{-x}}}={\frac{e^{x}}{e^{x}+e^{0}}}$$
- en: $$(24.15)$$
  id: totrans-7272
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.15)$$
- en: The energy contributed by the unit is −x if it is on and 0 if it is off. Equation
    24.15 makes it clear that the probability of each of the two possible states is
    proportional to the negative exponential of its energy. This can be generalized
    to K alternative states.
  id: totrans-7273
  prefs: []
  type: TYPE_NORMAL
  zh: 该单元贡献的能量是 −x 如果它处于开启状态，若关闭则为 0。方程 24.15 明确表明，每两种可能状态的概率与其能量的负指数成正比。这可以推广到 K
    种替代状态。
- en: $$(24.16)$$
  id: totrans-7274
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.16)$$
- en: $$p_{j}={\frac{e^{x_{j}}}{\sum_{i=1}^{K}e^{x_{i}}}}$$
  id: totrans-7275
  prefs: []
  type: TYPE_NORMAL
  zh: $$p_{j}={\frac{e^{x_{j}}}{\sum_{i=1}^{K}e^{x_{i}}}}$$
- en: This is often called a "softmax" unit. It is the appropriate way to deal with
    a quantity that has K alternative values which are not ordered in any way. A softmax
    can be viewed as a set of binary units whose states are mutually constrained so
    that exactly one of the K states has value 1 and the rest have value 0. When viewed
    in this way, the learning rule for the binary units in a softmax is identical
    to the rule for standard binary units. The only difference is in the way the probabilities
    of the states are computed and the samples are taken.
  id: totrans-7276
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常被称为“softmax”单元。这是处理具有 K 个不按任何方式排序的替代值的量的适当方法。softmax 可以看作是一组二元单元，其状态相互约束，使得
    K 个状态中恰好有一个状态的值为 1，其余为 0。从这个角度看，softmax 中二元单元的学习规则与标准二元单元的规则是相同的。唯一的区别在于状态的概率计算方式和样本的获取方式。
- en: A further generalization of the softmax unit is to sample N times (with replacement)
    from the probability distribution instead of just sampling once. The K different
    states can then have integer values bigger than 1, but the values must add to
    N. This is called a multinomial unit and, again, the learning rule is unchanged.
  id: totrans-7277
  prefs: []
  type: TYPE_NORMAL
  zh: 对 softmax 单元的进一步推广是从概率分布中进行 N 次抽样（有放回），而不仅仅是抽样一次。K 个不同的状态可以有大于 1 的整数值，但这些值必须加起来等于
    N。这被称为多项式单元，学习规则依然不变。
- en: 24.13.2 Gaussian Visible Units
  id: totrans-7278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.13.2 高斯可见单元
- en: 'For data such as patches of natural images or the Mel-Cepstrum coefficients
    used to represent speech, logistic units are a very poor representation. One solution
    is to replace the binary visible units by linear units with independent Gaussian
    noise. The energy function then becomes:'
  id: totrans-7279
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自然图像的补丁或用于表示语音的 Mel-Cepstrum 系数等数据，逻辑单元的表示效果非常差。一种解决方案是用具有独立高斯噪声的线性单元替换二元可见单元。能量函数变为：
- en: $$E({\bf v},{\bf h})=\sum_{i\in{\rm vis}}\frac{(v_{i}-a_{i})^{2}}{2\sigma_{i}^{2}}\quad-\sum_{j\in{\rm
    hid}}b_{j}h_{j}-\sum_{i,j}\frac{v_{i}}{\sigma_{i}}h_{j}w_{ij}\tag{24.17}$$
  id: totrans-7280
  prefs: []
  type: TYPE_NORMAL
  zh: $$E({\bf v},{\bf h})=\sum_{i\in{\rm vis}}\frac{(v_{i}-a_{i})^{2}}{2\sigma_{i}^{2}}\quad-\sum_{j\in{\rm
    hid}}b_{j}h_{j}-\sum_{i,j}\frac{v_{i}}{\sigma_{i}}h_{j}w_{ij}\tag{24.17}$$
- en: where σi is the standard deviation of the Gaussian noise for visible unit i.
  id: totrans-7281
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 σi 是可见单元 i 的高斯噪声标准差。
- en: It is possible to learn the variance of the noise for each visible unit but
    this is difficult using CD1. In many applications, it is much easier to first
    normalise each component of the data to have zero mean and unit variance and then
    to use noise free reconstructions, with the variance in equation 24.17 set to
    1. The reconstructed value of a Gaussian visible unit is then equal to its top-down
    input from the binary hidden units plus its bias.
  id: totrans-7282
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个可见单元学习噪声方差是可能的，但使用 CD1 进行这项操作很困难。在许多应用中，首先将数据的每个分量归一化为零均值和单位方差，然后使用无噪声重建，方程
    24.17 中的方差设为 1，这样做要容易得多。高斯可见单元的重建值等于其来自二元隐藏单元的自上而下输入加上其偏置。
- en: The learning rate needs to be about one or two orders of magnitude smaller than
    when using binary visible units and some of the failures reported in the literature
    are probably due to using a learning rate that is much too big. A smaller learning
    rate is required because there is no upper bound to the size of a component in
    the reconstruction and if one component becomes very large, the weights emanating
    from it will get a very big learning signal. With binary hidden and visible units,
    the learning signal for each training case must lie between −1 and 1, so binary-binary
    nets are much more stable.
  id: totrans-7283
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率需要比使用二进制可见单元时小一个或两个数量级，文献中报告的一些失败可能是由于使用的学习率过大。需要更小的学习率，因为重构中组件的大小没有上限，如果某个组件变得非常大，来自它的权重将获得非常大的学习信号。对于二进制隐藏和可见单元，每个训练案例的学习信号必须介于−1和1之间，因此二进制-二进制网络要稳定得多。
- en: 24.13.3 Gaussian Visible And Hidden Units
  id: totrans-7284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.13.3 高斯可见单元和隐藏单元
- en: 'If both the visible and the hidden units are Gaussian, the instability problems
    become much worse. The individual activities are held close to their means by
    quadratic "containment" terms with coefficients determined by the standard deviations
    of the assumed noise levels:'
  id: totrans-7285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可见单元和隐藏单元都是高斯的，不稳定性问题会变得更严重。个体活动被二次“约束”项保持在接近其均值的位置，其系数由假设的噪声水平的标准差决定：
- en: $$E({\bf v},{\bf h})=\sum_{i\in{\rm vis}}\frac{(v_{i}-a_{i})^{2}}{2\sigma_{i}^{2}}\quad+\sum_{j\in{\rm
    hid}}\frac{(h_{j}-b_{j})^{2}}{2\sigma_{j}^{2}}-\sum_{i,j}\frac{v_{i}}{\sigma_{i}}\frac{h_{j}}{\sigma_{j}}w_{ij}\tag{24.18}$$
  id: totrans-7286
  prefs: []
  type: TYPE_NORMAL
  zh: $$E({\bf v},{\bf h})=\sum_{i\in{\rm vis}}\frac{(v_{i}-a_{i})^{2}}{2\sigma_{i}^{2}}\quad+\sum_{j\in{\rm
    hid}}\frac{(h_{j}-b_{j})^{2}}{2\sigma_{j}^{2}}-\sum_{i,j}\frac{v_{i}}{\sigma_{i}}\frac{h_{j}}{\sigma_{j}}w_{ij}\tag{24.18}$$
- en: If any of the eigenvalues of the weight matrix become sufficiently large, the
    quadratic interaction terms can dominate the containment terms and there is then
    no lower bound to the energy that can be achieved by scaling up the activities
    in the direction of the corresponding eigenvector. With a sufficiently small learning
    rate, CD1 can detect and correct these directions so it is possible to learn an
    undirected version of a factor analysis model [10] using all Gaussian units, but
    this is harder than using EM [3] to learn a directed model.
  id: totrans-7287
  prefs: []
  type: TYPE_NORMAL
  zh: 如果权重矩阵的任何特征值变得足够大，二次交互项可能主导约束项，从而没有任何可通过沿相应特征向量的活动缩放来实现的能量下限。通过足够小的学习率，CD1可以检测并修正这些方向，因此可以使用所有高斯单元学习因子分析模型的无向版本[10]，但这比使用EM[3]学习有向模型要困难得多。
- en: 24.13.4 Binomial Units
  id: totrans-7288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.13.4 二项单元
- en: A simple way to get a unit with noisy integer values in the range 0 to N is
    to make N separate copies of a binary unit and give them all the same weights
    and bias [21]. Since all copies receive the same total input, they all have the
    same probability, p, of turning on and this only has to be computed once. The
    expected number that are on is N p and the variance in this number is N p(1 −
    p). For small p, this acts like a Poisson unit, but as p approaches 1 the variance
    becomes small again which may not be desireable. Also, for small values of p the
    growth in p is exponential in the total input. This makes learning much less stable
    than for the rectified linear units described in section 24.13.5.
  id: totrans-7289
  prefs: []
  type: TYPE_NORMAL
  zh: 获取一个带有噪声整数值的单位，范围从0到N，一个简单的方法是制作N个独立的二进制单元副本，并给它们相同的权重和偏置[21]。由于所有副本接收到相同的总输入，它们都有相同的开机概率p，这只需计算一次。预期的开启数量为N
    p，而这个数量的方差为N p(1 − p)。对于小p，这表现得像一个泊松单元，但随着p接近1，方差再次变小，这可能并不理想。此外，对于小p，p的增长在总输入中是指数级的。这使得学习比第24.13.5节中描述的修正线性单元要不稳定得多。
- en: One nice thing about using weight-sharing to synthesize a new type of unit out
    of binary units is that the mathematics underlying binary-binary RBM's remains
    unchanged.
  id: totrans-7290
  prefs: []
  type: TYPE_NORMAL
  zh: 使用权重共享将二进制单元合成新型单元的一大优点是，二进制-二进制RBM背后的数学仍然不变。
- en: 24.13.5 Rectified Linear Units
  id: totrans-7291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.13.5 修正线性单元
- en: A small modification to binomial units makes them far more interesting as models
    of real neurons and also more useful for practical applications. All copies still
    have the same learned weight vector w and the same learned bias, b, but each copy
    has a different, fixed offset to the bias. If the offsets are
  id: totrans-7292
  prefs: []
  type: TYPE_NORMAL
  zh: 对二项单元的小修改使它们作为真实神经元模型变得更有趣，并且对实际应用更有用。所有副本仍然具有相同的学习权重向量w和相同的学习偏置b，但每个副本都有一个不同的、固定的偏置偏移量。如果这些偏移量是
- en: '−0.5, −1.5, −2.5*, ...* − (N − 0.5) the sum of the probabilities of the copies
    is extremely close to having a closed form:'
  id: totrans-7293
  prefs: []
  type: TYPE_NORMAL
  zh: −0.5, −1.5, −2.5*, ...* − (N − 0.5) 复制的概率总和极其接近于闭合形式：
- en: $$\sum_{i=1}^{\infty}\sigma(x-i+0.5)\approx\log(1+e^{x})$$
  id: totrans-7294
  prefs: []
  type: TYPE_NORMAL
  zh: $$\sum_{i=1}^{\infty}\sigma(x-i+0.5)\approx\log(1+e^{x})$$
- en: $$(24.19)$$
  id: totrans-7295
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.19)$$
- en: where x = vwT + b. So the total activity of all of the copies behaves like a
    smoothed version of a rectified linear unit that saturates for sufficiently large
    input. Even though log(1 + ex) is not in the exponential family, we can model
    it accurately using a set of binary units with shared weights and fixed bias offsets.
    This set has no more parameters than an ordinary binary unit, but it provides
    a much more expressive variable. The variance is σ(x) so units that are firmly
    off do not create noise and the noise does not become large when x is large.
  id: totrans-7296
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 x = vwT + b。因此，所有复制体的总活动像是一个平滑的修正线性单元，当输入足够大时会饱和。尽管 log(1 + ex) 不在指数家族中，我们仍然可以使用一组共享权重和固定偏置偏移的二元单元准确建模。这个集合的参数数量与普通的二元单元相同，但它提供了一个更具表现力的变量。方差为
    σ(x)，因此那些完全关闭的单元不会产生噪声，当 x 较大时，噪声不会变得很大。
- en: A drawback of giving each copy a bias that differs by a fixed offset is that
    the logistic function needs to be used many times to get the probabilities required
    for sampling an integer value correctly. It is possible, however, to use a fast
    approximation in which the sampled value of the rectified linear unit is not constrained
    to be an integer. Instead it is approximated by max(0, x + N(0, 1) where N(0,
    1) is Gaussian noise with zero mean and unit variance. This type of rectified
    linear unit seems to work fine for either visible units or hidden units when training
    with CD1 [14].
  id: totrans-7297
  prefs: []
  type: TYPE_NORMAL
  zh: 给每个复制体一个不同的固定偏置的缺点是，逻辑函数需要多次使用，以获取正确采样整数值所需的概率。然而，可以使用一种快速近似，其中修正线性单元的采样值不受限于整数。相反，它通过
    max(0, x + N(0, 1) 来近似，其中 N(0, 1) 是均值为零、方差为一的高斯噪声。这种修正线性单元在使用 CD1 训练可见单元或隐藏单元时似乎效果良好
    [14]。
- en: If both visible and hidden units are rectified linear, a much smaller learning
    rate may be needed to avoid unstable dynamics in the activity or weight updates.
    If the weight between two rectified linear units is greater than 1 there is no
    lower bound to the energy that can be achieved by giving both units very high
    activities so there is no proper probability distribution. Nevertheless, contrastive
    divergence learning may still work provided the learning rate is low enough to
    give the learning time to detect and correct directions in which the Markov chain
    would blow up if allowed to run for many iterations. RBM's composed of rectified
    linear units are more stable than RBM's composed of Gaussian units because the
    rectification prevents biphasic oscillations of the weight dynamics in which units
    alternate between very high positive activity for one mini-batch followed by very
    high negative activity for the next mini-batch.
  id: totrans-7298
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可见单元和隐藏单元都是修正线性单元，则可能需要一个更小的学习率，以避免活动或权重更新中的不稳定动态。如果两个修正线性单元之间的权重大于 1，则没有能量的下界可以通过给予这两个单元非常高的活动来实现，因此没有适当的概率分布。然而，只要学习率足够低，以给学习时间检测和纠正马尔可夫链在允许运行多次迭代时会爆炸的方向，对比散度学习仍然可以有效。由修正线性单元组成的
    RBM 比由高斯单元组成的 RBM 更稳定，因为修正防止了权重动态中的双相振荡，其中单元在一个小批次中交替具有非常高的正活动，随后在下一个小批次中具有非常高的负活动。
- en: 24.14 Varieties Of Contrastive Divergence
  id: totrans-7299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.14 对比散度的多样性
- en: Although CD1 is not a very good approximation to maximum likelihood learning,
    this does not seem to matter when an RBM is being learned in order to provide
    hidden features for training a higher-level RBM. CD1 ensures that the hidden features
    retain most of the information in the data vector and it is not necessarily a
    good idea to use a form of CD that is a closer approximation to maximum likelihood
    but is worse at retaining the information in the data vector. If, however, the
    aim is to learn an RBM that is a good density or joint-density model, CD1 is far
    from optimal.
  id: totrans-7300
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 CD1 对最大似然学习的近似效果不佳，但当 RBM 被学习以提供隐藏特征用于训练更高级的 RBM 时，这似乎并不重要。CD1 确保隐藏特征保留数据向量中的大部分信息，因此使用更接近最大似然的
    CD 形式并不一定是一个好主意，因为它在保留数据向量信息方面表现更差。然而，如果目标是学习一个好的密度或联合密度模型的 RBM，CD1 远非最佳。
- en: At the beginning of learning, the weights are small and mixing is fast so CD1
    provides a good approximation to maximum likelihood. As the weights grow, the
    mixing gets worse and it makes sense to gradually increase the n in CDn
  id: totrans-7301
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习的开始，权重较小，混合速度较快，因此CD1为最大似然提供了良好的近似。随着权重的增加，混合变差，因此逐渐增加CDn中的n是有意义的。
- en: '[1, 17]. When n is increased, the difference of pairwise statistics that is
    used for learning will increase so it may be necessary to reduce the learning
    rate.'
  id: totrans-7302
  prefs: []
  type: TYPE_NORMAL
  zh: '[1, 17]。当n增加时，用于学习的成对统计差异也会增加，因此可能需要降低学习率。'
- en: A more radical departure from CD1 is called "persistent contrastive divergence"
    [22]. Instead of initializing each alternating Gibbs Markov chain at a datavector,
    which is the essence of CD learning, we keep track of the states of a number of
    persistent chains or "fantasy particles". Each persisitent chain has its hidden
    and visible states updated one (or a few) times after each weight update. The
    learning signal is then the difference between the pairwise statistics measured
    on a mini-batch of data and the pairwise statistics measured on the persistent
    chains. Typically the number of persistent chains is the same as the size of a
    mini-batch, but there is no good reason for this. The persistent chains mix surprisingly
    fast because the weight-updates repel each chain from its current state by raising
    the energy of that state [23].
  id: totrans-7303
  prefs: []
  type: TYPE_NORMAL
  zh: 对CD1的更激进的改进称为“持久对比散度”[22]。我们不再在数据向量处初始化每个交替的吉布斯马尔可夫链，这是CD学习的本质，而是跟踪多个持久链或“幻想粒子”的状态。每个持久链在每次权重更新后更新其隐含和可见状态一次（或几次）。学习信号是基于小批量数据测得的成对统计与在持久链上测得的成对统计之间的差异。通常持久链的数量与小批量的大小相同，但这并没有什么好理由。持久链的混合速度令人惊讶，因为权重更新使每条链远离其当前状态，从而提高了该状态的能量[23]。
- en: When using persistent CD, the learning rate typically needs to be quite a lot
    smaller and the early phase of the learning is much slower in reducing the reconstruction
    error. In the early phase of learning the persistent chains often have very correlated
    states, but this goes away with time. The final reconstruction error is also typically
    larger than with CD1 because persistent CD is, asymptotically, performing maximum
    likelihood learning rather than trying to make the distribution of the one-step
    reconstructions resemble the distribution of the data. Persistent CD learns significantly
    better models than CD1 or even CD10
  id: totrans-7304
  prefs: []
  type: TYPE_NORMAL
  zh: 使用持久对比散度时，学习率通常需要小得多，学习的早期阶段在减少重建误差方面也要慢得多。在学习的早期阶段，持久链通常具有高度相关的状态，但随着时间推移这种情况会消失。最终的重建误差通常也比CD1大，因为持久对比散度渐近上表现为最大似然学习，而不是试图使一步重建的分布与数据的分布相似。持久对比散度学习的模型明显优于CD1甚至CD10。
- en: '[22] and is the recommended method if the aim is to build the best density
    model of the data.'
  id: totrans-7305
  prefs: []
  type: TYPE_NORMAL
  zh: '[22]，如果目标是建立数据的最佳密度模型，这是推荐的方法。'
- en: Persistent CD can be improved by adding to the standard parameters an overlay
    of "fast weights" which learn very rapidly but also decay very rapidly [23]. These
    fast weights improve the mixing of the persistent chains. However, the use of
    fast weights introduces yet more meta-parameters and will not be discussed further
    here.
  id: totrans-7306
  prefs: []
  type: TYPE_NORMAL
  zh: 持久对比散度（Persistent CD）可以通过在标准参数上添加快速权重的覆盖层来改进，这些快速权重学习非常迅速，但也衰减得很快[23]。这些快速权重改善了持久链的混合。然而，使用快速权重引入了更多的元参数，这里将不再进一步讨论。
- en: 24.15 Displaying What Is Happening During Learning
  id: totrans-7307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.15 显示学习过程中发生的情况
- en: There are many ways in which learning can go wrong and most of the common problems
    are easy to diagnose with the right graphical displays. The three types of display
    described below give much more insight into what is happening than simply monitoring
    the reconstruction error.
  id: totrans-7308
  prefs: []
  type: TYPE_NORMAL
  zh: 学习可能出错的方式有很多，大多数常见问题在合适的图形显示下都容易诊断。下面描述的三种显示方式比单纯监测重建误差更能深入了解发生了什么。
- en: Histograms of the weights, the visible biases and the hidden biases are very
    useful. In addition, it is useful to examine histograms of the increments to these
    parameters when they are updated, though it is wasteful to make these histograms
    after every update.
  id: totrans-7309
  prefs: []
  type: TYPE_NORMAL
  zh: 权重、可见偏差和隐含偏差的直方图非常有用。此外，检查这些参数在更新时的增量直方图也很有帮助，尽管在每次更新后生成这些直方图是浪费的。
- en: For domains in which the visible units have spatial or temporal structure (*e.g.*
  id: totrans-7310
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可见单元具有空间或时间结构的领域（*例如*）。
- en: images or speech) it is very helpful to display, for each hidden unit, the weights
    connecting that hidden unit to the visible units. These "receptive" fields are
    a good way of visualizing what features the hidden units have learned. When displaying
    the receptive fields of many hidden units it can be very misleading to use different
    scales for different hidden units. Gray-scale displays of receptive fields are
    usually less pretty but much more informative than false colour displays.
  id: totrans-7311
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像或语音中，为每个隐藏单元显示连接该隐藏单元与可见单元的权重是非常有帮助的。这些“感受”域是可视化隐藏单元学习到的特征的好方法。当显示多个隐藏单元的感受域时，使用不同的尺度可能会非常误导。灰度显示的感受域通常比伪彩色显示更不美观，但信息量要大得多。
- en: For a single minibatch, it is very useful to see a two-dimensional, gray-scale
    display with a range of [0,1] that shows the probability of each binary hidden
    unit on each training case in a mini-batch9. This immediately allows you to see
    if some hidden units are never used or if some training cases activate an unusually
    large or small number of hidden units. It also shows how certain the hidden units
    are. When learning is working properly, this display should look thoroughly random
    without any obvious vertical or horizontal lines. Histograms can be used instead
    of this display, but it takes quite a few histograms to convey the same information.
  id: totrans-7312
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个单一的小批量，查看一个范围为[0,1]的二维灰度显示，显示每个二元隐藏单元在小批量中每个训练案例的概率是非常有用的。这立即让你看到某些隐藏单元是否从未使用，或者某些训练案例是否激活了异常多或少的隐藏单元。它还显示了隐藏单元的确定性。当学习正常工作时，这个显示应该看起来完全随机，没有明显的垂直或水平线。直方图可以替代这个显示，但需要相当多的直方图才能传达相同的信息。
- en: 24.16 Using Rbm'S For Discrimination
  id: totrans-7313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.16 使用RBM进行区分
- en: There are three obvious ways of using RBMs for discrimination. The first is
    to use the hidden features learned by the RBM as the inputs for some standard
    discriminative method. This will not be discussed further here, though it is probably
    the most important way of using RBM's, especially when many layers of hidden features
    are learned unsupervised before starting on the discriminative training.
  id: totrans-7314
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RBM进行区分有三种明显的方法。第一种是使用RBM学习到的隐藏特征作为某种标准区分方法的输入。虽然这不会在这里进一步讨论，但这可能是使用RBM的最重要方式，特别是在开始进行区分训练之前，许多隐藏特征是无监督学习的。
- en: The second method is to train a separate RBM on each class. After training,
    the free energy of a test vector, t, is computed (see subsection 24.16.1)for each
  id: totrans-7315
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是在每个类别上训练一个单独的RBM。训练后，计算测试向量t的自由能（参见子节24.16.1）。
- en: 9 If there are more than a few hundred hidden units, just use a subset of them.
  id: totrans-7316
  prefs: []
  type: TYPE_NORMAL
  zh: 9 如果隐藏单元超过几百个，只需使用其中的一部分。
- en: 'class-specific RBM. The log probability that the RBM trained on class c assigns
    to the test vector is given by:'
  id: totrans-7317
  prefs: []
  type: TYPE_NORMAL
  zh: 类别特定的RBM。RBM对测试向量赋予的对数概率由以下公式给出：
- en: $\log p({\bf t}|c)=-F_{c}({\bf t})-\log Z_{c}$ (24.20)
  id: totrans-7318
  prefs: []
  type: TYPE_NORMAL
  zh: $\log p({\bf t}|c)=-F_{c}({\bf t})-\log Z_{c}$ (24.20)
- en: where Zc is the partition function of that RBM. Since each class-specific RBM
  id: totrans-7319
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Zc是该RBM的分区函数。由于每个类别特定的RBM
- en: 'will have a different, unknown partition function, the free energies cannot
    be used directly for discrimination. However, if the number of classes is small
    it is easy to deal with the unknown log partition functions by simply training
    a "softmax" model (on a separate training set) to predict the class from the free
    energies of all of the class specific RBMs:'
  id: totrans-7320
  prefs: []
  type: TYPE_NORMAL
  zh: 将会有一个不同的、未知的分区函数，自由能不能直接用于区分。然而，如果类别数量较少，通过简单地在一个单独的训练集上训练一个“softmax”模型来预测每个类别特定RBM的自由能，是很容易处理未知的对数分区函数的。
- en: $$\log p(class=c|\mathbf{t})=\frac{e^{-F_{c}(\mathbf{t})-\log\hat{Z}_{c}}}{\sum_{d}e^{-F_{d}(\mathbf{t})-\log\hat{Z}_{d}}}\tag{24.21}$$
  id: totrans-7321
  prefs: []
  type: TYPE_NORMAL
  zh: $$\log p(class=c|\mathbf{t})=\frac{e^{-F_{c}(\mathbf{t})-\log\hat{Z}_{c}}}{\sum_{d}e^{-F_{d}(\mathbf{t})-\log\hat{Z}_{d}}}\tag{24.21}$$
- en: where the Zˆ are parameters that are learned by maximum likleihood training
    of the softmax. Of course, equation 24.21 can also be used to learn the weights
    and biases of each RBM but this requires a lot of data to avoid overfitting. Combining
    the discriminative gradients for the weights and biases that come from equation
    24.21 with the approximate gradients that come from contrastive divergence will
    often do better than either method alone. The approximate gradient produced by
    contrastive divergence acts as a strong regularizer to prevent overfitting and
    the discriminative gradient ensures that there is some pressure to use the weights
    and biases in a way that helps discrimination.
  id: totrans-7322
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Zˆ是通过softmax的最大似然训练学习的参数。当然，方程24.21也可以用来学习每个RBM的权重和偏置，但这需要大量数据以避免过拟合。结合来自方程24.21的权重和偏置的判别梯度与来自对比散度的近似梯度，通常比单独使用任何一种方法效果更好。对比散度产生的近似梯度作为强正则化器，防止过拟合，而判别梯度确保在使用权重和偏置时有压力，以帮助区分。
- en: 'The third method is to train a joint density model using a single RBM that
    has two sets of visible units. In addition to the units that represent a data
    vector, there is a "softmax" label unit that represents the class. After training,
    each possible label is tried in turn with a test vector and the one that gives
    lowest free energy is chosen as the most likely class. The partition function
    is not a problem here, since it is the same for all classes. Again, it is possible
    to combine discriminiative and generative training of the joint RBM by using discriminative
    gradients that are the derivatives of the log probability of the correct class
    [6]:'
  id: totrans-7323
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法是使用一个具有两组可见单位的单个RBM来训练联合密度模型。除了表示数据向量的单位外，还有一个“softmax”标签单位表示类别。训练后，依次用测试向量尝试每个可能的标签，选择最低自由能的作为最可能的类别。这里的分区函数不是问题，因为它对所有类别都是相同的。同样，可以通过使用来自正确类别对数概率的导数的判别梯度，将判别和生成的联合RBM训练结合起来[6]：
- en: $$\log p(c l a s s=c|\mathbf{t})={\frac{e^{-F_{c}(\mathbf{t})}}{\sum_{d}e^{-F_{d}(\mathbf{t})}}}$$
  id: totrans-7324
  prefs: []
  type: TYPE_NORMAL
  zh: $$\log p(c l a s s=c|\mathbf{t})={\frac{e^{-F_{c}(\mathbf{t})}}{\sum_{d}e^{-F_{d}(\mathbf{t})}}}$$
- en: $$(24.22)$$
  id: totrans-7325
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.22)$$
- en: $$(24.23)$$
  id: totrans-7326
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.23)$$
- en: d e−Fd(t) (24.22)
  id: totrans-7327
  prefs: []
  type: TYPE_NORMAL
  zh: d e−Fd(t) (24.22)
- en: 24.16.1 Computing The Free Energy Of A Visible Vector
  id: totrans-7328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.16.1 计算可见向量的自由能
- en: 'The free energy of visible vector v is the energy that a single configuration
    would need to have in order to have the same probability as all of the configurations
    that contain v:'
  id: totrans-7329
  prefs: []
  type: TYPE_NORMAL
  zh: 可见向量v的自由能是单一配置所需的能量，以使其与包含v的所有配置具有相同概率：
- en: $$e^{-F(\mathbf{v})}=\sum_{\mathbf{h}}e^{-E(\mathbf{v,h})}$$
  id: totrans-7330
  prefs: []
  type: TYPE_NORMAL
  zh: $$e^{-F(\mathbf{v})}=\sum_{\mathbf{h}}e^{-E(\mathbf{v,h})}$$
- en: 'It is also given by the expected energy minus the entropy:'
  id: totrans-7331
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以通过期望能量减去熵来给出：
- en: $$F({\bf v})=-\sum_{i}v_{i}a_{i}-\sum_{j}p_{j}x_{j}+\sum_{j}\left(p_{j}\log
    p_{j}+(1-p_{j})\log(1-p_{j})\right).$$
  id: totrans-7332
  prefs: []
  type: TYPE_NORMAL
  zh: $$F({\bf v})=-\sum_{i}v_{i}a_{i}-\sum_{j}p_{j}x_{j}+\sum_{j}\left(p_{j}\log
    p_{j}+(1-p_{j})\log(1-p_{j})\right).$$
- en: 'where xj = bj + i viwij is the total input to hidden unit j and pj = σ(xj )
    is the probability that hj = 1 given v. A good way to compute F(v) is to use yet
    another expression for the free energy:'
  id: totrans-7333
  prefs: []
  type: TYPE_NORMAL
  zh: 其中xj = bj + i viwij是隐藏单元j的总输入，pj = σ(xj )是给定v时hj = 1的概率。计算F(v)的一个好方法是使用另一个自由能表达式：
- en: $$F(\mathbf{v})=-\sum_{i}v_{i}a_{i}-\sum_{j}\log(1+e^{x_{j}})$$
  id: totrans-7334
  prefs: []
  type: TYPE_NORMAL
  zh: $$F(\mathbf{v})=-\sum_{i}v_{i}a_{i}-\sum_{j}\log(1+e^{x_{j}})$$
- en: $$(24.24)$$
  id: totrans-7335
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.24)$$
- en: $$(24.25)$$
  id: totrans-7336
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.25)$$
- en: 24.17 Dealing With Missing Values
  id: totrans-7337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 24.17 处理缺失值
- en: In a directed belief net it is very easy to deal with missing values for visible
    variables. When performing inference, a missing value that is at the receiving
    end of a directed connection has no effect on the units that send connections
    to it. This is not true for the undirected connections used in an RBM. To perform
    inference in the standard way, the missing value must first be filled in and there
    are at least two ways to do this.
  id: totrans-7338
  prefs: []
  type: TYPE_NORMAL
  zh: 在有向信念网络中，处理可见变量的缺失值非常简单。当进行推理时，位于有向连接接收端的缺失值对发送连接的单位没有影响。这对于在RBM中使用的无向连接则不成立。要以标准方式进行推理，必须首先填补缺失值，并且至少有两种方法可以做到这一点。
- en: A particularly simple type of missing value occurs when learning a joint density
    for data in which each training case is composed of a vector v such as an image
    plus a single discrete label. If the label is missing from a subset of the cases,
    it can be Gibbs sampled from its exact conditional distribution. This is done
    by computing the free energy (see section 24.16.1) for each possible value of
    the label and then picking label l with probability proportional to exp(−F(l,
    v)).
  id: totrans-7339
  prefs: []
  type: TYPE_NORMAL
  zh: 一种特别简单的缺失值类型出现在学习联合密度时，其中每个训练案例由一个向量 v（如图像）加上一个单一的离散标签组成。如果某些案例的标签缺失，可以从其确切条件分布中进行
    Gibbs 采样。这是通过计算每个标签可能值的自由能量（见第 24.16.1 节），然后按比例选择标签 l，概率与 exp(−F(l, v)) 成正比来完成的。
- en: After this, the training case is treated just like a complete training case.
  id: totrans-7340
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，训练案例就像一个完整的训练案例一样处理。
- en: 'For real-valued visible units, there is a different way to impute missing values
    that still works well even if several values are missing from the same training
    case [8]. If the learning cycles through the training set many times, the missing
    values can be treated in just the same way as the other parameters. Starting with
    a sensible initial guess, a missing value is updated each time the weights are
    updated, but possibly using a different learning rate. The update for the missing
    value for visible unit i on training case c is:'
  id: totrans-7341
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实值可见单元，还有另一种插补缺失值的方法，即使同一训练案例中缺失多个值，这种方法仍然效果很好[8]。如果学习多次循环遍历训练集，可以将缺失值与其他参数以相同的方式处理。从一个合理的初始猜测开始，每次更新权重时都会更新缺失值，但可能使用不同的学习率。训练案例
    c 中可见单元 i 的缺失值更新为：
- en: $$\Delta v_{i}^{c}=\epsilon\left({\frac{\partial F}{\partial{\dot{v}}_{i}^{c}}}-{\frac{\partial
    F}{\partial v_{i}^{c}}}\right)$$
  id: totrans-7342
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta v_{i}^{c}=\epsilon\left({\frac{\partial F}{\partial{\dot{v}}_{i}^{c}}}-{\frac{\partial
    F}{\partial v_{i}^{c}}}\right)$$
- en: $$(24.26)$$
  id: totrans-7343
  prefs: []
  type: TYPE_NORMAL
  zh: $$(24.26)$$
- en: where vci is the imputed value and vˆci is the reconstruction of the imputed
    value.
  id: totrans-7344
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 vci 是插补值，vˆci 是插补值的重构。
- en: Momentum can be used for imputed values in just the same way as it is used for
    the usual parameters.
  id: totrans-7345
  prefs: []
  type: TYPE_NORMAL
  zh: 动量可以像用于通常参数那样用于插补值。
- en: There is a more radical way of dealing with missing values that can be used
    when the number of missing values is very large. This occurs, for example, with
    user preference data where most users do not express their preference for most
    objects [17]. Instead of trying to impute the missing values, we pretend they
    do not exist by using RBM's with different numbers of visible units for different
    training cases. The different RBMs form a family of different models with shared
    weights. Each RBM in the family can now do correct inference for its hidden states,
    but the tying of the weights means that they may not be ideal for any particular
    RBM. Adding a visible unit for a missing value and then performing correct inference
    that integrates out this missing value does not give the same distribution for
    the hidden units as simply ommitting the visible unit which is why this is a family
    of models rather than just one model. When using a family of models to deal with
    missing values, it can be very helpful to scale the hidden biases by the number
    of visible units in the RBM [15].
  id: totrans-7346
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失值还有一种更激进的方法，当缺失值数量非常大时可以使用。例如，在用户偏好数据中，大多数用户并未对大多数对象表达他们的偏好[17]。我们并不是试图插补缺失值，而是通过对不同训练案例使用具有不同可见单元数量的
    RBM 来假装它们不存在。不同的 RBM 形成一个共享权重的不同模型家族。家族中的每个 RBM 现在可以对其隐藏状态进行正确推断，但权重的绑定意味着它们可能不适合任何特定的
    RBM。为缺失值添加一个可见单元，然后执行正确的推断来整合这个缺失值，不会给出与简单省略可见单元相同的隐藏单元分布，这也是为什么这是一个模型家族而不仅仅是一个模型。当使用模型家族处理缺失值时，按
    RBM 中可见单元的数量缩放隐藏偏差可能会非常有帮助[15]。
- en: Acknowledgements. This research was supported by NSERC and the Canadian Institute
    for Advanced Research. Many of my past and present graduate students and postdocs
    have made valuable contributions to the body of practical knowledge described
    in this chapter. I have tried to acknowledge particularly valuable contributions
    in the chapter, but I cannot always recall who suggested what.
  id: totrans-7347
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢。本研究得到了 NSERC 和加拿大高级研究所的支持。我的许多过去和现在的研究生及博士后对本章描述的实用知识体做出了宝贵的贡献。我试图在章节中特别承认有价值的贡献，但我并不总能回忆起谁建议了什么。
- en: '[1] Carreira-Perpignan, M.A., Hinton, G.E.: On contrastive divergence learning.
    In:'
  id: totrans-7348
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Carreira-Perpignan, M.A., Hinton, G.E.: 关于对比散度学习。'
- en: Artificial Intelligence and Statistics (2005)
  id: totrans-7349
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能与统计 (2005)
- en: '[2] Freund, Y., Haussler, D.: Unsupervised learning of distributions on binary
    vectors using two layer networks. In: Advances in Neural Information Processing
    Systems 4, pp. 912–919. Morgan Kaufmann, San Mateo (1992)'
  id: totrans-7350
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Freund, Y., Haussler, D.: 使用双层网络对二进制向量分布进行无监督学习。在：神经信息处理系统进展 4, pp. 912–919.
    摩根·考夫曼, 圣马特奥 (1992)'
- en: '[3] Ghahramani, Z., Hinton, G.: The EM algorithm for mixtures of factor analyzers.'
  id: totrans-7351
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Ghahramani, Z., Hinton, G.: 因子分析器混合的EM算法。'
- en: Technical Report CRG-TR-96-1, University of Toronto (May 1996)
  id: totrans-7352
  prefs: []
  type: TYPE_NORMAL
  zh: 技术报告 CRG-TR-96-1, 多伦多大学 (1996年5月)
- en: '[4] Hinton, G.E.: Relaxation and its role in vision. PhD Thesis (1978)'
  id: totrans-7353
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Hinton, G.E.: 放松及其在视觉中的作用。博士论文 (1978)'
- en: '[5] Hinton, G.E.: Training products of experts by minimizing contrastive divergence.'
  id: totrans-7354
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Hinton, G.E.: 通过最小化对比散度训练专家的乘积。'
- en: Neural Computation 14(8), 1711–1800 (2002)
  id: totrans-7355
  prefs: []
  type: TYPE_NORMAL
  zh: 神经计算 14(8), 1711–1800 (2002)
- en: '[6] Hinton, G.E.: To recognize shapes, first learn to generate images. In:
    Computational Neuroscience: Theoretical Insights into Brain Function (2007)'
  id: totrans-7356
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Hinton, G.E.: 为了识别形状，首先学习生成图像。在：计算神经科学：对大脑功能的理论见解 (2007)'
- en: '[7] Hinton, G.E., Osindero, S., Teh, Y.W.: A fast learning algorithm for deep
    belief nets. Neural Computation 18(7), 1527–1554 (2006)'
  id: totrans-7357
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Hinton, G.E., Osindero, S., Teh, Y.W.: 深度信念网络的快速学习算法。神经计算 18(7), 1527–1554
    (2006)'
- en: '[8] Hinton, G.E., Osindero, S., Welling, M., Teh, Y.: Unsupervised discovery
    of nonlinear structure using contrastive backpropagation. Cognitive Science 30,
    725–731'
  id: totrans-7358
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Hinton, G.E., Osindero, S., Welling, M., Teh, Y.: 使用对比反向传播进行非线性结构的无监督发现。认知科学
    30, 725–731'
- en: (2006b)
  id: totrans-7359
  prefs: []
  type: TYPE_NORMAL
  zh: (2006b)
- en: '[9] Hopfield, J.J.: Neural networks and physical systems with emergent collective
    computational abilities. Proceedings of the National Academy of Sciences 79, 2554–'
  id: totrans-7360
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Hopfield, J.J.: 神经网络与具有涌现集体计算能力的物理系统。美国国家科学院院刊 79, 2554–'
- en: 2558 (1982)
  id: totrans-7361
  prefs: []
  type: TYPE_NORMAL
  zh: 2558 (1982)
- en: '[10] Marks, T.K., Movellan, J.R.: Diffusion networks, product of experts, and
    factor analysis. In: Proc. Int. Conf. on Independent Component Analysis, pp. 481–485
    (2001)'
  id: totrans-7362
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Marks, T.K., Movellan, J.R.: 扩散网络、专家乘积和因子分析。在：国际独立成分分析会议论文集, pp. 481–485
    (2001)'
- en: '[11] Mohamed, A.R., Hinton, G.E.: Phone recognition using restricted boltzmann
    machines. In: ICASSP 2010 (2010)'
  id: totrans-7363
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Mohamed, A.R., Hinton, G.E.: 使用限制玻尔兹曼机进行语音识别。在：ICASSP 2010 (2010)'
- en: '[12] Mohamed, A.R., Dahl, G., Hinton, G.E.: Deep belief networks for phone
    recognition. In: NIPS 22 Workshop on Deep Learning for Speech Recognition (2009)'
  id: totrans-7364
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Mohamed, A.R., Dahl, G., Hinton, G.E.: 用于语音识别的深度信念网络。在：NIPS 22 深度学习与语音识别研讨会
    (2009)'
- en: '[13] Nair, V., Hinton, G.E.: 3-d object recognition with deep belief nets.
    In: Advances in Neural Information Processing Systems, vol. 22, pp. 1339–1347
    (2009)'
  id: totrans-7365
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Nair, V., Hinton, G.E.: 用深度信念网络进行三维物体识别。在：神经信息处理系统进展, vol. 22, pp. 1339–1347
    (2009)'
- en: '[14] Nair, V., Hinton, G.E.: Rectified linear units improve restricted boltzmann
    machines. In: Proc. 27th International Conference on Machine Learning (2010)'
  id: totrans-7366
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Nair, V., Hinton, G.E.: 纠正线性单元改善限制玻尔兹曼机。在：第27届国际机器学习会议论文集 (2010)'
- en: '[15] Salakhutdinov, R.R., Hinton, G.E.: Replicated softmax: An undirected topic
    model. In: Advances in Neural Information Processing Systems, vol. 22 (2009)'
  id: totrans-7367
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Salakhutdinov, R.R., Hinton, G.E.: 复制softmax：一种无向主题模型。在：神经信息处理系统进展, vol.
    22 (2009)'
- en: '[16] Salakhutdinov, R.R., Murray, I.: On the quantitative analysis of deep
    belief networks. In: Proceedings of the International Conference on Machine Learning,
    vol. 25, pp. 872–879 (2008)'
  id: totrans-7368
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Salakhutdinov, R.R., Murray, I.: 深度信念网络的定量分析。在：国际机器学习会议论文集, vol. 25, pp.
    872–879 (2008)'
- en: '[17] Salakhutdinov, R.R., Mnih, A., Hinton, G.E.: Restricted Boltzmann machines
    for collaborative filtering. In: Ghahramani, Z. (ed.) Proceedings of the International
    Conference on Machine Learning, vol. 24, pp. 791–798. ACM (2007)'
  id: totrans-7369
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Salakhutdinov, R.R., Mnih, A., Hinton, G.E.: 用于协同过滤的限制玻尔兹曼机。在：Ghahramani,
    Z. (编辑) 国际机器学习会议论文集, vol. 24, pp. 791–798. ACM (2007)'
- en: '[18] Smolensky, P.: Information processing in dynamical systems: Foundations
    of harmony theory. In: Rumelhart, D.E., McClelland, J.L. (eds.) Parallel Distributed
    Processing, vol. 1, ch. 6, pp. 194–281. MIT Press, Cambridge (1986)'
  id: totrans-7370
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Smolensky, P.: 动态系统中的信息处理：和谐理论的基础。在：Rumelhart, D.E., McClelland, J.L.
    (编辑) 并行分布处理, vol. 1, ch. 6, pp. 194–281. MIT出版社, 剑桥 (1986)'
- en: '[19] Sutskever, I., Tieleman: On the convergence properties of contrastive
    divergence.'
  id: totrans-7371
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Sutskever, I., Tieleman: 对比散度的收敛性分析。'
- en: 'In: Proceedings of the 13th International Conference on Artificial Intelligence
    and Statistics (AISTATS), Sardinia, Italy (2010)'
  id: totrans-7372
  prefs: []
  type: TYPE_NORMAL
  zh: 载于第13届国际人工智能与统计会议论文集 (AISTATS)，意大利撒丁岛 (2010)
- en: '[20] Taylor, G., Hinton, G.E., Roweis, S.T.: Modeling human motion using binary
    latent variables. In: Advances in Neural Information Processing Systems. MIT'
  id: totrans-7373
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] 泰勒, G., 希顿, G.E., 罗威斯, S.T.: 使用二元潜变量建模人类运动。载于《神经信息处理系统进展》。MIT'
- en: Press (2006)
  id: totrans-7374
  prefs: []
  type: TYPE_NORMAL
  zh: 出版社 (2006)
- en: '[21] Teh, Y.W., Hinton, G.E.: Rate-coded restricted Boltzmann machines for
    face recognition. In: Advances in Neural Information Processing Systems, vol.
    13, pp. 908–914 (2001)'
  id: totrans-7375
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] 特赫, Y.W., 希顿, G.E.: 用于面部识别的速率编码限制玻尔兹曼机。载于《神经信息处理系统进展》，第13卷，第908–914页 (2001)'
- en: '[22] Tieleman, T.: Training restricted Boltzmann machines using approximations
    to the likelihood gradient. In: Proceedings of the Twenty-first International
    Conference on Machine Learning (ICML 2008). ACM (2008)'
  id: totrans-7376
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] 提尔曼, T.: 使用似然梯度近似训练限制玻尔兹曼机。载于第21届国际机器学习会议（ICML 2008）论文集。ACM (2008)'
- en: '[23] Tieleman, T., Hinton, G.E.: Using fast weights to improve persistent contrastive
    divergence. In: Proceedings of the 26th International Conference on Machine Learning,
    pp. 1033–1040. ACM, New York (2009)'
  id: totrans-7377
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] 提尔曼, T., 希顿, G.E.: 使用快速权重改善持续对比散度。载于第26届国际机器学习会议论文集，第1033–1040页。ACM, 纽约
    (2009)'
- en: '[24] Welling, M., Rosen-Zvi, M., Hinton, G.E.: Exponential family harmoniums
    with an application to information retrieval. In: Advances in Neural Information
    Processing Systems, pp. 1481–1488. MIT Press, Cambridge (2005)'
  id: totrans-7378
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] 韦林, M., 罗斯恩-兹维, M., 希顿, G.E.: 指数族和谐器及其在信息检索中的应用。载于《神经信息处理系统进展》，第1481–1488页。麻省理工出版社,
    剑桥 (2005)'
- en: 25 Deep Boltzmann Machines And The Centering Trick
  id: totrans-7379
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 25 深度玻尔兹曼机与中心化技巧
- en: Grégoire Montavon1 and Klaus-Robert Müller1,2 1 Technische Universität Berlin,
    10587 Berlin, Germany, Machine Learning Group 2 Korea University, Anam-dong, Seongbuk-gu,
    Seoul 136-713, Korea, Department of Brain and Cognitive Engineering
  id: totrans-7380
  prefs: []
  type: TYPE_NORMAL
  zh: 格雷戈里·蒙塔冯1 和克劳斯-罗伯特·穆勒1,2 1 柏林工业大学, 10587 柏林, 德国, 机器学习组 2 韩国大学, 安岩洞, 成北区, 首尔
    136-713, 韩国, 脑与认知工程系
- en: '{gregoire.montavon,klaus-robert.mueller}@tu-berlin.de Abstract. Deep Boltzmann
    machines are in theory capable of learning efficient representations of seemingly
    complex data. Designing an algorithm that effectively learns the data representation
    can be subject to multiple difficulties. In this chapter, we present the "centering
    trick" that consists of rewriting the energy of the system as a function of centered
    states. The centering trick improves the conditioning of the underlying optimization
    problem and makes learning more stable, leading to models with better generative
    and discriminative properties. Keywords: Deep Boltzmann machine, centering, reparameterization,
    unsupervised learning, optimization, representations.'
  id: totrans-7381
  prefs: []
  type: TYPE_NORMAL
  zh: '{gregoire.montavon,klaus-robert.mueller}@tu-berlin.de 摘要。理论上，深度玻尔兹曼机能够学习看似复杂数据的高效表示。设计一个有效学习数据表示的算法可能面临多重困难。在本章中，我们介绍了“中心化技巧”，其内容是将系统的能量重写为中心状态的函数。中心化技巧改善了基础优化问题的条件，使学习更加稳定，导致具有更好生成和区分属性的模型。关键词：深度玻尔兹曼机，中心化，重参数化，无监督学习，优化，表示。'
- en: 25.1 Introduction
  id: totrans-7382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.1 引言
- en: Deep Boltzmann machines are undirected networks of interconnected units that
    learn a joint probability density over these units by adapting connections between
    them. They are in theory capable of learning statistically and computationally
    efficient representations of seemingly complex data distributions.
  id: totrans-7383
  prefs: []
  type: TYPE_NORMAL
  zh: 深度玻尔兹曼机是相互连接单元的无向网络，通过调整它们之间的连接来学习这些单元的联合概率密度。理论上，它们能够学习看似复杂的数据分布的统计和计算上高效的表示。
- en: Designing an algorithm that effectively learns the data representation can be
    subject to multiple difficulties. Deep Boltzmann machines are sensitive to the
    parameterization of their energy function. In addition, the gradient of the optimization
    problem is not directly accessible and must instead be approximated stochastically
    by continuously querying the model throughout training.
  id: totrans-7384
  prefs: []
  type: TYPE_NORMAL
  zh: 设计一个有效学习数据表示的算法可能面临多重困难。深度玻尔兹曼机对其能量函数的参数化敏感。此外，优化问题的梯度并不能直接获得，而必须通过在训练过程中持续查询模型进行随机近似。
- en: In this chapter, we present the "centering trick" that consists of rewriting
    the energy function of the deep Boltzmann machine as a function of centered states.
  id: totrans-7385
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了“居中技巧”，其内容是将深度玻尔兹曼机的能量函数重写为居中状态的函数。
- en: We argue that centering improves the conditioning of the optimization problem
    and facilitates the emergence of complex structures in the deep Boltzmann machine.
  id: totrans-7386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为居中改善了优化问题的条件，并促进了深度玻尔兹曼机中复杂结构的出现。
- en: We demonstrate on the MNIST dataset that the centering trick allows midsized
    deep Boltzmann machines to be trained faster and to produce a solution which is
    a good generative model of data but also distills interesting discriminative features
    in the top layer.
  id: totrans-7387
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在MNIST数据集上演示了居中技巧如何使中型深度玻尔兹曼机训练更快，并产生一个好的生成数据模型，同时在顶层提炼出有趣的区分特征。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    621–637, 2012.'
  id: totrans-7388
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon等（编）：NN: 行业技巧，第2版，LNCS 7700，第621–637页，2012年。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-7389
  prefs: []
  type: TYPE_NORMAL
  zh: -c 施普林格-维尔哈姆柏 2012
- en: 25.2 Boltzmann Machines
  id: totrans-7390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.2 玻尔兹曼机
- en: 'In this section, we give some background on the Boltzmann machine [6]. We will
    use the following notation: The sigmoid function is defined as sigm(x) = ex ex+1
    ,'
  id: totrans-7391
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍玻尔兹曼机的一些背景[6]。我们将使用以下符号：Sigmoid函数定义为sigm(x) = ex ex+1，
- en: x ∼ B(p) denotes that the variable x is drawn randomly from a Bernoulli distribution
    of parameter p and ·P denotes the expectation operator with respect to a probability
    distribution P. All these operations apply element-wise to the input if the latter
    is a vector.
  id: totrans-7392
  prefs: []
  type: TYPE_NORMAL
  zh: x ∼ B(p)表示变量x是从参数为p的伯努利分布中随机抽取的，而·P表示相对于概率分布P的期望算子。如果输入是向量，则所有这些操作逐元素应用于输入。
- en: fully connected
  id: totrans-7393
  prefs: []
  type: TYPE_NORMAL
  zh: 完全连接
- en: '![611_image_0.png](611_image_0.png)'
  id: totrans-7394
  prefs: []
  type: TYPE_IMG
  zh: '![611_image_0.png](611_image_0.png)'
- en: '![611_image_1.png](611_image_1.png)'
  id: totrans-7395
  prefs: []
  type: TYPE_IMG
  zh: '![611_image_1.png](611_image_1.png)'
- en: '![611_image_2.png](611_image_2.png)'
  id: totrans-7396
  prefs: []
  type: TYPE_IMG
  zh: '![611_image_2.png](611_image_2.png)'
- en: Boltzmann machine deep Boltzmann machine (DBM)
  id: totrans-7397
  prefs: []
  type: TYPE_NORMAL
  zh: 玻尔兹曼机深度玻尔兹曼机（DBM）
- en: locally connected DBM (LC-DBM)
  id: totrans-7398
  prefs: []
  type: TYPE_NORMAL
  zh: 局部连接的DBM（LC-DBM）
- en: Fig. 25.1. Example of Boltzmann machines used in practice with visible units
    depicted in gray and hidden units depicted in white. The layered structure of
    a DBM is interesting because a particular representation of data forms at each
    layer, possibly enabling the emergence of interesting statistics.
  id: totrans-7399
  prefs: []
  type: TYPE_NORMAL
  zh: 图25.1. 玻尔兹曼机在实践中的示例，可见单元以灰色表示，隐藏单元以白色表示。DBM的分层结构很有趣，因为每一层形成了特定的数据表示，可能促进了有趣统计特征的出现。
- en: A Boltzmann machine is a network of Mx interconnected binary units that associates
    to each state x ∈ {0, 1}Mx the probability
  id: totrans-7400
  prefs: []
  type: TYPE_NORMAL
  zh: 玻尔兹曼机是一个Mx个互连的二元单元网络，它将每个状态x ∈ {0, 1}Mx与概率相关联
- en: $$p(x;\theta)={\frac{e^{-E(x;\theta)}}{\sum_{\xi}e^{-E(\xi;\theta)}}}.$$
  id: totrans-7401
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(x;\theta)={\frac{e^{-E(x;\theta)}}{\sum_{\xi}e^{-E(\xi;\theta)}}}.$$
- en: The term in the denominator is the called the partition function and makes probabilities
    sum to one. The function
  id: totrans-7402
  prefs: []
  type: TYPE_NORMAL
  zh: 分母中的项称为配分函数，使概率之和为1。该函数
- en: $$E(x;\theta)=-{\frac{1}{2}}x^{\top}W x-x^{\top}b.$$
  id: totrans-7403
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(x;\theta)=-{\frac{1}{2}}x^{\top}W x-x^{\top}b.$$
- en: is the energy of the state x given the model parameters θ = (*W, b*). From these
    equations, we can interpret a good model of data as a model θ that has low energy
    in regions of high data density and high energy elsewhere. The matrix W of size
    Mx × Mx is symmetric and contains the connection strengths between units. The
    vector b of size Mx contains the biases associated to each unit. The diagonal
    of W is constrained to be zero. Units are either visible units (representing the
    sensory input) or hidden units (representing latent variables that are not directly
    observable but contribute to explaining data).
  id: totrans-7404
  prefs: []
  type: TYPE_NORMAL
  zh: 是状态x在模型参数θ = (*W, b*)下的能量。从这些方程中，我们可以将数据的良好模型解释为一个在高数据密度区域能量低而在其他地方能量高的模型θ。大小为Mx
    × Mx的矩阵W是对称的，并包含单元之间的连接强度。大小为Mx的向量b包含与每个单元相关的偏差。W的对角线被限制为零。单元可以是可见单元（代表感官输入）或隐藏单元（代表不可直接观察但有助于解释数据的潜变量）。
- en: From the equations above, we can derive the conditional probability of each
    unit being activated given the other units
  id: totrans-7405
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述方程中，我们可以推导出每个单元在其他单元给定的情况下被激活的条件概率
- en: $$p(x_{i}=1|x_{-i};\theta)=\operatorname{sign}(b_{i}+\sum_{j\neq i}W_{i j}x_{j})$$
  id: totrans-7406
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(x_{i}=1|x_{-i};\theta)=\operatorname{sign}(b_{i}+\sum_{j\neq i}W_{i j}x_{j})$$
- en: where x−i denotes the set of all units but xi. The gradient of the data loglikelihood
    with respect to model parameters W and b takes the form
  id: totrans-7407
  prefs: []
  type: TYPE_NORMAL
  zh: 其中x−i表示除了xi以外的所有单元的集合。关于模型参数W和b的数据对数似然的梯度呈现形式
- en: $$\frac{\partial}{\partial W}\langle\log p(x_{\rm vis};\theta)\rangle_{\rm data}=\langle
    xx^{\top}\rangle_{\rm data}-\langle xx^{\top}\rangle_{\rm model}\tag{25.1}$$ $$\frac{\partial}{\partial
    b}\langle\log p(x_{\rm vis};\theta)\rangle_{\rm data}=\langle x\rangle_{\rm data}-\langle
    x\rangle_{\rm model}\tag{25.2}$$
  id: totrans-7408
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{\partial}{\partial W}\langle\log p(x_{\rm vis};\theta)\rangle_{\rm data}=\langle
    xx^{\top}\rangle_{\rm data}-\langle xx^{\top}\rangle_{\rm model}\tag{25.1}$$ $$\frac{\partial}{\partial
    b}\langle\log p(x_{\rm vis};\theta)\rangle_{\rm data}=\langle x\rangle_{\rm data}-\langle
    x\rangle_{\rm model}\tag{25.2}$$
- en: where xvis are the visible units (i.e. the subset of units that represent the
    sensory input). The terms ·data and ·model are respectively the data-dependent
    expectations (obtained by conditioning the joint distribution on the observed
    state of the visible units) and the data-independent expectations obtained by
    sampling freely from the joint probability distribution.
  id: totrans-7409
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 xvis 是可见单元（即代表感官输入的单元子集）。术语 ·data 和 ·model 分别是数据依赖的期望（通过对可见单元的观察状态进行联合分布的条件化获得）和数据独立的期望，通过从联合概率分布中自由采样获得。
- en: 25.2.1 Deep Boltzmann Machines
  id: totrans-7410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.2.1 深度玻尔兹曼机
- en: It is often desirable to incorporate some predefined structure to the Boltzmann
    machine. The most common way to achieve this is to remove certain connections
    in the network, that is, forcing parts of the matrix W to zero. Example of Boltzmann
    machines with different structures are shown in Figure 25.1. For example, in the
    deep Boltzmann machine (DBM) [12], units are organized in a deep layered manner
    where only adjacent layers communicate and where units within the same layer are
    disconnected. Locally connected deep Boltzmann machines add further constraints
    to the model by forcing the modeling of the interaction between remote parts of
    the input to take place in the top layer.
  id: totrans-7411
  prefs: []
  type: TYPE_NORMAL
  zh: 通常希望在玻尔兹曼机中纳入一些预定义的结构。实现这一点最常见的方法是去除网络中的某些连接，即强制矩阵 W 的某些部分为零。具有不同结构的玻尔兹曼机示例如图
    25.1 所示。例如，在深度玻尔兹曼机（DBM）[12]中，单元以深层次的方式组织，其中只有相邻层进行通信，而同一层内的单元是断开的。局部连接的深度玻尔兹曼机通过强制模型在顶层进行远程输入部分之间的交互建模，进一步增加了模型的约束。
- en: 'The special layered structure of the DBM and its multiple variants has two
    advantages: First, particular statistics can emerge at each layer that may capture
    interesting features of data. Second, the layered structure of the DBM can be
    folded into a bipartite graph (one side containing odd layers and the other side
    containing even layers) where each side of the graph is conditionally independent
    given the other side.'
  id: totrans-7412
  prefs: []
  type: TYPE_NORMAL
  zh: 深度玻尔兹曼机（DBM）及其多种变体的特殊分层结构有两个优点：首先，每一层可以出现特定的统计量，这些统计量可能捕捉到数据的有趣特征。其次，DBM 的分层结构可以折叠成一个二分图（一个侧包含奇数层，另一个侧包含偶数层），其中图的每一侧在给定另一侧的条件下是独立的。
- en: In the case of the deep Boltzmann machine shown in Figure 25.2 (left) with Mx,
    My and Mz units at each layer, the energy associated to each state (x, y, z) ∈
  id: totrans-7413
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 25.2（左）所示的深度玻尔兹曼机中，每层有 Mx, My 和 Mz 单元，状态（x, y, z）对应的能量为
- en: '{0, 1}Mx+My+Mz is'
  id: totrans-7414
  prefs: []
  type: TYPE_NORMAL
  zh: '{0, 1}Mx+My+Mz 是'
- en: $$E(x,y,z;\theta)=-\,y^{\top}W x-z^{\top}V y-x^{\top}a-y^{\top}b-z^{\top}c$$
  id: totrans-7415
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(x,y,z;\theta)=-\,y^{\top}W x-z^{\top}V y-x^{\top}a-y^{\top}b-z^{\top}c$$
- en: 'where θ = {*W, V, a, b, c*} groups the model parameters. The bipartite graph
    structure of the deep Boltzmann machine implies that an efficient alternating
    Gibbs sampler can be derived:'
  id: totrans-7416
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 θ = {*W, V, a, b, c*} 代表模型参数的组。深度玻尔兹曼机的二分图结构意味着可以导出一个高效的交替吉布斯采样器：
- en: $x\sim\mathcal{B}(\text{sigm}(W^{\top}y+a))$  $z\sim\mathcal{B}(\text{sigm}(Vy+c))$  $y\sim\mathcal{B}(\text{sigm}(Wx+V^{\top}z+b))$.
  id: totrans-7417
  prefs: []
  type: TYPE_NORMAL
  zh: $x\sim\mathcal{B}(\text{sigm}(W^{\top}y+a))$  $z\sim\mathcal{B}(\text{sigm}(Vy+c))$  $y\sim\mathcal{B}(\text{sigm}(Wx+V^{\top}z+b))$。
- en: $$(25.3)$$
  id: totrans-7418
  prefs: []
  type: TYPE_NORMAL
  zh: $$(25.3)$$
- en: 'A similar alternating Gibbs sampler can be used for sampling states when the
    input units x are clamped to the data:'
  id: totrans-7419
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的交替吉布斯采样器可以用于当输入单元 x 被固定到数据时的状态采样：
- en: $$z\sim{\mathcal{B}}(\operatorname{sigm}(V y+c))$$ $$y\sim{\mathcal{B}}(\operatorname{sigm}(W
    x_{\operatorname*{data}}+V^{\top}z+b)).$$
  id: totrans-7420
  prefs: []
  type: TYPE_NORMAL
  zh: $$z\sim{\mathcal{B}}(\operatorname{sigm}(V y+c))$$ $$y\sim{\mathcal{B}}(\operatorname{sigm}(W
    x_{\operatorname*{data}}+V^{\top}z+b)).$$
- en: These alternating Gibbs samplers are illustrated in Figure 25.2 (right) and
    allow us to collect the data-independent and data-dependent statistics that intervene
    in the computation of the gradient (see Equation 25.1 and 25.2).
  id: totrans-7421
  prefs: []
  type: TYPE_NORMAL
  zh: 这些交替吉布斯采样器在图 25.2（右）中进行了说明，允许我们收集在计算梯度时干预的数据独立和数据依赖统计量（见方程 25.1 和 25.2）。
- en: '![613_image_1.png](613_image_1.png)'
  id: totrans-7422
  prefs: []
  type: TYPE_IMG
  zh: '![613_image_1.png](613_image_1.png)'
- en: $$(25.4)$$
  id: totrans-7423
  prefs: []
  type: TYPE_NORMAL
  zh: $$(25.4)$$
- en: '![613_image_0.png](613_image_0.png)'
  id: totrans-7424
  prefs: []
  type: TYPE_IMG
  zh: '![613_image_0.png](613_image_0.png)'
- en: '![613_image_2.png](613_image_2.png)'
  id: totrans-7425
  prefs: []
  type: TYPE_IMG
  zh: '![613_image_2.png](613_image_2.png)'
- en: '![613_image_3.png](613_image_3.png)'
  id: totrans-7426
  prefs: []
  type: TYPE_IMG
  zh: '![613_image_3.png](613_image_3.png)'
- en: 'Fig. 25.2. On the left, diagram of a two-layer deep Boltzmann machine along
    with its parameters. On the right, different sampling methods: (i) the path followed
    by the alternating Gibbs sampler and (ii) the path followed by the alternating
    Gibbs sampler when the input is clamped to data.'
  id: totrans-7427
  prefs: []
  type: TYPE_NORMAL
  zh: 图25.2：左侧为两层深度玻尔兹曼机的示意图及其参数；右侧为不同的采样方法：（i）交替Gibbs采样器遵循的路径，以及（ii）当输入固定为数据时交替Gibbs采样器遵循的路径。
- en: 25.3 Training Boltzmann Machines
  id: totrans-7428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.3 训练玻尔兹曼机
- en: While Equation 25.1 and 25.2 provide an exact gradient for minimizing the log-likelihood
    of data, keeping track of data statistics and model statistics is computationally
    demanding. The mixing rate of the model (i.e. the speed at which the alternating
    Gibbs sampler converges to the model's true distribution)
  id: totrans-7429
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然方程25.1和25.2提供了最小化数据对数似然的精确梯度，但跟踪数据统计和模型统计的计算需求较高。模型的混合率（即交替Gibbs采样器收敛到模型真实分布的速度）
- en: is typically slow and implies that we need to resort to some approximation.
  id: totrans-7430
  prefs: []
  type: TYPE_NORMAL
  zh: 通常较慢，这意味着我们需要依赖某些近似。
- en: Collecting *data-dependent statistics* is relatively easy as the complexity
    of the distribution is reduced by the clamping of visible units to the data. In
    the case where hidden units are independent when conditioned on the visible units,
    sampling can be achieved exactly in only one pass of the Gibbs sampler. This is
    the case of the restricted Boltzmann machine [4] presented in Chapter 24 [5].
    In practice, when the number of hidden-to-hidden connections is relatively low
    or the connections are not particularly strong, reasonable approximations can
    be obtained by running a few steps of the alternating Gibbs sampler.
  id: totrans-7431
  prefs: []
  type: TYPE_NORMAL
  zh: 收集*数据依赖统计*相对简单，因为通过将可见单元固定在数据上，分布的复杂性降低。在隐藏单元在条件可见单元时相互独立的情况下，可以通过一次Gibbs采样器的传递准确地实现采样。这是第24章中介绍的限制玻尔兹曼机[4]的情况[5]。在实践中，当隐藏到隐藏的连接数量相对较低或连接不特别强时，可以通过运行几个交替Gibbs采样器的步骤获得合理的近似。
- en: Collecting *data-independent statistics* is much harder and typically requires
    hundreds or thousands of iterations before converging to the true probability
    distribution. A workaround to the problem is to approximate these statistics by
    a small set of persistent chains (or "free particles") {x1*,...,x*n} that are
    continuously updated throughout training. This idea called *persistent contrastive*
    divergence has been proposed by Tieleman [19].
  id: totrans-7432
  prefs: []
  type: TYPE_NORMAL
  zh: 收集*数据无关统计*要困难得多，通常需要数百或数千次迭代才能收敛到真实概率分布。解决该问题的一种方法是通过一小组持续链（或“自由粒子”）{x1*,...,x*n}来近似这些统计，这些链在整个训练过程中持续更新。这种被称为*持续对比*散度的想法是由Tieleman提出的[19]。
- en: 'The intuition behind persistent contrastive divergence is the following: let''s
    first remember that the minima of the energy function correspond to high probability
    states and that the free particles are therefore inclined to descend the energy
    function. As the model is trained, the energy of the free particles is raised
    under the effect of the gradient update and free particles are encouraged to slide
    down the "bump" created by the gradient update. The higher the learning rate,
    the higher the bumps, and the faster the particles are descending the energy function.
    This implies that free particles are mixing much faster under the effect of training
    than in a static setting.'
  id: totrans-7433
  prefs: []
  type: TYPE_NORMAL
  zh: 持续对比散度的直觉如下：首先要记住，能量函数的最小值对应于高概率状态，因此自由粒子倾向于沿着能量函数下降。随着模型的训练，自由粒子的能量在梯度更新的影响下提高，自由粒子被鼓励沿着梯度更新所创建的“凸起”滑下。学习率越高，凸起越高，粒子下降能量函数的速度越快。这意味着，自由粒子在训练影响下的混合速度远快于静态设置。
- en: 'When training a deep Boltzmann machine, at least two sources of instability
    can be identified: (1) *Approximation instability:* The stochastic and approximate
    nature of the learning algorithms described above implies that the estimation
    of the gradient is noisy. The noise comes in part from the stochastic gradient
    descent procedure, but principally from the approximate sampling procedure that
    may cause systematically biased estimates of the gradient. (2) *Structural* instability:
    As it has been identified by Cho et al. [3], in standard Boltzmann machines, the
    weight matrix W tends to model in the first steps of the learning algorithm a
    global bias instead of co-dependencies between each units as we would expect.
    This is particularly problematic in the case of a Boltzmann machine with hidden-to-hidden
    connections such as the DBM, because hidden units tend to conglomerate and form
    a bias that may speed up learning initially but that eventually destroys the learning
    signal between pairs of hidden units.'
  id: totrans-7434
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度玻尔兹曼机时，可以识别出至少两种不稳定性来源：(1) *近似不稳定性：* 上述学习算法的随机和近似性质意味着梯度的估计是嘈杂的。噪声部分来自随机梯度下降过程，但主要来自近似采样过程，这可能导致梯度的系统性偏差估计。(2)
    *结构*不稳定性：正如 Cho 等人 [3] 所指出的，在标准玻尔兹曼机中，权重矩阵 W 在学习算法的前几步往往倾向于建模全局偏差，而不是我们所期望的每个单元之间的共依赖性。这在具有隐藏到隐藏连接的玻尔兹曼机（如
    DBM）中尤其成问题，因为隐藏单元倾向于聚合并形成一个偏差，这可能会加速最初的学习，但最终会破坏隐藏单元对之间的学习信号。
- en: 25.3.1 The Centering Trick
  id: totrans-7435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.3.1 中心化技巧
- en: The centering trick attempts to mitigate these sources of instability by ensuring
    that units activations intervening in the computation of the gradient are centered.
    Centering aims to produce a better conditioned optimization problem that is more
    robust to the noise of the learning procedure and to avoid the use of units as
    a global bias. Centering was already advocated in Chapter 1 [7] and 10
  id: totrans-7436
  prefs: []
  type: TYPE_NORMAL
  zh: 中心化技巧试图通过确保在梯度计算中介入的单元激活是中心化的，从而缓解这些不稳定性来源。中心化旨在产生一个更好条件的优化问题，该问题对学习过程中的噪声更加稳健，并避免将单元作为全局偏差的使用。中心化在第
    1 章 [7] 和第 10 章中已被提倡。
- en: '[17] in the context of backpropagation networks. Centering can be achieved
    by rewriting the energy of the Boltzmann machine as a function of centered states:'
  id: totrans-7437
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] 在反向传播网络的背景下。中心化可以通过将玻尔兹曼机的能量重写为中心状态的函数来实现：'
- en: Center the Boltzmann machine
  id: totrans-7438
  prefs: []
  type: TYPE_NORMAL
  zh: 中心化玻尔兹曼机
- en: '![614_image_0.png](614_image_0.png)'
  id: totrans-7439
  prefs: []
  type: TYPE_IMG
  zh: '![614_image_0.png](614_image_0.png)'
- en: '| E(x; θ) = − 1 2 (x − β) W(x − β) − (x − β)    | b   |'
  id: totrans-7440
  prefs: []
  type: TYPE_TB
  zh: '| E(x; θ) = − 1 2 (x − β) W(x − β) − (x − β)    | b   |'
- en: '| --- | --- |'
  id: totrans-7441
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '![614_image_1.png](614_image_1.png)'
  id: totrans-7442
  prefs: []
  type: TYPE_IMG
  zh: '![614_image_1.png](614_image_1.png)'
- en: 'The new variable β represents the *offset* associated to each unit of the network
    and must be set to the mean activation of x. Setting β = sigm(b0) where b0 is
    the initial bias ensures that units are initially centered. A similar parameterization
    of the energy function has been proposed by Tang and Sutskever [18] where the
    offset parameters were restricted to visible units. As we will see later, the
    Boltzmann machine also benefits from centering the hidden units. From this new
    energy function, we can derive the conditional probability of each unit in the
    centered Boltzmann machine:'
  id: totrans-7443
  prefs: []
  type: TYPE_NORMAL
  zh: 新变量 β 表示与网络每个单元相关的 *偏移量*，并且必须设置为 x 的平均激活值。将 β = sigm(b0)（其中 b0 是初始偏差）确保单元最初是中心化的。Tang
    和 Sutskever [18] 提出了能量函数的类似参数化，其中偏移参数被限制在可见单元上。正如我们稍后将看到的，玻尔兹曼机也受益于隐藏单元的中心化。从这个新的能量函数中，我们可以推导出中心玻尔兹曼机中每个单元的条件概率：
- en: $$p(x_{i}=1|x_{-i};\theta)=\operatorname{sign}(b_{i}+\sum_{j\neq i}W_{i j}(x-\beta)_{j}).$$
  id: totrans-7444
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(x_{i}=1|x_{-i};\theta)=\operatorname{sign}(b_{i}+\sum_{j\neq i}W_{i j}(x-\beta)_{j}).$$
- en: 'Similarly, the gradient of the model log-likelihood with respect to W and b
    now takes the form:'
  id: totrans-7445
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，模型对 W 和 b 的对数似然的梯度现在呈现如下形式：
- en: $$\frac{\partial}{\partial W}\langle\log p(x_{\rm vis};\theta)\rangle_{\rm data}=\langle(x-\beta)(x-\beta)^{\top}\rangle_{\rm
    data}-\langle(x-\beta)(x-\beta)^{\top}\rangle_{\rm model}\tag{25.5}$$  $$\frac{\partial}{\partial
    b}\langle\log p(x_{\rm vis};\theta)\rangle_{\rm data}=\langle x-\beta\rangle_{\rm
    data}-\langle x-\beta\rangle_{\rm model}.\tag{25.6}$$
  id: totrans-7446
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{\partial}{\partial W}\langle\log p(x_{\rm vis};\theta)\rangle_{\rm data}=\langle(x-\beta)(x-\beta)^{\top}\rangle_{\rm
    data}-\langle(x-\beta)(x-\beta)^{\top}\rangle_{\rm model}\tag{25.5}$$  $$\frac{\partial}{\partial
    b}\langle\log p(x_{\rm vis};\theta)\rangle_{\rm data}=\langle x-\beta\rangle_{\rm
    data}-\langle x-\beta\rangle_{\rm model}.\tag{25.6}$$
- en: These gradients are similar to the *enhanced gradients* proposed by Cho et al.
    [3]
  id: totrans-7447
  prefs: []
  type: TYPE_NORMAL
  zh: 这些梯度类似于 Cho 等人 [3] 提出的 *增强梯度*
- en: and to those arising from the parameterization proposed by Arnold et al. [1]
    at the difference that our gradients do not account for the possibility that offsets
    β deviate from the mean activations of units throughout training. If the latter
    effect is problematic, it is possible to reparameterize the network continuously
    or at regular intervals so that the offsets correspond to the new expected means
    xdata. The reparameterization θ → θ must leave the energy function invariant up
    to a constant, that is, E(x; θ) = E(x; θ) + const. Solving the equation under
    the new centering constraints leads to the update equations W = W, b =
  id: totrans-7448
  prefs: []
  type: TYPE_NORMAL
  zh: 并且与 Arnold 等人提出的参数化方案相比[1]，我们的梯度并未考虑偏置 β 在训练过程中偏离单元的平均激活的可能性。如果后者的效果存在问题，可以不断或定期重新参数化网络，以使偏置对应新的期望均值
    xdata。重新参数化 θ → θ 必须使能量函数在常数项下不变，即 E(x; θ) = E(x; θ) + const。根据新的中心约束条件求解方程会导致更新方程
    W = W, b =
- en: b + W(xdata − β) and β = xdata.
  id: totrans-7449
  prefs: []
  type: TYPE_NORMAL
  zh: b + W(xdata − β) 和 β = xdata。
- en: Update biases and offsets at regular intervals
  id: totrans-7450
  prefs: []
  type: TYPE_NORMAL
  zh: 定期更新偏置和偏置
- en: '| b = b + W(xdata − β) β = xdata   |'
  id: totrans-7451
  prefs: []
  type: TYPE_TB
  zh: '| b = b + W(xdata − β) β = xdata   |'
- en: '| --- |'
  id: totrans-7452
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: Similar derivations can be made for the deep Boltzmann machine. The energy function
    of the deep Boltzmann machine becomes E(*x, y, z*; θ) = −(y−β)W(x−
  id: totrans-7453
  prefs: []
  type: TYPE_NORMAL
  zh: 深度玻尔兹曼机也可以进行类似的推导。深度玻尔兹曼机的能量函数变为 E(*x, y, z*; θ) = −(y−β)W(x−
- en: α)− (z − γ)V (y − β)− (x− α)a − (y − β)b − (z − γ)c where α, β and γ are the
    offsets associated to the units in each layer. A basic algorithm for training
    a centered deep Boltzmann machine is given in Figure 25.3.
  id: totrans-7454
  prefs: []
  type: TYPE_NORMAL
  zh: α)− (z − γ)V (y − β)− (x− α)a − (y − β)b − (z − γ)c，其中 α、β 和 γ 是每层中单元的偏置。训练中心深度玻尔兹曼机的基本算法如图
    25.3 所示。
- en: 25.3.2 Understanding The Centering Trick
  id: totrans-7455
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.3.2 理解中心技巧
- en: We look at the effect of centering on the stability of learning in a Boltzmann
    machine. We argue that when the Boltzmann machine is centered, the optimization
    problem is better conditioned (see Figure 25.5), more precisely, the ratio between
    the highest and the lowest eigenvalue of the Hessian H is smaller. We define ξ
    as the centered state ξ = x−β. Substituting x−β by ξ in Equation 25.5, the derivative
    of the data log-likelihood with respect to the weight parameter becomes
  id: totrans-7456
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考察了中心化对玻尔兹曼机学习稳定性的影响。我们认为，当玻尔兹曼机被中心化时，优化问题的条件更好（见图 25.5），更确切地说，Hessian H 的最大特征值与最小特征值之间的比率较小。我们定义
    ξ 为中心状态 ξ = x−β。在方程 25.5 中将 x−β 代入 ξ，数据对数似然相对于权重参数的导数变为
- en: $${\frac{\partial}{\partial W}}\langle\log p(x;\theta)\rangle_{\mathrm{data}}=\langle\xi\xi^{\top}\rangle_{W,\mathrm{data}}-\langle\xi\xi^{\top}\rangle_{W}$$
  id: totrans-7457
  prefs: []
  type: TYPE_NORMAL
  zh: $${\frac{\partial}{\partial W}}\langle\log p(x;\theta)\rangle_{\mathrm{data}}=\langle\xi\xi^{\top}\rangle_{W,\mathrm{data}}-\langle\xi\xi^{\top}\rangle_{W}$$
- en: Training a centered deep Boltzmann machine W, V = 0, 0 a, b, c = sigm−1(xdata),
    b0, c0 α, β, γ = sigm(a), sigm(b), sigm(c)
  id: totrans-7458
  prefs: []
  type: TYPE_NORMAL
  zh: 训练中心深度玻尔兹曼机 W, V = 0, 0 a, b, c = sigm−1(xdata), b0, c0 α, β, γ = sigm(a), sigm(b),
    sigm(c)
- en: initialize free particle (xm, ym, zm)=(*α, β, γ*)
  id: totrans-7459
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化自由粒子 (xm, ym, zm)=(*α, β, γ*)
- en: loop initialize data particle (xd, yd, zd)=(pick(data)*, β, γ*)
  id: totrans-7460
  prefs: []
  type: TYPE_NORMAL
  zh: 循环初始化数据粒子 (xd, yd, zd)=(pick(data)*, β, γ*)
- en: loop yd ∼ B(sigm(W(xd − α) + V (zd − γ) + b)) zd ∼ B(sigm(V (yd − β) + c))
  id: totrans-7461
  prefs: []
  type: TYPE_NORMAL
  zh: 循环 yd ∼ B(sigm(W(xd − α) + V (zd − γ) + b)) zd ∼ B(sigm(V (yd − β) + c))
- en: end loop ym ∼ B(sigm(W(xm − α) + V (zm − γ) + b)) xm ∼ B(sigm(W(ym − β) + a))
  id: totrans-7462
  prefs: []
  type: TYPE_NORMAL
  zh: 循环 ym ∼ B(sigm(W(xm − α) + V (zm − γ) + b)) xm ∼ B(sigm(W(ym − β) + a))
- en: zm ∼ B(sigm(V (ym − β) + c))
  id: totrans-7463
  prefs: []
  type: TYPE_NORMAL
  zh: zm ∼ B(sigm(V (ym − β) + c))
- en: W = W + η · [(yd − β)(xd − α)
  id: totrans-7464
  prefs: []
  type: TYPE_NORMAL
  zh: W = W + η · [(yd − β)(xd − α)
- en: − (ym − *β)(x*m − α)
  id: totrans-7465
  prefs: []
  type: TYPE_NORMAL
  zh: − (ym − *β)(x*m − α)
- en: ']'
  id: totrans-7466
  prefs: []
  type: TYPE_NORMAL
  zh: ']'
- en: V = V + η · [(zd − γ)(yd − β)
  id: totrans-7467
  prefs: []
  type: TYPE_NORMAL
  zh: V = V + η · [(zd − γ)(yd − β)
- en: − (zm − *γ)(y*m − β)
  id: totrans-7468
  prefs: []
  type: TYPE_NORMAL
  zh: − (zm − *γ)(y*m − β)
- en: ']'
  id: totrans-7469
  prefs: []
  type: TYPE_NORMAL
  zh: ']'
- en: a = a + η · (xd − xm) + ν · W(yd − β)
  id: totrans-7470
  prefs: []
  type: TYPE_NORMAL
  zh: a = a + η · (xd − xm) + ν · W(yd − β)
- en: b = b + η · (yd − ym) + ν · W(xd − α) + ν · V (zd − γ)
  id: totrans-7471
  prefs: []
  type: TYPE_NORMAL
  zh: b = b + η · (yd − ym) + ν · W(xd − α) + ν · V (zd − γ)
- en: c = c + η · (zd − zm) + ν · V (yd − β) α = (1 − ν) · α + ν · xd β = (1 − ν)
    · β + ν · yd γ = (1 − ν) · γ + ν · zd end loop Fig. 25.3. Basic algorithm for
    training a two-layer centered deep Boltzmann machine. The algorithm is based on
    persistent contrastive divergence and is kept minimal for the sake of simplicity.
    The variable η is the learning rate and the variable ν is the rate of the moving
    average necessary for the reparameterization. A Python implementation of the algorithm
    is available at http://gregoire.montavon.name/code/dbm.py.
  id: totrans-7472
  prefs: []
  type: TYPE_NORMAL
  zh: c = c + η · (zd − zm) + ν · V (yd − β) α = (1 − ν) · α + ν · xd β = (1 − ν)
    · β + ν · yd γ = (1 − ν) · γ + ν · zd 结束循环 图 25.3. 训练两层中心深度玻尔兹曼机的基本算法。该算法基于持久对比散度，为了简化起见保持最小。变量
    η 是学习率，变量 ν 是重新参数化所需的移动平均率。该算法的 Python 实现可在 http://gregoire.montavon.name/code/dbm.py
    找到。
- en: '![616_image_0.png](616_image_0.png)'
  id: totrans-7473
  prefs: []
  type: TYPE_IMG
  zh: '![616_image_0.png](616_image_0.png)'
- en: Fig. 25.4. Example of sigmoids f(x) = sigm(x + b) − β with different biases
    b and offsets β. This figure illustrates how setting β0 = sigm(b0) ensures that
    the sigmoid crosses the origin initially and do not contribute to modeling a bias
    component.
  id: totrans-7474
  prefs: []
  type: TYPE_NORMAL
  zh: 图25.4. 不同偏置b和偏移β的sigmoid示例f(x) = sigm(x + b) − β。该图说明了设置β0 = sigm(b0)如何确保sigmoid最初穿过原点，并且不对偏置成分的建模产生影响。
- en: 'where ·W denotes the expectation with respect to the probability distribution
    associated to a model of weight parameter W and ·W,data denotes the expectation
    of the same model with visible units clamped to data. Using the definition of
    the directional derivative, the second derivative with respect to a random direction
    V (which is equal to the projected Hessian HV ) can be expressed as:'
  id: totrans-7475
  prefs: []
  type: TYPE_NORMAL
  zh: 其中·W表示相对于与权重参数W相关的概率分布的期望，而·W,data表示同一模型在可见单元被钳制为数据时的期望。使用方向导数的定义，关于随机方向V的二阶导数（等于投影Hessian
    HV）可以表示为：
- en: "HV = ∂∂V \t ∂ ∂W log p(x; W)data = lim h→0 1 h \t ∂ ∂W log p(x; W + hV )data\
    \ − ∂ ∂W log p(x; W)data = lim h→0 1 h (ξξW+hV,data − ξξW+hV ) − (ξξW,data − ξξW\
    \ ) = lim h→0 1 h ξξW+hV,data − ξξW,data− lim h→0 1 h ξξW+hV − ξξW"
  id: totrans-7476
  prefs: []
  type: TYPE_NORMAL
  zh: "HV = ∂∂V \t ∂ ∂W \\log p(x; W)data = \\lim_{h→0} \\frac{1}{h} \t \\left( \\\
    partial \\frac{∂W \\log p(x; W + hV )}{data} - \\partial \\frac{∂W \\log p(x;\
    \ W)}{data} \\right) = \\lim_{h→0} \\frac{1}{h} \\left( \\xi\\xi_{W+hV,data} -\
    \ \\xi\\xi_{W+hV} \\right) - \\left( \\xi\\xi_{W,data} - \\xi\\xi_{W} \\right)\
    \ = \\lim_{h→0} \\frac{1}{h} \\left( \\xi\\xi_{W+hV,data} - \\xi\\xi_{W,data}\
    \ \\right) - \\lim_{h→0} \\frac{1}{h} \\left( \\xi\\xi_{W+hV} - \\xi\\xi_{W} \\\
    right)"
- en: From the last line, we can see that the Hessian can be decomposed into a datadependent
    term and a data-independent term. A remarkable fact is that in absence of hidden
    units, the data-dependent part of the Hessian is zero, because the model—and therefore,
    the perturbation of the model—have no influence on the states. The conditioning
    of the optimization problem can therefore be analyzed exclusively from a model
    perspective. The data-dependent term is likely to be small even in the presence
    of hidden variables due to the sharp reduction of entropy caused by the clamping
    of visible units to data.
  id: totrans-7477
  prefs: []
  type: TYPE_NORMAL
  zh: 从最后一行我们可以看出，Hessian可以分解为一个与数据相关的项和一个与数据无关的项。一个显著的事实是，在没有隐含单元的情况下，Hessian的数据相关部分为零，因为模型——因此模型的扰动——对状态没有影响。因此，优化问题的条件性可以完全从模型的角度进行分析。即使在存在隐含变量的情况下，由于可见单元钳制数据而导致的熵急剧降低，数据相关项可能仍然很小。
- en: We can think of a well-conditioned model as a model for which a perturbation
    of the model parameter W in any direction V causes a well-behaved perturbation
    of state expectations ξξW . Pearlmutter [11] showed that in a Boltzmann machine
    with no hidden units, the projected Hessian can be further reduced to
  id: totrans-7478
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将良好条件的模型视为一个模型，对于该模型参数W在任意方向V的扰动，会导致状态期望ξξW的良好行为扰动。Pearlmutter [11] 证明，在没有隐含单元的玻尔兹曼机中，投影Hessian可以进一步简化为
- en: $$\mathbf{H}V=\langle\xi\xi^{\top}\rangle_{W}\cdot\langle D\rangle_{W}-\langle\xi\xi^{\top}D\rangle_{W}\qquad\mbox{where}\quad
    D=\frac{1}{2}\xi^{\top}V\xi\tag{25.7}$$
  id: totrans-7479
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbf{H}V=\langle\xi\xi^{\top}\rangle_{W}\cdot\langle D\rangle_{W}-\langle\xi\xi^{\top}D\rangle_{W}\qquad\mbox{其中}\quad
    D=\frac{1}{2}\xi^{\top}V\xi\tag{25.7}$$
- en: $$(25.8)$$
  id: totrans-7480
  prefs: []
  type: TYPE_NORMAL
  zh: $$(25.8)$$
- en: thus, getting rid of the limit and leading to numerically more accurate estimates.
  id: totrans-7481
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，消除了极限，从而导致数值上更准确的估计。
- en: Chapter 1 [7] shows that the stability of the optimization problem can be quantified
    by the *condition number* defined as the ratio between the largest eigenvalue
    λ1 and the smallest eigenvalue λn of H. A geometrical interpretation of the condition
    number is given in Figure 25.5 (left). A low rank approximation of the Hessian
    can be obtained as
  id: totrans-7482
  prefs: []
  type: TYPE_NORMAL
  zh: 第一章 [7] 显示，优化问题的稳定性可以通过定义为H的最大特征值λ1与最小特征值λn之比的*条件数*来量化。条件数的几何解释见图25.5（左）。Hessian的低秩近似可以表示为
- en: $${\hat{\mathbf{H}}}=\mathbf{H}(V_{0}|\dots|V_{n})=(\mathbf{H}V_{0}|\dots|\mathbf{H}V_{n})$$
  id: totrans-7483
  prefs: []
  type: TYPE_NORMAL
  zh: $${\hat{\mathbf{H}}}=\mathbf{H}(V_{0}|\dots|V_{n})=(\mathbf{H}V_{0}|\dots|\mathbf{H}V_{n})$$
- en: Hˆ = H(V0| ... |Vn)=(HV0| ... |HVn) (25.8)
  id: totrans-7484
  prefs: []
  type: TYPE_NORMAL
  zh: Hˆ = H(V0| ... |Vn)=(HV0| ... |HVn) (25.8)
- en: where the columns of (V0| ... |Vn) form a basis of independent unit vectors
    that projects the Hessian on a low-dimensional random subspace. The condition
    number can then be estimated by performing a singular value decomposition of the
    projected Hessian Hˆ and taking the ratio between the largest and smallest resulting
    eigenvalues.
  id: totrans-7485
  prefs: []
  type: TYPE_NORMAL
  zh: 其中(V0| ... |Vn)的列形成独立单位向量的基，能够将Hessian投影到低维随机子空间中。条件数可以通过对投影Hessian Hˆ进行奇异值分解，并取最大特征值和最小特征值之间的比率来估计。
- en: We estimate the condition number λ1/λn of a fully connected Boltzmann machine
    of 50 units at initial state (W = 0) for different bias and offset pa-
  id: totrans-7486
  prefs: []
  type: TYPE_NORMAL
  zh: 我们估计一个具有50个单元的全连接玻尔兹曼机在初始状态（W = 0）下，对于不同偏置和偏移pa的条件数λ1/λn。
- en: '![618_image_0.png](618_image_0.png)'
  id: totrans-7487
  prefs: []
  type: TYPE_IMG
  zh: '![618_image_0.png](618_image_0.png)'
- en: '|                          | λ1/λn      | b = 2 b = 0 b = −2   |'
  id: totrans-7488
  prefs: []
  type: TYPE_TB
  zh: '|                          | λ1/λn      | b = 2 b = 0 b = −2   |'
- en: '| --- | --- | --- |'
  id: totrans-7489
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| β = sigm(2)              | 1.98 12.65 | 51.42                |'
  id: totrans-7490
  prefs: []
  type: TYPE_TB
  zh: '| β = sigm(2)              | 1.98 12.65 | 51.42                |'
- en: '| β = sigm(0)              | 22.97 1.82 | 22.20                |'
  id: totrans-7491
  prefs: []
  type: TYPE_TB
  zh: '| β = sigm(0)              | 22.97 1.82 | 22.20                |'
- en: '| β = sigm(−2) 52.72 13.40 | 1.94       |                      |'
  id: totrans-7492
  prefs: []
  type: TYPE_TB
  zh: '| β = sigm(−2) 52.72 13.40 | 1.94       |                      |'
- en: 'Fig. 25.5. Left: relation between the condition number λ1/λn and the shape
    of the optimization problem. Gradient descent is easier to achieve when the condition
    number is small. Right: condition number obtained for centered Boltzmann machines
    (shown in bold on the diagonal) and for non-centered deep Boltzmann machines (off-diagonal
    elements). It can clearly be seen that the condition number is much smaller when
    the Boltzmann machine is centered.'
  id: totrans-7493
  prefs: []
  type: TYPE_NORMAL
  zh: 图25.5。左侧：条件数λ1/λn与优化问题形状之间的关系。当条件数较小时，梯度下降更容易实现。右侧：为中心玻尔兹曼机（对角线处加粗显示）和非中心深度玻尔兹曼机（非对角元素）获得的条件数。可以清楚地看到，当玻尔兹曼机是中心时，条件数明显较小。
- en: rameters b and β using Equation 25.7 and 25.8. The condition numbers are obtained
    by drawing 100 random unit vectors for the projection matrix V and for each of
    them, estimating the statistics by sampling 1000 independent states ξ.
  id: totrans-7494
  prefs: []
  type: TYPE_NORMAL
  zh: 使用方程25.7和25.8来估算参数b和β。通过绘制100个随机单位向量得到条件数，对每个向量，通过抽样1000个独立状态ξ来估计统计量。
- en: Numerical estimates are given in Figure 25.5 (right) and clearly exhibit the
    better conditioning occurring when the Boltzmann machine is centered (i.e. when
    β = sigm(b)).
  id: totrans-7495
  prefs: []
  type: TYPE_NORMAL
  zh: 数值估计在图25.5（右）中给出，并清楚地显示出当玻尔兹曼机是中心时，条件更优（即当β = sigm(b)）。
- en: 25.4 Evaluating Boltzmann Machines
  id: totrans-7496
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.4 评估玻尔兹曼机
- en: In this section, we present two complementary approaches to evaluating a Boltzmann
    machine. The first method consists of looking at the discriminative components
    built in different portions of the Boltzmann machine (e.g. layers of a DBM) using
    kernels. The analysis is based on the work of Montavon et al. [8, 9] that characterizes
    the representation that emerges from the learning algorithm at each layer of a
    neural network. Second, we present a method introduced by Salakhutdinov and Hinton
    [13] that measures the generative performance of the Boltzmann machine in terms
    of log-likelihood of test data.
  id: totrans-7497
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提出两种互补的方法来评估玻尔兹曼机。第一种方法是使用核查看玻尔兹曼机不同部分（例如DBM的层）中构建的判别成分。该分析基于Montavon等人的工作[8,
    9]，该工作描述了学习算法在神经网络每层中产生的表示。其次，我们提出Salakhutdinov和Hinton [13]引入的方法，以测试数据的对数似然度来衡量玻尔兹曼机的生成性能。
- en: 25.4.1 Discriminative Analysis
  id: totrans-7498
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.4.1 判别分析
- en: When Boltzmann machines incorporate a special structure, for example, via restricted
    connectivity, it can be useful to measure the discriminative capability of the
    representations emerging in specific portions of the network. Measuring the discriminative
    capability of a group of units is non-trivial, because (1) the meaningful information
    is not carried in a symbolic form but instead, distributed across the multiple
    units of the group and (2) the mapping of each data point onto these units is
    not deterministic but instead corresponds to the conditional distribution over
    the units in the group given the input x. We present here a method that exploits
    the insight that the projection of the input distribution onto the group of units
    forms an kernel (with a well-defined feature space [16])
  id: totrans-7499
  prefs: []
  type: TYPE_NORMAL
  zh: 当玻尔兹曼机包含特殊结构，例如，通过限制连接时，测量在网络特定部分中出现的表示的判别能力是有用的。测量一组单元的判别能力并不简单，因为（1）有意义的信息不是以符号形式存在，而是分布在组内多个单元中，并且（2）每个数据点映射到这些单元上不是确定性的，而是对应于给定输入x时，组内单元的条件分布。我们在这里提出一种方法，利用输入分布投影到单元组上形成一个核（具有良定义特征空间
    [16]）。
- en: 'that can be analyzed with respect to certain properties (or labels) t. The
    method was first introduced by Montavon et al. [8] in the context of backpropagation
    networks. The method is based on the observation that leading kernel principal
    components can be approximated up to high accuracy from a finite, typically small
    set of samples [2]. We propose a family of kernels for representing the top layer
    of a DBM:'
  id: totrans-7500
  prefs: []
  type: TYPE_NORMAL
  zh: 可以根据某些属性（或标签）t进行分析。该方法最初由Montavon等人[8]在反向传播网络的背景下提出。该方法基于一个观察，即主核主成分可以通过有限的、通常是小样本集高精度近似得到[2]。我们提出了一系列核来表示深度玻尔兹曼机的顶层：
- en: 'A family of kernels for deep Boltzmann machines:'
  id: totrans-7501
  prefs: []
  type: TYPE_NORMAL
  zh: 深度玻尔兹曼机的一系列核：
- en: '![619_image_0.png](619_image_0.png)'
  id: totrans-7502
  prefs: []
  type: TYPE_IMG
  zh: '![619_image_0.png](619_image_0.png)'
- en: k(*x, x*) = Ez,z- [f(z|*x, z*|x)]
  id: totrans-7503
  prefs: []
  type: TYPE_NORMAL
  zh: k(*x, x*) = Ez,z- [f(z|*x, z*|x)]
- en: where z|x and z|x denote the random top-layer activities conditioned
  id: totrans-7504
  prefs: []
  type: TYPE_NORMAL
  zh: 其中z|x和z|x表示条件的随机顶层活动
- en: '![619_image_1.png](619_image_1.png)'
  id: totrans-7505
  prefs: []
  type: TYPE_IMG
  zh: '![619_image_1.png](619_image_1.png)'
- en: 'respectively on data points x and x and where the function f(z,z) is a similarity
    metric between z and z. Typical choices for f are:'
  id: totrans-7506
  prefs: []
  type: TYPE_NORMAL
  zh: 分别在数据点x和x上，其中函数f(z,z)是z和z之间的相似度度量。f的典型选择是：
- en: $$\mathrm{linear:}\quad f(z,z^{\prime})=\langle z,z^{\prime}\rangle$$
  id: totrans-7507
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{linear:}\quad f(z,z^{\prime})=\langle z,z^{\prime}\rangle$$
- en: $\text{xp}\left(\dfrac{-||z-z'|}{z}\right)$
  id: totrans-7508
  prefs: []
  type: TYPE_NORMAL
  zh: $\text{xp}\left(\dfrac{-||z-z'|}{z}\right)$
- en: 'radial basis function: f(z,z) = exp #−||z − z||'
  id: totrans-7509
  prefs: []
  type: TYPE_NORMAL
  zh: '径向基函数：f(z,z) = exp #−||z − z||'
- en: σ
  id: totrans-7510
  prefs: []
  type: TYPE_NORMAL
  zh: σ
- en: $
  id: totrans-7511
  prefs: []
  type: TYPE_NORMAL
  zh: $
- en: '![619_image_2.png](619_image_2.png)'
  id: totrans-7512
  prefs: []
  type: TYPE_IMG
  zh: '![619_image_2.png](619_image_2.png)'
- en: '![619_image_3.png](619_image_3.png)'
  id: totrans-7513
  prefs: []
  type: TYPE_IMG
  zh: '![619_image_3.png](619_image_3.png)'
- en: 'equality: f(z,z)=1{z=z-}'
  id: totrans-7514
  prefs: []
  type: TYPE_NORMAL
  zh: 相等：f(z,z)=1{z=z-}
- en: 'These kernels are able to gracefully deal with multimodal posteriors in the
    toplevel distribution as the expectation operator lies outside the "detection
    function" f and therefore, accounts for the all possible modalities of z|x and
    z|x. Note that since the units of the Boltzmann machine are binary, norms *||
    · ||*11,. . . ,*|| · ||*pp are equivalent. Also, the linear and equality functions
    correspond up to some normalization to the extremal cases of the radial basis
    function kernel (with σ very large or very small). Once a kernel has been chosen,
    the analysis proceeds in four steps:'
  id: totrans-7515
  prefs: []
  type: TYPE_NORMAL
  zh: 这些核能够优雅地处理顶层分布中的多模态后验，因为期望算子位于“检测函数”f之外，因此考虑了z|x和z|x的所有可能模态。注意，由于玻尔兹曼机的单位是二元的，范数*||
    · ||*11,. . . ,*|| · ||*pp是等价的。此外，线性和相等函数在某些归一化下与径向基函数核的极端情况相对应（当σ非常大或非常小时）。一旦选择了一个核，分析将分为四个步骤：
- en: 1. Collect a small test set X of size n × m and its associated label matrix
    T
  id: totrans-7516
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 收集一个小的测试集X，大小为n × m及其相关标签矩阵T
- en: of size n × c and compute the empirical kernel K of size n × n. The kernel can
    be built iteratively by running a Gibbs sampler on each data point and taking
    the average of all kernels. Alternatively a moving average of the kernel matrix
    can be maintained throughout training in order to keep track of the discriminative
    performance.
  id: totrans-7517
  prefs: []
  type: TYPE_NORMAL
  zh: 大小为n × c，并计算大小为n × n的经验核K。可以通过对每个数据点运行吉布斯采样器并取所有核的平均值来迭代构建该核。或者，可以在训练过程中维护核矩阵的移动平均，以跟踪判别性能。
- en: 2. Perform an eigendecomposition of the kernel matrix K = UΛU  where Λ
  id: totrans-7518
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 对核矩阵K = UΛU进行特征分解，其中Λ
- en: is a diagonal matrix representing the eigenvalues of K sorted by decreasing
    order, and where the columns of U represent the eigenvectors associated to each
    eigenvalue. These eigenvectors are the kernel principal components of X with respect
    to the kernel k and form a non-linear subspace that spans the main directions
    of variation in the data [15].
  id: totrans-7519
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个对角矩阵，表示按降序排列的K的特征值，其中U的列表示与每个特征值相关联的特征向量。这些特征向量是相对于核k的X的核主成分，并形成一个非线性子空间，跨越数据中变化的主要方向[15]。
- en: 3. The representation is then evaluated by looking at how many kernel principal
    components are capturing the task T . Let U1..d and Λ1..d be the matrices containing
    respectively the d leading eigenvectors and eigenvalues. Compute the projected
    outputs Yd = U1..dU 1..dT . These predictions are the optimal fit in the least
    square sense based on the d kernel principal components1.
  id: totrans-7520
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 然后通过查看多少个核主成分捕捉任务T来评估表示。设U1..d和Λ1..d分别为包含d个主特征向量和特征值的矩阵。计算投影输出Yd = U1..dU
    1..dT。这些预测是基于d个核主成分在最小二乘意义上的最佳拟合。
- en: 1 Alternatively, if we would like to focus on the discrimination boundary between
    classes, a logistic model of type maxβ trace(log(softmax(Φ1..d ·β)·T )) can be
    fitted, where Φ1..d = U1..dΛ0.5 1..d is the empirical feature space and β of size
    d × c contains the regression parameters.
  id: totrans-7521
  prefs: []
  type: TYPE_NORMAL
  zh: 1 或者，如果我们想关注类别之间的判别边界，可以拟合一种类型的逻辑模型maxβ trace(log(softmax(Φ1..d ·β)·T ))，其中Φ1..d
    = U1..dΛ0.5 1..d是经验特征空间，β的大小为d × c，包含回归参数。
- en: '4. Compute the residuals curve e(d) and the AUC:'
  id: totrans-7522
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 计算残差曲线e(d)和AUC：
- en: $$e(d)=||T-Y_{d}||_{\mathrm{F}}^{2}\qquad\mathrm{AUC}={\frac{1}{n}}\sum_{d=1}^{n}e(d)$$
  id: totrans-7523
  prefs: []
  type: TYPE_NORMAL
  zh: $$e(d)=||T-Y_{d}||_{\mathrm{F}}^{2}\qquad\mathrm{AUC}={\frac{1}{n}}\sum_{d=1}^{n}e(d)$$
- en: $$(25.9)$$
  id: totrans-7524
  prefs: []
  type: TYPE_NORMAL
  zh: $$(25.9)$$
- en: e(d) (25.9)
  id: totrans-7525
  prefs: []
  type: TYPE_NORMAL
  zh: e(d) (25.9)
- en: These two quantities serve as metrics for evaluating how well the task is represented
    by the kernel. An interpretation of residuals curves e(d) is given in Figure 25.6.
  id: totrans-7526
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个量作为评估任务由核表示得如何的度量。图25.6中给出了残差曲线e(d)的解释。
- en: '![620_image_0.png](620_image_0.png)'
  id: totrans-7527
  prefs: []
  type: TYPE_IMG
  zh: '![620_image_0.png](620_image_0.png)'
- en: 'Fig. 25.6. Cartoon showing how to interpret residuals curves yield by various
    kernels on a certain task. *Scenario 1*: the kernel contains all label-relevant
    information in its principal components. This is the optimal case. *Scenario 2*:
    a large amount of labelrelevant information is contained in the leading components,
    but remaining information is missing. *Scenario 3*: the task relevant information
    is contained in a large number of principal components but can be predicted accurately.
    *Scenario 4*: the kernel is not suited for the task of interest. Note that although
    Scenario 2 and 3 have similar AUC, their residuals curves are qualitatively very
    different.'
  id: totrans-7528
  prefs: []
  type: TYPE_NORMAL
  zh: 图25.6。卡通图示展示了如何解释由各种核在特定任务中产生的残差曲线。*场景1*：核在其主成分中包含了所有与标签相关的信息。这是最优情况。*场景2*：在主成分中包含大量与标签相关的信息，但缺失了其余信息。*场景3*：与任务相关的信息包含在大量主成分中，但可以被准确预测。*场景4*：该核不适合感兴趣的任务。请注意，尽管场景2和3的AUC相似，但它们的残差曲线在性质上是非常不同的。
- en: 25.4.2 Generative Analysis
  id: totrans-7529
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.4.2 生成分析
- en: The generative performance, measured in terms of data likelihood is what the
    Boltzmann machine is optimized for. Unfortunately, data likelihood can not be
    measured easily as it involves the estimation of the partition function that is
    generally intractable. As a consequence, we must recourse to sophisticated approximation
    schemes. We present an analysis introduced by Salakhutdinov and Murray [14] that
    estimates the likelihood of the learned Boltzmann machine based on annealed importance
    sampling (AIS) [10]. We describe here the basic analysis. Salakhutdinov and Hinton
    [13] introduced more elaborate procedures for particular types of Boltzmann machines
    such as restricted, semi-restricted and deep Boltzmann machines.
  id: totrans-7530
  prefs: []
  type: TYPE_NORMAL
  zh: 生成性能是通过数据似然性来衡量的，这是玻尔兹曼机优化的目标。不幸的是，数据似然性无法轻易测量，因为它涉及到分区函数的估计，而分区函数通常是不可解的。因此，我们必须借助复杂的近似方案。我们介绍了Salakhutdinov和Murray
    [14]提出的分析方法，该方法基于退火重要性采样（AIS）[10]来估计学习到的玻尔兹曼机的似然性。我们在这里描述基本分析。Salakhutdinov和Hinton
    [13]为特定类型的玻尔兹曼机，如限制性、半限制性和深度玻尔兹曼机，引入了更复杂的程序。
- en: As we have seen in Section 25.2, a deep Boltzmann machine associates to each
    input x a probability
  id: totrans-7531
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第25.2节中所见，深度玻尔兹曼机为每个输入x关联一个概率
- en: $$p(x;\theta)={\frac{\Psi(\theta,x)}{Z(\theta)}}$$
  id: totrans-7532
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(x;\theta)={\frac{\Psi(\theta,x)}{Z(\theta)}}$$
- en: where $\Psi(\theta,x)=\sum_{y,z}p^{\star}(x,y,z;\theta)$ and $Z(\theta)=\sum_{x,y,z}p^{\star}(x,y,z;\theta)$
  id: totrans-7533
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\Psi(\theta,x)=\sum_{y,z}p^{\star}(x,y,z;\theta)$和$Z(\theta)=\sum_{x,y,z}p^{\star}(x,y,z;\theta)$
- en: "and where p\f(*x, y, z*; θ) = e−E(*x,y,z*;θ) is the unnormalized probability\
    \ of state"
  id: totrans-7534
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 p(*x, y, z*; θ) = e−E(*x,y,z*;θ) 是状态的未归一化概率
- en: '(*x, y, z*). Computing Ψ(*θ, x*) and Z(θ) analytically is intractable because
    of the exponential number of elements involved in the sum. We must therefore resort
    to approximation procedures. Let us first rewrite the ratio of partition functions
    as follows:'
  id: totrans-7535
  prefs: []
  type: TYPE_NORMAL
  zh: (*x, y, z*). 由于涉及的元素数量呈指数级增长，因此解析计算Ψ(*θ, x*)和Z(θ)是不可行的。因此，我们必须求助于近似程序。让我们首先将分区函数的比率重写如下：
- en: $$p(x;\theta)={\frac{\Psi(\theta,x)}{Z(\theta)}}={\frac{{\frac{\Psi(\theta,x)}{\Psi(0,x)}}}{{\frac{Z(\theta)}{Z(0)}}}}\cdot{\frac{\Psi(0,x)}{Z(0)}}$$
  id: totrans-7536
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(x;\theta)={\frac{\Psi(\theta,x)}{Z(\theta)}}={\frac{{\frac{\Psi(\theta,x)}{\Psi(0,x)}}}{{\frac{Z(\theta)}{Z(0)}}}}\cdot{\frac{\Psi(0,x)}{Z(0)}}$$
- en: $$(25.10)$$
  id: totrans-7537
  prefs: []
  type: TYPE_NORMAL
  zh: $$(25.10)$$
- en: $$(25.11)$$
  id: totrans-7538
  prefs: []
  type: TYPE_NORMAL
  zh: $$(25.11)$$
- en: Z(0) (25.10)
  id: totrans-7539
  prefs: []
  type: TYPE_NORMAL
  zh: Z(0) (25.10)
- en: It can be noticed that the ratio of base-rate partition functions (θ = 0) is
    easy to compute as θ = 0 makes all units independent. It has the analytical form
  id: totrans-7540
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到基础速率分区函数（θ = 0）的比率很容易计算，因为θ = 0使所有单位独立。它具有解析形式。
- en: $${\frac{\Psi(0,x)}{Z(0)}}={\frac{1}{2^{M_{x}}}}.$$
  id: totrans-7541
  prefs: []
  type: TYPE_NORMAL
  zh: $${\frac{\Psi(0,x)}{Z(0)}}={\frac{1}{2^{M_{x}}}}.$$
- en: 2Mx . (25.11)
  id: totrans-7542
  prefs: []
  type: TYPE_NORMAL
  zh: 2Mx . (25.11)
- en: 'The two other ratios in Equation 25.10 can be estimated using annealed importance
    sampling. The annealed importance sampling method proceeds as follows:'
  id: totrans-7543
  prefs: []
  type: TYPE_NORMAL
  zh: 方程25.10中的两个比率可以使用退火重要性采样进行估计。退火重要性采样方法的过程如下：
- en: '![621_image_0.png](621_image_0.png)'
  id: totrans-7544
  prefs: []
  type: TYPE_IMG
  zh: '![621_image_0.png](621_image_0.png)'
- en: '![621_image_1.png](621_image_1.png)'
  id: totrans-7545
  prefs: []
  type: TYPE_IMG
  zh: '![621_image_1.png](621_image_1.png)'
- en: It can be shown that if the sequence of models θ0, θ1*,...,θ*K where θ0 = 0
    and θK = θ evolves slowly enough, the importance weight obtained with the annealed
    importance sampling procedure is an estimate for the ratio between the partition
    function of the model θ and the partition function of the base rate model.
  id: totrans-7546
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明，如果模型序列θ0, θ1*,...,θ*K缓慢演变，其中θ0 = 0和θK = θ，那么通过退火重要性采样程序获得的重要性权重是模型θ与基础速率模型的分区函数之间比率的估计。
- en: In our case, ξ denotes the state (*x, y, z*) of the DBM and the transition operator
    T (*ξ, ξ*; θ) corresponds to the alternating Gibbs samplers defined in Equation
    25.3 and 25.4. The sequence of parameters {θ0*,...,θ*K} can, for example, lie
    on the line between 0 and θ, that is, θk = αk · θK where α0 < ··· < αK. Alternatively,
    the sequence of parameters can be those that are observed throughout training.
    In that case, maintaining a moving average of the parameter throughout training
    is necessary as the learning noise creates unnecessarily large variations between
    two adjacent parameters.
  id: totrans-7547
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，ξ表示DBM的状态(*x, y, z*)，转移算子T(*ξ, ξ*; θ)对应于方程25.3和25.4中定义的交替吉布斯采样器。参数序列{θ0*,...,θ*K}可以，例如，位于0和θ之间的直线上，即θk
    = αk · θK，其中α0 < ··· < αK。或者，参数序列可以是训练过程中观察到的那些。在这种情况下，在训练过程中保持参数的移动平均是必要的，因为学习噪声会在两个相邻参数之间产生不必要的大变动。
- en: We can now compute the two ratios of partition functions of Equation 25.10 as
  id: totrans-7548
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以计算方程25.10中的两个分区函数比率为：
- en: $${\frac{Z(\theta)}{Z(0)}}\approx\mathrm{E}[\omega_{\mathrm{AIS}}]\quad{\mathrm{and}}\quad{\frac{\Psi(\theta,x)}{\Psi(0,x)}}\approx\mathrm{E}[\nu_{\mathrm{AIS}}(x)]$$
  id: totrans-7549
  prefs: []
  type: TYPE_NORMAL
  zh: $${\frac{Z(\theta)}{Z(0)}}\approx\mathrm{E}[\omega_{\mathrm{AIS}}]\quad{\mathrm{and}}\quad{\frac{\Psi(\theta,x)}{\Psi(0,x)}}\approx\mathrm{E}[\nu_{\mathrm{AIS}}(x)]$$
- en: where ωAIS is the importance weight resulting from the annealing process with
    the freely running Gibbs sampler and νAIS is the importance weight resulting from
    the annealing with input units clamped to the data point. Substituting Equation
    25.11 and 25.12 into Equation 25.10, we obtain
  id: totrans-7550
  prefs: []
  type: TYPE_NORMAL
  zh: 其中ωAIS是由自由运行的吉布斯采样器和νAIS通过将输入单位钳制到数据点所得到的退火过程中产生的重要性权重。将方程25.11和25.12代入方程25.10，我们得到：
- en: $$(25.12)$$
  id: totrans-7551
  prefs: []
  type: TYPE_NORMAL
  zh: $$(25.12)$$
- en: $$p(x;\theta)\approx{\frac{\operatorname{E}[\nu_{\mathrm{AIS}}(x)]}{\operatorname{E}[\omega_{\mathrm{AIS}}]}}\cdot{\frac{1}{2^{M_{x}}}}$$
  id: totrans-7552
  prefs: []
  type: TYPE_NORMAL
  zh: $$p(x;\theta)\approx{\frac{\operatorname{E}[\nu_{\mathrm{AIS}}(x)]}{\operatorname{E}[\omega_{\mathrm{AIS}}]}}\cdot{\frac{1}{2^{M_{x}}}}$$
- en: 'and therefore, the log-likelihood of the model is estimated as:'
  id: totrans-7553
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模型的对数似然被估计为：
- en: $${\rm E}_{X}[\log(p(x;\theta))]\approx{\rm E}_{X}[\log{\rm E}[\nu_{\rm AIS}(x)]]-\log{\rm
    E}[\omega_{\rm AIS}]-M_{x}\log(2).\tag{25.13}$$
  id: totrans-7554
  prefs: []
  type: TYPE_NORMAL
  zh: $${\rm E}_{X}[\log(p(x;\theta))]\approx{\rm E}_{X}[\log{\rm E}[\nu_{\rm AIS}(x)]]-\log{\rm
    E}[\omega_{\rm AIS}]-M_{x}\log(2).\tag{25.13}$$
- en: Generally, computing an average of the importance weight νAIS for each data
    point x can take a long time. In practice, we can approximate it with a single
    AIS run for each data point. In that case, it follows from Jensen's inequality
    that
  id: totrans-7555
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，对于每个数据点x计算重要性权重νAIS的平均值可能需要很长时间。在实践中，我们可以通过对每个数据点进行单次AIS运行来近似它。在这种情况下，由于詹森不等式的结果：
- en: $${\rm E}_{X}[\log\nu_{\rm AIS}(x)]-\log{\rm E}[\omega_{\rm AIS}]\leq{\rm E}_{X}[\log{\rm
    E}[\nu_{\rm AIS}(x)]]-\log{\rm E}[\omega_{\rm AIS}].\tag{25.14}$$
  id: totrans-7556
  prefs: []
  type: TYPE_NORMAL
  zh: $${\rm E}_{X}[\log\nu_{\rm AIS}(x)]-\log{\rm E}[\omega_{\rm AIS}]\leq{\rm E}_{X}[\log{\rm
    E}[\nu_{\rm AIS}(x)]]-\log{\rm E}[\omega_{\rm AIS}].\tag{25.14}$$
- en: Consequently, this approximation tends to produce slightly pessimistic estimates
    of the model log-likelihood, however the variance of νAIS is low compared to the
    variance of ωAIS because the clamping of visible units to data points reduces
    the diversity of AIS runs.
  id: totrans-7557
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这种近似往往会产生稍微悲观的模型对数似然估计，但由于将可见单位钳制到数据点减少了AIS运行的多样性，因此νAIS的方差相对于ωAIS的方差较低。
- en: 25.5 Experiments
  id: totrans-7558
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.5 实验
- en: In this section, we present a few experiments that demonstrate the effectiveness
    of the centering trick for learning deep Boltzmann machines. We use the MNIST
  id: totrans-7559
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了一些实验，证明了中心化技巧在学习深度玻尔兹曼机中的有效性。我们使用MNIST数据集。
- en: handwritten digits recognition dataset that consists of 60000 training samples
    and 10000 test samples. Each sample is a 28 × 28 grayscale image representing
    a handwritten digit along with its label. Grayscale values (between 0 and 1) are
    treated as probabilities.
  id: totrans-7560
  prefs: []
  type: TYPE_NORMAL
  zh: 手写数字识别数据集包含60000个训练样本和10000个测试样本。每个样本是一个28 × 28的灰度图像，表示一个手写数字及其标签。灰度值（介于0和1之间）被视为概率。
- en: 'Architectures: We consider a deep Boltzmann machine (DBM) made of 28 × 28 input
    units, 200 intermediate units and 25 top units and a locally-connected DBM (LC-DBM)
    made of 28×28 input units, 400 intermediate units that connect to random input
    patches of size 6 × 6 and 100 top units. These architectures are illustrated in
    Figure 25.7 (left). In the DBM, the modeling load is concentrated in the first
    layer with the top layer serving merely to model global digit-like features.'
  id: totrans-7561
  prefs: []
  type: TYPE_NORMAL
  zh: 结构：我们考虑一个由28 × 28输入单元、200个中间单元和25个顶层单元组成的深度玻尔兹曼机(DBM)，以及一个由28×28输入单元、400个连接到6
    × 6随机输入块的中间单元和100个顶层单元组成的局部连接DBM(LC-DBM)。这些结构在图25.7（左）中进行了说明。在DBM中，建模负担集中在第一层，而顶层仅用于建模全球数字特征。
- en: On the other hand, in the LC-DBM, most of the modeling load is postponed to
    the second layer and the first layer serves essentially as a low-level local preprocessor.
    The initial offsets and biases for visible units are set to α = xdata and a0 =
    sigm−1(α). We consider different initial biases (b0, c0 = −2, b0, c0 = 0 and b0,
    c0 = 2) and offsets (*β, γ* = sigm(−2), *β, γ* = sigm(0) and *β, γ* = sigm(2))
  id: totrans-7562
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在LC-DBM中，大部分建模负担被推迟到第二层，而第一层基本上作为低级局部预处理器。可见单元的初始偏移量和偏差设置为α = xdata，a0
    = sigm−1(α)。我们考虑不同的初始偏差(b0, c0 = −2, b0, c0 = 0 和 b0, c0 = 2)及偏移量(*β, γ* = sigm(−2),
    *β, γ* = sigm(0) 和 *β, γ* = sigm(2))。
- en: for the hidden units. These offsets and initial biases correspond to the sigmoids
    plotted in Figure 25.4.
  id: totrans-7563
  prefs: []
  type: TYPE_NORMAL
  zh: 对于隐藏单元，这些偏移量和初始偏差对应于图25.4中绘制的sigmoid曲线。
- en: 'Learning: We use persistent contrastive divergence [19] to train the network
    and keep track of 25 free particles in background of the learning procedure. We
    use a Gibbs sampler to collect *both* the data-independent and data-dependent
    statistics. At each iteration of the learning procedure, we run 3 iterations of
    the alternating Gibbs sampler for collecting the data-dependent statistics (from
    a minibatch of 25 data points) and one iteration for updating the data-independent
    statistics. We use a learning rate η = 0.0004 per sample for each layer.'
  id: totrans-7564
  prefs: []
  type: TYPE_NORMAL
  zh: 学习：我们使用持久对比散度[19]来训练网络，并在学习过程中跟踪25个自由粒子。我们使用吉布斯采样器收集*数据无关*和*数据依赖*的统计信息。在学习过程的每次迭代中，我们进行3次交替吉布斯采样器迭代以收集数据依赖统计（来自25个数据点的迷你批量），以及一次迭代以更新数据无关统计。我们对每一层使用学习率η
    = 0.0004。
- en: 'Evaluation: Evaluating the DBM in an online fashion requires to keep track
    of the model parameters throughout training. We reduce the learning noise by maintaining
    a moving average of the sequence of parameters observed during learning. The moving
    average is tuned to remember approximately 10% of the training history. We keep
    track of 500 data-dependent chains running on the smoothed sequence of parameters
    and from which top-layer statistics k(z,z) and ratios Ψ(θ, x)/Ψ(0, x) are estimated.
    We also keep track of 100 data-independent chains on the same sequence of parameters
    and from which the ratio Z(θ)/Z(0) is estimated. Discriminative performance is
    measured as the projection residuals of the labels on the kernel principal components
    and the area under the error curve'
  id: totrans-7565
  prefs: []
  type: TYPE_NORMAL
  zh: 评估：以在线方式评估DBM需要在训练过程中跟踪模型参数。我们通过维护观察到的参数序列的移动平均值来减少学习噪声。移动平均值被调整为记住大约10%的训练历史。我们跟踪500个依赖数据的链，这些链在平滑的参数序列上运行，从中估计顶层统计量k(z,z)和比例Ψ(θ,
    x)/Ψ(0, x)。我们还跟踪100个与数据无关的链，这些链在相同的参数序列上运行，从中估计比例Z(θ)/Z(0)。判别性能通过标签在核主成分上的投影残差和误差曲线下的面积来衡量。
- en: '(see Equation 25.9) using an exponential RBF kernel with σ set to the mean
    of pairwise distances between z and z. Generative performance is measured in terms
    of data log-likelihood (see Equation 25.13). Results: Figure 25.7 summarizes the
    results of our analysis and corroborates the importance of centering for obtaining
    a better discriminative and generative model of data. The centered DBM systematically
    produces better top-layer AUC errors and has higher log-likelihood. The importance
    of centering for improving generative models is particularly marked for the locally-connected
    DBM'
  id: totrans-7566
  prefs: []
  type: TYPE_NORMAL
  zh: （见方程25.9）使用指数RBF核，σ设置为z和z之间成对距离的平均值。生成性能通过数据对数似然度衡量（见方程25.13）。结果：图25.7总结了我们的分析结果，并证实了居中对于获得更好的数据区分性和生成模型的重要性。居中DBM系统性地产生更好的顶层AUC误差，并且具有更高的对数似然度。改善生成模型的居中重要性在局部连接DBM中尤为明显。
- en: (LC-DBM) where the top-layer is crucial for modeling long-range dependencies.
  id: totrans-7567
  prefs: []
  type: TYPE_NORMAL
  zh: （LC-DBM），其中顶层对于建模长程依赖性至关重要。
- en: These results suggest that the centering trick is particularly useful when dealing
    with hierarchical architectures where global statistics are handled only in the
    deep layers of the network. Figure 25.8 (left) shows that the centered DBM yields
    a kernel that contains most of the label information in its leading components
    and has a low level of noise in the remaining components. This corresponds to
    Scenario 1 of Figure 25.6. On the other hand, in the case of the non-centered
    DBM, the labels span a few leading components of the kernel, but the remaining
    components have a high level of noise. This corresponds to Scenario 2 of Figure
    25.6. As a comparison, the simple input-layer RBF kernel exhibits high dimensionality
    and low noise and thus, corresponds to Scenario 3 of Figure 25.6. The importance
    of centering for producing good top-layer kernels is further confirmed by looking
    at the second layer filters, visualized in Figure 25.8 (right) using a
  id: totrans-7568
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，当处理仅在网络深层处理全局统计的分层架构时，居中技巧特别有用。图25.8（左）显示，居中DBM产生的核在其主要成分中包含大部分标签信息，并且其余成分的噪声水平较低。这对应于图25.6的场景1。另一方面，在非居中DBM的情况下，标签跨越核的一些主要成分，但其余成分具有较高的噪声水平。这对应于图25.6的场景2。相比之下，简单的输入层RBF核表现出高维度和低噪声，因此对应于图25.6的场景3。通过查看第二层滤波器，进一步确认了居中对于生成良好顶层核的重要性，如图25.8（右）所示。
- en: '![624_image_0.png](624_image_0.png)'
  id: totrans-7569
  prefs: []
  type: TYPE_IMG
  zh: '![624_image_0.png](624_image_0.png)'
- en: Fig. 25.7. Evolution of the AUC error and log-likelihood throughout training.
    "Centered+" designates deep Boltzmann machines that are continuously recentered
    throughout training. In the DBM, reasonable generative performance can be achieved
    without centering as the top layer is simply ignored by the rest of the model.
    In the LC-DBM,
  id: totrans-7570
  prefs: []
  type: TYPE_NORMAL
  zh: 图25.7。训练过程中AUC误差和对数似然度的演变。“Centered+”表示在训练过程中持续居中的深度玻尔兹曼机。在DBM中，合理的生成性能可以在不居中的情况下实现，因为顶层被模型的其他部分简单忽略。在LC-DBM中，
- en: centering is important for both generative and discriminative performance as
    the top layer is required for modeling long-range dependencies. Continuously recentering
    yields the most robust performance.
  id: totrans-7571
  prefs: []
  type: TYPE_NORMAL
  zh: 居中对于生成和区分性能都很重要，因为顶层对于建模长程依赖性是必要的。持续居中可获得最稳健的性能。
- en: '![624_image_1.png](624_image_1.png)'
  id: totrans-7572
  prefs: []
  type: TYPE_IMG
  zh: '![624_image_1.png](624_image_1.png)'
- en: 'centered DBMs. Left: error residuals produced by centered DBMs and non-centered
    DBMs. Right: 2D-PCA (with a linear kernel) and second-layer filters. Results suggest
    richer top-layer representations for centered DBMs than for non-centered DBMs.'
  id: totrans-7573
  prefs: []
  type: TYPE_NORMAL
  zh: 居中DBM。左：居中DBM和非居中DBM产生的误差残差。右：2D-PCA（线性核）和第二层滤波器。结果表明，居中DBM的顶层表示比非居中DBM更丰富。
- en: linear back-projection, and observing that they are much richer for the centered
    DBM than for the non-centered one.
  id: totrans-7574
  prefs: []
  type: TYPE_NORMAL
  zh: 线性反投影，并观察到对于居中DBM来说，它们比非居中DBM要丰富得多。
- en: 25.6 Conclusion
  id: totrans-7575
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.6 结论
- en: Learning deep Boltzmann machines is a difficult optimization problem that can
    be highly sensitive to the parameterization of its energy function. In this chapter,
    we propose the *centering trick* that consists of rewriting the energy as a function
    of centered states. The centering trick improves the stability of deep Boltzmann
    machines and allows to learn models that exhibit both advantageous discriminative
    and generative properties.
  id: totrans-7576
  prefs: []
  type: TYPE_NORMAL
  zh: 学习深度玻尔兹曼机是一个困难的优化问题，对其能量函数的参数化非常敏感。在本章中，我们提出了*中心化技巧*，即将能量重写为中心状态的函数。中心化技巧提高了深度玻尔兹曼机的稳定性，并使我们能够学习同时具有有利的判别和生成特性的模型。
- en: Our experiments have been most successful on mid-scale models (in the range
    of a few hundred hidden units). The high representational power of deep Boltzmann
    machines makes it hard to extend our experiments to larger scale models
  id: totrans-7577
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验在中规模模型（大约几百个隐藏单元）上取得了最成功的结果。深度玻尔兹曼机的高表示能力使得我们难以将实验扩展到更大规模的模型。
- en: (of thousands of units) without using an explicit regularizer such as layer-wise
    pretraining or limited connectivity. We believe that applying the centering trick
    to large-scale models should be made in conjunction with a strong regularizer
    that limits the effective dimensionality of the model.
  id: totrans-7578
  prefs: []
  type: TYPE_NORMAL
  zh: （数千个单元）而不使用显式正则化器，如层级预训练或有限连接。我们认为，将中心化技巧应用于大规模模型应与强正则化器结合使用，以限制模型的有效维度。
- en: Acknowledgements. The authors thank Mikio Braun and the multiple reviewers for
    their useful comments. This work was supported by the World Class University Program
    through the National Research Foundation of Korea funded by the Ministry of Education,
    Science, and Technology, under Grant R31-10008. The authors also acknowledge partial
    support by DFG (MU 987/17-1).
  id: totrans-7579
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢。作者感谢Mikio Braun和多位审稿人提出的有用意见。本研究得到了韩国教育部、科学技术部资助的国家研究基金会通过世界一流大学项目的支持，资助号R31-10008。作者还感谢DFG（MU
    987/17-1）的部分支持。
- en: '[1] Arnold, L., Auger, A., Hansen, N., Ollivier, Y.: Information-geometric
    optimization algorithms: A unifying picture via invariance principles, arXiv:1106.3708'
  id: totrans-7580
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Arnold, L., Auger, A., Hansen, N., Ollivier, Y.：信息几何优化算法：通过不变性原理统一的视角，arXiv:1106.3708'
- en: (2011)
  id: totrans-7581
  prefs: []
  type: TYPE_NORMAL
  zh: （2011）
- en: '[2] Braun, M.L., Buhmann, J., Müller, K.-R.: On relevant dimensions in kernel
    feature spaces. Journal of Machine Learning Research 9, 1875–1908 (2008)'
  id: totrans-7582
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Braun, M.L., Buhmann, J., Müller, K.-R.：核特征空间中的相关维度。《机器学习研究杂志》9，1875–1908（2008）'
- en: '[3] Cho, K., Raiko, T., Ilin, A.: Enhanced gradient and adaptive learning rate
    for training restricted Boltzmann machines. In: Proceedings of the 28th International
    Conference on Machine Learning, pp. 105–112 (2011)'
  id: totrans-7583
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Cho, K., Raiko, T., Ilin, A.：用于训练限制玻尔兹曼机的增强梯度和自适应学习率。在：第28届国际机器学习会议论文集，页105–112（2011）'
- en: '[4] Hinton, G.E.: Training products of experts by minimizing contrastive divergence.'
  id: totrans-7584
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Hinton, G.E.：通过最小化对比散度训练专家的产品。'
- en: Neural Computation 14(8), 1771–1800 (2002)
  id: totrans-7585
  prefs: []
  type: TYPE_NORMAL
  zh: 神经计算 14(8)，1771–1800（2002）
- en: '[5] Hinton, G.E.: A Practical Guide to Training Restricted Boltzmann Machines.
    In:'
  id: totrans-7586
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Hinton, G.E.：训练限制玻尔兹曼机的实用指南。在：'
- en: 'Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the Trade, 2nd
    edn. LNCS, vol. 7700, pp. 599–619. Springer, Heidelberg (2012)'
  id: totrans-7587
  prefs: []
  type: TYPE_NORMAL
  zh: Montavon, G., Orr, G.B., Müller, K.-R.（编）《神经网络：行业技巧》，第2版。LNCS，第7700卷，页599–619。斯普林格，海德堡（2012）
- en: '[6] Hinton, G.E., Sejnowski, T.J.: Learning and relearning in Boltzmann machines.
    In:'
  id: totrans-7588
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Hinton, G.E., Sejnowski, T.J.：在玻尔兹曼机中学习与重学习。在：'
- en: 'Parallel Distributed Processing: Explorations in the Microstructure of Cognition,
    vol. 1, pp. 282–317. MIT Press (1986)'
  id: totrans-7589
  prefs: []
  type: TYPE_NORMAL
  zh: 并行分布式处理：认知微观结构的探索，第1卷，页282–317。MIT出版社（1986）
- en: '[7] LeCun, Y., Bottou, L., Orr, G.B., Müller, K.-R.: Efficient BackProp. In:
    Orr, G.B., Müller, K.-R. (eds.) NIPS-WS 1996. LNCS, vol. 1524, pp. 9–50. Springer,
    Heidelberg (1998)'
  id: totrans-7590
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] LeCun, Y., Bottou, L., Orr, G.B., Müller, K.-R.：高效的反向传播。在：Orr, G.B., Müller,
    K.-R.（编）《NIPS-WS 1996》。LNCS，第1524卷，页9–50。斯普林格，海德堡（1998）'
- en: '[8] Montavon, G., Braun, M.L., Müller, K.-R.: Kernel analysis of deep networks.'
  id: totrans-7591
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Montavon, G., Braun, M.L., Müller, K.-R.：深度网络的核分析。'
- en: Journal of Machine Learning Research 12, 2563–2581 (2011)
  id: totrans-7592
  prefs: []
  type: TYPE_NORMAL
  zh: 《机器学习研究杂志》12，2563–2581（2011）
- en: '[9] Montavon, G., Braun, M.L., Müller, K.-R.: Deep Boltzmann machines as feedforward
    hierarchies. Journal of Machine Learning Research - Proceedings Track 22, 789–804
    (2012)'
  id: totrans-7593
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Montavon, G., Braun, M.L., Müller, K.-R.：作为前馈层次的深度玻尔兹曼机。《机器学习研究杂志 - 会议记录》22，789–804（2012）'
- en: '[10] Neal, R.M.: Annealed importance sampling. Statistics and Computing 11(2),
    125–'
  id: totrans-7594
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Neal, R.M.：退火重要性采样。统计与计算 11(2)，125–'
- en: 139 (2001)
  id: totrans-7595
  prefs: []
  type: TYPE_NORMAL
  zh: 139（2001年）
- en: '[11] Pearlmutter, B.A.: Fast exact multiplication by the Hessian. Neural Computation
    6(1), 147–160 (1994)'
  id: totrans-7596
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Pearlmutter, B.A.: 快速精确的海森矩阵乘法。神经计算 6(1)，147–160（1994年）'
- en: '[12] Salakhutdinov, R., Hinton, G.E.: Deep Boltzmann machines. In: Proceedings
    of the International Conference on Artificial Intelligence and Statistics, vol.
    5, pp. 448–455 (2009)'
  id: totrans-7597
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Salakhutdinov, R., Hinton, G.E.: 深度玻尔兹曼机。载于：人工智能与统计国际会议论文集，第5卷，第448–455页（2009年）'
- en: '[13] Salakhutdinov, R.: Learning and Evaluating Boltzmann Machines. Technical
    Report UTML TR 2008-002, Dept. of Computer Science, University of Toronto'
  id: totrans-7598
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Salakhutdinov, R.: 玻尔兹曼机的学习与评估。技术报告 UTML TR 2008-002， 多伦多大学计算机科学系'
- en: (2008)
  id: totrans-7599
  prefs: []
  type: TYPE_NORMAL
  zh: （2008年）
- en: '[14] Salakhutdinov, R., Murray, I.: On the quantitative analysis of deep belief
    networks.'
  id: totrans-7600
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Salakhutdinov, R., Murray, I.: 深度信念网络的定量分析。'
- en: 'In: Proceedings of the 25th International Conference on Machine Learning, ICML
    2008, pp. 872–879 (2008)'
  id: totrans-7601
  prefs: []
  type: TYPE_NORMAL
  zh: 载于：第25届国际机器学习大会论文集，ICML 2008，第872–879页（2008年）
- en: '[15] Schölkopf, B., Smola, A., Müller, K.-R.: Nonlinear component analysis
    as a kernel eigenvalue problem. Neural Computation 10(5), 1299–1319 (1998)'
  id: totrans-7602
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Schölkopf, B., Smola, A., Müller, K.-R.: 非线性成分分析作为核特征值问题。神经计算 10(5)，1299–1319（1998年）'
- en: '[16] Schölkopf, B., Mika, S., Burges, C.J.C., Knirsch, P., Müller, K.-R., Rätsch,
    G.,'
  id: totrans-7603
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Schölkopf, B., Mika, S., Burges, C.J.C., Knirsch, P., Müller, K.-R., Rätsch,
    G.,'
- en: 'Smola, A.J.: Input space versus feature space in kernel-based methods. IEEE
    Transactions on Neural Networks 10(5), 1000–1017 (1999)'
  id: totrans-7604
  prefs: []
  type: TYPE_NORMAL
  zh: 'Smola, A.J.: 核方法中的输入空间与特征空间。IEEE神经网络汇刊 10(5)，1000–1017（1999年）'
- en: '[17] Schraudolph, N.N.: Centering Neural Network Gradient Factors. In: Orr,
    G.B.,'
  id: totrans-7605
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Schraudolph, N.N.: 中心化神经网络梯度因子。载于：Orr, G.B.,'
- en: Müller, K.-R. (eds.) NIPS-WS 1996. LNCS, vol. 1524, pp. 207–226. Springer, Heidelberg
    (1998)
  id: totrans-7606
  prefs: []
  type: TYPE_NORMAL
  zh: Müller, K.-R. (编辑) NIPS-WS 1996。LNCS，第1524卷，第207–226页。施普林格，海德堡（1998年）
- en: '[18] Tang, Y., Sutskever, I.: Data normalization in the learning of restricted
    Boltzmann machines. Technical Report UTML-TR-11-2, Department of Computer Science,
    University of Toronto (2011)'
  id: totrans-7607
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Tang, Y., Sutskever, I.: 在限制玻尔兹曼机学习中的数据归一化。技术报告 UTML-TR-11-2，多伦多大学计算机科学系（2011年）'
- en: '[19] Tieleman, T.: Training restricted Boltzmann machines using approximations
    to the likelihood gradient. In: Proceedings of the 25th International Conference
    on Machine Learning, pp. 1064–1071 (2008)'
  id: totrans-7608
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Tieleman, T.: 使用似然梯度近似训练限制玻尔兹曼机。载于：第25届国际机器学习大会论文集，第1064–1071页（2008年）'
- en: 26 Deep Learning Via Semi-Supervised Embedding
  id: totrans-7609
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 26 深度学习通过半监督嵌入
- en: Jason Weston1, Frédéric Ratle2, Hossein Mobahi3, and Ronan Collobert4, 1 Google,
    New York, USA
  id: totrans-7610
  prefs: []
  type: TYPE_NORMAL
  zh: Jason Weston1, Frédéric Ratle2, Hossein Mobahi3, 和 Ronan Collobert4，1 谷歌，美国纽约
- en: jweston@google.com 2 Nuance Communications, Montreal, Canada frederic.ratle@gmail.com
    3 Department of Computer Science, University of Illinois Urbana-Champaign, USA
  id: totrans-7611
  prefs: []
  type: TYPE_NORMAL
  zh: jweston@google.com 2 Nuance Communications, Montreal, Canada frederic.ratle@gmail.com
    3 伊利诺伊大学厄本那-香槟分校计算机科学系，美国
- en: hmobahi2@illinois.edu 4 IDIAP Research Institute, Martigny, Switzerland ronan@collobert.com
    Abstract. We show how nonlinear semi-supervised embedding algorithms popular for
    use with "shallow" learning techniques such as kernel methods can be easily applied
    to deep multi-layer architectures, either as a regularizer at the output layer,
    or on each layer of the architecture.
  id: totrans-7612
  prefs: []
  type: TYPE_NORMAL
  zh: hmobahi2@illinois.edu 4 IDIAP研究所，马尔蒂尼，瑞士 ronan@collobert.com 摘要。我们展示了非线性半监督嵌入算法如何轻松应用于深层多层架构，无论是作为输出层的正则化器，还是应用于每一层架构。
- en: Compared to standard supervised backpropagation this can give significant gains.
    This trick provides a simple alternative to existing approaches to semi-supervised
    deep learning whilst yielding competitive error rates compared to those methods,
    and existing shallow semi-supervised techniques.
  id: totrans-7613
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准监督反向传播相比，这可以带来显著的收益。这个技巧为现有的半监督深度学习方法提供了简单的替代方案，同时在错误率方面与这些方法及现有的浅层半监督技术相竞争。
- en: 26.1 Introduction
  id: totrans-7614
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.1 引言
- en: In this chapter we describe a trick for improving the generalization ability
    of neural networks by utilizing unlabeled *pairs* of examples for semi-supervised
    learning. The field of semi-supervised learning [7] has the goal of improving
    generalization on supervised tasks using unlabeled data. One of the tricks they
    use is the so-called embedding of data into a lower dimensional space (or the
    related task of clustering) which are unsupervised dimensionality reduction techniques
    that have been intensively studied. For example, researchers have used nonlinear
    embedding or cluster representations as features for a supervised classifier,
    with improved results. Many of those proposed architectures are *disjoint* and
    *shallow*, by which we mean the unsupervised dimensionality reduction algorithm
    is trained on unlabeled data separately as a first step, and then its results
    are fed to a supervised classifier which has a shallow architecture such as a
    (kernelized)
  id: totrans-7615
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们描述了一种通过利用未标记*示例对*来改善神经网络泛化能力的技巧。半监督学习[7]的目标是利用未标记数据来提高监督任务的泛化能力。他们使用的技巧之一是将数据嵌入到低维空间（或相关的聚类任务），这些都是已经被深入研究的无监督降维技术。例如，研究者们使用非线性嵌入或聚类表示作为监督分类器的特征，取得了更好的结果。许多提出的架构是*不相交*且*浅层*的，这意味着无监督降维算法在未标记数据上单独训练作为第一步，然后将其结果输入具有浅层架构的监督分类器，例如（核化的）。
- en: linear model. For example, several methods learn a clustering or a distance
    measure based on a nonlinear manifold embedding as a first step [8, 9]. Transductive
  id: totrans-7616
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型。例如，几种方法首先基于非线性流形嵌入学习聚类或距离度量[8, 9]。传导
- en: '- Much of the work in this chapter was completed while Jason Weston and Ronan
    Collobert were working at NEC labs, Princeton, USA, and while Frédéric Ratle was
    affiliated with the University of Lausanne, Switzerland. See [28] and [18] for
    conference papers on this subject.'
  id: totrans-7617
  prefs: []
  type: TYPE_NORMAL
  zh: '- 本章的大部分工作是在杰森·韦斯顿和罗南·科洛贝特在美国普林斯顿的NEC实验室工作期间完成的，同时弗雷德里克·拉特尔在瑞士洛桑大学任职。有关此主题的会议论文请参见[28]和[18]。'
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    639–655, 2012.'
  id: totrans-7618
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon等（编）：NN: 实用技巧，第二版，LNCS 7700，第639–655页，2012年。'
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-7619
  prefs: []
  type: TYPE_NORMAL
  zh: -c 斯普林格-弗拉克 柏林 海德堡 2012年
- en: Support Vector Machines (TSVMs) [26] (which employs a kind of clustering) and
    LapSVM [2] (which employs a kind of embedding) are examples of methods that are
    *joint* in their use of unlabeled data and labeled data, while their architecture
    is shallow. In this work we use the same embedding trick as those researchers,
    but apply it to (deep) neural networks.
  id: totrans-7620
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（TSVMs）[26]（使用某种聚类）和LapSVM [2]（使用某种嵌入）是同时使用未标记数据和标记数据的*联合*方法示例，尽管它们的架构较为浅层。在本研究中，我们使用与那些研究者相同的嵌入技巧，但将其应用于（深度）神经网络。
- en: Deep architectures seem a natural choice in hard AI tasks which involve several
    sub-tasks which can be coded into the layers of the architecture. As argued by
    several researchers [14, 3] semi-supervised learning is also natural in such a
    setting as otherwise one is not likely to ever have enough data to perform well.
    This is both because of the dearth of label data, and because of the difficulty
    of training the architectures. Secondly, intuitively one would think that training
    on labeled and unlabeled data *jointly* should help guide the best use of the
    unlabeled data for the labeled task compared to a two-stage disjoint approach.
    (However, to our knowledge there is no systematic evidence of the latter, and
    there might be reasons to train disjointly, for example label prediction tends
    to overfit faster than the embedding because you have less data to fit them. Doing
    unsupervised pretraining first and supervised fine-tuning afterwards might naturally
    solve this problem. On the other hand, it is only because the problem is non-convex
    that a two-stage approach does anything at all - all the learning from the first
    stage may be "forgotten").
  id: totrans-7621
  prefs: []
  type: TYPE_NORMAL
  zh: 深度架构似乎是处理涉及多个子任务的困难AI任务的自然选择，这些子任务可以编码到架构的层中。如几位研究者所论述的[14, 3]，在这种情况下，半监督学习也是自然而然的，因为否则可能永远没有足够的数据来表现良好。这既是由于标签数据的匮乏，也是由于训练架构的困难。其次，直观上认为在标记和未标记数据上*联合*训练应该有助于指导未标记数据在标记任务中的最佳使用，相较于两阶段不相交的方法。（然而，据我们所知，后者并没有系统证据，并且可能有理由进行不相交的训练，例如，标签预测往往比嵌入更快过拟合，因为你拥有更少的数据去拟合。先进行无监督预训练，再进行监督微调，可能自然地解决这个问题。另一方面，正是因为问题是非凸的，两阶段方法才会起作用——第一阶段的所有学习可能会被“遗忘”。）
- en: 'Several authors have recently proposed methods for using unlabeled data in
    deep neural network-based architectures. These methods either perform a greedy
    layer-wise pre-training of weights using unlabeled data alone followed by supervised
    fine-tuning (which can be compared to the *disjoint* shallow techniques for semi-supervised
    learning described before), or learn unsupervised encodings at multiple levels
    of the architecture jointly with a supervised signal. Only considering the latter,
    the basic setup we advocate is simple:'
  id: totrans-7622
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，几位作者提出了在基于深度神经网络的架构中使用未标记数据的方法。这些方法要么仅使用未标记数据进行贪婪的逐层预训练，然后进行监督的微调（这可以与之前描述的半监督学习的*不相交*浅层技术进行比较），要么在多个层次上与监督信号共同学习无监督编码。仅考虑后者，我们倡导的基本设置很简单：
- en: 1. Choose an unsupervised learning algorithm. 2. Choose a model with a deep
    architecture.
  id: totrans-7623
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 选择一个无监督学习算法。2. 选择一个具有深度架构的模型。
- en: 3. The unsupervised learning is plugged into any (or all) layers of the architecture
    as an *auxiliary task*.
  id: totrans-7624
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 无监督学习作为*辅助任务*被嵌入到架构的任何（或所有）层中。
- en: 4. Train supervised and unsupervised tasks using the same architecture *simultaneously*
    (with a joint objective function).
  id: totrans-7625
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 同时使用相同架构训练监督和无监督任务（采用联合目标函数）。
- en: The aim is that the unsupervised method will improve accuracy on the task at
    hand.
  id: totrans-7626
  prefs: []
  type: TYPE_NORMAL
  zh: 其目的是无监督方法将提高当前任务的准确性。
- en: 'In this chapter we advocate a simple way of performing deep learning by leveraging
    existing ideas from semi-supervised algorithms developed in *shallow* architectures.
    In particular, we focus on the idea of combining an *embedding*-based regularizer
    with a supervised learner to perform semi-supervised learning, such as is used
    in Laplacian SVMs [2]. We show that this method can be: (i) generalized to multilayer
    networks and trained by stochastic gradient descent; and (ii) is valid in the
    deep learning framework given above. Experimentally, we also show that it seems
    to work quite well.We expect this is due to several effects: firstly, the extra
    embedding objective acts both as a data-dependent regularizer but secondly also
    as a weaklysupervised task that is correlated well with the supervised task of
    interest. Finally, adding this training objective at multiple layers of the network
    helps to train all the layers rather than just backpropagating from the final
    layer as in supervised learning.'
  id: totrans-7627
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们提倡通过利用*浅层*架构中开发的半监督算法的现有思想，采用一种简单的深度学习方法。特别是，我们专注于将*嵌入*基础的正则化项与监督学习者结合以执行半监督学习的思想，如在拉普拉斯SVM
    [2] 中使用的那样。我们展示了该方法可以：(i)推广到多层网络，并通过随机梯度下降进行训练；(ii) 在上述深度学习框架中有效。从实验上看，我们也表明它似乎表现相当好。我们预计这归因于几个因素：首先，额外的嵌入目标既作为依赖于数据的正则化项，也作为与关注的监督任务良好相关的弱监督任务。最后，在网络的多个层中添加这个训练目标有助于训练所有层，而不仅仅是像监督学习那样从最后一层反向传播。
- en: Although the core of this chapter focuses on a particular algorithm (embedding)
    in a joint setup, we expect the approach would also work in a disjoint setup too,
    and with other unsupervised algorithms, for example the approach of Transductive
    SVM has also been generalized to the deep learning case [15].
  id: totrans-7628
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章的核心集中在联合设置中的特定算法（嵌入），但我们预计该方法在非联合设置中也有效，并且适用于其他无监督算法，例如，传导SVM的方法也已推广到深度学习的案例
    [15]。
- en: 26.2 Semi-Supervised Embedding
  id: totrans-7629
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.2 半监督嵌入
- en: 'Our method will adapt existing semi-supervised embedding techniques for shallow
    methods to neural networks. Hence, before we describe the method, let us first
    review existing semi-supervised approaches. A key assumption in many semisupervised
    algorithms is the structure assumption1: points within the same structure (such
    as a cluster or a manifold) are likely to have the same label. Given this assumption,
    the aim is to use unlabeled data to uncover this structure. In order to do this
    many algorithms such as cluster kernels [8], LDS [9], label propagation [30] and
    LapSVM [2], to name a few, make use of regularizers that are directly related
    to unsupervised embedding algorithms. To understand these methods we will first
    review some relevant approaches to linear and nonlinear embedding.'
  id: totrans-7630
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法将现有的半监督嵌入技术适应于神经网络的浅层方法。因此，在描述该方法之前，让我们首先回顾现有的半监督方法。许多半监督算法中的一个关键假设是结构假设1：同一结构（如簇或流形）中的点可能具有相同的标签。基于这个假设，目标是利用未标记数据揭示这一结构。为了实现这一点，许多算法，如聚类核
    [8]、LDS [9]、标签传播 [30] 和LapSVM [2]等，使用与无监督嵌入算法直接相关的正则化项。为了理解这些方法，我们将首先回顾一些与线性和非线性嵌入相关的方法。
- en: 26.2.1 Embedding Algorithms
  id: totrans-7631
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.2.1 嵌入算法
- en: 'We will focus on a rather general class of embedding algorithms that can be
    described by the following type of optimization problem: given the data x1*,...,x*U'
  id: totrans-7632
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于一类相当通用的嵌入算法，这些算法可以通过以下类型的优化问题来描述：给定数据x1*,...,x*U
- en: find an embedding f(xi) of each point xi by minimizing
  id: totrans-7633
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最小化找到每个点xi的嵌入f(xi)
- en: $$\sum_{i,j=1}^{U}L(f(x_{i},\alpha),f(x_{j},\alpha),W_{i j})$$
  id: totrans-7634
  prefs: []
  type: TYPE_NORMAL
  zh: $$\sum_{i,j=1}^{U}L(f(x_{i},\alpha),f(x_{j},\alpha),W_{i j})$$
- en: w.r.t. the learning paramaters α, subject to Balancing constraint.
  id: totrans-7635
  prefs: []
  type: TYPE_NORMAL
  zh: 关于学习参数α，受平衡约束的限制。
- en: 'This type of optimization problem has the following main ingredients:'
  id: totrans-7636
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的优化问题具有以下主要成分：
- en: '- f(x) ∈ Rn is the embedding one is trying to learn for a given example x ∈
    Rd. It is parametrized by α. In many techniques f(xi) = fi is a lookup table where
    each example i is assigned an independent vector fi.'
  id: totrans-7637
  prefs: []
  type: TYPE_NORMAL
  zh: '- f(x) ∈ Rn是为给定示例x ∈ Rd尝试学习的嵌入。它由α参数化。在许多技术中，f(xi) = fi是一个查找表，每个示例i被分配一个独立的向量fi。'
- en: '- L is a loss function between pairs of examples.'
  id: totrans-7638
  prefs: []
  type: TYPE_NORMAL
  zh: '- L是示例对之间的损失函数。'
- en: '- The matrix W of weights Wij specifies the similarity or dissimilarity between
    examples xi and xj . This is supplied in advance and serves as a kind of label
    for the loss function.'
  id: totrans-7639
  prefs: []
  type: TYPE_NORMAL
  zh: '- 权重矩阵 W 的 Wij 指定了示例 xi 和 xj 之间的相似性或不相似性。这是提前提供的，作为损失函数的一种标签。'
- en: '- A balancing constraint is often required for certain objective functions
    so that a trivial solution is not reached.'
  id: totrans-7640
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对于某些目标函数，通常需要平衡约束，以避免达到平凡解。'
- en: 1 This is often referred to as the cluster assumption or the manifold assumption
    [7].
  id: totrans-7641
  prefs: []
  type: TYPE_NORMAL
  zh: 1 这通常被称为聚类假设或流形假设 [7]。
- en: As is usually the case for such machine learning setups, one can specify the
    model type (family of functions) and the loss to get different algorithmic variants.
    Many well known methods fit into this framework, we describe some pertinent ones
    below.
  id: totrans-7642
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种机器学习设置，通常可以指定模型类型（函数族）和损失，以获得不同的算法变体。许多知名方法适合这个框架，我们在下面描述一些相关的方法。
- en: Multidimensional scaling (MDS) [16] is a classical algorithm that attempts to
    preserve the distance between points, whilst embedding them in a lower dimensional
    space, e.g. by using the loss function
  id: totrans-7643
  prefs: []
  type: TYPE_NORMAL
  zh: 多维标度法 (MDS) [16] 是一种经典算法，试图保持点之间的距离，同时将它们嵌入到较低维空间，例如，使用损失函数。
- en: $$L(f_{i},f_{j},W_{i j})=(||f_{i}-f_{j}||-W_{i j})^{2}$$
  id: totrans-7644
  prefs: []
  type: TYPE_NORMAL
  zh: $$L(f_{i},f_{j},W_{i j})=(||f_{i}-f_{j}||-W_{i j})^{2}$$
- en: MDS is equivalent to PCA if the metric is Euclidean [29].
  id: totrans-7645
  prefs: []
  type: TYPE_NORMAL
  zh: 如果度量是欧几里得的，那么 MDS 等价于 PCA [29]。
- en: ISOMAP [25] is a nonlinear embedding technique that attempts to capture manifold
    structure in the original data. It works by defining a similarity metric that
    measures distances along the manifold, e.g. Wij is defined by the shortest path
    on the neighborhood graph. One then uses those distances to embed using conventional
    MDS.
  id: totrans-7646
  prefs: []
  type: TYPE_NORMAL
  zh: ISOMAP [25] 是一种非线性嵌入技术，试图捕捉原始数据中的流形结构。它通过定义一个相似性度量来工作，该度量测量流形上的距离，例如，Wij 是通过邻域图上的最短路径定义的。然后使用这些距离通过传统的
    MDS 嵌入。
- en: Laplacian Eigenmaps [1] learn manifold structure by emphasizing the preservation
    of *local distances*. One defines the distance metric between the examples by
    encoding them in the Laplacian L˜ = W − D, where Dii = j Wij is diagonal.
  id: totrans-7647
  prefs: []
  type: TYPE_NORMAL
  zh: 拉普拉斯特征映射 [1] 通过强调 *局部距离* 的保持来学习流形结构。通过在拉普拉斯 L˜ = W − D 中编码它们来定义示例之间的距离度量，其中
    Dii = j Wij 是对角的。
- en: 'Then, the following optimization is used:'
  id: totrans-7648
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用以下优化：
- en: $$\sum_{i j}L(f_{i},f_{j},W_{i j})=\sum_{i j}W_{i j}||f_{i}-f_{j}||^{2}=f^{\top}\tilde{L}f$$
  id: totrans-7649
  prefs: []
  type: TYPE_NORMAL
  zh: $$\sum_{i j}L(f_{i},f_{j},W_{i j})=\sum_{i j}W_{i j}||f_{i}-f_{j}||^{2}=f^{\top}\tilde{L}f$$
- en: 'subject to the balancing constraint:'
  id: totrans-7650
  prefs: []
  type: TYPE_NORMAL
  zh: 受平衡约束的限制：
- en: $$(26.1)$$
  id: totrans-7651
  prefs: []
  type: TYPE_NORMAL
  zh: $$(26.1)$$
- en: $$f^{\top}D f=I\ \ \mathrm{and}\ \ f^{\top}D1=0.$$
  id: totrans-7652
  prefs: []
  type: TYPE_NORMAL
  zh: $$f^{\top}D f=I\ \ \mathrm{和}\ \ f^{\top}D1=0.$$
- en: $$(26.2)$$
  id: totrans-7653
  prefs: []
  type: TYPE_NORMAL
  zh: $$(26.2)$$
- en: f Df = I and f D1=0. (26.2)
  id: totrans-7654
  prefs: []
  type: TYPE_NORMAL
  zh: f Df = I 和 f D1=0. (26.2)
- en: Siamese Networks [4] are also a classical method for nonlinear embedding. Neural
    networks researchers think of such models as a network with two identical copies
    of the same function, with the same weights, fed into a "distance measuring" layer
    to compute whether the two examples are similar or not, given labeled data. In
    fact, this is exactly the same as the formulation given at the beginning of this
    section.
  id: totrans-7655
  prefs: []
  type: TYPE_NORMAL
  zh: 孪生网络 [4] 也是一种经典的非线性嵌入方法。神经网络研究者将此类模型视为具有两个相同函数的网络，使用相同的权重，输入到一个“距离测量”层，以计算两个示例是否相似，前提是有标记数据。实际上，这与本节开头给出的公式完全相同。
- en: 'Several loss functions have been proposed for *siamese networks*, here we describe
    a margin-based loss proposed by the authors of [13]:'
  id: totrans-7656
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了几种用于 *孪生网络* 的损失函数，这里我们描述了 [13] 作者提出的基于边际的损失。
- en: $$L(f_{i},f_{j},W_{ij})=\begin{cases}||f_{i}-f_{j}||_{2}&\text{if$W_{ij}=1$,}\\
    \max(0,m-||f_{i}-f_{j}||_{2})^{2}&\text{if$W_{ij}=0$}\end{cases}\tag{26.3}$$
  id: totrans-7657
  prefs: []
  type: TYPE_NORMAL
  zh: $$L(f_{i},f_{j},W_{ij})=\begin{cases}||f_{i}-f_{j}||_{2}&\text{如果$W_{ij}=1$,}\\
    \max(0,m-||f_{i}-f_{j}||_{2})^{2}&\text{如果$W_{ij}=0$}\end{cases}\tag{26.3}$$
- en: which encourages similar examples to be close, and dissimilar ones to have a
    distance of at least m from each other. Note that no balancing constraint is needed
    with such a choice of loss as the margin constraint inhibits a trivial solution.
    Compared to using constraints like (26.2) this is much easier to optimize by gradient
    descent.
  id: totrans-7658
  prefs: []
  type: TYPE_NORMAL
  zh: 这鼓励相似的示例靠近，而不相似的示例之间的距离至少为 m。注意，这种损失的选择不需要平衡约束，因为边际约束抑制了平凡解。与使用（26.2）这样的约束相比，这种方法更容易通过梯度下降优化。
- en: 26.2.2 Semi-Supervised Algorithms
  id: totrans-7659
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.2.2 半监督算法
- en: Several *semi-supervised* classification algorithms have been proposed which
    take advantage of the algorithms described in the last section. Here we assume
    the setting where one is given M +U examples xi, but only the first M have a known
    label yi.
  id: totrans-7660
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了几种*半监督*分类算法，利用了上一节描述的算法。在这里，我们假设设置为给定 M +U 示例 xi，但只有前 M 个具有已知标签 yi。
- en: 'Label Propagation [30] adds a Laplacian Eigenmap type regularization to a nearest-neighbor
    type classifier:'
  id: totrans-7661
  prefs: []
  type: TYPE_NORMAL
  zh: 标签传播 [30] 在最近邻分类器中添加了拉普拉斯特征映射类型的正则化：
- en: $$\operatorname*{min}_{f}\sum_{i=1}^{M}||f_{i}-y_{i}||^{2}+\lambda\sum_{i,j=1}^{M+U}W_{i
    j}||f_{i}-f_{j}||^{2}$$
  id: totrans-7662
  prefs: []
  type: TYPE_NORMAL
  zh: $$\operatorname*{min}_{f}\sum_{i=1}^{M}||f_{i}-y_{i}||^{2}+\lambda\sum_{i,j=1}^{M+U}W_{i
    j}||f_{i}-f_{j}||^{2}$$
- en: $$(26.4)$$
  id: totrans-7663
  prefs: []
  type: TYPE_NORMAL
  zh: $$(26.4)$$
- en: The algorithm tries to give two examples with large weighted edge Wij the same
    label. The neighbors of neighbors tend to also get the same label as each other
    by transitivity, hence the name *label propagation*.
  id: totrans-7664
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法试图给两个具有大权重边 Wij 的示例相同的标签。邻居的邻居往往也会由于传递性而获得相同的标签，因此得名*标签传播*。
- en: 'LapSVM [2] uses the Laplacian Eigenmaps type regularizer with an SVM:'
  id: totrans-7665
  prefs: []
  type: TYPE_NORMAL
  zh: LapSVM [2] 使用拉普拉斯特征映射类型的正则化器与 SVM：
- en: $$\min_{w,b}\ ||w||^{2}+\gamma\sum_{i=1}^{M}H(y_{i}f(x_{i}))+\lambda\sum_{i,j=1}^{M+U}W_{ij}||f(x_{i})-f(x_{j})||^{2}\tag{26.5}$$
  id: totrans-7666
  prefs: []
  type: TYPE_NORMAL
  zh: $$\min_{w,b}\ ||w||^{2}+\gamma\sum_{i=1}^{M}H(y_{i}f(x_{i}))+\lambda\sum_{i,j=1}^{M+U}W_{ij}||f(x_{i})-f(x_{j})||^{2}\tag{26.5}$$
- en: where H(x) = max(0, 1 − x) is the hinge loss, and the final classifier will
    be f(x) = w · x + b.
  id: totrans-7667
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 H(x) = max(0, 1 − x) 是铰链损失，最终分类器将是 f(x) = w · x + b。
- en: Other Methods In [9] a method called *graph* is suggested which combines a modified
    version of ISOMAP with an SVM. The authors also suggest to combine modified ISOMAP
    with TSVMs rather than SVMs, and call it *Low Density* Separation (LDS).
  id: totrans-7668
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法在 [9] 中建议了一种称为*图*的方法，它结合了修改版的 ISOMAP 和 SVM。作者还建议将修改的 ISOMAP 与 TSVM 而不是 SVM
    结合，称之为*低密度*分离 (LDS)。
- en: 26.3 Semi-Supervised Embedding For Deep Learning
  id: totrans-7669
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.3 深度学习的半监督嵌入
- en: We would like to use the ideas developed in semi-supervised learning for *deep*
    learning. Deep learning consists of learning a model with several layers of nonlinear
    mapping. In this chapter we will consider multi-layer networks with N
  id: totrans-7670
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在*深度*学习中使用半监督学习中开发的思想。深度学习由几个非线性映射层的模型学习组成。在本章中，我们将考虑具有 N 层的多层网络。
- en: 'layers of hidden units that give a C-dimensional output vector:'
  id: totrans-7671
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏单元的层生成一个 C 维输出向量：
- en: $$f_{i}(x)=\sum_{j=1}^{d}w_{j}^{O,i}\ h_{j}^{N}(x)+b^{O,i},\ \ i=1,\ldots,C$$
  id: totrans-7672
  prefs: []
  type: TYPE_NORMAL
  zh: $$f_{i}(x)=\sum_{j=1}^{d}w_{j}^{O,i}\ h_{j}^{N}(x)+b^{O,i},\ \ i=1,\ldots,C$$
- en: where wO are the weights for the output layer, and typically the kth layer is
    defined as
  id: totrans-7673
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 wO 是输出层的权重，通常第 k 层定义为
- en: $$h_{i}^{k}(x)=S\,\left(\sum_{j}w_{j}^{k,i}\;h_{j}^{k-1}(x)+b^{k,i}\;\right)\!,\;k>1$$
  id: totrans-7674
  prefs: []
  type: TYPE_NORMAL
  zh: $$h_{i}^{k}(x)=S\,\left(\sum_{j}w_{j}^{k,i}\;h_{j}^{k-1}(x)+b^{k,i}\;\right)\!,\;k>1$$
- en: $$(26.6)$$
  id: totrans-7675
  prefs: []
  type: TYPE_NORMAL
  zh: $$(26.6)$$
- en: $$(26.7)$$
  id: totrans-7676
  prefs: []
  type: TYPE_NORMAL
  zh: $$(26.7)$$
- en: $$h_{i}^{1}(x)=S\,\left(\,\sum_{j}w_{j}^{1,i}\ x_{j}+b^{1,i}\ \right)$$
  id: totrans-7677
  prefs: []
  type: TYPE_NORMAL
  zh: $$h_{i}^{1}(x)=S\,\left(\,\sum_{j}w_{j}^{1,i}\ x_{j}+b^{1,i}\ \right)$$
- en: $$(26.8)$$
  id: totrans-7678
  prefs: []
  type: TYPE_NORMAL
  zh: $$(26.8)$$
- en: j xj + b1,i $(26.8)
  id: totrans-7679
  prefs: []
  type: TYPE_NORMAL
  zh: j xj + b1,i $(26.8)
- en: and S is a non-linear squashing function such as tanh. Here, we describe a standard
    *fully connected* multi-layer network but prior knowledge about a particular problem
    could lead one to other network designs. For example in sequence and image recognition
    time delay and convolutional networks (TDNNs and CNNs) [17] have been very successful.
    In those approaches one introduces layers that apply convolutions on their input
    which take into account locality information in the data, i.e. they learn features
    from image patches or windows within a sequence.
  id: totrans-7680
  prefs: []
  type: TYPE_NORMAL
  zh: S 是一个非线性压缩函数，例如 tanh。这里，我们描述一个标准的*全连接*多层网络，但对于特定问题的先验知识可能会引导出其他网络设计。例如，在序列和图像识别中，时间延迟和卷积网络（TDNN
    和 CNN）[17] 非常成功。在这些方法中，引入了在输入上应用卷积的层，这些卷积考虑了数据中的局部信息，即它们从图像补丁或序列中的窗口学习特征。
- en: 'The general method we propose for *deep learning via semi-supervised embedding*
    is to add a semi-supervised regularizer in deep architectures in one of three
    different modes, as shown in Figure 26.1:'
  id: totrans-7681
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的*通过半监督嵌入进行深度学习*的一般方法是在深度架构中以三种不同模式之一添加半监督正则化，如图 26.1 所示：
- en: '(a) Add a semi-supervised loss (regularizer) to the supervised loss on the
    entire network''s output (26.6):'
  id: totrans-7682
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 在整个网络输出上添加一个半监督损失（正则化项）到监督损失中（26.6）：
- en: $$\sum_{i=1}^{M}\ell(f(x_{i}),y_{i})+\lambda\sum_{i,j=1}^{M+U}L(f(x_{i}),f(x_{j}),W_{ij})\tag{26.9}$$
  id: totrans-7683
  prefs: []
  type: TYPE_NORMAL
  zh: $$\sum_{i=1}^{M}\ell(f(x_{i}),y_{i})+\lambda\sum_{i,j=1}^{M+U}L(f(x_{i}),f(x_{j}),W_{ij})\tag{26.9}$$
- en: This is most similar to the *shallow* techniques described before, e.g. equation
    (26.5).
  id: totrans-7684
  prefs: []
  type: TYPE_NORMAL
  zh: 这与之前描述的 *浅层* 技术最为相似，例如方程 (26.5)。
- en: '(b) Regularize the kth hidden layer (26.7) directly:'
  id: totrans-7685
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 直接对第 k 个隐藏层进行正则化 (26.7)：
- en: $$\sum_{i=1}^{M}\ell(f(x_{i}),y_{i})+\lambda\sum_{i,j=1}^{M+U}L(f^{k}(x_{i}),f^{k}(x_{j}),W_{ij})\tag{26.10}$$
  id: totrans-7686
  prefs: []
  type: TYPE_NORMAL
  zh: $$\sum_{i=1}^{M}\ell(f(x_{i}),y_{i})+\lambda\sum_{i,j=1}^{M+U}L(f^{k}(x_{i}),f^{k}(x_{j}),W_{ij})\tag{26.10}$$
- en: where f k(x)=(hk1(x)*,...,h*kHUk (x)) is the output of the network up to the
    kth hidden layer (HUk is the number of hidden units on layer k).
  id: totrans-7687
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 f k(x)=(hk1(x)*,...,h*kHUk (x)) 是网络到第 k 个隐藏层的输出（HUk 是第 k 层的隐藏单元数）。
- en: '(c) Create an auxiliary network which shares the first k layers of the original
    network but has a new final set of weights:'
  id: totrans-7688
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 创建一个辅助网络，它共享原始网络的前 k 层，但有一组新的最终权重：
- en: $$g_{i}(x)=\sum_{j}w_{j}^{AUX,i}\ h_{j}^{k}(x)+b^{AUX,i}\tag{26.11}$$
  id: totrans-7689
  prefs: []
  type: TYPE_NORMAL
  zh: $$g_{i}(x)=\sum_{j}w_{j}^{AUX,i}\ h_{j}^{k}(x)+b^{AUX,i}\tag{26.11}$$
- en: We train this network to *embed* unlabeled data simultaneously as we train the
    original network on *labeled* data.
  id: totrans-7690
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练这个网络以同时 *嵌入* 未标记数据，同时训练原始网络以处理 *标记* 数据。
- en: One can use the loss function (26.3) for embedding, and the hinge loss
  id: totrans-7691
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用损失函数 (26.3) 进行嵌入，以及铰链损失
- en: $$\ell(f(x),y)=\sum_{c=1}^{C}H(y(c)f_{c}(x)),$$
  id: totrans-7692
  prefs: []
  type: TYPE_NORMAL
  zh: $$\ell(f(x),y)=\sum_{c=1}^{C}H(y(c)f_{c}(x)),$$
- en: for labeled examples, where y(c)=1 if y = c and -1 otherwise. For neighboring
    points, this is the same regularizer as used in LapSVM and Laplacian Eigenmaps.
  id: totrans-7693
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标记示例，如果 y(c)=1 则 y = c，否则为 -1。对于邻近点，这与 LapSVM 和拉普拉斯特征映射中使用的正则化器相同。
- en: '![633_image_0.png](633_image_0.png)'
  id: totrans-7694
  prefs: []
  type: TYPE_IMG
  zh: '![633_image_0.png](633_image_0.png)'
- en: Fig. 26.1. Three modes of embedding in deep architectures
  id: totrans-7695
  prefs: []
  type: TYPE_NORMAL
  zh: 图 26.1. 深度架构中的三种嵌入模式
- en: Algorithm 26.1 Embednn
  id: totrans-7696
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法 26.1 Embednn
- en: 'Input: labeled data (xi, yi), i = 1*,...,M*, unlabeled data xi, i = M+1*,...,U*,'
  id: totrans-7697
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：标记数据 (xi, yi)，i = 1*,...,M*，未标记数据 xi，i = M+1*,...,U*，
- en: set of functions f(·), and embedding functions gk(·), see Figure 26.1 and equations
    (26.9), (26.10) and (26.11).
  id: totrans-7698
  prefs: []
  type: TYPE_NORMAL
  zh: 函数集 f(·) 和嵌入函数 gk(·)，参见图 26.1 以及方程 (26.9)、(26.10) 和 (26.11)。
- en: repeat Pick a random *labeled* example (xi, yi) Make a gradient step to optimize
    (f(xi), yi)
  id: totrans-7699
  prefs: []
  type: TYPE_NORMAL
  zh: 重复选择一个随机的 *标记* 示例 (xi, yi)，进行一次梯度步进以优化 (f(xi), yi)
- en: for each embedding function gk(·) do Pick a random pair of neighbors xi, xj
    .
  id: totrans-7700
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个嵌入函数 gk(·)，随机选择一对邻居 xi, xj。
- en: Make a gradient step for λL(gk(xi), gk(xj ), 1)
  id: totrans-7701
  prefs: []
  type: TYPE_NORMAL
  zh: 对 λL(gk(xi), gk(xj ), 1) 进行一次梯度步进。
- en: Pick a random unlabeled example xn.
  id: totrans-7702
  prefs: []
  type: TYPE_NORMAL
  zh: 随机选择一个未标记的示例 xn。
- en: Make a gradient step for λL(gk(xi), gk(xn), 0)
  id: totrans-7703
  prefs: []
  type: TYPE_NORMAL
  zh: 对 λL(gk(xi), gk(xn), 0) 进行一次梯度步进。
- en: end for until stopping criteria is met.
  id: totrans-7704
  prefs: []
  type: TYPE_NORMAL
  zh: 直到满足停止标准为止结束。
- en: For non-neighbors, where Wij = 0, this loss "pulls" points apart, thus inhibiting
    trivial solutions without requiring difficult constraints such as (26.2). To achieve
    an embedding *without* labeled data the latter is necessary or all examples would
    collapse to a single point in the embedding space. This regularizer is therefore
    preferable to using (26.1) alone. Pseudocode of the overall approach is given
    in Algorithm 26.1.
  id: totrans-7705
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非邻居，当 Wij = 0 时，该损失“拉开”点，因此抑制了平凡解，而不需要如 (26.2) 所示的困难约束。为了实现 *没有* 标记数据的嵌入，后者是必要的，否则所有示例将在嵌入空间中塌缩到一个单一的点。因此，这种正则化器比单独使用
    (26.1) 更可取。整体方法的伪代码见算法 26.1。
- en: 'Some possible tricks to take into consideration are:'
  id: totrans-7706
  prefs: []
  type: TYPE_NORMAL
  zh: 一些需要考虑的可能技巧包括：
- en: '- The hyperparameter λ: in most of our experiments we simply set this to λ
    = 1 and it worked well due to the alternating updates in Algorithm 26.1. Note
    however if you are using many embedding loss functions they will dominate the
    objective in that case.'
  id: totrans-7707
  prefs: []
  type: TYPE_NORMAL
  zh: '- 超参数 λ：在我们的大多数实验中，我们简单地将其设置为 λ = 1，并且由于算法 26.1 中的交替更新，这样做效果良好。不过需要注意的是，如果使用多个嵌入损失函数，它们会主导目标函数。'
- en: '- We note that near the end of optimization it may be advantageous to reduce
    the learning rate of the regularizer more than the learning rate for the term
    that is minimizing the training error so that the training error can be as low
    as possible on noiseless tasks (however we did not try this in our experiments).'
  id: totrans-7708
  prefs: []
  type: TYPE_NORMAL
  zh: '- 我们注意到，在优化结束时，降低正则化器的学习率可能比降低最小化训练误差的项的学习率更有利，这样可以确保在无噪声任务上训练误差尽可能低（不过我们在实验中并没有尝试这一点）。'
- en: '- If you use an internal embedding on the first layer of your network, it is
    likely that this embedding problem is harder than an internal embedding on a later
    layer, so you might not want to give them all the same learning rate or margin,
    but that complicates the hyperparameter choices. An alternative idea would be
    to use auxiliary layers on earlier layers, or even go through two auxiliary layers,
    rather than one to make the embedding task easier. Auxiliary layers are thrown
    away at test time.'
  id: totrans-7709
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果你在网络的第一层使用内部嵌入，这个嵌入问题很可能比后面的层的内部嵌入更复杂，因此你可能不希望给它们相同的学习率或边际，但这会使超参数选择变得复杂。一个替代思路是在早期层上使用辅助层，甚至通过两个辅助层，而不是一个，以简化嵌入任务。辅助层在测试时会被丢弃。'
- en: '- Embedding on the last output layer may not always be a good idea, depending
    on the type of network. For example if you are using a softmax last layer the
    2-norm type embedding loss may not be appropriate for the log probability representation
    in the last layer. In that case we suggest to do the embedding on the last-but-one
    layer instead.'
  id: totrans-7710
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在最后输出层上进行嵌入可能并不总是一个好主意，这取决于网络的类型。例如，如果你使用的是softmax最后一层，则2范数类型的嵌入损失可能不适用于最后一层的对数概率表示。在这种情况下，我们建议在倒数第二层进行嵌入。'
- en: '- Finally, although we did not try it, training in a disjoint fashion, i.e.
    doing the embedding training first, and then continuing training with a fine tuning
    step with only the labeled data, might simplify these hyperparameter choices above.'
  id: totrans-7711
  prefs: []
  type: TYPE_NORMAL
  zh: '- 最后，尽管我们没有尝试，但以不相交的方式进行训练，即先进行嵌入训练，然后继续仅用标记数据进行微调步骤，可能会简化上述的超参数选择。'
- en: 26.3.1 Labeling Unlabeled Data As Neighbors (Building The Graph)
  id: totrans-7712
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.3.1 将未标记数据标记为邻居（构建图）
- en: Training neural networks online using stochastic gradient descent is fast and
    can scale to millions of examples. A possible bottleneck with the described approach
    is computation of the matrix W, that is, computing which unlabeled examples are
    neighbors and have value Wij = 1. Embedding algorithms often use k-nearest neighbor
    for this task. Many methods for its fast computation do exist, for example hashing
    and tree-based methods.
  id: totrans-7713
  prefs: []
  type: TYPE_NORMAL
  zh: 在线使用随机梯度下降训练神经网络速度快，并且可以扩展到数百万个示例。描述的方法可能的瓶颈是矩阵W的计算，即计算哪些未标记的示例是邻居并且具有值Wij =
    1。嵌入算法通常使用k近邻来完成此任务。确实存在许多快速计算的方法，例如哈希和基于树的方法。
- en: 'However, there are also many other ways of collecting neighboring unlabeled
    data that do not involve computing k-nn. For example, if one has access to unlabeled
    sequence data the following tricks can be used:'
  id: totrans-7714
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有许多其他收集邻近未标记数据的方法，并不涉及计算k-nn。例如，如果可以访问未标记的序列数据，可以使用以下技巧：
- en: '- For image tasks one can make use of the temporal coherence of unlabeled video:
    two successive frames are very likely to contain similar content and represent
    the same concept classes. Each object in the video is also likely to be subject
    to small transformations, such as translation, rotation or deformation over neighboring
    frames. Hence, using this with semi-supervised embedding could learn classes that
    are invariant to those changes. For example, one can take images from two consecutive
    (or close) frames of video as a neighboring pair with Wij = 1. Such pairs are
    likely to have the same label, and are collected cheaply. Frames that are far
    apart are assigned Wij = 0.'
  id: totrans-7715
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对于图像任务，可以利用未标记视频的时间一致性：两个连续帧很可能包含相似的内容并代表相同的概念类别。视频中的每个对象也很可能经历小的变换，如平移、旋转或在相邻帧上的形变。因此，使用这点与半监督嵌入结合，能够学习对这些变化不变的类别。例如，可以将来自两个连续（或相近）视频帧的图像作为邻居对，Wij
    = 1。这类对很可能具有相同的标签，并且收集成本较低。相距较远的帧则分配Wij = 0。'
- en: '- For text tasks one can use documents to collect unsupervised pairs. For example,
    one could consider sentences (or paragraphs) of a document as neighbors that contain
    semantically similar information (they are probably about the same topic).'
  id: totrans-7716
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对于文本任务，可以使用文档收集无监督对。例如，可以将文档的句子（或段落）视为包含语义相似信息的邻居（它们可能关于相同的主题）。'
- en: '- Similarly, for speech tasks it might be possible to use audio streams in
    the same way.'
  id: totrans-7717
  prefs: []
  type: TYPE_NORMAL
  zh: '- 类似地，对于语音任务，也可能以相同的方式使用音频流。'
- en: 26.3.2 When Do We Expect This Approach To Work?
  id: totrans-7718
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.3.2 我们预计这种方法何时有效？
- en: One can see the described approach as an instance of multi-task learning [6]
    using unsupervised auxiliary tasks. In common with other semi-supervised learning
    approaches, and indeed other deep learning approaches, given a k-nn type approach
    to building unlabeled pairs we only expect this to work if p(x) is useful for
    the supervised task p(y|x), i.e. if the structure assumption is true. That is,
    if the decision rule lies in a region of low density with respect to the distance
    metric chosen for k-nearest neighbors. We believe many natural tasks have this
    property.
  id: totrans-7719
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将所描述的方法视为使用无监督辅助任务的多任务学习实例[6]。与其他半监督学习方法以及其他深度学习方法相似，考虑到构建未标记对的k-nn类型的方法，只有当p(x)对监督任务p(y|x)有用时，我们才期望其有效，即结构假设成立。也就是说，如果决策规则位于相对于所选择的k近邻距离度量的低密度区域内。我们相信许多自然任务具有这种特性。
- en: However, if the graph is built using sequence data as described in the previous
    section, it is then possible that the method does not rely on the low density
    assumption at all. To see this, consider uniform two-dimensional data where the
    class label is positive if it is above the y-axis, and negative if it is below.
    A nearestneighbor graph gives no information about the class label, or equivalently
    there is no margin to optimize for TSVMs. However, if sequence data (analogous
    to a video) only has data points with the same class label in consecutive frames
    then this would carry information. Further, no computational cost is associated
    with collecting video data for computing the embedding loss, in contrast to building
    neighbor graphs. Finally, note that in high dimensional spaces nearest neighbors
    might also perform poorly, e.g. in the pixel space of images.
  id: totrans-7720
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果图是使用上一节描述的序列数据构建的，那么该方法就可能完全不依赖于低密度假设。为了解释这一点，考虑均匀的二维数据，其中如果类标签在y轴之上则为正，若在之下则为负。最近邻图对类标签没有提供任何信息，或者等价地，TSVMs没有优化的边际。然而，如果序列数据（类似于视频）在连续帧中只有相同类标签的数据点，则这将携带信息。此外，与构建邻居图相比，收集视频数据以计算嵌入损失没有计算成本。最后，注意在高维空间中，最近邻也可能表现不佳，例如在图像的像素空间中。
- en: 26.3.3 Why Is This Approach Good?
  id: totrans-7721
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.3.3 为什么这种方法好？
- en: 'There are a number of reasons why the deep semi-supervised embedding trick
    might be useful compared to competing approaches:'
  id: totrans-7722
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个理由说明与竞争方法相比，深度半监督嵌入技巧可能是有用的：
- en: '- Deep embedding is very easy to optimize by gradient descent as it has a very
    simple loss function. This means it can be applied to any kind of neural network
    architecture cheaply and efficiently. As well as being generally applicable, it
    is also quite easy to implement.'
  id: totrans-7723
  prefs: []
  type: TYPE_NORMAL
  zh: '- 深度嵌入非常容易通过梯度下降进行优化，因为它具有非常简单的损失函数。这意味着它可以廉价而高效地应用于任何类型的神经网络架构。除了普遍适用外，它的实现也相对简单。'
- en: '- Compared to a reconstruction based loss function, such as used in an autoencoder,
    our approach can be much cheaper to do the gradient updates. In our approach there
    is an encoding step, but no decoding step. That is, the loss is measured in the
    usually relatively low-dimensional embedding space. For high-dimensional input
    data (even if that data is sparse) e.g. text data, the reconstruction can be very
    slow, e.g. a bag-of-words representation with a dictionary of tens of thousands
    of words. Further, in a convolutional-pooling network architecture it might be
    hard to reconstruct the original data, so again an encoder-decoder system might
    be hard to do, but our method only requires an encoder.'
  id: totrans-7724
  prefs: []
  type: TYPE_NORMAL
  zh: '- 与基于重建的损失函数（例如在自编码器中使用的）相比，我们的方法在进行梯度更新时可以节省很多成本。在我们的方法中有一个编码步骤，但没有解码步骤。也就是说，损失是在通常相对低维的嵌入空间中测量的。对于高维输入数据（即使数据稀疏），例如文本数据，重建可能非常缓慢，例如，一个包含数万单词的词典的词袋表示。此外，在卷积池化网络架构中，重建原始数据可能很困难，因此编码器-解码器系统可能很难实现，但我们的方法只需要一个编码器。'
- en: '- Our approach does not necessarily require the so called low density assumption
    which most other approaches depend upon. Many methods only work on data when that
    assumption is true (which we do not know in advance in general). Our method may
    still work, depending on how the pair-data is collected. This point was elaborated
    in the previous subsection.'
  id: totrans-7725
  prefs: []
  type: TYPE_NORMAL
  zh: '- 我们的方法并不一定需要大多数其他方法依赖的所谓低密度假设。许多方法仅在该假设成立时对数据有效（通常我们事先并不知道这一点）。我们的办法可能仍然有效，这取决于对数据对的收集方式。这个观点在上一小节中进行了详细阐述。'
- en: Table 26.1. Datasets used in our experiments. The first three are small scale
    datasets used in the same experimental setup as found in [9, 24, 10]. The following
    six datasets are large scale. The Mnist 1h, 6h, 1k, 3k and 60k variants are MNIST
    with a labeled subset of data, following the experimental setup in [10]. SRL is
    a Semantic Role Labeling task [20] with one million labeled training examples
    and 631 million unlabeled examples. COIL100 is an object detection dataset [19].
  id: totrans-7726
  prefs: []
  type: TYPE_NORMAL
  zh: 表 26.1. 我们实验中使用的数据集。前三个是小规模数据集，使用的实验设置与文献[9, 24, 10]中的相同。接下来的六个数据集是大规模的。Mnist
    1h、6h、1k、3k和60k变种是带有标签数据子集的MNIST，遵循文献[10]中的实验设置。SRL是一个语义角色标注任务[20]，有一百万个标记的训练样本和6.31亿个未标记样本。COIL100是一个目标检测数据集[19]。
- en: '| data set              | classes          | dims points labeled   |      |       |'
  id: totrans-7727
  prefs: []
  type: TYPE_TB
  zh: '| 数据集                | 类别            | 维度 点数 标签       |      |       |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-7728
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| g50c                  | 2                | 50                    | 500  |
    50    |'
  id: totrans-7729
  prefs: []
  type: TYPE_TB
  zh: '| g50c                  | 2                | 50                    | 500  |
    50    |'
- en: '| Text                  | 2                | 7511                  | 1946 |
    50    |'
  id: totrans-7730
  prefs: []
  type: TYPE_TB
  zh: '| Text                  | 2                | 7511                  | 1946 |
    50    |'
- en: '| Uspst                 | 10               | 256                   | 2007 |
    50    |'
  id: totrans-7731
  prefs: []
  type: TYPE_TB
  zh: '| Uspst                 | 10               | 256                   | 2007 |
    50    |'
- en: '| Mnist1h               | 10               | 784                   | 70k  |
    100   |'
  id: totrans-7732
  prefs: []
  type: TYPE_TB
  zh: '| Mnist1h               | 10               | 784                   | 70k  |
    100   |'
- en: '| Mnist6h               | 10               | 784                   | 70k  |
    600   |'
  id: totrans-7733
  prefs: []
  type: TYPE_TB
  zh: '| Mnist6h               | 10               | 784                   | 70k  |
    600   |'
- en: '| Mnist1k               | 10               | 784                   | 70k  |
    1000  |'
  id: totrans-7734
  prefs: []
  type: TYPE_TB
  zh: '| Mnist1k               | 10               | 784                   | 70k  |
    1000  |'
- en: '| Mnist3k               | 10               | 784                   | 70k  |
    3000  |'
  id: totrans-7735
  prefs: []
  type: TYPE_TB
  zh: '| Mnist3k               | 10               | 784                   | 70k  |
    3000  |'
- en: '| Mnist60k              | 10               | 784                   | 70k  |
    60000 |'
  id: totrans-7736
  prefs: []
  type: TYPE_TB
  zh: '| Mnist60k              | 10               | 784                   | 70k  |
    60000 |'
- en: '| SRL                   | 16               | -                     | 631M |
    1M    |'
  id: totrans-7737
  prefs: []
  type: TYPE_TB
  zh: '| SRL                   | 16               | -                     | 631M |
    1M    |'
- en: '| COIL100 (30 objects)  | 30 72x72 pixels  | 7200                  | 120  |       |'
  id: totrans-7738
  prefs: []
  type: TYPE_TB
  zh: '| COIL100 (30个对象)    | 30 72x72 像素  | 7200                  | 120  |       |'
- en: '| COIL100 (100 objects) | 100 72x72 pixels | 7200                  | 400  |       |'
  id: totrans-7739
  prefs: []
  type: TYPE_TB
  zh: '| COIL100 (100个对象) | 100 72x72 像素 | 7200                  | 400  |       |'
- en: 26.4 Experimental Evaluation
  id: totrans-7740
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.4 实验评估
- en: We test the semi-supervised embedding approach on several datasets summarized
    in Table 26.1.
  id: totrans-7741
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表26.1中总结的多个数据集上测试半监督嵌入方法。
- en: 26.4.1 Small-Scale Experiments
  id: totrans-7742
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.4.1 小规模实验
- en: 'g50c, Text and Uspst are small-scale datasets often used for semi-supervised
    learning experiments [9, 24, 10]. We followed the same experimental setup, averaging
    results of ten splits of 50 labeled examples where the rest of the data is unlabeled.
    In these experiments we test the embedding regularizer on the output of a neural
    network (see equation (26.9) and Figure 26.1(a)). We define a two-layer neural
    network (NN) with hu hidden units. We define W so that the 10 nearest neighbors
    of i have Wij = 1, and Wij = 0 otherwise. We train for 50 epochs of stochastic
    gradient descent and fixed λ = 1, but for the first 5 we optimized the supervised
    target alone (without the embedding regularizer). This gives two free hyperparameters:
    the number of hidden units hu = {0, 5, 10, 20, 30, 40, 50} and the learning rate
    lr = {0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001}.'
  id: totrans-7743
  prefs: []
  type: TYPE_NORMAL
  zh: g50c、Text和Uspst是常用于半监督学习实验的小规模数据集[9, 24, 10]。我们遵循相同的实验设置，平均十个分割中50个标记样本的结果，其余数据为未标记。在这些实验中，我们测试了嵌入正则化器在神经网络输出上的效果（见方程(26.9)和图26.1(a)）。我们定义了一个具有hu个隐藏单元的两层神经网络（NN）。我们定义W，使得i的10个最近邻有Wij
    = 1，其他为Wij = 0。我们进行了50轮随机梯度下降训练，并固定λ = 1，但前5轮仅优化监督目标（不使用嵌入正则化器）。这给出了两个自由超参数：隐藏单元数量hu
    = {0, 5, 10, 20, 30, 40, 50}和学习率lr = {0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001}。
- en: We report the optimum choices of these values optimized both by 5-fold cross
    validation and by optimizing on the test set in Table 26.2. Note the datasets
    are very small, so cross validation is unreliable. Several methods from the literature
    optimized their hyperparameters using the test set (those that are not marked
    with (cv)). Our *Embed*NN is competitive with state-of-the-art semi-supervised
    methods based on SVMs, even outperforming them in some cases.
  id: totrans-7744
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告了这些值的最佳选择，这些值通过5折交叉验证和在测试集上优化而得，如表26.2所示。注意数据集非常小，因此交叉验证不可靠。文献中的一些方法使用测试集优化它们的超参数（未标记为(cv)的那些）。我们的
    *Embed*NN 在基于SVM的最先进的半监督方法中具有竞争力，甚至在某些情况下超越了它们。
- en: Table 26.2. Results on Small-Scale Datasets. We report the best test error over
    the hyperparameters of our method, *Embed*NN, as in the methodology of [9] as
    well as the error when optimizing the parameters by cross-validation, *Embed*NN(cv).
    LDS(cv) and LapSVM(cv) also use cross-validation.
  id: totrans-7745
  prefs: []
  type: TYPE_NORMAL
  zh: 表26.2 小规模数据集的结果。我们报告了我们的方法 *Embed*NN 的最佳测试错误，以及通过交叉验证优化参数时的错误 *Embed*NN(cv)。LDS(cv)
    和 LapSVM(cv) 也使用了交叉验证。
- en: '|                         | g50c   | Text Uspst   |       |'
  id: totrans-7746
  prefs: []
  type: TYPE_TB
  zh: '|                         | g50c   | Text Uspst   |       |'
- en: '| --- | --- | --- | --- |'
  id: totrans-7747
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| SVM                     | 8.32   | 18.86        | 23.18 |'
  id: totrans-7748
  prefs: []
  type: TYPE_TB
  zh: '| SVM                     | 8.32   | 18.86        | 23.18 |'
- en: '| TSVM                    | 5.80   | 5.71         | 17.61 |'
  id: totrans-7749
  prefs: []
  type: TYPE_TB
  zh: '| TSVM                    | 5.80   | 5.71         | 17.61 |'
- en: '| LapSVM(cv)              | 5.4    | 10.4         | 12.7  |'
  id: totrans-7750
  prefs: []
  type: TYPE_TB
  zh: '| LapSVM(cv)              | 5.4    | 10.4         | 12.7  |'
- en: '| LDS(cv)                 | 5.4    | 5.1          | 15.8  |'
  id: totrans-7751
  prefs: []
  type: TYPE_TB
  zh: '| LDS(cv)                 | 5.4    | 5.1          | 15.8  |'
- en: '| Label propagation 17.30 |        | 11.71        | 21.30 |'
  id: totrans-7752
  prefs: []
  type: TYPE_TB
  zh: '| 标签传播 17.30 |        | 11.71        | 21.30 |'
- en: '| Graph SVM               | 8.32   | 10.48        | 16.92 |'
  id: totrans-7753
  prefs: []
  type: TYPE_TB
  zh: '| Graph SVM               | 8.32   | 10.48        | 16.92 |'
- en: '| NN                      | 10.62  | 15.74        | 25.13 |'
  id: totrans-7754
  prefs: []
  type: TYPE_TB
  zh: '| NN                      | 10.62  | 15.74        | 25.13 |'
- en: '| EmbedNN                 | 5.66   | 5.82         | 15.49 |'
  id: totrans-7755
  prefs: []
  type: TYPE_TB
  zh: '| EmbedNN                 | 5.66   | 5.82         | 15.49 |'
- en: '| EmbedNN(cv)             | 6.78   | 6.19         | 15.84 |'
  id: totrans-7756
  prefs: []
  type: TYPE_TB
  zh: '| EmbedNN(cv)             | 6.78   | 6.19         | 15.84 |'
- en: '| Mnst1h Mnst6h Mnst1k Mnst3k   |       |       |       |      |'
  id: totrans-7757
  prefs: []
  type: TYPE_TB
  zh: '| Mnst1h Mnst6h Mnst1k Mnst3k   |       |       |       |      |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-7758
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| SVM                           | 23.44 | 8.85  | 7.77  | 4.21 |'
  id: totrans-7759
  prefs: []
  type: TYPE_TB
  zh: '| SVM                           | 23.44 | 8.85  | 7.77  | 4.21 |'
- en: '| TSVM                          | 16.81 | 6.16  | 5.38  | 3.45 |'
  id: totrans-7760
  prefs: []
  type: TYPE_TB
  zh: '| TSVM                          | 16.81 | 6.16  | 5.38  | 3.45 |'
- en: '| RBM(∗)                        | 21.5  | -     | 8.8   | -    |'
  id: totrans-7761
  prefs: []
  type: TYPE_TB
  zh: '| RBM(∗)                        | 21.5  | -     | 8.8   | -    |'
- en: '| SESM(∗)                       | 20.6  | -     | 9.6   | -    |'
  id: totrans-7762
  prefs: []
  type: TYPE_TB
  zh: '| SESM(∗)                       | 20.6  | -     | 9.6   | -    |'
- en: '| DBN-NCA(∗)                    | -     | 10.0  | -     | 3.8  |'
  id: totrans-7763
  prefs: []
  type: TYPE_TB
  zh: '| DBN-NCA(∗)                    | -     | 10.0  | -     | 3.8  |'
- en: '| DBN-rNCA(∗) -                 | 8.7   | -     | 3.3   |      |'
  id: totrans-7764
  prefs: []
  type: TYPE_TB
  zh: '| DBN-rNCA(∗) -                 | 8.7   | -     | 3.3   |      |'
- en: '| NN                            | 25.81 | 11.44 | 10.70 | 6.04 |'
  id: totrans-7765
  prefs: []
  type: TYPE_TB
  zh: '| NN                            | 25.81 | 11.44 | 10.70 | 6.04 |'
- en: '| EmbedONN                      | 17.05 | 5.97  | 5.73  | 3.59 |'
  id: totrans-7766
  prefs: []
  type: TYPE_TB
  zh: '| EmbedONN                      | 17.05 | 5.97  | 5.73  | 3.59 |'
- en: '| EmbedI1NN                     | 16.86 | 9.44  | 8.52  | 6.02 |'
  id: totrans-7767
  prefs: []
  type: TYPE_TB
  zh: '| EmbedI1NN                     | 16.86 | 9.44  | 8.52  | 6.02 |'
- en: '| EmbedA1NN                     | 17.17 | 7.56  | 7.89  | 4.93 |'
  id: totrans-7768
  prefs: []
  type: TYPE_TB
  zh: '| EmbedA1NN                     | 17.17 | 7.56  | 7.89  | 4.93 |'
- en: '| CNN                           | 22.98 | 7.68  | 6.45  | 3.35 |'
  id: totrans-7769
  prefs: []
  type: TYPE_TB
  zh: '| CNN                           | 22.98 | 7.68  | 6.45  | 3.35 |'
- en: '| EmbedOCNN                     | 11.73 | 3.42  | 3.34  | 2.28 |'
  id: totrans-7770
  prefs: []
  type: TYPE_TB
  zh: '| EmbedOCNN                     | 11.73 | 3.42  | 3.34  | 2.28 |'
- en: '| EmbedI5CNN                    | 7.75  | 3.82  | 2.73  | 1.83 |'
  id: totrans-7771
  prefs: []
  type: TYPE_TB
  zh: '| EmbedI5CNN                    | 7.75  | 3.82  | 2.73  | 1.83 |'
- en: '| EmbedA5CNN 7.87               | 3.82  | 2.76  | 2.07  |      |'
  id: totrans-7772
  prefs: []
  type: TYPE_TB
  zh: '| EmbedA5CNN 7.87               | 3.82  | 2.76  | 2.07  |      |'
- en: '| 2          | 4    | 6    | 8    | 10   | 15   |      |'
  id: totrans-7773
  prefs: []
  type: TYPE_TB
  zh: '| 2          | 4    | 6    | 8    | 10   | 15   |      |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-7774
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| NN         | 26.0 | 26.1 | 27.2 | 28.3 | 34.2 | 47.7 |'
  id: totrans-7775
  prefs: []
  type: TYPE_TB
  zh: '| NN         | 26.0 | 26.1 | 27.2 | 28.3 | 34.2 | 47.7 |'
- en: '| EmbedONN   | 19.7 | 15.1 | 15.1 | 15.0 | 13.7 | 11.8 |'
  id: totrans-7776
  prefs: []
  type: TYPE_TB
  zh: '| EmbedONN   | 19.7 | 15.1 | 15.1 | 15.0 | 13.7 | 11.8 |'
- en: '| EmbedALLNN | 18.2 | 12.6 | 7.9  | 8.5  | 6.3  | 9.3  |'
  id: totrans-7777
  prefs: []
  type: TYPE_TB
  zh: '| EmbedALLNN | 18.2 | 12.6 | 7.9  | 8.5  | 6.3  | 9.3  |'
- en: Table 26.3. Results on MNIST with 100, 600, 1000 and 3000 labels. A two-layer
    Neural Network (NN) is compared to an NN with Embedding regularizer (*Embed*NN)
    on the output (O), i th layer (Ii) or auxiliary embedding from the i th layer
    (Ai) (see Figure 26.1). A convolutional network (CNN) is also tested in the same
    way. We compare to SVMs and TSVMs. RBM, SESM, DBN-NCA and DBN-rNCA (marked with
  id: totrans-7778
  prefs: []
  type: TYPE_NORMAL
  zh: 表26.3。使用100、600、1000和3000个标签在MNIST上的结果。将一个两层神经网络（NN）与带嵌入正则化的NN（*Embed*NN）进行比较，后者在输出（O）、第i层（Ii）或来自第i层的辅助嵌入（Ai）（见图26.1）上。卷积网络（CNN）也以相同方式进行了测试。我们与SVM和TSVM进行了比较。RBM、SESM、DBN-NCA和DBN-rNCA（标记为
- en: (∗)) taken from [21, 23] are trained on a different data split.
  id: totrans-7779
  prefs: []
  type: TYPE_NORMAL
  zh: （∗）取自[21, 23]，在不同的数据划分上训练。
- en: Table 26.4. Mnist1h dataset with deep networks of 2, 6, 8, 10 and 15 layers;
    each hidden layer has 50 hidden units. We compare classical NN training with *Embed*NN
  id: totrans-7780
  prefs: []
  type: TYPE_NORMAL
  zh: 表26.4。使用2、6、8、10和15层深度网络的Mnist1h数据集；每个隐藏层有50个隐藏单元。我们将经典的NN训练与*Embed*NN进行比较。
- en: where we either learn an embedding at the output layer (O) or an auxiliary embedding
    on all layers at the same time (ALL).
  id: totrans-7781
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们要么在输出层（O）学习一个嵌入，要么同时在所有层学习一个辅助嵌入（ALL）。
- en: '| 2            | 4   | 6   | 8   | 10   | 15   |     |'
  id: totrans-7782
  prefs: []
  type: TYPE_TB
  zh: '| 2            | 4   | 6   | 8   | 10   | 15   |     |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-7783
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| NN (HUs=50)  | 2.9 | 2.6 | 2.8 | 3.1  | 3.1  | 4.2 |'
  id: totrans-7784
  prefs: []
  type: TYPE_TB
  zh: '| NN (HUs=50)  | 2.9 | 2.6 | 2.8 | 3.1  | 3.1  | 4.2 |'
- en: '| EmbedALLNN   | 2.8 | 1.9 | 2.0 | 2.2  | 2.4  | 2.6 |'
  id: totrans-7785
  prefs: []
  type: TYPE_TB
  zh: '| EmbedALLNN   | 2.8 | 1.9 | 2.0 | 2.2  | 2.4  | 2.6 |'
- en: '| NN (HUs=100) | 2.0 | 1.9 | 2.0 | 2.2  | 2.3  | 3.0 |'
  id: totrans-7786
  prefs: []
  type: TYPE_TB
  zh: '| NN (HUs=100) | 2.0 | 1.9 | 2.0 | 2.2  | 2.3  | 3.0 |'
- en: '| EmbedALLNN   | 1.9 | 1.5 | 1.6 | 1.7  | 1.8  | 2.4 |'
  id: totrans-7787
  prefs: []
  type: TYPE_TB
  zh: '| EmbedALLNN   | 1.9 | 1.5 | 1.6 | 1.7  | 1.8  | 2.4 |'
- en: Table 26.5. Full Mnist60k dataset with deep networks of 2, 6, 8, 10 and 15 layers,
    using either 50 or 100 hidden units. We compare classical NN training with EmbedALLNN
  id: totrans-7788
  prefs: []
  type: TYPE_NORMAL
  zh: 表26.5。使用2、6、8、10和15层深度网络的完整Mnist60k数据集，使用50或100个隐藏单元。我们将经典的NN训练与EmbedALLNN进行比较。
- en: where we learn an auxiliary embedding on all layers at the same time.
  id: totrans-7789
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们同时在所有层学习一个辅助嵌入。
- en: 26.4.2 Mnist Experiments
  id: totrans-7790
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.4.2 Mnist实验
- en: 'We compare our method in all three different modes (Figure 26.1) with conventional
    semi-supervised learning (TSVM) using the same data split and validation set as
    in [10]. We also compare to several deep learning methods: RBMs (Restricted Boltzmann
    Machines), SESM (Sparse Encoding Symmetric Machine),'
  id: totrans-7791
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在三种不同模式下（图26.1）将我们的方法与传统的半监督学习（TSVM）进行了比较，使用与[10]中相同的数据划分和验证集。我们还与几种深度学习方法进行了比较：RBMs（限制玻尔兹曼机），SESM（稀疏编码对称机），
- en: DBN-NCA and DBN-rNCA (Deep Belief Nets - (regularized) Neighbourhood Components
    Analysis). (Note, however the latter were trained on a different data split).
    In these experiments we consider 2-layer networks (NN) and 6-layer convolutional
    neural nets (CNN) for embedding. We optimize the parameters of NN ( hu = {50,
    100, 150, 200, 400} hidden units and learning rates as before)
  id: totrans-7792
  prefs: []
  type: TYPE_NORMAL
  zh: DBN-NCA和DBN-rNCA（深度信念网络-（正则化）邻域组件分析）。（注意，后者是在不同的数据划分上训练的）。在这些实验中，我们考虑了2层网络（NN）和6层卷积神经网络（CNN）进行嵌入。我们优化了NN的参数（hu
    = {50, 100, 150, 200, 400}隐藏单元和之前的学习率）
- en: 'on the validation set. The CNN architecture is fixed: 5 layers of image patchtype
    convolutions, followed by a linear layer of 50 hidden units, similar to [17].'
  id: totrans-7793
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证集上。CNN架构是固定的：5层图像块类型卷积，后面跟着一个包含50个隐藏单元的线性层，类似于[17]。
- en: The results given in Table 26.3 show the effectiveness of embedding in all three
    modes, with both NNs and CNNs.
  id: totrans-7794
  prefs: []
  type: TYPE_NORMAL
  zh: 表26.3中的结果显示了在所有三种模式下嵌入的有效性，包括NN和CNN。
- en: 26.4.3 Deeper Mnist Experiments
  id: totrans-7795
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.4.3 更深的Mnist实验
- en: We then conducted a similar set of experiments but with very deep architectures
    - up to 15 layers, where each hidden layer has 50 hidden units. Using Mnist1h,
    we first compare conventional NNs to EmbedALLNN where we learn an auxiliary nonlinear
    embedding (50 hidden units and a 10 dimensional embedding space) on each layer,
    as well as *Embed*ONN where we only embed the outputs.
  id: totrans-7796
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们进行了类似的一组实验，但使用非常深的架构——最多15层，每个隐藏层有50个隐藏单元。使用Mnist1h，我们首先比较传统的神经网络与EmbedALLNN，其中我们在每一层学习一个辅助非线性嵌入（50个隐藏单元和10维嵌入空间），以及*Embed*ONN，仅嵌入输出。
- en: Results are given in Table 26.4. When we increase the number of layers, NNs
    trained with conventional backpropagation overfit and yield steadily *worse* test
    error (although they are easily capable of achieving zero training error). In
    contrast, EmbedALLNN *improves* with increasing depth due to the semi-supervised
  id: totrans-7797
  prefs: []
  type: TYPE_NORMAL
  zh: 结果见表26.4。当我们增加层数时，使用传统反向传播训练的NN会过拟合，并持续产生*更差*的测试误差（尽管它们能够轻松实现零训练误差）。相反，由于半监督的原因，EmbedALLNN在深度增加时*改进*。
- en: '"regularization". Embedding on all layers of the network has made *deep learning*
    possible. *Embed*ONN (embedding on the outputs) also helps, but not as much.'
  id: totrans-7798
  prefs: []
  type: TYPE_NORMAL
  zh: “正则化”。在网络的所有层上嵌入使得*深度学习*成为可能。*Embed*ONN（输出嵌入）也有帮助，但效果不如前者。
- en: We also conducted some experiments using the full MNIST dataset, Mnist60k.
  id: totrans-7799
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用完整的MNIST数据集Mnist60k进行了实验。
- en: Again using deep networks of up to 15 layers using either 50 or 100 hidden units
    EmbedALLNN outperforms standard NN. Results are given in Table 26.5.
  id: totrans-7800
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用多达15层的深度网络，隐藏单元为50或100的EmbedALLNN优于标准NN。结果见表26.5。
- en: Despite the lack of availability of extra unlabeled data, we still the same
    effect as in the semi-supervised case that NN overfits with increasing capacity,
    whereas EmbedNN is more robust (even if it exhibits some overfitting compared
    to the optimal depth, it is nowhere near as pronounced.) Increasing the number
    of hidden units is likely to improve these results further, e.g. using 4 layers
    and 500 hidden units on each layer, one obtains 1.27% using EmbedALLNN. Overall,
    these results show that the regularization in EmbedNNALL is useful in settings
    outside of a semi-supervised learning.
  id: totrans-7801
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管缺乏额外未标注数据的可用性，我们仍然观察到与半监督情况相同的效果，即NN随着容量的增加而过拟合，而EmbedNN则更加稳健（即使与最佳深度相比，它表现出一些过拟合，但远没有那么明显。）增加隐藏单元的数量可能进一步改善这些结果，例如在每层使用4层和500个隐藏单元时，使用EmbedALLNN可以获得1.27%。总体而言，这些结果表明EmbedNNALL中的正则化在半监督学习以外的环境中也是有用的。
- en: Table 26.6. A deep architecture for Semantic Role Labeling with no prior knowledge
    outperforms state-of-the-art systems ASSERT and SENNA that incorporate knowledge
    about parts-of-speech and parse trees. A convolutional network (CNN) is improved
    by learning an auxiliary embedding (*Embed*A1CNN) for words represented as 100dimensional
    vectors using the entire Wikipedia website as unlabeled data.
  id: totrans-7802
  prefs: []
  type: TYPE_NORMAL
  zh: 表26.6。在没有先验知识的情况下，深度架构的语义角色标注优于结合词性和解析树知识的最先进系统ASSERT和SENNA。通过学习一个辅助嵌入（*Embed*A1CNN），使用整个维基百科网站作为未标注数据，将单词表示为100维向量，卷积网络（CNN）得到了改进。
- en: '| Method                          | Test Error   |'
  id: totrans-7803
  prefs: []
  type: TYPE_TB
  zh: '| 方法                             | 测试误差   |'
- en: '| --- | --- |'
  id: totrans-7804
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ASSERT [20]                     | 16.54%       |'
  id: totrans-7805
  prefs: []
  type: TYPE_TB
  zh: '| 断言 [20]                     | 16.54%       |'
- en: '| SENNA [11]                      | 16.36%       |'
  id: totrans-7806
  prefs: []
  type: TYPE_TB
  zh: '| SENNA [11]                      | 16.36%       |'
- en: '| CNN [no prior knowledge]        | 18.40%       |'
  id: totrans-7807
  prefs: []
  type: TYPE_TB
  zh: '| CNN [无先验知识]                | 18.40%       |'
- en: '| EmbedA1CNN [no prior knowledge] | 14.55%       |'
  id: totrans-7808
  prefs: []
  type: TYPE_TB
  zh: '| EmbedA1CNN [无先验知识]        | 14.55%       |'
- en: 26.4.4 Semantic Role Labeling
  id: totrans-7809
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.4.4 语义角色标注
- en: 'The goal of semantic role labeling (SRL) is, given a sentence and a relation
    of interest, to label each word with one of 16 tags that indicate that word''s
    semantic role with respect to the action of the relation. For example the sentence
    *"The cat eats the fish in the pond"* is labeled in the following way:'
  id: totrans-7810
  prefs: []
  type: TYPE_NORMAL
  zh: 语义角色标注（SRL）的目标是，给定一个句子和一个感兴趣的关系，为每个单词标注16个标签中的一个，这些标签指示该单词在关系的动作中的语义角色。例如，句子*“猫在池塘里吃鱼”*的标注方式如下：
- en: '"TheARG0 catARG0 eatsREL theARG1 fishARG1 inARGM−LOC theARGM−LOC pondARGM−LOC
    " where ARG0 and ARG1 effectively indicate the subject and object of the relation
    "eats" and ARGM-LOC indicates a locational modifier. The PropBank dataset includes
    around 1 million labeled words from the Wall Street Journal. We follow the experimental
    setup of [11] and train a 5-layer convolutional neural network for this task,
    where the first layer represents the input sentence words as 50-dimensional vectors.
    Unlike [11], we do not give any prior knowledge to our classifier. In that work
    words were stemmed and clustered using their parts-of-speech. Our classifier is
    trained using only the original input words.'
  id: totrans-7811
  prefs: []
  type: TYPE_NORMAL
  zh: “TheARG0 catARG0 eatsREL theARG1 fishARG1 inARGM−LOC theARGM−LOC pondARGM−LOC”，其中ARG0和ARG1有效地指示了关系“吃”的主语和宾语，ARGM-LOC指示一个位置修饰符。PropBank数据集包含大约100万个来自《华尔街日报》的标注单词。我们遵循[11]的实验设置，并为此任务训练一个5层的卷积神经网络，第一层将输入句子的单词表示为50维向量。与[11]不同，我们没有向分类器提供任何先验知识。在那项工作中，单词通过词性进行词干提取和聚类。我们的分类器仅使用原始输入单词进行训练。
- en: We attempt to improve this system by, as before, learning an *auxiliary embedding*
    task. Our embedding is learnt using unlabeled sentences from the Wikipedia web
    site, consisting of 631 million words in total using the scheme described in Section
    26.3. The same lookup table of word vectors as in the supervised task is used
    as input to an 11 word window around a given word, yielding 550 features. Then
    a linear layer projects these features into a 100 dimensional embedding space.
    All windows of text from Wikipedia are considered neighbors, and nonneighbors
    are constructed by replacing the middle word in a sentence window with a random
    word. Our lookup table indexes the most frequently used 30,000 words, and all
    other words are assigned index 30,001.
  id: totrans-7812
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图通过学习一个*辅助嵌入*任务来改进该系统。我们的嵌入是使用来自维基百科网站的未标记句子学习的，总共包含6.31亿个单词，采用第26.3节中描述的方案。与监督任务中相同的词向量查找表作为输入，围绕给定单词的11个单词窗口，生成550个特征。然后，线性层将这些特征投影到100维嵌入空间。维基百科中的所有文本窗口被视为邻居，而非邻居则通过用随机单词替换句子窗口中的中间单词来构造。我们的查找表索引最常用的30,000个单词，所有其他单词被分配索引30,001。
- en: The results in Table 26.6 indicate a clear improvement when learning an auxiliary
    embedding. ASSERT [20] is an SVM parser-based system with many hand-coded features,
    and SENNA is a NN which uses part-of-speech information to build its word vectors.
    In contrast, our system is the only state-of-the-art Table 26.7. Test Accuracy
    on COIL100 in various settings. Both 30 and 100 objects were used following [27].
    The semi-supervised embedding algorithm using temporal coherence of video (*Embed*
    CNN) on the last but one layer of an 8 layer CNN, with various choices of video,
    outperforms a standard (otherwise identical) 8-layer CNN
  id: totrans-7813
  prefs: []
  type: TYPE_NORMAL
  zh: 表26.6中的结果表明，当学习辅助嵌入时，性能明显提升。ASSERT [20]是一个基于SVM解析器的系统，具有许多手动编码的特征，而SENNA是一个利用词性信息构建词向量的神经网络。相比之下，我们的系统是唯一的最先进的技术。表26.7显示在各种设置下COIL100的测试准确率。根据[27]使用了30和100个对象。使用视频的时间一致性进行半监督嵌入算法（*Embed*
    CNN），在一个8层CNN的倒数第二层中，选择各种视频，超越了一个标准的（否则相同的）8层CNN。
- en: and other baselines. (Note that with 100 objects this is a transductive approach,
    as we use the test set as unlabeled data during training, whereas with 30 objects
    a semisupervised approach is used.)
  id: totrans-7814
  prefs: []
  type: TYPE_NORMAL
  zh: 以及其他基准。（注意，使用100个对象时这是一个传递性方法，因为我们在训练期间将测试集用作未标记数据，而使用30个对象时则采用半监督方法。）
- en: '|                  | Method   | 30 objects   | 100 objects   |'
  id: totrans-7815
  prefs: []
  type: TYPE_TB
  zh: '|                  | 方法     | 30个对象    | 100个对象    |'
- en: '| --- | --- | --- | --- |'
  id: totrans-7816
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Nearest Neighbor | 81.8     | 70.1         |               |'
  id: totrans-7817
  prefs: []
  type: TYPE_TB
  zh: '| Nearest Neighbor | 81.8     | 70.1         |               |'
- en: '| SVM              | 84.9     | 74.6         |               |'
  id: totrans-7818
  prefs: []
  type: TYPE_TB
  zh: '| SVM              | 84.9     | 74.6         |               |'
- en: '| SpinGlass MRF    | 82.79    | 69.41        |               |'
  id: totrans-7819
  prefs: []
  type: TYPE_TB
  zh: '| SpinGlass MRF    | 82.79    | 69.41        |               |'
- en: '| Eigen Spline     | 84.6     | 77.0         |               |'
  id: totrans-7820
  prefs: []
  type: TYPE_TB
  zh: '| Eigen Spline     | 84.6     | 77.0         |               |'
- en: '| VTU              | 89.9     | 79.1         |               |'
  id: totrans-7821
  prefs: []
  type: TYPE_TB
  zh: '| VTU              | 89.9     | 79.1         |               |'
- en: '| Standard CNN     | 84.88    | 71.49        |               |'
  id: totrans-7822
  prefs: []
  type: TYPE_TB
  zh: '| Standard CNN     | 84.88    | 71.49        |               |'
- en: '| Embed CNN        | 95.03    | 92.25        |               |'
  id: totrans-7823
  prefs: []
  type: TYPE_TB
  zh: '| Embed CNN        | 95.03    | 92.25        |               |'
- en: method that does not use prior knowledge in the form of features derived from
    parts-of-speech or parse tree data. The use of neural network techniques for this
    application is explored in much more detail in [12], although a different semi-supervised
    technique is used in that work.
  id: totrans-7824
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法不使用来自词性或解析树数据的特征的先验知识。虽然在[12]中详细探讨了该应用的神经网络技术，但那项工作中使用的是不同的半监督技术。
- en: 26.4.5 Object Recognition Using Unlabeled Video
  id: totrans-7825
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.4.5 使用未标记视频的目标识别
- en: Finally, we detail some experiments using unlabeled video for semi-supervised
    embedding, more details of these experiments can be found in [18]. We used the
    COIL100 image dataset [19] which contains color pictures of 100 objects, each
    72x72 pixels. There are 72 different views for every object, i.e. there are 7200
    images in total. The images were obtained by placing the objects on a turntable
    and taking a shot for each 5 degree turn. Note that the rotation of the objects
    can be viewed as an unlabeled video which we can use in our semi-supervised embedding
    approach.
  id: totrans-7826
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们详细描述了一些使用未标记视频进行半监督嵌入的实验，这些实验的更多细节可以在[18]中找到。我们使用了COIL100图像数据集[19]，该数据集包含100个对象的彩色图片，每张图片为72x72像素。每个对象有72个不同的视图，即总共有7200张图像。这些图像是通过将对象放在转盘上并在每次转动5度时拍摄的。请注意，物体的旋转可以视为我们在半监督嵌入方法中使用的未标记视频。
- en: The setup of our experiments is as follows. First, we use a standard convolutional
    neural network (CNN) without utilizing any temporal information to establish a
    baseline. We used an 8-layer network consisting of three sets of convolution followed
    by subsampling layers, a final convolution layer and a fully connected layer that
    predicts the outputs.
  id: totrans-7827
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验的设置如下。首先，我们使用标准卷积神经网络（CNN），不利用任何时间信息来建立基线。我们使用了一个8层网络，由三组卷积层、下采样层、最终卷积层和一个完全连接层组成，以预测输出。
- en: For comparability with the settings available from other studies on COIL100,
    we choose two experimental setups. These are (i) when all 100 objects of COIL
    are considered in the experiment and (ii) when only 30 labeled objects out of
    100 are studied (for both training and testing). In either case, 4 out of 72 views
    (at 0, 90, 180, and 270 degrees) per object are used for training, and the rest
    of the 68 views are used for testing. The results are given in Table 26.7 compared
    to some existing methods [22, 27, 5]. Note that using 100 objects is a harder
    task than using 30 objects (classes).
  id: totrans-7828
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与其他关于COIL100研究中可用的设置进行可比性，我们选择了两种实验设置。这些是（i）在实验中考虑所有100个COIL对象，以及（ii）仅研究100个对象中的30个标记对象（用于训练和测试）。在任一情况下，每个对象使用72个视图中的4个视图（分别为0、90、180和270度）进行训练，其余68个视图用于测试。结果在表26.7中给出，与一些现有方法[22,
    27, 5]进行比较。请注意，使用100个对象的任务比使用30个对象（类别）要困难。
- en: To use the semi-supervised embedding trick on our CNN for video, we treat COIL100
    as a continuous unlabeled video sequence of rotating objects with 72 consecutive
    frames per each object (after 72 frames the continuous video switches object).
    We perform the embedding on the last but one layer of our 8 layer CNN, i.e. on
    the representation yielded by the successive layers of the network just before
    the final softmax. For the 100 object result, the test set is hence part of the
    unlabeled video (a so-called "transductive" setting). Here we obtained 92.25%
  id: totrans-7829
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们的CNN上使用半监督嵌入技巧处理视频，我们将COIL100视为旋转对象的连续未标记视频序列，每个对象有72个连续帧（在72帧后，连续视频切换对象）。我们在8层CNN的倒数第二层上执行嵌入，即在网络各层成功输出的表示上，在最终softmax之前。因此，100个对象的结果中，测试集是未标记视频的一部分（所谓的“传导”设置）。在这里我们得到了92.25%。
- en: accuracy (*Embed* CNN) which is much higher than the best alternative method
    (VTU) and the standard CNN that we trained.
  id: totrans-7830
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率（*嵌入* CNN）远高于我们训练的最佳替代方法（VTU）和标准CNN。
- en: A natural question is what happens if we do not have access to test data during
    training, i.e. the setting is a typical semi-supervised situation rather than
    a "transductive" setting. To explore this, we used 30 objects as the primary task,
    i.e. 4 views of each object in this set were used for training, and the rest for
    test. The other 70 objects only were treated as an unlabeled video sequence (again,
    images of each object were put in consecutive frames of a video sequence). Training
    with 4 views of 30 objects (labeled data) and 72 views of 70 objects (unlabeled
    video sequence) resulted in an accuracy of 95.03% on recognizing 68 views of the
    30 objects. This is about 10% above the standard CNN performance.
  id: totrans-7831
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然的问题是，如果在训练期间没有访问测试数据，会发生什么，即设置是典型的半监督情况，而不是“传导”设置。为此，我们使用30个对象作为主要任务，即使用该集合中每个对象的4个视图进行训练，其余用于测试。其他70个对象仅被视为未标记视频序列（同样，每个对象的图像被放置在视频序列的连续帧中）。使用30个对象的4个视图（标记数据）和70个对象的72个视图（未标记视频序列）进行训练，在识别30个对象的68个视图时达到了95.03%的准确率。这比标准CNN的表现高出约10%。
- en: 26.5 Conclusion
  id: totrans-7832
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.5 结论
- en: In this chapter, we showed how one can improve supervised learning for deep
    architectures if one jointly learns an embedding task using unlabeled data.
  id: totrans-7833
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了如何通过共同学习嵌入任务来改善深层架构的监督学习，使用未标记数据。
- en: 'Researchers using *shallow* architectures already showed two ways of using
    embedding to improve generalization: (i) embedding unlabeled data as a *separate*
    pre-processing step (i.e., first layer training) and; (ii) using embedding as
    a regularizer (i.e., at the output layer). It appears similar techniques can also
    be used for multi-layer neural networks as well, using the tricks described in
    this chapter.'
  id: totrans-7834
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*浅层*架构的研究人员已经展示了两种利用嵌入改善泛化的方法：（i）将未标记数据作为*单独*的预处理步骤嵌入（即，第一层训练）；（ii）将嵌入用作正则化器（即，在输出层）。似乎类似的技术也可以用于多层神经网络，利用本章描述的技巧。
- en: '[1] Belkin, M., Niyogi, P.: Laplacian eigenmaps for dimensionality reduction
    and data representation. Neural Computation 15(6), 1373–1396 (2003)'
  id: totrans-7835
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Belkin, M., Niyogi, P.: 用于维度降低和数据表示的拉普拉斯特征图。神经计算15(6), 1373–1396 (2003年)'
- en: '[2] Belkin, M., Niyogi, P., Sindhwani, V.: Manifold regularization: a geometric
    framework for learning from Labeled and Unlabeled Examples. Journal of Machine
    Learning Research 7, 2399–2434 (2006)'
  id: totrans-7836
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Belkin, M., Niyogi, P., Sindhwani, V.: 流形正则化：从标记和未标记示例中学习的几何框架。《机器学习研究杂志》7,
    2399–2434 (2006年)'
- en: '[3] Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H.: Greedy layer-wise
    training of deep networks. In: Advances in Neural Information Processing Systems,
    NIPS'
  id: totrans-7837
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H.: 深度网络的贪婪分层训练。在：神经信息处理系统进展，NIPS'
- en: 19 (2007)
  id: totrans-7838
  prefs: []
  type: TYPE_NORMAL
  zh: 19 (2007年)
- en: '[4] Bromley, J., Bentz, J.W., Bottou, L., Guyon, I., LeCun, Y., Moore, C.,
    Sackinger, E., Shah, R.: Signature verification using a siamese time delay neural
    network. International Journal of Pattern Recognition and Artificial Intelligence
    7(4) (August 1993)'
  id: totrans-7839
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Bromley, J., Bentz, J.W., Bottou, L., Guyon, I., LeCun, Y., Moore, C.,
    Sackinger, E., Shah, R.: 使用西米尔时间延迟神经网络进行签名验证。《国际模式识别与人工智能杂志》7(4) (1993年8月)'
- en: '[5] Caputo, B., Hornegger, J., Paulus, D., Niemann, H.: A spin-glass markov
    random field for 3-d object recognition. Technical Report LME-TR-2002-01, Institut
    fur Informatik, Universitat Erlangen Nurnberg (2002)'
  id: totrans-7840
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Caputo, B., Hornegger, J., Paulus, D., Niemann, H.: 用于三维物体识别的自旋玻璃马尔可夫随机场。技术报告LME-TR-2002-01，因弗兰肯大学信息学院
    (2002年)'
- en: '[6] Caruana, R.: Multitask Learning. Machine Learning 28(1), 41–75 (1997)'
  id: totrans-7841
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Caruana, R.: 多任务学习。《机器学习》28(1), 41–75 (1997年)'
- en: '[7] Chapelle, O., Schölkopf, B., Zien, A.: Semi-Supervised Learning. Adaptive
    computation and machine learning. MIT Press, Cambridge (2006) [8] Chapelle, O.,
    Weston, J., Schölkopf, B.: Cluster kernels for semi-supervised learning. In: Becker,
    S., Thrun, S., Obermayer, K. (eds.) NIPS, vol. 15, pp. 585–592.'
  id: totrans-7842
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Chapelle, O., Schölkopf, B., Zien, A.: 半监督学习。《自适应计算与机器学习》。麻省理工学院出版社，剑桥
    (2006年) [8] Chapelle, O., Weston, J., Schölkopf, B.: 半监督学习的聚类核。见：Becker, S., Thrun,
    S., Obermayer, K.（编），NIPS，第15卷，页585–592。'
- en: MIT Press, Cambridge (2003)
  id: totrans-7843
  prefs: []
  type: TYPE_NORMAL
  zh: 麻省理工学院出版社，剑桥 (2003年)
- en: '[9] Chapelle, O., Zien, A.: Semi-supervised classification by low density separation.'
  id: totrans-7844
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Chapelle, O., Zien, A.: 通过低密度分离进行半监督分类。'
- en: 'In: International Conference on Artificial Intelligence and Statistics (AISTATS),'
  id: totrans-7845
  prefs: []
  type: TYPE_NORMAL
  zh: 在：人工智能与统计国际会议 (AISTATS),
- en: pp. 57–64 (January 2005)
  id: totrans-7846
  prefs: []
  type: TYPE_NORMAL
  zh: 第57–64页 (2005年1月)
- en: '[10] Collobert, R., Sinz, F., Weston, J., Bottou, L.: Large scale transductive
    svms.'
  id: totrans-7847
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Collobert, R., Sinz, F., Weston, J., Bottou, L.: 大规模转导SVM。'
- en: Journal of Machine Learning Research 7, 1687–1712 (2006)
  id: totrans-7848
  prefs: []
  type: TYPE_NORMAL
  zh: 《机器学习研究杂志》7, 1687–1712 (2006年)
- en: '[11] Collobert, R., Weston, J.: Fast semantic extraction using a novel neural
    network architecture. In: Proceedings of the 45th Annual Meeting of the Association
    of Computational Linguistics, pp. 25–32 (2007)'
  id: totrans-7849
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Collobert, R., Weston, J.: 使用新型神经网络架构进行快速语义提取。在：计算语言学协会第45届年会论文集，第25–32页
    (2007年)'
- en: '[12] Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa,
    P.:'
  id: totrans-7850
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa,
    P.：'
- en: Natural language processing (almost) from scratch. The Journal of Machine Learning
    Research 12, 2493–2537 (2011)
  id: totrans-7851
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎从零开始的自然语言处理。《机器学习研究杂志》12, 2493–2537 (2011年)
- en: '[13] Hadsell, R., Chopra, S., LeCun, Y.: Dimensionality reduction by learning
    an invariant mapping. In: Proc. Computer Vision and Pattern Recognition Conference'
  id: totrans-7852
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Hadsell, R., Chopra, S., LeCun, Y.: 通过学习不变映射进行维度降低。在：计算机视觉与模式识别会议论文集'
- en: (CVPR 2006). IEEE Press (2006)
  id: totrans-7853
  prefs: []
  type: TYPE_NORMAL
  zh: (CVPR 2006)。IEEE出版社 (2006年)
- en: '[14] Hinton, G.E., Osindero, S., Teh, Y.: A fast learning algorithm for deep
    belief nets.'
  id: totrans-7854
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Hinton, G.E., Osindero, S., Teh, Y.: 深度信念网络的快速学习算法。'
- en: Neural Comp. 18(7), 1527–1554 (2006)
  id: totrans-7855
  prefs: []
  type: TYPE_NORMAL
  zh: 神经计算 18(7)，1527–1554 (2006)
- en: '[15] Karlen, M., Weston, J., Erkan, A., Collobert, R.: Large scale manifold
    transduction. In: Proceedings of the 25th International Conference on Machine
    Learning, pp. 448–455. ACM (2008)'
  id: totrans-7856
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Karlen, M., Weston, J., Erkan, A., Collobert, R.：大规模流形传导。收录于：第25届国际机器学习会议论文集，pp.
    448–455。ACM (2008)'
- en: '[16] Kruskal, J.B.: Multidimensional scaling by optimizing goodness of fit
    to a nonmetric hypothesis. Psychometrika 29(1), 1–27 (1964)'
  id: totrans-7857
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Kruskal, J.B.：通过优化与非度量假设的拟合优度进行多维缩放。心理计量学 29(1)，1–27 (1964)'
- en: '[17] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning
    applied to document recognition. Proceedings of the IEEE 86(11) (1998)'
  id: totrans-7858
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.：基于梯度的学习应用于文档识别。IEEE 86(11)
    会议录 (1998)'
- en: '[18] Mobahi, H., Collobert, R., Weston, J.: Deep learning from temporal coherence
    in video. In: Proceedings of the 26th Annual International Conference on Machine
    Learning, pp. 737–744. ACM (2009)'
  id: totrans-7859
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Mobahi, H., Collobert, R., Weston, J.：从视频中的时间一致性进行深度学习。收录于：第26届国际机器学习年会论文集，pp.
    737–744。ACM (2009)'
- en: '[19] Nene, S.A., Nayar, S.K., Murase, H.: Columbia object image library (coil-100).'
  id: totrans-7860
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Nene, S.A., Nayar, S.K., Murase, H.：哥伦比亚物体图像库（coil-100）。'
- en: Technical Report CUCS-006-96 (February 1996)
  id: totrans-7861
  prefs: []
  type: TYPE_NORMAL
  zh: 技术报告 CUCS-006-96 (1996年2月)
- en: '[20] Pradhan, S., Ward, W., Hacioglu, K., Martin, J., Jurafsky, D.: Shallow
    semantic parsing using support vector machines. In: Proceedings of HLT/NAACL 2004'
  id: totrans-7862
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Pradhan, S., Ward, W., Hacioglu, K., Martin, J., Jurafsky, D.：使用支持向量机的浅语义解析。收录于：HLT/NAACL
    2004 会议论文集'
- en: (2004)
  id: totrans-7863
  prefs: []
  type: TYPE_NORMAL
  zh: (2004)
- en: '[21] Ranzato, M., Huang, F., Boureau, Y., LeCun, Y.: Unsupervised learning
    of invariant feature hierarchies with applications to object recognition. In:
    Proc. Computer Vision and Pattern Recognition Conference (CVPR 2007). IEEE Press
    (2007)'
  id: totrans-7864
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Ranzato, M., Huang, F., Boureau, Y., LeCun, Y.：无监督学习不变特征层次及其在物体识别中的应用。收录于：计算机视觉与模式识别会议（CVPR
    2007）论文集。IEEE出版社 (2007)'
- en: '[22] Roobaert, D., Van Hulle, M.: View-based 3d object recognition with support
    vector machines. In: IEEE International Workshop on Neural Networks for Signal
    Processing, pp. 77–84 (1999)'
  id: totrans-7865
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Roobaert, D., Van Hulle, M.：基于视图的3D物体识别与支持向量机。收录于：IEEE国际神经网络信号处理研讨会，pp.
    77–84 (1999)'
- en: '[23] Salakhutdinov, R., Hinton, G.: Learning a Nonlinear Embedding by Preserving
    Class Neighbourhood Structure. In: International Conference on Artificial Intelligence
    and Statistics, AISTATS (2007)'
  id: totrans-7866
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Salakhutdinov, R., Hinton, G.：通过保持类邻域结构学习非线性嵌入。收录于：国际人工智能与统计会议，AISTATS
    (2007)'
- en: '[24] Sindhwani, V., Niyogi, P., Belkin, M.: Beyond the point cloud: from transductive
    to semi-supervised learning. In: International Conference on Machine Learning,
    ICML (2005)'
  id: totrans-7867
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Sindhwani, V., Niyogi, P., Belkin, M.：超越点云：从传导学习到半监督学习。收录于：国际机器学习会议，ICML
    (2005)'
- en: '[25] Tenenbaum, J.B., de Silva, V., Langford, J.C.: A global geometric framework
    for nonlinear dimensionality reduction. Science 290(5500), 2319–2323 (2000)'
  id: totrans-7868
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Tenenbaum, J.B., de Silva, V., Langford, J.C.：用于非线性降维的全球几何框架。科学 290(5500)，2319–2323
    (2000)'
- en: '[26] Vapnik, V.: Statistical Learning Theory. John Wiley and Sons, New York
    (1998) [27] Wersing, H., Körner, E.: Learning optimized features for hierarchical
    models of invariant recognition. Neural Computation 15(7), 1559–1599 (2003)'
  id: totrans-7869
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Vapnik, V.：统计学习理论。约翰·威利与儿子出版社，纽约 (1998) [27] Wersing, H., Körner, E.：为不变识别的分层模型学习优化特征。神经计算
    15(7)，1559–1599 (2003)'
- en: '[28] Weston, J., Ratle, F., Collobert, R.: Deep learning via semi-supervised
    embedding.'
  id: totrans-7870
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] Weston, J., Ratle, F., Collobert, R.：通过半监督嵌入进行深度学习。'
- en: 'In: Proceedings of the 25th International Conference on Machine Learning, pp.
    1168–1175. ACM (2008)'
  id: totrans-7871
  prefs: []
  type: TYPE_NORMAL
  zh: 收录于：第25届国际机器学习会议论文集，pp. 1168–1175。ACM (2008)
- en: '[29] Williams, C.K.I.: On a connection between kernel PCA and metric multidimensional
    scaling. In: Advances in Neural Information Processing Systems, NIPS 13'
  id: totrans-7872
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] Williams, C.K.I.：关于核PCA与度量多维缩放之间的联系。收录于：神经信息处理系统进展，NIPS 13'
- en: (2001)
  id: totrans-7873
  prefs: []
  type: TYPE_NORMAL
  zh: (2001)
- en: '[30] Zhu, X., Ghahramani, Z.: Learning from labeled and unlabeled data with
    label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon University
    (2002)'
  id: totrans-7874
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] Zhu, X., Ghahramani, Z.：通过标签传播从标记和未标记数据中学习。技术报告 CMU-CALD-02-107，卡内基梅隆大学
    (2002)'
- en: Identifying Dynamical Systems For Forecasting And Control
  id: totrans-7875
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态系统识别用于预测和控制
- en: Preface
  id: totrans-7876
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: Identifying dynamical systems from data is a promising approach to data forecasting
    and optimal control. Data forecasting is an essential component of rational decision
    making in quantitative finance, marketing and planning. Optimal control systems,
    that is, systems that can sense the environment and react appropriately, enable
    the design of cost efficient gas turbines, smart grids and human-machine interfaces.
  id: totrans-7877
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中识别动态系统是一种有前景的数据预测和最优控制方法。数据预测是定量金融、市场营销和规划中理性决策的重要组成部分。最优控制系统，即能够感知环境并做出适当反应的系统，使得设计成本高效的燃气轮机、智能电网和人机界面成为可能。
- en: A successful architecture for the task of modeling a dynamical system is the
    recurrent neural network (RNN). The state of the dynamical system is represented
    by the set of units that compose the network and the transition between two consecutive
    states is determined by the recurrent connection between these units. The network
    can be trained with backpropagation through time, that is, standard backpropagation
    on the RNN unfolded in time. Recurrent neural networks are notoriously difficult
    to train, specially, when the neural network has to model long-term dependencies.
    Indeed, the local learning signal is of little use when when local variations
    (high frequency components of the time series) do not reflect global trends (low
    frequency components).
  id: totrans-7878
  prefs: []
  type: TYPE_NORMAL
  zh: 用于建模动态系统任务的成功架构是递归神经网络（RNN）。动态系统的状态由组成网络的单元集表示，两个连续状态之间的转换由这些单元之间的递归连接决定。该网络可以通过时间反向传播进行训练，即对时间展开的RNN进行标准反向传播。递归神经网络训练
    notoriously困难，尤其是当神经网络必须建模长期依赖关系时。实际上，当局部变化（时间序列的高频成分）不反映全球趋势（低频成分）时，局部学习信号的作用微乎其微。
- en: 'This state of affairs has led many to seek alternatives to backpropagation
    through time. A radical departure from backpropagation is the *Echo State Network*
    [2]. The idea behind echo state networks is simple: (1) Create a huge neural network
    with *random* recurrent connections and (2) fit a linear model between the activations
    of the network and the time series to predict. The huge random RNN is called a
    "reservoir" and implements an overcomplete set of nonlinear primitives, only a
    subset of which, are useful in order to model the time series to predict. Tuning
    the reservoir to produce the most task-relevant primitives requires some practical
    experience. Best practices for tuning echo state networks are described in Chapter
    27 [3].'
  id: totrans-7879
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现状使许多人寻求反向传播的替代方案。与反向传播的根本性突破是*回声状态网络*。回声状态网络背后的想法很简单：（1）创建一个具有*随机*递归连接的大型神经网络，和（2）在网络激活与时间序列之间拟合线性模型进行预测。这个巨大的随机RNN被称为“水库”，实现了一组过完备的非线性原语，其中只有一部分对于建模要预测的时间序列是有用的。调整水库以产生最相关任务的原语需要一些实践经验。调整回声状态网络的最佳实践在第27章中描述。
- en: An alternative approach for overcoming the inherent difficulties of backpropagation
    is to pay particular attention to the structure of the RNN. A carefully designed
    RNN helps error derivatives to flow on larger time scales. Tricks such as overshooting,
    error correction neural networks or variant-invariant separation are introduced
    in Chapter 28 [6]. The same type of networks can be applied to the identification
    of state-action representation for control systems. Chapter 29 [1] shows how to
    apply recurrent neural networks to the identification of a full-fledged Markov
    decision process from the observation of an existing control system. This so-called
    Markov decision process extraction network (MPEN) encourages the emergence of
    a joint state-action representation that best captures the relevant information
    for the control task.
  id: totrans-7880
  prefs: []
  type: TYPE_NORMAL
  zh: 克服反向传播固有困难的另一种方法是特别关注RNN的结构。精心设计的RNN有助于误差导数在更大时间尺度上流动。第28章引入了一些技巧，如过冲、误差修正神经网络或变体不变分离。相同类型的网络可以应用于控制系统的状态-动作表示识别。第29章展示了如何将递归神经网络应用于从现有控制系统的观察中识别完整的马尔可夫决策过程。这种所谓的马尔可夫决策过程提取网络（MPEN）促进了能够最佳捕捉控制任务相关信息的联合状态-动作表示的出现。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    657–658, 2012.'
  id: totrans-7881
  prefs: []
  type: TYPE_NORMAL
  zh: 'G. Montavon等（编辑）：NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp. 657–658,
    2012。'
- en: -c Springer-Verlag Berlin Heidelberg 2012 Q-learning [5] is a popular reinforcement
    learning algorithm for control systems. It associates to each state-action pairs
    a Q-value that indicates how close to the goal the action at a given state brings
    us. Q-values are determined dynamically by the Q-learning algorithm, as a result
    of the exploration of the state-action space by the controller. The question remains
    how can Qvalues generalize in a continuous state space. Chapter 30 [4] answers
    this question and provides a practical guide to set up step-by-step neural reinforcement
    controllers.
  id: totrans-7882
  prefs: []
  type: TYPE_NORMAL
  zh: -c 施普林格-维尔拉格 柏林 海德堡 2012 Q学习 [5] 是控制系统中一种流行的强化学习算法。它将每个状态-动作对与一个Q值关联，该值指示在给定状态下的动作离目标有多近。Q值由Q学习算法动态确定，作为控制器对状态-动作空间探索的结果。问题在于，Q值如何在连续状态空间中进行泛化。第30章
    [4] 解答了这个问题，并提供了逐步设置神经强化控制器的实用指南。
- en: Grégoire & Klaus
  id: totrans-7883
  prefs: []
  type: TYPE_NORMAL
  zh: Grégoire & Klaus
- en: '[1] Duell, S., Udluft, S., Sterzing, V.: Solving Partially Observable Reinforcement
    Learning Problems with Recurrent Neural Networks. In: Montavon, G., Orr, G.B.,
    Müller, K.-R. (eds.) NN: Tricks of the Trade, 2nd edn. LNCS, vol. 7700, pp. 687–
    707. Springer, Heidelberg (2012)'
  id: totrans-7884
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Duell, S., Udluft, S., Sterzing, V.: 使用递归神经网络解决部分可观测强化学习问题。见：Montavon,
    G., Orr, G.B., Müller, K.-R.（编）《神经网络：行业技巧》，第2版。LNCS，第7700卷，第687–707页。施普林格，海德堡（2012）'
- en: '[2] Jaeger, H., Haas, H.: Harnessing Nonlinearity: Predicting Chaotic Systems
    and Saving Energy in Wireless Communication. Science 304(5667), 78–80 (2004)'
  id: totrans-7885
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Jaeger, H., Haas, H.: 利用非线性：预测混沌系统并在无线通信中节省能源。《科学》304(5667), 78–80（2004）'
- en: '[3] Lukoševičius, M.: A Practical Guide to Applying Echo State Networks. In:
    Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the Trade, 2nd edn.
    LNCS,'
  id: totrans-7886
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Lukoševičius, M.: 应用回声状态网络的实用指南。见：Montavon, G., Orr, G.B., Müller, K.-R.（编）《神经网络：行业技巧》，第2版。LNCS，'
- en: vol. 7700, pp. 659–686. Springer, Heidelberg (2012)
  id: totrans-7887
  prefs: []
  type: TYPE_NORMAL
  zh: 第7700卷，第659–686页。施普林格，海德堡（2012）
- en: '[4] Riedmiller, M.: 10 Steps and Some Tricks to Set Up Neural Reinforcement
    Controllers. In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the
    Trade, 2nd edn. LNCS, vol. 7700, pp. 735–757. Springer, Heidelberg (2012)'
  id: totrans-7888
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Riedmiller, M.: 设置神经强化控制器的10个步骤和一些技巧。见：Montavon, G., Orr, G.B., Müller,
    K.-R.（编）《神经网络：行业技巧》，第2版。LNCS，第7700卷，第735–757页。施普林格，海德堡（2012）'
- en: '[5] Watkins, C.J.C.H.: Learning from Delayed Rewards. Ph.D. thesis, Cambridge
    University (1989)'
  id: totrans-7889
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Watkins, C.J.C.H.: 从延迟奖励中学习。剑桥大学博士论文（1989）'
- en: '[6] Zimmermann, H.-G., Tietz, C., Grothmann, R.: Forecasting with Recurrent
    Neural Networks: 12 Tricks. In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.)
    NN: Tricks of the Trade, 2nd edn. LNCS, vol. 7700, pp. 687–707. Springer, Heidelberg
    (2012)'
  id: totrans-7890
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Zimmermann, H.-G., Tietz, C., Grothmann, R.: 使用递归神经网络进行预测：12个技巧。见：Montavon,
    G., Orr, G.B., Müller, K.-R.（编）《神经网络：行业技巧》，第2版。LNCS，第7700卷，第687–707页。施普林格，海德堡（2012）'
- en: 27 A Practical Guide To Applying Echo State Networks
  id: totrans-7891
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 27 应用回声状态网络的实用指南
- en: Mantas Lukoševičius Jacobs University Bremen, Campus Ring 1, 28759 Bremen, Germany
    m.lukosevicius@jacobs-university.de Abstract. Reservoir computing has emerged
    in the last decade as an alternative to gradient descent methods for training
    recurrent neural networks. Echo State Network (ESN) is one of the key reservoir
    computing "flavors". While being practical, conceptually simple, and easy to implement,
    ESNs require some experience and insight to achieve the hailed good performance
    in many tasks. Here we present practical techniques and recommendations for successfully
    applying ESNs, as well as some more advanced application-specific modifications.
  id: totrans-7892
  prefs: []
  type: TYPE_NORMAL
  zh: Mantas Lukoševičius 雅各布大学不来梅，校园环路1，28759 不来梅，德国 m.lukosevicius@jacobs-university.de
    摘要。储层计算在过去十年中作为一种替代梯度下降方法出现，用于训练递归神经网络。回声状态网络（ESN）是储层计算的一种关键“风格”。尽管实用、概念简单且易于实现，ESN需要一些经验和洞察力，以在许多任务中达到备受推崇的良好性能。在这里，我们介绍成功应用ESN的实用技术和建议，以及一些更高级的应用特定修改。
- en: 27.1 Introduction
  id: totrans-7893
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.1 引言
- en: Training Recurrent Neural Networks (RNNs) is inherently difficult. This (de-)
  id: totrans-7894
  prefs: []
  type: TYPE_NORMAL
  zh: 训练递归神经网络（RNN）本质上是困难的。这（去-）
- en: motivates many to avoid them altogether. RNNs, however, represent a very powerful
    generic tool, integrating both large dynamical memory and highly adaptable computational
    capabilities. They are the Machine Learning (ML) model most closely resembling
    biological brains, the substrate of natural intelligence.
  id: totrans-7895
  prefs: []
  type: TYPE_NORMAL
  zh: 促使许多人完全避免它们。然而，RNN代表了一种非常强大的通用工具，集成了大规模动态记忆和高度可适应的计算能力。它们是与生物大脑最相似的机器学习（ML）模型，是自然智能的基础。
- en: Error backpropagation (BP) [40] is to this date one of the most important achievements
    in artificial neural network training. It has become the standard method to train
    especially Feed-Forward Neural Networks (FFNNs). Many useful practical aspects
    of BP are discussed in other chapters of this book and in its previous edition,
    e.g., [26]. BP methods have also been extended to RNNs [51, 52], but only with
    a partial success. One of the conceptual limitations of BP methods for RNNs is
    that bifurcations can make training non-converging
  id: totrans-7896
  prefs: []
  type: TYPE_NORMAL
  zh: 错误反向传播（BP）[40] 至今仍是人工神经网络训练中最重要的成就之一。它已成为训练特别是前馈神经网络（FFNNs）的标准方法。本书及其前一版的其他章节讨论了
    BP 的许多实用方面，例如 [26]。BP 方法也已扩展到 RNNs [51, 52]，但仅取得部分成功。BP 方法在 RNNs 的一个概念限制是，分叉可能使训练不收敛。
- en: '[8]. Even when they do converge, this convergence is slow, computationally
    expensive, and can lead to poor local minima.'
  id: totrans-7897
  prefs: []
  type: TYPE_NORMAL
  zh: '[8]。即使它们收敛，这种收敛也是缓慢的，计算成本高，并可能导致较差的局部最小值。'
- en: Ten years ago an alternative trend of understanding, training, and using RNNs
    has been proposed with Echo State Networks (ESNs) [16, 21] in ML, and Liquid State
    Machines (LSMs) [32] in computational neuroscience. It was shown that RNNs often
    work well enough even without full adaptation of all network weights. In the classical
    ESN approach the RNN (called *reservoir* ) is generated randomly, and only the
    readout from the reservoir is trained. It should be noted that this basic idea
    was first clearly spelled out in a neuroscientific model of the corticostriatal
    processing loop [7]. Perhaps surprisingly this approach yielded excellent performance
    in many benchmark tasks, e.g., [16, 15, 19, 22, 47, 48].
  id: totrans-7898
  prefs: []
  type: TYPE_NORMAL
  zh: 十年前，一种理解、训练和使用 RNNs 的替代趋势被提出，使用**回声状态网络**（ESNs）[16, 21] 在机器学习中，以及**液态状态机器**（LSMs）[32]
    在计算神经科学中。研究表明，RNNs 即使在没有完全调整所有网络权重的情况下，通常也能良好工作。在经典的 ESN 方法中，RNN（称为*储备*）是随机生成的，只有从储备中输出的部分被训练。值得注意的是，这一基本思想首次在皮质-纹状体处理回路的神经科学模型中被清晰阐述
    [7]。或许令人惊讶的是，这种方法在许多基准任务中表现出色，例如 [16, 15, 19, 22, 47, 48]。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    659–686, 2012.'
  id: totrans-7899
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon 等（编）：NN：行业技巧，第2版，LNCS 7700，页659–686，2012年。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-7900
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: The trend started by ESNs and LSMs became lately known as Reservoir Computing
    (RC) [49]. RC is currently a prolific research area, giving important insights
    into RNNs, procuring practical machine learning tools, as well as enabling computation
    with non-conventional hardware [31]. RC today subsumes a number of related methods
    and extensions of the original idea [29], but the original ESN approach still
    holds its ground for its simplicity and power.
  id: totrans-7901
  prefs: []
  type: TYPE_NORMAL
  zh: 由 ESNs 和 LSMs 开始的趋势最近被称为**储备计算**（RC）[49]。RC 目前是一个蓬勃发展的研究领域，为 RNNs 提供了重要见解，提供实用的机器学习工具，并使得与非传统硬件的计算成为可能
    [31]。今天的 RC 包含了许多相关方法和对原始思想的扩展 [29]，但原始的 ESN 方法因其简单性和强大功能依然占据一席之地。
- en: The latest developments in BP for RNNs, second-order gradient descent methods
    called Hessian-free optimization, presented in [34] and discussed in a chapter
  id: totrans-7902
  prefs: []
  type: TYPE_NORMAL
  zh: 对 RNNs 的 BP 最新进展，即二阶梯度下降方法称为无海森优化，在[34]中提出，并在一章中讨论
- en: '[35] of this book, alleviate some of the mentioned shortcomings. In particular,
    they perform better on problems which require long memory [34]. These are known
    to be hard for BP RNN training [1], unless networks are specifically designed
    to deal with them [13]. Structural damping of Hessian-free optimization'
  id: totrans-7903
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的[35]部分缓解了一些提到的缺点。特别是，它们在需要长记忆的问题上表现更好 [34]。这些问题被认为对 BP RNN 训练来说是困难的 [1]，除非网络特别设计来处理它们
    [13]。无海森优化的结构阻尼
- en: '[34], an online adaptation of the learning process which penalizes big changes
    in RNN activations, likely tends to drive the learning process away from passing
    through many bifurcations (that are exactly big changes in activations and can
    probably be anticipated and avoided to some extent). On a benchmark suite designed
    to challenge long short-term memory acquisition, ESNs however still outperform
    Hessian-free trained RNNs [23].'
  id: totrans-7904
  prefs: []
  type: TYPE_NORMAL
  zh: '[34]，在线学习过程的适应惩罚 RNN 激活的大幅变化，可能会使学习过程远离通过许多分叉（即激活的重大变化，可能在某种程度上被预见和避免）。然而，在一个旨在挑战长短期记忆获取的基准套件中，ESNs
    仍然优于无海森训练的 RNNs [23]。'
- en: ESNs from their beginning proved to be a highly practical approach to RNN
  id: totrans-7905
  prefs: []
  type: TYPE_NORMAL
  zh: 从一开始，ESNs 就证明是一种高度实用的 RNN 方法
- en: training. It is conceptually simple and computationally inexpensive. It reinvigorated
    interest in RNNs, by making them accessible to wider audiences. However, the apparent
    simplicity of ESNs can sometimes be deceptive. Successfully applying ESNs needs
    some experience. There is a number of things that can be done wrong. In particular,
    the initial generation of the raw reservoir network is influenced by a handful
    of global parameters, and these have to be set judiciously. The same, however,
    can be said about virtually every ML technique. Techniques and recommendations
    on successfully applying ESNs will be addressed in this work.
  id: totrans-7906
  prefs: []
  type: TYPE_NORMAL
  zh: 训练。它在概念上简单且计算成本低。通过使RNN可被更广泛的受众所接触，重新激发了对其的兴趣。然而，ESN的表面简单性有时可能是误导性的。成功应用ESN需要一定的经验。可以做错的事情有很多。特别是，原始储层网络的初始生成受到少数全局参数的影响，而这些参数必须明智地设置。然而，几乎每种机器学习技术也可以说是如此。成功应用ESN的技术和建议将在本工作中讨论。
- en: We will try to organize the "best practices" of ESNs into a logical order despite
    the fact that they are often non-sequentially interconnected. We will start with
    defining the ESN model and the basic learning procedure in Section 27.2. Then
    we will detail out guidelines on producing good reservoirs in Section 27.3, various
    aspects of training different types of readouts in Section 27.4, and dealing with
    output feedback in Section 27.5. We will end with a short summary in Section 27.6.
  id: totrans-7907
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试将ESN的“最佳实践”整理成逻辑顺序，尽管它们通常是非线性互联的。我们将从定义ESN模型和基本学习过程开始，在第27.2节中进行介绍。然后在第27.3节详细介绍生成良好储层的指南，在第27.4节中探讨训练不同类型读取的各个方面，在第27.5节中讨论输出反馈。最后，我们将在第27.6节中做一个简短的总结。
- en: 27.2 The Basic Model
  id: totrans-7908
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.2 基本模型
- en: ESNs are applied to supervised temporal ML tasks where for a given training
    input signal u(n) ∈ RNu a desired target output signal ytarget(n) ∈ RNy is known.
    Here n = 1*,...,T* is the discrete time and T is the number of data points in
    the training dataset. In practice the dataset can consist of multiple sequences
    of varying lengths, but this does not change the principles. The task is to learn
    a model with output y(n) ∈ RNy , where y(n) matches ytarget(n) as well as possible,
    minimizing an error measure E(y, ytarget), and, more importantly, generalizes
    well to unseen data. The error measure E is typically a Mean-Square Error (MSE),
    for example Root-Mean-Square Error (RMSE)
  id: totrans-7909
  prefs: []
  type: TYPE_NORMAL
  zh: ESN应用于监督的时间机器学习任务，其中给定的训练输入信号u(n) ∈ RNu已知期望的目标输出信号ytarget(n) ∈ RNy。在这里，n = 1*,...,T*是离散时间，T是训练数据集中的数据点数量。实际上，数据集可以由多个长度不同的序列组成，但这并不会改变原则。任务是学习一个模型，使得输出y(n)
    ∈ RNy，其中y(n)尽可能匹配ytarget(n)，最小化误差度量E(y, ytarget)，更重要的是，对未见数据具有良好的泛化能力。误差度量E通常是均方误差（MSE），例如均方根误差（RMSE）。
- en: $$(27.1)$$
  id: totrans-7910
  prefs: []
  type: TYPE_NORMAL
  zh: $$(27.1)$$
- en: $$E(\mathbf{y},\mathbf{y}^{\mathrm{target}})={\frac{1}{N_{\mathrm{y}}}}\sum_{i=1}^{N_{\mathrm{y}}}{\sqrt{{\frac{1}{T}}\sum_{n=1}^{T}\left(y_{i}(n)-y_{i}^{\mathrm{target}}(n)\right)^{2}}},$$
  id: totrans-7911
  prefs: []
  type: TYPE_NORMAL
  zh: $$E(\mathbf{y},\mathbf{y}^{\mathrm{target}})={\frac{1}{N_{\mathrm{y}}}}\sum_{i=1}^{N_{\mathrm{y}}}{\sqrt{{\frac{1}{T}}\sum_{n=1}^{T}\left(y_{i}(n)-y_{i}^{\mathrm{target}}(n)\right)^{2}}},$$
- en: which is also averaged over the Ny dimensions i of the output here.
  id: totrans-7912
  prefs: []
  type: TYPE_NORMAL
  zh: 这里也对输出的Ny维度i进行了平均。
- en: The RMSE can also be dimension-wise normalized (divided) by the variance of
    the target ytarget(n), producing a Normalized Root-Mean-Square Error
  id: totrans-7913
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE也可以通过目标ytarget(n)的方差进行维度归一化（除以），产生归一化均方根误差。
- en: '(NRMSE). The NRMSE has an absolute interpretation: it does not depend on the
    arbitrary scaling of the target ytarget(n) and the value of 1 can be achieved
    with a simple constant output y(n) set to the mean value of ytarget(n). This suggests
    that a reasonable model of a stationary process should achieve the NRMSE'
  id: totrans-7914
  prefs: []
  type: TYPE_NORMAL
  zh: (NRMSE)。NRMSE具有绝对解释：它不依赖于目标ytarget(n)的任意缩放，值为1可以通过将简单常量输出y(n)设为ytarget(n)的均值来实现。这表明，一个合理的平稳过程模型应当能够达到NRMSE。
- en: accuracy between zero and one.
  id: totrans-7915
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率介于零和一之间。
- en: The normalization and the square root parts are more for human interpretability,
    as the optimal output ytarget minimizing any MSE is equivalent to the one minimizing
    (27.1), as long as no additional penalties or weighting are introduced into the
    equation, such as discussed in Sections 27.4.2 and 27.4.6.
  id: totrans-7916
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化和平方根部分更多是为了人类的可解释性，因为最小化任何均方误差（MSE）的最优输出ytarget与最小化（27.1）的输出是等效的，只要没有引入额外的惩罚或权重，如在第27.4.2节和27.4.6节中讨论的那样。
- en: ESNs use an RNN type with leaky-integrated discrete-time continuous-value units.
    The typical update equations are
  id: totrans-7917
  prefs: []
  type: TYPE_NORMAL
  zh: ESNs 使用带有泄漏积分的离散时间连续值单元的 RNN 类型。典型的更新方程是
- en: $${\tilde{\mathbf{x}}}(n)=\operatorname{tanh}\left(\mathbf{W}^{\mathrm{in}}[1;\mathbf{u}(n)]+\mathbf{W}\mathbf{x}(n-1)\right),$$
  id: totrans-7918
  prefs: []
  type: TYPE_NORMAL
  zh: $${\tilde{\mathbf{x}}}(n)=\operatorname{tanh}\left(\mathbf{W}^{\mathrm{in}}[1;\mathbf{u}(n)]+\mathbf{W}\mathbf{x}(n-1)\right),$$
- en: $$(27.2)$$
  id: totrans-7919
  prefs: []
  type: TYPE_NORMAL
  zh: $$(27.2)$$
- en: $$\mathbf{x}(n)=(1-\alpha)\mathbf{x}(n-1)+\alpha{\tilde{\mathbf{x}}}(n),$$
  id: totrans-7920
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbf{x}(n)=(1-\alpha)\mathbf{x}(n-1)+\alpha{\tilde{\mathbf{x}}}(n),$$
- en: $$(27.3)$$
  id: totrans-7921
  prefs: []
  type: TYPE_NORMAL
  zh: $$(27.3)$$
- en: where x(n) ∈ RNx is a vector of reservoir neuron activations and x˜(n) ∈ RNx
    is its update, all at time step n, tanh(·) is applied element-wise, [·; ·] stands
    for a vertical vector (or matrix) concatenation, Win ∈ RNx×(1+Nu) and W ∈ RNx×Nx
    are the input and recurrent weight matrices respectively, and α ∈ (0, 1]
  id: totrans-7922
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 x(n) ∈ RNx 是储层神经元激活的向量，x˜(n) ∈ RNx 是其更新，均在时间步 n 处，tanh(·) 按元素应用，[·; ·] 表示垂直向量（或矩阵）连接，Win
    ∈ RNx×(1+Nu) 和 W ∈ RNx×Nx 是输入和递归权重矩阵，α ∈ (0, 1]
- en: is the leaking rate. Other sigmoid wrappers can be used besides the tanh, which
    however is the most common choice. The model is also sometimes used without the
    leaky integration, which is a special case of α = 1 and thus x˜(n) ≡ x(n).
  id: totrans-7923
  prefs: []
  type: TYPE_NORMAL
  zh: 是泄漏率。除了 tanh 之外，还可以使用其他 sigmoid 包装器，然而 tanh 是最常见的选择。该模型有时也可以在没有泄漏积分的情况下使用，这是一种特殊情况，其中
    α = 1，因此 x˜(n) ≡ x(n)。
- en: Fig. 27.1. An echo state network
  id: totrans-7924
  prefs: []
  type: TYPE_NORMAL
  zh: 图 27.1. 一个回声状态网络
- en: '![648_image_0.png](648_image_0.png)'
  id: totrans-7925
  prefs: []
  type: TYPE_IMG
  zh: '![648_image_0.png](648_image_0.png)'
- en: The linear readout layer is defined as
  id: totrans-7926
  prefs: []
  type: TYPE_NORMAL
  zh: 线性读取层定义为
- en: $$\mathbf{y}(n)=\mathbf{W}^{\mathrm{out}}[1;\mathbf{u}(n);\mathbf{x}(n)],$$
  id: totrans-7927
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbf{y}(n)=\mathbf{W}^{\mathrm{out}}[1;\mathbf{u}(n);\mathbf{x}(n)],$$
- en: $$(27.4)$$
  id: totrans-7928
  prefs: []
  type: TYPE_NORMAL
  zh: $$(27.4)$$
- en: y(n) = Wout[1; u(n); x(n)], (27.4)
  id: totrans-7929
  prefs: []
  type: TYPE_NORMAL
  zh: y(n) = Wout[1; u(n); x(n)], (27.4)
- en: where y(n) ∈ RNy is network output, Wout ∈ RNy×(1+Nu+Nx) the output weight matrix,
    and [·; ·; ·] again stands for a vertical vector (or matrix) concatenation.
  id: totrans-7930
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 y(n) ∈ RNy 是网络输出，Wout ∈ RNy×(1+Nu+Nx) 是输出权重矩阵，[·; ·; ·] 再次表示垂直向量（或矩阵）连接。
- en: An additional nonlinearity can be applied to y(n) in (27.4), as well as feedback
    connections Wfb from y(n − 1) to x˜(n) in (27.2). A graphical representation of
    an ESN illustrating our notation and the idea for training is depicted in Figure
    27.1.
  id: totrans-7931
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的非线性可以应用于 (27.4) 中的 y(n)，以及反馈连接 Wfb 从 y(n − 1) 到 x˜(n) 在 (27.2) 中。图 27.1 展示了我们的符号和训练思路的
    ESN 图示。
- en: 'The original method of RC introduced with ESNs [16] was to:'
  id: totrans-7932
  prefs: []
  type: TYPE_NORMAL
  zh: 与 ESNs 一起引入的 RC 原始方法 [16] 是：
- en: 1. generate a large random reservoir RNN (Win,W, α);
  id: totrans-7933
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 生成一个大型随机储层 RNN (Win, W, α)；
- en: 2. run it using the training input u(n) and collect the corresponding reservoir
    activation states x(n);
  id: totrans-7934
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 使用训练输入 u(n) 运行它，并收集相应的储层激活状态 x(n)；
- en: 3. compute the linear readout weights Wout from the reservoir using linear regression,
    minimizing the MSE between y(n) and ytarget(n);
  id: totrans-7935
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 使用线性回归从储层计算线性读取权重 Wout，最小化 y(n) 与 ytarget(n) 之间的均方误差；
- en: 4. use the trained network on new input data u(n) computing y(n) by employing
    the trained output weights Wout.
  id: totrans-7936
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 在新输入数据 u(n) 上使用训练好的网络，通过使用训练好的输出权重 Wout 计算 y(n)。
- en: In subsequent sections we will delve deeper into the hidden intricacies of this
    procedure which appears so simple on the surface, and spell out practical hints
    for the concrete design choices that wait on the way. More specifically, Step
    1 is elaborated on in Section 27.3; Step 2 is done by Equations (27.2) and (27.3),
  id: totrans-7937
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续部分，我们将深入探讨这一过程的隐藏复杂性，尽管表面上看起来如此简单，并详细说明具体设计选择中需要注意的实用提示。更具体地说，步骤 1 在第 27.3
    节中详细阐述；步骤 2 由方程 (27.2) 和 (27.3) 完成，
- en: with initialization discussed in Section 27.4.5; Step 3 is formally defined
    and options explained in Section 27.4 with additional options for some particular
    applications in Section 27.5; and Step 3 is again performed by Equations (27.2),
    (27.3), and (27.4).
  id: totrans-7938
  prefs: []
  type: TYPE_NORMAL
  zh: 其初始化在第 27.4.5 节中讨论；步骤 3 在第 27.4 节中正式定义，并解释选项，针对某些特定应用在第 27.5 节中提供额外选项；步骤 3 再次由方程
    (27.2)、(27.3) 和 (27.4) 执行。
- en: 27.3 Producing A Reservoir
  id: totrans-7939
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.3 生成一个储层
- en: For producing a good reservoir it is important to understand what function it
    is serving.
  id: totrans-7940
  prefs: []
  type: TYPE_NORMAL
  zh: 生成良好储层的重要性在于理解它所服务的功能。
- en: 27.3.1 Function Of The Reservoir
  id: totrans-7941
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.3.1 储层的功能
- en: In practice it is important to keep in mind that the reservoir acts (i) as a
    nonlinear expansion and (ii) as a memory of input u(n) at the same time.
  id: totrans-7942
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，重要的是要记住，储层同时充当 (i) 非线性扩展和 (ii) 输入 u(n) 的记忆。
- en: There is a parallel between RC and kernel methods in ML. The reservoir can be
    seen as (i) a nonlinear high-dimensional expansion x(n) of the input signal u(n).
    For classification tasks, input data u(n) which are not linearly separable in
    the original space RNu , often become so in the expanded space RNx of x(n), where
    they are separated by Wout. In fact, employing the "kernel trick" to integrate
    over all possible reservoirs is also possible in the context of RC, even though
    not really practical [12].
  id: totrans-7943
  prefs: []
  type: TYPE_NORMAL
  zh: RC和机器学习中的核方法之间存在类比。水库可以被视为输入信号u(n)的非线性高维扩展x(n)。对于分类任务，原始空间RNu中不可线性分离的输入数据u(n)，在扩展空间RNx的x(n)中通常可以被分离，其中它们被Wout分开。实际上，在RC的背景下，也可以使用“核技巧”来整合所有可能的水库，尽管这并不实际[12]。
- en: At the same time, (ii) the reservoir serves as a memory, providing temporal
    context. This is a crucial reason for using RNNs in the first place. In the tasks
    where memory is not necessary, non-temporal ML techniques implementing functional
    mappings from current input to current output should be used.
  id: totrans-7944
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，（ii）水库作为一种记忆，提供时间上下文。这是使用RNN的一个关键原因。在不需要记忆的任务中，应使用实现从当前输入到当前输出的功能映射的非时间机器学习技术。
- en: Both aspects (i) and (ii) combined, the reservoir, being an input-driven dynamical
    system, should provide a rich and relevant enough signal space in x(n),
  id: totrans-7945
  prefs: []
  type: TYPE_NORMAL
  zh: 综合考虑（i）和（ii），水库作为一个输入驱动的动态系统，应提供足够丰富和相关的信号空间x(n)，
- en: such that the desired ytarget(n) could be obtained by linear combination from
    it.
  id: totrans-7946
  prefs: []
  type: TYPE_NORMAL
  zh: 以便所需的ytarget(n)可以通过线性组合从中获得。
- en: There is however some trade-off between (i) and (ii) when setting the parameters
    of the reservoir [50], which we will explain in more detail.
  id: totrans-7947
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在设置水库的参数时（i）和（ii）之间存在某种权衡[50]，我们将更详细地解释这一点。
- en: 27.3.2 Global Parameters Of The Reservoir
  id: totrans-7948
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.3.2 水库的全局参数
- en: Given the RNN model (27.2),(27.3), the reservoir is defined by the tuple
  id: totrans-7949
  prefs: []
  type: TYPE_NORMAL
  zh: 根据RNN模型（27.2），（27.3），水库由元组定义。
- en: (Win,W, α). The input and recurrent connection matrices Win and W are generated
    randomly according to some parameters discussed later and the leaking rate α is
    selected as a free parameter itself.
  id: totrans-7950
  prefs: []
  type: TYPE_NORMAL
  zh: （Win,W, α）。输入和递归连接矩阵Win和W是根据稍后讨论的一些参数随机生成的，泄漏率α被选为自由参数。
- en: In analogy to other ML, and especially NN, approaches, what we call "parameters"
    here could as well be called "meta-parameters" or "hyper-parameters", as they
    are not concrete connection weights but parameters governing their distributions.
    We will call them "global parameters" to better reflect their nature, or simply
    "parameters" for brevity.
  id: totrans-7951
  prefs: []
  type: TYPE_NORMAL
  zh: 类比其他机器学习，特别是神经网络的方法，我们在这里称之为“参数”的东西也可以称为“元参数”或“超参数”，因为它们不是具体的连接权重，而是支配其分布的参数。为了更好地反映其本质，我们称之为“全局参数”，或为简洁起见，简单称为“参数”。
- en: 'The defining global parameters of the reservoir are: the size Nx, sparsity,
    distribution of nonzero elements, and spectral radius of W; scaling(-s) of Win;
    and the leaking rate α. We will now proceed in this order to give more details
    on each of these design choices and intuitions on how to make them. Then, in Section
    27.3.3, we will summarize by advising how to prioritize these global parameters
    and tune the really important, or rather task-specific, ones in a principled way.
    Size of the Reservoir. One obviously crucial parameter of the model'
  id: totrans-7952
  prefs: []
  type: TYPE_NORMAL
  zh: 水库的定义全局参数包括：大小Nx、稀疏度、非零元素的分布和W的谱半径；Win的缩放（-s）；以及泄漏率α。我们将按此顺序提供关于每个设计选择的更多细节，并提供如何进行选择的直觉。然后，在第27.3.3节中，我们将总结如何优先考虑这些全局参数，并以原则性的方式调整真正重要的，或者说特定任务的参数。水库的大小是模型中显然至关重要的参数。
- en: (27.2)(27.3) is Nx, the number of units in the reservoir.
  id: totrans-7953
  prefs: []
  type: TYPE_NORMAL
  zh: （27.2）（27.3）是Nx，即水库中单元的数量。
- en: The general wisdom is that the bigger the reservoir, the better the obtainable
    performance, *provided appropriate regularization measures are taken against*
    overfitting (see Section 27.4.1). Since training and running an ESN is computationally
    cheap compared to other RNN approaches, reservoir sizes of order 104 are not uncommon
    [47]. The bigger the space of reservoir signals x(n), the easier it is to find
    a linear combination of the signals to approximate ytarget(n). In our experience
    the reservoir can be too big only when the task is trivial and there is not enough
    data available T < 1 + Nu + Nx.
  id: totrans-7954
  prefs: []
  type: TYPE_NORMAL
  zh: 一般的经验法则是，储层越大，获得的性能越好，*前提是采取适当的正则化措施以防止* 过拟合（参见第 27.4.1 节）。由于训练和运行 ESN 相较于其他
    RNN 方法计算开销较小，大小约为 10^4 的储层并不少见 [47]。储层信号 x(n) 的空间越大，找到信号的线性组合以近似 ytarget(n) 就越容易。根据我们的经验，储层过大通常仅在任务简单且可用数据不足
    T < 1 + Nu + Nx 时出现。
- en: For challenging tasks use as big a reservoir as you can afford computationally.
  id: totrans-7955
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有挑战性的任务，尽可能使用你能承受的最大的储层。
- en: That being said, computational trade-offs are important. In academic settings,
    when comparing different approaches instead of going for the best possible performance,
    authors often limit their reservoir sizes for convenience and compatibility of
    results. Even when going for the best performance, starting with the biggest possible
    reservoir from the beginning is cumbersome.
  id: totrans-7956
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，计算上的权衡非常重要。在学术环境中，作者在比较不同方法时，通常为了结果的便利性和兼容性，而不是追求最佳性能，限制储层大小。即使是在追求最佳性能的情况下，从一开始就选择最大的储层也会显得笨重。
- en: Select global parameters with smaller reservoirs, then scale to bigger ones.
  id: totrans-7957
  prefs: []
  type: TYPE_NORMAL
  zh: 选择较小的储层作为全局参数，然后扩展到更大的储层。
- en: The tuning of global parameters (described below) often needs multiple trials,
    thus each should not consume too much time. Good parameters are usually transferable
    to bigger reservoirs, but some trials with big reservoirs can also be done to
    confirm this.
  id: totrans-7958
  prefs: []
  type: TYPE_NORMAL
  zh: 全球参数的调整（如下所述）通常需要多次试验，因此每次试验不应消耗太多时间。好的参数通常可以迁移到更大的储层，但也可以对大储层进行一些试验以确认这一点。
- en: A lower bound for the reservoir size Nx can roughly be estimated by considering
    the number of independent real values that the reservoir must remember from the
    input to successfully accomplish the task. The maximal number of stored values,
    called memory capacity, in ESN can not exceed Nx [17].
  id: totrans-7959
  prefs: []
  type: TYPE_NORMAL
  zh: 储层大小 Nx 的下限可以通过考虑储层必须从输入中记住的独立实值的数量来粗略估计，以成功完成任务。在 ESN 中，最大存储值的数量（称为记忆容量）不能超过
    Nx [17]。
- en: Nx should be at least equal to the estimate of independent real values the reservoir
    has to remember from the input to solve its task.
  id: totrans-7960
  prefs: []
  type: TYPE_NORMAL
  zh: Nx 至少应等于储层必须记住的独立实值的估计，以解决其任务。
- en: For i.i.d. inputs u(n), this estimate is Nu times a rough estimate of how many
    time steps the inputs should be remembered to solve the task. While the result
    in [17] is precise for i.i.d. inputs, in practice there are often temporal and
    inter-channel correlations in u(n), that make it somewhat "compressible". Also,
    the shapes of the "forgetting curves" of the reservoirs are typically not rectangular
    (depend on other parameters), i.e., the forgetting is not instantaneous but gradual.
    As a result, reservoir can often make do with smaller sizes.
  id: totrans-7961
  prefs: []
  type: TYPE_NORMAL
  zh: 对于独立同分布的输入 u(n)，这个估计是 Nu 乘以输入应记住的时间步数的粗略估计，以解决任务。尽管 [17] 中的结果对于独立同分布的输入是精确的，但在实践中，u(n)
    中常常存在时间和通道间的相关性，使其在某种程度上“可压缩”。此外，储层的“遗忘曲线”通常不是矩形的（取决于其他参数），即遗忘不是瞬时的，而是逐渐的。因此，储层通常可以使用更小的尺寸。
- en: Sparsity of the Reservoir. In the original ESN publications it is recommended
    to make the reservoir connections sparse, i.e., make most of elements in Win equal
    to zero. In our practical experience also sparse connections tend to give a slightly
    better performance. In general, sparsity of the reservoir does not affect the
    performance much and this parameter has a low priority to be optimized. However,
    sparsity enables fast reservoir updates if sparse matrix representations are used.
  id: totrans-7962
  prefs: []
  type: TYPE_NORMAL
  zh: 储层的稀疏性。在原始的 ESN 论文中，建议使储层连接稀疏，即使大多数 Win 中的元素等于零。在我们的实践经验中，稀疏连接通常会带来稍微更好的性能。一般而言，储层的稀疏性对性能的影响不大，这个参数的优化优先级较低。然而，如果使用稀疏矩阵表示，稀疏性可以实现快速的储层更新。
- en: Connect each reservoir node to a small fixed number of other nodes (e.g., 10)
    on average, irrespective of the reservoir size. Exploit this reservoir sparsity
    to speedup computation.
  id: totrans-7963
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个储存节点连接到平均数量较少的其他节点（例如，10个），而不考虑储存池的大小。利用这种储存稀疏性加速计算。
- en: If regardless of reservoir size, a fixed fanout number is chosen, the computational
    cost of network state updates grows only linearly with the network size instead
    of quadratically. This greatly reduces the cost of running big reservoirs. The
    computational savings require virtually no additional effort when the programming
    environment supports efficient representation and operations with sparse matrices,
    which many do. Distribution of Nonzero Elements. The matrix W is typically generated
    sparse, with nonzero elements having an either a symmetrical uniform, discrete
    bi-valued, or normal distribution centered around zero. Different authors prefer
    different distributions. We usually prefer a uniform distribution for its continuity
    of values and boundedness. Gaussian distributions are also popular. Both distributions
    give virtually the same performance which depends on the other parameters discussed
    here. The discrete bi-valued distribution tends to give a slightly less rich signal
    space (there is a non-zero probability of identical neurons), but might make analysis
    of what is happening in the reservoir easier. The width of the distributions does
    not matter, as it is reset in a way explained in the next section.
  id: totrans-7964
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不考虑储存池的大小，选择固定的扇出数量，网络状态更新的计算成本仅与网络大小线性增长，而不是以平方增长。这大大降低了运行大储存池的成本。当编程环境支持稀疏矩阵的高效表示和操作时，几乎不需要额外的努力来实现计算节省，许多环境都是如此。非零元素的分布。矩阵
    W 通常是稀疏生成的，非零元素具有对称均匀、离散双值或以零为中心的正态分布。不同的作者偏好不同的分布。我们通常更倾向于均匀分布，因为它的值连续且有界。高斯分布也很流行。这两种分布几乎表现相同，具体取决于这里讨论的其他参数。离散双值分布倾向于给出稍微不那么丰富的信号空间（存在相同神经元的非零概率），但可能使对储存池中发生的情况的分析更容易。分布的宽度并不重要，因为它在下一节中将以某种方式重置。
- en: The input matrix Win is usually generated according to the same type of distribution
    as W, but typically dense. Spectral Radius. One of the most central global parameters
    of an ESN is spectral radius of the reservoir connection matrix W, i.e., the maximal
    absolute eigenvalue of this matrix. It scales the matrix W, or viewed alternatively,
    scales the width of the distribution of its nonzero elements.
  id: totrans-7965
  prefs: []
  type: TYPE_NORMAL
  zh: 输入矩阵 Win 通常是根据与 W 相同类型的分布生成的，但通常是稠密的。谱半径。ESN 的一个核心全局参数是储存连接矩阵 W 的谱半径，即该矩阵的最大绝对特征值。它缩放矩阵
    W，或换句话说，缩放其非零元素的分布宽度。
- en: Typically a random sparse W is generated; its spectral radius ρ(W) is computed;
    then W is divided by ρ(W) to yield a matrix with a unit spectral radius; this
    is then conveniently scaled with the ultimate spectral radius to be determined
    in a tuning procedure.
  id: totrans-7966
  prefs: []
  type: TYPE_NORMAL
  zh: 通常会生成一个随机稀疏的 W；计算其谱半径 ρ(W)；然后将 W 除以 ρ(W)，以得到具有单位谱半径的矩阵；接着方便地用最终谱半径进行缩放，以便在调优过程中确定。
- en: 'For the ESN approach to work, the reservoir should satisfy the so-called echo
    state property: the state of the reservoir x(n) should be uniquely defined by
    the fading history of the input u(n) [16]. In other words, for a long enough input
    u(n), the reservoir state x(n) should not depend on the initial conditions that
    were before the input.'
  id: totrans-7967
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使 ESN 方法有效，储存池应满足所谓的回声状态特性：储存池的状态 x(n) 应由输入 u(n) 的衰减历史唯一定义[16]。换句话说，对于足够长的输入
    u(n)，储存池状态 x(n) 不应依赖于输入之前的初始条件。
- en: Large ρ(W) values can lead to reservoirs hosting multiple fixed point, periodic,
    or even chaotic (when sufficient nonlinearity in the reservoir is reached)
  id: totrans-7968
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的 ρ(W) 值可能导致储存池中存在多个固定点、周期性，甚至混沌（当储存池中达到足够的非线性时）。
- en: spontaneous attractor modes, violating the echo state property.
  id: totrans-7969
  prefs: []
  type: TYPE_NORMAL
  zh: 自发吸引子模式，违反回声状态特性。
- en: ρ(W) < 1 ensures echo state property in most situations.
  id: totrans-7970
  prefs: []
  type: TYPE_NORMAL
  zh: ρ(W) < 1 确保在大多数情况下具有回声状态特性。
- en: Even though it is possible to violate the echo state property even with ρ(W)
    < 1, this is unlikely to happen in practice. More importantly, the echo state
    property often holds for ρ(W) ≥ 1 for nonzero inputs u(n). This can be explained
    by the strong u(n) pushing activations of the neurons away from 0 where their
    tanh() nonlinearities have a unitary slope to regions where this slope is smaller,
    thus reducing the gains of the neurons and the effective strength of feedback
    connections. Intuitively speaking, due to activation-squashing nonlinearities,
    strong inputs "squeeze out" the autonomous activity from the reservoir activations.
    This means, that for nonzero u(n) ρ(W) < 1 is not a necessary condition for the
    echo state property and optimal ρ(W) values can sometimes be significantly greater
    than 1.
  id: totrans-7971
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管即使ρ(W) < 1也可能违反回声状态属性，但在实践中这种情况不太可能发生。更重要的是，对于非零输入u(n)，回声状态属性通常在ρ(W) ≥ 1时成立。这可以通过强u(n)将神经元的激活推离0的区域来解释，在该区域，tanh()非线性具有单位斜率，推向斜率较小的区域，从而减少神经元的增益和反馈连接的有效强度。直观而言，由于激活压缩的非线性，强输入“挤出”水库激活中的自主活动。这意味着，对于非零u(n)，ρ(W)
    < 1并不是回声状态属性的必要条件，最佳的ρ(W)值有时可以显著大于1。
- en: In practice ρ(W) should be selected to maximize the performance, with the value
    1 serving as an initial reference point.
  id: totrans-7972
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，应选择ρ(W)以最大化性能，初始参考点为值1。
- en: As a guiding principle, ρ(W) should be set greater for tasks where a more extensive
    history of the input is required to perform it, and smaller for tasks where the
    current output y(n) depends more on the recent history of u(n). The spectral radius
    determines how fast the influence of an input dies out in a reservoir with time,
    and how stable the reservoir activations are [50].
  id: totrans-7973
  prefs: []
  type: TYPE_NORMAL
  zh: 作为指导原则，对于需要更广泛输入历史的任务，ρ(W)应设置得更大；对于当前输出y(n)更依赖于最近历史的任务，ρ(W)应设置得更小。谱半径决定了输入在水库中随时间消失的速度，以及水库激活的稳定性[50]。
- en: The spectral radius should be greater in tasks requiring longer memory of the
    input.
  id: totrans-7974
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要较长输入记忆的任务中，谱半径应更大。
- en: Input Scaling. The scaling of the input weight matrix Win is another key parameter
    to optimize in an ESN. For uniformly distributed Win we usually define the input
    scaling a as the range of the interval [−a; a] from which values of Win are sampled;
    for normal distributed input weights one may take the standard deviation as a
    scaling measure.
  id: totrans-7975
  prefs: []
  type: TYPE_NORMAL
  zh: 输入缩放。输入权重矩阵Win的缩放是ESN中另一个关键的优化参数。对于均匀分布的Win，我们通常将输入缩放a定义为从区间[−a; a]中采样的Win值的范围；对于正态分布的输入权重，可以采用标准差作为缩放度量。
- en: In order to have a small number of freely adjustable parameters, often all the
    columns of Win are scaled together using a single scaling value. However, the
    scaling of the first column of Win corresponding to the bias input to the reservoir
    units in (27.2) can be optimized separately from the rest. If the remaining "active"
    input channels contribute to the task in very different ways, it is also advised
    to optimize their scalings separately.
  id: totrans-7976
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少自由可调参数的数量，通常会将Win的所有列使用一个单一的缩放值一起缩放。然而，Win的第一列对应于（27.2）中水库单元的偏置输入，可以单独优化。如果其余“活跃”输入通道对任务的贡献差异很大，也建议单独优化它们的缩放。
- en: '![653_image_0.png](653_image_0.png)'
  id: totrans-7977
  prefs: []
  type: TYPE_IMG
  zh: '![653_image_0.png](653_image_0.png)'
- en: This varies the number of free global parameters to set for Win from 1 up to
    Nu + 1.
  id: totrans-7978
  prefs: []
  type: TYPE_NORMAL
  zh: 这将自由全局参数的数量从1到Nu + 1进行变化，以设置Win。
- en: It was suggested in the original ESN publications to scale and shift the input
    data, optimizing the magnitude of both. But the same effect can be achieved by
    scaling the input weights of the active inputs and the bias, respectively.
  id: totrans-7979
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始ESN出版物中建议缩放和偏移输入数据，优化两者的幅度。但相同的效果也可以通过分别缩放活跃输入的输入权重和偏置来实现。
- en: Still, input data normalization is advisable for ESNs just as for any other
    ML approach. This puts each learning task into a more standardized setting. It
    may be helpful to have the range of the input data values bounded.
  id: totrans-7980
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，输入数据的归一化对ESN来说仍然是可取的，就像对任何其他机器学习方法一样。这使每个学习任务进入一个更标准化的环境。限制输入数据值的范围可能会有所帮助。
- en: For example, apply the tanh(·) squashing to u(n) if its distribution is unbounded.
  id: totrans-7981
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果u(n)的分布是无界的，则对u(n)应用tanh(·)压缩。
- en: Otherwise the outliers can throw the reservoir state x(n) into some "unfamiliar"
    regions not well covered by the usual working trajectories of x(n) for which the
    global parameters have been optimized or the outputs learned. This can lead to
    a virtual loss of useful memory (due to saturations in the activation nonlinearities)
    or unpredictable outputs at these points, respectively.
  id: totrans-7982
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，异常值可能会将水库状态x(n)抛入一些“陌生”的区域，这些区域不被x(n)的常规工作轨迹良好覆盖，而这些轨迹的全局参数已被优化或输出已被学习。这可能导致有用记忆的虚拟损失（由于激活非线性的饱和）或在这些点的不可预测输出。
- en: It is advisable to normalize the data and may help to keep the inputs u(n) bounded
    avoiding outliers (e.g.,
  id: totrans-7983
  prefs: []
  type: TYPE_NORMAL
  zh: 建议对数据进行归一化，这有助于保持输入u(n)的边界，避免异常值（例如，
- en: apply tanh(·) to u(n) if it is unbounded).
  id: totrans-7984
  prefs: []
  type: TYPE_NORMAL
  zh: 如果u(n)是无界的，应用tanh(·)）。
- en: Input scaling determines how nonlinear the reservoir responses are. For very
    linear tasks Win should be small, letting units operate around the 0 point where
    their activation tanh(·) is virtually linear. For large Win, the units will get
    easily saturated close to their 1 and −1 values, acting in a more nonlinear, binary
    switching manner. While ρ(W) also affects the nonlinearity, the reservoir activations
    become unstable when increasing ρ(W), as explained in Section 27.3.2, before it
    can make the reservoir highly nonlinear.
  id: totrans-7985
  prefs: []
  type: TYPE_NORMAL
  zh: 输入缩放决定了水库响应的非线性程度。对于非常线性的任务，Win应较小，使单元在激活tanh(·)几乎线性的0点附近工作。对于较大的Win，单元容易在其1和−1值附近饱和，以更非线性、二元开关的方式运作。虽然ρ(W)也会影响非线性，但随着ρ(W)的增加，水库激活变得不稳定，如第27.3.2节所述，直到能够使水库高度非线性。
- en: The amount of nonlinearity the task requires is not easy to judge. Finding a
    proper setting benefits from experience and intuitive insight into nonlinear dynamics.
    But also the masters of RC (if there are such) use trial and error to tune this
    characteristic.
  id: totrans-7986
  prefs: []
  type: TYPE_NORMAL
  zh: 任务所需的非线性程度不易判断。找到适当的设置依赖于经验和对非线性动态的直观洞察。不过，RC的高手（如果有的话）也会通过反复试验来调整这一特性。
- en: Looking at (27.2), it is clear that the scaling of Win, together with the scaling
    of W (i.e., ρ(W)) determines the proportion of how much the current state x(n)
  id: totrans-7987
  prefs: []
  type: TYPE_NORMAL
  zh: 从(27.2)可以看出，Win的缩放与W的缩放（即ρ(W)）共同决定了当前状态x(n)
- en: depends on the current input u(n) and how much on the previous state x(n−1),
  id: totrans-7988
  prefs: []
  type: TYPE_NORMAL
  zh: 在多大程度上依赖于当前输入u(n)，以及在多大程度上依赖于先前状态x(n−1)。
- en: respectively. The respective sizes Nu and Nx should also be taken into account.
  id: totrans-7989
  prefs: []
  type: TYPE_NORMAL
  zh: 相应地，Nu和Nx的大小也应考虑在内。
- en: '![654_image_0.png](654_image_0.png)'
  id: totrans-7990
  prefs: []
  type: TYPE_IMG
  zh: '![654_image_0.png](654_image_0.png)'
- en: '![654_image_1.png](654_image_1.png)'
  id: totrans-7991
  prefs: []
  type: TYPE_IMG
  zh: '![654_image_1.png](654_image_1.png)'
- en: '![654_image_2.png](654_image_2.png)'
  id: totrans-7992
  prefs: []
  type: TYPE_IMG
  zh: '![654_image_2.png](654_image_2.png)'
- en: '![654_image_3.png](654_image_3.png)'
  id: totrans-7993
  prefs: []
  type: TYPE_IMG
  zh: '![654_image_3.png](654_image_3.png)'
- en: It has been empirically observed that the representation of the different principle
    components of u(n) in x(n) is roughly proportional to the square root of their
    magnitudes in u(n) [11]. In other words, the reservoir tends to flatten the spectrum
    of principal components of u(n) in x(n) - something to keep in mind when choosing
    the right representation or preprocessing of the data. For example, if smaller
    principal components carry no useful information it might be helpful to remove
    them from the data by Principal Component Analysis (PCA) before feeding them to
    a reservoir, otherwise they will get relatively amplified there.
  id: totrans-7994
  prefs: []
  type: TYPE_NORMAL
  zh: 经经验观察，u(n)的不同主成分在x(n)中的表示大致与它们在u(n)中大小的平方根成正比[11]。换句话说，水库倾向于在x(n)中压平u(n)的主成分谱——在选择正确的表示或数据预处理时应牢记这一点。例如，如果较小的主成分没有有用信息，可能有助于通过主成分分析（PCA）在将其输入水库之前将其从数据中去除，否则它们将在那里相对放大。
- en: Leaking Rate. The leaking rate α of the reservoir nodes in (27.3) can be regarded
    as the speed of the reservoir update dynamics discretized in time. We can describe
    the reservoir update dynamics in continuous time as an Ordinary Differential Equation
    (ODE)
  id: totrans-7995
  prefs: []
  type: TYPE_NORMAL
  zh: 漏泄率。水库节点在(27.3)中的漏泄率α可以视为水库更新动态在时间上的离散化速度。我们可以用常微分方程（ODE）来描述水库更新动态在连续时间上的表现。
- en: $${\dot{\mathbf{x}}}=-\mathbf{x}+\operatorname{tanh}\left(\mathbf{W}^{\mathrm{in}}[1;\mathbf{u}]+\mathbf{W}\mathbf{x}\right).$$
  id: totrans-7996
  prefs: []
  type: TYPE_NORMAL
  zh: $${\dot{\mathbf{x}}}=-\mathbf{x}+\operatorname{tanh}\left(\mathbf{W}^{\mathrm{in}}[1;\mathbf{u}]+\mathbf{W}\mathbf{x}\right).$$
- en: If we make an Euler's discretization of this ODE (27.5) in time, taking
  id: totrans-7997
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对该常微分方程(27.5)进行时间上的欧拉离散化，得到
- en: $$(27.5)$$
  id: totrans-7998
  prefs: []
  type: TYPE_NORMAL
  zh: $$(27.5)$$
- en: $${\frac{\Delta\mathbf{x}}{\Delta t}}={\frac{\mathbf{x}(n+1)-\mathbf{x}(n)}{\Delta
    t}}\approx{\dot{\mathbf{x}}},$$
  id: totrans-7999
  prefs: []
  type: TYPE_NORMAL
  zh: $${\frac{\Delta\mathbf{x}}{\Delta t}}={\frac{\mathbf{x}(n+1)-\mathbf{x}(n)}{\Delta
    t}}\approx{\dot{\mathbf{x}}},$$
- en: $$(27.6)$$
  id: totrans-8000
  prefs: []
  type: TYPE_NORMAL
  zh: $$(27.6)$$
- en: we arrive at exactly (up to some time indexing conventions) the discrete time
    equations (27.2)(27.3) with α taking the place of the sampling interval Δt. Thus
    α can be regarded as the time interval in the continuous world between two consecutive
    time steps in the discrete realization. Also, empirically the effect of setting
    α is comparable to that of re-sampling u(n) and ytarget(n) when the signals are
    slow [27, 41]. The leaking rate α can even be adapted online to deal with time
    wrapping of the signals [27, 22]. Equivalently, α can be introduced as a time
    constant in (27.5), if keeping Δt ≡ 1.
  id: totrans-8001
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到了离散时间方程（27.2）（27.3），其中α代替了采样间隔Δt，因此α可以视为离散实现中两个连续时间步骤之间的时间间隔。在经验上，设置α的效果与对信号u(n)和ytarget(n)进行重采样的效果相当，尤其是在信号较慢时[27,
    41]。泄漏率α甚至可以在线调整，以处理信号的时间包裹问题[27, 22]。等效地，如果保持Δt ≡ 1，α可以作为（27.5）中的时间常数引入。
- en: While there are some slight variations alternative to those in (27.3), of how
    to do leaky integration (e.g., [22]), the version (27.3) has emerged as preferred,
    because it guarantees that x(n) never goes outside the (−1, 1) interval.
  id: totrans-8002
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有一些轻微的变体替代（27.3）中关于泄漏积分的实现方式（例如，[22]），但（27.3）版本已成为首选，因为它保证x(n)不会超出（−1, 1）区间。
- en: 7.3) to match the set () and/or $\mathbf{y}^{\mathrm{target}}(n)$.
  id: totrans-8003
  prefs: []
  type: TYPE_NORMAL
  zh: 7.3）以匹配设置的()和/或$\mathbf{y}^{\mathrm{target}}(n)$。
- en: Set the leaking rate α in (27.3) to match the speed of the dynamics of u(n)
    and/or ytarget(n).
  id: totrans-8004
  prefs: []
  type: TYPE_NORMAL
  zh: 在（27.3）中设置泄漏率α，以匹配u(n)和/或ytarget(n)的动态速度。
- en: This can, again, be difficult and subjective to determine in some cases. Especially
    when the timescales of u(n) and ytarget(n) are quite different. This is one more
    of the global parameters to be tuned by trial and error.
  id: totrans-8005
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，确定这一点可能困难且主观，尤其是当u(n)和ytarget(n)的时间尺度差异较大时。这是需要通过试错调整的另一个全局参数。
- en: When the task requires modeling the time series producing dynamical system on
    multiple time scales, it might be useful to set different leaking rates to different
    units (making α a vector α ∈ RNx ) [43], with a possible downside of having more
    parameters to optimize.
  id: totrans-8006
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务需要在多个时间尺度上对产生动态系统的时间序列进行建模时，可能需要为不同的单元设置不同的泄漏率（使α成为一个向量α ∈ RNx）[43]，这可能导致需要优化更多的参数。
- en: Alternatively, the leaky integration (27.3) can be seen as a simple digital
    low-pass filter, also known as exponential smoothing, applied to every node. Some
    contributions even suggest applying more powerful filters for this purpose [53,
    14].
  id: totrans-8007
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，泄漏积分（27.3）可以看作是一个简单的数字低通滤波器，也称为指数平滑，应用于每个节点。有些研究建议为此目的应用更强大的滤波器[53, 14]。
- en: In some cases setting a small α, and thus inducing slow dynamics of x(n), can
    dramatically increase the duration of the short-term memory in ESN [23].
  id: totrans-8008
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，设置小的α，从而诱导x(n)的慢动态，可以显著增加ESN中的短期记忆持续时间[23]。
- en: 27.3.3 Practical Approach To Reservoir Production
  id: totrans-8009
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.3.3 水库生产的实用方法
- en: Prioritizing Parameters. While all the ESN reservoir parameters discussed in
    Section 27.3.2 have their guiding intuitions in setting them, fixing some of them
    is more straightforward than others.
  id: totrans-8010
  prefs: []
  type: TYPE_NORMAL
  zh: 参数优先级。在第27.3.2节中讨论的所有ESN水库参数都有其设置的指导直觉，但修正其中一些参数比其他参数更直接。
- en: '![656_image_1.png](656_image_1.png)'
  id: totrans-8011
  prefs: []
  type: TYPE_IMG
  zh: '![656_image_1.png](656_image_1.png)'
- en: '![656_image_2.png](656_image_2.png)'
  id: totrans-8012
  prefs: []
  type: TYPE_IMG
  zh: '![656_image_2.png](656_image_2.png)'
- en: '![656_image_0.png](656_image_0.png)'
  id: totrans-8013
  prefs: []
  type: TYPE_IMG
  zh: '![656_image_0.png](656_image_0.png)'
- en: '![656_image_3.png](656_image_3.png)'
  id: totrans-8014
  prefs: []
  type: TYPE_IMG
  zh: '![656_image_3.png](656_image_3.png)'
- en: These three parameters, discussed in Sections 27.3.2, 27.3.2, and 27.3.2 respectively,
    are very important for a good performance and are quite task-specific.
  id: totrans-8015
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个参数分别在第27.3.2节中讨论，对于良好的性能非常重要，并且相当任务特定。
- en: The reservoir size Nx almost comes as an external restriction (Section 27.3.2),
  id: totrans-8016
  prefs: []
  type: TYPE_NORMAL
  zh: 水库大小Nx几乎是一个外部限制（第27.3.2节），
- en: 'and the rest of the parameters can be set to reasonable default values: reservoir
    sparseness (Section 27.3.2), weight distribution (Section 27.3.2), or details
    of the model (27.2)(27.3). It is still worth investigating several options for
    them, as a lower priority.'
  id: totrans-8017
  prefs: []
  type: TYPE_NORMAL
  zh: 其余参数可以设置为合理的默认值：水库稀疏性（第27.3.2节）、权重分布（第27.3.2节）或模型细节（27.2）（27.3）。对于这些参数，仍然值得探讨几种选择，作为优先级较低的任务。
- en: As explained before, the performance can also be additionally improved in many
    cases by "splitting" a single parameter into several. Setting different scalings
    to the columns of Win (corresponding to the bias input and possibly to different
    dimensions of input if they are of different nature) can go a long way. Also,
    setting leaking rates α differently for different units (e.g., by splitting them
    to several subpopulations with constant value) can help a lot in multi-timescale
    tasks.
  id: totrans-8018
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在许多情况下，通过将单一参数“分割”为多个参数，可以进一步提高性能。对Win的列设置不同的缩放（对应于偏置输入，可能对应于不同性质的输入的不同维度）会有很大帮助。此外，为不同单元设置不同的泄漏率α（例如，通过将其分割为多个具有恒定值的亚群体）在多时间尺度任务中也能有很大帮助。
- en: Setup for Parameter Selection. One of the main advantages of ESNs is that learning
    the outputs is fast. This should be exploited in evaluating how good a reservoir
    generated by a particular set of parameters is.
  id: totrans-8019
  prefs: []
  type: TYPE_NORMAL
  zh: 参数选择的设置。ESNs的主要优势之一是学习输出的速度快。这应该在评估特定参数集生成的水库质量时加以利用。
- en: The most pragmatic way to evaluate a reservoir is to train the output (27.4)
    and measure its error.
  id: totrans-8020
  prefs: []
  type: TYPE_NORMAL
  zh: 评估一个水库最务实的方法是训练输出（27.4）并测量其误差。
- en: Either validation or training error can be used. Validation is, of course, preferred
    if there is a danger of overfitting. Training error has the advantage of using
    less data and in some cases no need to rerun a trained network with it. If a validation
    data set is necessary for the output training (as explained in Section 27.4),
    the error on it might be utilized with no additional cost, as a compromise between
    the training and a yet separate second validation set.
  id: totrans-8021
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用验证误差或训练误差。显然，如果存在过拟合的风险，验证是更优先的选择。训练误差的优势在于使用的数据较少，并且在某些情况下不需要重新运行训练好的网络。如果输出训练需要验证数据集（如第27.4节所解释），则可以在没有额外成本的情况下利用其误差，作为训练和另一个独立的验证集之间的折衷。
- en: If training of the output and validation is not fast enough, smaller initial
    reservoirs (as stated in Section 27.3.2), or a reduced representative data set
    can be used. For the same reason it is often an overkill to use a k-fold cross-validation
    in global parameter optimization, at least in initial stages, unless the data
    are really scarce.
  id: totrans-8022
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输出的训练和验证速度不够快，可以使用较小的初始水库（如第27.3.2节所述），或者使用一个减少的代表性数据集。出于同样的原因，在全局参数优化的初始阶段，使用k折交叉验证往往过于复杂，除非数据真的稀缺。
- en: It is important to keep in mind that the randomly generated reservoirs even
    with the same parameters vary slightly in their performance. This variance is
    ever present but is typically more pronounced with smaller reservoirs than with
    bigger ones; the random variations inside of a big reservoir tend to "average
    out". It is nonetheless important to keep this random fluctuation of performance
    separate from the one caused by different parameter values.
  id: totrans-8023
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，即使参数相同，随机生成的水库在性能上也会略有不同。这种变异是始终存在的，但通常在较小的水库中表现得更为明显；较大水库内部的随机变化往往会“趋于平均”。然而，重要的是将这种性能的随机波动与由于不同参数值引起的波动区分开来。
- en: To eliminate the random fluctuation of performance, keep the random seed fixed
    and/or average over several reservoir samples.
  id: totrans-8024
  prefs: []
  type: TYPE_NORMAL
  zh: 为了消除性能的随机波动，保持随机种子固定和/或对多个水库样本进行平均。
- en: 'Fixing a random seed in the programming environment before generating the reservoirs
    makes the random aspect of the reservoirs identical across trials and thus the
    experiments deterministically repeatable. Using a single reservoir is faster,
    but with an obvious danger of below-average performance and/or overfitting the
    parameters to a particular instance of a randomly generated reservoir: good parameters
    might not carry over well to a different software implementation, or, e.g., different
    size of a reservoir. Manual Parameter Selection. Manual selection of parameters
    is unavoidable to some extent in virtually all ML approaches. Even when parameters
    are learned or selected through automated search, it is typically necessary to
    set meta-parameters (or rather "meta-meta-parameters") for these procedures.'
  id: totrans-8025
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成水库之前，在编程环境中固定随机种子，使得水库的随机性在各次实验中保持一致，从而使实验可确定性地重复。使用单一水库速度较快，但明显存在性能低于平均水平和/或对特定随机生成水库的参数过拟合的风险：好的参数可能不适用于不同的软件实现，或者例如不同大小的水库。手动参数选择。在几乎所有机器学习方法中，手动选择参数在某种程度上是不可避免的。即使通过自动搜索学习或选择参数，通常仍然需要为这些过程设置元参数（或者说“元元参数”）。
- en: When manually tuning the reservoir parameters, change one parameter at a time.
  id: totrans-8026
  prefs: []
  type: TYPE_NORMAL
  zh: 在手动调整水库参数时，一次只改变一个参数。
- en: Changes in several parameters at once often have opposing effects on performance,
    but it is impossible to tell which contributed what. A reasonable approach is
    to set a single parameter to a well enough value before starting changing another
    one, and repeating this until the performance is satisfactory.
  id: totrans-8027
  prefs: []
  type: TYPE_NORMAL
  zh: 同时改变多个参数往往对性能产生相反的影响，但很难判断每个参数的具体贡献。一个合理的方法是在开始改变另一个参数之前，将一个参数设置到一个足够好的值，并重复这个过程，直到性能令人满意。
- en: It is also advisable to take notes or log the performance automatically for
    extended optimizations, in order not to "go in circles" when repeating the same
    parameter values.
  id: totrans-8028
  prefs: []
  type: TYPE_NORMAL
  zh: 还建议在进行长期优化时记录笔记或自动记录性能，以避免在重复相同参数值时“打转”。
- en: An empirical direction of a gradient can be estimated for a parameter, making
    a small change to it and observing the change in performance. However, the error
    landscapes are often non-convex and trying distant values of the parameters can
    sometimes lead to dramatic improvements.
  id: totrans-8029
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过对一个参数进行小幅度的改变并观察性能变化来估计该参数的梯度的经验方向。然而，误差景观往往是非凸的，尝试远离的参数值有时会带来显著的改进。
- en: Always plot samples of reservoir activation signals x(n) to have a feeling of
    what is happening inside the reservoir.
  id: totrans-8030
  prefs: []
  type: TYPE_NORMAL
  zh: 始终绘制水库激活信号 x(n) 的样本，以便了解水库内部发生的情况。
- en: This may reveal that x(n) are over-saturated, under-activated, exhibiting autonomous
    cyclic or chaotic behavior, etc. Overall, plotting information additional to the
    error rate helps a lot in gaining more insight into how the parameters should
    be changed.
  id: totrans-8031
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能揭示 x(n) 过度饱和、未激活、表现出自主周期或混沌行为等。总的来说，绘制除误差率之外的信息对于深入了解参数如何变化非常有帮助。
- en: Typically, good average performance is not found in a very narrow parameter
    range, thus a very detailed fine-tuning of parameters does not give a significant
    improvement and is not necessary. Automated Parameter Selection. Since manual
    parameter optimization might quickly get tedious, automated approaches are often
    preferred.
  id: totrans-8032
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，好的平均性能不会出现在非常狭窄的参数范围内，因此对参数的非常详细的微调不会带来显著的改善，也不是必要的。自动参数选择。由于手动参数优化可能很快变得乏味，因此通常更倾向于使用自动化方法。
- en: Since ESNs have only a few parameters requiring more careful tuning, *grid*
    search is probably the most straightforward option. It is easy enough to implement
    with a few nested loops, and high-level ML programming libraries, such as Oger
    (mentioned in Section 27.6) in the case of RC, often have ready-made routines
    for this.
  id: totrans-8033
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 ESN 只有少数几个需要更仔细调节的参数，*网格*搜索可能是最直接的选择。通过几个嵌套循环来实现这一点相对简单，并且高层次的机器学习编程库（例如在
    RC 的情况下提到的 Oger）通常提供现成的例程。
- en: A reasonable approach is to do a coarser grid search over wider parameter intervals
    to identify promising regions and then do a finer search (smaller steps) in these
    regions. As mentioned, typically the grid must not be very dense to achieve a
    good performance.
  id: totrans-8034
  prefs: []
  type: TYPE_NORMAL
  zh: 一个合理的方法是在更宽的参数区间上进行较粗的网格搜索，以识别有前景的区域，然后在这些区域内进行更细的搜索（更小的步长）。如前所述，通常网格不必非常密集即可达到良好性能。
- en: The best performance being on a boundary of a covered grid is a good indication
    that the optimal performance might be outside the grid.
  id: totrans-8035
  prefs: []
  type: TYPE_NORMAL
  zh: 在被覆盖的网格边界上表现最佳，说明最佳性能可能在网格之外。
- en: In general, meta-parameter or hyper-parameter optimization is a very common
    topic in many branches of ML and beyond. There are numerous generic optimization
    methods applicable to this task described in the literature. They are often coping
    with much larger search spaces than a grid search is effectively capable of, such
    as random search, or more sophisticated methods trying to model the error landscape
    (see, e.g., [2]). They are in principle just as well applicable to ESNs with a
    possibility of also including in the optimization the parameters of second importance.
  id: totrans-8036
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，元参数或超参数优化是机器学习及其他领域中的一个非常常见的话题。文献中描述了许多通用优化方法可用于此任务。它们通常处理比网格搜索更大的搜索空间，例如随机搜索或更复杂的方法，试图建模误差景观（参见，例如
    [2]）。原则上，它们同样适用于ESN，并且可能还包括第二重要性的参数。
- en: There is also a way to optimize the global parameters of the reservoir through
    a gradient descent [22]. It has, however, not been widely applied in the literature.
  id: totrans-8037
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一种通过梯度下降优化储层全局参数的方法 [22]。然而，在文献中尚未广泛应用。
- en: 27.3.4 Pointers To Reservoir Extensions
  id: totrans-8038
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.3.4 储层扩展指针
- en: There are also alternative ways of generating and adapting reservoirs suggested
    in the literature, including deterministic, e.g., [39], and data-specific, e.g.,
    [36], ones. With a variety of such methods the modern field of RC has evolved
    from using the initial paradigm of a fixed reservoir and only training a readout
    from it, to also adapting the reservoir but differently from the readout, using
    generic, unsupervised, or even supervised methods. In some cases a hardware system
    is used as a reservoir and thus is predetermined by its specific features. See
    [29] and updated in Chapter 2 of [30] for a classification and overview. The classical
    ESN approach described here, however, still holds its ground for its simplicity
    and performance.
  id: totrans-8039
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中还提出了生成和调整储层的替代方法，包括确定性方法，例如 [39]，和数据特定方法，例如 [36]。通过多种方法，现代RC领域已从使用固定储层并仅从中训练输出，发展到也调整储层，但与输出方式不同，使用通用、无监督或甚至监督的方法。在某些情况下，硬件系统作为储层，因此由其特定特征预先确定。有关分类和概述，请参见
    [29] 和 [30] 的第二章更新。然而，这里描述的经典ESN方法仍因其简单性和性能而占据一席之地。
- en: 27.4 Training Readouts 27.4.1 Ridge Regression
  id: totrans-8040
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.4 训练输出 27.4.1 岭回归
- en: Since readouts from an ESN are typically linear and feed-forward, the Equation
    (27.4) can be written in a matrix notation as
  id: totrans-8041
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ESN的输出通常是线性和前馈的，方程 (27.4) 可以用矩阵表示为
- en: $$\mathbf{Y}=\mathbf{W}^{\mathrm{out}}\mathbf{X},$$
  id: totrans-8042
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbf{Y}=\mathbf{W}^{\mathrm{out}}\mathbf{X},$$
- en: $\left(27.7\right)^{2}$
  id: totrans-8043
  prefs: []
  type: TYPE_NORMAL
  zh: $\left(27.7\right)^{2}$
- en: Y = WoutX, (27.7)
  id: totrans-8044
  prefs: []
  type: TYPE_NORMAL
  zh: Y = WoutX, (27.7)
- en: where Y ∈ RNy×T are all y(n) and X ∈ R(1+Nu+Nx)×T are all [1; u(n); x(n)]
  id: totrans-8045
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Y ∈ RNy×T 是所有的 y(n)，而 X ∈ R(1+Nu+Nx)×T 是所有的 [1; u(n); x(n)]
- en: produced by presenting the reservoir with u(n), both collected into respective
    matrices by concatenating the column-vectors horizontally over the training period
    n = 1*,...,T* . We use here a single X instead of [1; U; X] for notational brevity.
  id: totrans-8046
  prefs: []
  type: TYPE_NORMAL
  zh: 通过呈现储层与 u(n) 生成，两个相应的矩阵通过在训练期间 n = 1*,...,T* 水平拼接列向量收集到一起。这里我们使用单个 X，而不是 [1;
    U; X] 以简化表示。
- en: Finding the optimal weights Wout that minimize the squared error between y(n)
    and ytarget(n) amounts to solving a typically overdetermined system of linear
    equations Ytarget = WoutX, (27.8)
  id: totrans-8047
  prefs: []
  type: TYPE_NORMAL
  zh: 找到最小化 y(n) 和 ytarget(n) 之间平方误差的最优权重 Wout，相当于求解一个通常过定的线性方程组 Ytarget = WoutX,
    (27.8)
- en: where Ytarget ∈ RNy×T are all y(n), with respect to Wout in a least-square sense
    - i.e., a case of linear regression. In this context X can be called the *design*
    matrix. The system is overdetermined, because typically T ! 1 + Nu + Nx.
  id: totrans-8048
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Ytarget ∈ RNy×T 是所有的 y(n)，相对于 Wout 以最小二乘法的意义 - 即线性回归的一种情况。在这个上下文中，X 可以称为
    *设计* 矩阵。该系统是过定的，因为通常 T ! 1 + Nu + Nx。
- en: There are standard well-known ways to solve (27.8), we will discuss a couple
    of good choices here.
  id: totrans-8049
  prefs: []
  type: TYPE_NORMAL
  zh: 有解决 (27.8) 的标准知名方法，我们将在此讨论几个良好的选择。
- en: 'Probably the most universal and stable solution to (27.8) in this context is
    ridge regression, also known as regression with Tikhonov regularization:'
  id: totrans-8050
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，对 (27.8) 最通用和稳定的解决方案是岭回归，也称为带 Tikhonov 正则化的回归：
- en: $$(27.8)$$
  id: totrans-8051
  prefs: []
  type: TYPE_NORMAL
  zh: $$(27.8)$$
- en: $$\mathbf{Y}^{\mathrm{target}}=\mathbf{W}^{\mathrm{out}}\mathbf{X},$$
  id: totrans-8052
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbf{Y}^{\mathrm{target}}=\mathbf{W}^{\mathrm{out}}\mathbf{X},$$
- en: $$\mathbf{W}^{\mathrm{out}}=\mathbf{Y}^{\mathrm{target}}\mathbf{X}^{\mathrm{T}}\left(\mathbf{X}\mathbf{X}^{\mathrm{T}}+\beta\mathbf{I}\right)^{-1},$$
  id: totrans-8053
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbf{W}^{\mathrm{out}}=\mathbf{Y}^{\mathrm{target}}\mathbf{X}^{\mathrm{T}}\left(\mathbf{X}\mathbf{X}^{\mathrm{T}}+\beta\mathbf{I}\right)^{-1},$$
- en: $$(27.9)$$
  id: totrans-8054
  prefs: []
  type: TYPE_NORMAL
  zh: $$(27.9)$$
- en: where β is a regularization coefficient explained in Section 27.4.2, and I is
    the identity matrix.
  id: totrans-8055
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 β 是在 27.4.2 节中解释的正则化系数，I 是单位矩阵。
- en: The most generally recommended way to learn linear output weights from an ESN
    is ridge regression (27.9)
  id: totrans-8056
  prefs: []
  type: TYPE_NORMAL
  zh: 从 ESN 学习线性输出权重的最推荐方法是岭回归 (27.9)。
- en: We start with this method because it should be the first choice, even though
    it is not the most trivial one. We will explain different aspects of this method
    in the coming sections together with reasons for why it should be preferred and
    alternatives that in some cases can be advantageous.
  id: totrans-8057
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从这种方法开始，因为它应该是首选，即使它不是最简单的选择。我们将在接下来的章节中解释该方法的不同方面，以及为什么它应该被优先选择和在某些情况下可以更具优势的替代方案。
- en: 27.4.2 Regularization
  id: totrans-8058
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.4.2 正则化
- en: To assess the quality of the solution produced by training, it is advisable
    to monitor the actual obtained output weights Wout. Large weights indicate that
    Wout exploits and amplifies tiny differences among the dimensions of x(n), and
    can be very sensitive to deviations from the exact conditions in which the network
    has been trained. This is a big problem in the setups where the network receives
    its output as the next input. The slight deviation of the output from the expected
    value quickly escalates in subsequent time steps. Ways of dealing with such setup
    are explained in Section 27.5.
  id: totrans-8059
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估训练所产生解的质量，建议监控实际获得的输出权重 Wout。较大的权重表明 Wout 利用并放大了 x(n) 各维度之间微小的差异，并且对网络训练时的精确条件的偏差非常敏感。这在网络将其输出作为下一个输入的设置中是一个大问题。输出与预期值的轻微偏差在后续时间步骤中会迅速升级。处理此类设置的方法在
    27.5 节中进行了解释。
- en: Extremely large Wout values may be an indication of a very sensitive and unstable
    solution.
  id: totrans-8060
  prefs: []
  type: TYPE_NORMAL
  zh: 极大的 Wout 值可能表明这是一个非常敏感和不稳定的解。
- en: To counteract this effect is exactly what the regularization part βI in the
    ridge regression (27.9) is for. Instead of just minimizing RMSE (27.1), ridge
    regression (27.9) solves
  id: totrans-8061
  prefs: []
  type: TYPE_NORMAL
  zh: 正是为了抵消这种影响，岭回归 (27.9) 中的正则化部分 βI 的作用。岭回归 (27.9) 不仅仅是最小化 RMSE (27.1)。
- en: $${\bf W}^{\rm out}=\mathop{\rm arg\,min}_{\bf W}\frac{1}{N_{\rm y}}\sum_{i=1}^{N_{\rm
    y}}\left(\sum_{n=1}^{T}\left(y_{i}(n)-y_{i}^{\rm target}(n)\right)^{2}+\beta\left\|{\bf
    w}_{i}^{\rm out}\right\|^{2}\right),\tag{27.10}$$
  id: totrans-8062
  prefs: []
  type: TYPE_NORMAL
  zh: $${\bf W}^{\rm out}=\mathop{\rm arg\,min}_{\bf W}\frac{1}{N_{\rm y}}\sum_{i=1}^{N_{\rm
    y}}\left(\sum_{n=1}^{T}\left(y_{i}(n)-y_{i}^{\rm target}(n)\right)^{2}+\beta\left\|{\bf
    w}_{i}^{\rm out}\right\|^{2}\right),\tag{27.10}$$
- en: where wout i is the ith row of Wout and · stands for the Euclidean norm.
  id: totrans-8063
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 wout i 是 Wout 的第 i 行，· 表示欧几里得范数。
- en: The objective function in (27.10) adds a regularization, or weight decay, term
    β wout i 2 penalizing large sizes of Wout to the square error between y(n) and
    ytarget(n). This is a sum of two objectives, a compromise between having a small
    training error and small output weights. The relative "importance" between these
    two objectives is controlled by the regularization parameter β.
  id: totrans-8064
  prefs: []
  type: TYPE_NORMAL
  zh: 公式 (27.10) 中的目标函数添加了正则化项或权重衰减项 β wout i 2，以惩罚 Wout 的大值与 y(n) 和 ytarget(n) 之间的平方误差。这是两个目标的总和，即在保持较小训练误差和小输出权重之间的折衷。这两个目标之间的相对“重要性”由正则化参数
    β 控制。
- en: Use regularization (e.g., (27.9)) whenever there is a danger of overfitting
    or feedback instability.
  id: totrans-8065
  prefs: []
  type: TYPE_NORMAL
  zh: 每当存在过拟合或反馈不稳定的危险时，使用正则化（例如，(27.9)）。
- en: In (27.9) the optimal regularization coefficient β depends on the concrete instantiation
    of the ESN. It should be selected individually for a concrete reservoir based
    on validation data.
  id: totrans-8066
  prefs: []
  type: TYPE_NORMAL
  zh: 在 (27.9) 中，最佳正则化系数 β 取决于 ESN 的具体实例化。它应该根据验证数据为具体水库单独选择。
- en: Select β for a concrete ESN using validation, without rerunning the reservoir
    through the training data.
  id: totrans-8067
  prefs: []
  type: TYPE_NORMAL
  zh: 通过验证为具体的 ESN 选择 β，而无需重新运行水库的训练数据。
- en: There is no need to rerun the model through the data with every value β, because
    none of the other variables in (27.9) are affected by its changes. Memory permitting,
    there is also no need to rerun the model with the (small) validation dataset,
    if you can store X for it and compute the validation output by (27.7). This makes
    testing β values computationally much less expensive than testing the reservoir
    parameters explained in Section 27.3.
  id: totrans-8068
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要对每个β值重新运行模型，因为（27.9）中的其他变量不受其变化的影响。如果内存允许，如果可以存储X以计算验证输出（27.7），则也不需要使用（小）验证数据集重新运行模型。这使得测试β值在计算上比测试第27.3节中解释的水库参数便宜得多。
- en: The optimal values of β can vary by many magnitudes of size, depending on the
    exact instance of the reservoir and length of the training data. If doing a simple
    exhaustive search, it is advisable to search on a logarithmic grid.
  id: totrans-8069
  prefs: []
  type: TYPE_NORMAL
  zh: β的最优值可以相差多个数量级，具体取决于水库的确切实例和训练数据的长度。如果进行简单的穷举搜索，建议在对数网格上搜索。
- en: 'Setting β to zero removes the regularization: the objective function in (27.10)'
  id: totrans-8070
  prefs: []
  type: TYPE_NORMAL
  zh: 将β设置为零去除了正则化：目标函数在（27.10）
- en: becomes equivalent to RMSE (27.1), making the ridge regression a generalization
    of a regular linear regression. The solution (27.9) with β = 0 becomes
  id: totrans-8071
  prefs: []
  type: TYPE_NORMAL
  zh: 相当于RMSE（27.1），使得岭回归成为常规线性回归的推广。当β = 0时，解（27.9）变为
- en: $$\mathbf{W}^{\mathrm{{out}}}=\mathbf{Y}^{\mathrm{{target}}}\mathbf{X}^{\mathrm{{T}}}\left(\mathbf{X}\mathbf{X}^{\mathrm{{T}}}\right)^{-1},$$
  id: totrans-8072
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbf{W}^{\mathrm{{out}}}=\mathbf{Y}^{\mathrm{{target}}}\mathbf{X}^{\mathrm{{T}}}\left(\mathbf{X}\mathbf{X}^{\mathrm{{T}}}\right)^{-1},$$
- en: $$(27.11)$$
  id: totrans-8073
  prefs: []
  type: TYPE_NORMAL
  zh: $$(27.11)$$
- en: known as normal equations method for solving linear regression (27.8). In practice,
    however, setting β = 0 often leads to numerical instabilities when inverting
  id: totrans-8074
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为求解线性回归的正态方程法（27.8）。然而，在实践中，设置β = 0通常导致数值不稳定，尤其是在求逆时。
- en: (XXT) in (27.11). This too recommends using a logarithmic scale for selecting
    β where it never goes to zero. The problem can in some cases also be alleviated
    by using a pseudoinverse instead of the real inverse in (27.11).
  id: totrans-8075
  prefs: []
  type: TYPE_NORMAL
  zh: （27.11）中的（XXT）。这也建议在选择β时使用对数尺度，其中它永远不会降到零。在某些情况下，可以通过在（27.11）中使用伪逆而不是真实逆来缓解此问题。
- en: A Gaussian process interpretation of the linear readout gives an alternative
    criterion for setting β directly [4].
  id: totrans-8076
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程对线性读出的解释提供了一种直接设置β的替代标准[4]。
- en: A similar regularization effect to Tikhonov (27.9) can be achieved by adding
    scaled white noise to x(n) in (27.3) - a method that predates ridge regression
    in ESNs [16]. Like in ridge regression, i.i.d. noise emphasizes the diagonal of
    (XXT).
  id: totrans-8077
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向（27.3）中的x(n)添加缩放白噪声，可以实现类似于Tikhonov（27.9）的正则化效果——这种方法早于ESN中的岭回归[16]。与岭回归一样，i.i.d.噪声强调（XXT）的对角线。
- en: The advantage is that it is also propagated through W in (27.2), modeling better
    the effects of noisy signals in the reservoir. The output learns to recover from
    perturbed signals, making the model more stable with feedback loops (Section 27.5).
    The downside of this noise immunization is that the model needs to be rerun with
    each value of the noise scaling.
  id: totrans-8078
  prefs: []
  type: TYPE_NORMAL
  zh: 其优点在于它也通过（27.2）中的W进行传播，更好地建模水库中噪声信号的影响。输出学习从扰动信号中恢复，使模型在反馈循环（第27.5节）中更加稳定。这种噪声免疫的缺点是模型需要对每个噪声缩放值重新运行。
- en: 27.4.3 Large Datasets
  id: totrans-8079
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.4.3 大数据集
- en: Ridge regression (27.9) (or the Wiener-Hopf solution (27.11) as a special case)
  id: totrans-8080
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归（27.9）（或维纳-霍夫特解（27.11）作为特例）
- en: also allows a one-shot training with virtually unlimited amounts of data.
  id: totrans-8081
  prefs: []
  type: TYPE_NORMAL
  zh: 还允许进行几乎无限数据量的一次性训练。
- en: Notice that the dimensions of the matrices (YtargetXT) ∈ RNy×Nx and
  id: totrans-8082
  prefs: []
  type: TYPE_NORMAL
  zh: 注意矩阵（YtargetXT）∈ RNy×Nx的维度和
- en: (XXT) ∈ RNx×Nx do not depend on the length T of the training sequence in their
    sizes. The two matrices can be updated by simply adding the corresponding results
    from the newly incoming data. This one-shot training approach in principle works
    with an unlimited amount of data - neither complexity of working memory, nor time
    of the training procedure (27.9) itself depend on the length of data T .
  id: totrans-8083
  prefs: []
  type: TYPE_NORMAL
  zh: （XXT）∈ RNx×Nx的大小不依赖于训练序列的长度T。两个矩阵可以通过简单地添加来自新数据的对应结果进行更新。原则上，这种一次性训练方法可以处理无限量的数据——工作记忆的复杂性和训练过程（27.9）本身的时间均不依赖于数据长度T。
- en: '![661_image_0.png](661_image_0.png)'
  id: totrans-8084
  prefs: []
  type: TYPE_IMG
  zh: '![661_image_0.png](661_image_0.png)'
- en: The eventual limitation of the straightforward summation comes from the finite
    precision of floating point numbers - adding large (like in the so-far accumulated
    matrix) and small (like the next update) numbers becomes inaccurate. A better
    summation scheme, such as a hierarchical multi-stage summation where the two added
    values are always of similar magnitude (e.g., coming from the same amount of time
    steps), or Kahan summation [24] that compensates for the accumulating errors,
    should be used instead.
  id: totrans-8085
  prefs: []
  type: TYPE_NORMAL
  zh: 直接求和的最终限制来自于浮点数的有限精度——将大数（如当前累计矩阵中的大数）和小数（如下一个更新）相加会变得不准确。应使用更好的求和方案，例如层次多阶段求和，其中两个相加值始终具有相似的量级（例如，来自相同数量的时间步），或补偿累积误差的
    Kahan 求和 [24]。
- en: '![661_image_1.png](661_image_1.png)'
  id: totrans-8086
  prefs: []
  type: TYPE_IMG
  zh: '![661_image_1.png](661_image_1.png)'
- en: Using extended precision numbers here could also help, as well as in other calculations.
  id: totrans-8087
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里使用扩展精度数也可能有帮助，以及在其他计算中。
- en: $\left(27.12\right)^{2}$
  id: totrans-8088
  prefs: []
  type: TYPE_NORMAL
  zh: $\left(27.12\right)^{2}$
- en: 27.4.4 Direct Pseudoinverse Solution A straightforward solution to (27.8) is
    Wout = YtargetX+, (27.12)
  id: totrans-8089
  prefs: []
  type: TYPE_NORMAL
  zh: 27.4.4 直接伪逆解 对 (27.8) 的直接解是 Wout = YtargetX+，(27.12)
- en: where X+ is the Moore-Penrose pseudoinverse of X. If (XXT) is invertible the
  id: totrans-8090
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 X+ 是 X 的摩尔-彭若斯伪逆。如果 (XXT) 是可逆的
- en: (27.12) in essence becomes equivalent to (27.11), but works even when it is
    not. The direct pseudoinverse calculation typically exhibits high numerical stability.
  id: totrans-8091
  prefs: []
  type: TYPE_NORMAL
  zh: (27.12) 本质上等同于 (27.11)，但即使在不满足条件时也能工作。直接伪逆计算通常表现出较高的数值稳定性。
- en: As a downside, it is expensive memory-wise for large design matrices X, thereby
    limiting the size of the reservoir Nx and/or the number of training samples T
    .
  id: totrans-8092
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个缺点，对于大的设计矩阵 X，内存开销昂贵，从而限制了水库的大小 Nx 和/或训练样本的数量 T。
- en: Since there is virtually no regularization, the system of linear equations (27.8)
  id: totrans-8093
  prefs: []
  type: TYPE_NORMAL
  zh: 由于几乎没有正则化，线性方程组 (27.8)
- en: should be well overdetermined, i.e., 1 + Nu + Nx T . In other words, the task
    should be difficult relatively to the capacity of the reservoir so that overfitting
    does not happen.
  id: totrans-8094
  prefs: []
  type: TYPE_NORMAL
  zh: 应该是良好超定的，即 1 + Nu + Nx T。换句话说，任务应该相对于水库的容量相对困难，以避免过拟合。
- en: Use direct pseudoinverse (27.12) to train ESNs with high precision and little
    regularization when memory and run time permit.
  id: totrans-8095
  prefs: []
  type: TYPE_NORMAL
  zh: 当内存和运行时间允许时，使用直接伪逆 (27.12) 以高精度和少量正则化训练 ESN。
- en: Many modern programming libraries dealing with linear algebra have implementations
    of a matrix pseudoinverse, which can be used "off the shelf". However, implementations
    vary in their precision, computational efficiency, and numerical stability.
  id: totrans-8096
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现代编程库处理线性代数时都有矩阵伪逆的实现，可以“开箱即用”。然而，实现的精度、计算效率和数值稳定性各不相同。
- en: For high precision tasks, check whether the regression
  id: totrans-8097
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高精度任务，检查回归。
- en: (Ytarget − WoutX)X+ on the error Ytarget − WoutX is actually all = 0, and add
    it to Wout if it is not.
  id: totrans-8098
  prefs: []
  type: TYPE_NORMAL
  zh: (Ytarget − WoutX)X+ 在误差 Ytarget − WoutX 上实际上都是 = 0，如果不是，则将其添加到 Wout。
- en: This computational trick should not work in theory (the regression on the error
    should be equal to zero), but sometimes does work in practice in Matlab [28],
  id: totrans-8099
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，这一计算技巧不应有效（误差的回归应等于零），但在实践中在 Matlab [28] 中有时有效。
- en: possibly because of some internal optimizations.
  id: totrans-8100
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是由于某些内部优化。
- en: Some high level linear algebra libraries have ready-made subroutines for doing
    regression, i.e., solving linear least-squares as in (27.8), where the exact methods
    are not made explicit and can internally be chosen depending on the conditioning
    of the problem. The use of them has an obvious disadvantage of lacking the control
    on the issues discussed here.
  id: totrans-8101
  prefs: []
  type: TYPE_NORMAL
  zh: 一些高级线性代数库有现成的子例程来进行回归，即求解 (27.8) 中的线性最小二乘，具体方法未明确列出，可以根据问题的条件性内部选择。使用这些方法有一个明显的缺点，即缺乏对这里讨论问题的控制。
- en: A powerful extension of the basic ESN approach is training (very) many (very
    small) ESNs in parallel and averaging their outputs, which in some cases has drastically
    improved performance [19, 22]. This might be not true for tasks requiring large
    memory, where one bigger reservoir may still be better than several smaller ones.
  id: totrans-8102
  prefs: []
  type: TYPE_NORMAL
  zh: 基本 ESN 方法的一个强大扩展是并行训练（非常多的）小型 ESN 并平均它们的输出，这在某些情况下显著提高了性能 [19, 22]。对于需要大量内存的任务，这可能不成立，其中一个较大的水库可能仍然比几个较小的水库更好。
- en: Averaging outputs from multiple reservoirs increases the performance.
  id: totrans-8103
  prefs: []
  type: TYPE_NORMAL
  zh: 从多个水库平均输出可以提高性能。
- en: 27.4.5 Initial Transient
  id: totrans-8104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.4.5 初始瞬态
- en: Usually x(n) data from the beginning of the training run are discarded (i.e.,
    not used for learning Wout) since they are contaminated by initial transients.
    To keep notation simple let us assume they come before n = 1.
  id: totrans-8105
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在训练运行开始时的x(n)数据会被丢弃（即不用于学习Wout），因为它们受到初始瞬态的影响。为了简化符号，我们假设它们在n = 1之前。
- en: For long sequences discard the initial time steps of activations x(n) for training
    that are affected by initial transient.
  id: totrans-8106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于长序列，丢弃受初始瞬态影响的激活x(n)的初始时间步进行训练。
- en: The initial transient is in essence a result of an arbitrary setting of x(0),
    which is typically x(0) = 0. This introduces an unnatural starting state which
    is not normally visited once the network has "warmed up" to the task. The amount
    of time steps to discard depends on the memory of the network (which in turn depends
    on reservoir parameters), and typically are in the order of tens or hundreds.
  id: totrans-8107
  prefs: []
  type: TYPE_NORMAL
  zh: 初始瞬态本质上是x(0)任意设置的结果，通常情况下x(0)=0。这引入了一个不自然的初始状态，一旦网络“适应”了任务，就不会正常访问。丢弃的时间步数量取决于网络的记忆（这又依赖于水库参数），通常在几十或几百的范围内。
- en: However, if the data consists of multiple short separate sequences (like in
    sequence classification), "the initial transient" might be the usual working mode
    of the ESN. In this case discarding the precious (possibly all!) data might be
    disadvantageous. See Section 27.4.7 for more on this. Note, that you would want
    to reset the state to some initial (the same) value before each sequence to make
    the classification of sequences independent.
  id: totrans-8108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果数据由多个短的独立序列组成（如在序列分类中），那么“初始瞬态”可能是ESN的常见工作模式。在这种情况下，丢弃宝贵的数据（可能是全部数据！）可能是不利的。有关更多信息，请参见第27.4.7节。请注意，你希望在每个序列之前将状态重置为某个初始（相同）值，以使序列的分类相互独立。
- en: With multiple sequences of data the time steps on which the learning should
    be performed should be concatenated in X and Ytarget, the same way as there would
    only be a single long sequence.
  id: totrans-8109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多个数据序列，学习应在的时间步应该以与只有单个长序列相同的方式连接在X和Ytarget中。
- en: A generalization of discarding data is presented in the next Section 27.4.6.
  id: totrans-8110
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节27.4.6中介绍了丢弃数据的概括。
- en: 27.4.6 Regression Weighting
  id: totrans-8111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.4.6 回归加权
- en: 'In the regression learning of ESNs it is easy to make some time steps count
    more than others by weighting the minimized square error differently. For this,
    the time steps of the square error are weighted with a weight vector s(n) ∈ R:'
  id: totrans-8112
  prefs: []
  type: TYPE_NORMAL
  zh: 在ESN的回归学习中，通过对最小化平方误差的加权，可以轻松使某些时间步的权重大于其他时间步。为此，平方误差的时间步与权重向量s(n) ∈ R相乘：
- en: $$E({\bf y},{\bf y}^{\rm target})=\frac{1}{N_{\rm y}}\sum_{i=1}^{N_{\rm y}}\sum_{n=1}^{T}s(n)\left(y_{i}(n)-y_{i}^{\rm
    target}(n)\right)^{2}.\tag{27.13}$$
  id: totrans-8113
  prefs: []
  type: TYPE_NORMAL
  zh: $$E({\bf y},{\bf y}^{\rm target})=\frac{1}{N_{\rm y}}\sum_{i=1}^{N_{\rm y}}\sum_{n=1}^{T}s(n)\left(y_{i}(n)-y_{i}^{\rm
    target}(n)\right)^{2}.\tag{27.13}$$
- en: This error is minimized with the same learning algorithms, but at each time
    step n the vectors [1; u(n); x(n)] and ytarget
  id: totrans-8114
  prefs: []
  type: TYPE_NORMAL
  zh: 该误差使用相同的学习算法最小化，但在每个时间步n，向量[1; u(n); x(n)]和ytarget
- en: '&'
  id: totrans-8115
  prefs: []
  type: TYPE_NORMAL
  zh: '&'
- en: (n) are element-wise multiplied with s(n) before collecting them into X and
    Ytarget. Higher values of s(n) put more emphasis on minimizing the error between
    y(n) and ytarget(n). Putting a weight s(n) on a time step n has the same effect
    as if [1; u(n); x(n)] and ytarget(n) have appeared s(n) times in the training
    with a regular weight s(n)=1. Setting s(n)=0 is equivalent to discarding the time
    steps from training altogether.
  id: totrans-8116
  prefs: []
  type: TYPE_NORMAL
  zh: (n)与s(n)逐元素相乘，然后收集到X和Ytarget中。s(n)的更高值使得在y(n)和ytarget(n)之间最小化误差的重点更加突出。在时间步n上施加权重s(n)的效果就像[1;
    u(n); x(n)]和ytarget(n)在训练中出现了s(n)次，权重为s(n)=1。将s(n)=0相当于完全丢弃该时间步的训练。
- en: Use weighting to assign different importance to different time steps when training.
  id: totrans-8117
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时使用加权为不同的时间步分配不同的重要性。
- en: This weighted least squares scheme can be useful in different situations like
    discarding or weighting down the signals affected by the initial transients, corrupted
    or missing data, emphasizing the ending of the signal [6], etc. It is fully compatible
    with ridge regression (27.9) where the weighting (27.13) is applied to the square
    error part of the objective function in (27.10).
  id: totrans-8118
  prefs: []
  type: TYPE_NORMAL
  zh: 这种加权最小二乘方案在不同情况下非常有用，例如丢弃或降低受初始瞬态影响的信号，加工或缺失数据，强调信号的结尾[6]等。它与岭回归（27.9）完全兼容，其中权重（27.13）应用于目标函数（27.10）的平方误差部分。
- en: For an even more refined version different channels of y(n) can be trained separately
    and with different s(n). The weighting can for example be used to counter an imbalance
    between positive vs. negative samples in a classification or detection task.
  id: totrans-8119
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个更精细的版本，可以分别训练y(n)的不同通道，并使用不同的s(n)。加权可以用来对抗分类或检测任务中正负样本之间的不平衡。
- en: 27.4.7 Readouts For Classification
  id: totrans-8120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.4.7 分类的读取结果
- en: When the task is to classify separate short time series, the training is typically
    set up such that the output y(n) has a dimension for every class and ytarget(n)
  id: totrans-8121
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务是对单独的短时间序列进行分类时，训练通常设定为输出y(n)为每个类别都有一个维度，并且ytarget(n)
- en: is equal to one in the dimension corresponding to the correct class and zero
    everywhere else. The model is trained to approximate ytarget(n) and the class
    for single sequence u(n) is very often decided by
  id: totrans-8122
  prefs: []
  type: TYPE_NORMAL
  zh: 在与正确类别对应的维度上等于1，其他地方为0。模型被训练以逼近ytarget(n)，而单个序列u(n)的类别通常由
- en: $$\operatorname{class}\left(\mathbf{u}(n)\right)=\arg\operatorname*{max}_{k}\left({\frac{1}{|\tau|}}\sum_{n\in\tau}y_{k}(n)\right)=\arg\operatorname*{max}_{k}\left((\Sigma\mathbf{y})_{k}\right),$$
  id: totrans-8123
  prefs: []
  type: TYPE_NORMAL
  zh: $$\operatorname{class}\left(\mathbf{u}(n)\right)=\arg\operatorname*{max}_{k}\left({\frac{1}{|\tau|}}\sum_{n\in\tau}y_{k}(n)\right)=\arg\operatorname*{max}_{k}\left((\Sigma\mathbf{y})_{k}\right)，$$
- en: $$(27.14)$$
  id: totrans-8124
  prefs: []
  type: TYPE_NORMAL
  zh: $$(27.14)$$
- en: ((Σy)k), (27.14)
  id: totrans-8125
  prefs: []
  type: TYPE_NORMAL
  zh: ((Σy)k)，(27.14)
- en: where yk(n) is the kth dimension of y(n) produced by ESN from u(n), τ is some
    integration interval (can be the length of the whole sequence u(n)), and Σy stands
    for a shorthand notation of y(n) time-averaged over τ.
  id: totrans-8126
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 yk(n) 是由ESN从u(n)生成的y(n)的第k维，τ是某个积分区间（可以是整个序列u(n)的长度），Σy表示y(n)在τ上时间平均的简写符号。
- en: There is a better way to do this. Notice, that in this case
  id: totrans-8127
  prefs: []
  type: TYPE_NORMAL
  zh: 有更好的方法来实现这一点。请注意，在这种情况下
- en: $$\Sigma\mathbf{y}={\frac{1}{|\tau|}}\sum_{n\in\tau}\mathbf{y}(n)={\frac{1}{|\tau|}}\sum_{n\in\tau}\mathbf{W}^{\mathrm{out}}[1;\mathbf{u}(n);\mathbf{x}(n)]=$$
    $$=\mathbf{W}^{\mathrm{out}}{\frac{1}{|\tau|}}\sum_{n\in\tau}[1;\mathbf{u}(n);\mathbf{x}(n)]=\mathbf{W}^{\mathrm{out}}\,\Sigma\mathbf{x},$$
  id: totrans-8128
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Sigma\mathbf{y}={\frac{1}{|\tau|}}\sum_{n\in\tau}\mathbf{y}(n)={\frac{1}{|\tau|}}\sum_{n\in\tau}\mathbf{W}^{\mathrm{out}}[1;\mathbf{u}(n);\mathbf{x}(n)]=$$
    $$=\mathbf{W}^{\mathrm{out}}{\frac{1}{|\tau|}}\sum_{n\in\tau}[1;\mathbf{u}(n);\mathbf{x}(n)]=\mathbf{W}^{\mathrm{out}}\,\Sigma\mathbf{x}，$$
- en: $$(27.15)$$  $$(27.16)$$
  id: totrans-8129
  prefs: []
  type: TYPE_NORMAL
  zh: $$(27.15)$$  $$(27.16)$$
- en: where Σx is a shorthand for [1; u(n); x(n)] time-averaged over τ. The form
  id: totrans-8130
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Σx 是[1; u(n); x(n)]在τ上时间平均的简写形式。该形式
- en: (27.16) is a more efficient way to compute Σy, since there is only one multiplication
    with Wout.
  id: totrans-8131
  prefs: []
  type: TYPE_NORMAL
  zh: (27.16)是一种更有效的计算Σy的方法，因为只需与Wout进行一次乘法运算。
- en: More importantly, (27.16) can be used to make training more efficient and powerful.
    For a given short sequence u(n), instead of finding Wout that minimizes E (ytarget(n),
    y(n)) for every n ∈ τ, it is better to find the one that minimizes the error between
    the time-averaged values E(ytarget, Σy). In this case y(n) (which is not actually
    explicitly computed) is allowed to deviate from ytarget(n) as long as the time-averaged
    Σy is close to ytarget. Here ytarget ≡ Σytarget = ytarget(n) = const for a single
    short sequence.
  id: totrans-8132
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，(27.16)可以用来提高训练的效率和能力。对于给定的短序列u(n)，与其在每个n ∈ τ中找到最小化E(ytarget(n), y(n))的Wout相比，找到最小化时间平均值E(ytarget,
    Σy)的Wout更为合适。在这种情况下，y(n)（实际上并没有被明确计算）可以偏离ytarget(n)，只要时间平均的Σy接近ytarget即可。这里ytarget
    ≡ Σytarget = ytarget(n) = const，对于单个短序列。
- en: To classify sequences, train and use readouts from time-averaged activations
    Σx (27.16), instead of x(n).
  id: totrans-8133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分类序列，训练并使用从时间平均激活Σx (27.16)中读取的结果，而不是x(n)。
- en: Note that weighting is still possible both among the short sequences and inside
    each sequence over the intervals τ, using weighted average instead of a simple
    one. Actually, weighting over τ is often recommendable, emphasizing the ending
    of the sequence where the whole information to make the classification decision
    has been fed into the reservoir.
  id: totrans-8134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在短序列之间以及每个序列内的区间τ中，仍然可以使用加权，采用加权平均而不是简单平均。实际上，通常推荐在τ上加权，强调序列结束时的信息，此时所有信息已输入到储备池中以做出分类决策。
- en: To retain information from different times in the short sequence, weighted averages
    Σ1x*,...,Σ*kx over several time intervals τ1*,...,τ*k during the short sequence
    can be computed and concatenated into an extended state Σ∗x = [Σ1x; ... ; Σkx].
    This extended state Σ∗x can be used instead of Σx for an even more powerful classification.
    In this case Wout is also extended to Wout
  id: totrans-8135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保留短序列中不同时间的信息，可以计算并连接在短序列中的多个时间区间τ1*,...,τ*k上的加权平均Σ1x*,...,Σ*kx，形成扩展状态Σ∗x
    = [Σ1x; ... ; Σkx]。这个扩展状态Σ∗x可以替代Σx，用于更强大的分类。在这种情况下，Wout也扩展为Wout。
- en: ∗ ∈
  id: totrans-8136
  prefs: []
  type: TYPE_NORMAL
  zh: ∗ ∈
- en: RNy×k·(1+Nu+Nx).
  id: totrans-8137
  prefs: []
  type: TYPE_NORMAL
  zh: RNy×k·(1+Nu+Nx)。
- en: Concatenate weighted time-averages over different intervals to read out from
    for an even more powerful classification.
  id: totrans-8138
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同区间上连接加权时间平均值，以实现更强大的分类。
- en: Since the short sequences are typically of different lengths (the advantage
    of using temporal classification methods), the intervals τ1*,...,τ*k should be
    scaled to match the length of each sequence.
  id: totrans-8139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于短序列通常具有不同的长度（使用时间分类方法的优势），因此区间τ1*,...,τ*k应缩放以匹配每个序列的长度。
- en: The techniques so far described in this section effectively reduce the time
    series classification to a static data classification problem by reducing the
    variable-length inputsu(n)to fixed-size feature vectorsΣ∗x ∈ Rk·(1+Nu+Nx). There
    are many powerful machine learning methods available to solve the static classification
    problem that can be employed at this point, such as logistic regression or maximum
    margin classifiers. See, e.g., [3] for different options. These methods define
    the error function differently and offer different, mostly iterative, optimization
    algorithms.
  id: totrans-8140
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中描述的技术通过将可变长度输入u(n)减少到固定大小的特征向量Σ∗x ∈ Rk·(1+Nu+Nx)，有效地将时间序列分类问题转化为静态数据分类问题。此时可以使用许多强大的机器学习方法来解决静态分类问题，例如逻辑回归或最大边际分类器。有关不同选项，请参见例如[3]。这些方法以不同的方式定义误差函数，并提供不同的、主要是迭代的优化算法。
- en: Different powerful classification methods for static data can be employed as
    the readout from the time-averaged activations Σ∗x.
  id: totrans-8141
  prefs: []
  type: TYPE_NORMAL
  zh: 可以采用不同的强大分类方法来处理从时间平均激活Σ∗x的读出。
- en: Among others, the same regression methods as for temporal data can be used to
    train a linear readout y = Wout
  id: totrans-8142
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，针对时间数据的相同回归方法可以用于训练线性读出y = Wout。
- en: ∗ Σ∗x and decide the class by maximum as in (27.14). In this case, for every
    short sequence u(n) only one pair of vectors ytarget and Σ∗x is collected into
    Ytarget and X respectively for training by (27.9)
  id: totrans-8143
  prefs: []
  type: TYPE_NORMAL
  zh: ∗ Σ∗x，并通过最大值来决定类别，如(27.14)所示。在这种情况下，对于每个短序列u(n)，仅收集一对向量ytarget和Σ∗x到Ytarget和X中，分别用于通过(27.9)进行训练。
- en: or (27.12). Since this reduces training data points from the total number of
    time steps to the number of short sequences, precautions against overfitting should
    be taken. Such regression training for classification has an advantage of the
    singleshot closed form solution, however, it is not optimal because it does not
    directly optimize the correct classification rates.
  id: totrans-8144
  prefs: []
  type: TYPE_NORMAL
  zh: 或者 (27.12)。由于这将训练数据点从总时间步数减少到短序列的数量，因此应采取防止过拟合的预防措施。这种用于分类的回归训练具有一次性闭合形式解的优势，但它并不是最优的，因为它并未直接优化正确分类率。
- en: For temporal pattern recognition tasks in a long sequence (i.e., detection plus
    classification), ytarget(n) should be designed cleverly. The shapes, durations,
    and delays of the signals in ytarget(n) indicating patterns in u(n) are also parameters
    that have to be optimized; as well as algorithms producing the final recognition
    (in the form of discrete symbol sequences or annotations) from the continuous
    signals y(n). But this goes beyond the scope of this paper. Alternatively, dynamic
    programming methods (such as Viterbi algorithm) can be used for trainable recognizers
    at the output layer, see [10].
  id: totrans-8145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于长序列中的时间模式识别任务（即检测加分类），ytarget(n)应设计得巧妙。ytarget(n)中指示u(n)中模式的信号的形状、持续时间和延迟也是必须优化的参数；以及从连续信号y(n)生成最终识别的算法（以离散符号序列或注释的形式）。但这超出了本文的范围。或者，可以使用动态规划方法（如维特比算法）用于输出层的可训练识别器，见[10]。
- en: 27.4.8 Online Learning
  id: totrans-8146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.4.8 在线学习
- en: Some applications require online model adaptation, e.g., [19]. In such cases
    the process generating the data is often not assumed to be stationary and is tracked
    by the constantly adapting model. Wout here acts as an adaptive linear combiner.
  id: totrans-8147
  prefs: []
  type: TYPE_NORMAL
  zh: 一些应用需要在线模型适应，例如[19]。在这种情况下，生成数据的过程通常不假定是平稳的，并通过不断调整的模型进行跟踪。这里的Wout作为一个自适应线性组合器。
- en: The simplest way to train Wout is the method known as the Least Mean Squares
    (LMS) algorithm [9], it has many extensions and modifications. It is a stochastic
    gradient descent algorithm which at every time step n changes Wout in the direction
    of minimizing the instantaneous squared error ytarget(n) − y(n)
  id: totrans-8148
  prefs: []
  type: TYPE_NORMAL
  zh: 训练Wout的最简单方法是称为最小均方（LMS）算法的方法[9]，它有许多扩展和修改。它是一种随机梯度下降算法，在每个时间步n中，Wout沿着最小化瞬时平方误差ytarget(n)
    − y(n)的方向变化。
- en: '2.'
  id: totrans-8149
  prefs: []
  type: TYPE_NORMAL
  zh: '2.'
- en: LMS is a first-order gradient descent method, locally approximating the error
    surface with a hyperplane. This approximation is poor then curvature of the error
    surface is very different in different directions, which is signified by large
    eigenvalue spreads of XXT. In such a situation the convergence performance of
    LMS is unfortunately severely impaired.
  id: totrans-8150
  prefs: []
  type: TYPE_NORMAL
  zh: LMS是一种一阶梯度下降方法，局部地用超平面近似误差面。当误差面的曲率在不同方向上差异很大时，这种近似效果较差，这通过XXT的大特征值分散表示。在这种情况下，LMS的收敛性能不幸受到严重损害。
- en: 'An alternative linear readout learning to LMS, known in linear signal processing
    as the *Recursive Least Squares* (RLS) algorithm, is insensitive to the detrimental
    effects of eigenvalue spread and boasts a much faster convergence. It explicitly
    at each time step n minimizes a square error that is exponentially discounted
    going back in time:'
  id: totrans-8151
  prefs: []
  type: TYPE_NORMAL
  zh: 与LMS相比，另一种线性读出学习被称为*递归最小二乘*（RLS）算法，它对特征值分散的有害影响不敏感，并且具有更快的收敛速度。它在每个时间步n明确最小化一个向后时间指数折扣的平方误差：
- en: $$E({\bf y},{\bf y}^{\rm target},n)=\frac{1}{N_{\rm y}}\sum_{i=1}^{N_{\rm y}}\sum_{j=1}^{n}\lambda^{n-j}\left(y_{i}(j)-y_{i}^{\rm
    target}(j)\right)^{2},\tag{27.17}$$
  id: totrans-8152
  prefs: []
  type: TYPE_NORMAL
  zh: $$E({\bf y},{\bf y}^{\rm target},n)=\frac{1}{N_{\rm y}}\sum_{i=1}^{N_{\rm y}}\sum_{j=1}^{n}\lambda^{n-j}\left(y_{i}(j)-y_{i}^{\rm
    target}(j)\right)^{2},\tag{27.17}$$
- en: where 0 < λ ≤ 1 is the error "forgetting" parameter. This weighting is not unlike
    the one discussed in Section 27.4.6 where s(n)(j) = λn−j at time step n. RLS can
    be seen as a method for minimizing (27.17) at each time step n similar to WienerHopf
    (27.11), but optimized by keeping and updating the estimate of (XXT)−1 from time
    n − 1 instead of recomputing it from scratch. The downside of RLS is it being
    computationally more expensive (quadratic in number of weights instead of linear
    like LMS) and notorious for numerical stability issues. Demonstrations of RLS
    for ESNs are presented in [19, 18]. A careful and comprehensive comparison of
    variants of RLS as ESN readouts is carried out in a Master's thesis [25], which
    may be helpful for practitioners.
  id: totrans-8153
  prefs: []
  type: TYPE_NORMAL
  zh: 其中0 < λ ≤ 1是误差“遗忘”参数。这种加权与27.4.6节讨论的情况相似，其中s(n)(j) = λn−j在时间步n处。RLS可以被视为在每个时间步n最小化（27.17）的方法，类似于WienerHopf（27.11），但通过保持和更新（XXT)−1的估计，而不是从头开始重新计算，来进行优化。RLS的缺点是计算开销更大（相对于LMS是二次的，而非线性的），且以数值稳定性问题而闻名。RLS在ESNs中的演示见于[19,
    18]。对RLS在ESN读出变体的仔细全面比较在一篇硕士论文中进行[25]，这可能对从业者有帮助。
- en: The BackPropagation-DeCorrelation (BPDC) [44] and FORCE [46] learning algorithms
    discussed in Section 27.5.3 are two other powerful methods for online training
    of single-layer readouts with feedback connections from the reservoirs.
  id: totrans-8154
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播-去相关（BPDC）[44]和FORCE[46]学习算法在27.5.3节中讨论，是两种强大的方法，用于在线训练带有反馈连接的单层读出。
- en: 27.4.9 Pointers To Readouts Extensions
  id: totrans-8155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.4.9 指针到读出扩展
- en: There are also alternative ways of training outputs from the reservoirs suggested
    in the literature, e.g., Gaussian process [4], copula [5], or Support Vector Machine
    [42] style outputs. See [29] and updated in Chapter 2 of [30] for an overview.
  id: totrans-8156
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中还建议了从储层训练输出的替代方法，例如，高斯过程[4]、copula[5]或支持向量机[42]风格的输出。有关概述，请参见[29]和[30]第二章中的更新。
- en: 27.5 Dealing With Output Feedbacks 27.5.1 Output Feedbacks
  id: totrans-8157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.5 处理输出反馈 27.5.1 输出反馈
- en: Even if the reservoir is kept fixed, for some tasks the trained readouts are
    fed back to the reservoir and thus the training process changes its dynamics.
    In other words, a recurrence exists between the reservoir and the trained readout.
    Pattern generation is a typical example of such task. This can be realized in
    two ways. Either by feedback connections Wfb ∈ RNx×Ny from the output to the reservoir,
    replacing (27.2) with
  id: totrans-8158
  prefs: []
  type: TYPE_NORMAL
  zh: 即使储层保持固定，对于某些任务，训练好的读出被反馈到储层，从而训练过程的动态发生变化。换句话说，储层和训练的读出之间存在递归。模式生成就是这样任务的典型例子。这可以通过两种方式实现。要么通过输出到储层的反馈连接Wfb
    ∈ RNx×Ny，将（27.2）替换为
- en: $${\bar{\mathbf{x}}}(n)=\operatorname{tanh}\left(\mathbf{W}^{\mathrm{in}}[1;\mathbf{u}(n)]+\mathbf{W}\mathbf{x}(n-1)+\mathbf{W}^{\mathrm{fb}}\mathbf{y}(n-1)\right),$$
  id: totrans-8159
  prefs: []
  type: TYPE_NORMAL
  zh: $${\bar{\mathbf{x}}}(n)=\operatorname{tanh}\left(\mathbf{W}^{\mathrm{in}}[1;\mathbf{u}(n)]+\mathbf{W}\mathbf{x}(n-1)+\mathbf{W}^{\mathrm{fb}}\mathbf{y}(n-1)\right),$$
- en: $$(27.18)$$
  id: totrans-8160
  prefs: []
  type: TYPE_NORMAL
  zh: $$(27.18)$$
- en: 'or by looping the output y(n − 1) as an input u(n) for the next update step
    n in (27.2), in effect turning a trained one step predictor ESN into a pattern
    generator. Note that these two options are equivalent and are just a matter of
    notation: u(n) and Win instead of y(n − 1) and Wfb, respectively. The same principles
    thus apply to producing Wfb as to Win. In some cases, however, both external input
    and output feedback can be present.'
  id: totrans-8161
  prefs: []
  type: TYPE_NORMAL
  zh: 或者通过将输出 y(n − 1) 作为下一个更新步骤 n 的输入 u(n) 循环（27.2），实际上将训练好的单步预测器 ESN 转变为模式生成器。注意，这两种选项是等效的，仅仅是符号的不同：分别用
    u(n) 和 Win 代替 y(n − 1) 和 Wfb。因此，产生 Wfb 的原理与 Win 相同。然而，在某些情况下，外部输入和输出反馈都可以存在。
- en: This extends the power of RC, because it no longer relies on fixed random input-driven
    dynamics to construct the output, but the dynamics are adapted to the task. This
    power has its price, because stability issues arise here.
  id: totrans-8162
  prefs: []
  type: TYPE_NORMAL
  zh: 这扩展了 RC 的能力，因为它不再依赖固定的随机输入驱动动态来构建输出，而是动态适应任务。这种能力有其代价，因为在这里会出现稳定性问题。
- en: Use output feedbacks to the reservoir only if they are necessary for the task.
  id: totrans-8163
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在任务必要时使用输出反馈到水库。
- en: This may include tasks that simply cannot be learned well enough without feedbacks.
    Feedbacks enable reservoirs to achieve universal computational capabilities [33]
    and can in practice be beneficial even where they are not an integral part of
    the task [28].
  id: totrans-8164
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能包括一些在没有反馈的情况下根本无法学得足够好的任务。反馈使水库能够实现通用计算能力 [33]，并且在实际上，即使它们不是任务的固有部分，也能带来好处
    [28]。
- en: 'In order to avoid falling prey to the same difficulties as with full RNN training
    algorithms, two strategies are used in RC when learning outputs with feedbacks:'
  id: totrans-8165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在全 RNN 训练算法中遇到相同的困难，在 RC 中学习带反馈的输出时使用两种策略：
- en: '- Breaking the feedback loop during the training, Section 27.5.2;'
  id: totrans-8166
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在训练期间打破反馈循环，第 27.5.2 节；'
- en: '- Adapting Wout online with specialized algorithms in the presence of real
    feedbacks, Section 27.5.3.'
  id: totrans-8167
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在存在真实反馈的情况下，使用专门算法在线调整 Wout，见第 27.5.3 节。'
- en: 27.5.2 Teacher Forcing
  id: totrans-8168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.5.2 教师强制
- en: The first strategy is to disengage the recurrent relationship between the reservoir
    and the readout using *teacher forcing* and treat output learning as a feedforward
    task. This is done by feeding the desired output ytarget(n − 1) through the feedback
    connections Wfb in (27.18) instead of the real output y(n − 1) while learning
    (Figure 27.2a). The target signal ytarget(n) "bootstraps" the learning process
    and if the output is learned with high precision (i.e., y(n) ≈ ytarget(n)),
  id: totrans-8169
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种策略是使用 *教师强制* 解除水库与读取输出之间的递归关系，并将输出学习视为前馈任务。这是通过在学习过程中通过反馈连接 Wfb 送入所需输出 ytarget(n
    − 1)，而不是实际输出 y(n − 1) 来实现的（图 27.2a）。目标信号 ytarget(n) 为学习过程提供“启动”，如果输出以高精度学习（即 y(n)
    ≈ ytarget(n)），
- en: the recurrent system runs much in the same way with the real y(n) in feedbacks
    after training as it did with ytarget(n) during training (Figure 27.2b). In a
    pure
  id: totrans-8170
  prefs: []
  type: TYPE_NORMAL
  zh: 递归系统在训练后与真实的 y(n) 在反馈中运行方式类似于在训练期间与 ytarget(n) 的运行方式（图 27.2b）。在纯粹的
- en: '![668_image_0.png](668_image_0.png)'
  id: totrans-8171
  prefs: []
  type: TYPE_IMG
  zh: '![668_image_0.png](668_image_0.png)'
- en: pattern generator setup with no additional inputs, u(n) and Win may not be present
    at all - after training the ESN is run to autonomously generate a pattern, using
    teacher forcing initially to start the pattern. As noted before, this is equivalent
    to training a one time step predictor without feedbacks Wfb and looping its output
    y(n − 1) as input u(n) through Win.
  id: totrans-8172
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有额外输入的情况下设置模式生成器，u(n) 和 Win 可能完全不存在 - 在训练后，ESN 被运行以自主生成模式，最初使用教师强制启动模式。如前所述，这相当于训练一个不带反馈的单步预测器
    Wfb，并将其输出 y(n − 1) 作为输入 u(n) 通过 Win 循环。
- en: (a) Training with teacher forcing (b) Running in a generative mode Fig. 27.2.
    An ESN with output feedbacks trained with teacher forcing For simple tasks, feed
    ytarget(n) instead of y(n) in
  id: totrans-8173
  prefs: []
  type: TYPE_NORMAL
  zh: （a）使用教师强制进行训练（b）以生成模式运行 图 27.2。一个经过教师强制训练的具有输出反馈的 ESN 对于简单任务，将 ytarget(n) 而非
    y(n) 送入
- en: (27.18) while learning to break the recurrence.
  id: totrans-8174
  prefs: []
  type: TYPE_NORMAL
  zh: （27.18）在学习时打破递归。
- en: This way Wout can be learned in the same efficient batch mode by linear regression,
    as explained before.
  id: totrans-8175
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，Wout 可以通过线性回归以同样高效的批处理模式学习，如前所述。
- en: Teacher forcing applied to speedup error backpropagation RNN training is also
    discussed in chapter [54] of this book.
  id: totrans-8176
  prefs: []
  type: TYPE_NORMAL
  zh: 本书第[54]章中也讨论了应用教师强制以加速错误反向传播RNN训练的方法。
- en: There are some caveats here. The approach works very well if the output can
    be learned precisely [16]. However, if this is not the case, the distorted feedback
    leads to an even more distorted output and feedback at the next time step, and
    so on, with the actual generated output y(n) quickly diverging from the desired
    ytarget(n). Even with well-learned outputs the dynamical stability of the autonomous
    running system is often an issue.
  id: totrans-8177
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些注意事项。如果输出可以精确学习，这种方法效果非常好[16]。然而，如果情况并非如此，扭曲的反馈将导致下一个时间步的输出和反馈更加扭曲，实际生成的输出y(n)很快就会偏离期望的ytarget(n)。即使输出已学习良好，自主运行系统的动态稳定性也常常是一个问题。
- en: Stability with Feedbacks. In both cases regularization of the output by ridge
    regression or/and noise immunization, as explained in Section 27.4.2, is the key
    to success.
  id: totrans-8178
  prefs: []
  type: TYPE_NORMAL
  zh: 反馈的稳定性。在这两种情况下，通过岭回归或/和噪声免疫来对输出进行正则化，如第27.4.2节所解释的，是成功的关键。
- en: Regularization by ridge regression or noise is crucial to make teacher-forced
    feedbacks stable.
  id: totrans-8179
  prefs: []
  type: TYPE_NORMAL
  zh: 通过岭回归或噪声进行正则化对于使教师强制反馈稳定至关重要。
- en: Some additional options to the ones in Section 27.4.2 are available here. One
    is adding scaled noise to the forced teacher signal ytarget(n), emulating an imperfectly
    learned ytarget(n) by y(n) and making the network robust to this. In fact, a readout
    can be trained to ignore some inputs or feedbacks altogether by feeding strong
    noise into them during training [20].
  id: totrans-8180
  prefs: []
  type: TYPE_NORMAL
  zh: 除第27.4.2节中的选项外，还有一些其他选项可供选择。一个是向强制教师信号ytarget(n)添加缩放噪声，通过y(n)模拟不完美学习的ytarget(n)，并使网络对这种情况具有鲁棒性。实际上，通过在训练期间向输入或反馈中注入强噪声，可以训练读取器完全忽略某些输入或反馈[20]。
- en: Another is doing training in several iterations and feeding back the signals
    that are in between the perfect ytarget(n) and the actual y(n) obtained from the
    previous iteration. For example, a one time step prediction of the signal by the
    ESN (as opposed to running with real feedbacks) can be used as teacher forcer
    for the next iteration of training [19]. This way the model learns to recover
    from the directions of deviations from the correct signal that it actually produces,
    not from just random ones as in the case with noise; while at the same time the
    teacher signal does not diverge from the target too far.
  id: totrans-8181
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是在多个迭代中进行训练，并反馈介于完美ytarget(n)和从上一次迭代获得的实际y(n)之间的信号。例如，ESN对信号的一次时间步预测（与真实反馈运行相对）可以用作下一次训练迭代的教师强制[19]。这样，模型学习从它实际产生的信号的偏差方向中恢复，而不仅仅是从噪声的随机偏差中恢复；同时教师信号也不会过于偏离目标。
- en: Another recently proposed option is to also regularize the recurrent connections
    W themselves. A one-shot relearning of W with regularization (similar to ridge
    regression (27.9) for Wout) to produce the same x(n), as the one from the initially
    randomly generated W, reduces the recurrent connection strengths and helps making
    the ESN generator more stable [38, 37].
  id: totrans-8182
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种最近提出的选项是对递归连接W本身进行正则化。对W进行一次性重新学习并进行正则化（类似于Wout的岭回归(27.9)），以产生与最初随机生成的W相同的x(n)，可以降低递归连接的强度，并帮助使ESN生成器更加稳定[38,
    37]。
- en: 27.5.3 Online Learning With Real Feedbacks
  id: totrans-8183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.5.3 在线学习与真实反馈
- en: The second strategy to deal with output feedbacks in ESNs is using online (instead
    of one-shot) learning algorithms to train the outputs Wout while the feedbacks
    are enabled and feed back the (yet imperfectly) learned output, not the teacher
    signal. This way the model learns to stabilize itself in the real generative setting.
  id: totrans-8184
  prefs: []
  type: TYPE_NORMAL
  zh: 处理ESN中的输出反馈的第二种策略是使用在线（而非一次性）学习算法在启用反馈的情况下训练输出Wout，并反馈（尚未完美学习的）输出，而不是教师信号。这样模型可以在真实生成设置中学习自我稳定。
- en: General purpose online learning algorithms, such as discussed in Section 27.4.8,
    can be used for this. However, there exist a couple of online RC learning algorithms
    that are specialized in training outputs with feedbacks, and in fact would not
    work without them.
  id: totrans-8185
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用第27.4.8节中讨论的通用在线学习算法。然而，确实存在一些在线RC学习算法，专门用于训练带反馈的输出，实际上如果没有它们则无法工作。
- en: BackPropagation-DeCorrelation (BPDC) [44] is such a highly optimized RC
  id: totrans-8186
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播去相关（BPDC）[44]是一种高度优化的RC。
- en: online learning algorithm which runs with a linear time complexity in the number
    of connections. The algorithm is said to be insensitive to reservoir settings
    and capable of tracking quickly changing signals. As a downside of the latter
    feature, the trained network forgets the previously seen data and is highly biased
    by the recent data. Some remedies for reducing this effect are reported in [45].
  id: totrans-8187
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习算法以线性时间复杂度运行，依赖于连接数。该算法被认为对储层设置不敏感，并能够跟踪快速变化的信号。作为这种特性的一个缺点，训练后的网络会忘记之前看到的数据，并且会受到近期数据的高度偏见。[45]中报道了一些减少这一影响的补救措施。
- en: A recent RC approach named *FORCE* learning uses the RLS (Section 27.4.8)
  id: totrans-8188
  prefs: []
  type: TYPE_NORMAL
  zh: 最近一种名为*FORCE*学习的RC方法使用RLS（第27.4.8节）
- en: online learning algorithm to vigorously adapt Wout in the presence of the real
    feedbacks [46]. By the initial fast and strong adaptation of Wout the feedbacks
    y(n) are kept close to the desired ytarget(n) already from the beginning of the
    learning process, similar to teacher forcing. The algorithm benefits from initial
    spontaneous chaotic activations inside the reservoir which are then subdued by
    the feedbacks. It appears that FORCE learning is well suited to yield very stable
    and accurate neural pattern generators.
  id: totrans-8189
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习算法能够在真实反馈存在的情况下积极适应Wout [46]。通过对Wout的初始快速和强适应，反馈y(n)从学习过程开始就保持接近期望的ytarget(n)，类似于教师强制法。该算法受益于储层内部初始自发的混沌激活，这些激活随后受到反馈的抑制。看起来，FORCE学习非常适合生成非常稳定和准确的神经模式生成器。
- en: 27.6 Summary And Implementations
  id: totrans-8190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 27.6 摘要与实现
- en: We have presented many practical aspects for successfully applying ESNs. Some
    of them are not universal and should be filtered depending on a particular task.
    They are also not the only possible approaches, and can most likely be improved
    upon. They collect, however, the best practices accumulated in the field over
    the ten years from its start, and should serve well as guiding principles for
    ESN researchers and practitioners.
  id: totrans-8191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了许多成功应用ESN的实际方面。其中一些不是通用的，应根据特定任务进行筛选。它们也不是唯一可能的方法，并且很可能可以进一步改进。然而，它们汇集了自该领域开始十年来积累的最佳实践，应作为ESN研究人员和从业者的指导原则。
- en: Implementing an ESN is relatively straightforward - minimalistic one-page self-contained
    code examples in several programming languages are available through http://reservoir-computing.org/software/minimal.
    There is also a number of ready-made and expandable software libraries available
    which incorporate many of the techniques described here. A collection of open
    source RC toolboxes in different programming languages and varying degrees of
    sophistication can be found at http://reservoir-computing.org/software.
  id: totrans-8192
  prefs: []
  type: TYPE_NORMAL
  zh: 实现一个ESN相对简单——可以通过 [http://reservoir-computing.org/software/minimal](http://reservoir-computing.org/software/minimal)
    获取几种编程语言中的简约一页自包含代码示例。还有许多现成的可扩展软件库，这些库结合了此处描述的许多技术。可以在 [http://reservoir-computing.org/software](http://reservoir-computing.org/software)
    找到不同编程语言和不同复杂程度的开源RC工具箱集合。
- en: The most comprehensive of them is the Oger toolbox in Python http://reservoir-computing.org/oger.
  id: totrans-8193
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最全面的是Python中的Oger工具箱 [http://reservoir-computing.org/oger](http://reservoir-computing.org/oger)。
- en: The http://reservoir-computing.org website is an overall good hub of reservoir
    computing related resources, where new people can also register and contribute.
  id: totrans-8194
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://reservoir-computing.org](http://reservoir-computing.org) 网站是一个整体上良好的储层计算相关资源中心，新人也可以注册并贡献内容。'
- en: Acknowledgments. The author was supported through funds from the European FP7
    projects ORGANIC and AMARSi. He would like to specially thank Herbert Jaeger for
    proof-reading this chapter and valuable suggestions, as well as the anonymous
    reviewers, and the whole reservoir computing community on whose collective wisdom
    this chapter is to some extent based.
  id: totrans-8195
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢。作者得到了来自欧洲第七框架计划（FP7）项目ORGANIC和AMARSi的资金支持。他特别感谢Herbert Jaeger对本章的校对和宝贵建议，以及匿名评审和整个储层计算社区的集体智慧，在某种程度上，本章基于此。
- en: '[1] Bengio, Y., Simard, P., Frasconi, P.: Learning long-term dependencies with
    gradient descent is difficult. IEEE Transactions on Neural Networks 5(2), 157–166'
  id: totrans-8196
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Bengio, Y., Simard, P., Frasconi, P.: 使用梯度下降学习长期依赖关系是困难的。IEEE神经网络交易 5(2),
    157–166'
- en: (1994)
  id: totrans-8197
  prefs: []
  type: TYPE_NORMAL
  zh: （1994）
- en: '[2] Bergstra, J.S., Bardenet, R., Bengio, Y., Kégl, B.: Algorithms for hyper-parameter
    optimization. In: Shawe-Taylor, J., Zemel, R.S., Bartlett, P., Pereira, F.C.N.,'
  id: totrans-8198
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Bergstra, J.S., Bardenet, R., Bengio, Y., Kégl, B.: 超参数优化的算法。见：Shawe-Taylor,
    J., Zemel, R.S., Bartlett, P., Pereira, F.C.N.,'
- en: Weinberger, K.Q. (eds.) Advances in Neural Information Processing Systems 23
  id: totrans-8199
  prefs: []
  type: TYPE_NORMAL
  zh: Weinberger, K.Q.（主编）《神经信息处理系统进展 23》
- en: (NIPS 2010), pp. 2546–2554 (2011)
  id: totrans-8200
  prefs: []
  type: TYPE_NORMAL
  zh: (NIPS 2010)，pp. 2546–2554 (2011)
- en: '[3] Bishop, C.M.: Pattern Recognition and Machine Learning (Information Science
    and Statistics). Springer-Verlag New York, Inc., Secaucus (2006)'
  id: totrans-8201
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Bishop, C.M.: 模式识别与机器学习（信息科学与统计学）。Springer-Verlag New York, Inc., Secaucus
    (2006)'
- en: '[4] Chatzis, S.P., Demiris, Y.: Echo state Gaussian process. IEEE Transactions
    on Neural Networks 22(9), 1435–1445 (2011)'
  id: totrans-8202
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Chatzis, S.P., Demiris, Y.: 回声状态高斯过程。IEEE神经网络交易 22(9), 1435–1445 (2011)'
- en: '[5] Chatzis, S.P., Demiris, Y.: The copula echo state network. Pattern Recognition
    45(1), 570–577 (2012)'
  id: totrans-8203
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Chatzis, S.P., Demiris, Y.: Copula 回声状态网络。模式识别 45(1), 570–577 (2012)'
- en: '[6] Daukantas, S., Lukoševičius, M., Marozas, V., Lukoševičius, A.: Comparison
    of'
  id: totrans-8204
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Daukantas, S., Lukoševičius, M., Marozas, V., Lukoševičius, A.: 比较'
- en: '"black box" and "gray box" methods for lost data reconstruction in multichannel
    signals. In: Proceedings of the 14th International Conference "Biomedical Engineering",
    Kaunas, pp. 135–138 (2010)'
  id: totrans-8205
  prefs: []
  type: TYPE_NORMAL
  zh: 在多通道信号中用于丢失数据重建的“黑箱”和“灰箱”方法。见：第14届国际会议“生物医学工程”会议论文集，考纳斯，pp. 135–138 (2010)
- en: '[7] Dominey, P.F., Ramus, F.: Neural network processing of natural language:
    I. sensitivity to serial, temporal and abstract structure of language in the infant.
    Language and Cognitive Processes 15(1), 87–127 (2000)'
  id: totrans-8206
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Dominey, P.F., Ramus, F.: 自然语言的神经网络处理：I. 对婴儿语言的序列、时间和抽象结构的敏感性。语言与认知过程 15(1),
    87–127 (2000)'
- en: '[8] Doya, K.: Bifurcations in the learning of recurrent neural networks. In:
    Proceedings of IEEE International Symposium on Circuits and Systems, vol. 6, pp.
    2777–2780 (1992)'
  id: totrans-8207
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Doya, K.: 递归神经网络学习中的分岔。在：IEEE国际电路与系统研讨会会议论文集，第6卷，pp. 2777–2780 (1992)'
- en: '[9] Farhang-Boroujeny, B.: Adaptive Filters: Theory and Applications. Wiley
    (1998)'
  id: totrans-8208
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Farhang-Boroujeny, B.: 自适应滤波器：理论与应用。Wiley (1998)'
- en: '[10] Graves, A.: Supervised Sequence Labelling with Recurrent Neural Networks.
    PhD'
  id: totrans-8209
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Graves, A.: 使用递归神经网络的监督序列标注。博士论文'
- en: thesis, Technical University Munich, Munich, Germany (2008)
  id: totrans-8210
  prefs: []
  type: TYPE_NORMAL
  zh: 论文，慕尼黑技术大学，德国慕尼黑 (2008)
- en: '[11] Hermans, M., Schrauwen, B.: Memory in reservoirs for high dimensional
    input.'
  id: totrans-8211
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Hermans, M., Schrauwen, B.: 高维输入的水库中的记忆。'
- en: 'In: Proceedings of the IEEE International Joint Conference on Neural Networks
    (IJCNN 2010), pp. 1–7 (2010)'
  id: totrans-8212
  prefs: []
  type: TYPE_NORMAL
  zh: 在：IEEE国际神经网络联合会议论文集 (IJCNN 2010)，pp. 1–7 (2010)
- en: '[12] Hermans, M., Schrauwen, B.: Recurrent kernel machines: Computing with
    infinite echo state networks. Neural Computation 24(1), 104–133 (2012)'
  id: totrans-8213
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Hermans, M., Schrauwen, B.: 递归核机器：与无限回声状态网络计算。神经计算 24(1), 104–133 (2012)'
- en: '[13] Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation
    9(8), 1735–1780 (1997)'
  id: totrans-8214
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Hochreiter, S., Schmidhuber, J.: 长短期记忆。神经计算 9(8), 1735–1780 (1997)'
- en: '[14] Holzmann, G., Hauser, H.: Echo state networks with filter neurons and
    a delay and sum readout. Neural Networks 23(2), 244–256 (2010)'
  id: totrans-8215
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Holzmann, G., Hauser, H.: 具有滤波神经元和延迟和求和读取的回声状态网络。神经网络 23(2), 244–256 (2010)'
- en: '[15] Ilies, I., Jaeger, H., Kosuchinas, O., Rincon, M., Šak˙enas, V., Vaškevičius,
    N.:'
  id: totrans-8216
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Ilies, I., Jaeger, H., Kosuchinas, O., Rincon, M., Šak˙enas, V., Vaškevičius,
    N.：'
- en: 'Stepping forward through echoes of the past: forecasting with echo state networks.
    Short report on the winning entry to the NN3 financial forecasting competition'
  id: totrans-8217
  prefs: []
  type: TYPE_NORMAL
  zh: 通过过去的回声向前迈进：使用回声状态网络进行预测。关于NN3金融预测竞赛获胜作品的简短报告
- en: (2007),
  id: totrans-8218
  prefs: []
  type: TYPE_NORMAL
  zh: (2007),
- en: http://www.neural-forecasting-competition.com/
  id: totrans-8219
  prefs: []
  type: TYPE_NORMAL
  zh: http://www.neural-forecasting-competition.com/
- en: downloads/NN3/methods/27-NN3_Herbert_Jaeger_report.pdf
  id: totrans-8220
  prefs: []
  type: TYPE_NORMAL
  zh: downloads/NN3/methods/27-NN3_Herbert_Jaeger_report.pdf
- en: '[16] Jaeger, H.: The "echo state" approach to analysing and training recurrent
    neural networks. Technical Report GMD Report 148, German National Research Center
    for Information Technology (2001)'
  id: totrans-8221
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Jaeger, H.: “回声状态”方法分析和训练递归神经网络。技术报告 GMD 报告 148，德国国家信息技术研究中心 (2001)'
- en: '[17] Jaeger, H.: Short term memory in echo state networks. Technical Report
    GMD'
  id: totrans-8222
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Jaeger, H.: 回声状态网络中的短期记忆。技术报告 GMD'
- en: Report 152, German National Research Center for Information Technology (2002)
  id: totrans-8223
  prefs: []
  type: TYPE_NORMAL
  zh: 报告 152，德国国家信息技术研究中心 (2002)
- en: '[18] Jaeger, H.: Adaptive nonlinear system identification with echo state networks.
    In:'
  id: totrans-8224
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Jaeger, H.: 使用回声状态网络的自适应非线性系统识别。在：'
- en: Advances in Neural Information Processing Systems 15 (NIPS 2002), pp. 593–600.
    MIT Press, Cambridge (2003)
  id: totrans-8225
  prefs: []
  type: TYPE_NORMAL
  zh: 神经信息处理系统进展 15 (NIPS 2002)，pp. 593–600。MIT出版社，剑桥 (2003)
- en: '[19] Jaeger, H., Haas, H.: Harnessing nonlinearity: predicting chaotic systems
    and saving energy in wireless communication. Science 304(5667), 78–80 (2004)'
  id: totrans-8226
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Jaeger, H., Haas, H.: 利用非线性：预测混沌系统并节省无线通信中的能量。科学 304(5667), 78–80 (2004)'
- en: '[20] Jaeger, H.: Generating exponentially many periodic attractors with linearly
    growing echo state networks. Technical Report No. 3, Jacobs University Bremen
    (2006)'
  id: totrans-8227
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Jaeger, H.：利用线性增长的回声状态网络生成指数多的周期吸引子。技术报告第3号，雅各布斯大学不来梅（2006）'
- en: '[21] Jaeger, H.: Echo state network. Scholarpedia 2(9), 2330 (2007)'
  id: totrans-8228
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Jaeger, H.：回声状态网络。学术百科 2(9)，2330（2007）'
- en: '[22] Jaeger, H., Lukoševičius, M., Popovici, D., Siewert, U.: Optimization
    and applications of echo state networks with leaky-integrator neurons. Neural
    Networks 20(3),'
  id: totrans-8229
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Jaeger, H., Lukoševičius, M., Popovici, D., Siewert, U.：具有漏积分神经元的回声状态网络的优化与应用。神经网络
    20(3)，'
- en: 335–352 (2007)
  id: totrans-8230
  prefs: []
  type: TYPE_NORMAL
  zh: 335–352（2007）
- en: '[23] Jaeger, H.: Long short-term memory in echo state networks: Details of
    a simulation study. Technical Report No. 27, Jacobs University Bremen (2012)'
  id: totrans-8231
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Jaeger, H.：回声状态网络中的长短期记忆：模拟研究的详细信息。技术报告第27号，雅各布斯大学不来梅（2012）'
- en: '[24] Kahan, W.: Pracniques: further remarks on reducing truncation errors.
    Communications of the ACM 8(1), 40 (1965)'
  id: totrans-8232
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Kahan, W.：实用技巧：关于减少截断误差的进一步说明。《计算机协会通讯》8(1)，40（1965）'
- en: '[25] Küçükemre, A.U.: Echo state networks for adaptive filtering. Master''s
    thesis, University of Applied Sciences Bohn-Rhein-Sieg, Germany (2006),'
  id: totrans-8233
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Küçükemre, A.U.：用于自适应滤波的回声状态网络。硕士论文，博恩-莱因-西格应用科技大学，德国（2006）'
- en: http://reservoir-computing.org/publications/
  id: totrans-8234
  prefs: []
  type: TYPE_NORMAL
  zh: http://reservoir-computing.org/publications/
- en: 2006-echo-state-networks-adaptive-filtering
  id: totrans-8235
  prefs: []
  type: TYPE_NORMAL
  zh: 2006-回声状态网络-自适应滤波
- en: '[26] LeCun, Y.A., Bottou, L., Orr, G.B., Müller, K.-R.: Efficient BackProp.
    In: Orr, G.B., Müller, K.-R. (eds.) NIPS-WS 1996. LNCS, vol. 1524, pp. 9–50. Springer,
    Heidelberg (1998)'
  id: totrans-8236
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] LeCun, Y.A., Bottou, L., Orr, G.B., Müller, K.-R.：高效的反向传播。见：Orr, G.B.,
    Müller, K.-R.（主编）NIPS-WS 1996。LNCS，第1524卷，第9–50页。施普林格，海德堡（1998）'
- en: '[27] Lukoševičius, M., Popovici, D., Jaeger, H., Siewert, U.: Time warping
    invariant echo state networks. Technical Report No. 2, Jacobs University Bremen
    (May 2006)'
  id: totrans-8237
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Lukoševičius, M., Popovici, D., Jaeger, H., Siewert, U.：时间扭曲不变的回声状态网络。技术报告第2号，雅各布斯大学不来梅（2006年5月）'
- en: '[28] Lukoševičius, M.: Echo state networks with trained feedbacks. Technical
    Report No. 4, Jacobs University Bremen (February 2007)'
  id: totrans-8238
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] Lukoševičius, M.：具有训练反馈的回声状态网络。技术报告第4号，雅各布斯大学不来梅（2007年2月）'
- en: '[29] Lukoševičius, M., Jaeger, H.: Reservoir computing approaches to recurrent
    neural network training. Computer Science Review 3(3), 127–149 (2009)'
  id: totrans-8239
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] Lukoševičius, M., Jaeger, H.：递归神经网络训练的蓄水池计算方法。计算机科学评论 3(3)，127–149（2009）'
- en: '[30] Lukoševičius, M.: Reservoir Computing and Self-Organized Neural Hierarchies.'
  id: totrans-8240
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] Lukoševičius, M.：蓄水池计算与自组织神经层次结构。'
- en: PhD thesis, Jacobs University Bremen, Bremen, Germany (2011)
  id: totrans-8241
  prefs: []
  type: TYPE_NORMAL
  zh: 博士论文，雅各布斯大学，不来梅，德国（2011）
- en: '[31] Lukoševičius, M., Jaeger, H., Schrauwen, B.: Reservoir computing trends.
    KI -'
  id: totrans-8242
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Lukoševičius, M., Jaeger, H., Schrauwen, B.：蓄水池计算趋势。KI -'
- en: Künstliche Intelligenz, pp. 1–7 (2012)
  id: totrans-8243
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能，第1–7页（2012）
- en: '[32] Maass, W., Natschläger, T., Markram, H.: Real-time computing without stable
    states: a new framework for neural computation based on perturbations. Neural
    Computation 14(11), 2531–2560 (2002)'
  id: totrans-8244
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] Maass, W., Natschläger, T., Markram, H.：无稳定状态的实时计算：基于扰动的神经计算新框架。神经计算 14(11)，2531–2560（2002）'
- en: '[33] Maass, W., Joshi, P., Sontag, E.D.: Principles of real-time computing
    with feedback applied to cortical microcircuit models. In: Advances in Neural
    Information Processing Systems 18 (NIPS 2005), pp. 835–842. MIT Press, Cambridge
    (2006)'
  id: totrans-8245
  prefs: []
  type: TYPE_NORMAL
  zh: '[33] Maass, W., Joshi, P., Sontag, E.D.：应用于皮层微电路模型的反馈实时计算原则。在：神经信息处理系统18（NIPS
    2005），第835–842页。麻省理工学院出版社，剑桥（2006）'
- en: '[34] Martens, J., Sutskever, I.: Learning recurrent neural networks with Hessian-free
    optimization. In: Proc. 28th Int. Conf. on Machine Learning (2011)'
  id: totrans-8246
  prefs: []
  type: TYPE_NORMAL
  zh: '[34] Martens, J., Sutskever, I.：无海森优化的递归神经网络学习。在：第28届国际机器学习会议论文集（2011）'
- en: '[35] Martens, J., Sutskever, I.: Training Deep and Recurrent Networks with
    Hessianfree Optimization. In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN:
    Tricks of the Trade, 2nd edn. LNCS, vol. 7700, pp. 479–535. Springer, Heidelberg
    (2012)'
  id: totrans-8247
  prefs: []
  type: TYPE_NORMAL
  zh: '[35] Martens, J., Sutskever, I.：使用无海森优化训练深度和递归网络。在：Montavon, G., Orr, G.B.,
    Müller, K.-R.（主编）NN：交易技巧，第2版。LNCS，第7700卷，第479–535页。施普林格，海德堡（2012）'
- en: '[36] Ozturk, M.C., Xu, D., Príncipe, J.C.: Analysis and design of echo state
    networks.'
  id: totrans-8248
  prefs: []
  type: TYPE_NORMAL
  zh: '[36] Ozturk, M.C., Xu, D., Príncipe, J.C.：回声状态网络的分析与设计。'
- en: Neural Computation 19(1), 111–138 (2007)
  id: totrans-8249
  prefs: []
  type: TYPE_NORMAL
  zh: 神经计算 19(1)，111–138（2007）
- en: '[37] Reinhart, F.R., Steil, J.J.: A constrained regularization approach for
    input-driven recurrent neural networks. Differential Equations and Dynamical Systems
    19, 27–'
  id: totrans-8250
  prefs: []
  type: TYPE_NORMAL
  zh: '[37] Reinhart, F.R., Steil, J.J.：输入驱动的递归神经网络的约束正则化方法。《微分方程与动力系统》19，第27–'
- en: 46 (2011)
  id: totrans-8251
  prefs: []
  type: TYPE_NORMAL
  zh: 46（2011）
- en: '[38] Reinhart, F.R., Steil, J.J.: Reservoir regularization stabilizes learning
    of echo state networks with output feedback. In: Proceedings of the 19th European
    Symposium on Artificial Neural Networks, ESANN 2011 (2011) (in Press)'
  id: totrans-8252
  prefs: []
  type: TYPE_NORMAL
  zh: '[38] Reinhart, F.R., Steil, J.J.: 水库正则化稳定回声状态网络的学习，带有输出反馈。见：第19届欧洲人工神经网络研讨会论文集，ESANN
    2011（2011）（即将出版）'
- en: '[39] Rodan, A., Tiňo, P.: Minimum complexity echo state network. IEEE Transactions
    on Neural Networks 22(1), 131–144 (2011)'
  id: totrans-8253
  prefs: []
  type: TYPE_NORMAL
  zh: '[39] Rodan, A., Tiňo, P.: 最小复杂度回声状态网络。IEEE神经网络交易 22(1)，131–144（2011）'
- en: '[40] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning internal representations
    by error propagation. In: Neurocomputing: Foundations of Research, pp. 673–695.
    MIT Press, Cambridge (1988)'
  id: totrans-8254
  prefs: []
  type: TYPE_NORMAL
  zh: '[40] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: 通过误差传播学习内部表征。见：神经计算：研究基础，页
    673–695。MIT出版社，剑桥（1988）'
- en: '[41] Schrauwen, B., Defour, J., Verstraeten, D., Van Campenhout, J.: The Introduction
    of Time-Scales in Reservoir Computing, Applied to Isolated Digits Recognition.'
  id: totrans-8255
  prefs: []
  type: TYPE_NORMAL
  zh: '[41] Schrauwen, B., Defour, J., Verstraeten, D., Van Campenhout, J.: 在水库计算中引入时间尺度，应用于孤立数字识别。'
- en: 'In: de Sá, J.M., Alexandre, L.A., Duch, W., Mandic, D.P. (eds.) ICANN 2007.
    LNCS, vol. 4668, pp. 471–479. Springer, Heidelberg (2007)'
  id: totrans-8256
  prefs: []
  type: TYPE_NORMAL
  zh: 见：de Sá, J.M., Alexandre, L.A., Duch, W., Mandic, D.P.（编辑）ICANN 2007。LNCS, 第
    4668 卷，页 471–479。施普林格，海德堡（2007）
- en: '[42] Shi, Z., Han, M.: Support vector echo-state machine for chaotic time-series
    prediction. IEEE Transactions on Neural Networks 18(2), 359–372 (2007)'
  id: totrans-8257
  prefs: []
  type: TYPE_NORMAL
  zh: '[42] Shi, Z., Han, M.: 用于混沌时间序列预测的支持向量回声状态机。IEEE神经网络交易 18(2)，359–372（2007）'
- en: '[43] Siewert, U., Wustlich, W.: Echo-state networks with band-pass neurons:
    towards generic time-scale-independent reservoir structures. Internal status report,
    PLANET intelligent systems GmbH (2007),'
  id: totrans-8258
  prefs: []
  type: TYPE_NORMAL
  zh: '[43] Siewert, U., Wustlich, W.: 带通神经元的回声状态网络：朝着通用时间尺度独立的水库结构迈进。内部状态报告，PLANET智能系统有限公司（2007），'
- en: http://reslab.elis.ugent.be/node/112
  id: totrans-8259
  prefs: []
  type: TYPE_NORMAL
  zh: http://reslab.elis.ugent.be/node/112
- en: '[44] Steil, J.J.: Backpropagation-decorrelation: recurrent learning with O(N)
    complexity. In: Proceedings of the IEEE International Joint Conference on Neural
    Networks (IJCNN 2004), vol. 2, pp. 843–848 (2004)'
  id: totrans-8260
  prefs: []
  type: TYPE_NORMAL
  zh: '[44] Steil, J.J.: 反向传播-去相关：具有 O(N) 复杂度的递归学习。见：IEEE国际联合神经网络会议论文集（IJCNN 2004），第
    2 卷，页 843–848（2004）'
- en: '[45] Steil, J.J.: Memory in Backpropagation-Decorrelation O(N) Efficient Online
    Recurrent Learning. In: Duch, W., Kacprzyk, J., Oja, E., Zadrożny, S. (eds.) ICANN'
  id: totrans-8261
  prefs: []
  type: TYPE_NORMAL
  zh: '[45] Steil, J.J.: 反向传播-去相关 O(N) 高效在线递归学习。见：Duch, W., Kacprzyk, J., Oja, E.,
    Zadrożny, S.（编辑）ICANN'
- en: 2005. LNCS, vol. 3697, pp. 649–654. Springer, Heidelberg (2005)
  id: totrans-8262
  prefs: []
  type: TYPE_NORMAL
  zh: 2005。LNCS，第 3697 卷，页 649–654。施普林格，海德堡（2005）
- en: '[46] Sussillo, D., Abbott, L.F.: Generating coherent patterns of activity from
    chaotic neural networks. Neuron 63(4), 544–557 (2009)'
  id: totrans-8263
  prefs: []
  type: TYPE_NORMAL
  zh: '[46] Sussillo, D., Abbott, L.F.: 从混沌神经网络生成一致的活动模式。神经元 63(4)，544–557（2009）'
- en: '[47] Triefenbach, F., Jalalvand, A., Schrauwen, B., Martens, J.-P.: Phoneme
    recognition with large hierarchical reservoirs. In: Advances in Neural Information
    Processing Systems 23 (NIPS 2010), pp. 2307–2315. MIT Press, Cambridge (2011)'
  id: totrans-8264
  prefs: []
  type: TYPE_NORMAL
  zh: '[47] Triefenbach, F., Jalalvand, A., Schrauwen, B., Martens, J.-P.: 使用大型层次水库的音素识别。见：神经信息处理系统进展
    23（NIPS 2010），页 2307–2315。MIT出版社，剑桥（2011）'
- en: '[48] Verstraeten, D., Schrauwen, B., Stroobandt, D.: Reservoir-based techniques
    for speech recognition. In: Proceedings of the IEEE International Joint Conference
    on Neural Networks (IJCNN 2006), pp. 1050–1053 (2006)'
  id: totrans-8265
  prefs: []
  type: TYPE_NORMAL
  zh: '[48] Verstraeten, D., Schrauwen, B., Stroobandt, D.: 基于水库的语音识别技术。见：IEEE国际联合神经网络会议论文集（IJCNN
    2006），页 1050–1053（2006）'
- en: '[49] Verstraeten, D., Schrauwen, B., D''Haene, M., Stroobandt, D.: An experimental
    unification of reservoir computing methods. Neural Networks 20(3), 391–403'
  id: totrans-8266
  prefs: []
  type: TYPE_NORMAL
  zh: '[49] Verstraeten, D., Schrauwen, B., D''Haene, M., Stroobandt, D.: 水库计算方法的实验统一。神经网络
    20(3)，391–403'
- en: (2007)
  id: totrans-8267
  prefs: []
  type: TYPE_NORMAL
  zh: （2007）
- en: '[50] Verstraeten, D., Dambre, J., Dutoit, X., Schrauwen, B.: Memory versus
    nonlinearity in reservoirs. In: Proc. Int. Neural Networks (IJCNN) Joint Conf.,
    pp.'
  id: totrans-8268
  prefs: []
  type: TYPE_NORMAL
  zh: '[50] Verstraeten, D., Dambre, J., Dutoit, X., Schrauwen, B.: 水库中的记忆与非线性。见：国际神经网络会议（IJCNN）联合会议论文集，页'
- en: 1–8 (2010)
  id: totrans-8269
  prefs: []
  type: TYPE_NORMAL
  zh: 1–8（2010）
- en: '[51] Werbos, P.J.: Backpropagation through time: what it does and how to do
    it.'
  id: totrans-8270
  prefs: []
  type: TYPE_NORMAL
  zh: '[51] Werbos, P.J.: 时间反向传播：它的功能及其实现方法。'
- en: Proceedings of the IEEE 78(10), 1550–1560 (1990)
  id: totrans-8271
  prefs: []
  type: TYPE_NORMAL
  zh: IEEE 78(10)，1550–1560（1990）
- en: '[52] Williams, R.J., Zipser, D.: A learning algorithm for continually running
    fully recurrent neural networks. Neural Computation 1, 270–280 (1989)'
  id: totrans-8272
  prefs: []
  type: TYPE_NORMAL
  zh: '[52] Williams, R.J., Zipser, D.: 一种用于持续运行完全递归神经网络的学习算法。神经计算 1，270–280（1989）'
- en: '[53] Wyffels, F., Schrauwen, B., Verstraeten, D., Stroobandt, D.: Band-pass
    reservoir computing. In: Hou, Z., Zhang, N. (eds.) Proceedings of the IEEE International
    Joint Conference on Neural Networks (IJCNN 2008), Hong Kong, pp. 3204–3209 (2008)'
  id: totrans-8273
  prefs: []
  type: TYPE_NORMAL
  zh: '[53] Wyffels, F., Schrauwen, B., Verstraeten, D., Stroobandt, D.: 带通水库计算。在：Hou,
    Z., Zhang, N. (编) 《IEEE国际联合神经网络会议（IJCNN 2008）论文集》，香港，第3204–3209页（2008年）'
- en: '[54] Zimmermann, H.-G., Tietz, C., Grothmann, R.: Forecasting with Recurrent
    Neural Networks: 12 Tricks. In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.)
    NN: Tricks of the Trade, 2nd edn. LNCS, vol. 7700, pp. 687–707. Springer, Heidelberg
    (2012)'
  id: totrans-8274
  prefs: []
  type: TYPE_NORMAL
  zh: '[54] Zimmermann, H.-G., Tietz, C., Grothmann, R.: 使用递归神经网络进行预测：12个技巧。在：Montavon,
    G., Orr, G.B., Müller, K.-R. (编) 《神经网络：交易技巧，第2版》，LNCS，卷7700，第687–707页。施普林格，海德堡（2012年）'
- en: '28 Forecasting With Recurrent Neural Networks: 12 Tricks'
  id: totrans-8275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 28 使用递归神经网络进行预测：12个技巧
- en: 'Hans-Georg Zimmermann, Christoph Tietz, and Ralph Grothmann Siemens AG, Corporate
    Technology Otto-Hahn-Ring 6, D-81739 Munich, Germany Hans_Georg.Zimmermann@siemens.com
    Abstract. Recurrent neural networks (RNNs) are typically considered as relatively
    simple architectures, which come along with complicated learning algorithms. This
    paper has a different view: We start from the fact that RNNs can model any high
    dimensional, nonlinear dynamical system. Rather than focusing on learning algorithms,
    we concentrate on the design of network architectures. Unfolding in time is a
    well-known example of this modeling philosophy. Here a temporal algorithm is transferred
    into an architectural framework such that the learning can be performed by an
    extension of standard error backpropagation.'
  id: totrans-8276
  prefs: []
  type: TYPE_NORMAL
  zh: Hans-Georg Zimmermann, Christoph Tietz, 和 Ralph Grothmann，西门子股份公司，企业技术，Otto-Hahn-Ring
    6，D-81739 慕尼黑，德国，Hans_Georg.Zimmermann@siemens.com 摘要：递归神经网络（RNN）通常被认为是相对简单的架构，但伴随复杂的学习算法。本文持有不同的观点：我们从RNN可以建模任何高维非线性动态系统的事实出发。我们不专注于学习算法，而是集中在网络架构的设计上。时间展开是这一建模理念的一个著名例子。在这里，时间算法被转化为一个架构框架，使得学习可以通过标准误差反向传播的扩展来执行。
- en: We introduce 12 tricks that not only provide deeper insights in the functioning
    of RNNs but also improve the identification of underlying dynamical system from
    data.
  id: totrans-8277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍12个技巧，这些技巧不仅提供了对RNN运作的更深入见解，还改善了从数据中识别潜在动态系统的能力。
- en: 28.1 Introduction
  id: totrans-8278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 28.1 引言
- en: In many business management disciplines, complex planning and decisionmaking
    can be supported by quantitative forecast models that take into account a wide
    range of influencing factors with non-linear cause and effect relationships.
  id: totrans-8279
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多商业管理学科中，复杂的规划和决策可以通过考虑广泛影响因素的定量预测模型来支持，这些因素具有非线性的因果关系。
- en: 'Furthermore, the uncertainty in forecasting should be considered. The procurement
    of raw materials or demand planning are prime examples: The timing of copper purchases
    can be optimized with accurate market price forecasts, whereas precise forecasts
    of product sales increase the deliver reliability and reduce costs.'
  id: totrans-8280
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，预测中的不确定性应予以考虑。原材料采购或需求规划就是典型例子：通过准确的市场价格预测，可以优化铜采购的时机，而准确的产品销售预测则提高了交付的可靠性并降低了成本。
- en: Likewise technical applications, e.g. energy generation, also require the modeling
    of complex dynamical systems.
  id: totrans-8281
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的技术应用，例如能源生产，也需要对复杂动态系统进行建模。
- en: In this contribution we deal with time-delay recurrent neural networks (RNNs)
  id: totrans-8282
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们处理时间延迟递归神经网络（RNN）。
- en: for time series forecasting and introduce 12 tricks that not only ease the handling
    of RNNs, but also improve the forecast accuracy. The RNNs and associated tricks
    are applied in many of our customer projects from economics and industry.
  id: totrans-8283
  prefs: []
  type: TYPE_NORMAL
  zh: 用于时间序列预测，并介绍12个技巧，这些技巧不仅简化了对RNN的处理，还提高了预测准确性。RNN及其相关技巧在我们许多客户的经济和工业项目中得到了应用。
- en: RNNs offer significant benefits for dealing with the typical challenges associated
    with forecasting. With their universal approximation properties [11], RNNs can
    model high-dimensional, non-linear relationships. The time-delayed information
    processing addresses temporal structures. In contrast, conventional econometrics
    generally uses linear models (e.g. autoregressive models (AR), multivariate linear
  id: totrans-8284
  prefs: []
  type: TYPE_NORMAL
  zh: RNN在处理与预测相关的典型挑战方面提供了显著的好处。凭借其通用逼近性质[11]，RNN可以建模高维非线性关系。时间延迟的信息处理解决了时间结构的问题。相较之下，传统经济计量学一般使用线性模型（例如自回归模型（AR）、多元线性模型）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    687–707, 2012.'
  id: totrans-8285
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon 等（编）：神经网络：交易技巧，第2版，LNCS 7700，第687–707页，2012年。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-8286
  prefs: []
  type: TYPE_NORMAL
  zh: -c Springer-Verlag Berlin Heidelberg 2012
- en: regression) which can be efficiently estimated from historical data, but provide
    only an inadequate framework for non-linear dynamical systems [12].
  id: totrans-8287
  prefs: []
  type: TYPE_NORMAL
  zh: 回归) 可以从历史数据中高效估计，但仅为非线性动态系统提供了不充分的框架[12]。
- en: In Section 28.2 we introduce the so-called correspondence principle for neural
    networks (Trick 1). For us neural networks are a class of functions which can
    be transformed into architectures. We will work only with algorithms that process
    information locally within the architectures. As we will outline, for some problems
    it is easier to start off with the NN architecture and formulate the equations
    afterwards and for other problems vice versa. The locality of the algorithms enables
    us to model even large systems. The correspondence principle is the basis for
    different RNN models and associated tricks.
  id: totrans-8288
  prefs: []
  type: TYPE_NORMAL
  zh: 在第28.2节中，我们介绍所谓的神经网络对应原则（Trick 1）。对我们来说，神经网络是一类可以转化为架构的函数。我们将仅使用在架构内局部处理信息的算法。正如我们将阐述的，对于某些问题，从NN架构开始并随后制定方程更容易，而对于其他问题则相反。算法的局部性使我们能够建模甚至是大型系统。对应原则是不同RNN模型及相关技巧的基础。
- en: We will start with a basic RNN in state space formulation for the modeling of
    partly autonomous and partly externally driven dynamical systems (so-called open
    systems). The associated parameter optimization task is solved by (finite)
  id: totrans-8289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从基本的状态空间形式的RNN开始，以建模部分自主和部分外部驱动的动态系统（所谓的开放系统）。相关的参数优化任务通过（有限）解决。
- en: unfolding in time, which can be handled by a shared weights extension of standard
    backpropagation. Dealing with state space models, we are able to utilize memory
    effects. Therefore, there is no need of a complicated input preprocessing in order
    to represent temporal relationships. Nevertheless, learning of open dynamical
    systems tends to focus on the external drivers and, thus, neglects the identification
    of the autonomous part. On this problem Trick 2 enforces the autonomous flow of
    the dynamics and thus, enables long-term forecasting. Trick 3 finds a proper initialization
    for the first state vector of recurrent neural network in the finite unfolding.
  id: totrans-8290
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间展开，可以通过标准反向传播的共享权重扩展来处理。处理状态空间模型时，我们能够利用记忆效应。因此，不需要复杂的输入预处理来表示时间关系。然而，开放动态系统的学习往往侧重于外部驱动，从而忽视了自主部分的识别。在这个问题上，Trick
    2加强了动态的自主流动，从而使长期预测成为可能。Trick 3为有限展开中的递归神经网络的第一个状态向量找到合适的初始化。
- en: Typically we do not know all external drivers of the open dynamical system.
  id: totrans-8291
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们并不知道开放动态系统的所有外部驱动因素。
- en: This may cause the identification of pseudo causalities. Trick 4 is the extension
    of the RNN with an error correction term, resulting in a so-called error correction
    neural network (ECNN), which enables us to handle missing information, hidden
    external factors or shocks on the dynamics. ECNNs are an appropriate framework
    for low-dimensional dynamical systems with less than 5 target variables.
  id: totrans-8292
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能导致伪因果关系的识别。Trick 4是将错误修正项扩展到RNN，形成所谓的错误修正神经网络（ECNN），使我们能够处理缺失信息、隐藏的外部因素或动态冲击。ECNN是适用于目标变量少于5的低维动态系统的合适框架。
- en: For the modeling of high-dimensional systems on low dimensional manifolds as
    in electrical load curves Trick 5 adds a coordinate transformation (so-called
    bottleneck) to the ECNN.
  id: totrans-8293
  prefs: []
  type: TYPE_NORMAL
  zh: 在电负荷曲线中，Trick 5为低维流形上的高维系统建模增加了坐标变换（所谓的瓶颈）到ECNN。
- en: Standard RNNs use external drivers in the past and assume constant environmental
    conditions from present time on. For fast changing environments this is a questionable
    assumption. Internalizing the environment of the dynamics into the model, leads
    to Trick 6, so-called historically consistent neural networks
  id: totrans-8294
  prefs: []
  type: TYPE_NORMAL
  zh: 标准RNN使用过去的外部驱动，并假设从现在开始环境条件保持不变。对于快速变化的环境，这一假设是可疑的。将动态环境内部化到模型中，导致Trick 6，即所谓的历史一致神经网络。
- en: (HCNN). The special feature of the HCNN is that it not only models the individual
    dynamics of interest, but also models the external drivers. This leads to a closed
    dynamical system formulation. Therefore, HCNNs are symmetric in their input and
    output variables, i.e. the system description does not draw any distinction between
    input, output and internal state variables.
  id: totrans-8295
  prefs: []
  type: TYPE_NORMAL
  zh: (HCNN)。HCNN 的特殊之处在于，它不仅建模感兴趣的个体动态，还建模外部驱动因素。这导致了一个封闭的动态系统公式。因此，HCNN 在输入和输出变量上是对称的，即系统描述不区分输入、输出和内部状态变量。
- en: In practice, HCNNs are difficult to train, because the models have no input
    signals and are unfolded across the complete data horizon. This implies that we
    learn from a single data pattern, which is the unique data history. In Trick 7
    we therefore introduce an architectural teacher forcing to make the best possible
    use of the data from the observables and to accelerate training of the HCNN.
  id: totrans-8296
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，HCNN 难以训练，因为模型没有输入信号，并且在整个数据范围内展开。这意味着我们是从一个唯一的数据模式中学习，即独特的数据历史。因此，在技巧
    7 中，我们引入了结构性教师强制，以尽可能充分地利用可观察量的数据，并加快 HCNN 的训练。
- en: The HCNN models the dynamics for all of the observables and their interaction
    in parallel. For this purpose a high-dimensional state transition matrix is required.
    A fully connected state transition matrix can, however, lead to a signal overload
    during the training of the neural network using error backpropagation through
    time (EBTT). With Trick 8 we solve this problem by introducing sparse state transition
    matrices.
  id: totrans-8297
  prefs: []
  type: TYPE_NORMAL
  zh: HCNN 同时建模所有可观察量及其交互的动态。为此，需要一个高维状态转移矩阵。然而，完全连接的状态转移矩阵可能在使用时间反向传播（EBTT）训练神经网络时导致信号过载。我们通过引入稀疏状态转移矩阵来解决这个问题，使用技巧
    8。
- en: The information flow within a HCNN is from the past to present and future time,
    i.e. we have a causal model to explain the highly-interacting non-linear dynamical
    systems across multiple time scales. Trick 9 extends this modeling framework with
    an information flow from the future into past. As we will show this enables us
    to incorporate the effects of rational decision making and planning into the modeling.
    The resulting models are called *causal-retro-causal* historically consistent
    neural networks (CRCNNs). Likewise to HCNNs, CRCNNs are difficult to train. In
    Trick 10 we extend the basic CRCNN architecture by an architectural teacher forcing
    mechanism, which allows us to learn the CRCNN
  id: totrans-8298
  prefs: []
  type: TYPE_NORMAL
  zh: HCNN 内的信息流是从过去到现在和未来时间，即我们有一个因果模型来解释跨多个时间尺度的高度互动的非线性动态系统。技巧 9 扩展了这一建模框架，从未来流向过去的信息流。正如我们将展示的，这使我们能够将理性决策和规划的影响纳入建模。得到的模型被称为
    *因果-逆因果* 历史一致神经网络（CRCNNs）。与 HCNN 一样，CRCNN 也难以训练。在技巧 10 中，我们通过一个结构性教师强制机制扩展了基本的
    CRCNN 架构，使我们能够学习 CRCNN。
- en: using the standard EBTT algorithm. Trick 11 introduces a way to improve the
    modeling of deterministic chaotic systems.
  id: totrans-8299
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准的 EBTT 算法。技巧 11 引入了一种改善确定性混沌系统建模的方法。
- en: Finally, Trick 12 is dedicated to the modeling of uncertainty and risk. We calculate
    ensembles of either HCNNs or CRCNNs to forecast probability distributions. Both
    modeling frameworks give a perfect description of the dynamic of the observables
    in the past. However, the partial observability of the world results in a non-unique
    reconstruction of the hidden variables and thus, different future scenarios. Since
    the genuine development of the dynamics is unknown and all paths have the same
    probability, the average of the ensemble is the best forecast, whereas the ensemble
    bandwidth describes the market risk.
  id: totrans-8300
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，技巧 12 专注于不确定性和风险的建模。我们计算 HCNNs 或 CRCNNs 的集合来预测概率分布。这两种建模框架完美描述了过去可观察量的动态。然而，世界的部分可观察性导致隐藏变量的重建不是唯一的，从而产生不同的未来情景。由于动态的真实发展未知，所有路径的概率相同，因此集合的平均值是最佳预测，而集合带宽描述了市场风险。
- en: Section 28.3 summarizes the primary findings of this contribution and points
    to future directions of research.
  id: totrans-8301
  prefs: []
  type: TYPE_NORMAL
  zh: 第 28.3 节总结了本贡献的主要发现，并指出了未来研究的方向。
- en: 28.2 Tricks For Recurrent Neural Networks Trick 1. The Correspondence Principle
    For Neural Networks
  id: totrans-8302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 28.2 循环神经网络的技巧技巧 1. 神经网络的对应原则
- en: In order to gain a deeper understanding in the functioning and composition of
    RNNs we introduce our first conceptual trick, which is called correspondence principle
    between equations, architectures and local algorithms. The correspondence principle
    for neural networks (NN) implies that any equation for a NN can be portrayed in
    graphical form by means of an architecture which represents the individual layers
    of the network in the form of nodes and the matrices between the layers in the
    form of edges. This correspondence is most beneficial in combination with local
    optimization algorithms that provide the basis for the training of the NNs. For
    example, the error back propagation algorithm needs only locally available information
    from the forward and backward flow of the network in order to calculate the partial
    derivatives of the NN error function[13]. The use of local algorithms here provides
    an elegant basis for the expansion of the neural network towards the modeling
    of large systems. Used in combination with an appropriate (stochastic) learning
    rule, it is possible to use the gradients as a basis for the identification of
    robust minima[8].
  id: totrans-8303
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地理解 RNN 的功能和组成，我们引入第一个概念性技巧，即方程、架构和局部算法之间的对应原则。神经网络 (NN) 的对应原则意味着任何 NN
    的方程都可以通过一种图形形式表现出来，架构表示网络的各个层，以节点的形式表示层之间的矩阵，以边的形式连接。这个对应原则与局部优化算法结合时最为有利，这些算法为
    NN 的训练提供基础。例如，误差反向传播算法只需从网络的前向和后向流中获取局部可用的信息，以计算 NN 误差函数的偏导数[13]。在这里使用局部算法为神经网络扩展到大型系统建模提供了优雅的基础。结合合适的（随机）学习规则，可以使用梯度作为识别稳健最小值的基础[8]。
- en: 'Now let us introduce a basic recurrent neural network (RNN) in state space
    formulation. We start from the assumption that a vector time series yτ is created
    by an open dynamical system, which can be described in discrete time τ using a
    state transition and output equation[3]:'
  id: totrans-8304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们引入一个基本的递归神经网络 (RNN) 的状态空间形式。我们从假设开始，一个向量时间序列 yτ 是由一个开放的动态系统生成的，这个系统可以用离散时间
    τ 的状态转移和输出方程来描述[3]：
- en: $$\begin{array}{r l}{{\mathrm{state~transition}}}&{{s_{\tau+1}=f(s_{\tau},u_{\tau})}}\\
    {{\mathrm{output~equation}}}&{{y_{\tau}}}&{{=g(s_{\tau})}}\end{array}$$
  id: totrans-8305
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{r l}{{\mathrm{状态~转移}}}&{{s_{\tau+1}=f(s_{\tau},u_{\tau})}}\\
    {{\mathrm{输出~方程}}}&{{y_{\tau}}}&{{=g(s_{\tau})}}\end{array}$$
- en: $$(28.1)$$
  id: totrans-8306
  prefs: []
  type: TYPE_NORMAL
  zh: $$(28.1)$$
- en: output equation yτ = g(sτ ) (28.1)
  id: totrans-8307
  prefs: []
  type: TYPE_NORMAL
  zh: 输出方程 yτ = g(sτ ) (28.1)
- en: The hidden time-recurrent state transition equation sτ+1 = f(sτ , uτ ) describes
    the upcoming state sτ+1 by means of a function of the current system state sτ
    and of the external factors uτ . The system formulated in the state transition
    equation can therefore be interpreted as a partially autonomous and partially
    externally driven dynamic. We call this an open dynamical system.
  id: totrans-8308
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏的时间递归状态转移方程 sτ+1 = f(sτ , uτ ) 通过当前系统状态 sτ 和外部因素 uτ 的函数描述即将到来的状态 sτ+1。因此，状态转移方程中所形成的系统可以被解释为一个部分自主和部分外部驱动的动态。我们称之为开放动态系统。
- en: The output, also called observer, equation yτ uses the non-observable system
    state sτ in every time step τ to calculate the output of the dynamic system yτ
    . The data-driven system identification is based on the selected parameterized
    functions f() and g(). We chose the parameters in f() and g() such that an appropriate
    error function is minimized (see Eq. 28.2).
  id: totrans-8309
  prefs: []
  type: TYPE_NORMAL
  zh: 输出方程 yτ，也称为观察者方程，在每个时间步 τ 中使用不可观察的系统状态 sτ 来计算动态系统的输出 yτ 。基于所选择的参数化函数 f() 和 g()
    进行数据驱动的系统识别。我们选择 f() 和 g() 中的参数，使得适当的误差函数最小化（见 Eq. 28.2）。
- en: $$\frac{1}{T}\sum_{\tau=1}^{T}\left(y_{\tau}-y_{\tau}^{d}\right)^{2}\rightarrow\operatorname*{min}_{f,g}\;\;.$$
  id: totrans-8310
  prefs: []
  type: TYPE_NORMAL
  zh: $$\frac{1}{T}\sum_{\tau=1}^{T}\left(y_{\tau}-y_{\tau}^{d}\right)^{2}\rightarrow\operatorname*{min}_{f,g}\;\;.$$
- en: $$(28.2)$$
  id: totrans-8311
  prefs: []
  type: TYPE_NORMAL
  zh: $$(28.2)$$
- en: The two functions f() and g() are estimated using the quadratic error function
    (Eq. 28.2) in such a way that the average distance between the observed data ydτ
    and the system outputs yτ across a number of observations τ = 1*,...,T* is minimal.
  id: totrans-8312
  prefs: []
  type: TYPE_NORMAL
  zh: 两个函数 f() 和 g() 是使用二次误差函数 (Eq. 28.2) 进行估计的，目的是使观察数据 ydτ 和系统输出 yτ 在多个观察 τ = 1*,...,T*
    之间的平均距离最小化。
- en: 'Thus far, we have given a general description of the state transition and the
    output equation for open dynamical systems. Without loss of generality we can
    specify the functions f() and g() by means of a recurrent neural network (RNN)[11,
    15]:'
  id: totrans-8313
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们对开放动态系统的状态转移和输出方程进行了概述。我们可以通过递归神经网络 (RNN)[11, 15] 来具体定义函数 f() 和 g()，而不失一般性：
- en: $$\begin{array}{r l}{{\mathrm{state~transition}}}&{{s_{\tau+1}=\operatorname{tanh}(A
    s_{\tau}+B u_{\tau})}}\\ {{\mathrm{output~equation}}}&{{y_{\tau}}}&{{=C s_{\tau}}}\end{array}$$
  id: totrans-8314
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{r l}{{\mathrm{状态~转移}}}&{{s_{\tau+1}=\operatorname{tanh}(A s_{\tau}+B
    u_{\tau})}}\\ {{\mathrm{输出~方程}}}&{{y_{\tau}}}&{{=C s_{\tau}}}\end{array}$$
- en: $$(28.3)$$
  id: totrans-8315
  prefs: []
  type: TYPE_NORMAL
  zh: $$(28.3)$$
- en: 'Eq. 28.3 specifies an RNN with weight matrices A, B and C to model the open
    dynamical system. The RNN is designed as a non-linear state-space model, which
    is able to approximate any function f() and g(). We choose the hyperbolic tangent
    tanh() as the activation function for the hidden network layer sτ+1. The output
    equation is specified as a linear function. The RNN output is generated by a superposition
    of two components: (i) the autonomous dynamics (coded in A), which accumulates
    information over time (memory), and (ii) the influence of external factors (coded
    in B).'
  id: totrans-8316
  prefs: []
  type: TYPE_NORMAL
  zh: 方程28.3规定了一个具有权重矩阵A、B和C的RNN，用于建模开放动态系统。该RNN被设计为非线性状态空间模型，能够逼近任何函数f()和g()。我们选择双曲正切tanh()作为隐藏网络层sτ+1的激活函数。输出方程被指定为线性函数。RNN输出由两个组件的叠加生成：（i）自主动态（编码在A中），随着时间累积信息（记忆），以及（ii）外部因素的影响（编码在B中）。
- en: Note, that the state transition in Eq. 28.3 does not need an additional matrix
    leading the hyperbolic tangent tanh() activation function, since the additional
  id: totrans-8317
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，方程28.3中的状态转移不需要额外的矩阵来引导双曲正切tanh()激活函数，因为额外的
- en: '![678_image_0.png](678_image_0.png)'
  id: totrans-8318
  prefs: []
  type: TYPE_IMG
  zh: '![678_image_0.png](678_image_0.png)'
- en: matrix can be merged into matrix A. Furthermore, without loss of generality
    we can use a linear output equation in Eq. 28.3. If we would have a non-linearity
    in the output equation (Eq. 28.3), it could be merged in the state transition
    equation (Eq. 28.3). For details see Schäfer et al. [11].
  id: totrans-8319
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵可以合并到矩阵A中。此外，在不失一般性的情况下，我们可以在方程28.3中使用线性输出方程。如果输出方程（方程28.3）中有非线性，它可以合并到状态转移方程（方程28.3）中。有关详细信息，请参见Schäfer等人[11]。
- en: We use the technique of finite unfolding in time[10] to solve the temporal system
    identification, i.e. for the selection of appropriate matrices A, B and C
  id: totrans-8320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用有限时间展开的技术[10]来解决时间系统识别，即选择适当的矩阵A、B和C。
- en: to minimize the error function (Eq. 28.1). The underlying idea here is that
    any RNN can be reformulated to form an equivalent feedforward neural network,
    if matrices A, B and C are identical in the individual time steps (shared weights).
    Fig. 28.1 depicts the resulting RNN.
  id: totrans-8321
  prefs: []
  type: TYPE_NORMAL
  zh: 以最小化误差函数（方程28.1）。这里的基本思想是，任何RNN都可以重新表述为等效的前馈神经网络，只要矩阵A、B和C在各个时间步上是相同的（共享权重）。图28.1描绘了得到的RNN。
- en: Fig. 28.1. Basic RNN unfolded in time with shared weights A, B and C
  id: totrans-8322
  prefs: []
  type: TYPE_NORMAL
  zh: 图28.1. 基本RNN在时间上展开，具有共享权重A、B和C
- en: One advantage of the shared matrices is the moderate number of free parameters
    (weights), which reduces the risk of over-fitting[18]. The actual training is
    conducted using error backpropagation through time (EBTT) together with a stochastic
    learning rule[13, 10]. For algorithmic solutions, the reader is referred to the
    overview article by B. Pearlmutter[9].
  id: totrans-8323
  prefs: []
  type: TYPE_NORMAL
  zh: 共享矩阵的一个优势是自由参数（权重）的数量适中，这降低了过拟合的风险[18]。实际训练是通过时间误差反向传播（EBTT）与随机学习规则一起进行的[13,
    10]。关于算法解决方案，读者可以参考B. Pearlmutter的概述文章[9]。
- en: Trick 2. Overshooting
  id: totrans-8324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧2. 过冲
- en: In applications we often observed that RNNs tend to focus on only the most recent
    external inputs in order to explain the dynamics. To balance the information flow,
    we use a trick called overshooting. Overshooting extends the autonomous system
    dynamics (coded matrix A) into the future (here t + 2, t + 3, t + 4)[18] (see
    Fig. 28.2). In order to describe the development of the dynamics in one of these
    future time steps adequately, matrix A must be able to transfer information over
    time. The different instances of matrix A refer to the same prototype matrix A.
    Thus the shared weights principle allow us to maintain the locality of the correspondence
    principle (see Trick 1). By this we can compute consistent multi-step forecasts.
    A corresponding RNN architecture is depicted in Fig. 28.2. For the RNN we typically
    use an input preprocessing uτ = xτ − xτ−1 as the transformation for the raw data
    x. This avoids trends in the input or
  id: totrans-8325
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用中，我们经常观察到RNN倾向于只关注最近的外部输入，以解释动态。为了平衡信息流，我们使用了一个叫做超调的技巧。超调将自主系统动态（编码矩阵A）扩展到未来（这里是t
    + 2, t + 3, t + 4）[18]（见图28.2）。为了适当地描述这些未来时间步骤中动态的发展，矩阵A必须能够在时间上转移信息。矩阵A的不同实例指的是同一个原型矩阵A。因此，共享权重原则允许我们保持对应原则的局部性（见技巧1）。通过这个，我们可以计算一致的多步预测。相应的RNN架构在图28.2中描述。对于RNN，我们通常使用输入预处理uτ
    = xτ − xτ−1作为原始数据x的变换。这避免了输入中的趋势或
- en: '![679_image_0.png](679_image_0.png)'
  id: totrans-8326
  prefs: []
  type: TYPE_IMG
  zh: '![679_image_0.png](679_image_0.png)'
- en: Fig. 28.2. RNN incorporating overshooting
  id: totrans-8327
  prefs: []
  type: TYPE_NORMAL
  zh: 图28.2. 包含超调的RNN
- en: target variables of the RNN. The missing external inputs uτ>0 in the future
    can be interpreted as a constant environment (uτ>0 ≈ 0). The effectiveness of
    overshooting depends on the strength of the autonomous dynamics. The stronger
    the autonomous flow, the better is the forecast accuracy for the future overshooting
    time steps. Furthermore, overshooting has an implication for the learning itself.
    Without overshooting, RNNs have the tendency to focus only on shortterm input-output
    relationships. With overshooting the learning has to work out mid- to long-term
    input-output relationships.
  id: totrans-8328
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的目标变量。在未来缺失的外部输入uτ>0可以被解释为一个常量环境（uτ>0 ≈ 0）。超调的有效性依赖于自主动态的强度。自主流越强，未来超调时间步骤的预测准确性就越好。此外，超调对学习本身也有影响。如果没有超调，RNN往往只关注短期的输入输出关系。通过超调，学习需要处理中长期的输入输出关系。
- en: Summarizing, it should be noted, that overshooting generates additional valuable
    forecast information about the analyzed dynamical system and acts as a regularization
    method for the learning.
  id: totrans-8329
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，应该注意的是，超调产生了关于被分析的动态系统的额外有价值的预测信息，并且作为学习的正则化方法。
- en: Trick 3. Handling The Uncertainty Of The Initial State
  id: totrans-8330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧3. 处理初始状态的不确定性
- en: One of the difficulties with finite unfolding in time is to find a proper initialization
    for the first state vector of the RNN. Our next trick takes care of this problem.
  id: totrans-8331
  prefs: []
  type: TYPE_NORMAL
  zh: 有限时间展开的一个困难是找到RNN的第一个状态向量的适当初始化。我们下一个技巧可以解决这个问题。
- en: An obvious solution is to set the first state s0 equal to *zero*. We assume
    that the unfolding includes enough (past) time steps such that the misspecification
    of the initialization phase is overwritten along the state transitions. In other
    words, the network accumulates information over time and thus, may eliminate the
    impact of the arbitrary initial state on the network outputs.
  id: totrans-8332
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显而易见的解决方案是将第一个状态s0设置为*零*。我们假设展开包含足够的（过去）时间步骤，以便初始化阶段的错误在状态转换过程中被覆盖。换句话说，网络随着时间的推移积累信息，因此可能消除任意初始状态对网络输出的影响。
- en: Beyond this the modeling can be improved if we are able to make the unfolded
    RNN less sensitive from the unknown initial state s0. A first approach to stiff
    the model against the unknown s0 is to apply a noise term Θ to the state s0.
  id: totrans-8333
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们能够使展开的RNN对未知的初始状态s0不那么敏感，则建模可以得到改善。使模型对未知s0稳健的第一种方法是对状态s0应用噪声项Θ。
- en: A fixed noise term Θ which is drawn from a certain noise distribution is clearly
    inadequate to handle the uncertainty of the initial state. Since we do not know
    what is an appropriate level of noise, we have to find a way to estimate the noise
    level. We propose to apply an adaptive noise Θ, which fits best to the level of
    uncertainty of the unknown s0. The characteristic of the adaptive noise term Θ
    is automatically determined as a by-product of the error backpropagation algorithm.
  id: totrans-8334
  prefs: []
  type: TYPE_NORMAL
  zh: 一个固定的噪声项 Θ 来自于某种噪声分布，显然不足以应对初始状态的不确定性。由于我们不知道什么是合适的噪声水平，我们必须找到一种方法来估计噪声水平。我们建议应用一个自适应噪声
    Θ，它最适合未知状态 s0 的不确定性水平。自适应噪声项 Θ 的特性是作为错误反向传播算法的副产品自动确定的。
- en: 'The basic idea is as follows [8]: We use the residual error  of the neural
    network as computed by error backpropagation for s0. The residual error  as measured
    at the initial state s0 can be interpreted as the uncertainty which is'
  id: totrans-8335
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思路如下 [8]：我们使用通过错误反向传播计算的神经网络的残差误差作为 s0 的依据。在初始状态 s0 处测得的残差误差可以被解释为不确定性。
- en: '![680_image_0.png](680_image_0.png)'
  id: totrans-8336
  prefs: []
  type: TYPE_IMG
  zh: '![680_image_0.png](680_image_0.png)'
- en: due to the missing information about the true initial state vector s0. We disturb
    the initial state s0 with a noise term Θ which follows the distribution of the
    residual error . Given the uncertain initial states, learning tries to fulfill
    the output-target relationships along the dynamics. As a result of the learning
    we get a state transition matrix in form of a contraction, which squeezes out
    the initial uncertainty. A corresponding network architecture is depicted in Fig.
    28.3.
  id: totrans-8337
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缺失有关真实初始状态向量 s0 的信息。我们用一个遵循残差误差分布的噪声项 Θ 来干扰初始状态 s0。考虑到不确定的初始状态，学习尝试在动态中满足输出目标关系。学习的结果是我们得到一个以收缩形式出现的状态转移矩阵，从而排除了初始的不确定性。相应的网络架构在图
    28.3 中展示。
- en: Fig. 28.3. Handling the uncertainty of the initial state s0 by applying adaptive
    noise
  id: totrans-8338
  prefs: []
  type: TYPE_NORMAL
  zh: 图 28.3. 通过应用自适应噪声来处理初始状态 s0 的不确定性
- en: It is important to notice, that the noise term Θ is drawn from the observed
    residual errors without any assumption on the underlying noise distribution. The
    desensitization of the network to the initial state vector s0 can therefore be
    seen as a self-scaling stabilizer of the modeling.
  id: totrans-8339
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，噪声项 Θ 是从观察到的残差误差中提取的，而没有对潜在噪声分布做出任何假设。因此，网络对初始状态向量 s0 的去敏感化可以被视为建模的自我缩放稳定器。
- en: In general, a time discrete trajectory can be seen as a sequence of points over
    time. Such a trajectory is comparable to a fine thread in the internal state space.
  id: totrans-8340
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，时间离散轨迹可以看作是随时间变化的一系列点。这种轨迹可与内部状态空间中的一根细线相比较。
- en: The trajectory is very sensitive to the initial state vector s0. If we apply
    noise to s0, the trajectory becomes a tube in the internal state space. Due to
    the characteristics of the adaptive noise term, the tube contracts over time.
    This enforces the identification of a stable dynamical system.
  id: totrans-8341
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹对初始状态向量 s0 非常敏感。如果我们对 s0 应用噪声，轨迹将在内部状态空间中变成一个管道。由于自适应噪声项的特性，管道随时间收缩。这促进了对稳定动态系统的识别。
- en: Trick 4. Error Correction Neural Networks (Ecnn)
  id: totrans-8342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧 4. 错误修正神经网络（Ecnn）
- en: 'A weakness of the RNN (see Fig. 28.1 or 28.2) is, that modeling might be disturbed
    by unknown external influences or shocks. As a remedy, the next trick called error
    correction neural networks (ECNN) introduces an additional term zτ = tanh(yτ −ydτ
    ) in the state transition (28.4). The term can be interpreted as a correctional
    factor: The model error (yτ − ydτ ) at time τ quantifies the misfit and may help
    to adjust the model output afterwards.'
  id: totrans-8343
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的一个弱点（参见图 28.1 或 28.2）是，建模可能会受到未知外部影响或冲击的干扰。作为补救措施，下一个技巧称为错误修正神经网络（ECNN），在状态转换
    (28.4) 中引入了额外的项 zτ = tanh(yτ − ydτ)。这一项可以被解释为一个修正因子：在时间 τ 的模型误差 (yτ − ydτ) 量化了不匹配，并可能有助于随后调整模型输出。
- en: $$|\rangle$$
  id: totrans-8344
  prefs: []
  type: TYPE_NORMAL
  zh: $$|\rangle$$
- en: $$(28.4)$$
  id: totrans-8345
  prefs: []
  type: TYPE_NORMAL
  zh: $$(28.4)$$
- en: state transition sτ+1 = tanh(Asτ + Buτ + D tanh(yτ − ydτ ))
  id: totrans-8346
  prefs: []
  type: TYPE_NORMAL
  zh: 状态转换 sτ+1 = tanh(Asτ + Buτ + D tanh(yτ − ydτ ))
- en: output equation yτ = Csτ(28.4)
  id: totrans-8347
  prefs: []
  type: TYPE_NORMAL
  zh: 输出方程 yτ = Csτ(28.4)
- en: 'In Eq. 28.4 the model output yτ is computed by Csτ and compared with the observation
    ydτ . The output clusters of the ECNN which generate error signals during the
    learning phase are zτ (τ ≤ t) and yτ (τ>t). Have in mind, that the target values
    of the sequence of output clusters zτ are *zero*, because we want to optimize
    the compensation mechanism yτ − ydτ between the expected value yτ and its observation
    ydτ . The additional non-linear squashing function in zτ = tanh(yτ − ydτ ) absorbs
    large errors respectively shocks. A special case occurs at all future time steps
    t + 1: here we have no compensation ydt+1 of the internal expected value, and
    thus the system is offering a forecast yt+1 = Cst+1.'
  id: totrans-8348
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式 28.4 中，模型输出 yτ 由 Csτ 计算，并与观察值 ydτ 比较。生成学习阶段误差信号的 ECNN 输出聚类是 zτ (τ ≤ t) 和
    yτ (τ>t)。请注意，输出聚类 zτ 的目标值是 *零*，因为我们希望优化期望值 yτ 和其观察值 ydτ 之间的补偿机制 yτ − ydτ。 zτ =
    tanh(yτ − ydτ ) 中的额外非线性压缩函数吸收了较大的误差和冲击。在所有未来时间步 t + 1 处出现特殊情况：此时我们没有内部期望值的补偿 ydt+1，因此系统提供预测
    yt+1 = Cst+1。
- en: The system identification task (see Eq. 28.2) is once again solved by finite
    unfolding in time [3]. Fig. 28.4 depicts the resulting ECNN[15].
  id: totrans-8349
  prefs: []
  type: TYPE_NORMAL
  zh: 系统识别任务（见公式 28.2）再次通过时间有限展开来解决 [3]。图 28.4 展示了得到的 ECNN [15]。
- en: Fig. 28.4. ECNN incorporating overshooting. Note, that −Id is the fixed negative
  id: totrans-8350
  prefs: []
  type: TYPE_NORMAL
  zh: 图 28.4. 包含过冲的 ECNN。请注意，−Id 是固定的负
- en: '![681_image_0.png](681_image_0.png)'
  id: totrans-8351
  prefs: []
  type: TYPE_IMG
  zh: '![681_image_0.png](681_image_0.png)'
- en: of an identity matrix, while zt−τ are output clusters to model the error correction
    mechanism.
  id: totrans-8352
  prefs: []
  type: TYPE_NORMAL
  zh: 作为单位矩阵的输出聚类，而 zt−τ 是建模误差校正机制的输出聚类。
- en: 'The ECNN has two different inputs: (i) the externals uτ , which directly influence
    the state transition, and (ii) the targets ydτ . Only the difference between yτ
    and ydτ has an impact on sτ+1. In the future τ>t, we have no compensation for
    yτ and thus compute forecasts yτ = Csτ . We have successfully applied ECNN models
    to predict the demand of products and product groups within the context of supply
    chain management.'
  id: totrans-8353
  prefs: []
  type: TYPE_NORMAL
  zh: ECNN 有两个不同的输入：(i) 外部 uτ，直接影响状态转移，(ii) 目标 ydτ。只有 yτ 和 ydτ 之间的差异对 sτ+1 有影响。在未来
    τ>t 时，我们对 yτ 没有补偿，因此计算预测 yτ = Csτ。我们成功地应用 ECNN 模型来预测供应链管理中产品和产品组的需求。
- en: Trick 5. Variant-Invariant Separation
  id: totrans-8354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧 5. 变体-不变分离
- en: For the modeling of high-dimensional dynamical systems, the next trick called
    variant-invariant separation extends the architecture of the ECNN (Fig. 28.5)
    by a coordinate transformation to reduce the dimensionality of the original forecast
    problem. The dimension reduction is realized in form of a bottleneck sub-network
    (Fig. 28.5, left). The compressor E removes time invariant structures from the
    dynamics. The reconstruction of the complete dynamics (decompression) is done
    by matrix F. The bottleneck is implicitly connected to the ECNN via the shared
    matrices E and F [15]. We compress the high-dimensional vector ydt to a lower
    dimensional vector xt using a bottleneck neural network. The vector xt contains
    all relevant information about ydt . The ECNN predicts the low dimensional vector
    xτ instead of the high dimensional vector yτ . The error correction compares
  id: totrans-8355
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高维动态系统的建模，下一步称为变体-不变分离的技巧，通过坐标变换扩展了 ECNN 的架构（图 28.5），以降低原始预测问题的维度。维度降低以瓶颈子网络的形式实现（图
    28.5，左）。压缩器 E 从动态中移除时间不变结构。完整动态的重建（解压缩）由矩阵 F 完成。瓶颈通过共享矩阵 E 和 F [15] 隐式连接到 ECNN。我们使用瓶颈神经网络将高维向量
    ydt 压缩为低维向量 xt。向量 xt 包含关于 ydt 的所有相关信息。ECNN 预测低维向量 xτ 而不是高维向量 yτ。误差校正进行比较。
- en: '![682_image_0.png](682_image_0.png)'
  id: totrans-8356
  prefs: []
  type: TYPE_IMG
  zh: '![682_image_0.png](682_image_0.png)'
- en: Fig. 28.5. ECNN with Variant-Invariant Separation
  id: totrans-8357
  prefs: []
  type: TYPE_NORMAL
  zh: 图 28.5. 带有变体-不变分离的 ECNN
- en: the expectations xτ = Csτ with the transformed observations −xdτ = E(−ydτ ).
  id: totrans-8358
  prefs: []
  type: TYPE_NORMAL
  zh: 期望值 xτ = Csτ 与变换后的观察值 −xdτ = E(−ydτ ) 进行比较。
- en: Note, that the negative inputs −ydτ are required by the ECNN to generate the
    transformed targets −xdτ . In our experience we found, that the training of the
    extended ECNN (Fig. 28.5) is very robust, i.e. the coordinate transformation and
    the forecasting can be trained in parallel. The time-invariant information is
    not lost in the bottleneck network, but simply relegated to lower components of
    variance of the representation. Furthermore, node pruning can be applied to the
    middle layer. Due to shared weights the result of a pruning in the bottleneck
    network is transfered to the ECNN branch as well.
  id: totrans-8359
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，负输入 −ydτ 是 ECNN 生成变换目标 −xdτ 所必需的。根据我们的经验，我们发现扩展的 ECNN（图 28.5）的训练非常稳健，即坐标变换和预测可以并行训练。时间不变的信息不会在瓶颈网络中丢失，而只是被降级到表示的较低方差成分。此外，可以对中间层应用节点剪枝。由于共享权重，瓶颈网络中剪枝的结果也会转移到
    ECNN 分支。
- en: We have successfully applied the ECNN depicted in Fig. 28.5 to forecast electrical
    load curves and traffic flows. In load forecasting the typical application scenario
    is that one has to forecast the load curve in 15 minute time buckets, i.e. 96
    observations per day. To avoid the error accumulation of a pure iterative model
    it is more useful to forecast the load curve day-by-day. From our experience a
    load curve (i.e. 96 observations per day) can be compressed to an dim ≈ 8 dimensional
    indicator vector from which the load curve can in turn be reconstructed. From
    a mathematical viewpoint this approach corresponds to the modeling of dynamical
    systems on manifolds. More complicated manifolds can be generated by deep bottleneck
    neural networks.
  id: totrans-8360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功地应用了图 28.5 所示的 ECNN 来预测电力负荷曲线和交通流。在负荷预测中，典型的应用场景是需要在 15 分钟的时间段内预测负荷曲线，即每天
    96 次观察。为了避免纯迭代模型的误差累积，逐日预测负荷曲线更为有用。根据我们的经验，负荷曲线（即每天 96 次观察）可以压缩成一个维度约为 8 的指标向量，从中可以重建负荷曲线。从数学角度来看，这种方法对应于在流形上建模动态系统。更复杂的流形可以通过深瓶颈神经网络生成。
- en: Trick 6. Historically Consistent Neural Networks (Hcnns)
  id: totrans-8361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧 6. 历史一致神经网络（HCNNs）
- en: Many real-world technical and economic applications can be seen in the context
    of large systems in which various (non-linear) dynamics interact with one another
    (in time). Unfortunately only a small sub-set of variables can be observed.
  id: totrans-8362
  prefs: []
  type: TYPE_NORMAL
  zh: 在大系统的背景下，许多现实世界的技术和经济应用可以看到，其中各种（非线性）动态相互作用（在时间上）。不幸的是，只有一小部分变量可以被观察到。
- en: 'From the sub-set of observed variables we have to reconstruct the hidden variables
    of the large system in order to understand the dynamics. Here the term observables
    includes the input and output variables in conventional modeling approaches (i.e.
    yτ := (yτ , uτ )). This indicates a consistency problem in the RNN (Fig. 28.1)
    or ECNN (Fig. 28.4) respectively: on the output side, the RNN'
  id: totrans-8363
  prefs: []
  type: TYPE_NORMAL
  zh: 从观察到的变量子集，我们必须重建大系统的隐藏变量，以理解动态。在这里，可观察量一词包括传统建模方法中的输入和输出变量（即 yτ := (yτ , uτ
    )）。这表明 RNN（图 28.1）或 ECNN（图 28.4）中存在一致性问题：在输出端，RNN
- en: (ECNN) provides forecasts of the dynamics in the observables yτ , whereas the
    input side assumes that the observables yτ will not change from present time on.
    This lack of consistency represents a clear contradiction within the model framework.
    If, on the other hand, we are able to implement a model framework in which common
    descriptions and forecasts can be used for the trend in all of the observables,
    we will be in a position to close the open system - in other words, we will model
    a closed large dynamic system.
  id: totrans-8364
  prefs: []
  type: TYPE_NORMAL
  zh: (ECNN) 提供了可观察量 yτ 的动态预测，而输入端假设可观察量 yτ 从现在开始不会变化。这种一致性的缺失在模型框架内表示出明显的矛盾。另一方面，如果我们能够实现一个模型框架，其中可以使用共同描述和预测来表示所有可观察量的趋势，我们将能够关闭开放系统——换句话说，我们将对一个封闭的大动态系统建模。
- en: The next trick called historically consistent neural networks (HCNNs) introduces
    a model class which follows the design principles for modeling of large dynamic
    systems and overcomes the conceptual weaknesses of conventional models.
  id: totrans-8365
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个技巧称为历史一致神经网络（HCNNs），它引入了一种模型类，遵循大动态系统建模的设计原则，并克服传统模型的概念缺陷。
- en: Equation 28.5 formulates the historically consistent neural network (HCNN).
  id: totrans-8366
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 28.5 构造了历史一致神经网络（HCNN）。
- en: $$i_{\tau}\}$$
  id: totrans-8367
  prefs: []
  type: TYPE_NORMAL
  zh: $$i_{\tau}\}$$
- en: $$(28.5)$$
  id: totrans-8368
  prefs: []
  type: TYPE_NORMAL
  zh: $$(28.5)$$
- en: state transition sτ+1 = A tanh(sτ )
  id: totrans-8369
  prefs: []
  type: TYPE_NORMAL
  zh: 状态转移 sτ+1 = A tanh(sτ )
- en: output equation yτ = [Id, 0]sτ(28.5)
  id: totrans-8370
  prefs: []
  type: TYPE_NORMAL
  zh: 输出方程 yτ = [Id, 0]sτ(28.5)
- en: The joint dynamics for all observables is characterized in the HCNN (28.5) by
    the sequence of states sτ . The observables (i = 1*,...,N*) are arranged on the
    first N state neurons sτ and followed by non-observable (hidden) variables as
    subsequent neurons. The connector [Id, 0] is a fixed matrix which reads out the
    observables. The initial state s0 is described as a bias vector. The bias s0 and
    matrix A contain the only free parameters.
  id: totrans-8371
  prefs: []
  type: TYPE_NORMAL
  zh: 所有可观察量的联合动态在HCNN（28.5）中由状态序列sτ特征化。可观察量(i = 1*,...,N*)排列在前N个状态神经元sτ上，后面是作为后续神经元的不可观察（隐藏）变量。连接器[Id,
    0]是一个固定矩阵，用于读取可观察量。初始状态s0被描述为偏置向量。偏置s0和矩阵A包含唯一的自由参数。
- en: 'Like standard RNNs (Fig. 28.1) HCNNs also have universal approximation capabilities.
    The proof for the RNN can be found in [11]. Fig. 28.6 outlines the proof for HCNNs.
    As depicted in Fig. 28.6 the proof of the universal approximation capabilities
    for HCNN can be divided in six steps:'
  id: totrans-8372
  prefs: []
  type: TYPE_NORMAL
  zh: 像标准RNN（图28.1）一样，HCNN也具有通用逼近能力。RNN的证明可以在[11]中找到。图28.6概述了HCNN的证明。如图28.6所示，HCNN的通用逼近能力证明可以分为六个步骤：
- en: 1. The output equation is shifted one time step into the future and the resulting
    sτ+1 is substituted by the system transition equation.
  id: totrans-8373
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 输出方程被推移一步到未来，得到的sτ+1被系统转移方程替代。
- en: 2. By combining outputs and state variables into an extended state we get an
    extended state transition equation. The output of the system is derived from the
    first components of the extended internal state.
  id: totrans-8374
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 通过将输出和状态变量组合成扩展状态，我们得到一个扩展状态转移方程。系统的输出来源于扩展内部状态的第一个组件。
- en: 3. For the extended state transition equation we apply the feedforward universal
    approximation theorem. At least for a finite time horizon this guarantees a small
    approximation error. Note, that in RNNs at least one large component of the state
    vector together with the hyperbolic tangent can mimic a bias vector. Thus, we
    have omitted the explicit notation of a bias vector in the NN equations.
  id: totrans-8375
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 对于扩展状态转移方程，我们应用前馈通用逼近定理。至少对于有限时间范围，这保证了较小的逼近误差。请注意，在RNN中，状态向量中的至少一个大分量与双曲正切可以模拟偏置向量。因此，我们在NN方程中省略了偏置向量的显式表示。
- en: 4. In this step we remove one of the two matrices within the state transition
    equation. We apply a state transformation rτ = Asτ . This results in two state
    transition equations.
  id: totrans-8376
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 在这一步中，我们移除了状态转移方程中的两个矩阵之一。我们应用状态变换rτ = Asτ。这导致两个状态转移方程。
- en: 5. The two state transition equations can be reorganized in one state transition,
    which has twice the dimensionality of the original equation.
  id: totrans-8377
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 这两个状态转移方程可以重新组织为一个状态转移方程，其维数是原始方程的两倍。
- en: 6. Rewriting the matrix located on front of the tanh activation function results
    in the claimed formulation for closed systems.
  id: totrans-8378
  prefs: []
  type: TYPE_NORMAL
  zh: 6. 重新写位于tanh激活函数前的矩阵得出了封闭系统的声明性公式。
- en: Instead of being applied inside the tanh activation function, matrix A is used
    outside the tanh activation function. This has the advantage that the states and
    thus, also the system outputs are not limited to the finite state space (−1; 1)n
    created by the tanh(.) nonlinearity. The output equation has a simple and application
    independent form. Note, that we can only observe the first elements of the state
    vector.
  id: totrans-8379
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵A并不是在tanh激活函数内部应用，而是在tanh激活函数外部使用。这有一个优势，即状态以及系统输出不受tanh(.)非线性所创建的有限状态空间(−1;
    1)n的限制。输出方程具有简单且与应用无关的形式。请注意，我们只能观察状态向量的第一个元素。
- en: '![684_image_0.png](684_image_0.png)'
  id: totrans-8380
  prefs: []
  type: TYPE_IMG
  zh: '![684_image_0.png](684_image_0.png)'
- en: Fig. 28.6. Proof of the universal approximation capabilities for HCNNs
  id: totrans-8381
  prefs: []
  type: TYPE_NORMAL
  zh: 图28.6. HCNN的通用逼近能力证明
- en: '![684_image_1.png](684_image_1.png)'
  id: totrans-8382
  prefs: []
  type: TYPE_IMG
  zh: '![684_image_1.png](684_image_1.png)'
- en: Fig. 28.7. Historically Consistent Neural Network (HCNN)
  id: totrans-8383
  prefs: []
  type: TYPE_NORMAL
  zh: 图28.7. 历史一致神经网络（HCNN）
- en: Fig.28.7 depicts the HCNN architecture. The HCNN states sτ are hidden layers
    with tanh squashing. The forecasts are supplied by the output layers yτ .
  id: totrans-8384
  prefs: []
  type: TYPE_NORMAL
  zh: 图28.7展示了HCNN架构。HCNN状态sτ是具有tanh压缩的隐藏层。预测由输出层yτ提供。
- en: There are no target values available for the future time steps. The expected
    values yτ>t can be read out at the corresponding future time steps of the network.
  id: totrans-8385
  prefs: []
  type: TYPE_NORMAL
  zh: 未来时间步没有可用的目标值。期望值yτ>t可以在网络对应的未来时间步中读取。
- en: Since the HCNN model has no inputs, we have to unfold the neural network along
    the complete data history. This is different to small recurrent neural networks
    (see e.g. Fig. 28.2), where we construct training data patterns in form of sliding
    windows. The HCNN learns the large dynamics from a single history of observations
    (i.e. a single training data pattern). Forecasting commodity prices with HCNN,
    we unfold the neural network over 440 trading days in the past to predict the
    next 20 days.
  id: totrans-8386
  prefs: []
  type: TYPE_NORMAL
  zh: 由于HCNN模型没有输入，我们必须沿着完整的数据历史展开神经网络。这与小型递归神经网络不同（参见图28.2），后者是通过滑动窗口构建训练数据模式。HCNN从单一观察历史（即单一训练数据模式）中学习大规模动态。在用HCNN预测商品价格时，我们将在过去的440个交易日上展开神经网络，以预测接下来的20天。
- en: Trick 7. Architectural Teacher Forcing (Atf) For Hcnns
  id: totrans-8387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧7. 针对HCNN的架构教师强制（ATF）
- en: In practice we observe that HCNNs are difficult to train since the models do
    not have any input signals and are unfolded across the complete data set. Our
    next trick, architectural teacher forcing (ATF) for HCNNs, makes the best possible
  id: totrans-8388
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，我们观察到HCNN的训练比较困难，因为模型没有任何输入信号，并且在整个数据集上展开。我们的下一个技巧，针对HCNN的架构教师强制（ATF），使得训练尽可能高效。
- en: '![685_image_0.png](685_image_0.png)'
  id: totrans-8389
  prefs: []
  type: TYPE_IMG
  zh: '![685_image_0.png](685_image_0.png)'
- en: use of the data from the observables and accelerates the training of the HCNN
    [20, 14, 9]. The HCNN with integrated teacher forcing is shown in Fig. 28.8 below.
  id: totrans-8390
  prefs: []
  type: TYPE_NORMAL
  zh: 使用可观测量的数据，并加速HCNN的训练[20, 14, 9]。下图28.8显示了集成教师强制的HCNN。
- en: In the HCNN with ATF (Fig. 28.8) the expected values for all observables up
  id: totrans-8391
  prefs: []
  type: TYPE_NORMAL
  zh: 在带有ATF的HCNN（图28.8）中，所有可观测量的期望值
- en: Fig. 28.8. HCNN with Architectural Teacher Forcing (ATF)
  id: totrans-8392
  prefs: []
  type: TYPE_NORMAL
  zh: 图28.8. 带有架构教师强制（ATF）的HCNN
- en: to time τ = t are replaced with the actual observations. The output layers of
    the HCNN are given fixed target values of zero. The negative observed values
  id: totrans-8393
  prefs: []
  type: TYPE_NORMAL
  zh: 到时间τ = t的实际观察值替代了期望值。HCNN的输出层被赋予固定的目标值为零。负的观察值
- en: −ydτ for the observables are added to the output layer. This forces the HCNN
  id: totrans-8394
  prefs: []
  type: TYPE_NORMAL
  zh: −ydτ 被添加到输出层。这迫使HCNN
- en: to create the expected values yτ to compensate for the negative observed values
  id: totrans-8395
  prefs: []
  type: TYPE_NORMAL
  zh: 以创建期望值yτ来补偿负的观察值
- en: −ydτ . The content of the output layer, i.e. yτ − ydτ , is now transferred to
    the first N neurons of the hidden layer rτ on a component-by-component basis with
    a minus symbol. In addition, we copy the expected values yτ from the state sτ
    to the intermediate hidden layer rτ . As a result, the expected values yτ on the
    first N components of the state vector sτ are replaced by the observed values
    ydτ = yτ −(yτ −ydτ ) (Fig. 28.8). All connections of the ATF mechanism are fixed.
  id: totrans-8396
  prefs: []
  type: TYPE_NORMAL
  zh: −ydτ 。输出层的内容，即yτ − ydτ ，现在以组件为单位传递到隐藏层rτ的前N个神经元，并带有负号。此外，我们将状态sτ中的期望值yτ复制到中间隐藏层rτ。结果，状态向量sτ的前N个组件上的期望值yτ被观察值ydτ
    = yτ −(yτ −ydτ )所替代（图28.8）。ATF机制的所有连接都是固定的。
- en: Following the ATF step, the state transition matrix A is applied, to move the
    system into the next time step. By definition, we have no observations for the
    observables in future time steps. Here, the system is iterated exclusively upon
    the expected values. This turns an open into a closed dynamic system. The HCNN
    in Fig. 28.8 is equivalent to the architecture in Fig.28.7, if it converges to
    zero error in the training. In this case we have solved the original problem.
  id: totrans-8397
  prefs: []
  type: TYPE_NORMAL
  zh: 在ATF步骤之后，状态转移矩阵A被应用，以将系统移动到下一个时间步。根据定义，我们对未来时间步的可观测量没有任何观察。在这里，系统仅基于期望值进行迭代。这将一个开放动态系统转变为一个闭合动态系统。如果HCNN在训练中收敛到零误差，那么图28.8中的HCNN等同于图28.7中的架构。在这种情况下，我们已经解决了原始问题。
- en: Trick 8. Sparsity, Dimensionality Vs. Connectivity And Memory
  id: totrans-8398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧8. 稀疏性、维度与连接性和记忆
- en: 'HCNNs may have to model ten, twenty or even more observables in parallel over
    time. It is clear that we have to work with high dimensional dynamical systems
    (e.g. in our commodity price models we use dim(s) = 300). The iteration with a
    fully connected state transition matrix A of such a dimension is dangerous: Sometimes
    the matrix vector operations will produce large numbers which will be spread in
    the recursive computation all over the network and will generate an arithmetic
    overflow. To avoid this phenomena we can choose a sparse matrix A. Thus, the linear
    algebra does not accumulate large numbers and the spread of large numbers through
    the network is damped by the sparsity too.'
  id: totrans-8399
  prefs: []
  type: TYPE_NORMAL
  zh: HCNNs可能需要在时间上并行建模十个、二十个甚至更多的可观察量。显然，我们必须处理高维动态系统（例如，在我们的商品价格模型中，我们使用dim(s) =
    300）。这样维度的全连接状态转移矩阵A的迭代是危险的：有时矩阵向量运算会产生大的数字，这些数字会在递归计算中传播到整个网络并导致算术溢出。为了避免这种现象，我们可以选择稀疏矩阵A。因此，线性代数不会累积大数字，稀疏性也会抑制大数字在网络中的传播。
- en: $$(28.6)$$
  id: totrans-8400
  prefs: []
  type: TYPE_NORMAL
  zh: $$(28.6)$$
- en: 'We have to answer the questions which dimensionality and which sparsity we
    will choose. In [16] we have worked out that dimensionality and sparsity are related
    to another pair of meta-parameters: Connectivity (con) and memory length (mem).
    Connectivity is defined as the number of nonzero elements in each row of matrix
    A. The memory length is the number of steps from which we have to collect information
    in order to reach a Markovian state, i.e. the state vector contains all necessary
    information from the past. We propose the following parameterization for the state
    dimension (dim(s)) and sparsity [16]:'
  id: totrans-8401
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须回答选择哪个维度和稀疏度的问题。在[16]中，我们发现维度和稀疏度与另一对元参数相关：连接性（con）和记忆长度（mem）。连接性定义为矩阵A每行中的非零元素数量。记忆长度是我们必须收集信息以达到马尔可夫状态所需的步骤数，即状态向量包含过去的所有必要信息。我们提出以下参数化来定义状态维度（dim(s)）和稀疏度[16]：
- en: $${\mathrm{Dimension~of~}}A\quad\quad d i m(s)\ =c o n\cdot m e m$$
  id: totrans-8402
  prefs: []
  type: TYPE_NORMAL
  zh: $${\mathrm{维度~of~}}A\quad\quad d i m(s)\ =c o n\cdot m e m$$
- en: imension of $A\quad\quad dim(s)\ =con\cdot mem$ (28.6)  $$\text{Sparsity of}A\quad\text{Sparsity}=random\left(\frac{con}{mem\cdot
    con}\right)=random\left(\frac{1}{mem}\right)\tag{28.7}$$
  id: totrans-8403
  prefs: []
  type: TYPE_NORMAL
  zh: $A$的维度\quad\quad dim(s)\ =con\cdot mem$ (28.6) $$\text{稀疏度}A\quad\text{稀疏度}=random\left(\frac{con}{mem\cdot
    con}\right)=random\left(\frac{1}{mem}\right)\tag{28.7}$$
- en: Eq. 28.7 represents the insight that a sparse system conserves information over
    a longer time period before it diffuses in the network. For instance a shift register
    is very sparse and behaves only as a memory, whereas in a fully connected matrix
    the superposition of information masks the information sources. Let us assume
    that we have initialized the state transition matrix with a uniform random sparse
    matrix A. Following Eq. 28.7 the more dense parts of A will model the faster sub-dynamics
    within the overall dynamics, while the highly sparse parts of A will focus on
    slow subsystems. As a result a sparse random initialization allows the combined
    modeling of systems on different time scales.
  id: totrans-8404
  prefs: []
  type: TYPE_NORMAL
  zh: 等式28.7表明稀疏系统在信息扩散到网络之前，可以在更长的时间内保持信息。例如，移位寄存器非常稀疏，只作为一种存储，而在完全连接的矩阵中，信息的叠加会掩盖信息源。假设我们已经用均匀随机稀疏矩阵A初始化了状态转移矩阵。根据等式28.7，A中更稠密的部分将模拟整体动态中的快速子动态，而A中高度稀疏的部分将专注于慢速子系统。因此，稀疏随机初始化允许对不同时间尺度的系统进行综合建模。
- en: Unfortunately, Eq. 28.6 favors very large dimensions. Our earlier work on the
    subject (see [16]) started with the predefinition of the systems memory length
    mem, because for RNNs the memory length is equal to the length of the past unfolding
    in time. On the other hand, connectivity has to be chosen larger than the number
    of the observables. Working with HCNNs the memory length is less important, because
    we unfold the neural network along the whole data horizon. Here the connectivity
    plays the superior role. From our experience we know that the EBTT algorithm works
    stably with a connectivity which is equal or smaller than 50 (con ≤ 50). For computational
    performance we usually limit the state dimensionality to dim(s) = 300. This implies
    a sparsity of 50/300 ≈ 17%. We leave the fine tuning of the parameters to the
    EBTT learning.
  id: totrans-8405
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，公式28.6倾向于非常大的维度。我们早期的研究（见[16]）以系统的记忆长度mem的预定义开始，因为对于RNN，记忆长度等于时间中过去展开的长度。另一方面，连接性必须选择大于可观测量的数量。使用HCNN时，记忆长度不那么重要，因为我们沿整个数据视野展开神经网络。在这里，连接性发挥着更重要的作用。根据我们的经验，我们知道EBTT算法在连接性等于或小于50（con
    ≤ 50）时稳定运行。为了计算性能，我们通常将状态维度限制为dim(s) = 300。这意味着稀疏性为50/300 ≈ 17%。我们将参数的微调留给EBTT学习。
- en: We propose to initialize the neural network with a randomly chosen sparsity
    grid. The sparsity grid is therefore chosen arbitrary and not optimized by e.g.
    pruning algorithms. This raises the question if a random sparse initialization
    biases the network towards inferior solutions. This is handled by ensemble forecasts.
    We have performed ensemble experiments with different sparsity grids versus ensembles
    based on the same sparsity grid. We found, that the average of the ensemble as
    well as the ensemble width are unaffected by the initialization of the sparsity
    grid (for more details on ensemble forecasting see Trick 12). These considerations
    hold only for large systems.
  id: totrans-8406
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议用随机选择的稀疏网格初始化神经网络。因此，稀疏网格是任意选择的，而不是通过如修剪算法等进行优化。这引发了一个问题，即随机稀疏初始化是否会使网络偏向较差的解决方案。这通过集合预测来处理。我们进行了不同稀疏网格与基于相同稀疏网格的集合的集合实验。我们发现，集合的平均值以及集合宽度不受稀疏网格初始化的影响（有关集合预测的更多细节见技巧12）。这些考虑仅适用于大系统。
- en: Trick 9. Causal-Retro-Causal Neural Networks (Crcnns)
  id: totrans-8407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧9. 因果-逆因果神经网络（Crcnns）
- en: The fundamental idea of the HCNN is to explain the joint dynamics of the observables
    in a causal manner, i.e. with an information flow from the past to the future.
    However, rational planning is not only a consequence of a causal information flow
    but also of anticipating future developments and responding to them on the basis
    of a certain goal function. This is similar to the adjoint equation in optimal
    control theory [6]. In other words a retro causal information flow is equivalent
    to asking for the motivation of a behavior as a goal. In turn, this is the anchor
    point for the reconstruction of the dynamics.
  id: totrans-8408
  prefs: []
  type: TYPE_NORMAL
  zh: HCNN的基本思想是以因果方式解释可观测量的联合动态，即信息流从过去流向未来。然而，合理的规划不仅是因果信息流的结果，还包括预测未来发展并根据某个目标函数作出反应。这类似于最优控制理论中的伴随方程[6]。换句话说，逆因果信息流相当于询问行为的动机作为目标。这反过来是重建动态的锚点。
- en: In order to incorporate the effects of rational decision making and planning
    into the modeling, the next trick introduces causal-retro-causal neural networks
    (CRCNNs). The idea behind the CRCNN is to enrich the causal information flow within
    the HCNN, which is directed from the past to the future, by a retrocausal information
    flow, directed from the future into the past. The CRCNN
  id: totrans-8409
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将理性决策和规划的影响纳入建模，下一个技巧引入因果-逆因果神经网络（CRCNNs）。CRCNN背后的想法是通过从未来到过去的逆因果信息流来丰富HCNN中从过去到未来的因果信息流。CRCNN
- en: model is given by the following set of equations 28.8.
  id: totrans-8410
  prefs: []
  type: TYPE_NORMAL
  zh: 模型由以下一组方程28.8给出。
- en: causal state transition $s_{\tau}=A\tanh(s_{\tau-1})$  retro-causal state transition
    $s^{\prime}_{\tau}=A^{\prime}\tanh(s^{\prime}_{\tau+1})$  output equation $y_{\tau}=[Id,0]s_{\tau}+[Id,0]s^{\prime}_{\tau}$.
  id: totrans-8411
  prefs: []
  type: TYPE_NORMAL
  zh: 因果状态转移 $s_{\tau}=A\tanh(s_{\tau-1})$  逆因果状态转移 $s^{\prime}_{\tau}=A^{\prime}\tanh(s^{\prime}_{\tau+1})$  输出方程
    $y_{\tau}=[Id,0]s_{\tau}+[Id,0]s^{\prime}_{\tau}$。
- en: The output equation yτ of the CRCNN (Eq. 28.8) is a mixture of causal and
  id: totrans-8412
  prefs: []
  type: TYPE_NORMAL
  zh: CRCNN的输出方程yτ（公式28.8）是因果和
- en: '![687_image_0.png](687_image_0.png)'
  id: totrans-8413
  prefs: []
  type: TYPE_IMG
  zh: '![687_image_0.png](687_image_0.png)'
- en: retro-causal influences. The dynamics of all observables is hence explained
    by a sequence of causal (sτ ) and retro-causal states sτ using transition matrices
    A and A for the causal and retro-causal information flow. Upon the basis of Eq.
    28.8, we draw the network architecture for the CRCNN as depicted in Fig. 28.9.
  id: totrans-8414
  prefs: []
  type: TYPE_NORMAL
  zh: 反因果影响。因此，所有可观察量的动态是通过因果状态 (sτ) 和反因果状态 sτ 的序列来解释的，使用因果和反因果信息流的转移矩阵 A 和 A。基于方程
    28.8，我们绘制了图 28.9 中所示的 CRCNN 网络架构。
- en: Fig. 28.9. A Causal-Retro-Causal Historically Consistent Neural Network (CRCNN)
  id: totrans-8415
  prefs: []
  type: TYPE_NORMAL
  zh: 图 28.9. 一种因果-反因果历史一致神经网络 (CRCNN)。
- en: Trick 10. Architectural Teacher Forcing (Atf) For Crcnns
  id: totrans-8416
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧 10. 建筑教师强制 (Atf) 用于 CRCNN。
- en: The CRCNN (Fig. 28.9) is also unfolded across the entire time path, i.e. we
    learn the unique history of the system. Likewise to the training of the HCNN,
    CRCNNs are difficult to train. Our next trick called architectural teacher forcing
    (ATF) for CRCNNs formulates TF as a part of the CRCNN architecture, which allows
    us to learn the CRCNN using the standard EBTT algorithm[3]. ATF enables us to
    exploit the information contained in the data more efficiently and accelerates
    the training itself. Fig. 28.10 depicts the CRCNN architecture incorporating ATF.
  id: totrans-8417
  prefs: []
  type: TYPE_NORMAL
  zh: CRCNN (图 28.9) 也在整个时间路径上展开，即我们学习系统的独特历史。与 HCNN 的训练类似，CRCNN 的训练也较为困难。我们的下一个技巧称为用于
    CRCNN 的建筑教师强制 (ATF)，将 TF 形式化为 CRCNN 架构的一部分，这使我们能够使用标准的 EBTT 算法学习 CRCNN。ATF 使我们能够更有效地利用数据中的信息，并加快训练过程。图
    28.10 描绘了结合 ATF 的 CRCNN 架构。
- en: '![688_image_0.png](688_image_0.png)'
  id: totrans-8418
  prefs: []
  type: TYPE_IMG
  zh: '![688_image_0.png](688_image_0.png)。'
- en: Fig. 28.10. Extended CRCNN with an architectural Teacher Forcing (ATF) mechanism
  id: totrans-8419
  prefs: []
  type: TYPE_NORMAL
  zh: 图 28.10. 带有建筑教师强制 (ATF) 机制的扩展 CRCNN。
- en: Let us explain the ATF mechanism in the extended CRCNN model
  id: totrans-8420
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释扩展 CRCNN 模型中的 ATF 机制。
- en: '(Fig. 28.10): The extended CRCNN uses a causal-retro-causal network to correct
    the error of the opposite part of the network in a symmetric manner. In every
    time step τ ≤ t the expected values yτ are replaced by the observations ydτ using
    the intermediate tanh() layers for the causal and for the retro-causal part. Since
    the causal and the retro-causal part *jointly* explain the observables yτ , we
    have to inject the causal into the retro-causal part and vice versa. This is done
    by compensating the actual observations −ydτ with the output of the causal (Δτ
    )'
  id: totrans-8421
  prefs: []
  type: TYPE_NORMAL
  zh: (图 28.10)：扩展的 CRCNN 使用因果-反因果网络对网络的对侧部分进行对称性误差校正。在每个时间步 τ ≤ t 上，期望值 yτ 被观察值 ydτ
    替代，使用中间的 tanh() 层用于因果部分和反因果部分。由于因果和反因果部分 *共同* 解释了可观察量 yτ ，我们必须将因果部分注入反因果部分，反之亦然。这是通过用因果输出
    (Δτ) 来补偿实际观察值 −ydτ 完成的。
- en: and the retro-causal part (Δτ ) within output layers with fixed target values
    of zero. The resulting content (Δτ + Δτ − ydτ = 0) of the output layers is negated
    and transferred to the causal and retro-causal part of the CRCNN using the fixed
  id: totrans-8422
  prefs: []
  type: TYPE_NORMAL
  zh: 和输出层中固定目标值为零的反因果部分 (Δτ)。输出层的结果内容 (Δτ + Δτ − ydτ = 0) 被取反并转移到 CRCNN 的因果和反因果部分。
- en: '[−Id, 0] connector. Within the intermediate tanh() layers the expectations
    of yτ are replaced with the actual observations ydτ , whereby the contribution
    to yτ from the opposite part of the CRCNN is considered. Note, that ATF does not
    lead to a larger number of free network parameters, since all new connections
    are fixed and are used only to transfer data in the NN. In future direction τ>t
    the CRCNN is iterated exclusively on the basis of expected values.'
  id: totrans-8423
  prefs: []
  type: TYPE_NORMAL
  zh: '[−Id, 0] 连接器。在中间的 tanh() 层中，yτ 的期望被实际观察值 ydτ 替代，同时考虑来自 CRCNN 对侧的贡献。注意，ATF 不会导致更多的自由网络参数，因为所有新的连接都是固定的，仅用于在神经网络中传递数据。在未来的方向
    τ>t 上，CRCNN 仅基于期望值进行迭代。'
- en: The usage of ATF does not reintroduce an input / output modeling, since we replace
    the expected value of the observables with their actual observations, while simultaneously
    considering the causal and retro-causal part of the dynamics. For sufficiently
    large CRCNNs and convergence of the output error to zero, the architecture in
    Fig. 28.10 converges towards the fundamental CRCNN architecture shown in Fig.
    28.9. The advantage of the CRCNN is, that it allows a fully dynamical superposition
    of the causal and retro-causal information flows.
  id: totrans-8424
  prefs: []
  type: TYPE_NORMAL
  zh: ATF 的使用并不会重新引入输入/输出建模，因为我们用实际观察值替代可观察量的期望值，同时考虑动态的因果和反因果部分。对于足够大的 CRCNN 和输出误差收敛到零，图
    28.10 中的架构收敛于图 28.9 所示的基本 CRCNN 架构。CRCNN 的优势在于它允许因果和反因果信息流的完全动态叠加。
- en: The CRCNN depicted in Fig. 28.10 describes a dynamics on a manifold. In every
    time step the information flows incorporates closed loops, which technically can
    be seen as equality constraints. These constraints are only implicitly defined
    through the interaction of the causal and retro-causal parts. The closed loops
    in the CRCNN architecture (Fig. 28.10) lead to fix-point recurrent substructures
    within the model, which are hard to handle with EBTT. As a remedy we propose
  id: totrans-8425
  prefs: []
  type: TYPE_NORMAL
  zh: 图 28.10 中描绘的 CRCNN 描述了在流形上的动态。在每个时间步，信息流包含闭环，这在技术上可以看作是等式约束。这些约束通过因果部分和反因果部分的相互作用隐式定义。CRCNN
    结构中的闭环（图 28.10）导致模型中固定点递归子结构的出现，这些结构在 EBTT 中难以处理。为此，我们提出
- en: '![689_image_0.png](689_image_0.png)'
  id: totrans-8426
  prefs: []
  type: TYPE_IMG
  zh: '![689_image_0.png](689_image_0.png)'
- en: 'a solution concept similar to Trick 7: We embed the CRCNN model into a larger
    network architecture, which is easier to solve and converges to the same solution
    as the original system. Fig. 28.11 depicts an initial draft for such an embedding.'
  id: totrans-8427
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于技巧 7 的解决方案概念：我们将 CRCNN 模型嵌入到一个更大的网络架构中，这样更容易解决，并且收敛到与原系统相同的解决方案。图 28.11 描绘了这样一个嵌入的初步草图。
- en: The extended architecture in Fig. 28.11 is a duplication of the original model
    depicted in Fig. 28.10. The CRCNN architecture in Fig. 28.11 does not contain
    closed loops, because we split the ATF mechanism for the causal and retrocausal
    part into two branches. It is important to notice that these branches are implicitly
    connected through the shared weights in the causal and retro-causal part. If this
    architecture converges, the ATF is no longer required and we have two identical
    copies of the CRCNN model depicted in Fig. 28.9. The solution proposed for the
    embedding is not the only feasible way to handle the fix-point loops. We will
    outline alternative solutions in an upcoming paper. The CRCNN is the basis for
    our projects on forecasting commodity prices.
  id: totrans-8428
  prefs: []
  type: TYPE_NORMAL
  zh: 图 28.11 中的扩展架构是图 28.10 中原始模型的复制。图 28.11 中的 CRCNN 架构不包含闭环，因为我们将因果和反因果部分的 ATF
    机制分成两个分支。重要的是要注意，这些分支通过因果和反因果部分的共享权重隐式连接。如果该架构收敛，则不再需要 ATF，我们将得到两个与图 28.9 中描述的
    CRCNN 模型相同的副本。我们为嵌入提出的解决方案并不是处理固定点循环的唯一可行方法。我们将在即将发表的论文中概述替代解决方案。CRCNN 是我们预测商品价格项目的基础。
- en: Fig. 28.11. Asymmetric split of ATF in CRC neural networks
  id: totrans-8429
  prefs: []
  type: TYPE_NORMAL
  zh: 图 28.11. CRC 神经网络中 ATF 的不对称分割
- en: Trick 11. Stable & Instable Information Flows In Dynamical Systems
  id: totrans-8430
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧 11. 动态系统中的稳定与不稳定信息流
- en: 'Following Trick 3 it is natural to apply noise in the causal as well as in
    retrocausal branch (see also Fig. 28.12). This should improve the stability of
    both time directions resp. information flows. In CRCNNs this feature has a special
    interpretation: Stability in the causal information flow means that the uncertainty
    in the beginning is damped along the time path from past to future. Instability
    in the causal system means that a small disturbance in the past diffuses to very
    different future scenarios under a chaotic regime (see Fig. 28.12, upper part).'
  id: totrans-8431
  prefs: []
  type: TYPE_NORMAL
  zh: 根据技巧 3，给因果分支和反因果分支施加噪声是自然而然的（另见图 28.12）。这应该改善两个时间方向即信息流的稳定性。在 CRCNN 中，这一特性具有特殊的解释：因果信息流的稳定性意味着一开始的不确定性沿着从过去到未来的时间路径被减弱。因果系统的不稳定性意味着在混沌状态下，过去的小干扰会扩散到非常不同的未来情境（见图
    28.12，上部分）。
- en: '![690_image_0.png](690_image_0.png)'
  id: totrans-8432
  prefs: []
  type: TYPE_IMG
  zh: '![690_image_0.png](690_image_0.png)'
- en: Fig. 28.12. An instable causal dynamics is converted to a stable retro-causal
    dynamics
  id: totrans-8433
  prefs: []
  type: TYPE_NORMAL
  zh: 图 28.12. 不稳定的因果动态转变为稳定的反因果动态
- en: If the causal information flow of a sub-dynamics is instable, then the retrocausal
    description of this sub-system is stable. On the other hand, an instable retro-causal
    dynamics is stable from a causal perspective. In a combined causalretro-causal
    neural network the causal and the retro-causal branch can be simultaneously stable,
    even if the underlying dynamics is partially instable. In order to enforce the
    stability of the causal and the retro-causal part we apply noise at the origins
    of both branches.1
  id: totrans-8434
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个子动态的因果信息流不稳定，则该子系统的反因果描述是稳定的。另一方面，从因果的角度来看，不稳定的反因果动态是稳定的。在一个结合因果和反因果的神经网络中，即使基础动态部分不稳定，因果和反因果分支也可以同时稳定。为了加强因果和反因果部分的稳定性，我们在两个分支的起始点施加噪声。1
- en: Trick 12. Uncertainty And Risk
  id: totrans-8435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧 12. 不确定性与风险
- en: The experience gained during the latest financial crisis has triggered a farreaching
    discussion on the limitations of quantitative forecasting models and made investors
    very conscious of risk[2]. In order to understand risk distributions, traditional
    risk management uses diffusion models. Risk is understood as a random walk, in
    which the diffusion process is calibrated by the observed past error of the underlying
    model[7]. In contrast the next trick called uncertainty and risk focuses on ensemble
    forecasts in order to provide important insights into complex risk relationships,
    since internal model (unobserved) variables can be reconstructed from the trend
    in observed variables (observables).
  id: totrans-8436
  prefs: []
  type: TYPE_NORMAL
  zh: 在最新金融危机中获得的经验引发了关于定量预测模型局限性的广泛讨论，使得投资者对风险有了更强的意识[2]。为了理解风险分布，传统风险管理使用扩散模型。风险被理解为随机游走，其中扩散过程是通过观察到的潜在模型的过去误差进行校准的[7]。相比之下，下一个称为不确定性与风险的技巧侧重于集成预测，以便提供对复杂风险关系的重要洞察，因为内部模型（未观察到的）变量可以从观察变量（可观察）的趋势中重建。
- en: If the system identification is calculated repeatedly for HCNNs or CRCNNs, an
    ensemble of solutions will be produced, which all have a forecast error of zero
    in the past, but which differ from one another in the future. Since every HCNN
    or CRCNN model gives a perfect description of the observed data, the complete
    ensemble is the true solution. A way to simplify the forecast is to take the arithmetical
    average of the individual ensemble members as the expected value, provided the
    ensemble histogram is unimodal in every time step.
  id: totrans-8437
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对HCNNs或CRCNNs反复进行系统识别，将会产生一个解的集成，所有解在过去的预测误差为零，但在未来却相互不同。由于每个HCNN或CRCNN模型对观察数据的描述都很完美，因此完整集成是正确解。简化预测的一种方法是将个别集成成员的算术平均值作为期望值，前提是集成直方图在每个时间步骤中是单峰的。
- en: 'In addition to the expected value, we consider the bandwidth of the ensemble,
    i.e. its distribution. The form of the ensemble is governed by differences in
    the reconstruction of the hidden system variables from the observables: for every
    finite'
  id: totrans-8438
  prefs: []
  type: TYPE_NORMAL
  zh: 除了期望值外，我们还考虑集成的带宽，即其分布。集成的形式受可观察变量对隐藏系统变量重建差异的影响：对于每个有限的
- en: 1 Thanks to Prof. Jürgen Jost, MPI Leipzig, for a fruitful discussion on this
    topic.
  id: totrans-8439
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢莱比锡MPI的Jürgen Jost教授对此主题的有益讨论。
- en: volume of observations there is an infinite number of explanation models which
    describe the data perfectly, but differ in their forecasts, since the observations
    make it possible to reconstruct the hidden variables in different forms during
    the training. In other words, our risk concept is based on the partial observability
    of the world, leading to different reconstructions of the hidden variables and
    thus, different future scenarios. Since all scenarios are perfectly consistent
    with the history, we do not know which of the scenarios describes the future trend
    best and risk emerges.
  id: totrans-8440
  prefs: []
  type: TYPE_NORMAL
  zh: 在观察量方面，有无限多的解释模型可以完美描述数据，但在其预测上有所不同，因为观察使得在训练期间以不同形式重建隐藏变量成为可能。换句话说，我们的风险概念基于世界的部分可观察性，导致对隐藏变量的不同重建，从而产生不同的未来场景。由于所有场景都与历史完全一致，我们不知道哪个场景最能描述未来趋势，因此风险产生。
- en: This approach directly addresses the model risk. For HCNN and CRCNN
  id: totrans-8441
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法直接解决了模型风险。对于HCNN和CRCNN
- en: 'modeling we claim that the model risk is equal to the forecast risk. The reasons
    can be summarized as follows: First, HCNNs are universal approximators, which
    are therefore able to describe every market scenario. Second, the form of the
    ensemble distribution is caused by an underlying dynamics, which interpret the
    market dynamics as the result of interacting decisions[17]. Third, in experiments
    we have shown that the ensemble distribution is independent from the details of
    the model configuration, if we use large models and large ensembles.'
  id: totrans-8442
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模中，我们声称模型风险等于预测风险。原因可以总结如下：首先，HCNNs是通用逼近器，因此能够描述每一个市场场景。其次，集成分布的形式是由潜在动态引起的，这将市场动态解释为相互决策的结果[17]。第三，在实验中，我们已表明，集成分布独立于模型配置的细节，如果我们使用大型模型和大规模集成。
- en: Let us exemplify our risk concept. The diagram below (Fig. 28.13, left) shows
    the approach applied to the Dow Jones Industrial Index (DJX). For the ensemble,
    a HCNN was used to generate 250 individual forecasts for the DJX. For every forecast
    date, all of the individual forecasts for the ensemble represent the empirical
    density function, i.e. a probability distribution over many possible market prices
    at a single point in time (see Fig. 28.13, right). It is noticeable that the actual
    development of the DJX is always within the ensemble channel (see gray lines,
    Fig. 28.13, left). The expected value for the forecast distribution is also an
    adequate point forecast for the DJX (see Fig. 28.13, right).
  id: totrans-8443
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举例说明我们的风险概念。下面的图（图 28.13，左侧）展示了应用于道琼斯工业指数（DJX）的方法。对于集成，使用 HCNN 生成了 250 个针对
    DJX 的个体预测。对于每个预测日期，所有个体预测共同代表经验密度函数，即在单一时刻许多可能市场价格的概率分布（见图 28.13，右侧）。值得注意的是，DJX
    的实际发展始终在集成通道内（见灰色线条，图 28.13，左侧）。预测分布的期望值也是 DJX 的一个合适点预测（见图 28.13，右侧）。
- en: Fig. 28.13. HCNN ensemble forecast for the Dow Jones Index (12 weeks forecast
    horizon), left, and associated index point distribution for the ensemble in time
    step t + 12,
  id: totrans-8444
  prefs: []
  type: TYPE_NORMAL
  zh: 图 28.13. HCNN 对道琼斯指数的集成预测（12 周预测范围），左侧为相关指数点在时间步 t + 12 的分布，
- en: '![691_image_0.png](691_image_0.png)'
  id: totrans-8445
  prefs: []
  type: TYPE_IMG
  zh: '![691_image_0.png](691_image_0.png)'
- en: right
  id: totrans-8446
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧
- en: 28.3 Conclusion And Outlook
  id: totrans-8447
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 28.3 结论与展望
- en: Recurrent neural networks model dynamical systems in the form of non-linear
    state space models. Just like any other NN, the equation of the RNNs, ECNNs, HCNNs
    or CRCNNs can be expressed as an architecture which represents the individual
    layers in the form of nodes and the connections between the layers in the form
    of links. In the graphical architecture we can apply local learning algorithms
    like error back propagation and an appropriate (stochastic) learning rule to train
    the NN[13, 17, 3]. This relationship is called the correspondence principle between
    equations, architectures and the local algorithms associated with them (Trick
    1).
  id: totrans-8448
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络以非线性状态空间模型的形式对动态系统进行建模。与其他神经网络一样，RNN、ECNN、HCNN 或 CRCNN 的方程可以表示为一种架构，该架构以节点的形式表示各层，以链接的形式表示层之间的连接。在图形架构中，我们可以应用局部学习算法，如误差反向传播和适当的（随机）学习规则来训练神经网络[13,
    17, 3]。这种关系称为方程、架构及其相关的局部算法之间的对应原则（技巧 1）。
- en: Finite unfolding in time of RNN using shared weight matrices enables us to stick
    to the correspondence principle. Overshooting enforces the autonomous dynamics
    and enables long-term forecasting (Trick 2), whereas an adaptive noise term handles
    the uncertainty of the finite unfolding in time (Trick 3).
  id: totrans-8449
  prefs: []
  type: TYPE_NORMAL
  zh: 使用共享权重矩阵的 RNN 的有限时间展开使我们能够坚持对应原则。超调强化了自主动态，并实现了长期预测（技巧 2），而自适应噪声项则处理了有限时间展开的不确定性（技巧
    3）。
- en: ECNN utilizes the previous model error as an additional input. Hence, the learning
    can interpret the models misfit as an external shock which is used to guide the
    model dynamics afterwards. This allows us to prevent the autonomous part of the
    model to adapt misleading inter-temporal causalities. If we know that a dynamical
    system is influenced by external shocks, the error correction mechanism of the
    ECNN is an important prestructuring element of the networks architecture to compensate
    missing inputs (Trick 4).
  id: totrans-8450
  prefs: []
  type: TYPE_NORMAL
  zh: ECNN 利用先前模型误差作为额外输入。因此，学习可以将模型的不匹配解释为外部冲击，并用于指导模型后续的动态。这使我们能够防止模型的自主部分适应误导性的时间间因果关系。如果我们知道一个动态系统受外部冲击的影响，ECNN
    的误差校正机制则是网络架构的重要预结构元素，以补偿缺失输入（技巧 4）。
- en: Extending the ECNN by variants-invariants separation, one is able to include
    additional prior structural knowledge of the underlying dynamics into the model.
  id: totrans-8451
  prefs: []
  type: TYPE_NORMAL
  zh: 通过变元与不变元的分离扩展 ECNN，可以将基础动态的额外先验结构知识纳入模型中。
- en: The separation of variants and invariants with a bottleneck coordinate transformation
    allows to handle high dimensional problems (Trick 5).
  id: totrans-8452
  prefs: []
  type: TYPE_NORMAL
  zh: 通过瓶颈坐标变换将变元和不变元分离，能够处理高维问题（技巧 5）。
- en: HCNNs model not just an individual dynamics, but complex systems made up of
    a number of interacting sub-dynamics. HCNNs are symmetrical in their input and
    output variables, i.e. the system description does not draw any distinction between
    input, output and internal state variables. Thus, an open system becomes a closed
    system (Trick 6). Sparse transition matrices enable us to model different time
    scales and stabilize the training (Trick 8). Causal and retro-causal information
    flow within an integrated model (CRCNN) can be used to model rational planning
    and decision making in markets. CRCNNs dynamically combine causal and retro-causal
    information to describe the prevailing market regime
  id: totrans-8453
  prefs: []
  type: TYPE_NORMAL
  zh: HCNN不仅建模个体动态，还建模由多个相互作用的子动态组成的复杂系统。HCNN在输入和输出变量上是对称的，即系统描述并不区分输入、输出和内部状态变量。因此，开放系统变成了封闭系统（技巧6）。稀疏转移矩阵使我们能够建模不同的时间尺度并稳定训练（技巧8）。在集成模型（CRCNN）中，因果和逆因果信息流可用于建模市场中的理性规划和决策。CRCNN动态结合因果和逆因果信息，以描述当前的市场状态。
- en: (Trick 9). Architectural teacher forcing can be applied to efficiently train
    the HCNN or CRCNN (Trick 7 and 10). An architectural extension (see Fig. 28.12)
    enables us to balance the causal and retro-causal information flow during the
    learning of the CRCNN (Trick 11).
  id: totrans-8454
  prefs: []
  type: TYPE_NORMAL
  zh: （技巧9）。架构教师强迫可有效训练HCNN或CRCNN（技巧7和10）。架构扩展（见图28.12）使我们能够在学习CRCNN时平衡因果和逆因果信息流（技巧11）。
- en: We usually work with ensembles of HCNN or CRCNN to predict commodity prices.
    All solutions have a model error of zero in the past, but show a different behavior
    in the future. The reason for this lies in different ways of reconstructing the
    hidden variables from the observations and is independent of different random
    sparse initializations. Since every model gives a perfect description of the observed
    data, we can use the simple average of the individual forecasts as the expected
    value, assuming that the distribution of the ensemble is unimodal. The analysis
    of the ensemble spread opens up new perspectives on market risks. We claim that
    the model risk of a CRCNN or HCNN is equal to the forecast risk (Trick 12).
  id: totrans-8455
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常使用HCNN或CRCNN的集合来预测商品价格。所有解决方案在过去的模型误差为零，但在未来表现出不同的行为。这是由于从观测中重构隐藏变量的不同方式，与不同的随机稀疏初始化无关。由于每个模型对观察数据提供了完美的描述，我们可以使用单个预测的简单平均值作为期望值，假设集合的分布是单峰的。对集合分布的分析为市场风险提供了新的视角。我们主张CRCNN或HCNN的模型风险等于预测风险（技巧12）。
- en: Work currently in progress aims to improve the embedding of the CRCNN
  id: totrans-8456
  prefs: []
  type: TYPE_NORMAL
  zh: 当前正在进行的工作旨在改善CRCNN的嵌入
- en: architecture (see Fig. 28.10) in order to simplify and stabilize the training.
    On the other hand, we analyze the micro-structure of the ensembles and implement
    the models in practical risk management and financial market applications.
  id: totrans-8457
  prefs: []
  type: TYPE_NORMAL
  zh: 架构（见图28.10）旨在简化和稳定训练。另一方面，我们分析集合的微观结构，并在实际风险管理和金融市场应用中实现模型。
- en: All NN architectures and algorithms are implemented in the Simulation Environment
    for Neural Networks (SENN), a product of Siemens Corporate Technology. Work is
    partially funded by German Federal Research Ministry (BMBF
  id: totrans-8458
  prefs: []
  type: TYPE_NORMAL
  zh: 所有NN架构和算法均在神经网络模拟环境（SENN）中实现，这是西门子企业技术的一款产品。该工作部分由德国联邦研究部（BMBF）资助。
- en: grant Alice, 01 IB10003 A-C).
  id: totrans-8459
  prefs: []
  type: TYPE_NORMAL
  zh: 赠款爱丽丝，01 IB10003 A-C）。
- en: '[1] Calvert, D., Kremer, S.: Networks with Adaptive State Transitions. In:
    Kolen, J.F., Kremer, S. (eds.) A Field Guide to Dynamical Recurrent Networks,
    pp. 15– 25. IEEE (2001)'
  id: totrans-8460
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Calvert, D., Kremer, S.: 自适应状态转移网络。在：Kolen, J.F., Kremer, S.（编）《动态递归网络实用指南》，第15–25页。IEEE（2001）'
- en: '[2] Föllmer, H.: Alles richtig und trotzdem falsch?, Anmerkungen zur Finanzkrise
    und Finanzmathematik. In: MDMV, vol. 17, pp. 148–154 (2009)'
  id: totrans-8461
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Föllmer, H.: 一切正确却依然错误？关于金融危机和金融数学的注释。在：MDMV，第17卷，第148–154页（2009）'
- en: '[3] Haykin, S.: Neural Networks and Learning Machines, 3rd edn. Prentice Hall
    (2008)'
  id: totrans-8462
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Haykin, S.: 《神经网络与学习机器》，第3版。普伦蒂斯霍尔（2008）'
- en: '[4] Hull, J.: Options, Futures & Other Derivative Securities. Prentice Hall
    (2001)'
  id: totrans-8463
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Hull, J.: 《期权、期货与其他衍生证券》。普伦蒂斯霍尔（2001）'
- en: '[5] Hornik, K., Stinchcombe, M., White, H.: Multilayer Feedforward Networks
    are Universal Approximators. Neural Networks 2, 359–366 (1989)'
  id: totrans-8464
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Hornik, K., Stinchcombe, M., White, H.: 多层前馈网络是通用近似器。神经网络2，359–366（1989）'
- en: '[6] Kamien, M., Schwartz, N.: Dynamic Optimization: The Calculus of Variations
    and Optimal Control in Economics and Management, 2nd edn. Elsevier Science (October
    1991)'
  id: totrans-8465
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Kamien, M., Schwartz, N.: 动态优化：经济与管理中的变分法与最优控制，第2版。爱思唯尔科学（1991年10月）'
- en: '[7] McNeil, A., Frey, R., Embrechts, P.: Quantitative Risk Management: Concepts,
    Techniques and Tools. Princeton University Press, Princeton (2005)'
  id: totrans-8466
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] McNeil, A., Frey, R., Embrechts, P.: 定量风险管理：概念、技术与工具。普林斯顿大学出版社，普林斯顿（2005年）'
- en: '[8] Neuneier, R., Zimmermann, H.-G.: How to Train Neural Networks. In: Orr,
    G.B.,'
  id: totrans-8467
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Neuneier, R., Zimmermann, H.-G.: 如何训练神经网络。见：Orr, G.B.,'
- en: Müller, K.-R. (eds.) NIPS-WS 1996. LNCS, vol. 1524, pp. 373–423. Springer, Heidelberg
    (1998)
  id: totrans-8468
  prefs: []
  type: TYPE_NORMAL
  zh: Müller, K.-R.（编）NIPS-WS 1996。LNCS，卷1524，第373–423页。施普林格，海德堡（1998年）
- en: '[9] Pearlmutter, B.: Gradient Calculations for Dynamic Recurrent Neural Networks.'
  id: totrans-8469
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Pearlmutter, B.: 动态递归神经网络的梯度计算。'
- en: 'In: Kolen, J.F., Kremer, S. (eds.) A Field Guide to Dynamical Recurrent Networks,
    pp. 179–206. IEEE Press (2001)'
  id: totrans-8470
  prefs: []
  type: TYPE_NORMAL
  zh: 见：Kolen, J.F., Kremer, S.（编）。动态递归网络领域指南，第179–206页。IEEE出版社（2001年）
- en: '[10] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning Internal Representations
    by Error Propagation. In: Rumelhart, D.E., McClelland, J.L., et al. (eds.) Parallel
    Distributed Processing, vol. 1: Foundations. MIT Press, Cambridge (1986)'
  id: totrans-8471
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: 通过误差传播学习内部表示。见：Rumelhart,
    D.E., McClelland, J.L.等（编）。并行分布式处理，第1卷：基础。MIT出版社，剑桥（1986年）'
- en: '[11] Schäfer, A.M., Zimmermann, H.-G.: Recurrent Neural Networks Are Universal
    Approximators. In: Kollias, S.D., Stafylopatis, A., Duch, W., Oja, E. (eds.) ICANN
    2006. LNCS, vol. 4131, pp. 632–640. Springer, Heidelberg (2006)'
  id: totrans-8472
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Schäfer, A.M., Zimmermann, H.-G.: 递归神经网络是通用逼近器。见：Kollias, S.D., Stafylopatis,
    A., Duch, W., Oja, E.（编）。ICANN 2006。LNCS，卷4131，第632–640页。施普林格，海德堡（2006年）'
- en: '[12] Wei, W.S.: Time Series Analysis: Univariate and Multivariate Methods.
    AddisonWesley Publishing Company, N.Y. (1990)'
  id: totrans-8473
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Wei, W.S.: 时间序列分析：单变量与多变量方法。艾迪森-韦斯利出版公司，纽约（1990年）'
- en: '[13] Werbos, P.J.: Beyond Regression: New Tools for Prediction and Analysis
    in the Behavioral Sciences. PhD Thesis, Harvard University (1974)'
  id: totrans-8474
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Werbos, P.J.: 超越回归：行为科学中的预测与分析新工具。博士论文，哈佛大学（1974年）'
- en: '[14] Williams, R.J., Zipser, D.: A Learning Algorithm for continually running
    fully recurrent neural networks. Neural Computation 1(2), 270–280 (1989)'
  id: totrans-8475
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Williams, R.J., Zipser, D.: 一种用于持续运行完全递归神经网络的学习算法。神经计算 1(2)，270–280（1989年）'
- en: '[15] Zimmermann, H.G., Grothmann, R., Neuneier, R.: Modeling of Dynamical Systems
    by Error Correction Neural Networks. In: Soofi, A., Cao, L. (eds.) Modeling and
    Forecasting Financial Data, Techniques of Nonlinear Dynamics. Kluwer (2002)'
  id: totrans-8476
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Zimmermann, H.G., Grothmann, R., Neuneier, R.: 通过误差修正神经网络建模动态系统。见：Soofi,
    A., Cao, L.（编）。建模与预测金融数据，非线性动力学技术。克鲁威尔（2002年）'
- en: '[16] Zimmermann, H.G., Grothmann, R., Schäfer, A.M., Tietz, Ch.: Modeling Large
    Dynamical Systems with Dynamical Consistent Neural Networks. In: Haykin, S., et
    al. (eds.) New Directions in Statistical Signal Processing. MIT Press, Cambridge
    (2006)'
  id: totrans-8477
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Zimmermann, H.G., Grothmann, R., Schäfer, A.M., Tietz, Ch.: 用动态一致神经网络建模大型动态系统。见：Haykin,
    S.等（编）。统计信号处理的新方向。MIT出版社，剑桥（2006年）'
- en: '[17] Zimmermann, H.G.: Neuronale Netze als Entscheidungskalkül. In: Rehkugler,
    H.,'
  id: totrans-8478
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Zimmermann, H.G.: 神经网络作为决策计算。见：Rehkugler, H.,'
- en: Zimmermann, H.G. (eds.) Neuronale Netze in der Ökonomie, Grundlagen und wissenschaftliche
    Anwendungen. Vahlen, Munich (1994)
  id: totrans-8479
  prefs: []
  type: TYPE_NORMAL
  zh: Zimmermann, H.G.（编）经济学中的神经网络，基础与科学应用。瓦伦，慕尼黑（1994年）
- en: '[18] Zimmermann, H.G., Neuneier, R.: Neural Network Architectures for the Modeling
    of Dynamical Systems. In: Kolen, J.F., Kremer, S. (eds.) A Field Guide to Dynamical
    Recurrent Networks, pp. 311–350. IEEE Press (2001)'
  id: totrans-8480
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Zimmermann, H.G., Neuneier, R.: 动态系统建模的神经网络架构。见：Kolen, J.F., Kremer, S.（编）。动态递归网络领域指南，第311–350页。IEEE出版社（2001年）'
- en: '[19] Zimmermann, H.G., Grothmann, R., Tietz, C., von Jouanne-Diedrich, H.:
    Market Modeling, Forecasting and Risk Analysis with Historical Consistent Neural
    Networks. In: Hu, B., et al. (eds.) Operations Research Proceedings 2010, Selected
    Papers of the Annual Int. Conferences of the German OR Society (GOR), Munich.
    Springer, Heidelberg (September 2011)'
  id: totrans-8481
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Zimmermann, H.G., Grothmann, R., Tietz, C., von Jouanne-Diedrich, H.:
    市场建模、预测和风险分析与历史一致的神经网络。见：Hu, B.等（编）。2010年运筹学会议论文集，德国运筹学会（GOR）年会的精选论文，慕尼黑。施普林格，海德堡（2011年9月）'
- en: '[20] Zimmermann, H.G., Grothmann, R., Tietz, Ch.: Forecasting Market Prices
    with Causal-Retro-Causal Neural Networks. In: Klatte, D., Lüthi, H.-J., Schmedders,
    K. (eds.) Operations Research Proceedings 2011, Selected Papers of the Int. Conference
    on Operations Research 2011 (OR 2011), Zurich, Switzerland. Springer'
  id: totrans-8482
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Zimmermann, H.G., Grothmann, R., Tietz, Ch.：使用因果-逆因果神经网络预测市场价格。在：Klatte,
    D., Lüthi, H.-J., Schmedders, K.（编）《2011年运筹学会议论文集》，国际运筹学会议2011（OR 2011）精选论文，瑞士苏黎世。施普林格'
- en: (2012)
  id: totrans-8483
  prefs: []
  type: TYPE_NORMAL
  zh: （2012）
- en: 29 Solving Partially Observable Reinforcement Learning Problems With Recurrent
    Neural Networks
  id: totrans-8484
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 29 解决部分可观测强化学习问题的递归神经网络
- en: Siegmund Duell1,2, Steffen Udluft1, and Volkmar Sterzing1 1 Siemens AG, Corporate
    Technology, Intelligent Systems and Control 2 Berlin University of Technology,
    Machine Learning
  id: totrans-8485
  prefs: []
  type: TYPE_NORMAL
  zh: Siegmund Duell1,2, Steffen Udluft1, 和 Volkmar Sterzing1 1 西门子公司，企业技术部，智能系统与控制
    2 柏林工业大学，机器学习
- en: '{duell.siegmund.ext,steffen.udluft,volkmar.sterzing}@siemens.com Abstract.
    The aim of this chapter is to provide a series of tricks and recipes for neural
    state estimation, particularly for real world applications of reinforcement learning.
    We use various topologies of recurrent neural networks as they allow to identify
    the continuous valued, possibly high dimensional state space of complex dynamical
    systems. Recurrent neural networks explicitly offer possibilities to account for
    time and memory, in principle they are able to model any type of dynamical system.'
  id: totrans-8486
  prefs: []
  type: TYPE_NORMAL
  zh: '{duell.siegmund.ext,steffen.udluft,volkmar.sterzing}@siemens.com 摘要。本章的目的是提供一系列用于神经状态估计的技巧和方法，特别是针对强化学习的现实世界应用。我们使用多种递归神经网络拓扑，因为它们能够识别复杂动态系统的连续值，可能是高维的状态空间。递归神经网络明确提供了考虑时间和记忆的可能性，原则上能够建模任何类型的动态系统。'
- en: Because of these capabilities recurrent neural networks are a suitable tool
    to approximate a Markovian state space of dynamical systems. In a second step,
    reinforcement learning methods can be applied to solve a defined control problem.
    Besides the trick of using a recurrent neural network for state estimation, various
    issues regarding real world problems such as, large sets of observables and long-term
    dependencies are addressed.
  id: totrans-8487
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些能力，递归神经网络是近似动态系统马尔可夫状态空间的合适工具。在第二步中，可以应用强化学习方法来解决定义好的控制问题。除了使用递归神经网络进行状态估计的技巧外，还解决了许多与现实世界问题相关的各种问题，如大规模观测集和长期依赖。
- en: 29.1 Introduction
  id: totrans-8488
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.1 引言
- en: In this chapter we present a state estimation approach to tackle partially observable
    reinforcement learning [26] problems in discrete time. Reinforcement learning
    is the machine learning approach to the optimal control problem. Instead of designing
    the control strategy, reinforcement learning learns it from actual observations
    of the system to be controlled. Combined with powerful function approximators
    like neural networks, impressive results could be achieved
  id: totrans-8489
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们提出了一种状态估计方法，以解决离散时间中的部分可观测强化学习[26]问题。强化学习是针对最优控制问题的机器学习方法。它不是设计控制策略，而是从被控制系统的实际观察中学习。结合强大的函数逼近器如神经网络，可以取得显著的成果。
- en: '[28, 13, 19]. In most real world applications, some form of state estimation
    is necessary to fulfill the requirements of reinforcement learning.'
  id: totrans-8490
  prefs: []
  type: TYPE_NORMAL
  zh: '[28, 13, 19]。在大多数现实世界应用中，某种形式的状态估计是满足强化学习要求的必要条件。'
- en: Consider the task to reduce the emissions of a gas turbine while keeping humming,
    caused by combustion dynamics, low. A gas turbine consists of a compressor, providing
    compressed air. Within the combustion chamber, this air is burned with gas to
    drive the turbine and the coupled generator. The gas is injected through multiple
    burners. At each burner, the fuel flow is split into different fractions enabling
    control over the combustion process. The combustion process results in emissions
    of NOx and CO, which have to be kept below
  id: totrans-8491
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个任务，即在保持低噪音（由燃烧动力学引起）的同时，减少燃气涡轮的排放。燃气涡轮由压缩机组成，提供压缩空气。在燃烧室内，这种空气与气体一起燃烧，以驱动涡轮及其耦合的发电机。气体通过多个燃烧器注入。在每个燃烧器，燃料流被分成不同的部分，以便控制燃烧过程。燃烧过程产生的NOx和CO排放必须控制在一定水平以下。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    709–733, 2012.'
  id: totrans-8492
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon 等（编）：NN：行业技巧，第2版，LNCS 7700，第709–733页，2012年。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-8493
  prefs: []
  type: TYPE_NORMAL
  zh: -c 施普林格-瓦格 2012年
- en: legal limits. At the same time, the combustion process causes humming. This
    humming reduces the life time of the machine and can cause fatal damage to the
    turbine. Reinforcement learning can address the problem of optimizing the combustion
    process by tuning fuel fractions in an elegant way. Reinforcement learning can
    also account for seasonal effects and wear because these effects can be found
    within sensor readings. Approaches that are based on (fixed) physical models are
    usually unable to find such effects. To successfully develop a reinforcement learning
    agent, the problem of state estimation (see chapter 30 [23]
  id: totrans-8494
  prefs: []
  type: TYPE_NORMAL
  zh: 法律限制。同时，燃烧过程会产生嗡嗡声。这种嗡嗡声缩短了机器的使用寿命，并可能对涡轮机造成致命损害。强化学习可以通过优雅地调节燃料比例来解决优化燃烧过程的问题。强化学习还可以考虑季节性影响和磨损，因为这些影响可以在传感器读数中找到。基于（固定）物理模型的方法通常无法发现此类影响。为了成功开发一个强化学习代理，状态估计的问题必须得到解决（见第30章
    [23]）。
- en: for other problem solutions to real world reinforcement learning applications)
  id: totrans-8495
  prefs: []
  type: TYPE_NORMAL
  zh: 其他问题解决方案以应用于现实世界的强化学习。
- en: has to be solved, in order to fulfill the requirements of the reinforcement
    learning framework. Since a gas turbine provides a vast number of sensor readings,
    such as temperature and pressure readings, mass flows and actor settings such
    as valve positions and set points of low level controllers, the state is very
    high dimensional. Even experts cannot anticipate all effects that could be caused
    by the various subsystems and their interactions. A state estimation approach
    can overcome this problem.
  id: totrans-8496
  prefs: []
  type: TYPE_NORMAL
  zh: 必须解决，以满足强化学习框架的要求。由于燃气涡轮机提供大量传感器读数，例如温度和压力读数、质量流量以及阀门位置和低级控制器设定点等执行器设置，因此状态是非常高维的。即使是专家也无法预见各种子系统及其相互作用可能造成的所有影响。状态估计方法可以克服这个问题。
- en: Various topologies of recurrent neural networks (RNNs) are used for state estimation
    as they allow to identify the continuous valued, possibly high dimensional state
    space of real world applications. RNNs explicitly offer possibilities to account
    for time and memory, in principle they are able to model any type of dynamical
    system [9, 15, 11, 32]. Because of these capabilities RNNs are a valuable tool
    for state estimation, especially for real world applications. Based on these estimates,
    methods of reinforcement learning can be applied to solve a defined control problem.
  id: totrans-8497
  prefs: []
  type: TYPE_NORMAL
  zh: 各种递归神经网络（RNN）的拓扑被用于状态估计，因为它们能够识别现实世界应用的连续值、高维状态空间。RNN明确提供了考虑时间和记忆的可能性，原则上它们能够建模任何类型的动态系统
    [9, 15, 11, 32]。由于这些能力，RNN成为状态估计的宝贵工具，特别是在现实世界应用中。基于这些估计，可以应用强化学习的方法来解决定义的控制问题。
- en: The chapter is divided into four parts. After a brief introduction to reinforcement
    learning and its requirements, the main trick of modeling a Markovian state space
    using an RNN is described in section 29.3. Section 29.4 proceeds with tricks to
    extract a Markov decision process from a possibly large set of observables and
    adapts a neural topology further towards the task of state estimation for reinforcement
    learning. To address the task to capture different time scales of dynamical dependencies
    a solution to the long-term dependency problem is provided in section 29.5. For
    all presented tricks, recipes as practical guides are introduced, to avoid potential
    pitfalls and to improve general applicability. Further, some experiments to demonstrate
    the applicability of the presented procedures are presented.
  id: totrans-8498
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分为四个部分。在简要介绍强化学习及其要求后，第29.3节描述了使用RNN建模马尔可夫状态空间的主要技巧。第29.4节继续介绍从可能的大量可观察变量中提取马尔可夫决策过程的技巧，并进一步调整神经拓扑以适应强化学习的状态估计任务。为了解决捕捉动态依赖的不同时间尺度的任务，第29.5节提供了长期依赖问题的解决方案。对于所有呈现的技巧，提供了作为实用指南的配方，以避免潜在的陷阱并提高一般适用性。此外，还介绍了一些实验以证明所呈现程序的适用性。
- en: 29.2 Background
  id: totrans-8499
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.2 背景
- en: 'Reinforcement learning usually requires the system of interest to be described
    as a Markov decision process (MDP) M := (S, A,P, R), where S and A denote the
    state and action spaces, respectively, P : *S×A×S* → [0, 1] the state transition
    probabilities, i.e., the probability of entering a successor state st+1 by executing
    an action at in state st, and R : *S×A×S* → R the reward function assigning a
    transition its immediate cost or gain. The aim of reinforcement learning is to
    derive a policy π : S → A mapping each state to an action that maximizes the return,
    i.e., the sum of all (discounted) future rewards. A central characteristic of
    an MDP is the Markov property which states that the probability of reaching a
    certain successor state st+1 depends on the current state st and action at only.'
  id: totrans-8500
  prefs: []
  type: TYPE_NORMAL
  zh: '强化学习通常要求将感兴趣的系统描述为马尔可夫决策过程（MDP）M := (S, A, P, R)，其中S和A分别表示状态和动作空间，P : *S×A×S*
    → [0, 1]是状态转移概率，即在状态st中执行动作at进入后继状态st+1的概率，R : *S×A×S* → R是奖励函数，为转移分配其即时成本或收益。强化学习的目标是推导出一个策略π
    : S → A，将每个状态映射到最大化回报的动作，即所有（折扣）未来奖励的总和。MDP的一个中心特征是马尔可夫属性，它表明到达某个后继状态st+1的概率仅依赖于当前状态st和动作at。'
- en: 'However, in many real-world control problems the current state is not directly
    accessible. Instead, only a number of observables zt ∈ Z can be used as source
    of information about the true current state st, rendering the MDP into a partially
    observable Markov decision process (POMDP) M := (S, Z, A,P, R, O). In addition
    to the components of an MDP, a POMDP contains an observation space Z and an (usually
    unknown) observation function O : S×A → Z, describing the mapping from state-action
    pairs to observations.'
  id: totrans-8501
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，在许多现实世界的控制问题中，当前状态并不可直接访问。相反，只能使用一组可观察量zt ∈ Z作为关于真实当前状态st的信息源，从而将MDP转变为部分可观察的马尔可夫决策过程（POMDP）M
    := (S, Z, A, P, R, O)。除了MDP的组件外，POMDP还包含一个观察空间Z和一个（通常未知的）观察函数O : S×A → Z，描述从状态-动作对到观察的映射。'
- en: In the gas turbine setting, presented in the introduction one also has to deal
    with a POMDP. Various measurements as well as actor settings like valve positions
    are provided as observables z. The state of the turbine can be influenced by changing
    the fuel flow valve positions. As a result, an immediate reward signal is calculated,
    providing information about the quality of performed actions.
  id: totrans-8502
  prefs: []
  type: TYPE_NORMAL
  zh: 在引言中提到的燃气涡轮环境中，也必须处理POMDP。各种测量以及阀门位置等演员设置作为可观察量z提供。通过改变燃料流量阀的位置，可以影响涡轮的状态。因此，计算出一个即时奖励信号，提供关于所执行动作质量的信息。
- en: A common approach for dealing with POMDPs is to model a distribution of possible
    current states [12], i.e., a belief state. When using such approaches, the selection
    of action at is based on the most probable current state and additionally its
    uncertainty [20]. When dealing with technical systems, the partial observability
    mostly stems from the limited available measurements and the fact that one time
    step is insufficient to describe the current system state, i.e., most technical
    systems can also be described as MDPs of higher order. By simply aggregating a
    sufficient number of prior time slices, such Markov processes can be reduced to
    MDPs of first order, i.e., the type of MDPs required by reinforcement learning.
    In many cases a simple aggregation leads to a high dimensional state space and
    therefore turns out to be impractical in most cases (Bellman's "curse of dimensionality"
  id: totrans-8503
  prefs: []
  type: TYPE_NORMAL
  zh: 处理POMDP的常见方法是对可能的当前状态建模分布[12]，即信念状态。当使用这种方法时，选择动作at是基于最可能的当前状态及其不确定性[20]。在处理技术系统时，部分可观察性主要源于有限的可用测量和一个时间步不足以描述当前系统状态，即大多数技术系统也可以描述为更高阶的MDP。通过简单地聚合足够数量的先前时间片，这些马尔可夫过程可以简化为一阶MDP，即强化学习所需的MDP类型。在许多情况下，简单的聚合导致高维状态空间，因此在大多数情况下被认为不实用（贝尔曼的“维数诅咒”）。
- en: '[2]). Either a deep understanding of the underlying system or, especially in
    more autonomous settings, a state estimator can overcome this problem. In the
    following, recurrent neural approaches to model a Markovian state space from partially
    observable dynamics are derived. The modeled state allows to apply any well understood
    powerful reinforcement learning algorithms in the offline setting, such as the
    neural fitted Q iteration [24], the recurrent control neural network [31], or
    online approaches such as actor critic algorithms [21].'
  id: totrans-8504
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]。对基础系统的深入理解，或特别是在更自主的环境中，状态估计器可以克服这个问题。接下来，将推导出用于从部分可观察动态建模马尔可夫状态空间的递归神经方法。建模的状态允许在离线设置中应用任何被充分理解的强大强化学习算法，例如神经拟合Q迭代[24]、递归控制神经网络[31]或在线方法如演员-评论家算法[21]。'
- en: 29.3 The Trick Of Modeling A Markovian State Space Using A Recurrent Neural
    Network
  id: totrans-8505
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.3 使用递归神经网络建模马尔可夫状态空间的技巧
- en: A reinforcement learning agent interacts with a system thereby commonly altering
    its evolution. This development can be described as an open dynamical system.
    Note that in comparison to the description found in chapter 28 [36], the adjustable
    external drives of the system, a are explicitly disjoint from other external drivers
    z. In practice a dynamical system like a gas turbine can be influenced by adjustable
    external drives like valve positions of fuel or cooling flows but at the same
    time, other external driver such as ambient conditions might also be relevant.
    For discrete time grids (t = 1*,...,T* and T ∈ N) this can be
  id: totrans-8506
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习代理与系统进行交互，从而常常改变其演变。这一发展可以描述为一个开放的动态系统。请注意，与第28章[36]中的描述相比，系统的可调外部驱动a与其他外部驱动z显式分离。在实践中，像燃气涡轮这样的动态系统可以受到可调外部驱动的影响，例如燃料或冷却流的阀门位置，但同时，其他外部驱动，例如环境条件，可能也很重要。对于离散时间网格（t
    = 1*,...,T* 且 T ∈ N），这可以
- en: '![698_image_0.png](698_image_0.png)'
  id: totrans-8507
  prefs: []
  type: TYPE_IMG
  zh: '![698_image_0.png](698_image_0.png)'
- en: $$(29.1)$$
  id: totrans-8508
  prefs: []
  type: TYPE_NORMAL
  zh: $$(29.1)$$
- en: Fig. 29.1. A dynamics can be affected by actions a, performed by a (reinforcement
    learning) controller but also by external drivers z, e.g., ambient conditions
    of a gas turbine. Both alter the state s of the dynamics and cause a state transition,
    resulting in a set of observables y. For a gas turbine, observables are sensor
    readings such as temperatures or pressure levels as well as actor settings such
    as valve positions.
  id: totrans-8509
  prefs: []
  type: TYPE_NORMAL
  zh: 图29.1。动态系统可以受到动作a的影响，这些动作由（强化学习）控制器执行，同时也受到外部驱动z的影响，例如燃气涡轮的环境条件。两者都会改变动态的状态s，并导致状态转移，结果生成一组可观察量y。对于燃气涡轮，可观察量是温度或压力水平等传感器读数，以及阀门位置等执行者设置。
- en: 'represented as a set of equations consisting of a state transition and an output
    equation [9, 11]:'
  id: totrans-8510
  prefs: []
  type: TYPE_NORMAL
  zh: 表示为一个由状态转移和输出方程组成的方程组[9, 11]：
- en: $$\begin{array}{r l}{s_{t+1}=f(s_{t},a_{t},z_{t})}&{{}{\mathrm{state~transition}}}\\
    {y_{t}}&{{}=g(s_{t})}&{{}{\mathrm{output~equation}}.}\end{array}$$
  id: totrans-8511
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{r l}{s_{t+1}=f(s_{t},a_{t},z_{t})}&{{}{\mathrm{状态~转移}}}\\ {y_{t}}&{{}=g(s_{t})}&{{}{\mathrm{输出~方程}}.}\end{array}$$
- en: Figure 29.1 illustrates equation 29.1. The state transition is a mapping from
    the present internal state of the system st and the influence of external inputs
    at and zt, to the new state st+1. In the context of reinforcement learning, the
    inputs zt are the agent's (partial) information about the system and at represents
    an action that has been selected by a control strategy. The output equation determines
    the observable output yt. In a basic framework, the output is equivalent to the
    resulting change of the system, in other words the subsequent observables of the
    system zt+1.
  id: totrans-8512
  prefs: []
  type: TYPE_NORMAL
  zh: 图29.1说明了方程29.1。状态转移是将系统当前内部状态st和外部输入at及zt的影响映射到新状态st+1。在强化学习的背景下，输入zt是代理对系统的（部分）信息，而at代表由控制策略选择的动作。输出方程决定可观察输出yt。在基本框架中，输出等同于系统的结果变化，换句话说，是系统的后续可观察量zt+1。
- en: 'The task of identifying a dynamical system of Eq. 29.1 can be stated as the
    problem to find (parametrized) functions f and g such that a distance measurement
    (Eq. 29.2) between the observed data ydt and the model output yt is minimal:'
  id: totrans-8513
  prefs: []
  type: TYPE_NORMAL
  zh: 识别方程29.1的动态系统的任务可以表述为寻找（参数化的）函数f和g，使得观察数据ydt与模型输出yt之间的距离测量（方程29.2）最小：
- en: $$\sum_{t=1}^{T}\left(y_{t}-y_{t}^{d}\right)^{2}\to\operatorname*{min}_{f,g}$$
  id: totrans-8514
  prefs: []
  type: TYPE_NORMAL
  zh: $$\sum_{t=1}^{T}\left(y_{t}-y_{t}^{d}\right)^{2}\to\operatorname*{min}_{f,g}$$
- en: $$(29.2)$$
  id: totrans-8515
  prefs: []
  type: TYPE_NORMAL
  zh: $$(29.2)$$
- en: $$(29.3)$$
  id: totrans-8516
  prefs: []
  type: TYPE_NORMAL
  zh: $$(29.3)$$
- en: The identification task of Eq. 29.1 and 29.2 can be formulated by an RNN of
    the form
  id: totrans-8517
  prefs: []
  type: TYPE_NORMAL
  zh: 方程29.1和29.2的识别任务可以通过如下形式的RNN进行表述：
- en: $\begin{array}{ll}s_{t+1}=\text{tanh}(As_{t}+c+Bz_{t}+Ca_{t})&\text{state transition}\\
    y_{t}&=Ds_{t}\\ \end{array}$  Consider D, we might want to use for a consistent
    dimension.
  id: totrans-8518
  prefs: []
  type: TYPE_NORMAL
  zh: $\begin{array}{ll}s_{t+1}=\text{tanh}(As_{t}+c+Bz_{t}+Ca_{t})&\text{状态转移}\\
    y_{t}&=Ds_{t}\\ \end{array}$  考虑 D，我们可能希望使用一致的维度。
- en: where A, B, C, and D are weight matrices of appropriate dimensions and c is
    a bias.
  id: totrans-8519
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 A, B, C 和 D 是适当维度的权重矩阵，c 是一个偏置。
- en: 'By approximating the functions f and g with an RNN using the weight matrices
    *A, B*, C, D, and a bias vector c, of fixed dimensions, the system identification
    task of Eq. 29.2 is transformed into a parameter optimization problem:'
  id: totrans-8520
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用固定维度的权重矩阵 *A, B*, C, D 和偏置向量 c 来逼近函数 f 和 g，方程 29.2 的系统识别任务转化为参数优化问题：
- en: $$\sum_{t=1}^{T}\left(y_{t}-y_{t}^{d}\right)^{2}\rightarrow\operatorname*{min}_{A,B,C,D,c}$$
  id: totrans-8521
  prefs: []
  type: TYPE_NORMAL
  zh: $$\sum_{t=1}^{T}\left(y_{t}-y_{t}^{d}\right)^{2}\rightarrow\operatorname*{min}_{A,B,C,D,c}$$
- en: $$(29.4)$$
  id: totrans-8522
  prefs: []
  type: TYPE_NORMAL
  zh: $$(29.4)$$
- en: A,B,C,D,c (29.4)
  id: totrans-8523
  prefs: []
  type: TYPE_NORMAL
  zh: A,B,C,D,c (29.4)
- en: '![699_image_0.png](699_image_0.png)'
  id: totrans-8524
  prefs: []
  type: TYPE_IMG
  zh: '![699_image_0.png](699_image_0.png)'
- en: Fig. 29.2. A standard RNN used as state estimator during the training phase.
    Each circle represents a hidden layer of neurons (called cluster). Each arrow
    depicts a weight matrix, connecting all neurons between two layers. Note that
    all hidden clusters as well as all output clusters are connected to a bias (not
    shown in the figure).
  id: totrans-8525
  prefs: []
  type: TYPE_NORMAL
  zh: 图 29.2. 在训练阶段用作状态估计器的标准 RNN。每个圆圈表示一个隐藏层的神经元（称为集群）。每个箭头描绘一个权重矩阵，连接两个层之间的所有神经元。注意所有隐藏集群和所有输出集群都连接到一个偏置（图中未显示）。
- en: This parameter optimization problem is solved by an RNN with finite unfolding
    in time using shared weight matrices *A, B*, C, and D [22, 9]. Fig. 29.2 depicts
    the resulting spatial neural network architecture. The RNN is trained with backpropagation
    through time which is a shared weights extension of standard backpropagation [22,
    9]. The training of an RNN using shared weights is straightforward and delivers
    sound results. RNNs are—other than feed forward networks—able to establish memory
    due to the temporal structure with unfolding towards the past and the future.
    This allows to model inter-temporal dependencies and latent variables. E.g., the
    emissions of a gas turbine are delayed multiple steps because of sensor response
    characteristics and the distance between causing the emissions (within the combustion
    chamber) and the point of measurement (within the exhaust gas flow).
  id: totrans-8526
  prefs: []
  type: TYPE_NORMAL
  zh: 这个参数优化问题是通过一个 RNN 解决的，该 RNN 在时间上有限展开，使用共享权重矩阵 *A, B*, C 和 D [22, 9]。图 29.2 描绘了生成的空间神经网络架构。RNN
    通过时间反向传播进行训练，这是一种标准反向传播的共享权重扩展 [22, 9]。使用共享权重训练 RNN 是直接的，并且可以获得良好的结果。与前馈网络不同，RNN
    能够由于时间结构而建立记忆，向过去和未来展开。这使得能够建模跨时间依赖关系和潜变量。例如，燃气涡轮的排放延迟多个步骤，因为传感器响应特性和导致排放的点（在燃烧室内）与测量点（在废气流中）之间的距离。
- en: In order to map the state estimation to the RNN the hidden units s of the RNN
    (Fig. 29.2) describe the state variables for the reinforcement learning task.
  id: totrans-8527
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将状态估计映射到 RNN，隐藏单元 s 的 RNN（图 29.2）描述了强化学习任务的状态变量。
- en: In other words, after training, these variables are used as outputs of the state
    estimation function. The externally available observables and the performed action
    are used as input vector zt and action vector at, ydt = zt+1 defines the targets.
  id: totrans-8528
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，经过训练后，这些变量被用作状态估计函数的输出。外部可用的观测值和执行的动作被用作输入向量 zt 和动作向量 at，ydt = zt+1 定义了目标。
- en: The RNN is extended into the future by so-called overshooting [35], i.e., the
    network is unfolded beyond the present time into the future (Fig. 29.2). This
    results in a whole sequence of forecasts as outputs. Overshooting regularizes
    the learning and thus improves the model's performance [35]. Overshooting does
    not require additional network parameters as shared weight matrices A, B, C, and
    D are used.
  id: totrans-8529
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 通过所谓的超越向未来展开 [35]，即网络超出当前时间向未来展开（图 29.2）。这导致输出一整序列的预测。超越正则化学习，从而提高模型性能 [35]。超越不需要额外的网络参数，因为使用的是共享权重矩阵
    A, B, C 和 D。
- en: The resulting trained network models the state transition of the underlying
    dynamical system with respect to the sequence of actions. If the system is able
    to model the forecast horizon sufficiently well, the Markov property for the hidden
    cluster s0 is arbitrarily well approximated [30, 27]. Therefore a sufficiently
    well trained RNN results in a state estimator for a subsequent use in reinforcement
    learning algorithms [29].
  id: totrans-8530
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的训练网络根据行动序列建模底层动态系统的状态转换。如果系统能够充分好地建模预测视野，隐藏簇s0的马尔可夫性质可以被任意好地逼近[30，27]。因此，经过充分训练的RNN会生成一个状态估计器，以便在后续的强化学习算法中使用[29]。
- en: In this section the basic idea of using an RNN for state estimation was introduced.
    Subsequent to the trick of using RNNs as state estimators, a series of recipes
    to overcome various problems when designing neural state estimators will be presented.
    First, the important role of actions on the state estimation task and the requirement
    to generalize over all actions within the action space
  id: totrans-8531
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了使用RNN进行状态估计的基本思路。在使用RNN作为状态估计器的技巧之后，将介绍一系列方案，以克服设计神经状态估计器时遇到的各种问题。首先，行动在状态估计任务中的重要作用，以及对所有行动进行泛化的要求。
- en: (Sec. 29.3.1, 29.3.2) is discussed. Next, data and pattern preprocessing methods
    to provide a suitable training and test set (Sec. 29.3.3, 29.3.4, 29.3.5) are
    introduced. Thereafter, the learning process is introduced (Sec. 29.3.6, 29.3.7).
    Finally, the reduced recall-topology of a trained state estimator (Sec. 29.3.8)
  id: totrans-8532
  prefs: []
  type: TYPE_NORMAL
  zh: （第29.3.1节，29.3.2节）进行讨论。接下来，介绍数据和模式预处理方法，以提供合适的训练和测试集（第29.3.3节，29.3.4节，29.3.5节）。随后，介绍学习过程（第29.3.6节，29.3.7节）。最后，呈现训练状态估计器的简化回忆拓扑（第29.3.8节）。
- en: is presented. The applicability of the approaches is illustrated on the cart-pole
    problem in Sec. 29.3.9.
  id: totrans-8533
  prefs: []
  type: TYPE_NORMAL
  zh: 进行呈现。方法的适用性在第29.3.9节的摆杆问题中进行了说明。
- en: 29.3.1 Improving The Generalization Capabilities With Respect To Actions
  id: totrans-8534
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.3.1 改善针对行动的泛化能力
- en: In contrast to the standard definition of an open dynamical system and the description
    of overshooting [35], the known sequence of actions is used as past and future
    inputs ap, ap−1, ..., a0, a1, ..., af , where p describes the number of past and
    f the number of future steps (Fig. 29.2). This is crucial, since the data might
    have been generated by an arbitrary, possibly unknown control strategy.
  id: totrans-8535
  prefs: []
  type: TYPE_NORMAL
  zh: 与开放动态系统的标准定义和过冲的描述[35]相比，已知的行动序列作为过去和未来的输入ap，ap−1，...，a0，a1，...，af，其中p表示过去的步数，f表示未来的步数（图29.2）。这至关重要，因为数据可能是由任意的、可能未知的控制策略生成的。
- en: The resulting network should be capable to model the dynamics of the system
    based on the influence of these external drivers at, rather than predict a sequence
    of actions from the Markovian state. E.g., the data of a gas turbine might be
    generated by an arbitrary control strategy. If this effect is neglected, the state
    estimator is forced to encode the underlying policy within the network. The resulting
    network is most likely useless since it does not generalize to different action
    sequences as they occur when following a reinforcement learning policy. In general,
    it is also recommended to test a trained state estimator for its generalization
    capabilities towards different actions sequences. This is possible for most systems.
    E.g., for the gas turbine certain actions are known to reduce emissions, applying
    them to the turbine should lead to a change of NOx or CO.
  id: totrans-8536
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的网络应能够基于这些外部驱动因素的影响来建模系统的动态，而不是从马尔可夫状态预测一系列行动。例如，燃气涡轮的数据可能是由任意控制策略生成的。如果忽略这种影响，状态估计器将被迫在网络中编码潜在策略。生成的网络很可能无用，因为它无法对不同的行动序列进行泛化，这些序列在遵循强化学习策略时会出现。一般来说，还建议测试训练好的状态估计器对不同行动序列的泛化能力。这对于大多数系统都是可能的。例如，对于燃气涡轮，某些行动被证明可以减少排放，将其应用于涡轮应导致NOx或CO的变化。
- en: 29.3.2 A Recipe To Improve The Modeling Of State Transitions
  id: totrans-8537
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.3.2 改进状态转换建模的方案
- en: The standard RNN shown in Fig. 29.2 uses a single hidden cluster of neurons
    to encode a state s, the state transition is modeled by the matrix A. In case
    of state estimation for reinforcement learning we are interested in modeling a
    state transition, i.e., stat −→ st+1. This can be explicitly expressed in the
    neural topology, improving the overall performance of the estimator. Especially
    generalization capabilities with respect to different actions are improved. Note
    that a good state estimator not only minimizes the training error of the optimization
    task described in Eq. 29.4, but also generalizes well to different sequences of
    actions (Sec. 29.3.1). The neural representation of a state transition is shown
    in Fig. 29.3.
  id: totrans-8538
  prefs: []
  type: TYPE_NORMAL
  zh: 图29.2中显示的标准RNN使用单个隐藏神经元集群来编码状态s，状态转移由矩阵A建模。在强化学习中的状态估计情况下，我们关注状态转移的建模，即，stat
    −→ st+1。这可以在神经拓扑中明确表达，从而提高估计器的整体性能。特别是关于不同动作的泛化能力得到了改善。请注意，一个好的状态估计器不仅最小化优化任务的训练误差（如方程29.4所述），而且对不同的动作序列具有良好的泛化能力（见29.3.1节）。状态转移的神经表示见图29.3。
- en: 29.3.3 Scaling Of Inputs And Targets
  id: totrans-8539
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.3.3 输入和目标的缩放
- en: Like in other supervised learning approaches scaling of input and target values
    is an essential preprocessing step. Usually input scaling to receive a mean of
    zero and a standard deviation of one is preferred. Other scaling approaches can
    be used as well, e.g., to encode prior knowledge about a dynamics. E.g., for concentrations
    like the emissions of a gas turbine, a logarithm can be used to avoid negative,
    implausible output values.
  id: totrans-8540
  prefs: []
  type: TYPE_NORMAL
  zh: 像其他监督学习方法一样，输入和目标值的缩放是一个重要的预处理步骤。通常，更倾向于将输入缩放为均值为零，标准差为一。也可以使用其他缩放方法，例如编码关于动态的先验知识。例如，对于浓度，如燃气涡轮的排放，可以使用对数以避免负值和不合理的输出值。
- en: 29.3.4 Block Validation
  id: totrans-8541
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.3.4 区块验证
- en: As the patterns for an RNN comprise a whole sequence of input and target values,
    subsequent patterns are highly correlated. Each pattern differs from the preceding
    one only by a single time slice. Consequently, assigning patterns to the training
    (used to update weights) or validation set (not used to update weights but to
    evaluate the performance of the current set of weights) at random would lead to
    a strong correlation between training and validation set and hence the training
    and validation error. In order to de-correlate the pattern sets an increased granularity
    of the partitioning is achieved by using blocks of patterns that do not contain
    overlapping information. A block is either used for training or testing. Ideally,
    sets with identical statistical properties are generated.
  id: totrans-8542
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RNN的模式包含一整套输入和目标值，因此后续模式高度相关。每个模式仅与前一个模式在一个时间片上有所不同。因此，随机分配模式到训练集（用于更新权重）或验证集（不用于更新权重，但用于评估当前权重集的性能）会导致训练集和验证集之间的强相关性，从而影响训练和验证误差。为了去相关化模式集，通过使用不包含重叠信息的模式块，实现了更高粒度的划分。一个块要么用于训练，要么用于测试。理想情况下，生成具有相同统计特性的集合。
- en: The usage of blocks is motivated by the occurrence of new *situations*. E.g.,
    a gas turbine might operate at full load and change to a reduced load level. After
    a transient period, the turbine reaches the new load level. Such processes take
    minutes, whereas the controller is designed to perform an action every couple
    of seconds. By grouping the patterns to blocks, the sets are de-correlated and
    still capture all operational *situations*. Choosing an appropriate block size
    for the given problem is essential for good results and a reliable test set. If
    one would simply split a data set of, e.g., two days of operation into two sets,
    a complete day each, both sets could have significantly different statistical
    properties and a set might easily miss some operational *situations*.
  id: totrans-8543
  prefs: []
  type: TYPE_NORMAL
  zh: 使用区块的原因是由于新*情况*的出现。例如，一个燃气涡轮可能在满负荷下运行并转变为降低负荷水平。在瞬态期间后，涡轮达到新的负荷水平。这类过程需要几分钟，而控制器设计为每隔几秒执行一次动作。通过将模式分组为区块，数据集去相关化并仍然捕获所有操作*情况*。为给定问题选择合适的区块大小对良好结果和可靠测试集至关重要。如果简单地将例如两天的操作数据集分成两个集合，每个集合一整天，这两个集合可能具有显著不同的统计特性，且某个集合可能会轻易错过某些操作*情况*。
- en: The random block validation algorithm (Alg. 29.1) addresses these problems.
  id: totrans-8544
  prefs: []
  type: TYPE_NORMAL
  zh: 随机区块验证算法（Alg. 29.1）解决了这些问题。
- en: The algorithm uses a data file containing M observations, i.e., M time slices
    of data vectors. First the size of a block j is determined randomly within predefined
    limits minblocksize and maxblocksize. Next j patterns are generated from subsequential
    time slices of observations. Each pattern is unfolded over its entire time range.
    In other words, if the topology defines p past time slices and f future
  id: totrans-8545
  prefs: []
  type: TYPE_NORMAL
  zh: 算法使用一个包含 M 个观测值的数据文件，即 M 个时间片段的数据向量。首先，在预定义的限制 minblocksize 和 maxblocksize 内随机确定块
    j 的大小。接下来，从后续的观测时间片段生成 j 个模式。每个模式在其整个时间范围内展开。换句话说，如果拓扑定义了 p 个过去的时间片段和 f 个未来的时间片段，则
- en: '![701_image_0.png](701_image_0.png)'
  id: totrans-8546
  prefs: []
  type: TYPE_IMG
  zh: '![701_image_0.png](701_image_0.png)'
- en: 'Fig. 29.3. An RNN modeling explicit state transitions depending on an applied
    action:'
  id: totrans-8547
  prefs: []
  type: TYPE_NORMAL
  zh: 图 29.3. 一个 RNN 建模依赖于施加动作的显式状态转移：
- en: stat −→ st+1. Note that all hidden clusters as well as all output clusters are
    connected to a bias (not shown in the figure).
  id: totrans-8548
  prefs: []
  type: TYPE_NORMAL
  zh: stat −→ st+1。注意，所有隐藏簇以及所有输出簇都连接到一个偏置（图中未显示）。
- en: '![702_image_0.png](702_image_0.png)'
  id: totrans-8549
  prefs: []
  type: TYPE_IMG
  zh: '![702_image_0.png](702_image_0.png)'
- en: Fig. 29.4. Pattern generated with the block validation procedure. All blocks
    are decorrelated to provide reliable, uncorrelated test sets (i.e., for validation
    and generalization tests).
  id: totrans-8550
  prefs: []
  type: TYPE_NORMAL
  zh: 图 29.4. 采用块验证程序生成的模式。所有块都是去相关的，以提供可靠、无相关性的测试集（即用于验证和泛化测试）。
- en: time slices, the entire pattern covers p + f observations. Further each pattern
    is scaled (see Sec. 29.3.3). The process is repeated until the block is filled
    or one runs out of observations. Finally the type (training or test set) of a
    block is determined according to predefined probabilities. A complete block is
    added to the final pattern set. The resulting generated patterns are organized
    as shown in Fig. 29.4. The resulting data set fulfills our requirement of a set
    of patterns, where each pattern set (training set and test set) are de-correlated
    but have similar properties, since the different operational *situations* are
    distributed over all sets.
  id: totrans-8551
  prefs: []
  type: TYPE_NORMAL
  zh: 时间片段，整个模式覆盖 p + f 观测数据。进一步地，每个模式会被缩放（参见第 29.3.3 节）。这个过程会重复，直到块被填满或者没有观测数据为止。最后，根据预定义的概率确定一个块的类型（训练集或测试集）。完整的块会被添加到最终的模式集中。生成的模式按图
    29.4 所示的方式组织。生成的数据集满足我们对一组模式的要求，其中每个模式集（训练集和测试集）是去相关的，但具有相似的属性，因为不同的操作*情境*在所有集合中分布。
- en: 29.3.5 Removal Of Invalid Data Patterns
  id: totrans-8552
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.3.5 无效数据模式的移除
- en: In all applications with training data unfolded in time the problem of invalid
    patterns can occur. An invalid data pattern contains data that does not fit into
    a predefined time grid. E.g., every time the gas turbine is restarted, the stream
    of data is restarted as well and causes a gap. Any pattern that is generated and
    contains data from the previous shutdown, probably a couple of hours ago, and
    the new start-up would be invalid. It does not describe the behavior of the turbine
    and should therefore be removed from the used patterns.
  id: totrans-8553
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有以时间展开的训练数据应用中，都可能出现无效模式的问题。无效数据模式包含不符合预定义时间网格的数据。例如，每次燃气涡轮重新启动时，数据流也会重新启动，导致出现间隙。任何生成的模式如果包含来自先前关闭（可能是几小时前）和新的启动的数据，则被视为无效。它并不描述涡轮的行为，因此应从使用的模式中移除。
- en: A valid pattern contains, for each step unfolded in time, inputs that match
    the defined time grid. E.g., if a time grid of t = τ is selected, a valid pattern
    unfolds over n equidistant steps t = τ, 2*τ, . . . , nτ*, where n defines the
    number of unfolded steps of the pattern. Any pattern that has gaps in the time
    line should be excluded to avoid invalid training patterns. This problem occurs
    in many real world applications but also for episodic benchmark problems such
    as a cart-pole or the acrobot [26].
  id: totrans-8554
  prefs: []
  type: TYPE_NORMAL
  zh: 有效模式包含每个时间展开步骤的输入，这些输入与定义的时间网格相匹配。例如，如果选择时间网格 t = τ，有效模式将在 n 个等距步骤 t = τ, 2*τ,
    . . . , nτ* 上展开，其中 n 定义了模式展开步骤的数量。任何时间线中有间隙的模式应被排除，以避免无效的训练模式。这个问题在许多实际应用中都会出现，也适用于诸如倒立摆或倒立机器人等情景基准问题
    [26]。
- en: 'In practice this task can be solved by simply extending the block validation
    algorithm (Alg. 29.1). Before the subroutine: generate(*tm, ..., tm*+n), each
    data vector is tested to match the defined time grid. All data vectors violating
    the grid are ignored.'
  id: totrans-8555
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这项任务可以通过简单地扩展块验证算法（算法 29.1）来解决。在子程序：generate(*tm, ..., tm*+n) 之前，测试每个数据向量以匹配定义的时间网格。所有违反网格的数据显示向量将被忽略。
- en: $\epsilon\;\;i$) ...
  id: totrans-8556
  prefs: []
  type: TYPE_NORMAL
  zh: $\epsilon\;\;i$) ...
- en: 'Algorithm 29.1 . Random block validation Input: set of observations O = {tm
    = (zm, am, rm)|m = 1*,...,M*}'
  id: totrans-8557
  prefs: []
  type: TYPE_NORMAL
  zh: 算法29.1。随机块验证 输入：观察集O = {tm = (zm, am, rm)|m = 1*,...,M*}
- en: 'Result: block validated set of patterns for training and testing m := 1 while
    m ≤ M do j := rand(minblocksize, maxblocksize) " determine the block size within
    given limits i := 0 while (i<j) ∧ (m + n) ≤ M do " generate a block of patterns,
    containing j patterns, where each pattern covers n time slices, break if insufficient
    data available pattern := generate(tm*,...,t*m+n) " generate a valid pattern using
    the selected scaling block.add(pattern) i := i + 1 m := m + 1 end while type :=
    rand(ptraining, ptest) " choose the type of the new block according to the specified
    probabilities p for training and test set patternSet.add(block,type) " Add valid
    block to final pattern set m := m + n " Skip data to avoid overlapping pattern,
    n depicts the network''s unfolding in time end while return patternSet'
  id: totrans-8558
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：用于训练和测试的块验证模式集 m := 1 当 m ≤ M 时执行 j := rand(minblocksize, maxblocksize) "
    确定在给定限制内的块大小 i := 0 当 (i<j) ∧ (m + n) ≤ M 时执行 " 生成一个包含j个模式的块，每个模式覆盖n个时间切片，如果数据不足则中断
    pattern := generate(tm*,...,t*m+n) " 使用所选缩放生成有效模式 block.add(pattern) i := i +
    1 m := m + 1 结束循环 type := rand(ptraining, ptest) " 根据指定概率p选择新块的类型用于训练和测试集 patternSet.add(block,type)
    " 将有效块添加到最终模式集中 m := m + n " 跳过数据以避免重叠模式，n表示网络在时间上的展开 结束循环 返回 patternSet
- en: 29.3.6 Learning Settings
  id: totrans-8559
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.3.6 学习设置
- en: We made good experience using standard backpropagation on the described neural
    network topologies. This effectively realizes backpropagation through time for
    a fixed horizon, i.e., over all past and future time slices covered by the neural
    network. Robust learning is achieved by small randomly chosen batches of patterns
    with a batch size B of, e.g. B = 1/1000 N, where N is the number of all patterns.
    The weights updates Δw are calculated as
  id: totrans-8560
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在所描述的神经网络拓扑上使用标准反向传播获得了良好的经验。这有效地实现了在固定范围内的时间反向传播，即覆盖神经网络的所有过去和未来时间切片。通过小批量随机选择的模式进行稳健学习，批量大小B例如为B
    = 1/1000 N，其中N为所有模式的数量。权重更新Δw的计算方式为
- en: $$\Delta w=-\eta\,\frac{1}{B}\sum_{i=1}^{B}\partial E_{i}/\partial w\,,$$
  id: totrans-8561
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w=-\eta\,\frac{1}{B}\sum_{i=1}^{B}\partial E_{i}/\partial w\,,$$
- en: $$(29.5)$$
  id: totrans-8562
  prefs: []
  type: TYPE_NORMAL
  zh: $$(29.5)$$
- en: ∂Ei*/∂w ,* (29.5)
  id: totrans-8563
  prefs: []
  type: TYPE_NORMAL
  zh: ∂Ei*/∂w ,* (29.5)
- en: where η is the learning rate and Ei is the error for pattern i. The learning
    rate is chosen between 0.01 and 0.001 in most cases.
  id: totrans-8564
  prefs: []
  type: TYPE_NORMAL
  zh: 其中η为学习率，Ei为模式i的误差。在大多数情况下，学习率选择在0.01和0.001之间。
- en: In many applications the learning process can be sped up significantly by an
    extension to standard backpropagation called VarioEta [18]. VarioEta scales the
    weight updates individually by the standard deviation over all patterns
  id: totrans-8565
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多应用中，通过对标准反向传播的扩展称为VarioEta，可以显著加快学习过程[18]。VarioEta根据所有模式的标准差单独缩放权重更新。
- en: $$\sigma_{w}={\sqrt{{\frac{1}{N}}\sum_{i=1}^{N}(\partial E_{i}/\partial w-D_{w})^{2}}}\,,$$
  id: totrans-8566
  prefs: []
  type: TYPE_NORMAL
  zh: $$\sigma_{w}={\sqrt{{\frac{1}{N}}\sum_{i=1}^{N}(\partial E_{i}/\partial w-D_{w})^{2}}}\,,$$
- en: (∂Ei/∂w − Dw)2 , (29.6)
  id: totrans-8567
  prefs: []
  type: TYPE_NORMAL
  zh: (∂Ei/∂w − Dw)² , (29.6)
- en: where
  id: totrans-8568
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: $$D_{w}={\frac{1}{N}}\sum_{i=1}^{N}\partial E_{i}/\partial w$$
  id: totrans-8569
  prefs: []
  type: TYPE_NORMAL
  zh: $$D_{w}={\frac{1}{N}}\sum_{i=1}^{N}\partial E_{i}/\partial w$$
- en: $$(29.6)$$
  id: totrans-8570
  prefs: []
  type: TYPE_NORMAL
  zh: $$(29.6)$$
- en: $$(29.7)$$
  id: totrans-8571
  prefs: []
  type: TYPE_NORMAL
  zh: $$(29.7)$$
- en: ∂Ei/∂w (29.7)
  id: totrans-8572
  prefs: []
  type: TYPE_NORMAL
  zh: ∂Ei/∂w (29.7)
- en: leading to the VarioEta update rule
  id: totrans-8573
  prefs: []
  type: TYPE_NORMAL
  zh: 导致VarioEta更新规则
- en: $$\Delta w^{\prime}=-\eta\,\frac{1}{B}\sum_{i=1}^{B}\partial E_{i}/\partial
    w\,\frac{1}{\sigma_{w}}\,.$$
  id: totrans-8574
  prefs: []
  type: TYPE_NORMAL
  zh: $$\Delta w^{\prime}=-\eta\,\frac{1}{B}\sum_{i=1}^{B}\partial E_{i}/\partial
    w\,\frac{1}{\sigma_{w}}\,.$$
- en: $$(29.8)$$
  id: totrans-8575
  prefs: []
  type: TYPE_NORMAL
  zh: $$(29.8)$$
- en: . (29.8)
  id: totrans-8576
  prefs: []
  type: TYPE_NORMAL
  zh: . (29.8)
- en: 29.3.7 Double Rest Learning
  id: totrans-8577
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.3.7 双重休息学习
- en: The training process of RNNs with shared weights has proven empirically to be
    reliable and robust. Nonetheless our goal is to get the best training result without
    wasting time on unnecessary network training. For this purpose we developed the
    double rest learning procedure (Alg. 29.2).
  id: totrans-8578
  prefs: []
  type: TYPE_NORMAL
  zh: 实证证明，具有共享权重的RNN的训练过程是可靠和稳健的。然而，我们的目标是获得最佳的训练结果，而不浪费时间在不必要的网络训练上。为此，我们开发了双重休息学习程序（算法29.2）。
- en: In principle the same network is trained multiple times on a constant data set
    but different random initializations of the initial neural weights. If no better
    network was found after multiple trials of training, the process terminates. A
    set of pattern that contains a training set and at least one test set used for
    validation is required. The algorithm is initialized with a maximum number of
    trials, tmax and
  id: totrans-8579
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，同一网络在一个恒定的数据集上多次训练，但使用不同的随机初始化的初始神经权重。如果在多次训练尝试后没有找到更好的网络，则过程终止。需要包含训练集和至少一个用于验证的测试集的模式集。算法以最大尝试次数tmax初始化，
- en: Algorithm 29.2 . Double rest learning
  id: totrans-8580
  prefs: []
  type: TYPE_NORMAL
  zh: 算法29.2。双重休息学习
- en: 'Input: maximal epochs per training emax, maximal training trials tmax, rest'
  id: totrans-8581
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：每次训练的最大时期 emax，最大训练试验 tmax，休息
- en: epochs erest, rest trials trest
  id: totrans-8582
  prefs: []
  type: TYPE_NORMAL
  zh: epochs erest, rest trials trest
- en: 'Result: trained neural network'
  id: totrans-8583
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：训练好的神经网络
- en: t := 0
  id: totrans-8584
  prefs: []
  type: TYPE_NORMAL
  zh: t := 0
- en: ttotal := 0
  id: totrans-8585
  prefs: []
  type: TYPE_NORMAL
  zh: ttotal := 0
- en: v := ∞
  id: totrans-8586
  prefs: []
  type: TYPE_NORMAL
  zh: v := ∞
- en: while t<trest ∧ ttotal < tmax do
  id: totrans-8587
  prefs: []
  type: TYPE_NORMAL
  zh: 当 t<trest 且 ttotal < tmax 时
- en: net := restLearning(erest, emax)
  id: totrans-8588
  prefs: []
  type: TYPE_NORMAL
  zh: net := restLearning(erest, emax)
- en: vt := net.min() if vt < v then
  id: totrans-8589
  prefs: []
  type: TYPE_NORMAL
  zh: vt := net.min() 如果 vt < v 则
- en: $$\begin{array}{l}{{v:=v_{t}}}\\ {{\mathrm{bestNN}:=\mathrm{net}}}\\ {{v:=v_{t}}}\end{array}$$
  id: totrans-8590
  prefs: []
  type: TYPE_NORMAL
  zh: $$\begin{array}{l}{{v:=v_{t}}}\\ {{\mathrm{bestNN}:=\mathrm{net}}}\\ {{v:=v_{t}}}\end{array}$$
- en: t := 0
  id: totrans-8591
  prefs: []
  type: TYPE_NORMAL
  zh: t := 0
- en: end if
  id: totrans-8592
  prefs: []
  type: TYPE_NORMAL
  zh: 如果的话
- en: t := t + 1;
  id: totrans-8593
  prefs: []
  type: TYPE_NORMAL
  zh: t := t + 1;
- en: ttotal := ttotal + 1
  id: totrans-8594
  prefs: []
  type: TYPE_NORMAL
  zh: ttotal := ttotal + 1
- en: end while return bestNN
  id: totrans-8595
  prefs: []
  type: TYPE_NORMAL
  zh: 当返回 bestNN
- en: the number of trials one wants to try to find a better solution, trest. For
    the rest learning algorithm (Alg. 29.3) the maximum number of epochs allowed for
    training a single network, emax and the number of epochs a single network is trained
    beyond the epoch that achieved the best validation error erest is required.
  id: totrans-8596
  prefs: []
  type: TYPE_NORMAL
  zh: 试图尝试找到更好解决方案的试验次数，trest。对于休息学习算法（Alg. 29.3），允许训练单个网络的最大时期 emax，以及在达到最佳验证错误的时期后训练单个网络的时期
    erest 是必需的。
- en: Double rest learning starts training single networks using the rest learning
    algorithm (Alg. 29.3). Whenever a new network is found to have a better validation
    error than a previous one, the number of rested trials t is reset to 0. The algorithm
    terminates as soon as no better network was found for trest trials or the maximum
    number of trials tmax is exceeded.
  id: totrans-8597
  prefs: []
  type: TYPE_NORMAL
  zh: 双 rest 学习开始使用 rest 学习算法（Alg. 29.3）训练单个网络。每当新的网络发现比前一个网络有更好的验证错误时，休息试验的数量 t 被重置为
    0。一旦 trest 次没有找到更好的网络，或者超过了最大试验次数 tmax，则算法终止。
- en: The rest learning process for a single network follows a similar approach
  id: totrans-8598
  prefs: []
  type: TYPE_NORMAL
  zh: 单个网络的休息学习过程采用类似方法
- en: (Alg. 29.3). The algorithm allows us to select the best network according to
    a validation set. A single network is trained for e epochs. If a better solution
    is found within erest, the epoch i with the best validation error is determined.
  id: totrans-8599
  prefs: []
  type: TYPE_NORMAL
  zh: （Alg. 29.3）。该算法允许我们根据验证集选择最佳网络。单个网络训练 e 个时期。如果在 erest 内找到更好的解决方案，则确定具有最佳验证错误的时期
    i。
- en: The best set of weights, bestNN, from epoch i is stored, and the network is
    trained for another erest − i epochs. If the validation error did not improve
    for erest epochs or the maximum number of training epochs emax is exceeded, the
    training process terminates.
  id: totrans-8600
  prefs: []
  type: TYPE_NORMAL
  zh: 从第 i 个时期存储的最佳权重集 bestNN，网络训练另外 erest − i 个时期。如果验证错误在 erest 个时期内没有改善，或者超过了最大训练时期
    emax，则训练过程终止。
- en: 'Algorithm 29.3 . Rest learning Input: maximal epochs per training emax, number
    of rest epochs erest Result: trained neural network t := 0 vmin := ∞ ttotal :=
    0 e := erest while t<erest ∧ ttotal < emax do net.learn(e) " train the network
    for e epochs ttotal := ttotal + e v := net.min() " retrieve the minimal validation
    error i := net.index(v) " retrieve the epoch resulting in the minimal validation
    error if v < vmin then vmin := v bestNN := net.weights(i) " retrieve the network
    with the weights, resulting in the minimal validation error t := i e := erest
    elset := t + i e := erest − i end if end while return bestNN'
  id: totrans-8601
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 29.3 . 休息学习 输入：每次训练的最大时期 emax，休息时期 erest 结果：训练好的神经网络 t := 0 vmin := ∞ ttotal
    := 0 e := erest 当 t<erest 且 ttotal < emax 时 net.learn(e) " 训练网络 e 时期 ttotal :=
    ttotal + e v := net.min() " 检索最小验证错误 i := net.index(v) " 检索导致最小验证错误的时期如果 v < vmin
    那么 vmin := v bestNN := net.weights(i) " 检索具有权重的网络，导致最小验证错误 t := i e := erest elset
    := t + i e := erest − i end if end while 返回 bestNN
- en: Double rest learning can be applied to any neural network training where one
    wants to select the best network from multiple trials using a robust stopping
    criterion. It is especially useful when the meta-parameters of the training process
    are subject to optimization and are modified at each trial3. Another advantage
    3 In case of identical meta-parameters for all trials, it is adequate (and even
    more efficient) to use a fixed number of trials, i.e., tmax = trest.
  id: totrans-8602
  prefs: []
  type: TYPE_NORMAL
  zh: 双休息学习可以应用于任何神经网络训练，其中希望使用强大的停止准则从多次试验中选择最佳网络。当训练过程的元参数需要优化并且在每个试验中进行修改时，特别有用。另一个优势
    3 在所有试验中具有相同的元参数情况下，使用固定数量的试验即 tmax = trest 是适当的（甚至更有效）。
- en: 'is the parametrization of the algorithm. Even though there are four parameters,
    they can be assumed to be constant. For most applications we use: erest = 50,
    emax = 5000, trest = 10, and tmax = ∞. This parametrization has shown robust results
    for a wide range of applications and 500 to 50000 training patterns.'
  id: totrans-8603
  prefs: []
  type: TYPE_NORMAL
  zh: 是算法的参数化。尽管有四个参数，但可以认为它们是常数。对于大多数应用，我们使用：erest = 50, emax = 5000, trest = 10,
    和tmax = ∞。这种参数化在广泛的应用和500到50000个训练模式中显示出稳健的结果。
- en: 29.3.8 A Recipe To Generate An Efficient State Estimation Function
  id: totrans-8604
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.3.8 生成高效状态估计函数的方案
- en: 'After training an RNN the neural network is truncated to receive a function
    of the form:'
  id: totrans-8605
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练完RNN后，神经网络被截断，以接收形式为：
- en: $$s_{t}=f(z_{t},z_{t-1},...,z_{t-n},a_{t},a_{t-1},...,a_{t-n}),$$
  id: totrans-8606
  prefs: []
  type: TYPE_NORMAL
  zh: $$s_{t}=f(z_{t},z_{t-1},...,z_{t-n},a_{t},a_{t-1},...,a_{t-n}),$$
- en: where n denotes the length of the considered history of the network. The optimal
    length n can be found by subsequentially reducing the length of the past network
    until the error of the forecast, i.e., for outputs y0, y1, ... , ym, increases.
  id: totrans-8607
  prefs: []
  type: TYPE_NORMAL
  zh: 其中n表示网络考虑的历史长度。可以通过逐渐减少过去网络的长度，直到预测的误差增加，即对于输出y0, y1, ... , ym，来找到最优长度n。
- en: An instantaneously increasing error either indicates a perfect choice of n or
    an insufficient history. Another training with an increased past horizon ensures
    that all prior required information is provided to the neural network.
  id: totrans-8608
  prefs: []
  type: TYPE_NORMAL
  zh: 瞬时增加的误差要么表示n的完美选择，要么表示历史不够。另一次以增加的过去视野进行的训练确保神经网络获得所有先前所需的信息。
- en: The topology of the resulting network is shown in Fig. 29.5. This function can
    be used to transform observations into tuples of {*s, a, s*} and apply a reinforcement
    learning method of choice to solve the control problem.
  id: totrans-8609
  prefs: []
  type: TYPE_NORMAL
  zh: 结果网络的拓扑结构如图29.5所示。此函数可用于将观察转换为{*s, a, s*}的元组，并应用所选择的强化学习方法来解决控制问题。
- en: $$(29.9)$$
  id: totrans-8610
  prefs: []
  type: TYPE_NORMAL
  zh: $$(29.9)$$
- en: '![706_image_0.png](706_image_0.png)'
  id: totrans-8611
  prefs: []
  type: TYPE_IMG
  zh: '![706_image_0.png](706_image_0.png)'
- en: (z−3, a−3)(z−2, a−2)(z−1, a−1)(z0, a0)
  id: totrans-8612
  prefs: []
  type: TYPE_NORMAL
  zh: (z−3, a−3)(z−2, a−2)(z−1, a−1)(z0, a0)
- en: Fig. 29.5. A finalized state estimator after training. The former hidden unit
    s0 is now used as output of the function and provides a Markovian state representation
    for a given sequence of observations and actions. Note that all hidden clusters
    are connected to a bias (not shown in the figure).
  id: totrans-8613
  prefs: []
  type: TYPE_NORMAL
  zh: 图29.5. 训练后的最终状态估计器。前隐藏单元s0现在用作函数的输出，并为给定的观察和动作序列提供马尔可夫状态表示。请注意，所有隐藏聚类都连接到一个偏置（图中未显示）。
- en: 29.3.9 Application Of A Neural State Estimator
  id: totrans-8614
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.3.9 神经状态估计器的应用
- en: To demonstrate the capabilities of an RNN as state estimator, we chose the cartpole
    problem, which has been extensively studied in control and reinforcement learning
    theory. Since more than 30 years it serves as a benchmark for new ideas, because
    it is easy to understand and also quite representative for related questions.
    The classical problem has been completely solved in the past. E.g., Sutton [26]
    demonstrated that the pole can be balanced for an arbitrary number of time steps
    within a remarkable short training sequence. There are two major directions to
    make the cart-pole problem more challenging. One is to make the task itself more
    difficult by taking for example two poles [8] or regarding a two dimensional cart
    [7]. The other one is to make the original problem partially observable [17, 1,
    8].
  id: totrans-8615
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示RNN作为状态估计器的能力，我们选择了倒立摆问题，该问题在控制和强化学习理论中得到了广泛研究。三十多年来，它作为新思想的基准，因为它易于理解，并且在相关问题上具有代表性。经典问题在过去已完全解决。例如，Sutton
    [26] 证明了杆可以在相当短的训练序列内，在任意时间步内保持平衡。有两个主要方向可以使倒立摆问题更具挑战性。一是通过例如使用两根杆[8]或考虑二维小车[7]来使任务本身更加困难。另一个是使原始问题部分可观察[17,
    1, 8]。
- en: We will focus on the latter, since all other variants provide a Markovian state
    representation in the first place, and therefore do not require state estimation.
    A reinforcement learning solution to the original (simulated) benchmark problem,
    as well as to a real cart-pole system can be found in chapter 30 [23].
  id: totrans-8616
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点关注后者，因为所有其他变体首先提供了马尔可夫状态表示，因此不需要状态估计。有关原始（模拟）基准问题的强化学习解决方案，以及真实的倒立摆系统，可以在第30章找到[23]。
- en: Problem Description. The classical cart-pole problem consists of a cart moving
    on a bounded track and trying to balance a pole on its top. This cart-pole system
    is illustrated in Fig. 29.6 [17].
  id: totrans-8617
  prefs: []
  type: TYPE_NORMAL
  zh: 问题描述。经典的倒立摆问题由在有限轨道上移动的小车和试图在其顶部平衡一根杆组成。该倒立摆系统在图29.6中进行了说明[17]。
- en: '![707_image_0.png](707_image_0.png)'
  id: totrans-8618
  prefs: []
  type: TYPE_IMG
  zh: '![707_image_0.png](707_image_0.png)'
- en: Fig. 29.6. The cart-pole problem system
  id: totrans-8619
  prefs: []
  type: TYPE_NORMAL
  zh: 图29.6 小车-杆问题系统
- en: 'The system is fully defined through four variables (t = 1*,...,T* ):'
  id: totrans-8620
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统通过四个变量完全定义（t = 1*,...,T*）：
- en: xt := horizontal cart position x˙ t := horizontal velocity of the cart θt :=
    angle between pole and vertical
  id: totrans-8621
  prefs: []
  type: TYPE_NORMAL
  zh: xt := 水平小车位置 x˙ t := 小车的水平速度 θt := 杆与垂直线之间的角度
- en: ˙θt := angular velocity of the pole
  id: totrans-8622
  prefs: []
  type: TYPE_NORMAL
  zh: ˙θt := 杆的角速度
- en: $$\mathrm{{\cal~cent}}$$ $$\mathrm{{\cal~vertical}}$$
  id: totrans-8623
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathrm{{\cal~cent}}$$ $$\mathrm{{\cal~vertical}}$$
- en: $$(29.10)$$
  id: totrans-8624
  prefs: []
  type: TYPE_NORMAL
  zh: $$(29.10)$$
- en: The goal is to balance the pole for a preferably long sequence of time steps
    without moving out of the limits. Possible actions are to push the cart left or
    right with a constant force F. The pole tilts when its angle θt is higher than
    12 degrees. Either then or when the cart hits one of the boundaries, the system
    is punished with a negative reinforcement signal. In all other cases the reward
    is zero.
  id: totrans-8625
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是尽可能长时间地保持杆的平衡而不超出限制。可能的行动是用恒定的力F将小车向左或向右推动。当杆的角度θt超过12度时，杆会倾斜。此时或当小车撞到边界时，系统会受到负强化信号的惩罚。在所有其他情况下，奖励为零。
- en: As already mentioned the system has been extensively studied in its several
    forms. When the system was studied as partially observable, one usually omitted
    the two velocities, x˙ t and ˙θt, i.e., only the cart's position and the angle
    between the pole and the vertical where given as inputs [17, 1, 8]. Solving this
    problem is not difficult because the model or algorithm just needs the memory
    of one past time step to calculate the missing information.
  id: totrans-8626
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，该系统已经在多个形式下进行了广泛研究。当该系统作为部分可观察时，通常会省略两个速度，x˙ t和˙θt，即仅将小车的位置和杆与垂直线之间的角度作为输入
    [17, 1, 8]。解决这个问题并不困难，因为模型或算法只需要一个过去时间步的记忆来计算缺失的信息。
- en: To demonstrate the advantages of RNNs (unfolded in time) only the horizontal
    position of the cart, xt is observable [29]. All other information is unknown
    to the system.
  id: totrans-8627
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示RNN（时间展开）的优势，仅可观察到小车的水平位置xt [29]。系统无法获取所有其他信息。
- en: Model Description. To solve the problem described above, an RNN (Sec. 29.3)
  id: totrans-8628
  prefs: []
  type: TYPE_NORMAL
  zh: 模型描述。为了解决上述问题，采用了RNN（第29.3节）。
- en: was used to develop the full dynamics of the cart-pole system. Input ze and
    target y consist of the horizontal cart position xt as well as the preprocessing
    transformations xt - xt-1. The input at contains the agent's action. No other
    information is observable by the model. The internal state space st is limited
    to four neurons, allowing the network to reconstruct the complete but only partially
    observable dynamics (Eq. 29.10) in its internal state space. The network is unfolded
    ten time steps into the past and future. Preceding experiments have shown that
    this memory length is sufficient to identify the dynamics. To make the network
    independent from the last unfolded time slice a technique called cleaning noise
    is used as a start initialization [10]. The network is trained by backpropagation
    through time [22, 9].
  id: totrans-8629
  prefs: []
  type: TYPE_NORMAL
  zh: 被用于开发小车-杆系统的完整动力学。输入ze和目标y由水平小车位置xt及其预处理变换xt - xt-1组成。输入at包含代理的行动。模型无法观察到其他信息。内部状态空间st限制为四个神经元，使网络能够在其内部状态空间中重建完整但仅部分可观察的动力学（方程29.10）。网络向过去和未来展开了十个时间步。先前的实验表明，这个记忆长度足以识别动力学。为了使网络独立于最后展开的时间片，采用了一种称为清除噪声的技术作为初始初始化
    [10]。网络通过时间反向传播进行训练 [22, 9]。
- en: In a second step the evolved state space is extracted from the RNN, i.e., a
    state estimation function was exported to calculate estimated states from observations
    (see Sec. 29.3.8). Then a generalized form of Samuel's adaptive heuristic critic
    (AHC) algorithm [25] was used to solve the control problem based on the state
    estimates. Note, that the algorithm has to be started with an already filled lag
    structure, i.e, the past lags of the state estimator need to be filled with observations
    in order to provide a first state estimate. Otherwise there is a high probability
    that the algorithm is faced with a tilted pole in its first learning step, as
    a minimum of ten uncontrolled time steps would be necessary to fill all the lags.
  id: totrans-8630
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步中，从 RNN 提取演变的状态空间，即导出一个状态估计函数以根据观察计算估计状态（见第 29.3.8 节）。然后，使用 Samuel 的自适应启发式评估器
    (AHC) 算法 [25] 的广义形式来解决基于状态估计的控制问题。注意，该算法必须以已填充的滞后结构启动，即状态估计器的过去滞后需要用观察填充，以提供初始状态估计。否则，算法在其首次学习步骤中很可能会面临倾斜的杆，因为需要至少十个不受控的时间步来填充所有滞后。
- en: '![708_image_0.png](708_image_0.png)'
  id: totrans-8631
  prefs: []
  type: TYPE_IMG
  zh: '![708_image_0.png](708_image_0.png)'
- en: Fig. 29.7. Correlation between the best quadratic combination of the reconstructed
    state space variables (st)1, ... , (st)4 of the RNN and the original ones (Eq.
    29.10)
  id: totrans-8632
  prefs: []
  type: TYPE_NORMAL
  zh: 图 29.7. 重构状态空间变量 (st)1, ... , (st)4 与原始变量之间的最佳二次组合的相关性 (方程 29.10)
- en: Results. As a first result the estimation quality of the different state variables
    of the cart-pole is illustrated in Fig. 29.7. The four plots show the correlation
    between the original state space variables of the dynamics, xt, x˙ t, θt, ˙θt,
  id: totrans-8633
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。作为第一个结果，图 29.7 展示了推车-杆不同状态变量的估计质量。四个图展示了动力学原始状态空间变量 xt, x˙ t, θt, ˙θt 之间的相关性。
- en: (Eq. 29.10) and the best linear combination of the reconstructed state space
    variables (st)1*,...,*(st)4 and their squares (st)21*,...,*(st)24 in each case.
    The high correlation for each state space variable demonstrates the reconstruction
    quality of the RNN. It also supports the use of RNNs for partially observable
    reinforcement learning problems.
  id: totrans-8634
  prefs: []
  type: TYPE_NORMAL
  zh: （方程 29.10）以及重构状态空间变量 (st)1*,...,*(st)4 及其平方 (st)21*,...,*(st)24 的最佳线性组合。在每种情况下，各状态空间变量的高相关性展示了
    RNN 的重构质量。这也支持了 RNN 在部分可观察强化学习问题中的应用。
- en: We compared the results of our approach to a direct application of the AHC
  id: totrans-8635
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的方法的结果与 AHC 的直接应用进行了比较
- en: algorithm to the problem, i.e., without using an RNN in the first step. Note,
    that no adaptive binning has been used. In both cases the discretization of the
    state space was chosen to yield the best results.
  id: totrans-8636
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在问题上的直接应用，即在第一步不使用 RNN。注意，未使用自适应分箱。在两种情况下，状态空间的离散化都选择以获得最佳结果。
- en: Fig. 29.8 plots the achieved number of steps, the pole could be balanced, to
    the number of trials. The training process was stopped as soon as the first method
    was able to balance the pole for a minimum of 1000 steps. Fig. 29.8 shows how
    the RNN approach outperforms a direct application of the AHC algorithm.
  id: totrans-8637
  prefs: []
  type: TYPE_NORMAL
  zh: 图 29.8 显示了杆能够平衡的步骤数与试验次数的关系。训练过程在第一种方法能够在至少 1000 步内平衡杆时停止。图 29.8 显示了 RNN 方法优于
    AHC 算法的直接应用。
- en: '![709_image_0.png](709_image_0.png)'
  id: totrans-8638
  prefs: []
  type: TYPE_IMG
  zh: '![709_image_0.png](709_image_0.png)'
- en: Fig. 29.8. Comparison of the performance in the partially observable cart-pole
    problem of our RNN approach (upper curve) to a direct application of the AHC algorithm
    (lower curve)
  id: totrans-8639
  prefs: []
  type: TYPE_NORMAL
  zh: 图 29.8. 我们的 RNN 方法（上曲线）与 AHC 算法直接应用（下曲线）在部分可观察的推车-杆问题中的性能比较
- en: 29.4 The Markov Decision Process Extraction Network
  id: totrans-8640
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.4 马尔可夫决策过程提取网络
- en: When applying reinforcement learning algorithms to real world problems such
    as a gas turbine, one not only faces the problem of observations that do not provide
    the Markov property but in many applications it is not even obvious which variables
    to consider for the state definition. E.g., the measurements provided by a gas
    turbine include information about various subsystems. In practice it is often
    hard to determine if a certain variable has to be considered or a whole set of
    variables from a certain subsystem can be dismissed.
  id: totrans-8641
  prefs: []
  type: TYPE_NORMAL
  zh: 在将强化学习算法应用于实际问题（如燃气轮机）时，不仅面临观察不提供马尔可夫性质的问题，而且在许多应用中，甚至不明显应考虑哪些变量用于状态定义。例如，燃气轮机提供的测量数据包含关于各种子系统的信息。在实践中，通常很难确定是否需要考虑某个特定变量，或者某个子系统的一整组变量可以被忽略。
- en: '![710_image_0.png](710_image_0.png)'
  id: totrans-8642
  prefs: []
  type: TYPE_IMG
  zh: '![710_image_0.png](710_image_0.png)'
- en: 'Fig. 29.9. The Markov decision process extraction network (MPEN) consists of
    a past (left) and a future (right) subnetwork. The input variables are split into
    two groups:'
  id: totrans-8643
  prefs: []
  type: TYPE_NORMAL
  zh: 图29.9。马尔可夫决策过程提取网络（MPEN）由一个过去子网络（左）和一个未来子网络（右）组成。输入变量被分成两组：
- en: Actions at are controllable by the reinforcement learning agent, zt denote observable
    variables from the dynamics. The future subnetwork has outputs rt only. State
    transitions are modeled by sit as well as st. Note that all weight matrices in
    the past
  id: totrans-8644
  prefs: []
  type: TYPE_NORMAL
  zh: 动作at由强化学习代理控制，zt表示动态中的可观测变量。未来子网络只有输出rt。状态转移由sit和st建模。注意，过去的所有权重矩阵
- en: (*A, . . . , D*) are different from the future matrices (*E,...,H*).
  id: totrans-8645
  prefs: []
  type: TYPE_NORMAL
  zh: (*A, . . . , D*)与未来矩阵(*E,...,H*)不同。
- en: 'In addition to considering expert knowledge and methods for input selection
    we developed an approach, designed for optimal control problems, where a reward
    signal is available. From a reinforcement learning point of view, the performed
    action at, a possibly large set of observables zt as well as the reward rt for
    the given tuple of observation and action is available for all steps in time t.
    In many real world scenarios, the reward function rt = fr(zt, at) is known, because
    it describes the desired goal of the problem to be addressed and was most likely
    designed by us. This knowledge can be modeled in an advanced neural state estimator
    by splitting the network into past and future subnetworks to match following equations:'
  id: totrans-8646
  prefs: []
  type: TYPE_NORMAL
  zh: 除了考虑专家知识和输入选择方法外，我们开发了一种方法，旨在优化控制问题，在这种情况下，奖励信号是可用的。从强化学习的角度来看，执行的动作at、可能的大量可观测量zt以及给定观测和动作元组的奖励rt在时间t的所有步骤中都是可用的。在许多现实场景中，奖励函数rt
    = fr(zt, at)是已知的，因为它描述了要解决问题的期望目标，并且很可能是我们设计的。这种知识可以通过将网络拆分为过去和未来子网络，在高级神经状态估计器中建模以匹配以下方程：
- en: $s_{t+1}=f_{\rm past}(s_{t},z_{t},a_{t}),\ t\leq0$ (29.11) $s_{t+1}=f_{\rm future}(s_{t},a_{t}),\
    t>0.$ (29.12)
  id: totrans-8647
  prefs: []
  type: TYPE_NORMAL
  zh: $s_{t+1}=f_{\rm past}(s_{t},z_{t},a_{t}),\ t\leq0$ (29.11) $s_{t+1}=f_{\rm future}(s_{t},a_{t}),\
    t>0.$ (29.12)
- en: 'The split of the network addresses an inconsistency within the topology of
    the RNN since observables z are not available for future time slices (t > 0).
    Another topology to address this problem would be the dynamically consistent recurrent
    neural network [34]. This topology however must predict all observables z which
    can particularly cause problems when some variables are harder to predict than
    others. Since the split into past and future allows us to use any target, variables
    not included in the input set can also be considered. This allows to use the reward
    signal or, in case of a known reward function, the arguments of the reward function
    as targets:'
  id: totrans-8648
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的分裂解决了RNN拓扑中的不一致性，因为可观测量z在未来时间片（t > 0）中不可用。为了解决这个问题，另一种拓扑是动态一致的递归神经网络[34]。然而，这种拓扑必须预测所有可观测量z，这在某些变量较难预测时可能会造成问题。由于将数据分为过去和未来，我们可以使用任何目标，未包含在输入集中的变量也可以被考虑。这使得可以使用奖励信号，或者在已知奖励函数的情况下，作为目标的奖励函数的参数：
- en: $$(29.13)$$
  id: totrans-8649
  prefs: []
  type: TYPE_NORMAL
  zh: $$(29.13)$$
- en: $$r_{t}=g(s_{t},a_{t}),t\geq0$$
  id: totrans-8650
  prefs: []
  type: TYPE_NORMAL
  zh: $$r_{t}=g(s_{t},a_{t}),t\geq0$$
- en: rt = g(st, at), t ≥ 0 (29.13)
  id: totrans-8651
  prefs: []
  type: TYPE_NORMAL
  zh: rt = g(st, at), t ≥ 0 (29.13)
- en: 'Note that the current state st and the applied action at are sufficient to
    describe the expectation value of all relevant reward functions, because the entire
    information about the successor state''s st+1 probability distribution must be
    included in these two arguments. This target allows us to change the objective
    of the neural network to the desired dynamics:'
  id: totrans-8652
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当前状态st和施加的动作at足以描述所有相关奖励函数的期望值，因为关于后继状态st+1概率分布的所有信息必须包含在这两个参数中。这个目标使我们可以将神经网络的目标更改为所需的动态：
- en: $$\sum_{t=1}^{T}(r_{t}-r_{t}^{d})^{2}\rightarrow\operatorname*{min}_{f,g}$$
  id: totrans-8653
  prefs: []
  type: TYPE_NORMAL
  zh: $$\sum_{t=1}^{T}(r_{t}-r_{t}^{d})^{2}\rightarrow\operatorname*{min}_{f,g}$$
- en: $$(29.14)$$
  id: totrans-8654
  prefs: []
  type: TYPE_NORMAL
  zh: $$(29.14)$$
- en: f,g (29.14)
  id: totrans-8655
  prefs: []
  type: TYPE_NORMAL
  zh: f,g (29.14)
- en: The resulting topology allows the neural network to accumulate all information
    required for the Markov property from the provided past observations in the past
    network, while the future network remodels the state transitions.
  id: totrans-8656
  prefs: []
  type: TYPE_NORMAL
  zh: 结果拓扑允许神经网络从提供的过去观察中累积满足马尔可夫性质所需的所有信息，而未来网络则重塑状态转移。
- en: It is also possible to encode prior knowledge about the distribution of relevant
    information over the considered past horizon by adapting the topology of the past
    network accordingly. A possible modification could be to add additional inputs
    directly to the hidden cluster s0, for instance if they are known to be constant
    over the considered past horizon. E.g., the ambient temperature, pressure, and
    humidity for the combustion process of a gas turbine. Since these variables are
    constant over the considered unfolded past network, they can directly be fed to
    the cluster s0. The only constraint for any input of past information is that
    it has to be passed through the hidden cluster s0 which is ultimately used as
    the output of the Markovian state representation. Therefore, possibilities for
    topologies of the past network are countless.
  id: totrans-8657
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以通过相应调整过去网络的拓扑来编码有关相关信息在考虑的过去时间范围内分布的先验知识。一个可能的修改是直接向隐藏簇s0添加额外输入，例如，如果它们已知在考虑的过去时间范围内是恒定的。比如，燃气涡轮的燃烧过程中的环境温度、压力和湿度。由于这些变量在考虑的展开过去网络中是恒定的，因此可以直接馈入簇s0。任何过去信息输入的唯一约束是，它必须经过隐藏簇s0，最终用于马尔可夫状态表示的输出。因此，过去网络的拓扑可能性是无穷无尽的。
- en: The representation of these findings leads to the Markov decision process extraction
    network (MPEN) shown in Fig. 29.9. The subnetwork on the left (past network) has
    no explicit target and provides information to the neural branch on the right
    (future network). The two subnetworks are connected via an arbitrary neural structure,
    e.g., a weight matrix or a multi-layer perceptron.
  id: totrans-8658
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现的表示导致了图29.9中所示的马尔可夫决策过程提取网络（MPEN）。左侧的子网络（过去网络）没有明确的目标，并向右侧的神经分支（未来网络）提供信息。这两个子网络通过任意神经结构连接，例如，权重矩阵或多层感知器。
- en: The future network uses the information provided by s0 as well as the future
    actions to learn a system's dynamic capable to predict the sequence of future
    rewards. Note that future actions are important inputs preventing the network
    to predict the sequence of actions which induced the state transitions. This is
    important because action selection can be based on external information which
    is not included in the set of observables, or might even be unpredictable due
    to random exploration. See Sec. 29.3.1 for more details regarding this issue.
  id: totrans-8659
  prefs: []
  type: TYPE_NORMAL
  zh: 未来网络利用s0提供的信息以及未来的行动来学习一个动态系统，能够预测未来奖励的序列。请注意，未来的行动是重要的输入，防止网络预测导致状态转变的行动序列。这一点很重要，因为行动选择可能基于外部信息，而这些信息并不包含在可观测集合中，或者可能由于随机探索而无法预测。有关此问题的更多细节，请参见第29.3.1节。
- en: As proven in [27], an RNN can be used to approximate an MDP by predicting all
    expected future successor states based on a history of observations. The structure
    of an RNN forces each hidden cluster s to encode all necessary information to
    estimate a successor state with respect to the influence of an action.
  id: totrans-8660
  prefs: []
  type: TYPE_NORMAL
  zh: 如[27]所证明，RNN可以通过基于观察历史预测所有预期的未来后继状态来近似MDP。RNN的结构强制每个隐藏簇s编码所有必要的信息，以便在考虑行动影响的情况下估计后继状态。
- en: For this reason an RNN must be capable to estimate the expected rewards for
    each future state because a reward function can only use a state, action, and
    the resulting successor state as arguments. Therefore, it is sufficient to model
    a dynamical system predicting the reward for all future time slices. Based on
    this conclusion, the approach is designed to model the minimal required dynamics
    of a regarded system [4].
  id: totrans-8661
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RNN必须能够估计每个未来状态的预期奖励，因为奖励函数只能使用状态、行动和结果后继状态作为参数。因此，建模一个动态系统以预测所有未来时间片的奖励是足够的。基于这一结论，该方法旨在建模被视为系统的最小所需动态[4]。
- en: The major advantage of the introduced neural topology over other RNN based state
    estimators is the capability to model a minimal dynamics from a set of observables
    without manual selection of variables. A further advantage is the capability to
    extract a minimal state space. Networks that need to forecast all observables
    such as the dynamically consistent RNN [34] encode all information into the state
    space and are therefore not minimal. This is of special interest if the set of
    observables contains unimportant variables that are possibly difficult to predict
    and therefore cause a drop in forecast performance. Additionally, such variables
    interfere with the training process, since the validation error can be highly
    influenced by these variables. In other words, resulting largest residuals causing
    the training result of the entire neural network to be less robust.
  id: totrans-8662
  prefs: []
  type: TYPE_NORMAL
  zh: 引入的神经拓扑相对于其他基于 RNN 的状态估计器的主要优势在于能够从一组可观察量中建模最小动态，而无需手动选择变量。另一个优势是提取最小状态空间的能力。需要预测所有可观察量的网络，例如动态一致的
    RNN [34]，将所有信息编码到状态空间中，因此不是最小的。如果可观察量集合中包含不重要的变量，这些变量可能难以预测，从而导致预测性能下降，这一点尤为重要。此外，这些变量会干扰训练过程，因为验证误差可能会受到这些变量的高度影响。换句话说，产生的最大残差会导致整个神经网络的训练结果不够稳健。
- en: 29.4.1 Reward Function Design Influences The Performance Of A State Estimator
  id: totrans-8663
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.4.1 奖励函数设计影响状态估计器的性能
- en: In contrast to the standard RNN, the difficulty to train the neural topology
    depends on the reinforcement learning problem itself since the definition of the
    reward function provides the target. We could show that for episodic problems
    like a cart-pole it is possible to train the network with the standard reward
    function that gives some positive or negative feedback at the end of trajectories
    [4]. However, the problem becomes much easier to solve when more information about
    the quality of the current state is provided. This is reflected in additional
    gradient information speeding up the learning process. For most real world applications,
    such a reward function can be easily provided since we usually participate in
    its design. In case the reward function is known, the arguments of the reward
    function are usually preferred to be used as targets.
  id: totrans-8664
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准的 RNN 相比，训练神经拓扑的难度取决于强化学习问题本身，因为奖励函数的定义提供了目标。我们可以证明，对于像杠杆小车这样的情景问题，可以使用标准的奖励函数训练网络，该函数在轨迹结束时提供一些正或负反馈
    [4]。然而，当提供有关当前状态质量的更多信息时，问题变得更容易解决。这反映在额外的梯度信息中，加速了学习过程。对于大多数实际应用，通常可以轻松提供这样的奖励函数，因为我们通常参与其设计。如果奖励函数已知，奖励函数的参数通常更倾向于作为目标使用。
- en: 29.4.2 Choosing The Forecast Horizon Of A State Estimator
  id: totrans-8665
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.4.2 选择状态估计器的预测视野
- en: When designing a neural state estimator network, the question about the number
    of future steps, that should be included into the prediction, arises immediately.
    Since the networks are unfolded in time towards the past and the future for a
    limited number of steps, a practical answer to that problem has to be found.
  id: totrans-8666
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计神经状态估计网络时，关于应包含在预测中的未来步数的问题立即出现。由于网络在时间上向过去和未来展开，步数是有限的，因此必须找到该问题的实际答案。
- en: 'Fortunately, the definition of the reinforcement learning problem itself provides
    an answer. Using a discount factor γ to define the return, limits the significant
    horizon of future time steps. The return, defining the performance of an agent''s
    strategy, is defined by:'
  id: totrans-8667
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，强化学习问题本身的定义提供了答案。使用折扣因子 γ 来定义回报，限制了未来时间步数的重要视野。回报定义了代理策略的性能，其定义如下：
- en: $$\sum_{t=1}^{\infty}r_{t}\gamma^{t}.$$
  id: totrans-8668
  prefs: []
  type: TYPE_NORMAL
  zh: $$\sum_{t=1}^{\infty}r_{t}\gamma^{t}.$$
- en: $$(29.15)$$
  id: totrans-8669
  prefs: []
  type: TYPE_NORMAL
  zh: $$(29.15)$$
- en: rtγt. (29.15)
  id: totrans-8670
  prefs: []
  type: TYPE_NORMAL
  zh: rtγt. (29.15)
- en: In practice one can safely limit the number of future time steps accordingly,
    since the impact of the rewards decreases exponentially.
  id: totrans-8671
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，可以安全地相应限制未来时间步数，因为奖励的影响呈指数下降。
- en: 29.5 The Trick Of Addressing Long Term Dependencies
  id: totrans-8672
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.5 解决长期依赖问题的技巧
- en: Most state estimation approaches rely on Takens's theorem [33] which states
    that a sufficient number of past time slices contain all information necessary
    to
  id: totrans-8673
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数状态估计方法依赖于塔肯斯定理 [33]，该定理表明，足够数量的过去时间切片包含了所有必要的信息。
- en: '![713_image_0.png](713_image_0.png)'
  id: totrans-8674
  prefs: []
  type: TYPE_IMG
  zh: '![713_image_0.png](713_image_0.png)'
- en: Fig. 29.10. The Markov decision process extraction network with shortcuts (MPEN-S)
  id: totrans-8675
  prefs: []
  type: TYPE_NORMAL
  zh: 图 29.10. 带快捷方式的马尔可夫决策过程提取网络 (MPEN-S)
- en: 'consists of a past (left) and a future (right) subnetwork. The input variables
    are split into two groups: Actions at are controllable by the reinforcement learning
    agent, zt denote observable variables from the dynamics. The future subnetwork
    has outputs rt.'
  id: totrans-8676
  prefs: []
  type: TYPE_NORMAL
  zh: 由过去（左）和未来（右）子网络组成。输入变量分为两组：动作at由强化学习代理控制，zt表示来自动态的可观测变量。未来子网络的输出为rt。
- en: State transitions are modeled by sit as well as st. Note that all weight matrices
    in the past (*A, . . . , E*) are different from future matrices (*G, . . . , L*).
    All hidden clusters as well as all output clusters are connected to a bias (not
    shown in the figure).
  id: totrans-8677
  prefs: []
  type: TYPE_NORMAL
  zh: 状态转移由sit和st建模。请注意，过去的所有权重矩阵（*A, . . . , E*）与未来的矩阵（*G, . . . , L*）不同。所有隐藏集群以及所有输出集群都连接到一个偏置（图中未显示）。
- en: estimate a Markovian state. Recurrent state estimators show remarkable performance
    in various applications such as state estimation for gas turbine control
  id: totrans-8678
  prefs: []
  type: TYPE_NORMAL
  zh: 估计一个马尔可夫状态。递归状态估计器在各种应用中表现出色，例如用于气体涡轮机控制的状态估计。
- en: '[28], but despite the advantages of RNNs there have been concerns regarding
    their capability to model long-term dependencies [3]. In a system exhibiting longterm
    dependencies the system''s output at time T is dependent on the input at time
    t T [14]. The problem was discovered by Mozer [16] who found RNNs to be unable
    to capture global effects in classical music. The main reason for this effect
    are vanishing gradients in gradient-based learning methods [6, 14]. Longterm dependencies
    occur in many time series ranging from technical systems to financial data. To
    overcome this issue the Markov decision process extraction network with shortcuts
    (MPEN-S) [5] is introduced. It is an extension of the previously introduced MPEN
    topology (Sec. 29.4), where additional shortcuts connect clusters across a fixed
    number of time slices (Fig. 29.10). Examples are shortcuts of length n in a network
    with p steps in the past and f steps into the future that connect s−n → s0, s−n+1
    → s1, ..., sp → sp+n. In the future part, shortcuts of the same length are added
    from s1 → sn+1, s2 → sn+2, ...,'
  id: totrans-8679
  prefs: []
  type: TYPE_NORMAL
  zh: '[28]，但尽管RNN有诸多优点，人们仍对其建模长期依赖的能力表示担忧[3]。在一个表现出长期依赖的系统中，系统在时间T的输出依赖于时间t T的输入[14]。这一问题由Mozer发现[16]，他发现RNN无法捕捉古典音乐中的全局效应。造成这种效应的主要原因是基于梯度的学习方法中的梯度消失问题[6,
    14]。长期依赖存在于许多时间序列中，从技术系统到金融数据。为了解决这个问题，引入了带捷径的马尔可夫决策过程提取网络（MPEN-S）[5]。这是先前提出的MPEN拓扑的扩展（第29.4节），在该拓扑中，额外的捷径连接跨越固定数量时间片的集群（图29.10）。示例包括在过去p步和未来f步的网络中，长度为n的捷径连接s−n
    → s0, s−n+1 → s1, ..., sp → sp+n。在未来部分，添加相同长度的捷径从s1 → sn+1, s2 → sn+2，...。'
- en: sf−n → sf .
  id: totrans-8680
  prefs: []
  type: TYPE_NORMAL
  zh: sf−n → sf。
- en: The resulting topology is successfully used for state estimation problems that
    face the problem of delayed observables or actions that show a delayed effect
    on a dynamics. For instance, gas turbine emission measurements are delayed by
    about one to two minutes, whereas the combustion dynamics of the turbine occurs
    almost instantaneously. Both effects are influenced by the same action applied
    to a turbine, therefore the underlying state contains information about both,
    the highly delayed as well as the short term effect.
  id: totrans-8681
  prefs: []
  type: TYPE_NORMAL
  zh: 结果拓扑成功用于面临延迟可观测量或对动态产生延迟影响的动作的状态估计问题。例如，气体涡轮机的排放测量延迟约为一到两分钟，而涡轮机的燃烧动态几乎是瞬时发生的。这两种效应都受到施加于涡轮机的相同动作的影响，因此底层状态包含关于这两者的信息，即高度延迟的影响以及短期效应。
- en: After presenting a recipe to find a good shortcut length in Sec. 29.5.1, we
    present experiments to demonstrate the capabilities of the MPEN-S topology. Experiments
    include a gas turbine simulation with highly delayed effects of action on the
    dynamics.
  id: totrans-8682
  prefs: []
  type: TYPE_NORMAL
  zh: 在第29.5.1节中提出寻找良好捷径长度的食谱后，我们进行了实验以展示MPEN-S拓扑的能力。实验包括一个气体涡轮机的模拟，具有高度延迟的作用对动态的影响。
- en: 29.5.1 A Recipe To Find A Good Shortcut Length
  id: totrans-8683
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.5.1 寻找良好捷径长度的食谱。
- en: 'For selecting an adequate shortcut length n, the following heuristic can be
    considered: The severity of the vanishing-gradient problem is correlated with
    the number of steps information has to be forwarded within the network. Therefore,
    the value n is chosen to minimizes the total number of steps information has to
    travel from any state in the past to the current state s0. I.e., p i=1 steps(s0,
    s−i, n) →n min, where steps(s0, s−i, n) gives the minimum number of steps to travel
    from s−i to s0, including possible shortcuts. E.g., if n = 2, steps(s0, s−1)=1,
    steps(s0, s−2)=1, steps(s0, s−3)=2, steps(s0, s−4)=2. The only information required
    for this heuristic is the maximum number of past time slices that are assumed
    to influence the current state. Fig. 29.11 shows results from experiments with
    different shortcut lengths indicating that the heuristic leads to reasonable results.'
  id: totrans-8684
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适当的快捷长度n时，可以考虑以下启发式方法：消失梯度问题的严重性与信息在网络中转发所需的步数相关。因此，选择的n值旨在最小化信息从过去任何状态到当前状态s0的总步数。即，p
    i=1 steps(s0, s−i, n) →n min，其中steps(s0, s−i, n)给出从s−i到s0的最小步数，包括可能的快捷方式。例如，如果n
    = 2，steps(s0, s−1)=1，steps(s0, s−2)=1，steps(s0, s−3)=2，steps(s0, s−4)=2。这个启发式方法所需的唯一信息是假定会影响当前状态的过去时间片的最大数量。图
    29.11 显示了不同快捷长度实验的结果，表明该启发式方法导致了合理的结果。
- en: 29.5.2 Experiments On Long Term Dependencies Problems
  id: totrans-8685
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.5.2 关于长期依赖问题的实验
- en: To demonstrate the capabilities of the MPEN-S two benchmarks are used, a sequence
    of random numbers as well as a gas turbine simulation. We compare the MPEN-S with
    shortcuts of length n = 4 to the MPEN. The shortcut length n = 4 was chosen according
    to a heuristic (Sec. 29.5.1). For each architecture and benchmark, 10 networks
    are trained by online backpropagation with a learning rate of 0.001 on 10,000
    observations, of which 30% are used for validation.
  id: totrans-8686
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示MPEN-S的能力，使用了两个基准，一个是随机数字序列，另一个是燃气轮机模拟。我们将带有长度为n = 4的快捷方式的MPEN-S与MPEN进行比较。根据启发式方法（第29.5.1节）选择了快捷长度n
    = 4。对于每个架构和基准，通过在线反向传播训练10个网络，学习率为0.001，基于10,000个观察数据，其中30%用于验证。
- en: Evaluation is based on another set of the same size but without noise. To judge
    the quality of the state estimate, represented by the activation values in s0,
    we test the content of information within s0. To do so we feed the estimated states
    as inputs to a feed forward neural network (two hidden layers with 150 neurons
  id: totrans-8687
  prefs: []
  type: TYPE_NORMAL
  zh: 评估基于另一组相同大小但没有噪声的数据。为了判断状态估计的质量，用s0中的激活值表示，我们测试s0中的信息内容。为此，我们将估计的状态作为输入馈送到一个前馈神经网络（两个隐藏层，每层150个神经元）。
- en: '![714_image_0.png](714_image_0.png)'
  id: totrans-8688
  prefs: []
  type: TYPE_IMG
  zh: '![714_image_0.png](714_image_0.png)'
- en: Fig. 29.11. Results from experiments with different shortcut lengths and a past
    of p = 20. (a) shows the sum pi=1 steps(s0, s−i, n) of a network without shortcuts
    and networks with shortcuts of different lengths (n ∈ {4, 5, 6} minimizes the
    sum). (b)
  id: totrans-8689
  prefs: []
  type: TYPE_NORMAL
  zh: 图 29.11. 使用不同快捷长度和过去p = 20的实验结果。(a)显示了没有快捷方式的网络和具有不同长度（n ∈ {4, 5, 6}）的网络的总和pi=1
    steps(s0, s−i, n)（最小化总和）。(b)
- en: shows the validation errors of these networks for a random numbers experiment
    (Sec. 29.5.2). The correlation between the sum of steps and the validation error
    is obvious.
  id: totrans-8690
  prefs: []
  type: TYPE_NORMAL
  zh: 显示这些网络在随机数字实验中的验证错误（第 29.5.2 节）。步数之和与验证错误之间的相关性显而易见。
- en: each) whose targets are the true Markovian states of the benchmark applications.
    In the best case, the estimated states include all relevant information and consequently
    allow a perfect mapping to the true Markovian states, i.e., the correlation between
    the target and output is 1.
  id: totrans-8691
  prefs: []
  type: TYPE_NORMAL
  zh: 每个网络的目标是基准应用的真实马尔可夫状态。在最佳情况下，估计的状态包含所有相关信息，从而允许与真实马尔可夫状态的完美映射，即目标与输出之间的相关性为1。
- en: Random Numbers Experiment. In a first experiment, sequences of equally distributed
    random numbers xi ∈ [0, 1] are used. The network with a past and a future horizon
    of i steps receives a sequence xt, xt+1*,...,x*t+i as inputs in the past part
    of the network. The sequence is also used as targets for the future part of the
    network, introducing a delay between input and corresponding output. This way,
    the network has to output an input given at time step t at time step t+i to minimize
    its error. In addition, equally distributed noise e ∈ [−0.05, 0.05]
  id: totrans-8692
  prefs: []
  type: TYPE_NORMAL
  zh: 随机数实验。在第一次实验中，使用均匀分布的随机数序列 xi ∈ [0, 1]。具有 i 步过去和未来视野的网络接收序列 xt, xt+1*,...,x*t+i
    作为网络过去部分的输入。该序列也用作网络未来部分的目标，在输入和相应输出之间引入延迟。通过这种方式，网络必须在时间步 t+i 输出在时间步 t 给出的输入，以最小化其误差。此外，还引入了均匀分布的噪声
    e ∈ [−0.05, 0.05]。
- en: is added to the target values for training. The goal of the state estimation
    for the random numbers problem is to encode information about the past i random
    numbers.
  id: totrans-8693
  prefs: []
  type: TYPE_NORMAL
  zh: 被添加到训练的目标值中。随机数问题的状态估计目标是编码关于过去 i 个随机数的信息。
- en: Gas Turbine Simulation. To demonstrate the capabilities of the presented approach
    on a problem similar to the real-world application of our interest, a gas turbine
    simulation is used. The simulation provides a controllable variable *pilot* affecting
    the emissions and humming of the turbine. The pilot fraction is one of the most
    important fuel fractions regarding the combustion control. In order to keep the
    simulation as simple as possible, the combustion tuning is reduced to this variable.
    The goal of a strategy is to minimize emissions and avoid critical humming, which
    is reflected in the reward function. While humming reacts instantaneously, the
    emissions, like in real world, have a long delay and a defined blur over several
    steps in time. Each step, the simulator provides observations about its current
    state. The only additional information for the state estimation model is the maximal
    expected delay d of the simulation. The goal of the state estimator is to encode
    all relevant information about the past d steps.
  id: totrans-8694
  prefs: []
  type: TYPE_NORMAL
  zh: 燃气涡轮模拟。为了展示所提出方法在与我们关注的实际应用类似的问题上的能力，使用了燃气涡轮模拟。该模拟提供了一个可控变量 *pilot*，影响涡轮的排放和嗡嗡声。飞行员比例是与燃烧控制相关的最重要的燃料比例之一。为了使模拟尽可能简单，燃烧调节仅限于这个变量。策略的目标是最小化排放并避免关键嗡嗡声，这在奖励函数中得以体现。尽管嗡嗡声瞬间反应，排放如同现实世界一样，存在较长的延迟和在多个时间步长上的明确模糊。每一步，模拟器提供关于其当前状态的观察。状态估计模型的唯一附加信息是模拟的最大期望延迟
    d。状态估计器的目标是编码过去 d 步的所有相关信息。
- en: Results. Fig. 29.12 illustrates the correlation of the estimated and true Markovian
    states of the random numbers experiment ((a) and (b)) and the results of the gas
    turbine simulation experiment ((c) and (d)). Both benchmark results indicate that
    the MPEN approach is capable of estimating the Markovian states well for small
    delays. For longer dependencies however, the approximation quality drops significantly,
    while the MPEN-S can maintain its performance. Table 29.1 shows the average correlation
    and validation errors of all state variables. The numbers show that the MPEN-S
    outperforms the MPEN both in reconstruction quality of the Markovian state (resulting
    in a better correlation) as well as raw forecast performance (lower validation
    error). The validation error is a good indicator for estimation quality, which
    is especially relevant for real-world applications.
  id: totrans-8695
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。图 29.12 说明了随机数实验中估计的马尔可夫状态与真实马尔可夫状态的相关性（（a）和（b））以及燃气涡轮模拟实验的结果（（c）和（d））。这两个基准结果表明，MPEN
    方法能够很好地估计小延迟下的马尔可夫状态。然而，对于较长的依赖关系，近似质量显著下降，而 MPEN-S 可以保持其性能。表 29.1 显示了所有状态变量的平均相关性和验证误差。数字显示
    MPEN-S 在马尔可夫状态的重构质量（导致更好的相关性）以及原始预测性能（较低的验证误差）方面均优于 MPEN。验证误差是估计质量的良好指标，这在现实世界应用中尤为重要。
- en: '![716_image_0.png](716_image_0.png)'
  id: totrans-8696
  prefs: []
  type: TYPE_IMG
  zh: '![716_image_0.png](716_image_0.png)'
- en: Fig. 29.12. Average correlations of true and estimated Markovian state. A higher
    variable index indicates a variable closer to the present. (a) and (b) show the
    estimation quality for the random numbers problem with a delay of 60 steps. (c)
    and (d) illustrate the estimation quality for the gas turbine simulation with
    a delay of 30 steps.
  id: totrans-8697
  prefs: []
  type: TYPE_NORMAL
  zh: 图29.12. 真实和估计马尔可夫状态的平均相关性。较高的变量索引表示更接近当前的变量。(a)和(b)展示了延迟60步的随机数问题的估计质量。(c)和(d)展示了延迟30步的燃气涡轮仿真的估计质量。
- en: Table 29.1. Comparison of the MPEN and the MPEN-S in terms of validation error
  id: totrans-8698
  prefs: []
  type: TYPE_NORMAL
  zh: 表29.1. MPEN和MPEN-S在验证误差方面的比较
- en: (MSE) as well as correlation between true and estimated Markovian state
  id: totrans-8699
  prefs: []
  type: TYPE_NORMAL
  zh: (均方误差)以及真实和估计的马尔可夫状态之间的相关性
- en: '| architecture   | experiment   | delay correlation validation error   |       |        |'
  id: totrans-8700
  prefs: []
  type: TYPE_TB
  zh: '| 架构         | 实验       | 延迟相关性验证误差               |       |        |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-8701
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| random numbers | 60           | 0.4                                  | 0.9   |        |'
  id: totrans-8702
  prefs: []
  type: TYPE_TB
  zh: '| 随机数 | 60           | 0.4                                  | 0.9   |        |'
- en: '| MPEN           | gas turbine  | 30                                   | 0.988
    | 0.0160 |'
  id: totrans-8703
  prefs: []
  type: TYPE_TB
  zh: '| MPEN           | 燃气涡轮  | 30                                   | 0.988 | 0.0160
    |'
- en: '| random numbers | 60           | 0.99                                 | 0.012
    |        |'
  id: totrans-8704
  prefs: []
  type: TYPE_TB
  zh: '| 随机数 | 60           | 0.99                                 | 0.012 |        |'
- en: '| MPEN-S         | gas turbine  | 30                                   | 0.995
    | 0.0058 |'
  id: totrans-8705
  prefs: []
  type: TYPE_TB
  zh: '| MPEN-S         | 燃气涡轮  | 30                                   | 0.995 | 0.0058
    |'
- en: 29.6 Conclusion
  id: totrans-8706
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 29.6 结论
- en: In this chapter we introduced methods for neural state estimation, particularly
    for reinforcement learning applications. However, the methods can also be applied
    to other applications that require a Markovian state representation. Further,
    a set of practical recipes to overcome problems especially relevant to real world
    applications was presented.
  id: totrans-8707
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了神经状态估计的方法，特别是针对强化学习应用的方法。然而，这些方法也可应用于其他需要马尔可夫状态表示的应用。此外，提出了一系列实用方案，以克服特别与实际应用相关的问题。
- en: Sec. 29.3 describes the usage of recurrent neural networks (RNNs) as state estimators.
    Thereafter, a series of recipes to improve the applicability and to overcome possible
    pitfalls are introduced. A partially observable cart-pole is used to demonstrate
    the approach in Sec. 29.3.9. In Sec. 29.4, the idea of state estimation based
    on standard RNNs is further developed towards the Markov decision process extraction
    network (MPEN). This topology is dynamically consistent and is capable to autonomously
    model a minimal Markov decision process (MDP) from a large number of observables.
    Finally, long-term effects are addressed by the Markov decision process extraction
    network with shortcuts (MPEN-S) in Sec. 29.5. This topology is especially relevant
    to real world applications where different effects can occur on various time scales.
    The capabilities of the MPEN-S
  id: totrans-8708
  prefs: []
  type: TYPE_NORMAL
  zh: 第29.3节描述了使用递归神经网络（RNN）作为状态估计器的用法。随后，介绍了一系列改进适用性和克服潜在问题的方案。部分可观察的倒立摆在第29.3.9节中用于演示该方法。在第29.4节中，基于标准RNN的状态估计思想进一步发展为马尔可夫决策过程提取网络（MPEN）。该拓扑是动态一致的，能够自主从大量观测中建模最小的马尔可夫决策过程（MDP）。最后，第29.5节通过带捷径的马尔可夫决策过程提取网络（MPEN-S）讨论了长期效果。这种拓扑在不同时间尺度上可能出现多种效应的实际应用中尤其相关。
- en: are demonstrated using benchmarks, including a gas turbine simulation. Results
    on the benchmark applications indicate a significant improvement over previous
    approaches. This is reflected in a smaller validation error of the forecast as
    well as an improved estimation quality, especially for state variables dependent
    on highly delayed observables. Another important conclusion to draw from the experiments
    is the correlation between the validation error and estimation quality.
  id: totrans-8709
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基准测试进行演示，包括燃气涡轮仿真。基准应用的结果表明，相较于以前的方法有显著改善。这体现在预测的验证误差更小，以及估计质量的提高，特别是对高度延迟观测值依赖的状态变量。另一个重要的结论是验证误差与估计质量之间的相关性。
- en: This information is of high value, since in any real-world application one can
    only rely on the measure of the validation error. Acknowledgment. Part of this
    work has been funded by the Federal German Ministry for Education and Research
    under the grant ALICE, 01 IB10003 A-C.
  id: totrans-8710
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息具有高度价值，因为在任何实际应用中，人们只能依赖于验证误差的度量。致谢。本部分工作得到德国联邦教育与研究部资助，拨款号ALICE，01 IB10003
    A-C。
- en: '[1] Bakker, B.: Reinforcement Learning with Long Short-Term Memory. In: Becker,
    S., Dietterich, T.G., Ghahramani, Y. (eds.) Advances in Neural Information Processing
    Systems, pp. 1475–1482. MIT Press (2002)'
  id: totrans-8711
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Bakker, B.: 带长短期记忆的强化学习. 在: Becker, S., Dietterich, T.G., Ghahramani, Y.
    (编者) 神经信息处理系统进展, pp. 1475–1482. MIT出版社 (2002)'
- en: '[2] Bellman, R.E.: Dynamic Programming. Princeton University Press (1957)'
  id: totrans-8712
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Bellman, R.E.: 动态规划. 普林斯顿大学出版社 (1957)'
- en: '[3] Bengio, Y., Simard, P., Frasconi, P.: Learning long-term dependencies with
    gradient descent is difficult. IEEE Transactions on Neural Networks 5(2), 157–166'
  id: totrans-8713
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Bengio, Y., Simard, P., Frasconi, P.: 使用梯度下降学习长期依赖是困难的. IEEE神经网络汇刊 5(2),
    157–166'
- en: (1994)
  id: totrans-8714
  prefs: []
  type: TYPE_NORMAL
  zh: (1994)
- en: '[4] Duell, S., Hans, A., Udluft, S.: The Markov Decision Process Extraction
    Network.'
  id: totrans-8715
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Duell, S., Hans, A., Udluft, S.: 马尔可夫决策过程提取网络.'
- en: 'In: Proc. of the 18th European Symposium on Artificial Neural Networks (2010)'
  id: totrans-8716
  prefs: []
  type: TYPE_NORMAL
  zh: '在: 第18届欧洲人工神经网络研讨会论文集 (2010)'
- en: '[5] Duell, S., Weichbrodt, L., Hans, A., Udluft, S.: Recurrent Neural State
    Estimation in Domains with Long-Term Dependencies. In: Proc. of the 20th European
    Symposium on Artificial Neural Networks (2012)'
  id: totrans-8717
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Duell, S., Weichbrodt, L., Hans, A., Udluft, S.: 在具有长期依赖的领域中进行递归神经状态估计.
    在: 第20届欧洲人工神经网络研讨会论文集 (2012)'
- en: '[6] Frasconi, P., Gori, M., Soda, G.: Local feedback multilayered networks.
    Neural Computation 4(1), 120–130 (1992)'
  id: totrans-8718
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Frasconi, P., Gori, M., Soda, G.: 局部反馈多层网络. 神经计算 4(1), 120–130 (1992)'
- en: '[7] Gomez, F., Miikkulainen, R.: 2-D Balancing with Recurrent Evolutionary
    Networks. In: Proceedings of the International Conference on Artificial Neural
    Networks (ICANN 1998), pp. 425–430. Springer (1998)'
  id: totrans-8719
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Gomez, F., Miikkulainen, R.: 使用递归进化网络的二维平衡. 在: 国际人工神经网络会议论文集 (ICANN 1998),
    pp. 425–430. Springer (1998)'
- en: '[8] Gomez, F.: Robust Non-Linear Control through Neuroevolution. PhD thesis,
    Departement of Computer Sciences Technical Report AI-TR-03-3003 (2003)'
  id: totrans-8720
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Gomez, F.: 通过神经进化实现鲁棒非线性控制. 博士论文，计算机科学系技术报告 AI-TR-03-3003 (2003)'
- en: '[9] Haykin, S.: Neural networks and learning machines, vol. 3. Prentice-Hall
    (2009)'
  id: totrans-8721
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Haykin, S.: 神经网络与学习机器, vol. 3. Prentice-Hall (2009)'
- en: '[10] Haykin, S., Principe, J., Sejnowski, T., McWhirter, J.: New directions
    in statistical signal processing: from systems to brain. MIT Press (2007)'
  id: totrans-8722
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Haykin, S., Principe, J., Sejnowski, T., McWhirter, J.: 统计信号处理的新方向：从系统到大脑.
    MIT出版社 (2007)'
- en: '[11] Kolen, J.F., Kremer, S.C.: A field guide to dynamical recurrent networks.
    IEEE'
  id: totrans-8723
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Kolen, J.F., Kremer, S.C.: 动态递归网络的实地指南. IEEE'
- en: Press (2001)
  id: totrans-8724
  prefs: []
  type: TYPE_NORMAL
  zh: 出版社 (2001)
- en: '[12] Kaelbling, L.P., Littman, M.L., Cassandra, A.R.: Planning and acting in
    partially observable stochastic domains. Artificial Intelligence 101, 99–134 (1998)'
  id: totrans-8725
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Kaelbling, L.P., Littman, M.L., Cassandra, A.R.: 在部分可观察随机领域中的规划与行动. 人工智能
    101, 99–134 (1998)'
- en: '[13] Kietzmann, T.C., Riedmiller, M.: The Neuro Slot Car Racer: Reinforcement
    Learning in a Real World Setting. In: Proc. of the Int. Conf. on Machine Learning
    and Applications. IEEE (2009)'
  id: totrans-8726
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Kietzmann, T.C., Riedmiller, M.: 神经插槽赛车：在现实世界环境中的强化学习. 在: 国际机器学习与应用会议论文集.
    IEEE (2009)'
- en: '[14] Lin, T., Horne, B.G., Tino, P., Giles, C.L.: Learning long-term dependencies
    in NARX recurrent neural networks. IEEE Transactions on Neural Networks 7(6) (1996)'
  id: totrans-8727
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Lin, T., Horne, B.G., Tino, P., Giles, C.L.: 在NARX递归神经网络中学习长期依赖. IEEE神经网络汇刊
    7(6) (1996)'
- en: '[15] Medsker, L., Jain, L.: Recurrent Neural Networks: Design and Application.
    International Series on Comp. Intelligence, vol. I. CRC Press (1999)'
  id: totrans-8728
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Medsker, L., Jain, L.: 递归神经网络：设计与应用. 计算智能国际系列, vol. I. CRC出版社 (1999)'
- en: '[16] Mozer, M.C.: Induction of multiscale temporal structure. In: Advances
    in Neural Information Processing Systems, vol. 4, pp. 275–282 (1992)'
  id: totrans-8729
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Mozer, M.C.: 多尺度时间结构的归纳. 在: 神经信息处理系统进展, vol. 4, pp. 275–282 (1992)'
- en: '[17] Meuleau, N., Peshkin, L., Kee-Eung, K., Kaebling, L.P.: Learning Finite-State
    Controllers for Partially Observable Environments. In: Proceedings of the Fifteenth
    International Conference on Uncertainty in Artificial Intelligence (UAI'
  id: totrans-8730
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Meuleau, N., Peshkin, L., Kee-Eung, K., Kaebling, L.P.: 为部分可观察环境学习有限状态控制器.
    在: 第十五届国际人工智能不确定性会议论文集 (UAI)'
- en: 1999), pp. 427–436 (1999)
  id: totrans-8731
  prefs: []
  type: TYPE_NORMAL
  zh: 1999), pp. 427–436 (1999)
- en: '[18] Neuneier, R., Zimmermann, H.-G.: How to Train Neural Networks. In: Orr,
    G.B.,'
  id: totrans-8732
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Neuneier, R., Zimmermann, H.-G.: 如何训练神经网络. 在: Orr, G.B.,'
- en: Müller, K.-R. (eds.) NIPS-WS 1996. LNCS, vol. 1524, pp. 373–423. Springer, Heidelberg
    (1998)
  id: totrans-8733
  prefs: []
  type: TYPE_NORMAL
  zh: Müller, K.-R. (编者) NIPS-WS 1996. LNCS, vol. 1524, pp. 373–423. Springer, Heidelberg
    (1998)
- en: '[19] Peters, J., Schaal, A.: Reinforcement learning of motor skills with policy
    gradients.'
  id: totrans-8734
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Peters, J., Schaal, A.: 使用策略梯度的运动技能强化学习. '
- en: Neural Networks 21(4) (2008)
  id: totrans-8735
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络 21(4) (2008)
- en: '[20] Ramachandran, D.: Knowledge and Ignorance in Reinforcement Learning. PhD'
  id: totrans-8736
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Ramachandran, D.: 强化学习中的知识与无知。博士论文'
- en: thesis, University of Illinois (2011)
  id: totrans-8737
  prefs: []
  type: TYPE_NORMAL
  zh: 论文，伊利诺伊大学 (2011)
- en: '[21] Rosenstein, M.T., Barto, A.G., Si, J., Powell, W., Wunsch, D.: Supervised
    actorcritic reinforcement learning. In: Handbook of Learning and Approximate Dynamic
    Programming, pp. 359–380 (2012)'
  id: totrans-8738
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Rosenstein, M.T., Barto, A.G., Si, J., Powell, W., Wunsch, D.: 监督式演员-评论家强化学习。在《学习与近似动态规划手册》中，第359–380页
    (2012)'
- en: '[22] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning representations
    by backpropagating errors. Nature 323(9), 533–536 (1986)'
  id: totrans-8739
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: 通过反向传播误差学习表示。自然 323(9),
    533–536 (1986)'
- en: '[23] Riedmiller, M.: 10 Steps and Some Tricks to Set Up Neural Reinforcement
    Controllers. In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.) NN: Tricks of the
    Trade, 2nd edn. LNCS, vol. 7700, pp. 735–757. Springer, Heidelberg (2012)'
  id: totrans-8740
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Riedmiller, M.: 设置神经强化控制器的10个步骤和一些技巧。在Montavon, G., Orr, G.B., Müller,
    K.-R. (编)《神经网络：行业技巧》，第二版。LNCS，卷7700，第735–757页。施普林格，海德堡 (2012)'
- en: '[24] Riedmiller, M.: Neural Fitted Q Iteration - First Experiences with a Data
    Efficient Neural Reinforcement Learning Method. In: Gama, J., Camacho, R., Brazdil,
    P.B., Jorge, A.M., Torgo, L. (eds.) ECML 2005. LNCS (LNAI), vol. 3720, pp. 317–328.'
  id: totrans-8741
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Riedmiller, M.: 神经拟合Q迭代 - 数据高效神经强化学习方法的初步经验。在Gama, J., Camacho, R., Brazdil,
    P.B., Jorge, A.M., Torgo, L. (编)《ECML 2005》。LNCS (LNAI)，卷3720，第317–328页。'
- en: Springer, Heidelberg (2005)
  id: totrans-8742
  prefs: []
  type: TYPE_NORMAL
  zh: 施普林格，海德堡 (2005)
- en: '[25] Samuel, A.L.: Some studies in machine learning using the game of checkers.
    IBM'
  id: totrans-8743
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Samuel, A.L.: 使用跳棋游戏进行机器学习的一些研究。IBM'
- en: Journal on Research and Developement, 210–229 (1959)
  id: totrans-8744
  prefs: []
  type: TYPE_NORMAL
  zh: 《研究与开发期刊》，210–229 (1959)
- en: '[26] Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. MIT
    Press'
  id: totrans-8745
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Sutton, R.S., Barto, A.G.: 强化学习：导论。MIT出版社'
- en: (1998)
  id: totrans-8746
  prefs: []
  type: TYPE_NORMAL
  zh: (1998)
- en: '[27] Schneegass, D.: Steigerung der Informationseffizienz im Reinforcement-Learning.'
  id: totrans-8747
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Schneegass, D.: 提高强化学习中的信息效率。'
- en: PhD thesis, Luebeck University (2008)
  id: totrans-8748
  prefs: []
  type: TYPE_NORMAL
  zh: 博士论文，吕贝克大学 (2008)
- en: '[28] Schäfer, A.M., Schneegass, D., Sterzing, V., Udluft, S.: A Neural Reinforcement
    Learning Approach to Gas Turbine Control. In: Proc. of the Int. Joint Conf. on
    Neural Networks (2007)'
  id: totrans-8749
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] Schäfer, A.M., Schneegass, D., Sterzing, V., Udluft, S.: 一种用于燃气轮机控制的神经强化学习方法。在《国际联合神经网络会议论文集中》
    (2007)'
- en: '[29] Schäfer, A.M., Udluft, S.: Solving Partially Observable Reinforcement
    Learning Problems with Recurrent Neural Networks. In: Workshop Proc. of the European
    Conf. on Machine Learning (2005)'
  id: totrans-8750
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] Schäfer, A.M., Udluft, S.: 使用循环神经网络解决部分可观察的强化学习问题。在《欧洲机器学习会议研讨会论文集中》 (2005)'
- en: '[30] Schneegass, D., Udluft, S., Martinetz, T.: Neural Rewards Regression for
    NearOptimal Policy Identification in Markovian and Partial Observable Environments.'
  id: totrans-8751
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] Schneegass, D., Udluft, S., Martinetz, T.: 用于马尔可夫和部分可观察环境中近似最优策略识别的神经奖励回归。'
- en: 'In: Proc. of the European Symposium on Artificial Neural Networks, pp. 301–306
    (2007)'
  id: totrans-8752
  prefs: []
  type: TYPE_NORMAL
  zh: 在《欧洲人工神经网络研讨会论文集中》，第301–306页 (2007)
- en: '[31] Schäfer, A.M., Udluft, S., Zimmermann, H.G.: The Recurrent Control Neural
    Network. In: Proc. of the European Symposium on Artificial Neural Networks, pp.'
  id: totrans-8753
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Schäfer, A.M., Udluft, S., Zimmermann, H.G.: 循环控制神经网络。在《欧洲人工神经网络研讨会论文集中》，第'
- en: 319–324 (2007)
  id: totrans-8754
  prefs: []
  type: TYPE_NORMAL
  zh: 319–324 (2007)
- en: '[32] Schäfer, A.M., Zimmermann, H.-G.: Recurrent Neural Networks Are Universal
    Approximators. In: Kollias, S.D., Stafylopatis, A., Duch, W., Oja, E. (eds.) ICANN'
  id: totrans-8755
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] Schäfer, A.M., Zimmermann, H.-G.: 循环神经网络是通用逼近器。在Kollias, S.D., Stafylopatis,
    A., Duch, W., Oja, E. (编)《ICANN》'
- en: 2006. LNCS, vol. 4131, pp. 632–640. Springer, Heidelberg (2006)
  id: totrans-8756
  prefs: []
  type: TYPE_NORMAL
  zh: 2006。LNCS，卷4131，第632–640页。施普林格，海德堡 (2006)
- en: '[33] Takens, F.: Detecting strange attractors in turbulence. Dynamical Systems
    and Turbulence 898, 366–381 (1981)'
  id: totrans-8757
  prefs: []
  type: TYPE_NORMAL
  zh: '[33] Takens, F.: 检测湍流中的奇怪吸引子。《动力系统与湍流》898, 366–381 (1981)'
- en: '[34] Zimmermann, H.G., Grothmann, R., Schäfer, A.M., Tietz, C.: Identification
    and Forecasting of Large Dynamical Systems by Dynamical Consistent Neural Networks.
    In: New Directions in Statistical Signal Processing: From Systems to Brain, pp.
    203–242. MIT Press (2006)'
  id: totrans-8758
  prefs: []
  type: TYPE_NORMAL
  zh: '[34] Zimmermann, H.G., Grothmann, R., Schäfer, A.M., Tietz, C.: 通过动态一致神经网络识别和预测大型动态系统。在《统计信号处理的新方向：从系统到大脑》中，第203–242页。MIT出版社
    (2006)'
- en: '[35] Zimmermann, H.G., Neuneier, R.: Neural network architectures for the modeling
    of dynamical systems. In: Kolen, J.F., Kremer, S.C. (eds.) A Field Guide to Dynamical
    Recurrent Networks, pp. 311–350. IEEE Press (2001)'
  id: totrans-8759
  prefs: []
  type: TYPE_NORMAL
  zh: '[35] Zimmermann, H.G., Neuneier, R.: 动态系统建模的神经网络架构。在Kolen, J.F., Kremer, S.C.
    (编)《动态递归网络的实地指南》中，第311–350页。IEEE出版社 (2001)'
- en: '[36] Zimmermann, H.G., Tietz, C., Grothmann, R.: Forecasting with Recurrent
    Neural Networks: 12 Tricks. In: Montavon, G., Orr, G.B., Müller, K.-R. (eds.)
    NN: Tricks of the Trade, 2nd edn. LNCS, vol. 7700, pp. 687–707. Springer, Heidelberg
    (2012)'
  id: totrans-8760
  prefs: []
  type: TYPE_NORMAL
  zh: '[36] Zimmermann, H.G., Tietz, C., Grothmann, R.：使用递归神经网络进行预测：12个技巧。在：Montavon,
    G., Orr, G.B., Müller, K.-R.（编），NN：行业技巧，第2版，LNCS，第7700卷，第687–707页，施普林格，海德堡（2012）'
- en: 30 10 Steps And Some Tricks To Set Up Neural Reinforcement Controllers
  id: totrans-8761
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 30 10个步骤和一些技巧来建立神经强化控制器
- en: Martin Riedmiller Machine Learning Lab Computer Science Department Albert-Ludwigs
    Universitaet Freiburg riedmiller@informatik.uni-freiburg.de http://ml.informatik.uni-freiburg.de
    Abstract. The paper discusses the steps necessary to set up a neural reinforcement
    controller for successfully solving typical (real world) control tasks. The major
    intention is to provide a code of practice of crucial steps that show how to transform
    control task requirements into the specification of a reinforcement learning task.
    Thereby, we do not necessarily claim that the way we propose is the only one (this
    would require a lot of empirical work, which is beyond the scope of the paper),
    but wherever possible we try to provide insights why we do it the one way or the
    other. Our procedure of setting up a neural reinforcement learning system worked
    well for a large range of real, realistic or benchmark-style control applications.
  id: totrans-8762
  prefs: []
  type: TYPE_NORMAL
  zh: Martin Riedmiller 机器学习实验室 计算机科学系 阿尔伯特-路德维希大学弗赖堡 riedmiller@informatik.uni-freiburg.de
    http://ml.informatik.uni-freiburg.de 摘要。本论文讨论了成功解决典型（真实世界）控制任务所需建立神经强化控制器的步骤。主要目的是提供一套实践规范，展示如何将控制任务要求转化为强化学习任务的规范。在此过程中，我们并不一定声称我们提出的方法是唯一的（这需要大量的实证工作，超出本论文的范围），但我们尽可能提供我们为何选择这样或那样方法的见解。我们的神经强化学习系统建立过程在多个真实、现实或基准风格的控制应用中表现良好。
- en: 'Keywords: Neural reinforcement learning, fitted Q, batch reinforcement learning,
    learning control.'
  id: totrans-8763
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词：神经强化学习，拟合Q，批量强化学习，学习控制。
- en: 30.1 Overview
  id: totrans-8764
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.1 概述
- en: The paper discusses the steps necessary to set up a neural reinforcement controller
    for successfully solving typical (real world) control tasks. The major intention
    is to provide a code of practice containing crucial steps necessary to transform
    control task specifications into the specification and parameterization of a reinforcement
    learning task. Thereby, we do not necessarily claim that the way we propose is
    the only one (this would require a lot of empirical work, which is beyond the
    scope of the paper). But, wherever possible we try to provide insights why we
    do it the one way or the other. In that spirit, this paper is mainly intended
    to be a subjective report on how we tackle control problems by reinforcement learning
    in practice. It is not meant as a general review article and therefore, many related
    and alternative methods are not mentioned here.
  id: totrans-8765
  prefs: []
  type: TYPE_NORMAL
  zh: 本论文讨论了成功解决典型（真实世界）控制任务所需建立神经强化控制器的步骤。主要目的是提供一套实践规范，包含将控制任务规范转化为强化学习任务规范和参数化所需的关键步骤。在此过程中，我们并不一定声称我们提出的方法是唯一的（这需要大量的实证工作，超出本论文的范围）。但，我们尽可能提供我们为何选择这样或那样方法的见解。本论文主要旨在主观报告我们如何在实践中通过强化学习解决控制问题。它并不旨在成为一般性的综述文章，因此，许多相关和替代方法在这里并未提及。
- en: When faced with a real world system, typically a very large number of ways exist
    to formulate it as a learning problem. This is somewhat different from the situation
    usually found in reinforcement learning papers, where all the main settings (like
    state description, actions, control interval) are usually given. In
  id: totrans-8766
  prefs: []
  type: TYPE_NORMAL
  zh: 在面对一个真实世界系统时，通常存在许多将其表述为学习问题的方法。这与强化学习论文中通常的情况有所不同，后者通常会给出所有主要设置（如状态描述、动作、控制间隔）。
- en: 'G. Montavon et al. (Eds.): NN: Tricks of the Trade, 2nd edn., LNCS 7700, pp.
    735–757, 2012.'
  id: totrans-8767
  prefs: []
  type: TYPE_NORMAL
  zh: G. Montavon等（编）：NN：行业技巧，第2版，LNCS 7700，第735–757页，2012。
- en: -c Springer-Verlag Berlin Heidelberg 2012
  id: totrans-8768
  prefs: []
  type: TYPE_NORMAL
  zh: -c 施普林格-维拉格 柏林 海德堡 2012
- en: the following we therefore carefully distinguish between the (real world) control
    problem (which is given) and the learning problem (which we have to design). Of
    course, ideally, when the learning task is solved, the resulting policy should
    fulfill the original controller task. The goal of this paper is to show how we
    can use the degrees of freedom in the modelling of the learning task to successfully
    solve the original control task. Our procedure of setting up a neural reinforcement
    learning system worked well for a large range of real, realistic or benchmark-style
    control applications, e.g. [9, 24, 10, 26, 7, 19, 13, 6].
  id: totrans-8769
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在以下内容中，我们仔细区分（现实世界）控制问题（已给出）和学习问题（需要设计）。当然，理想情况下，当学习任务解决后，所得到的策略应该满足原始控制任务。本文的目标是展示我们如何利用学习任务建模中的自由度，成功解决原始控制任务。我们建立神经强化学习系统的过程在广泛的真实、现实或基准风格的控制应用中表现良好，例如[9,
    24, 10, 26, 7, 19, 13, 6]。
- en: 30.2 The Reinforcement Learning Framework 30.2.1 Learning In Markovian Decision
    Processes
  id: totrans-8770
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.2 强化学习框架 30.2.1 马尔可夫决策过程中的学习
- en: The approach for learning controllers followed here tackles control problems
    as discrete-time Markovian Decision Processes (MDPs). An MDP is described by a
    set S of states, a set A of actions, a stochastic transition function p(*s, a,
    s*)
  id: totrans-8771
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的控制器学习方法将控制问题视为离散时间马尔可夫决策过程（MDPs）。一个MDP由状态集S、动作集A和随机转移函数p(*s, a, s*)描述。
- en: 'describing the (stochastic) system behavior and an immediate reward or cost
    function c : S × A → R. The goal is to find an optimal policy π∗ : S → A, that
    minimizes the expected cumulated costs for each state. In particular, we allow
    S to be continuous, assume A to be finite, and p to be unknown to our learning
    system (model-free approach). Decisions are taken in regular time steps with a
    constant cycle time "t.'
  id: totrans-8772
  prefs: []
  type: TYPE_NORMAL
  zh: '描述（随机）系统行为和即时奖励或成本函数c : S × A → R。目标是找到一个最优策略π∗ : S → A，最小化每个状态的期望累积成本。特别是，我们允许S为连续，假设A为有限，并且p对我们的学习系统是未知的（无模型方法）。决策在常规时间步长内进行，周期时间为"t。'
- en: The underlying learning principle is based on Q-learning [30], a model-free
    variant of the value iteration idea from dynamic programming. The basic idea is
    to iteratively learn a value function, Q, that maps state-action pairs to their
    expected optimal path costs. In Q-learning, the update rule is given by
  id: totrans-8773
  prefs: []
  type: TYPE_NORMAL
  zh: 基础学习原理基于Q学习[30]，这是一种无模型的动态规划值迭代思想的变体。基本思想是迭代学习一个值函数Q，将状态-动作对映射到其期望的最优路径成本。在Q学习中，更新规则为
- en: $$s,a):=(1-a$$
  id: totrans-8774
  prefs: []
  type: TYPE_NORMAL
  zh: $$s,a):=(1-a$$
- en: Qk+1(*s, a*) := (1 − α)Q(*s, a*) + α(c(*s, a*) + γ min b Qk(s, b)).
  id: totrans-8775
  prefs: []
  type: TYPE_NORMAL
  zh: Qk+1(*s, a*) := (1 − α)Q(*s, a*) + α(c(*s, a*) + γ min b Qk(s, b))。
- en: 'Here, s denotes the state where the transition starts, a is the action that
    is applied, and s is the resulting state. α is a learning rate that has to be
    decreased in the course of learning in order to fulfill the conditions of stochastic
    approximation and γ is a discounting factor. It can be shown, that under mild
    assumptions Q-learning converges for finite state and action spaces, as long as
    every state action pair is updated infinitely often (see e.g. [3] ). Then, in
    the limit, the optimal Q-function, Q∗, is reached. The optimal policy π∗ can then
    be derived by greedily evaluating the optimal Q-function:'
  id: totrans-8776
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，s表示转移开始的状态，a是应用的动作，s是结果状态。α是学习率，必须在学习过程中逐渐降低，以满足随机逼近的条件，γ是折扣因子。可以证明，在温和假设下，Q学习对有限状态和动作空间收敛，只要每个状态动作对被无限次更新（参见[3]）。然后，在极限情况下，最优Q函数Q∗得以达到。最优策略π∗可以通过贪婪地评估最优Q函数来推导：
- en: $$\mathbf{a})(Q)$$
  id: totrans-8777
  prefs: []
  type: TYPE_NORMAL
  zh: $$\mathbf{a})(Q)$$
- en: $$-\ \alpha(c(s,$$
  id: totrans-8778
  prefs: []
  type: TYPE_NORMAL
  zh: $$-\ \alpha(c(s,$$
- en: $$\pi^{*}(s)\in\arg\operatorname*{min}_{a\in A}Q^{*}(s,a)$$
  id: totrans-8779
  prefs: []
  type: TYPE_NORMAL
  zh: $$\pi^{*}(s)\in\arg\operatorname*{min}_{a\in A}Q^{*}(s,a)$$
- en: For a detailed introduction to reinforcement learning the reader is referred
    to the excellent textbooks [3, 27].
  id: totrans-8780
  prefs: []
  type: TYPE_NORMAL
  zh: 有关强化学习的详细介绍，读者可以参考优秀的教科书[3, 27]。
- en: 30.2.2 Q-Learning With Function Approximation
  id: totrans-8781
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.2.2 使用函数逼近的Q学习
- en: When dealing with large or even continuous state spaces, a tabular representation
    of the Q-function comes to its limits or is simply infeasible. A standard way
    to tackle this, is the use of function approximation to represent the Q-function.
    We focus on neural networks in the following, but other approximation schemes
    (like e.g. Gaussian processes [4], CMACs [28, 29], . . . ) are being used as well.
  id: totrans-8782
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大型或甚至连续状态空间时，Q函数的表格表示法达到了极限或根本不可行。解决这个问题的标准方法是使用函数逼近来表示Q函数。我们接下来将重点关注神经网络，但也使用其他逼近方案（例如高斯过程[4]，CMACs[28,
    29]，等）。
- en: 'One big advantage of using neural networks is their capability to generalize
    to unseen situations - a fact particularly useful in large or continuous state
    spaces, where one can not expect to experience all situations during training.
    However, this positive feature has also a negative impact: when the standard Q-learning
    rule is applied to a certain state transition, it will also influence the value
    at other inputs in an unforeseeable manner.'
  id: totrans-8783
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络的一个重大优势是它们能够推广到未见过的情况——这一事实在大型或连续状态空间中特别有用，因为在训练期间不能期望经历所有情况。然而，这一积极特性也有负面影响：当标准Q学习规则应用于某个状态过渡时，也会以不可预见的方式影响其他输入的值。
- en: To work against this effect, we developed a neural Q-learning framework, that
    is based on updating batches of transitions instead of single transition updates
    as in the original Q-learning rule. This approach has turned out to be an instance
    of the Fitted Q Iteration family of algorithms [5], and was named 'Neural Fitted
    Q Iteration (NFQ)' accordingly [23].
  id: totrans-8784
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对抗这种影响，我们开发了一种基于更新过渡批次的神经Q学习框架，而不是原始Q学习规则中的单个过渡更新。这种方法被证明是拟合Q迭代算法家族[5]的一个实例，并因此被命名为“神经拟合Q迭代（NFQ）”[23]。
- en: 'The basic idea underlying NFQ is simple but decisive: the update of the value
    function is performed considering the complete set of transition experiences instead
    of single transitions. Transitions are collected in triples of the form (*s, a,
    s*)'
  id: totrans-8785
  prefs: []
  type: TYPE_NORMAL
  zh: NFQ的基本思想简单但关键：价值函数的更新是考虑完整的过渡经验集，而不是单个过渡。过渡以(*s, a, s*)的三元组形式收集。
- en: by interacting with the environment. Here, s is the original state, a is the
    chosen action and s is the resulting state. The set of experiences is called the
    sample set D.
  id: totrans-8786
  prefs: []
  type: TYPE_NORMAL
  zh: 通过与环境互动。这里，s是原始状态，a是选择的动作，s是结果状态。经验集称为样本集D。
- en: 'The algorithm is displayed in figure 30.1. It consists of two major steps:
    The generation of the training set P and the training of these patterns within
    a multilayer perceptron. The input part of each training pattern consists of the
    state sl and action al of training experience l. The target value of each pattern
    is computed as suggested by the Q-learning rule: it is the sum of the transition
    costs c(sl, al) plus the expected minimal path costs for the successor state sl.'
  id: totrans-8787
  prefs: []
  type: TYPE_NORMAL
  zh: 算法如图30.1所示。它由两个主要步骤组成：生成训练集P和在多层感知器中训练这些模式。每个训练模式的输入部分由训练经验l的状态sl和动作al组成。每个模式的目标值根据Q学习规则计算：它是过渡成本c(sl,
    al)加上后继状态sl的期望最小路径成本的总和。
- en: The latter is computed on the basis of the current estimate of the Q−function,
    Qk.
  id: totrans-8788
  prefs: []
  type: TYPE_NORMAL
  zh: 后者是基于当前对Q函数的估计Qk计算的。
- en: Since at this point, training the Q-function can be done as batch learning of
    a fixed pattern set, we can use more advanced supervised learning techniques,
    that converge more quickly and more reliably than ordinary gradient descent techniques.
    In particular, NFQ uses the Rprop algorithm for fast supervised learning.
  id: totrans-8789
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此时，Q函数的训练可以作为固定模式集的批量学习进行，我们可以使用更先进的监督学习技术，这些技术的收敛速度比普通的梯度下降技术更快、更可靠。特别地，NFQ使用Rprop算法进行快速监督学习。
- en: Rprop adapts its search step size based on the signs of the partial derivatives
    and has proven to be very fast and yet robust with respect to the choice of its
    parameters. For a detailed description of Rprop see [17]. The training of the
    pattern set is executed either for a predefined number of epochs (=complete sweeps
    through the pattern set), or until the error is below a certain predefined limit.
    Although simple and straight-forward, training for a fixed number of epochs works
    well and therefore is our standard choice. For a more detailed discussion about
    NFQ,
  id: totrans-8790
  prefs: []
  type: TYPE_NORMAL
  zh: Rprop根据偏导数的符号调整搜索步长，并已证明在参数选择上非常快速且稳健。有关Rprop的详细描述，请参见[17]。模式集的训练要么在预定义的周期数内执行（即对模式集的完整遍历），要么直到误差低于某个预定义限制。尽管简单明了，固定周期数的训练效果良好，因此是我们的标准选择。关于NFQ的更详细讨论，
- en: please refer to [23].
  id: totrans-8791
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见[23]。
- en: NFQ_main() {
  id: totrans-8792
  prefs: []
  type: TYPE_NORMAL
  zh: NFQ_main() {
- en: 'input: a set of transition samples D; output: Q-value function QN'
  id: totrans-8793
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：一组过渡样本D；输出：Q值函数QN
- en: 'k=0 init_MLP() → Q0; Do {generate_pattern_set P = {(inputl,targetl), l = 1*,...,*
    \#D} where:'
  id: totrans-8794
  prefs: []
  type: TYPE_NORMAL
  zh: k=0 init_MLP() → Q0; 循环 {生成模式集 P = {(inputl,targetl), l = 1*,...,* \#D} 其中：
- en: inputl = sl, al, targetl = c(sl, al) + γ minb Qk(sl, b)
  id: totrans-8795
  prefs: []
  type: TYPE_NORMAL
  zh: inputl = sl, al, targetl = c(sl, al) + γ minb Qk(sl, b)
- en: Rprop_training(P) → Qk+1 k:= k+1
  id: totrans-8796
  prefs: []
  type: TYPE_NORMAL
  zh: Rprop_training(P) → Qk+1 k:= k+1
- en: '} While (k<kmax)'
  id: totrans-8797
  prefs: []
  type: TYPE_NORMAL
  zh: '} 当 (k<kmax) 时循环'
- en: Fig. 30.1. Main loop of NFQ . k counts the number of iterations, kmax denotes
    the maximum number of iterations. init_MLP() returns a multilayer perceptron with
    randomly initialized weights. Rprop_*training*(P) takes pattern set P and returns
    a mulitlayer perceptron that has been trained on P using Rprop as the supervised
    training method.
  id: totrans-8798
  prefs: []
  type: TYPE_NORMAL
  zh: 图30.1 NFQ的主循环。k计数迭代次数，kmax表示最大迭代次数。init_MLP()返回一个权重随机初始化的多层感知器。Rprop_*training*(P)接收模式集P并返回一个已使用Rprop作为监督训练方法在P上训练的多层感知器。
- en: 30.3 Characteristics Of The Control Task
  id: totrans-8799
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.3 控制任务的特点
- en: In this work, we consider control scenarios of the following type. The controller
    has to control a technical system or process such that finally a desired target
    situation is achieved. The current situation of the system is measured by sensors.
    Thus, the control goal is usually defined by making one or more sensor values
    equal to externally given reference values within some tolerance bounds. To do
    so, the controller has to apply an appropriate sequence of control actions. The
    control system is realized as a closed-loop system, that acts at discrete time
    steps. At every time-step, the sensor values are measured, and the controller
    computes a control action, which is then applied to the process.
  id: totrans-8800
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们考虑以下类型的控制场景。控制器必须控制一个技术系统或过程，以最终实现期望的目标状态。系统的当前状态通过传感器进行测量。因此，控制目标通常是通过使一个或多个传感器值在某些容差范围内等于外部给定的参考值来定义的。为此，控制器必须应用适当的控制动作序列。控制系统作为一个闭环系统实现，在离散时间步长上运行。在每个时间步，传感器值被测量，控制器计算出一个控制动作，然后应用于过程。
- en: Different types of control tasks exist within this framework. An important characterization
    is if the control task has a defined termination or not. In the first case, control
    terminates immediately, once a certain goal criterion has been achieved. A typical
    example would be to drive a mobile robot to a certain target location. The task
    is terminated, once the target location is reached.
  id: totrans-8801
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个框架内存在不同类型的控制任务。一个重要的特征是控制任务是否有明确的终止。第一种情况下，一旦达成某个目标标准，控制会立即终止。一个典型的例子是将移动机器人驱动到某个目标位置。当目标位置到达时，任务就终止了。
- en: 'The second case is more challenging: the control task does not end, if a target
    condition is reached once. Instead, the controller has to actively keep the system
    in a set of goal states, that all fulfill a certain success criterion. Typically,
    this criterion is given by the sensor values being equal to their target reference
    values within a small tolerance band. This is a very common scenario in the control
    of technical systems. A typical example would be to achieve and hold a certain
    temperature in a room. From a control perspective, this latter scenario is much
    more challenging (since it contains the first one as a special case).'
  id: totrans-8802
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种情况更具挑战性：如果达到目标条件，控制任务并不会结束。相反，控制器必须主动保持系统在一组目标状态中，这些状态都满足某个成功标准。通常，这个标准是传感器值在一个小的容差带内等于其目标参考值。这在技术系统控制中是非常常见的场景。一个典型的例子是实现并保持房间内的特定温度。从控制的角度来看，这后一种情况更具挑战性（因为它将第一种情况作为特殊情况）。
- en: 30.4 Modeling The Learning Task
  id: totrans-8803
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.4 建模学习任务
- en: This section discusses how to model a given (real-world) control task appropriately
    within the neural reinforcement learning framework. We discuss alternatives and
    'tricks' while always trying to stay as close as possible to the framework proposed
    by the theory of dynamic programming.
  id: totrans-8804
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论如何在神经强化学习框架内适当地建模给定的（现实世界）控制任务。我们讨论替代方案和“技巧”，始终尽量贴近动态编程理论提出的框架。
- en: 30.4.1 State Information
  id: totrans-8805
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.4.1 状态信息
- en: 'The underlying reinforcement learning framework crucially depends on the assumption
    of the Markov property of state transitions: the successor state is a'
  id: totrans-8806
  prefs: []
  type: TYPE_NORMAL
  zh: 基础强化学习框架关键依赖于状态转移的马尔可夫特性假设：后继状态是
- en: (probabilistic) function of the current state and action. As a consequence,
    state information provided to the learning system must be 'rich' enough - 'rich'
    in the sense that the observed state transition does not depend on additional
    historical information. In a real application, we can not necessarily expect to
    get the complete state information out of the values provided by the sensors.
    In classical control theory, the concept of an observer is known to deduce state
    information out of the sequence of observed sensor information. However, this
    requires the availability of a system model, which we assume not to have in our
    learning framework. A standard way to tackle this problem is to provide historical
    sensor and action information from previous time steps. Since we are learning
    anyhow, we do not rely on a particular semantic interpretability of the state
    information.
  id: totrans-8807
  prefs: []
  type: TYPE_NORMAL
  zh: （概率）当前状态和动作的函数。因此，提供给学习系统的状态信息必须“丰富”——在观察到的状态转移不依赖于额外历史信息的意义上“丰富”。在实际应用中，我们不能指望从传感器提供的值中获得完整的状态信息。在经典控制理论中，观察者的概念已知可以从观察到的传感器信息序列中推导状态信息。然而，这需要系统模型的可用性，我们假设在学习框架中没有这种模型。解决此问题的标准方法是提供来自先前时间步的历史传感器和动作信息。由于我们无论如何都在学习，因此我们不依赖于状态信息的特定语义可解释性。
- en: This allows for example to provide more information than necessary or redundant
    information, to be on the safe side. As a tradeoff, state information should be
    kept as small and concise as possible to support good generalization, which will
    generally lead to faster learning and better control performance. In technical
    dynamical systems, we often use temporal differences of sensor values as an approximation
    to physically meaningful values like velocity or acceleration.
  id: totrans-8808
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许例如提供超过必要的信息或冗余信息，以确保安全。作为权衡，状态信息应尽可能小且简洁，以支持良好的泛化，这通常会导致更快的学习和更好的控制性能。在技术动态系统中，我们常用传感器值的时间差作为物理有意义值（如速度或加速度）的近似值。
- en: 'Like in supervised learning, using meaningful features instead of raw sensor
    values to enforce generalization is often helpful. However, also like in supervised
    learning, the design of good features typically requires deep insight into the
    application (here: knowledge about system behavior which in the strong sense we
    assume not to have). A current research direction is to autonomously learn meaningful
    state representations directly out of high-dimensional raw sensor data (like e.g.
    cameras) [15, 16, 25, 2].'
  id: totrans-8809
  prefs: []
  type: TYPE_NORMAL
  zh: 像在监督学习中一样，使用有意义的特征而不是原始传感器值来增强泛化通常是有帮助的。然而，同样像在监督学习中，良好特征的设计通常需要对应用有深入的理解（在此：关于系统行为的知识，我们在强意义上假设没有）。当前的研究方向是从高维原始传感器数据（如摄像头）中自主学习有意义的状态表示[15,
    16, 25, 2]。
- en: 'Summary:'
  id: totrans-8810
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要：
- en: '- state information must be designed out of sensor information and must be
    rich enough to support Markov property'
  id: totrans-8811
  prefs: []
  type: TYPE_NORMAL
  zh: '- 状态信息必须由传感器信息设计，并且必须足够丰富以支持马尔可夫特性。'
- en: '- redundant information is not a problem, but it is preferable to keep the
    input dimensionality as low as possible'
  id: totrans-8812
  prefs: []
  type: TYPE_NORMAL
  zh: '- 冗余信息不是问题，但最好保持输入维度尽可能低。'
- en: '- state representation can be transformed into features to enforce generalization'
  id: totrans-8813
  prefs: []
  type: TYPE_NORMAL
  zh: '- 状态表示可以转化为特征以增强泛化。'
- en: '- state information does not necessarily have a human understandable meaning'
  id: totrans-8814
  prefs: []
  type: TYPE_NORMAL
  zh: '- 状态信息不一定具有人类可理解的含义。'
- en: 30.4.2 Actions
  id: totrans-8815
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.4.2 动作
- en: The original control task often allows the application of (quasi) continuous
    control values, typically in a certain range between a minimum and a maximum value.
    While in principle methods exist to learn continuous control actions (e.g. [11,
    22]), we will here focus on providing a discrete set of control actions to our
    learning system. This is the most common framework in reinforcement learning and
    corresponds to the framework as presented in section 30.2.
  id: totrans-8816
  prefs: []
  type: TYPE_NORMAL
  zh: 原始控制任务通常允许应用（准）连续控制值，通常在最小值和最大值之间的某个范围内。虽然原则上存在学习连续控制动作的方法（例如[11, 22]），但我们将在这里重点提供一组离散控制动作给我们的学习系统。这是强化学习中最常见的框架，对应于第30.2节中提出的框架。
- en: This means, for setting up the learning system, one has to explicitly choose
    a discrete set of actions within the range of potential control signals. One potential
    choice is a two action set, consisting out of the minimum and maximum control
    action ('bang-bang'-control). In classical control theory, such a two-value control
    policy is the basis of time-optimal controllers. Of course, oscillating back and
    forth between two extreme control signals, e.g. to keep the system close to a
    desired sensor output, often is not acceptable when it comes to the control of
    real hardware. Therefore it is often advisable, to add additional actions to the
    learning set, e.g. a neutral action that does not put additional energy into the
    system.
  id: totrans-8817
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，在建立学习系统时，必须明确选择一个在潜在控制信号范围内的离散动作集。一个可能的选择是一个由最小和最大控制动作组成的双动作集（“开关”控制）。在经典控制理论中，这样的双值控制策略是时间最优控制器的基础。当然，在控制真实硬件时，在两个极端控制信号之间来回振荡，诸如保持系统接近期望传感器输出，往往是不可接受的。因此，通常建议向学习集添加额外的动作，例如不向系统施加额外能量的中性动作。
- en: 'The search space of available policies increases exponentially with the number
    of actions. Therefore, from the perspective of learning time, one should try to
    keep the number of available actions limited. However, there is of course a tradeoff:
    a smaller number of actions leads to a coarser control behavior and the learning
    controller might not be able to fulfill the required accuracy in control.'
  id: totrans-8818
  prefs: []
  type: TYPE_NORMAL
  zh: 可用策略的搜索空间随着动作数量的增加而指数增长。因此，从学习时间的角度来看，应尽量限制可用动作的数量。然而，当然存在权衡：较少的动作会导致较粗糙的控制行为，学习控制器可能无法满足所需的控制精度。
- en: 'There is a close interplay with the length of the control interval "t: a larger
    control interval might require a larger action set to achieve the same level of
    controllability and vice-versa: the smaller the control interval, the coarser
    the action set may be, since more frequent interaction (and thus correction) is
    possible. The dynamic output element framework exploits this close relationship
    between temporal and structural aspects of the action set to enable more flexible
    control policies [22].'
  id: totrans-8819
  prefs: []
  type: TYPE_NORMAL
  zh: 控制间隔“t”的长度与此密切相关：较大的控制间隔可能需要更大的动作集以实现相同的可控性水平，反之亦然：控制间隔越小，动作集可能越粗糙，因为更频繁的交互（以及纠正）是可能的。动态输出元素框架利用了动作集的时间和结构方面之间的密切关系，以实现更灵活的控制策略[22]。
- en: Trivially, a least requirement to the action set is that a policy must exist,
    that transfers the system to the goal state and - in case of the non-terminal
    state framework (see below, section 30.4.4) - keeps it within the goal area.
  id: totrans-8820
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，对动作集的最低要求是必须存在一个策略，该策略将系统转移到目标状态，并在非终端状态框架下（见下文，第30.4.4节）保持其在目标区域内。
- en: 'Summary:'
  id: totrans-8821
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要：
- en: '- action set should be kept small to allow fast learning - tradeoff: more actions
    can enhance quality or accuracy of control policy'
  id: totrans-8822
  prefs: []
  type: TYPE_NORMAL
  zh: '- 动作集应保持较小，以便快速学习 - 权衡：更多动作可以增强控制策略的质量或准确性'
- en: '- actions must allow to reach goal states and to keep the system within goal
    area in the non-terminal goal state setting.'
  id: totrans-8823
  prefs: []
  type: TYPE_NORMAL
  zh: 动作必须允许达到目标状态，并在非终端目标状态设置中保持系统在目标区域内。
- en: 30.4.3 Choice Of Control Interval T
  id: totrans-8824
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.4.3 控制间隔T的选择
- en: The control interval "t denotes the time between two control interventions of
    the learning system. In classical control theory, controller design is often assuming
    that interaction happens in continuous time (like e.g. in classical PID-control).
  id: totrans-8825
  prefs: []
  type: TYPE_NORMAL
  zh: 控制间隔“t”表示学习系统两次控制干预之间的时间。在经典控制理论中，控制器设计通常假设交互发生在连续时间内（如经典PID控制中）。
- en: Therefore one aims to make the control interval "t as small as possible to approximate
    the assumed continuous time scenario - otherwise, the controller will not work
    as expected. This is no necessary requirement in the learning framework proposed
    here. Instead, the controller learns to adapt its behavior to the control interval
    given - therefore also larger control intervals can be managed. This additional
    degree of freedom is a big advantage, since for example a slower controller might
    be realized on less expensive hardware.
  id: totrans-8826
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，目标是使控制间隔“t尽可能小，以逼近假设的连续时间场景——否则，控制器将无法按预期工作。这在这里提出的学习框架中并不是必要的要求。相反，控制器学习根据给定的控制间隔调整其行为——因此也可以管理较大的控制间隔。这一额外的自由度是一个巨大优势，因为例如一个较慢的控制器可能在较便宜的硬件上实现。
- en: As a general tradeoff, learning gets easier, if the control interval is larger,
    since there are less decision points. On the other hand, the smaller the control
    interval, the more accurately the system can be controlled. If absolutely no prior
    knowledge of the system behavior is available, then "t must be chosen empirically.
    A potential strategy to determine "t is to start with a relatively large time
    step, which helps to learn faster, and then to refine it until the desired accuracy
    is achieved.
  id: totrans-8827
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，如果控制间隔较大，学习会变得更容易，因为决策点更少。另一方面，控制间隔越小，系统的控制精度越高。如果对系统行为完全没有先验知识，则“t必须通过经验选择。确定“t的潜在策略是从相对较大的时间步长开始，这有助于更快学习，然后细化它，直到达到所需的精度。
- en: As already discussed in section 30.4.2 there is a close interplay between the
    available action set, the control interval "t, and the potential accuracy in control.
  id: totrans-8828
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在第30.4.2节中讨论的，可用动作集、控制间隔“t与控制的潜在精度之间存在密切的相互作用。
- en: 'Summary:'
  id: totrans-8829
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结：
- en: '- the larger the control interval "t, the fewer decisions have to be taken
    to reach the goal, therefore learning is generally faster'
  id: totrans-8830
  prefs: []
  type: TYPE_NORMAL
  zh: '- 控制间隔“t越大，达到目标所需做出的决策就越少，因此学习通常更快。'
- en: '- tradeoff: a smaller control interval "t potentially allows more accurate
    control and better control quality'
  id: totrans-8831
  prefs: []
  type: TYPE_NORMAL
  zh: '- 权衡：较小的控制间隔“t可能允许更准确的控制和更好的控制质量。'
- en: 30.4.4 The Terminal Goal State And The Non-Terminal Goal State Setting
  id: totrans-8832
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.4.4 终端目标状态与非终端目标状态设置
- en: As already discussed in section 30.3, control tasks can either terminate once
    a goal criterion is met, or (virtually) continue forever. In the latter case,
    the control goal is not only to reach a state fulfilling certain success criteria,
    but to actively keep the system in a set of goal states, that all fulfill these
    success criteria. A typical success criterion for a state could be, for example,
    that all sensor values correspond to their target values within some tolerance
    bound. In the following we discuss, how these two control scenarios can be modeled
    in our learning framework.
  id: totrans-8833
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在第30.3节中讨论的，控制任务可以在满足目标标准后终止，或者（实际上）无限期继续。在后者的情况下，控制目标不仅是达到满足某些成功标准的状态，而是积极地将系统保持在一组满足这些成功标准的目标状态中。一个状态的典型成功标准可以是，所有传感器值在某个容差范围内对应其目标值。接下来我们讨论如何在我们的学习框架中建模这两种控制场景。
- en: 'For control tasks, mainly two learning scenarios are most appropriate: the
    terminal goal state and the non-terminal goal state framework. From the learning
    perspective, the terminal goal state setting is the simpler one. The task is to
    transfer the controlled system from an initial state to a terminal goal state
    by an appropriate sequence of actions. Once the goal state is reached, the episode
    is stopped, and the target Q-value of the last state action pair is set to the
    transition costs plus final costs of the terminal state.'
  id: totrans-8834
  prefs: []
  type: TYPE_NORMAL
  zh: 对于控制任务，主要有两种学习场景最为适合：终端目标状态和非终端目标状态框架。从学习的角度来看，终端目标状态设置是更简单的。任务是通过适当的动作序列将受控系统从初始状态转移到终端目标状态。一旦达到目标状态，剧集将停止，最后状态动作对的目标Q值被设置为转移成本加上终端状态的最终成本。
- en: This can be implemented by computing the target Q values as follows
  id: totrans-8835
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过如下计算目标Q值来实现。
- en: $$Q(s,u)=\left\{\begin{array}{l l}{{c(s,u)+\gamma\cdot t e r m i n a l\_c o
    s t s(s^{\prime})}}&{{,\quad\mathrm{if}\ s^{\prime}\in X^{+}}}\\ {{c(s,u)+\gamma\cdot\operatorname*{min}_{b}Q(s^{\prime},b)}}&{{,\quad\mathrm{else}}}\end{array}\right.$$
  id: totrans-8836
  prefs: []
  type: TYPE_NORMAL
  zh: $$Q(s,u)=\left\{\begin{array}{l l}{{c(s,u)+\gamma\cdot t e r m i n a l\_c o
    s t s(s^{\prime})}}&{{,\quad\mathrm{if}\ s^{\prime}\in X^{+}}}\\ {{c(s,u)+\gamma\cdot\operatorname*{min}_{b}Q(s^{\prime},b)}}&{{,\quad\mathrm{else}}}\end{array}\right.$$
- en: $$(30.1)$$
  id: totrans-8837
  prefs: []
  type: TYPE_NORMAL
  zh: $$(30.1)$$
- en: where c(*s, u*) denotes the immediate costs of a transition (see below). Here,
    X+ denotes the set of all terminal goal states, that fulfill the success criteria
    and by being reached, terminate the control task. For each terminal goal state,
    terminal_*costs*() assigns the corresponding terminal costs.
  id: totrans-8838
  prefs: []
  type: TYPE_NORMAL
  zh: 其中c(*s, u*)表示转移的即时成本（见下文）。这里，X+表示所有满足成功标准的终端目标状态的集合，达到这些状态将终止控制任务。对于每个终端目标状态，terminal_*costs*()分配相应的终端成本。
- en: On the other side, the non terminal goal state framework is particularly tailored
    to control tasks, where the controller also has to actively keep the system in
    a set of goal states. Here, X+ again denotes the set of all goal states, that
    fulfill the success criteria, but in contrast to the above framework, the control
    task is not terminated, when one of those states is reached.
  id: totrans-8839
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，非终端目标状态框架特别适用于控制任务，在这些任务中，控制器还必须积极保持系统处于一组目标状态中。在这里，X+再次表示所有满足成功标准的目标状态的集合，但与上述框架不同，当达到这些状态中的一个时，控制任务不会终止。
- en: This results in the following rule for the update of the Q-values
  id: totrans-8840
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下Q值更新规则
- en: $$Q(s,u)=\left\{\begin{array}{c c}{{0+\gamma\,\cdot\operatorname*{min}_{b}Q(s^{\prime},b)}}&{{,\quad\mathrm{if}\
    s^{\prime}\in X^{+}}}\\ {{c(s,u)+\gamma\,\cdot\operatorname*{min}_{b}Q(s^{\prime},b)}}&{{,\quad\mathrm{else}}}\end{array}\right.$$
  id: totrans-8841
  prefs: []
  type: TYPE_NORMAL
  zh: $$Q(s,u)=\left\{\begin{array}{c c}{{0+\gamma\,\cdot\operatorname*{min}_{b}Q(s^{\prime},b)}}&{{,\quad\mathrm{if}\
    s^{\prime}\in X^{+}}}\\ {{c(s,u)+\gamma\,\cdot\operatorname*{min}_{b}Q(s^{\prime},b)}}&{{,\quad\mathrm{else}}}\end{array}\right.$$
- en: $$(30.2)$$
  id: totrans-8842
  prefs: []
  type: TYPE_NORMAL
  zh: $$(30.2)$$
- en: where as before c(*s, u*) denotes the immediate costs of a transition outside
    the goal region (see below).
  id: totrans-8843
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，c(*s, u*)表示目标区域外转移的即时成本（见下文）。
- en: 'This seemingly slight modification has two important consequences: the episode
    is not stopped, once a state in the goal area is reached, and secondly no'
  id: totrans-8844
  prefs: []
  type: TYPE_NORMAL
  zh: 这个看似轻微的修改有两个重要后果：一旦到达目标区域的某个状态，事件不会停止，其次没有
- en: '''grounding'' of the Q values to a terminal value occurs. This has a nasty
    effect to the value function, when a multilayer perceptron is used to approximate
    it. Due to interpolation effects and the lack of grounding, the value function
    tends to steadily increase. We will discuss this problem in further detail in
    section 30.5.3.'
  id: totrans-8845
  prefs: []
  type: TYPE_NORMAL
  zh: Q值的“基准化”到终端值会发生。当使用多层感知器近似时，这对价值函数产生不良影响。由于插值效应和缺乏基准，价值函数往往会持续增加。我们将在30.5.3节中更详细地讨论这个问题。
- en: Since learning in the terminal goal state framework is usually easier, it sometimes
    makes sense to model a per-se non-terminal control problem as a terminal state
    learning problem. The general idea is to consider goal states with a low change-rate
    as pseudo terminal states. Then, the terminal goal state framework according to
    equation 30.1 can be applied. During learning, the task is always stopped, when
    one of these pseudo terminal states is reached. In the application phase, the
    policy learned by this procedure is then applied without stopping.
  id: totrans-8846
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在终端目标状态框架下学习通常更容易，有时将一个本质上非终端的控制问题建模为终端状态学习问题是有意义的。一般的想法是将变化率低的目标状态视为伪终端状态。然后，可以根据方程30.1应用终端目标状态框架。在学习过程中，一旦达到这些伪终端状态，任务就会停止。在应用阶段，随后将应用通过这一过程学到的策略，而不进行停止。
- en: The idea behind this is, that whenever the system drifts away from its goal
    region during application, then the controller immediately brings it back to its
    goal region.
  id: totrans-8847
  prefs: []
  type: TYPE_NORMAL
  zh: 其背后的想法是，无论何时系统在应用过程中偏离其目标区域，控制器都会立即将其带回目标区域。
- en: However, this method is only an approximation to the actually desired behavior.
    It moreover requires to heuristically define what a 'low change-rate' means within
    the particular setting. So for non terminal control tasks we recommend to use
    the non terminal goal state learning framework whenever possible. We only wanted
    to mention this possibility, because sometimes a control task might be too difficult
    to learn within the non-terminal goal state framework. Then, an approximation
    by a terminal goal state problem might constitute a practical way to make it work.
  id: totrans-8848
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法仅仅是对实际期望行为的近似。此外，它还需要根据特定设置启发式地定义“低变化率”的含义。因此，对于非终端控制任务，我们建议尽可能使用非终端目标状态学习框架。我们之所以提到这一可能性，是因为有时控制任务可能过于困难，以至于无法在非终端目标状态框架内学习。此时，通过终端目标状态问题的近似可能构成一种有效的解决方案。
- en: 'Summary:'
  id: totrans-8849
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结：
- en: '- in general, terminal goal states make learning easier'
  id: totrans-8850
  prefs: []
  type: TYPE_NORMAL
  zh: '- 一般来说，终端目标状态使学习变得更容易'
- en: '- for control applications, often the nonterminal goal state framework is appropriate,
    since finding a control policy that also stabilizes the system within the target
    region is required.'
  id: totrans-8851
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对于控制应用，通常非终端目标状态框架是合适的，因为需要找到一个控制策略，使得系统能够在目标区域内稳定。'
- en: 30.4.5 Choice of X+
  id: totrans-8852
  prefs: []
  type: TYPE_NORMAL
  zh: 30.4.5 X+ 的选择
- en: The set X+ comprises all states, which fulfill the goal criteria as defined
    by the control tasks. One typical way to define X+ is to denote ranges for the
    values of each state variable. For the state variables that we want to control,
    we typically define a range around their specified target value, i.e. target value
    ±δ, where δ > 0 denotes the allowed tolerance. For other state variables, the
    allowed ranges might be infinitely large, denoting that we do not care what value
    they have for judging membership to X+.
  id: totrans-8853
  prefs: []
  type: TYPE_NORMAL
  zh: 集合 X+ 包括所有符合控制任务定义的目标标准的状态。定义 X+ 的一种典型方法是为每个状态变量的值指定范围。对于我们希望控制的状态变量，我们通常定义一个围绕其指定目标值的范围，即目标值
    ±δ，其中 δ > 0 表示允许的公差。对于其他状态变量，允许的范围可能是无限大的，表示我们不关心它们在判断是否属于 X+ 时的具体值。
- en: 'Again, there is a tradeoff: the smaller we choose X+, the more accurate the
    successfully learned final controller will be. The larger X+, the easier it will
    be to learn, but we also have to accept less accurate controllers as a result.'
  id: totrans-8854
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提到，有一个权衡：我们选择的 X+ 越小，最终成功学习到的控制器将越准确。X+ 越大，学习就越容易，但我们也必须接受相应的控制器准确性降低。
- en: An important requirement from the perspective of the learning framework is that
    X+ is large enough, so that a policy exists, that X+ can be reached from every
    starting state. Therefore, the choice of X+ is highly related to the choice of
    the action set and the choice of the control interval "t.
  id: totrans-8855
  prefs: []
  type: TYPE_NORMAL
  zh: 从学习框架的角度来看，一个重要要求是 X+ 足够大，以便存在一个策略使得可以从每个起始状态到达 X+。因此，X+ 的选择与动作集的选择和控制时间间隔 "t
    的选择高度相关。
- en: In the undiscounted (γ = 1) nonterminal goal state case, an additional requirement
    applies for X+. It must be chosen such that a policy exists, that keeps the system
    permanently within X+. This policy need not to be known in advance. Again, this
    requirement implies the interplay between the choice of X+, the control interval
    "t and the available action set.
  id: totrans-8856
  prefs: []
  type: TYPE_NORMAL
  zh: 在无折现（γ = 1）的非终端目标状态情况下，对 X+ 有一个额外要求。必须选择一个策略，使得系统能够永久保持在 X+ 内。该策略不必事先已知。再次强调，这一要求意味着
    X+ 的选择、控制时间间隔 "t 和可用动作集之间的相互作用。
- en: 'Summary:'
  id: totrans-8857
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：
- en: '- the larger X+, the easier it is to learn'
  id: totrans-8858
  prefs: []
  type: TYPE_NORMAL
  zh: '- X+ 越大，学习就越容易。'
- en: '- the smaller X+, the more accurate the learned controller will be 30.4.6 Choice
    of X−'
  id: totrans-8859
  prefs: []
  type: TYPE_NORMAL
  zh: '- X+ 越小，学习到的控制器将越准确。30.4.6 X− 的选择'
- en: In many control problems, constraints on the state variables exist, that must
    not be violated by a successful control policy. The definition of the set of undesired
    states, X− constitutes a way to model this requirement within the proposed learning
    framework. In a typical setting, a state is within X− whenever a constraint of
    the original control problem is violated. Whenever a state within X−
  id: totrans-8860
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多控制问题中，状态变量存在约束，成功的控制策略必须遵循这些约束。定义不期望状态的集合 X− 是在所提出的学习框架内建模这一要求的一种方式。在典型设置中，当原始控制问题的约束被违反时，状态就处于
    X− 内。
- en: is encountered, the control episode is stopped. Below, we show the resulting
    computation for the target of the Q-value as an extension of the equation for
    the non-terminal goal state framework (equation 30.2). The application within
    the terminal goal state framework is straightforward.
  id: totrans-8861
  prefs: []
  type: TYPE_NORMAL
  zh: 遇到时，控制阶段将停止。下面，我们展示了 Q 值目标的计算结果，作为非终端目标状态框架方程（方程 30.2）的扩展。在终端目标状态框架中的应用非常直接。
- en: $$Q(s,u)=\left\{\begin{array}{ccc}0+\gamma\,\cdot\min_{b}Q(s^{\prime},b)&,&\mbox{if$s^{\prime}\in
    X^{+}$}\\ c(s,u)+\gamma\,\cdot terminal\_cost(s^{\prime})&,&\mbox{if$s^{\prime}\in
    X^{-}$}\\ c(s,u)+\gamma\,\cdot\min_{b}Q(s^{\prime},b)&,&\mbox{else}\end{array}\right.\tag{30.3}$$
  id: totrans-8862
  prefs: []
  type: TYPE_NORMAL
  zh: $$Q(s,u)=\left\{\begin{array}{ccc}0+\gamma\,\cdot\min_{b}Q(s^{\prime},b)&,&\mbox{如果
    $s^{\prime}\in X^{+}$}\\ c(s,u)+\gamma\,\cdot terminal\_cost(s^{\prime})&,&\mbox{如果
    $s^{\prime}\in X^{-}$}\\ c(s,u)+\gamma\,\cdot\min_{b}Q(s^{\prime},b)&,&\mbox{其他情况}\end{array}\right.\tag{30.3}$$
- en: The terminal costs for a state within X− should ideally be larger than the path
    costs for any successful policy. When using a multilayer perceptron with a sigmoid
    output function, we typically use a value close to 1 as terminal costs of a state
    within X−.
  id: totrans-8863
  prefs: []
  type: TYPE_NORMAL
  zh: 在 X− 内的状态的终端成本理想情况下应大于任何成功策略的路径成本。当使用具有 sigmoid 输出函数的多层感知器时，我们通常使用接近 1 的值作为
    X− 内状态的终端成本。
- en: 'Summary:'
  id: totrans-8864
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：
- en: '- the learning framework allows the modeling of hard constraints on state variables'
  id: totrans-8865
  prefs: []
  type: TYPE_NORMAL
  zh: '- 学习框架允许对状态变量施加严格的约束'
- en: 30.4.7 Choice Of Immediate And Final Costs
  id: totrans-8866
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.4.7 即时成本和最终成本的选择
- en: The choice of the immediate cost function c(*s, u*) determines the course of
    the control trajectory until the target region is reached. It is not uncommon
    in reinforcement learning to make c(*s, u*) a function of the distance to the
    target region. This has the advantage, that the immediate costs already contain
    a local hint to the goal, which may help learning considerably. From the perspective
    of the control task, however, one has to keep in mind, that the final controller
    optimizes the path costs to the goal. Optimizing the integrated distances to the
    goal might not always be the ideal realization of what is actually intended
  id: totrans-8867
  prefs: []
  type: TYPE_NORMAL
  zh: 即时成本函数c(*s, u*)的选择决定了控制轨迹的走向，直到达到目标区域。在强化学习中，将c(*s, u*)设置为与目标区域距离的函数并不罕见。这具有优势，即即时成本已经包含了指向目标的局部提示，这可能大大帮助学习。然而，从控制任务的角度来看，必须记住，最终控制器优化的是通往目标的路径成本。优化与目标的综合距离可能并不总是理想实现实际意图的方式。
- en: (imagine a situation, where a policy first makes a large error but then reaches
    the goal in a few time steps, instead of a policy that only makes small errors
    but for a long period of time). The situation gets even more difficult, if one
    has to design an immediate cost function that trades off between two or more sensor
    values, that all have to finally achieve a certain target value.
  id: totrans-8868
  prefs: []
  type: TYPE_NORMAL
  zh: （想象一个情况，其中一个策略首先犯了一个大错误，但随后在几个时间步内达到了目标，而不是一个只犯小错误但持续时间较长的策略）。如果需要设计一个在多个传感器值之间进行权衡的即时成本函数，这个情况就变得更加困难，这些传感器值都必须最终实现某个目标值。
- en: 'We therefore prefer a cost formulation, that has the advantage of a very simple
    and thus broadly applicable cost function:'
  id: totrans-8869
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们更倾向于一种成本表述，这种表述的优势在于具有非常简单且因此广泛适用的成本函数：
- en: $$c(s,u)={\left\{\begin{array}{l l}{0}&{\mathrm{,~if~}s^{\prime}\in X^{+}}\\
    {c}&{\mathrm{,~else}}\end{array}\right.}$$
  id: totrans-8870
  prefs: []
  type: TYPE_NORMAL
  zh: $$c(s,u)={\left\{\begin{array}{l l}{0}&{\mathrm{,~如果~}s^{\prime}\in X^{+}}\\
    {c}&{\mathrm{,~否则}}\end{array}\right.}$$
- en: $$(30.4)$$
  id: totrans-8871
  prefs: []
  type: TYPE_NORMAL
  zh: $$(30.4)$$
- en: where c > 0 is a constant value. A reasonable choice of c is that c multiplied
    by the estimated number of time steps of the optimal policy should be considerably
    below the maximum path costs that can be represented by the neural network (which,
    when using a the standard sigmoid output function, is 1).
  id: totrans-8872
  prefs: []
  type: TYPE_NORMAL
  zh: 其中c > 0是一个常数值。一个合理的c选择是，c乘以最优策略的估计时间步数应该远低于神经网络所能表示的最大路径成本（当使用标准的sigmoid输出函数时，为1）。
- en: 'The immediate cost function proposed above moreover has the advantage, that
    the learned optimal policy has a clear interpretation: it is the minimum time
    controller. As a side note: using this immediate cost function, one can also check
    the ability of a learning system to learn correct value functions: within this
    framework, learning can only be successful, if the learned value function is actually
    meaningful, since no hint towards the goal is provided by the immediate cost function.'
  id: totrans-8873
  prefs: []
  type: TYPE_NORMAL
  zh: 上述提出的即时成本函数还有一个优势，即学习到的最优策略有明确的解释：它是最小时间控制器。附带说明：使用这个即时成本函数，可以检查学习系统学习正确价值函数的能力：在这个框架内，学习只有在学习到的价值函数实际上有意义的情况下才能成功，因为即时成本函数没有提供任何朝向目标的提示。
- en: 'The terminal cost function is simple as well: terminal costs are 0, if a terminal
    goal state is reached, and 1, if a constraint is violated. Of course, these values
    may depend on the potential range of the output values of the neural network.'
  id: totrans-8874
  prefs: []
  type: TYPE_NORMAL
  zh: 终端成本函数也很简单：如果达到终端目标状态，终端成本为0；如果违反约束，则为1。当然，这些值可能依赖于神经网络输出值的潜在范围。
- en: 'Summary:'
  id: totrans-8875
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要：
- en: '- costs should serve the purpose of meeting the specifications of the original
    control task as close as possible'
  id: totrans-8876
  prefs: []
  type: TYPE_NORMAL
  zh: '- 成本应尽可能满足原始控制任务的规格要求'
- en: '- immediate costs may reflect local hints to the goal to help learning but
    this might not necessarily reflect the intention of the original control task'
  id: totrans-8877
  prefs: []
  type: TYPE_NORMAL
  zh: '- 即时成本可能反映指向目标的局部提示以帮助学习，但这可能并不一定反映原始控制任务的意图'
- en: 30.4.8 Discounting
  id: totrans-8878
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.4.8 折扣
- en: 'In the above equations, γ with 0 ≤ γ ≤ 1 denotes a discounting parameter. Using
    γ < 1 may make learning the value function easier, since the horizon of the future
    costs considered is reduced (consider e.g. the extreme case where γ = 0. Then
    only the immediate costs are relevant). On the other hand, choosing a discount
    rate also has an influence on the resulting optimal policies: if γ < 1, immediate
    costs that occur later in the sequence are weighted less. One has to make sure,
    that this is in accordance with the initial control task formulation. We therefore
    usually prefer a formulation with no discounting, i.e. γ = 1 and therefore have
    to make sure that for successful learning, additional assumptions are fulfilled
    (e.g.'
  id: totrans-8879
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，γ 的范围为 0 ≤ γ ≤ 1，表示折扣参数。使用 γ < 1 可能会使值函数的学习变得更容易，因为考虑的未来成本的范围缩小（例如考虑极端情况
    γ = 0，这样只有即时成本是相关的）。另一方面，选择折扣率也会影响最终的最优策略：如果 γ < 1，则序列中稍后的即时成本的权重较小。必须确保这与初始控制任务的表述一致。因此，我们通常偏好没有折扣的表述，即
    γ = 1，因此必须确保成功学习时满足额外的假设（例如，
- en: the existence of proper policies, which basically means that X+ can be reached
    from every state with non-zero probability. For a detailed discussion see e.g.
    [1]).
  id: totrans-8880
  prefs: []
  type: TYPE_NORMAL
  zh: 存在适当策略的情况，这基本上意味着从每个状态出发可以以非零概率达到 X+。有关详细讨论，请参见例如 [1]）。
- en: 'Summary:'
  id: totrans-8881
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结：
- en: '- discounting requires less assumptions and therefore can make learning simpler
    and/ or more robust'
  id: totrans-8882
  prefs: []
  type: TYPE_NORMAL
  zh: '- 折扣要求较少的假设，因此可以使学习更简单和/或更稳健'
- en: '- it must be checked, whether introducing a discounting rate for the sake of
    better learning still matches the intention of the original control task.'
  id: totrans-8883
  prefs: []
  type: TYPE_NORMAL
  zh: '- 必须检查，为了更好的学习引入折扣率是否仍符合原始控制任务的意图。'
- en: 30.4.9 Choice of X0 In a typical setup, at the start of each episode, an initial
    starting state is randomly drawn from the starting state set X0. Ideally X0 is
    chosen such that it covers the whole range of initial conditions that occur in
    the original control task.
  id: totrans-8884
  prefs: []
  type: TYPE_NORMAL
  zh: 30.4.9 X0 的选择 在典型的设置中，每个情节开始时，初始状态是从初始状态集 X0 中随机抽取的。理想情况下，X0 的选择应该覆盖原始控制任务中出现的所有初始条件范围。
- en: 'In tasks, that in average require a large number of steps to reach the goal
    states, the probability of hitting the goal region by chance can be pretty low.
    Here, a method that we call the *''growing-competence''-heuristic* [21] might
    help: First, start with initial states close to the goal area and then incrementally
    increase the set of starting states until it finally covers the complete original
    starting state area.'
  id: totrans-8885
  prefs: []
  type: TYPE_NORMAL
  zh: 在平均需要大量步骤才能达到目标状态的任务中，偶然击中目标区域的概率可能非常低。在这里，我们称之为 *'逐步提升能力'-启发式* [21] 可能会有所帮助：首先，从接近目标区域的初始状态开始，然后逐步增加初始状态集，直到最终覆盖完整的原始初始状态区域。
- en: 'Summary:'
  id: totrans-8886
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结：
- en: '- the set of initial starting states for learning should cover the intended
    working space of the original control problem'
  id: totrans-8887
  prefs: []
  type: TYPE_NORMAL
  zh: '- 学习的初始状态集应覆盖原始控制问题的预期工作空间'
- en: '- if applicable, then starting with simple states first and then increasing
    the range might help to improve the learning process dramatically 30.4.10 Choice
    of the Maximal Episode Length N'
  id: totrans-8888
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果适用，首先从简单状态开始，然后逐步增加范围可能有助于显著改善学习过程 30.4.10 最大情节长度 N 的选择'
- en: Control episodes might take infinitely long - this is inherently the case in
    the non-terminal goal state framework and can also occur in the terminal goal
    state setting, if the policy neither finds to the goal region nor crashes. Therefore,
    while learning, one typically stops the episode after some predefined number of
    time steps. This is called the maximal episode length N in the following.
  id: totrans-8889
  prefs: []
  type: TYPE_NORMAL
  zh: 控制情节可能会无限长——在非终端目标状态框架中这本质上是如此，在终端目标状态设置中也可能发生，如果策略既不找到目标区域也不崩溃。因此，在学习过程中，通常会在预定义的时间步数后停止情节。这在下文中称为最大情节长度
    N。
- en: Theoretically, within the fitted Q learning framework, N is not a critical choice.
    It just denotes the number of transitions sampled in a row (this is different
    from learning methods that rely on complete trajectories). Actually, N might be
    as low as 2. Then, per episode only one transition sample is collected. From a
    practical perspective, however, it typically makes more sense to consider longer
    episodes - in particular, when the policy used to sample the transitions drives
    the system closer towards the goal region and therefore allows to collect more
    and more 'interesting' transitions.
  id: totrans-8890
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，在拟合的 Q 学习框架内，N 并不是一个关键选择。它只是表示连续采样的过渡数量（这与依赖完整轨迹的学习方法不同）。实际上，N 可以低至 2。然后，每个回合只收集一个过渡样本。然而，从实际角度来看，通常考虑较长的回合更有意义——特别是当用于采样过渡的策略将系统逐步引导向目标区域，从而允许收集越来越多的“有趣”过渡时。
- en: A rough heuristic that we use is to make N double or three times as large as
    the expected average time a successful controller will need to reach the target
    region.
  id: totrans-8891
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的一个粗略启发是将 N 设为成功控制器到达目标区域所需平均时间的两倍或三倍。
- en: If N is chosen too large, then a lot of useless information might be collected
    - consider for example very long episodes that just contain cycles of ever the
    same states.
  id: totrans-8892
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 N 选择得过大，那么可能会收集到很多无用的信息——例如考虑非常长的回合，仅包含相同状态的循环。
- en: 'Summary:'
  id: totrans-8893
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要：
- en: '- theoretically, the choice of N is not critical - practically, N can considerably
    influence learning behavior, since it influences the distribution of the collected
    transitions.'
  id: totrans-8894
  prefs: []
  type: TYPE_NORMAL
  zh: '- 从理论上讲，N 的选择并不关键——在实践中，N 可以显著影响学习行为，因为它影响收集的过渡分布。'
- en: 30.5 Tricks 30.5.1 Scaling The Input Values
  id: totrans-8895
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.5 技巧 30.5.1 输入值的缩放
- en: Like in normal supervised learning, scaling the input values is an important
    preprocessing step. Various methods and according explanations are discussed in
    [14]. As a standard method, in all our learning experiments, we normalize the
    input values to have mean of 0 and a standard deviation of 1.
  id: totrans-8896
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常的监督学习中，缩放输入值是一个重要的预处理步骤。各种方法及其解释在[14]中讨论。作为标准方法，在我们所有的学习实验中，我们将输入值归一化，使其均值为
    0，标准差为 1。
- en: 'Summary:'
  id: totrans-8897
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要：
- en: '- like in supervised learning, it is important that all input values have a
    similar level'
  id: totrans-8898
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在监督学习中，所有输入值具有相似水平是很重要的。'
- en: '- a simple scaling to mean 0 and standard deviation 1 works well in all our
    learning experiments so far 30.5.2 The X++-Trick If no explicit terminal state
    exists (which is the case in the nonterminal goal state framework), the output
    of the neural network tends to constantly increase from iteration to iteration.
    This is due to the choice of the transition costs, which are 0 (within the target
    region) or positive (outside the target region). Therefore, the target value of
    each state action pair is larger or at least equally large than the evaluation
    of its successor state. Amplified by the generalization property of the multilayer
    perceptron, this leads to the tendency to ever increase the output values of all
    state action pairs.'
  id: totrans-8899
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在我们迄今为止的所有学习实验中，简单地将均值设为 0，标准差设为 1 的缩放效果很好。30.5.2 X++技巧 如果不存在显式的终止状态（在非终止目标状态框架中就是这种情况），神经网络的输出往往会随着迭代不断增加。这是由于过渡成本的选择，目标区域内为
    0，目标区域外为正数。因此，每个状态动作对的目标值大于或至少等于其后继状态的评估。在多层感知器的泛化特性放大下，这导致了所有状态动作对的输出值不断增加的趋势。'
- en: A simple but effective remedy against this effect is to actually fix the values
    of some state action pairs to 0. We call the set of such states, for which we
    assume this to be true, X++. This heuristic is in accordance with a correct working
    of the value iteration scheme, as long as 0 is the expected optimal path costs
    for the respective state action pairs in X++. Of course, usually state action
    pairs for which this is true, cannot be assumed to be known a priori. Therefore,
    in order to apply this trick, one has to rely on heuristics. One reasonable choice
    of X++ are states, that lie in the center of X+, the region of zero transition
    costs.
  id: totrans-8900
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这种效应的一个简单但有效的补救措施是实际将某些状态动作对的值固定为 0。我们称这些状态的集合为 X++，我们假设这一点是成立的。只要 0 是 X++
    中各状态动作对的期望最优路径成本，这一启发是符合价值迭代方案的正确工作的。当然，通常不能假设这些状态动作对是已知的。因此，为了应用这个技巧，人们必须依赖启发式方法。X++
    的一个合理选择是位于 X+ 中心的状态，即零过渡成本区域。
- en: 'The reasoning behind this is the following: if one starts at a state x at the
    center of X+, then a good control policy has a very high chance of keeping the
    system within X+ forever - which justifies to assign zero path costs to that starting
    state.'
  id: totrans-8901
  prefs: []
  type: TYPE_NORMAL
  zh: 其背后的推理如下：如果从状态 x 开始，位于 X+ 的中心，那么一个好的控制策略将有很高的机会使系统永远保持在 X+ 内，这就证明了将零路径成本分配给该起始状态的合理性。
- en: If X++ is chosen too large, then for some states within X++ the assumption of
    optimal path costs of 0 may be violated. As a consequence, the resulting policy
    most likely will not fulfill the expected property of reliably keeping the system
    within X+. On the other hand, if X++ is too small, the chance, that a state actually
    falls into X++ is very low and therefore the heuristic becomes ineffective. A
    remedy against this, is to actually force the learning system to face states in
    X++. One possibility to do that, is to enforce starting episodes close to X++,
    another possibility is to introduce artificial state transitions, which is discussed
    in the context of the *hint-to-goal heuristic* in the next section 30.5.3.
  id: totrans-8902
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 X++ 选择得过大，那么在某些 X++ 内的状态中，最优路径成本为 0 的假设可能会被违反。因此，最终得到的策略很可能无法满足可靠地将系统保持在
    X+ 的预期属性。另一方面，如果 X++ 选择得过小，则某个状态实际落入 X++ 的机会非常低，因此启发式方法变得无效。对此的解决方法是强制学习系统面临 X++
    内的状态。实现这一点的一种可能性是强制在 X++ 附近开始回合，另一种可能性是引入人工状态过渡，这在下节 30.5.3 的*提示-目标启发式方法*中进行了讨论。
- en: 'Summary:'
  id: totrans-8903
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要：
- en: '- the X++ heuristic is a method to prevent the value function to steadily increase'
  id: totrans-8904
  prefs: []
  type: TYPE_NORMAL
  zh: '- X++ 启发式方法是一种防止价值函数不断增加的方法。'
- en: '- if applied carefully, it is in perfect accordance with the value iteration
    scheme'
  id: totrans-8905
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果谨慎应用，它与价值迭代方案完全一致。'
- en: 30.5.3 Artificial Training Transitions
  id: totrans-8906
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.5.3 人工训练过渡
- en: In a certain sense, the learning process can be interpreted as spreading its
    knowledge of the optimal value function from the goal states to the rest of the
    state space. Therefore, it is crucially required to actually have a reasonable
    number of state action pairs that lead to a goal state within the overall transition
    sample set. An obvious recipe would be to try to enforce the occurrence of such
    goal states, e.g. by starting episodes close to the goal area. However, this is
    not possible for all systems because they do not allow to set arbitrary initial
    states.
  id: totrans-8907
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，学习过程可以被解释为将最优价值函数的知识从目标状态传播到状态空间的其他部分。因此，确实需要拥有合理数量的状态-动作对，以便在整体过渡样本集中导向目标状态。一种明显的做法是强制出现这些目标状态，例如通过在接近目标区域的地方开始回合。然而，这并非所有系统都可以实现，因为它们不允许设置任意初始状态。
- en: An unconventional method to cope with the situation is to add artificial state
    transitions to the sample set. Then, the pattern set used for training consists
    of actually collected transitions, as well as additionally added artificial transitions.
    This method was first introduced as part of the *hint-to-goal* heuristic in our
    first NFQ paper [23] and has meanwhile also been successfully applied by other
    researchers using other function approximation schemes (e.g. Gaussian processes,
    [4]). The idea of the *hint-to-goal* heuristic is to introduce artificial state
    transitions, that start in X++ and end in X++. Those states have - by definition
    of X++ - terminal costs of 0. As a consequence, the value function is 'clamped'
    to zero at these input patterns. Supported by the generalization ability of the
    function approximation, also the neighboring states will tend to have a low and
    thus attractive value.
  id: totrans-8908
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非常规方法是将人工状态过渡添加到样本集中。这样，用于训练的模式集就由实际收集的过渡以及额外添加的人工过渡组成。该方法首次作为*提示-目标*启发式的一部分在我们的第一篇
    NFQ 论文中介绍[23]，并且目前也被其他研究者成功应用于其他函数逼近方案（例如，高斯过程，[4]）。*提示-目标*启发式方法的想法是引入人工状态过渡，从
    X++ 开始并在 X++ 结束。这些状态根据 X++ 的定义，终端成本为 0。因此，价值函数在这些输入模式处被“固定”为零。在函数逼近的泛化能力的支持下，邻近状态也会倾向于具有较低且因此具有吸引力的价值。
- en: If for the artificially introduced state action pairs the optimal path costs
    are actually zero, the hint-to-goal heuristic will not negatively interfere with
    the correct working of the value iteration process. An obvious choice therefore
    is a state action pair, where the state is well embedded in X+ such that the assumption
    of optimal path costs of 0 is most likely fulfilled. We usually generate such
    an artificial state action pair by combining such a state with every action in
    the action set.
  id: totrans-8909
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对于人工引入的状态-动作对，最优路径成本实际上为零，则*提示-目标*启发式不会对价值迭代过程的正确工作产生负面干扰。因此，一个显而易见的选择是一个状态-动作对，其中状态在X+中嵌入良好，以至于最优路径成本为0的假设最有可能成立。我们通常通过将这样的状态与动作集中的每个动作组合来生成这样的人工状态-动作对。
- en: The number of artificial patterns should be chosen such that a 'reasonable balance'
    between experience of success and regular state transitions exists (as a rule
    of thumb, something between 1:100 and 1:10). This is of course a number, that
    has to be determined empirically. We are currently working on methods that automatically
    find such a balance, but this is ongoing work and beyond the scope of this paper.
  id: totrans-8910
  prefs: []
  type: TYPE_NORMAL
  zh: 人工模式的数量应选择，以便在成功经验和常规状态转移之间存在“合理的平衡”（作为经验法则，通常在1:100和1:10之间）。当然，这个数字需要通过经验来确定。我们目前正在研究自动找到这种平衡的方法，但这项工作仍在进行中，超出了本文的范围。
- en: 'Summary:'
  id: totrans-8911
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要：
- en: '- the *hint-to-goal* heuristic might help to establish a goal region in the
    value function, if real experiences of success are difficult to achieve during
    regular learning'
  id: totrans-8912
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示-目标*启发式可能有助于在价值函数中建立目标区域，如果在常规学习中难以获得真正的成功经验。'
- en: 30.5.4 Growing Batch
  id: totrans-8913
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.5.4增长批次
- en: The fitted Q iteration framework originally works with a fixed set of transitions.
    No particular assumption is made, how these transitions are collected. In the
    extreme case, these experiences are randomly sampled all over the working space
    of the controller in advance. In a practical setting, however, this is not always
    feasible. One reason is, that arbitrary sampling all over the working space is
    not realizable, since initial states can not be set arbitrarily. Another reason
    is, that to sample transitions equally over the working space might just be infeasible
    due to the huge amount of data required to cover the complete space.
  id: totrans-8914
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合Q迭代框架最初使用固定的转移集。并没有特别的假设说明这些转移是如何收集的。在极端情况下，这些经验是在控制器的工作空间中随机采样的。然而，在实际设置中，这并不总是可行的。一个原因是，无法任意设置初始状态，随意采样整个工作空间不可实现。另一个原因是，在工作空间中均匀采样转移可能由于需要覆盖完整空间的巨大数据量而不可行。
- en: Therefore it is desirable to concentrate on regions of the state space that
    are relevant for the final controller. One method to realize this is the *growing
    batch* method. The idea is, that one starts with an empty transition set. After
    the first episode, the value function is updated and the new episode is controlled
    by exploiting the new value function. Different variants exist, e.g. the value
    function can only be updated after n episodes, or the number kmax of NFQ
  id: totrans-8915
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，集中于与最终控制器相关的状态空间区域是理想的。实现这一点的一种方法是*增长批次*方法。其思路是从一个空的转移集开始。在第一次经历后，更新价值函数，并通过利用新的价值函数来控制新的经历。存在不同的变体，例如价值函数仅在n次经历后更新，或者NFQ的最大数量kmax。
- en: iterations between two episodes can be varied. In most of our experiments so
    far we successfully used this growing batch procedure with the choice of n =
  id: totrans-8916
  prefs: []
  type: TYPE_NORMAL
  zh: 两次经历之间的迭代可以变化。在迄今为止的大多数实验中，我们成功地使用了这个增长批次程序，选择n=30。
- en: kmax = 1.
  id: totrans-8917
  prefs: []
  type: TYPE_NORMAL
  zh: kmax = 1。
- en: 'Summary:'
  id: totrans-8918
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要：
- en: '- the *growing batch* method aims at collecting more and more relevant transitions
    when the performance of the policy increases.'
  id: totrans-8919
  prefs: []
  type: TYPE_NORMAL
  zh: '- *增长批次*方法旨在在策略性能提高时收集越来越多的相关转移。'
- en: 30.5.5 Training The Neural Q-Function
  id: totrans-8920
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.5.5训练神经Q函数
- en: To represent the value function, we use a neural multilayer perceptron. Although
    it is often believed that setting up such kind of networks is a black art and
    its parameters are hard to find, we found that this is not particularly critical
    in the proposed neural fitted Q framework. One crucial point however is to use
    a powerful training algorithm to train the weights. The *Rprop* learning algorithm
    combines the advantage of fast learning and uncritical parameter choice [17].
    We always use Rprop with its standard parameters. Also we found, that the number
    of epochs (sweeps through the training set) is not particularly critical. We therefore
    always train for 300 epochs and get good results. One can also think of ways to
    monitor the training error and find some stopping criterion to make this more
    flexible (e.g. to adapt to different network sizes, to different pattern set sizes,
    etc.), but for the applications we had so far, we found this a minor issue for
    learning success.
  id: totrans-8921
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示价值函数，我们使用了神经多层感知器。虽然通常认为设置这种网络是一门黑魔法，其参数难以找到，但在提出的神经拟合Q框架中，我们发现这并不特别关键。然而，其中一个关键点是使用强大的训练算法来训练权重。*Rprop*学习算法结合了快速学习和不关键的参数选择优势
    [17]。我们总是使用Rprop及其标准参数。此外，我们发现，训练轮数（通过训练集进行的扫描次数）并不特别关键。因此，我们总是进行300轮的训练，并获得良好的结果。人们也可以考虑监视训练误差的方法，并找到一些停止标准，使其更加灵活（例如，适应不同的网络大小、不同的模式集大小等），但就迄今为止的应用而言，我们发现这对学习成功影响不大。
- en: Surprisingly, the same robustness is observed for the choice of the neural network
    size and structure. In our experience, a multilayer perceptron with 2 hidden layers
    and 20 neurons per layer works well over a wide range of applications. We use
    the tanh activation function for the hidden neurons and the standard sigmoid function
    at the output neuron. The latter restricts the output range of estimated path
    costs between 0 and 1 and the choice of the immediate costs and terminal costs
    have to be done accordingly. This means, in a typical setting, terminal goal costs
    are 0, terminal failure costs are 1 and immediate costs are usually set to a small
    value, e.g. c = 0.01. The latter is done with the consideration, that the expected
    maximum episode length times the transition costs should be well below 1 to distinguish
    successful trajectories from failures.
  id: totrans-8922
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，观察到在神经网络大小和结构选择上同样具有鲁棒性。根据我们的经验，每层有20个神经元的2隐藏层的多层感知器在各种应用中表现良好。我们在隐藏神经元中使用双曲正切激活函数，并在输出神经元中使用标准的sigmoid函数。后者限制了估计路径成本在0和1之间的输出范围，并且需要相应地选择即时成本和终端成本。这意味着在典型设置中，终端目标成本为0，终端失败成本为1，即时成本通常设置为一个小值，例如c
    = 0.01。后者是考虑到预期的最大情节长度乘以转移成本应远低于1，以区分成功的轨迹和失败。
- en: As a general impression, the success of learning depends much more on the proper
    setting of other parameters of the learning framework. The neural network and
    its training procedure work very robustly over a wide range of choices.
  id: totrans-8923
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一般印象，学习的成功更多地取决于学习框架其他参数的正确设置。神经网络及其训练过程在各种选择范围内表现非常稳健。
- en: 'Summary:'
  id: totrans-8924
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概要：
- en: '- choice of multilayer perceptron is rather uncritical - important to have
    a powerful learning algorithm to adjust the weights - advantage, if the supervised
    learning algorithm is not particularly dependent on the choice of its parameters.'
  id: totrans-8925
  prefs: []
  type: TYPE_NORMAL
  zh: '- 多层感知器的选择相对不关键 - 使用强大的学习算法调整权重很重要 - 如果监督学习算法对其参数的选择不特别依赖，这是一种优势。'
- en: 30.5.6 Exploration
  id: totrans-8926
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.5.6 探索
- en: In reinforcement learning, exploration - the deviation from a greedy exploitation
    of the current value function - is important to explore the state space.
  id: totrans-8927
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，探索 - 从当前值函数的贪婪开发中偏离 - 对于探索状态空间至关重要。
- en: Various suggestions for good exploration strategies have been proposed, e.g.
  id: totrans-8928
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了各种良好的探索策略建议，例如：
- en: considering a safe control behavior in the learning phase [12]. From our experience
    with NFQ, a simple -greedy exploration scheme is often sufficient. This means
    that in every time step, with a certain probability (e.g. 0.1), the action is
    chosen randomly instead of greedily exploiting the value function.
  id: totrans-8929
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习阶段考虑安全控制行为 [12]。根据我们对NFQ的经验，一个简单的-贪心探索方案通常就足够了。这意味着在每个时间步骤中，以一定的概率（例如0.1），会随机选择动作，而不是贪心地利用价值函数。
- en: In many application cases, we also observe good results even with no explicit
    exploration at all. This is due to the fact, that the learning process itself
    —
  id: totrans-8930
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多应用案例中，即使没有明确的探索，我们也观察到了良好的结果。这是因为学习过程本身——
- en: the randomly initialized neural value function, the growing experience, the
    randomly distributed starting states - already bears a fair amount of randomness.
  id: totrans-8931
  prefs: []
  type: TYPE_NORMAL
  zh: 随机初始化的神经价值函数、不断增长的经验、随机分布的初始状态——已经包含了相当数量的随机性。
- en: To learn without explicit exploration is also of practical interest. When always
    acting greedily, the performance achieved in a training episode is already the
    performance, that the final greedy controller will show. This reduces the effort
    of additional testing and therefore is particularly interesting for real world
    tasks.
  id: totrans-8932
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有明确探索的情况下学习也是实际上的一个重要问题。当始终贪婪地行动时，训练阶段所取得的表现已经是最终贪婪控制器将展现的表现。这减少了额外测试的工作量，因此对于现实世界任务特别有趣。
- en: 'Summary:'
  id: totrans-8933
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结：
- en: '- a simple  greedy exploration scheme is often sufficient'
  id: totrans-8934
  prefs: []
  type: TYPE_NORMAL
  zh: '- 一个简单的贪婪探索方案通常就足够了。'
- en: '- if the starting states are well distributed in the working space, then in
    conjunction with the *growing batch* method, even an always greedy exploitation
    of the current value function works in many cases'
  id: totrans-8935
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果初始状态在工作空间中分布良好，那么结合*增长批次*方法，甚至在许多情况下对当前价值函数的始终贪婪利用也是有效的。'
- en: 30.5.7 Delays
  id: totrans-8936
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.5.7 延迟
- en: In practical systems delays play a crucial role. Delays may occur both on the
    sensor side - i.e. a sensor value is available only n time steps later, or on
    the actor side - a control action has an effect only some time steps later. Simply
    neglecting these effects typically leads to bad control behavior or even failures.
    Various methods exist, e.g. to use prediction or filter methods to synchronize
    the information available to the controller with the current world situation.
    One simple but effective method is to augment state information with historical
    information about previous actions applied to the system [31, 20].
  id: totrans-8937
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际系统中，延迟起着至关重要的作用。延迟可能发生在传感器端——即传感器值仅在n个时间步后可用，或者在执行器端——控制动作的效果仅在若干时间步后显现。简单地忽略这些影响通常会导致糟糕的控制行为甚至失败。存在各种方法，例如使用预测或滤波方法将可用于控制器的信息与当前世界状态同步。一种简单但有效的方法是用关于施加于系统的先前动作的历史信息来增强状态信息
    [31, 20]。
- en: 'Summary:'
  id: totrans-8938
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结：
- en: '- in practice, actuator and sensor delays may often be not neglectable - a
    simple remedy is to add historical information about previous action values to
    the current state information'
  id: totrans-8939
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在实践中，执行器和传感器延迟往往不可忽视——一个简单的补救方法是将关于先前动作值的历史信息添加到当前状态信息中。'
- en: 30.6 Experiments 30.6.1 The Control Task
  id: totrans-8940
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.6 实验 30.6.1 控制任务
- en: The control task tackled in the following is to control a real cart pole system.
  id: totrans-8941
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来要解决的控制任务是控制一个真实的摆杆系统。
- en: 'While cart-pole is a well known benchmark [27], this real world task is characterized
    by additional challenging features:'
  id: totrans-8942
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然摆杆是一个众所周知的基准 [27]，但这个现实世界的任务具有额外的挑战性特征：
- en: '- the initial states can not be set to arbitrary values. We moreover assume,
    that no human intervention is allowed, in particular, the system can initially
    only be started with the pole hanging down'
  id: totrans-8943
  prefs: []
  type: TYPE_NORMAL
  zh: '- 初始状态不能设置为任意值。此外，我们假设不允许有人干预，特别是系统最初只能在杆垂直向下的情况下启动。'
- en: Fig. 30.2. The real cart pole system
  id: totrans-8944
  prefs: []
  type: TYPE_NORMAL
  zh: 图 30.2. 真实的摆杆系统
- en: '![736_image_0.png](736_image_0.png)'
  id: totrans-8945
  prefs: []
  type: TYPE_IMG
  zh: '![736_image_0.png](736_image_0.png)'
- en: '- the control task is to balance the pole upright with high accuracy and with
    the cart at a given target position (here: in the middle of the track). The controller
    therefore not only needs to learn to swing-up the pole from the downright position,
    but also to do it in such a sophisticated manner, that finally it can be balanced
    at the requested position.'
  id: totrans-8946
  prefs: []
  type: TYPE_NORMAL
  zh: '- 控制任务是以高精度将杆平衡竖直并使小车位于给定目标位置（此处为：轨道中间）。因此，控制器不仅需要学习将杆从垂直向下的位置摆起，还需以一种复杂的方式进行，以最终能够在请求的位置保持平衡。'
- en: '- one cannot directly control the force applied to the cart, but only the voltage
    given to the DC motor driving the car. This introduces additional dynamical effects
    into the system.'
  id: totrans-8947
  prefs: []
  type: TYPE_NORMAL
  zh: '- 无法直接控制施加于小车的力，只能控制施加于驱动电动机的电压。这为系统引入了额外的动态效应。'
- en: '- due to communication effects, the sensor information is delayed - there is
    considerable noise in both actuation and sensor values.'
  id: totrans-8948
  prefs: []
  type: TYPE_NORMAL
  zh: '- 由于通信效应，传感器信息存在延迟——执行和传感器值中都有相当大的噪声。'
- en: '- there is a discontinuity (jump) in sensor values from −π to +π when the pole
    is in the downright position.'
  id: totrans-8949
  prefs: []
  type: TYPE_NORMAL
  zh: '- 当杆子处于垂直向下位置时，传感器值存在不连续性（跳变）从−π到+π。'
- en: '- there is a hard constraint: the position of the cart may not be less than'
  id: totrans-8950
  prefs: []
  type: TYPE_NORMAL
  zh: '- 存在一个硬约束：小车的位置不得低于'
- en: -0.25m and more than 0.25m, since the track is bounded.
  id: totrans-8951
  prefs: []
  type: TYPE_NORMAL
  zh: -0.25米及超过0.25米，因为轨道是有限的。
- en: '- the final controller should be able to work from arbitrary initial start
    states, not only from one position.'
  id: totrans-8952
  prefs: []
  type: TYPE_NORMAL
  zh: '- 最终控制器应能够从任意初始起始状态工作，而不仅仅是从一个位置。'
- en: The range of control inputs is (quasi) continuous from -12 volt to 12 volt.
    Sensor information provided by the system is the position of the cart and the
    pole; no velocity information can be measured. The target values for the sensor
    values should be reached as fast as possible. The minimum control interval allowed
    by the hardware is "t = 0.01s.
  id: totrans-8953
  prefs: []
  type: TYPE_NORMAL
  zh: 控制输入的范围是（准）连续的，从-12伏到12伏。系统提供的传感器信息包括小车和杆子的位置信息；无法测量速度信息。传感器值的目标值应尽快达到。硬件允许的最小控制间隔为“t
    = 0.01秒”。
- en: Since on the real system we can only perform a limited number of experiments,
    we also report some results on a reliable simulation of the real system (section
    30.6.5). The input and output interfaces are exactly the same for both simulated
    and real plant. The simulation model was derived by parameterizing a physical
    model of the plant using real data. The accurate match between real and simulated
    system behavior allowed us to do keep all modelling decisions and learning parameter
    settings the same for both the simulated and the real system. Therefore in the
    following, we only describe the real system setup. An implementation of the simulated
    plant is available within our open-source learning framework CLSquare available
    at http://ml.informatik.uni-freiburg.de/research/clsquare.
  id: totrans-8954
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在真实系统中我们只能进行有限数量的实验，我们还报告了一些关于真实系统可靠模拟的结果（第30.6.5节）。模拟和真实系统的输入输出接口完全相同。模拟模型是通过使用真实数据对植物的物理模型进行参数化而得出的。真实与模拟系统行为之间的准确匹配使我们能够保持所有建模决策和学习参数设置在模拟和真实系统中一致。因此，在以下内容中，我们只描述真实系统的设置。模拟植物的实现可以在我们的开源学习框架CLSquare中获得，网址是http://ml.informatik.uni-freiburg.de/research/clsquare。
- en: 30.6.2 Modeling As A Learning Task
  id: totrans-8955
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.6.2 建模作为学习任务
- en: State Description. State information provided to the learning system consists
    of sensor values of position (pt), angle (αt), the normalized temporal difference
    of these measurements pt−pt−1 t and αt−αt−1 t . The angle is zero, when the pole
    is upright. The angular value has a discontinuity (a jump from −π to +π) when
    the pole is hanging down. Nothing particularly is done to resolve this discontinuity.
  id: totrans-8956
  prefs: []
  type: TYPE_NORMAL
  zh: 状态描述。提供给学习系统的状态信息包括位置（pt）、角度（αt）的传感器值，以及这些测量值的归一化时间差pt−pt−1 t和αt−αt−1 t。当杆子竖直时，角度为零。当杆子垂下时，角度值存在不连续性（从−π跳到+π）。对此不连续性没有特别的解决方法。
- en: Instead, we expect the learning algorithm to be able to deal with that. To cope
    with the sensor delay, additionally the value of the previous control action at−1
    is added to the state information.
  id: totrans-8957
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们期望学习算法能够处理这一问题。为了应对传感器延迟，先前控制动作的值at−1也被加入到状态信息中。
- en: Actions. The action set available for the learning system consists of the 'standard'
    choice of minimal and maximal control signal plus the 'neutral' action 0V.
  id: totrans-8958
  prefs: []
  type: TYPE_NORMAL
  zh: 行动。学习系统可用的动作集包括“标准”选择的最小和最大控制信号，以及“中立”动作0V。
- en: Thus A = {−12V, 0V, +12V }.
  id: totrans-8959
  prefs: []
  type: TYPE_NORMAL
  zh: 因此A = {−12V, 0V, +12V}。
- en: Control Interval t. As defined by the hardware, the minimum length of the control
    interval is 0.01s. After some initial experiments, we found that a control interval
    of "t = 0.05s is still sufficient for an acceptable control quality while at the
    same time allowing a fast and successful learning process. Non-terminal Goal State
    Framework. For the cart-pole task, control must be continued once pole angle and
    cart position reached their target values to actively keep the system within the
    goal states. This means, that the correct formulation of the learning problem
    is the non-terminal goal state setting. As a consequence, every episode is only
    interrupted, if the system state entered the failure set X− or if the maximum
    number of steps per episode, N is reached.
  id: totrans-8960
  prefs: []
  type: TYPE_NORMAL
  zh: 控制间隔t。根据硬件定义，控制间隔的最小长度为0.01秒。经过一些初步实验，我们发现控制间隔“t = 0.05秒”对于可接受的控制质量仍然足够，同时允许快速和成功的学习过程。非终止目标状态框架。对于倒车杆任务，一旦杆角和小车位置达到目标值，控制必须继续以主动保持系统在目标状态内。这意味着，学习问题的正确表述是非终止目标状态设置。因此，只有当系统状态进入失败集X−或每个回合的最大步骤数N达到时，每个回合才会被打断。
- en: 'Choice of X+. A state is in X+, if the following two conditions are fulfilled:'
  id: totrans-8961
  prefs: []
  type: TYPE_NORMAL
  zh: X+的选择。如果满足以下两个条件，则状态在X+中：
- en: 'the cart position is at most 0.1m away from the target position (here: middle
    of the track) and the pole angle deviates from 0 rad by maximally ±0.15 rad. The
    rest of the state entries is not considered for judging membership to X+.'
  id: totrans-8962
  prefs: []
  type: TYPE_NORMAL
  zh: 当小车位置距离目标位置（此处：轨道中间）最多为0.1米时，杆角最大偏差为±0.15弧度。其余状态条目不考虑以判断是否属于X+。
- en: Choice of X−. A state is in X−, if the cart position is less than -0.25m or
    more than 0.25m. This corresponds to the physical boundaries of the track. The
    rest of the state entries is not considered for judging membership to X−.
  id: totrans-8963
  prefs: []
  type: TYPE_NORMAL
  zh: X−的选择。如果小车位置小于-0.25米或大于0.25米，则状态在X−中。这对应于轨道的物理边界。其余状态条目不考虑以判断是否属于X−。
- en: $$(30.5)$$
  id: totrans-8964
  prefs: []
  type: TYPE_NORMAL
  zh: $$(30.5)$$
- en: Immediate and Final Cost Functions. As immediate costs we use the standard minimum-time
    formulation with constant transition costs of 0.01. Thus,
  id: totrans-8965
  prefs: []
  type: TYPE_NORMAL
  zh: 立即和最终成本函数。作为立即成本，我们使用标准最小时间公式，常数过渡成本为0.01。因此，
- en: $$c(s,u)=\left\{\begin{array}{l l}{{0}}&{{,\quad\mathrm{if}\ |p_{t}|\leq0.1m\
    \mathrm{and}\ |\alpha_{t}|\leq0.15\mathrm{rad}}}\\ {{0.01}}&{{,\quad\mathrm{else}}}\end{array}\right.$$
  id: totrans-8966
  prefs: []
  type: TYPE_NORMAL
  zh: $$c(s,u)=\left\{\begin{array}{l l}{{0}}&{{,\quad\mathrm{if}\ |p_{t}|\leq0.1m\
    \mathrm{and}\ |\alpha_{t}|\leq0.15\mathrm{rad}}}\\ {{0.01}}&{{,\quad\mathrm{else}}}\end{array}\right.$$
- en: When a state from X− is observed, the episode is stopped and final costs of
    +1 are assigned.
  id: totrans-8967
  prefs: []
  type: TYPE_NORMAL
  zh: 当观察到来自X−的状态时，回合停止，并赋予最终成本+1。
- en: Episode Length. Empirically, a good episode length was found to be N = 200.
    30.6.3 Applied Tricks Scaling. We applied our standard input scaling procedure
    as described in 30.5.1.
  id: totrans-8968
  prefs: []
  type: TYPE_NORMAL
  zh: 回合长度。经验表明，一个好的回合长度为N = 200。30.6.3应用技巧缩放。我们应用了标准输入缩放程序，如30.5.1所述。
- en: Choice of X++ and Artificial Transitions. X++ contains only one state, namely
    if all state variables are exactly 0. This corresponds to the center of X+. Of
    course, this state will most likely not occur by chance in the learning process.
    Therefore, this definition makes only sense in conjunction with adding artificial
    transitions in the spirit of the *hint-to-goal* heuristic. Here, we used 3 different
    artificial patterns, namely state (0,0,0,0,0) combined with all 3 actions.
  id: totrans-8969
  prefs: []
  type: TYPE_NORMAL
  zh: X++的选择和人工过渡。X++仅包含一个状态，即所有状态变量恰好为0。这对应于X+的中心。当然，这种状态在学习过程中很可能不会偶然出现。因此，这一定义只有与添加人工过渡的*提示到目标*启发式结合时才有意义。在这里，我们使用了三种不同的人工模式，即状态(0,0,0,0,0)与所有三种动作结合。
- en: These transitions were repeated 100 times in the training pattern set, in order
    to establish some balance between the (huge) number of normal transitions and
    those special transitions. The target values for those artificial patterns is
    0. Growing Batch. Learning was implemented as a 'growing batch' process. This
    means, that after every episode, one NFQ iteration (new calculation of Q-target
    values, supervised learning of the neural Q function) was performed. Then the
    next episode was controlled by -greedy exploitation of this new Q function.
  id: totrans-8970
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在（巨大的）正常过渡和那些特殊过渡之间建立一些平衡，这些过渡在训练模式集中重复了100次。这些人工模式的目标值为0。增长批次。学习被实现为“增长批次”过程。这意味着，在每个回合后，进行一次NFQ迭代（Q目标值的新计算，神经Q函数的监督学习）。然后，通过对这个新Q函数的-贪婪利用来控制下一个回合。
- en: Training the Neural Q Function. The neural Q function is represented by a multilayer
    perceptron with 6 input neurons, two hidden layers with 20 neurons each and one
    output neuron. Hidden neurons use the tanh activation function, the output neuron
    uses the standard sigmoid function. Rprop with standard parameters was used for
    weight updates. In every NFQ iteration step, the network weights of the learning
    network were randomly initialized between -0.5 and 0.5 before training. The network
    was trained for 300 epochs per NFQ iteration. Exploration. No explicit exploration
    scheme was used for the experiments done here, i.e. the current Q function was
    always exploited greedily to determine the action. This has the advantage, that
    the application performance can already be determined during training.
  id: totrans-8971
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经Q函数。神经Q函数由一个具有6个输入神经元的多层感知器表示，两个隐藏层各有20个神经元和一个输出神经元。隐藏神经元使用tanh激活函数，输出神经元使用标准sigmoid函数。权重更新采用标准参数的Rprop。在每个NFQ迭代步骤中，学习网络的网络权重在训练前随机初始化在-0.5和0.5之间。网络在每次NFQ迭代中训练300个周期。探索。在这里进行的实验中没有使用显式的探索方案，即当前的Q函数总是贪婪地利用以确定动作。这具有一个优势，即在训练期间可以确定应用性能。
- en: 30.6.4 Measuring Quality
  id: totrans-8972
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.6.4 质量测量
- en: 'The quality of a learning control approach has two important aspects: the quality
    of the learning process and the quality of the resulting controller. The quality
    of the learning process is measured by the learning effort needed, usually measured
    in the number of transitions (or the number of episodes) needed, the quality of
    the achieved solution with respect to the used cost function, and the reliability
    of the results over a number of learning trials.'
  id: totrans-8973
  prefs: []
  type: TYPE_NORMAL
  zh: 学习控制方法的质量有两个重要方面：学习过程的质量和最终控制器的质量。学习过程的质量通过所需的学习努力来衡量，通常以所需的过渡次数（或回合数）为单位，所实现解决方案相对于使用的成本函数的质量，以及在多次学习试验中的结果可靠性来衡量。
- en: The quality of the resulting controller is measured with respect to the specification
    of the original control task. Relevant criteria are for example accuracy, robustness,
    working area, and performance measures like e.g. the time outside the tolerated
    error zone. For a detailed discussion of different criteria also see [11].
  id: totrans-8974
  prefs: []
  type: TYPE_NORMAL
  zh: 最终控制器的质量相对于原始控制任务的规范来衡量。相关标准例如准确性、鲁棒性、工作区域和性能指标，例如超出容忍误差区域的时间。有关不同标准的详细讨论，请参见[11]。
- en: Here, we first report results achieved in a realistic simulation. This allows
    us to conduct a series of 10 experiments with different seeds of the random generator
    in reasonable time. For learning on the real system, we used exactly the same
    setup and parameters. The only difference we made was, that the controller was
    allowed to learn for a maximum of 500 episodes on the simulated cart-pole and
  id: totrans-8975
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先报告在现实模拟中取得的结果。这使我们能够在合理的时间内进行一系列10次实验，使用不同种子的随机生成器。对于真实系统的学习，我们使用完全相同的设置和参数。我们唯一的不同之处是，控制器在模拟的推杆上最多允许学习500个回合。
- en: '- due to time restrictions - for a maximum of 300 episodes on the real cart-pole
    system.'
  id: totrans-8976
  prefs: []
  type: TYPE_NORMAL
  zh: '- 由于时间限制 - 在真实推杆系统上最多300个回合。'
- en: 30.6.5 Results On The Simulated Cart Pole
  id: totrans-8977
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.6.5 模拟推杆的结果
- en: For the simulated system, all 10 runs delivered a successful controller. 'Successful'
    means, that for a test set of 100 random initial starting situations, the controller
    was able to swing up the pole and then steadily keep the system within the desired
    tolerance. A test run lasted 20s. In average over 10 runs, the best controller
    was found after an average training of 392 episodes with a standard deviation
    of 80. The average time needed by the best controllers was 3.23s with a standard
    deviation of 0.16s.
  id: totrans-8978
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模拟系统，所有10次实验都成功交付了控制器。“成功”意味着，在100个随机初始起始情况的测试集中，控制器能够将杆摆起并稳稳地保持系统在期望的容忍范围内。每次测试运行持续20秒。在10次运行的平均值中，最佳控制器在平均392个回合的训练后找到，标准差为80。最佳控制器所需的平均时间为3.23秒，标准差为0.16秒。
- en: Table 30.1. Results on the simulated cart-pole for the standard setup, averaged
    over 10 trials. Shown are the average number of episode to train the best controller
    and its control performance, measured in time outside the target region. The number
    in brackets shows the respective standard deviation.
  id: totrans-8979
  prefs: []
  type: TYPE_NORMAL
  zh: 表30.1. 模拟推杆的结果，基于标准设置，平均10次试验。显示了训练最佳控制器所需的平均回合数及其控制性能，后者通过超出目标区域的时间来衡量。括号中的数字显示了相应的标准差。
- en: '| setup   | successful trials best controller at episode time outside of X+   |            |               |'
  id: totrans-8980
  prefs: []
  type: TYPE_TB
  zh: '| 设置   | 成功的实验 最佳控制器 在回合时间 外部X+   |            |               |'
- en: '| --- | --- | --- | --- |'
  id: totrans-8981
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Default | 10/10                                                             |
    392 (80.7) | 3.23s (0.16s) |'
  id: totrans-8982
  prefs: []
  type: TYPE_TB
  zh: '| 默认 | 10/10                                                             |
    392 (80.7) | 3.23s (0.16s) |'
- en: 30.6.6 Results On The Real Cart Pole
  id: totrans-8983
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.6.6 在真实的倒立摆上的结果
- en: The evaluation on the real cart-pole system was slightly different, due to the
    effort it takes to do experiments with the real device. However, the overall picture
    of the learning behavior on the simulated and real system was consistent.
  id: totrans-8984
  prefs: []
  type: TYPE_NORMAL
  zh: 在真实的倒立摆系统上的评估略有不同，因为进行真实设备实验需要花费精力。然而，模拟系统和真实系统的学习行为总体一致。
- en: We performed three learning trials with different initializations of the random
    generator. Each learning trial lasted 300 episodes. Besides the reduced number
    of maximum episodes, the setup of the learning system was exactly the same as
    for the simulated system. In all 3 trials performed, successful controllers were
    learned within less than 300 episodes of training. In particular, the controllers
    are very robust with respect to varying initial states or to disturbance from
    outside.
  id: totrans-8985
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了三次学习实验，随机生成器的初始设置不同。每次学习实验持续了300个回合。除了最大回合数减少外，学习系统的设置与模拟系统完全相同。在进行的三次实验中，成功的控制器在少于300个训练回合内学习到。特别是，这些控制器对于变化的初始状态或外部干扰非常稳健。
- en: A video documenting learning and final controller performance is available at
    http://www.youtube.com/watch?v=Lt-KLtkDlh8
  id: totrans-8986
  prefs: []
  type: TYPE_NORMAL
  zh: 记录学习和最终控制器性能的视频可在 [http://www.youtube.com/watch?v=Lt-KLtkDlh8](http://www.youtube.com/watch?v=Lt-KLtkDlh8)
    查看。
- en: 30.7 Conclusion
  id: totrans-8987
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 30.7 结论
- en: This paper discusses many of the basic modeling and methodological tricks to
    set up a reinforcement learning task. These insights should help to successfully
    handle a wide range of interesting control problems. The proposed method builds
    on neural fitted Q iteration (NFQ), a method that considers the complete batch
    of collected transitions to update the Q function. While the paper is written
    from the perspective of using a neural network, it should also give useful insights
    when using other kinds of function approximation schemes.
  id: totrans-8988
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论了建立强化学习任务的许多基本建模和方法技巧。这些见解应有助于成功处理广泛有趣的控制问题。所提出的方法建立在神经拟合Q迭代（NFQ）基础上，该方法考虑了收集的所有转移批量来更新Q函数。虽然本文是从使用神经网络的角度撰写的，但在使用其他类型的函数逼近方案时也应提供有用的见解。
- en: Current and future work is aiming to further improve the method in several directions.
    One big direction is to improve NFQ with respect to resulting controller quality
    (e.g. accuracy, continuous actions, interpretation of control policies, increasing
    complexity of control tasks, etc). Some steps in this direction have already been
    made and are discussed in [11]. Another area of ongoing and future research is
    to further improve NFQ with respect to robustness and autonomy of the learning
    process. A third area is to improve efficiency with respect to the data required
    for learning. Beyond that, distributed reinforcement learning algorithms that
    cooperatively control a complex system in a multi-agent setting is a vital research
    area. Distributed learning systems that are based on the neural learning framework
    presented here have been successfully applied in typical multi-agent scenarios
    like distributed job-shop scheduling [18, 8]).
  id: totrans-8989
  prefs: []
  type: TYPE_NORMAL
  zh: 当前和未来的工作旨在从多个方向进一步改进该方法。其中一个重要方向是提高NFQ在控制器质量（例如，准确性、连续动作、控制策略的解释、控制任务的复杂性增加等）方面的表现。这方面已采取了一些步骤，并在[11]中讨论。另一个正在进行和未来的研究领域是进一步提高NFQ在学习过程的稳健性和自主性方面的表现。第三个领域是提高学习所需数据的效率。此外，分布式强化学习算法在多智能体环境中协作控制复杂系统是一个重要的研究领域。基于这里提出的神经学习框架的分布式学习系统已成功应用于典型的多智能体场景，如分布式作业车间调度[18,
    8]。
- en: Acknowledgment. The author wants to especially thank Roland Hafner from Cognit
    GmbH for the important initial ignition for writing this article.
  id: totrans-8990
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢。作者特别感谢来自Cognit GmbH的Roland Hafner，为撰写本文提供了重要的初步启发。
- en: '[1] Bertsekas, D.P.: Dynamic Programming and Optimal Control, vol. I, II. Athena
    Scientific, Belmont (1995)'
  id: totrans-8991
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Bertsekas, D.P.: 动态编程与最优控制，第一卷，第二卷。Athena Scientific，Belmont (1995)。'
- en: '[2] Blum, M., Springenberg, J.T., Wülfing, J., Riedmiller, M.: A Learned Feature
    Descriptor for Object Recognition in RGB-D Data. In: Proceedings of the IEEE'
  id: totrans-8992
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Blum, M., Springenberg, J.T., Wülfing, J., Riedmiller, M.: 用于RGB-D数据对象识别的学习特征描述符。载于：IEEE会议论文集'
- en: International Conference on Robotics and Automation (ICRA), St. Paul, Minnesota,
    USA (2012)
  id: totrans-8993
  prefs: []
  type: TYPE_NORMAL
  zh: 国际机器人与自动化会议（ICRA），美国明尼苏达州圣保罗（2012）
- en: '[3] Bertsekas, D.P., Tsitsiklis, J.N.: Neuro Dynamic Programming. Athena Scientific,
    Belmont (1996)'
  id: totrans-8994
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Bertsekas, D.P., Tsitsiklis, J.N.: 神经动态编程。阿西纳科学，贝尔蒙特（1996）'
- en: '[4] Deisenroth, M.P., Rasmussen, C.E., Peters, J.: Gaussian Process Dynamic
    Programming. Neurocomputing 72(7–9), 1508–1524 (2009)'
  id: totrans-8995
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Deisenroth, M.P., Rasmussen, C.E., Peters, J.: 高斯过程动态编程。神经计算 72(7–9), 1508–1524（2009）'
- en: '[5] Ernst, D., Wehenkel, L., Geurts, P.: Tree-based batch mode reinforcement
    learning. Journal of Machine Learning Research 6, 503–556 (2005)'
  id: totrans-8996
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Ernst, D., Wehenkel, L., Geurts, P.: 基于树的批量模式强化学习。机器学习研究杂志 6, 503–556（2005）'
- en: '[6] Gabel, T., Lutz, C., Riedmiller, M.: Improved Neural Fitted Q Iteration
    Applied to a Novel Computer Gaming and Learning Benchmark. In: Proceedings of
    the IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning
    (ADPRL 2011), Paris, France. IEEE Press (April 2011)'
  id: totrans-8997
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Gabel, T., Lutz, C., Riedmiller, M.: 应用于新型计算机游戏与学习基准的改进神经拟合Q迭代。载于：IEEE近似动态编程与强化学习研讨会论文集（ADPRL
    2011），法国巴黎。IEEE出版社（2011年4月）'
- en: '[7] Gabel, T., Riedmiller, M.: On Experiences in a Complex and Competitive
    Gaming Domain: Reinforcement Learning Meets RoboCup. In: Proceedings of the IEEE
    Symposium on Computational Intelligence and Games, Honolulu, USA (2007)'
  id: totrans-8998
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Gabel, T., Riedmiller, M.: 在复杂竞争游戏领域的经验：强化学习与RoboCup的结合。载于：IEEE计算智能与游戏研讨会论文集，美国檀香山（2007）'
- en: '[8] Gabel, T., Riedmiller, M.: Adaptive Reactive Job-Shop Scheduling with Reinforcement
    Learning Agents. International Journal of Information Technology and Intelligent
    Computing 24(4) (2008)'
  id: totrans-8999
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Gabel, T., Riedmiller, M.: 基于强化学习代理的自适应反应式作业车间调度。国际信息技术与智能计算杂志 24(4)（2008）'
- en: '[9] Hafner, R., Riedmiller, M.: Reinforcement learning on an omnidirectional
    mobile robot. In: Proceedings of the 2003 IEEE/RSJ International Conference on
    Intelligent Robots and Systems (IROS 2003), Las Vegas (2003)'
  id: totrans-9000
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Hafner, R., Riedmiller, M.: 在全向移动机器人上的强化学习。载于：2003 IEEE/RSJ国际智能机器人与系统会议论文集（IROS
    2003），拉斯维加斯（2003）'
- en: '[10] Hafner, R., Riedmiller, M.: Neural Reinforcement Learning Controllers
    for a Real Robot Application. In: Proceedings of the IEEE International Conference
    on Robotics and Automation (ICRA 2007), Rome, Italy (2007)'
  id: totrans-9001
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Hafner, R., Riedmiller, M.: 用于真实机器人应用的神经强化学习控制器。载于：IEEE国际机器人与自动化会议论文集（ICRA
    2007），意大利罗马（2007）'
- en: '[11] Hafner, R., Riedmiller, M.: Reinforcement learning in feedback control.
    Machine Learning 27(1), 55–74 (2011), 10.1007/s10994-011-5235-x'
  id: totrans-9002
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Hafner, R., Riedmiller, M.: 反馈控制中的强化学习。机器学习 27(1), 55–74（2011），10.1007/s10994-011-5235-x'
- en: '[12] Hans, A., Schneegass, D., Schäfer, A.M., Udluft, S.: Safe exploration
    for reinforcement learning. In: ESANN, pp. 143–148 (2008)'
  id: totrans-9003
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Hans, A., Schneegass, D., Schäfer, A.M., Udluft, S.: 强化学习的安全探索。载于：ESANN，页143–148（2008）'
- en: '[13] Kietzmann, T., Riedmiller, M.: The Neuro Slot Car Racer: Reinforcement
    Learning in a Real World Setting. In: Proceedings of the Int. Conference on Machine
    Learning Applications (ICMLA 2009), Miami, Florida. Springer (December 2009)'
  id: totrans-9004
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Kietzmann, T., Riedmiller, M.: 神经插槽赛车：在现实环境中的强化学习。载于：国际机器学习应用会议论文集（ICMLA
    2009），佛罗里达州迈阿密。施普林格（2009年12月）'
- en: '[14] LeCun, Y., Bottou, L., Orr, G.B., Müller, K.-R.: Efficient backProp. In:
    Orr, G.B., Müller, K.-R. (eds.) NIPS-WS 1996. LNCS, vol. 1524, pp. 9–50. Springer,
    Heidelberg (1998)'
  id: totrans-9005
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] LeCun, Y., Bottou, L., Orr, G.B., Müller, K.-R.: 高效的反向传播。在：Orr, G.B.,
    Müller, K.-R.（编）NIPS-WS 1996。LNCS，第1524卷，页9–50。施普林格，海德堡（1998）'
- en: '[15] Lange, S., Riedmiller, M.: Deep auto-encoder neural networks in reinforcement
    learning. In: International Joint Conference on Neural Networks (IJCNN 2010),
    Barcelona, Spain (2010)'
  id: totrans-9006
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Lange, S., Riedmiller, M.: 强化学习中的深度自编码神经网络。载于：国际神经网络联合会议（IJCNN 2010），西班牙巴萨罗那（2010）'
- en: '[16] Lange, S., Riedmiller, M.: Deep learning of visual control policies. In:
    European Symposium on Artificial Neural Networks, Computational Intelligence and
    Machine Learning (ESANN 2010), Brugge, Belgium (2010)'
  id: totrans-9007
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Lange, S., Riedmiller, M.: 视觉控制策略的深度学习。载于：欧洲人工神经网络、计算智能与机器学习研讨会（ESANN
    2010），比利时布鲁日（2010）'
- en: '[17] Riedmiller, M., Braun, H.: A direct adaptive method for faster backpropagation
    learning: The RPROP algorithm. In: Ruspini, H. (ed.) Proceedings of the IEEE International
    Conference on Neural Networks (ICNN), San Francisco, pp. 586–591 (1993)'
  id: totrans-9008
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Riedmiller, M., Braun, H.: 一种用于更快反向传播学习的直接自适应方法：RPROP算法。在：Ruspini, H.（编辑）1993年IEEE国际神经网络会议（ICNN）论文集，旧金山，pp.
    586–591（1993年）'
- en: '[18] Riedmiller, M., Gabel, T.: Distributed Policy Search Reinforcement Learning
    for Job-Shop Scheduling Tasks. TPRS International Journal of Production Research
    50(1) (2012); Available online from (May 2011)'
  id: totrans-9009
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Riedmiller, M., Gabel, T.: 用于作业车间调度任务的分布式策略搜索强化学习。《TPRS国际生产研究杂志》50(1)（2012年）；在线可用（2011年5月）'
- en: '[19] Riedmiller, M., Gabel, T., Hafner, R., Lange, S.: Reinforcement Learning
    for Robot Soccer. Autonomous Robots 27(1), 55–74 (2009)'
  id: totrans-9010
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Riedmiller, M., Gabel, T., Hafner, R., Lange, S.: 用于机器人足球的强化学习。《自主机器人》27(1)，55–74（2009年）'
- en: '[20] Riedmiller, M., Hafner, R., Lange, S., Lauer, M.: Learning to Dribble
    on a Real Robot by Success and Failure. In: Proceedings of the 2008 International
    Conference on Robotics and Automation (ICRA 2008), Pasadena CA. Springer (2008)
    (video presentation)'
  id: totrans-9011
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Riedmiller, M., Hafner, R., Lange, S., Lauer, M.: 通过成功与失败学习在真实机器人上运球。在：2008年国际机器人与自动化会议（ICRA
    2008）论文集，加州帕萨迪纳。施普林格（2008年）（视频展示）'
- en: '[21] Riedmiller, M.: Learning to control dynamic systems. In: Trappl, R. (ed.)
    Proceedings of the 13th European Meeting on Cybernetics and Systems Research,
    EMCSR 1996 (1996)'
  id: totrans-9012
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Riedmiller, M.: 学习控制动态系统。在：Trappl, R.（编辑）第13届欧洲控制论与系统研究会议论文集，EMCSR 1996（1996年）'
- en: '[22] Riedmiller, M.: Generating continuous control signals for reinforcement
    controllers using dynamic output elements. In: European Symposium on Artificial
    Neural Networks, ESANN 1997, Bruges (1997)'
  id: totrans-9013
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Riedmiller, M.: 使用动态输出元件生成强化控制器的连续控制信号。在：1997年欧洲人工神经网络研讨会，ESANN 1997，布鲁日（1997年）'
- en: '[23] Riedmiller, M.: Neural Fitted Q Iteration - First Experiences with a Data
    Efficient Neural Reinforcement Learning Method. In: Gama, J., Camacho, R., Brazdil,
    P.B., Jorge, A.M., Torgo, L. (eds.) ECML 2005. LNCS (LNAI), vol. 3720, pp. 317–328.'
  id: totrans-9014
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Riedmiller, M.: 神经拟合Q迭代 - 数据高效的神经强化学习方法的初步经验。在：Gama, J., Camacho, R.,
    Brazdil, P.B., Jorge, A.M., Torgo, L.（编辑）ECML 2005。LNCS（LNAI），第3720卷，pp. 317–328。'
- en: Springer, Heidelberg (2005)
  id: totrans-9015
  prefs: []
  type: TYPE_NORMAL
  zh: 施普林格，海德堡（2005年）
- en: '[24] Riedmiller, M.: Neural reinforcement learning to swing-up and balance
    a real pole.'
  id: totrans-9016
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Riedmiller, M.: 神经强化学习以摆动和保持一个真实的杆子。'
- en: 'In: Proc. of the Int. Conference on Systems, Man and Cybernetics, 2005, Big
    Island, USA (October 2005)'
  id: totrans-9017
  prefs: []
  type: TYPE_NORMAL
  zh: 在：国际系统、人机与控制论会议论文集，2005年，美国大岛（2005年10月）
- en: '[25] Riedmiller, M., Lange, S., Voigtländer, A.: Autonomous reinforcement learning
    on raw visual input data in a real world application. In: Proceedings of the International
    Joint Conference on Neural Networks, Brisbane, Australia (2012)'
  id: totrans-9018
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Riedmiller, M., Lange, S., Voigtländer, A.: 在实际应用中对原始视觉输入数据进行自主强化学习。在：2012年国际联合神经网络会议论文集，布里斯班，澳大利亚（2012年）'
- en: '[26] Riedmiller, M., Montemerlo, M., Dahlkamp, H.: Learning to Drive in 20
    Minutes.'
  id: totrans-9019
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Riedmiller, M., Montemerlo, M., Dahlkamp, H.: 学习在20分钟内驾驶。'
- en: 'In: Proceedings of the FBIT 2007 Conference, Jeju, Korea. Springer (2007) (Best
    Paper Award)'
  id: totrans-9020
  prefs: []
  type: TYPE_NORMAL
  zh: 在：2007年FBIT会议论文集，韩国济州岛。施普林格（2007年）（最佳论文奖）
- en: '[27] Sutton, R.S., Barto, A.G.: Reinforcement Learning. MIT Press, Cambridge
    (1998) [28] Sutton, R.S.: Generalization in reinforcement learning: Successful
    examples using sparse coarse coding. In: Touretzky, D.S., Mozer, M.C., Hasselmo,
    M.E. (eds.) Advances in Neural Information Processing Systems, vol. 8, pp. 1038–1044.
    MIT Press, Cambridge (1996)'
  id: totrans-9021
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Sutton, R.S., Barto, A.G.: 强化学习。麻省理工出版社，剑桥（1998年） [28] Sutton, R.S.: 强化学习中的泛化：使用稀疏粗编码的成功示例。在：Touretzky,
    D.S., Mozer, M.C., Hasselmo, M.E.（编辑）《神经信息处理系统进展》，第8卷，pp. 1038–1044。麻省理工出版社，剑桥（1996年）'
- en: '[29] Timmer, S., Riedmiller, M.: Fitted Q Iteration with CMACs. In: Proceedings
    of the IEEE International Symposium on Approximate Dynamic Programming and Reinforcement
    Learning (ADPRL 2007), Honolulu, USA (2007)'
  id: totrans-9022
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] Timmer, S., Riedmiller, M.: 使用CMAC的拟合Q迭代。在：2007年IEEE国际近似动态编程与强化学习研讨会（ADPRL
    2007）论文集，美国檀香山（2007年）'
- en: '[30] Watkins, C.J.: Learning from Delayed Rewards. Phd thesis, Cambridge University'
  id: totrans-9023
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] Watkins, C.J.: 从延迟奖励中学习。博士论文，剑桥大学'
- en: (1989)
  id: totrans-9024
  prefs: []
  type: TYPE_NORMAL
  zh: （1989年）
- en: '[31] Walsh, T.J., Nouri, A., Li, L., Littman, M.L.: Planning and Learning in
    Environments with Delayed Feedback. In: Kok, J.N., Koronacki, J., Lopez de Mantaras,
    R., Matwin, S., Mladenič, D., Skowron, A. (eds.) ECML 2007. LNCS (LNAI), vol.
    4701, pp. 442–453. Springer, Heidelberg (2007)'
  id: totrans-9025
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Walsh, T.J., Nouri, A., Li, L., Littman, M.L.：在具有延迟反馈的环境中进行规划和学习。在：Kok,
    J.N., Koronacki, J., Lopez de Mantaras, R., Matwin, S., Mladenič, D., Skowron,
    A.（编）ECML 2007。LNCS（LNAI），第4701卷，442–453页。施普林格出版社，海德堡（2007）'
