- en: '© The Author(s) 2023G. Paaß, S. GiesselbachFoundation Models for Natural Language
    ProcessingArtificial Intelligence: Foundations, Theory, and Algorithms[https://doi.org/10.1007/978-3-031-23190-2_7](https://doi.org/10.1007/978-3-031-23190-2_7)'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: © 作者(们) 2023 G. Paaß, S. Giesselbach基础模型在自然语言处理中的应用人工智能：基础、理论和算法[https://doi.org/10.1007/978-3-031-23190-2_7](https://doi.org/10.1007/978-3-031-23190-2_7)
- en: 7. Foundation Models for Speech, Images, Videos, and Control
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7. 基础模型在语音、图像、视频和控制中的应用
- en: Gerhard Paaß^([1](#Aff5)  ) and Sven Giesselbach^([1](#Aff5))(1)Knowledge Discovery
    Department, Team NLU, Fraunhofer Institute for Intelligent Analysis and Information
    Systems (IAIS), Sankt Augustin, Nordrhein-Westfalen, Germany
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Gerhard Paaß^([1](#Aff5)  ) 和 Sven Giesselbach^([1](#Aff5))(1)知识发现部门，NLU团队，弗劳恩霍夫智能分析和信息系统研究所（IAIS），圣奥古斯丁，北莱茵-威斯特法伦，德国
- en: Abstract
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Foundation Models are able to model not only tokens of natural language but
    also token elements of arbitrary sequences. For images, square image patches can
    be represented as tokens; for videos, we can define tubelets that span an image
    patch across multiple frames. Subsequently, the proven self-attention algorithms
    can be applied to these tokens. Most importantly, several modalities like text
    and images can be processed in the same sequence allowing, for instance, the generation
    of images from text and text descriptions from video. In addition, the models
    are scalable to very large networks and huge datasets. The following multimedia
    types are covered in the subsequent sections. Speech recognition and text-to-speech
    models describe the translation of spoken language into text and vice versa. Image
    processing has the task to interpret images, describe them by captions, and generate
    new images according to textual descriptions. Video interpretation aims at recognizing
    action in videos and describing them through text. Furthermore, new videos can
    be created according to a textual description. Dynamical system trajectories characterize
    sequential decision problems, which can be simulated and controlled. DNA and protein
    sequences can be analyzed with Foundation Models to predict the structure and
    properties of the corresponding molecules.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型不仅能够对自然语言的标记进行建模，还能对任意序列的标记元素进行建模。对于图像，正方形图像块可以表示为标记；对于视频，我们可以定义跨越多个帧的图像块管段。随后，经过验证的自注意力算法可以应用于这些标记。最重要的是，文本和图像等几种模态可以在同一序列中处理，例如，从文本生成图像，从视频生成文本描述。此外，这些模型可以扩展到非常大的网络和巨大的数据集。以下章节涵盖了以下多媒体类型。语音识别和文本生成语音模型描述了将口语语言转换为文本以及相反的过程。图像处理的任务是解释图像，通过标题描述它们，并根据文本描述生成新的图像。视频解释旨在识别视频中的动作并通过文本描述它们。此外，可以根据文本描述创建新的视频。动态系统轨迹表征了序列决策问题，这些问题可以模拟和控制。DNA和蛋白质序列可以使用基础模型进行分析，以预测相应分子的结构和性质。
- en: KeywordsSpeech recognitionText-to-speechImage captioningText-to-imageVideo interpretationRobot
    controlDNA
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词语音识别文本生成图像描述文本到图像视频解释机器人控制DNA
- en: Astonishing results of Foundation Models in natural language tasks have led
    the multimedia processing community to study their application to speech recognition
    and computer vision problems. Among the most important advantages of Foundation
    Models is that they can model long dependencies between elements of the input
    sequence and support parallel processing of the sequence in contrast to recurrent
    networks. Unlike convolutional networks, Foundation Models require minimal restrictions
    in the modeling of dependencies and are able to define maps between high-dimensional
    quantities. In addition, the simple design of Foundation Models allows simultaneous
    processing of multiple modalities (e.g., images, videos, text and speech) using
    similar processing blocks. Moreover, the models are scalable to very large networks
    and huge datasets. These strengths of Foundation Models have led to comprehensive
    advances on a number of multimedia tasks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型在自然语言任务中取得的惊人成果，使得多媒体处理社区开始研究它们在语音识别和计算机视觉问题中的应用。基础模型最重要的优点之一是它们可以建模输入序列元素之间的长依赖关系，并支持与循环网络相比的序列并行处理。与卷积网络不同，基础模型在建模依赖关系时需要最小的限制，并且能够定义高维量之间的映射。此外，基础模型的简单设计允许使用类似的处理块同时处理多个模态（例如，图像、视频、文本和语音）。此外，这些模型可以扩展到非常大的网络和巨大的数据集。基础模型的这些优势导致了多媒体任务上的全面进步。
- en: We will describe multimedia applications in five areas and we will review the
    currently best approaches, taking into account necessary resources, e.g. computation
    and memory effort.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在五个领域描述多媒体应用，并回顾目前最佳的方法，考虑到必要的资源，例如计算和内存努力。
- en: '*Speech* recognition and text-to-speech models (Sect. [7.1](#Sec1)).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语音*识别和语音合成模型（第[7.1](#Sec1)节）。'
- en: '*Image* description by text and generating images from text (Sect. [7.2](#Sec12)).'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过文本描述*图像*和从文本生成图像（第[7.2](#Sec12)节）。
- en: '*Video* interpretation and video generation (Sect. [7.3](#Sec23)).'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*视频*解释和视频生成（第[7.3](#Sec23)节）。'
- en: '*Dynamical system trajectories* describe sequential decision problems, which
    can be simulated and controlled (Sect. [7.4](#Sec30)).'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*动态系统轨迹*描述了顺序决策问题，可以对其进行模拟和控制（第[7.4](#Sec30)节）。'
- en: '*DNA and protein sequences* can be analyzed with Foundation Models to predict
    the structure and properties of the corresponding molecules (Sect. [7.5](#Sec35)).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用基础模型分析*DNA和蛋白质序列*以预测相应分子的结构和性质（第[7.5](#Sec35)节）。
- en: 'In addition, there are a number of applications, where several media types
    are processed simultaneously. There is a large list of more specialized media
    types, where multimodal PLMs have been used: tables [[25](#CR25)], text layout
    [[61](#CR61)], depth images [[119](#CR119)], scene graphs [[60](#CR60)], SQL [[18](#CR18)],
    sign language [[199](#CR199)], point cloud [[197](#CR197)], symbolic knowledge
    graph [[4](#CR4)], multimodal knowledge graph [[201](#CR201)], abstract syntax
    tree [[202](#CR202)], optical flow [[50](#CR50)], etc. Processing these media
    types with Foundation Models is similar to the approaches described in the following
    sections.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有许多应用，其中同时处理多种媒体类型。有一长串更专业的媒体类型列表，其中已经使用了多模态PLM：表格 [[25](#CR25)]，文本布局 [[61](#CR61)]，深度图像
    [[119](#CR119)]，场景图 [[60](#CR60)]，SQL [[18](#CR18)]，手语 [[199](#CR199)]，点云 [[197](#CR197)]，符号知识图
    [[4](#CR4)]，多模态知识图 [[201](#CR201)]，抽象语法树 [[202](#CR202)]，光流 [[50](#CR50)]，等等。使用基础模型处理这些媒体类型的方法与下文所述的方法类似。
- en: Due to the enormous number of different Foundation Models in the literature,
    we focus on representative models that have high performance at the time of writing.
    We outline the inner logic and main features of the methods, taking into account
    the resources required, e.g., computational and memory requirements. For standard
    PLMs, a link to descriptions in earlier chapters is provided. Xu et al. [[183](#CR183)]
    compiled a survey on multimodal learning with transformers. Under the heading
    “Available Implementations” we list links to available code and pre-trained models
    for that task. Good sources for code are the websites [https://​paperswithcode.​com/​](https://paperswithcode.com/),
    the NLP index [https://​index.​quantumstat.​com/​](https://index.quantumstat.com/),
    and GitHub [https://​github.​com/​github](https://github.com/github). Processing
    these media types with PLMs is similar to the approaches described in the following
    sections.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文献中存在大量不同的基础模型，我们专注于在撰写时具有高性能的代表性模型。我们概述了方法的内部逻辑和主要特征，考虑到所需的资源，例如计算和内存需求。对于标准PLM，提供了早期章节中的描述链接。Xu等人
    [[183](#CR183)] 编制了一份关于使用transformers的多模态学习的调查。在“可用实现”标题下，我们列出了该任务的可用代码和预训练模型链接。代码的好来源是网站
    [https://paperswithcode.com/](https://paperswithcode.com/)，NLP索引 [https://index.quantumstat.com/](https://index.quantumstat.com/)
    和GitHub [https://github.com/github](https://github.com/github)。使用PLM处理这些媒体类型的方法与下文所述的方法类似。
- en: 7.1 Speech Recognition and Generation
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 语音识别和生成
- en: Spoken language is the most efficient and natural type of communication between
    humans. Therefore, it is also a preferred type of interaction with computer systems.
    In the next sections we describe advanced models for automatic speech recognition
    and text-to-speech systems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 人类之间最有效和自然的方式是口语交流。因此，它也是与计算机系统交互的首选方式。在接下来的几节中，我们将描述自动语音识别和语音合成系统的先进模型。
- en: 7.1.1 Basics of Automatic Speech Recognition
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 自动语音识别基础
- en: '*Automatic speech recognition* (*ASR*) receives a speech input as an audio
    file and converts it into natural language text. Speech is strongly influenced
    by gender, social style, dialect, speaking style, and speed. Human speech and
    accents vary widely, and these differences in speech patterns are one of the major
    obstacles in developing an automatic speech recognition system. Another impediment
    to the development of an ASR is finding sufficient training collections to train
    the ASR model. Currently, training data is available for only a few of the approximately
    7000 world languages.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*自动语音识别* (*ASR*) 接收语音输入作为音频文件，并将其转换为自然语言文本。语音受到性别、社会风格、方言、说话方式和速度的强烈影响。人类语音和口音差异很大，这些语音模式上的差异是开发自动语音识别系统的主要障碍之一。另一个阻碍
    ASR 发展的障碍是找到足够的训练数据集来训练 ASR 模型。目前，只有大约 7000 种世界语言中的少数几种有可用的训练数据。'
- en: Since the advent of the computer in the 1950s, researchers started to develop
    speech recognition systems. In 1984, IBM introduced the first speech recognition
    system that could recognize about 5000 individual English words, and in 1993,
    a consumer ASR was offered. The predominant techniques were *n*-gram models, hidden
    Markov models, and neural networks [[102](#CR102)]. After 2010, speech recognition
    based on RNNs was widely used for virtual assistants like Apple’s Siri, Amazon
    Alexa, and Google Assistant. Meanwhile, ASR is in use on most smartphones to enter
    text by voice even without an Internet connection.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 20 世纪 50 年代计算机问世以来，研究人员开始开发语音识别系统。1984 年，IBM 推出了第一个能够识别约 5000 个单独英语单词的语音识别系统，1993
    年，推出了面向消费者的 ASR。主要技术包括 *n*-gram 模型、隐马尔可夫模型和神经网络 [[102](#CR102)]。2010 年后，基于 RNN
    的语音识别被广泛用于虚拟助手，如苹果的 Siri、亚马逊的 Alexa 和谷歌助手。同时，ASR 在大多数智能手机上使用，可以通过语音输入文本，即使没有互联网连接。
- en: The most important evaluation measure of ASR systems is the *word error rate*![$$\text{WER}=\frac
    {S+D+I}{N}$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq1.png)
    measuring the deviation from a ground truth text. Here *S* is the number of word
    substitutions, *D* is the number of deletions, and *I* is the number of insertions
    in the output as compared to the ground truth with *N* words.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ASR 系统最重要的评估指标是 *词错误率*![$$\text{WER}=\frac {S+D+I}{N}$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq1.png)，它衡量了与基准文本的偏差。其中
    *S* 是替换的单词数量，*D* 是删除的数量，*I* 是插入的数量，与包含 *N* 个单词的基准文本相比。
- en: Conventional ASR systems usually consist of independent parts, such as an acoustic
    model, a pronunciation model, and a language model. These parts are trained separately
    and then combined for inference. Usually, a pre-processing module is employed
    to reduce the signal-to-noise ratio in the audio recording. There are different
    filters and methods that can be applied to a sound signal to reduce the associated
    noise. In addition, the speaker may be recorded with several microphones, which
    can localize the speaker and drastically reduce background noise (beamforming)
    [[24](#CR24)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 传统 ASR 系统通常由独立的部分组成，如声学模型、发音模型和语言模型。这些部分分别训练，然后结合进行推理。通常，会使用预处理模块来降低音频记录中的信噪比。有不同类型的滤波器和方法可以应用于声音信号以减少相关的噪声。此外，说话者可能由多个麦克风录制，这可以定位说话者并极大地减少背景噪声（波束成形）[[24](#CR24)]。
- en: Subsequently, a feature extraction module has the task to generate features
    relevant for speech recognition, remove irrelevant information from the signal
    and reduce the input size. This often involves variants of Fourier transforms
    extracting the frequency of waveforms. Most commonly used feature extraction methods
    are *Mel Frequency Cepstral Coefficients* (MFCCs), discrete wavelet transform
    (DWT), and linear predictive coding (LPC) [[101](#CR101)]. An example is shown
    in Fig. [7.1](#Fig1).![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig1_HTML.png)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，特征提取模块的任务是生成与语音识别相关的特征，从信号中去除无关信息并减少输入大小。这通常涉及傅里叶变换的变体，提取波形频率。最常用的特征提取方法是
    *梅尔频率倒谱系数* (MFCCs)、离散小波变换 (DWT) 和线性预测编码 (LPC) [[101](#CR101)]。一个示例如图 [7.1](#Fig1)
    所示。![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig1_HTML.png)
- en: A heat map demonstrates the M F C C coefficients, frequency in kilo hertz, and
    amplitude versus time in seconds. The O S R us 0000010 8 k dot w a v displays
    fluctuations in amplitude, frequency and M F C C coefficients highlighted in colors.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 热图展示了 M F C C 系数、千赫兹频率和秒时间内的振幅与时间的关系。O S R us 0000010 8 k dot w a v 显示了振幅、频率和
    M F C C 系数的波动，并用颜色突出显示。
- en: Fig. 7.1
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1
- en: Audio signal (top) with the frequency extracted by Fourier transform (middle)
    and the corresponding MFCCs (bottom). Image credits in Table [A.​3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 音频信号（顶部）及其通过傅里叶变换提取的频率（中间）以及相应的MFCCs（底部）。图像版权信息见表[A.3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
- en: The final module is a classifier receiving a vector of fixed length characterizing
    the signal in the given time slot. It estimates the probability of output words
    or phonemes for the next time slot. Early classifiers could only handle a single
    speaker. New models were developed to recognize the speech utterances of multiple
    speakers. An example is an ASR system yielding a 5.1% word error rate (WER) on
    the switchboard test set [[181](#CR181)]. It consists of CNN models like ResNet
    and LACE and bidirectional LSTMs for modeling acoustics. A survey of prior systems
    is provided by Malik et al. [[101](#CR101)]. A survey of more recent ASR systems
    is given by Papastratis [[117](#CR117)], who discuss RNN, CNN and Transformer
    models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最终模块是一个分类器，它接收一个固定长度的向量，该向量表征给定时间槽内的信号。它估计下一个时间槽输出单词或音素的概率。早期的分类器只能处理单个说话者。开发了新的模型来识别多个说话者的语音。一个例子是一个ASR系统，在交换测试集上实现了5.1%的单词错误率（WER）[[181](#CR181)]。它由类似于ResNet和LACE的CNN模型以及用于建模声学的双向LSTM组成。Malik等人提供了一项先前系统的调查[[101](#CR101)]。Papastratis提供了一项更近期ASR系统的调查[[117](#CR117)]，讨论了RNN、CNN和Transformer模型。
- en: 7.1.2 Transformer-Based Speech Recognition
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 基于Transformer的语音识别
- en: PLMs based on self-attention are a good choice for sequence modeling because
    they are able to capture interactions over long distances and require less computational
    effort. An overview is given in Table [7.1](#Tab1). However, PLMs are less capable
    of extracting fine-grained local feature patterns. Therefore, combinations of
    PLMs and CNNs are often used for ASR. The currently best LSTM-based ASR system
    **ContextNet + NST** [[121](#CR121)] achieved an WER of 1.7% on LibriSpeech (clean).Table
    7.1
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 基于自注意力的PLM是序列建模的好选择，因为它们能够捕捉长距离的交互，并且需要的计算量更少。概述见表[7.1](#Tab1)。然而，PLM在提取细粒度局部特征模式方面能力较弱。因此，PLM和CNN的组合通常用于ASR。目前最好的基于LSTM的ASR系统**ContextNet
    + NST** [[121](#CR121)] 在LibriSpeech（清洁）上实现了1.7%的WER。表7.1
- en: Main speech recognition techniques
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 主要语音识别技术
- en: '| Model | Mechanism | Performance |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 机制 | 性能 |'
- en: '| --- | --- | --- |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| ContextNet + NST | Currently best LSTM-based ASR system | Librispeech WER
    1.7% |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| ContextNet + NST | 目前最好的基于LSTM的ASR系统 | Librispeech WER 1.7% |'
- en: '| Conformer | CNN + self-attention in transformer block, LSTM as language model
    | Librispeech WER 1.9% |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| Conformer | 在Transformer块中使用CNN和自注意力，LSTM作为语言模型 | Librispeech WER 1.9% |'
- en: '| wav2vec 2.0 | Encode speech by CNN, discretize input to transformer, predict
    masked input. Fine-tune for speech recognition | Librispeech WER 1.5% |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| wav2vec 2.0 | 通过CNN编码语音，将输入离散化到Transformer，预测掩码输入。针对语音识别进行微调 | Librispeech
    WER 1.5% |'
- en: '| Combined SSL | Conformer model + unsupervised wav2vec 2.0, SpecAugment to
    generate noisy training data | Librispeech WER 1.4% |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 结合SSL | Conformer模型 + 无监督wav2vec 2.0，SpecAugment生成噪声训练数据 | Librispeech WER
    1.4% |'
- en: '| SpeechStew | Similar to Combined SSL, trained on 7 datasets, fine-tuned for
    speech recognition | Librispeech WER 1.7% without Language model |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| SpeechStew | 与Combined SSL类似，在7个数据集上训练，针对语音识别微调 | Librispeech WER 1.7% 无语言模型
    |'
- en: The **Conformer** [[59](#CR59)] is a convolution-augmented Transformer. The
    Conformer integrates a convolutional module (Sect. [1.​7](528393_1_En_1_Chapter.xhtml#Sec7))
    and a self-attention module (Sect. [2.​3](528393_1_En_2_Chapter.xhtml#Sec19))
    as layers inside an encoder block. The convolution module contains a 1 × 1 pointwise
    convolution with an expansion factor of 2 projecting the number of channels with
    a *Gated Linear Unit* (GLU) activation layer, which allows the selection of features
    that are important for prediction. This is followed by a *1-D depthwise convolution*,
    which applies a single convolutional filter for each input channel. Subsequently,
    there is a batch normalization and then a *Swish* [[131](#CR131)] activation layer.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**Conformer** [[59](#CR59)] 是一个卷积增强的Transformer。Conformer将一个卷积模块（第[1.7节](528393_1_En_1_Chapter.xhtml#Sec7)）和一个自注意力模块（第[2.3节](528393_1_En_2_Chapter.xhtml#Sec19)）作为编码块内部的层。卷积模块包含一个1×1点卷积，其扩展因子为2，通过一个带有*门控线性单元*（GLU）激活层的通道数投影，该层允许选择对预测重要的特征。随后是一个*1-D深度卷积*，为每个输入通道应用单个卷积滤波器。随后是一个批量归一化，然后是一个*Swish*
    [[131](#CR131)] 激活层。'
- en: The resulting model with 17 conformer blocks has up to 118M parameters and is
    trained on the *LibriSpeech* [[116](#CR116)] dataset, which contains audiobooks
    spoken by different speakers. It gets a vector of 80 filterbank features (Fig.
    [7.1](#Fig1)) for each time slot of 10ms. The authors use SpecAugment [[120](#CR120)]
    masking varying parts of the input signal to regularize the model. In addition,
    they train a 3-layer LSTM language model on the LibriSpeech corpus predicting
    the next word. The output of the language model is combined with the transformer
    output to emphasize words which are syntactically and semantically correct. Together
    with the LM the Conformer achieves a WER of 1.9% on LibriSpeech (clean). Without
    LM the WER was 2.1%.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 具有高达 17 个 conformer 块的模型具有高达 118M 个参数，并在 *LibriSpeech* [[116](#CR116)] 数据集上进行训练，该数据集包含不同说话者朗读的有声读物。它为每个
    10ms 的时间槽获得 80 个滤波器组特征向量（图 [7.1](#Fig1)）。作者使用 SpecAugment [[120](#CR120)] 对输入信号的不同部分进行掩码，以正则化模型。此外，他们在
    LibriSpeech 语料库上训练了一个 3 层 LSTM 语言模型，预测下一个单词。语言模型的输出与 transformer 的输出相结合，以强调语法和语义上正确的单词。与
    LM 一起，Conformer 在 LibriSpeech（清洁）上实现了 1.9% 的 WER。没有 LM 时，WER 为 2.1%。
- en: The **S4** [[58](#CR58)] model is able to process long input sequences of up
    to 16k elements (Sect. [3.​2.​2](528393_1_En_3_Chapter.xhtml#Sec9)). It was applied
    to speech classification and was able to improve Sota to 98.3% while processing
    raw speech signals. This is an enormous error reduction compared to the prior
    Sota accuracy of 95.3%. It can be expected that this model will also lead to a
    considerable reduction of errors in other speech recognition tasks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**S4** [[58](#CR58)] 模型能够处理长达 16k 个元素的长时间输入序列（第 [3.2.2](528393_1_En_3_Chapter.xhtml#Sec9)
    节）。它被应用于语音分类，并能够将 Sota 提高到 98.3%，同时处理原始语音信号。与先前 Sota 准确率 95.3% 相比，这是一个巨大的错误减少。预计该模型也将导致其他语音识别任务中的错误显著减少。'
- en: 7.1.3 Self-supervised Learning for Speech Recognition
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 语音识别的自监督学习
- en: Self-supervised learning of speech has the potential to enhance speech recognition
    results with additional unlabeled data. It can be shown that self-training on
    a large set of unlabeled data leads to a strong improvement of models which achieve
    superior performance with relatively little fine-tuning data [[184](#CR184)].
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别的自监督学习有潜力通过额外的未标记数据来增强语音识别结果。可以证明，在大量未标记数据上进行的自训练会导致模型性能显著提高，而需要相对较少的微调数据
    [[184](#CR184)]。
- en: '**wav2vec 2.0** [[10](#CR10)] performs unsupervised learning on speech data
    without transcripts. Similar to the BERT model for text, it learns to predict
    masked sound “tokens”. wav2vec encodes raw speech audio by a multi-layer CNN yielding
    a latent representation of speech for every time slot. The continuous latent representation
    is discretized to tokens ***q***[*t*] with a quantization module. This discretization
    is a discontinuous operation and hinders gradient backpropagation.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**wav2vec 2.0** [[10](#CR10)] 在没有转录本的情况下对语音数据进行无监督学习。类似于用于文本的 BERT 模型，它学习预测掩码声音“标记”。wav2vec
    通过多层 CNN 对原始语音音频进行编码，为每个时间槽生成语音的潜在表示。连续的潜在表示通过量化模块离散化为标记 ***q***[*t*]。这种离散化是一个不连续的操作，阻碍了梯度反向传播。'
- en: One solution is to use an interpolation between the discrete result of sampling
    and the probability distribution. This can be achieved with the *Gumbel-Softmax
    distribution* [[75](#CR75)]. To sample a discrete distribution with probabilities
    *p*[1], …, *p*[*k*] we can draw a random uniform variable *U* ∼uniform(0, 1) and
    compute *Z* = onehot(max[*i*]*p*[1] + ⋯*p*[*i*−1] ≤ *U*), where *i* = 1, …, *k*
    is the discrete index, and onehot(*j*) generates a vector of zeros with a one
    at position *j*. This sampling is not differentiable because of the max function.
    An alternative formula is![$$\displaystyle \begin{aligned} Z = \text{onehot}(\text{argmax}_i
    (G_i + \log(p_i))), \end{aligned} $$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_Equ1.png)(7.1)where
    *G*[*i*] ∼Gumbel(0, 1) are i.i.d. samples drawn from the standard Gumbel distribution.
    This refactors the sampling of *Z* into a deterministic function of the parameters
    and some independent noise with a fixed distribution. Now a softmax function can
    be used as a differential approximation of argmax:![$$\displaystyle \begin{aligned}
    y_i = \frac{\exp((G_i + \log p_i)/\tau)}{\sum_j \exp((G_j + \log p_j)/\tau)}.
    \end{aligned} $$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_Equ2.png)(7.2)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是在采样结果的离散值和概率分布之间进行插值。这可以通过*Gumbel-Softmax分布*[[75](#CR75)]来实现。为了从概率*p*[1]，…，*p*[*k*]采样一个离散分布，我们可以抽取一个随机均匀变量*U*∼uniform(0,
    1)，并计算*Z*=onehot(max[*i*]*p*[1]+⋯*p*[*i*−1]≤*U*)，其中*i*=1, …, *k*是离散索引，onehot(*j*)生成一个在位置*j*处为1的零向量。由于max函数，这种采样不可微分。一个替代公式是![公式](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_Equ1.png)(7.1)，其中*G*[*i*]∼Gumbel(0,
    1)是从标准Gumbel分布中抽取的独立同分布样本。这把*Z*的采样重构为参数的确定性函数和一些具有固定分布的独立噪声。现在可以使用softmax函数作为argmax的微分近似：![公式](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_Equ2.png)(7.2)。
- en: '*τ* is the temperature parameter that controls how closely the new samples
    approximate the discrete vectors. This approximation is used during training and
    the discretized onehot vectors are computed during evaluation. wav2vec computes
    discrete vectors ***q***[*t*] by this approach.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*τ*是温度参数，它控制新样本与离散向量之间的近似程度。这种近似在训练期间使用，在评估期间计算离散的onehot向量。wav2vec通过这种方法计算离散向量***q***[*t*]。'
- en: The ***q***[*t*] representations of 10 randomly sampled consecutive time steps
    are masked and have to be reconstructed by a Transformer similar to BERT. The
    self-attention captures dependencies over the entire sequence of latent representations.
    This model was pre-trained on more than 1000h of labeled and unlabeled speech
    data. The pre-trained model is fine-tuned for speech recognition by adding a randomly
    initialized linear projection on top of the context network into *C* classes,
    which were the characters as well as a word boundary marker. To accommodate characters
    spanning several time slots the *connectionist temporal classification* (CTC)
    loss [[57](#CR57)] was employed. The fine-tuning used 5h of audio data annotated
    with phonemes. On LibriSpeech the authors achieve a WER of 2.1%. A similar model
    with 300M parameters using 53k hours of unlabeled data for wave2vec and 10m of
    labeled data for fine-tuning achieves a WER of 3.0% on LibriSpeech [[184](#CR184)].
    Training on all data decreases WER to 1.5%.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 10个随机抽取的连续时间步的***q***[*t*]表示被掩码，需要通过类似于BERT的Transformer进行重建。自注意力机制捕捉了整个潜在表示序列的依赖关系。该模型在超过1000小时的标注和无标注语音数据上进行了预训练。预训练模型通过在上下文网络之上添加一个随机初始化的线性投影到*C*个类别（包括字符以及单词边界标记）进行微调，以适应跨越多个时间槽的字符。为了适应这种情况，使用了*连接主义时序分类*（CTC）损失[[57](#CR57)]。微调使用了5小时的带有音素标注的音频数据。在LibriSpeech上，作者实现了2.1%的词错误率（WER）。一个具有300M参数的类似模型，使用53k小时的未标注数据用于wave2vec和1000万小时的标注数据用于微调，在LibriSpeech上实现了3.0%的词错误率[[184](#CR184)]。在所有数据上训练可以将词错误率降低到1.5%。
- en: '**Combined SSL** [[196](#CR196)] combines wave2vec unsupervised pre-training
    with the Conformer. The ASR network is a sequence ‘translator’ consisting of a
    Conformer encoder with up to 1B parameters and a multilayer LSTM decoder. In addition,
    the authors use Noisy Student Training (NST), where a teacher model is employed
    to generate transcripts for the unlabeled data via inference on audio. The teacher-labeled
    data, after filtering and balancing, are then used to train the next generation
    ASR model. On LibriSpeech the model achieves Sota with 1.4% WER.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**联合SSL** [[196](#CR196)] 结合了 wave2vec 无监督预训练和 Conformer。ASR 网络是一个序列‘翻译器’，由一个最多包含
    1B 参数的 Conformer 编码器和多层 LSTM 解码器组成。此外，作者使用了 Noisy Student 训练 (NST)，其中使用一个教师模型通过音频推理生成未标记数据的转录。经过过滤和平衡的教师标记数据随后用于训练下一代
    ASR 模型。在 LibriSpeech 上，该模型实现了 1.4% 的 WER 的 Sota。'
- en: '**w2v-BERT** [[31](#CR31)] on the one hand performs contrastive learning discretizing
    continuous speech signals into a finite set of discriminative speech tokens. On
    the other hand, the model learns contextualized speech representations by solving
    a masked prediction task with the discretized tokens as input. During pre-training
    both tasks are simultaneously optimized in an end-to-end fashion. During fine-tuning
    the output of the pre-trained w2v-BERT model with 1B parameters is aggregated
    by a LSTM decoder. On the Librispeech benchmark it has a similar WER of 1.4% as
    the leading system and on the Librispeech benchmark test-other the model achieves
    a Sota of 2.5% WER. In addition, the model with 600M parameters was fine-tuned
    on a voice search task that allows users to use Google Search by speaking on a
    mobile phone or computer. It consists of voice snippets with an average duration
    of 5.5sec. The model was able to decrease errors by about 30% to 6.2\. **SpeechStew**
    [[21](#CR21)] uses the Conformer 1B with wav2vec pre-training. It is pre-trained
    on 7 available speech recognition datasets without any domain-dependent re-balancing
    or re-weighting. Without a language model it achieves a WER of 1.7% on LibriSpeech.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**w2v-BERT** [[31](#CR31)] 一方面通过对比学习将连续语音信号离散化为有限集的判别性语音标记。另一方面，该模型通过解决一个带有离散标记作为输入的掩码预测任务来学习上下文化的语音表示。在预训练期间，这两个任务以端到端的方式同时优化。在微调期间，使用
    LSTM 解码器聚合了预训练的 w2v-BERT 模型（1B 参数）的输出。在 Librispeech 基准测试中，它与领先系统的 WER 相似，为 1.4%。在
    Librispeech 基准测试的 test-other 中，该模型实现了 2.5% 的 WER 的 Sota。此外，具有 600M 参数的模型在语音搜索任务上进行了微调，允许用户通过在手机或计算机上说话来使用
    Google 搜索。它由平均持续时间为 5.5 秒的语音片段组成。该模型能够将错误率降低约 30%，达到 6.2。**SpeechStew** [[21](#CR21)]
    使用了带有 wav2vec 预训练的 Conformer 1B。它在 7 个可用的语音识别数据集上进行了预训练，没有任何领域相关的重新平衡或重新加权。在没有语言模型的情况下，它在
    LibriSpeech 上实现了 1.7% 的 WER。'
- en: '**TERA** [[98](#CR98)] is a self-supervised speech model using a multi-target
    auxiliary task to pre-train a transformer encoder on a large training set of unlabeled
    speech. The input can be any acoustic features, such as MFCC. The model learns
    by reconstructing acoustic frames from modified samples which were randomly changed
    with respect to three properties: Time alteration requires the reconstruction
    from corrupted blocks of time steps. Channel alteration has to restore the signal
    from missing blocks of frequency channels. Magnitude alteration involves the regeneration
    of altered feature magnitudes. By reconstructing these data changes, the model
    learns a better contextualized representation. The time alteration width is set
    to 85 ms of speech, which is about the average phoneme duration. The largest model
    similar to BERT has 170M parameters. The model has strong results for phone classification,
    speaker recognition, and speech recognition, e.g. on the TIMIT benchmark with
    14.5% phone error rate (PER).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**TERA** [[98](#CR98)] 是一个使用多目标辅助任务进行预训练的自监督语音模型，在大量未标记语音的大型训练集上预训练一个 transformer
    编码器。输入可以是任何声学特征，例如 MFCC。该模型通过从修改后的样本中重建声学帧来学习，这些样本在三个属性上进行了随机改变：时间改变需要从损坏的时间步块中重建。通道改变需要从缺失的频率通道块中恢复信号。幅度改变涉及改变特征幅度的再生。通过重建这些数据变化，模型学习了一个更好的上下文表示。时间改变宽度设置为
    85 毫秒的语音，大约是平均音素持续时间。与 BERT 相似的最大模型有 170M 参数。该模型在电话分类、说话人识别和语音识别方面具有强大的结果，例如在
    TIMIT 基准测试中，电话错误率 (PER) 为 14.5%。'
- en: In a comprehensive analysis, Zhang et al. [[195](#CR195)] evaluate the benefit
    of self-supervised pre-training for ASR. They employ Conformer models with 600M
    to 8B parameters pre-trained and self-trained on extremely large and diverse unlabeled
    datasets containing thousands to a million hours of audio (*BigSSL*). Using only
    3% of the labeled data they obtain comparable results to the Sota of the Voice
    Search benchmark. On eight ASR benchmarks they are able to match or improve Sota
    after pre-training. On five non-ASR task such as language identification and emotion
    detection, they can improve Sota. For large datasets, the gains from pre-training
    are smaller but still significant.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在一项全面的分析中，张等人 [[195](#CR195)] 评估了自监督预训练对ASR的益处。他们使用600M到8B参数的Conformer模型进行预训练和自训练，这些模型在包含数千到数百万小时音频的极其大型和多样化的未标记数据集上进行了训练
    (*BigSSL*)。他们仅使用3%的标记数据就获得了与语音搜索基准的Sota相当的结果。在八个ASR基准测试中，他们在预训练后能够匹配或提高Sota。在语言识别和情感检测等五个非ASR任务上，他们能够提高Sota。对于大型数据集，预训练的收益较小但仍然显著。
- en: Many applications benefit from understanding not only words but also other information,
    such as a person’s emotion during an utterance, whether the speaker is wearing
    a mask, or whether the speech is synthetic. Shor [[156](#CR156)] presents a large-scale,
    conformer-based architecture with more than 600M parameters that can be fine-tuned
    to detect these additional features and delivers Sota performance.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用受益于不仅理解词语，还包括其他信息，例如说话者在说话时的情绪、说话者是否戴着口罩，或者语音是否为合成语音。Shor [[156](#CR156)]
    提出了一种基于Conformer的大规模架构，拥有超过600M的参数，可以微调以检测这些附加特征，并实现了Sota性能。
- en: Available Implementations
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: 'Conformer: [https://​github.​com/​PaddlePaddle/​PaddleSpeech](https://github.com/PaddlePaddle/PaddleSpeech)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Conformer: [https://github.com/PaddlePaddle/PaddleSpeech](https://github.com/PaddlePaddle/PaddleSpeech)'
- en: 'wav2vec: [https://​github.​com/​facebookresearch​/​fairseq](https://github.com/facebookresearch/fairseq)
    sequence modeling toolkit for translation, summarization, language modeling and
    other text generation tasks.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'wav2vec: [https://github.com/facebookresearch/fairseq](https://github.com/facebookresearch/fairseq)
    翻译、摘要、语言建模和其他文本生成任务的序列建模工具包。'
- en: 'Tera: [https://​github.​com/​s3prl/​s3prl](https://github.com/s3prl/s3prl)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tera: [https://github.com/s3prl/s3prl](https://github.com/s3prl/s3prl)'
- en: 'Hugging Face speech recognition: [https://​huggingface.​co/​models?​pipeline_​tag=​automatic-speech-recognition](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hugging Face语音识别: [https://huggingface.co/models?pipeline_tag=automatic-speech-recognition](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition)'
- en: 'TensorFlow SST: [https://​tfhub.​dev/​s?​module-type=​audio-stt](https://tfhub.dev/s?module-type=audio-stt)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'TensorFlow SST: [https://tfhub.dev/s?module-type=audio-stt](https://tfhub.dev/s?module-type=audio-stt)'
- en: 7.1.4 Text-to-Speech
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.4 文本到语音
- en: Speech synthesis is about generating speech from another modality like text,
    lip movements, etc. A *Text-to-Speech* (*TTS*) system aims to convert natural
    language text into speech. *Mean Opinion Score* (*MOS*) is the most frequently
    used method to evaluate the quality of the generated speech. MOS is defined as
    the arithmetic mean over single ratings performed by human raters for a given
    stimulus in a subjective quality evaluation test. MOS has a range from 0 to 5,
    where real human speech is between 4.5 and 4.8\. A comprehensive and up-to-date
    survey of TTS systems is provided by Tan et al. [[163](#CR163)].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 语音合成是指从文本、唇语等另一种模态生成语音。*文本到语音* (*TTS*) 系统旨在将自然语言文本转换为语音。*平均意见评分* (*MOS*) 是评估生成语音质量最常用的方法。MOS定义为在主观质量评估测试中对给定刺激进行的单个评分的人评数的算术平均值。MOS的范围从0到5，其中真实人类语音在4.5到4.8之间。Tan等人
    [[163](#CR163)] 提供了TTS系统的全面和最新的调查。
- en: While earlier TTS systems simply concatenated prerecorded speech segments, modern
    systems perform a complete synthesis of speech. **WaveNet** [[114](#CR114)] was
    the first model that successfully modeled the raw waveform of the audio signal
    instead of the acoustic features. It is able to generate new speech-like waveforms
    at 16,000 samples per second. WaveNet in its core is an autoregressive model consisting
    of dilated convolutions where each sample depends on the previous ones. In each
    layer the number of included time steps is doubled. WaveNet was able to increase
    the MOS-value from 3.86 to 4.21\. *Fast WaveNet* was able to reduce the quadratic
    time complexity to linear complexity by caching previous calculations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然早期的TTS系统只是简单地将预先录制的语音片段连接起来，但现代系统执行完整的语音合成。**WaveNet** [[114](#CR114)] 是第一个成功模拟音频信号原始波形而不是声学特征的模型。它能够以每秒16,000个样本的速度生成新的类似语音的波形。WaveNet的核心是一个自回归模型，由扩张卷积组成，其中每个样本依赖于前一个样本。在每一层中，包含的时间步数翻倍。WaveNet能够将MOS值从3.86提高到4.21。*Fast
    WaveNet* 通过缓存先前计算将二次时间复杂度降低到线性复杂度。
- en: '**Tacotron 2** is a neural network architecture for speech synthesis directly
    from text. It consists of a recurrent LSTM sequence-to-sequence feature prediction
    network with attention, which predicts a sequence of mel spectrogram frames from
    an input character sequence and a modified version of WaveNet, which generates
    time-domain waveform samples conditioned on the predicted mel spectrogram frames.
    Tacotron 2 achieved an impressive MOS of 4.53.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tacotron 2** 是一种直接从文本进行语音合成的神经网络架构。它由一个具有注意力的循环LSTM序列到序列特征预测网络组成，该网络从输入字符序列预测一系列梅尔频谱图帧，以及WaveNet的修改版本，该版本根据预测的梅尔频谱图帧生成时间域波形样本。Tacotron
    2实现了令人印象深刻的MOS值4.53。'
- en: 'As TTS performs sequence processing similar to NLP, it is only natural that
    PLMs are also used in this area. Transformer-based models aim to mitigate two
    problems of previous TTS methods such as Tacotron 2: their high computational
    cost for training and inference, and the difficulty of modeling long dependencies
    with LSTMs.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TTS执行与NLP类似的序列处理，因此自然地，PLMs也被用于这个领域。基于Transformer的模型旨在缓解先前TTS方法（如Tacotron
    2）的两个问题：它们在训练和推理中的高计算成本，以及使用LSTMs建模长依赖关系的困难。
- en: '**Transformer TTS** [[94](#CR94)] adapts the original transformer encoder-decoder
    [[168](#CR168)] to speech synthesis. The encoder receives phonemes as input, which
    are adapted by an encoder pre-net consisting of a CNN and a fully connected layer.
    The standard transformer encoder outputs contextual phoneme embeddings (Fig. [7.2](#Fig2)).
    The decoder receives mel frames as input, which are converted by a decoder pre-net
    with two fully connected layers to generate appropriate embeddings. The standard
    decoder generates mel frames output embeddings. These are further processed by
    two different linear projections to predict the mel spectrogram and the stop token
    respectively. A 5-layer CNN produces a residual to refine the reconstruction of
    mel spectrogram. A WaveNet vocoder generates the final audio output. Both the
    encoder and decoder of the Transformer consists of 6 layers with 8 heads. The
    model is about 4.25 times faster than Tacotron 2 and achieves a MOS of 4.39 close
    to human quality.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig2_HTML.png)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer TTS** [[94](#CR94)] 将原始的transformer编码器-解码器 [[168](#CR168)] 适配到语音合成。编码器接收音素作为输入，这些音素通过由CNN和全连接层组成的编码器预网络进行适配。标准的transformer编码器输出上下文音素嵌入（图[7.2](#Fig2)）。解码器接收梅尔帧作为输入，这些输入通过具有两个全连接层的解码器预网络转换，以生成适当的嵌入。标准的解码器生成梅尔帧输出嵌入。这些嵌入随后通过两个不同的线性投影进一步处理，分别预测梅尔频谱图和停止标记。一个5层的CNN产生一个残差以细化梅尔频谱图的重建。WaveNet语音合成器生成最终的音频输出。Transformer的编码器和解码器都由6层和8个头组成。该模型比Tacotron
    2快约4.25倍，并实现了接近人类质量的MOS值4.39。![图7.2](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig2_HTML.png)'
- en: A flow diagram illustrates the predicted mel spectrogram, 5 layer C N N, linear
    projection, 6 layers with 8 heads, scaled positional embeddings 3 layer C N N,
    and rules. From bottom to top it has text to phonem converter, encoder pre net,
    decoder pre net, post net and stop token.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 流程图说明了预测的梅尔频谱图、5层CNN、线性投影、6层带有8个头的网络、缩放位置嵌入3层CNN和规则。从下到上依次是文本到音素转换器、编码器预网络、解码器预网络、后网络和停止标记。
- en: Fig. 7.2
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2
- en: Speech synthesis with the transformer TTS. The encoder as well as the decoder
    have 6 layers with 8 attention heads and residual connections. The resulting mel
    spectrogram is transformed into the final audio output by a WaveNet vocoder [[94](#CR94)].
    Image credits in Table [A.​3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Transformer TTS进行语音合成。编码器和解码器均包含6层，每层有8个注意力头和残差连接。生成的梅尔频谱图通过WaveNet语音合成器转换为最终的音频输出
    [[94](#CR94)]。图像归功于表[A.3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)。
- en: '**FastSpeech 2** [[138](#CR138)] tackles the problem that an input text can
    correspond to multiple possible speech sequences due to variations in speech,
    such as pitch, duration, sound volume and prosody. It encodes the input phonemes
    by a transformer encoder to generate embeddings. Then a variance adaptor adds
    different variance information such as duration, pitch and energy into the hidden
    sequence. Finally, the mel-spectrogram decoder converts the adapted hidden sequence
    into mel-spectrogram sequence in parallel. Both the encoder as well as the mel-spectrogram
    decoder have layers containing transformer blocks and 1D-convolutions. The variance
    adaptor predicts not only the duration, but also pitch and energy, using layers
    with 1D convolutions, feedforward layers, and layer normalization with dropout
    for regularization.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**FastSpeech 2** [[138](#CR138)] 解决了由于语音的变化（如音调、时长、音量和韵律）而导致的输入文本可以对应多个可能的语音序列的问题。它通过Transformer编码器对输入音素进行编码以生成嵌入。然后，方差适配器将诸如时长、音调和能量等不同的方差信息添加到隐藏序列中。最后，梅尔频谱图解码器将适配后的隐藏序列并行转换为梅尔频谱图序列。编码器和梅尔频谱图解码器都包含包含Transformer块和1D卷积的层。方差适配器使用包含1D卷积、前馈层和具有dropout的正则化的层来预测时长、音调和能量。'
- en: The variant *Fastspeech 2s* directly generates waveform from text without cascaded
    mel-spectrogram generation (acoustic model) and waveform generation (for example
    a vocoder, like wav2vec). The final waveform decoder consist of gated activations
    as well as different types of 1d-convolutions and dilated 1d-convolutions to cover
    a wider time range. The authors employ adversarial training in the waveform decoder
    to force it to implicitly recover the phase information by itself.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 变体 *Fastspeech 2s* 直接从文本生成波形，而不需要级联梅尔频谱图生成（声学模型）和波形生成（例如wav2vec这样的语音合成器）。最终的波形解码器由门控激活以及不同类型的1D卷积和扩张1D卷积组成，以覆盖更宽的时间范围。作者在波形解码器中采用对抗性训练，迫使它自行隐式恢复相位信息。
- en: 'In their experiments the authors determine the following MOS-values: Tacotron
    2: 3.70, Transformer TTS: 3.72, FastSpeech 2: 3.83, FastSpeech 2s: 3.71, and human
    speech: 4.30\. Note that the difference to human speech is mainly caused by the
    vocoder. In addition, FastSpeech 2 and FastSpeech 2s are about 50 times faster
    than Transformer TTS at inference time.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的实验中，作者确定了以下MOS值：Tacotron 2：3.70，Transformer TTS：3.72，FastSpeech 2：3.83，FastSpeech
    2s：3.71，以及人类语音：4.30。请注意，与人类语音的差异主要是由语音合成器引起的。此外，FastSpeech 2和FastSpeech 2s在推理时间上比Transformer
    TTS快约50倍。
- en: '**AdaSpeech 2** [[186](#CR186)] adapts a TTS system to a target speaker. Only
    sound recordings of the target speaker without text transcription are required.
    The authors apply a mel-spectrogram encoder to a well-trained TTS model to conduct
    speech reconstruction, and at the same time constrain the output sequence of the
    mel-spectrogram encoder to be close to that of the original phoneme encoder. The
    mel encoder also consists of 4 feed-forward Transformer blocks. Note that the
    original system does not need to be retrained, only the mel encoder. During the
    fine-tuning to the target speaker, the mel decoder parameters are adapted. The
    model achieves on-par MOS voice quality with the transcribed TTS adaptation.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**AdaSpeech 2** [[186](#CR186)] 将TTS系统适配到目标说话人。只需要目标说话人的声音录音，无需文本转录。作者将梅尔频谱图编码器应用于一个经过良好训练的TTS模型以进行语音重建，同时约束梅尔频谱图编码器的输出序列接近原始音素编码器的输出。梅尔编码器也由4个前馈Transformer块组成。请注意，原始系统无需重新训练，只需梅尔编码器。在针对目标说话人的微调过程中，梅尔解码器的参数被适配。该模型在转录TTS适配方面实现了与转录TTS相当的平均意见分数（MOS）语音质量。'
- en: Recently Amazon has announced that Alexa will be able to mimic the voices of
    other persons [[17](#CR17)]. To “make memories last” Alexa could, for instance,
    tell stories and play music using the voice of the deceased grandmother. Amazon
    notes, that it would take only about a minute of audio recording to imitate a
    voice.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最近亚马逊宣布，Alexa 将能够模仿其他人的声音 [[17](#CR17)]。为了“让记忆持久”，例如，Alexa 可以用已故祖母的声音讲故事和播放音乐。亚马逊指出，模仿一个声音只需要大约一分钟的音频录音。
- en: Available Implementations
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: 'Tacotron 2: [https://​github.​com/​NVIDIA/​tacotron2](https://github.com/NVIDIA/tacotron2)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tacotron 2: [https://github.com/NVIDIA/tacotron2](https://github.com/NVIDIA/tacotron2)'
- en: 'TransformerTTS: [https://​github.​com/​as-ideas/​TransformerTTS](https://github.com/as-ideas/TransformerTTS)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'TransformerTTS: [https://github.com/as-ideas/TransformerTTS](https://github.com/as-ideas/TransformerTTS)'
- en: 'FastSpeech 2: [https://​github.​com/​ming024/​FastSpeech2](https://github.com/ming024/FastSpeech2)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'FastSpeech 2: [https://github.com/ming024/FastSpeech2](https://github.com/ming024/FastSpeech2)'
- en: 'AdaSpeech 2: [https://​github.​com/​rishikksh20/​AdaSpeech2](https://github.com/rishikksh20/AdaSpeech2)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AdaSpeech 2: [https://github.com/rishikksh20/AdaSpeech2](https://github.com/rishikksh20/AdaSpeech2)'
- en: 'Hugging Face TTS: [https://​huggingface.​co/​models?​pipeline_​tag=​text-to-speech](https://huggingface.co/models?pipeline_tag=text-to-speech)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hugging Face TTS: [https://huggingface.co/models?pipeline_tag=text-to-speech](https://huggingface.co/models?pipeline_tag=text-to-speech)'
- en: 'Mozilla TTS Text-to-Speech for all: [https://​github.​com/​mozilla/​TTS](https://github.com/mozilla/TTS)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mozilla TTS 文本到语音：所有人都可以使用 [https://github.com/mozilla/TTS](https://github.com/mozilla/TTS)
- en: 'TensorFlow TTS: [https://​tfhub.​dev/​s?​module-type=​audio-speech-synthesis](https://tfhub.dev/s?module-type=audio-speech-synthesis)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'TensorFlow TTS: [https://tfhub.dev/s?module-type=audio-speech-synthesis](https://tfhub.dev/s?module-type=audio-speech-synthesis)'
- en: 7.1.5 Speech-to-Speech Language Model
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.5 语音到语音语言模型
- en: '**GSLM** [[89](#CR89)] is a language model which receives raw speech audio
    as input and directly generate outputs. It can, for instance, be used to create
    a dialog system without intermediate text representation. Internally the model
    converts incoming raw speech to discrete pseudo-text units. As discretizers CPC
    [[113](#CR113)], wave2vec 2.0 [[10](#CR10)], and HuBERT [[68](#CR68)] were used
    to create embeddings of varying length (50, 100, 200). The selection of units
    is difficult, as there is no vocabulary of sound units, and sound units have variable
    length with no obvious segmentation. Similar to BERT, HuBERT is trained with a
    masked prediction task using masked continuous audio signals as inputs. In experiments
    HuBERT performed best in most cases, followed by CPC.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**GSLM** [[89](#CR89)] 是一个语言模型，它接收原始语音音频作为输入并直接生成输出。例如，它可以用来创建一个没有中间文本表示的对话系统。内部模型将传入的原始语音转换为离散的伪文本单元。为了创建不同长度的嵌入（50、100、200），使用了
    CPC [[113](#CR113)]、wave2vec 2.0 [[10](#CR10)] 和 HuBERT [[68](#CR68)] 作为离散化器。单元的选择很困难，因为没有声音单元的词汇表，而且声音单元长度可变，没有明显的分割。与
    BERT 类似，HuBERT 使用带掩码的预测任务进行训练，输入为带掩码的连续音频信号。在实验中，HuBERT 在大多数情况下表现最佳，其次是 CPC。'
- en: The autoregressive “unit-based” language model has 12 layers and is trained
    on samples with up to 3k units generated from the 6k hours *LibriLight speech
    data* [[139](#CR139)]. To generate speech from units a modified version of the
    Tacotron-2 model [[154](#CR154)] was employed, which takes pseudo-text units as
    input and outputs a log Mel spectrogram. To generate waveforms the pre-trained
    vocoder *WaveGlow* [[125](#CR125)] was used, which converts the log Mel spectrogram
    to speech.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归的“基于单元”的语言模型有 12 层，并在从 6k 小时 *LibriLight 语音数据* 生成的最多 3k 个单元的样本上进行训练 [[139](#CR139)]。为了从单元生成语音，使用了修改版的
    Tacotron-2 模型 [[154](#CR154)]，它以伪文本单元作为输入并输出对数梅尔频谱图。为了生成波形，使用了预训练的声码器 *WaveGlow*
    [[125](#CR125)]，它将对数梅尔频谱图转换为语音。
- en: In a first test the speech input was encoded into units, which were translated
    to speech. Here the intelligibility of the resulting speech is assessed by a human
    MOS opinion score. When trained on the *LJ Speech data* [[74](#CR74)] the unsupervised
    model achieved a MOS (Mean Opinion Score) score of 4.00, while the combination
    of an ASR and TTS system achieved a slightly better score of 4.04 [[89](#CR89)].
    When testing the full language model generation, the model achieved a MOS score
    of 4.01, while the combination of ASR and a language model yielded a score of
    3.91\. According to the authors, the generated speech sounds like English, has
    recognizable phonemes and words. Examples show that improvements are needed at
    the language and syntax level. For sound transcription 200 units were good, while
    for language modeling a smaller number of units seems to be better. It can be
    expected that the quality can be improved with additional training data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次测试中，语音输入被编码成单元，这些单元被转换成语音。在这里，通过人类MOS意见评分来评估所产生语音的可懂度。当在*LJ Speech数据*[[74](#CR74)]上训练时，无监督模型实现了MOS（平均意见评分）4.00的分数，而ASR和TTS系统的组合实现了略高的4.04分[[89](#CR89)]。在测试完整的语言模型生成时，模型实现了4.01的MOS分数，而ASR和语言模型的组合产生了3.91分的评分。根据作者的说法，生成的语音听起来像英语，具有可识别的音素和单词。示例表明，在语言和句法层面需要改进。对于声音转录，200个单元是好的，而对于语言建模，似乎更少的单元更好。可以预期，随着额外训练数据的增加，质量可以得到提高。
- en: 7.1.6 Music Generation
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.6 音乐生成
- en: Foundation Models can also be applied to other sequence data, e.g. music. On
    the one hand a music language model can be trained, which is able to generate
    new music corresponding to the training data. On the other hand, a model can generate
    music conditioned on external information, e.g. lyrics or video. Bilici [[14](#CR14)]
    provide a survey on recent music generation models.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型也可以应用于其他序列数据，例如音乐。一方面，可以训练一个音乐语言模型，它能够根据训练数据生成新的音乐。另一方面，模型可以根据外部信息生成音乐，例如歌词或视频。Bilici
    [[14](#CR14)]提供了一篇关于最近音乐生成模型的综述。
- en: A prominent approach to music generation is **MuseNet** [[123](#CR123)] which
    employs the Sparse Transformer, a variant of GPT-2\. It calculates attention patterns
    over a context of 4096 MIDI characters. To generate new compositions, one can
    select a composer and use the starting notes of a known piece. Then up to ten
    different instruments can be selected, and the system will generate a piece of
    music with the required characteristics. The ratings of experts are quite favorable.
    Similarly, the **Music Transformer** [[71](#CR71)] generates piano pieces. **Theme
    Transformer** [[155](#CR155)] receives a theme as input and is trained to include
    this theme multiple times in its generation result.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐生成的一个显著方法是**MuseNet**[[123](#CR123)]，它采用稀疏Transformer，这是GPT-2的一个变体。它在一个4096个MIDI字符的上下文中计算注意力模式。为了生成新的作品，可以选择一个作曲家并使用已知作品的起始音符。然后可以选择多达十种不同的乐器，系统将生成具有所需特性的音乐作品。专家的评分相当有利。同样，**音乐Transformer**[[71](#CR71)]生成钢琴曲。**主题Transformer**[[155](#CR155)]接收一个主题作为输入，并训练在生成结果中多次包含这个主题。
- en: '**Jukebox** [[36](#CR36)] adopts a multiscale vector quantizer variational
    autoencoder model (VQ-VAE) [[113](#CR113)] to compress raw audio to discrete codes.
    This is based on an autoregressive Transformer and works also for human voices.
    Three separate VQ-VAE models with different temporal resolutions are employed.
    The trained model can be conditioned on an artist and a genre to steer the musical
    and vocal style, and on unaligned lyrics to make the singing more controllable.
    The model is capable of generating pieces that are many minutes long, and with
    recognizable singing in natural-sounding voices. A number of samples are available
    [[35](#CR35)].'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**点唱机**[[36](#CR36)]采用多尺度向量量化变分自编码器模型（VQ-VAE）[[113](#CR113)]将原始音频压缩成离散代码。这是基于自回归Transformer的，也适用于人声。使用了三个具有不同时间分辨率的独立VQ-VAE模型。训练好的模型可以根据艺术家和流派来引导音乐和声乐风格，以及根据未对齐的歌词来使唱歌更可控。该模型能够生成长达数分钟的作品，并且以自然的声音演唱。有许多样本可供参考[[35](#CR35)]。'
- en: '**CMT** [[38](#CR38)] generates background music for a specific video. It aims
    to match the rhythm, timing, and movement speed of the video. CMT extracts these
    features from the video and allows global control of the music genre and instruments.
    The model does not require paired video and music training data. Experiments demonstrate
    that the generated background music has achieved satisfactory compatibility with
    the input videos, and at the same time, impressive music quality.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**CMT** [[38](#CR38)] 为特定视频生成背景音乐。它的目标是匹配视频的节奏、时序和运动速度。CMT从视频中提取这些特征，并允许全局控制音乐类型和乐器。该模型不需要配对的视频和音乐训练数据。实验表明，生成的背景音乐与输入视频的兼容性达到了令人满意的程度，同时音乐质量也非常出色。'
- en: Available Implementations
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: CMT Controllable Music Transformer [https://​github.​com/​wzk1015/​video-bgm-generation](https://github.com/wzk1015/video-bgm-generation)
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CMT 可控音乐变换器 [https://github.com/wzk1015/video-bgm-generation](https://github.com/wzk1015/video-bgm-generation)
- en: 'Jukebox: A Generative Model for Music [https://​github.​com/​openai/​jukebox](https://github.com/openai/jukebox)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jukebox：音乐生成器模型 [https://github.com/openai/jukebox](https://github.com/openai/jukebox)
- en: 7.1.7 Summary
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.7 摘要
- en: Speech recognition has shown an enormous progress in recent years and Foundation
    Models are now an established approach to this task. They are combined with CNN
    blocks and are able to capture interactions over long distances and reduce processing
    times. Similar to NLP, self-supervised learning has led to great performance gains.
    Instead of tokens, as in NLP, discrete sound representations are generated. A
    number of different models follow this scheme, and they are able to increase Sota
    on different benchmarks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别在近年来取得了巨大的进步，基础模型现在已成为这一任务的成熟方法。它们与卷积神经网络块相结合，能够捕捉长距离的交互并减少处理时间。与自然语言处理类似，自监督学习也带来了巨大的性能提升。与自然语言处理中的标记不同，这里生成的是离散的声音表示。许多不同的模型遵循这一方案，并且它们能够提高在不同基准上的Sota（State-of-the-Art）。
- en: The generation of speech from text has improved dramatically in recent years.
    WaveNet was the first model to generate speech-like waveforms at 16,000 samples
    per second. Transformers can be used to convert input phonemes to mel spectrograms,
    from which a vocoder can generate speech audio. There are variants like FastSpeech
    2s, which directly transform text to an audio signal. The output quality of the
    models is close to human speech. Some models are able to adapt their output to
    the voice of individual speakers. This is impressive, but also a major security
    problem if in this way false utterances are produced imitating a person’s voice.
    The recent S4 state-space model for long input sequences was able to reduce errors
    by 60% for classifying speech signals. It can be expected that this model will
    also lead to a considerable reduction of errors in other speech recognition tasks.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成语音在近年来有了显著改进。WaveNet 是第一个以每秒16,000个样本生成类似语音波形模型。Transformer 可以用于将输入音素转换为梅尔频谱图，从而通过声码器生成语音音频。还有一些变体，如FastSpeech
    2s，它们可以直接将文本转换为音频信号。这些模型输出质量接近人类语音。一些模型能够调整它们的输出以适应个别说话者的声音。这很令人印象深刻，但如果以这种方式产生模仿某人声音的虚假陈述，这也可能是一个重大的安全问题。最近的长输入序列S4状态空间模型在分类语音信号时能够将错误减少60%。可以预期，该模型也将导致其他语音识别任务中的错误显著减少。
- en: Speech recognition and text-to-speech can be integrated with other applications.
    SpeechBert [[30](#CR30)] is an end-to-end Speech Question Answering (SQA) model
    by encoding audio and text with a single Transformer encoder, which is pre-trained
    with MLM on speech and text corpora and fine-tuned on Question Answering. Live
    speech translations are generated on-the-fly in a smartphone and allow a seamless
    communication in a foreign language [[78](#CR78), [81](#CR81)]. And GSLM is a
    generative language model, which directly processes discretized sound tokens.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别和文本到语音可以与其他应用集成。SpeechBert [[30](#CR30)] 是一个端到端语音问答（SQA）模型，通过单个Transformer编码器对音频和文本进行编码，该编码器在语音和文本语料库上预训练了MLM（掩码语言模型），并在问答上进行微调。在智能手机上即时生成实时语音翻译，允许无缝进行外语交流
    [[78](#CR78)，[81](#CR81)]。GSLM 是一个生成语言模型，它直接处理离散的声音标记。
- en: Music generation is a related topic. Autoregressive PLMs, e.g. MuseNet or Music
    Transformer, can be used to generate music based on a pre-training with a large
    corpus. Here the composer style and the instrument may be selected. In addition,
    music can be conditioned on some input, e.g. lyric text for the Jukebox model
    or a video to compose background music.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐生成是一个相关的话题。自回归PLM，例如MuseNet或Music Transformer，可以根据与大型语料库的预训练生成音乐。在这里可以选择作曲家的风格和乐器。此外，音乐可以根据某些输入进行条件化，例如Jukebox模型的歌词文本或用于创作背景音乐的视频。
- en: 7.2 Image Processing and Generation
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 图像处理和生成
- en: 'The breakthrough of Foundation Models in NLP has generated tremendous interest
    in the computer vision community to adapt these models for vision and multi-modal
    learning tasks. Two factors are important for their success: self-attention and
    self-supervision. Self-attention layers generate representations that take into
    account the relationships between the tokens (text token and/or visual tokens).
    Self-supervision predicts masked or modified parts of data elements during training
    in large-scale datasets. It allows gaining enormous knowledge about the data without
    manually annotating it and assumes minimal inductive biases compared to other
    models like CNN and RNN. Comprehensive surveys on Foundation Models for vision
    and language applications are provided by Khan et al. [[84](#CR84)] and Du et
    al. [[43](#CR43)]. Hafiz et al. [[62](#CR62)] give an overview over attention
    mechanisms and Deep Learning for machine vision. There is a recent tutorial on
    vision and language research [[6](#CR6)]. The main features of the models discussed
    in this section are compiled in Table [7.2](#Tab2).Table 7.2'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理中的基础模型突破在计算机视觉社区中引发了极大的兴趣，以适应这些模型用于视觉和多模态学习任务。成功的关键因素有两个：自注意力机制和自监督。自注意力层生成考虑了标记（文本标记和/或视觉标记）之间关系的表示。自监督在训练大规模数据集时预测数据元素中被遮挡或修改的部分。它允许在不手动标注数据的情况下获得大量关于数据的知识，并且与CNN和RNN等其他模型相比，具有最小的归纳偏差。Khan等人
    [[84](#CR84)] 和Du等人 [[43](#CR43)] 提供了关于视觉和语言应用的基础模型的全面调查。Hafiz等人 [[62](#CR62)]
    对注意力机制和机器视觉的深度学习进行了概述。最近有一篇关于视觉和语言研究的教程 [[6](#CR6)]。本节讨论的模型的主要特性汇总在表[7.2](#Tab2)中。表7.2
- en: 'Main techniques to combine text and images.**Benchmarks:** VQA: COCO Visual
    Question Answering dataset (Sect. [7.2.5](#Sec17)) [[56](#CR56)]; img-gen: MS-COCO
    image generation benchmark with fine-tuning; img-gen-0: MS-COCO image generation
    benchmark zero-shot; ImageNet: ImageNet classification top1 accuracy; captions:
    MS-COCO image captioning benchmark; FID: Fréchet Inception Distance should be
    small (Sect. [7.2.6](#Sec18)) [[64](#CR64)]. Numbers in parentheses are parameter
    counts'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 结合文本和图像的主要技术。**基准测试：** VQA：COCO视觉问答数据集（Sect. [7.2.5](#Sec17)） [[56](#CR56) ];
    img-gen：MS-COCO图像生成基准测试，带有微调；img-gen-0：MS-COCO图像生成基准测试零样本；ImageNet：ImageNet分类top1准确率；标题：MS-COCO图像标题生成基准测试；FID：Fréchet
    Inception Distance应尽可能小（Sect. [7.2.6](#Sec18)） [[64](#CR64)]。括号中的数字是参数数量
- en: '| Model | Approach | Benchmark |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 基准 |'
- en: '| --- | --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Vision Transformer (ViT) Sect. [7.2.2](#Sec14) | Concatenate text tokens
    and image token generated from image patches. Process with a BERT autoencoder
    and perform classification (632M) | ImageNet Sota acc. 90.5% |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Vision Transformer (ViT) Sect. [7.2.2](#Sec14) | 将从图像补丁生成的文本标记和图像标记连接起来。使用BERT自动编码器进行处理并执行分类（632M）
    | ImageNet Sota准确率 90.5% |'
- en: '| CLIP Sect. [7.2.4](#Sec16) | Encode image with vision transformer and text
    with a GPT autoencoder. Maximize similarity of image and text embeddings, predict
    if they belong together |   |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| CLIP Sect. [7.2.4](#Sec16) | 使用视觉Transformer对图像进行编码，使用GPT自动编码器对文本进行编码。最大化图像和文本嵌入的相似度，预测它们是否属于一起
    |   |'
- en: '| VilBERT Sect. [7.2.5](#Sec17) | Extract bounding boxes with Faster R-CNN.
    Image regions and text are encoded by two BERT autoencoders and perform cross-attention.
    Fine-tuned to VQA | VQA Sota 70.9% |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| VilBERT Sect. [7.2.5](#Sec17) | 使用Faster R-CNN提取边界框。图像区域和文本由两个BERT自动编码器编码，并执行交叉注意力。针对VQA进行微调
    | VQA Sota 70.9% |'
- en: '| OSCAR Sect. [7.2.5](#Sec17) | Extract bounding boxes with Faster R-CNN. A
    BERT autoencoder associates region descriptions with text. Fine-tuned for 7 tasks,
    e.g. image captioning | captions Sota 41.7 Bleu-4 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| OSCAR Sect. [7.2.5](#Sec17) | 使用Faster R-CNN提取边界框。一个BERT自动编码器将区域描述与文本关联。针对7个任务进行微调，例如图像标题生成
    | 标题 Sota 41.7 Bleu-4 |'
- en: '| VinVL Sect. [7.2.5](#Sec17) | Uses ResNeXT model as region extractor and
    OSCAR. Fine-tuned for image captioning | captions 40.4 Bleu-4 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| VinVL Sect. [7.2.5](#Sec17) | 使用ResNeXT模型作为区域提取器，与OSCAR结合。针对图像标题生成进行微调 |
    标题 40.4 Bleu-4 |'
- en: '| DALL-E Sect. [7.2.6](#Sec18) | Text is encoded as tokens, image is transformed
    to image tokens by variational autoencoders (VAE). Uses GPT-3 (12B) to generate
    new image tokens | img-gen-0 17.9 FID |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| DALL-E章节[7.2.6](#Sec18) | 文本被编码为标记，图像通过变分自编码器（VAE）转换为图像标记。使用GPT-3（12B）生成新的图像标记
    | 图像生成-0 17.9 FID |'
- en: '| GLIDE Sect. [7.2.7](#Sec19) | Reverses diffusion which destroys an image.
    Generates image by small changes with U-Net model (3.8B) | img-gen-0 Sota 12.2
    FID |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GLIDE章节[7.2.7](#Sec19) | 反转扩散以破坏图像。通过U-Net模型（3.8B）进行微小变化生成图像 | 图像生成-0 Sota
    12.2 FID |'
- en: '| XMC-GAN Sect. [7.2.7](#Sec19) | GAN-based image generator, generator creates
    images, discriminator discriminates fake and real images | img-gen Sota 9.3 FID
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| XMC-GAN章节[7.2.7](#Sec19) | 基于GAN的图像生成器，生成器创建图像，判别器区分伪造和真实图像 | 图像生成Sota 9.3
    FID |'
- en: '| CogView Sect. [7.2.7](#Sec19) | Vector quantized VAE. GPT-model (4B) is trained
    with text tokens and quantized image tokens | img-gen Sota on blurred images |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| CogView章节[7.2.7](#Sec19) | 向量化VAE。GPT模型（4B）使用文本标记和量化图像标记进行训练 | 图像生成在模糊图像上Sota
    |'
- en: '| LAFITE Sect. [7.2.7](#Sec19) | Uses CLIP to transform text to image embeddings.
    Train to modulate layers of StyleGAN2 [[82](#CR82)] to generate images | img-gen
    Sota 8.1 FID img-gen-0 16.9 FID |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LAFITE章节[7.2.7](#Sec19) | 使用CLIP将文本转换为图像嵌入。训练以调节StyleGAN2的层[[82](#CR82)]以生成图像
    | 图像生成Sota 8.1 FID 图像生成-0 16.9 FID |'
- en: Table 7.2
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.2
- en: '| Model | Approach | Benchmark |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 基准 |'
- en: '| OFA Sect. [7.2.8](#Sec20) | Uses text, image tokens and objects with bounding
    boxes. Seq2seq model (472M) pre-trained to associate tokens and objects. Text
    instructions control 9 different tasks | img-gen Sota 10.5 FID captions Sota 43.5
    Bleu-4 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| OFA章节[7.2.8](#Sec20) | 使用文本、图像标记和带有边界框的对象。Seq2seq模型（472M）预训练以关联标记和对象。文本指令控制9个不同的任务
    | 图像生成Sota 10.5 FID 标题Sota 43.5 Bleu-4 |'
- en: '| DALL-E 2 Sect. [7.2.7](#Sec19) | Generate in image embedding from text by
    CLIP, transform to 1024 × 1024 image by diffusion decoder | img-gen-0 Sota 10.4
    FID |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| DALL-E 2章节[7.2.7](#Sec19) | 通过CLIP从文本生成图像嵌入，通过扩散解码器转换为1024×1024图像 | 图像生成-0
    Sota 10.4 FID |'
- en: '| Imagen Sect. [7.2.7](#Sec19) | Generate text embeddings by T5-XXL, generate
    image patches by diffusion model, upsampling to 1024 × 1024 by two superresolution
    diffusion models | img-gen-0 Sota 7.3 FID |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Imagen章节[7.2.7](#Sec19) | 通过T5-XXL生成文本嵌入，通过扩散模型生成图像块，通过两个超分辨率扩散模型上采样到1024×1024
    | 图像生成-0 Sota 7.3 FID |'
- en: '| Stable Diffusion Sect. [7.2.7](#Sec19) | Generate images using U-Net and
    diffusion | ImageNet conditional 3.6 FID |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Stable Diffusion章节[7.2.7](#Sec19) | 使用U-Net和扩散生成图像 | ImageNet条件3.6 FID |'
- en: 7.2.1 Basics of Image Processing
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 图像处理基础
- en: Image processing can solve a variety of tasks, as shown in Fig. [7.3](#Fig3).
    The main content of an image can be described by classifying the most important
    object in the image. More demanding is the identification and classification of
    relevant objects in an image. This also requires the description of the object
    positions by bounding boxes. Creating a caption for an image involves identifying
    the most important objects in the image, how they relate to each other, and describing
    them using a natural language sentence. Related to this is the retrieval of an
    image that corresponds to a caption. Visual question answering requires interpreting
    a question and analyzing the image to generate an answer in natural language.
    A variant is multimodal verification, where the truth of a statement about the
    image has to be assessed.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig3_HTML.png)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图像处理可以解决各种任务，如图[7.3](#Fig3)所示。图像的主要内容可以通过分类图像中最重要的对象来描述。更具有挑战性的是图像中相关对象的识别和分类。这也需要通过边界框描述对象位置。为图像创建标题涉及识别图像中最重要的对象，它们如何相互关联，并使用自然语言句子来描述它们。与此相关的是检索与标题对应的图像。视觉问答需要解释问题并分析图像以生成自然语言的答案。一种变体是多模态验证，其中需要对关于图像的陈述的真实性进行评估。![图7.3](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig3_HTML.png)
- en: A photo of a child wears a shirt and pants and stands on a grass lane and holds
    a bread. 2 crows stands beside the child. A chart on the right explains, visual
    question answering, object classification and identification, verification, caption
    based image retrieval, and automatic image captioning.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一张孩子的照片，孩子穿着衬衫和裤子，站在草地上，手里拿着面包。两只乌鸦站在孩子旁边。右边的图表解释了视觉问答、对象分类和识别、验证、基于标题的图像检索和自动图像标题。
- en: Fig. 7.3
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3
- en: Image analysis can be used to solve a number of different tasks. Depending on
    the task, the system receives a text (green) and an image as input and generates
    a text (blue) and an image as output. Image credits in Table [A.​3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分析可用于解决许多不同的任务。根据任务的不同，系统接收文本（绿色）和图像作为输入，并生成文本（蓝色）和图像作为输出。图像来源见表 [A.​3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
- en: Many tasks involve the creation of a new image. A prominent example is the generation
    of a completely new image according to a caption. Alternatively a missing image
    area can be filled in. A variant is to change the style of an image according
    to a caption, e.g. from a photo to a painting in the style of van Gogh. This can
    be also performed for a specific image region.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 许多任务涉及创建新的图像。一个突出的例子是根据标题生成一个全新的图像。或者，可以填充缺失的图像区域。另一种变体是根据标题更改图像的风格，例如将照片转换为梵高的画风。这也可以应用于特定的图像区域。
- en: An important aspect is the representation of images for transformers. Language
    models partition text into a sequence of tokens, which form the input of a transformer.
    The same approach is chosen for images, which are partitioned into small image
    patches. The contents of each patch can be represented by a vector, which forms
    the input of the transformer. The location of the patch is encoded by a position
    embedding, which is added to the input embedding.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器中图像表示的一个重要方面是将图像分割成小块。语言模型将文本分割成一系列标记，这些标记构成了变换器的输入。对于图像，也采用相同的方法，将其分割成小块。每个小块的内容可以用一个向量表示，这个向量构成了变换器的输入。小块的位置通过位置嵌入进行编码，该嵌入被添加到输入嵌入中。
- en: The embedding of an image patch can be simply a learnable linear transformation
    of its pixel values. Other transformations may be used, e.g. small CNN models
    or variational autoencoders (Sect. [1.​7](528393_1_En_1_Chapter.xhtml#Sec7)).
    To get more robust representations, the generated vectors are often discretized
    to get rid of local noise. In addition, text from a caption or region annotation
    can be used as input. As usual, this text is converted to tokens from a vocabulary.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图像块的嵌入可以简单地是其像素值的可学习线性变换。也可以使用其他变换，例如小的CNN模型或变分自编码器（见第 [1.​7](528393_1_En_1_Chapter.xhtml#Sec7)
    节）。为了获得更鲁棒的表现，生成的向量通常会被离散化以消除局部噪声。此外，可以从标题或区域注释中使用的文本作为输入。通常，此文本会被转换为词汇表中的标记。
- en: To model the interaction between image elements and text, different transformer
    architectures can be used (Table [7.2](#Tab2)). A *single stream architecture*
    concatenates all inputs and processes them with a single transformer. This allows
    to determine interactions between different input elements, but requires the handling
    of long sequences. Dual-stream or *multi-stream architectures* process different
    modalities or image resolutions by separate PLMs. In this case the input sequences
    are shorter. Various forms of interaction between the streams have been proposed
    (e.g. cross-attention). Later the outputs may be compared by similarity measures
    or combined by other PLMs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟图像元素与文本之间的交互，可以使用不同的变换器架构（见表 [7.2](#Tab2)）。*单流架构*将所有输入连接起来，并用单个变换器进行处理。这允许确定不同输入元素之间的交互，但需要处理长序列。双流或*多流架构*通过单独的PLM处理不同的模态或图像分辨率。在这种情况下，输入序列更短。已经提出了各种流之间的交互形式（例如，交叉注意力）。随后，可以通过相似度度量比较输出，或者通过其他PLM进行组合。
- en: The pre-training task for vision follows the pattern of the text transformer.
    *Masked language modeling* (MLM) masks a fraction of the input tokens and requires
    the model to predict the tokens from the context. If there are text and image
    tokens, the information in both modalities can be utilized for this task and the
    model learns the association between text and image elements. Similarly, image
    regions can be masked and reconstructed from the text and image context. In a
    classification task, the model can determine whether a caption correctly describes
    an image or is some random text. In this way, the correlation between text and
    images can be trained. Another goal is to learn a joint image and word representation
    in the same semantic space by pushing together the embeddings of matched image-text
    pairs, while pushing apart the non-matched pairs. For this image-to-text *contrastive
    loss*, the proximity of embeddings is measured by a scalar product between the
    embeddings.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉的预训练任务遵循文本Transformer的模式。*掩码语言建模*（MLM）掩码输入标记的一部分，并要求模型从上下文中预测标记。如果有文本和图像标记，可以利用这两种模态的信息来完成此任务，并且模型学习文本和图像元素之间的关联。同样，可以掩码图像区域，并从文本和图像上下文中重建。在分类任务中，模型可以确定一个标题是否正确描述了图像或是一些随机文本。这样，可以训练文本和图像之间的相关性。另一个目标是通过对匹配的图像-文本对嵌入进行拉近，而对非匹配对进行推远，在相同的语义空间中学习联合图像和词表示。为此，图像到文本的*对比损失*通过嵌入之间的标量积来衡量嵌入的邻近度。
- en: 7.2.2 Vision Transformer
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 视觉Transformer
- en: The **ViT** (Vision Transformer) [[42](#CR42)] applies a pure Transformer encoder
    (Sect. [2.​3.​1](528393_1_En_2_Chapter.xhtml#Sec20)) to image patches. The input
    image ![$${\boldsymbol {x}}\in \mathbb {R}^{H\times W\times c}$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq2.png)
    has *H* × *W* pixels and *c* color channels. It is partitioned into patches of
    *s* × *s* pixel, e.g. *s* = 16\. Each of the *N* = *HW*∕*s*² patches consist of
    *s*² ∗ *c* numbers, which are linearly mapped to a vector of length *d* used as
    the inputs of the transformer. Usually, a one-dimensional position embedding is
    added, because two-dimensional positions gave no significant performance improvement.
    Different models ViT[Base], ViT[Large], and ViT[Huge] with 12, 24, and 32 layers
    and 86M, 307M and 632M parameters respectively are employed.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**ViT**（视觉Transformer）[[42](#CR42)] 将纯Transformer编码器（第[2.3.1](528393_1_En_2_Chapter.xhtml#Sec20)节）应用于图像块。输入图像
    ![$${\boldsymbol {x}}\in \mathbb {R}^{H\times W\times c}$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq2.png)
    有 *H*×*W* 像素和 *c* 个颜色通道。它被分割成 *s*×*s* 像素的块，例如 *s* = 16。每个 *N* = *HW*∕*s*² 块包含
    *s*²∗*c* 个数字，这些数字被线性映射到长度为 *d* 的向量，用作Transformer的输入。通常，会添加一维位置嵌入，因为二维位置没有带来显著的性能提升。使用了不同模型ViT[Base]、ViT[Large]和ViT[Huge]，它们分别有12、24和32层，以及86M、307M和632M个参数。'
- en: The transformer encoder has an input sequence length of *N* consisting of vectors
    of size *d*. Each layer generates *N* embeddings of length *d*. The output embedding
    of the [CLS] token in the last encoder block is the input to a logistic classifier
    to compute probabilities of the image classes. The architecture is shown in Fig.
    [7.4](#Fig4).![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig4_HTML.png)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer编码器有一个由大小为 *d* 的向量组成的输入序列长度 *N*。每一层生成长度为 *d* 的 *N* 个嵌入。最后一个编码块中[CLS]标记的输出嵌入是逻辑分类器的输入，用于计算图像类别的概率。架构如图[7.4](#Fig4)所示.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig4_HTML.png)
- en: A photo of a tall building from a bottom view. The image processing steps include
    C L S a special token used to summarize the entire sequence of patches, linear
    projection of patches. Each patch is treated as a word in N L P, Transformer encoder
    layers, and L the length of the sequence of patches.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从底部视角拍摄的一座高楼的照片。图像处理步骤包括C L S，这是一个用于总结整个块序列的特殊标记，块的线性投影。每个块被当作N L P中的一个单词，Transformer编码器层，以及L，即块序列的长度。
- en: Fig. 7.4
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4
- en: The Vision Transformer ViT partitions an image into square patches of fixed
    size. For each patch an embedding is calculated by a linear projection. A standard
    encoder computes contextual embeddings. The embeddings of the [CLS] token is used
    to compute a class by a logistic classifier [[42](#CR42)]. Image adapted from
    [[42](#CR42)] with permission of the authors, credits in Table [A.​3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉Transformer ViT将图像分割成固定大小的正方形块。对于每个块，通过线性投影计算一个嵌入。标准的编码器计算上下文嵌入。使用[CLS]标记的嵌入通过逻辑分类器计算类别
    [[42](#CR42)]。图像经作者许可改编自 [[42](#CR42)]，详见表[A.3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
- en: It is remarkable that the images may be trained with varying input image resolutions.
    But patch size is always the same yielding different input size lengths. To take
    the new resolution into account, a 2D interpolation of the position embeddings
    is performed. The model is typically pre-trained on a large dataset JFT-300M [[161](#CR161)]
    to predict masked inputs. It is fine-tuned with a smaller task using a different
    classifier layer. It is often beneficial to fine-tune at higher resolution than
    pre-training [[189](#CR189)]. The models were pre-trained on datasets with up
    to 300M images.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，图像可以使用不同的输入图像分辨率进行训练。但块的大小始终相同，导致不同的输入长度。为了考虑新的分辨率，对位置嵌入执行2D插值。该模型通常在大数据集JFT-300M
    [[161](#CR161)]上预训练，以预测掩码输入。使用不同的分类器层进行小任务的微调。通常，在比预训练更高的分辨率上进行微调是有益的 [[189](#CR189)]。模型在包含高达3000万张图像的数据集上进行了预训练。
- en: The largest model ViT[Huge] has input patches of size 14 × 14\. It was able
    to outperform an improved and pre-trained ResNet152 [[63](#CR63)] with 152 CNN
    layers and EfficientNet [[92](#CR92)] on ImageNet, and achieved a Sota of 90.5%
    Top-1 accuracy for the classification of images into 1000 object categories [[118](#CR118)].
    Pre-training increases absolute accuracy by 13% on the test set of ImageNet. With
    2.5k TPUv3 days, it required only 25% of the computing effort (including pre-training)
    required for ResNet. It improved Sota for another 5 popular image classification
    benchmarks. The smaller ViT[Large] with input patches of size 16 × 16 also outperformed
    ResNet152 requiring only 6.8% of ResNet152’s compute effort.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的模型ViT[Huge]的输入块大小为14×14。它能够超越改进并预训练的ResNet152 [[63](#CR63)]，后者有152个CNN层和EfficientNet
    [[92](#CR92)]，在ImageNet上表现优异，并在将图像分类为1000个对象类别时达到了90.5%的Top-1准确率 [[118](#CR118)]。预训练使ImageNet测试集的绝对准确率提高了13%。使用2.5k
    TPUv3天，它只需要ResNet所需计算努力的25%（包括预训练）。它还提高了其他5个流行图像分类基准的Sota。具有16×16输入块大小的较小ViT[Large]也优于ResNet152，只需6.8%的ResNet152计算努力。
- en: When ViT is trained on a moderate dataset like ImageNet, the model achieves
    a performance below that of ResNet (Sect. [1.​7](528393_1_En_1_Chapter.xhtml#Sec7))
    with a comparable parameter count. It seems that CNNs have more appropriate inductive
    biases, such as translation equivariance and locality, which the transformer must
    learn through pre-training. Therefore, only pre-trained transformers can outperform
    CNNs, but this requires a lower computational effort. Cao et al. [[20](#CR20)]
    present a method how ViTs can be trained with limited data and achieve good results.
    Chefer et al. [[22](#CR22)] present a new method based on Taylor decomposition
    methods to visualize the parts of the image that led to a certain image classification.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当ViT在类似ImageNet的中等数据集上训练时，该模型在参数数量可比的情况下，性能低于ResNet（第[1.7](528393_1_En_1_Chapter.xhtml#Sec7)节）。似乎CNN具有更合适的归纳偏差，例如平移等变性和局部性，这些是transformer必须通过预训练来学习的。因此，只有预训练的transformer才能超越CNN，但这需要更低的计算努力。Cao等人
    [[20](#CR20)] 提出了一种方法，说明ViT如何在有限的数据下进行训练并取得良好的结果。Chefer等人 [[22](#CR22)] 提出了一种基于泰勒级数分解方法的新方法，以可视化导致特定图像分类的图像部分。
- en: It is instructive to analyze the inner structure of a trained model. It turns
    out that the trained position embeddings reflect the row and column structure
    of the input image, and patches in the same row/column have similar embeddings.
    Based on the attention weights, it can be determined which image parts are considered
    by a specific attention head. Some attention heads take into account the whole
    image while others have consistently small attention distances in the lower layers.
    This could have a similar function as early convolutional layers in CNNs [[130](#CR130)].
    An experimental investigation has shown that transformers are highly robust to
    severe occlusions [[108](#CR108)]. In contrast to CNNs, which often detect an
    object based on texture and less on shape, ViTs are comparable to humans on shape
    recognition. Figure [7.5](#Fig5) shows attention regions for the whole ViT model
    corresponding to semantically relevant areas.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig5_HTML.png)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 分析训练模型的内部结构是有益的。结果表明，训练的位置嵌入反映了输入图像的行和列结构，同一行/列中的块具有相似的嵌入。基于注意力权重，可以确定特定注意力头考虑的图像部分。一些注意力头考虑整个图像，而其他注意力头在底层具有一致的小注意力距离。这可能在CNN的早期卷积层中起到类似的作用
    [[130](#CR130)]。一项实验研究表明，变换器对严重遮挡具有高度鲁棒性 [[108](#CR108)]。与CNN不同，CNN通常基于纹理而不是形状来检测对象，ViT在形状识别上与人类相当。图[7.5](#Fig5)显示了整个ViT模型对应于语义相关区域的注意力区域。[![图7.5](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig5_HTML.png)](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig5_HTML.png)
- en: 3 panel of photos in row 1 represent a clear view of a dog, a flight, and a
    snake. 3 panel of photos in row 2 represent the same pictures replicated in a
    darker mode with a focused view on the dog, flight, and snake.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行中的3个面板照片清晰地展示了狗、飞行和蛇的景象。第二行中的3个面板照片以较暗的模式重复了相同的图片，并聚焦于狗、飞行和蛇。
- en: Fig. 7.5
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5
- en: The input image is shown in the upper row. The lower row depicts the area of
    main attention computed by the Vision Transformer model to the input space for
    classification. Image reprinted with kind permission of the authors [[42](#CR42),
    p. 8]
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像显示在上行。下行描述了视觉变换器模型计算出的对输入空间进行分类的主要关注区域。图像经作者许可重印 [[42](#CR42)，第8页]
- en: A number of researchers have investigated the robustness of ViT. In a series
    of experiments, Mao et al. [[103](#CR103)] found that the ViT tends to employ
    local features containing textures and noise, and to some extend ignores global
    context such as shape and structure. In response, they propose to discretize the
    continuous input features to image tokens using a vector quantizer based on a
    variational autoencoder (*VQ-VAE*) [[113](#CR113)]. They report accuracy improvements
    of up to 12% on several ImageNet classification benchmarks. A similar adaptive
    token generation methods for the ViT was proposed by Ryoo et al. [[146](#CR146)].
    **BEiT** [[11](#CR11)] outperforms, the supervised pre-trained ViT using a self-supervised
    method inspired by BERT (masked image modeling) and based on a VQ-VAE.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究人员已经研究了ViT的鲁棒性。在一系列实验中，Mao等人 [[103](#CR103)] 发现ViT倾向于使用包含纹理和噪声的局部特征，并在一定程度上忽略了形状和结构等全局上下文。作为回应，他们提出使用基于变分自动编码器（*VQ-VAE*）的向量量化器对连续输入特征进行离散化，以图像标记的形式使用
    [[113](#CR113)]。他们报告说，在几个ImageNet分类基准测试中，准确率提高了高达12%。Ryoo等人 [[146](#CR146)] 提出了一种类似的自适应标记生成方法，用于ViT。**BEiT**
    [[11](#CR11)] 使用受BERT（掩码图像建模）启发的自监督方法，并基于VQ-VAE，优于监督预训练的ViT。
- en: 7.2.3 Image Generation
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 图像生成
- en: There are a number of Foundation Models for various image enhancement tasks.
    Image *super-resolution* converts a low-resolution image to a higher resolution.
    **SwinIR** [[96](#CR96)] is based on a hierarchical representation starting from
    small-sized image patches and gradually merging neighboring image patches in deeper
    layers. For training, the model gets a small-scale image as input, which is preprocessed
    with a CNN layer. The transformer block contains transformer and CNN layers and
    is trained to reconstruct the high-resolution image. SwinIR achieves Sota on benchmarks
    for super-resolution, image denoising, and JPEG compression artifact resolution,
    while having only 12M parameters.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多用于各种图像增强任务的基础模型。图像**超分辨率**将低分辨率图像转换为高分辨率。**SwinIR** [[96](#CR96)] 基于从小型图像块开始的分层表示，并在更深层次中逐渐合并相邻的图像块。对于训练，模型以小规模图像作为输入，该图像通过CNN层进行预处理。变换器块包含变换器和CNN层，并训练以重建高分辨率图像。SwinIR在超分辨率、图像去噪和JPEG压缩伪影分辨率基准测试中实现了Sota，同时只有1200万个参数。
- en: '**ColTran** [[88](#CR88)] transforms a grayscale image to a fully colored image
    by using transformers with column and row attention. It first predicts colors
    by a conditional transformer for a spatially reduced image with only 512 coarse
    colors. Two subsequent fully parallel transformers upsample the coarse colored
    low resolution image into a fully colored high resolution image. The model achieves
    the best FID-score (Sect. [7.2.6](#Sec18)) of 19.7 on ImageNet data compared to
    different alternatives. Examples of colorizations are shown in Fig. [7.6](#Fig6).![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig6_HTML.png)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**ColTran** [[88](#CR88)] 通过使用具有列和行注意力的Transformer将灰度图像转换为全彩色图像。它首先通过条件Transformer预测一个只有512种粗略颜色的空间减少图像的颜色。两个随后的完全并行Transformer将粗略着色的低分辨率图像上采样到全彩色的全分辨率图像。该模型在ImageNet数据上实现了最佳FID分数（第[7.2.6](#Sec18)节）为19.7，与不同的替代方案相比。颜色化的示例显示在图[7.6](#Fig6)中。![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig6_HTML.png)'
- en: 4 panel of photos in row 1 represents a person stands in a shop in 4 different
    color variations and shades. 4 panel of photos in row 2 represents a few people
    walk around, a person holds ropes in 4 different color variations and shades.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行中的4个照片面板代表一个人站在商店里，有4种不同的颜色变化和阴影。第二行中的4个照片面板代表一些人四处走动，一个人手持绳索，有4种不同的颜色变化和阴影。
- en: Fig. 7.6
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6
- en: Different colorizations of grayscale images (left) by ColTRan [[88](#CR88)].
    Note that semantic constraints, e.g. the color of the skin and the tree leaves,
    are usually respected. Image reprinted with kind permission of the authors [[88](#CR88),
    p. 1]
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由ColTRan [[88](#CR88)] 对灰度图像进行不同颜色化（左侧）。请注意，通常尊重语义约束，例如肤色和树叶的颜色。图像经作者许可重新印刷
    [[88](#CR88)，第1页]。
- en: The **Swin Transformer** [[99](#CR99)] constructs a hierarchical representation
    of an image by starting from small-sized image patches and gradually merging neighboring
    patches in deeper Transformer layers. A linear computational complexity is achieved
    by computing self-attention locally within non-overlapping windows of size 7 that
    partition an image. Between consecutive layers the attention windows are shifted
    such that there is an overlay with the neighboring windows of the prior self-attention
    layer. The largest model version has 197M parameters and processes images of resolution
    384 × 384\. On ImageNet classification the model achieves a top-1 accuracy of
    87.3%. Also on object detection in images, the Swin Transformer is able to improve
    the prior best results.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**Swin Transformer** [[99](#CR99)] 通过从小型图像块开始，并在更深层的Transformer层中逐步合并相邻块，构建了一个图像的层次表示。通过在大小为7的非重叠窗口内局部计算自注意力，实现了线性计算复杂度，这些窗口将图像分割。在连续层之间，注意力窗口被移动，以便与先前自注意力层的相邻窗口重叠。最大的模型版本有1970万个参数，处理分辨率为384×384的图像。在ImageNet分类中，该模型达到了87.3%的top-1准确率。在图像目标检测方面，Swin
    Transformer也能提高先前最佳结果。'
- en: '**VQ-GAN** [[45](#CR45)] uses a CNN to efficiently learn a codebook of context-rich
    visual patches, and subsequently learns a model of their global structure. The
    long-range interactions within these patches require an expressive GPT-2 to model
    distributions of the visual patches. The dictionary of image patches captures
    perceptually important local structure according to *perceptual loss* [[41](#CR41),
    [194](#CR194)]. This loss is optimized with an adversarial training procedure
    with a patch-based image discriminator that aims to differentiate between real
    and reconstructed images.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**VQ-GAN** [[45](#CR45)] 使用CNN高效地学习一个包含丰富上下文视觉块的代码簿，然后学习它们的全局结构模型。这些块内的长距离相互作用需要一个表达丰富的GPT-2来建模视觉块的概率分布。图像块字典根据*感知损失*
    [[41](#CR41)，[194](#CR194)] 捕获感知上重要的局部结构。该损失通过基于块的图像判别器的对抗训练程序进行优化，该判别器旨在区分真实图像和重建图像。'
- en: A GPT-2 model with 307M parameters is pre-trained to generate the code sequence
    of encoded images in an image corpus. Each image is partitioned to 16 × 16 patches
    with a sequence length of 1024\. An example image is shown in Fig. [7.7](#Fig7).
    If the training corpus contains class information *c*, images of specific classes
    can be generated. Class information can also be restricted to specific image regions.
    While VQ-VAE yields an FID of about 10 for the reconstruction of ImageNet photos,
    VQ-GAN achieves a much better value of 1.7.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig7_HTML.png)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有 307M 参数的 GPT-2 模型经过预训练，用于生成图像语料库中编码图像的代码序列。每个图像被划分为 16×16 的块，序列长度为 1024。一个示例图像如图
    [7.7](#Fig7) 所示。如果训练语料库包含类别信息 *c*，则可以生成特定类别的图像。类别信息也可以限制到特定的图像区域。虽然 VQ-VAE 在重建
    ImageNet 照片时产生大约 10 的 FID，但 VQ-GAN 实现了更好的 1.7 的值。![图 7.7](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig7_HTML.png)
- en: A photograph displays snow-covered mountains and hills near a lake.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一张照片展示了湖边覆盖着雪的山丘和山峦。
- en: Fig. 7.7
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7
- en: VQ-GAN [[45](#CR45)] enables transformers to synthesize high-resolution images
    with 1280 × 460 pixels. Image reprinted with kind permission of the authors [[45](#CR45),
    p. 12873]
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: VQ-GAN [[45](#CR45)] 允许变压器合成具有 1280×460 像素的高分辨率图像。图像经作者许可重新印刷 [[45](#CR45)，第
    12873 页]
- en: '**StyleSwin** [[191](#CR191)] is a further development of VQ-GAN. It uses the
    *Swin transformer* [[99](#CR99)] discussed above. StyleSwin employs a wavelet
    discriminator in the spectral domain to suppress blocking artifacts. The model
    with 41M parameters achieves Sota quality on multiple established benchmarks.
    Example images are shown in Fig. [7.8](#Fig8) having a coherent global geometry
    and high-fidelity details. On the CelebA-HQ 1024 benchmark StyleSwin yields an
    FID of 4.4, which is better than all prior models including StyleGAN2 [[82](#CR82)].
    For the task of generating churches based on the LSUN dataset StyleSwin has an
    FID-score of 3.1, which is nearly as good as the best scoring adversarial CIPS
    model [[7](#CR7)] with an FID-score of 2.9.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig8_HTML.png)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**StyleSwin** [[191](#CR191)] 是 VQ-GAN 的一种进一步发展。它使用了上面讨论过的 *Swin 变换器* [[99](#CR99)]。StyleSwin
    在频谱域使用小波判别器来抑制块状伪影。具有 41M 参数的模型在多个已建立的基准测试中实现了 Sota 质量水平。示例图像如图 [7.8](#Fig8) 所示，具有一致的全球几何形状和高度保真的细节。在
    CelebA-HQ 1024 基准测试中，StyleSwin 的 FID 为 4.4，优于包括 StyleGAN2 [[82](#CR82)] 在内的所有先前模型。对于基于
    LSUN 数据集生成教堂的任务，StyleSwin 的 FID 分数为 3.1，几乎与 FID 分数为 2.9 的最佳评分对抗 CIPS 模型 [[7](#CR7)]
    相当！![图 7.8](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig8_HTML.png)'
- en: 4 photographs of the faces of different individuals.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 不同个体的 4 张面部照片。
- en: Fig. 7.8
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8
- en: Images in the 1024 × 1024 resolution generated by StyleSwin [[191](#CR191)]
    on FFHQ 1024 × 1024 data (left) and CelebA-HQ 1024 × 1024 data (right). Best seen
    with zoom. Image reprinted with kind permission of the authors [[191](#CR191),
    p. 8]
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: StyleSwin [[191](#CR191)] 在 FFHQ 1024×1024 数据（左侧）和 CelebA-HQ 1024×1024 数据（右侧）上生成的
    1024×1024 分辨率的图像。最佳查看方式为放大查看。图像经作者许可重新印刷 [[191](#CR191)，第 8 页]
- en: '**Data2vec** [[9](#CR9)] proposes a new training criterion for self-supervised
    learning, which can be applied to image, text and speech data. It has two kinds
    of models: a teacher model, which processes the whole input, and a student model,
    which processes the input while masking some data.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**Data2vec** [[9](#CR9)] 提出了一种新的自监督学习训练标准，该标准可应用于图像、文本和语音数据。它有两种模型：一种教师模型，处理整个输入，一种学生模型，在掩码一些数据的同时处理输入。'
- en: The model employs a standard transformer architecture with media-specific input
    encoding. Images are encoded by linearly transformed image patches similar to
    ViT. Speech data is encoded by multi-layer 1-D convolutions. Text data is encoded
    as subword tokens. Training targets for the student model are constructed from
    the averaged top *K* encoder blocks of the teacher network, which processes the
    complete input. This target has to be predicted by the student model, which only
    receives the masked inputs. Representations of data2vec are continuous and contextualized
    through the use of self-attention, which makes them richer than a discrete set
    of tokens used for other approaches.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型采用标准的变压器架构，具有媒体特定的输入编码。图像通过类似于 ViT 的线性变换图像块进行编码。语音数据通过多层 1-D 卷积进行编码。文本数据编码为子词标记。学生模型的训练目标是教师网络平均顶部
    *K* 编码块的平均值，该网络处理完整输入。这个目标必须由只接收掩码输入的学生模型预测。data2vec 的表示是连续的，并通过使用自注意力进行上下文化，这使得它们比其他方法使用的离散标记集更丰富。
- en: Separate models are trained according to this scheme for speech, images and
    text. For images a Data2vec model achieves a new Sota of 86.2% top-1 accuracy
    on ImageNet-1k with restricted training set. For speech data, the model reaches
    a WER of 5.5% on the Librispeech test-other benchmark. For language processing,
    Data2vec has an average score of 82.9 on GLUE, which is better than RoBERTa. This
    demonstrates that the model can be effective for multiple modalities. It can be
    expected that this model will be extended to learn across modalities.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 根据此方案分别训练语音、图像和文本的独立模型。对于图像，Data2vec模型在ImageNet-1k数据集上实现了86.2%的新Sota top-1准确率，训练集受限。对于语音数据，模型在Librispeech测试-其他基准上达到了5.5%的WER。对于语言处理，Data2vec在GLUE上的平均得分为82.9，优于RoBERTa。这表明该模型可以有效地处理多种模态。可以预期，该模型将被扩展以跨模态学习。
- en: 7.2.4 Joint Processing of Text and Images
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.4 文本和图像的联合处理
- en: 'Once transformers were applied to text and images, joint processing of both
    modalities became an obvious alternative. Three steps are required for this:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦将变压器应用于文本和图像，联合处理这两种模态就成为一种明显的替代方案。为此需要三个步骤：
- en: encoding images and texts into embeddings preserving their semantics;
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像和文本编码成嵌入，保留其语义；
- en: designing powerful architectures to model the interaction between both modalities;
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计强大的架构来模拟两种模态之间的交互。
- en: developing effective pre-training tasks.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发有效的预训练任务。
- en: After learning universal vision and language features, these PLMs can be fine-tuned
    on various downstream vision-language tasks.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习通用的视觉和语言特征后，这些PLM可以在各种下游视觉-语言任务上进行微调。
- en: For pre-training large scale datasets of text-image pairs (***v***, ***u***)
    are required. Each pair consists of a sequence ***v***[1], …, ***v***[*T*] of
    text tokens and a sequence ***u***[1], …, ***u***[*R*] of image features or *visual
    tokens*, e.g. image patches. In this way, we can unify input representation as
    sequence of embeddings for both modalities. An example dataset is *COCO captions*
    [[26](#CR26)], which contains 328k images of 91 object types of common objects
    in their natural context together with the corresponding image captions (Fig.
    [7.9](#Fig9)). Other datasets like *Conceptual Captions* (CC) [[153](#CR153)],
    *RedCaps* [[34](#CR34)], and *Laion* [[151](#CR151)] contain 3.1M, 12M and 400M
    images respectively together with captions or descriptive text.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig9_HTML.png)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预训练大规模文本-图像对数据集（***v***，***u**），需要。每个对由一个文本标记序列***v***[1]，…，***v***[*T*]和一个图像特征或*视觉标记*序列***u***[1]，…，***u***[*R*]组成，例如图像块。这样，我们可以统一两种模态的输入表示，作为嵌入序列。一个示例数据集是*COCO标题*
    [[26](#CR26)]，它包含328k张自然环境中常见物体的91种类型的图像及其相应的图像标题（图[7.9](#Fig9)）。其他数据集如*概念标题*（CC）
    [[153](#CR153)]，*RedCaps* [[34](#CR34)]和*Laion* [[151](#CR151)]分别包含3.1M、12M和4亿张图像，以及标题或描述性文本。![图片](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig9_HTML.png)
- en: A series of 3 photos. Photo 1, the player at bat hits the baseball while the
    umpire looks on. Photo 2, a school bus on a parking lot with snow next to a building.
    Photo 3, 2 horses pull a hay wagon with 2 men on the load.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列3张照片。照片1，击球手击打棒球，裁判在旁边观看。照片2，一辆校车停在停车场，旁边有一座建筑物和雪。照片3，两匹马拉着装满干草的马车，车上载着两名男子。
- en: Fig. 7.9
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9
- en: 'MS-COCO dataset [[26](#CR26)]: images similar to sample images from the dataset.
    The corresponding captions indicate the level of detail. Image credits in Table
    [A.​3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 'MS-COCO数据集 [[26](#CR26)]: 与数据集样本图像相似的照片。相应的标题表明了细节程度。图像来源见表[A.3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)'
- en: Pre-training tasks have to be designed in such a way that the model has to reconstruct
    parts of the text or image based on the remaining contextual text and image features.
    For *Cross-modal MLM* (Masked Language Modeling) the model has to predict masked
    tokens or image patches based on the other unmasked text tokens and visual tokens.
    Here different masking strategies can be used such as whole word masking, masking
    text spans, or permuting tokens (Sect. [3.​1](528393_1_En_3_Chapter.xhtml#Sec1)).
    *Masked region prediction* aims to predict the content of an image region. Objects
    and their regions are annotated manually or by an auxiliary model. Then the model
    is required to predict the object (or a distribution over objects) for that region.
    In this way, the model learns to locate objects in an image.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练任务必须设计得使模型能够根据剩余的上下文文本和图像特征重建文本或图像的部分。对于 *跨模态 MLM*（掩码语言模型），模型必须根据其他未掩码的文本标记和视觉标记预测掩码标记或图像块。这里可以使用不同的掩码策略，例如全词掩码、掩码文本跨度或标记置换（第
    [3.1](528393_1_En_3_Chapter.xhtml#Sec1) 节）。*掩码区域预测*旨在预测图像区域的内容。对象及其区域是手动或通过辅助模型标注的。然后模型需要预测该区域的对象（或对象分布）。通过这种方式，模型学习在图像中定位对象。
- en: '**CLIP** [[126](#CR126), [127](#CR127)] is trained to predict a score indicating
    which image caption corresponds to which image. Given a batch (***v***[1], ***u***[1]),
    …, (***v***[*n*], ***u***[*n*]) of tokenized text-image pairs, CLIP has to predict
    which of the *n* × *n* possible (***v***[*i*], ***u***[*j*]) pairings across the
    batch actually occurred. By *contrastive learning*, CLIP creates a multi-modal
    embedding space by jointly training an image encoder and text encoder to maximize
    the cosine similarity of the image and text embeddings of the *n* real pairs in
    the batch while minimizing the cosine similarity of the embeddings of the *n*² − *n*
    incorrect pairings. This contrastive training with positive and negative examples
    has been shown to outperform alternatives. As image encoder a Vision Transformer
    (ViT) with images patches of size 14 × 14 (Sect. [7.2.2](#Sec14)) was employed,
    which works better than a ResNet [[63](#CR63)] encoder based on CNNs. Text was
    enclosed by [SOS] and [EOS] tokens and a 12 layer autoregressive GPT model was
    used to compute embeddings. The embedding of [EOS] in the highest layer was employed
    as the representation of the whole text.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**CLIP** [[126](#CR126), [127](#CR127)] 是经过训练以预测一个分数，表示哪个图像标题对应哪个图像。给定一个包含（***v***[1],
    ***u***[1]），…，（***v***[*n*], ***u***[*n*]) 的标记化文本-图像对的批次，CLIP 需要预测在批次中实际发生的 *n*×*n*
    种可能的（***v***[*i*], ***u***[*j*]) 配对中的哪一种。通过 **对比学习**，CLIP 通过联合训练图像编码器和文本编码器来最大化批次中
    *n* 个真实配对的图像和文本嵌入的余弦相似度，同时最小化 *n*²−*n* 个不正确配对的嵌入的余弦相似度。这种带有正负例的对比训练已被证明优于其他方法。作为图像编码器，采用了大小为
    14×14 的图像块 Vision Transformer (ViT)（第 [7.2.2](#Sec14) 节），它比基于 CNN 的 ResNet [[63](#CR63)]
    编码器表现更好。文本被 [SOS] 和 [EOS] 标记包围，并使用一个 12 层的自回归 GPT 模型来计算嵌入。最高层中的 [EOS] 嵌入被用作整个文本的表示。'
- en: CLIP was trained on 400M image-text pairs of the *WIT data* [[127](#CR127)]
    to associate an image with the best-matching caption. In addition, the prediction
    of the next token was used as an auxiliary loss term for the GPT model. The model
    can be used to retrieve a text best fitting to an image, or an image optimally
    corresponding to a text.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 在 *WIT 数据* [[127](#CR127)] 的 400M 图像-文本对上进行了训练，以将图像与最佳匹配的标题关联起来。此外，预测下一个标记被用作
    GPT 模型的辅助损失项。该模型可用于检索与图像最匹配的文本，或与文本最优化对应的图像。
- en: The resulting model has acquired a comprehensive knowledge about text and images.
    With a top-1 classification accuracy of 76.2%, it even surpasses the top-1 classification
    accuracy of 75.0% of the original ResNet50 on ImageNet zero-shot classification
    without the need to use any of the 1.28M training examples that ResNet50 was trained
    on. Hence, CLIP can be considered a ‘zero-shot classifier’. This also holds for
    16 out of 27 other image classification benchmarks. When a linear classifier is
    fitted on top of CLIP’s features, it improves CLIP’s accuracy on the ImageNet
    test set by almost 10% [[126](#CR126)]. If the image distribution is changed,
    e.g. to sketches, CLIP-based classifiers are much more robust. Zero-shot CLIP
    classifiers improve effective robustness by a large amount, especially with respect
    to distribution shift. This demonstrates that the inclusion of caption text into
    vision models enhances performance and robustness.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 结果模型已经获得了关于文本和图像的全面知识。其top-1分类准确率达到76.2%，甚至超过了在ImageNet零样本分类中原始ResNet50的75.0%的top-1分类准确率，而无需使用ResNet50训练所用的任何1.28M个训练示例。因此，CLIP可以被认为是一个“零样本分类器”。这一点也适用于27个其他图像分类基准中的16个。当在CLIP的特征之上拟合一个线性分类器时，它将CLIP在ImageNet测试集上的准确率提高了近10%
    [[126](#CR126)]。如果图像分布发生变化，例如变为草图，基于CLIP的分类器将更加鲁棒。零样本CLIP分类器通过大量提高有效鲁棒性，特别是在分布偏移方面。这表明，将字幕文本纳入视觉模型可以增强性能和鲁棒性。
- en: '**BriVL** [[46](#CR46)] is a similar model for Chinese language, which uses
    a larger set of negative examples stored in a queue. It uses a huge training dataset
    of 650M weakly correlated text-image pairs, where, for instance, an image of a
    birthday cake has the caption *“Happy birthday! Make a wish”*. It achieves Sota
    results for cross-modal retrieval and visual question answering.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**BriVL** [[46](#CR46)]是一个针对中文的类似模型，它使用了一个存储在队列中的更大的负例集。它使用了一个包含650M个弱相关文本-图像对的巨大训练数据集，例如，一个生日蛋糕的图像的标题是*“生日快乐！许个愿”*。它在跨模态检索和视觉问答方面实现了Sota结果。'
- en: '**ALIGN** [[77](#CR77)] also uses separate encoders for text and images with
    a cosine-similarity combination function at the top. As image encoder an EfficientNet
    CNN is employed. BERT is trained to produce a text embedding for the [CLS] token.
    Again the similarity is minimized for genuine image-text pairs and maximized for
    random pairs. ALIGN has 675M parameters and uses a huge training set of 1.8B noisy
    image pairs. In spite of the noisy data the model achieves a slightly better accuracy
    (85.5%) on ImageNet top-1 classification than CLIP.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**ALIGN** [[77](#CR77)]也使用文本和图像的单独编码器，并在顶部使用余弦相似度组合函数。作为图像编码器，使用了EfficientNet
    CNN。BERT被训练来为[CLS]标记生成文本嵌入。同样，对于真实的图像-文本对，相似度被最小化，而对于随机对，相似度被最大化。ALIGN有675M个参数，并使用了一个包含1.8B个噪声图像对的巨大训练集。尽管数据有噪声，该模型在ImageNet顶级1分类上的准确率（85.5%）略高于CLIP。'
- en: 7.2.5 Describing Images by Text
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.5 通过文本描述图像
- en: The automatic generation of a natural language description of an image is also
    called *image annotation* or *image captioning*. The task is challenging, as it
    requires visual perception, recognition, and real-world knowledge, as well as
    the *grounding* of language expressions in the image space. *Symbol grounding*
    describes, how words acquire their meaning, e.g. by associating a word with an
    object in an image. Aside from determining and extracting the important objects
    and details of an image, the model has to infer the semantic relationship of the
    objects and the scene (Fig. [7.9](#Fig9)).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成图像的自然语言描述也称为**图像标注**或**图像标题**。这项任务具有挑战性，因为它需要视觉感知、识别、现实世界知识，以及将语言表达在图像空间中的**定位**。**符号定位**描述了词语如何获得其意义，例如通过将一个词与图像中的物体关联起来。除了确定和提取图像中的重要对象和细节之外，模型还必须推断对象和场景的语义关系（图[7.9](#Fig9)）。
- en: 'Current top models for describing images work in two stages:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当前用于描述图像的最顶级模型工作在两个阶段：
- en: an *object detection* model is pre-trained to encode an image and the visual
    objects in the image to feature vectors,
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**目标检测**模型被预先训练来将图像及其中的视觉对象编码为特征向量，
- en: a crossmodal PLM is pre-trained to associate text and visual features and generate
    a caption for an image.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个跨模态PLM被预先训练来关联文本和视觉特征并为图像生成标题。
- en: Similar to language translation, various metrics are used to evaluate the generated
    texts, e.g. Bleu or Rouge (Sect. [2.​3.​3](528393_1_En_2_Chapter.xhtml#Sec23)).
    Surveys of image captioning techniques are provided by Hossain et al. [[67](#CR67)],
    Oluwasammi et al. [[112](#CR112)], and Stefanini et al. [[159](#CR159)].
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 与语言翻译类似，使用各种指标来评估生成的文本，例如Bleu或Rouge（见第[2.3.3](528393_1_En_2_Chapter.xhtml#Sec23)节）。Hossain等人
    [[67](#CR67)]、Oluwasammi等人 [[112](#CR112)] 和 Stefanini等人 [[159](#CR159)] 提供了图像描述技术调查。
- en: '**VilBERT** [[100](#CR100)] aims to learn representations that can jointly
    model images and natural language. It extracts bounding boxes and their visual
    features using a pre-trained object detection network (Faster R-CNN [[137](#CR137)]).
    These image region features as well as the text are input to two separate transformer
    encoders (two-stream architecture). Subsequently, transformer layers with cross-attention
    in both directions are applied to learn cross-modal relationships. VilBERT was
    pre-trained on Conceptual Captions data.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**VilBERT** [[100](#CR100)] 旨在学习能够联合建模图像和自然语言的表示。它使用预训练的目标检测网络（Faster R-CNN
    [[137](#CR137)]）提取边界框及其视觉特征。这些图像区域特征以及文本输入到两个独立的变压器编码器（双流架构）。随后，应用具有双向交叉注意力的变压器层来学习跨模态关系。VilBERT在概念描述数据上进行了预训练。'
- en: The model was fine-tuned and evaluated on different tasks. *Visual question
    answering* (*VQA*) answers natural language questions about images. VQA is treated
    as a multi-label classification task with 3129 possible answers. Final embeddings
    of the text and image parts are fed into a classifier to estimate class probabilities.
    On the COCO test set VilBERT achieved a new Sota with an accuracy of 70.9%. *Caption-based
    image retrieval* is the task of identifying an image from a pool given a caption
    describing its content. The model was fine-tuned on a Flickr dataset and had a
    recall@1 of 58.2%, thus establishing a new Sota.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在不同的任务上进行了微调和评估。*视觉问答* (*VQA*) 回答关于图像的自然语言问题。VQA被视为一个有3129个可能答案的多标签分类任务。文本和图像部分的最终嵌入被输入到分类器中以估计类概率。在COCO测试集上，VilBERT实现了70.9%的新Sota准确率。*基于描述的图像检索*是从描述其内容的描述中识别图像的任务。该模型在Flickr数据集上进行了微调，召回率@1为58.2%，从而建立了新的Sota。
- en: '**OSCAR** [[95](#CR95)] has the strategy to connect the relevant objects in
    the image with the corresponding phrases in the caption text. The authors use
    self-attention to learn these alignments, which can be significantly improved
    by additional object tags detected in images as reference points. Oscar represents
    each input image-text pair as a Word-Tag-Image triple (*w*;*q*;*v*), where *w*
    is the sequence of words of the caption text, *q* contains the words of the textual
    object tags detected in the image, and *v* is the set of the corresponding region
    images. A CNN model (Faster R-CNN [[137](#CR137)]) is used to discover the objects
    in *q* as well as to the corresponding regions *v*. For pre-training the transformer
    encoder, part of the tokens in (*w*;*q*;*v*) are masked, and the model learns
    to predict the masked tokens. In addition, sometimes the *q*-terms are changed
    randomly. The model has the additional task to identify these modifications. A
    small and a large model version are trained with a sequence length of 768 and
    1024 using a public corpus of 6.5 million text-image pairs. The model is fine-tuned
    to generate the caption according to the sequence-to-sequence objective. The model
    achieves a new Sota on COCO-captions with respect to Bleu-4 (41.7%), Meteor and
    Rouge-L as well as for several other captioning benchmarks.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**OSCAR** [[95](#CR95)] 的策略是将图像中的相关对象与描述文本中的对应短语相连接。作者使用自注意力来学习这些对齐，这些对齐可以通过图像中检测到的附加对象标签作为参考点显著改进。Oscar将每个输入图像-文本对表示为一个词-标签-图像三元组
    (*w*;*q*;*v*)，其中 *w* 是描述文本的单词序列，*q* 包含在图像中检测到的文本对象标签的单词，而 *v* 是相应的区域图像集合。使用CNN模型（Faster
    R-CNN [[137](#CR137)]）来发现 *q* 中的对象以及相应的区域 *v*。为了预训练变压器编码器，(*w*;*q*;*v*) 中的部分标记被掩码，模型学习预测掩码标记。此外，有时会随机更改
    *q* 项。模型有额外任务来识别这些修改。使用6.5百万个文本-图像对的公共语料库，以序列长度768和1024训练了小型和大型模型版本。模型根据序列到序列目标进行微调。该模型在COCO-captions上实现了新的Sota，Bleu-4为41.7%，Meteor和Rouge-L以及几个其他描述基准。'
- en: '**VinVL** [[193](#CR193)] is pre-trained on three text-image corpora with 2.5M
    images, and can generate visual features with a richer collection of visual objects
    and concepts. VinVL pre-trains a large-scale object-attribute detection model
    based on the CNN-based ResNeXt-152 C4 architecture [[179](#CR179)]. The model
    does not describe objects by a single noun, but by a large number of attributes
    and details, which enhances the performance in joint image-language tasks (Fig.
    [7.10](#Fig10)). The approach is combined with OSCAR and yields an improved Sota
    on image captioning. **VIVO** [[70](#CR70)] is a similar transformer model trained
    to label image regions with 6.4k different object tags. VIVO is fine-tuned with
    COCO image-caption pairs and learns to generate caption sentences, also using
    object tags not appearing in the caption data. This is possible as VIVO can exploit
    large amounts of paired image-tag data to learn rich descriptions for images.
    On the test set VIVO generates better captions than humans according to the *CIDEr
    metric* [[69](#CR69)], which counts the common words weighted by tf-idf in the
    generated and the reference text [[169](#CR169)].![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig10_HTML.png)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**VinVL** [[193](#CR193)] 在三个包含2.5M张图片的文本-图像语料库上进行了预训练，并能生成包含更丰富视觉对象和概念的视觉特征。VinVL基于基于CNN的ResNeXt-152
    C4架构[[179](#CR179)]预训练了一个大规模的对象-属性检测模型。该模型不是通过单个名词来描述对象，而是通过大量属性和细节，这增强了联合图像-语言任务（图[7.10](#Fig10)）的性能。该方法与OSCAR结合，在图像描述方面实现了改进的Sota。**VIVO**
    [[70](#CR70)] 是一个类似的transformer模型，用于用6.4k个不同的对象标签标记图像区域。VIVO使用COCO图像-描述对进行微调，并学习生成描述句子，也使用在描述数据中未出现的对象标签。这是可能的，因为VIVO可以利用大量成对的图像-标签数据来学习图像的丰富描述。在测试集上，根据*CIDEr指标*
    [[69](#CR69)]，VIVO生成的描述比人类更好，该指标计算生成文本和参考文本中按tf-idf加权的共同单词！[](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig10_HTML.png)'
- en: 2 similar photos. Photo 1, the detected parts are labeled as a man, a wetsuit,
    a human arm, a human leg, a surfboard, and a surfboard. Photo 2, additional parts
    such as colors, hands, water splashes, drops, a rolling wave, and emotions are
    detected and labeled.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 2张相似的照片。照片1，检测到的部分被标记为一个人、一套湿式潜水服、一条人臂、一条人腿、一块冲浪板和一块冲浪板。照片2，检测并标记了额外的部分，如颜色、手、水花、水滴、滚动波浪和情感。
- en: Fig. 7.10
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10
- en: Standard bounding-box object descriptions (left) and detailed annotations, which
    can be generated by VinVL (right) and contain visual concepts and attribute information
    [[193](#CR193)]. Image credits in Table [A.​3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
    **SimVLM** [[171](#CR171)] is a transformer encoder-decoder, which uses the first
    three blocks of ResNet to extract contextualized patches from images, and associates
    the image tokens with text tokens. The decoder then predicts the continuation
    of the textual sequence as shown in Fig. [7.11](#Fig11). It is trained on 1.8B
    noisy image text pairs and 800GB text documents. SimVLM achieves a new Sota for
    visual question answering on the *VQA v2 benchmark* [[56](#CR56)] with 80.3% accuracy.
    In addition, it reaches Sota for visual entailment, visual reasoning, and image
    captioning on COCO captions with respect to Meteor (33.7).![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig11_HTML.png)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 标准边界框对象描述（左）和详细注释，这些注释可以由VinVL（右）生成，并包含视觉概念和属性信息[[193](#CR193)]。图像归功于表[A.3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)。**SimVLM**
    [[171](#CR171)] 是一个transformer编码器-解码器，它使用ResNet的前三个块从图像中提取上下文化的补丁，并将图像标记与文本标记关联。然后解码器预测文本序列的后续内容，如图[7.11](#Fig11)所示。它在1.8B个有噪声的图像-文本对和800GB的文本文档上进行了训练。SimVLM在*VQA
    v2基准* [[56](#CR56)]上实现了视觉问答的新Sota，准确率为80.3%。此外，它在COCO描述中相对于Meteor（33.7）实现了视觉蕴涵、视觉推理和图像描述的Sota。！[](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig11_HTML.png)
- en: 4 photos. 1, a group of people sitting at a table with drinks in a dark restaurant.
    Photo 2 has a question what is the profession of this person? surgeon. 3, this
    dish food is a kind of American breakfast dish. Photo 4, has a question where
    can you observe this animal? giant panda is native in China.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 4张照片。1，一张在昏暗餐厅里坐在桌子旁，桌上摆满了饮料的一群人。照片2有一个问题：这个人的职业是什么？外科医生。3，这道菜是美式早餐的一种。照片4，有一个问题：在哪里可以观察到这种动物？大熊猫是中国的原生动物。
- en: Fig. 7.11
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11
- en: The SimVLM encoder-decoder model receives an image (top) and a text (middle)
    as input and produces an output text (bottom) [[171](#CR171)]. The image patches
    are encoded by the first layers of ResNet. Image reprinted with kind permission
    of the authors [[171](#CR171), p. 3]
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: SimVLM 编码器-解码器模型接收一个图像（顶部）和一个文本（中部）作为输入，并生成一个输出文本（底部）[[171](#CR171)]。图像块由 ResNet
    的第一层进行编码。图像经作者许可重新印刷，见第 3 页 [[171](#CR171)]。
- en: '**Frozen** is a Foundation Model trained to associate text with images. It
    can be instructed by few-shot learning to answer question on an image [[166](#CR166)].
    The language model is a pre-trained autoregressive model with 7B parameters trained
    on the C4 dataset with 807GB text [[129](#CR129)]. The vision encoder is based
    on NF-ResNet-50 [[16](#CR16)] and provides an embedding vector characterizing
    the image. During training the image embedding is used as a prefix before the
    token embeddings of the generated text. Using the *conceptual captions* dataset
    the vision encoder is trained while freezing the language model. The training
    target is to generate a caption for the image.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**冻结**是一个训练用来将文本与图像关联的基础模型。它可以通过少样本学习来回答关于图像的问题 [[166](#CR166)]。语言模型是一个在 C4
    数据集上训练的 7B 参数预训练自回归模型，该数据集包含 807GB 的文本 [[129](#CR129)]。视觉编码器基于 NF-ResNet-50 [[16](#CR16)]，并提供一个表征图像的嵌入向量。在训练过程中，图像嵌入被用作生成文本的标记嵌入的前缀。使用
    *概念性标题* 数据集，在冻结语言模型的同时训练视觉编码器。训练目标是生成图像的标题。'
- en: During inference, several examples consisting of an image embedding and token
    embeddings are fed into the language model, which generates an answer. An example
    is to caption a microscope with *“This was invented by Zacharias Janssen.”*, and
    a light bulb with *“This was invented by Thomas Edison.”*. After five seeds and
    the input of an airplane together with *“This was invented by”* the model generates
    the output *“the Wright brothers”*. In this way, different categorizations of
    images can be defined on the fly. These samples demonstrate the ability to generate
    open-ended outputs that adapt to both images and text, and to make use of facts
    that it has learned during language-only pre-training. The model is a proof-of-concept
    and shows a way to generate few-shot models for image-text tasks.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，将包含图像嵌入和标记嵌入的几个示例输入到语言模型中，该模型生成一个答案。例如，用 *“这是由 Zacharias Janssen 发明的。”*
    来描述显微镜，用 *“这是由 Thomas Edison 发明的。”* 来描述灯泡。在输入飞机图像和 *“这是由”* 后，经过五个种子，模型生成输出 *“莱特兄弟”*。这样，可以即时定义图像的不同分类。这些样本展示了模型生成开放式输出并适应图像和文本的能力，以及利用它在语言预训练期间学习到的知识。该模型是一个概念证明，展示了为图像-文本任务生成少样本模型的方法。
- en: 7.2.6 Generating Images from Text
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.6 从文本生成图像
- en: By training on text-image pairs, transformers can acquire the knowledge to generate
    images corresponding to text descriptions. By successively producing the next
    token with a language model, it is possible to predict visual tokens, which then
    can be synthesized to images. However, there are other image generation techniques.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在文本-图像对上训练，转换器可以获取生成与文本描述相对应的图像的知识。通过使用语言模型逐个产生下一个标记，可以预测视觉标记，然后将这些标记合成图像。然而，还有其他图像生成技术。
- en: '*Variational Auto-Encoders* (*VAE*) compress an input image to a small latent
    representation and reconstruct the image as good as possible. An additional loss
    term ensures that the distribution of latent representations follows a Gaussian
    [[79](#CR79)].'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*变分自编码器* (*VAE*) 将输入图像压缩到一个小的潜在表示，并尽可能好地重建图像。一个额外的损失项确保潜在表示的分布遵循高斯分布 [[79](#CR79)]。'
- en: '*Generative Adversarial Networks* (*GAN*) use a generator to transform a noise
    vector ***s*** to an image ![$$\tilde {{\boldsymbol {x}}}=G(\boldsymbol {s})$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq3.png).
    Then a discriminator *D*(***x***) has the task to classify its input as synthetic
    image ![$$\tilde {{\boldsymbol {x}}}$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq4.png)
    or real image ***x*** [[53](#CR53)]. Both networks are trained alternately with
    an adversarial loss.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成对抗网络* (*GAN*) 使用一个生成器将噪声向量 ***s*** 转换为图像 ![~{{\boldsymbol{x}}}=G(\boldsymbol{s})](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq3.png)。然后，判别器
    *D*(***x***) 的任务是将其输入分类为合成图像 ![~{{\boldsymbol{x}}}](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq4.png)
    或真实图像 ***x*** [[53](#CR53)]。这两个网络交替使用对抗性损失进行训练。'
- en: Lee et al. [[91](#CR91)] give a survey of techniques for text driven image generation
    and manipulation.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Lee 等人 [[91](#CR91)] 对文本驱动图像生成和操作的技术进行了综述。
- en: There are a number of approaches to measure the quality of generated images.
    The *Inception Score* (*IS*) [[150](#CR150)] applies a CNN-based *Inception model*
    [[162](#CR162)] trained on ImageNet to every generated image to get a conditional
    class label distribution, which should concentrate on few classes, i.e. have low
    entropy. In addition, many different classes should be generated for the test
    data, which is captured by the defined IS measure. The *Fréchet Inception Distance*
    (*FID*) [[64](#CR64)] is an improved measure using the Fréchet distance between
    ImageNet classifier distributions, which measures the similarity of the distributions
    taking into account the location and ordering of the points along the graph. *CLIP
    Similarity Score* (CLIPSIM) [[72](#CR72)] is based on the CLIP model (Sect. [7.2.4](#Sec16)).
    It generates image and text embeddings with CLIP and calculates their cosine similarity.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以衡量生成图像的质量。*Inception Score* (*IS*) [[150](#CR150)] 将基于 ImageNet 训练的 CNN-based
    *Inception 模型* [[162](#CR162)] 应用到每个生成的图像上，以获得条件类别标签分布，这些分布应集中在少数类别上，即具有低熵。此外，对于测试数据，应生成许多不同的类别，这被定义为
    IS 衡量。*Fréchet Inception Distance* (*FID*) [[64](#CR64)] 是一个改进的度量，使用 ImageNet
    分类器分布之间的 Fréchet 距离，它测量了考虑了沿图的位置和点顺序的分布相似性。*CLIP Similarity Score* (CLIPSIM) [[72](#CR72)]
    基于 CLIP 模型（第 [7.2.4](#Sec16) 节）。它使用 CLIP 生成图像和文本嵌入，并计算它们的余弦相似度。
- en: '**DALL-E** [[133](#CR133)] uses a *GPT-3* autoregressive language model with
    12B parameters to generate a new image from a textual description. The caption
    text of the image is BPE-encoded into 256 tokens. Then each 256 × 256 image is
    compressed to a 32 × 32 grid of image tokens using a discrete variational autoencoder.
    Each image token represents its 8 × 8 pixels by 8192 possible values. The caption
    tokens are concatenated with the 32 × 32 = 1024 image tokens forming the input
    sequence of GPT-3.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**DALL-E** [[133](#CR133)] 使用了具有 12B 参数的 *GPT-3* 自回归语言模型，从文本描述中生成新的图像。图像的标题文本被
    BPE 编码成 256 个标记。然后，每个 256×256 的图像通过离散变分自动编码器压缩成 32×32 的图像标记网格。每个图像标记代表其 8×8 像素通过
    8192 个可能值。标题标记与 32×32=1024 个图像标记连接，形成 GPT-3 的输入序列。'
- en: 'In the first stage the image tokens are trained yielding continuous image values.
    Then the discrete image tokens are obtained by training with a *Gumbel-softmax
    relaxation* [[75](#CR75)] (Sect. [7.1.3](#Sec4)). In the second stage a *Sparse
    Transformer* [[27](#CR27)] with 64 self-attention layers and 12B parameters is
    trained to sequentially generate the joint input sequence. For the image tokens,
    special attention masks are used: row, column, or convolutional attention masks.
    The model was trained on 250M text-image pairs from the Internet.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段，通过训练图像标记生成连续的图像值。然后，通过使用 *Gumbel-softmax relaxation* [[75](#CR75)]（第 [7.1.3](#Sec4)
    节）进行训练，获得离散的图像标记。在第二阶段，训练了一个具有 64 个自注意力层和 12B 参数的 *Sparse Transformer* [[27](#CR27)]，以顺序生成联合输入序列。对于图像标记，使用了特殊注意力掩码：行、列或卷积注意力掩码。该模型在来自互联网的
    250M 个文本-图像对上进行了训练。
- en: For image generation, the authors rerank the samples drawn from the transformer
    using a pre-trained contrastive model, which assigns a score based on how well
    the image matches the caption. Figure [7.12](#Fig12) shows different images sampled
    from the algorithm. In a comparison to the prior model DF-GAN [[165](#CR165)],
    the images generated by DALL-E were chosen as most realistic and more matching
    the caption in more than 90% of the time. Similarly, the images generated by X-LXMERT
    [[28](#CR28)] look inferior.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig12_HTML.png)
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像生成，作者使用预训练的对比模型重新排序从 Transformer 中抽取的样本，该模型根据图像与标题匹配的程度分配分数。图 [7.12](#Fig12)
    展示了算法抽取的不同图像。在与先前的模型 DF-GAN [[165](#CR165)] 的比较中，DALL-E 生成的图像在超过 90% 的时间中被选为最真实且与标题更匹配的图像。同样，X-LXMERT
    [[28](#CR28)] 生成的图像看起来较差。![图片](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig12_HTML.png)
- en: 2 sets of 4 panels of photo display the best of 1 and the best of 512\. The
    photos are a group of urinals near the trees, a woman and a man stand next to
    a bush bench, a man rides a bike down a street past a young man, and a truck stopped
    at an intersection where construction barriers are up.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 2 组 4 个面板的图片展示了 1 和 512 中的最佳图片。照片展示了一组树木附近的洗手间，一位女士和一位男士站在灌木丛的长椅旁，一位男士骑着自行车穿过街道，旁边是一位年轻人，还有一辆停在路口的卡车，那里设置了施工障碍物。
- en: Fig. 7.12
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12
- en: According to a natural language caption (top) a number of images are generated
    by DALL-E [[133](#CR133)]. The middle row shows images generated by DALL-E corresponding
    to the caption. The lower row shows the best image from a sample of 512 automatically
    selected by a quality score. Image reprinted with kind permission of the authors
    [[133](#CR133), p. 6]
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 根据自然语言描述（顶部），DALL-E [[133](#CR133)] 生成了一系列图像。中间行显示了与描述对应的 DALL-E 生成的图像。底部行显示了从自动选择的
    512 个样本中选出的最佳图像。图像经作者许可重印 [[133](#CR133)，第 6 页]
- en: '**GauGAN2** [[122](#CR122), [149](#CR149)] combines segmentation mapping, inpainting
    and text-to-image generation in a single model. It is one of the first semantic
    image synthesis models that can produce photorealistic outputs for diverse scenes
    including indoor, outdoor, landscape, and street scenes. The recent version also
    can generate images according to text input. The model behind GauGAN2 was trained
    on 10 million high-quality landscape images. Details of the model are not known.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**GauGAN2** [[122](#CR122), [149](#CR149)] 将分割映射、修复和文本到图像生成结合在一个单一模型中。它是首批能够为室内、室外、风景和街景等多样化场景生成逼真输出的语义图像合成模型之一。最新版本还可以根据文本输入生成图像。GauGAN2
    背后的模型是在 1000 万张高质量风景图像上训练的。该模型的详细信息尚不清楚。'
- en: '**XMC-GAN** [[192](#CR192)] is a GAN-based text-to-image generation model containing
    a generator for synthesizing images, and a discriminator that is trained to discriminate
    real and generated images. It maximizes the mutual information between the corresponding
    pairs: (1) images (real or generated) with a sentence describing the scene; (2)
    a generated image and a real image with the same description; and (3) regions
    of an image (real or generated) and words or phrases associated with them. The
    goal is for the matching pairs (both text-to-image and real image-to-generated
    image) to have high similarity scores and for non-matching pairs to have low scores.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**XMC-GAN** [[192](#CR192)] 是一个基于 GAN 的文本到图像生成模型，包含用于合成图像的生成器以及训练用于区分真实和生成图像的判别器。它最大化对应对之间的互信息：(1)
    描述场景的句子与（真实或生成的）图像；(2) 具有相同描述的生成图像和真实图像；(3) 图像（真实或生成的）区域及其相关的单词或短语。目标是使匹配对（文本到图像和真实图像到生成图像）具有高相似度得分，而使不匹配对具有低得分。'
- en: For the input text the model computes a global sentence embedding *emb*[*s*]
    and the word embeddings *emb*[*w*] from a pre-trained BERT module. *emb*[*s*]
    and random noise *z* from a standard Gaussian distribution are concatenated to
    form the *global condition*, which is passed through several up-sampling blocks
    to generate a 16 × 16 feature map. The global condition is also used as the condition
    to calculate scale parameter and shift parameter in conditional batch normalization
    layers. The word embeddings *emb*[*w*] are input for an “attentional self-modulation
    layer” to generate fine-grained image regions. On MS-COCO, XMC-GAN improves the
    Sota FID-score (Sect. [7.2.6](#Sec18)) from 24.7 to 9.3, and is significantly
    preferred by human evaluators. Similarly, human raters prefer the image quality
    of XMC-GAN generated images 77% of the time, and 74% prefer its image-text alignment
    compared to three other Sota approaches (CP-GAN, SD-GAN, and OP-GAN).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入文本，模型从预训练的 BERT 模块计算全局句子嵌入 *emb*[*s*] 和单词嵌入 *emb*[*w*]。*emb*[*s*] 和来自标准高斯分布的随机噪声
    *z* 连接形成 *全局条件*，该条件通过几个上采样块生成一个 16×16 特征图。全局条件还用作计算条件批量归一化层中的尺度参数和偏移参数的条件。单词嵌入
    *emb*[*w*] 输入到“注意力自调制层”以生成精细的图像区域。在 MS-COCO 上，XMC-GAN 将 Sota FID-score（第 [7.2.6](#Sec18)
    节）从 24.7 提高到 9.3，并且显著优于人类评估者。同样，人类评分者 77% 的时间更喜欢 XMC-GAN 生成的图像质量，74% 的人更喜欢其图像与文本的对齐，与三种其他
    Sota 方法（CP-GAN、SD-GAN 和 OP-GAN）相比。
- en: '**Cogview** [[40](#CR40)] employs a *Vector Quantized Variational AutoEncoder*
    (*VQ-VAE*). In the first stage, a discrete autoencoder is used to transform the
    image into a discrete sequence of tokens. In the second stage a GPT model learns
    to generate image tokens based on a prompt of SentencePiece text tokens. To generate
    image tokens, an encoder maps an image ![$${\boldsymbol {x}}\in \mathbb {R}^{H\times
    W \times 3}$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq5.png)
    to *h* × *w* image patches, which are quantized to a nearby embedding in a learnable
    set {***u***[1], …, ***u***[*k*]} of embedding vectors ![$$\boldsymbol {u}_i\in
    \mathbb {R}^d$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq6.png)
    [[113](#CR113)]. The decoder maps the embeddings back to the image, and the embeddings
    are selected to minimize the difference between output and input image.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**Cogview** [[40](#CR40)] 采用了一种 *向量量化变分自编码器* (*VQ-VAE*)。在第一阶段，使用一个离散自编码器将图像转换为一个离散的标记序列。在第二阶段，一个
    GPT 模型学习根据 SentencePiece 文本标记的提示生成图像标记。为了生成图像标记，编码器将图像 ![$${\boldsymbol {x}}\in
    \mathbb {R}^{H\times W \times 3}$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq5.png)
    映射到 *h*×*w* 图像块，这些块被量化到一个可学习的嵌入集 {***u***[1], …, ***u***[*k*]} 中的附近嵌入，该嵌入集包含嵌入向量
    ![$$\boldsymbol {u}_i\in \mathbb {R}^d$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq6.png)
    [[113](#CR113)]。解码器将嵌入映射回图像，并选择嵌入以最小化输出图像与输入图像之间的差异。'
- en: The GPT-model of CogView has 48 layers with a hidden size of 2560, 40 attention
    heads and 4B parameters. The input to the model is of the form *“[ROI1]* < *text
    tokens*>  *[BASE] [BOI1]* < *image tokens*>  *[EOI1]”* and contains special tokens.
    The pre-training task is to predict tokens from left to right for 30M text-image
    pairs in English and Chinese. A sparse attention pattern similar to BigBird (Sect.
    [3.​2.​1](528393_1_En_3_Chapter.xhtml#Sec8)) is used.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: CogView 的 GPT 模型有 48 层，隐藏大小为 2560，40 个注意力头和 4B 个参数。模型的输入形式为 *“[ROI1]* < *text
    tokens*>  *[BASE] [BOI1]* < *image tokens*>  *[EOI1]”*，并包含特殊标记。预训练任务是预测 30M 英文和中文文本-图像对的标记，从左到右。使用与
    BigBird (Sect. [3.​2.​1](528393_1_En_3_Chapter.xhtml#Sec8)) 类似的稀疏注意力模式。
- en: As shown in Fig. [7.13](#Fig13), CogView has a similar performance in image
    generation as DALL-E. It achieves the Sota FID on the blurred MS COCO dataset,
    outperforming previous GAN-based models and DALL-E, although DALL-E has three
    times more parameters. When evaluated by humans, CogView was able to beat GAN-based
    models by a large margin. However, generation of images with CogView is rather
    slow, because each image is generated token-by-token. In addition, the quantization
    leads to some blurriness in the images.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig13_HTML.png)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [7.13](#Fig13) 所示，CogView 在图像生成方面的性能与 DALL-E 类似。它在模糊的 MS COCO 数据集上实现了 Sota
    FID，超过了之前的基于 GAN 的模型和 DALL-E，尽管 DALL-E 的参数多出三倍。当由人类评估时，CogView 能够以很大的优势击败基于 GAN
    的模型。然而，由于每个图像都是逐个标记生成的，因此使用 CogView 生成图像相当缓慢。此外，量化导致图像中存在一些模糊。
- en: 4 photos. 1 a beautiful young blonde woman talking on a phone. 2, a Big Ben
    clock towering over the city of London. 3, Chinese traditional drawing of the
    Statue of Liberty. 4, an oil painting of a lion.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 4 张照片。1 张美丽的年轻金发女郎在打电话。2，大本钟矗立在伦敦市上空。3，中国传统的自由女神像绘画。4，一头狮子的油画。
- en: Fig. 7.13
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13
- en: Images generated by CogView [[40](#CR40)] controlled by the text input (top).
    The image style can be influenced by the input text. The best of a sample of 60
    images is selected. Image reprinted with kind permission of the authors [[40](#CR40),
    p. 1]
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 由 CogView [[40](#CR40)] 生成的图像受文本输入控制（顶部）。图像风格可以受到输入文本的影响。从 60 张图像样本中选出的最佳图像。图像经作者许可重新印刷
    [[40](#CR40), p. 1]
- en: '**LAFITE** [[200](#CR200)] is a model for generating images from text. Image
    generation is based on *StyleGAN2* [[82](#CR82)], which creates various image
    attributes by modulating the weights of the convolution kernels [[177](#CR177)].
    LAFITE generates these modulating signals based on language input. It relies on
    the multimodal semantic space of the pre-trained CLIP model (Sect. [7.2.4](#Sec16))
    to produce an image embedding *emb*(***x***) from a text ***x***, and therefore
    does not need extra text data. This image embedding is inserted into the image
    generation model similar to StyleGAN2 by a GAN architecture. On the MS-COCO benchmark,
    LAFITE achieves a zero-shot FID value of 26.9, which is better than the values
    of DALL-E (27.5) and CogView (27.1). When fine-tuned on MS-COCO, LAFITE has a
    FID-score of 8.1, which is better than that of XMC-GAN (9.3) and other GAN models.
    Note that LAFITE only has 75M trainable parameters.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**LAFITE** [[200](#CR200)] 是一种从文本生成图像的模型。图像生成基于 *StyleGAN2* [[82](#CR82)]，通过调节卷积核的权重来创建各种图像属性
    [[177](#CR177)]。LAFITE根据语言输入生成这些调节信号。它依赖于预训练的 CLIP 模型的多模态语义空间（第 [7.2.4](#Sec16)
    节），从文本 ***x*** 生成图像嵌入 *emb*(***x***)，因此不需要额外的文本数据。这个图像嵌入通过 GAN 架构类似于 StyleGAN2
    插入到图像生成模型中。在 MS-COCO 基准测试中，LAFITE 实现了零样本 FID 值 26.9，优于 DALL-E (27.5) 和 CogView
    (27.1)。当在 MS-COCO 上微调时，LAFITE 的 FID 分数为 8.1，优于 XMC-GAN (9.3) 和其他 GAN 模型。请注意，LAFITE
    只有 75M 个可训练参数。'
- en: 7.2.7 Diffusion Models Restore an Image Destructed by Noise
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.7 扩散模型恢复由噪声破坏的图像
- en: '**GLIDE** [[109](#CR109)] is an image generation technique based on a *diffusion
    model*. A diffusion model describes the process of systematically and slowly destroying
    structure in a data distribution through an iterative forward *diffusion process*,
    e.g. the addition of noise [[157](#CR157)]. To the data ***x***^([0]), e.g. a
    matrix of pixel values, we can apply Gaussian diffusion distribution *q*(***x***^([*t*])|***x***^([*t*−1])),
    where a Gaussian with expectation ***0*** and covariance *β****I*** is added.
    This yields a series ***x***^([0]), …, ***x***^([*T*]) where the final distribution
    ***x***^([*T*]) approximately is a Gaussian distribution with identity covariance
    (similar results hold for the binomial distribution).'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**GLIDE** [[109](#CR109)] 是一种基于 *扩散模型* 的图像生成技术。扩散模型描述了通过迭代的前向 *扩散过程* 系统性地和缓慢地破坏数据分布中的结构的过程，例如添加噪声
    [[157](#CR157)]。对于数据 ***x***^([0])，例如像素值的矩阵，我们可以应用高斯扩散分布 *q*(***x***^([*t*])|***x***^([*t*−1]))，其中添加了一个期望为
    ***0*** 和协方差 *β****I*** 的高斯。这产生了一系列 ***x***^([0])，…，***x***^([*T*])，其中最终的分布 ***x***^([*T*])
    大约是一个具有单位协方差的高斯分布（对于二项分布也得到类似的结果）。'
- en: Now the reversal of the diffusion process can be defined, i.e. the generative
    distribution with ***x***^([*t*−1]) ∼ *p*(***x***^([*t*−1])|***x***^([*t*])).
    It has been shown by Feller [[47](#CR47)] that for small step size *β* the conditional
    distribution *p*(***x***^([*t*−1])|***x***^([*t*])) will approximately be a Gaussian
    distribution. Hence, the chain ***x***^([*T*]), …, ***x***^([0]) can be generated
    by a Gaussian distribution![$$\displaystyle \begin{aligned} {\boldsymbol{x}}^{[t-1]}\sim
    N(\boldsymbol{\mu}_{\boldsymbol{w}}({\boldsymbol{x}}^{[t]});\boldsymbol{S}_{\boldsymbol{w}}({\boldsymbol{x}}^{[t]}))
    \quad \text{and} \quad  {\boldsymbol{x}}^{[T]}\sim N(\boldsymbol{0};\boldsymbol{I}))
    {}. \end{aligned} $$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_Equ3.png)(7.3)
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以定义扩散过程的逆过程，即生成分布 ***x***^([*t*−1]) ∼ *p*(***x***^([*t*−1])|***x***^([*t*])).
    费勒 [[47](#CR47)] 已经证明，对于小的步长 *β*，条件分布 *p*(***x***^([*t*−1])|***x***^([*t*])) 将近似为高斯分布。因此，链
    ***x***^([*T*])，…，***x***^([0]) 可以通过高斯分布生成！[$$\displaystyle \begin{aligned} {\boldsymbol{x}}^{[t-1]}\sim
    N(\boldsymbol{\mu}_{\boldsymbol{w}}({\boldsymbol{x}}^{[t]});\boldsymbol{S}_{\boldsymbol{w}}({\boldsymbol{x}}^{[t]}))
    \quad \text{and} \quad  {\boldsymbol{x}}^{[T]}\sim N(\boldsymbol{0};\boldsymbol{I}))
    {}. \end{aligned} $$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_Equ3.png)(7.3)
- en: This Gaussian distribution is completely defined by the mean and covariance
    of ***x***^([*t*]).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这个高斯分布完全由 ***x***^([*t*]) 的均值和协方差定义。
- en: For the training, noisy samples ***x***^([*t*]) are generated by *q*(***x***^([*t*])|***x***^([*t*−1]))
    starting with the observed ***x***^([0]). From this the inverse *p*(***x***^([*t*−1])|***x***^([*t*]))
    may be reconstructed by optimizing the *variational lower bound* on negative log
    likelihood [[65](#CR65)]. With the trained model one can start with a sample ***x***^([*T*]) ∼ *N*(***0***,
    ***I***) and gradually reduce noise in a sequence of steps ***x***^([*T*−1]),
    …, ***x***^([0]), where![$$\displaystyle \begin{aligned} {\boldsymbol{x}}^{[t-1]}\sim
    p({\boldsymbol{x}}^{[t-1]}|{\boldsymbol{x}}^{[t]}) \approx N(\boldsymbol{\mu}_{\boldsymbol{w}}({\boldsymbol{x}}^{[t]});\boldsymbol{S}_{\boldsymbol{w}}({\boldsymbol{x}}^{[t]}))
    . \end{aligned} $$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_Equ4.png)(7.4)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，通过 *q*(***x***^([*t*])|***x***^([*t*−1])) 从观察到的 ***x***^([0]) 生成带噪声的样本
    ***x***^([*t*])。从这些样本中，可以通过优化负对数似然率的 *变分下界* [[65](#CR65)] 来重建逆 *p*(***x***^([*t*−1])|***x***^([*t*]))。使用训练好的模型，可以从一个样本
    ***x***^([*T*]) ∼ *N*(***0***, ***I***) 开始，并在一系列步骤 ***x***^([*T*−1]), …, ***x***^([0])
    中逐步减少噪声，其中![$$\displaystyle \begin{aligned} {\boldsymbol{x}}^{[t-1]}\sim p({\boldsymbol{x}}^{[t-1]}|{\boldsymbol{x}}^{[t]})
    \approx N(\boldsymbol{\mu}_{\boldsymbol{w}}({\boldsymbol{x}}^{[t]});\boldsymbol{S}_{\boldsymbol{w}}({\boldsymbol{x}}^{[t]}))
    . \end{aligned} $$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_Equ4.png)(7.4)
- en: The distributions *p*(***x***^([*t*−1])|***x***^([*t*])) may be estimated conditional
    to image classes [[37](#CR37)]. Instead of a finite number of image classes one
    may even use a caption text as condition. The text is first encoded into a sequence
    of *k* tokens and fed into a Transformer model. The Transformer outputs a class
    embedding as well as *k* token embeddings, which are used as additional model
    inputs. Here a normal noise term *𝜖*[***w***](***x***^([*t*])|∅) for reconstruction
    is estimated and in addition conditional to the caption *c* a noise term *𝜖*[***w***](***x***^([*t*])|*c*).
    During the *classifier-free reconstruction* both terms are mixed.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(***x***^([*t*−1])|***x***^([*t*])) 的分布可以基于图像类别进行估计 [[37](#CR37)]。除了有限数量的图像类别外，甚至可以使用标题文本作为条件。首先将文本编码成
    *k* 个令牌的序列，然后输入到 Transformer 模型中。Transformer 输出一个类别嵌入以及 *k* 个令牌嵌入，这些嵌入被用作额外的模型输入。这里估计了一个用于重建的正常噪声项
    *𝜖*[***w***](***x***^([*t*])|∅)，以及一个基于标题 *c* 的噪声项 *𝜖*[***w***](***x***^([*t*])|*c*)。在
    *无分类器重建* 过程中，这两个项被混合。'
- en: The diffusion model is approximated by a *U-Net* model [[144](#CR144)] with
    2.3B parameters, performing a downsampling of the 64 pixel image to a smaller
    resolution with many features and a subsequent upsampling. An additional 1.5B
    parameter model is used for upsampling to a 256 × 256 resolution. The caption
    text is processed by a transformer model with 1.2B parameters and the final token
    embedding is used in place of a class embedding.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型由一个具有 2.3B 个参数的 *U-Net* 模型 [[144](#CR144)] 近似，它将 64 像素图像下采样到较小分辨率，具有许多特征，然后进行上采样。还使用了一个额外的
    1.5B 参数模型用于上采样到 256×256 分辨率。标题文本由一个具有 1.2B 参数的变压器模型处理，最终令牌嵌入用于替代类别嵌入。
- en: In tests, GLIDE produced high-quality images with realistic reflections, textures,
    and shadows. The model can also combine multiple concepts (for example, dragon,
    psychedelic, and hamster) and attach attributes like colors to these concepts.
    On the MS-COCO benchmark with 256 × 256 images DALL-E achieves a FID-value of
    28, while LAFITE gets 26.9 and GLIDE 12.2\. Also in human evaluations, the results
    of GLIDE are clearly preferred. This is remarkable as GLIDE has far less parameters
    than DALL-E. Figure [7.14](#Fig14) shows some images generated by GLIDE. GLIDE
    can also be used for restoring a masked image patch according to a textual prompt,
    e.g. *“tie with black and yellow stripes”*. In most cases, GLIDE produces better
    results than competitor models and the corresponding image patch is restored with
    realistic lighting, shadows and textures. Finally, GLIDE can add shadows and reflections
    to images and transform simple line sketches into photorealistic images.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig14_HTML.png)
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试中，GLIDE 生成了具有逼真反射、纹理和阴影的高质量图像。该模型还可以结合多个概念（例如，龙、迷幻和仓鼠）并将属性如颜色附加到这些概念上。在 MS-COCO
    基准测试中，256×256 图像的 DALL-E 实现了 28 的 FID 值，而 LAFITE 得到 26.9，GLIDE 得到 12.2。在人类评估中，GLIDE
    的结果也明显更受欢迎。这很引人注目，因为 GLIDE 的参数远少于 DALL-E。图 [7.14](#Fig14) 展示了一些由 GLIDE 生成的图像。GLIDE
    还可以根据文本提示恢复掩码图像块，例如 *“用黑白条纹系上”*。在大多数情况下，GLIDE 的结果比竞争对手模型更好，并且相应的图像块以逼真的光照、阴影和纹理恢复。最后，GLIDE
    可以向图像添加阴影和反射，并将简单的线条草图转换为逼真的图像。![图片](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig14_HTML.png)
- en: 4 photos. 1, a group of elephants walking in muddy water. 2, a group of skiers
    preparing to ski. 3, a hedgehog using a calculator. 4, a high-quality oil painting
    of a psychedelic hamster dragon.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 4 张照片。1，一群大象在泥水中行走。2，一群滑雪者准备滑雪。3，一只刺猬在使用计算器。4，一幅高质量的迷幻仓鼠龙油画。
- en: Fig. 7.14
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14
- en: Images generated by GLIDE [[109](#CR109)] according to the captions in the lower
    row. The best of a sample of 60 is shown. Image reprinted with kind permission
    of the authors [[109](#CR109), p. 7]
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 根据下行的标题生成的 GLIDE [[109](#CR109)] 图像。展示了 60 个样本中的最佳图像。图片经作者许可重新印刷 [[109](#CR109)，第
    7 页]
- en: '**DALL-E 2** [[132](#CR132)] is an improved version of DALL-E that can create
    more realistic art and images from a descriptive sentence in natural language.
    It works in two steps (Fig. [7.15](#Fig15)): first a CLIP (Sect. [7.2.4](#Sec16))
    image embedding *z*[*i*] based on a text description *y* is generated according
    to a prior *p*(*z*[*i*]|*y*). Then a diffusion-based decoder generates an image
    *x* conditioned on an image embedding *z*[*i*]. The decoder *p*(*x*|*z*[*i*],
    *y*) inverts the CLIP image encoder, is non-deterministic, and can produce multiple
    images corresponding to a given image embedding. The CLIP model is frozen during
    training of the prior and decoder. The dimensionality of the image embeddings
    *z*[*i*] is reduced to 319 from 1024 by principal component analysis while preserving
    nearly all information. Each of the 319 dimensions is quantized into 1024 discrete
    buckets. For the encoder, experiments are performed with both autoregressive and
    diffusion models for the prior. It turns out that diffusion models are computationally
    more efficient and produce higher-quality samples. Examples are shown in Fig.
    [7.16](#Fig16).![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig15_HTML.png)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**DALL-E 2** [[132](#CR132)] 是 DALL-E 的改进版本，可以从自然语言的描述句子中创建更逼真的艺术和图像。它分为两个步骤（图
    [7.15](#Fig15)）：首先根据文本描述 *y* 生成一个基于 CLIP（第 [7.2.4](#Sec16) 节）图像嵌入 *z*[*i*] 的先验
    *p*(*z*[*i*]|*y*)。然后，基于图像嵌入 *z*[*i*] 的扩散解码器生成图像 *x*。解码器 *p*(*x*|*z*[*i*], *y*)
    反转 CLIP 图像编码器，是非确定性的，并且可以产生与给定图像嵌入相对应的多个图像。在训练先验和解码器期间，CLIP 模型被冻结。通过主成分分析将图像嵌入
    *z*[*i*] 的维度从 1024 减少到 319，同时保留几乎所有信息。每个 319 维度被量化为 1024 个离散桶。对于编码器，对先验进行了自回归和扩散模型的实验。结果表明，扩散模型在计算上更高效，并产生更高质量的样本。示例如图
    [7.16](#Fig16) 所示。![图片](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig15_HTML.png)'
- en: A block diagram illustrates a corgi playing a flame-throwing trumpet, along
    with a text encoder, C L I P objective, prior, image encoder, decoder. Encoder
    and decoder has a photo of a dog holding a trumpet in its mouth in 2 opposite
    directions.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一个框图展示了柯基吹火焰喷射号角的场景，以及文本编码器、C L I P 目标、先验、图像编码器、解码器。编码器和解码器展示了一只狗嘴里拿着喇叭的照片，方向相反。
- en: Fig. 7.15
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15
- en: 'A high-level overview of DALL-E 2 [[132](#CR132)]. Above the dotted line the
    CLIP training process is shown minimizing the difference between the embeddings
    for an image and the corresponding text. Below the dotted line, the text-to-image
    generation process is illustrated: a CLIP text embedding is first fed to an autoregressive
    transformer (higher box) or diffusion prior (lower box) to produce an image embedding.
    This embedding is used as input to the diffusion decoder which produces a final
    image. Image reprinted with kind permission of the authors [[132](#CR132), p.
    3]'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: DALL-E 2 [[132](#CR132)] 的高级概述。虚线以上展示了 CLIP 训练过程，最小化图像嵌入与对应文本之间的差异。虚线以下，展示了文本到图像的生成过程：首先将
    CLIP 文本嵌入输入到一个自回归变压器（上方框）或扩散先验（下方框）以生成图像嵌入。这个嵌入被用作扩散解码器的输入，生成最终图像。图片经作者许可重新印刷
    [[132](#CR132)，第 3 页]
- en: '![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig16_HTML.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig16_HTML.png)'
- en: 2 panels of 4 photos in 2 rows. Row 1 illustrates a person named Salvador Dali
    with robotic sketches on different parts of their face. Row illustrates a teddy
    bear on a skateboard in different positions on the skateboard.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 2 行 4 张照片的 2 个面板。第一行展示了名叫萨尔瓦多·达利的一个人在脸部不同部位画有机器人草图。第二行展示了一只熊在滑板上的不同位置。
- en: Fig. 7.16
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16
- en: Random samples from DALL-E 2 [[132](#CR132)] for the prompt *“Vibrant portrait
    painting of Salvador Dali with a robotic half face”* (upper row), and *“A teddybear
    on a skateboard in Times Square”*. Image reprinted with kind permission of the
    authors [[132](#CR132), p. 25,27]
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 从 DALL-E 2 中随机抽取的样本 [[132](#CR132)]，用于提示 *“萨尔瓦多·达利的充满活力的肖像画，一半是机器人脸”*（上排），以及
    *“时代广场上的滑板熊”*。图片经作者同意重新印刷 [[132](#CR132)，第 25、27 页]
- en: The decoder is conditioned on image representations and can produce variations
    of an image that preserve both its semantics and style, while varying the nonessential
    details that are missing from the image embeddings. CLIP’s shared embedding space
    allows for language-guided image manipulations and modifications in a zero-shot
    manner. For example two images *x*[1] and *x*[2] can be blended, interpolating
    all of the concepts in CLIP’s embedding space that occur between them. With respect
    to MSCOCO it turns out that DALL-E 2 has a better zero-shot FID of 10.4 than GLIDE
    (12.2). Human comparisons show that DALL-E 2 and GLIDE are similar in terms of
    photorealism and caption similarity, while DALL-E 2 produces images with greater
    diversity. DALL-E 2 struggles more than GLIDE with a prompt that requires it to
    connect two separate objects (cubes) to two separate attributes (colors). A public
    access to DALL-E is now available for users to create images [[115](#CR115)].
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器基于图像表示进行条件化，可以生成保留图像语义和风格的图像变体，同时改变图像嵌入中缺失的非必要细节。CLIP 的共享嵌入空间允许以零样本方式通过语言引导图像操作和修改。例如，两个图像
    *x*[1] 和 *x*[2] 可以混合，在 CLIP 的嵌入空间中插值它们之间出现的所有概念。对于 MSCOCO，结果发现 DALL-E 2 的零样本 FID
    为 10.4，优于 GLIDE（12.2）。人类比较显示，DALL-E 2 和 GLIDE 在照片真实性和标题相似性方面相似，而 DALL-E 2 生成的图像具有更大的多样性。当需要将两个单独的物体（立方体）连接到两个单独的属性（颜色）时，DALL-E
    2 比 GLIDE 遇到的挑战更多。现在，DALL-E 2 的公开访问已对用户开放，以便他们创建图像 [[115](#CR115)]。
- en: '**Imagen** [[148](#CR148)] is a text-to-image model presented by Google. It
    encodes the input text into text embeddings by a pre-trained T5-XXL encoder-decoder
    Transformer with 4.6B frozen parameters. A conditional text-to-image diffusion
    model ([7.3](#Equ3)) maps the text embeddings into a 64 × 64 image. Subsequently
    these small images are upsampled in two steps to 256 × 256 and to 1024 × 1024
    by two super-resolution diffusion models with 600M and 400M parameters (Fig. [7.17](#Fig17)).
    The models are trained on 860M image-text pairs.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig17_HTML.png)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**Imagen** [[148](#CR148)] 是由 Google 提出的一种文本到图像模型。它通过一个预训练的具有 4.6B 个冻结参数的 T5-XXL
    编码器-解码器 Transformer 将输入文本编码为文本嵌入。条件文本到图像扩散模型 ([7.3](#Equ3)) 将文本嵌入映射到一个 64×64 的图像。随后，这些小图像通过两个超分辨率扩散模型（具有
    600M 和 400M 个参数）在两步中上采样到 256×256 和 1024×1024（图 [7.17](#Fig17)）。这些模型在 860M 个图像-文本对上进行了训练。![图片](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig17_HTML.png)'
- en: A block diagram begins with text a golden retriever dog wearing a blue checkered
    beret and red dotted turtleneck. Text is fed into T 5 X X L encoder, followed
    by text-to-image diffusion, 2 steps of super resolution diffusion to produce a
    256 by 256 image and a final resolution of 1024 by 1024.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 块图从一只穿着蓝色格子贝雷帽和红色圆点高领毛衣的金毛犬的文本开始。文本被输入到 T 5 X X L 编码器，然后是文本到图像的扩散，经过 2 步超分辨率扩散以生成
    256×256 的图像和最终的 1024×1024 分辨率。
- en: Fig. 7.17
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17
- en: Imagen encodes the input text by the pre-trained T5-XXL text encoder. The resulting
    text embeddings are transformed to 64 × 64 images by a diffusion model [[148](#CR148)].
    This image is upscaled to 1024 × 1024 resolution by two super-resolution diffusion
    models. Image reprinted with kind permission of the authors [[148](#CR148), p.
    19] Nichol et al. [[110](#CR110)] proposed some modifications for denoising diffusion
    probabilistic models, which can sample much faster and achieve better log-likelihoods
    with little impact on sample quality. They deliver the same sample quality as
    GANs, but achieve a much better mode coverage as measured by recall. This model
    is also employed by Imagen for text-to-image conversion, using the pooled embedding
    vector as input. This network is used for upsampling and is extended to improve
    memory efficiency, inference time, and convergence speed. Figure [7.18](#Fig18)
    shows randomly selected images generated by Imagen for a caption input.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig18_HTML.png)
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Imagen 通过预训练的 T5-XXL 文本编码器对输入文本进行编码。通过扩散模型 [[148](#CR148)] 将得到的文本嵌入转换为 64×64
    的图像。此图像通过两个超分辨率扩散模型上采样到 1024×1024 分辨率。图像经作者许可重新印刷 [[148](#CR148), p. 19]。Nichol
    等人 [[110](#CR110)] 提出了一些对去噪扩散概率模型的修改，这些模型可以以更快的速度采样并实现更好的对数似然，同时对样本质量的影响很小。它们提供的样本质量与
    GANs 相同，但通过召回率测量的模式覆盖范围要好得多。该模型也被 Imagen 用于文本到图像的转换，使用池化嵌入向量作为输入。这个网络用于上采样，并扩展以提高内存效率、推理时间和收敛速度。图
    [7.18](#Fig18) 显示了由 Imagen 为标题输入随机选择的图像。![图 7.18](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig18_HTML.png)
- en: 2 pairs of similar photos. Pair 1, a confused grizzly bear in calculus class.
    Pair 2, The Rhine River below a castle and with a forest and a vineyard.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 两对相似的照片。第一对，一只困惑的灰熊在微积分课堂上。第二对，城堡下的莱茵河，以及森林和葡萄园。
- en: Fig. 7.18
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.18
- en: Images generated by Imagen [[148](#CR148), p.6] (left) and Stable Diffusion
    [[142](#CR142)] (right) given two different text captions. Images reprinted with
    kind permission of the authors [[148](#CR148), p. 6] and [[158](#CR158)], credits
    in Table [A.​3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Imagen [[148](#CR148), p.6] (左) 和 Stable Diffusion [[142](#CR142)] (右) 生成的图像，基于两个不同的文本标题。图像经作者许可重新印刷
    [[148](#CR148), p. 6] 和 [[158](#CR158)]，具体见表 [A.​3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)。
- en: Imagen achieves a Sota zero-shot FID (Fréchet Inception Distance) on *COCO*
    with a value of 7.3, which is better than the FID of DALL-E 2 and is even better
    than other models trained on COCO (Table [7.2](#Tab2)). Human raters evaluated
    Imagen with respect to photorealism and alignment to the text caption. For photorealism,
    people preferred Imagen images in 39.5% of cases to the original images, indicating
    a relatively high realism. On caption similarity, Imagen’s score is on-par with
    the original reference images. On the *DrawBench* [[147](#CR147)] the images generated
    by Imagen are always preferred to images created by DALL-E 2, GLIDE, VQGAN+CLIP
    or Latent Diffusion in more than 60% of the cases. The authors emphasize that
    in the future they will increase the size of the language model, as this promises
    a greater gain than increasing the size of the diffusion models. They do not publish
    Imagen’s code or provide a demo API because it could potentially be abused, for
    example to create fake images. Gafni et al. [[48](#CR48)] demonstrate how a system
    can be extended to support artists during the creation of images.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Imagen 在 *COCO* 数据集上实现了 Sota 零样本 FID (Fréchet Inception Distance) 的值 7.3，这比
    DALL-E 2 的 FID 更好，甚至比在 COCO 上训练的其他模型更好（见表 [7.2](#Tab2)）。人类评分员评估了 Imagen 在逼真度和与文本标题的一致性方面的表现。在逼真度方面，39.5%
    的情况下人们更喜欢 Imagen 的图像而不是原始图像，这表明相对较高的逼真度。在标题相似度方面，Imagen 的得分与原始参考图像相当。在 *DrawBench*
    [[147](#CR147)] 上，超过 60% 的情况下，由 Imagen 生成的图像比由 DALL-E 2、GLIDE、VQGAN+CLIP 或 Latent
    Diffusion 创建的图像更受欢迎。作者强调，未来他们将会增加语言模型的大小，因为这比增加扩散模型的大小能带来更大的收益。他们没有发布 Imagen 的代码或提供演示
    API，因为这可能会被滥用，例如创建假图像。Gafni 等人 [[48](#CR48)] 展示了如何将系统扩展以支持艺术家在图像创作过程中的需求。
- en: '**Stable Diffusion** is another model with currently 5.7B parameters for generating
    images of up to 1024 × 1024 pixels using diffusion. An example is shown in Fig.
    [7.18](#Fig18). It works similar to DALLE-2 employing a denoising U-Net for image
    compression and expansion [[142](#CR142)]. For training, Stable Diffusion used
    an image dataset from the freely available LAION-5B database [[12](#CR12)], which
    contains about 5.85 billion CLIP-filtered image-text pairs, fourteen times larger
    than its predecessor LAION-400M. A model conditioned on ImageNet classes achieved
    an FID of 3.6 for image generation. A variant of the model employs an image search
    returning images with similar visual features from the neighborhood of each training
    instance by the CLIP model [[15](#CR15)]. The model includes the retrieved images
    during image generation. It can be applied to unconditional image synthesis, inpainting,
    and stochastic super-resolution, and achieves competitive performance while significantly
    lowering computational cost. Model inference code and model weights to run the
    retrieval-augmented diffusion models are now available [[141](#CR141)] and can
    be downloaded. The model was heavily employed by users creating 1.7M images per
    day.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '**稳定扩散**是另一个具有目前5.7B参数的模型，用于使用扩散生成高达1024×1024像素的图像。一个示例如图[7.18](#Fig18)所示。它的工作原理与DALLE-2类似，使用去噪U-Net进行图像压缩和扩展[[142](#CR142)]。对于训练，稳定扩散使用了来自免费可用的LAION-5B数据库的图像数据集[[12](#CR12)]，包含约58.5亿个CLIP过滤的图像-文本对，是其前辈LAION-400M的十四倍。一个基于ImageNet类别的模型在图像生成上达到了3.6的FID。该模型的变体使用CLIP模型通过每个训练实例的邻域返回具有相似视觉特征的图像进行图像搜索[[15](#CR15)]。在图像生成过程中，模型包括检索到的图像。它可以应用于无条件图像合成、修复和随机超分辨率，并在显著降低计算成本的同时实现具有竞争力的性能。现在可以获取模型推理代码和模型权重，以运行检索增强的扩散模型[[141](#CR141)]，并可下载。该模型被用户广泛使用，每天创建170万张图像。'
- en: 7.2.8 Multipurpose Models
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.8 多用途模型
- en: '**OFA** (One For All) [[170](#CR170)] provides a unified model for a range
    of multimodal tasks. It can process text and images in the form of text and visual
    tokens. OFA has an encoder-decoder transformer architecture (Sect. [2.​3.​1](528393_1_En_2_Chapter.xhtml#Sec20))
    and is pre-trained on various text and image datasets. Similar to the T5 model
    (Sect. [3.​1.​3](528393_1_En_3_Chapter.xhtml#Sec4)), it receives a textual instruction
    along with an image and generates the appropriate output.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**OFA**（一应俱全）[[170](#CR170)]为一系列多模态任务提供了一个统一的模型。它可以处理文本和图像，形式为文本和视觉标记。OFA具有编码器-解码器transformer架构（第[2.3.1](528393_1_En_2_Chapter.xhtml#Sec20)节），并在各种文本和图像数据集上进行预训练。与T5模型（第[3.1.3](528393_1_En_3_Chapter.xhtml#Sec4)节）类似，它接收一个文本指令以及一个图像，并生成适当的输出。'
- en: Different modalities are represented in the same space, and text, images, and
    objects are discretized into a unified output vocabulary. An image with 256 × 256
    pixels is represented as 16 × 16 image patches. Each image patch of 16 × 16 pixels
    is “tokenized” into discrete visual tokens, such that each visual token strongly
    correlates to the corresponding patch [[11](#CR11)]. In addition, objects have
    a specific representation consisting of a label and its bounding box. The continuous
    corner coordinates of the bounding box are uniformly discretized to integers as
    location tokens (*x*[1];*y*[1];*x*[2];*y*[2]). Finally, a unified vocabulary is
    used for all linguistic and visual tokens, including subwords, image codes, and
    location tokens.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的模态在同一空间中表示，文本、图像和对象被离散化为统一的输出词汇表。一个256×256像素的图像表示为16×16图像块。每个16×16像素的图像块被“标记化”为离散的视觉标记，使得每个视觉标记与相应的块强相关[[11](#CR11)]。此外，对象具有由标签及其边界框组成的特定表示。边界框的连续角坐标被统一离散化为整数作为位置标记（*x*[1];*y*[1];*x*[2];*y*[2]）。最后，统一词汇表用于所有语言和视觉标记，包括子词、图像代码和位置标记。
- en: Similar to T5 (Sect. [3.​1.​3](528393_1_En_3_Chapter.xhtml#Sec4)) the transformer
    encoder-decoder is controlled by instructions. It receives a text instruction
    and an input image and generates a corresponding output, a text response and an
    image. A number of tasks are described by the examples shown in Fig. [7.19](#Fig19).
    Usually, the OFA model is fine-tuned on specific datasets to solve various tasks.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig19_HTML.png)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 与T5（第[3.1.3](528393_1_En_3_Chapter.xhtml#Sec4)节）类似，transformer编码器-解码器由指令控制。它接收一个文本指令和一个输入图像，并生成相应的输出，包括文本响应和图像。图[7.19](#Fig19)展示了描述的多个任务示例。通常，OFA模型在特定数据集上进行微调以解决各种任务！[](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig19_HTML.png)
- en: A photo of 3 members playing football. A block diagram explains photo leads
    to processing steps, include visual grounding, grounded captioning, text matching
    and captioning, visual question answering, object detection, image infilling,
    image generation, text infilling, and O F A encoder and decoder.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 三名成员踢足球的照片。一个框图解释了照片的处理步骤，包括视觉定位、基于视觉的标题、文本匹配和标题、视觉问答、目标检测、图像填充、图像生成、文本填充以及OFA编码器和解码器。
- en: Fig. 7.19
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19
- en: OFA [[170](#CR170), p. 3] receives an instruction and an input image. As output
    it generates a text and (optionally) an image. For each of the eight instructions
    (left) an example output (right) is shown. Image credits in Table [A.​3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: OFA [[170](#CR170), p. 3] 接收一条指令和一张输入图像。作为输出，它生成文本和（可选的）图像。对于每个八个指令（左侧）都展示了相应的示例输出（右侧）。图像版权信息见表[A.3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)。
- en: The OFA model has an OFA[Base] variant with 6 encoder and decoder layers, hidden
    size 768, and 12 attention heads. The OFA[Large] variant has 12 encoder and decoder
    layers, hidden size 1024, 16 attention heads and 472M parameters.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: OFA模型有一个OFA[Base]变体，包含6个编码器和解码器层，隐藏大小为768，以及12个注意力头。OFA[Large]变体有12个编码器和解码器层，隐藏大小为1024，16个注意力头和472M参数。
- en: During pre-training, the model has to solve three tasks requested by the corresponding
    instructions (Fig. [7.19](#Fig19)). The first task is image infilling, where the
    model has to reconstruct the central parts of the image. This requires the model
    to learn the relation of image parts and the generation of images. The second
    task is object detection. This task establishes the correspondence between image
    parts and language descriptions. The last pre-training task is text infilling
    to learn the structure of language. The model is pre-trained on publicly available
    datasets for the different tasks on data with more than 50M images and more than
    160GB text. Images are resized to 384 × 384 pixels with a fixed patch size of
    16 × 16 pixel. For each patch a feature vector is computed by the first three
    blocks of a ResNet CNN.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练期间，模型必须解决由相应指令请求的三个任务（图[7.19](#Fig19)）。第一个任务是图像填充，其中模型必须重建图像的中心部分。这要求模型学习图像部分之间的关系以及图像的生成。第二个任务是目标检测。该任务建立了图像部分与语言描述之间的对应关系。最后一个预训练任务是文本填充，以学习语言的结构。模型在公开可用的数据集上进行了预训练，这些数据集包含超过50M张图像和超过160GB的文本。图像被调整到384×384像素，固定块大小为16×16像素。对于每个块，通过ResNet
    CNN的前三个块计算一个特征向量。
- en: Fine-tuning is performed on task-specific datasets for the tasks shown in Fig.
    [7.19](#Fig19), e.g. MS COCO for image captioning. In addition, OFA is fine-tuned
    on several NLP tasks such as the GLUE benchmark for natural language understanding,
    the Gigaword benchmark for abstractive summarization, and the ImageNet-1K dataset
    for image classification. For inference the authors apply beam search and develop
    a search strategy based on a prefix tree. This trie-based search strategy ensures
    that the output generated by OFA is constrained to the appropriate candidate set.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[7.19](#Fig19)中显示的任务上对特定任务的数据集进行微调，例如用于图像标题的MS COCO。此外，OFA在多个NLP任务上进行微调，如用于自然语言理解的GLUE基准、用于抽象摘要的Gigaword基准以及用于图像分类的ImageNet-1K数据集。为了推理，作者应用了束搜索并开发了一种基于前缀树搜索策略。这种基于前缀树的搜索策略确保了OFA生成的输出被限制在适当的候选集内。
- en: For image captioning the model is fine-tuned on MS COCO [[26](#CR26)]. With
    a Bleu-4 score of 43.5 it establishes a new Sota for the MS COCO benchmark [[32](#CR32)].
    For Visual Question Answering the model is fine-tuned on VQAv2 [[56](#CR56)] and
    similar datasets. A search strategy based on a prefix tree ensures that the output
    generated by OFA is constrained to the candidate set. It achieves a new Sota accuracy
    of 80.0%.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像标题，模型在MS COCO [[26](#CR26)]上进行微调。Bleu-4得分为43.5，为MS COCO基准建立了新的Sota。对于视觉问答，模型在VQAv2
    [[56](#CR56)]和类似数据集上进行微调。基于前缀树的搜索策略确保了OFA生成的输出被限制在候选集内。它实现了80.0%的新Sota准确率。
- en: For the *visual entailment task* the model has to determine, if the image entails,
    contradicts or is neutral to the text. OFA is fine-tuned on *SNLI-VE* [[178](#CR178)]
    and achieves a Sota accuracy of 90.2% on the test set, which is 3.1% better than
    the prior best model. To understand referring expressions, the model has to locate
    an image region described by a language query. Here the model was fine-tuned on
    the *RefCOCO benchmark* [[187](#CR187)] and related benchmarks. It achieved a
    new Sota with a text accuracy of 92.9%, outperforming competitors by a large margin.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*视觉蕴涵任务*，模型需要确定图像是否蕴涵、矛盾或对文本中立。OFA在*SNLI-VE* [[178](#CR178)]上进行微调，并在测试集上达到了90.2%的Sota准确率，比之前最佳模型高出3.1%。为了理解指称表达式，模型需要定位由语言查询描述的图像区域。在这里，模型在*RefCOCO基准*
    [[187](#CR187)]和相关基准上进行微调，以92.9%的文本准确率创造了新的Sota，大幅超越了竞争对手。
- en: For image generation the model is fine-tuned on MS COCO [[26](#CR26)]. It achieves
    an Fréchet Inception Distance (FID) of 10.5\. This is better than the scores for
    DALL-E [[133](#CR133)] (27.5) or GLIDE [[109](#CR109)] (12.2), which have far
    more parameters (12B resp. 3.5B) than OFA with 472M. On the leaderboard, only
    LAFITE (Sect. [7.2.6](#Sec18)) has a better FID-value of 8.1\. Note that competing
    models selected their results from 60 to 512 trial outputs, while OFA only selected
    the best of 24 images according to FID scores.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像生成，模型在MS COCO [[26](#CR26)]上进行微调。它达到了10.5的Fréchet Inception Distance (FID)。这比DALL-E
    [[133](#CR133)]（27.5）或GLIDE [[109](#CR109)]（12.2）的分数要好，这些模型的参数（分别为12B和3.5B）远多于OFA的472M。在排行榜上，只有LAFITE（第[7.2.6](#Sec18)节）的FID值更好，为8.1。请注意，竞争模型从60到512个试验输出中选择结果，而OFA仅根据FID分数选择24张图像中的最佳图像。
- en: For image classification in ImageNet, OFA uses no extra labeled training data
    and has a similar performance (84.9% top-1 accuracy) as EfficientNet-B7 (84.3%),
    whereas the current Sota is 88.3%. Surprisingly, OFA also achieves good results
    on language-only benchmarks, such as the GLUE natural language understanding benchmark
    (Sect. [4.​1.​1](528393_1_En_4_Chapter.xhtml#Sec2)) and the Gigaword summarization
    (Sect. [6.​4.​1](528393_1_En_6_Chapter.xhtml#Sec26)). Code, demos, and trained
    models are available for download.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 对于ImageNet中的图像分类，OFA没有使用额外的标记训练数据，其性能（84.9%的top-1准确率）与EfficientNet-B7（84.3%）相似，而当前的最佳水平是88.3%。令人惊讶的是，OFA在仅语言基准测试中也取得了良好的结果，例如GLUE自然语言理解基准（第[4.1.1](528393_1_En_4_Chapter.xhtml#Sec2)节）和Gigaword摘要（第[6.4.1](528393_1_En_6_Chapter.xhtml#Sec26)节）。代码、演示和训练模型均可下载。
- en: An alternative multipurpose model is **NÜWA**, which is described in Sect. [7.3.4](#Sec27).
    It provides realistic text-to-image generation, image editing, and image region
    editing controlled by text. In addition, NÜWA performs text-to-video creation
    and the prediction of the next video frames.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代的多功能模型是**NÜWA**，它在第[7.3.4](#Sec27)节中进行了描述。它提供了逼真的文本到图像生成、图像编辑以及通过文本控制的图像区域编辑功能。此外，NÜWA还能进行文本到视频的创建和预测下一个视频帧。
- en: '**WuDao-2.0** [[140](#CR140), [143](#CR143), [198](#CR198)] is a giant mixture-of-experts
    model with 1075B parameters and has been introduced in Sect. [3.​5.​2](528393_1_En_3_Chapter.xhtml#Sec26).
    It is based on the GLM 2.0 architecture (Sect. [3.​1.​3](528393_1_En_3_Chapter.xhtml#Sec4))
    combining the different learning paradigms of BERT, GPT and the encoder-decoder
    transformer. For image modeling, it uses the CogView approach (Sect. [7.2.6](#Sec18)).
    However, implementation details are not available. The training data consist of
    2.5TB image data and 2.5TB Chinese and English text data (e.g. from the *Pile*
    corpus [[49](#CR49)]). WuDao-2.0 can be applied to a wide range of text analysis
    and generation tasks, and has matched or surpassed Sota levels on five image benchmarks,
    e.g. on classifying land use in image data, image generation, and graphic retrieval.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '**WuDao-2.0** [[140](#CR140), [143](#CR143), [198](#CR198)]是一个拥有1075B参数的巨大专家混合模型，并在第[3.5.2](528393_1_En_3_Chapter.xhtml#Sec26)节中进行了介绍。它基于GLM
    2.0架构（第[3.1.3](528393_1_En_3_Chapter.xhtml#Sec4)节），结合了BERT、GPT和编码器-解码器Transformer的不同学习范式。对于图像建模，它使用CogView方法（第[7.2.6](#Sec18)节）。然而，实现细节不可用。训练数据包括2.5TB的图像数据和2.5TB的中英文文本数据（例如来自*Pile*语料库
    [[49](#CR49)]）。WuDao-2.0可以应用于广泛的文本分析和生成任务，并在五个图像基准测试中达到了或超过了Sota水平，例如在图像数据中分类土地利用、图像生成和图形检索。'
- en: Available Implementations
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: Vision transformer code, trained models and notebooks [github.​com/​google-resear
    ch/​vision_​transformer](https://github.com/google-research/vision_transformer)
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉Transformer代码、训练模型和笔记本[github.com/google-research/vision_transformer](https://github.com/google-research/vision_transformer)
- en: OSCAR code and pre-trained models [github.​com/​microsoft/​Oscar](https://github.com/microsoft/Oscar),
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OSCAR代码和预训练模型[github.com/microsoft/Oscar](https://github.com/microsoft/Oscar)
- en: VinVL code and pre-trained Oscar-VinVL models [github.​com/​pzzhang/​VinVL](https://github.com/pzzhang/VinVL).
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VinVL代码和预训练Oscar-VinVL模型[github.com/pzzhang/VinVL](https://github.com/pzzhang/VinVL).
- en: DALL-E code and notebook [github.​com/​openai/​DALL-E](https://github.com/openai/DALL-E)
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DALL-E代码和笔记本[github.com/openai/DALL-E](https://github.com/openai/DALL-E)
- en: OFA model code, pre-trained models and online demos [github.​com/​OFA-Sys/​OFA](https://github.com/OFA-Sys/OFA)
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OFA模型代码、预训练模型和在线演示[github.com/OFA-Sys/OFA](https://github.com/OFA-Sys/OFA)
- en: GLIDE code, trained models and notebook [github.​com/​openai/​glide-text2im](https://github.com/openai/glide-text2im)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GLIDE代码、训练模型和笔记本[github.com/openai/glide-text2im](https://github.com/openai/glide-text2im)
- en: Stable Diffusion [https://​github.​com/​CompVis/​latent-diffusion](https://github.com/CompVis/latent-diffusion)
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定扩散[https://github.com/CompVis/latent-diffusion](https://github.com/CompVis/latent-diffusion)
- en: 7.2.9 Summary
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.9 摘要
- en: Recently, the Vision Transformer (ViT) emerged as a competitive alternative
    to Convolutional Neural Networks (CNNs) for image recognition tasks. ViT models
    outperform CNNs in terms of accuracy on various benchmarks and require much less
    computational effort.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，视觉Transformer（ViT）作为卷积神经网络（CNNs）在图像识别任务中的竞争性替代品出现。在多个基准测试中，ViT模型在准确性方面优于CNNs，并且所需的计算工作量要小得多。
- en: Foundation Models for image processing receive image patches as input. The embeddings
    of these image patches are generated by different methods, e.g. linear transformations
    of image pixels, by the first few layers of CNN models, by variational autoencoders
    (VAE), or by Generative Adversarial Networks (GANs). A completely different approach
    is taken by diffusion models, which reverse the process of image degradation by
    adding noise (GLIDE). It has been shown to be beneficial to discretize representations
    of image patches to reduce noise and low-level texture dependence.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图像处理的基础模型接收图像块作为输入。这些图像块的嵌入通过不同的方法生成，例如通过图像像素的线性变换，通过CNN模型的前几层，通过变分自编码器（VAE），或者通过生成对抗网络（GANs）。扩散模型采取了完全不同的方法，通过添加噪声（GLIDE）来逆转图像退化的过程。已经证明，将图像块的表示离散化以减少噪声和低级纹理依赖性是有益的。
- en: There are two alternatives for including text. Sometimes text and image tokens
    are processed by separate transformers. Subsequently the distances between the
    two types of embeddings are minimized (CLIP) or the resulting embeddings are correlated
    by cross-attention (VilBERT). Otherwise, text and image tokens are concatenated
    to form the input of Foundation Models (autoencoders, autoregressive, or encoder-decoder).
    It seems that recent models (DALL-E, CogView, OFA) prefer the single-stream architecture.
    A number of different tasks are employed for pre-training. These include the masked
    language model (MLM), where masked image and language tokens have to be reconstructed,
    masked region classification (MRC), and masked region reconstruction. Sentence-image
    alignment (SIA) classifies whether image-text pairs belong together.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 包含文本有两种替代方案。有时文本和图像标记由不同的Transformer处理。随后，最小化这两种类型嵌入之间的距离（CLIP）或通过交叉注意力（VilBERT）关联生成的嵌入。否则，文本和图像标记被连接起来，形成基础模型（自编码器、自回归或编码器-解码器）的输入。似乎最近的模型（DALL-E、CogView、OFA）更喜欢单流架构。用于预训练的任务有很多种。这包括掩码语言模型（MLM），其中需要重建掩码图像和语言标记，掩码区域分类（MRC）和掩码区域重建。句子-图像对齐（SIA）分类图像-文本对是否属于一起。
- en: The generation of captions constructs a sentence with the characterization of
    the image (VilBERT, OSCAR, VinVL, SimVLM) in fluent and correct language, which
    is usually an accurate description according to human evaluations. The generation
    of longer captions is not yet investigated and is probably more relevant for video
    captioning. There are studies to investigate the attention patterns in vision-language
    models [[19](#CR19)].
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 标题生成的构建一个句子，用流畅和正确的语言描述图像的特征（VilBERT、OSCAR、VinVL、SimVLM），这通常是根据人类评估的准确描述。长标题的生成尚未研究，可能更适合视频标题。有研究调查视觉-语言模型中的注意力模式[[19](#CR19)]。
- en: 'The creation of images that match captions has made a huge leap in quality
    over the past year. Various architectures are used: Generative Adversarial Networks
    (GAN), diffusion models, VAEs. These models are in general combined with PLMs.
    It seems that pure transformer models have advantages (OFA), but diffusion models
    like DALL-E 2.0 gain momentum. Usually, a sample of images is created, and the
    best image is automatically selected by a quality score. Images generated by the
    model often have the resolution of 256 × 256 and already cover many details. Expect
    to see models with higher resolutions next year, e.g. 1024 × 1024.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 与过去一年中匹配标题的图像创作相比，质量有了巨大的飞跃。使用了各种架构：生成对抗网络（GAN）、扩散模型、VAEs。这些模型通常与PLMs结合使用。似乎纯transformer模型（如OFA）具有优势，但像DALL-E
    2.0这样的扩散模型正在获得动力。通常，会创建一个图像样本，并通过质量分数自动选择最佳图像。模型生成的图像通常分辨率为256×256，并且已经覆盖了许多细节。预计明年将看到具有更高分辨率的模型，例如1024×1024。
- en: Cao et al. [[19](#CR19)] investigate the inner mechanics of vision and language
    models. They conclude that deeper layers lead to more intertwined multimodal fusion.
    Usually, the textual modality is more dominant for taking decisions than image
    features, as models tend to attend to text rather than images during inference.
    It turns out that a subset of attention heads is specialized for cross-modal interaction.
    There are attention patterns that align image regions and textual words. Finally,
    there is no reduction in linguistic capabilities, as pre-trained vision and language
    models encode rich linguistic knowledge.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: Cao等人[[19](#CR19)]研究了视觉和语言模型的内部机制。他们得出结论，深层层导致更紧密的多模态融合。通常，文本模态在做出决策时比图像特征更重要，因为模型在推理期间倾向于关注文本而不是图像。结果发现，有一组注意力头专门用于跨模态交互。存在一些注意力模式，它们将图像区域和文本单词对齐。最后，语言能力没有减少，因为预训练的视觉和语言模型编码了丰富的语言知识。
- en: Recently, multipurpose models have been presented that are trained to solve
    a large number of different language, vision, and language-vision tasks. One example
    is OFA, which has 472M parameters, significantly fewer than DALL-E (12B). OFA
    is a transformer encoder-decoder with image and text tokens as input, controlled
    by text instructions similar to T5\. It achieves Sota in image captioning, image
    generation, visual question answering, visual entailment, and even on pure language
    tasks. Contrast this with the huge WuDao 2.0 model with 1750B parameters, which
    is based on the encoder-decoder GLM model with a mixture-of-experts architecture.
    The model claims Sota performance on a number of image and text tasks, but no
    technical details are known.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，已经提出了多种用途的模型，这些模型被训练来解决大量不同的语言、视觉和语言-视觉任务。一个例子是OFA，它有472M个参数，比DALL-E（12B）少得多。OFA是一个具有图像和文本标记作为输入的transformer编码器-解码器，由类似于T5的文本指令控制。它在图像标题、图像生成、视觉问答、视觉蕴涵甚至纯语言任务上都实现了Sota。与此相对比的是拥有1750B参数的巨大WuDao
    2.0模型，它基于具有专家混合架构的编码器-解码器GLM模型。该模型声称在多个图像和文本任务上实现了Sota性能，但没有任何技术细节是已知的。
- en: In the future, it is expected that these text-image models will be extended
    to other modalities such as video, speech, and 3D. In addition, more data will
    be used, Moreover, they will be enhanced by retrieval techniques to include additional
    external and up-to-date knowledge. Text-image models are a big step towards *symbol
    grounding*, which allows to attach symbols (words) to their real-world meaning.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，预计这些文本-图像模型将被扩展到其他模态，如视频、语音和3D。此外，将使用更多数据，并且它们将通过检索技术得到增强，以包括额外的外部和最新知识。文本-图像模型是向*符号接地*迈出的重要一步，这允许将符号（单词）与其现实世界的意义联系起来。
- en: 7.3 Video Interpretation and Generation
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 视频解释和生成
- en: 'As the Web is becoming a constantly growing communication vehicle, expressing
    content by text and images is often not sufficient. Video brings together three
    things that catch our attention like nothing else: image, movement, and audio.
    Therefore, videos are more and more important as a means of communication. There
    are 2 billion users active on YouTube each month and over 1 billion on TikTok
    with an average usage of 52 min per day. Hence, the automatic analysis, interpretation,
    and generation of videos is extremely valuable. For visual data, the most comprehensive
    self-supervision is available in videos. Their various modalities such as images,
    speech, ASR text, and captions are temporally aligned and do not require human
    annotation. The extreme number of multimodal videos potentially allows Foundation
    Models to acquire a model of the visual world.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 随着网络成为不断增长的通信工具，仅通过文本和图像表达内容通常是不够的。视频结合了三种能吸引我们注意力的元素，这是其他任何东西都无法比拟的：图像、运动和音频。因此，视频作为沟通手段的重要性越来越突出。YouTube每月有20亿活跃用户，TikTok有超过10亿用户，平均每天使用时间为52分钟。因此，视频的自动分析、解释和生成具有极高的价值。对于视觉数据，视频提供了最全面的自我监督。它们的多种模态，如图像、语音、ASR文本和字幕，在时间上对齐，且不需要人工标注。大量多模态视频可能使基础模型获得视觉世界的模型。
- en: 7.3.1 Basics of Video Processing
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 视频处理基础
- en: Video analysis and understanding is more challenging than image processing,
    because video has an additional time dimension and usually has to handle images,
    speech, and text from speech or subtitles simultaneously. Recently Foundation
    Models have been used for video understanding. Compared to CNNs and RNNs, the
    major advantage of transformers is the ability to simultaneously capture global
    information and compute this in parallel. Furthermore, the concise and stackable
    architecture of transformers enables training on larger datasets. Table [7.3](#Tab4)
    list the main variants of Foundation Models for video.Table 7.3
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 视频分析和理解比图像处理更具挑战性，因为视频具有额外的时序维度，通常需要同时处理来自图像、语音或字幕的图像、语音和文本。最近，基础模型已被用于视频理解。与CNN和RNN相比，transformers的主要优势在于能够同时捕捉全局信息并在并行中进行计算。此外，transformers简洁且可堆叠的架构使得在大数据集上进行训练成为可能。表[7.3](#Tab4)列出了视频基础模型的主要变体。Table
    7.3
- en: Main techniques using PLMs for video. The numbers in parenthesis indicate parameter
    count
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PLMs进行视频的主要技术。括号中的数字表示参数数量
- en: '| Model | Approach | Benchmark |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 基准 |'
- en: '| --- | --- | --- |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Video to text** |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| **视频到文本** |'
- en: '| VideoBERT | Partition video into 30 clips and generate embeddings by CNN.
    Cluster embedding by *k*-means. ASR speech generates text tokens. Concatenate
    inputs to BERT | YouCook II video captioning 4.3 Bleu-4 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| VideoBERT | 将视频分成30个片段，通过CNN生成嵌入。通过*k*-means聚类嵌入。ASR语音生成文本标记。将输入连接到BERT |
    YouCook II视频字幕4.3 Bleu-4 |'
- en: '| COOT | Image, video and text are processed in 3 different hierarchy levels.
    Separate transformers for each level. Special attention for cooperation in each
    level (10.6M) | YouCook II video captioning 11.3 Bleu-4 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| COOT | 图像、视频和文本在3个不同的层次级别上处理。每个级别都有单独的transformers。在每个级别上特别关注合作（10.6M） |
    YouCook II视频字幕11.3 Bleu-4 |'
- en: '| DeCEMBERT | Video 2D and 3D features, region captions, ASR text. Inputs linearly
    transformed and fed into a single BERT | YouCook II video captioning 11.9 Bleu-4
    |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| DeCEMBERT | 视频的2D和3D特征、区域字幕、ASR文本。输入线性转换后输入到单个BERT | YouCook II视频字幕11.9 Bleu-4
    |'
- en: '| VATT | Generate image-time patches, separate BERT models for video, audio,
    and text. Contrastive estimation to reduce embedding distances | Kinetics-400
    action recognition 81.1% |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| VATT | 生成图像-时间补丁，为视频、音频和文本分别使用BERT模型。通过对比估计来减少嵌入距离 | Kinetics-400动作识别81.1%
    |'
- en: '| Omnivore | Image, video and 3D views are converted and fed into Swin transformer
    with shifted windows | Kinetics-400 action recognition 84.1% (no extra data) |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Omnivore | 将图像、视频和3D视图转换为带有偏移窗口的Swin transformer输入 | Kinetics-400动作识别84.1%（无额外数据）|'
- en: '| MeMViT | Attention computation with memory of past video frames. Memory not
    trained. Uses memory compression module with pooling | Action recognition on EPIC-KITCHENS-100
    accuracy 48.4% |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| MeMViT | 使用过去视频帧的记忆进行注意力计算。记忆未进行训练。使用带有池化的记忆压缩模块 | 在EPIC-KITCHENS-100上的动作识别准确率48.4%
    |'
- en: '| CoVeR | Separate image and temporal aggregation. Parallel fine-tuning for
    image and video recognition | Kinetics-400 action recognition 87.2% |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| CoVeR | 分别进行图像和时间聚合。并行微调图像和视频识别 | Kinetics-400动作识别87.2% |'
- en: '| MTV | Temporal aggregation by multiple views. Use different Vision Transformers
    for each view (1B) | Kinetics-400 action recognition 89.1% |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| MTV | 通过多个视图进行时间聚合。每个视图使用不同的视觉Transformer（1B） | Kinetics-400动作识别 89.1% |'
- en: '| Merlot | Joint processing of video and ASR text. MLM for text and video.
    Reorder scrambled frames | Visual question answering 43.1% |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 梅洛特 | 视频和ASR文本的联合处理。对文本和视频使用MLM。重新排列打乱的视频帧 | 视觉问答 43.1% |'
- en: '| Flamingo | Process images, video by vision transformer (80B). Include image
    information into language model (Chinchilla) by adapters and cross-attention layers.
    Allows few-shot prompts | Sota on all of 8 image benchmarks and all of 8 video
    benchmarks |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 鹤 | 通过视觉Transformer（80B）处理图像和视频。通过适配器和交叉注意力层将图像信息纳入语言模型（Chinchilla）。允许少量提示
    | 在所有8个图像基准和所有8个视频基准上均达到Sota |'
- en: '| **Text to video** |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| **文本到视频** |'
- en: '| Video transformer | Partition video to 3D blocks with varying dimensions
    in different layers (373M) | AR video generation FVD score 94 on BAIR Robot data
    |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 视频Transformer | 将视频分割为不同层中具有不同维度的3D块（373M） | 在BAIR机器人数据上的AR视频生成FVD分数为94 |'
- en: '| NÜWA | Image, video and text data are represented as 3D tokens. Discretized
    by VQ-GAN. Use localized attention computations. Trained for text-to image, video
    prediction and text-to-video. More applications | AR video generation FVD score
    86.9 on BAIR Robot data (Sota) text-to-video FID-img 28.5 on Kinetics |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| NÜWA | 图像、视频和文本数据表示为3D标记。通过VQ-GAN进行离散化。使用局部注意力计算。用于文本到图像、视频预测和文本到视频。更多应用
    | 在BAIR机器人数据上的AR视频生成FVD分数为86.9（Sota），在Kinetics上的文本到视频FID-img为28.5 |'
- en: '| Imagen video | Base video generation model + several spatial and temporal
    video super-resolution diffusion models | FVD score of about 9.0 for the model
    with 5.6B parameters |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| Imagen视频 | 基础视频生成模型 + 几个空间和时间视频超分辨率扩散模型 | 5.6B参数模型的FVD分数约为9.0 |'
- en: Early models for image processing, e.g. CNNs and GANs, performed the analysis
    of images pixel-by-pixel. However, this is no longer possible for videos due to
    the high computational and memory effort, and there has to be an aggregation of
    image information. Therefore, special spatio-temporal aggregation modules are
    developed to adapt this to the limited sequence length of transformers.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 早期图像处理模型，例如CNN和GAN，是逐像素分析图像。然而，由于视频的高计算和内存需求，这不再可能，必须对图像信息进行聚合。因此，开发了特殊的时空聚合模块来适应Transformer有限的序列长度。
- en: A simple solution is the aggregation of 30 video frames (VideoBERT).
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的解决方案是聚合30个视频帧（VideoBERT）。
- en: Videos can be processed by considering 3D *video patches*, which cover a small
    pixel region in a small number of frames. It is possible to aggregate video and
    text over different temporal levels and compute associations between the levels
    (COOT, MTV). Regional and temporal aggregation may be separated (CoVeR).
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过考虑3D *视频补丁*来处理视频，这些补丁覆盖少量帧中的小像素区域。可以在不同的时间级别上聚合视频和文本，并计算不同级别之间的关联（COOT，MTV）。区域和时间聚合可以分开（CoVeR）。
- en: Additionally the video patches may be processed to extract salient information.
    An example is video quantization by variational autoencoders (VQ-VAE), which already
    was used for image processing, e.g. by DALL-E or CogView (Sect. [7.2.6](#Sec18)).
    Image patches can be extended in time to obtain 3D voxels (VATT, Omnivore).
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，视频补丁还可以被处理以提取显著信息。一个例子是通过变分自动编码器（VQ-VAE）的视频量化，这已经被用于图像处理，例如DALL-E或CogView（第[7.2.6](#Sec18)节）。图像补丁可以在时间上扩展以获得3D体素（VATT，Omnivore）。
- en: A video can be partitioned into short time clips. Prior clips can enter the
    self-attention computations but no update of prior embeddings is necessary (MeMViT).
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频可以被分割成短时间剪辑。先前的剪辑可以进入自注意力计算，但不需要更新先前的嵌入（MeMViT）。
- en: To further reduce computational effort, a sparse self-attention can be used,
    where attention is mostly computed to nearby video pixels.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步减少计算工作量，可以使用稀疏自注意力，其中注意力主要计算到附近的视频像素。
- en: Unsupervised training may be performed similar to BERT. For instance, masked
    video tokens can be predicted based on neighboring video and text tokens [[145](#CR145)].
    In the same way, masked text tokens can be predicted from neighboring text and
    video tokens. Contrastive learning can be used to discriminate between genuine
    text-video pairs and random pairs. Other tasks include classifying whether a video
    and some text belong together, predicting the next frame, or reconstructing the
    order of shuffled video or text tokens. Recent surveys on video understanding
    are provided by Islam et al. [[73](#CR73)], Khurana et al. [[85](#CR85)], and
    Ruan et al. [[145](#CR145)]
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督训练可以类似于BERT进行。例如，可以根据邻近的视频和文本标记预测掩码视频标记[[145](#CR145)]。同样，可以从邻近的文本和视频标记预测掩码文本标记。对比学习可以用来区分真实的文本-视频对和随机对。其他任务包括分类视频和某些文本是否属于一起，预测下一帧，或重建打乱的视频或文本标记的顺序。最近关于视频理解的调查由Islam等人[[73](#CR73)]、Khurana等人[[85](#CR85)]和Ruan等人[[145](#CR145)]提供。
- en: There are a number of training data sources for video. *Kinetics* [[83](#CR83)]
    is a collection of 306k large-scale, high-quality datasets of 10s video clips
    focusing on human actions. The variants Kinetics 400, 600, and 700 are annotated
    with 400, 600, and 700 classes, respectively. Example frames of annotated videos
    are shown in Fig. [7.21](#Fig21). *Moments in Time* [[107](#CR107)] is a collection
    of 800k labeled 3s videos, involving people, animals, objects or natural phenomena
    that capture the gist of a dynamic scene. *Epic-Kitchens-100* [[33](#CR33)] consists
    of 90k egocentric videos, totaling 100 h, recorded in kitchens. Each video is
    labeled with a “noun” and a “verb”. Three accuracy scores (“noun”, “verb”, and
    “action”) are usually reported. The action score assesses correct noun-verb pairs
    and is most important. *Something-Something V2* [[55](#CR55)] consists of more
    than 220k short video clips that show humans interacting with everyday objects.
    Similar objects and backgrounds appear in videos across different classes. This
    data challenges a model’s capability to distinguish classes from motion cues,
    in contrast to other datasets.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 视频的训练数据来源有很多。*Kinetics* [[83](#CR83)]是一个包含306k个大规模、高质量数据集的集合，这些数据集由10秒的视频剪辑组成，专注于人类行为。Kinetics
    400、600和700的变体分别用400、600和700个类别进行标注。标注视频的示例帧如图[7.21](#Fig21)所示。*Moments in Time*
    [[107](#CR107)]是一个包含800k个标记的3秒视频集合，涉及捕捉动态场景精髓的人物、动物、物体或自然现象。*Epic-Kitchens-100*
    [[33](#CR33)]由90k个自拍摄像头视频组成，总时长100小时，在厨房中录制。每个视频都标记了一个“名词”和一个“动词”。通常报告三个准确度分数（“名词”、“动词”和“动作”）。动作分数评估正确的名词-动词对，是最重要的。*Something-Something
    V2* [[55](#CR55)]由超过220k个短视频剪辑组成，展示了人类与日常物体的互动。不同类别的视频中出现了相似的对象和背景。这些数据挑战了一个模型从运动线索中区分类别的能力，与其他数据集不同。
- en: 7.3.2 Video Captioning
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 视频字幕
- en: '*Video captioning* aims at automatically generating natural language descriptions
    of videos. Video captioning is substantially more difficult than image captioning
    because the spatial-temporal information in videos as well as the corresponding
    ASR text from the video introduces an additional complexity. On the other hand,
    huge video collections like YouTube are available on the Internet and can be used
    as training material. A recent survey is given by Perez-Martin et al. [[124](#CR124)].'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '*视频字幕*旨在自动生成视频的自然语言描述。视频字幕比图像字幕要困难得多，因为视频中的时空信息以及相应的语音识别文本引入了额外的复杂性。另一方面，像YouTube这样的大型视频集合在互联网上可用，可以用作训练材料。Perez-Martin等人提供了一项最近的调查[[124](#CR124)]。'
- en: '**VideoBERT** [[160](#CR160)] applies a BERT model to video-text pairs. The
    video is partitioned into clips of 30 frames (1.5sec) and processed by the S3D
    CNN with a temporal convolution [[180](#CR180)], which generates a clip embedding
    vector of size 1024\. The clip embeddings are partitioned by *k*-means clustering
    into 20,736 clusters and quantized to video tokens. Speech is processed by ASR
    and partitioned into sentences. The text is tokenized by WordPiece with a vocabulary
    of 30k tokens. The video tokens corresponding to the sentence time period are
    collected in a video token sequence. As shown in Fig. [7.20](#Fig20) the video
    tokens are appended to the corresponding text tokens separated by special tokens.
    Note that text-only and video-only training is possible as well.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig20_HTML.png)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '**VideoBERT** [[160](#CR160)] 将BERT模型应用于视频-文本对。视频被分割成30帧（1.5秒）的片段，并通过S3D CNN进行时间卷积[[180](#CR180)]处理，生成大小为1024的片段嵌入向量。片段嵌入通过*k*-means聚类分成20736个簇，并量化为视频标记。语音通过ASR处理，并分割成句子。文本通过WordPiece进行标记化，词汇量为30k个标记。与句子时间周期对应的视频标记被收集到一个视频标记序列中。如图[7.20](#Fig20)所示，视频标记被附加到相应的文本标记上，并用特殊标记分隔。请注意，也可以进行仅文本和仅视频的训练！![图7.20](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig20_HTML.png)'
- en: A diagram illustrates the placement of the C L S token and MASK token in a sequence,
    followed by the input image, the masked image, 3 masked images with random tokens,
    and S E P token. The video features BERT in different frames with proper images
    and text for a clearer understanding of the concept.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图表说明了序列中CLS标记和MASK标记的位置，随后是输入图像、掩码图像、3个带有随机标记的掩码图像和SEP标记。视频通过在不同帧中展示适当的图像和文本来为BERT提供视频特征，以便更清晰地理解该概念。
- en: Fig. 7.20
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.20
- en: A text generated by ASR and the corresponding video tokens are the input of
    VideoBERT [[160](#CR160)]. Both modalities are bounded by special tokens. The
    masked tokens have to be predicted. Image credits in Table [A.​3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 由ASR生成的文本和相应的视频标记是VideoBERT [[160](#CR160)]的输入。两种模态都由特殊标记限定。必须预测掩码标记。图像归功于表[A.3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
- en: '![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig21_HTML.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![图7.21](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig21_HTML.png)'
- en: A series of 5 photographs with different frame positions in the first row and
    the second row. Row 1 represents dribbling basketball. Row 2 represents dunking
    basketball.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列5张照片，第一行和第二行展示了不同的帧位置。第1行代表运球篮球。第2行代表扣篮篮球。
- en: Fig. 7.21
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21
- en: Two videos annotated with descriptions (left) similar to videos of the Kinetics
    dataset [[83](#CR83)]. Representative frames of the videos are shown. Obviously,
    a single frame is sometimes not enough to reach a decision, e.g. to distinguish
    “dribbling basketball” and “dunking basketball”. Image credits in Table [A.​3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 两个带有描述的（左侧）视频与Kinetics数据集的视频（[83](#CR83)）相似。展示了视频的代表帧。显然，有时单个帧不足以做出判断，例如区分“运球篮球”和“扣篮篮球”。图像归功于表[A.3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
- en: The BERT[LARGE] model is pre-trained on a video set of 312k cooking videos with
    a total duration of 966 days. The text is obtained by ASR. Training tasks are
    masked token and frame prediction, and detecting text matching a video. VideoBERT
    yields Sota on video captioning on the YouCook II data with Bleu-4 score of 4.3.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: BERT[LARGE]模型在包含312k个烹饪视频的数据集上进行了预训练，总时长为966天。文本通过ASR获得。训练任务包括掩码标记和帧预测，以及检测与视频匹配的文本。VideoBERT在YouCook
    II数据集的视频标题生成任务上取得了Sota，Bleu-4得分为4.3。
- en: '**COOT** [[51](#CR51)] jointly processes image, video and text information
    with an universal representation by embedding vectors. In the representation of
    videos, time is added as a third dimension to the two-dimensional description
    of images. The COOT model considers the data on 3 different levels of hierarchy:
    frame/word, clip/sentence and video/paragraph. For each level there exists a pair
    of transformers processing the input. To model intra-level cooperation, COOT uses
    a feature aggregation layer to focus on temporal interactions between low-level
    entities. To aggregate information to the sentence level, the model uses a special
    attention formula, where all corresponding embeddings enter the scalar product.
    An additional loss term aims to reduce the difference between sentence and clip
    encodings. At the top level, a contextual transformer links the text and video
    embeddings.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '**COOT** [[51](#CR51)] 通过嵌入向量以通用表示方式联合处理图像、视频和文本信息。在视频表示中，时间被添加为图像二维描述的第三个维度。COOT模型在3个不同层次的数据上进行考虑：帧/词、片段/句子和视频/段落。对于每个层次，都存在一对处理输入的转换器。为了建模同层次的合作，COOT使用一个特征聚合层来关注低级实体之间的时序交互。为了将信息聚合到句子层面，模型使用一个特殊的注意力公式，其中所有相应的嵌入都进入标量积。一个额外的损失项旨在减少句子和片段编码之间的差异。在最高层，一个上下文转换器将文本和视频嵌入连接起来。'
- en: The model is trained with videos that have subtitles for individual scenes and
    longer segments. Subsequently, the model can create subtitles for new videos.
    For the YouCook2 video subtitling benchmark dataset, the model can greatly improve
    the Sota to 11.3 Bleu-4\. In addition, the model can also be used for other tasks,
    such as searching when a textual description or a video scene is entered. Since
    the model includes only 10.6M parameters, it is expected that performance can
    be greatly improved by increasing the size of the model.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用具有场景和较长片段字幕的视频进行训练。随后，该模型可以为新的视频创建字幕。对于YouCook2视频字幕基准数据集，该模型可以将Sota提升到11.3
    Bleu-4。此外，该模型还可以用于其他任务，例如在输入文本描述或视频场景时进行搜索。由于该模型仅包含1060万个参数，预计通过增加模型的大小可以显著提高性能。
- en: '**DeCEMBERT** [[164](#CR164)] aims to enhance a video by *region captions*
    in addition to the ASR-text extracted by speech recognition. The input text is
    represented by BPE-tokens. Each second of video is characterized by 2D-features
    extracted by a pre-trained Resnet-152 CNN [[63](#CR63)] as well as by motion features
    extracted by a 3D ResNeXT CNN [[179](#CR179)], which together are mapped to embedding
    vectors. The video embeddings and speech recognition text representations are
    concatenated forming a single sequence as inputs to a 12-layer autoencoder for
    pre-training and downstream task fine-tuning. To align video with ASR captions,
    a constrained attention loss is used that encourages the model to select the best
    matched ASR caption from a pool of candidates. During pre-training on 1.2M YouTube
    instructional videos, the association between text and video is learned by masking
    tokens and by a classification, if a text corresponds to a video. On the YouCook2
    captioning task the model improves Sota to a Bleu-4 score of 11.9\. In addition,
    DeCEMBERT yields good results for video retrieval and video question answering.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '**DeCEMBERT** [[164](#CR164)] 旨在通过语音识别提取的ASR文本之外，通过*区域字幕*来增强视频。输入文本由BPE标记表示。每个视频秒由预训练的Resnet-152
    CNN [[63](#CR63)] 提取的2D特征以及由3D ResNeXT CNN [[179](#CR179)] 提取的运动特征来表征，这些特征共同映射到嵌入向量。视频嵌入和语音识别文本表示被连接起来，形成一个单一序列，作为12层自动编码器的预训练和下游任务微调的输入。为了使视频与ASR字幕对齐，使用了一个约束注意力损失，鼓励模型从候选集中选择最佳匹配的ASR字幕。在1.2M个YouTube教学视频上的预训练过程中，通过掩码标记和分类（如果文本对应视频）学习文本和视频之间的关联。在YouCook2字幕任务中，该模型将Sota提升到11.9
    Bleu-4分数。此外，DeCEMBERT在视频检索和视频问答方面也取得了良好的结果。'
- en: 7.3.3 Action Recognition in Videos
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.3 视频中的动作识别
- en: '**VATT** [[2](#CR2)] uses raw RGB frames of Internet videos, audio waveforms,
    and ASR text of the speech audio as input data. The video of size *T* × *W* × *H*
    with *T* frames is partitioned to a sequence of ⌈*T*∕*t*⌉∗⌈*H*∕*h*⌉∗⌈*W*∕*w*⌉
    patches, where each patch is a *voxel* in ![$$\mathbb {R}^{t\times h\times w\times
    3}$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq7.png) with
    an additional color dimension. This is an extension of the image patches of ViT.
    The position encoding is a sum ***e***[*i*,*j*,*k*] = ***e***[temp;*i*] + ***e***[horiz;*j*] +
    ***e***[vert;*k*] where each of the summands is a learnable vector of length *d*.
    The raw audio waveform is partitioned into *t*^(*′*) segments and each segment
    gets a learnable position embedding. For the text a vocabulary is created and
    each word is mapped to a learnable embedding. The *DropToken* procedure removes
    a random sample of the video or audio tokens to reduce computational cost and
    improve regularization.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '**VATT** [[2](#CR2)] 使用互联网视频的原始 RGB 帧、音频波形和语音音频的 ASR 文本作为输入数据。大小为 *T*×*W*×*H*
    的视频，其中 *T* 帧被划分为 ⌈*T*∕*t*⌉∗⌈*H*∕*h*⌉∗⌈*W*∕*w*⌉ 个补丁，其中每个补丁是 ![$$\mathbb {R}^{t\times
    h\times w\times 3}$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq7.png)
    中的一个 *voxel*，并额外包含一个颜色维度。这是 ViT 图像补丁的扩展。位置编码是 ***e***[*i*,*j*,*k*] = ***e***[temp;*i*]
    + ***e***[horiz;*j*] + ***e***[vert;*k*]，其中每个加数都是一个长度为 *d* 的可学习向量。原始音频波形被划分为 *t*^(*′*)
    段，每段都获得一个可学习的位置嵌入。对于文本，创建了一个词汇表，并将每个词映射到一个可学习嵌入。*DropToken* 程序随机移除视频或音频标记的一部分，以减少计算成本并提高正则化。'
- en: VATT linearly projects each modality into a feature vector of length *d* and
    feeds it into a separate BERT encoder. The model uses Noise Contrastive Estimation
    to reduce the distance between projections of the audio and video embeddings.
    Positive pairs are taken from the same location in the video, and negative pairs
    from different locations. A similar criterion is employed to reduce the distance
    of video and text embeddings. The training data covers clips of 32 frames at 10
    fps taken from the *HowTo100M data* [[105](#CR105)]. The largest model has 415M
    parameters. For action recognition on Kinetics-400 it achieves Sota with a top-1
    accuracy of 82.1% and a top-5 accuracy of 95.6%.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: VATT 线性地将每个模态投影到长度为 *d* 的特征向量，并将其输入到单独的 BERT 编码器。该模型使用噪声对比估计来减少音频和视频嵌入投影之间的距离。正对是从视频中相同位置取出的，负对是从不同位置取出的。采用类似的准则来减少视频和文本嵌入的距离。训练数据包括从
    *HowTo100M 数据* [[105](#CR105)] 中提取的 32 帧/10 fps 的剪辑。最大的模型有 415M 个参数。在 Kinetics-400
    的动作识别上，它实现了 Sota，top-1 准确率为 82.1%，top-5 准确率为 95.6%。
- en: '**Omnivore** [[52](#CR52)] is a model for classifying images, videos, and single-view
    3D data using exactly the same model parameters. A single-view 3D is a color image
    with an additional depth channel. Image, video, and single-view 3D modalities
    are converted into embeddings that are fed into a Transformer model. The images
    are partitioned into image patches, videos are divided into spatio-temporal tubes
    covering separate image regions, and the single-view 3D images are converted into
    RGB patches and depth patches. The patches are projected into embeddings using
    linear layers. The same linear layer is used for image and video RGB patches.
    A separate layer is applied to depth patches. Separate positional embeddings for
    the spatial and the temporal dimension are used.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '**Omnivore** [[52](#CR52)] 是一个用于使用完全相同的模型参数对图像、视频和单视图 3D 数据进行分类的模型。单视图 3D 是一个包含额外深度通道的颜色图像。图像、视频和单视图
    3D 模态被转换为嵌入，并输入到 Transformer 模型中。图像被划分为图像补丁，视频被划分为覆盖不同图像区域的空间时间管，单视图 3D 图像被转换为
    RGB 补丁和深度补丁。补丁使用线性层投影到嵌入中。相同的线性层用于图像和视频 RGB 补丁。深度补丁应用单独的层。空间和时间维度使用单独的位置嵌入。'
- en: Omnivore employs the *Swin transformer* (Sect. [7.2.3](#Sec15)) as base model,
    a hierarchical vision transformer using shifted windows. Self-attention involves
    patch embeddings from spatially and temporally nearby patches. The models are
    jointly trained on the ImageNet-1K dataset for image classification (1.2M images),
    the Kinetics-400 dataset for action recognition (240k videos), and the *SUN RGB-D
    dataset* (5k) for single-view 3D scene classification, with dataset-specific linear
    classification layers transforming the final embeddings. On Kinetics-400 without
    extra data, Omnivore achieved an action recognition accuracy of 84.1%, which was
    the second best. The fine-tuned Omnivore scored Sota on two video classification
    benchmarks. When using RGB and the 3D channel, Omnivore again had a Sota performance
    on the NYU-v2 benchmark.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: Omnivore采用*Swin transformer*（第[7.2.3](#Sec15)节）作为基础模型，这是一个使用平移窗口的分层视觉transformer。自注意力涉及来自空间和时间上邻近补丁的补丁嵌入。这些模型在ImageNet-1K数据集（1.2M张图片）上进行图像分类训练，在Kinetics-400数据集（240k个视频）上进行动作识别训练，以及在*SUN
    RGB-D数据集*（5k）上进行单视图3D场景分类，使用特定于数据集的线性分类层将最终嵌入转换为分类。在Kinetics-400上没有额外数据的情况下，Omnivore实现了84.1%的动作识别准确率，这是第二好的。微调后的Omnivore在两个视频分类基准测试中得分最高。当使用RGB和3D通道时，Omnivore在NYU-v2基准测试上再次实现了最佳性能。
- en: '**MeMViT** [[173](#CR173)] aims to process videos longer than 5s, in contrast
    to most current models. MeMViT handles videos in an online fashion and caches
    key and value vectors of a transformer as memory at each iteration. Through the
    memory, the model has access to prior context for long-term modeling, with little
    additional cost, as memory embeddings are not trained. The queries of the current
    video clip attend to an extended set of key and value vectors, which come from
    both the current time and the past. Similar to the dilated convolutions of WaveNet
    [[114](#CR114)], higher layers attend further down into the past, resulting in
    a significantly longer receptive field. In addition, a memory compression module
    with learnable pooling is effective for reducing the memory footprint.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '**MeMViT** [[173](#CR173)]旨在处理超过5秒的视频，与大多数当前模型不同。MeMViT以在线方式处理视频，并在每个迭代中将transformer的关键和值向量作为记忆缓存。通过记忆，模型可以以较低的成本访问先前的上下文进行长期建模，因为记忆嵌入没有进行训练。当前视频片段的查询关注一个扩展的关键和值向量集，这些向量来自当前时间和过去。类似于WaveNet
    [[114](#CR114)]的扩张卷积，较高层关注更远的过去，从而产生一个显著更长的感受野。此外，一个具有可学习池化的记忆压缩模块对于减少内存占用有效。'
- en: A video is split into a sequence of short *T* × *H* × *W* clips and processed
    sequentially. Similar to MTV, multiple resolutions are used, starting from a fine-grained
    modeling of smaller patches to high-level modeling of larger patches in later
    stages, where the dimensionality of embeddings increases. The aggregation between
    stages is done by strided pooling. The memory representations are frozen and not
    changed by optimization. The model is pre-trained on Kinetics-400 data Fig. [7.21](#Fig21).
    On the AVA v2.2 dataset [[54](#CR54)] MeMViT achieves a mean average precision
    (mAP) of 35.4%. On the action anticipation dateset (EPIC-KITCHENS-100) it has
    a Sota of 17.7% recall@5\. On the action recognition on EPIC-KITCHENS-100 MeMViT
    yields an accuracy of 48.4%.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 视频被分割成一系列短小的 *T*×*H*×*W* 影片，并按顺序进行处理。类似于MTV，使用了多个分辨率，从对较小补丁的精细建模到后续阶段对较大补丁的高级建模，其中嵌入的维度增加。阶段间的聚合是通过步长池化完成的。记忆表示被冻结，不会被优化所改变。该模型在Kinetics-400数据集上预训练（图[7.21](#Fig21)）。在AVA
    v2.2数据集[[54](#CR54)]上，MeMViT实现了35.4%的平均平均精度（mAP）。在动作预测数据集（EPIC-KITCHENS-100）上，它达到了17.7%的召回率@5。在EPIC-KITCHENS-100的动作识别上，MeMViT的准确率为48.4%。
- en: '**CoVeR** [[190](#CR190)] evaluates the effect of different pre-training strategies
    on classification accuracy. The authors use a special transformer architecture,
    which has spatial attention layers across related regions in the same video frame
    and temporal attention layers across the neighboring frames of a video clip. CoVeR
    first pre-trains the model on the *JFT-3B benchmark* [[189](#CR189)] of 3B images
    annotated with a class-hierarchy of around 30k labels. During pre-training all
    temporal attention layers are removed. During fine-tuning, it simultaneously trains
    a single model with 24 layers on multiple action recognition and image datasets
    (Kinetics versions, ImageNet, Moments in Time, SomethingSomethingv2) to build
    robust spatial and temporal representations of video data (Fig. [7.22](#Fig22)).
    For the Kinetics-400 action recognition task CoVeR achieves an accuracy of 87.2%
    and for the Moments in Time action classification it has a Sota accuracy of 46.1%.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig22_HTML.png)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**CoVeR** [[190](#CR190)] 评估了不同预训练策略对分类准确性的影响。作者使用一种特殊的transformer架构，该架构在同一视频帧的相关区域具有空间注意力层，在视频剪辑的相邻帧之间具有时间注意力层。CoVeR首先在3B图像的*JFT-3B基准*
    [[189](#CR189)] 上预训练模型，这些图像带有大约30k个标签的类层次结构。在预训练期间，所有时间注意力层都被移除。在微调期间，它同时在多个动作识别和图像数据集（Kinetics版本、ImageNet、Moment
    in Time、SomethingSomethingv2）上训练一个具有24层的单个模型，以构建视频数据的鲁棒空间和时间表示（图[7.22](#Fig22)）。对于Kinetics-400动作识别任务，CoVeR实现了87.2%的准确率，对于Moment
    in Time动作分类，它达到了46.1%的Sota准确率。[![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig22_HTML.png)]'
- en: A photograph of Times former block has been transformed into a single-frame
    video. The Bull video processed into 24 by model, which incorporates a temporal
    attention layer, spatial attention layer, MLP layer, image classifier, and video
    classifier.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: Times前楼的照片已被转换成单帧视频。Bull视频通过模型处理成24帧，该模型包含时间注意力层、空间注意力层、MLP层、图像分类器和视频分类器。
- en: Fig. 7.22
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.22
- en: During fine-tuning CoVeR [[190](#CR190), p. 5] simultaneously is trained on
    multiple image and video datasets. Each dataset has its own classifier as there
    are different class definitions. Images are single frame videos. Therefore, image
    classification is not affected by temporal attention. Image credits in Table [A.​3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调CoVeR [[190](#CR190)，第5页] 时，同时训练在多个图像和视频数据集上。由于有不同的类别定义，每个数据集都有自己的分类器。图像是单帧视频。因此，图像分类不受时间注意力的影响。图像归功于表[A.3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)。
- en: '**MTV** [[185](#CR185)] performs temporal aggregation by multiple input representations
    (views) of the input video. MTV extracts tokens from the input video over multiple
    time spans. Video tokens derived from long time intervals capture the overall
    scene description, while video tokens taken from short segments capture fine-grained
    details, such as a person’s gesture. Different transformer encoders are used to
    process these different views, with short segment models having higher capacity.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '**MTV** [[185](#CR185)] 通过输入视频的多个输入表示（视图）进行时间聚合。MTV在多个时间段内从输入视频中提取标记。来自长时间间隔的视频标记捕获整体场景描述，而从短段中提取的视频标记捕获细粒度细节，例如一个人的手势。使用不同的transformer编码器来处理这些不同的视图，短段模型具有更高的容量。'
- en: The different encoders are interfaced by lateral connections to fuse cross-view
    information. A cross-view attention is computed between adjacent views similar
    to the multi-head cross-attention in the transformer (Sect. [2.​3.​1](528393_1_En_2_Chapter.xhtml#Sec20)).
    Note that these fusion operations are performed only for specific layers. The
    tokens from all views are aggregated with a global encoder, which performs the
    final classification.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 通过横向连接将不同的编码器接口连接起来以融合跨视图信息。相邻视图之间计算跨视图注意力，类似于transformer中的多头跨注意力（Sect. [2.3.1](528393_1_En_2_Chapter.xhtml#Sec20)）。请注意，这些融合操作仅针对特定层执行。所有视图的标记通过全局编码器聚合，该编码器执行最终的分类。
- en: The models are initialized with Vision Transformer weights (Sect. [7.2.2](#Sec14))
    and trained with videos of 32 frames and a resolution of 224 × 224\. It turned
    out that the cross-view attention was better than alternatives to fuse information
    from different views. In addition, three views gave better results than fewer
    views. The largest model with over a billion parameters achieved Sota accuracy
    of 89.1% for action recognition on kinetics-400.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 模型使用视觉转换器权重（第 [7.2.2](#Sec14) 节）初始化，并使用 32 帧、分辨率为 224×224 的视频进行训练。结果证明，跨视图注意力比融合来自不同视图信息的替代方案更好。此外，三个视图比更少的视图给出了更好的结果。具有超过十亿参数的最大模型在
    kinetics-400 动作识别任务上达到了 Sota 准确率 89.1%。
- en: '**AV-ASR** [[152](#CR152)] applies a PLM to audio-visual speech recognition.
    As usual, audio is converted to 80 log Mel filterbank features in steps of 10
    ms. The videos are cropped to a near mouth region and converted to video embeddings
    with length 512\. Both embeddings are concatenated and fed into a Conformer encoder
    (Sect. [7.1.2](#Sec3)) with 17 layers. The model outperforms previous Sota for
    lipreading on the LRS3-TED benchmark [[1](#CR1)] with a WER of 19.3%. If both
    modalities are used, the WER drops to 1.6%. If babbling noise is added the WER
    of audio-only ASR on LRS3-TED is increased to 6.1%, while speech recognition with
    both modalities has a WER of only 2.9%. There is another approach to associate
    video and audio by generating video background music that matches the speed of
    movement, mood, and rhythm of the video [[38](#CR38)].'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '**AV-ASR** [[152](#CR152)] 将 PLM 应用于音频-视觉语音识别。通常，音频以 10 毫秒的步骤转换为 80 个对数梅尔滤波器组特征。视频被裁剪到接近嘴巴的区域，并转换为长度为
    512 的视频嵌入。这两个嵌入被连接起来，并输入到具有 17 层的 Conformer 编码器（第 [7.1.2](#Sec3) 节）中。该模型在 LRS3-TED
    基准测试 [[1](#CR1)] 上的唇读任务上优于之前的 Sota，WER 为 19.3%。如果同时使用两种模态，WER 降至 1.6%。如果添加咿呀噪声，LRS3-TED
    上仅音频 ASR 的 WER 增加到 6.1%，而同时使用两种模态的语音识别 WER 只有 2.9%。还有另一种方法通过生成与视频动作速度、情绪和节奏相匹配的视频背景音乐来关联视频和音频
    [[38](#CR38)]。'
- en: '**Aloe** [[39](#CR39)] wants to do more than simply describing an image or
    video, but aims at explaining or reasoning about the scene. The model uses an
    unsupervised object segmentation module that partitions each image into object
    representations. A transformer receives the questions and the image descriptions
    including object representations. On several *visual reasoning* benchmarks, the
    model has to answer complex question such as explanatory questions like *“why
    did something happen?”*, predictive questions such as *“what will happen next?”*,
    and counterfactual questions like *“what would happen in an unseen circumstance,
    e.g. if an object is removed?”*. The model is able to improve Sota on nearly all
    benchmark dataset.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '**芦荟** [[39](#CR39)] 不仅想要简单地描述图像或视频，而是旨在解释或推理场景。该模型使用一个无监督的对象分割模块，将每张图像分割成对象表示。一个转换器接收问题和图像描述，包括对象表示。在多个
    *视觉推理* 基准测试中，该模型必须回答复杂问题，例如解释性问题如 *“为什么发生了某事？”*，预测性问题如 *“接下来会发生什么？”*，以及反事实问题如
    *“在未见过的情况下会发生什么，例如如果移除了某个对象？”*。该模型能够几乎在所有基准数据集上提高 Sota。'
- en: '**Merlot** [[188](#CR188)] is a vision and language model that learns multimodal
    world representations from videos with thousands of frames and their ASR text.
    It encodes each frame using an image encoder, embeds tokens using learned embeddings,
    and a Transformer similar to RoBERTa jointly processes both representations. A
    first pre-training task uses contrastive loss to match the language transcript
    embedding and the corresponding video embedding. The MLM task requires replacing
    masked language tokens. The temporal reordering task involves reordering scrambled
    video frames. Hence, Merlot not only learns to match images to temporally corresponding
    words, but also to contextualize what is happening globally over time, achieving
    temporal common sense knowledge. The model is trained on 6M unlabeled YouTube
    videos. Merlot outperforms Sota methods in 12 downstream benchmarks that include
    short and long videos. An example is Visual Question Answering on MSRVTT-QA [[182](#CR182)]
    with a new Sota of 43.1%. A related model for complex event extraction [[93](#CR93)]
    uses a similar contrastive learning approach.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '**梅洛特** [[188](#CR188)] 是一个视觉和语言模型，它从包含数千帧及其ASR文本的视频中学习多模态世界表示。它使用图像编码器对每一帧进行编码，使用学习到的嵌入来嵌入标记，并使用与RoBERTa类似的Transformer共同处理这两种表示。第一个预训练任务使用对比损失来匹配语言转录嵌入和相应的视频嵌入。MLM任务需要替换被掩码的语言标记。时间重排任务涉及重新排列打乱的视频帧。因此，梅洛特不仅学会了将图像与时间上对应的单词匹配，而且还能够对全局事件进行语境化，实现了时间常识知识。该模型在600万未标记的YouTube视频上进行了训练。梅洛特在包括短视频和长视频在内的12个下游基准测试中优于Sota方法。一个例子是在MSRVTT-QA
    [[182](#CR182)]上的视觉问答，达到了新的Sota水平43.1%。一个用于复杂事件提取 [[93](#CR93)] 的相关模型使用了类似的对比学习方法。'
- en: '**Flamingo** [[3](#CR3)] is a visual language model, which can handle sequences
    of arbitrarily interleaved image, video and text data. Flamingo employs the 70B
    parameter pre-trained language model *Chinchilla* trained on a large and diverse
    text corpus (Sect. [3.​1.​2](528393_1_En_3_Chapter.xhtml#Sec3)). The encoder blocks
    of the language model are used with frozen parameters. With this submodel, Flamingo
    has strong generative language abilities and access to a large amount of knowledge
    stored in the Chinchilla weights. Similar to *Frozen* (Sect. [7.2.5](#Sec17)),
    it can be instructed by few-shot learning to answer questions on an image [[166](#CR166)].'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '**火焰鸟** [[3](#CR3)] 是一个视觉语言模型，它可以处理任意交错图像、视频和文本数据的序列。火焰鸟采用在大型和多样化的文本语料库上训练的70B参数预训练语言模型
    *Chinchilla*（第[3.1.2](528393_1_En_3_Chapter.xhtml#Sec3)节）。语言模型的编码器块使用冻结的参数。使用这个子模型，火焰鸟具有强大的生成语言能力和访问存储在Chinchilla权重中的大量知识。类似于
    *Frozen*（第[7.2.5](#Sec17)节），它可以通过少样本学习来指导回答图像上的问题 [[166](#CR166)]。'
- en: For processing images and videos, a contrastive text-image approach is pre-trained
    (Fig. [7.23](#Fig23)). The authors use a variant of ResNet [[16](#CR16)]. The
    vision encoder is pre-trained using a contrastive objective on our datasets of
    image and text pairs, using the two-term contrastive loss from [[127](#CR127)].
    Much like CLIP (Sect. [7.2.4](#Sec16)), similarities are computed as a dot-product
    of the mean pooled output of the image encoder and the mean pooled output of a
    BERT model. This model extracts semantic spatially oriented features from the
    image including color, shape, nature, positions of objects, etc. The model is
    pre-trained separately, and the parameters are frozen during the main training
    of Flamingo.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig23_HTML.png)
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像和视频的处理，使用对比文本-图像方法进行预训练（图[7.23](#Fig23)）。作者使用ResNet [[16](#CR16)] 的一个变体。视觉编码器使用对比目标在我们的图像和文本对数据集上进行预训练，使用来自
    [[127](#CR127)] 的两项对比损失。与CLIP（第[7.2.4](#Sec16)节）类似，相似度是通过图像编码器的平均池化输出的点积和BERT模型的平均池化输出的点积来计算的。该模型从图像中提取语义空间方向特征，包括颜色、形状、性质、物体位置等。该模型单独进行预训练，并在Flamingo的主要训练期间冻结参数。![图片](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig23_HTML.png)
- en: A block diagram displays input test and visual data are interleaved at the bottom.
    The diagram has a cute dog, followed by vision encoder, perceiver resampler, visual
    data processing, first gated cross-attention dense model block, and serious cat.
    The output text is generated from these components.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 一个框图显示了输入测试和视觉数据在底部交错。图中有一只可爱的狗，接着是视觉编码器、感知器重采样器、视觉数据处理、第一个门控交叉注意力密集模型块和一只严肃的猫。输出文本由这些组件生成。
- en: Fig. 7.23
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.23
- en: Flamingo [[3](#CR3)] receives an input consisting of a sequence containing image,
    text, and video in arbitrary order (bottom). The images and videos are processed
    by a frozen vision encoder similar to CLIP. The trainable perceiver resampler
    reduces them to a finite number of image tokens, which are included by a trainable
    cross-attention layer into the language model. The output created by the language
    model is the natural continuation of the input sequence. Image adapted from [[3](#CR3)]
    with kind permission of authors, credits in Table [A.​3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: Flamingo [[3](#CR3)] 接收一个包含图像、文本和视频序列的输入，这些元素以任意顺序排列（底部）。图像和视频通过类似于CLIP的冻结视觉编码器进行处理。可训练的感知重采样器将它们减少到有限数量的图像标记，这些标记通过可训练的交叉注意力层被包含到语言模型中。语言模型创建的输出是输入序列的自然延续。图像改编自
    [[3](#CR3)]，经作者友好许可，见表 [A.3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
- en: Two modules are trained to interface these frozen models. The first is a *perceiver
    resampler*, which receives spatio-temporal features from the vision encoder and
    outputs a fixed-size set of visual tokens (usually 64). This output is generated
    for single images as well as videos independently of the input image resolution
    or the number of input video frames. The extracted visual tokens are then included
    into the language model by interspersed cross-attention layers. In this way the
    language model can incorporate the visual information at each layer. The frozen
    language and vision models have 70B and 435M parameters, while the trainable layers
    have 10B parameters and the resampler has 194M parameters yielding a total of
    80.6B parameters.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 训练了两个模块来接口这些冻结模型。第一个是一个 *感知重采样器*，它从视觉编码器接收时空特征，并输出一组固定大小的视觉标记（通常为64）。这个输出是为单张图像以及视频独立于输入图像分辨率或输入视频帧数生成的。提取的视觉标记随后通过交叉注意力层被包含到语言模型中。这样，语言模型可以在每一层中包含视觉信息。冻结的语言和视觉模型有70B和435M个参数，而可训练的层有10B个参数，重采样器有194M个参数，总共有80.6B个参数。
- en: For training, Flamingo uses a number of datasets with 182GB of text. This collection
    is amended with further mixed text, image and video sequences with a total of
    about 2.3B images and 27M videos.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，Flamingo 使用了包含182GB文本的多个数据集。这个集合通过进一步混合文本、图像和视频序列得到补充，总共有约2.3B张图像和27M个视频。
- en: As shown in Fig. [7.24](#Fig24) Flamingo can answer question on single images
    by simply predicting the next text token in the mixed image-text sequence. In
    their simplest form, the question can ask for the description of objects in the
    scene, as is shown in the upper right example. More difficult is the interpretation
    of the scene as the language model needs world knowledge to decide which aspects
    of an image are noteworthy. In many of these examples, Flamingo can do at least
    one step of implicit inference. Some of the objects are not named in the prompt
    (e.g. the elephant), but their properties are queried directly. In order to answer
    these questions, the model needs to infer the referred object and then recall
    the relevant knowledge to form the answer. This can lead to a single answer (as
    for the elephant on the truck) or to an extended dialog, where the model can answer
    a series of queries about an image (e.g. the dog damaging the sofa). Even after
    several interactions, Flamingo can still successfully attend to the image and
    reply to questions that require to interpret the image. The authors observed that
    multiple images can be separately attended to, simple comparisons and inferences
    are handled properly. Flamingo’s dialog capabilities could enable non-expert end
    users to get answers without the need of fine-tuning.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig24_HTML.png)
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [7.24](#Fig24) 所示，Flamingo 可以通过简单地预测混合图像-文本序列中的下一个文本标记来回答单张图像上的问题。在最简单的形式中，问题可以要求描述场景中的对象，如右上角的示例所示。更难的是对场景的解释，因为语言模型需要世界知识来决定图像的哪些方面是值得注意的。在这些例子中，Flamingo
    至少可以进行一步隐式推理。一些对象在提示中没有命名（例如，大象），但它们的属性被直接查询。为了回答这些问题，模型需要推断所指的对象，然后回忆相关的知识来形成答案。这可能导致单个答案（例如卡车上的大象）或扩展对话，其中模型可以回答一系列关于图像的查询（例如，狗损坏沙发）。即使经过多次交互，Flamingo
    仍然可以成功关注图像并回答需要解释图像的问题。作者观察到可以单独关注多张图像，简单比较和推理被适当处理。Flamingo 的对话能力可以使非专家最终用户在没有微调需求的情况下获得答案。![图7.24](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig24_HTML.png)
- en: 3 photographs, an elephant travels on a truck, a dog damages a sofa set, and
    a person holds a coffee mug. The meat, vegetables, and American food are placed
    on a plate. Each has questions with answers, what the person is holding, the ingredients
    of this dish, and what is odd about this image.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 3 张照片，一头大象在卡车上旅行，一只狗损坏了一套沙发，一个人拿着咖啡杯。肉、蔬菜和美式食物放在盘子上。每个都有问题及答案，包括这个人手持的是什么，这道菜的成分，以及这张图片有什么奇怪之处。
- en: Fig. 7.24
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.24
- en: Flamingo can interpret images and describe them by text. Gray boxes are user
    input and the pink boxes are Flamingo output. In the upper row Flamingo answers
    questions about images. In the lower row there is a dialog about a photo. Image
    adapted from [[3](#CR3), p. 31] and [[3](#CR3), p. 32], reprinted with kind permission
    of the authors
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 火烈鸟能够通过文本解释图像并描述它们。灰色框是用户输入，粉色框是火烈鸟的输出。在上行中，火烈鸟回答有关图像的问题。在下行中，有一段关于照片的对话。图片改编自
    [[3](#CR3)，第 31 页] 和 [[3](#CR3)，第 32 页]，经作者许可重新印刷。
- en: In the same way Flamingo can answer question about videos, as shown in Fig.
    [7.25](#Fig25). However, the performance in this task is not as stable as would
    be desirable.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig25_HTML.png)
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，火烈鸟可以回答关于视频的问题，如图 [7.25](#Fig25) 所示。然而，在这个任务中的表现并不像期望的那样稳定。![图 7.25](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig25_HTML.png)
- en: 4 photos a weight machine, a dog climbs over it. The question is, What is happening
    here? and the answer is dachshund puppy is being weighed on a scale. 4 photos
    of a person plays golf. The question is, What happens to the man after hitting
    the ball? and the answer he falls down.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 4 张照片，一台体重秤，一只狗爬在上面。问题是，这里发生了什么？答案是腊肠犬幼崽正在秤上称重。4 张照片显示一个人打高尔夫。问题是，击球后这个人发生了什么？答案是，他跌倒了。
- en: Fig. 7.25
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.25
- en: Flamingo answers question on videos. Some video frames are shown. Gray boxes
    are user input and the pink boxes are Flamingo output. Image adapted from [[3](#CR3),
    p. 33], reprinted with kind permission of the authors
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 火烈鸟回答视频中的问题。显示了几个视频帧。灰色框是用户输入，粉色框是火烈鸟的输出。图片改编自 [[3](#CR3)，第 33 页]，经作者许可重新印刷。
- en: Flamingo is able to perform *few-shot prompting* on mixed text-video-image sequences.
    Examples are shown in Fig. [7.26](#Fig26). Here a number of images are provided
    and the added text specifies by example the desired way to extract an answer.
    In the first row this amounts to extracting text from the image and in the second
    row the counting of objects of equal type is required. In this way the model can
    be instructed on the fly to perform a large number of tasks, e.g. captioning,
    visual dialogue, classification or visual question answering.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig26_HTML.png)
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 火烈鸟能够对混合文本-视频-图像序列执行*少量样本提示*。示例如图 [7.26](#Fig26) 所示。这里提供了一些图像，并通过示例指定了提取答案所需的方式。在第一行，这相当于从图像中提取文本，而在第二行则需要计算相同类型的对象数量。这样，模型可以即时被指导执行大量任务，例如标题生成、视觉对话、分类或视觉问答。![图
    7.26](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig26_HTML.png)
- en: 6 photographs of the flamingo output result, namely underground, congress, soulomes,
    pandas 3, dogs 2, and giraffes 4.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 火烈鸟输出的 6 张照片结果，分别是地下、国会、索洛梅斯、熊猫 3、狗 2 和长颈鹿 4。
- en: Fig. 7.26
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.26
- en: Few-shot querying of Flamingo [[3](#CR3)] with a mixture of images and text.
    Note that in the second example Flamingo did not count the trees but stayed with
    the animals. The usual number of few-shot queries is 32\. Image adapted from [[3](#CR3),
    p. 2], reprinted with kind permission of the authors
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 火烈鸟使用图像和文本混合进行少量样本查询 [[3](#CR3)]。注意，在第二个示例中，火烈鸟没有计算树木，而是停留在动物上。通常的少量样本查询数量是
    32。图片改编自 [[3](#CR3)，第 2 页]，经作者许可重新印刷。
- en: The performance of the model was tested on 9 image-text benchmarks on scene
    description, visual dialogue, and visual QA, among them MS-COCO captioning. On
    the eight mixed-media benchmarks Flamingo established a few-shot Sota by a wide
    margin using 16 or 32 shots. For three benchmarks the score is even better than
    the prior fine-tuned Sota. On ImageNet top-1 classification Flamingo achieves
    76.0% compared to a fine-tuned Sota of 91.0%. The test array on video contains
    9 benchmarks, eight of whom require free form text answers and one benchmark (Kinetics
    700) needs classification. On all eight free-form benchmarks Flamingo can increase
    few-shot Sota, often by a huge margin. On four of these benchmarks Flamingo even
    exceeds the fine-tuned results. This is even more remarkable as Flamingo uses
    only 32 task-specific examples which is around 1000 times less task-specific training
    data than current state-of-the-art.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在场景描述、视觉对话和视觉问答等9个图像-文本基准测试中进行了性能测试，其中包括MS-COCO标题。在Flamingo建立的8个混合媒体基准测试中，通过使用16或32个样本，以很大的优势实现了几样本Sota。对于三个基准，得分甚至优于先前微调的Sota。在ImageNet
    top-1分类中，Flamingo达到了76.0%，而微调的Sota为91.0%。视频测试数组包含9个基准，其中八个需要自由文本答案，一个基准（Kinetics
    700）需要进行分类。在所有八个自由形式基准测试中，Flamingo可以增加几样本Sota，通常增幅很大。在这些基准中的四个，Flamingo甚至超过了微调的结果。这更加引人注目，因为Flamingo仅使用了32个任务特定示例，这比当前最先进的任务特定训练数据少约1000倍。
- en: Flamingo can be fine-tuned on specific benchmarks to increase performance. During
    fine-tuning, the frozen model parts are not changed. When fine-tuning on 9 example
    tasks Flamingo could increase fine-tuned Sota on five of these tasks. This shows
    that by fine-tuning the 10B free parameters of the model, the performance can
    in many cases be increase to new levels.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: Flamingo可以在特定基准上进行微调以提高性能。在微调过程中，冻结的模型部分不会改变。当在9个示例任务上微调时，Flamingo可以在这些任务中的五个上提高微调的Sota。这表明，通过微调模型的10B自由参数，在许多情况下可以将性能提升到新的水平。
- en: 7.3.4 Generating Videos from Text
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.4 从文本生成视频
- en: The creation of videos following a textual description is an important issue,
    e.g. for education or illustration of dynamic content. While there are a number
    of models for describing images and videos through text, there are not many proposals
    for the other direction. The concepts for encoding text and videos are similar
    to the captioning of videos. The quality of generated videos can be judged by
    several measures comparing the similarity of actual and generated videos. The
    *FVD* (Fréchet Video Distance) is the spatiotemporal extension of the Fréchet
    Inception Distance (FID) (Sect. [7.2.6](#Sec18)), and is sensitive to visual quality,
    temporal coherence and diversity of samples.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 根据文本描述创建视频是一个重要问题，例如用于教育或动态内容的说明。虽然有许多模型可以通过文本描述图像和视频，但关于相反方向的提议却不多。编码文本和视频的概念与视频标题相似。生成的视频质量可以通过比较实际生成视频的相似性来衡量。*FVD*（Fréchet视频距离）是Fréchet
    Inception距离（FID）（第[7.2.6](#Sec18)节）的空间时间扩展，对视觉质量、时间一致性和样本多样性敏感。
- en: The **Video Transformer** [[172](#CR172)] generalizes the one-dimensional transformer
    encoder-decoder to videos. A video is represented as ![$${\boldsymbol {x}}\in
    \mathbb {R}^{h\times w\times s\times d}$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq8.png),
    where *h* and *w* denote the number of tokens in the spatial height and width,
    *s* denotes the number of tokens in the temporal axis, and *d* is the number of
    channels (e.g. colors). The video is partitioned into small 3D blocks in time
    and space. Self-attention is applied separately with each block. To allow direct
    information exchange between blocks, the block sizes between each layer are varied.
    The blocks contain 4 frames with a spatial resolution 32 × 32\. Self-attention
    varies between 1 and 32 in different layers and dimensions. The largest model
    has a hidden size of 2048, 8 layers and 373M parameters. On the BAIR Robot Pushing
    data [[44](#CR44)] the model achieved an *FVD* (Fréchet Video Distance) score
    [[167](#CR167)] of 94\. which was Sota at the time of publication.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '**视频转换器** [[172](#CR172)] 将一维转换器编码器-解码器推广到视频中。视频表示为 ![$${\boldsymbol {x}}\in
    \mathbb {R}^{h\times w\times s\times d}$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq8.png)，其中
    *h* 和 *w* 表示空间高度和宽度中的标记数量，*s* 表示时间轴上的标记数量，而 *d* 是通道数（例如颜色）。视频在时间和空间上被划分为小的 3D
    块。每个块分别应用自注意力。为了允许块之间直接的信息交换，每层之间的块大小是变化的。这些块包含 4 帧，空间分辨率为 32×32。自注意力在不同层和维度之间变化，从
    1 到 32。最大的模型具有 2048 个隐藏大小、8 层和 373M 个参数。在 BAIR 机器人推动数据 [[44](#CR44)] 上，该模型实现了
    *FVD*（Fréchet 视频距离）得分为 94，这在发表时是 Sota。'
- en: '**NÜWA** [[175](#CR175)] is a recent transformer encoder-decoder model that
    provides a solution for generating video from text. It uses a so called *3D Nearby
    Attention* mechanism to capture the locality characteristic for both spatial and
    temporal axes. Image, video and text data is represented as tokens ![$${\boldsymbol
    {x}}\in \mathbb {R}^{h\times w\times s\times d}$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq9.png),
    where *h* and *w* denote the number of tokens in the spatial height and width,
    *s* denotes the number of tokens in the temporal axis, and *d* is the dimension
    of each token. The raw input regions are transformed into discrete tokens for
    image patches by a trainable VQ-GAN (Sect. [7.2.3](#Sec15)). This GAN-based quantization
    module provides a much better image quality than VQ-VAE used by CogView (Sect.
    [7.2.6](#Sec18)).'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**NÜWA** [[175](#CR175)] 是一种最近推出的转换器编码器-解码器模型，它为从文本生成视频提供了解决方案。它使用所谓的 *3D 近邻注意力*
    机制来捕捉空间和时间轴的局部特征。图像、视频和文本数据表示为标记 ![$${\boldsymbol {x}}\in \mathbb {R}^{h\times
    w\times s\times d}$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq9.png)，其中
    *h* 和 *w* 表示空间高度和宽度中的标记数量，*s* 表示时间轴上的标记数量，而 *d* 是每个标记的维度。原始输入区域通过可训练的 VQ-GAN（第
    [7.2.3](#Sec15) 节）转换为图像补丁的离散标记。这种基于 GAN 的量化模块比 CogView（第 [7.2.6](#Sec18) 节）使用的
    VQ-VAE 提供了更好的图像质量。'
- en: 'The model modifies attention computations and considers a local neighborhood
    with respect to width, height and temporal extent called 3D Nearby Self-Attention.
    Three different positional encoder embeddings are used for width, height and time.
    Each 336 × 336 pixel video frame is partitioned into 21 × 21 patches and 10 frames
    of a video are sampled with 2.5 frames per second. The size of the neighborhood
    in width, height and time is 3\. The model is pre-trained on three tasks: Text-to-image
    for 2.9M text-image pairs from Conceptual Captions, video prediction with 727k
    videos from Moments in Time, and text-to-video generation for 241k text-video
    pairs.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型修改了注意力计算，并考虑了关于宽度、高度和时间范围的局部邻域，称为 3D 近邻自注意力。对于宽度、高度和时间，使用了三个不同的位置编码嵌入。每个
    336×336 像素的视频帧被划分为 21×21 的补丁，每秒采样 2.5 帧的视频中的 10 帧。邻域在宽度、高度和时间上的大小为 3。该模型在三个任务上进行了预训练：从
    Conceptual Captions 的 2.9M 个文本-图像对中进行文本到图像，从 Moments in Time 的 727k 个视频中预测视频，以及从
    241k 个文本-视频对中进行文本到视频生成。
- en: For text-to-image generation, NÜWA is fine-tuned on the MS COCO dataset. Sixty
    images are generated for each text and the best image is selected by CLIP (Sect.
    [7.2.4](#Sec16)). NÜWA outperforms CogView with an FID-0 of 12.9, which is good,
    as shown in Fig. [7.27](#Fig27), but worse than LAFITE (FID 8.1) and OFA (FID
    10.5). For text-to-video, NÜWA is fine-tuned on the Kinetics dataset. Some frames
    of two generated examples are shown in Fig. [7.28](#Fig28). NÜWA achieves the
    best performance on the FID-img and FID-vid metrics with values of 28.5 and 7.0\.
    Video prediction has to generate the sequence of the next frames of a video from
    a starting frame. On the BAIR Robot Pushing dataset, NÜWA achieves a new Sota
    of 86.9 FVD score for this task.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig27_HTML.png)
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本到图像的生成，NÜWA 在 MS COCO 数据集上进行微调。每个文本生成 60 张图像，并通过 CLIP（第 [7.2.4](#Sec16)
    节）选择最佳图像。NÜWA 以 FID-0 为 12.9 的成绩优于 CogView，如图 [7.27](#Fig27) 所示，但不如 LAFITE（FID
    8.1）和 OFA（FID 10.5）。对于文本到视频，NÜWA 在 Kinetics 数据集上进行微调。图 [7.28](#Fig28) 展示了两个生成示例的一些帧。NÜWA
    在 FID-img 和 FID-vid 指标上取得了最佳性能，分别为 28.5 和 7.0。视频预测需要从起始帧生成视频的下一帧序列。在 BAIR Robot
    Pushing 数据集上，NÜWA 在此任务上实现了 86.9 的新 Sota FVD 分数。![图 7.27](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig27_HTML.png)
- en: A series of photos in row 1 represents a video clippings of British shorthair
    jumping over a couch. A series of 5 photos in row 2 represents coffee is being
    poured into a cup.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行的一系列照片代表英国短毛猫跳过沙发的视频剪辑。第二行的一系列 5 张照片代表咖啡正倒入杯子中。
- en: Fig. 7.27
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.27
- en: 256 × 256 images generated from the text above the images by NÜWA [[175](#CR175)]
    for the MS COCO benchmark. Image reprinted with kind permission of the authors
    [[175](#CR175), p. 5]
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: NÜWA 从图像上方的文本生成的 256×256 图像 [[175](#CR175)] 用于 MS COCO 基准测试。图像经作者许可重新印刷 [[175](#CR175)，第
    5 页]。
- en: '![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig28_HTML.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.28](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig28_HTML.png)'
- en: 4 photographs. 1, a green train is coming down the tracks. 2, a group of skiers
    are preparing to ski down a mountain. 3, a living area with a television and a
    table. 4, a child eating a birthday cake near some balloons.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 4 张照片。1，一列绿色的火车正在轨道上行驶。2，一群滑雪者正在准备下山滑雪。3，一个带有电视和桌子的生活区。4，一个孩子在一些气球附近吃生日蛋糕。
- en: Fig. 7.28
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.28
- en: Frames of two videos generated by NÜWA [[175](#CR175)] from text (left) for
    the text-to-video task on the Kinetics dataset. Note that an input text like “running
    on the sea” has never been seen by the model. Image reprinted with kind permission
    of the authors [[175](#CR175), p. 5]
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: NÜWA 从文本（左侧）生成的两个视频的帧 [[175](#CR175)] 用于 Kinetics 数据集上的文本到视频任务。请注意，模型从未见过像“在海边跑步”这样的输入文本。图像经作者许可重新印刷
    [[175](#CR175)，第 5 页]。
- en: NÜWA supports a number of other tasks. For image editing, it can reconstruct
    parts of an image. Alternatively, it can edit a marked image region according
    to a text, e.g. *“a horse is running on the grassland”*. Image sketches annotated
    with text are transformed to photos. This pattern can also be applied to videos,
    such that a video is generated from a series of images with annotated regions.
    Finally, it can change the contents in a video, e.g. modify the movements of a
    diver as shown in the lower row of Fig. [7.29](#Fig29). Moreover, a series of
    image sketches annotated with text can be transformed to a video. Further examples
    are shown here [[174](#CR174)]. **GODIVA** [[176](#CR176)] is a similar prior
    approach from the same authors based on VQ-VAE variational autoencoders.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig29_HTML.png)
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: NÜWA 支持许多其他任务。对于图像编辑，它可以重建图像的部分区域。或者，它可以根据文本编辑标记的图像区域，例如 *“一匹马在草原上奔跑”*。带有文本注释的图像草图被转换为照片。这种模式也可以应用于视频，从而从一系列带有注释区域的图像生成视频。最后，它可以更改视频中的内容，例如修改图
    [7.29](#Fig29) 下排所示潜水员的动作。此外，一系列带有文本注释的图像草图可以被转换为视频。更多示例在此处展示 [[174](#CR174)]。**GODIVA**
    [[176](#CR176)] 是同一作者基于 VQ-VAE 变分自编码器的一个类似先验方法。![图 7.29](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig29_HTML.png)
- en: A series of 4 photographs in row 1 represents a person playing gold on grass.
    A series of 4 photographs in row 2 represents a person running on the sea.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行的一系列 4 张照片代表一个人在草地上打高尔夫。第二行的一系列 4 张照片代表一个人在海边跑步。
- en: Fig. 7.29
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.29
- en: NÜWA [[175](#CR175)] can edit videos. In the upper row the raw video is shown.
    In the lower row NÜWA gets the input “The diver is swimming to the bottom” and
    modifies the video accordingly. Image reprinted with kind permission of the authors
    [[175](#CR175), p. 28] **Imagen Video** is a recent high definition text-to-video
    model based on Imagen (Fig. [7.17](#Fig17)). By a frozen T5 text encoder-decoder
    and a base video diffusion model a low-resolution video is generated. This is
    augmented by a cascade of video diffusion models that alternately increase spatial
    and temporal resolution [[66](#CR66)] to construct 128 realistic video frames
    at 24 frames per second with a resolution of 1280 × 768\. Figure [7.30](#Fig30)
    shows videos generated for text prompts by Imagen Video.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig30_HTML.png)
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: NÜWA [[175](#CR175)] 可以编辑视频。在上行中显示原始视频。在下行中，NÜWA 接收输入“潜水员正在向底部游去”并相应地修改视频。图片经作者同意重新印刷
    [[175](#CR175)，第 28 页] **Imagen Video** 是一个基于 Imagen（图 [7.17](#Fig17)）的最近的高清文本到视频模型。通过冻结的
    T5 文本编码器-解码器和基础视频扩散模型生成低分辨率视频。然后通过一系列交替增加空间和时间分辨率的视频扩散模型进行增强 [[66](#CR66)]，以构建每秒
    24 帧、分辨率为 1280×768 的 128 个逼真视频帧。图 [7.30](#Fig30) 显示了 Imagen Video 通过文本提示生成的视频。![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig30_HTML.png)
- en: 10 photographs of a scuba diver under the sea. The photos are captured from
    different angles.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 海底潜水员 10 张照片。照片从不同的角度拍摄。
- en: Fig. 7.30
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.30
- en: Videos generated from the text prompts (below) by Imagen video [[66](#CR66)].
    The model produces diverse and temporally coherent videos that are well matched
    to the given request. Image reprinted with kind permission of the authors [[66](#CR66),
    p. 2]
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Imagen video 从以下文本提示生成的视频（以下）。该模型产生多样化的、时间上连贯的视频，与给定的请求非常匹配。图片经作者同意重新印刷 [[66](#CR66)，第
    2 页]
- en: Available Implementations
  id: totrans-380
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: VideoBERT code [https://​github.​com/​ammesatyajit/​VideoBERT](https://github.com/ammesatyajit/VideoBERT)
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VideoBERT 代码 [https://github.com/ammesatyajit/VideoBERT](https://github.com/ammesatyajit/VideoBERT)
- en: COOT code [https://​github.​com/​gingsi/​coot-videotext](https://github.com/gingsi/coot-videotext)
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: COOT 代码 [https://github.com/gingsi/coot-videotext](https://github.com/gingsi/coot-videotext)
- en: DeCEMBERT code [https://​github.​com/​zinengtang/​decembert](https://github.com/zinengtang/decembert)
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeCEMBERT 代码 [https://github.com/zinengtang/decembert](https://github.com/zinengtang/decembert)
- en: VATT code [https://​github.​com/​google-research/​google-research/​tree/​master/​vatt](https://github.com/google-research/google-research/tree/master/vatt)
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VATT 代码 [https://github.com/google-research/google-research/tree/master/vatt](https://github.com/google-research/google-research/tree/master/vatt)
- en: Omnivore code [https://​github.​com/​facebookresearch​/​omnivore](https://github.com/facebookresearch/omnivore)
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Omnivore 代码 [https://github.com/facebookresearch/omnivore](https://github.com/facebookresearch/omnivore)
- en: Video Transformer code [https://​github.​com/​rakhimovv/​lvt](https://github.com/rakhimovv/lvt)
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Video Transformer 代码 [https://github.com/rakhimovv/lvt](https://github.com/rakhimovv/lvt)
- en: MTV code and models [https://​github.​com/​google-research/​scenic](https://github.com/google-research/scenic)
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MTV 代码和模型 [https://github.com/google-research/scenic](https://github.com/google-research/scenic)
- en: NÜWA code [https://​github.​com/​lucidrains/​nuwa-pytorch](https://github.com/lucidrains/nuwa-pytorch)
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NÜWA 代码 [https://github.com/lucidrains/nuwa-pytorch](https://github.com/lucidrains/nuwa-pytorch)
- en: 7.3.5 Summary
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.5 摘要
- en: The processing of videos requires to integrate different modalities like image,
    text in the form of video captions, and speech possibly translated to text by
    ASR. Video processing introduces an additional time dimension to image processing.
    Furthermore, depth information and camera movements can be important. Since 2019
    large scale transformers using self-supervised pre-training are the prevailing
    models for video processing. The models can solve different tasks, such as video
    captioning, action recognition, video question answering, video generation from
    text, prediction of next frames, video retrieval, audio-visual ASR, etc.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 处理视频需要整合不同模态，如图像、以视频字幕形式存在的文本，以及可能通过 ASR 转换为文本的语音。视频处理为图像处理引入了额外的时维。此外，深度信息和相机运动也可能很重要。自
    2019 年以来，使用自监督预训练的大规模 Transformer 成为视频处理的流行模型。这些模型可以解决不同的任务，例如视频字幕、动作识别、视频问答、从文本生成视频、预测下一帧、视频检索、音频-视觉
    ASR 等。
- en: Existing cross-modal Foundation Models mainly focus on (1) improving model architecture,
    (2) utilizing more data, and (3) designing better pre-training tasks. Due to the
    limited input length, the video has to be partitioned into appropriate tokens.
    This ranges from aggregates over 30 clips (VideoBERT) over fixed video patches
    (VATT) to video patches with varying dimensions (COOT, MTV, Video Transformer).
    Some models (VideoBERT, DeCEMBERT) use CNN convolutions to generate low-level
    features. More common is the aggregation with VQ-VAE autoencoders or the GAN-bases
    VQ-GAN. Sometimes video and text are processed with separate PLMs and merged later
    (VATT). Alternatively, video and text tokens are concatenated and processed by
    single a PLM (Omnivore, Merlot). Transformers use attention over spatial and temporal
    dimensions, which is often localized to reduce computational effort.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的跨模态基础模型主要关注（1）改进模型架构，（2）利用更多数据，（3）设计更好的预训练任务。由于输入长度的限制，视频必须分割成适当的标记。这从VideoBERT的30多个剪辑的聚合到固定视频补丁（VATT）再到具有不同维度的视频补丁（COOT、MTV、Video
    Transformer）不等。一些模型（VideoBERT、DeCEMBERT）使用CNN卷积来生成低级特征。更常见的是使用VQ-VAE自动编码器或基于GAN的VQ-GAN进行聚合。有时视频和文本分别使用PLM处理，然后合并（VATT）。或者，视频和文本标记被连接，并由单个PLM（Omnivore、Merlot）处理。Transformer在空间和时间维度上使用注意力，这通常被局部化以减少计算工作量。
- en: The integration of different modalities is crucial. Text and language are associated
    by pre-training tasks, where masked video or text tokens have to be predicted
    using tokens from the other modality. CoVeR shows that performance can be enhanced
    when the model is simultaneously fine-tuned for video and image tasks. It is even
    possible to combine audio, text and video tokens.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 不同模态的集成至关重要。文本和语言通过预训练任务相关联，其中必须使用来自另一模态的标记来预测掩码视频或文本标记。CoVeR表明，当模型同时针对视频和图像任务进行微调时，性能可以得到提升。甚至可以将音频、文本和视频标记结合起来。
- en: The performance of video analysis models has taken a dramatic development. The
    action classification error on the Kinetics-400 benchmark has fallen within 1
    year to 10.9% using Foundation Models, which is a drop of 33%. Despite the significant
    progress, Sota methods fail to extract/capture all the complex spatiotemporal
    information present in videos. There is still much work to do for understanding
    the diversity of visual content in videos and the structure of associated textual
    descriptions.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 视频分析模型的性能经历了显著的发展。在Kinetics-400基准测试中，动作分类错误在1年内降至使用基础模型后的10.9%，降幅为33%。尽管取得了显著进展，但Sota方法仍无法提取/捕捉视频中存在的所有复杂时空信息。在理解视频中的视觉内容多样性和相关文本描述的结构方面，仍有大量工作要做。
- en: Generating videos from captions is in its early stages, and only very short
    high-resolution videos can be generated. However, the current models are relatively
    small compared to the Foundation Models like GPT-3 or Gopher. Therefore, it can
    be expected that models with more parameters will see considerable performance
    improvements, as has been demonstrated by Imagen Video.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 从字幕生成视频还处于早期阶段，只能生成非常短的高分辨率视频。然而，与GPT-3或Gopher等基础模型相比，当前模型相对较小。因此，可以预期，具有更多参数的模型将看到相当大的性能提升，正如Imagen
    Video所展示的那样。
- en: There is a trend to general-purpose models, like Nüwa that can handle multiple
    modalities of data and solve a number of tasks. Training with different media
    mutually supports the performance in different tasks. Flamingo with 80B parameters
    is based on a large pre-trained language model and a separately pre-trained vision
    encoder. In can process mixed sequences of images, text and videos. By building
    adapter modules and a cross-attention layer, the language model can include the
    results of the visual modalities and perform a variety of analysis tasks like
    visual question answering, image caption, etc. In addition, it can be instructed
    by few-shot prompts to solve many task without a specific fine-tuning.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 出现了一种趋势，即通用模型，如能够处理多种数据模态并解决多个任务的Nüwa。使用不同媒体进行训练相互支持不同任务中的性能。拥有80B参数的Flamingo基于一个大型预训练语言模型和一个单独预训练的视觉编码器。它可以处理图像、文本和视频的混合序列。通过构建适配器模块和交叉注意力层，语言模型可以包含视觉模态的结果，并执行各种分析任务，如视觉问答、图像描述等。此外，它可以通过少量提示指令来解决许多任务，而无需特定的微调。
- en: Although Flamingo cannot generate images or videos corresponding to a caption,
    it is a step in the direction of multimodal Foundation Models, which promise to
    be a general-purpose tool of multimedia processing. By few-shot prompts they could
    solve thousands or millions of tasks. Substantial progress can be expected in
    this area, as ideas can be combined that were developed independently for different
    media. Further development directions are larger training data, which, however,
    are already quite large. In addition, the development of multilingual video models
    is a logical consequence of the current state of research in this area.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Flamingo无法生成与标题对应的图像或视频，但它是在多模态基础模型方向上迈出的一步，这些模型有望成为多媒体处理的一般性工具。通过少量提示，它们可以解决成千上万的任务。在这个领域可以期待取得重大进展，因为可以结合为不同媒体独立开发的观点。进一步的发展方向包括更大的训练数据，尽管这些数据已经相当庞大。此外，多语言视频模型的发展是该领域当前研究状态的逻辑结果。
- en: 7.4 Controlling Dynamic Systems
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 控制动态系统
- en: Foundation Models can process many types of sequences. These include sequential
    decision problems where the agent must choose an action based on a state. Subsequently,
    the environment generates a new state and a reward for the agent. This is repeated
    a number of times until the final sum of rewards is known. Then the task is to
    select the actions based on the states in such a way that the sum of rewards is
    maximal. This goal can be formulated as a sequence problem, and a PLM can be used
    to predict the next optimal action.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型可以处理多种类型的序列。这包括序列决策问题，其中智能体必须根据状态选择一个动作。随后，环境为智能体生成一个新的状态和一个奖励。这个过程重复多次，直到最终的总奖励已知。然后任务是选择基于状态的行动，使得总奖励最大化。这个目标可以表述为一个序列问题，并且可以使用预训练语言模型（PLM）来预测下一个最佳动作。
- en: 7.4.1 The Decision Transformer
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 决策转换器
- en: PLMs are able to predict sequences, e.g. the tokens of a text or video frames.
    Following this pattern, PLMs are also able to model the evolution of arbitrary
    states. *Reinforcement learning* considers a system with *states**s*[*t*], *actions**a*[*t*],
    and *rewards**r*[*t*] = *R*(*s*[*t*], *a*[*t*]) at a given time step *t*. Based
    on the current state, the agent selects an action, while the next state and reward
    are determined by the environment. The target of reinforcement learning is to
    learn a *policy**a* = *π*(*s*[*t*]), which generates actions maximizing the expected
    sum of rewards ![$$E(\sum _{t=1}^Tr_t)$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq10.png).
    During online reinforcement learning the environment can be accessed, and for
    a given (*s*[*t*], *r*[*t*], *a*[*t*]) it returns the next state (*s*[*t*+1],
    *r*[*t*+1]). In offline reinforcement learning there is only a limited set of
    observed trajectories from the environment. The latter setting is more difficult
    as the agent can no longer explore the environment.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: PLM能够预测序列，例如文本或视频帧的标记。遵循这一模式，PLM也能够模拟任意状态的演变。*强化学习*考虑一个在给定时间步长*t*的系统，其具有*状态**s*[*t*]，*动作**a*[*t*]，和*奖励**r*[*t*]
    = *R*(*s*[*t*]，*a*[*t*])。基于当前状态，智能体选择一个动作，而下一个状态和奖励由环境决定。强化学习的目标是学习一个*策略**a* =
    *π*(*s*[*t*])，该策略生成最大化预期总奖励 ![$$E(\sum _{t=1}^Tr_t)$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq10.png)
    的动作。在在线强化学习中，可以访问环境，并且对于给定的 (*s*[*t*]，*r*[*t*]，*a*[*t*])，它返回下一个状态 (*s*[*t*+1]，*r*[*t*+1])。在离线强化学习中，只有从环境中观察到的有限轨迹集合。后者设置更困难，因为智能体不能再探索环境。
- en: The **Decision Transformer** [[23](#CR23)] operates in an offline reinforcement
    setting. Instead of using the returns *r*[*t*] directly, the Decision Transformer
    considers the *forward sum of rewards*![$$\hat {R}_t = \sum _{t'=t}^T r_{t'}$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq11.png).
    Hence, a trajectory is represented as follows![$$\displaystyle \begin{aligned}
    \tau = \left(\hat{R}_1,s_1,a_1,\hat{R}_2,s_2,a_2,\ldots,\hat{R}_T,s_T,a_T\right)
    \end{aligned} $$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_Equ5.png)(7.5)The
    input token embeddings for (*s*[*t*], *r*[*t*], *a*[*t*]) are computed with a
    linear layer, which is different for each modality (Fig. [7.31](#Fig31)). If the
    state is an image, it is transformed by a convolutional encoder instead of a linear
    layer. Subsequently the embeddings are normalized by a layer normalization. For
    each time step with three inputs a position embedding is learned and added to
    the embeddings of that time step. The embeddings are then processed by an autoregressive
    GPT model, which predicts future actions by autoregressive modeling.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig31_HTML.png)
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策转换器** [[23](#CR23)] 在离线强化学习环境中运行。它不是直接使用回报 *r*[*t*]，而是考虑了 *奖励的前向和*![$$\hat
    {R}_t = \sum _{t''=t}^T r_{t''}$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq11.png)。因此，轨迹被表示如下![$$\displaystyle
    \begin{aligned} \tau = \left(\hat{R}_1,s_1,a_1,\hat{R}_2,s_2,a_2,\ldots,\hat{R}_T,s_T,a_T\right)
    \end{aligned} $$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_Equ5.png)(7.5)对于
    (*s*[*t*], *r*[*t*], *a*[*t*]) 的输入标记嵌入是通过一个线性层计算的，每个模态不同（图 [7.31](#Fig31)）。如果状态是图像，则通过卷积编码器而不是线性层进行转换。随后，嵌入通过层归一化进行归一化。对于每个具有三个输入的时间步，学习并添加一个位置嵌入到该时间步的嵌入中。然后，嵌入通过一个自回归的
    GPT 模型进行处理，该模型通过自回归建模来预测未来的动作![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig31_HTML.png)'
- en: An autoregressive language model takes 3 distinct types of input, input tokens,
    type-specific preprocessing, and input embeddings. It then moves to states, actions,
    rewards, output embeddings, logistic classifiers, and action probabilities.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归语言模型接受 3 种不同类型的输入，输入标记、类型特定的预处理和输入嵌入。然后它移动到状态、动作、奖励、输出嵌入、逻辑分类器和动作概率。
- en: Fig. 7.31
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.31
- en: The Decision Transformer applies an autoregressive language model to the forward
    sums of rewards ![$$\hat {R}_t$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq12.png),
    states *s*[*t*] and actions *a*[*t*]. In the example the state is given in the
    form of video frames, e.g. for the Pong game. The model predicts the next action
    in the trajectory conditional to a given forward sums of rewards [[23](#CR23)]
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 决策转换器将自回归语言模型应用于奖励的前向和 ![$$\hat {R}_t$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq12.png)、状态
    *s*[*t*] 和动作 *a*[*t*]。在示例中，状态以视频帧的形式给出，例如对于 Pong 游戏。该模型在给定的奖励的前向和的条件下预测轨迹中的下一个动作
    [[23](#CR23)]
- en: The training was based on a dataset of observed trajectories. From these trajectories
    minibatches of length *K* were sampled. Then the GPT model for each *t* = 1, …,
    *K* predicted *a*[*t*] given a trajectory up to *s*[*t*]. As a loss function the
    cross-entropy loss was used for discrete actions with the target to increase the
    probability of the actual action at time *t*. For continuous actions, e.g. a speed,
    the mean squared error was used as loss to minimize the square difference to the
    observed control value. It was not necessary to predict states or the forward
    sum of rewards.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 训练基于观察到的轨迹数据集。从这些轨迹中采样了长度为 *K* 的微批。然后，对于每个 *t* = 1, …, *K* 的 GPT 模型预测 *a*[*t*]，给定一个直到
    *s*[*t*] 的轨迹。作为损失函数，对于离散动作使用了交叉熵损失，目标是增加时间 *t* 实际动作的概率。对于连续动作，例如速度，使用均方误差作为损失来最小化与观察到的控制值的平方差。没有必要预测状态或奖励的前向和。
- en: For the application to a starting state *s*[1], a target forward sum of rewards
    ![$$\hat {R}_1$$](../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq13.png)
    based on the desired performance (or even maximum possible return) is specified.
    After the generated action *a*[1] is executed, the target return is reduced by
    the achieved reward and the next state *s*[2] is determined. This process of generating
    actions and applying them to get the next forward sum of rewards and the next
    state is repeated until the trajectory ends. Note that the actual forward sum
    of rewards should be close to the desired performance specified before. Although
    the model is only trained on randomly selected subsequences, it can learn to ‘merge’
    subsequences from different training trajectories in order to produce optimal
    trajectories at test time. Obviously a large set of subsequences has to evaluated
    during training to arrive at good solutions.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 对于应用于起始状态*s*[1]，基于期望的性能（甚至最大可能的回报）指定了一个目标前向奖励和。在执行生成的动作*a*[1]之后，目标回报会减少所获得的奖励，并确定下一个状态*s*[2]。这个过程会重复生成动作并将它们应用于获取下一个前向奖励和下一个状态，直到轨迹结束。请注意，实际的前向奖励和应该接近之前指定的期望性能。尽管模型仅在随机选择的子序列上进行训练，但它可以学会在测试时“合并”来自不同训练轨迹的子序列，以产生最优轨迹。显然，在训练过程中必须评估大量子序列，才能得到好的解决方案。
- en: The *Atari benchmark* [[13](#CR13)] has discrete actions, uses four video frames
    as state descriptions, and processes these frames by a convolutional encoder.
    Only 1% of the available data is used. On four Atari tasks (Breakout, Qbert, Pong,
    and Seaquest) usually a context length of *K* = 30 is taken into account. With
    the exception of Qbert, Decision Transformer is competitive with state of the
    art methods, and for two games it reaches the best results (Breakout, Seaquest).
    The most effective alternative is the *CQL* [[87](#CR87)] Q-learner.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '*Atari基准测试*[[13](#CR13)]具有离散动作，使用四个视频帧作为状态描述，并通过卷积编码器处理这些帧。仅使用了1%的可用数据。在四个Atari任务（Breakout、Qbert、Pong和Seaquest）中，通常考虑的上下文长度为*K*
    = 30。除了Qbert之外，决策转换器与最先进的方法具有竞争力，并且对于两款游戏达到了最佳结果（Breakout、Seaquest）。最有效的替代方案是*CQL*
    [[87](#CR87)] Q学习器。'
- en: The *D4RL benchmark* simulates simple robots (HalfCheetah, Hopper, and Walker)
    which are controlled by continuous-valued actions. On this benchmark Decision
    transformer in most cases achieves better results than the alternative approaches
    and has the highest average performance. Again CQL is the best alternative.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '*D4RL基准测试*模拟了由连续值动作控制的简单机器人（HalfCheetah、Hopper和Walker）。在这个基准测试中，决策转换器在大多数情况下比替代方法取得了更好的结果，并且具有最高的平均性能。再次，CQL是最好的替代方案。'
- en: 'The authors evaluate the performance of approaches for an environment, where
    it is necessary to propagate rewards over a long time period. The *Key-to-Door
    benchmark* [[104](#CR104)] has three phases:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 作者评估了在需要长时间传播奖励的环境中的方法性能。*开门基准测试*[[104](#CR104)]有三个阶段：
- en: in the first phase, the agent is placed in a room with a key;
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一阶段，智能体被放置在一个带有钥匙的房间里；
- en: then, the agent is placed in an empty room;
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，智能体被放置在一个空房间里；
- en: and finally, the agent is placed in a room with a door.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，智能体被放置在一个带有门的房间里。
- en: The agent receives a binary reward when reaching the door in the third phase,
    but only if he picked up the key in the first phase. On this benchmark Decision
    Transformer and related methods clearly outperform Q-learning approaches, which
    cannot effectively propagate rewards over a long horizon.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个阶段，当智能体到达门口时，它会收到一个二元奖励，但前提是在第一阶段他拿起了钥匙。在这个基准测试中，决策转换器和相关方法明显优于Q学习方法，后者无法有效地在长期内传播奖励。
- en: Reid et al. [[136](#CR136)] modify the details of the decision transformer yielding
    improved performance. Kumar et al. [[86](#CR86)] show by theoretical analysis
    that offline reinforcement learning—as done by the decision transformer—enjoys
    better guarantees on long-horizon tasks than simply cloning the behavior of experts.
    This especially holds in the case of sufficiently noisy data.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: Reid等人[[136](#CR136)]修改了决策转换器的细节，从而提高了性能。Kumar等人[[86](#CR86)]通过理论分析表明，与简单地克隆专家的行为相比，决策转换器所执行的离线强化学习在长期任务上享有更好的保证。这在足够嘈杂的数据情况下尤其如此。
- en: 7.4.2 The GATO Model for Text, Images and Control
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 文本、图像和控制用GATO模型
- en: '**GATO** [[134](#CR134)] is a Foundation Model, which has been trained on about
    600 different tasks, including text generation, image captioning, stacking physical
    blocks with a robot arm and playing Atari console games. Depending on the context,
    it independently decides which tokens to generate: Text, torques for joints, keystrokes,
    or another variant of the output within its comparatively extensive possibilities.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '**GATO** [[134](#CR134)]是一个基础模型，它在大约600个不同的任务上进行了训练，包括文本生成、图像标题、用机械臂堆叠物理积木以及玩Atari游戏机。根据上下文，它独立决定生成哪些标记：文本、关节扭矩、按键或其输出变体，其可能性相对广泛。'
- en: Depending on the modality the input is tokenized
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 根据模态，输入被标记化
- en: Text is encoded via SentencePiece with 32,000 tokens.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本通过SentencePiece编码，包含32,000个标记。
- en: Images are transformed into sequences of non-overlapping 16 × 16 images patches
    similar to the vision transformer (Sect. [7.2.2](#Sec14)).
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像被转换为类似于视觉transformer（第[7.2.2](#Sec14)节）的非重叠16×16图像补丁序列。
- en: Discrete values, e.g. Atari button presses, are flattened into sequences of
    integers in row-major order. The tokenized result is a sequence of integers within
    the range of [0, 1024].
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离散值，例如Atari按钮按压，按行主序顺序被转换为整数序列。标记化结果是一个位于[0, 1024]范围内的整数序列。
- en: Continuous values, e.g. proprioceptive inputs (sense of self-movement, force,
    and body position) or joint torques, are preprocessed and discretized in 1024
    bins. The discrete integers are then shifted to the range of [32, 000, 33, 024].
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续值，例如本体感觉输入（自我运动、力量和身体位置的感觉）或关节扭矩，在1024个箱子中进行预处理和离散化。然后，离散整数被移位到[32,000, 33,024]的范围。
- en: Tokens belonging to text, discrete- or continuous-valued observations, or actions
    for any time step are embedded into a learned vector embedding space using a lookup
    table. Learned position encodings are added for all tokens based on their local
    token position within their corresponding time step. Tokens belonging to image
    patches for any time step are embedded using a single ResNet [[63](#CR63)] block
    to obtain a vector per image patch. In addition, a learnable within-image position
    encoding vector is added (Fig. [7.32](#Fig32)).![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig32_HTML.png)
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 属于文本、离散或连续值观测或任何时间步动作的标记被嵌入到一个学习向量嵌入空间中，使用查找表。基于它们在相应时间步内的局部标记位置，为所有标记添加学习位置编码。属于任何时间步图像补丁的标记使用单个ResNet
    [[63](#CR63)]块进行嵌入，以获得每个图像补丁的向量。此外，还添加了一个可学习的图像内位置编码向量（图[7.32](#Fig32)）。![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig32_HTML.png)
- en: A block diagram of Atari system components, include frames, actions, input,
    continuous action, and technical features like batched input and masked shifted
    targets.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: Atari系统组件的框图，包括帧、动作、输入、连续动作以及批处理输入和掩码移位目标等技术特性。
- en: Fig. 7.32
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.32
- en: Data from different tasks and modalities are converted to sequences, e.g. frames
    and actions from Atari games, text token sequences, images patch tokens, continuous
    sensory inputs and outputs. In Gato [[134](#CR134), [135](#CR135)], a large decoder-only
    transformer processes the sequence. During training, specific variables, e.g.
    actions, are used to compute a loss. Image adapted from [[135](#CR135), fig.2],
    credits in Table [A.​3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 来自不同任务和模态的数据被转换为序列，例如Atari游戏的帧和动作、文本标记序列、图像补丁标记、连续感觉输入和输出。在Gato [[134](#CR134),
    [135](#CR135)]中，一个大型仅解码器transformer处理序列。在训练过程中，使用特定变量，例如动作，来计算损失。图像改编自 [[135](#CR135),
    图2]，见表[A.3](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4)。
- en: Gato consists of a 1.2B parameter decoder-only transformer with 24 layers, and
    an embedding size of 2048\. As in every language model, all tokens are predicted
    and therefore can be set as targets for training. Currently, only text tokens,
    discrete and continuous values, and actions are currently used as targets. As
    usual, the probabilities of the observed target tokens have to be maximized during
    training.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: Gato由一个1.2B参数的仅解码器transformer组成，具有24层，嵌入大小为2048。与每个语言模型一样，所有标记都被预测，因此可以作为训练的目标。目前，仅使用文本标记、离散和连续值以及动作作为目标。通常，在训练过程中必须最大化观察到的目标标记的概率。
- en: To focus GATO on a specific task, a prompt is used coming from a trajectory
    generated by the same source agent on the same task. GATO was trained on 596 different
    control tasks, among them the Atari benchmark [[13](#CR13)]. The authors included
    only “good” trajectories that yield at least 80% of the expert reward for the
    task. Moreover, GATO was trained on 8 vision and language tasks, e.g. image captioning
    with MS-COCO Captions [[26](#CR26)] and Conceptual Captions [[153](#CR153)], as
    well as visual question-answering datasets. In addition, GATO is trained on the
    large MassiveText [[128](#CR128)] data with 300 billion text tokens.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使GATO专注于特定任务，使用了一个来自同一源代理在相同任务上生成的轨迹的提示。GATO在596个不同的控制任务上进行了训练，其中包括Atari基准测试[[13](#CR13)]。作者只包含了至少产生80%专家奖励的“良好”轨迹。此外，GATO还在8个视觉和语言任务上进行了训练，例如使用MS-COCO
    Captions [[26](#CR26)]和Conceptual Captions [[153](#CR153)]进行图像描述，以及视觉问答数据集。此外，GATO还在包含3000亿个文本标记的大型MassiveText
    [[128](#CR128)]数据上进行了训练。
- en: The performance of GATO has been evaluated for different applications. On the
    Atari benchmark, the model reached average human score or better for 23 of 51
    Atari games. In a robot stacking benchmark, GATO achieved a comparable performance
    as the BC-IMP baseline [[90](#CR90)]. The model has only rudimentary dialog and
    caption functions, which is not surprising due to the small model size.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: GATO的性能在不同应用中进行了评估。在Atari基准测试中，该模型在51个Atari游戏中达到了平均人类分数或更好，有23个。在一个机器人堆叠基准测试中，GATO实现了与BC-IMP基线
    [[90](#CR90)] 相当的性能。该模型只有基本的对话和描述功能，这在模型尺寸较小的情况下并不令人惊讶。
- en: The Gato model is a first attempt to simultaneously solve text, image, and control
    tasks with the same Foundation Model. For control tasks it yielded respectable
    results while for the text and image tasks it had only mediocre performance. Perhaps
    it could benefit from the forward sum of rewards representation of the Decision
    Transformer. Actual Foundation Models have hundreds of billions of parameters
    and require a corresponding computing effort. If the GATO model is extended to
    this order of magnitude, its performance can be expected to improve accordingly.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: Gato模型是首次尝试同时使用相同的Foundation Model来解决文本、图像和控制任务。对于控制任务，它取得了令人尊重的结果，而对于文本和图像任务，它的表现则相当平庸。也许它可以从决策Transformer的奖励表示的前向求和中受益。实际的Foundation
    Model拥有数百亿个参数，需要相应的计算努力。如果GATO模型扩展到这个数量级，其性能预计将相应提高。
- en: Available Implementations
  id: totrans-430
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: Decision Transformer code [https://​sites.​google.​com/​berkeley.​edu/​decision-transformer](https://sites.google.com/berkeley.edu/decision-transformer)
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策Transformer代码 [https://sites.google.com/berkeley.edu/decision-transformer](https://sites.google.com/berkeley.edu/decision-transformer)
- en: 7.4.3 Summary
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.3 摘要
- en: Pre-trained language models can be applied to sequences with mixtures of element
    types. The Decision Transformer considers sequences of rewards, states and actions
    at specific time steps, which occur during a sequential decision problem, e.g.
    video game playing, robot control, or automatic driving. It models observed trajectories
    of these quantities. Instead of using the reward as input, the sum of the rewards
    up to the end of the trajectory is considered, which is the quantity to be maximized.
    For each type of input some preprocessing is performed to generate embeddings.
    The Decision Transformer is trained to predict the actions in short subsequences
    of 30 time steps.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的语言模型可以应用于混合元素类型的序列。决策Transformer考虑在特定时间步长发生的奖励、状态和动作的序列，这些发生在顺序决策问题中，例如视频游戏、机器人控制和自动驾驶。它对这些数量的观察轨迹进行建模。而不是使用奖励作为输入，考虑的是轨迹末尾的奖励总和，这是要最大化的量。对于每种类型的输入，都会进行一些预处理以生成嵌入。决策Transformer被训练来预测30个时间步长的短子序列中的动作。
- en: During application, the desired forward sum of rewards can be set as a condition.
    Then, the model is able to stitch together the information from different subsequences
    in the training data to obtain near-optimal actions reaching a maximal sum of
    rewards. This was shown by extensive experiments with various benchmarks.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用过程中，可以设置所需的奖励前向求和作为条件。然后，模型能够将训练数据中不同子序列的信息拼接在一起，以获得接近最优的动作，达到最大奖励总和。这一点通过在各个基准测试中的大量实验得到了证明。
- en: The GATO model demonstrates that PLMs at the same time can be used to solve
    reinforcement learning tasks simultaneously with text and image tasks. The model
    is trained with nearly 600 control benchmarks, 8 image tasks and on 300B text
    tokens. The model has only rudimentary text and image description capabilities,
    but performs relatively well on the Atari benchmark. It is only a proof of concept
    and could be improved by increasing the model size and, for instance, by using
    the forward sum of rewards.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: GATO模型表明，同时使用PLM可以解决强化学习任务、文本任务和图像任务。该模型使用近600个控制基准、8个图像任务和300B个文本标记进行训练。该模型只有基本的文本和图像描述能力，但在Atari基准测试中表现相对较好。这只是一个概念验证，可以通过增加模型大小和使用例如奖励的前向和等方法进行改进。
- en: 7.5 Interpretation of DNA and Protein Sequences
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 DNA和蛋白质序列的解释
- en: Deciphering the language of DNA is one of the most important goals of biological
    research. The genetic code is universal and explains how DNA is translated into
    proteins. In contrast, the regulatory code, which determines when and how genes
    are expressed, varies between different cell types and organisms. This is similar
    to the polysemy and distant semantic relationships in natural language texts.
    **DNABERT** [[76](#CR76)] tokenizes the DNA sequence into overlapping 3-grams
    and trains a standard BERT model to predict masked tokens (Fig. [7.33](#Fig33)).
    After pre-training on a large set of DNA sequences, it can improve the Sota by
    fine-tuning for many specific DNA prediction tasks. Among them are analysis of
    sequence motifs (DNA segments with biological relevance) and prediction of promoter
    regions (nucleotide sequence that enables regulated expression of a gene). MoDNA
    [[5](#CR5)] and GeneBERT [[106](#CR106)] have similar functionality.![](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig33_HTML.png)
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 解码DNA语言是生物研究最重要的目标之一。遗传密码是通用的，解释了DNA如何被翻译成蛋白质。相比之下，调控密码，它决定了基因何时以及如何表达，在不同细胞类型和生物体之间是不同的。这与自然语言文本中的多义性和远距离语义关系相似。**DNABERT**
    [[76](#CR76)] 将DNA序列标记为重叠的3-gram，并训练一个标准的BERT模型来预测掩码标记（图[7.33](#Fig33)）。在大量DNA序列上进行预训练后，它可以通过针对许多特定的DNA预测任务进行微调来提高Sota。其中还包括分析序列基序（具有生物学相关性的DNA片段）和预测启动子区域（使基因受调控表达的核苷酸序列）。MoDNA
    [[5](#CR5)] 和GeneBERT [[106](#CR106)] 具有类似的功能。![图7.33](../images/528393_1_En_7_Chapter/528393_1_En_7_Fig33_HTML.png)
- en: A schematic of B E R T encoder block displays the result of the process displays
    the final embedding of logistic regression, token plus position embedding, masked
    sequence, tokenized sequence, and the original D N A sequence.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: BERT编码器块的示意图显示了该过程的最终结果，包括逻辑回归的最终嵌入、标记加位置嵌入、掩码序列、标记化序列和原始DNA序列。
- en: Fig. 7.33
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.33
- en: DNABERT tokenizes the DNA sequence into overlapping 3-grams and trains a standard
    BERT model to predict masked tokens [[76](#CR76)]. The resulting model can be
    fine-tuned to many DNA interpretation tasks
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: DNABERT将DNA序列标记为重叠的3-gram，并训练一个标准的BERT模型来预测掩码标记 [[76](#CR76)]。该模型可以微调到许多DNA解释任务。
- en: Proteins are linear chains of amino acids linked by covalent bonds. Amino acids
    can be represented by an alphabet with 25 characters. The strings are ideally
    suited for many NLP methods [[111](#CR111)]. **AminoBERT** [[29](#CR29)] is a
    language model that predicts the 3D protein structure given a protein sequence
    as input. It also uses a natural method to describe polypeptide geometry that
    is rotation and translation invariant at the level of the polypeptide as a whole.
    On average, the model outperforms AlphaFold2 [[80](#CR80)] and RoseTTAFold [[8](#CR8)]
    on orphan proteins and classes of engineered proteins, achieving up to a 106-fold
    reduction in computational time.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质是由共价键连接的氨基酸线性链。氨基酸可以用一个包含25个字符的字母表来表示。这些字符串非常适合许多NLP方法 [[111](#CR111)]。**AminoBERT**
    [[29](#CR29)] 是一个语言模型，它根据蛋白质序列作为输入预测3D蛋白质结构。它还使用一种自然的方法来描述多肽几何形状，该几何形状在多肽整体水平上是旋转和平移不变的。平均而言，该模型在孤儿蛋白质和工程蛋白质类别上优于AlphaFold2
    [[80](#CR80)] 和RoseTTAFold [[8](#CR8)]，在计算时间上实现了高达106倍的减少。
- en: There are a number of other models with similar results [[97](#CR97)], e.g.,
    the protein language model **ESMFold**. It generates embeddings that can be used
    in downstream tasks, for example to capture the structural properties of proteins.
    A model with 15B parameters can predict the three-dimensional structure of a protein
    at the resolution of individual atoms.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多其他模型具有类似的结果 [[97](#CR97)]，例如，蛋白质语言模型**ESMFold**。它生成的嵌入可以用于下游任务，例如捕获蛋白质的结构特性。一个拥有15B参数的模型可以预测蛋白质的三维结构，达到单个原子的分辨率。
- en: Available Implementations
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 可用实现
- en: DNABERT code and models [https://​github.​com/​jerryji1993/​DNABERT](https://github.com/jerryji1993/DNABERT)
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNABERT代码和模型[https://github.com/jerryji1993/DNABERT](https://github.com/jerryji1993/DNABERT)
- en: GeneBERT code and models [https://​github.​com/​ZovcIfzm/​GeneBERT/​tree/​main/​GeneBERT](https://github.com/ZovcIfzm/GeneBERT/tree/main/GeneBERT)
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GeneBERT代码和模型[https://github.com/ZovcIfzm/GeneBERT/tree/main/GeneBERT](https://github.com/ZovcIfzm/GeneBERT/tree/main/GeneBERT)
- en: ProteinBERT code and models [https://​github.​com/​nadavbra/​protein_​bert](https://github.com/nadavbra/protein_bert)
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ProteinBERT代码和模型[https://github.com/nadavbra/protein_bert](https://github.com/nadavbra/protein_bert)
- en: AlphaFold 2 code and models [https://​github.​com/​deepmind/​alphafold](https://github.com/deepmind/alphafold)
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlphaFold 2代码和模型[https://github.com/deepmind/alphafold](https://github.com/deepmind/alphafold)
- en: RoseTTAFold code and models [https://​github.​com/​RosettaCommons/​RoseTTA Fold](https://github.com/RosettaCommons/RoseTTAFold)
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RoseTTAFold代码和模型[https://github.com/RosettaCommons/RoseTTAFold](https://github.com/RosettaCommons/RoseTTAFold)
- en: ESMFold code and models [https://​github.​com/​facebookresearch​/​esm](https://github.com/facebookresearch/esm)
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ESMFold代码和模型[https://github.com/facebookresearch/esm](https://github.com/facebookresearch/esm)
- en: 7.5.1 Summary
  id: totrans-450
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 摘要
- en: Foundation Models can also be applied to DNA and protein sequences to derive
    contextual embeddings of the sequence elements. By this approach, the models are
    able to accumulate much knowledge about these sequences and achieve Sota performance
    across various downstream tasks, largely surpassing existing tools. The models
    can help to predict the 3-D structure of the protein. This is crucial for its
    function and may be instrumental in developing active substances to influence
    it.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型也可以应用于DNA和蛋白质序列，以推导序列元素的上下文嵌入。通过这种方法，模型能够积累大量关于这些序列的知识，并在各种下游任务中实现Sota性能，很大程度上超越了现有工具。这些模型可以帮助预测蛋白质的3-D结构。这对于其功能至关重要，并且可能在开发影响它的活性物质方面起到关键作用。
- en: '[![Creative Commons](../css/cc-by.png)](https://creativecommons.org/licenses/by/4.0)'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Creative Commons](../css/cc-by.png)](https://creativecommons.org/licenses/by/4.0)'
- en: '**Open Access** This chapter is licensed under the terms of the Creative Commons
    Attribution 4.0 International License ([http://​creativecommons.​org/​licenses/​by/​4.​0/​](http://creativecommons.org/licenses/by/4.0/)),
    which permits use, sharing, adaptation, distribution and reproduction in any medium
    or format, as long as you give appropriate credit to the original author(s) and
    the source, provide a link to the Creative Commons license and indicate if changes
    were made.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '**开放获取**本章根据Creative Commons Attribution 4.0国际许可协议([http://creativecommons.org/licenses/by/4.0/](http://creativecommons.org/licenses/by/4.0/))进行许可，允许在任何媒介或格式中使用、分享、改编、分发和复制，只要您适当引用原始作者和来源，提供Creative
    Commons许可的链接，并指出是否进行了更改。'
- en: The images or other third party material in this chapter are included in the
    chapter's Creative Commons license, unless indicated otherwise in a credit line
    to the material. If material is not included in the chapter's Creative Commons
    license and your intended use is not permitted by statutory regulation or exceeds
    the permitted use, you will need to obtain permission directly from the copyright
    holder.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的图像或其他第三方材料包含在本章的Creative Commons许可之下，除非在材料引用行中另有说明。如果材料未包含在本章的Creative Commons许可之下，且您的使用未得到法定规定的允许或超出了允许的使用范围，您需要直接从版权持有人处获得许可。
