<html><head></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="b978-3-031-23190-2_7"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">G. Paaß, S. Giesselbach</span><span class="ContextInformationBookTitles"><span class="BookTitle">Foundation Models for Natural Language Processing</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" lang="en">Artificial Intelligence: Foundations, Theory, and Algorithms</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-3-031-23190-2_7">https://doi.org/10.1007/978-3-031-23190-2_7</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">7. Foundation Models for Speech, Images, Videos, and Control</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Gerhard Paaß</span><sup><a href="#Aff5">1</a> <span class="ContactIcon"> </span></sup> and </span><span class="Author"><span class="AuthorName">Sven Giesselbach</span><sup><a href="#Aff5">1</a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff5"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Knowledge Discovery Department, Team NLU, Fraunhofer Institute for Intelligent Analysis and Information Systems (IAIS), Sankt Augustin, Nordrhein-Westfalen, Germany</div></div><div class="ClearBoth"> </div></div></div><section class="Abstract" id="Abs1" lang="en" role="doc-abstract"><h2 class="Heading">Abstract</h2><p class="Para" id="Par1">Foundation Models are able to model not only tokens of natural language but also token elements of arbitrary sequences. For images, square image patches can be represented as tokens; for videos, we can define tubelets that span an image patch across multiple frames. Subsequently, the proven self-attention algorithms can be applied to these tokens. Most importantly, several modalities like text and images can be processed in the same sequence allowing, for instance, the generation of images from text and text descriptions from video. In addition, the models are scalable to very large networks and huge datasets. The following multimedia types are covered in the subsequent sections. Speech recognition and text-to-speech models describe the translation of spoken language into text and vice versa. Image processing has the task to interpret images, describe them by captions, and generate new images according to textual descriptions. Video interpretation aims at recognizing action in videos and describing them through text. Furthermore, new videos can be created according to a textual description. Dynamical system trajectories characterize sequential decision problems, which can be simulated and controlled. DNA and protein sequences can be analyzed with Foundation Models to predict the structure and properties of the corresponding molecules.</p></section><div class="KeywordGroup" lang="en"><div class="Heading">Keywords</div><span class="Keyword" epub:type="keyword">Speech recognition</span><span class="Keyword" epub:type="keyword">Text-to-speech</span><span class="Keyword" epub:type="keyword">Image captioning</span><span class="Keyword" epub:type="keyword">Text-to-image</span><span class="Keyword" epub:type="keyword">Video interpretation</span><span class="Keyword" epub:type="keyword">Robot control</span><span class="Keyword" epub:type="keyword">DNA</span></div><!--End Abstract--><div class="Fulltext"><p class="Para" id="Par2">Astonishing results of Foundation Models in natural language tasks have led the multimedia processing community to study their application to speech recognition and computer vision problems. Among the most important advantages of Foundation Models is that they can model long dependencies between elements of the input sequence and support parallel processing of the sequence in contrast to recurrent networks. Unlike convolutional networks, Foundation Models require minimal restrictions in the modeling of dependencies and are able to define maps between high-dimensional quantities. In addition, the simple design of Foundation Models allows simultaneous processing of multiple modalities (e.g., images, videos, text and speech) using similar processing blocks. Moreover, the models are scalable to very large networks and huge datasets. These strengths of Foundation Models have led to comprehensive advances on a number of multimedia tasks.</p><div class="Para" id="Par3">We will describe multimedia applications in five areas and we will review the currently best approaches, taking into account necessary resources, e.g. computation and memory effort. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par4"><em class="EmphasisTypeItalic ">Speech</em> recognition and text-to-speech models (Sect. <span class="InternalRef"><a href="#Sec1">7.1</a></span>).</p></li><li><p class="Para" id="Par5"><em class="EmphasisTypeItalic ">Image</em> description by text and generating images from text (Sect. <span class="InternalRef"><a href="#Sec12">7.2</a></span>).</p></li><li><p class="Para" id="Par6"><em class="EmphasisTypeItalic ">Video</em> interpretation and video generation (Sect. <span class="InternalRef"><a href="#Sec23">7.3</a></span>).</p></li><li><p class="Para" id="Par7"><em class="EmphasisTypeItalic ">Dynamical system trajectories</em> describe sequential decision problems, which can be simulated and controlled (Sect. <span class="InternalRef"><a href="#Sec30">7.4</a></span>).</p></li><li><p class="Para" id="Par8"><em class="EmphasisTypeItalic ">DNA and protein sequences</em> can be analyzed with Foundation Models to predict the structure and properties of the corresponding molecules (Sect. <span class="InternalRef"><a href="#Sec35">7.5</a></span>).</p></li></ul></div></div><p class="Para" id="Par9">In addition, there are a number of applications, where several media types are processed simultaneously. There is a large list of more specialized media types, where multimodal PLMs have been used: tables [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>], text layout [<span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>], depth images [<span class="CitationRef"><a epub:type="biblioref" href="#CR119" role="doc-biblioref">119</a></span>], scene graphs [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>], SQL [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>], sign language [<span class="CitationRef"><a epub:type="biblioref" href="#CR199" role="doc-biblioref">199</a></span>], point cloud [<span class="CitationRef"><a epub:type="biblioref" href="#CR197" role="doc-biblioref">197</a></span>], symbolic knowledge graph [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>], multimodal knowledge graph [<span class="CitationRef"><a epub:type="biblioref" href="#CR201" role="doc-biblioref">201</a></span>], abstract syntax tree [<span class="CitationRef"><a epub:type="biblioref" href="#CR202" role="doc-biblioref">202</a></span>], optical flow [<span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>], etc. Processing these media types with Foundation Models is similar to the approaches described in the following sections.</p><p class="Para" id="Par10">Due to the enormous number of different Foundation Models in the literature, we focus on representative models that have high performance at the time of writing. We outline the inner logic and main features of the methods, taking into account the resources required, e.g., computational and memory requirements. For standard PLMs, a link to descriptions in earlier chapters is provided. Xu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR183" role="doc-biblioref">183</a></span>] compiled a survey on multimodal learning with transformers. Under the heading “Available Implementations” we list links to available code and pre-trained models for that task. Good sources for code are the websites <span class="ExternalRef"><a href="https://paperswithcode.com/"><span class="RefSource">https://​paperswithcode.​com/​</span></a></span>, the NLP index <span class="ExternalRef"><a href="https://index.quantumstat.com/"><span class="RefSource">https://​index.​quantumstat.​com/​</span></a></span>, and GitHub <span class="ExternalRef"><a href="https://github.com/github"><span class="RefSource">https://​github.​com/​github</span></a></span>. Processing these media types with PLMs is similar to the approaches described in the following sections.</p><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading"><span class="HeadingNumber">7.1 </span>Speech Recognition and Generation</h2><p class="Para" id="Par11">Spoken language is the most efficient and natural type of communication between humans. Therefore, it is also a preferred type of interaction with computer systems. In the next sections we describe advanced models for automatic speech recognition and text-to-speech systems.</p><section class="Section2 RenderAsSection2" id="Sec2"><h3 class="Heading"><span class="HeadingNumber">7.1.1 </span>Basics of Automatic Speech Recognition</h3><p class="Para" id="Par12"><em class="EmphasisTypeItalic ">Automatic speech recognition</em><span id="ITerm1"/> (<em class="EmphasisTypeItalic ">ASR</em>) receives a speech input as an audio file and converts it into natural language text. Speech is strongly influenced by gender, social style, dialect, speaking style, and speed. Human speech and accents vary widely, and these differences in speech patterns are one of the major obstacles in developing an automatic speech recognition system. Another impediment to the development of an ASR is finding sufficient training collections to train the ASR model. Currently, training data is available for only a few of the approximately 7000 world languages.</p><p class="Para" id="Par13">Since the advent of the computer in the 1950s, researchers started to develop speech recognition systems. In 1984, IBM introduced the first speech recognition system that could recognize about 5000 individual English words, and in 1993, a consumer ASR was offered. The predominant techniques were <em class="EmphasisTypeItalic ">n</em>-gram models, hidden Markov models, and neural networks [<span class="CitationRef"><a epub:type="biblioref" href="#CR102" role="doc-biblioref">102</a></span>]. After 2010, speech recognition based on RNNs was widely used for virtual assistants like Apple’s Siri, Amazon Alexa, and Google Assistant. Meanwhile, ASR is in use on most smartphones to enter text by voice even without an Internet connection.</p><p class="Para" id="Par14">The most important evaluation measure of ASR systems is the <em class="EmphasisTypeItalic ">word error rate</em><span id="ITerm2"/><span id="ITerm3"/><span class="InlineEquation" id="IEq1"><img alt="$$\text{WER}=\frac {S+D+I}{N}$$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq1.png" style="width:6.75em"/></span> measuring the deviation from a ground truth text. Here <em class="EmphasisTypeItalic ">S</em> is the number of word substitutions, <em class="EmphasisTypeItalic ">D</em> is the number of deletions, and <em class="EmphasisTypeItalic ">I</em> is the number of insertions in the output as compared to the ground truth with <em class="EmphasisTypeItalic ">N</em> words.</p><p class="Para" id="Par15">Conventional ASR systems usually consist of independent parts, such as an acoustic model, a pronunciation model, and a language model. These parts are trained separately and then combined for inference. Usually, a pre-processing module is employed to reduce the signal-to-noise ratio in the audio recording. There are different filters and methods that can be applied to a sound signal to reduce the associated noise. In addition, the speaker may be recorded with several microphones, which can localize the speaker and drastically reduce background noise (beamforming) [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>].</p><div class="Para" id="Par16">Subsequently, a feature extraction module has the task to generate features relevant for speech recognition, remove irrelevant information from the signal and reduce the input size. This often involves variants of Fourier transforms extracting the frequency of waveforms. Most commonly used feature extraction methods are <em class="EmphasisTypeItalic ">Mel Frequency Cepstral Coefficients</em> (MFCCs)<span id="ITerm4"/>, discrete wavelet transform (DWT), and linear predictive coding (LPC) [<span class="CitationRef"><a epub:type="biblioref" href="#CR101" role="doc-biblioref">101</a></span>]. An example is shown in Fig. <span class="InternalRef"><a href="#Fig1">7.1</a></span>.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><img alt="" aria-describedby="d64e518" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig1_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e518"><p class="Para" id="Par281">A heat map demonstrates the M F C C coefficients, frequency in kilo hertz, and amplitude versus time in seconds. The O S R us 0000010 8 k dot w a v displays fluctuations in amplitude, frequency and M F C C coefficients highlighted in colors.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.1</span><p class="SimplePara">Audio signal (top) with the frequency extracted by Fourier transform (middle) and the corresponding MFCCs (bottom). Image credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4"><span class="RefSource">A.​3</span></a></span></p></div></figcaption></figure></div><p class="Para" id="Par17">The final module is a classifier receiving a vector of fixed length characterizing the signal in the given time slot. It estimates the probability of output words or phonemes for the next time slot. Early classifiers could only handle a single speaker. New models were developed to recognize the speech utterances of multiple speakers. An example is an ASR system yielding a 5.1% word error rate (WER) on the switchboard test set [<span class="CitationRef"><a epub:type="biblioref" href="#CR181" role="doc-biblioref">181</a></span>]. It consists of CNN models like ResNet and LACE and bidirectional LSTMs for modeling acoustics. A survey of prior systems is provided by Malik et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR101" role="doc-biblioref">101</a></span>]. A survey of more recent ASR systems is given by Papastratis [<span class="CitationRef"><a epub:type="biblioref" href="#CR117" role="doc-biblioref">117</a></span>], who discuss RNN, CNN and Transformer models.</p></section>
<section class="Section2 RenderAsSection2" id="Sec3"><h3 class="Heading"><span class="HeadingNumber">7.1.2 </span>Transformer-Based Speech Recognition</h3><div class="Para" id="Par18">PLMs based on self-attention are a good choice for sequence modeling because they are able to capture interactions over long distances and require less computational effort. An overview is given in Table <span class="InternalRef"><a href="#Tab1">7.1</a></span>. However, PLMs are less capable of extracting fine-grained local feature patterns. Therefore, combinations of PLMs and CNNs are often used for ASR. The currently best LSTM-based ASR system <strong class="EmphasisTypeBold ">ContextNet + NST</strong><span id="ITerm5"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR121" role="doc-biblioref">121</a></span>] achieved an WER of 1.7% on LibriSpeech (clean). <div class="Table" id="Tab1"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 7.1</span><p class="SimplePara">Main speech recognition techniques</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Mechanism</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Performance</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">ContextNet + NST</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Currently best LSTM-based ASR system</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Librispeech WER 1.7%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Conformer</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">CNN + self-attention in transformer block, LSTM as language model</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Librispeech WER 1.9%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">wav2vec 2.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Encode speech by CNN, discretize input to transformer, predict masked input. Fine-tune for speech recognition</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Librispeech WER 1.5%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Combined SSL</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Conformer model + unsupervised wav2vec 2.0, SpecAugment to generate noisy training data</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Librispeech WER 1.4%</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">SpeechStew</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Similar to Combined SSL, trained on 7 datasets, fine-tuned for speech recognition</p></td><td style="text-align: left;"><p class="SimplePara">Librispeech WER 1.7% without Language model</p></td></tr></tbody></table></div></div><p class="Para" id="Par19">The <strong class="EmphasisTypeBold ">Conformer</strong><span id="ITerm6"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR59" role="doc-biblioref">59</a></span>] is a convolution-augmented Transformer. The Conformer integrates a convolutional module (Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec7"><span class="RefSource">1.​7</span></a></span>) and a self-attention module (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec19"><span class="RefSource">2.​3</span></a></span>) as layers inside an encoder block. The convolution module contains a 1 × 1 pointwise convolution with an expansion factor of 2 projecting the number of channels with a <em class="EmphasisTypeItalic ">Gated Linear Unit</em><span id="ITerm7"/> (GLU) activation layer, which allows the selection of features that are important for prediction. This is followed by a <em class="EmphasisTypeItalic ">1-D depthwise convolution</em><span id="ITerm8"/>, which applies a single convolutional filter for each input channel. Subsequently, there is a batch normalization and then a <em class="EmphasisTypeItalic ">Swish</em><span id="ITerm9"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR131" role="doc-biblioref">131</a></span>] activation layer.</p><p class="Para" id="Par20">The resulting model with 17 conformer blocks has up to 118M parameters and is trained on the <em class="EmphasisTypeItalic ">LibriSpeech</em><span id="ITerm10"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR116" role="doc-biblioref">116</a></span>] dataset, which contains audiobooks spoken by different speakers. It gets a vector of 80 filterbank features (Fig. <span class="InternalRef"><a href="#Fig1">7.1</a></span>) for each time slot of 10ms. The authors use SpecAugment [<span class="CitationRef"><a epub:type="biblioref" href="#CR120" role="doc-biblioref">120</a></span>] masking varying parts of the input signal to regularize the model. In addition, they train a 3-layer LSTM language model on the LibriSpeech corpus predicting the next word. The output of the language model is combined with the transformer output to emphasize words which are syntactically and semantically correct. Together with the LM the Conformer achieves a WER of 1.9% on LibriSpeech (clean). Without LM the WER was 2.1%.</p><p class="Para" id="Par21">The <strong class="EmphasisTypeBold ">S4</strong><span id="ITerm11"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>] model is able to process long input sequences of up to 16k elements (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec9"><span class="RefSource">3.​2.​2</span></a></span>). It was applied to speech classification and was able to improve <span class="EmphasisTypeSmallCaps ">Sota</span> to 98.3% while processing raw speech signals. This is an enormous error reduction compared to the prior <span class="EmphasisTypeSmallCaps ">Sota</span> accuracy of 95.3%. It can be expected that this model will also lead to a considerable reduction of errors in other speech recognition tasks.</p></section>
<section class="Section2 RenderAsSection2" id="Sec4"><h3 class="Heading"><span class="HeadingNumber">7.1.3 </span>Self-supervised Learning for Speech Recognition</h3><p class="Para" id="Par22">Self-supervised learning of speech has the potential to enhance speech recognition results with additional unlabeled data. It can be shown that self-training on a large set of unlabeled data leads to a strong improvement of models which achieve superior performance with relatively little fine-tuning data [<span class="CitationRef"><a epub:type="biblioref" href="#CR184" role="doc-biblioref">184</a></span>].</p><p class="Para" id="Par23"><strong class="EmphasisTypeBold ">wav2vec 2.0</strong><span id="ITerm12"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>] performs unsupervised learning on speech data without transcripts. Similar to the BERT model for text, it learns to predict masked sound “tokens”. wav2vec encodes raw speech audio by a multi-layer CNN yielding a latent representation of speech for every time slot. The continuous latent representation is discretized to tokens <em><strong class="EmphasisTypeBoldItalic ">q</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> with a quantization module. This discretization is a discontinuous operation and hinders gradient backpropagation.</p><div class="Para" id="Par24">One solution is to use an interpolation between the discrete result of sampling and the probability distribution. This can be achieved with the <em class="EmphasisTypeItalic ">Gumbel-Softmax distribution</em><span id="ITerm13"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR75" role="doc-biblioref">75</a></span>]. To sample a discrete distribution with probabilities <em class="EmphasisTypeItalic ">p</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">p</em><sub><em class="EmphasisTypeItalic ">k</em></sub> we can draw a random uniform variable <em class="EmphasisTypeItalic ">U</em> ∼uniform(0, 1) and compute <em class="EmphasisTypeItalic ">Z</em> = onehot(max<sub><em class="EmphasisTypeItalic ">i</em></sub><em class="EmphasisTypeItalic ">p</em><sub>1</sub> + ⋯<em class="EmphasisTypeItalic ">p</em><sub><em class="EmphasisTypeItalic ">i</em>−1</sub> ≤ <em class="EmphasisTypeItalic ">U</em>), where <em class="EmphasisTypeItalic ">i</em> = 1, …, <em class="EmphasisTypeItalic ">k</em> is the discrete index, and onehot(<em class="EmphasisTypeItalic ">j</em>) generates a vector of zeros with a one at position <em class="EmphasisTypeItalic ">j</em>. This sampling is not differentiable because of the max function. An alternative formula is <div class="Equation NumberedEquation" id="Equ1"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} Z = \text{onehot}(\text{argmax}_i (G_i + \log(p_i))), \end{aligned} $$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_Equ1.png" style="width:15.87em"/></div></div> <div class="EquationNumber">(7.1)</div></div></div></div><div class="Para" id="Par25">where <em class="EmphasisTypeItalic ">G</em><sub><em class="EmphasisTypeItalic ">i</em></sub> ∼Gumbel(0, 1) are i.i.d. samples drawn from the standard Gumbel distribution. This refactors the sampling of <em class="EmphasisTypeItalic ">Z</em> into a deterministic function of the parameters and some independent noise with a fixed distribution. Now a softmax function can be used as a differential approximation of argmax: <div class="Equation NumberedEquation" id="Equ2"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} y_i = \frac{\exp((G_i + \log p_i)/\tau)}{\sum_j \exp((G_j + \log p_j)/\tau)}. \end{aligned} $$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_Equ2.png" style="width:13.44em"/></div></div> <div class="EquationNumber">(7.2)</div></div></div></div><p class="Para" id="Par26"><em class="EmphasisTypeItalic ">τ</em> is the temperature parameter that controls how closely the new samples approximate the discrete vectors. This approximation is used during training and the discretized onehot vectors are computed during evaluation. wav2vec computes discrete vectors <em><strong class="EmphasisTypeBoldItalic ">q</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> by this approach.</p><p class="Para" id="Par27">The <em><strong class="EmphasisTypeBoldItalic ">q</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> representations of 10 randomly sampled consecutive time steps are masked and have to be reconstructed by a Transformer similar to BERT. The self-attention captures dependencies over the entire sequence of latent representations. This model was pre-trained on more than 1000h of labeled and unlabeled speech data. The pre-trained model is fine-tuned for speech recognition by adding a randomly initialized linear projection on top of the context network into <em class="EmphasisTypeItalic ">C</em> classes, which were the characters as well as a word boundary marker. To accommodate characters spanning several time slots the <em class="EmphasisTypeItalic ">connectionist temporal classification</em><span id="ITerm14"/> (CTC) loss [<span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>] was employed. The fine-tuning used 5h of audio data annotated with phonemes. On LibriSpeech the authors achieve a WER of 2.1%. A similar model with 300M parameters using 53k hours of unlabeled data for wave2vec and 10m of labeled data for fine-tuning achieves a WER of 3.0% on LibriSpeech [<span class="CitationRef"><a epub:type="biblioref" href="#CR184" role="doc-biblioref">184</a></span>]. Training on all data decreases WER to 1.5%.</p><p class="Para" id="Par28"><strong class="EmphasisTypeBold ">Combined SSL</strong><span id="ITerm15"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR196" role="doc-biblioref">196</a></span>] combines wave2vec unsupervised pre-training with the Conformer. The ASR network is a sequence ‘translator’ consisting of a Conformer encoder with up to 1B parameters and a multilayer LSTM decoder. In addition, the authors use Noisy Student Training (NST), where a teacher model is employed to generate transcripts for the unlabeled data via inference on audio. The teacher-labeled data, after filtering and balancing, are then used to train the next generation ASR model. On LibriSpeech the model achieves <span class="EmphasisTypeSmallCaps ">Sota</span> with 1.4% WER.</p><p class="Para" id="Par29"><strong class="EmphasisTypeBold ">w2v-BERT</strong><span id="ITerm16"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>] on the one hand performs contrastive learning discretizing continuous speech signals into a finite set of discriminative speech tokens. On the other hand, the model learns contextualized speech representations by solving a masked prediction task with the discretized tokens as input. During pre-training both tasks are simultaneously optimized in an end-to-end fashion. During fine-tuning the output of the pre-trained w2v-BERT model with 1B parameters is aggregated by a LSTM decoder. On the Librispeech benchmark it has a similar WER of 1.4% as the leading system and on the Librispeech benchmark test-other the model achieves a <span class="EmphasisTypeSmallCaps ">Sota</span> of 2.5% WER. In addition, the model with 600M parameters was fine-tuned on a voice search task that allows users to use Google Search by speaking on a mobile phone or computer. It consists of voice snippets with an average duration of 5.5sec. The model was able to decrease errors by about 30% to 6.2. <strong class="EmphasisTypeBold ">SpeechStew</strong><span id="ITerm17"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>] uses the Conformer 1B with wav2vec pre-training. It is pre-trained on 7 available speech recognition datasets without any domain-dependent re-balancing or re-weighting. Without a language model it achieves a WER of 1.7% on LibriSpeech.</p><p class="Para" id="Par30"><strong class="EmphasisTypeBold ">TERA</strong><span id="ITerm18"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR98" role="doc-biblioref">98</a></span>] is a self-supervised speech model using a multi-target auxiliary task to pre-train a transformer encoder on a large training set of unlabeled speech. The input can be any acoustic features, such as MFCC. The model learns by reconstructing acoustic frames from modified samples which were randomly changed with respect to three properties: Time alteration requires the reconstruction from corrupted blocks of time steps. Channel alteration has to restore the signal from missing blocks of frequency channels. Magnitude alteration involves the regeneration of altered feature magnitudes. By reconstructing these data changes, the model learns a better contextualized representation. The time alteration width is set to 85 ms of speech, which is about the average phoneme duration. The largest model similar to BERT has 170M parameters. The model has strong results for phone classification, speaker recognition, and speech recognition, e.g. on the TIMIT benchmark with 14.5% phone error rate (PER).</p><p class="Para" id="Par31">In a comprehensive analysis, Zhang et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR195" role="doc-biblioref">195</a></span>] evaluate the benefit of self-supervised pre-training for ASR. They employ Conformer models with 600M to 8B parameters pre-trained and self-trained on extremely large and diverse unlabeled datasets containing thousands to a million hours of audio (<em class="EmphasisTypeItalic ">BigSSL</em><span id="ITerm19"/>). Using only 3% of the labeled data they obtain comparable results to the <span class="EmphasisTypeSmallCaps ">Sota</span> of the Voice Search benchmark. On eight ASR benchmarks they are able to match or improve <span class="EmphasisTypeSmallCaps ">Sota</span> after pre-training. On five non-ASR task such as language identification and emotion detection, they can improve <span class="EmphasisTypeSmallCaps ">Sota</span>. For large datasets, the gains from pre-training are smaller but still significant.</p><p class="Para" id="Par32">Many applications benefit from understanding not only words but also other information, such as a person’s emotion during an utterance, whether the speaker is wearing a mask, or whether the speech is synthetic. Shor [<span class="CitationRef"><a epub:type="biblioref" href="#CR156" role="doc-biblioref">156</a></span>] presents a large-scale, conformer-based architecture with more than 600M parameters that can be fine-tuned to detect these additional features and delivers <span class="EmphasisTypeSmallCaps ">Sota</span> performance.</p><section class="Section3 RenderAsSection3" id="Sec5"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par33"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par34">Conformer: <span class="ExternalRef"><a href="https://github.com/PaddlePaddle/PaddleSpeech"><span class="RefSource">https://​github.​com/​PaddlePaddle/​PaddleSpeech</span></a></span></p></li><li><p class="Para" id="Par35">wav2vec: <span class="ExternalRef"><a href="https://github.com/facebookresearch/fairseq"><span class="RefSource">https://​github.​com/​facebookresearch​/​fairseq</span></a></span> sequence modeling toolkit for translation, summarization, language modeling and other text generation tasks.</p></li><li><p class="Para" id="Par36">Tera: <span class="ExternalRef"><a href="https://github.com/s3prl/s3prl"><span class="RefSource">https://​github.​com/​s3prl/​s3prl</span></a></span></p></li><li><p class="Para" id="Par37">Hugging Face speech recognition: <span class="ExternalRef"><a href="https://huggingface.co/models?pipeline_tag=automatic-speech-recognition"><span class="RefSource">https://​huggingface.​co/​models?​pipeline_​tag=​automatic-speech-recognition</span></a></span></p></li><li><p class="Para" id="Par38">TensorFlow SST: <span class="ExternalRef"><a href="https://tfhub.dev/s?module-type=audio-stt"><span class="RefSource">https://​tfhub.​dev/​s?​module-type=​audio-stt</span></a></span></p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec6"><h3 class="Heading"><span class="HeadingNumber">7.1.4 </span>Text-to-Speech</h3><p class="Para" id="Par39">Speech synthesis is about generating speech from another modality like text, lip movements, etc. A <em class="EmphasisTypeItalic ">Text-to-Speech</em><span id="ITerm20"/> (<em class="EmphasisTypeItalic ">TTS</em>) system aims to convert natural language text into speech. <em class="EmphasisTypeItalic ">Mean Opinion Score</em><span id="ITerm21"/> (<em class="EmphasisTypeItalic ">MOS</em><span id="ITerm22"/>) is the most frequently used method to evaluate the quality of the generated speech. MOS is defined as the arithmetic mean over single ratings performed by human raters for a given stimulus in a subjective quality evaluation test. MOS has a range from 0 to 5, where real human speech is between 4.5 and 4.8. A comprehensive and up-to-date survey of TTS systems is provided by Tan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR163" role="doc-biblioref">163</a></span>].</p><p class="Para" id="Par40">While earlier TTS systems simply concatenated prerecorded speech segments, modern systems perform a complete synthesis of speech. <strong class="EmphasisTypeBold ">WaveNet</strong><span id="ITerm23"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR114" role="doc-biblioref">114</a></span>] was the first model that successfully modeled the raw waveform of the audio signal instead of the acoustic features. It is able to generate new speech-like waveforms at 16,000 samples per second. WaveNet in its core is an autoregressive model consisting of dilated convolutions where each sample depends on the previous ones. In each layer the number of included time steps is doubled. WaveNet was able to increase the MOS-value from 3.86 to 4.21. <em class="EmphasisTypeItalic ">Fast WaveNet</em><span id="ITerm24"/> was able to reduce the quadratic time complexity to linear complexity by caching previous calculations.</p><p class="Para" id="Par41"><strong class="EmphasisTypeBold ">Tacotron 2</strong><span id="ITerm25"/> is a neural network architecture for speech synthesis directly from text. It consists of a recurrent LSTM sequence-to-sequence feature prediction network with attention, which predicts a sequence of mel spectrogram frames from an input character sequence and a modified version of WaveNet, which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames. Tacotron 2 achieved an impressive MOS of 4.53.</p><p class="Para" id="Par42">As TTS performs sequence processing similar to NLP, it is only natural that PLMs are also used in this area. Transformer-based models aim to mitigate two problems of previous TTS methods such as Tacotron 2: their high computational cost for training and inference, and the difficulty of modeling long dependencies with LSTMs.</p><div class="Para" id="Par43"><strong class="EmphasisTypeBold ">Transformer TTS</strong><span id="ITerm26"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR94" role="doc-biblioref">94</a></span>] adapts the original transformer encoder-decoder [<span class="CitationRef"><a epub:type="biblioref" href="#CR168" role="doc-biblioref">168</a></span>] to speech synthesis. The encoder receives phonemes as input, which are adapted by an encoder pre-net consisting of a CNN and a fully connected layer. The standard transformer encoder outputs contextual phoneme embeddings (Fig. <span class="InternalRef"><a href="#Fig2">7.2</a></span>). The decoder receives mel frames as input, which are converted by a decoder pre-net with two fully connected layers to generate appropriate embeddings. The standard decoder generates mel frames output embeddings. These are further processed by two different linear projections to predict the mel spectrogram and the stop token respectively. A 5-layer CNN produces a residual to refine the reconstruction of mel spectrogram. A WaveNet vocoder generates the final audio output. Both the encoder and decoder of the Transformer consists of 6 layers with 8 heads. The model is about 4.25 times faster than Tacotron 2 and achieves a MOS of 4.39 close to human quality.<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO2"><img alt="" aria-describedby="d64e1196" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig2_HTML.png" style="width:23.12em"/><div class="TextObject" id="d64e1196"><p class="Para" id="Par282">A flow diagram illustrates the predicted mel spectrogram, 5 layer C N N, linear projection, 6 layers with 8 heads, scaled positional embeddings 3 layer C N N, and rules. From bottom to top it has text to phonem converter, encoder pre net, decoder pre net, post net and stop token.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.2</span><p class="SimplePara">Speech synthesis with the transformer TTS. The encoder as well as the decoder have 6 layers with 8 attention heads and residual connections. The resulting mel spectrogram is transformed into the final audio output by a WaveNet vocoder [<span class="CitationRef"><a epub:type="biblioref" href="#CR94" role="doc-biblioref">94</a></span>]. Image credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4"><span class="RefSource">A.​3</span></a></span></p></div></figcaption></figure></div><p class="Para" id="Par44"><strong class="EmphasisTypeBold ">FastSpeech 2</strong><span id="ITerm27"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR138" role="doc-biblioref">138</a></span>] tackles the problem that an input text can correspond to multiple possible speech sequences due to variations in speech, such as pitch, duration, sound volume and prosody. It encodes the input phonemes by a transformer encoder to generate embeddings. Then a variance adaptor adds different variance information such as duration, pitch and energy into the hidden sequence. Finally, the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. Both the encoder as well as the mel-spectrogram decoder have layers containing transformer blocks and 1D-convolutions. The variance adaptor predicts not only the duration, but also pitch and energy, using layers with 1D convolutions, feedforward layers, and layer normalization with dropout for regularization.</p><p class="Para" id="Par45">The variant <em class="EmphasisTypeItalic ">Fastspeech 2s</em><span id="ITerm28"/> directly generates waveform from text without cascaded mel-spectrogram generation (acoustic model) and waveform generation (for example a vocoder, like wav2vec). The final waveform decoder consist of gated activations as well as different types of 1d-convolutions and dilated 1d-convolutions to cover a wider time range. The authors employ adversarial training in the waveform decoder to force it to implicitly recover the phase information by itself.</p><p class="Para" id="Par46">In their experiments the authors determine the following MOS-values: Tacotron 2: 3.70, Transformer TTS: 3.72, FastSpeech 2: 3.83, FastSpeech 2s: 3.71, and human speech: 4.30. Note that the difference to human speech is mainly caused by the vocoder. In addition, FastSpeech 2 and FastSpeech 2s are about 50 times faster than Transformer TTS at inference time.</p><p class="Para" id="Par47"><strong class="EmphasisTypeBold ">AdaSpeech 2</strong><span id="ITerm29"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR186" role="doc-biblioref">186</a></span>] adapts a TTS system to a target speaker. Only sound recordings of the target speaker without text transcription are required. The authors apply a mel-spectrogram encoder to a well-trained TTS model to conduct speech reconstruction, and at the same time constrain the output sequence of the mel-spectrogram encoder to be close to that of the original phoneme encoder. The mel encoder also consists of 4 feed-forward Transformer blocks. Note that the original system does not need to be retrained, only the mel encoder. During the fine-tuning to the target speaker, the mel decoder parameters are adapted. The model achieves on-par MOS voice quality with the transcribed TTS adaptation.</p><p class="Para" id="Par48">Recently Amazon has announced that Alexa will be able to mimic the voices of other persons [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>]. To “make memories last” Alexa could, for instance, tell stories and play music using the voice of the deceased grandmother. Amazon notes, that it would take only about a minute of audio recording to imitate a voice.</p><section class="Section3 RenderAsSection3" id="Sec7"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par49"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par50">Tacotron 2: <span class="ExternalRef"><a href="https://github.com/NVIDIA/tacotron2"><span class="RefSource">https://​github.​com/​NVIDIA/​tacotron2</span></a></span></p></li><li><p class="Para" id="Par51">TransformerTTS: <span class="ExternalRef"><a href="https://github.com/as-ideas/TransformerTTS"><span class="RefSource">https://​github.​com/​as-ideas/​TransformerTTS</span></a></span></p></li><li><p class="Para" id="Par52">FastSpeech 2: <span class="ExternalRef"><a href="https://github.com/ming024/FastSpeech2"><span class="RefSource">https://​github.​com/​ming024/​FastSpeech2</span></a></span></p></li><li><p class="Para" id="Par53">AdaSpeech 2: <span class="ExternalRef"><a href="https://github.com/rishikksh20/AdaSpeech2"><span class="RefSource">https://​github.​com/​rishikksh20/​AdaSpeech2</span></a></span></p></li><li><p class="Para" id="Par54">Hugging Face TTS: <span class="ExternalRef"><a href="https://huggingface.co/models?pipeline_tag=text-to-speech"><span class="RefSource">https://​huggingface.​co/​models?​pipeline_​tag=​text-to-speech</span></a></span></p></li><li><p class="Para" id="Par55">Mozilla TTS Text-to-Speech for all: <span class="ExternalRef"><a href="https://github.com/mozilla/TTS"><span class="RefSource">https://​github.​com/​mozilla/​TTS</span></a></span></p></li><li><p class="Para" id="Par56">TensorFlow TTS: <span class="ExternalRef"><a href="https://tfhub.dev/s?module-type=audio-speech-synthesis"><span class="RefSource">https://​tfhub.​dev/​s?​module-type=​audio-speech-synthesis</span></a></span></p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec8"><h3 class="Heading"><span class="HeadingNumber">7.1.5 </span>Speech-to-Speech Language Model</h3><p class="Para" id="Par57"><strong class="EmphasisTypeBold ">GSLM</strong><span id="ITerm30"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR89" role="doc-biblioref">89</a></span>] is a language model which receives raw speech audio as input and directly generate outputs. It can, for instance, be used to create a dialog system without intermediate text representation. Internally the model converts incoming raw speech to discrete pseudo-text units. As discretizers CPC [<span class="CitationRef"><a epub:type="biblioref" href="#CR113" role="doc-biblioref">113</a></span>], wave2vec 2.0 [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>], and HuBERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>] were used to create embeddings of varying length (50, 100, 200). The selection of units is difficult, as there is no vocabulary of sound units, and sound units have variable length with no obvious segmentation. Similar to BERT, HuBERT is trained with a masked prediction task using masked continuous audio signals as inputs. In experiments HuBERT performed best in most cases, followed by CPC.</p><p class="Para" id="Par58">The autoregressive “unit-based” language model has 12 layers and is trained on samples with up to 3k units generated from the 6k hours <em class="EmphasisTypeItalic ">LibriLight speech data</em><span id="ITerm31"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR139" role="doc-biblioref">139</a></span>]. To generate speech from units a modified version of the Tacotron-2 model [<span class="CitationRef"><a epub:type="biblioref" href="#CR154" role="doc-biblioref">154</a></span>] was employed, which takes pseudo-text units as input and outputs a log Mel spectrogram. To generate waveforms the pre-trained vocoder <em class="EmphasisTypeItalic ">WaveGlow</em><span id="ITerm32"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR125" role="doc-biblioref">125</a></span>] was used, which converts the log Mel spectrogram to speech.</p><p class="Para" id="Par59">In a first test the speech input was encoded into units, which were translated to speech. Here the intelligibility of the resulting speech is assessed by a human MOS opinion score. When trained on the <em class="EmphasisTypeItalic ">LJ Speech data</em><span id="ITerm33"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR74" role="doc-biblioref">74</a></span>] the unsupervised model achieved a MOS (Mean Opinion Score) score of 4.00, while the combination of an ASR and TTS system achieved a slightly better score of 4.04 [<span class="CitationRef"><a epub:type="biblioref" href="#CR89" role="doc-biblioref">89</a></span>]. When testing the full language model generation, the model achieved a MOS score of 4.01, while the combination of ASR and a language model yielded a score of 3.91. According to the authors, the generated speech sounds like English, has recognizable phonemes and words. Examples show that improvements are needed at the language and syntax level. For sound transcription 200 units were good, while for language modeling a smaller number of units seems to be better. It can be expected that the quality can be improved with additional training data.</p></section>
<section class="Section2 RenderAsSection2" id="Sec9"><h3 class="Heading"><span class="HeadingNumber">7.1.6 </span>Music Generation</h3><p class="Para" id="Par60">Foundation Models can also be applied to other sequence data, e.g. music. On the one hand a music language model can be trained, which is able to generate new music corresponding to the training data. On the other hand, a model can generate music conditioned on external information, e.g. lyrics or video. Bilici [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>] provide a survey on recent music generation models.</p><p class="Para" id="Par61">A prominent approach to music generation is <strong class="EmphasisTypeBold ">MuseNet</strong><span id="ITerm34"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR123" role="doc-biblioref">123</a></span>] which employs the Sparse Transformer, a variant of GPT-2. It calculates attention patterns over a context of 4096 MIDI characters. To generate new compositions, one can select a composer and use the starting notes of a known piece. Then up to ten different instruments can be selected, and the system will generate a piece of music with the required characteristics. The ratings of experts are quite favorable. Similarly, the <strong class="EmphasisTypeBold ">Music Transformer</strong><span id="ITerm35"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR71" role="doc-biblioref">71</a></span>] generates piano pieces. <strong class="EmphasisTypeBold ">Theme Transformer</strong><span id="ITerm36"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR155" role="doc-biblioref">155</a></span>] receives a theme as input and is trained to include this theme multiple times in its generation result.</p><p class="Para" id="Par62"><strong class="EmphasisTypeBold ">Jukebox</strong><span id="ITerm37"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>] adopts a multiscale vector quantizer variational autoencoder model (VQ-VAE) [<span class="CitationRef"><a epub:type="biblioref" href="#CR113" role="doc-biblioref">113</a></span>] to compress raw audio to discrete codes. This is based on an autoregressive Transformer and works also for human voices. Three separate VQ-VAE models with different temporal resolutions are employed. The trained model can be conditioned on an artist and a genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. The model is capable of generating pieces that are many minutes long, and with recognizable singing in natural-sounding voices. A number of samples are available [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>].</p><p class="Para" id="Par63"><strong class="EmphasisTypeBold ">CMT</strong><span id="ITerm38"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>] generates background music for a specific video. It aims to match the rhythm, timing, and movement speed of the video. CMT extracts these features from the video and allows global control of the music genre and instruments. The model does not require paired video and music training data. Experiments demonstrate that the generated background music has achieved satisfactory compatibility with the input videos, and at the same time, impressive music quality.</p><section class="Section3 RenderAsSection3" id="Sec10"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par64"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par65">CMT Controllable Music Transformer <span class="ExternalRef"><a href="https://github.com/wzk1015/video-bgm-generation"><span class="RefSource">https://​github.​com/​wzk1015/​video-bgm-generation</span></a></span></p></li><li><p class="Para" id="Par66">Jukebox: A Generative Model for Music <span class="ExternalRef"><a href="https://github.com/openai/jukebox"><span class="RefSource">https://​github.​com/​openai/​jukebox</span></a></span></p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec11"><h3 class="Heading"><span class="HeadingNumber">7.1.7 </span>Summary</h3><p class="Para" id="Par67">Speech recognition has shown an enormous progress in recent years and Foundation Models are now an established approach to this task. They are combined with CNN blocks and are able to capture interactions over long distances and reduce processing times. Similar to NLP, self-supervised learning has led to great performance gains. Instead of tokens, as in NLP, discrete sound representations are generated. A number of different models follow this scheme, and they are able to increase <span class="EmphasisTypeSmallCaps ">Sota</span> on different benchmarks.</p><p class="Para" id="Par68">The generation of speech from text has improved dramatically in recent years. WaveNet was the first model to generate speech-like waveforms at 16,000 samples per second. Transformers can be used to convert input phonemes to mel spectrograms, from which a vocoder can generate speech audio. There are variants like FastSpeech 2s, which directly transform text to an audio signal. The output quality of the models is close to human speech. Some models are able to adapt their output to the voice of individual speakers. This is impressive, but also a major security problem if in this way false utterances are produced imitating a person’s voice. The recent S4 state-space model for long input sequences was able to reduce errors by 60% for classifying speech signals. It can be expected that this model will also lead to a considerable reduction of errors in other speech recognition tasks.</p><p class="Para" id="Par69">Speech recognition and text-to-speech can be integrated with other applications. SpeechBert [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>] is an end-to-end Speech Question Answering (SQA) model by encoding audio and text with a single Transformer encoder, which is pre-trained with MLM on speech and text corpora and fine-tuned on Question Answering. Live speech translations are generated on-the-fly in a smartphone and allow a seamless communication in a foreign language [<span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR81" role="doc-biblioref">81</a></span>]. And GSLM is a generative language model, which directly processes discretized sound tokens.</p><p class="Para" id="Par70">Music generation is a related topic. Autoregressive PLMs, e.g. MuseNet or Music Transformer, can be used to generate music based on a pre-training with a large corpus. Here the composer style and the instrument may be selected. In addition, music can be conditioned on some input, e.g. lyric text for the Jukebox model or a video to compose background music.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec12"><h2 class="Heading"><span class="HeadingNumber">7.2 </span>Image Processing and Generation</h2><div class="Para" id="Par71">The breakthrough of Foundation Models in NLP has generated tremendous interest in the computer vision community to adapt these models for vision and multi-modal learning tasks. Two factors are important for their success: self-attention and self-supervision. Self-attention layers generate representations that take into account the relationships between the tokens (text token and/or visual tokens). Self-supervision predicts masked or modified parts of data elements during training in large-scale datasets. It allows gaining enormous knowledge about the data without manually annotating it and assumes minimal inductive biases compared to other models like CNN and RNN. Comprehensive surveys on Foundation Models for vision and language applications are provided by Khan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR84" role="doc-biblioref">84</a></span>] and Du et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>]. Hafiz et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>] give an overview over attention mechanisms and Deep Learning for machine vision. There is a recent tutorial on vision and language research [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>]. The main features of the models discussed in this section are compiled in Table <span class="InternalRef"><a href="#Tab2">7.2</a></span>. <div class="Table" id="Tab2"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 7.2</span><p class="SimplePara">Main techniques to combine text and images.<strong class="EmphasisTypeBold ">Benchmarks:</strong> VQA: COCO Visual Question Answering dataset (Sect. <span class="InternalRef"><a href="#Sec17">7.2.5</a></span>) [<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>]; img-gen: MS-COCO image generation benchmark with fine-tuning; img-gen-0: MS-COCO image generation benchmark zero-shot; ImageNet: ImageNet classification top1 accuracy; captions: MS-COCO image captioning benchmark; FID: Fréchet Inception Distance should be small (Sect. <span class="InternalRef"><a href="#Sec18">7.2.6</a></span>) [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>]. Numbers in parentheses are parameter counts</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Approach</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Benchmark</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Vision Transformer (ViT) Sect. <span class="InternalRef"><a href="#Sec14">7.2.2</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Concatenate text tokens and image token generated from image patches. Process with a BERT autoencoder and perform classification (632M)</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">ImageNet <span class="EmphasisTypeSmallCaps ">Sota</span> acc. 90.5%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">CLIP Sect. <span class="InternalRef"><a href="#Sec16">7.2.4</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Encode image with vision transformer and text with a GPT autoencoder. Maximize similarity of image and text embeddings, predict if they belong together</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"> </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">VilBERT Sect. <span class="InternalRef"><a href="#Sec17">7.2.5</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Extract bounding boxes with Faster R-CNN. Image regions and text are encoded by two BERT autoencoders and perform cross-attention. Fine-tuned to VQA</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">VQA <span class="EmphasisTypeSmallCaps ">Sota</span> 70.9%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">OSCAR Sect. <span class="InternalRef"><a href="#Sec17">7.2.5</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Extract bounding boxes with Faster R-CNN. A BERT autoencoder associates region descriptions with text. Fine-tuned for 7 tasks, e.g. image captioning</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">captions <span class="EmphasisTypeSmallCaps ">Sota</span> 41.7 <span class="EmphasisTypeSmallCaps ">Bleu</span>-4</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">VinVL Sect. <span class="InternalRef"><a href="#Sec17">7.2.5</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Uses ResNeXT model as region extractor and OSCAR. Fine-tuned for image captioning</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">captions 40.4 <span class="EmphasisTypeSmallCaps ">Bleu</span>-4</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">DALL-E Sect. <span class="InternalRef"><a href="#Sec18">7.2.6</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Text is encoded as tokens, image is transformed to image tokens by variational autoencoders (VAE). Uses GPT-3 (12B) to generate new image tokens</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">img-gen-0 17.9 FID</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLIDE Sect. <span class="InternalRef"><a href="#Sec19">7.2.7</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Reverses diffusion which destroys an image. Generates image by small changes with U-Net model (3.8B)</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">img-gen-0 <span class="EmphasisTypeSmallCaps ">Sota</span> 12.2 FID</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">XMC-GAN Sect. <span class="InternalRef"><a href="#Sec19">7.2.7</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GAN-based image generator, generator creates images, discriminator discriminates fake and real images</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">img-gen <span class="EmphasisTypeSmallCaps ">Sota</span> 9.3 FID</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">CogView Sect. <span class="InternalRef"><a href="#Sec19">7.2.7</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Vector quantized VAE. GPT-model (4B) is trained with text tokens and quantized image tokens</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">img-gen <span class="EmphasisTypeSmallCaps ">Sota</span> on blurred images</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">LAFITE Sect. <span class="InternalRef"><a href="#Sec19">7.2.7</a></span></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Uses CLIP to transform text to image embeddings. Train to modulate layers of StyleGAN2 [<span class="CitationRef"><a epub:type="biblioref" href="#CR82" role="doc-biblioref">82</a></span>] to generate images</p></td><td style="text-align: left;"><p class="SimplePara">img-gen <span class="EmphasisTypeSmallCaps ">Sota</span> 8.1 FID img-gen-0 16.9 FID</p></td></tr></tbody></table></div></div><div class="Para" id="Par72"><div class="Table" id="Tab3"><div class="Caption" lang="en"><div class="CaptionContent"><p class="SimplePara">Table 7.2</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Approach</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Benchmark</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">OFA Sect. <span class="InternalRef"><a href="#Sec20">7.2.8</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Uses text, image tokens and objects with bounding boxes. Seq2seq model (472M) pre-trained to associate tokens and objects. Text instructions control 9 different tasks</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">img-gen <span class="EmphasisTypeSmallCaps ">Sota</span> 10.5 FID captions <span class="EmphasisTypeSmallCaps ">Sota</span> 43.5 <span class="EmphasisTypeSmallCaps ">Bleu</span>-4</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">DALL-E 2 Sect. <span class="InternalRef"><a href="#Sec19">7.2.7</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Generate in image embedding from text by CLIP, transform to 1024 × 1024 image by diffusion decoder</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">img-gen-0 <span class="EmphasisTypeSmallCaps ">Sota</span> 10.4 FID</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Imagen Sect. <span class="InternalRef"><a href="#Sec19">7.2.7</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Generate text embeddings by T5-XXL, generate image patches by diffusion model, upsampling to 1024 × 1024 by two superresolution diffusion models</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">img-gen-0 <span class="EmphasisTypeSmallCaps ">Sota</span> 7.3 FID</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Stable Diffusion Sect. <span class="InternalRef"><a href="#Sec19">7.2.7</a></span></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Generate images using U-Net and diffusion</p></td><td style="text-align: left;"><p class="SimplePara">ImageNet conditional 3.6 FID</p></td></tr></tbody></table></div></div><section class="Section2 RenderAsSection2" id="Sec13"><h3 class="Heading"><span class="HeadingNumber">7.2.1 </span>Basics of Image Processing</h3><div class="Para" id="Par73">Image processing can solve a variety of tasks, as shown in Fig. <span class="InternalRef"><a href="#Fig3">7.3</a></span>. The main content of an image can be described by classifying the most important object in the image. More demanding is the identification and classification of relevant objects in an image. This also requires the description of the object positions by bounding boxes. Creating a caption for an image involves identifying the most important objects in the image, how they relate to each other, and describing them using a natural language sentence. Related to this is the retrieval of an image that corresponds to a caption. Visual question answering requires interpreting a question and analyzing the image to generate an answer in natural language. A variant is multimodal verification, where the truth of a statement about the image has to be assessed.<figure class="Figure" id="Fig3"><div class="MediaObject" id="MO3"><img alt="" aria-describedby="d64e1767" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig3_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e1767"><p class="Para" id="Par283">A photo of a child wears a shirt and pants and stands on a grass lane and holds a bread. 2 crows stands beside the child. A chart on the right explains, visual question answering, object classification and identification, verification, caption based image retrieval, and automatic image captioning.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.3</span><p class="SimplePara">Image analysis can be used to solve a number of different tasks. Depending on the task, the system receives a text (green) and an image as input and generates a text (blue) and an image as output. Image credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4"><span class="RefSource">A.​3</span></a></span></p></div></figcaption></figure></div><p class="Para" id="Par74">Many tasks involve the creation of a new image. A prominent example is the generation of a completely new image according to a caption. Alternatively a missing image area can be filled in. A variant is to change the style of an image according to a caption, e.g. from a photo to a painting in the style of van Gogh. This can be also performed for a specific image region.</p><p class="Para" id="Par75">An important aspect is the representation of images for transformers. Language models partition text into a sequence of tokens, which form the input of a transformer. The same approach is chosen for images, which are partitioned into small image patches. The contents of each patch can be represented by a vector, which forms the input of the transformer. The location of the patch is encoded by a position embedding, which is added to the input embedding.</p><p class="Para" id="Par76">The embedding of an image patch can be simply a learnable linear transformation of its pixel values. Other transformations may be used, e.g. small CNN models or variational autoencoders (Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec7"><span class="RefSource">1.​7</span></a></span>). To get more robust representations, the generated vectors are often discretized to get rid of local noise. In addition, text from a caption or region annotation can be used as input. As usual, this text is converted to tokens from a vocabulary.</p><p class="Para" id="Par77">To model the interaction between image elements and text, different transformer architectures can be used (Table <span class="InternalRef"><a href="#Tab2">7.2</a></span>). A <em class="EmphasisTypeItalic ">single stream architecture</em><span id="ITerm39"/> concatenates all inputs and processes them with a single transformer. This allows to determine interactions between different input elements, but requires the handling of long sequences. Dual-stream or <em class="EmphasisTypeItalic ">multi-stream architectures</em><span id="ITerm40"/> process different modalities or image resolutions by separate PLMs. In this case the input sequences are shorter. Various forms of interaction between the streams have been proposed (e.g. cross-attention). Later the outputs may be compared by similarity measures or combined by other PLMs.</p><p class="Para" id="Par78">The pre-training task for vision follows the pattern of the text transformer. <em class="EmphasisTypeItalic ">Masked language modeling</em><span id="ITerm41"/> (MLM) masks a fraction of the input tokens and requires the model to predict the tokens from the context. If there are text and image tokens, the information in both modalities can be utilized for this task and the model learns the association between text and image elements. Similarly, image regions can be masked and reconstructed from the text and image context. In a classification task, the model can determine whether a caption correctly describes an image or is some random text. In this way, the correlation between text and images can be trained. Another goal is to learn a joint image and word representation in the same semantic space by pushing together the embeddings of matched image-text pairs, while pushing apart the non-matched pairs. For this image-to-text <em class="EmphasisTypeItalic ">contrastive loss</em><span id="ITerm42"/>, the proximity of embeddings is measured by a scalar product between the embeddings.</p></section>
<section class="Section2 RenderAsSection2" id="Sec14"><h3 class="Heading"><span class="HeadingNumber">7.2.2 </span>Vision Transformer</h3><p class="Para" id="Par79">The <strong class="EmphasisTypeBold ">ViT</strong><span id="ITerm43"/> (Vision Transformer) [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>] applies a pure Transformer encoder (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec20"><span class="RefSource">2.​3.​1</span></a></span>) to image patches. The input image <span class="InlineEquation" id="IEq2"><img alt="$${\boldsymbol {x}}\in \mathbb {R}^{H\times W\times c}$$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq2.png" style="width:5.82em"/></span> has <em class="EmphasisTypeItalic ">H</em> × <em class="EmphasisTypeItalic ">W</em> pixels and <em class="EmphasisTypeItalic ">c</em> color channels. It is partitioned into patches of <em class="EmphasisTypeItalic ">s</em> × <em class="EmphasisTypeItalic ">s</em> pixel, e.g. <em class="EmphasisTypeItalic ">s</em> = 16. Each of the <em class="EmphasisTypeItalic ">N</em> = <em class="EmphasisTypeItalic ">HW</em>∕<em class="EmphasisTypeItalic ">s</em><sup>2</sup> patches consist of <em class="EmphasisTypeItalic ">s</em><sup>2</sup> ∗ <em class="EmphasisTypeItalic ">c</em> numbers, which are linearly mapped to a vector of length <em class="EmphasisTypeItalic ">d</em> used as the inputs of the transformer. Usually, a one-dimensional position embedding is added, because two-dimensional positions gave no significant performance improvement. Different models ViT<sub>Base</sub>, ViT<sub>Large</sub>, and ViT<sub>Huge</sub> with 12, 24, and 32 layers and 86M, 307M and 632M parameters respectively are employed.</p><div class="Para" id="Par80">The transformer encoder has an input sequence length of <em class="EmphasisTypeItalic ">N</em> consisting of vectors of size <em class="EmphasisTypeItalic ">d</em>. Each layer generates <em class="EmphasisTypeItalic ">N</em> embeddings of length <em class="EmphasisTypeItalic ">d</em>. The output embedding of the [CLS] token in the last encoder block is the input to a logistic classifier to compute probabilities of the image classes. The architecture is shown in Fig. <span class="InternalRef"><a href="#Fig4">7.4</a></span>.<figure class="Figure" id="Fig4"><div class="MediaObject" id="MO4"><img alt="" aria-describedby="d64e1947" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig4_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e1947"><p class="Para" id="Par284">A photo of a tall building from a bottom view. The image processing steps include C L S a special token used to summarize the entire sequence of patches, linear projection of patches. Each patch is treated as a word in N L P, Transformer encoder layers, and L the length of the sequence of patches.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.4</span><p class="SimplePara">The Vision Transformer ViT partitions an image into square patches of fixed size. For each patch an embedding is calculated by a linear projection. A standard encoder computes contextual embeddings. The embeddings of the [CLS] token is used to compute a class by a logistic classifier [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>]. Image adapted from [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>] with permission of the authors, credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4"><span class="RefSource">A.​3</span></a></span></p></div></figcaption></figure></div><p class="Para" id="Par81">It is remarkable that the images may be trained with varying input image resolutions. But patch size is always the same yielding different input size lengths. To take the new resolution into account, a 2D interpolation of the position embeddings is performed. The model is typically pre-trained on a large dataset JFT-300M [<span class="CitationRef"><a epub:type="biblioref" href="#CR161" role="doc-biblioref">161</a></span>] to predict masked inputs. It is fine-tuned with a smaller task using a different classifier layer. It is often beneficial to fine-tune at higher resolution than pre-training [<span class="CitationRef"><a epub:type="biblioref" href="#CR189" role="doc-biblioref">189</a></span>]. The models were pre-trained on datasets with up to 300M images.</p><p class="Para" id="Par82">The largest model ViT<sub>Huge</sub> has input patches of size 14 × 14. It was able to outperform an improved and pre-trained ResNet152 [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>] with 152 CNN layers and EfficientNet [<span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>] on ImageNet, and achieved a <span class="EmphasisTypeSmallCaps ">Sota</span> of 90.5% Top-1 accuracy for the classification of images into 1000 object categories [<span class="CitationRef"><a epub:type="biblioref" href="#CR118" role="doc-biblioref">118</a></span>]. Pre-training increases absolute accuracy by 13% on the test set of ImageNet. With 2.5k TPUv3 days, it required only 25% of the computing effort (including pre-training) required for ResNet. It improved <span class="EmphasisTypeSmallCaps ">Sota</span> for another 5 popular image classification benchmarks. The smaller ViT<sub>Large</sub> with input patches of size 16 × 16 also outperformed ResNet152 requiring only 6.8% of ResNet152’s compute effort.</p><p class="Para" id="Par83">When ViT is trained on a moderate dataset like ImageNet, the model achieves a performance below that of ResNet (Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec7"><span class="RefSource">1.​7</span></a></span>) with a comparable parameter count. It seems that CNNs have more appropriate inductive biases, such as translation equivariance and locality, which the transformer must learn through pre-training. Therefore, only pre-trained transformers can outperform CNNs, but this requires a lower computational effort. Cao et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>] present a method how ViTs can be trained with limited data and achieve good results. Chefer et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>] present a new method based on Taylor decomposition methods to visualize the parts of the image that led to a certain image classification.</p><div class="Para" id="Par84">It is instructive to analyze the inner structure of a trained model. It turns out that the trained position embeddings reflect the row and column structure of the input image, and patches in the same row/column have similar embeddings. Based on the attention weights, it can be determined which image parts are considered by a specific attention head. Some attention heads take into account the whole image while others have consistently small attention distances in the lower layers. This could have a similar function as early convolutional layers in CNNs [<span class="CitationRef"><a epub:type="biblioref" href="#CR130" role="doc-biblioref">130</a></span>]. An experimental investigation has shown that transformers are highly robust to severe occlusions [<span class="CitationRef"><a epub:type="biblioref" href="#CR108" role="doc-biblioref">108</a></span>]. In contrast to CNNs, which often detect an object based on texture and less on shape, ViTs are comparable to humans on shape recognition. Figure <span class="InternalRef"><a href="#Fig5">7.5</a></span> shows attention regions for the whole ViT model corresponding to semantically relevant areas.<figure class="Figure" id="Fig5"><div class="MediaObject" id="MO5"><img alt="" aria-describedby="d64e2018" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig5_HTML.png" style="width:33.88em"/><div class="TextObject" id="d64e2018"><p class="Para" id="Par285">3 panel of photos in row 1 represent a clear view of a dog, a flight, and a snake. 3 panel of photos in row 2 represent the same pictures replicated in a darker mode with a focused view on the dog, flight, and snake.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.5</span><p class="SimplePara">The input image is shown in the upper row. The lower row depicts the area of main attention computed by the Vision Transformer model to the input space for classification. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>, p. 8]</p></div></figcaption></figure></div><p class="Para" id="Par85">A number of researchers have investigated the robustness of ViT. In a series of experiments, Mao et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR103" role="doc-biblioref">103</a></span>] found that the ViT tends to employ local features containing textures and noise, and to some extend ignores global context such as shape and structure. In response, they propose to discretize the continuous input features to image tokens using a vector quantizer based on a variational autoencoder (<em class="EmphasisTypeItalic ">VQ-VAE</em><span id="ITerm44"/>) [<span class="CitationRef"><a epub:type="biblioref" href="#CR113" role="doc-biblioref">113</a></span>]. They report accuracy improvements of up to 12% on several ImageNet classification benchmarks. A similar adaptive token generation methods for the ViT was proposed by Ryoo et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR146" role="doc-biblioref">146</a></span>]. <strong class="EmphasisTypeBold ">BEiT</strong><span id="ITerm45"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>] outperforms, the supervised pre-trained ViT using a self-supervised method inspired by BERT (masked image modeling) and based on a VQ-VAE.</p></section>
<section class="Section2 RenderAsSection2" id="Sec15"><h3 class="Heading"><span class="HeadingNumber">7.2.3 </span>Image Generation</h3><p class="Para" id="Par86">There are a number of Foundation Models for various image enhancement tasks. Image <em class="EmphasisTypeItalic ">super-resolution</em><span id="ITerm46"/> converts a low-resolution image to a higher resolution. <strong class="EmphasisTypeBold ">SwinIR</strong><span id="ITerm47"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR96" role="doc-biblioref">96</a></span>] is based on a hierarchical representation starting from small-sized image patches and gradually merging neighboring image patches in deeper layers. For training, the model gets a small-scale image as input, which is preprocessed with a CNN layer. The transformer block contains transformer and CNN layers and is trained to reconstruct the high-resolution image. SwinIR achieves <span class="EmphasisTypeSmallCaps ">Sota</span> on benchmarks for super-resolution, image denoising, and JPEG compression artifact resolution, while having only 12M parameters.</p><div class="Para" id="Par87"><strong class="EmphasisTypeBold ">ColTran</strong><span id="ITerm48"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR88" role="doc-biblioref">88</a></span>] transforms a grayscale image to a fully colored image by using transformers with column and row attention. It first predicts colors by a conditional transformer for a spatially reduced image with only 512 coarse colors. Two subsequent fully parallel transformers upsample the coarse colored low resolution image into a fully colored high resolution image. The model achieves the best FID-score (Sect. <span class="InternalRef"><a href="#Sec18">7.2.6</a></span>) of 19.7 on ImageNet data compared to different alternatives. Examples of colorizations are shown in Fig. <span class="InternalRef"><a href="#Fig6">7.6</a></span>.<figure class="Figure" id="Fig6"><div class="MediaObject" id="MO6"><img alt="" aria-describedby="d64e2102" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig6_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e2102"><p class="Para" id="Par286">4 panel of photos in row 1 represents a person stands in a shop in 4 different color variations and shades. 4 panel of photos in row 2 represents a few people walk around, a person holds ropes in 4 different color variations and shades.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.6</span><p class="SimplePara">Different colorizations of grayscale images (left) by ColTRan [<span class="CitationRef"><a epub:type="biblioref" href="#CR88" role="doc-biblioref">88</a></span>]. Note that semantic constraints, e.g. the color of the skin and the tree leaves, are usually respected. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR88" role="doc-biblioref">88</a></span>, p. 1]</p></div></figcaption></figure></div><p class="Para" id="Par88">The <strong class="EmphasisTypeBold ">Swin Transformer</strong><span id="ITerm49"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR99" role="doc-biblioref">99</a></span>] constructs a hierarchical representation of an image by starting from small-sized image patches and gradually merging neighboring patches in deeper Transformer layers. A linear computational complexity is achieved by computing self-attention locally within non-overlapping windows of size 7 that partition an image. Between consecutive layers the attention windows are shifted such that there is an overlay with the neighboring windows of the prior self-attention layer. The largest model version has 197M parameters and processes images of resolution 384 × 384. On ImageNet classification the model achieves a top-1 accuracy of 87.3%. Also on object detection in images, the Swin Transformer is able to improve the prior best results.</p><p class="Para" id="Par89"><strong class="EmphasisTypeBold ">VQ-GAN</strong><span id="ITerm50"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>] uses a CNN to efficiently learn a codebook of context-rich visual patches, and subsequently learns a model of their global structure. The long-range interactions within these patches require an expressive GPT-2 to model distributions of the visual patches. The dictionary of image patches captures perceptually important local structure according to <em class="EmphasisTypeItalic ">perceptual loss</em><span id="ITerm51"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR194" role="doc-biblioref">194</a></span>]. This loss is optimized with an adversarial training procedure with a patch-based image discriminator that aims to differentiate between real and reconstructed images.</p><div class="Para" id="Par90">A GPT-2 model with 307M parameters is pre-trained to generate the code sequence of encoded images in an image corpus. Each image is partitioned to 16 × 16 patches with a sequence length of 1024. An example image is shown in Fig. <span class="InternalRef"><a href="#Fig7">7.7</a></span>. If the training corpus contains class information <em class="EmphasisTypeItalic ">c</em>, images of specific classes can be generated. Class information can also be restricted to specific image regions. While VQ-VAE yields an FID of about 10 for the reconstruction of ImageNet photos, VQ-GAN achieves a much better value of 1.7.<figure class="Figure" id="Fig7"><div class="MediaObject" id="MO7"><img alt="" aria-describedby="d64e2161" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig7_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e2161"><p class="Para" id="Par287">A photograph displays snow-covered mountains and hills near a lake.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.7</span><p class="SimplePara">VQ-GAN [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>] enables transformers to synthesize high-resolution images with 1280 × 460 pixels. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>, p. 12873]</p></div></figcaption></figure></div><div class="Para" id="Par91"><strong class="EmphasisTypeBold ">StyleSwin</strong><span id="ITerm52"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR191" role="doc-biblioref">191</a></span>] is a further development of VQ-GAN. It uses the <em class="EmphasisTypeItalic ">Swin transformer</em><span id="ITerm53"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR99" role="doc-biblioref">99</a></span>] discussed above. StyleSwin employs a wavelet discriminator in the spectral domain to suppress blocking artifacts. The model with 41M parameters achieves <span class="EmphasisTypeSmallCaps ">Sota</span> quality on multiple established benchmarks. Example images are shown in Fig. <span class="InternalRef"><a href="#Fig8">7.8</a></span> having a coherent global geometry and high-fidelity details. On the CelebA-HQ 1024 benchmark StyleSwin yields an FID of 4.4, which is better than all prior models including StyleGAN2 [<span class="CitationRef"><a epub:type="biblioref" href="#CR82" role="doc-biblioref">82</a></span>]. For the task of generating churches based on the LSUN dataset StyleSwin has an FID-score of 3.1, which is nearly as good as the best scoring adversarial CIPS model [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>] with an FID-score of 2.9.<figure class="Figure" id="Fig8"><div class="MediaObject" id="MO8"><img alt="" aria-describedby="d64e2211" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig8_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e2211"><p class="Para" id="Par288">4 photographs of the faces of different individuals.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.8</span><p class="SimplePara">Images in the 1024 × 1024 resolution generated by StyleSwin [<span class="CitationRef"><a epub:type="biblioref" href="#CR191" role="doc-biblioref">191</a></span>] on FFHQ 1024 × 1024 data (left) and CelebA-HQ 1024 × 1024 data (right). Best seen with zoom. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR191" role="doc-biblioref">191</a></span>, p. 8]</p></div></figcaption></figure></div><p class="Para" id="Par92"><strong class="EmphasisTypeBold ">Data2vec</strong><span id="ITerm54"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>] proposes a new training criterion for self-supervised learning, which can be applied to image, text and speech data. It has two kinds of models: a teacher model, which processes the whole input, and a student model, which processes the input while masking some data.</p><p class="Para" id="Par93">The model employs a standard transformer architecture with media-specific input encoding. Images are encoded by linearly transformed image patches similar to ViT. Speech data is encoded by multi-layer 1-D convolutions. Text data is encoded as subword tokens. Training targets for the student model are constructed from the averaged top <em class="EmphasisTypeItalic ">K</em> encoder blocks of the teacher network, which processes the complete input. This target has to be predicted by the student model, which only receives the masked inputs. Representations of data2vec are continuous and contextualized through the use of self-attention, which makes them richer than a discrete set of tokens used for other approaches.</p><p class="Para" id="Par94">Separate models are trained according to this scheme for speech, images and text. For images a Data2vec model achieves a new <span class="EmphasisTypeSmallCaps ">Sota</span> of 86.2% top-1 accuracy on ImageNet-1k with restricted training set. For speech data, the model reaches a WER of 5.5% on the Librispeech test-other benchmark. For language processing, Data2vec has an average score of 82.9 on GLUE, which is better than RoBERTa. This demonstrates that the model can be effective for multiple modalities. It can be expected that this model will be extended to learn across modalities.</p></section>
<section class="Section2 RenderAsSection2" id="Sec16"><h3 class="Heading"><span class="HeadingNumber">7.2.4 </span>Joint Processing of Text and Images</h3><div class="Para" id="Par95">Once transformers were applied to text and images, joint processing of both modalities became an obvious alternative. Three steps are required for this: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par96">encoding images and texts into embeddings preserving their semantics;</p></li><li><p class="Para" id="Par97">designing powerful architectures to model the interaction between both modalities;</p></li><li><p class="Para" id="Par98">developing effective pre-training tasks.</p></li></ul></div></div><p class="Para" id="Par99">After learning universal vision and language features, these PLMs can be fine-tuned on various downstream vision-language tasks.</p><div class="Para" id="Par100">For pre-training large scale datasets of text-image pairs (<em><strong class="EmphasisTypeBoldItalic ">v</strong></em>, <em><strong class="EmphasisTypeBoldItalic ">u</strong></em>) are required. Each pair consists of a sequence <em><strong class="EmphasisTypeBoldItalic ">v</strong></em><sub>1</sub>, …, <em><strong class="EmphasisTypeBoldItalic ">v</strong></em><sub><em class="EmphasisTypeItalic ">T</em></sub> of text tokens and a sequence <em><strong class="EmphasisTypeBoldItalic ">u</strong></em><sub>1</sub>, …, <em><strong class="EmphasisTypeBoldItalic ">u</strong></em><sub><em class="EmphasisTypeItalic ">R</em></sub> of image features or <em class="EmphasisTypeItalic ">visual tokens</em><span id="ITerm55"/>, e.g. image patches. In this way, we can unify input representation as sequence of embeddings for both modalities. An example dataset is <em class="EmphasisTypeItalic ">COCO captions</em><span id="ITerm56"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>], which contains 328k images of 91 object types of common objects in their natural context together with the corresponding image captions (Fig. <span class="InternalRef"><a href="#Fig9">7.9</a></span>). Other datasets like <em class="EmphasisTypeItalic ">Conceptual Captions</em><span id="ITerm57"/> (CC) [<span class="CitationRef"><a epub:type="biblioref" href="#CR153" role="doc-biblioref">153</a></span>], <em class="EmphasisTypeItalic ">RedCaps</em><span id="ITerm58"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>], and <em class="EmphasisTypeItalic ">Laion</em><span id="ITerm59"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR151" role="doc-biblioref">151</a></span>] contain 3.1M, 12M and 400M images respectively together with captions or descriptive text.<figure class="Figure" id="Fig9"><div class="MediaObject" id="MO9"><img alt="" aria-describedby="d64e2345" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig9_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e2345"><p class="Para" id="Par289">A series of 3 photos. Photo 1, the player at bat hits the baseball while the umpire looks on. Photo 2, a school bus on a parking lot with snow next to a building. Photo 3, 2 horses pull a hay wagon with 2 men on the load.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.9</span><p class="SimplePara">MS-COCO dataset [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>]: images similar to sample images from the dataset. The corresponding captions indicate the level of detail. Image credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4"><span class="RefSource">A.​3</span></a></span></p></div></figcaption></figure></div><p class="Para" id="Par101">Pre-training tasks have to be designed in such a way that the model has to reconstruct parts of the text or image based on the remaining contextual text and image features. For <em class="EmphasisTypeItalic ">Cross-modal MLM</em><span id="ITerm60"/> (Masked Language Modeling) the model has to predict masked tokens or image patches based on the other unmasked text tokens and visual tokens. Here different masking strategies can be used such as whole word masking, masking text spans, or permuting tokens (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec1"><span class="RefSource">3.​1</span></a></span>). <em class="EmphasisTypeItalic ">Masked region prediction</em><span id="ITerm61"/> aims to predict the content of an image region. Objects and their regions are annotated manually or by an auxiliary model. Then the model is required to predict the object (or a distribution over objects) for that region. In this way, the model learns to locate objects in an image.</p><p class="Para" id="Par102"><strong class="EmphasisTypeBold ">CLIP</strong><span id="ITerm62"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR126" role="doc-biblioref">126</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR127" role="doc-biblioref">127</a></span>] is trained to predict a score indicating which image caption corresponds to which image. Given a batch (<em><strong class="EmphasisTypeBoldItalic ">v</strong></em><sub>1</sub>, <em><strong class="EmphasisTypeBoldItalic ">u</strong></em><sub>1</sub>), …, (<em><strong class="EmphasisTypeBoldItalic ">v</strong></em><sub><em class="EmphasisTypeItalic ">n</em></sub>, <em><strong class="EmphasisTypeBoldItalic ">u</strong></em><sub><em class="EmphasisTypeItalic ">n</em></sub>) of tokenized text-image pairs, CLIP has to predict which of the <em class="EmphasisTypeItalic ">n</em> × <em class="EmphasisTypeItalic ">n</em> possible (<em><strong class="EmphasisTypeBoldItalic ">v</strong></em><sub><em class="EmphasisTypeItalic ">i</em></sub>, <em><strong class="EmphasisTypeBoldItalic ">u</strong></em><sub><em class="EmphasisTypeItalic ">j</em></sub>) pairings across the batch actually occurred. By <em class="EmphasisTypeItalic ">contrastive learning</em><span id="ITerm63"/>, CLIP creates a multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the <em class="EmphasisTypeItalic ">n</em> real pairs in the batch while minimizing the cosine similarity of the embeddings of the <em class="EmphasisTypeItalic ">n</em><sup>2</sup> − <em class="EmphasisTypeItalic ">n</em> incorrect pairings. This contrastive training with positive and negative examples has been shown to outperform alternatives. As image encoder a Vision Transformer (ViT) with images patches of size 14 × 14 (Sect. <span class="InternalRef"><a href="#Sec14">7.2.2</a></span>) was employed, which works better than a ResNet [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>] encoder based on CNNs. Text was enclosed by [SOS] and [EOS] tokens and a 12 layer autoregressive GPT model was used to compute embeddings. The embedding of [EOS] in the highest layer was employed as the representation of the whole text.</p><p class="Para" id="Par103">CLIP was trained on 400M image-text pairs of the <em class="EmphasisTypeItalic ">WIT data</em><span id="ITerm64"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR127" role="doc-biblioref">127</a></span>] to associate an image with the best-matching caption. In addition, the prediction of the next token was used as an auxiliary loss term for the GPT model. The model can be used to retrieve a text best fitting to an image, or an image optimally corresponding to a text.</p><p class="Para" id="Par104">The resulting model has acquired a comprehensive knowledge about text and images. With a top-1 classification accuracy of 76.2%, it even surpasses the top-1 classification accuracy of 75.0% of the original ResNet50 on ImageNet zero-shot classification without the need to use any of the 1.28M training examples that ResNet50 was trained on. Hence, CLIP can be considered a ‘zero-shot classifier’. This also holds for 16 out of 27 other image classification benchmarks. When a linear classifier is fitted on top of CLIP’s features, it improves CLIP’s accuracy on the ImageNet test set by almost 10% [<span class="CitationRef"><a epub:type="biblioref" href="#CR126" role="doc-biblioref">126</a></span>]. If the image distribution is changed, e.g. to sketches, CLIP-based classifiers are much more robust. Zero-shot CLIP classifiers improve effective robustness by a large amount, especially with respect to distribution shift. This demonstrates that the inclusion of caption text into vision models enhances performance and robustness.</p><p class="Para" id="Par105"><strong class="EmphasisTypeBold ">BriVL</strong><span id="ITerm65"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>] is a similar model for Chinese language, which uses a larger set of negative examples stored in a queue. It uses a huge training dataset of 650M weakly correlated text-image pairs, where, for instance, an image of a birthday cake has the caption <em class="EmphasisTypeItalic ">“Happy birthday! Make a wish”</em>. It achieves <span class="EmphasisTypeSmallCaps ">Sota</span> results for cross-modal retrieval and visual question answering.</p><p class="Para" id="Par106"><strong class="EmphasisTypeBold ">ALIGN</strong><span id="ITerm66"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>] also uses separate encoders for text and images with a cosine-similarity combination function at the top. As image encoder an EfficientNet CNN is employed. BERT is trained to produce a text embedding for the [CLS] token. Again the similarity is minimized for genuine image-text pairs and maximized for random pairs. ALIGN has 675M parameters and uses a huge training set of 1.8B noisy image pairs. In spite of the noisy data the model achieves a slightly better accuracy (85.5%) on ImageNet top-1 classification than CLIP.</p></section>
<section class="Section2 RenderAsSection2" id="Sec17"><h3 class="Heading"><span class="HeadingNumber">7.2.5 </span>Describing Images by Text</h3><p class="Para" id="Par107">The automatic generation of a natural language description of an image is also called <em class="EmphasisTypeItalic ">image annotation</em><span id="ITerm67"/> or <em class="EmphasisTypeItalic ">image captioning</em><span id="ITerm68"/>. The task is challenging, as it requires visual perception, recognition, and real-world knowledge, as well as the <em class="EmphasisTypeItalic ">grounding</em><span id="ITerm69"/> of language expressions in the image space. <em class="EmphasisTypeItalic ">Symbol grounding</em><span id="ITerm70"/> describes, how words acquire their meaning, e.g. by associating a word with an object in an image. Aside from determining and extracting the important objects and details of an image, the model has to infer the semantic relationship of the objects and the scene (Fig. <span class="InternalRef"><a href="#Fig9">7.9</a></span>).</p><div class="Para" id="Par108">Current top models for describing images work in two stages: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par109">an <em class="EmphasisTypeItalic ">object detection</em><span id="ITerm71"/> model is pre-trained to encode an image and the visual objects in the image to feature vectors,</p></li><li><p class="Para" id="Par110">a crossmodal PLM is pre-trained to associate text and visual features and generate a caption for an image.</p></li></ul></div></div><p class="Para" id="Par111">Similar to language translation, various metrics are used to evaluate the generated texts, e.g. <span class="EmphasisTypeSmallCaps ">Bleu</span> or <span class="EmphasisTypeSmallCaps ">Rouge</span> (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec23"><span class="RefSource">2.​3.​3</span></a></span>). Surveys of image captioning techniques are provided by Hossain et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>], Oluwasammi et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR112" role="doc-biblioref">112</a></span>], and Stefanini et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR159" role="doc-biblioref">159</a></span>].</p><p class="Para" id="Par112"><strong class="EmphasisTypeBold ">VilBERT</strong><span id="ITerm72"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR100" role="doc-biblioref">100</a></span>] aims to learn representations that can jointly model images and natural language. It extracts bounding boxes and their visual features using a pre-trained object detection network (Faster R-CNN [<span class="CitationRef"><a epub:type="biblioref" href="#CR137" role="doc-biblioref">137</a></span>]). These image region features as well as the text are input to two separate transformer encoders (two-stream architecture). Subsequently, transformer layers with cross-attention in both directions are applied to learn cross-modal relationships. VilBERT was pre-trained on Conceptual Captions data.</p><p class="Para" id="Par113">The model was fine-tuned and evaluated on different tasks. <em class="EmphasisTypeItalic ">Visual question answering</em><span id="ITerm73"/> (<em class="EmphasisTypeItalic ">VQA</em>) answers natural language questions about images. VQA is treated as a multi-label classification task with 3129 possible answers. Final embeddings of the text and image parts are fed into a classifier to estimate class probabilities. On the COCO test set VilBERT achieved a new <span class="EmphasisTypeSmallCaps ">Sota</span> with an accuracy of 70.9%. <em class="EmphasisTypeItalic ">Caption-based image retrieval</em><span id="ITerm74"/> is the task of identifying an image from a pool given a caption describing its content. The model was fine-tuned on a Flickr dataset and had a recall@1 of 58.2%, thus establishing a new <span class="EmphasisTypeSmallCaps ">Sota</span>.</p><p class="Para" id="Par114"><strong class="EmphasisTypeBold ">OSCAR</strong><span id="ITerm75"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR95" role="doc-biblioref">95</a></span>] has the strategy to connect the relevant objects in the image with the corresponding phrases in the caption text. The authors use self-attention to learn these alignments, which can be significantly improved by additional object tags detected in images as reference points. Oscar represents each input image-text pair as a Word-Tag-Image triple (<em class="EmphasisTypeItalic ">w</em>;<em class="EmphasisTypeItalic ">q</em>;<em class="EmphasisTypeItalic ">v</em>), where <em class="EmphasisTypeItalic ">w</em> is the sequence of words of the caption text, <em class="EmphasisTypeItalic ">q</em> contains the words of the textual object tags detected in the image, and <em class="EmphasisTypeItalic ">v</em> is the set of the corresponding region images. A CNN model (Faster R-CNN [<span class="CitationRef"><a epub:type="biblioref" href="#CR137" role="doc-biblioref">137</a></span>]) is used to discover the objects in <em class="EmphasisTypeItalic ">q</em> as well as to the corresponding regions <em class="EmphasisTypeItalic ">v</em>. For pre-training the transformer encoder, part of the tokens in (<em class="EmphasisTypeItalic ">w</em>;<em class="EmphasisTypeItalic ">q</em>;<em class="EmphasisTypeItalic ">v</em>) are masked, and the model learns to predict the masked tokens. In addition, sometimes the <em class="EmphasisTypeItalic ">q</em>-terms are changed randomly. The model has the additional task to identify these modifications. A small and a large model version are trained with a sequence length of 768 and 1024 using a public corpus of 6.5 million text-image pairs. The model is fine-tuned to generate the caption according to the sequence-to-sequence objective. The model achieves a new <span class="EmphasisTypeSmallCaps ">Sota</span> on COCO-captions with respect to <span class="EmphasisTypeSmallCaps ">Bleu</span>-4 (41.7%), <span class="EmphasisTypeSmallCaps ">Meteor</span> and <span class="EmphasisTypeSmallCaps ">Rouge-L</span> as well as for several other captioning benchmarks.</p><div class="Para" id="Par115"><strong class="EmphasisTypeBold ">VinVL</strong><span id="ITerm76"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR193" role="doc-biblioref">193</a></span>] is pre-trained on three text-image corpora with 2.5M images, and can generate visual features with a richer collection of visual objects and concepts. VinVL pre-trains a large-scale object-attribute detection model based on the CNN-based ResNeXt-152 C4 architecture [<span class="CitationRef"><a epub:type="biblioref" href="#CR179" role="doc-biblioref">179</a></span>]. The model does not describe objects by a single noun, but by a large number of attributes and details, which enhances the performance in joint image-language tasks (Fig. <span class="InternalRef"><a href="#Fig10">7.10</a></span>). The approach is combined with OSCAR and yields an improved <span class="EmphasisTypeSmallCaps ">Sota</span> on image captioning. <strong class="EmphasisTypeBold ">VIVO</strong><span id="ITerm77"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>] is a similar transformer model trained to label image regions with 6.4k different object tags. VIVO is fine-tuned with COCO image-caption pairs and learns to generate caption sentences, also using object tags not appearing in the caption data. This is possible as VIVO can exploit large amounts of paired image-tag data to learn rich descriptions for images. On the test set VIVO generates better captions than humans according to the <em class="EmphasisTypeItalic ">CIDEr metric</em><span id="ITerm78"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR69" role="doc-biblioref">69</a></span>], which counts the common words weighted by tf-idf in the generated and the reference text [<span class="CitationRef"><a epub:type="biblioref" href="#CR169" role="doc-biblioref">169</a></span>].<figure class="Figure" id="Fig10"><div class="MediaObject" id="MO10"><img alt="" aria-describedby="d64e2718" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig10_HTML.png" style="width:33.98em"/><div class="TextObject" id="d64e2718"><p class="Para" id="Par290">2 similar photos. Photo 1, the detected parts are labeled as a man, a wetsuit, a human arm, a human leg, a surfboard, and a surfboard. Photo 2, additional parts such as colors, hands, water splashes, drops, a rolling wave, and emotions are detected and labeled.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.10</span><p class="SimplePara">Standard bounding-box object descriptions (left) and detailed annotations, which can be generated by VinVL (right) and contain visual concepts and attribute information [<span class="CitationRef"><a epub:type="biblioref" href="#CR193" role="doc-biblioref">193</a></span>]. Image credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4"><span class="RefSource">A.​3</span></a></span></p></div></figcaption></figure></div><div class="Para" id="Par116"><strong class="EmphasisTypeBold ">SimVLM</strong><span id="ITerm79"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR171" role="doc-biblioref">171</a></span>] is a transformer encoder-decoder, which uses the first three blocks of ResNet to extract contextualized patches from images, and associates the image tokens with text tokens. The decoder then predicts the continuation of the textual sequence as shown in Fig. <span class="InternalRef"><a href="#Fig11">7.11</a></span>. It is trained on 1.8B noisy image text pairs and 800GB text documents. SimVLM achieves a new <span class="EmphasisTypeSmallCaps ">Sota</span> for visual question answering on the <em class="EmphasisTypeItalic ">VQA v2 benchmark</em><span id="ITerm80"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>] with 80.3% accuracy. In addition, it reaches <span class="EmphasisTypeSmallCaps ">Sota</span> for visual entailment, visual reasoning, and image captioning on COCO captions with respect to Meteor (33.7).<figure class="Figure" id="Fig11"><div class="MediaObject" id="MO11"><img alt="" aria-describedby="d64e2765" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig11_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e2765"><p class="Para" id="Par291">4 photos. 1, a group of people sitting at a table with drinks in a dark restaurant. Photo 2 has a question what is the profession of this person? surgeon. 3, this dish food is a kind of American breakfast dish. Photo 4, has a question where can you observe this animal? giant panda is native in China.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.11</span><p class="SimplePara">The SimVLM encoder-decoder model receives an image (top) and a text (middle) as input and produces an output text (bottom) [<span class="CitationRef"><a epub:type="biblioref" href="#CR171" role="doc-biblioref">171</a></span>]. The image patches are encoded by the first layers of ResNet. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR171" role="doc-biblioref">171</a></span>, p. 3]</p></div></figcaption></figure></div><p class="Para" id="Par117"><strong class="EmphasisTypeBold ">Frozen</strong><span id="ITerm81"/> is a Foundation Model trained to associate text with images. It can be instructed by few-shot learning to answer question on an image [<span class="CitationRef"><a epub:type="biblioref" href="#CR166" role="doc-biblioref">166</a></span>]. The language model is a pre-trained autoregressive model with 7B parameters trained on the C4 dataset with 807GB text [<span class="CitationRef"><a epub:type="biblioref" href="#CR129" role="doc-biblioref">129</a></span>]. The vision encoder is based on NF-ResNet-50 [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>] and provides an embedding vector characterizing the image. During training the image embedding is used as a prefix before the token embeddings of the generated text. Using the <em class="EmphasisTypeItalic ">conceptual captions</em><span id="ITerm82"/> dataset the vision encoder is trained while freezing the language model. The training target is to generate a caption for the image.</p><p class="Para" id="Par118">During inference, several examples consisting of an image embedding and token embeddings are fed into the language model, which generates an answer. An example is to caption a microscope with <em class="EmphasisTypeItalic ">“This was invented by Zacharias Janssen.”</em>, and a light bulb with <em class="EmphasisTypeItalic ">“This was invented by Thomas Edison.”</em>. After five seeds and the input of an airplane together with <em class="EmphasisTypeItalic ">“This was invented by”</em> the model generates the output <em class="EmphasisTypeItalic ">“the Wright brothers”</em>. In this way, different categorizations of images can be defined on the fly. These samples demonstrate the ability to generate open-ended outputs that adapt to both images and text, and to make use of facts that it has learned during language-only pre-training. The model is a proof-of-concept and shows a way to generate few-shot models for image-text tasks.</p></section>
<section class="Section2 RenderAsSection2" id="Sec18"><h3 class="Heading"><span class="HeadingNumber">7.2.6 </span>Generating Images from Text</h3><div class="Para" id="Par119">By training on text-image pairs, transformers can acquire the knowledge to generate images corresponding to text descriptions. By successively producing the next token with a language model, it is possible to predict visual tokens, which then can be synthesized to images. However, there are other image generation techniques. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par120"><em class="EmphasisTypeItalic ">Variational Auto-Encoders</em><span id="ITerm83"/> (<em class="EmphasisTypeItalic ">VAE</em>) compress an input image to a small latent representation and reconstruct the image as good as possible. An additional loss term ensures that the distribution of latent representations follows a Gaussian [<span class="CitationRef"><a epub:type="biblioref" href="#CR79" role="doc-biblioref">79</a></span>].</p></li><li><p class="Para" id="Par121"><em class="EmphasisTypeItalic ">Generative Adversarial Networks</em><span id="ITerm84"/> (<em class="EmphasisTypeItalic ">GAN</em>) use a generator to transform a noise vector <em><strong class="EmphasisTypeBoldItalic ">s</strong></em> to an image <span class="InlineEquation" id="IEq3"><img alt="$$\tilde {{\boldsymbol {x}}}=G(\boldsymbol {s})$$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq3.png" style="width:4.38em"/></span>. Then a discriminator <em class="EmphasisTypeItalic ">D</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>) has the task to classify its input as synthetic image <span class="InlineEquation" id="IEq4"><img alt="$$\tilde {{\boldsymbol {x}}}$$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq4.png" style="width:0.94em"/></span> or real image <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>]. Both networks are trained alternately with an adversarial loss.</p></li></ul></div></div><p class="Para" id="Par122">Lee et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>] give a survey of techniques for text driven image generation and manipulation.</p><p class="Para" id="Par123">There are a number of approaches to measure the quality of generated images. The <em class="EmphasisTypeItalic ">Inception Score</em><span id="ITerm85"/> (<em class="EmphasisTypeItalic ">IS</em>) [<span class="CitationRef"><a epub:type="biblioref" href="#CR150" role="doc-biblioref">150</a></span>] applies a CNN-based <em class="EmphasisTypeItalic ">Inception model</em><span id="ITerm86"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR162" role="doc-biblioref">162</a></span>] trained on ImageNet to every generated image to get a conditional class label distribution, which should concentrate on few classes, i.e. have low entropy. In addition, many different classes should be generated for the test data, which is captured by the defined IS measure. The <em class="EmphasisTypeItalic ">Fréchet Inception Distance</em><span id="ITerm87"/> (<em class="EmphasisTypeItalic ">FID</em>) [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>] is an improved measure using the Fréchet distance between ImageNet classifier distributions, which measures the similarity of the distributions taking into account the location and ordering of the points along the graph. <em class="EmphasisTypeItalic ">CLIP Similarity Score</em><span id="ITerm88"/> (CLIPSIM) [<span class="CitationRef"><a epub:type="biblioref" href="#CR72" role="doc-biblioref">72</a></span>] is based on the CLIP model (Sect. <span class="InternalRef"><a href="#Sec16">7.2.4</a></span>). It generates image and text embeddings with CLIP and calculates their cosine similarity.</p><p class="Para" id="Par124"><strong class="EmphasisTypeBold ">DALL-E</strong><span id="ITerm89"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR133" role="doc-biblioref">133</a></span>] uses a <em class="EmphasisTypeItalic ">GPT-3</em><span id="ITerm90"/> autoregressive language model with 12B parameters to generate a new image from a textual description. The caption text of the image is BPE-encoded into 256 tokens. Then each 256 × 256 image is compressed to a 32 × 32 grid of image tokens using a discrete variational autoencoder. Each image token represents its 8 × 8 pixels by 8192 possible values. The caption tokens are concatenated with the 32 × 32 = 1024 image tokens forming the input sequence of GPT-3.</p><p class="Para" id="Par125">In the first stage the image tokens are trained yielding continuous image values. Then the discrete image tokens are obtained by training with a <em class="EmphasisTypeItalic ">Gumbel-softmax relaxation</em><span id="ITerm91"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR75" role="doc-biblioref">75</a></span>] (Sect. <span class="InternalRef"><a href="#Sec4">7.1.3</a></span>). In the second stage a <em class="EmphasisTypeItalic ">Sparse Transformer</em><span id="ITerm92"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>] with 64 self-attention layers and 12B parameters is trained to sequentially generate the joint input sequence. For the image tokens, special attention masks are used: row, column, or convolutional attention masks. The model was trained on 250M text-image pairs from the Internet.</p><div class="Para" id="Par126">For image generation, the authors rerank the samples drawn from the transformer using a pre-trained contrastive model, which assigns a score based on how well the image matches the caption. Figure <span class="InternalRef"><a href="#Fig12">7.12</a></span> shows different images sampled from the algorithm. In a comparison to the prior model DF-GAN [<span class="CitationRef"><a epub:type="biblioref" href="#CR165" role="doc-biblioref">165</a></span>], the images generated by DALL-E were chosen as most realistic and more matching the caption in more than 90% of the time. Similarly, the images generated by X-LXMERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR28" role="doc-biblioref">28</a></span>] look inferior.<figure class="Figure" id="Fig12"><div class="MediaObject" id="MO12"><img alt="" aria-describedby="d64e3009" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig12_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e3009"><p class="Para" id="Par292">2 sets of 4 panels of photo display the best of 1 and the best of 512. The photos are a group of urinals near the trees, a woman and a man stand next to a bush bench, a man rides a bike down a street past a young man, and a truck stopped at an intersection where construction barriers are up.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.12</span><p class="SimplePara">According to a natural language caption (top) a number of images are generated by DALL-E [<span class="CitationRef"><a epub:type="biblioref" href="#CR133" role="doc-biblioref">133</a></span>]. The middle row shows images generated by DALL-E corresponding to the caption. The lower row shows the best image from a sample of 512 automatically selected by a quality score. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR133" role="doc-biblioref">133</a></span>, p. 6]</p></div></figcaption></figure></div><p class="Para" id="Par127"><strong class="EmphasisTypeBold ">GauGAN2</strong><span id="ITerm93"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR122" role="doc-biblioref">122</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR149" role="doc-biblioref">149</a></span>] combines segmentation mapping, inpainting and text-to-image generation in a single model. It is one of the first semantic image synthesis models that can produce photorealistic outputs for diverse scenes including indoor, outdoor, landscape, and street scenes. The recent version also can generate images according to text input. The model behind GauGAN2 was trained on 10 million high-quality landscape images. Details of the model are not known.</p><p class="Para" id="Par128"><strong class="EmphasisTypeBold ">XMC-GAN</strong><span id="ITerm94"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR192" role="doc-biblioref">192</a></span>] is a GAN-based text-to-image generation model containing a generator for synthesizing images, and a discriminator that is trained to discriminate real and generated images. It maximizes the mutual information between the corresponding pairs: (1) images (real or generated) with a sentence describing the scene; (2) a generated image and a real image with the same description; and (3) regions of an image (real or generated) and words or phrases associated with them. The goal is for the matching pairs (both text-to-image and real image-to-generated image) to have high similarity scores and for non-matching pairs to have low scores.</p><p class="Para" id="Par129">For the input text the model computes a global sentence embedding <em class="EmphasisTypeItalic ">emb</em><sub><em class="EmphasisTypeItalic ">s</em></sub> and the word embeddings <em class="EmphasisTypeItalic ">emb</em><sub><em class="EmphasisTypeItalic ">w</em></sub> from a pre-trained BERT module. <em class="EmphasisTypeItalic ">emb</em><sub><em class="EmphasisTypeItalic ">s</em></sub> and random noise <em class="EmphasisTypeItalic ">z</em> from a standard Gaussian distribution are concatenated to form the <em class="EmphasisTypeItalic ">global condition</em>, which is passed through several up-sampling blocks to generate a 16 × 16 feature map. The global condition is also used as the condition to calculate scale parameter and shift parameter in conditional batch normalization layers. The word embeddings <em class="EmphasisTypeItalic ">emb</em><sub><em class="EmphasisTypeItalic ">w</em></sub> are input for an “attentional self-modulation layer” to generate fine-grained image regions. On MS-COCO, XMC-GAN improves the <span class="EmphasisTypeSmallCaps ">Sota</span> FID-score (Sect. <span class="InternalRef"><a href="#Sec18">7.2.6</a></span>) from 24.7 to 9.3, and is significantly preferred by human evaluators. Similarly, human raters prefer the image quality of XMC-GAN generated images 77% of the time, and 74% prefer its image-text alignment compared to three other <span class="EmphasisTypeSmallCaps ">Sota</span> approaches (CP-GAN, SD-GAN, and OP-GAN).</p><p class="Para" id="Par130"><strong class="EmphasisTypeBold ">Cogview</strong><span id="ITerm95"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>] employs a <em class="EmphasisTypeItalic ">Vector Quantized Variational AutoEncoder</em><span id="ITerm96"/> (<em class="EmphasisTypeItalic ">VQ-VAE</em>). In the first stage, a discrete autoencoder is used to transform the image into a discrete sequence of tokens. In the second stage a GPT model learns to generate image tokens based on a prompt of SentencePiece text tokens. To generate image tokens, an encoder maps an image <span class="InlineEquation" id="IEq5"><img alt="$${\boldsymbol {x}}\in \mathbb {R}^{H\times W \times 3}$$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq5.png" style="width:5.74em"/></span> to <em class="EmphasisTypeItalic ">h</em> × <em class="EmphasisTypeItalic ">w</em> image patches, which are quantized to a nearby embedding in a learnable set {<em><strong class="EmphasisTypeBoldItalic ">u</strong></em><sub>1</sub>, …, <em><strong class="EmphasisTypeBoldItalic ">u</strong></em><sub><em class="EmphasisTypeItalic ">k</em></sub>} of embedding vectors <span class="InlineEquation" id="IEq6"><img alt="$$\boldsymbol {u}_i\in \mathbb {R}^d$$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq6.png" style="width:3.57em"/></span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR113" role="doc-biblioref">113</a></span>]. The decoder maps the embeddings back to the image, and the embeddings are selected to minimize the difference between output and input image.</p><p class="Para" id="Par131">The GPT-model of CogView has 48 layers with a hidden size of 2560, 40 attention heads and 4B parameters. The input to the model is of the form <em class="EmphasisTypeItalic ">“[ROI1]</em> &lt; <em class="EmphasisTypeItalic ">text tokens</em>&gt;  <em class="EmphasisTypeItalic ">[BASE] [BOI1]</em> &lt; <em class="EmphasisTypeItalic ">image tokens</em>&gt;  <em class="EmphasisTypeItalic ">[EOI1]”</em> and contains special tokens. The pre-training task is to predict tokens from left to right for 30M text-image pairs in English and Chinese. A sparse attention pattern similar to BigBird (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec8"><span class="RefSource">3.​2.​1</span></a></span>) is used.</p><div class="Para" id="Par132">As shown in Fig. <span class="InternalRef"><a href="#Fig13">7.13</a></span>, CogView has a similar performance in image generation as DALL-E. It achieves the <span class="EmphasisTypeSmallCaps ">Sota</span> FID on the blurred MS COCO dataset, outperforming previous GAN-based models and DALL-E, although DALL-E has three times more parameters. When evaluated by humans, CogView was able to beat GAN-based models by a large margin. However, generation of images with CogView is rather slow, because each image is generated token-by-token. In addition, the quantization leads to some blurriness in the images.<figure class="Figure" id="Fig13"><div class="MediaObject" id="MO13"><img alt="" aria-describedby="d64e3216" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig13_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e3216"><p class="Para" id="Par293">4 photos. 1 a beautiful young blonde woman talking on a phone. 2, a Big Ben clock towering over the city of London. 3, Chinese traditional drawing of the Statue of Liberty. 4, an oil painting of a lion.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.13</span><p class="SimplePara">Images generated by CogView [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>] controlled by the text input (top). The image style can be influenced by the input text. The best of a sample of 60 images is selected. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>, p. 1]</p></div></figcaption></figure></div><p class="Para" id="Par133"><strong class="EmphasisTypeBold ">LAFITE</strong><span id="ITerm97"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR200" role="doc-biblioref">200</a></span>] is a model for generating images from text. Image generation is based on <em class="EmphasisTypeItalic ">StyleGAN2</em><span id="ITerm98"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR82" role="doc-biblioref">82</a></span>], which creates various image attributes by modulating the weights of the convolution kernels [<span class="CitationRef"><a epub:type="biblioref" href="#CR177" role="doc-biblioref">177</a></span>]. LAFITE generates these modulating signals based on language input. It relies on the multimodal semantic space of the pre-trained CLIP model (Sect. <span class="InternalRef"><a href="#Sec16">7.2.4</a></span>) to produce an image embedding <em class="EmphasisTypeItalic ">emb</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>) from a text <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>, and therefore does not need extra text data. This image embedding is inserted into the image generation model similar to StyleGAN2 by a GAN architecture. On the MS-COCO benchmark, LAFITE achieves a zero-shot FID value of 26.9, which is better than the values of DALL-E (27.5) and CogView (27.1). When fine-tuned on MS-COCO, LAFITE has a FID-score of 8.1, which is better than that of XMC-GAN (9.3) and other GAN models. Note that LAFITE only has 75M trainable parameters.</p></section>
<section class="Section2 RenderAsSection2" id="Sec19"><h3 class="Heading"><span class="HeadingNumber">7.2.7 </span>Diffusion Models Restore an Image Destructed by Noise</h3><p class="Para" id="Par134"><strong class="EmphasisTypeBold ">GLIDE</strong><span id="ITerm99"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>] is an image generation technique based on a <em class="EmphasisTypeItalic ">diffusion model</em><span id="ITerm100"/>. A diffusion model describes the process of systematically and slowly destroying structure in a data distribution through an iterative forward <em class="EmphasisTypeItalic ">diffusion process</em><span id="ITerm101"/>, e.g. the addition of noise [<span class="CitationRef"><a epub:type="biblioref" href="#CR157" role="doc-biblioref">157</a></span>]. To the data <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[0]</sup>, e.g. a matrix of pixel values, we can apply Gaussian diffusion distribution <em class="EmphasisTypeItalic ">q</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>]</sup>|<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>−1]</sup>), where a Gaussian with expectation <em><strong class="EmphasisTypeBoldItalic ">0</strong></em> and covariance <em class="EmphasisTypeItalic ">β</em><em><strong class="EmphasisTypeBoldItalic ">I</strong></em> is added. This yields a series <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[0]</sup>, …, <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">T</em>]</sup> where the final distribution <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">T</em>]</sup> approximately is a Gaussian distribution with identity covariance (similar results hold for the binomial distribution).</p><div class="Para" id="Par135">Now the reversal of the diffusion process can be defined, i.e. the generative distribution with <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>−1]</sup> ∼ <em class="EmphasisTypeItalic ">p</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>−1]</sup>|<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>]</sup>). It has been shown by Feller [<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>] that for small step size <em class="EmphasisTypeItalic ">β</em> the conditional distribution <em class="EmphasisTypeItalic ">p</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>−1]</sup>|<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>]</sup>) will approximately be a Gaussian distribution. Hence, the chain <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">T</em>]</sup>, …, <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[0]</sup> can be generated by a Gaussian distribution <div class="Equation NumberedEquation" id="Equ3"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} {\boldsymbol{x}}^{[t-1]}\sim N(\boldsymbol{\mu}_{\boldsymbol{w}}({\boldsymbol{x}}^{[t]});\boldsymbol{S}_{\boldsymbol{w}}({\boldsymbol{x}}^{[t]})) \quad \text{and} \quad  {\boldsymbol{x}}^{[T]}\sim N(\boldsymbol{0};\boldsymbol{I})) {}. \end{aligned} $$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_Equ3.png" style="width:24.2em"/></div></div> <div class="EquationNumber">(7.3)</div></div></div></div><p class="Para" id="Par136">This Gaussian distribution is completely defined by the mean and covariance of <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>]</sup>.</p><div class="Para" id="Par137">For the training, noisy samples <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>]</sup> are generated by <em class="EmphasisTypeItalic ">q</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>]</sup>|<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>−1]</sup>) starting with the observed <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[0]</sup>. From this the inverse <em class="EmphasisTypeItalic ">p</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>−1]</sup>|<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>]</sup>) may be reconstructed by optimizing the <em class="EmphasisTypeItalic ">variational lower bound</em><span id="ITerm102"/> on negative log likelihood [<span class="CitationRef"><a epub:type="biblioref" href="#CR65" role="doc-biblioref">65</a></span>]. With the trained model one can start with a sample <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">T</em>]</sup> ∼ <em class="EmphasisTypeItalic ">N</em>(<em><strong class="EmphasisTypeBoldItalic ">0</strong></em>, <em><strong class="EmphasisTypeBoldItalic ">I</strong></em>) and gradually reduce noise in a sequence of steps <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">T</em>−1]</sup>, …, <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[0]</sup>, where <div class="Equation NumberedEquation" id="Equ4"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} {\boldsymbol{x}}^{[t-1]}\sim p({\boldsymbol{x}}^{[t-1]}|{\boldsymbol{x}}^{[t]}) \approx N(\boldsymbol{\mu}_{\boldsymbol{w}}({\boldsymbol{x}}^{[t]});\boldsymbol{S}_{\boldsymbol{w}}({\boldsymbol{x}}^{[t]})) . \end{aligned} $$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_Equ4.png" style="width:20.44em"/></div></div> <div class="EquationNumber">(7.4)</div></div></div></div><p class="Para" id="Par138">The distributions <em class="EmphasisTypeItalic ">p</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>−1]</sup>|<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>]</sup>) may be estimated conditional to image classes [<span class="CitationRef"><a epub:type="biblioref" href="#CR37" role="doc-biblioref">37</a></span>]. Instead of a finite number of image classes one may even use a caption text as condition. The text is first encoded into a sequence of <em class="EmphasisTypeItalic ">k</em> tokens and fed into a Transformer model. The Transformer outputs a class embedding as well as <em class="EmphasisTypeItalic ">k</em> token embeddings, which are used as additional model inputs. Here a normal noise term <em class="EmphasisTypeItalic ">𝜖</em><sub><em><strong class="EmphasisTypeBoldItalic ">w</strong></em></sub>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>]</sup>|∅) for reconstruction is estimated and in addition conditional to the caption <em class="EmphasisTypeItalic ">c</em> a noise term <em class="EmphasisTypeItalic ">𝜖</em><sub><em><strong class="EmphasisTypeBoldItalic ">w</strong></em></sub>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">t</em>]</sup>|<em class="EmphasisTypeItalic ">c</em>). During the <em class="EmphasisTypeItalic ">classifier-free reconstruction</em><span id="ITerm103"/> both terms are mixed.</p><p class="Para" id="Par139">The diffusion model is approximated by a <em class="EmphasisTypeItalic ">U-Net</em><span id="ITerm104"/> model [<span class="CitationRef"><a epub:type="biblioref" href="#CR144" role="doc-biblioref">144</a></span>] with 2.3B parameters, performing a downsampling of the 64 pixel image to a smaller resolution with many features and a subsequent upsampling. An additional 1.5B parameter model is used for upsampling to a 256 × 256 resolution. The caption text is processed by a transformer model with 1.2B parameters and the final token embedding is used in place of a class embedding.</p><div class="Para" id="Par140">In tests, GLIDE produced high-quality images with realistic reflections, textures, and shadows. The model can also combine multiple concepts (for example, dragon, psychedelic, and hamster) and attach attributes like colors to these concepts. On the MS-COCO benchmark with 256 × 256 images DALL-E achieves a FID-value of 28, while LAFITE gets 26.9 and GLIDE 12.2. Also in human evaluations, the results of GLIDE are clearly preferred. This is remarkable as GLIDE has far less parameters than DALL-E. Figure <span class="InternalRef"><a href="#Fig14">7.14</a></span> shows some images generated by GLIDE. GLIDE can also be used for restoring a masked image patch according to a textual prompt, e.g. <em class="EmphasisTypeItalic ">“tie with black and yellow stripes”</em>. In most cases, GLIDE produces better results than competitor models and the corresponding image patch is restored with realistic lighting, shadows and textures. Finally, GLIDE can add shadows and reflections to images and transform simple line sketches into photorealistic images.<figure class="Figure" id="Fig14"><div class="MediaObject" id="MO14"><img alt="" aria-describedby="d64e3854" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig14_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e3854"><p class="Para" id="Par294">4 photos. 1, a group of elephants walking in muddy water. 2, a group of skiers preparing to ski. 3, a hedgehog using a calculator. 4, a high-quality oil painting of a psychedelic hamster dragon.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.14</span><p class="SimplePara">Images generated by GLIDE [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>] according to the captions in the lower row. The best of a sample of 60 is shown. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>, p. 7]</p></div></figcaption></figure></div><div class="Para" id="Par141"><strong class="EmphasisTypeBold ">DALL-E 2</strong><span id="ITerm105"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR132" role="doc-biblioref">132</a></span>] is an improved version of DALL-E that can create more realistic art and images from a descriptive sentence in natural language. It works in two steps (Fig. <span class="InternalRef"><a href="#Fig15">7.15</a></span>): first a CLIP (Sect. <span class="InternalRef"><a href="#Sec16">7.2.4</a></span>) image embedding <em class="EmphasisTypeItalic ">z</em><sub><em class="EmphasisTypeItalic ">i</em></sub> based on a text description <em class="EmphasisTypeItalic ">y</em> is generated according to a prior <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">z</em><sub><em class="EmphasisTypeItalic ">i</em></sub>|<em class="EmphasisTypeItalic ">y</em>). Then a diffusion-based decoder generates an image <em class="EmphasisTypeItalic ">x</em> conditioned on an image embedding <em class="EmphasisTypeItalic ">z</em><sub><em class="EmphasisTypeItalic ">i</em></sub>. The decoder <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">x</em>|<em class="EmphasisTypeItalic ">z</em><sub><em class="EmphasisTypeItalic ">i</em></sub>, <em class="EmphasisTypeItalic ">y</em>) inverts the CLIP image encoder, is non-deterministic, and can produce multiple images corresponding to a given image embedding. The CLIP model is frozen during training of the prior and decoder. The dimensionality of the image embeddings <em class="EmphasisTypeItalic ">z</em><sub><em class="EmphasisTypeItalic ">i</em></sub> is reduced to 319 from 1024 by principal component analysis while preserving nearly all information. Each of the 319 dimensions is quantized into 1024 discrete buckets. For the encoder, experiments are performed with both autoregressive and diffusion models for the prior. It turns out that diffusion models are computationally more efficient and produce higher-quality samples. Examples are shown in Fig. <span class="InternalRef"><a href="#Fig16">7.16</a></span>.<figure class="Figure" id="Fig15"><div class="MediaObject" id="MO15"><img alt="" aria-describedby="d64e3945" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig15_HTML.png" style="width:33.78em"/><div class="TextObject" id="d64e3945"><p class="Para" id="Par295">A block diagram illustrates a corgi playing a flame-throwing trumpet, along with a text encoder, C L I P objective, prior, image encoder, decoder. Encoder and decoder has a photo of a dog holding a trumpet in its mouth in 2 opposite directions.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.15</span><p class="SimplePara">A high-level overview of DALL-E 2 [<span class="CitationRef"><a epub:type="biblioref" href="#CR132" role="doc-biblioref">132</a></span>]. Above the dotted line the CLIP training process is shown minimizing the difference between the embeddings for an image and the corresponding text. Below the dotted line, the text-to-image generation process is illustrated: a CLIP text embedding is first fed to an autoregressive transformer (higher box) or diffusion prior (lower box) to produce an image embedding. This embedding is used as input to the diffusion decoder which produces a final image. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR132" role="doc-biblioref">132</a></span>, p. 3]</p></div></figcaption></figure><figure class="Figure" id="Fig16"><div class="MediaObject" id="MO16"><img alt="" aria-describedby="d64e3969" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig16_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e3969"><p class="Para" id="Par296">2 panels of 4 photos in 2 rows. Row 1 illustrates a person named Salvador Dali with robotic sketches on different parts of their face. Row illustrates a teddy bear on a skateboard in different positions on the skateboard.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.16</span><p class="SimplePara">Random samples from DALL-E 2 [<span class="CitationRef"><a epub:type="biblioref" href="#CR132" role="doc-biblioref">132</a></span>] for the prompt <em class="EmphasisTypeItalic ">“Vibrant portrait painting of Salvador Dali with a robotic half face”</em> (upper row), and <em class="EmphasisTypeItalic ">“A teddybear on a skateboard in Times Square”</em>. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR132" role="doc-biblioref">132</a></span>, p. 25,27]</p></div></figcaption></figure></div><p class="Para" id="Par142">The decoder is conditioned on image representations and can produce variations of an image that preserve both its semantics and style, while varying the nonessential details that are missing from the image embeddings. CLIP’s shared embedding space allows for language-guided image manipulations and modifications in a zero-shot manner. For example two images <em class="EmphasisTypeItalic ">x</em><sub>1</sub> and <em class="EmphasisTypeItalic ">x</em><sub>2</sub> can be blended, interpolating all of the concepts in CLIP’s embedding space that occur between them. With respect to MSCOCO it turns out that DALL-E 2 has a better zero-shot FID of 10.4 than GLIDE (12.2). Human comparisons show that DALL-E 2 and GLIDE are similar in terms of photorealism and caption similarity, while DALL-E 2 produces images with greater diversity. DALL-E 2 struggles more than GLIDE with a prompt that requires it to connect two separate objects (cubes) to two separate attributes (colors). A public access to DALL-E is now available for users to create images [<span class="CitationRef"><a epub:type="biblioref" href="#CR115" role="doc-biblioref">115</a></span>].</p><div class="Para" id="Par143"><strong class="EmphasisTypeBold ">Imagen</strong><span id="ITerm106"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR148" role="doc-biblioref">148</a></span>] is a text-to-image model presented by Google. It encodes the input text into text embeddings by a pre-trained T5-XXL encoder-decoder Transformer with 4.6B frozen parameters. A conditional text-to-image diffusion model (<span class="InternalRef"><a href="#Equ3">7.3</a></span>) maps the text embeddings into a 64 × 64 image. Subsequently these small images are upsampled in two steps to 256 × 256 and to 1024 × 1024 by two super-resolution diffusion models with 600M and 400M parameters (Fig. <span class="InternalRef"><a href="#Fig17">7.17</a></span>). The models are trained on 860M image-text pairs.<figure class="Figure" id="Fig17"><div class="MediaObject" id="MO17"><img alt="" aria-describedby="d64e4018" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig17_HTML.png" style="width:33.85em"/><div class="TextObject" id="d64e4018"><p class="Para" id="Par297">A block diagram begins with text a golden retriever dog wearing a blue checkered beret and red dotted turtleneck. Text is fed into T 5 X X L encoder, followed by text-to-image diffusion, 2 steps of super resolution diffusion to produce a 256 by 256 image and a final resolution of 1024 by 1024.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.17</span><p class="SimplePara">Imagen encodes the input text by the pre-trained T5-XXL text encoder. The resulting text embeddings are transformed to 64 × 64 images by a diffusion model [<span class="CitationRef"><a epub:type="biblioref" href="#CR148" role="doc-biblioref">148</a></span>]. This image is upscaled to 1024 × 1024 resolution by two super-resolution diffusion models. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR148" role="doc-biblioref">148</a></span>, p. 19]</p></div></figcaption></figure></div><div class="Para" id="Par144">Nichol et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR110" role="doc-biblioref">110</a></span>] proposed some modifications for denoising diffusion probabilistic models, which can sample much faster and achieve better log-likelihoods with little impact on sample quality. They deliver the same sample quality as GANs, but achieve a much better mode coverage as measured by recall. This model is also employed by Imagen for text-to-image conversion, using the pooled embedding vector as input. This network is used for upsampling and is extended to improve memory efficiency, inference time, and convergence speed. Figure <span class="InternalRef"><a href="#Fig18">7.18</a></span> shows randomly selected images generated by Imagen for a caption input.<figure class="Figure" id="Fig18"><div class="MediaObject" id="MO18"><img alt="" aria-describedby="d64e4055" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig18_HTML.png" style="width:33.98em"/><div class="TextObject" id="d64e4055"><p class="Para" id="Par298">2 pairs of similar photos. Pair 1, a confused grizzly bear in calculus class. Pair 2, The Rhine River below a castle and with a forest and a vineyard.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.18</span><p class="SimplePara">Images generated by Imagen [<span class="CitationRef"><a epub:type="biblioref" href="#CR148" role="doc-biblioref">148</a></span>, p.6] (left) and Stable Diffusion [<span class="CitationRef"><a epub:type="biblioref" href="#CR142" role="doc-biblioref">142</a></span>] (right) given two different text captions. Images reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR148" role="doc-biblioref">148</a></span>, p. 6] and [<span class="CitationRef"><a epub:type="biblioref" href="#CR158" role="doc-biblioref">158</a></span>], credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4"><span class="RefSource">A.​3</span></a></span></p></div></figcaption></figure></div><p class="Para" id="Par145">Imagen achieves a <span class="EmphasisTypeSmallCaps ">Sota</span> zero-shot FID (Fréchet Inception Distance) on <em class="EmphasisTypeItalic ">COCO</em><span id="ITerm107"/> with a value of 7.3, which is better than the FID of DALL-E 2 and is even better than other models trained on COCO (Table <span class="InternalRef"><a href="#Tab2">7.2</a></span>). Human raters evaluated Imagen with respect to photorealism and alignment to the text caption. For photorealism, people preferred Imagen images in 39.5% of cases to the original images, indicating a relatively high realism. On caption similarity, Imagen’s score is on-par with the original reference images. On the <em class="EmphasisTypeItalic ">DrawBench</em><span id="ITerm108"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR147" role="doc-biblioref">147</a></span>] the images generated by Imagen are always preferred to images created by DALL-E 2, GLIDE, VQGAN+CLIP or Latent Diffusion in more than 60% of the cases. The authors emphasize that in the future they will increase the size of the language model, as this promises a greater gain than increasing the size of the diffusion models. They do not publish Imagen’s code or provide a demo API because it could potentially be abused, for example to create fake images. Gafni et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>] demonstrate how a system can be extended to support artists during the creation of images.</p><p class="Para" id="Par146"><strong class="EmphasisTypeBold ">Stable Diffusion</strong> is another model with currently 5.7B parameters for generating images of up to 1024 × 1024 pixels using diffusion. An example is shown in Fig. <span class="InternalRef"><a href="#Fig18">7.18</a></span>. It works similar to DALLE-2 employing a denoising U-Net for image compression and expansion [<span class="CitationRef"><a epub:type="biblioref" href="#CR142" role="doc-biblioref">142</a></span>]. For training, Stable Diffusion used an image dataset from the freely available LAION-5B database [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>]<span id="ITerm109"/>, which contains about 5.85 billion CLIP-filtered image-text pairs, fourteen times larger than its predecessor LAION-400M. A model conditioned on ImageNet classes achieved an FID of 3.6 for image generation. A variant of the model employs an image search returning images with similar visual features from the neighborhood of each training instance by the CLIP model [<span class="CitationRef"><a epub:type="biblioref" href="#CR15" role="doc-biblioref">15</a></span>]. The model includes the retrieved images during image generation. It can be applied to unconditional image synthesis, inpainting, and stochastic super-resolution, and achieves competitive performance while significantly lowering computational cost. Model inference code and model weights to run the retrieval-augmented diffusion models are now available [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>] and can be downloaded. The model was heavily employed by users creating 1.7M images per day.</p></section>
<section class="Section2 RenderAsSection2" id="Sec20"><h3 class="Heading"><span class="HeadingNumber">7.2.8 </span>Multipurpose Models</h3><p class="Para" id="Par147"><strong class="EmphasisTypeBold ">OFA</strong><span id="ITerm110"/> (One For All) [<span class="CitationRef"><a epub:type="biblioref" href="#CR170" role="doc-biblioref">170</a></span>] provides a unified model for a range of multimodal tasks. It can process text and images in the form of text and visual tokens. OFA has an encoder-decoder transformer architecture (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec20"><span class="RefSource">2.​3.​1</span></a></span>) and is pre-trained on various text and image datasets. Similar to the T5 model (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec4"><span class="RefSource">3.​1.​3</span></a></span>), it receives a textual instruction along with an image and generates the appropriate output.</p><p class="Para" id="Par148">Different modalities are represented in the same space, and text, images, and objects are discretized into a unified output vocabulary. An image with 256 × 256 pixels is represented as 16 × 16 image patches. Each image patch of 16 × 16 pixels is “tokenized” into discrete visual tokens, such that each visual token strongly correlates to the corresponding patch [<span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>]. In addition, objects have a specific representation consisting of a label and its bounding box. The continuous corner coordinates of the bounding box are uniformly discretized to integers as location tokens (<em class="EmphasisTypeItalic ">x</em><sub>1</sub>;<em class="EmphasisTypeItalic ">y</em><sub>1</sub>;<em class="EmphasisTypeItalic ">x</em><sub>2</sub>;<em class="EmphasisTypeItalic ">y</em><sub>2</sub>). Finally, a unified vocabulary is used for all linguistic and visual tokens, including subwords, image codes, and location tokens.</p><div class="Para" id="Par149">Similar to T5 (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec4"><span class="RefSource">3.​1.​3</span></a></span>) the transformer encoder-decoder is controlled by instructions. It receives a text instruction and an input image and generates a corresponding output, a text response and an image. A number of tasks are described by the examples shown in Fig. <span class="InternalRef"><a href="#Fig19">7.19</a></span>. Usually, the OFA model is fine-tuned on specific datasets to solve various tasks.<figure class="Figure" id="Fig19"><div class="MediaObject" id="MO19"><img alt="" aria-describedby="d64e4184" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig19_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e4184"><p class="Para" id="Par299">A photo of 3 members playing football. A block diagram explains photo leads to processing steps, include visual grounding, grounded captioning, text matching and captioning, visual question answering, object detection, image infilling, image generation, text infilling, and O F A encoder and decoder.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.19</span><p class="SimplePara">OFA [<span class="CitationRef"><a epub:type="biblioref" href="#CR170" role="doc-biblioref">170</a></span>, p. 3] receives an instruction and an input image. As output it generates a text and (optionally) an image. For each of the eight instructions (left) an example output (right) is shown. Image credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4"><span class="RefSource">A.​3</span></a></span></p></div></figcaption></figure></div><p class="Para" id="Par150">The OFA model has an OFA<sub>Base</sub> variant with 6 encoder and decoder layers, hidden size 768, and 12 attention heads. The OFA<sub>Large</sub> variant has 12 encoder and decoder layers, hidden size 1024, 16 attention heads and 472M parameters.</p><p class="Para" id="Par151">During pre-training, the model has to solve three tasks requested by the corresponding instructions (Fig. <span class="InternalRef"><a href="#Fig19">7.19</a></span>). The first task is image infilling, where the model has to reconstruct the central parts of the image. This requires the model to learn the relation of image parts and the generation of images. The second task is object detection. This task establishes the correspondence between image parts and language descriptions. The last pre-training task is text infilling to learn the structure of language. The model is pre-trained on publicly available datasets for the different tasks on data with more than 50M images and more than 160GB text. Images are resized to 384 × 384 pixels with a fixed patch size of 16 × 16 pixel. For each patch a feature vector is computed by the first three blocks of a ResNet CNN.</p><p class="Para" id="Par152">Fine-tuning is performed on task-specific datasets for the tasks shown in Fig. <span class="InternalRef"><a href="#Fig19">7.19</a></span>, e.g. MS COCO for image captioning. In addition, OFA is fine-tuned on several NLP tasks such as the GLUE benchmark for natural language understanding, the Gigaword benchmark for abstractive summarization, and the ImageNet-1K dataset for image classification. For inference the authors apply beam search and develop a search strategy based on a prefix tree. This trie-based search strategy ensures that the output generated by OFA is constrained to the appropriate candidate set.</p><p class="Para" id="Par153">For image captioning the model is fine-tuned on MS COCO [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>]. With a <span class="EmphasisTypeSmallCaps ">Bleu</span>-4 score of 43.5 it establishes a new <span class="EmphasisTypeSmallCaps ">Sota</span> for the MS COCO benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>]. For Visual Question Answering the model is fine-tuned on VQAv2 [<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>] and similar datasets. A search strategy based on a prefix tree ensures that the output generated by OFA is constrained to the candidate set. It achieves a new <span class="EmphasisTypeSmallCaps ">Sota</span> accuracy of 80.0%.</p><p class="Para" id="Par154">For the <em class="EmphasisTypeItalic ">visual entailment task</em><span id="ITerm111"/> the model has to determine, if the image entails, contradicts or is neutral to the text. OFA is fine-tuned on <em class="EmphasisTypeItalic ">SNLI-VE</em><span id="ITerm112"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR178" role="doc-biblioref">178</a></span>] and achieves a <span class="EmphasisTypeSmallCaps ">Sota</span> accuracy of 90.2% on the test set, which is 3.1% better than the prior best model. To understand referring expressions, the model has to locate an image region described by a language query. Here the model was fine-tuned on the <em class="EmphasisTypeItalic ">RefCOCO benchmark</em><span id="ITerm113"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR187" role="doc-biblioref">187</a></span>] and related benchmarks. It achieved a new <span class="EmphasisTypeSmallCaps ">Sota</span> with a text accuracy of 92.9%, outperforming competitors by a large margin.</p><p class="Para" id="Par155">For image generation the model is fine-tuned on MS COCO [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>]. It achieves an Fréchet Inception Distance (FID) of 10.5. This is better than the scores for DALL-E [<span class="CitationRef"><a epub:type="biblioref" href="#CR133" role="doc-biblioref">133</a></span>] (27.5) or GLIDE [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>] (12.2), which have far more parameters (12B resp. 3.5B) than OFA with 472M. On the leaderboard, only LAFITE (Sect. <span class="InternalRef"><a href="#Sec18">7.2.6</a></span>) has a better FID-value of 8.1. Note that competing models selected their results from 60 to 512 trial outputs, while OFA only selected the best of 24 images according to FID scores.</p><p class="Para" id="Par156">For image classification in ImageNet, OFA uses no extra labeled training data and has a similar performance (84.9% top-1 accuracy) as EfficientNet-B7 (84.3%), whereas the current <span class="EmphasisTypeSmallCaps ">Sota</span> is 88.3%. Surprisingly, OFA also achieves good results on language-only benchmarks, such as the GLUE natural language understanding benchmark (Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec2"><span class="RefSource">4.​1.​1</span></a></span>) and the Gigaword summarization (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec26"><span class="RefSource">6.​4.​1</span></a></span>). Code, demos, and trained models are available for download.</p><p class="Para" id="Par157">An alternative multipurpose model is <strong class="EmphasisTypeBold ">NÜWA</strong><span id="ITerm114"/>, which is described in Sect. <span class="InternalRef"><a href="#Sec27">7.3.4</a></span>. It provides realistic text-to-image generation, image editing, and image region editing controlled by text. In addition, NÜWA performs text-to-video creation and the prediction of the next video frames.</p><p class="Para" id="Par158"><strong class="EmphasisTypeBold ">WuDao-2.0</strong><span id="ITerm115"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR140" role="doc-biblioref">140</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR143" role="doc-biblioref">143</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR198" role="doc-biblioref">198</a></span>] is a giant mixture-of-experts model with 1075B parameters and has been introduced in Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec26"><span class="RefSource">3.​5.​2</span></a></span>. It is based on the GLM 2.0 architecture (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec4"><span class="RefSource">3.​1.​3</span></a></span>) combining the different learning paradigms of BERT, GPT and the encoder-decoder transformer. For image modeling, it uses the CogView approach (Sect. <span class="InternalRef"><a href="#Sec18">7.2.6</a></span>). However, implementation details are not available. The training data consist of 2.5TB image data and 2.5TB Chinese and English text data (e.g. from the <em class="EmphasisTypeItalic ">Pile</em><span id="ITerm116"/> corpus [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>]). WuDao-2.0 can be applied to a wide range of text analysis and generation tasks, and has matched or surpassed <span class="EmphasisTypeSmallCaps ">Sota</span> levels on five image benchmarks, e.g. on classifying land use in image data, image generation, and graphic retrieval.</p><section class="Section3 RenderAsSection3" id="Sec21"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par159"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par160">Vision transformer code, trained models and notebooks <span class="ExternalRef"><a href="https://github.com/google-research/vision_transformer"><span class="RefSource">github.​com/​google-resear ch/​vision_​transformer</span></a></span></p></li><li><p class="Para" id="Par161">OSCAR code and pre-trained models <span class="ExternalRef"><a href="https://github.com/microsoft/Oscar"><span class="RefSource">github.​com/​microsoft/​Oscar</span></a></span>,</p></li><li><p class="Para" id="Par162">VinVL code and pre-trained Oscar-VinVL models <span class="ExternalRef"><a href="https://github.com/pzzhang/VinVL"><span class="RefSource">github.​com/​pzzhang/​VinVL</span></a></span>.</p></li><li><p class="Para" id="Par163">DALL-E code and notebook <span class="ExternalRef"><a href="https://github.com/openai/DALL-E"><span class="RefSource">github.​com/​openai/​DALL-E</span></a></span></p></li><li><p class="Para" id="Par164">OFA model code, pre-trained models and online demos <span class="ExternalRef"><a href="https://github.com/OFA-Sys/OFA"><span class="RefSource">github.​com/​OFA-Sys/​OFA</span></a></span></p></li><li><p class="Para" id="Par165">GLIDE code, trained models and notebook <span class="ExternalRef"><a href="https://github.com/openai/glide-text2im"><span class="RefSource">github.​com/​openai/​glide-text2im</span></a></span></p></li><li><p class="Para" id="Par166">Stable Diffusion <span class="ExternalRef"><a href="https://github.com/CompVis/latent-diffusion"><span class="RefSource">https://​github.​com/​CompVis/​latent-diffusion</span></a></span></p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec22"><h3 class="Heading"><span class="HeadingNumber">7.2.9 </span>Summary</h3><p class="Para" id="Par167">Recently, the Vision Transformer (ViT) emerged as a competitive alternative to Convolutional Neural Networks (CNNs) for image recognition tasks. ViT models outperform CNNs in terms of accuracy on various benchmarks and require much less computational effort.</p><p class="Para" id="Par168">Foundation Models for image processing receive image patches as input. The embeddings of these image patches are generated by different methods, e.g. linear transformations of image pixels, by the first few layers of CNN models, by variational autoencoders (VAE), or by Generative Adversarial Networks (GANs). A completely different approach is taken by diffusion models, which reverse the process of image degradation by adding noise (GLIDE). It has been shown to be beneficial to discretize representations of image patches to reduce noise and low-level texture dependence.</p><p class="Para" id="Par169">There are two alternatives for including text. Sometimes text and image tokens are processed by separate transformers. Subsequently the distances between the two types of embeddings are minimized (CLIP) or the resulting embeddings are correlated by cross-attention (VilBERT). Otherwise, text and image tokens are concatenated to form the input of Foundation Models (autoencoders, autoregressive, or encoder-decoder). It seems that recent models (DALL-E, CogView, OFA) prefer the single-stream architecture. A number of different tasks are employed for pre-training. These include the masked language model (MLM), where masked image and language tokens have to be reconstructed, masked region classification (MRC), and masked region reconstruction. Sentence-image alignment (SIA) classifies whether image-text pairs belong together.</p><p class="Para" id="Par170">The generation of captions constructs a sentence with the characterization of the image (VilBERT, OSCAR, VinVL, SimVLM) in fluent and correct language, which is usually an accurate description according to human evaluations. The generation of longer captions is not yet investigated and is probably more relevant for video captioning. There are studies to investigate the attention patterns in vision-language models [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>].</p><p class="Para" id="Par171">The creation of images that match captions has made a huge leap in quality over the past year. Various architectures are used: Generative Adversarial Networks (GAN), diffusion models, VAEs. These models are in general combined with PLMs. It seems that pure transformer models have advantages (OFA), but diffusion models like DALL-E 2.0 gain momentum. Usually, a sample of images is created, and the best image is automatically selected by a quality score. Images generated by the model often have the resolution of 256 × 256 and already cover many details. Expect to see models with higher resolutions next year, e.g. 1024 × 1024.</p><p class="Para" id="Par172">Cao et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>] investigate the inner mechanics of vision and language models. They conclude that deeper layers lead to more intertwined multimodal fusion. Usually, the textual modality is more dominant for taking decisions than image features, as models tend to attend to text rather than images during inference. It turns out that a subset of attention heads is specialized for cross-modal interaction. There are attention patterns that align image regions and textual words. Finally, there is no reduction in linguistic capabilities, as pre-trained vision and language models encode rich linguistic knowledge.</p><p class="Para" id="Par173">Recently, multipurpose models have been presented that are trained to solve a large number of different language, vision, and language-vision tasks. One example is OFA, which has 472M parameters, significantly fewer than DALL-E (12B). OFA is a transformer encoder-decoder with image and text tokens as input, controlled by text instructions similar to T5. It achieves <span class="EmphasisTypeSmallCaps ">Sota</span> in image captioning, image generation, visual question answering, visual entailment, and even on pure language tasks. Contrast this with the huge WuDao 2.0 model with 1750B parameters, which is based on the encoder-decoder GLM model with a mixture-of-experts architecture. The model claims <span class="EmphasisTypeSmallCaps ">Sota</span> performance on a number of image and text tasks, but no technical details are known.</p><p class="Para" id="Par174">In the future, it is expected that these text-image models will be extended to other modalities such as video, speech, and 3D. In addition, more data will be used, Moreover, they will be enhanced by retrieval techniques to include additional external and up-to-date knowledge. Text-image models are a big step towards <em class="EmphasisTypeItalic ">symbol grounding</em><span id="ITerm117"/>, which allows to attach symbols (words) to their real-world meaning.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec23"><h2 class="Heading"><span class="HeadingNumber">7.3 </span>Video Interpretation and Generation</h2><p class="Para" id="Par175">As the Web is becoming a constantly growing communication vehicle, expressing content by text and images is often not sufficient. Video brings together three things that catch our attention like nothing else: image, movement, and audio. Therefore, videos are more and more important as a means of communication. There are 2 billion users active on YouTube each month and over 1 billion on TikTok with an average usage of 52 min per day. Hence, the automatic analysis, interpretation, and generation of videos is extremely valuable. For visual data, the most comprehensive self-supervision is available in videos. Their various modalities such as images, speech, ASR text, and captions are temporally aligned and do not require human annotation. The extreme number of multimodal videos potentially allows Foundation Models to acquire a model of the visual world.</p><section class="Section2 RenderAsSection2" id="Sec24"><h3 class="Heading"><span class="HeadingNumber">7.3.1 </span>Basics of Video Processing</h3><div class="Para" id="Par176">Video analysis and understanding is more challenging than image processing, because video has an additional time dimension and usually has to handle images, speech, and text from speech or subtitles simultaneously. Recently Foundation Models have been used for video understanding. Compared to CNNs and RNNs, the major advantage of transformers is the ability to simultaneously capture global information and compute this in parallel. Furthermore, the concise and stackable architecture of transformers enables training on larger datasets. Table <span class="InternalRef"><a href="#Tab4">7.3</a></span> list the main variants of Foundation Models for video. <div class="Table" id="Tab4"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 7.3</span><p class="SimplePara">Main techniques using PLMs for video. The numbers in parenthesis indicate parameter count</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Approach</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Benchmark</p></th></tr></thead><tbody><tr><td colspan="3" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">Video to text</strong></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">VideoBERT</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Partition video into 30 clips and generate embeddings by CNN. Cluster embedding by <em class="EmphasisTypeItalic ">k</em>-means. ASR speech generates text tokens. Concatenate inputs to BERT</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">YouCook II video captioning 4.3 <span class="EmphasisTypeSmallCaps ">Bleu</span>-4</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">COOT</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Image, video and text are processed in 3 different hierarchy levels. Separate transformers for each level. Special attention for cooperation in each level (10.6M)</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">YouCook II video captioning 11.3 <span class="EmphasisTypeSmallCaps ">Bleu</span>-4</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">DeCEMBERT</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Video 2D and 3D features, region captions, ASR text. Inputs linearly transformed and fed into a single BERT</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">YouCook II video captioning 11.9 <span class="EmphasisTypeSmallCaps ">Bleu</span>-4</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">VATT</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Generate image-time patches, separate BERT models for video, audio, and text. Contrastive estimation to reduce embedding distances</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Kinetics-400 action recognition 81.1%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Omnivore</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Image, video and 3D views are converted and fed into Swin transformer with shifted windows</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Kinetics-400 action recognition 84.1% (no extra data)</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MeMViT</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Attention computation with memory of past video frames. Memory not trained. Uses memory compression module with pooling</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Action recognition on EPIC-KITCHENS-100 accuracy 48.4%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">CoVeR</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Separate image and temporal aggregation. Parallel fine-tuning for image and video recognition</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Kinetics-400 action recognition 87.2%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MTV</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Temporal aggregation by multiple views. Use different Vision Transformers for each view (1B)</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Kinetics-400 action recognition 89.1%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Merlot</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Joint processing of video and ASR text. MLM for text and video. Reorder scrambled frames</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Visual question answering 43.1%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Flamingo</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Process images, video by vision transformer (80B). Include image information into language model (Chinchilla) by adapters and cross-attention layers. Allows few-shot prompts</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeSmallCaps ">Sota</span> on all of 8 image benchmarks and all of 8 video benchmarks</p></td></tr><tr><td colspan="3" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">Text to video</strong></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Video transformer</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Partition video to 3D blocks with varying dimensions in different layers (373M)</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">AR video generation FVD score 94 on BAIR Robot data</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NÜWA</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Image, video and text data are represented as 3D tokens. Discretized by VQ-GAN. Use localized attention computations. Trained for text-to image, video prediction and text-to-video. More applications</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">AR video generation FVD score 86.9 on BAIR Robot data (<span class="EmphasisTypeSmallCaps ">Sota</span>) text-to-video FID-img 28.5 on Kinetics</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Imagen video</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Base video generation model + several spatial and temporal video super-resolution diffusion models</p></td><td style="text-align: left;"><p class="SimplePara">FVD score of about 9.0 for the model with 5.6B parameters</p></td></tr></tbody></table></div></div><div class="Para" id="Par177">Early models for image processing, e.g. CNNs and GANs, performed the analysis of images pixel-by-pixel. However, this is no longer possible for videos due to the high computational and memory effort, and there has to be an aggregation of image information. Therefore, special spatio-temporal aggregation modules are developed to adapt this to the limited sequence length of transformers. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par178">A simple solution is the aggregation of 30 video frames (VideoBERT).</p></li><li><p class="Para" id="Par179">Videos can be processed by considering 3D <em class="EmphasisTypeItalic ">video patches</em><span id="ITerm118"/>, which cover a small pixel region in a small number of frames. It is possible to aggregate video and text over different temporal levels and compute associations between the levels (COOT, MTV). Regional and temporal aggregation may be separated (CoVeR).</p></li><li><p class="Para" id="Par180">Additionally the video patches may be processed to extract salient information. An example is video quantization by variational autoencoders (VQ-VAE), which already was used for image processing, e.g. by DALL-E or CogView (Sect. <span class="InternalRef"><a href="#Sec18">7.2.6</a></span>). Image patches can be extended in time to obtain 3D voxels (VATT, Omnivore).</p></li><li><p class="Para" id="Par181">A video can be partitioned into short time clips. Prior clips can enter the self-attention computations but no update of prior embeddings is necessary (MeMViT).</p></li></ul></div></div><p class="Para" id="Par182">To further reduce computational effort, a sparse self-attention can be used, where attention is mostly computed to nearby video pixels.</p><p class="Para" id="Par183">Unsupervised training may be performed similar to BERT. For instance, masked video tokens can be predicted based on neighboring video and text tokens [<span class="CitationRef"><a epub:type="biblioref" href="#CR145" role="doc-biblioref">145</a></span>]. In the same way, masked text tokens can be predicted from neighboring text and video tokens. Contrastive learning can be used to discriminate between genuine text-video pairs and random pairs. Other tasks include classifying whether a video and some text belong together, predicting the next frame, or reconstructing the order of shuffled video or text tokens. Recent surveys on video understanding are provided by Islam et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>], Khurana et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR85" role="doc-biblioref">85</a></span>], and Ruan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR145" role="doc-biblioref">145</a></span>]</p><p class="Para" id="Par184">There are a number of training data sources for video. <em class="EmphasisTypeItalic ">Kinetics</em><span id="ITerm119"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR83" role="doc-biblioref">83</a></span>] is a collection of 306k large-scale, high-quality datasets of 10s video clips focusing on human actions. The variants Kinetics 400, 600, and 700 are annotated with 400, 600, and 700 classes, respectively. Example frames of annotated videos are shown in Fig. <span class="InternalRef"><a href="#Fig21">7.21</a></span>. <em class="EmphasisTypeItalic ">Moments in Time</em><span id="ITerm120"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR107" role="doc-biblioref">107</a></span>] is a collection of 800k labeled 3s videos, involving people, animals, objects or natural phenomena that capture the gist of a dynamic scene. <em class="EmphasisTypeItalic ">Epic-Kitchens-100</em><span id="ITerm121"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>] consists of 90k egocentric videos, totaling 100 h, recorded in kitchens. Each video is labeled with a “noun” and a “verb”. Three accuracy scores (“noun”, “verb”, and “action”) are usually reported. The action score assesses correct noun-verb pairs and is most important. <em class="EmphasisTypeItalic ">Something-Something V2</em><span id="ITerm122"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>] consists of more than 220k short video clips that show humans interacting with everyday objects. Similar objects and backgrounds appear in videos across different classes. This data challenges a model’s capability to distinguish classes from motion cues, in contrast to other datasets.</p></section>
<section class="Section2 RenderAsSection2" id="Sec25"><h3 class="Heading"><span class="HeadingNumber">7.3.2 </span>Video Captioning</h3><p class="Para" id="Par185"><em class="EmphasisTypeItalic ">Video captioning</em><span id="ITerm123"/> aims at automatically generating natural language descriptions of videos. Video captioning is substantially more difficult than image captioning because the spatial-temporal information in videos as well as the corresponding ASR text from the video introduces an additional complexity. On the other hand, huge video collections like YouTube are available on the Internet and can be used as training material. A recent survey is given by Perez-Martin et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR124" role="doc-biblioref">124</a></span>].</p><div class="Para" id="Par186"><strong class="EmphasisTypeBold ">VideoBERT</strong><span id="ITerm124"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR160" role="doc-biblioref">160</a></span>] applies a BERT model to video-text pairs. The video is partitioned into clips of 30 frames (1.5sec) and processed by the S3D CNN with a temporal convolution [<span class="CitationRef"><a epub:type="biblioref" href="#CR180" role="doc-biblioref">180</a></span>], which generates a clip embedding vector of size 1024. The clip embeddings are partitioned by <em class="EmphasisTypeItalic ">k</em>-means clustering into 20,736 clusters and quantized to video tokens. Speech is processed by ASR and partitioned into sentences. The text is tokenized by WordPiece with a vocabulary of 30k tokens. The video tokens corresponding to the sentence time period are collected in a video token sequence. As shown in Fig. <span class="InternalRef"><a href="#Fig20">7.20</a></span> the video tokens are appended to the corresponding text tokens separated by special tokens. Note that text-only and video-only training is possible as well.<figure class="Figure" id="Fig20"><div class="MediaObject" id="MO20"><img alt="" aria-describedby="d64e4761" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig20_HTML.png" style="width:34.07em"/><div class="TextObject" id="d64e4761"><p class="Para" id="Par300">A diagram illustrates the placement of the C L S token and MASK token in a sequence, followed by the input image, the masked image, 3 masked images with random tokens, and S E P token. The video features BERT in different frames with proper images and text for a clearer understanding of the concept.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.20</span><p class="SimplePara">A text generated by ASR and the corresponding video tokens are the input of VideoBERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR160" role="doc-biblioref">160</a></span>]. Both modalities are bounded by special tokens. The masked tokens have to be predicted. Image credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4"><span class="RefSource">A.​3</span></a></span></p></div></figcaption></figure><figure class="Figure" id="Fig21"><div class="MediaObject" id="MO21"><img alt="" aria-describedby="d64e4780" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig21_HTML.png" style="width:34.07em"/><div class="TextObject" id="d64e4780"><p class="Para" id="Par301">A series of 5 photographs with different frame positions in the first row and the second row. Row 1 represents dribbling basketball. Row 2 represents dunking basketball.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.21</span><p class="SimplePara">Two videos annotated with descriptions (left) similar to videos of the Kinetics dataset [<span class="CitationRef"><a epub:type="biblioref" href="#CR83" role="doc-biblioref">83</a></span>]. Representative frames of the videos are shown. Obviously, a single frame is sometimes not enough to reach a decision, e.g. to distinguish “dribbling basketball” and “dunking basketball”. Image credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4"><span class="RefSource">A.​3</span></a></span></p></div></figcaption></figure></div><p class="Para" id="Par187">The BERT<sub>LARGE</sub> model is pre-trained on a video set of 312k cooking videos with a total duration of 966 days. The text is obtained by ASR. Training tasks are masked token and frame prediction, and detecting text matching a video. VideoBERT yields <span class="EmphasisTypeSmallCaps ">Sota</span> on video captioning on the YouCook II data with <span class="EmphasisTypeSmallCaps ">Bleu</span>-4 score of 4.3.</p><p class="Para" id="Par188"><strong class="EmphasisTypeBold ">COOT</strong><span id="ITerm125"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>] jointly processes image, video and text information with an universal representation by embedding vectors. In the representation of videos, time is added as a third dimension to the two-dimensional description of images. The COOT model considers the data on 3 different levels of hierarchy: frame/word, clip/sentence and video/paragraph. For each level there exists a pair of transformers processing the input. To model intra-level cooperation, COOT uses a feature aggregation layer to focus on temporal interactions between low-level entities. To aggregate information to the sentence level, the model uses a special attention formula, where all corresponding embeddings enter the scalar product. An additional loss term aims to reduce the difference between sentence and clip encodings. At the top level, a contextual transformer links the text and video embeddings.</p><p class="Para" id="Par189">The model is trained with videos that have subtitles for individual scenes and longer segments. Subsequently, the model can create subtitles for new videos. For the YouCook2 video subtitling benchmark dataset, the model can greatly improve the <span class="EmphasisTypeSmallCaps ">Sota</span> to 11.3 <span class="EmphasisTypeSmallCaps ">Bleu</span>-4. In addition, the model can also be used for other tasks, such as searching when a textual description or a video scene is entered. Since the model includes only 10.6M parameters, it is expected that performance can be greatly improved by increasing the size of the model.</p><p class="Para" id="Par190"><strong class="EmphasisTypeBold ">DeCEMBERT</strong><span id="ITerm126"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR164" role="doc-biblioref">164</a></span>] aims to enhance a video by <em class="EmphasisTypeItalic ">region captions</em><span id="ITerm127"/> in addition to the ASR-text extracted by speech recognition. The input text is represented by BPE-tokens. Each second of video is characterized by 2D-features extracted by a pre-trained Resnet-152 CNN [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>] as well as by motion features extracted by a 3D ResNeXT CNN [<span class="CitationRef"><a epub:type="biblioref" href="#CR179" role="doc-biblioref">179</a></span>], which together are mapped to embedding vectors. The video embeddings and speech recognition text representations are concatenated forming a single sequence as inputs to a 12-layer autoencoder for pre-training and downstream task fine-tuning. To align video with ASR captions, a constrained attention loss is used that encourages the model to select the best matched ASR caption from a pool of candidates. During pre-training on 1.2M YouTube instructional videos, the association between text and video is learned by masking tokens and by a classification, if a text corresponds to a video. On the YouCook2 captioning task the model improves <span class="EmphasisTypeSmallCaps ">Sota</span> to a <span class="EmphasisTypeSmallCaps ">Bleu</span>-4 score of 11.9. In addition, DeCEMBERT yields good results for video retrieval and video question answering.</p></section>
<section class="Section2 RenderAsSection2" id="Sec26"><h3 class="Heading"><span class="HeadingNumber">7.3.3 </span>Action Recognition in Videos</h3><p class="Para" id="Par191"><strong class="EmphasisTypeBold ">VATT</strong><span id="ITerm128"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>] uses raw RGB frames of Internet videos, audio waveforms, and ASR text of the speech audio as input data. The video of size <em class="EmphasisTypeItalic ">T</em> × <em class="EmphasisTypeItalic ">W</em> × <em class="EmphasisTypeItalic ">H</em> with <em class="EmphasisTypeItalic ">T</em> frames is partitioned to a sequence of ⌈<em class="EmphasisTypeItalic ">T</em>∕<em class="EmphasisTypeItalic ">t</em>⌉∗⌈<em class="EmphasisTypeItalic ">H</em>∕<em class="EmphasisTypeItalic ">h</em>⌉∗⌈<em class="EmphasisTypeItalic ">W</em>∕<em class="EmphasisTypeItalic ">w</em>⌉ patches, where each patch is a <em class="EmphasisTypeItalic ">voxel</em><span id="ITerm129"/> in <span class="InlineEquation" id="IEq7"><img alt="$$\mathbb {R}^{t\times h\times w\times 3}$$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq7.png" style="width:4.06em"/></span> with an additional color dimension. This is an extension of the image patches of ViT. The position encoding is a sum <em><strong class="EmphasisTypeBoldItalic ">e</strong></em><sub><em class="EmphasisTypeItalic ">i</em>,<em class="EmphasisTypeItalic ">j</em>,<em class="EmphasisTypeItalic ">k</em></sub> = <em><strong class="EmphasisTypeBoldItalic ">e</strong></em><sub>temp;<em class="EmphasisTypeItalic ">i</em></sub> + <em><strong class="EmphasisTypeBoldItalic ">e</strong></em><sub>horiz;<em class="EmphasisTypeItalic ">j</em></sub> + <em><strong class="EmphasisTypeBoldItalic ">e</strong></em><sub>vert;<em class="EmphasisTypeItalic ">k</em></sub> where each of the summands is a learnable vector of length <em class="EmphasisTypeItalic ">d</em>. The raw audio waveform is partitioned into <em class="EmphasisTypeItalic ">t</em><sup><em class="EmphasisTypeItalic ">′</em></sup> segments and each segment gets a learnable position embedding. For the text a vocabulary is created and each word is mapped to a learnable embedding. The <em class="EmphasisTypeItalic ">DropToken</em><span id="ITerm130"/> procedure removes a random sample of the video or audio tokens to reduce computational cost and improve regularization.</p><p class="Para" id="Par192">VATT linearly projects each modality into a feature vector of length <em class="EmphasisTypeItalic ">d</em> and feeds it into a separate BERT encoder. The model uses Noise Contrastive Estimation to reduce the distance between projections of the audio and video embeddings. Positive pairs are taken from the same location in the video, and negative pairs from different locations. A similar criterion is employed to reduce the distance of video and text embeddings. The training data covers clips of 32 frames at 10 fps taken from the <em class="EmphasisTypeItalic ">HowTo100M data</em><span id="ITerm131"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR105" role="doc-biblioref">105</a></span>]. The largest model has 415M parameters. For action recognition on Kinetics-400 it achieves <span class="EmphasisTypeSmallCaps ">Sota</span> with a top-1 accuracy of 82.1% and a top-5 accuracy of 95.6%.</p><p class="Para" id="Par193"><strong class="EmphasisTypeBold ">Omnivore</strong><span id="ITerm132"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>] is a model for classifying images, videos, and single-view 3D data using exactly the same model parameters. A single-view 3D is a color image with an additional depth channel. Image, video, and single-view 3D modalities are converted into embeddings that are fed into a Transformer model. The images are partitioned into image patches, videos are divided into spatio-temporal tubes covering separate image regions, and the single-view 3D images are converted into RGB patches and depth patches. The patches are projected into embeddings using linear layers. The same linear layer is used for image and video RGB patches. A separate layer is applied to depth patches. Separate positional embeddings for the spatial and the temporal dimension are used.</p><p class="Para" id="Par194">Omnivore employs the <em class="EmphasisTypeItalic ">Swin transformer</em><span id="ITerm133"/> (Sect. <span class="InternalRef"><a href="#Sec15">7.2.3</a></span>) as base model, a hierarchical vision transformer using shifted windows. Self-attention involves patch embeddings from spatially and temporally nearby patches. The models are jointly trained on the ImageNet-1K dataset for image classification (1.2M images), the Kinetics-400 dataset for action recognition (240k videos), and the <em class="EmphasisTypeItalic ">SUN RGB-D dataset</em><span id="ITerm134"/> (5k) for single-view 3D scene classification, with dataset-specific linear classification layers transforming the final embeddings. On Kinetics-400 without extra data, Omnivore achieved an action recognition accuracy of 84.1%, which was the second best. The fine-tuned Omnivore scored <span class="EmphasisTypeSmallCaps ">Sota</span> on two video classification benchmarks. When using RGB and the 3D channel, Omnivore again had a <span class="EmphasisTypeSmallCaps ">Sota</span> performance on the NYU-v2 benchmark.</p><p class="Para" id="Par195"><strong class="EmphasisTypeBold ">MeMViT</strong><span id="ITerm135"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR173" role="doc-biblioref">173</a></span>] aims to process videos longer than 5s, in contrast to most current models. MeMViT handles videos in an online fashion and caches key and value vectors of a transformer as memory at each iteration. Through the memory, the model has access to prior context for long-term modeling, with little additional cost, as memory embeddings are not trained. The queries of the current video clip attend to an extended set of key and value vectors, which come from both the current time and the past. Similar to the dilated convolutions of WaveNet [<span class="CitationRef"><a epub:type="biblioref" href="#CR114" role="doc-biblioref">114</a></span>], higher layers attend further down into the past, resulting in a significantly longer receptive field. In addition, a memory compression module with learnable pooling is effective for reducing the memory footprint.</p><p class="Para" id="Par196">A video is split into a sequence of short <em class="EmphasisTypeItalic ">T</em> × <em class="EmphasisTypeItalic ">H</em> × <em class="EmphasisTypeItalic ">W</em> clips and processed sequentially. Similar to MTV, multiple resolutions are used, starting from a fine-grained modeling of smaller patches to high-level modeling of larger patches in later stages, where the dimensionality of embeddings increases. The aggregation between stages is done by strided pooling. The memory representations are frozen and not changed by optimization. The model is pre-trained on Kinetics-400 data Fig. <span class="InternalRef"><a href="#Fig21">7.21</a></span>. On the AVA v2.2 dataset [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>] MeMViT achieves a mean average precision (mAP) of 35.4%. On the action anticipation dateset (EPIC-KITCHENS-100) it has a <span class="EmphasisTypeSmallCaps ">Sota</span> of 17.7% recall@5. On the action recognition on EPIC-KITCHENS-100 MeMViT yields an accuracy of 48.4%.</p><div class="Para" id="Par197"><strong class="EmphasisTypeBold ">CoVeR</strong><span id="ITerm136"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR190" role="doc-biblioref">190</a></span>] evaluates the effect of different pre-training strategies on classification accuracy. The authors use a special transformer architecture, which has spatial attention layers across related regions in the same video frame and temporal attention layers across the neighboring frames of a video clip. CoVeR first pre-trains the model on the <em class="EmphasisTypeItalic ">JFT-3B benchmark</em><span id="ITerm137"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR189" role="doc-biblioref">189</a></span>] of 3B images annotated with a class-hierarchy of around 30k labels. During pre-training all temporal attention layers are removed. During fine-tuning, it simultaneously trains a single model with 24 layers on multiple action recognition and image datasets (Kinetics versions, ImageNet, Moments in Time, SomethingSomethingv2) to build robust spatial and temporal representations of video data (Fig. <span class="InternalRef"><a href="#Fig22">7.22</a></span>). For the Kinetics-400 action recognition task CoVeR achieves an accuracy of 87.2% and for the Moments in Time action classification it has a <span class="EmphasisTypeSmallCaps ">Sota</span> accuracy of 46.1%.<figure class="Figure" id="Fig22"><div class="MediaObject" id="MO22"><img alt="" aria-describedby="d64e5096" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig22_HTML.png" style="width:34.07em"/><div class="TextObject" id="d64e5096"><p class="Para" id="Par302">A photograph of Times former block has been transformed into a single-frame video. The Bull video processed into 24 by model, which incorporates a temporal attention layer, spatial attention layer, MLP layer, image classifier, and video classifier.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.22</span><p class="SimplePara">During fine-tuning CoVeR [<span class="CitationRef"><a epub:type="biblioref" href="#CR190" role="doc-biblioref">190</a></span>, p. 5] simultaneously is trained on multiple image and video datasets. Each dataset has its own classifier as there are different class definitions. Images are single frame videos. Therefore, image classification is not affected by temporal attention. Image credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4"><span class="RefSource">A.​3</span></a></span></p></div></figcaption></figure></div><p class="Para" id="Par198"><strong class="EmphasisTypeBold ">MTV</strong><span id="ITerm138"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR185" role="doc-biblioref">185</a></span>] performs temporal aggregation by multiple input representations (views) of the input video. MTV extracts tokens from the input video over multiple time spans. Video tokens derived from long time intervals capture the overall scene description, while video tokens taken from short segments capture fine-grained details, such as a person’s gesture. Different transformer encoders are used to process these different views, with short segment models having higher capacity.</p><p class="Para" id="Par199">The different encoders are interfaced by lateral connections to fuse cross-view information. A cross-view attention is computed between adjacent views similar to the multi-head cross-attention in the transformer (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec20"><span class="RefSource">2.​3.​1</span></a></span>). Note that these fusion operations are performed only for specific layers. The tokens from all views are aggregated with a global encoder, which performs the final classification.</p><p class="Para" id="Par200">The models are initialized with Vision Transformer weights (Sect. <span class="InternalRef"><a href="#Sec14">7.2.2</a></span>) and trained with videos of 32 frames and a resolution of 224 × 224. It turned out that the cross-view attention was better than alternatives to fuse information from different views. In addition, three views gave better results than fewer views. The largest model with over a billion parameters achieved <span class="EmphasisTypeSmallCaps ">Sota</span> accuracy of 89.1% for action recognition on kinetics-400.</p><p class="Para" id="Par201"><strong class="EmphasisTypeBold ">AV-ASR</strong><span id="ITerm139"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR152" role="doc-biblioref">152</a></span>] applies a PLM to audio-visual speech recognition. As usual, audio is converted to 80 log Mel filterbank features in steps of 10 ms. The videos are cropped to a near mouth region and converted to video embeddings with length 512. Both embeddings are concatenated and fed into a Conformer encoder (Sect. <span class="InternalRef"><a href="#Sec3">7.1.2</a></span>) with 17 layers. The model outperforms previous <span class="EmphasisTypeSmallCaps ">Sota</span> for lipreading on the LRS3-TED benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>] with a WER of 19.3%. If both modalities are used, the WER drops to 1.6%. If babbling noise is added the WER of audio-only ASR on LRS3-TED is increased to 6.1%, while speech recognition with both modalities has a WER of only 2.9%. There is another approach to associate video and audio by generating video background music that matches the speed of movement, mood, and rhythm of the video [<span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>].</p><p class="Para" id="Par202"><strong class="EmphasisTypeBold ">Aloe</strong><span id="ITerm140"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>] wants to do more than simply describing an image or video, but aims at explaining or reasoning about the scene. The model uses an unsupervised object segmentation module that partitions each image into object representations. A transformer receives the questions and the image descriptions including object representations. On several <em class="EmphasisTypeItalic ">visual reasoning</em><span id="ITerm141"/> benchmarks, the model has to answer complex question such as explanatory questions like <em class="EmphasisTypeItalic ">“why did something happen?”</em>, predictive questions such as <em class="EmphasisTypeItalic ">“what will happen next?”</em>, and counterfactual questions like <em class="EmphasisTypeItalic ">“what would happen in an unseen circumstance, e.g. if an object is removed?”</em>. The model is able to improve <span class="EmphasisTypeSmallCaps ">Sota</span> on nearly all benchmark dataset.</p><p class="Para" id="Par203"><strong class="EmphasisTypeBold ">Merlot</strong><span id="ITerm142"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR188" role="doc-biblioref">188</a></span>] is a vision and language model that learns multimodal world representations from videos with thousands of frames and their ASR text. It encodes each frame using an image encoder, embeds tokens using learned embeddings, and a Transformer similar to RoBERTa jointly processes both representations. A first pre-training task uses contrastive loss to match the language transcript embedding and the corresponding video embedding. The MLM task requires replacing masked language tokens. The temporal reordering task involves reordering scrambled video frames. Hence, Merlot not only learns to match images to temporally corresponding words, but also to contextualize what is happening globally over time, achieving temporal common sense knowledge. The model is trained on 6M unlabeled YouTube videos. Merlot outperforms <span class="EmphasisTypeSmallCaps ">Sota</span> methods in 12 downstream benchmarks that include short and long videos. An example is Visual Question Answering on MSRVTT-QA [<span class="CitationRef"><a epub:type="biblioref" href="#CR182" role="doc-biblioref">182</a></span>] with a new <span class="EmphasisTypeSmallCaps ">Sota</span> of 43.1%. A related model for complex event extraction [<span class="CitationRef"><a epub:type="biblioref" href="#CR93" role="doc-biblioref">93</a></span>] uses a similar contrastive learning approach.</p><p class="Para" id="Par204"><strong class="EmphasisTypeBold ">Flamingo</strong><span id="ITerm143"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>] is a visual language model, which can handle sequences of arbitrarily interleaved image, video and text data. Flamingo employs the 70B parameter pre-trained language model <em class="EmphasisTypeItalic ">Chinchilla</em><span id="ITerm144"/> trained on a large and diverse text corpus (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec3"><span class="RefSource">3.​1.​2</span></a></span>). The encoder blocks of the language model are used with frozen parameters. With this submodel, Flamingo has strong generative language abilities and access to a large amount of knowledge stored in the Chinchilla weights. Similar to <em class="EmphasisTypeItalic ">Frozen</em><span id="ITerm145"/> (Sect. <span class="InternalRef"><a href="#Sec17">7.2.5</a></span>), it can be instructed by few-shot learning to answer questions on an image [<span class="CitationRef"><a epub:type="biblioref" href="#CR166" role="doc-biblioref">166</a></span>].</p><div class="Para" id="Par205">For processing images and videos, a contrastive text-image approach is pre-trained (Fig. <span class="InternalRef"><a href="#Fig23">7.23</a></span>). The authors use a variant of ResNet [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>]. The vision encoder is pre-trained using a contrastive objective on our datasets of image and text pairs, using the two-term contrastive loss from [<span class="CitationRef"><a epub:type="biblioref" href="#CR127" role="doc-biblioref">127</a></span>]. Much like CLIP (Sect. <span class="InternalRef"><a href="#Sec16">7.2.4</a></span>), similarities are computed as a dot-product of the mean pooled output of the image encoder and the mean pooled output of a BERT model. This model extracts semantic spatially oriented features from the image including color, shape, nature, positions of objects, etc. The model is pre-trained separately, and the parameters are frozen during the main training of Flamingo.<figure class="Figure" id="Fig23"><div class="MediaObject" id="MO23"><img alt="" aria-describedby="d64e5267" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig23_HTML.png" style="width:34.07em"/><div class="TextObject" id="d64e5267"><p class="Para" id="Par303">A block diagram displays input test and visual data are interleaved at the bottom. The diagram has a cute dog, followed by vision encoder, perceiver resampler, visual data processing, first gated cross-attention dense model block, and serious cat. The output text is generated from these components.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.23</span><p class="SimplePara">Flamingo [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>] receives an input consisting of a sequence containing image, text, and video in arbitrary order (bottom). The images and videos are processed by a frozen vision encoder similar to CLIP. The trainable perceiver resampler reduces them to a finite number of image tokens, which are included by a trainable cross-attention layer into the language model. The output created by the language model is the natural continuation of the input sequence. Image adapted from [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>] with kind permission of authors, credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4"><span class="RefSource">A.​3</span></a></span></p></div></figcaption></figure></div><p class="Para" id="Par206">Two modules are trained to interface these frozen models. The first is a <em class="EmphasisTypeItalic ">perceiver resampler</em><span id="ITerm146"/>, which receives spatio-temporal features from the vision encoder and outputs a fixed-size set of visual tokens (usually 64). This output is generated for single images as well as videos independently of the input image resolution or the number of input video frames. The extracted visual tokens are then included into the language model by interspersed cross-attention layers. In this way the language model can incorporate the visual information at each layer. The frozen language and vision models have 70B and 435M parameters, while the trainable layers have 10B parameters and the resampler has 194M parameters yielding a total of 80.6B parameters.</p><p class="Para" id="Par207">For training, Flamingo uses a number of datasets with 182GB of text. This collection is amended with further mixed text, image and video sequences with a total of about 2.3B images and 27M videos.</p><div class="Para" id="Par208">As shown in Fig. <span class="InternalRef"><a href="#Fig24">7.24</a></span> Flamingo can answer question on single images by simply predicting the next text token in the mixed image-text sequence. In their simplest form, the question can ask for the description of objects in the scene, as is shown in the upper right example. More difficult is the interpretation of the scene as the language model needs world knowledge to decide which aspects of an image are noteworthy. In many of these examples, Flamingo can do at least one step of implicit inference. Some of the objects are not named in the prompt (e.g. the elephant), but their properties are queried directly. In order to answer these questions, the model needs to infer the referred object and then recall the relevant knowledge to form the answer. This can lead to a single answer (as for the elephant on the truck) or to an extended dialog, where the model can answer a series of queries about an image (e.g. the dog damaging the sofa). Even after several interactions, Flamingo can still successfully attend to the image and reply to questions that require to interpret the image. The authors observed that multiple images can be separately attended to, simple comparisons and inferences are handled properly. Flamingo’s dialog capabilities could enable non-expert end users to get answers without the need of fine-tuning.<figure class="Figure" id="Fig24"><div class="MediaObject" id="MO24"><img alt="" aria-describedby="d64e5300" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig24_HTML.png" style="width:34.38em"/><div class="TextObject" id="d64e5300"><p class="Para" id="Par304">3 photographs, an elephant travels on a truck, a dog damages a sofa set, and a person holds a coffee mug. The meat, vegetables, and American food are placed on a plate. Each has questions with answers, what the person is holding, the ingredients of this dish, and what is odd about this image. </p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.24</span><p class="SimplePara">Flamingo can interpret images and describe them by text. Gray boxes are user input and the pink boxes are Flamingo output. In the upper row Flamingo answers questions about images. In the lower row there is a dialog about a photo. Image adapted from [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>, p. 31] and [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>, p. 32], reprinted with kind permission of the authors</p></div></figcaption></figure></div><div class="Para" id="Par209">In the same way Flamingo can answer question about videos, as shown in Fig. <span class="InternalRef"><a href="#Fig25">7.25</a></span>. However, the performance in this task is not as stable as would be desirable.<figure class="Figure" id="Fig25"><div class="MediaObject" id="MO25"><img alt="" aria-describedby="d64e5320" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig25_HTML.png" style="width:34.38em"/><div class="TextObject" id="d64e5320"><p class="Para" id="Par305">4 photos a weight machine, a dog climbs over it. The question is, What is happening here? and the answer is dachshund puppy is being weighed on a scale. 4 photos of a person plays golf. The question is, What happens to the man after hitting the ball? and the answer he falls down.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.25</span><p class="SimplePara">Flamingo answers question on videos. Some video frames are shown. Gray boxes are user input and the pink boxes are Flamingo output. Image adapted from [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>, p. 33], reprinted with kind permission of the authors</p></div></figcaption></figure></div><div class="Para" id="Par210">Flamingo is able to perform <em class="EmphasisTypeItalic ">few-shot prompting</em><span id="ITerm147"/> on mixed text-video-image sequences. Examples are shown in Fig. <span class="InternalRef"><a href="#Fig26">7.26</a></span>. Here a number of images are provided and the added text specifies by example the desired way to extract an answer. In the first row this amounts to extracting text from the image and in the second row the counting of objects of equal type is required. In this way the model can be instructed on the fly to perform a large number of tasks, e.g. captioning, visual dialogue, classification or visual question answering.<figure class="Figure" id="Fig26"><div class="MediaObject" id="MO26"><img alt="" aria-describedby="d64e5349" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig26_HTML.png" style="width:34.07em"/><div class="TextObject" id="d64e5349"><p class="Para" id="Par306">6 photographs of the flamingo output result, namely underground, congress, soulomes, pandas 3, dogs 2, and giraffes 4.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.26</span><p class="SimplePara">Few-shot querying of Flamingo [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>] with a mixture of images and text. Note that in the second example Flamingo did not count the trees but stayed with the animals. The usual number of few-shot queries is 32. Image adapted from [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>, p. 2], reprinted with kind permission of the authors</p></div></figcaption></figure></div><p class="Para" id="Par211">The performance of the model was tested on 9 image-text benchmarks on scene description, visual dialogue, and visual QA, among them MS-COCO captioning. On the eight mixed-media benchmarks Flamingo established a few-shot <span class="EmphasisTypeSmallCaps ">Sota</span> by a wide margin using 16 or 32 shots. For three benchmarks the score is even better than the prior fine-tuned <span class="EmphasisTypeSmallCaps ">Sota</span>. On ImageNet top-1 classification Flamingo achieves 76.0% compared to a fine-tuned <span class="EmphasisTypeSmallCaps ">Sota</span> of 91.0%. The test array on video contains 9 benchmarks, eight of whom require free form text answers and one benchmark (Kinetics 700) needs classification. On all eight free-form benchmarks Flamingo can increase few-shot <span class="EmphasisTypeSmallCaps ">Sota</span>, often by a huge margin. On four of these benchmarks Flamingo even exceeds the fine-tuned results. This is even more remarkable as Flamingo uses only 32 task-specific examples which is around 1000 times less task-specific training data than current state-of-the-art.</p><p class="Para" id="Par212">Flamingo can be fine-tuned on specific benchmarks to increase performance. During fine-tuning, the frozen model parts are not changed. When fine-tuning on 9 example tasks Flamingo could increase fine-tuned <span class="EmphasisTypeSmallCaps ">Sota</span> on five of these tasks. This shows that by fine-tuning the 10B free parameters of the model, the performance can in many cases be increase to new levels.</p></section>
<section class="Section2 RenderAsSection2" id="Sec27"><h3 class="Heading"><span class="HeadingNumber">7.3.4 </span>Generating Videos from Text</h3><p class="Para" id="Par213">The creation of videos following a textual description is an important issue, e.g. for education or illustration of dynamic content. While there are a number of models for describing images and videos through text, there are not many proposals for the other direction. The concepts for encoding text and videos are similar to the captioning of videos. The quality of generated videos can be judged by several measures comparing the similarity of actual and generated videos. The <em class="EmphasisTypeItalic ">FVD</em><span id="ITerm148"/> (Fréchet Video Distance) is the spatiotemporal extension of the Fréchet Inception Distance (FID) (Sect. <span class="InternalRef"><a href="#Sec18">7.2.6</a></span>), and is sensitive to visual quality, temporal coherence and diversity of samples.</p><p class="Para" id="Par214">The <strong class="EmphasisTypeBold ">Video Transformer</strong><span id="ITerm149"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR172" role="doc-biblioref">172</a></span>] generalizes the one-dimensional transformer encoder-decoder to videos. A video is represented as <span class="InlineEquation" id="IEq8"><img alt="$${\boldsymbol {x}}\in \mathbb {R}^{h\times w\times s\times d}$$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq8.png" style="width:6.31em"/></span>, where <em class="EmphasisTypeItalic ">h</em> and <em class="EmphasisTypeItalic ">w</em> denote the number of tokens in the spatial height and width, <em class="EmphasisTypeItalic ">s</em> denotes the number of tokens in the temporal axis, and <em class="EmphasisTypeItalic ">d</em> is the number of channels (e.g. colors). The video is partitioned into small 3D blocks in time and space. Self-attention is applied separately with each block. To allow direct information exchange between blocks, the block sizes between each layer are varied. The blocks contain 4 frames with a spatial resolution 32 × 32. Self-attention varies between 1 and 32 in different layers and dimensions. The largest model has a hidden size of 2048, 8 layers and 373M parameters. On the BAIR Robot Pushing data [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>] the model achieved an <em class="EmphasisTypeItalic ">FVD</em><span id="ITerm150"/> (Fréchet Video Distance) score [<span class="CitationRef"><a epub:type="biblioref" href="#CR167" role="doc-biblioref">167</a></span>] of 94. which was <span class="EmphasisTypeSmallCaps ">Sota</span> at the time of publication.</p><p class="Para" id="Par215"><strong class="EmphasisTypeBold ">NÜWA</strong><span id="ITerm151"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR175" role="doc-biblioref">175</a></span>] is a recent transformer encoder-decoder model that provides a solution for generating video from text. It uses a so called <em class="EmphasisTypeItalic ">3D Nearby Attention</em><span id="ITerm152"/> mechanism to capture the locality characteristic for both spatial and temporal axes. Image, video and text data is represented as tokens <span class="InlineEquation" id="IEq9"><img alt="$${\boldsymbol {x}}\in \mathbb {R}^{h\times w\times s\times d}$$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq9.png" style="width:6.31em"/></span>, where <em class="EmphasisTypeItalic ">h</em> and <em class="EmphasisTypeItalic ">w</em> denote the number of tokens in the spatial height and width, <em class="EmphasisTypeItalic ">s</em> denotes the number of tokens in the temporal axis, and <em class="EmphasisTypeItalic ">d</em> is the dimension of each token. The raw input regions are transformed into discrete tokens for image patches by a trainable VQ-GAN (Sect. <span class="InternalRef"><a href="#Sec15">7.2.3</a></span>). This GAN-based quantization module provides a much better image quality than VQ-VAE used by CogView (Sect. <span class="InternalRef"><a href="#Sec18">7.2.6</a></span>).</p><p class="Para" id="Par216">The model modifies attention computations and considers a local neighborhood with respect to width, height and temporal extent called 3D Nearby Self-Attention. Three different positional encoder embeddings are used for width, height and time. Each 336 × 336 pixel video frame is partitioned into 21 × 21 patches and 10 frames of a video are sampled with 2.5 frames per second. The size of the neighborhood in width, height and time is 3. The model is pre-trained on three tasks: Text-to-image for 2.9M text-image pairs from Conceptual Captions, video prediction with 727k videos from Moments in Time, and text-to-video generation for 241k text-video pairs.</p><div class="Para" id="Par217">For text-to-image generation, NÜWA is fine-tuned on the MS COCO dataset. Sixty images are generated for each text and the best image is selected by CLIP (Sect. <span class="InternalRef"><a href="#Sec16">7.2.4</a></span>). NÜWA outperforms CogView with an FID-0 of 12.9, which is good, as shown in Fig. <span class="InternalRef"><a href="#Fig27">7.27</a></span>, but worse than LAFITE (FID 8.1) and OFA (FID 10.5). For text-to-video, NÜWA is fine-tuned on the Kinetics dataset. Some frames of two generated examples are shown in Fig. <span class="InternalRef"><a href="#Fig28">7.28</a></span>. NÜWA achieves the best performance on the FID-img and FID-vid metrics with values of 28.5 and 7.0. Video prediction has to generate the sequence of the next frames of a video from a starting frame. On the BAIR Robot Pushing dataset, NÜWA achieves a new <span class="EmphasisTypeSmallCaps ">Sota</span> of 86.9 FVD score for this task.<figure class="Figure" id="Fig27"><div class="MediaObject" id="MO27"><img alt="" aria-describedby="d64e5554" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig27_HTML.png" style="width:34.07em"/><div class="TextObject" id="d64e5554"><p class="Para" id="Par276">A series of photos in row 1 represents a video clippings of British shorthair jumping over a couch. A series of 5 photos in row 2 represents coffee is being poured into a cup.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.27</span><p class="SimplePara">256 × 256 images generated from the text above the images by NÜWA [<span class="CitationRef"><a epub:type="biblioref" href="#CR175" role="doc-biblioref">175</a></span>] for the MS COCO benchmark. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR175" role="doc-biblioref">175</a></span>, p. 5]</p></div></figcaption></figure><figure class="Figure" id="Fig28"><div class="MediaObject" id="MO28"><img alt="" aria-describedby="d64e5572" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig28_HTML.png" style="width:34.07em"/><div class="TextObject" id="d64e5572"><p class="Para" id="Par277">4 photographs. 1, a green train is coming down the tracks. 2, a group of skiers are preparing to ski down a mountain. 3, a living area with a television and a table. 4, a child eating a birthday cake near some balloons.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.28</span><p class="SimplePara">Frames of two videos generated by NÜWA [<span class="CitationRef"><a epub:type="biblioref" href="#CR175" role="doc-biblioref">175</a></span>] from text (left) for the text-to-video task on the Kinetics dataset. Note that an input text like “running on the sea” has never been seen by the model. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR175" role="doc-biblioref">175</a></span>, p. 5]</p></div></figcaption></figure></div><div class="Para" id="Par218">NÜWA supports a number of other tasks. For image editing, it can reconstruct parts of an image. Alternatively, it can edit a marked image region according to a text, e.g. <em class="EmphasisTypeItalic ">“a horse is running on the grassland”</em>. Image sketches annotated with text are transformed to photos. This pattern can also be applied to videos, such that a video is generated from a series of images with annotated regions. Finally, it can change the contents in a video, e.g. modify the movements of a diver as shown in the lower row of Fig. <span class="InternalRef"><a href="#Fig29">7.29</a></span>. Moreover, a series of image sketches annotated with text can be transformed to a video. Further examples are shown here [<span class="CitationRef"><a epub:type="biblioref" href="#CR174" role="doc-biblioref">174</a></span>]. <strong class="EmphasisTypeBold ">GODIVA</strong><span id="ITerm153"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR176" role="doc-biblioref">176</a></span>] is a similar prior approach from the same authors based on VQ-VAE variational autoencoders.<figure class="Figure" id="Fig29"><div class="MediaObject" id="MO29"><img alt="" aria-describedby="d64e5611" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig29_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e5611"><p class="Para" id="Par278">A series of 4 photographs in row 1 represents a person playing gold on grass. A series of 4 photographs in row 2 represents a person running on the sea.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.29</span><p class="SimplePara">NÜWA [<span class="CitationRef"><a epub:type="biblioref" href="#CR175" role="doc-biblioref">175</a></span>] can edit videos. In the upper row the raw video is shown. In the lower row NÜWA gets the input “The diver is swimming to the bottom” and modifies the video accordingly. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR175" role="doc-biblioref">175</a></span>, p. 28]</p></div></figcaption></figure></div><div class="Para" id="Par219"><strong class="EmphasisTypeBold ">Imagen Video</strong><span id="ITerm154"/> is a recent high definition text-to-video model based on Imagen (Fig. <span class="InternalRef"><a href="#Fig17">7.17</a></span>). By a frozen T5 text encoder-decoder and a base video diffusion model a low-resolution video is generated. This is augmented by a cascade of video diffusion models that alternately increase spatial and temporal resolution [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>] to construct 128 realistic video frames at 24 frames per second with a resolution of 1280 × 768. Figure <span class="InternalRef"><a href="#Fig30">7.30</a></span> shows videos generated for text prompts by Imagen Video.<figure class="Figure" id="Fig30"><div class="MediaObject" id="MO30"><img alt="" aria-describedby="d64e5645" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig30_HTML.png" style="width:33.98em"/><div class="TextObject" id="d64e5645"><p class="Para" id="Par279">10 photographs of a scuba diver under the sea. The photos are captured from different angles.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.30</span><p class="SimplePara">Videos generated from the text prompts (below) by Imagen video [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>]. The model produces diverse and temporally coherent videos that are well matched to the given request. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>, p. 2]</p></div></figcaption></figure></div><section class="Section3 RenderAsSection3" id="Sec28"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par220"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par221">VideoBERT code <span class="ExternalRef"><a href="https://github.com/ammesatyajit/VideoBERT"><span class="RefSource">https://​github.​com/​ammesatyajit/​VideoBERT</span></a></span></p></li><li><p class="Para" id="Par222">COOT code <span class="ExternalRef"><a href="https://github.com/gingsi/coot-videotext"><span class="RefSource">https://​github.​com/​gingsi/​coot-videotext</span></a></span></p></li><li><p class="Para" id="Par223">DeCEMBERT code <span class="ExternalRef"><a href="https://github.com/zinengtang/decembert"><span class="RefSource">https://​github.​com/​zinengtang/​decembert</span></a></span></p></li><li><p class="Para" id="Par224">VATT code <span class="ExternalRef"><a href="https://github.com/google-research/google-research/tree/master/vatt"><span class="RefSource">https://​github.​com/​google-research/​google-research/​tree/​master/​vatt</span></a></span></p></li><li><p class="Para" id="Par225">Omnivore code <span class="ExternalRef"><a href="https://github.com/facebookresearch/omnivore"><span class="RefSource">https://​github.​com/​facebookresearch​/​omnivore</span></a></span></p></li><li><p class="Para" id="Par226">Video Transformer code <span class="ExternalRef"><a href="https://github.com/rakhimovv/lvt"><span class="RefSource">https://​github.​com/​rakhimovv/​lvt</span></a></span></p></li><li><p class="Para" id="Par227">MTV code and models <span class="ExternalRef"><a href="https://github.com/google-research/scenic"><span class="RefSource">https://​github.​com/​google-research/​scenic</span></a></span></p></li><li><p class="Para" id="Par228">NÜWA code <span class="ExternalRef"><a href="https://github.com/lucidrains/nuwa-pytorch"><span class="RefSource">https://​github.​com/​lucidrains/​nuwa-pytorch</span></a></span></p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec29"><h3 class="Heading"><span class="HeadingNumber">7.3.5 </span>Summary</h3><p class="Para" id="Par229">The processing of videos requires to integrate different modalities like image, text in the form of video captions, and speech possibly translated to text by ASR. Video processing introduces an additional time dimension to image processing. Furthermore, depth information and camera movements can be important. Since 2019 large scale transformers using self-supervised pre-training are the prevailing models for video processing. The models can solve different tasks, such as video captioning, action recognition, video question answering, video generation from text, prediction of next frames, video retrieval, audio-visual ASR, etc.</p><p class="Para" id="Par230">Existing cross-modal Foundation Models mainly focus on (1) improving model architecture, (2) utilizing more data, and (3) designing better pre-training tasks. Due to the limited input length, the video has to be partitioned into appropriate tokens. This ranges from aggregates over 30 clips (VideoBERT) over fixed video patches (VATT) to video patches with varying dimensions (COOT, MTV, Video Transformer). Some models (VideoBERT, DeCEMBERT) use CNN convolutions to generate low-level features. More common is the aggregation with VQ-VAE autoencoders or the GAN-bases VQ-GAN. Sometimes video and text are processed with separate PLMs and merged later (VATT). Alternatively, video and text tokens are concatenated and processed by single a PLM (Omnivore, Merlot). Transformers use attention over spatial and temporal dimensions, which is often localized to reduce computational effort.</p><p class="Para" id="Par231">The integration of different modalities is crucial. Text and language are associated by pre-training tasks, where masked video or text tokens have to be predicted using tokens from the other modality. CoVeR shows that performance can be enhanced when the model is simultaneously fine-tuned for video and image tasks. It is even possible to combine audio, text and video tokens.</p><p class="Para" id="Par232">The performance of video analysis models has taken a dramatic development. The action classification error on the Kinetics-400 benchmark has fallen within 1 year to 10.9% using Foundation Models, which is a drop of 33%. Despite the significant progress, <span class="EmphasisTypeSmallCaps ">Sota</span> methods fail to extract/capture all the complex spatiotemporal information present in videos. There is still much work to do for understanding the diversity of visual content in videos and the structure of associated textual descriptions.</p><p class="Para" id="Par233">Generating videos from captions is in its early stages, and only very short high-resolution videos can be generated. However, the current models are relatively small compared to the Foundation Models like GPT-3 or Gopher. Therefore, it can be expected that models with more parameters will see considerable performance improvements, as has been demonstrated by Imagen Video.</p><p class="Para" id="Par234">There is a trend to general-purpose models, like Nüwa that can handle multiple modalities of data and solve a number of tasks. Training with different media mutually supports the performance in different tasks. Flamingo with 80B parameters is based on a large pre-trained language model and a separately pre-trained vision encoder. In can process mixed sequences of images, text and videos. By building adapter modules and a cross-attention layer, the language model can include the results of the visual modalities and perform a variety of analysis tasks like visual question answering, image caption, etc. In addition, it can be instructed by few-shot prompts to solve many task without a specific fine-tuning.</p><p class="Para" id="Par235">Although Flamingo cannot generate images or videos corresponding to a caption, it is a step in the direction of multimodal Foundation Models, which promise to be a general-purpose tool of multimedia processing. By few-shot prompts they could solve thousands or millions of tasks. Substantial progress can be expected in this area, as ideas can be combined that were developed independently for different media. Further development directions are larger training data, which, however, are already quite large. In addition, the development of multilingual video models is a logical consequence of the current state of research in this area.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec30"><h2 class="Heading"><span class="HeadingNumber">7.4 </span>Controlling Dynamic Systems</h2><p class="Para" id="Par236">Foundation Models can process many types of sequences. These include sequential decision problems where the agent must choose an action based on a state. Subsequently, the environment generates a new state and a reward for the agent. This is repeated a number of times until the final sum of rewards is known. Then the task is to select the actions based on the states in such a way that the sum of rewards is maximal. This goal can be formulated as a sequence problem, and a PLM can be used to predict the next optimal action.</p><section class="Section2 RenderAsSection2" id="Sec31"><h3 class="Heading"><span class="HeadingNumber">7.4.1 </span>The Decision Transformer</h3><p class="Para" id="Par237">PLMs are able to predict sequences, e.g. the tokens of a text or video frames. Following this pattern, PLMs are also able to model the evolution of arbitrary states. <em class="EmphasisTypeItalic ">Reinforcement learning</em><span id="ITerm155"/> considers a system with <em class="EmphasisTypeItalic ">states</em><span id="ITerm156"/><em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em></sub>, <em class="EmphasisTypeItalic ">actions</em><span id="ITerm157"/><em class="EmphasisTypeItalic ">a</em><sub><em class="EmphasisTypeItalic ">t</em></sub>, and <em class="EmphasisTypeItalic ">rewards</em><span id="ITerm158"/><em class="EmphasisTypeItalic ">r</em><sub><em class="EmphasisTypeItalic ">t</em></sub> = <em class="EmphasisTypeItalic ">R</em>(<em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em></sub>, <em class="EmphasisTypeItalic ">a</em><sub><em class="EmphasisTypeItalic ">t</em></sub>) at a given time step <em class="EmphasisTypeItalic ">t</em>. Based on the current state, the agent selects an action, while the next state and reward are determined by the environment. The target of reinforcement learning is to learn a <em class="EmphasisTypeItalic ">policy</em><span id="ITerm159"/><em class="EmphasisTypeItalic ">a</em> = <em class="EmphasisTypeItalic ">π</em>(<em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em></sub>), which generates actions maximizing the expected sum of rewards <span class="InlineEquation" id="IEq10"><img alt="$$E(\sum _{t=1}^Tr_t)$$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq10.png" style="width:5em"/></span>. During online reinforcement learning the environment can be accessed, and for a given (<em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em></sub>, <em class="EmphasisTypeItalic ">r</em><sub><em class="EmphasisTypeItalic ">t</em></sub>, <em class="EmphasisTypeItalic ">a</em><sub><em class="EmphasisTypeItalic ">t</em></sub>) it returns the next state (<em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub>, <em class="EmphasisTypeItalic ">r</em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub>). In offline reinforcement learning there is only a limited set of observed trajectories from the environment. The latter setting is more difficult as the agent can no longer explore the environment.</p><div class="Para" id="Par238">The <strong class="EmphasisTypeBold ">Decision Transformer</strong><span id="ITerm160"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>] operates in an offline reinforcement setting. Instead of using the returns <em class="EmphasisTypeItalic ">r</em><sub><em class="EmphasisTypeItalic ">t</em></sub> directly, the Decision Transformer considers the <em class="EmphasisTypeItalic ">forward sum of rewards</em><span id="ITerm161"/><span class="InlineEquation" id="IEq11"><img alt="$$\hat {R}_t = \sum _{t'=t}^T r_{t'}$$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq11.png" style="width:6.26em"/></span>. Hence, a trajectory is represented as follows <div class="Equation NumberedEquation" id="Equ5"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \tau = \left(\hat{R}_1,s_1,a_1,\hat{R}_2,s_2,a_2,\ldots,\hat{R}_T,s_T,a_T\right) \end{aligned} $$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_Equ5.png" style="width:18.63em"/></div></div> <div class="EquationNumber">(7.5)</div></div></div> The input token embeddings for (<em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em></sub>, <em class="EmphasisTypeItalic ">r</em><sub><em class="EmphasisTypeItalic ">t</em></sub>, <em class="EmphasisTypeItalic ">a</em><sub><em class="EmphasisTypeItalic ">t</em></sub>) are computed with a linear layer, which is different for each modality (Fig. <span class="InternalRef"><a href="#Fig31">7.31</a></span>). If the state is an image, it is transformed by a convolutional encoder instead of a linear layer. Subsequently the embeddings are normalized by a layer normalization. For each time step with three inputs a position embedding is learned and added to the embeddings of that time step. The embeddings are then processed by an autoregressive GPT model, which predicts future actions by autoregressive modeling.<figure class="Figure" id="Fig31"><div class="MediaObject" id="MO31"><img alt="" aria-describedby="d64e6134" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig31_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e6134"><p class="Para" id="Par280">An autoregressive language model takes 3 distinct types of input, input tokens, type-specific preprocessing, and input embeddings. It then moves to states, actions, rewards, output embeddings, logistic classifiers, and action probabilities.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.31</span><p class="SimplePara">The Decision Transformer applies an autoregressive language model to the forward sums of rewards <span class="InlineEquation" id="IEq12"><img alt="$$\hat {R}_t$$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq12.png" style="width:1.32em"/></span>, states <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em></sub> and actions <em class="EmphasisTypeItalic ">a</em><sub><em class="EmphasisTypeItalic ">t</em></sub>. In the example the state is given in the form of video frames, e.g. for the Pong game. The model predicts the next action in the trajectory conditional to a given forward sums of rewards [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par239">The training was based on a dataset of observed trajectories. From these trajectories minibatches of length <em class="EmphasisTypeItalic ">K</em> were sampled. Then the GPT model for each <em class="EmphasisTypeItalic ">t</em> = 1, …, <em class="EmphasisTypeItalic ">K</em> predicted <em class="EmphasisTypeItalic ">a</em><sub><em class="EmphasisTypeItalic ">t</em></sub> given a trajectory up to <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em></sub>. As a loss function the cross-entropy loss was used for discrete actions with the target to increase the probability of the actual action at time <em class="EmphasisTypeItalic ">t</em>. For continuous actions, e.g. a speed, the mean squared error was used as loss to minimize the square difference to the observed control value. It was not necessary to predict states or the forward sum of rewards.</p><p class="Para" id="Par240">For the application to a starting state <em class="EmphasisTypeItalic ">s</em><sub>1</sub>, a target forward sum of rewards <span class="InlineEquation" id="IEq13"><img alt="$$\hat {R}_1$$" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Chapter_TeX_IEq13.png" style="width:1.24em"/></span> based on the desired performance (or even maximum possible return) is specified. After the generated action <em class="EmphasisTypeItalic ">a</em><sub>1</sub> is executed, the target return is reduced by the achieved reward and the next state <em class="EmphasisTypeItalic ">s</em><sub>2</sub> is determined. This process of generating actions and applying them to get the next forward sum of rewards and the next state is repeated until the trajectory ends. Note that the actual forward sum of rewards should be close to the desired performance specified before. Although the model is only trained on randomly selected subsequences, it can learn to ‘merge’ subsequences from different training trajectories in order to produce optimal trajectories at test time. Obviously a large set of subsequences has to evaluated during training to arrive at good solutions.</p><p class="Para" id="Par241">The <em class="EmphasisTypeItalic ">Atari benchmark</em><span id="ITerm162"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>] has discrete actions, uses four video frames as state descriptions, and processes these frames by a convolutional encoder. Only 1% of the available data is used. On four Atari tasks (Breakout, Qbert, Pong, and Seaquest) usually a context length of <em class="EmphasisTypeItalic ">K</em> = 30 is taken into account. With the exception of Qbert, Decision Transformer is competitive with state of the art methods, and for two games it reaches the best results (Breakout, Seaquest). The most effective alternative is the <em class="EmphasisTypeItalic ">CQL</em><span id="ITerm163"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR87" role="doc-biblioref">87</a></span>] Q-learner.</p><p class="Para" id="Par242">The <em class="EmphasisTypeItalic ">D4RL benchmark</em><span id="ITerm164"/> simulates simple robots (HalfCheetah, Hopper, and Walker) which are controlled by continuous-valued actions. On this benchmark Decision transformer in most cases achieves better results than the alternative approaches and has the highest average performance. Again CQL is the best alternative.</p><div class="Para" id="Par243">The authors evaluate the performance of approaches for an environment, where it is necessary to propagate rewards over a long time period. The <em class="EmphasisTypeItalic ">Key-to-Door benchmark</em><span id="ITerm165"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR104" role="doc-biblioref">104</a></span>] has three phases: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par244">in the first phase, the agent is placed in a room with a key;</p></li><li><p class="Para" id="Par245">then, the agent is placed in an empty room;</p></li><li><p class="Para" id="Par246">and finally, the agent is placed in a room with a door.</p></li></ul></div></div><p class="Para" id="Par247">The agent receives a binary reward when reaching the door in the third phase, but only if he picked up the key in the first phase. On this benchmark Decision Transformer and related methods clearly outperform Q-learning approaches, which cannot effectively propagate rewards over a long horizon.</p><p class="Para" id="Par248">Reid et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR136" role="doc-biblioref">136</a></span>] modify the details of the decision transformer yielding improved performance. Kumar et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR86" role="doc-biblioref">86</a></span>] show by theoretical analysis that offline reinforcement learning—as done by the decision transformer—enjoys better guarantees on long-horizon tasks than simply cloning the behavior of experts. This especially holds in the case of sufficiently noisy data.</p></section>
<section class="Section2 RenderAsSection2" id="Sec32"><h3 class="Heading"><span class="HeadingNumber">7.4.2 </span>The GATO Model for Text, Images and Control</h3><p class="Para" id="Par249"><strong class="EmphasisTypeBold ">GATO</strong><span id="ITerm166"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR134" role="doc-biblioref">134</a></span>] is a Foundation Model, which has been trained on about 600 different tasks, including text generation, image captioning, stacking physical blocks with a robot arm and playing Atari console games. Depending on the context, it independently decides which tokens to generate: Text, torques for joints, keystrokes, or another variant of the output within its comparatively extensive possibilities.</p><div class="Para" id="Par250">Depending on the modality the input is tokenized <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par251">Text is encoded via SentencePiece with 32,000 tokens.</p></li><li><p class="Para" id="Par252">Images are transformed into sequences of non-overlapping 16 × 16 images patches similar to the vision transformer (Sect. <span class="InternalRef"><a href="#Sec14">7.2.2</a></span>).</p></li><li><p class="Para" id="Par253">Discrete values, e.g. Atari button presses, are flattened into sequences of integers in row-major order. The tokenized result is a sequence of integers within the range of [0, 1024].</p></li><li><p class="Para" id="Par254">Continuous values, e.g. proprioceptive inputs (sense of self-movement, force, and body position) or joint torques, are preprocessed and discretized in 1024 bins. The discrete integers are then shifted to the range of [32, 000, 33, 024].</p></li></ul></div></div><div class="Para" id="Par255">Tokens belonging to text, discrete- or continuous-valued observations, or actions for any time step are embedded into a learned vector embedding space using a lookup table. Learned position encodings are added for all tokens based on their local token position within their corresponding time step. Tokens belonging to image patches for any time step are embedded using a single ResNet [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>] block to obtain a vector per image patch. In addition, a learnable within-image position encoding vector is added (Fig. <span class="InternalRef"><a href="#Fig32">7.32</a></span>).<figure class="Figure" id="Fig32"><div class="MediaObject" id="MO32"><img alt="" aria-describedby="d64e6324" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig32_HTML.png" style="width:34.5em"/><div class="TextObject" id="d64e6324"><p class="Para" id="Par307">A block diagram of Atari system components, include frames, actions, input, continuous action, and technical features like batched input and masked shifted targets.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.32</span><p class="SimplePara">Data from different tasks and modalities are converted to sequences, e.g. frames and actions from Atari games, text token sequences, images patch tokens, continuous sensory inputs and outputs. In Gato [<span class="CitationRef"><a epub:type="biblioref" href="#CR134" role="doc-biblioref">134</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR135" role="doc-biblioref">135</a></span>], a large decoder-only transformer processes the sequence. During training, specific variables, e.g. actions, are used to compute a loss. Image adapted from [<span class="CitationRef"><a epub:type="biblioref" href="#CR135" role="doc-biblioref">135</a></span>, fig.2], credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab4"><span class="RefSource">A.​3</span></a></span></p></div></figcaption></figure></div><p class="Para" id="Par256">Gato consists of a 1.2B parameter decoder-only transformer with 24 layers, and an embedding size of 2048. As in every language model, all tokens are predicted and therefore can be set as targets for training. Currently, only text tokens, discrete and continuous values, and actions are currently used as targets. As usual, the probabilities of the observed target tokens have to be maximized during training.</p><p class="Para" id="Par257">To focus GATO on a specific task, a prompt is used coming from a trajectory generated by the same source agent on the same task. GATO was trained on 596 different control tasks, among them the Atari benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>]. The authors included only “good” trajectories that yield at least 80% of the expert reward for the task. Moreover, GATO was trained on 8 vision and language tasks, e.g. image captioning with MS-COCO Captions [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>] and Conceptual Captions [<span class="CitationRef"><a epub:type="biblioref" href="#CR153" role="doc-biblioref">153</a></span>], as well as visual question-answering datasets. In addition, GATO is trained on the large MassiveText [<span class="CitationRef"><a epub:type="biblioref" href="#CR128" role="doc-biblioref">128</a></span>] data with 300 billion text tokens.</p><p class="Para" id="Par258">The performance of GATO has been evaluated for different applications. On the Atari benchmark, the model reached average human score or better for 23 of 51 Atari games. In a robot stacking benchmark, GATO achieved a comparable performance as the BC-IMP baseline [<span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>]. The model has only rudimentary dialog and caption functions, which is not surprising due to the small model size.</p><p class="Para" id="Par259">The Gato model is a first attempt to simultaneously solve text, image, and control tasks with the same Foundation Model. For control tasks it yielded respectable results while for the text and image tasks it had only mediocre performance. Perhaps it could benefit from the forward sum of rewards representation of the Decision Transformer. Actual Foundation Models have hundreds of billions of parameters and require a corresponding computing effort. If the GATO model is extended to this order of magnitude, its performance can be expected to improve accordingly.</p><section class="Section3 RenderAsSection3" id="Sec33"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par260"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par261">Decision Transformer code <span class="ExternalRef"><a href="https://sites.google.com/berkeley.edu/decision-transformer"><span class="RefSource">https://​sites.​google.​com/​berkeley.​edu/​decision-transformer</span></a></span></p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec34"><h3 class="Heading"><span class="HeadingNumber">7.4.3 </span>Summary</h3><p class="Para" id="Par262">Pre-trained language models can be applied to sequences with mixtures of element types. The Decision Transformer considers sequences of rewards, states and actions at specific time steps, which occur during a sequential decision problem, e.g. video game playing, robot control, or automatic driving. It models observed trajectories of these quantities. Instead of using the reward as input, the sum of the rewards up to the end of the trajectory is considered, which is the quantity to be maximized. For each type of input some preprocessing is performed to generate embeddings. The Decision Transformer is trained to predict the actions in short subsequences of 30 time steps.</p><p class="Para" id="Par263">During application, the desired forward sum of rewards can be set as a condition. Then, the model is able to stitch together the information from different subsequences in the training data to obtain near-optimal actions reaching a maximal sum of rewards. This was shown by extensive experiments with various benchmarks.</p><p class="Para" id="Par264">The GATO model demonstrates that PLMs at the same time can be used to solve reinforcement learning tasks simultaneously with text and image tasks. The model is trained with nearly 600 control benchmarks, 8 image tasks and on 300B text tokens. The model has only rudimentary text and image description capabilities, but performs relatively well on the Atari benchmark. It is only a proof of concept and could be improved by increasing the model size and, for instance, by using the forward sum of rewards.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec35"><h2 class="Heading"><span class="HeadingNumber">7.5 </span>Interpretation of DNA and Protein Sequences</h2><div class="Para" id="Par265">Deciphering the language of <span id="ITerm167">DNA</span> is one of the most important goals of biological research. The genetic code is universal and explains how DNA is translated into proteins. In contrast, the regulatory code, which determines when and how genes are expressed, varies between different cell types and organisms. This is similar to the polysemy and distant semantic relationships in natural language texts. <strong class="EmphasisTypeBold ">DNABERT</strong> [<span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>]<span id="ITerm168"/> tokenizes the DNA sequence into overlapping 3-grams and trains a standard BERT model to predict masked tokens (Fig. <span class="InternalRef"><a href="#Fig33">7.33</a></span>). After pre-training on a large set of DNA sequences, it can improve the <span class="EmphasisTypeSmallCaps ">Sota</span> by fine-tuning for many specific DNA prediction tasks. Among them are analysis of sequence motifs (DNA segments with biological relevance) and prediction of promoter regions (nucleotide sequence that enables regulated expression of a gene). MoDNA [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>] and GeneBERT<span id="ITerm169"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR106" role="doc-biblioref">106</a></span>] have similar functionality.<figure class="Figure" id="Fig33"><div class="MediaObject" id="MO33"><img alt="" aria-describedby="d64e6420" src="../images/528393_1_En_7_Chapter/528393_1_En_7_Fig33_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e6420"><p class="Para" id="Par308">A schematic of B E R T encoder block displays the result of the process displays the final embedding of logistic regression, token plus position embedding, masked sequence, tokenized sequence, and the original D N A sequence.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7.33</span><p class="SimplePara">DNABERT tokenizes the DNA sequence into overlapping 3-grams and trains a standard BERT model to predict masked tokens [<span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>]. The resulting model can be fine-tuned to many DNA interpretation tasks</p></div></figcaption></figure></div><p class="Para" id="Par266">Proteins are linear chains of amino acids linked by covalent bonds. Amino acids can be represented by an alphabet with 25 characters. The strings are ideally suited for many NLP methods [<span class="CitationRef"><a epub:type="biblioref" href="#CR111" role="doc-biblioref">111</a></span>]. <strong class="EmphasisTypeBold ">AminoBERT</strong> [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>]<span id="ITerm170"/> is a language model that predicts the 3D protein structure given a protein sequence as input. It also uses a natural method to describe polypeptide geometry that is rotation and translation invariant at the level of the polypeptide as a whole. On average, the model outperforms AlphaFold2<span id="ITerm171"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR80" role="doc-biblioref">80</a></span>] and RoseTTAFold<span id="ITerm172"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>] on orphan proteins and classes of engineered proteins, achieving up to a 106-fold reduction in computational time.</p><p class="Para" id="Par267">There are a number of other models with similar results [<span class="CitationRef"><a epub:type="biblioref" href="#CR97" role="doc-biblioref">97</a></span>], e.g., the protein language model <strong class="EmphasisTypeBold ">ESMFold</strong><span id="ITerm173"/>. It generates embeddings that can be used in downstream tasks, for example to capture the structural properties of proteins. A model with 15B parameters can predict the three-dimensional structure of a protein at the resolution of individual atoms.</p><div class="FormalPara FormalParaRenderingStyle3"><div class="Heading">Available Implementations</div><div class="Para FirstParaInFormalPara" id="Par268"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par269">DNABERT code and models <span class="ExternalRef"><a href="https://github.com/jerryji1993/DNABERT"><span class="RefSource">https://​github.​com/​jerryji1993/​DNABERT</span></a></span></p></li><li><p class="Para" id="Par270">GeneBERT code and models <span class="ExternalRef"><a href="https://github.com/ZovcIfzm/GeneBERT/tree/main/GeneBERT"><span class="RefSource">https://​github.​com/​ZovcIfzm/​GeneBERT/​tree/​main/​GeneBERT</span></a></span></p></li><li><p class="Para" id="Par271">ProteinBERT code and models <span class="ExternalRef"><a href="https://github.com/nadavbra/protein_bert"><span class="RefSource">https://​github.​com/​nadavbra/​protein_​bert</span></a></span></p></li><li><p class="Para" id="Par272">AlphaFold 2 code and models <span class="ExternalRef"><a href="https://github.com/deepmind/alphafold"><span class="RefSource">https://​github.​com/​deepmind/​alphafold</span></a></span></p></li><li><p class="Para" id="Par273">RoseTTAFold code and models <span class="ExternalRef"><a href="https://github.com/RosettaCommons/RoseTTAFold"><span class="RefSource">https://​github.​com/​RosettaCommons/​RoseTTA Fold</span></a></span></p></li><li><p class="Para" id="Par274">ESMFold code and models <span class="ExternalRef"><a href="https://github.com/facebookresearch/esm"><span class="RefSource">https://​github.​com/​facebookresearch​/​esm</span></a></span></p></li></ul></div></div></div><section class="Section2 RenderAsSection2" id="Sec36"><h3 class="Heading"><span class="HeadingNumber">7.5.1 </span>Summary</h3><p class="Para" id="Par275">Foundation Models can also be applied to DNA and protein sequences to derive contextual embeddings of the sequence elements. By this approach, the models are able to accumulate much knowledge about these sequences and achieve <span class="EmphasisTypeSmallCaps ">Sota</span> performance across various downstream tasks, largely surpassing existing tools. The models can help to predict the 3-D structure of the protein. This is crucial for its function and may be instrumental in developing active substances to influence it.</p></section>
</section>
<div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a><p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p><p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p></div><aside aria-labelledby="Bib1Heading" class="Bibliography" id="Bib1"><div epub:type="bibliography" role="doc-bibliography"><div class="Heading" id="Bib1Heading">References</div><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">T. Afouras, J. S. Chung, and A. Zisserman. “LRS3-TED: A Large-Scale Dataset for Visual Speech Recognition”. 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1809.00496</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">H. Akbari, L. Yuan, R. Qian, W.-H. Chuang, S.-F. Chang, Y. Cui, and B. Gong. “VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text”. Dec. 6, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2104.11178 [cs, eess]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">J.-B. Alayrac et al. <em class="EmphasisTypeItalic ">Flamingo: A Visual Language Model for Few-Shot Learning</em>. Apr. 29, 2022. <span class="EmphasisTypeSmallCaps ">doi</span>: <span class="ExternalRef"><a href="https://doi.org/10.48550/arXiv.2204.14198"><span class="RefSource">https://​doi.​org/​10.​48550/​arXiv.​2204.​14198</span></a></span>. arXiv: <span class="EmphasisFontCategoryNonProportional ">2204.14198 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">P. Ammanabrolu and M. Riedl. “Learning Knowledge Graph-Based World Models of Textual Environments”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst.</em> 34 (2021), pp. 3720–3731.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">W. An, Y. Guo, Y. Bian, H. Ma, J. Yang, C. Li, and J. Huang. “MoDNA: Motif-Oriented Pre-Training for DNA Language Model”. In: <em class="EmphasisTypeItalic ">Proc. 13th ACM Int. Conf. Bioinforma. Comput. Biol. Health Inform.</em> BCB ’22. New York, NY, USA: Association for Computing Machinery, Aug. 7, 2022, pp. 1–5. <span class="EmphasisTypeSmallCaps ">isbn</span>: 978-1-4503-9386-7. <span class="EmphasisTypeSmallCaps ">doi</span>: <span class="ExternalRef"><a href="https://doi.org/10.1145/3535508.3545512"><span class="RefSource">https://​doi.​org/​10.​1145/​3535508.​3545512</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">P. Anderson. <em class="EmphasisTypeItalic ">VQA2VLN Tutorial 2021. From VQA to VLN: Recent Advances in Vision-and-Language Research</em>. June 20, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://vqa2vln-tutorial.github.io/"><span class="RefSource">https://​vqa2vln-tutorial.​github.​io/​</span></a></span> (visited on 03/25/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">I. Anokhin, K. Demochkin, T. Khakhulin, G. Sterkin, V. Lempitsky, and D. Korzhenkov. “Image Generators with Conditionally-Independent Pixel Synthesis”. In: <em class="EmphasisTypeItalic ">Proc. IEEECVF Conf. Comput. Vis. Pattern Recognit.</em> 2021, pp. 14278–14287.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">M. Baek et al. “Accurate Prediction of Protein Structures and Interactions Using a Three-Track Neural Network”. In: <em class="EmphasisTypeItalic ">Science</em> 373.6557 (Aug. 20, 2021), pp. 871–876. <span class="EmphasisTypeSmallCaps ">doi</span>: <span class="ExternalRef"><a href="https://doi.org/10.1126/science.abj8754"><span class="RefSource">https://​doi.​org/​10.​1126/​science.​abj8754</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli. “Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language”. Jan. 22, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2202.03555</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">A. Baevski, H. Zhou, A. Mohamed, and M. Auli. “Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2006.11477</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">H. Bao, L. Dong, and F. Wei. “Beit: Bert Pre-Training of Image Transformers”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2106.08254</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">R. Beaumont. <em class="EmphasisTypeItalic ">LAION-5B: A NEW ERA OF OPEN LARGE-SCALE MULTI-MODAL DATASETS — LAION</em>. Aug. 8, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://laion.ai/blog/laion-5b"><span class="RefSource">https://​laion.​ai/​blog/​laion-5b</span></a></span> (visited on 08/29/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. “The Arcade Learning Environment: An Evaluation Platform for General Agents”. In: <em class="EmphasisTypeItalic ">J. Artif. Intell. Res.</em> 47 (2013), pp. 253–279.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">Ş. Bilici. <em class="EmphasisTypeItalic ">A Survey On Music Generation With Deep Neural Networks</em>. Safak’s Blog. Oct. 15, 2020. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://safakkbilici.github.io/a-survey-on-music-generation/"><span class="RefSource">https://​safakkbilici.​github.​io/​a-survey-on-music-generation/​</span></a></span> (visited on 03/03/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">A. Blattmann, R. Rombach, K. Oktay, and B. Ommer. <em class="EmphasisTypeItalic ">Retrieval-Augmented Diffusion Models</em>. Apr. 26, 2022. <span class="EmphasisTypeSmallCaps ">doi</span>: <span class="ExternalRef"><a href="https://doi.org/10.48550/arXiv.2204.11824"><span class="RefSource">https://​doi.​org/​10.​48550/​arXiv.​2204.​11824</span></a></span>. arXiv: <span class="EmphasisFontCategoryNonProportional ">2204.11824 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">A. Brock, S. De, S. L. Smith, and K. Simonyan. “High-Performance Large-Scale Image Recognition Without Normalization”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2102.06171</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">S. Cable. “Alexa, Read Me This Book in My Grandmother’s Voice”. In: <em class="EmphasisTypeItalic ">news</em> (June 24, 2022). <span class="EmphasisTypeSmallCaps ">issn</span>: 0140-0460. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.thetimes.co.uk/article/alexa-read-me-this-book-in-my-grandmothers-voice-cfdtjbjcc"><span class="RefSource">https://​www.​thetimes.​co.​uk/​article/​alexa-read-me-this-book-in-my-grandmothers-voice-cfdtjbjcc</span></a></span> (visited on 07/08/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">R. Cai, J. Yuan, B. Xu, and Z. Hao. “SADGA: Structure-Aware Dual Graph Aggregation Network for Text-to-SQL”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst.</em> 34 (2021), pp. 7664–7676.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">J. Cao, Z. Gan, Y. Cheng, L. Yu, Y.-C. Chen, and J. Liu. “Behind the Scene: Revealing the Secrets of Pre-Trained Vision-and-Language Models”. In: <em class="EmphasisTypeItalic ">Eur. Conf. Comput. Vis.</em> Springer, 2020, pp. 565–580.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">Y.-H. Cao, H. Yu, and J. Wu. “Training Vision Transformers with Only 2040 Images”. Jan. 25, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2201.10728 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">W. Chan, D. Park, C. Lee, Y. Zhang, Q. Le, and M. Norouzi. “Speechstew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2104.02133</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">H. Chefer, S. Gur, and L. Wolf. “Transformer Interpretability beyond Attention Visualization”. In: <em class="EmphasisTypeItalic ">Proc. IEEECVF Conf. Comput. Vis. Pattern Recognit.</em> 2021, pp. 782–791.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">L. Chen et al. “Decision Transformer: Reinforcement Learning via Sequence Modeling”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst.</em> 34 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">S.-J. Chen, A. S. Subramanian, H. Xu, and S. Watanabe. “Building State-of-the-Art Distant Speech Recognition Using the CHiME-4 Challenge with a Setup of Speech Enhancement Baseline”. 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1803.10109</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">W. Chen, M.-W. Chang, E. Schlinger, W. Wang, and W. W. Cohen. “Open Question Answering over Tables and Text”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.10439</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollár, and C. L. Zitnick. “Microsoft Coco Captions: Data Collection and Evaluation Server”. 2015. arXiv: <span class="EmphasisFontCategoryNonProportional ">1504.00325</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">R. Child, S. Gray, A. Radford, and I. Sutskever. “Generating Long Sequences with Sparse Transformers”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1904.10509</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">J. Cho, J. Lu, D. Schwenk, H. Hajishirzi, and A. Kembhavi. “X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2009.11278</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">R. Chowdhury, N. Bouatta, and S. Biswas. “Single-Sequence Protein Structure Prediction Using a Language Model and Deep Learning”. In: <em class="EmphasisTypeItalic ">Nat. Biotechnol.</em> (Oct. 3, 2022), pp. 1–7. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.nature.com/articles/s41587-022-01432-w"><span class="RefSource">https://​www.​nature.​com/​articles/​s41587-022-01432-w</span></a></span> (visited on 10/14/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">Y.-S. Chuang, C.-L. Liu, H.-Y. Lee, and L.-s. Lee. “SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering”. Aug. 11, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">1910.11559 [cs, eess]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu. “W2v-Bert: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2108.06209</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">coco. <em class="EmphasisTypeItalic ">Papers with Code - COCO Captions Benchmark (Image Captioning)</em>. Mar. 6, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://paperswithcode.com/sota/image-captioning-on-coco-captions"><span class="RefSource">https://​paperswithcode.​com/​sota/​image-captioning-on-coco-captions</span></a></span> (visited on 03/06/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">D. Damen et al. “Rescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-KITCHENS-100”. In: <em class="EmphasisTypeItalic ">Int. J. Comput. Vis.</em> 130.1 (2022), pp. 33–55.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">K. Desai, G. Kaul, Z. Aysola, and J. Johnson. “RedCaps: Web-curated Image-Text Data Created by the People, for the People”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2111.11431</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">P. Dhariwal. <em class="EmphasisTypeItalic ">OpenAI Jukebox Sample Explorer</em>. 2020. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://jukebox.openai.com/"><span class="RefSource">https://​jukebox.​openai.​com/​</span></a></span> (visited on 03/03/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever. “Jukebox: A Generative Model for Music”. Apr. 30, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2005.00341 [cs, eess, stat]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">P. Dhariwal and A. Nichol. “Diffusion Models Beat Gans on Image Synthesis”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst.</em> 34 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">S. Di et al. “Video Background Music Generation with Controllable Music Transformer”. In: <em class="EmphasisTypeItalic ">Proc. 29th ACM Int. Conf. Multimed.</em> 2021, pp. 2037–2045.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">D. Ding, F. Hill, A. Santoro, M. Reynolds, and M. Botvinick. “Attention over Learned Object Embeddings Enables Complex Visual Reasoning”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst.</em> 34 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">M. Ding et al. “CogView: Mastering Text-to-Image Generation via Transformers”. Nov. 5, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2105.13290 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">A. Dosovitskiy and T. Brox. “Generating Images with Perceptual Similarity Metrics Based on Deep Networks”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst.</em> 29 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">A. Dosovitskiy et al. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.11929</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">Y. Du, Z. Liu, J. Li, and W. X. Zhao. “A Survey of Vision-Language Pre-Trained Models”. 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2202.10936</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">F. Ebert, C. Finn, A. X. Lee, and S. Levine. “Self-Supervised Visual Planning with Temporal Skip Connections.” In: <em class="EmphasisTypeItalic ">CoRL.</em> 2017, pp. 344–356.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">P. Esser, R. Rombach, and B. Ommer. “Taming Transformers for High-Resolution Image Synthesis”. In: <em class="EmphasisTypeItalic ">Proc. IEEECVF Conf. Comput. Vis. Pattern Recognit.</em> 2021, pp. 12873–12883.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">N. Fei et al. “WenLan 2.0: Make AI Imagine via a Multimodal Foundation Model”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2110.14378</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">W. Feller. “On the Theory of Stochastic Processes, with Particular Reference to Applications”. In: <em class="EmphasisTypeItalic ">Proc. First Berkeley Symp. Math. Stat. Probab.</em> University of California Press, 1949, pp. 403–432.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">O. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh, and Y. Taigman. <em class="EmphasisTypeItalic ">Greater creative control for AI image generation</em>. July 14, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://ai.facebook.com/blog/greater-creative-control-for-ai-image-generation/"><span class="RefSource">https://​ai.​facebook.​com/​blog/​greater-creative-control-for-ai-image-generation/​</span></a></span> (visited on 07/29/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">L. Gao et al. “The Pile: An 800GB Dataset of Diverse Text for Language Modeling”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2101.00027</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">K. Gavrilyuk, R. Sanford, M. Javan, and C. G. Snoek. “Actor-Transformers for Group Activity Recognition”. In: Proc. IEEECVF <em class="EmphasisTypeItalic ">Conf. Comput. Vis. Pattern Recognit.</em> 2020, pp. 839–848.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">S. Ging, M. Zolfaghari, H. Pirsiavash, and T. Brox. “COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning”. Nov. 1, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2011.00597</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">R. Girdhar, M. Singh, N. Ravi, L. van der Maaten, A. Joulin, and I. Misra. “Omnivore: A Single Model for Many Visual Modalities”. 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2201.08377</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">I. Goodfellow et al. “Generative Adversarial Nets”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst.</em> 2014, pp. 2672–2680.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">google. <em class="EmphasisTypeItalic ">AVA: A Video Dataset of Atomic Visual Action</em>. 2020. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://research.google.com/ava/"><span class="RefSource">https://​research.​google.​com/​ava/​</span></a></span> (visited on 03/12/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">R. Goyal et al. “The” Something Something” Video Database for Learning and Evaluating Visual Common Sense”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Int. Conf. Comput. Vis.</em> 2017, pp. 5842–5850.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. “Making the v in Vqa Matter: Elevating the Role of Image Understanding in Visual Question Answering”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</em> 2017, pp. 6904–6913.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber. “Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks”. In: <em class="EmphasisTypeItalic ">Proc. 23rd Int. Conf. Mach. Learn.</em> 2006, pp. 369–376.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">A. Gu, K. Goel, and C. Ré. “Efficiently Modeling Long Sequences with Structured State Spaces”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2111.00396</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">A. Gulati et al. “Conformer: Convolution-augmented Transformer for Speech Recognition”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2005.08100</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">Y. Guo et al. “From General to Specific: Informative Scene Graph Generation via Balance Adjustment”. In: <em class="EmphasisTypeItalic ">Proc. IEEECVF Int. Conf. Comput. Vis.</em> 2021, pp. 16383–16392.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">61.</div><div class="CitationContent" id="CR61">K. Gupta, J. Lazarow, A. Achille, L. S. Davis, V. Mahadevan, and A. Shrivastava. “Layout-transformer: Layout Generation and Completion with Self-Attention”. In: <em class="EmphasisTypeItalic ">Proc. IEEECVF Int. Conf. Comput. Vis.</em> 2021, pp. 1004–1014.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">62.</div><div class="CitationContent" id="CR62">A. M. Hafiz, S. A. Parah, and R. U. A. Bhat. “Attention Mechanisms and Deep Learning for Machine Vision: A Survey of the State of the Art”. June 3, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2106.07550</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">63.</div><div class="CitationContent" id="CR63">K. He, X. Zhang, S. Ren, and J. Sun. “Deep Residual Learning for Image Recognition”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</em> 2016, pp. 770–778.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">64.</div><div class="CitationContent" id="CR64">M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. “Gans Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst.</em> 30 (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">65.</div><div class="CitationContent" id="CR65">J. Ho, A. Jain, and P. Abbeel. “Denoising Diffusion Probabilistic Models”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst.</em> 33 (2020), pp. 6840–6851.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">66.</div><div class="CitationContent" id="CR66">J. Ho et al. “Imagen Video: High Definition Video Generation with Diffusion Models”. 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2210.02303</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">67.</div><div class="CitationContent" id="CR67">M. Z. Hossain, F. Sohel, M. F. Shiratuddin, and H. Laga. “A Comprehensive Survey of Deep Learning for Image Captioning”. In: <em class="EmphasisTypeItalic ">ACM Comput. Surv. CsUR</em> 51.6 (2019), pp. 1–36.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">68.</div><div class="CitationContent" id="CR68">W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed. “Hubert: Self-supervised Speech Representation Learning by Masked Prediction of Hidden Units”. In: <em class="EmphasisTypeItalic ">IEEEACM Trans. Audio Speech Lang. Process.</em> 29 (2021), pp. 3451–3460.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">69.</div><div class="CitationContent" id="CR69">X. Hu, X. Yin, K. Lin, L. Wang, L. Zhang, J. Gao, and Z. Liu. “Vivo: Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-Training”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2009.13682</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">70.</div><div class="CitationContent" id="CR70">X. Hu, X. Yin, K. Lin, L. Zhang, J. Gao, L. Wang, and Z. Liu. “VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning”. In: <em class="EmphasisTypeItalic ">Proc. AAAI Conf. Artif. Intell.</em> Vol. 35. 2. 2021, pp. 1575–1583.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">71.</div><div class="CitationContent" id="CR71">C.-Z. A. Huang et al. “Music Transformer: Generating Music with Long-Term Structure”. In: <em class="EmphasisTypeItalic ">Int. Conf. Learn. Represent</em>. ICLR. 2019.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">72.</div><div class="CitationContent" id="CR72">Y. Huang, H. Xue, B. Liu, and Y. Lu. “Unifying Multimodal Transformer for Bi-Directional Image and Text Generation”. In: <em class="EmphasisTypeItalic ">Proc. 29th ACM Int. Conf. Multimed.</em> 2021, pp. 1138–1147.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">73.</div><div class="CitationContent" id="CR73">S. Islam, A. Dash, A. Seum, A. H. Raj, T. Hossain, and F. M. Shah. “Exploring Video Captioning Techniques: A Comprehensive Survey on Deep Learning Methods”. In: <em class="EmphasisTypeItalic ">SN Comput. Sci.</em> 2.2 (2021), pp. 1–28.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">74.</div><div class="CitationContent" id="CR74">K. Ito and L. Johnson. <em class="EmphasisTypeItalic ">The LJ Speech Dataset.</em> 2017. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://keithito.com/LJ-Speech-Dataset"><span class="RefSource">https://​keithito.​com/​LJ-Speech-Dataset</span></a></span> (visited on 03/24/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">75.</div><div class="CitationContent" id="CR75">E. Jang, S. Gu, and B. Poole. “Categorical Reparameterization with Gumbel-Softmax”. 2016. arXiv: <span class="EmphasisFontCategoryNonProportional ">1611.01144</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">76.</div><div class="CitationContent" id="CR76">Y. Ji, Z. Zhou, H. Liu, and R. V. Davuluri. “DNABERT: Pre-Trained Bidirectional Encoder Representations from Transformers Model for DNA-language in Genome”. In: <em class="EmphasisTypeItalic ">Bioinformatics</em> 37.15 (2021), pp. 2112–2120.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">77.</div><div class="CitationContent" id="CR77">C. Jia and Y. Yang. <em class="EmphasisTypeItalic ">ALIGN: Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision</em>. Google AI Blog. May 11, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="http://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision.html"><span class="RefSource">http://​ai.​googleblog.​com/​2021/​05/​align-scaling-up-visual-and-vision.​html</span></a></span> (visited on 06/08/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">78.</div><div class="CitationContent" id="CR78">Y. Jia. High-Quality, <em class="EmphasisTypeItalic ">Robust and Responsible Direct Speech-to-Speech Translation</em>. Google AI Blog. Sept. 23, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="http://ai.googleblog.com/2021/09/high-quality-robust-and-responsible.html"><span class="RefSource">http://​ai.​googleblog.​com/​2021/​09/​high-quality-robust-and-responsible.​html</span></a></span> (visited on 10/25/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">79.</div><div class="CitationContent" id="CR79">D. Jin, Z. Jin, and R. Mihalcea. “Deep Learning for Text Attribute Transfer: A Survey”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2011.00416</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">80.</div><div class="CitationContent" id="CR80">J. Jumper et al. “Highly Accurate Protein Structure Prediction with AlphaFold”. In: <em class="EmphasisTypeItalic ">Nature</em> 596.7873 (7873 Aug. 2021), pp. 583–589. <span class="EmphasisFontCategoryNonProportional ">issn</span>: 1476-4687. <span class="EmphasisTypeSmallCaps ">doi</span>: <span class="ExternalRef"><a href="https://doi.org/10.1038/s41586-021-03819-2"><span class="RefSource">https://​doi.​org/​10.​1038/​s41586-021-03819-2</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">81.</div><div class="CitationContent" id="CR81">T. Kano, S. Sakti, and S. Nakamura. “Transformer-Based Direct Speech-to-Speech Translation with Transcoder”. In: (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">82.</div><div class="CitationContent" id="CR82">T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila. “Analyzing and Improving the Image Quality of Stylegan”. In: <em class="EmphasisTypeItalic ">Proc. IEEECVF Conf. Comput. Vis. Pattern Recognit.</em> 2020, pp. 8110–8119.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">83.</div><div class="CitationContent" id="CR83">W. Kay et al. “The Kinetics Human Action Video Dataset”. 2017. arXiv: <span class="EmphasisFontCategoryNonProportional ">1705.06950</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">84.</div><div class="CitationContent" id="CR84">S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah. “Transformers in Vision: A Survey”. In: <em class="EmphasisTypeItalic ">ACM Comput. Surv.</em> (Jan. 6, 2022), p. 3505244. <span class="EmphasisFontCategoryNonProportional ">issn</span>: 0360-0300, 1557-7341. <span class="EmphasisTypeSmallCaps ">doi</span>: <span class="ExternalRef"><a href="https://doi.org/10.1145/3505244"><span class="RefSource">https://​doi.​org/​10.​1145/​3505244</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">85.</div><div class="CitationContent" id="CR85">K. Khurana and U. Deshpande. “Video Question-Answering Techniques, Benchmark Datasets and Evaluation Metrics Leveraging Video Captioning: A Comprehensive Survey.” In: <em class="EmphasisTypeItalic ">IEEE Access</em> (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">86.</div><div class="CitationContent" id="CR86">A. Kumar, J. Hong, A. Singh, and S. Levine. <em class="EmphasisTypeItalic ">When Should We Prefer Offline Reinforcement Learning Over Behavioral Cloning?</em> Apr. 12, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2204.05618 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">87.</div><div class="CitationContent" id="CR87">A. Kumar, A. Zhou, G. Tucker, and S. Levine. “Conservative Q-Learning for Offline Reinforcement Learning”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst.</em> 33 (2020), pp. 1179–1191.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">88.</div><div class="CitationContent" id="CR88">M. Kumar, D. Weissenborn, and N. Kalchbrenner. “Colorization Transformer”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2102.04432</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">89.</div><div class="CitationContent" id="CR89">K. Lakhotia et al. “Generative Spoken Language Modeling from Raw Audio”. Sept. 9, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2102.01192 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">90.</div><div class="CitationContent" id="CR90">A. X. Lee et al. “Beyond Pick-and-Place: Tackling Robotic Stacking of Diverse Shapes”. In: <em class="EmphasisTypeItalic ">5th Annu. Conf. Robot Learn.</em> 2021.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">91.</div><div class="CitationContent" id="CR91">H. Lee, U. Ullah, J.-S. Lee, B. Jeong, and H.-C. Choi. “A Brief Survey of Text Driven Image Generation and Maniulation”. In: <em class="EmphasisTypeItalic ">2021 IEEE Int. Conf. Consum. Electron.-Asia ICCE-Asia</em>. IEEE, 2021, pp. 1–4.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">92.</div><div class="CitationContent" id="CR92">Z. Leng, M. Tan, C. Liu, E. D. Cubuk, J. Shi, S. Cheng, and D. Anguelov. “PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions”. In: <em class="EmphasisTypeItalic ">Int. Conf. Learn. Represent.</em> 2021.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">93.</div><div class="CitationContent" id="CR93">M. Li et al. “CLIP-Event: Connecting Text and Images with Event Structures”. 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2201.05078</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">94.</div><div class="CitationContent" id="CR94">N. Li, S. Liu, Y. Liu, S. Zhao, and M. Liu. “Neural Speech Synthesis with Transformer Network”. In: <em class="EmphasisTypeItalic ">Proc. AAAI Conf. Artif. Intell.</em> Vol. 33. 01. 2019, pp. 6706–6713.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">95.</div><div class="CitationContent" id="CR95">X. Li et al. “Oscar: Object-semantics Aligned Pre-Training for Vision-Language Tasks”. In: <em class="EmphasisTypeItalic ">Eur. Conf. Comput. Vis.</em> Springer, 2020, pp. 121–137.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">96.</div><div class="CitationContent" id="CR96">J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte. “Swinir: Image Restoration Using Swin Transformer”. In: <em class="EmphasisTypeItalic ">Proc. IEEECVF Int. Conf. Comput. Vis.</em> 2021, pp. 1833–1844.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">97.</div><div class="CitationContent" id="CR97">Z. Lin et al. “Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction”. In: <em class="EmphasisTypeItalic ">bioRxiv</em> (2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">98.</div><div class="CitationContent" id="CR98">A. T. Liu, S.-W. Li, and H.-y. Lee. “Tera: Self-supervised Learning of Transformer Encoder Representation for Speech”. In: <em class="EmphasisTypeItalic ">IEEEACM Trans. Audio Speech Lang. Process.</em> 29 (2021), pp. 2351–2366.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">99.</div><div class="CitationContent" id="CR99">Z. Liu et al. “Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows”. In: <em class="EmphasisTypeItalic ">Proc. IEEECVF Int. Conf. Comput. Vis.</em> 2021, pp. 10012–10022.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">100.</div><div class="CitationContent" id="CR100">J. Lu, D. Batra, D. Parikh, and S. Lee. “Vilbert: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst.</em> 2019, pp. 13–23.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">101.</div><div class="CitationContent" id="CR101">M. Malik, M. K. Malik, K. Mehmood, and I. Makhdoom. “Automatic Speech Recognition: A Survey”. In: <em class="EmphasisTypeItalic ">Multimed. Tools Appl.</em> (2020), pp. 1–47.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">102.</div><div class="CitationContent" id="CR102">M. Malik, M. K. Malik, K. Mehmood, and I. Makhdoom. “Automatic Speech Recognition: A Survey”. In: <em class="EmphasisTypeItalic ">Multimed. Tools Appl.</em> 80.6 (2021), pp. 9411–9457.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">103.</div><div class="CitationContent" id="CR103">C. Mao, L. Jiang, M. Dehghani, C. Vondrick, R. Sukthankar, and I. Essa. “Discrete Representations Strengthen Vision Transformer Robustness”. Nov. 19, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2111.10493 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">104.</div><div class="CitationContent" id="CR104">T. Mesnard et al. “Counterfactual Credit Assignment in Model-Free Reinforcement Learning”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2011.09464</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">105.</div><div class="CitationContent" id="CR105">A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic. “HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips”. July 31, 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1906.03327 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">106.</div><div class="CitationContent" id="CR106">S. Mo et al. “Multi-Modal Self-supervised Pre-training for Regulatory Genome Across Cell Types”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2110.05231</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">107.</div><div class="CitationContent" id="CR107">M. Monfort et al. “Moments in Time Dataset: One Million Videos for Event Understanding”. In: <em class="EmphasisTypeItalic ">IEEE Trans. Pattern Anal. Mach. Intell.</em> 42.2 (2019), pp. 502–508.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">108.</div><div class="CitationContent" id="CR108">M. M. Naseer, K. Ranasinghe, S. H. Khan, M. Hayat, F. Shahbaz Khan, and M.-H. Yang. “Intriguing Properties of Vision Transformers”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst.</em> 34 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">109.</div><div class="CitationContent" id="CR109">A. Nichol et al. “Glide: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2112.10741</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">110.</div><div class="CitationContent" id="CR110">A. Q. Nichol and P. Dhariwal. “Improved Denoising Diffusion Probabilistic Models”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn.</em> PMLR, 2021, pp. 8162–8171.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">111.</div><div class="CitationContent" id="CR111">D. Ofer, N. Brandes, and M. Linial. “The Language of Proteins: NLP, Machine Learning &amp; Protein Sequences”. In: <em class="EmphasisTypeItalic ">Comput. Struct. Biotechnol. J.</em> 19 (2021), pp. 1750–1758.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">112.</div><div class="CitationContent" id="CR112">A. Oluwasammi et al. “Features to Text: A Comprehensive Survey of Deep Learning on Semantic Segmentation and Image Captioning”. In: <em class="EmphasisTypeItalic ">Complexity</em> 2021 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">113.</div><div class="CitationContent" id="CR113">A. van den Oord, O. Vinyals, and K. Kavukcuoglu. “Neural Discrete Representation Learning”. May 30, 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1711.00937 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">114.</div><div class="CitationContent" id="CR114">A. van den Oord et al. “Wavenet: A Generative Model for Raw Audio”. 2016. arXiv: <em class="EmphasisTypeItalic ">1609.03499</em>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">115.</div><div class="CitationContent" id="CR115">OpenAI. DALL⋅E Now Available in Beta. July 20, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://openai.com/blog/dall-e-now-available-in-beta/"><span class="RefSource">https://​openai.​com/​blog/​dall-e-now-available-in-beta/​</span></a></span> (visited on 07/29/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">116.</div><div class="CitationContent" id="CR116">V. Panayotov, G. Chen, D. Povey, and S. Khudanpur. “Librispeech: An ASR Corpus Based on Public Domain Audio Books”. In: <em class="EmphasisTypeItalic ">2015 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP.</em> IEEE, 2015, pp. 5206–5210.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">117.</div><div class="CitationContent" id="CR117">I. Papastratis. <em class="EmphasisTypeItalic ">Speech Recognition: A Review of the Different Deep Learning Approaches</em>. AI Summer. July 14, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://theaisummer.com/speech-recognition/"><span class="RefSource">https://​theaisummer.​com/​speech-recognition/​</span></a></span> (visited on 03/02/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">118.</div><div class="CitationContent" id="CR118">papers-with-code. <em class="EmphasisTypeItalic ">Papers with Code - ImageNet Benchmark (Image Classification)</em>. 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://paperswithcode.com/sota/image-classification-on-imagenet"><span class="RefSource">https://​paperswithcode.​com/​sota/​image-classification-on-imagenet</span></a></span> (visited on 03/05/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">119.</div><div class="CitationContent" id="CR119">K. K. Parida, S. Srivastava, and G. Sharma. “Beyond Mono to Binaural: Generating Binaural Audio from Mono Audio with Depth and Cross Modal Attention”. In: <em class="EmphasisTypeItalic ">Proc. IEEECVF Winter Conf. Appl. Comput. Vis.</em> 2022, pp. 3347–3356.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">120.</div><div class="CitationContent" id="CR120">D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le. “Specaugment: A Simple Data Augmentation Method for Automatic Speech Recognition”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1904.08779</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">121.</div><div class="CitationContent" id="CR121">D. S. Park et al. “Improved Noisy Student Training for Automatic Speech Recognition”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2005.09629</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">122.</div><div class="CitationContent" id="CR122">T. Park, M.-Y. Liu, T.-C. Wang, and J.-Y. Zhu. “Semantic Image Synthesis with Spatially-Adaptive Normalization”. Nov. 5, 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1903.07291 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">123.</div><div class="CitationContent" id="CR123">C. Payne. “MuseNet”. In: <em class="EmphasisTypeItalic ">OpenAI Blog</em> (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">124.</div><div class="CitationContent" id="CR124">J. Perez-Martin, B. Bustos, S. J. F. Guimarães, I. Sipiran, J. Pérez, and G. C. Said. “Bridging Vision and Language from the Video-to-Text Perspective: A Comprehensive Review”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2103.14785</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">125.</div><div class="CitationContent" id="CR125">R. Prenger, R. Valle, and B. Catanzaro. “Waveglow: A Flow-Based Generative Network for Speech Synthesis”. In: <em class="EmphasisTypeItalic ">ICASSP 2019-2019 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP</em>. IEEE, 2019, pp. 3617–3621.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">126.</div><div class="CitationContent" id="CR126">A. Radford, I. Sutskever, J. W. Kim, G. Krueger, and S. Agarwal. <em class="EmphasisTypeItalic ">CLIP: Connecting Text and Images.</em> Jan. 5, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://openai.com/blog/clip/"><span class="RefSource">https://​openai.​com/​blog/​clip/​</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">127.</div><div class="CitationContent" id="CR127">A. Radford et al. “Learning Transferable Visual Models from Natural Language Supervision”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn.</em> PMLR, 2021, pp. 8748–8763.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">128.</div><div class="CitationContent" id="CR128">J. W. Rae et al. “Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher”. In: <em class="EmphasisTypeItalic ">ArXiv Prepr. ArXiv211211446</em> (Dec. 8, 2021), p. 118.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">129.</div><div class="CitationContent" id="CR129">c. raffel. <em class="EmphasisTypeItalic ">C4 — TensorFlow Datasets</em>. TensorFlow. 2019. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.tensorflow.org/datasets/catalog/c4"><span class="RefSource">https://​www.​tensorflow.​org/​datasets/​catalog/​c4</span></a></span> (visited on 12/14/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">130.</div><div class="CitationContent" id="CR130">M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang, and A. Dosovitskiy. “Do Vision Transformers See Like Convolutional Neural Networks?” In: (Dec. 1, 2021), p. 13.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">131.</div><div class="CitationContent" id="CR131">P. Ramachandran, B. Zoph, and Q. V. Le. “Searching for Activation Functions”. 2017. arXiv: <span class="EmphasisFontCategoryNonProportional ">1710.05941</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">132.</div><div class="CitationContent" id="CR132">A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. “Hierarchical Text-Conditional Image Generation with CLIP Latents”. Apr. 12, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2204.06125 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">133.</div><div class="CitationContent" id="CR133">A. Ramesh et al. “Zero-Shot Text-to-Image Generation”. Feb. 26, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2102.12092</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">134.</div><div class="CitationContent" id="CR134">S. Reed. A Generalist Agent. May 12, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.deepmind.com/publications/a-generalist-agent"><span class="RefSource">https://​www.​deepmind.​com/​publications/​a-generalist-agent</span></a></span> (visited on 05/19/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">135.</div><div class="CitationContent" id="CR135">S. Reed et al. A Generalist Agent. May 12, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2205.06175 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">136.</div><div class="CitationContent" id="CR136">M. Reid, Y. Yamada, and S. S. Gu. “Can Wikipedia Help Offline Reinforcement Learning?” Jan. 28, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2201.12122 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">137.</div><div class="CitationContent" id="CR137">S. Ren, K. He, R. Girshick, and J. Sun. “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”. Jan. 6, 2016. arXiv: <span class="EmphasisFontCategoryNonProportional ">1506.01497 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">138.</div><div class="CitationContent" id="CR138">Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu. “FastSpeech 2: Fast and High-Quality End-to-End Text to Speech”. Mar. 4, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2006.04558</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">139.</div><div class="CitationContent" id="CR139">M. Rivière and E. Dupoux. “Towards Unsupervised Learning of Speech Features in the Wild”. In: <em class="EmphasisTypeItalic ">2021 IEEE Spok. Lang. Technol. Workshop SLT.</em> IEEE, 2021, pp. 156–163.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">140.</div><div class="CitationContent" id="CR140">J. Rodriguez. <em class="EmphasisTypeItalic ">Five Key Facts Wu Dao 2.0: The Largest Transformer Model Ever Built.</em> DataSeries. Sept. 21, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://medium.com/dataseries/five-key-facts-wu-dao-2-0-the-largest-transformer-model-ever-built-19316159796b"><span class="RefSource">https://​medium.​com/​dataseries/​five-key-facts-wu-dao-2-0-the-largest-transformer-model-ever-built-19316159796b</span></a></span> (visited on 12/12/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">141.</div><div class="CitationContent" id="CR141">R. Rombach. <em class="EmphasisTypeItalic ">Latent Diffusion Models</em>. CompVis - Machine Vision and Learning LMU Munich, Aug. 29, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/CompVis/latent-diffusion"><span class="RefSource">https://​github.​com/​CompVis/​latent-diffusion</span></a></span> (visited on 08/29/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">142.</div><div class="CitationContent" id="CR142">R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. <em class="EmphasisTypeItalic ">High Resolution Image Synthesiss with Latent Diffusion</em>... -. CVPR 22. Apr. 13, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://scholar.google.com/scholar?hl=de%2526as_sdt=0%252C5%2526q=high+resolution+image+synthesiss+with+latent+diffusion+models%2526btnG="><span class="RefSource">https://​scholar.​google.​com/​scholar?​hl=​de&amp;​as_​sdt=​0%2C5&amp;​q=​high+resolution+​image+synthesiss​+with+latent+dif​fusion+models&amp;​btnG=​</span></a></span> (visited on 08/29/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">143.</div><div class="CitationContent" id="CR143">A. Romero. <em class="EmphasisTypeItalic ">GPT-3 Scared You? Meet Wu Dao 2.0: A Monster of 1.75 Trillion Parameters</em>. Medium. June 8, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://towardsdatascience.com/gpt-3-scared-you-meet-wu-dao-2-0-a-monster-of-1-75-trillion-parameters-832cd83db484"><span class="RefSource">https://​towardsdatascien​ce.​com/​gpt-3-scared-you-meet-wu-dao-2-0-a-monster-of-1-75-trillion-parameters-832cd83db484</span></a></span> (visited on 07/29/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">144.</div><div class="CitationContent" id="CR144">O. Ronneberger, P. Fischer, and T. Brox. “U-Net: Convolutional Networks for Biomed- ical Image Segmentation”. In: <em class="EmphasisTypeItalic ">Int. Conf. Med. Image Comput. Comput.-Assist. Interv.</em> Springer, 2015, pp. 234–241.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">145.</div><div class="CitationContent" id="CR145">L. Ruan and Q. Jin. “Survey: Transformer Based Video-Language Pre-Training”. In: <em class="EmphasisTypeItalic ">AI Open</em> 3 (Jan. 1, 2022), pp. 1–13. <span class="EmphasisFontCategoryNonProportional ">issn</span>: 2666-6510. <span class="EmphasisTypeSmallCaps ">doi</span>: <span class="ExternalRef"><a href="https://doi.org/10.1016/j.aiopen.2022.01.001"><span class="RefSource">https://​doi.​org/​10.​1016/​j.​aiopen.​2022.​01.​001</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">146.</div><div class="CitationContent" id="CR146">M. S. Ryoo, A. J. Piergiovanni, A. Arnab, M. Dehghani, and A. Angelova. “TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?” 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2106.11297</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">147.</div><div class="CitationContent" id="CR147">C. Saharia, W. Chan, and S. Saxena. Imagen: Text-to-Image Diffusion Models. May 25, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://imagen.research.google/"><span class="RefSource">https://​imagen.​research.​google/​</span></a></span> (visited on 05/26/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">148.</div><div class="CitationContent" id="CR148">C. Saharia et al. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding”. May 23, 2022. <span class="EmphasisTypeSmallCaps ">doi</span>: <span class="ExternalRef"><a href="https://doi.org/10.48550/arXiv.2205.11487"><span class="RefSource">https://​doi.​org/​10.​48550/​arXiv.​2205.​11487</span></a></span>. arXiv: <span class="EmphasisFontCategoryNonProportional ">2205.11487 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">149.</div><div class="CitationContent" id="CR149">I. Salian. <em class="EmphasisTypeItalic ">NVIDIA Research’s GauGAN AI Art Demo Responds to Words</em>. NVIDIA Blog. Nov. 22, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://blogs.nvidia.com/blog/2021/11/22/gaugan2-ai-art-demo/"><span class="RefSource">https://​blogs.​nvidia.​com/​blog/​2021/​11/​22/​gaugan2-ai-art-demo/​</span></a></span> (visited on 03/06/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">150.</div><div class="CitationContent" id="CR150">T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. “Improved Techniques for Training Gans”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst.</em> 29 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">151.</div><div class="CitationContent" id="CR151">C. Schuhmann. <em class="EmphasisTypeItalic ">LAION-400-Million Open Dataset</em>. LAION. Aug. 20, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://laion.ai/laion-400-open-dataset/"><span class="RefSource">https://​laion.​ai/​laion-400-open-dataset/​</span></a></span> (visited on 03/05/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">152.</div><div class="CitationContent" id="CR152">D. Serdyuk, O. Braga, and O. Siohan. “Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition”. 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2201.10439</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">153.</div><div class="CitationContent" id="CR153">P. Sharma, N. Ding, S. Goodman, and R. Soricut. “Conceptual Captions: A Cleaned, Hy- pernymed, Image Alt-Text Dataset for Automatic Image Captioning”. In: <em class="EmphasisTypeItalic ">Proc. 56th Annu. Meet. Assoc. Comput. Linguist. Vol. 1 Long Pap.</em> 2018, pp. 2556–2565.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">154.</div><div class="CitationContent" id="CR154">J. Shen et al. “Natural Tts Synthesis by Conditioning Wavenet on Mel Spectrogram Predictions”. In: <em class="EmphasisTypeItalic ">2018 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP.</em> IEEE, 2018, pp. 4779–4783.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">155.</div><div class="CitationContent" id="CR155">Y.-J. Shih, S.-L. Wu, F. Zalkow, M. Müller, and Y.-H. Yang. “Theme Transformer: Symbolic Music Generation with Theme-Conditioned Transformer”. Nov. 7, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2111.04093 [cs, eess]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">156.</div><div class="CitationContent" id="CR156">J. Shor. <em class="EmphasisTypeItalic ">TRILLsson: Small, Universal Speech Representations for Paralinguistic Tasks</em>. Google AI Blog. Mar. 3, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="http://ai.googleblog.com/2022/03/trillsson-small-universal-speech.html"><span class="RefSource">http://​ai.​googleblog.​com/​2022/​03/​trillsson-small-universal-speech.​html</span></a></span> (visited on 03/29/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">157.</div><div class="CitationContent" id="CR157">J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. “Deep Unsupervised Learning Using Nonequilibrium Thermodynamics”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn.</em> PMLR, 2015, pp. 2256–2265.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">158.</div><div class="CitationContent" id="CR158">Stable. <em class="EmphasisTypeItalic ">Stable Diffusion Online.</em> 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://stablediffusionweb.com/"><span class="RefSource">https://​stablediffusionw​eb.​com/​</span></a></span> (visited on 12/31/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">159.</div><div class="CitationContent" id="CR159">M. Stefanini, M. Cornia, L. Baraldi, S. Cascianelli, G. Fiameni, and R. Cucchiara. “From Show to Tell: A Survey on Image Captioning”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2107.06912</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">160.</div><div class="CitationContent" id="CR160">C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid. “Videobert: A Joint Model for Video and Language Representation Learning”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Int. Conf. Comput. Vis.</em> 2019, pp. 7464–7473.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">161.</div><div class="CitationContent" id="CR161">C. Sun, A. Shrivastava, S. Singh, and A. Gupta. “Revisiting Unreasonable Effectiveness of Data in Deep Learning Era”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Int. Conf. Comput. Vis.</em> 2017, pp. 843–852.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">162.</div><div class="CitationContent" id="CR162">C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. “Rethinking the Inception Architecture for Computer Vision”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</em> 2016, pp. 2818–2826.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">163.</div><div class="CitationContent" id="CR163">X. Tan, T. Qin, F. Soong, and T.-Y. Liu. “A Survey on Neural Speech Synthesis”. July 23, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2106.15561</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">164.</div><div class="CitationContent" id="CR164">Z. Tang, J. Lei, and M. Bansal. “Decembert: Learning from Noisy Instructional Videos via Dense Captions and Entropy Minimization”. In: <em class="EmphasisTypeItalic ">Proc. 2021 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol.</em> 2021, pp. 2415–2426.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">165.</div><div class="CitationContent" id="CR165">M. Tao, H. Tang, S. Wu, N. Sebe, X.-Y. Jing, F. Wu, and B. Bao. “DF-GAN: Deep Fusion Generative Adversarial Networks for Text-to-Image Synthesis”. Mar. 24, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2008.05865</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">166.</div><div class="CitationContent" id="CR166">M. Tsimpoukelli, J. L. Menick, S. Cabi, S. M. Eslami, O. Vinyals, and F. Hill. “Multimodal Few-Shot Learning with Frozen Language Models”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst.</em> 34 (2021), pp. 200–212.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">167.</div><div class="CitationContent" id="CR167">T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly. “Towards Accurate Generative Models of Video: A New Metric &amp; Challenges”. 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1812.01717</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">168.</div><div class="CitationContent" id="CR168">A. Vaswani et al. “Attention Is All You Need”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst.</em> 2017, pp. 5998–6008.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">169.</div><div class="CitationContent" id="CR169">R. Vedantam, C. Lawrence Zitnick, and D. Parikh. “Cider: Consensus-based Image De- scription Evaluation”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</em> 2015, pp. 4566–4575.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">170.</div><div class="CitationContent" id="CR170">P. Wang et al. “OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework”. 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2202.03052</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">171.</div><div class="CitationContent" id="CR171">Z. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao. “SimVLM: Simple Visual Language Model Pretraining with Weak Supervision”. Aug. 24, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2108.10904</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">172.</div><div class="CitationContent" id="CR172">D. Weissenborn, O. Täckström, and J. Uszkoreit. “Scaling Autoregressive Video Models”. In: <em class="EmphasisTypeItalic ">ICLR</em> (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">173.</div><div class="CitationContent" id="CR173">C.-Y. Wu, Y. Li, K. Mangalam, H. Fan, B. Xiong, J. Malik, and C. Feichtenhofer. “MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition”. 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2201.08383</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">174.</div><div class="CitationContent" id="CR174">C. Wu. <em class="EmphasisTypeItalic ">Overview.</em> Microsoft, Mar. 14, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/microsoft/NUWA"><span class="RefSource">https://​github.​com/​microsoft/​NUWA</span></a></span> (visited on 03/14/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">175.</div><div class="CitationContent" id="CR175">C. Wu, J. Liang, L. Ji, F. Yang, Y. Fang, D. Jiang, and N. Duan. “Nüwa: Visual Synthesis Pre-Training for Neural Visual World Creation”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2111.12417</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">176.</div><div class="CitationContent" id="CR176">C. Wu et al. “Godiva: Generating Open-Domain Videos from Natural Descriptions”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2104.14806</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">177.</div><div class="CitationContent" id="CR177">Z. Wu, D. Lischinski, and E. Shechtman. “StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation”. Dec. 3, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2011.12799 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">178.</div><div class="CitationContent" id="CR178">N. Xie, F. Lai, D. Doran, and A. Kadav. “Visual Entailment: A Novel Task for Fine-Grained Image Understanding”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1901.06706</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">179.</div><div class="CitationContent" id="CR179">S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He. “Aggregated Residual Transformations for Deep Neural Networks”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</em> 2017, pp. 1492–1500.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">180.</div><div class="CitationContent" id="CR180">S. Xie, C. Sun, J. Huang, Z. Tu, and K. Murphy. “Rethinking Spatiotemporal Feature Learning for Video Understanding”. 2017. arXiv: <span class="EmphasisFontCategoryNonProportional ">1712.04851</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">181.</div><div class="CitationContent" id="CR181">W. Xiong, L. Wu, F. Alleva, J. Droppo, X. Huang, and A. Stolcke. “The Microsoft 2017 Conversational Speech Recognition System”. In: <em class="EmphasisTypeItalic ">2018 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP</em>. IEEE, 2018, pp. 5934–5938.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">182.</div><div class="CitationContent" id="CR182">J. Xu, T. Mei, T. Yao, and Y. Rui. “Msr-Vtt: A Large Video Description Dataset for Bridging Video and Language”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</em> 2016, pp. 5288–5296.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">183.</div><div class="CitationContent" id="CR183">P. Xu, X. Zhu, and D. A. Clifton. <em class="EmphasisTypeItalic ">Multimodal Learning with Transformers: A Survey</em>. June 13, 2022. <span class="EmphasisTypeSmallCaps ">doi</span>: <span class="ExternalRef"><a href="https://doi.org/10.48550/arXiv.2206.06488"><span class="RefSource">https://​doi.​org/​10.​48550/​arXiv.​2206.​06488</span></a></span>. arXiv: <span class="EmphasisFontCategoryNonProportional ">2206.06488 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">184.</div><div class="CitationContent" id="CR184">Q. Xu et al. “Self-Training and Pre-training Are Complementary for Speech Recognition”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.11430</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">185.</div><div class="CitationContent" id="CR185">S. Yan, X. Xiong, A. Arnab, Z. Lu, M. Zhang, C. Sun, and C. Schmid. “Multiview Transformers for Video Recognition”. In: <em class="EmphasisTypeItalic ">Proc. IEEECVF Conf. Comput. Vis. Pattern Recognit.</em> 2022, pp. 3333–3343.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">186.</div><div class="CitationContent" id="CR186">Y. Yan, X. Tan, B. Li, T. Qin, S. Zhao, Y. Shen, and T.-Y. Liu. “AdaSpeech 2: Adaptive Text to Speech with Untranscribed Data”. Apr. 19, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2104.09715 [cs, eess]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">187.</div><div class="CitationContent" id="CR187">L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. “Modeling Context in Referring Expressions”. In: <em class="EmphasisTypeItalic ">Eur. Conf. Comput. Vis.</em> Springer, 2016, pp. 69–85.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">188.</div><div class="CitationContent" id="CR188">R. Zellers et al. “Merlot: Multimodal Neural Script Knowledge Models”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst.</em> 34 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">189.</div><div class="CitationContent" id="CR189">X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. “Scaling Vision Transformers”. June 8, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2106.04560 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">190.</div><div class="CitationContent" id="CR190">B. Zhang, J. Yu, C. Fifty, W. Han, A. M. Dai, R. Pang, and F. Sha. “Co-Training Transformer with Videos and Images Improves Action Recognition”. Dec. 14, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2112.07175 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">191.</div><div class="CitationContent" id="CR191">B. Zhang et al. “StyleSwin: Transformer-based GAN for High-resolution Image Generation”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2112.10762</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">192.</div><div class="CitationContent" id="CR192">H. Zhang, J. Y. Koh, J. Baldridge, H. Lee, and Y. Yang. “Cross-Modal Contrastive Learning for Text-to-Image Generation”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2101.04702</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">193.</div><div class="CitationContent" id="CR193">P. Zhang et al. “VinVL: Making Visual Representations Matter in Vision-Language Models”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2101.00529</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">194.</div><div class="CitationContent" id="CR194">R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. “The Unreasonable Effectiveness of Deep Features as a Perceptual Metric”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</em> 2018, pp. 586–595.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">195.</div><div class="CitationContent" id="CR195">Y. Zhang et al. “BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition”. Oct. 1, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2109.13226 [cs, eess]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">196.</div><div class="CitationContent" id="CR196">Y. Zhang et al. “Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.10504</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">197.</div><div class="CitationContent" id="CR197">L. Zhao, D. Cai, L. Sheng, and D. Xu. “3DVG-Transformer: Relation Modeling for Visual Grounding on Point Clouds”. In: <em class="EmphasisTypeItalic ">Proc. IEEECVF Int. Conf. Comput. Vis.</em> 2021, pp. 2928–2937.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">198.</div><div class="CitationContent" id="CR198">A. Zhavoronkov. <em class="EmphasisTypeItalic ">Wu Dao 2.0 - Bigger, Stronger, Faster AI From China</em>. Forbes. July 19, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.forbes.com/sites/alexzhavoronkov/2021/07/19/wu-dao-20bigger-stronger-faster-ai-from-china/"><span class="RefSource">https://​www.​forbes.​com/​sites/​alexzhavoronkov/​2021/​07/​19/​wu-dao-20bigger-stronger-faster-ai-from-china/​</span></a></span> (visited on 07/29/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">199.</div><div class="CitationContent" id="CR199">H. Zhou, W. Zhou, W. Qi, J. Pu, and H. Li. “Improving Sign Language Translation with Monolingual Data by Sign Back-Translation”. In: <em class="EmphasisTypeItalic ">Proc. IEEECVF Conf. Comput. Vis. Pattern Recognit.</em> 2021, pp. 1316–1325.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">200.</div><div class="CitationContent" id="CR200">Y. Zhou et al. “LAFITE: Towards Language-Free Training for Text-to-Image Generation”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2111.13792</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">201.</div><div class="CitationContent" id="CR201">X. Zhu et al. “Multi-Modal Knowledge Graph Construction and Application: A Survey”. 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2202.05786</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">202.</div><div class="CitationContent" id="CR202">D. Zügner, T. Kirschstein, M. Catasta, J. Leskovec, and S. Günnemann. “Language-Agnostic Representation Learning of Source Code from Structure and Context”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2103.11318</span>.</div></li></ol></div></aside></div></div></body></html>