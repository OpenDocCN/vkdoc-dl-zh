<html><head></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="b978-3-031-23190-2_1"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">G. Paaß, S. Giesselbach</span><span class="ContextInformationBookTitles"><span class="BookTitle">Foundation Models for Natural Language Processing</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" lang="en">Artificial Intelligence: Foundations, Theory, and Algorithms</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-3-031-23190-2_1">https://doi.org/10.1007/978-3-031-23190-2_1</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">1. Introduction</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Gerhard Paaß</span><sup><a href="#Aff5">1</a> <span class="ContactIcon"> </span></sup> and </span><span class="Author"><span class="AuthorName">Sven Giesselbach</span><sup><a href="#Aff5">1</a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff5"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Knowledge Discovery Department, Team NLU, Fraunhofer Institute for Intelligent Analysis and Information Systems (IAIS), Sankt Augustin, Nordrhein-Westfalen, Germany</div></div><div class="ClearBoth"> </div></div></div><section class="Abstract" id="Abs1" lang="en" role="doc-abstract"><h2 class="Heading">Abstract</h2><p class="Para" id="Par1">With the development of efficient Deep Learning models about a decade ago, many Deep Neural Networks have been used to solve pattern recognition tasks such as natural language processing and image recognition. An advantage of these models is that they automatically create features arranged in layers which represent the content and do not require manually constructed features. These models rely on Machine Learning employing statistical techniques to give machines the capability to ‘learn’ from data without being given explicit instructions on what to do. Deep Learning models transform the input in layers step by step in such a way that complex patterns in the data can be recognized. This chapter first describes how a text is pre-processed and partitioned into tokens, which form the basis for natural language processing. Then we outline a number of classical Machine Learning models, which are often used as modules in advanced models. Examples include the logistic classifier model, fully connected layers, recurrent neural networks and convolutional neural networks.</p></section><div class="KeywordGroup" lang="en"><div class="Heading">Keywords</div><span class="Keyword" epub:type="keyword">Natural language processing</span><span class="Keyword" epub:type="keyword">Text preprocessing</span><span class="Keyword" epub:type="keyword">Vector space model</span><span class="Keyword" epub:type="keyword">Static embeddings</span><span class="Keyword" epub:type="keyword">Recurrent networks</span><span class="Keyword" epub:type="keyword">Convolutional networks</span></div><!--End Abstract--><div class="Fulltext"><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading"><span class="HeadingNumber">1.1 </span>Scope of the Book</h2><p class="Para" id="Par2">With the development of efficient Deep Learning models about a decade ago, many Deep Neural Networks have been used to solve pattern recognition tasks such as <em class="EmphasisTypeItalic ">natural language processing</em><span id="ITerm1"/> (<em class="EmphasisTypeItalic ">NLP</em>) and image processing. Typically, the models have to capture the meaning of a text or an image and make an appropriate decision. Alternatively they can generate a new text or image according to the task at hand. An advantage of these models is that they create intermediate features arranged in layers and do not require manually constructed features. <em class="EmphasisTypeItalic ">Deep Neural Networks</em><span id="ITerm2"/> such as Convolutional Neural Networks (CNNs) [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>] and Recurrent Neural Networks (RNNs) [<span class="CitationRef"><a epub:type="biblioref" href="#CR65" role="doc-biblioref">65</a></span>] use low-dimensional dense vectors as a kind of distributed representation to express the syntactic and semantic features of language.</p><p class="Para" id="Par3">All these models can be considered as <em class="EmphasisTypeItalic ">Artificial Intelligence</em><span id="ITerm3"/> (<em class="EmphasisTypeItalic ">AI</em>) Systems. AI is a broad research field aimed at creating intelligent machines, acting similar to humans and animals having natural intelligence. It captures the field’s long-term goal of building machines that mimic and then surpass the full spectrum of human cognition. <em class="EmphasisTypeItalic ">Machine Learning</em><span id="ITerm4"/><em class="EmphasisTypeItalic ">(ML)</em> is a subfield of artificial intelligence that employs statistical techniques to give machines the capability to ‘learn’ from data without being given explicit instructions on what to do. This process is also called ‘training’, whereby a ‘learning algorithm’ gradually improves the model’s performance on a given task. <em class="EmphasisTypeItalic ">Deep Learning</em><span id="ITerm5"/> is an area of ML in which an input is transformed in layers step by step in such a way that complex patterns in the data can be recognized. The adjective ‘deep’ refers to the large number of layers in modern ML models that help to learn expressive representations of data to achieve better performance.</p><p class="Para" id="Par4">In contrast to computer vision, the size of <em class="EmphasisTypeItalic ">annotated</em> training data for NLP applications was rather small, comprising only a few thousand sentences (except for machine translation). The main reason for this was the high cost of manual annotation. To avoid overfitting, i.e. overadapting models to random fluctuations, only relatively small models could be trained, which did not yield high performance. In the last 5 years, new NLP methods have been developed based on the <em class="EmphasisTypeItalic ">Transformer</em> introduced by Vaswani et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>]. They represent the meaning of each word by a vector of real numbers called <em class="EmphasisTypeItalic ">embedding</em><span id="ITerm6"/>. Between these embeddings various kinds of “attentions” can be computed, which can be considered as a sort of “correlation” between different words. In higher layers of the network, attention computations are used to generate new embeddings that can capture subtle nuances in the meaning of words. In particular, they can grasp different meanings of the same word that arise from context. A key advantage of these models is that they can be trained with unannotated text, which is almost infinitely available, and overfitting is not a problem.</p><p class="Para" id="Par5">Currently, there is a rapid development of new methods in the research field, which makes many approaches from earlier years obsolete. These models are usually trained in two steps: In a first <em class="EmphasisTypeItalic ">pre-training</em><span id="ITerm7"/> step, they are trained on a large text corpus containing billions of words without any annotations. A typical pre-training task is to predict single words in the text that have been masked in the input. In this way, the model learns fine subtleties of natural language syntax and semantics. Because enough data is available, the models can be extended to many layers with millions or billions of parameters.</p><p class="Para" id="Par6">In a second <em class="EmphasisTypeItalic ">fine-tuning</em><span id="ITerm8"/> step, the model is trained on a small annotated training set. In this way, the model can be adapted to new specific tasks. Since the fine-tuning data is very small compared to the pre-training data and the model has a high capacity with many millions of parameters, it can be adapted to the fine-tuning task without losing the stored information about the language structure. It was demonstrated that this idea can be applied to most NLP tasks, leading to unprecedented performance gains in semantic understanding. This <em class="EmphasisTypeItalic ">transfer learning</em><span id="ITerm9"/> allows knowledge from the pre-training phase to be transferred to the fine-tuned model. These models are referred to as <em class="EmphasisTypeItalic ">Pre-trained Language Models</em><span id="ITerm10"/> (<em class="EmphasisTypeItalic ">PLM</em>).</p><p class="Para" id="Par7">In the last years the number of parameters of these PLMs was systematically enlarged together with more training data. It turned out that in contrast to conventional wisdom the performance of these models got better and better without suffering from overfitting. Models with billions of parameters are able to generate syntactically correct and semantically consistent fluent text if prompted with some starting text. They can answer questions and react meaningful to different types of prompts.</p><p class="Para" id="Par8">Moreover, the same PLM architecture can simultaneously be pre-trained with different types of sequences, e.g. tokens in a text, image patches in a picture, sound snippet of speech, image patch sequences in video frames, DNA snippets, etc. They are able to process these media types simultaneously and establish connections between the different modalities. They can be adapted via natural language prompts to perform acceptably on a wide variety of tasks, even though they have not been explicitly trained on these tasks. Because of this flexibility, these models are promising candidates to develop overarching applications. Therefore, large PLMs with billions of parameters are often called <em class="EmphasisTypeItalic ">Foundation Models</em><span id="ITerm11"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>].</p><div class="Para" id="Par9">This book is intended to provide an up-to-date overview of the current Pre-trained Language Models and Foundation Models, with a focus on applications in NLP: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par10">We describe the necessary background knowledge, model architectures, pre-training and fine-tuning tasks, as well as evaluation metrics.</p></li><li><p class="Para" id="Par11">We discuss the most relevant models for each NLP application group that currently have the best accuracy or performance, i.e. are close to the <em class="EmphasisTypeItalic ">state of the art</em><span id="ITerm12"/> (<span class="EmphasisTypeSmallCaps ">Sota</span>). Our purpose here is not to describe a spectrum of all models developed in recent years, but to explain some representative models so that their internal workings can be understood.</p></li><li><p class="Para" id="Par12">Recently PLMs have been applied to a number of speech, image and video processing tasks giving rise to the term Foundation Models. We give an overview of most relevant models, which often allow the joint processing of different media, e.g. text and images</p></li><li><p class="Para" id="Par13">We provide links to available model codes and pre-trained model parameters.</p></li><li><p class="Para" id="Par14">We discuss strengths and limitations of the models and give an outlook on possible future developments.</p></li></ul></div> There are a number of previous surveys of Deep Learning and NLP [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>–<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR15" role="doc-biblioref">15</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR59" role="doc-biblioref">59</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>]. The surveys of Han et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>], Lin et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>], and Kalyan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>] are the most up-to-date and comprehensive. Jurafsky and Martin [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>] prepare an up-to-date book on this field. In addition, there are numerous surveys for specific model variants or application areas. Where appropriate, we provide references to these surveys. New terminology is usually printed in <em class="EmphasisTypeItalic ">italics</em> and models in <strong class="EmphasisTypeBold ">bold</strong>.</div><p class="Para" id="Par15">The rest of this chapter introduces text preprocessing and <em class="EmphasisTypeItalic ">classical NLP models</em>, which in part are reused inside PLMs. The second chapter describes the main architectures of <em class="EmphasisTypeItalic ">Pre-trained Language Models</em>, which are currently the workhorses of NLP. The third chapter considers a large number of <em class="EmphasisTypeItalic ">PLM variants</em> that extend the capabilities of the basic models. The fourth chapter describes the information captured by PLMs and Foundation Models and analyses their syntactic skills, world knowledge, and reasoning capabilities.</p><p class="Para" id="Par16">The remainder of the book considers various application domains and identifies PLMs and Foundation Models that currently provide the best results in each domain at a reasonable cost. The fifth chapter reviews <em class="EmphasisTypeItalic ">information extraction</em> methods that automatically identify structured information and language features in text documents, e.g. for relation extraction. The sixth chapter deals with <em class="EmphasisTypeItalic ">natural language generation</em> approaches that automatically generate new text in natural language, usually in response to a prompt. The seventh chapter is devoted to models for analyzing and creating <em class="EmphasisTypeItalic ">multimodal content</em> that typically integrate content understanding and production across two or more modalities, such as text, speech, image, video, etc. The general trend is that more data, computational power, and larger parameter sets lead to better performance. This is explained in the last <em class="EmphasisTypeItalic ">summary</em> chapter, which also considers social and ethical aspects of Foundation Models and summarizes possible further developments.</p></section>
<section class="Section1 RenderAsSection1" id="Sec2"><h2 class="Heading"><span class="HeadingNumber">1.2 </span>Preprocessing of Text</h2><p class="Para" id="Par17">The first step in preprocessing is to extract the actual text. For each type of text document, e.g. pdf, html, xml, docx, ePUB, there are specific parsers, which resolve the text into characters, words, and formatting information. Usually, the layout and formatting information is removed.</p><p class="Para" id="Par18">Then, the extracted text is routinely divided into <em class="EmphasisTypeItalic ">tokens</em><span id="ITerm13"/>, i.e. words, numbers, and punctuation marks. This process is not trivial, as text usually contains special units like phone numbers or email addresses that must be handled in a special way. Some text mining tasks require the splitting of text into sentences. Tokenizers and sentence splitters for different languages have been developed in the past decades and can be included from many programming toolboxes, e.g. <em class="EmphasisTypeItalic ">Spacy</em><span id="ITerm14"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>].</p><p class="Para" id="Par19">In the past, many preprocessing methods aimed at generating new relevant features (part-of-speech tags, syntax parse trees) and removing unnecessary tokens (stemming, stop word removal, lemmatization). In most cases, this is no longer necessary with modern approaches that internally automatically derive the features relevant for the task at hand.</p><p class="Para" id="Par20">In an optional final step, the word-tokens can be further subdivided and rearranged. A simple technique creates <em class="EmphasisTypeItalic ">character</em><em class="EmphasisTypeItalic ">n-grams</em><span id="ITerm15"/><span id="ITerm16"/> (i.e. all sequences of <em class="EmphasisTypeItalic ">n</em> adjacent characters in a word) as additional features. Alternatively, <em class="EmphasisTypeItalic ">word</em><em class="EmphasisTypeItalic ">n-grams</em><span id="ITerm17"/> can be formed consisting of <em class="EmphasisTypeItalic ">n</em> consecutive words.</p><p class="Para" id="Par21">Currently, the most popular approach tries to limit the number of different words in a vocabulary. A common choice is <em class="EmphasisTypeItalic ">byte-pair encoding</em><span id="ITerm18"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>]. This method first selects all characters as tokens. Then, successively the most frequent token pair is merged into a new token and all instances of the token pair are replaced by the new token. This is repeated until a vocabulary of prescribed size is obtained. Note that new words can always be represented by a sequence of vocabulary tokens and characters. Common words end up being a part of the vocabulary, while rarer words are split into components, which often retain some linguistic meaning. In this way, out-of-vocabulary words are avoided.</p><p class="Para" id="Par22">The <em class="EmphasisTypeItalic ">WordPiece</em><span id="ITerm19"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR69" role="doc-biblioref">69</a></span>] algorithm also starts by selecting all characters of the collection as tokens. Then it assumes that the text corpus has been generated by randomly sampling tokens according to their observed frequencies. It merges tokens <em class="EmphasisTypeItalic ">a</em> and <em class="EmphasisTypeItalic ">b</em> (inside words) in such a way that the likelihood of the training data is maximally increased [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>]. There is a fast variant whose computational complexity is linear in the input length [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>]. <em class="EmphasisTypeItalic ">SentencePiece</em><span id="ITerm20"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>] is a package containing several subword tokenizers and can also be applied to all Asian languages. All the approaches effectively interpolate between word level inputs for frequent words and character level inputs for infrequent words.</p><p class="Para" id="Par23">Often the language of the input text has to be determined [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>]. Most <em class="EmphasisTypeItalic ">language identification methods</em><span id="ITerm21"/> extract character <em class="EmphasisTypeItalic ">n</em>-grams from the input text and evaluate their relative frequencies. Some methods can be applied to texts containing different languages at the same time [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR71" role="doc-biblioref">71</a></span>]. To filter out offensive words from a text, one can use lists of such toxic words in different languages [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>].</p></section>
<section class="Section1 RenderAsSection1" id="Sec3"><h2 class="Heading"><span class="HeadingNumber">1.3 </span>Vector Space Models and Document Classification</h2><p class="Para" id="Par24">To apply Machine Learning to documents, their text has to be transformed into scalars, vectors, matrices, or higher-dimensional arrangements of numbers, which are collectively called <em class="EmphasisTypeItalic ">tensors</em><span id="ITerm22"/>. In the previous section, text documents in a corpus were converted into a sequence of tokens by preprocessing. These tokens now have to be translated into tensors.</p><div class="Para" id="Par25">The <em class="EmphasisTypeItalic ">bag-of-words</em><span id="ITerm23"/> representation describes a given text document <em class="EmphasisTypeItalic ">d</em> by a vector <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> of token counts. The <em class="EmphasisTypeItalic ">vocabulary</em><span id="ITerm24"/> is a list of all different tokens contained in the collection of training documents, the <em class="EmphasisTypeItalic ">training corpus</em><span id="ITerm25"/><span id="ITerm26"/>. Ignoring the order of tokens, this bag-of-words vector records how often each token of the vocabulary appears in document <em class="EmphasisTypeItalic ">d</em>. Note that most vector entries will be zero, as each document will only contain a small fraction of vocabulary tokens. The vector of counts may be modified to emphasize tokens with high information content, e.g. by using the <em class="EmphasisTypeItalic ">tf-idf</em><span id="ITerm27"/> statistic [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>]. Table <span class="InternalRef"><a href="#Tab1">1.1</a></span> summarizes different representations for documents used for NLP. <div class="Table" id="Tab1"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 1.1</span><p class="SimplePara">Representations for documents used in NLP Models.</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Type</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Generated by ...</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Used by ...</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Bag-of-words</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Tokenization and counting</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Logistic classifier, SVM. Section <span class="InternalRef"><a href="#Sec3">1.3</a></span>.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Simple embeddings</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Correlation and regression: topic models [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>], Word2Vec [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>], GloVe [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>].</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Classifiers, clustering, visualization, RNN, etc. Section <span class="InternalRef"><a href="#Sec5">1.5</a></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Contextual embeddings</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Attention computation: ElMo [<span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>], Transformer [<span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>], GPT [<span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>], BERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>] and many others.</p></td><td style="text-align: left;"><p class="SimplePara">Fine-tuning with supervised training data. Section <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec1"><span class="RefSource">2.​1</span></a></span>.</p></td></tr></tbody></table></div></div><div class="Para" id="Par26"><em class="EmphasisTypeItalic ">Document classification</em><span id="ITerm28"/> methods aim to categorize text documents according to their content [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>]. An important example is the logistic classifier, which uses a bag-of-words vector <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> as input and predicts the probability of each of the <em class="EmphasisTypeItalic ">k</em> possible output classes <em class="EmphasisTypeItalic ">y</em> ∈{1, …, <em class="EmphasisTypeItalic ">k</em>}. More precisely, there is a random variable <em class="EmphasisTypeItalic ">Y</em>  which may take the values 1, …, <em class="EmphasisTypeItalic ">k</em>. To predict the output class <em class="EmphasisTypeItalic ">y</em> from the input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>, a score vector is first generated as <div class="Equation NumberedEquation" id="Equ1"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \boldsymbol{u}=A{\boldsymbol{x}}+\boldsymbol{b} {} \end{aligned} $$" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ1.png" style="width:5.69em"/></div></div> <div class="EquationNumber">(1.1)</div></div></div> using an <em class="EmphasisTypeItalic ">affine transformation</em><span id="ITerm29"/> of the input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>. Here, the vector <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> is transformed by a <em class="EmphasisTypeItalic ">linear transformation</em><span id="ITerm30"/><em class="EmphasisTypeItalic ">A</em><em><strong class="EmphasisTypeBoldItalic ">x</strong></em> and then a <em class="EmphasisTypeItalic ">bias</em><span id="ITerm31"/> vector <em><strong class="EmphasisTypeBoldItalic ">b</strong></em> is added. The resulting <em class="EmphasisTypeItalic ">score vector</em><span id="ITerm32"/><em><strong class="EmphasisTypeBoldItalic ">u</strong></em> of length <em class="EmphasisTypeItalic ">k</em> is then transformed to a probability distribution over the <em class="EmphasisTypeItalic ">k</em> classes by the <em class="EmphasisTypeItalic ">softmax function</em><span id="ITerm33"/><div class="Equation NumberedEquation" id="Equ2"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \operatorname{\mathrm{softmax}}(u_1,\ldots, u_k) &amp;amp;= \frac{(\exp(u_1),\ldots, \exp(u_k))}{\exp(u_1)+\cdots+ \exp(u_k)} {}, \end{aligned} $$" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ2.png" style="width:20.82em"/></div></div> <div class="EquationNumber">(1.2)</div></div></div><div class="Equation NumberedEquation" id="Equ3"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} p(Y\mkern1.5mu{=}\mkern1.5mu m|{\boldsymbol{x}};A,\boldsymbol{b}) &amp;amp;= \operatorname{\mathrm{softmax}}(A{\boldsymbol{x}}+\boldsymbol{b}) {}. \end{aligned} $$" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ3.png" style="width:16.62em"/></div></div> <div class="EquationNumber">(1.3)</div></div></div> Since the softmax function converts any vector into a probability vector, we obtain the conditional probability of output class <em class="EmphasisTypeItalic ">m</em> as a function of input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>. The function <div class="Equation NumberedEquation" id="Equ4"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \text{LRM}({\boldsymbol{x}})=\operatorname{\mathrm{softmax}}(A{\boldsymbol{x}}+\boldsymbol{b}) {} \end{aligned} $$" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ4.png" style="width:12.75em"/></div></div> <div class="EquationNumber">(1.4)</div></div></div> is called a <em class="EmphasisTypeItalic ">logistic classifier</em><span id="ITerm34"/> model [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>] with parameter vector <em><strong class="EmphasisTypeBoldItalic ">w</strong></em> = <em class="EmphasisTypeItalic ">vec</em>(<em class="EmphasisTypeItalic ">A</em>, <em class="EmphasisTypeItalic ">b</em>). In general, a function mapping the input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> to the output <em class="EmphasisTypeItalic ">y</em> or a probability distribution over the output is called a <em class="EmphasisTypeItalic ">model</em><span id="ITerm35"/><em class="EmphasisTypeItalic ">f</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>;<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>).</div><div class="Para" id="Par27">The model is trained using <em class="EmphasisTypeItalic ">training data</em><span id="ITerm36"/><em class="EmphasisTypeItalic ">Tr</em> = {(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[1]</sup>, <em class="EmphasisTypeItalic ">y</em><sup>[1]</sup>), …, (<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">N</em>]</sup>, <em class="EmphasisTypeItalic ">y</em><sup>[<em class="EmphasisTypeItalic ">N</em>]</sup>)}, whose <em class="EmphasisTypeItalic ">examples</em><span id="ITerm37"/> (<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">i</em>]</sup>, <em class="EmphasisTypeItalic ">y</em><sup>[<em class="EmphasisTypeItalic ">i</em>]</sup>) have to be independent and identically distributed (<em class="EmphasisTypeItalic ">i.i.d.</em><span id="ITerm38"/>). The task is to adjust the parameters <em><strong class="EmphasisTypeBoldItalic ">w</strong></em> such that the predicted probability <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">Y</em> =<em class="EmphasisTypeItalic ">m</em>|<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>;<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>) is maximized. Following the <em class="EmphasisTypeItalic ">Maximum Likelihood principle</em><span id="ITerm39"/>, this can be achieved by modifying the parameter vector <em><strong class="EmphasisTypeBoldItalic ">w</strong></em> such that the complete training data has a maximal probability [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>, p. 31] <div class="Equation NumberedEquation" id="Equ5"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \max_{\boldsymbol{w}}p(y^{[1]}|{\boldsymbol{x}}^{[1]};{\boldsymbol{w}})*\cdots*p(y^{[N]}|{\boldsymbol{x}}^{[N]};{\boldsymbol{w}}). \end{aligned} $$" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ5.png" style="width:18em"/></div></div> <div class="EquationNumber">(1.5)</div></div></div> Transforming the expression by log and multiplying by − 1.0 gives the <em class="EmphasisTypeItalic ">classification loss</em><span id="ITerm40"/> function <em class="EmphasisTypeItalic ">L</em><sub>MC</sub>(<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>), also called <em class="EmphasisTypeItalic ">maximum entropy loss</em><span id="ITerm41"/>. <div class="Equation NumberedEquation" id="Equ6"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} L_{\text{MC}}({\boldsymbol{w}})=-\left[\log p(y^{[1]}|{\boldsymbol{x}}^{[1]};{\boldsymbol{w}})+\cdots+\log p(y^{[N]}|{\boldsymbol{x}}^{[N]};{\boldsymbol{w}})\right]. {} \end{aligned} $$" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ6.png" style="width:26.56em"/></div></div> <div class="EquationNumber">(1.6)</div></div></div> To optimize the loss function, its gradient is computed and minimized by stochastic gradient descent or another optimizer (c.f. Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec28"><span class="RefSource">2.​4.​1</span></a></span>).</div><p class="Para" id="Par28">The performance of classifiers is measured on separate <em class="EmphasisTypeItalic ">test data</em><span id="ITerm42"/> by accuracy, precision, recall, F1-value, etc. [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>, p. 410f]. Because the bag-of-words representation ignores important word order information, document classification by a logistic classifier is less commonly used today. However, this model is still a component in most Deep Learning architectures.</p></section>
<section class="Section1 RenderAsSection1" id="Sec4"><h2 class="Heading"><span class="HeadingNumber">1.4 </span>Nonlinear Classifiers</h2><div class="Para" id="Par29">It turns out that the logistic classifier partitions the input space by linear hyperplanes that are not able to solve more complex classification tasks, e.g., the XOR problem [<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>]. An alternative is to generate an internal <em class="EmphasisTypeItalic ">hidden vector</em><span id="ITerm43"/><em><strong class="EmphasisTypeBoldItalic ">h</strong></em> by an additional <em class="EmphasisTypeItalic ">affine transformation</em><span id="ITerm44"/><em class="EmphasisTypeItalic ">A</em><sub>1</sub><em><strong class="EmphasisTypeBoldItalic ">x</strong></em> + <em><strong class="EmphasisTypeBoldItalic ">b</strong></em><sub>1</sub> followed by a monotonically non-decreasing nonlinear <em class="EmphasisTypeItalic ">activation function</em><span id="ITerm45"/><em class="EmphasisTypeItalic ">g</em> and use this hidden vector as input for the logistic classifier to predict the random variable <em class="EmphasisTypeItalic ">Y</em>  <div class="Equation NumberedEquation" id="Equ7"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} {\boldsymbol{h}} &amp;amp;= g(A_1{\boldsymbol{x}}+\boldsymbol{b}_1) , {} \end{aligned} $$" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ7.png" style="width:7.94em"/></div></div> <div class="EquationNumber">(1.7)</div></div></div><div class="Equation NumberedEquation" id="Equ8"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} p(Y\mkern1.5mu{=}\mkern1.5mu m|{\boldsymbol{x}}; {\boldsymbol{w}}) &amp;amp;= \operatorname{\mathrm{softmax}}(A_2{\boldsymbol{h}}+\boldsymbol{b}_2) , {} \end{aligned} $$" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ8.png" style="width:16.56em"/></div></div> <div class="EquationNumber">(1.8)</div></div></div> where the parameters of this model can be collected in a parameter vector <em><strong class="EmphasisTypeBoldItalic ">w</strong></em> = <em class="EmphasisTypeItalic ">vec</em>(<em class="EmphasisTypeItalic ">A</em><sub>1</sub>, <em class="EmphasisTypeItalic ">b</em><sub>1</sub>, <em class="EmphasisTypeItalic ">A</em><sub>2</sub>, <em class="EmphasisTypeItalic ">b</em><sub>2</sub>). The form of the nonlinear activation function <em class="EmphasisTypeItalic ">g</em> is quite arbitrary, often <span class="InlineEquation" id="IEq1"><img alt="$$\tanh (x)$$" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_IEq1.png" style="width:3.44em"/></span> or a <em class="EmphasisTypeItalic ">rectified linear unit</em><span id="ITerm46"/><span class="InlineEquation" id="IEq2"><img alt="$$\text{ReLU}(x)=\max (0,x)$$" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_IEq2.png" style="width:9.63em"/></span> is used. <span class="EmphasisTypeSmallCaps ">Fcl</span>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>) = <em class="EmphasisTypeItalic ">g</em>(<em class="EmphasisTypeItalic ">A</em><sub>1</sub><em><strong class="EmphasisTypeBoldItalic ">x</strong></em> + <em><strong class="EmphasisTypeBoldItalic ">b</strong></em><sub>1</sub>) is called a <em class="EmphasisTypeItalic ">fully connected layer</em><span id="ITerm47"/>.</div><div class="Para" id="Par30">This model (Fig. <span class="InternalRef"><a href="#Fig1">1.1</a></span>) is able to solve any classification problem arbitrarily well, provided the length of <em><strong class="EmphasisTypeBoldItalic ">h</strong></em> is large enough [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>, p. 192]. By prepending more fully connected layers to the network we get a <em class="EmphasisTypeItalic ">Deep Neural Network</em><span id="ITerm48"/>, which needs fewer parameters than a shallow network to approximate more complex functions. Historically, it has been called <em class="EmphasisTypeItalic ">Multilayer Perceptron</em><span id="ITerm49"/> (MLP). Liang et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>] show that for a large class of piecewise smooth functions, the sizes of hidden vectors needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><img alt="" aria-describedby="d64e1866" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Fig1_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e1866"><p class="Para" id="Par57">An illustration of a neural network has input nodes of 0.1, 1.3, and negative 0.4 linked to six blocks of u 1, Relu of u 1, hidden vector, via A 2 h + b 2 to 4 blocks of u 2, soft max of u 2, and ends with 4 output probabilities of 0.2, 0.3, 0.1, and 0.4.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 1.1</span><p class="SimplePara">A neural network for classification transforms the input by layers with affine transformations and nonlinear activation functions, e.g. ReLU. The final layer usually is a logistic classifier</p></div></figcaption></figure></div><p class="Para" id="Par31">The <em class="EmphasisTypeItalic ">support vector machine</em><span id="ITerm50"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>] follows a different approach and tries to create a hyperplane, which is located between the training examples of the two classes in the input space. In addition, this hyperplane should have a large distance (<em class="EmphasisTypeItalic ">margin</em><span id="ITerm51"/>) to the examples. This model reduces overfitting and usually has a high classification accuracy, even if the number of input variables is high, e.g. for document classification [<span class="CitationRef"><a epub:type="biblioref" href="#CR28" role="doc-biblioref">28</a></span>]. It was extended to different kernel loss criteria, e.g. graph kernels [<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>] which include grammatical features. Besides SVM, many alternative classifiers are used, such as random forests [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>, p.588f] and gradient boosted trees [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>, p.360], which are among the most popular classifiers.</p><div class="Para" id="Par32">For these conventional classifiers the analyst usually has to construct input features manually. Modern classifiers for text analysis are able to create relevant features automatically (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec1"><span class="RefSource">2.​1</span></a></span>). For the training of NLP models there exist three main paradigms: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par33"><em class="EmphasisTypeItalic ">Supervised training</em><span id="ITerm52"/><span id="ITerm53"/><span id="ITerm54"/> is based on training data consisting of pairs (<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>, <em><strong class="EmphasisTypeBoldItalic ">y</strong></em>) of an input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>, e.g. a document text, and an output <em><strong class="EmphasisTypeBoldItalic ">y</strong></em>, where <em><strong class="EmphasisTypeBoldItalic ">y</strong></em> usually is a manual annotation, e.g. a sentiment. By optimization the unknown parameters of the model are adapted to predict the output from the input in an optimal way.</p></li><li><p class="Para" id="Par34"><em class="EmphasisTypeItalic ">Unsupervised training</em><span id="ITerm55"/><span id="ITerm56"/><span id="ITerm57"/> just considers some data <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> and derives some intrinsic knowledge from unlabeled data, such as clusters, densities, or latent representations.</p></li><li><p class="Para" id="Par35"><em class="EmphasisTypeItalic ">Self-supervised training</em><span id="ITerm58"/><span id="ITerm59"/><span id="ITerm60"/> selects parts of the observed data vector as input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> and output <em><strong class="EmphasisTypeBoldItalic ">y</strong></em>. The key idea is to predict <em><strong class="EmphasisTypeBoldItalic ">y</strong></em> from <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> in a supervised manner. For example, the language model is a self-supervised task that attempts to predict the next token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub> from the previous tokens <em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub>. For NLP models, this type of training is used very often.</p></li></ul></div></div></section>
<section class="Section1 RenderAsSection1" id="Sec5"><h2 class="Heading"><span class="HeadingNumber">1.5 </span>Generating Static Word Embeddings</h2><p class="Para" id="Par36">One problem with bag-of word representations is that frequency vectors of tokens are unable to capture relationships between words, such as synonymy and homonymy, and give no indication of their semantic similarity. An alternative are more expressive representations of words and documents based on the idea of <em class="EmphasisTypeItalic ">distributional semantics</em><span id="ITerm61"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>], popularized by Zellig Harris [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>] and John Firth [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>]. According to Firth <em class="EmphasisTypeItalic ">“a word is characterized by the company it keeps”</em>. This states that words occurring in the same neighborhood tend to have similar meanings.</p><p class="Para" id="Par37">Based on this idea each word can be characterized by a <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub>-dimensional vector <span class="InlineEquation" id="IEq3"><img alt="" aria-describedby="d64e2045" src="../images/528393_1_En_1_Chapter/528393_1_En_1_IEq3_HTML.gif" style="width:7.62em"/><span class="TextObject" id="d64e2045"><span class="Para" id="Par38">italic e m b left parenthesis word right parenthesis element of double struck upper R Superscript d Super Subscript italic e m b</span></span></span>), a <em class="EmphasisTypeItalic ">word embedding</em><span id="ITerm62"/><span id="ITerm63"/><span id="ITerm64"/>. Usually, a value between 100 and 1000 is chosen for <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub>. These embeddings have to be created such that words that occur in similar contexts have embeddings with a small vector distance, such as the Euclidean distance. A document then can be represented by a sequence of such embeddings. It turns out that words usually have a similar meaning, if their embeddings have a low distance. Embeddings can be used as input for downstream text mining tasks, e.g. sentiment analysis. Goldberg [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>] gives an excellent introduction to static word embeddings. The embeddings are called <em class="EmphasisTypeItalic ">static embeddings</em><span id="ITerm65"/><span id="ITerm66"/> as each word has a single embedding independent of the context.</p><p class="Para" id="Par39">There are a number of different approaches to generate word embeddings in an unsupervised way. Collobert et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>] show that word embeddings obtained by predicting neighbor words can be used to improve the performance of downstream tasks such as named entity recognition and semantic role labeling.</p><div class="Para" id="Par40"><strong class="EmphasisTypeBold ">Word2vec</strong><span id="ITerm67"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>] predicts the words in the neighborhood of a central word with an extremely simple model. As shown in Fig. <span class="InternalRef"><a href="#Fig2">1.2</a></span> it uses the embedding vector of the central word as input for a logistic classifier (<span class="InternalRef"><a href="#Equ3">1.3</a></span>) to infer the probabilities of words in the neighborhood of about five to seven positions. The training target is to forecast all neighboring words in the training set with a high probability. For training, Word2Vec repeats this prediction for all words of a corpus, and the parameters of the logistic classifier as well as the values of the embeddings are optimized by stochastic gradient descent to improve the prediction of neighboring words.<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO2"><img alt="" aria-describedby="d64e2126" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Fig2_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e2126"><p class="Para" id="Par58">An architecture has input words of Biden, has, been, U S, president, since, and 2021. These are linked to embedding vector. The word U S via logistic classifier SoftMax, gives 4 word probabilities, that point to the words, has, been, president, and since.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 1.2</span><p class="SimplePara">Word2vec predicts the words in the neighborhood of a central word by logistic classifier <em class="EmphasisTypeItalic ">L</em>. The input to <em class="EmphasisTypeItalic ">L</em> is the embedding of the central word. By training with a large set of documents, the parameters of <em class="EmphasisTypeItalic ">L</em> as well as the embeddings are learned [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>, p. 2]</p></div></figcaption></figure></div><div class="Para" id="Par41">The vocabulary of a text collection contains <em class="EmphasisTypeItalic ">k</em> different words, e.g. <em class="EmphasisTypeItalic ">k</em> = 100, 000. To predict the probability of the <em class="EmphasisTypeItalic ">i</em>-th word by softmax (<span class="InternalRef"><a href="#Equ2">1.2</a></span>), <em class="EmphasisTypeItalic ">k</em> exponential terms <span class="InlineEquation" id="IEq4"><img alt="$$\exp (u_i)$$" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_IEq4.png" style="width:3.26em"/></span> have to be computed. To avoid this effort, the fraction is approximated as <div class="Equation NumberedEquation" id="Equ9"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \frac{\exp(u_i)}{\exp(u_1)+\cdots+exp(u_k)} \approx \frac{\exp(u_i)}{\exp(u_i)+\sum_{j\in S} exp(u_j)}, {} \end{aligned} $$" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ9.png" style="width:23em"/></div></div> <div class="EquationNumber">(1.9)</div></div></div> where <em class="EmphasisTypeItalic ">S</em> is a small sample of, say, 10 randomly selected indices of words. This technique is called <em class="EmphasisTypeItalic ">noise contrastive estimation</em><span id="ITerm68"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>, p. 612]. There are several variants available, which are used for almost all classification tasks involving softmax computations with many classes. Since stochastic gradient descent works with noisy gradients, the additional noise introduced by the approximation of the softmax function is not harmful and can even help the model escape local minima. The shallow architecture of Word2Vec proved to be far more efficient than previous architectures for representation learning.</div><p class="Para" id="Par42">Word2Vec embeddings have been used for many downstream tasks, e.g. document classification. In addition, words with a similar meaning may be detected by simply searching for words whose embeddings have a small Euclidean distance to the embedding of a target word. The closest neighbors of <em class="EmphasisTypeItalic ">“neutron”</em>, for example, are <em class="EmphasisTypeItalic ">“neutrons”</em>, <em class="EmphasisTypeItalic ">“protons”</em>, <em class="EmphasisTypeItalic ">“deuterium”</em>, <em class="EmphasisTypeItalic ">“positron”</em>, and <em class="EmphasisTypeItalic ">“decay”</em>. In this way, synonyms can be revealed. Projections of embeddings on two dimensions may be used for the exploratory analysis of the content of a corpus. <strong class="EmphasisTypeBold ">GloVe</strong><span id="ITerm69"/> generates similar embedding vectors using aggregated global word-word co-occurrence statistics from a corpus [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>].</p><p class="Para" id="Par43">It turns out that differences between the embeddings often have an interpretation. For example, the result of <em class="EmphasisTypeItalic ">emb</em>(Germany) − <em class="EmphasisTypeItalic ">emb</em>(Berlin) + <em class="EmphasisTypeItalic ">emb</em>(Paris) has <em class="EmphasisTypeItalic ">emb</em>(France) as its nearest neighbor with respect to Euclidean distance. This property is called <em class="EmphasisTypeItalic ">analogy</em><span id="ITerm70"/> and holds for a majority of examples of many relations such as capital-country, currency-country, etc. [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>].</p><p class="Para" id="Par44"><strong class="EmphasisTypeBold ">FastText</strong><span id="ITerm71"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>] representations enrich static word embeddings by using subword information. Character <em class="EmphasisTypeItalic ">n</em>-grams of a given length range, e.g., 3–6, are extracted from each word. Then, embedding vectors are defined for the words as well as their character <em class="EmphasisTypeItalic ">n</em>-grams. To train the embeddings all word and character <em class="EmphasisTypeItalic ">n</em>-gram embeddings in the neighborhood of a central word are averaged, and the probabilities of the central word and its character <em class="EmphasisTypeItalic ">n</em>-grams are predicted by a logistic classifier. To improve the probability prediction, the parameters of the model are optimized by stochastic gradient descent. This is repeated for all words in a training corpus. After training, unseen words can be reconstructed using only their <em class="EmphasisTypeItalic ">n</em>-gram embeddings. <em class="EmphasisTypeItalic ">Starspace</em><span id="ITerm72"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>] was introduced as a generalization of FastText. It allows embedding arbitrary entities (such as authors, products) by analyzing texts related to them and evaluating graph structures. An alternative are <em class="EmphasisTypeItalic ">spherical embeddings</em><span id="ITerm73"/>, where unsupervised word and paragraph embeddings are constrained to a hypersphere [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>].</p></section>
<section class="Section1 RenderAsSection1" id="Sec6"><h2 class="Heading"><span class="HeadingNumber">1.6 </span>Recurrent Neural Networks</h2><div class="Para" id="Par45"><em class="EmphasisTypeItalic ">Recurrent Neural Networks</em><span id="ITerm74"/> were developed to model sequences <em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">T</em></sub> of varying length <em class="EmphasisTypeItalic ">T</em>, for example the tokens of a text document. Consider the task to predict the next token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub> given the previous tokens (<em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub>). As proposed by Bengio et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>] each token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub> is represented by an embedding vector <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> = <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub>) indicating the meaning of <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub>. The previous tokens are characterized by a hidden vector <em><strong class="EmphasisTypeBoldItalic ">h</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub>, which describes the state of the subsequence (<em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em>−1</sub>). The RNN is a function <span class="EmphasisTypeSmallCaps ">Rnn</span>(<em><strong class="EmphasisTypeBoldItalic ">h</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub>, <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub>) predicting the next hidden vector <em><strong class="EmphasisTypeBoldItalic ">h</strong></em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub> by <div class="Equation NumberedEquation" id="Equ10"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} {\boldsymbol{h}}_{t+1}=\text{RNN}({\boldsymbol{h}}_t , {\boldsymbol{x}}_t). \end{aligned} $$" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ10.png" style="width:9.25em"/></div></div> <div class="EquationNumber">(1.10)</div></div></div> Subsequently, a <em class="EmphasisTypeItalic ">logistic classifier</em><span id="ITerm75"/> (<span class="InternalRef"><a href="#Equ3">1.3</a></span>) with parameters <em class="EmphasisTypeItalic ">H</em> and <em><strong class="EmphasisTypeBoldItalic ">g</strong></em> predicts a probability vector for the next token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub> using the information contained in <em class="EmphasisTypeItalic ">h</em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub>, <div class="Equation NumberedEquation" id="Equ11"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} p(V_{t+1}|v_1,\ldots,v_t)=\operatorname{\mathrm{softmax}}(H*{\boldsymbol{h}}_{t+1}+\boldsymbol{g}), \end{aligned} $$" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ11.png" style="width:19.32em"/></div></div> <div class="EquationNumber">(1.11)</div></div></div> as shown in Fig. <span class="InternalRef"><a href="#Fig3">1.3</a></span>. Here <em class="EmphasisTypeItalic ">V</em><sub><em class="EmphasisTypeItalic ">t</em></sub> is the random variable of possible tokens at position <em class="EmphasisTypeItalic ">t</em>. According to the definition of the conditional probability the joint probability of the whole sequence can be factorized as <div class="Equation NumberedEquation" id="Equ12"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} p(v_1,\ldots,v_T) = p(V_T\mkern1.5mu{=}\mkern1.5mu v_T|v_1,\ldots,v_{T-1})* \cdots* p(V_2\mkern1.5mu{=}\mkern1.5mu v_2|v_1)*p(V_1\mkern1.5mu{=}\mkern1.5mu v_1). {} \end{aligned} $$" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ12.png" style="width:31.38em"/></div></div> <div class="EquationNumber">(1.12)</div></div></div> A model that either computes the joint probability or the conditional probability of natural language texts is called <em class="EmphasisTypeItalic ">language model</em><span id="ITerm76"/> as it potentially covers all information about the language. A language model sequentially predicting the next word by the conditional probability is often referred to <em class="EmphasisTypeItalic ">autoregressive language model</em><span id="ITerm77"/><span id="ITerm78"/>. According to (<span class="InternalRef"><a href="#Equ12">1.12</a></span>), the observed tokens (<em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub>) can be used as input to predict the probability of the next token <em class="EmphasisTypeItalic ">V</em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub>. The product of these probabilities yields the correct joint probability of the observed token sequence (<em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">T</em></sub>). The same model <span class="EmphasisTypeSmallCaps ">Rnn</span>(<em><strong class="EmphasisTypeBoldItalic ">h</strong></em>, <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>) is repeatedly applied and generates a sequence of hidden vectors <em><strong class="EmphasisTypeBoldItalic ">h</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub>. A <em class="EmphasisTypeItalic ">simple RNN</em><span id="ITerm79"/> just consists of a single <em class="EmphasisTypeItalic ">fully connected layer</em><span id="ITerm80"/><div class="Equation NumberedEquation" id="Equ13"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \text{RNN}({\boldsymbol{h}}_t , {\boldsymbol{x}}_t) = \tanh \left(A*\begin{bmatrix} {\boldsymbol{h}}_t\\ {\boldsymbol{x}}_t\end{bmatrix}+{\boldsymbol{b}}\right). \end{aligned} $$" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ13.png" style="width:17em"/></div></div> <div class="EquationNumber">(1.13)</div></div></div> The probabilities of the predicted words <em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">T</em></sub> depend on the parameters <em><strong class="EmphasisTypeBoldItalic ">w</strong></em> = <em class="EmphasisTypeItalic ">vec</em>(<em class="EmphasisTypeItalic ">H</em>, <em><strong class="EmphasisTypeBoldItalic ">g</strong></em>, <em class="EmphasisTypeItalic ">A</em>, <em><strong class="EmphasisTypeBoldItalic ">b</strong></em>, <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">v</em><sub>1</sub>), …, <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">T</em></sub>)). To improve these probabilities, we may use the stochastic gradient descent optimizer (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec29"><span class="RefSource">2.​4.​1</span></a></span>) and adapt the unknown parameters in <em><strong class="EmphasisTypeBoldItalic ">w</strong></em>. Note that this also includes the estimation of new token embeddings <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub>). A recent overview is given in [<span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>, Ch. 8–9].<figure class="Figure" id="Fig3"><div class="MediaObject" id="MO3"><img alt="" aria-describedby="d64e3103" src="../images/528393_1_En_1_Chapter/528393_1_En_1_Fig3_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e3103"><p class="Para" id="Par59">An architecture has input words of, the, cat, sat, on, and the. These are linked to embedding vector, hidden vector, logistic classifier, token probabilities, and ends with tokens to be predicted which are, the, cat, sat, on, the, mat.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 1.3</span><p class="SimplePara">The RNN starts on the left side and successively predicts the probability of the next token with the previous tokens as conditions using a logistic classifier <em class="EmphasisTypeItalic ">L</em>. The hidden vector <em><strong class="EmphasisTypeBoldItalic ">h</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> stores information about the tokens that occur before position <em class="EmphasisTypeItalic ">t</em></p></div></figcaption></figure></div><p class="Para" id="Par46">It turns out that this model has difficulties to reconstruct the relation between distant sequence elements, since gradients tend to vanish or “explode” as the sequences get longer. Therefore, new RNN types have been developed, e.g. the <em class="EmphasisTypeItalic ">Long Short-Term Memory</em><span id="ITerm81"/> (LSTM) [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>] and the <em class="EmphasisTypeItalic ">Gated Recurrent Unit</em><span id="ITerm82"/> (GRU) [<span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>], which capture long-range dependencies in the sequence much better.</p><p class="Para" id="Par47">Besides predicting the next word in a sequence, RNNs have been successfully applied to predict properties of sequence elements, e.g. named entity recognition [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>] and relation extraction [<span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>]. For these applications <em class="EmphasisTypeItalic ">bidirectional RNNs</em><span id="ITerm83"/> have been developed, consisting of a forward and a backward language model. The <em class="EmphasisTypeItalic ">forward language model</em> starts at the beginning of a text and predicts the next token, while the <em class="EmphasisTypeItalic ">backward language model</em> starts at the end of a text and predicts the previous token. Bidirectional LSTMs are also called <em class="EmphasisTypeItalic ">biLSTMs</em><span id="ITerm84"/>. In addition, <em class="EmphasisTypeItalic ">multilayer RNNs</em><span id="ITerm85"/> were proposed [<span class="CitationRef"><a epub:type="biblioref" href="#CR72" role="doc-biblioref">72</a></span>], where the hidden vector generated by the RNN-cell in one layer is used as the input to the RNN-cell in the next layer, and the last layer provides the prediction of the current task.</p><p class="Para" id="Par48"><em class="EmphasisTypeItalic ">Machine translation</em><span id="ITerm86"/> from one language to another is an important application of RNNs [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>]. In this process, an input sentence first is encoded by an <em class="EmphasisTypeItalic ">encoder</em><span id="ITerm87"/> RNN as a hidden vector <em><strong class="EmphasisTypeBoldItalic ">h</strong></em><sub><em class="EmphasisTypeItalic ">T</em></sub>. This hidden vector is in turn used by a second <em class="EmphasisTypeItalic ">decoder</em><span id="ITerm88"/> RNN as an initial hidden vector to generate the words of the target language sentence. However, RNNs still have difficulties to capture relationships over long distances between sequence elements because RNNs do not cover direct relations between distant sequence elements.</p><p class="Para" id="Par49"><em class="EmphasisTypeItalic ">Attention</em><span id="ITerm89"/> was first used in the context of machine translation to communicate information over long distances. It computes the correlation between hidden vectors of the decoder RNN and hidden vectors of the encoder RNN at different positions. This correlation is used to build a <em class="EmphasisTypeItalic ">context vector</em><span id="ITerm90"/> as a weighted average of relevant encoder hidden vectors. Then, this context vector is exploited to improve the final translation result [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>]. The resulting translations were much better than those with the original RNN. We will see in later sections that attention is a fundamental principle to construct better NLP model.</p><p class="Para" id="Par50"><strong class="EmphasisTypeBold ">ELMo</strong><span id="ITerm91"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>] generates embeddings with bidirectional LSTM language models in several layers. The model is pre-trained as forward and backward language model with a large non-annotated text corpus. During fine-tuning, averages of the hidden vectors are used to predict the properties of words based on an annotated training set. These language models take into account the words before and after a position, and thus employ contextual representations for the word in the central position. For a variety of tasks such as sentiment analysis, question answering, and textual entailment, ELMo was able to improve <span class="EmphasisTypeSmallCaps ">Sota</span> performance.</p></section>
<section class="Section1 RenderAsSection1" id="Sec7"><h2 class="Heading"><span class="HeadingNumber">1.7 </span>Convolutional Neural Networks</h2><p class="Para" id="Par51"><em class="EmphasisTypeItalic ">Convolutional Neural Networks</em><span id="ITerm92"/> (<em class="EmphasisTypeItalic ">CNNs</em>) [<span class="CitationRef"><a epub:type="biblioref" href="#CR37" role="doc-biblioref">37</a></span>] are widely known for their success in the image domain. They start with a small quadratic arrangement of parameters called <em class="EmphasisTypeItalic ">filter kernel</em><span id="ITerm93"/>, which is moved over the input pixel matrix of the image. The values of the filter kernel are multiplied with the underlying pixel values and generate an output value. This is repeated for every position of the input pixel matrix. During training the parameters of a filter kernel are automatically tuned such that they can detect local image patterns such as blobs or lines. Each layer of the network, which is also called <em class="EmphasisTypeItalic ">convolution layer</em><span id="ITerm94"/>, consists of many filter kernels and a network contains a number of convolution layers. Interspersed <em class="EmphasisTypeItalic ">max pooling</em><span id="ITerm95"/> layers perform a local aggregation of pixels by maximum. The final layer of a Convolutional Neural Network usually is a fully connected layer with a softmax classifier.</p><p class="Para" id="Par52">Their breakthrough was <em class="EmphasisTypeItalic ">AlexNet</em><span id="ITerm96"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>], which receives the RGB pixel matrix of an image as input and is tasked with assigning a content class to the image. This model won the 2012 <em class="EmphasisTypeItalic ">ImageNet</em><span id="ITerm97"/> competition, where images had to be assigned to one of 1000 classes, and demonstrated the superior performance of Deep Neural Networks. Even earlier the deep CNN of Cireşan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>] achieved <span class="EmphasisTypeSmallCaps ">Sota</span> performance on a number of image classification benchmarks. A highly successful CNN is <em class="EmphasisTypeItalic ">ResNet</em><span id="ITerm98"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>] which employs a so-called <em class="EmphasisTypeItalic ">residual connection</em><span id="ITerm99"/> working as a bypass. It can circumvent many layers in the beginning of the training and is the key to training neural networks with many hundred layers. It resulted in image classifiers which have a higher accuracy than humans.</p><p class="Para" id="Par53">While Recurrent Neural Networks were regarded as the best way to process sequential input such as text, some CNN-based architectures were introduced, which achieved high performance on some NLP tasks. Kim [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>] proposed a rather shallow CNN for sentence classification. It contains an embedding layer, a convolutional layer, a max-pooling layer, and a fully connected layer with softmax output. <em class="EmphasisTypeItalic ">1-D convolutions</em><span id="ITerm100"/> were applied to the embeddings of the input words, basically combining the information stored in adjacent words, treating them as <em class="EmphasisTypeItalic ">n</em>-grams. The embeddings are processed by a moving average with trainable weights. Using this architecture for classification proved to be very efficient, having a similar performance as recurrent architectures that are more difficult to train.</p><p class="Para" id="Par54">Another interesting CNN architecture is <em class="EmphasisTypeItalic ">wavenet</em><span id="ITerm101"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>], a deeper network used mainly for text-to-speech synthesis. It consists of multiple convolutional layers stacked on top of each other, with its main ingredient being <em class="EmphasisTypeItalic ">dilated causal convolutions</em><span id="ITerm102"/>. Causal means that the convolutions at position <em class="EmphasisTypeItalic ">t</em> can only utilize prior information <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub>1</sub>, …, <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em class="EmphasisTypeItalic ">t</em>−1</sub>. Dilated means that the convolutions can skip input values with a certain step size <em class="EmphasisTypeItalic ">k</em>, i.e. that in some layer the features at position <em class="EmphasisTypeItalic ">t</em> are predicted using information from positions <em class="EmphasisTypeItalic ">t</em>, <em class="EmphasisTypeItalic ">t</em> − <em class="EmphasisTypeItalic ">k</em>, <em class="EmphasisTypeItalic ">t</em> − 2<em class="EmphasisTypeItalic ">k</em>, …. This step size <em class="EmphasisTypeItalic ">k</em> is doubled in each successive layer, yielding dilations of size <em class="EmphasisTypeItalic ">k</em><sup>0</sup>, <em class="EmphasisTypeItalic ">k</em><sup>1</sup>, <em class="EmphasisTypeItalic ">k</em><sup>2</sup>, …. In this way, very long time spans can be included in the prediction. This model architecture has been shown to give very good results for text-to-speech synthesis.</p></section>
<section class="Section1 RenderAsSection1" id="Sec8"><h2 class="Heading"><span class="HeadingNumber">1.8 </span>Summary</h2><p class="Para" id="Par55">Classical NLP has a long history, and machine learning models have been used in the field for several decades. They all require some preprocessing steps to generate words or tokens from the input text. Tokens are particularly valuable because they form a dictionary of finite size and allow arbitrary words to be represented by combination. Therefore, they are used by most PLMs. Early document representations like bag-of-words are now obsolete because they ignore sequence information. Nevertheless, classifiers based on them like logistic classifiers and fully connected layers, are important building blocks of PLMs.</p><p class="Para" id="Par56">The concept of static word embeddings initiated the revolution in NLP, which is based on contextual word embeddings. These ideas are elaborated in the next chapter. Recurrent neural networks have been used to implement the first successful language models, but were completely superseded by attention-based models. Convolutional neural networks for image processing are still employed in many applications. PLMs today often have a similar performance on image data, and sometimes CNNs are combined with PLMs to exploit their respective strengths, as discussed in Chap. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml"><span class="RefSource">7</span></a></span>.</p></section>
<div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a><p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p><p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p></div><aside aria-labelledby="Bib1Heading" class="Bibliography" id="Bib1"><div epub:type="bibliography" role="doc-bibliography"><div class="Heading" id="Bib1Heading">References</div><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">M. Allahyari, S. Pouriyeh, M. Assefi, S. Safaei, E. D. Trippe, J. B. Gutierrez, and K. Kochut. “A Brief Survey of Text Mining: Classification, Clustering and Extraction Techniques”. 2017. arXiv: 1707.02919.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">M. Z. Alom et al. “A State-of-the-Art Survey on Deep Learning Theory and Architectures”. In: <em class="EmphasisTypeItalic ">Electronics</em> 8.3 (2019), p. 292.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">M. Z. Alom et al. “The History Began from Alexnet: A Comprehensive Survey on Deep Learning Approaches”. 2018. arXiv: 1803.01164.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">Z. Alyafeai, M. S. AlShaibani, and I. Ahmad. “A Survey on Transfer Learning in Natural Language Processing”. 2020. arXiv: 2007.04239.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">D. Bahdanau, K. Cho, and Y. Bengio. “Neural Machine Translation by Jointly Learning to Align and Translate”. 2014. arXiv: 1409.0473.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. “A Neural Probabilistic Language Model”. In: <em class="EmphasisTypeItalic ">J. Mach. Learn. Res.</em> 3 (Feb 2003), pp. 1137–1155.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">D. M. Blei. “Introduction to Probabilistic Topic Models”. In: <em class="EmphasisTypeItalic ">Commun. ACM</em> 55.4 (2011), pp. 77–84.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov. “Enriching Word Vectors with Subword Information”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 5 (2017), pp. 135–146.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">R. Bommasani et al. “On the Opportunities and Risks of Foundation Models”. 2021. arXiv: 2108.07258.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">J. Chai and A. Li. “Deep Learning in Natural Language Processing: A State-of-the-Art Survey”. In: <em class="EmphasisTypeItalic ">2019 Int. Conf. Mach. Learn. Cybern. ICMLC</em>. 2019 International Conference on Machine Learning and Cybernetics (ICMLC). July 2019, pp. 1–6. <span class="ExternalRef"><a href="https://doi.org/10.1109/ICMLC48188.2019.8949185"><span class="RefSource">https://​doi.​org/​10.​1109/​ICMLC48188.​2019.​8949185</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling”. 2014. arXiv: 1412.3555.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">D. Cireşan, U. Meier, and J. Schmidhuber. “Multi-Column Deep Neural Networks for Image Classification”. Feb. 13, 2012. arXiv: 1202.2745.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. “Natural Language Processing (Almost) from Scratch”. In: <em class="EmphasisTypeItalic ">J. Mach. Learn. Res</em>. 12 (2011), pp. 2493–2537.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">C. Cortes and V. Vapnik. “Support-Vector Networks”. In: <em class="EmphasisTypeItalic ">Mach. Learn</em>. 20.3 (1995), pp. 273–297.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">M. Danilevsky, K. Qian, R. Aharonov, Y. Katsis, B. Kawas, and P. Sen. “A Survey of the State of Explainable AI for Natural Language Processing”. 2020. arXiv: 2010.00711.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">S. Dargan, M. Kumar, M. R. Ayyagari, and G. Kumar. “A Survey of Deep Learning and Its Applications: A New Paradigm to Machine Learning”. In: <em class="EmphasisTypeItalic ">Arch. Comput. Methods Eng</em>. (2019), pp. 1–22.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. “Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding”. 2018. arXiv: 1810.04805.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">J. R. Firth. “A Synopsis of Linguistic Theory 1930–1955, Volume 1952-59”. In: <em class="EmphasisTypeItalic ">Philol. Soc</em>. (1957).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">P. Gage. “A New Algorithm for Data Compression”. In: <em class="EmphasisTypeItalic ">C Users J.</em> 12 (Feb. 1, 1994).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">Y. Goldberg. “A Primer on Neural Network Models for Natural Language Processing”. In: <em class="EmphasisTypeItalic ">J. Artif. Intell. Res.</em> 57 (2016), pp. 345–420.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">I. Goodfellow, Y. Bengio, and A. Courville. <em class="EmphasisTypeItalic ">Deep Learning</em>. Vol. 1. MIT press Cambridge, 2016. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.deeplearningbook.org/"><span class="RefSource">https://​www.​deeplearningbook​.​org/​</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">X. Han et al. “Pre-Trained Models: Past, Present and Future”. In: <em class="EmphasisTypeItalic ">AI Open</em> (Aug. 26, 2021). <span class="EmphasisTypeSmallCaps ">issn</span>: 2666-6510. <span class="ExternalRef"><a href="https://doi.org/10.1016/j.aiopen.2021.08.002"><span class="RefSource">https://​doi.​org/​10.​1016/​j.​aiopen.​2021.​08.​002</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">Z. S. Harris. “Distributional Structure”. In: <em class="EmphasisTypeItalic ">Word</em> 10.2-3 (1954), pp. 146–162.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">T. Hastie, R. Tibshirani, and J. Friedman. <em class="EmphasisTypeItalic ">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. 2nd Edition, corrected 12th printing. Springer Science &amp; Business Media, 2017. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf"><span class="RefSource">https://​web.​stanford.​edu/​~hastie/​Papers/​ESLII.​pdf</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">K. He, X. Zhang, S. Ren, and J. Sun. “Deep Residual Learning for Image Recognition”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</em>. 2016, pp. 770–778.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">S. Hochreiter and J. Schmidhuber. “Long Short-Term Memory”. In: <em class="EmphasisTypeItalic ">Neural Comput</em>. 9.8 (1997), pp. 1735–1780.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">A. Hotho, A. Nürnberger, and G. Paaß. “A Brief Survey of Text Mining.” In: <em class="EmphasisTypeItalic ">Ldv Forum</em>. Vol. 20. 1. 2005, pp. 19–62.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">T. Joachims. “Text Categorization with Support Vector Machines: Learning with Many Relevant Features”. In: <em class="EmphasisTypeItalic ">Eur. Conf. Mach. Learn</em>. Springer, 1998, pp. 137–142.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov. “Bag of Tricks for Efficient Text Classification”. 2016. arXiv: 1607.01759.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">D. Jurafsky and J. H. Martin. <em class="EmphasisTypeItalic ">Speech and Language ProcessingAn Introduction to Natural Language Processing,Computational Linguistics, and Speech Recognition</em>. 3rd Draft. Jan. 12, 2022.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">K. S. Kalyan, A. Rajasekharan, and S. Sangeetha. “Ammus: A Survey of Transformer-Based Pretrained Models in Natural Language Processing”. 2021. arXiv: 2108.05542.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">Y. Kim. “Convolutional Neural Networks for Sentence Classification”. 2014. arXiv: 1408. 5882.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">K. Kowsari, K. Jafari Meimandi, M. Heidarysafa, S. Mendu, L. Barnes, and D. Brown. “Text Classification Algorithms: A Survey”. In: <em class="EmphasisTypeItalic ">Information</em> 10.4 (2019), p. 150.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">A. Krizhevsky, I. Sutskever, and G. E. Hinton. “Imagenet Classification with Deep Convolutional Neural Networks”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2012, pp. 1097–1105.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">T. Kudo and J. Richardson. “Sentencepiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing”. 2018. arXiv: 1808.06226.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer. “Neural Architectures for Named Entity Recognition”. In: <em class="EmphasisTypeItalic ">Proc. 2016 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol</em>. 2016, pp. 260–270.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">Y. LeCun and Y. Bengio. “Convolutional Networks for Images, Speech, and Time Series”. In: <em class="EmphasisTypeItalic ">Handb. Brain Theory Neural Netw</em>. 3361.10 (1995), p. 1995.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">F. Li, M. Zhang, G. Fu, and D. Ji. “A Neural Joint Model for Entity and Relation Extraction from Biomedical Text”. In: <em class="EmphasisTypeItalic ">BMC bioinformatics</em> 18.1 (2017), pp. 1–11.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">Q. Li et al. “A Survey on Text Classification: From Shallow to Deep Learning”. 2020. arXiv: 2008.00364.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">S. Liang and R. Srikant. “Why Deep Neural Networks for Function Approximation?” Mar. 3, 2017. arXiv: 1610.04161 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">T. Lin, Y. Wang, X. Liu, and X. Qiu. “A Survey of Transformers”. 2021. arXiv: 2106.04554.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">M. Lui, J. H. Lau, and T. Baldwin. “Automatic Detection and Language Identification of Multilingual Documents”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 2 (2014), pp. 27–40.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">C. D. Manning, P. Raghavan, and H. Schütze. <em class="EmphasisTypeItalic ">Introduction to Information Retrieval</em>. Vol. 39. Cambridge University Press Cambridge, 2008.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">Y. Meng, J. Huang, G. Wang, C. Zhang, H. Zhuang, L. Kaplan, and J. Han. “Spherical Text Embedding”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 32 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">T. Mikolov, K. Chen, G. Corrado, and J. Dean. “Efficient Estimation of Word Representations in Vector Space”. 2013. arXiv: 1301.3781.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. “Distributed Representations of Words and Phrases and Their Compositionality”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2013, pp. 3111–3119.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">M. Minsky and S. A. Papert. <em class="EmphasisTypeItalic ">Perceptrons: An Introduction to Computational Geometry</em>. MIT press, 1969.<span class="Occurrences"><span class="Occurrence OccurrenceZLBID"><a href="http://www.emis.de/MATH-item?0197.43702"><span><span>zbMATH</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">K. Nigam, J. Lafferty, and A. McCallum. “Using Maximum Entropy for Text Classification”. In: <em class="EmphasisTypeItalic ">IJCAI-99 Workshop Mach. Learn. Inf. Filter</em>. Vol. 1. 1. Stockholom, Sweden, 1999, pp. 61–67.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">A. van den Oord et al. “Wavenet: A Generative Model for Raw Audio”. 2016. arXiv: 1609.03499.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">D. W. Otter, J. R. Medina, and J. K. Kalita. “A Survey of the Usages of Deep Learning for Natural Language Processing”. In: <em class="EmphasisTypeItalic ">IEEE Trans. Neural Netw. Learn. Syst</em>. (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">J. Pennington, R. Socher, and C. D. Manning. “Glove: Global Vectors for Word Representation”. In: <em class="EmphasisTypeItalic ">Proc. 2014 Conf. Empir. Methods Nat. Lang. Process. EMNLP</em>. 2014, pp. 1532–1543.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. “Deep Contextualized Word Representations”. In: <em class="EmphasisTypeItalic ">Proc. NAACL-HLT</em>. 2018, pp. 2227–2237.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">S. Pouyanfar et al. “A Survey on Deep Learning: Algorithms, Techniques, and Applications”. In: <em class="EmphasisTypeItalic ">ACM Comput. Surv. CSUR</em> 51.5 (2018), pp. 1–36.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang. “Pre-Trained Models for Natural Language Processing: A Survey”. In: <em class="EmphasisTypeItalic ">Sci. China Technol. Sci</em>. 63.10 (June 23, 2021), pp. 1872–1897. <span class="EmphasisTypeSmallCaps ">issn</span>: 1674-7321, 1869-1900. <span class="ExternalRef"><a href="https://doi.org/10.1007/s11431-020-1647-3"><span class="RefSource">https://​doi.​org/​10.​1007/​s11431-020-1647-3</span></a></span>. arXiv: 2003.08271.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. “Improving Language Understanding by Generative Pre-Training”. In: (2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">F. Reichartz, H. Korte, and G. Paass. “Dependency Tree Kernels for Relation Extraction from Natural Language Text”. In: <em class="EmphasisTypeItalic ">Jt. Eur. Conf. Mach. Learn. Knowl. Discov. Databases</em>. Springer, 2009, pp. 270–285.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">R. Al-Rfou. <em class="EmphasisTypeItalic ">Cld3 at Github</em>. Google, Apr. 8, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/google/cld3"><span class="RefSource">https://​github.​com/​google/​cld3</span></a></span> (visited on 04/12/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">M. Sahlgren. “The Distributional Hypothesis”. In: <em class="EmphasisTypeItalic ">Ital. J. Disabil. Stud</em>. 20 (2008), pp. 33–53.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">J. Schmidhuber. “Deep Learning in Neural Networks: An Overview”. In: <em class="EmphasisTypeItalic ">Neural Netw</em>. 61 (2015), pp. 85–117.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">M. Schuster and K. Nakajima. “Japanese and Korean Voice Search”. In: <em class="EmphasisTypeItalic ">2012 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP</em>. IEEE, 2012, pp. 5149–5152.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">61.</div><div class="CitationContent" id="CR61">F. Sebastiani. “Machine Learning in Automated Text Categorization”. In: <em class="EmphasisTypeItalic ">ACM Comput. Surv. CSUR</em> 34.1 (2002), pp. 1–47.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">62.</div><div class="CitationContent" id="CR62">Shutterstock. <em class="EmphasisTypeItalic ">List of Dirty Naughty Obscene and Otherwise Bad Words</em>. LDNOOBW, Apr. 11, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words"><span class="RefSource">https://​github.​com/​LDNOOBW/​List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words</span></a></span> (visited on 04/12/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">63.</div><div class="CitationContent" id="CR63">X. Song, A. Salcianu, Y. Song, D. Dopson, and D. Zhou. “Fast WordPiece Tokenization”. Oct. 5, 2021. arXiv: 2012.15524 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">64.</div><div class="CitationContent" id="CR64">Spacy. <em class="EmphasisTypeItalic ">Spacy - Industrial-Stregth Natural Language Processing</em>. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://spacy.io/"><span class="RefSource">https://​spacy.​io/​</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">65.</div><div class="CitationContent" id="CR65">I. Sutskever, O. Vinyals, and Q. V. Le. “Sequence to Sequence Learning with Neural Networks”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2014, pp. 3104–3112.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">66.</div><div class="CitationContent" id="CR66">Y. Tay, M. Dehghani, D. Bahri, and D. Metzler. “Efficient Transformers: A Survey”. 2020. arXiv: 2009.06732.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">67.</div><div class="CitationContent" id="CR67">A. Vaswani et al. “Attention Is All You Need”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2017, pp. 5998–6008.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">68.</div><div class="CitationContent" id="CR68">L. Wu, A. Fisch, S. Chopra, K. Adams, A. Bordes, and J. Weston. “Starspace: Embed All the Things!” 2017. arXiv: 1709.03856.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">69.</div><div class="CitationContent" id="CR69">Y. Wu et al. “Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation”. 2016. arXiv: 1609.08144.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">70.</div><div class="CitationContent" id="CR70">A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola. “Dive into Deep Learning”. In: <em class="EmphasisTypeItalic ">Unpubl. Draft Retrieved</em> 19 (Release 0.16.1 Jan. 23, 2021), p. 1021.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">71.</div><div class="CitationContent" id="CR71">Y. Zhang, J. Riesa, D. Gillick, A. Bakalov, J. Baldridge, and D. Weiss. “A Fast, Compact, Accurate Model for Language Identification of Codemixed Text”. Oct. 9, 2018. arXiv: 1810.04142 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">72.</div><div class="CitationContent" id="CR72">J. G. Zilly, R. K. Srivastava, J. Koutnık, and J. Schmidhuber. “Recurrent Highway Networks”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn</em>. PMLR, 2017, pp. 4189–4198.</div></li></ol></div></aside></div></div></body></html>