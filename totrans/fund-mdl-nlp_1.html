<html><head></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="b978-3-031-23190-2_2"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">G. Paaß, S. Giesselbach</span><span class="ContextInformationBookTitles"><span class="BookTitle">Foundation Models for Natural Language Processing</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" lang="en">Artificial Intelligence: Foundations, Theory, and Algorithms</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-3-031-23190-2_2">https://doi.org/10.1007/978-3-031-23190-2_2</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">2. Pre-trained Language Models</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Gerhard Paaß</span><sup><a href="#Aff5">1</a> <span class="ContactIcon"> </span></sup> and </span><span class="Author"><span class="AuthorName">Sven Giesselbach</span><sup><a href="#Aff5">1</a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff5"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Knowledge Discovery Department, Team NLU, Fraunhofer Institute for Intelligent Analysis and Information Systems (IAIS), Sankt Augustin, Nordrhein-Westfalen, Germany</div></div><div class="ClearBoth"> </div></div></div><section class="Abstract" id="Abs1" lang="en" role="doc-abstract"><h2 class="Heading">Abstract</h2><p class="Para" id="Par1">This chapter presents the main architecture types of attention-based language models, which describe the distribution of tokens in texts: Autoencoders similar to BERT receive an input text and produce a contextual embedding for each token. Autoregressive language models similar to GPT receive a subsequence of tokens as input. They produce a contextual embedding for each token and predict the next token. In this way, all tokens of a text can successively be generated. Transformer Encoder-Decoders have the task to translate an input sequence to another sequence, e.g. for language translation. First they generate a contextual embedding for each input token by an autoencoder. Then these embeddings are used as input to an autoregressive language model, which sequentially generates the output sequence tokens. These models are usually pre-trained on a large general training set and often fine-tuned for a specific task. Therefore, they are collectively called Pre-trained Language Models (PLM). When the number of parameters of these models gets large, they often can be instructed by prompts and are called Foundation Models. In further sections we described details on optimization and regularization methods used for training. Finally, we analyze the uncertainty of model predictions and how predictions may be explained.</p></section><div class="KeywordGroup" lang="en"><div class="Heading">Keywords</div><span class="Keyword" epub:type="keyword">BERT</span><span class="Keyword" epub:type="keyword">Language model</span><span class="Keyword" epub:type="keyword">GPT-2</span><span class="Keyword" epub:type="keyword">Transformer</span><span class="Keyword" epub:type="keyword">Pre-training</span><span class="Keyword" epub:type="keyword">Fine-tuning</span><span class="Keyword" epub:type="keyword">Sequence-to-sequence model</span></div><!--End Abstract--><div class="Fulltext"><div class="Para" id="Par2">A model that either computes the joint probability or the conditional probability of natural language texts is called a <em class="EmphasisTypeItalic ">language model</em> as it potentially covers all information about the language. In this chapter, we present the main architecture types of attention-based <em class="EmphasisTypeItalic ">language models</em><span id="ITerm1"/> (<em class="EmphasisTypeItalic ">LMs</em>), which process texts consisting of sequences of <em class="EmphasisTypeItalic ">tokens</em><span id="ITerm2"/>, i.e. words, numbers, punctuation, etc.: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par3"><em class="EmphasisTypeItalic ">Autoencoders</em><span id="ITerm3"/><span id="ITerm4"/> (<em class="EmphasisTypeItalic ">AE</em>) receive an input text and produce a contextual embedding for each token. These models are also called <em class="EmphasisTypeItalic ">BERT models</em><span id="ITerm5"/> and are described in Sect. <span class="InternalRef"><a href="#Sec1">2.1</a></span>.</p></li><li><p class="Para" id="Par4"><em class="EmphasisTypeItalic ">Autoregressive</em><span id="ITerm6"/><span id="ITerm7"/><em class="EmphasisTypeItalic ">language models</em> (<em class="EmphasisTypeItalic ">AR</em>) receive a subsequence <em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em>−1</sub> of tokens of the input text. They generate contextual embeddings for each token and use them to predict the next token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub>. In this way, they can successively predict all tokens of the sequence. These models are also called <em class="EmphasisTypeItalic ">GPT models</em> and are outlined in Sect. <span class="InternalRef"><a href="#Sec11">2.2</a></span>.</p></li><li><p class="Para" id="Par5"><em class="EmphasisTypeItalic ">Transformer Encoder-Decoders</em><span id="ITerm8"/><span id="ITerm9"/> have the task to translate an input sequence in to another sequence, e.g. for language translation. First they generate a contextual embedding for each input token by an autoencoder. Then these embeddings are used as input to an autoregressive language model, which sequentially generates the output sequence tokens. These models are also called <em class="EmphasisTypeItalic ">Transformers</em><span id="ITerm10"/> and are defined in Sect. <span class="InternalRef"><a href="#Sec19">2.3</a></span>.<span id="ITerm11"/></p></li></ul></div> In this chapter, we focus on NLP, where we consider sequences of text tokens. Historically, the transformer encoder-decoder was developed in 2017 by Vaswani et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>] to perform translation of text into another language. The <em class="EmphasisTypeItalic ">autoencoder</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>] and the <em class="EmphasisTypeItalic ">autoregressive language model</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR118" role="doc-biblioref">118</a></span>] are the encoder-part and the decoder-part of this transformer encoder-decoder and were proposed later. As they are conceptually simpler, they are introduced in preceding sections. A final section (Sect. <span class="InternalRef"><a href="#Sec27">2.4</a></span>) describes methods for optimizing models during training, determining a model architecture, and estimating the uncertainty of model predictions.</div><p class="Para" id="Par6">It turned out that the models can first be trained on a large training set of general text documents and are able to acquire the distribution of tokens in correct and fluent language. Subsequently, they can be adapted to a specific task, e.g. by fine-tuning with a small supervised classification task. Therefore, the models are called <em class="EmphasisTypeItalic ">Pre-trained Language models</em>.</p><p class="Para" id="Par7">As we will see later, all models can be applied to arbitrary sequences, e.g. musical notes, sound, speech, images, or even videos. When the number of parameters of these models gets large, they often can be instructed by prompts and are called <em class="EmphasisTypeItalic ">Foundation Models</em>.</p><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading"><span class="HeadingNumber">2.1 </span>BERT: Self-Attention and Contextual Embeddings</h2><p class="Para" id="Par8">Common words often have a large number of different meanings. For the word <em class="EmphasisTypeItalic ">“bank”</em>, for instance, the lexical database WordNet [<span class="CitationRef"><a epub:type="biblioref" href="#CR94" role="doc-biblioref">94</a></span>] lists 18 different senses from <em class="EmphasisTypeItalic ">“sloping land”</em> to <em class="EmphasisTypeItalic ">“financial institution”</em>. In a simple embedding of the word <em class="EmphasisTypeItalic ">“bank”</em> introduced in Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec5"><span class="RefSource">1.​5</span></a></span> all these meanings are conflated. As a consequence, the interpretation of text based on these embeddings is flawed.</p><p class="Para" id="Par9">As an alternative, <em class="EmphasisTypeItalic ">contextual embeddings</em><span id="ITerm12"/><span id="ITerm13"/> or contextualized embeddings<span id="ITerm14"/> were developed, where the details of a word embedding depend on the word itself as well as on the neighboring words occurring in the specific document. Consequently, each occurrence of the same word in the text has a different embedding depending on the context. Starting with the Transformer [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>], a number of approaches have been designed to generate these contextual embeddings, which are generally trained in an unsupervised manner using a large corpus of documents.</p><p class="Para" id="Par10"><strong class="EmphasisTypeBold ">BERT</strong><span id="ITerm15"/> (Bidirectional Encoder Representations from Transformers) was proposed by Devlin et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>] and is the most important approach for generating contextual embeddings. BERT is based on the concept of attention [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>] and on prior work by Vaswani et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>]. The notion of <strong class="EmphasisTypeBold ">attention</strong><span id="ITerm16"/> is inspired by a brain mechanism that tends to focus on distinctive parts of memory when processing large amounts of information. The details of the computations are explained by Rush [<span class="CitationRef"><a epub:type="biblioref" href="#CR126" role="doc-biblioref">126</a></span>].</p><section class="Section2 RenderAsSection2" id="Sec2"><h3 class="Heading"><span class="HeadingNumber">2.1.1 </span>BERT Input Embeddings and Self-Attention</h3><p class="Para" id="Par11">As input BERT takes some text which is converted to tokens, e.g. by the Wordpiece tokenizer (Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec2"><span class="RefSource">1.​2</span></a></span>) with a vocabulary of a selected size, e.g. 30,000. This means that frequent words like <em class="EmphasisTypeItalic ">“dog”</em> are represented by a token of their own, but more rare words like <em class="EmphasisTypeItalic ">“playing”</em> are split into several tokens, e.g. <em class="EmphasisTypeItalic ">“play”</em> and <em class="EmphasisTypeItalic ">“##ing”</em>, where <em class="EmphasisTypeItalic ">“##”</em> indicates that the token is part of a word. As all characters are retained as tokens, arbitrary words may be represented by a few tokens. In addition, there are special tokens like <em class="EmphasisTypeItalic ">[CLS]</em> at the first position of the input text and two <em class="EmphasisTypeItalic ">“[SEP]”</em> tokens marking the end of text segments. Finally, during training, there are <em class="EmphasisTypeItalic ">[MASK]</em> tokens as explained later. Each token is represented by a <em class="EmphasisTypeItalic ">token embedding</em><span id="ITerm17"/>, a vector of fixed length <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub>, e.g. <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub> = 768. Input sequences of variable length are padded to the maximal length with a special padding token.</p><div class="Para" id="Par12">Since all token embeddings are processed simultaneously, the tokens need an indication of their position in the input text. Therefore, each position is marked with <em class="EmphasisTypeItalic ">position embeddings</em><span id="ITerm18"/> of the same length as the token embeddings, which encode the position index. The BERT paper encodes the position number by trainable embeddings, which are added to the input token embeddings [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>]. Finally, BERT compares the first and second input segment. Therefore, the algorithm needs the information, which token belongs to the first and second segment. This is also encoded by a trainable segment embedding added to the token and position embedding. The sum of all embeddings is used as <em class="EmphasisTypeItalic ">input embedding</em><span id="ITerm19"/> for BERT. An example is shown in Fig. <span class="InternalRef"><a href="#Fig1">2.1</a></span>.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><img alt="" aria-describedby="d64e603" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig1_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e603"><p class="Para" id="Par225">A table has 3 rows of position embeddings, segment embeddings, and token embeddings x subscript t for the input tokens v subscript t with their respective rows of data.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.1</span><p class="SimplePara">The input of the BERT model consist of a sequence of embeddings corresponding to the input tokens. Each token is represented by a sum consisting of the embedding of the token text, the embedding of its segment indicator and an embedding of its position [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>]</p></div></figcaption></figure></div><section class="Section3 RenderAsSection3" id="Sec3"><h4 class="Heading">Self-Attention to Generate Contextual Embeddings</h4><div class="Para" id="Par13">BERT starts with input embeddings <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> of length <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub> for each token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub> of the input sequence <em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">T</em></sub>. These embeddings are transformed by linear mappings to so-called <em class="EmphasisTypeItalic ">query-vectors</em><span id="ITerm20"/><em><strong class="EmphasisTypeBoldItalic ">q</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub>, <em class="EmphasisTypeItalic ">key-vectors</em><span id="ITerm21"/><em><strong class="EmphasisTypeBoldItalic ">k</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> and <em class="EmphasisTypeItalic ">value-vectors</em><span id="ITerm22"/><em><strong class="EmphasisTypeBoldItalic ">v</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub>. These are computed by multiplying <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> with the matrices <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sup>(<em class="EmphasisTypeItalic ">q</em>)</sup>, <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sup>(<em class="EmphasisTypeItalic ">k</em>)</sup>, and <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sup>(<em class="EmphasisTypeItalic ">v</em>)</sup> with dimensions <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub> × <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">q</em></sub>, <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub> × <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">q</em></sub> and <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub> × <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">v</em></sub> respectively <div class="Equation NumberedEquation" id="Equ1"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \boldsymbol{q}_t^\intercal={\boldsymbol{x}}_t^\intercal {\boldsymbol{W}}^{(q)} \qquad  \boldsymbol{k}_t^\intercal = {\boldsymbol{x}}_t^\intercal {\boldsymbol{W}}^{(k)} \qquad  {\boldsymbol{v}}_t^\intercal={\boldsymbol{x}}_t^\intercal {\boldsymbol{W}}^{(v)}. {} \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ1.png" style="width:22.38em"/></div></div> <div class="EquationNumber">(2.1)</div></div></div> Note that the query- and key-vectors have the same length. Then scalar products <span class="InlineEquation" id="IEq1"><img alt="$$\boldsymbol {q}^\intercal _r\boldsymbol {k}_t$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq1.png" style="width:2.31em"/></span> between the query-vector <em><strong class="EmphasisTypeBoldItalic ">q</strong></em><sub><em class="EmphasisTypeItalic ">r</em></sub> of a target token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">r</em></sub> and the key-vectors <em><strong class="EmphasisTypeBoldItalic ">k</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> of all tokens of the sequence are computed: <div class="Equation NumberedEquation" id="Equ2"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} (\alpha_{r,1},\ldots,\alpha_{r,T})=\operatorname{\mathrm{softmax}}\left( \frac{\boldsymbol{q}^\intercal_r\boldsymbol{k}_1}{\sqrt{d_k}},\ldots, \frac{\boldsymbol{q}^\intercal_r\boldsymbol{k}_T}{\sqrt{d_k}}\right). {} \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ2.png" style="width:20.68em"/></div></div> <div class="EquationNumber">(2.2)</div></div></div> Each scalar product yields a real-valued <em class="EmphasisTypeItalic ">association score</em><span id="ITerm23"/><span class="InlineEquation" id="IEq2"><img alt="$$(\boldsymbol {q}^\intercal _r\boldsymbol {k}_t)/\sqrt {d_k}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq2.png" style="width:5.37em"/></span> between the tokens, which depends on the matrices <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sup>(<em class="EmphasisTypeItalic ">q</em>)</sup> and <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sup>(<em class="EmphasisTypeItalic ">k</em>)</sup>. This association score is called <em class="EmphasisTypeItalic ">scaled dot-product attention</em><span id="ITerm24"/>. It is normalized to a probability score <em class="EmphasisTypeItalic ">α</em><sub><em class="EmphasisTypeItalic ">r</em>,<em class="EmphasisTypeItalic ">t</em></sub> by the softmax function. The factor <span class="InlineEquation" id="IEq3"><img alt="$$1/\sqrt {d_k}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq3.png" style="width:3.06em"/></span> avoids large values, where the softmax function has only tiny gradients. With these weights a weighted average of the value vectors <em><strong class="EmphasisTypeBoldItalic ">v</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> of all sequence elements is formed yielding the new embedding <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>̆<sub><em class="EmphasisTypeItalic ">r</em></sub> of length <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">v</em></sub> for the target token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">r</em></sub>: <div class="Equation NumberedEquation" id="Equ3"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \breve{{\boldsymbol{x}}}_r = \alpha_{r,1}*{\boldsymbol{v}}_1+\cdots+\alpha_{r,T}*{\boldsymbol{v}}_T {}. \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ3.png" style="width:14.25em"/></div></div> <div class="EquationNumber">(2.3)</div></div></div> This algorithm is called <em class="EmphasisTypeItalic ">self-attention</em><span id="ITerm25"/> and was first proposed by Vaswani et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>]. Figure <span class="InternalRef"><a href="#Fig2">2.2</a></span> shows the computations for the <em class="EmphasisTypeItalic ">r</em>-th token <em class="EmphasisTypeItalic ">“mouse”</em>. Note that the resulting embedding is a <em class="EmphasisTypeItalic ">contextual embedding</em><span id="ITerm26"/> as it includes information about all words in the input text. A component of <em><strong class="EmphasisTypeBoldItalic ">v</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> gets a high weight whenever the scalar product <span class="InlineEquation" id="IEq4"><img alt="$$\boldsymbol {q}^\intercal _r\boldsymbol {k}_t$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq4.png" style="width:2.31em"/></span> is large. It measures a specific form of a correlation between <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em class="EmphasisTypeItalic ">r</em></sub> and <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> and is maximal if the vector <span class="InlineEquation" id="IEq5"><img alt="$${\boldsymbol {x}}_r^\intercal {\boldsymbol {W}}^{(q)}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq5.png" style="width:3.57em"/></span> points in the same direction as <span class="InlineEquation" id="IEq6"><img alt="$${\boldsymbol {x}}_t^\intercal {\boldsymbol {W}}^{(k)}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq6.png" style="width:3.57em"/></span>.<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO2"><img alt="" aria-describedby="d64e1365" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig2_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e1365"><p class="Para" id="Par226">A flow diagram of the input tokens the, mouse, MASK within square brackets, and cheese have an interconnected system of embedding. They are divided into sections of embedding vector, query and key and value vectors, association and probability scores, weighted value vectors, and new embedding.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.2</span><p class="SimplePara">Computation of a contextual embedding for a single token <em class="EmphasisTypeItalic ">“mouse”</em> by self-attention. By including the embedding of “cheese”, the embedding of mouse can be shifted to the meaning of “rodent” and away from “computer pointing device”. Such an embedding is computed for every word of the input sequence</p></div></figcaption></figure></div><p class="Para" id="Par14">The self-attention mechanism in general is non-symmetric, as the matrices <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sup>(<em class="EmphasisTypeItalic ">q</em>)</sup> and <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sup>(<em class="EmphasisTypeItalic ">k</em>)</sup> are different. If token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">i</em></sub> has a high attention to token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">j</em></sub> (i.e. <span class="InlineEquation" id="IEq7"><img alt="$$\boldsymbol {q}^\intercal _i\boldsymbol {k}_j$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq7.png" style="width:2.31em"/></span> is large), this does not necessarily mean that <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">j</em></sub> will highly attend to token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">i</em></sub> (i.e. <span class="InlineEquation" id="IEq8"><img alt="$$\boldsymbol {q}^\intercal _j\boldsymbol {k}_i$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq8.png" style="width:2.31em"/></span> also is large). The influence of <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">i</em></sub> on the contextual embedding of <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">j</em></sub> therefore is different from the influence of <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">j</em></sub> on the contextual embedding of <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">i</em></sub>. Consider the following example text <em class="EmphasisTypeItalic ">“Fred gave roses to Mary”</em>. Here the word <em class="EmphasisTypeItalic ">“gave”</em> has different relations to the remaining words. <em class="EmphasisTypeItalic ">“Fred”</em> is the person who is performing the giving, <em class="EmphasisTypeItalic ">“roses”</em> are the objects been given, and <em class="EmphasisTypeItalic ">“Mary”</em> is the recipient of the given objects. Obviously these semantic role relations are non-symmetric. Therefore, they can be captured with the different matrices <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sup>(<em class="EmphasisTypeItalic ">q</em>)</sup> and <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sup>(<em class="EmphasisTypeItalic ">k</em>)</sup> and can be encoded in the embeddings.</p><p class="Para" id="Par15">Self-attention allows for shorter computation paths and provides direct avenues to compare distant elements in the input sequence, such as a pronoun and its antecedent in a sentence. The multiplicative interaction involved in attention provides a more flexible alternative to the inflexible fixed-weight computation of MLPs and CNNs by dynamically adjusting the computation to the input at hand. This is especially useful for language modeling, where, for instance, the sentence <em class="EmphasisTypeItalic ">“She ate the ice-cream with the X”</em> is processed. While a feed-forward network would always process it in the same way, an attention-based model could adapt its computation to the input and update the contextual embedding of the word <em class="EmphasisTypeItalic ">“ate”</em> if <em class="EmphasisTypeItalic ">X</em> is <em class="EmphasisTypeItalic ">“spoon”</em>, or update the embedding of <em class="EmphasisTypeItalic ">“ice-cream”</em> if <em class="EmphasisTypeItalic ">X</em> refers to <em class="EmphasisTypeItalic ">“strawberries”</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>].</p><div class="Para" id="Par16">In practice all query, key, and value vectors are computed in parallel by <em><strong class="EmphasisTypeBoldItalic ">Q</strong></em> = <em><strong class="EmphasisTypeBoldItalic ">XW</strong></em><sup>(<em class="EmphasisTypeItalic ">q</em>)</sup>, <em><strong class="EmphasisTypeBoldItalic ">K</strong></em> = <em><strong class="EmphasisTypeBoldItalic ">XW</strong></em><sup>(<em class="EmphasisTypeItalic ">k</em>)</sup>, <em><strong class="EmphasisTypeBoldItalic ">V</strong></em>  = <em><strong class="EmphasisTypeBoldItalic ">XW</strong></em><sup>(<em class="EmphasisTypeItalic ">v</em>)</sup>, where <em><strong class="EmphasisTypeBoldItalic ">X</strong></em> is the <em class="EmphasisTypeItalic ">T</em> × <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub> matrix of input embeddings [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>]. The query-vectors <em><strong class="EmphasisTypeBoldItalic ">q</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub>, key-vectors <em><strong class="EmphasisTypeBoldItalic ">k</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> and value vectors <em><strong class="EmphasisTypeBoldItalic ">v</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> are the rows of <em><strong class="EmphasisTypeBoldItalic ">Q</strong></em>, <em><strong class="EmphasisTypeBoldItalic ">K</strong></em>, <em><strong class="EmphasisTypeBoldItalic ">V</strong></em> respectively. Then the self-attention output matrix ATTL(X) is calculated by one large matrix expression <div class="Equation NumberedEquation" id="Equ4"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \breve{{\boldsymbol{X}}}=\text{ATTL}({\boldsymbol{X}})=\text{ATTL}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})=\operatorname{\mathrm{softmax}}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\intercal}{\sqrt{d_k}}\right)\boldsymbol{V} {}, \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ4.png" style="width:26.82em"/></div></div> <div class="EquationNumber">(2.4)</div></div></div> resulting in a <em class="EmphasisTypeItalic ">T</em> × <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">v</em></sub>-matrix <em><strong class="EmphasisTypeBoldItalic ">X</strong></em>̆. Its <em class="EmphasisTypeItalic ">r</em>-th row contains the new embedding <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>̆<sub><em class="EmphasisTypeItalic ">r</em></sub> of the <em class="EmphasisTypeItalic ">r</em>-th token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">r</em></sub>.</div><p class="Para" id="Par17">A number of alternative compatibility measures instead of the scaled dot-product attention (<span class="InternalRef"><a href="#Equ2">2.2</a></span>) have been proposed. They are, however, rarely used in PLMs, as described in the surveys [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>].</p><div class="Para" id="Par18">It turns out that a single self-attention module is not sufficient to characterize the tokens. Therefore, in a layer <em class="EmphasisTypeItalic ">d</em><sub>head</sub> parallel self-attentions are computed with different matrices <span class="InlineEquation" id="IEq9"><img alt="$${\boldsymbol {W}}^{(q)}_m$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq9.png" style="width:2.37em"/></span>, <span class="InlineEquation" id="IEq10"><img alt="$${\boldsymbol {W}}^{(k)}_m$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq10.png" style="width:2.44em"/></span>, and <span class="InlineEquation" id="IEq11"><img alt="$${\boldsymbol {W}}^{(v)}_m$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq11.png" style="width:2.37em"/></span>, <em class="EmphasisTypeItalic ">m</em> = 1, …, <em class="EmphasisTypeItalic ">d</em><sub>head</sub>, yielding partial new embeddings <div class="Equation NumberedEquation" id="Equ5"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \breve{{\boldsymbol{X}}}_m = \text{ATTL}({\boldsymbol{X}}{\boldsymbol{W}}^{(q)}_m, {\boldsymbol{X}}{\boldsymbol{W}}^{(k)}_m, {\boldsymbol{X}}{\boldsymbol{W}}^{(v)}_m) {}. \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ5.png" style="width:18.32em"/></div></div> <div class="EquationNumber">(2.5)</div></div></div> The emerging partial embeddings <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>̆<sub><em class="EmphasisTypeItalic ">m</em>,<em class="EmphasisTypeItalic ">t</em></sub> for a token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub> are able to concentrate on complementary semantic aspects, which develop during training.</div><div class="Para" id="Par19">The BERT<sub>BASE</sub> model has <em class="EmphasisTypeItalic ">d</em><sub>head</sub>=12 of these parallel <em class="EmphasisTypeItalic ">attention heads</em><span id="ITerm27"/>. The lengths of these head embeddings are only a fraction <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub>∕<em class="EmphasisTypeItalic ">d</em><sub>head</sub> of the original length <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub>. The resulting embeddings are concatenated and multiplied with a (<em class="EmphasisTypeItalic ">d</em><sub>head</sub> ∗ <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">v</em></sub>) × <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub>-matrix <em class="EmphasisTypeItalic ">W</em><sup>(<em class="EmphasisTypeItalic ">o</em>)</sup> yielding the matrix of intermediate embeddings <div class="Equation NumberedEquation" id="Equ6"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \breve{{\boldsymbol{X}}} &amp;amp;= \left[\breve{{\boldsymbol{X}}}_1,\ldots,\breve{{\boldsymbol{X}}}_{d_{\text{head}}}\right] {\boldsymbol{W}}_0 {}, \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ6.png" style="width:12.5em"/></div></div> <div class="EquationNumber">(2.6)</div></div></div> where <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sub>0</sub> is a parameter matrix. If the length of the input embeddings is <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub>, the length of the query, key, and value vector is chosen as <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">k</em></sub> = <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">v</em></sub> = <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub>∕<em class="EmphasisTypeItalic ">d</em><sub>head</sub>. Therefore, the concatenation again creates a <em class="EmphasisTypeItalic ">T</em> × <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub> matrix <em><strong class="EmphasisTypeBoldItalic ">X</strong></em>̆. This setup is called <em class="EmphasisTypeItalic ">multi-head self-attention</em><span id="ITerm28"/>. Because of the reduced dimension of the individual heads, the total computational cost is similar to that of a single-head attention with full dimensionality.</div><div class="Para" id="Par20">Subsequently, each row of <em><strong class="EmphasisTypeBoldItalic ">X</strong></em>̆, the intermediate embedding vectors <span class="InlineEquation" id="IEq12"><img alt="$$\breve {{\boldsymbol {x}}}_t^\intercal $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq12.png" style="width:1.24em"/></span>, is converted by a <em class="EmphasisTypeItalic ">fully connected layer</em><span id="ITerm29"/><span class="EmphasisTypeSmallCaps ">Fcl</span> with a ReLU activation followed by another linear transformation [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>] <div class="Equation NumberedEquation" id="Equ7"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \tilde{{\boldsymbol{x}}}_t^\intercal &amp;amp;= \text{FCL}(\breve{{\boldsymbol{x}}}_t) =ReLU(\breve{{\boldsymbol{x}}}_t^\intercal*{\boldsymbol{W}}_1+\boldsymbol{b}_1^\intercal)*{\boldsymbol{W}}_2 + \boldsymbol{b}_2^\intercal {}. \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ7.png" style="width:22.88em"/></div></div> <div class="EquationNumber">(2.7)</div></div></div> The matrices <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sub>0</sub>, <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sub>1</sub>, <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sub>2</sub> and the vectors <em><strong class="EmphasisTypeBoldItalic ">b</strong></em><sub>1</sub>, <em><strong class="EmphasisTypeBoldItalic ">b</strong></em><sub>2</sub> are parameters. These transformations are the same for each token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub> of the sequence yielding the embedding <span class="InlineEquation" id="IEq13"><img alt="$$\tilde {{\boldsymbol {x}}}_t $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq13.png" style="width:1.18em"/></span>.</div><div class="Para" id="Par21">To improve training speed, <em class="EmphasisTypeItalic ">residual connections</em><span id="ITerm30"/> are added as a “bypass”, which simply copy the input. They were shown to be extremely helpful for the optimization of multi-layer image classifiers [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>]. In addition, <em class="EmphasisTypeItalic ">layer normalization</em><span id="ITerm31"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>] is used for regularization (Sect. <span class="InternalRef"><a href="#Sec32">2.4.2</a></span>), as shown in Fig. <span class="InternalRef"><a href="#Fig3">2.3</a></span>. Together the multi-head self-attention (<span class="InternalRef"><a href="#Equ5">2.5</a></span>), the concatenation (<span class="InternalRef"><a href="#Equ6">2.6</a></span>), and the fully connected layer (<span class="InternalRef"><a href="#Equ7">2.7</a></span>) form an <em class="EmphasisTypeItalic ">encoder block</em><span id="ITerm32"/>.<figure class="Figure" id="Fig3"><div class="MediaObject" id="MO3"><img alt="" aria-describedby="d64e2469" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig3_HTML.png" style="width:33.85em"/><div class="TextObject" id="d64e2469"><p class="Para" id="Par227">A flow diagram of the inputs the, mouse, MASK within square brackets, and cheese have an interconnected system of embedding divided into sections of input embeddings, self-attention, concatenation of partial embeddings, feed-forward and non-linearity, and output embedding via residual connections.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.3</span><p class="SimplePara">Multi-head self-attention computes self-attentions for each layer <em class="EmphasisTypeItalic ">l</em> and head <em class="EmphasisTypeItalic ">m</em> with different matrices <span class="InlineEquation" id="IEq14"><img alt="$${ \boldsymbol {W}}^{(q)}_{l,m}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq14.png" style="width:2.62em"/></span>, <span class="InlineEquation" id="IEq15"><img alt="$${ \boldsymbol {W}}^{(k)}_{l,m}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq15.png" style="width:2.62em"/></span>, and <span class="InlineEquation" id="IEq16"><img alt="$${ \boldsymbol {W}}^{(v)}_{l,m}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq16.png" style="width:2.62em"/></span>. In this way, different aspects of the association between token pairs, e.g. “mouse” and “cheese”, can be computed. The resulting embeddings are concatenated and transformed by a feedforward network. In addition, residual connections and layer normalization improve training convergence [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>]</p></div></figcaption></figure></div><div class="Para" id="Par22">This procedure is repeated for a number of <em class="EmphasisTypeItalic ">k</em> layers with different encoder blocks, using the output embeddings of one block as input embeddings of the next block. This setup is shown in Fig. <span class="InternalRef"><a href="#Fig4">2.4</a></span>. The embeddings <span class="InlineEquation" id="IEq17"><img alt="$$\tilde {{\boldsymbol {x}}}_{k,t}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq17.png" style="width:1.82em"/></span> of the last encoder block provides the desired contextual embeddings. The structure of an encoder block overcomes the limitations of RNNs (namely the sequential nature of RNNs) by allowing each token in the input sequence to directly determine associations with every other token in the sequence. BERT<sub>BASE</sub> has <em class="EmphasisTypeItalic ">k</em>=12 encoder blocks. It was developed at Google by Devlin et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>]. More details on the implementation of self-attention can be found in these papers [<span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR126" role="doc-biblioref">126</a></span>].<figure class="Figure" id="Fig4"><div class="MediaObject" id="MO4"><img alt="" aria-describedby="d64e2534" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig4_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e2534"><p class="Para" id="Par228">A flow diagram of the input tokens the, mouse, MASK within square brackets, and cheese have an interconnected system of embedding. They are divided into sections of input embedding, parallel self-attention, embedding vector, fully connected layer, and target word, among others via encoder blocks.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.4</span><p class="SimplePara">Parallel computation of contextual embeddings in each encoder block by BERT. The output embeddings of an encoder block are used as input embeddings of the next encoder block. Finally, masked tokens are predicted by a logistic classifier <em class="EmphasisTypeItalic ">L</em> using the corresponding contextual embedding of the last encoder block as input</p></div></figcaption></figure></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec4"><h3 class="Heading"><span class="HeadingNumber">2.1.2 </span>Training BERT by Predicting Masked Tokens</h3><div class="Para" id="Par23">The BERT model has a large number of unknown parameters. These parameters are trained in a two-step procedure. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par24"><em class="EmphasisTypeItalic ">Pre-training</em><span id="ITerm33"/> enables the model to acquire general knowledge about language in an unsupervised way. The model has the task to fill in missing words in a text. As no manual annotation is required, pre-training can use large text corpora.</p></li><li><p class="Para" id="Par25"><em class="EmphasisTypeItalic ">Fine-tuning</em><span id="ITerm34"/> adjusts the pre-trained model to a specific task, e.g. sentiment analysis. Here, the model parameters are adapted to solve this task using a smaller labeled training dataset.</p></li></ul></div> The performance on the fine-tuning task is much better than without pre-training because the model can use the knowledge acquired during pre-training through <em class="EmphasisTypeItalic ">transfer learning</em><span id="ITerm35"/>.</div><div class="Para" id="Par26">To pre-train the model parameters, a training task is designed: the <em class="EmphasisTypeItalic ">masked language model</em><span id="ITerm36"/><span id="ITerm37"/> (<em class="EmphasisTypeItalic ">MLM</em><span id="ITerm38"/>). Roughly 15% of the input tokens in the training documents are selected for prediction, which is performed by a logistic classifier (Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec3"><span class="RefSource">1.​3</span></a></span>) <div class="Equation NumberedEquation" id="Equ8"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} p(V_t|v_1,\ldots,v_{t-1},v_{t+1}\ldots,v_T)=\operatorname{\mathrm{softmax}}(A\tilde{{\boldsymbol{x}}}_{k,t}+\boldsymbol{b}) {}, \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ8.png" style="width:23.13em"/></div></div> <div class="EquationNumber">(2.8)</div></div></div> receiving the embedding <span class="InlineEquation" id="IEq18"><img alt="$$\tilde {{\boldsymbol {x}}}_{k,t}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq18.png" style="width:1.82em"/></span> of the last layer at position <em class="EmphasisTypeItalic ">t</em> as input to predict the random variable <em class="EmphasisTypeItalic ">V</em><sub><em class="EmphasisTypeItalic ">t</em></sub> of possible tokens at position <em class="EmphasisTypeItalic ">t</em>. This approach avoids cycles where words can indirectly “see themselves”.</div><div class="Para" id="Par27">The tokens to be predicted have to be changed, as otherwise the prediction would be trivial. Therefore, a token selected for prediction is replaced by: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par28">a special <em class="EmphasisTypeItalic ">[MASK]</em> token for 80% of the time (e.g., <em class="EmphasisTypeItalic ">“the mouse likes cheese”</em> becomes <em class="EmphasisTypeItalic ">“the mouse [MASK] cheese”</em>);</p></li><li><p class="Para" id="Par29">a random token for 10% of the time (e.g., <em class="EmphasisTypeItalic ">“the mouse likes cheese”</em> becomes <em class="EmphasisTypeItalic ">“the mouse absent cheese”</em>);</p></li><li><p class="Para" id="Par30">the unchanged label token for 10% of the time (e.g., <em class="EmphasisTypeItalic ">“the mouse likes cheese”</em> becomes <em class="EmphasisTypeItalic ">“the mouse likes cheese”</em>).</p></li></ul></div> The second and third variants were introduced, as there is a discrepancy between pre-training and the subsequent fine-tuning, were there is no <em class="EmphasisTypeItalic ">[MASK]</em> token. The authors mitigate this issue by occasionally replacing <em class="EmphasisTypeItalic ">[MASK]</em> with the original token, or by sampling from the vocabulary. Note that in 1.5% of the cases a random token is inserted. This occasional noise encourages BERT to be less biased towards the masked token (especially when the label token remains unchanged) in its bidirectional context encoding. To predict the masked token, BERT has to concentrate all knowledge about this token in the corresponding output embedding of the last layer, which is the input to the logistic classifier. Therefore, it is often called an <em class="EmphasisTypeItalic ">autoencoder</em><span id="ITerm39"/>, which generates extremely rich output embeddings.</div><p class="Para" id="Par31">In addition to predicting the masked tokens, BERT also has to predict, whether the next sentence is a randomly chosen sentence or the actual following sentence (<em class="EmphasisTypeItalic ">next sentence prediction</em><span id="ITerm40"/>). This requires BERT to consider the relation between two consecutive pieces of text. Again a logistic classifier receiving the embedding of the first <em class="EmphasisTypeItalic ">[CLS]</em> token is used for this classification. However, this task did not have a major impact on BERT’s performance, as BERT simply learned if the topics of both sentences are similar [<span class="CitationRef"><a epub:type="biblioref" href="#CR158" role="doc-biblioref">158</a></span>].</p><p class="Para" id="Par32">In Fig. <span class="InternalRef"><a href="#Fig4">2.4</a></span> the task is to predict a high probability of the token <em class="EmphasisTypeItalic ">“likes”</em> for the input text <em class="EmphasisTypeItalic ">“The mouse [MASK] cheese”</em>. At the beginning of the training this probability will be very small (≈ 1∕no. of tokens). By backpropagation for each unknown parameter the derivative can be determined, indicating how the parameters should be changed to increase the probability of <em class="EmphasisTypeItalic ">“likes”</em>. The unknown parameters of BERT comprise the input embeddings for each token of the vocabulary, the position embeddings for each position, matrices<span class="InlineEquation" id="IEq19"><img alt="$${\boldsymbol {W}}^{(q)}_{l,m}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq19.png" style="width:2.62em"/></span>, <span class="InlineEquation" id="IEq20"><img alt="$${\boldsymbol {W}}^{(k)}_{l,m}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq20.png" style="width:2.62em"/></span>, <span class="InlineEquation" id="IEq21"><img alt="$${\boldsymbol {W}}^{(v)}_{l,m}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq21.png" style="width:2.62em"/></span> for each layer <em class="EmphasisTypeItalic ">l</em> and attention head <em class="EmphasisTypeItalic ">m</em> (<span class="InternalRef"><a href="#Equ4">2.4</a></span>), the parameters of the fully connected layers (<span class="InternalRef"><a href="#Equ7">2.7</a></span>) as well as <em class="EmphasisTypeItalic ">A</em>, <em><strong class="EmphasisTypeBoldItalic ">b</strong></em> of the logistic classifier (<span class="InternalRef"><a href="#Equ8">2.8</a></span>). BERT uses the Adam algorithm [<span class="CitationRef"><a epub:type="biblioref" href="#CR69" role="doc-biblioref">69</a></span>] for stochastic gradient descent.</p><p class="Para" id="Par33">The BERT<sub>BASE</sub> model has a hidden size of <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub>=768, <em class="EmphasisTypeItalic ">k</em>=12 encoder blocks each with <em class="EmphasisTypeItalic ">d</em><sub>head</sub>=12 attention heads, and a total of 110 million parameters. The BERT<sub>LARGE</sub> model has a hidden size of <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">emb</em></sub>=1024, and <em class="EmphasisTypeItalic ">k</em>=24 encoder blocks each with <em class="EmphasisTypeItalic ">d</em><sub>head</sub>=16 attention heads and a total of 340 million parameters [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>]. The English Wikipedia and a book corpus with 3.3 billion words were encoded by the WordPiece tokenizer [<span class="CitationRef"><a epub:type="biblioref" href="#CR154" role="doc-biblioref">154</a></span>] with a vocabulary of 30,000 tokens and used to pre-train BERT. No annotations of the texts by humans were required, so the training is self-supervised. The pre-training took 4 days on 64 TPU chips, which are very fast GPU chips allowing parallel processing. Fine-tuning can be done on a single Graphical Processing Unit (GPU). <span id="ITerm41"/></p><p class="Para" id="Par34">To predict the masked tokens, the model has to learn many types of language understanding features: syntax (<em class="EmphasisTypeItalic ">[MASK]</em> is a good position for a verb), semantics (e.g. the mouse prefers cheese), pragmatics, coreference, etc. Note that the computations can be processed in parallel for each token of the input sequence, eliminating the sequential dependency in Recurrent Neural Networks. This parallelism enables BERT and related models to leverage the full power of modern SIMD (single instruction multiple data) hardware accelerators like GPUs/TPUs, thereby facilitating training of NLP models on datasets of unprecedented size. Reconstructing missing tokens in a sentence has long been used in psychology. Therefore, predicting masked tokens is also called a <em class="EmphasisTypeItalic ">cloze task</em><span id="ITerm42"/> from ‘closure’ in Gestalt theory (a school of psychology).</p><p class="Para" id="Par35">It turns out that BERT achieves excellent results for the prediction of the masked tokens, and that additional encoder blocks markedly increase the accuracy. For example, BERT is able to predict the original words (or parts of words) with an accuracy of 45.9%, although in many cases several values are valid at the target position [<span class="CitationRef"><a epub:type="biblioref" href="#CR125" role="doc-biblioref">125</a></span>]. In contrast to conventional language models, the MLM takes into account the tokens before and after the masked target token. Hence, it is called a <em class="EmphasisTypeItalic ">bidirectional encoder</em><span id="ITerm43"/>. In addition, self-attention directly provides the relation to distant tokens without recurrent model application. Finally, self-attention is fast, as it can be computed in parallel for all input tokens of an encoder block.</p></section>
<section class="Section2 RenderAsSection2" id="Sec5"><h3 class="Heading"><span class="HeadingNumber">2.1.3 </span>Fine-Tuning BERT to Downstream Tasks</h3><p class="Para" id="Par36">Neural networks have already been pre-trained many years ago [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>], but the success of pre-training has become more evident in recent years. During pre-training BERT learns general syntactic and semantic properties of the language. This can be exploited for a special training task during subsequent <em class="EmphasisTypeItalic ">fine-tuning</em><span id="ITerm44"/> with a modified training task. This approach is also called <em class="EmphasisTypeItalic ">transfer learning</em><span id="ITerm45"/> as the knowledge acquired during pre-training is transferred to a related application. In contrast to other models, BERT requires minimal architecture changes for a wide range of natural language processing tasks. At the time of its publication, BERT improved the <span class="EmphasisTypeSmallCaps ">Sota</span> on various natural language processing tasks.</p><div class="Para" id="Par37">Usually, a fine-tuning task requires a classification, solved by applying a logistic classifier <em class="EmphasisTypeItalic ">L</em> to the output embedding <span class="InlineEquation" id="IEq22"><img alt="$$\tilde {{\boldsymbol {x}}}_{k,1}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq22.png" style="width:1.82em"/></span> of the <em class="EmphasisTypeItalic ">[CLS]</em> token at position 1 of BERT’s last encoder block. There are different types of fine-tuning tasks, as shown in Fig. <span class="InternalRef"><a href="#Fig5">2.5</a></span>.<figure class="Figure" id="Fig5"><div class="MediaObject" id="MO5"><img alt="" aria-describedby="d64e3073" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig5_HTML.png" style="width:34.38em"/><div class="TextObject" id="d64e3073"><p class="Para" id="Par229">4 block diagrams explain the text classification and text annotation on the left and text pair classification and span prediction on the right via B E R T encoder blocks.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.5</span><p class="SimplePara">For fine-tuning, BERT is enhanced with an additional layer containing one or more logistic classifiers <em class="EmphasisTypeItalic ">L</em> using the embeddings of the last layer as inputs. This setup may be employed for text classification and comparison of texts with the embedding of <em class="EmphasisTypeItalic ">[CLS]</em> as input of the logistic classifier. For sequence tagging, <em class="EmphasisTypeItalic ">L</em> predicts a class for each sequence token. For span prediction, two logistic classifiers <em class="EmphasisTypeItalic ">L</em><sub>1</sub> and <em class="EmphasisTypeItalic ">L</em><sub>2</sub> predict the start and end of the answer phrase [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>]</p></div></figcaption></figure></div><div class="Para" id="Par38"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par39"><em class="EmphasisTypeItalic ">Text classification</em><span id="ITerm46"/> assigns a sentence to one of two or more classes. Examples are the classification of restaurant reviews as positive/negative or the categorization of sentences as good/bad English. Here the output embedding of the start token <em class="EmphasisTypeItalic ">[CLS]</em> is used as input to <em class="EmphasisTypeItalic ">L</em> to generate the final classification.</p></li><li><p class="Para" id="Par40"><em class="EmphasisTypeItalic ">Text pair classification</em><span id="ITerm47"/> compares two sentences separated by <em class="EmphasisTypeItalic ">“[SEP]”</em>. Examples include classifying whether the second sentence implies, contradicts, or is neutral with respect to the first sentence, or whether the two sentences are semantically equivalent. Again the output embedding of the start token <em class="EmphasisTypeItalic ">[CLS]</em> is used as input to <em class="EmphasisTypeItalic ">L</em>. Sometimes more than one sentence is compared to the root sentence. Then outputs are computed for every sentence pair and jointly normalized to a probability.</p></li><li><p class="Para" id="Par41"><em class="EmphasisTypeItalic ">Word annotation</em><span id="ITerm48"/> marks each word or token of the input text with a specific property. An example is <em class="EmphasisTypeItalic ">Named Entity Recognition</em><span id="ITerm49"/> (<em class="EmphasisTypeItalic ">NER</em>) annotating the tokens with five name classes (e.g. “person”, “location”, …, “other”). Here the same logistic model <em class="EmphasisTypeItalic ">L</em> is applied to every token output embedding <span class="InlineEquation" id="IEq23"><img alt="$$\tilde {{\boldsymbol {x}}}_{k,t}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq23.png" style="width:1.82em"/></span> at position <em class="EmphasisTypeItalic ">t</em> and yields a probability vector of the different entity classes.</p></li><li><p class="Para" id="Par42"><em class="EmphasisTypeItalic ">Span prediction</em><span id="ITerm50"/> tags a short sequence of tokens within a text. An example is <em class="EmphasisTypeItalic ">question answering</em><span id="ITerm51"/>. The input to BERT consists of a question followed by <em class="EmphasisTypeItalic ">“[SEP]”</em> and a context text, which is assumed to contain the answer. Here two different logistic classifiers <em class="EmphasisTypeItalic ">L</em> and <span class="InlineEquation" id="IEq24"><img alt="$$\tilde {L}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq24.png" style="width:1em"/></span> are applied to every token output embedding <span class="InlineEquation" id="IEq25"><img alt="$$\tilde {{\boldsymbol {x}}}_{k,t}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq25.png" style="width:1.82em"/></span> of the context and generate the probability that the answer to the question starts/ends at the specific position. The valid span (i.e. the end is not before the start) with the highest sum of start/end scores is selected as the answer. An example is the input <em class="EmphasisTypeItalic ">“[CLS] When did Caesar die ? [SEP] … On the Ides of March, 44 BC, Caesar was assassinated by a group of rebellious senators …”</em>, where the answer to the question is the span <em class="EmphasisTypeItalic ">“Ides</em><sub><em class="EmphasisTypeItalic ">start</em></sub><em class="EmphasisTypeItalic ">of March, 44 BC</em><sub><em class="EmphasisTypeItalic ">end</em></sub><em class="EmphasisTypeItalic ">”</em>. Span prediction may be applied to a number of similar tasks.</p></li></ul></div></div><p class="Para" id="Par43">Therefore, BERT just needs an extra layer with one or more logistic classifiers for fine-tuning. During fine-tuning with a downstream application, parameters of the logistic models are learned from scratch and usually all parameters in the pre-trained BERT model are adapted. The parameters for the logistic classifiers of the masked language model and the next sentence prediction are not used during fine-tuning.</p></section>
<section class="Section2 RenderAsSection2" id="Sec6"><h3 class="Heading"><span class="HeadingNumber">2.1.4 </span>Visualizing Attentions and Embeddings</h3><p class="Para" id="Par44">According to Bengio et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>], a good representation of language should capture the implicit linguistic rules and common sense knowledge contained in text data, such as lexical meanings, syntactic relations, semantic roles, and the pragmatics of language use. The contextual word embeddings of BERT can be seen as a big step in this direction. They may be used to disambiguate different meanings of the same word.</p><p class="Para" id="Par45">The self-attention mechanism of BERT computes a large number of “associations” between tokens and merges embeddings according to the strengths of these associations. If <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub>1</sub>, …, <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em class="EmphasisTypeItalic ">T</em></sub> are the embeddings of the input tokens <em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">T</em></sub>, the associations <span class="InlineEquation" id="IEq26"><img alt="$$\boldsymbol {q}^\intercal _r\boldsymbol {k}_t$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq26.png" style="width:2.31em"/></span> are determined between the query <span class="InlineEquation" id="IEq27"><img alt="$$\boldsymbol {q}_r^\intercal ={\boldsymbol {x}}_r^\intercal {\boldsymbol {W}}^{(q)}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq27.png" style="width:6.06em"/></span> and the key <span class="InlineEquation" id="IEq28"><img alt="$$\boldsymbol {k}_t^\intercal = {\boldsymbol {x}}_t^\intercal {\boldsymbol {W}}^{(k)}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq28.png" style="width:6.06em"/></span> vectors (<span class="InternalRef"><a href="#Equ1">2.1</a></span>). Then a sum of value vectors <span class="InlineEquation" id="IEq29"><img alt="$${\boldsymbol {v}}_t^\intercal ={\boldsymbol {x}}_t^\intercal {\boldsymbol {W}}^{(v)}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq29.png" style="width:6em"/></span> weighted with the normalized associations is formed yielding the new embeddings (<span class="InternalRef"><a href="#Equ3">2.3</a></span>).</p><div class="Para" id="Par46">This is repeated with different matrices <span class="InlineEquation" id="IEq30"><img alt="$${\boldsymbol {W}}^{(q)}_{l,m},{\boldsymbol {W}}^{(k)}_{l,m},{\boldsymbol {W}}^{(v)}_{l,m}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq30.png" style="width:8.5em"/></span> in <em class="EmphasisTypeItalic ">m</em> self-attention heads and <em class="EmphasisTypeItalic ">l</em> layers. Each layer and head the new embeddings thus captures different aspects of the relations between the embeddings of each layer. For BERT<sub>BASE</sub> we have <em class="EmphasisTypeItalic ">l</em> = 12 layers and <em class="EmphasisTypeItalic ">m</em> = 12 bidirectional self-attention heads in each layer yielding 144 different “associations” or self-attentions. For the input sentence <em class="EmphasisTypeItalic ">“The girl and the boy went home. She entered the door.”</em> Figure <span class="InternalRef"><a href="#Fig6">2.6</a></span> shows on the left side the strength of associations for one of the 144 self-attention heads. Between every pair of tokens of the sentence an attention value is calculated and its strength is symbolized by lines of different widths. We see that the pronoun <em class="EmphasisTypeItalic ">“she”</em> is strongly associated with <em class="EmphasisTypeItalic ">“the girl”</em>. In the subsequent calculations (c.f. Fig. <span class="InternalRef"><a href="#Fig2">2.2</a></span>) the word <em class="EmphasisTypeItalic ">“she”</em> is disambiguated by merging its embedding with the embeddings of <em class="EmphasisTypeItalic ">“the”</em> and <em class="EmphasisTypeItalic ">“girl”</em> generating a new <em class="EmphasisTypeItalic ">contextual embedding</em><span id="ITerm52"/> of <em class="EmphasisTypeItalic ">“she”</em>, which includes its relation to <em class="EmphasisTypeItalic ">“girl”</em>. On the right side of the figure the input <em class="EmphasisTypeItalic ">“The girl and the boy went home. He entered the door.”</em> is processed. Then the model creates an association of <em class="EmphasisTypeItalic ">“boy”</em> with <em class="EmphasisTypeItalic ">“he”</em>.<figure class="Figure" id="Fig6"><div class="MediaObject" id="MO6"><img alt="" aria-describedby="d64e3579" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig6_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e3579"><p class="Para" id="Par230">2 screenshots with layer spin boxes have a horizontal color gradient bar. A list of words read the, girl, and, the, boy, walked, home divided into 2 sections. In the first one, she is linked to the and girl. In the second one, he is linked to the and boy.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.6</span><p class="SimplePara">Visualization of a specific self-attention in the fifth layer of a BERT model with BERTviz [<span class="CitationRef"><a epub:type="biblioref" href="#CR142" role="doc-biblioref">142</a></span>]. If the next sentence contains the pronoun <em class="EmphasisTypeItalic ">“she”</em> this is associated with <em class="EmphasisTypeItalic ">“the girl”</em>. If this pronoun is changed to <em class="EmphasisTypeItalic ">“he”</em> it is related to <em class="EmphasisTypeItalic ">“the boy”</em>. Image created with BERTviz [<span class="CitationRef"><a epub:type="biblioref" href="#CR142" role="doc-biblioref">142</a></span>], with kind permission of the author</p></div></figcaption></figure></div><div class="Para" id="Par47">Figure <span class="InternalRef"><a href="#Fig7">2.7</a></span> shows a subset of the self-attention patterns for the sentence <em class="EmphasisTypeItalic ">“[CLS] the cat sat on the mat [SEP] the cat lay on the rug [SEP]”</em>. The self-attention patterns are automatically optimized in such a way that they jointly lead to an optimal prediction of the masked tokens. It can be seen that the special tokens <em class="EmphasisTypeItalic ">[CLS]</em> and <em class="EmphasisTypeItalic ">[SEP]</em> often are prominent targets of attentions. They usually function as representatives of the whole sentence [<span class="CitationRef"><a epub:type="biblioref" href="#CR124" role="doc-biblioref">124</a></span>]. Note, however, that in a multilayer PLM the embeddings generated by different heads are concatenated and transformed by a nonlinear transformation. Therefore, the attention patterns of a single head do not contain the complete information [<span class="CitationRef"><a epub:type="biblioref" href="#CR124" role="doc-biblioref">124</a></span>]. Whenever the matrices are randomly initialized, the self-attention patterns will be completely different, if the training is restarted with new random parameter values. However, the overall pattern of attentions between tokens will be similar.<figure class="Figure" id="Fig7"><div class="MediaObject" id="MO7"><img alt="" aria-describedby="d64e3618" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig7_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e3618"><p class="Para" id="Par231">A matrix of 24 color gradient interconnected patterns divided into 0 by 3 and 0 by 5 grids labeled heads.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.7</span><p class="SimplePara">Visualization of some of the 144 self-attention patterns computed for the sentence <em class="EmphasisTypeItalic ">“[CLS] the cat sat on the mat [SEP] the cat lay on the rug[SEP]”</em> with BERTviz. Image reprinted with kind permission of the author [<span class="CitationRef"><a epub:type="biblioref" href="#CR142" role="doc-biblioref">142</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par48">Figure <span class="InternalRef"><a href="#Fig10">2.10</a></span> shows on the left side a plot of six different senses of the token embeddings of <em class="EmphasisTypeItalic ">“bank”</em> in the <em class="EmphasisTypeItalic ">Senseval-3 dataset</em><span id="ITerm53"/> projected to two dimensions by <em class="EmphasisTypeItalic ">T-SNE</em><span id="ITerm54"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR140" role="doc-biblioref">140</a></span>]. The different senses are identified by different colors and form well-separated clusters of their own. Senses which are difficult to distinguish, like <em class="EmphasisTypeItalic ">“bank building”</em> and <em class="EmphasisTypeItalic ">“financial institution”</em> show a strong overlap [<span class="CitationRef"><a epub:type="biblioref" href="#CR153" role="doc-biblioref">153</a></span>]. The graphic demonstrates that BERT embeddings have the ability to distinguish different senses of words which are observed frequently enough.</p><p class="Para" id="Par49">There is an ongoing discussion on the inner workings of self attention.Tay et al [<span class="CitationRef"><a epub:type="biblioref" href="#CR134" role="doc-biblioref">134</a></span>] empirically evaluated the importance of the dot product <span class="InlineEquation" id="IEq31"><img alt="$$\boldsymbol {q}^\intercal _r\boldsymbol {k}_s$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq31.png" style="width:2.31em"/></span> on natural language processing tasks and concluded that query-key interaction is “useful but not that important”. Consequently they derived alternative formulae, which in some cases worked well and failed in others. A survey of attention approaches is provided by de Santana Correia et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR37" role="doc-biblioref">37</a></span>]. There are a number of different attention mechanisms computing the association between embedding vectors [<span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR104" role="doc-biblioref">104</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR151" role="doc-biblioref">151</a></span>]. However, most current large-scale models still use the original scaled dot-product attention with minor variations, such as other activation functions and regularizers (c.f. Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec5"><span class="RefSource">3.​1.​4</span></a></span>).</p><p class="Para" id="Par50">The fully connected layers <span class="EmphasisTypeSmallCaps ">Fcl</span>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>̆<sub><em class="EmphasisTypeItalic ">t</em></sub>) in (<span class="InternalRef"><a href="#Equ7">2.7</a></span>) contain 2/3 of the parameters of BERT, but their role in the network has hardly been discussed. Geva et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>] show that fully connected layers operate as key-value memories, where each key is correlated with text patterns in the training samples, and each value induces a distribution over the output vocabulary. For a key the authors retrieve the training inputs, which yield the highest activation of the key. Experts were able to assign one or more interpretations to each key. Usually lower fully connected layers were associated with shallow patterns often sharing the last word. The upper layers are characterized by more semantic patterns that describe similar contexts. The authors demonstrate that the output of a feed-forward layer is a composition of its memories.</p></section>
<section class="Section2 RenderAsSection2" id="Sec7"><h3 class="Heading"><span class="HeadingNumber">2.1.5 </span>Natural Language Understanding by BERT</h3><p class="Para" id="Par51">An outstanding goal of PLMs is <em class="EmphasisTypeItalic ">Natural Language Understanding</em><span id="ITerm55"/> (<em class="EmphasisTypeItalic ">NLU</em>). This cannot be evaluated against a single task, but requires a set of benchmarks covering different areas to assess the ability of machines to understand natural language text and acquire linguistic, common sense, and world knowledge. Therefore, PLMs are fine-tuned to corresponding real-world downstream tasks.</p><div class="Para" id="Par52"><strong class="EmphasisTypeBold ">GLUE</strong><span id="ITerm56"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR146" role="doc-biblioref">146</a></span>] is a prominent benchmark for NLU. It is a collection of nine NLU tasks with public training data, and an evaluation server using private test data. Its benchmarks cover a number of different aspects, which can be formulated as classification problems: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par53">Determine the sentiment (positive/negative) of a sentences (SST-2).</p></li><li><p class="Para" id="Par54">Classify a sentence as grammatically acceptable or unacceptable (CoLA).</p></li><li><p class="Para" id="Par55">Check if two sentences are similar or are paraphrases (MPRC, STS-B, QQP).</p></li><li><p class="Para" id="Par56">Determine if the first sentence entails the second one (MNLI, RTE).</p></li><li><p class="Para" id="Par57">Check if sentence <em class="EmphasisTypeItalic ">B</em> contains the answer to question <em class="EmphasisTypeItalic ">A</em> (QNLI).</p></li><li><p class="Para" id="Par58">Specify the target of a pronoun from a set of alternatives (WNLI).</p></li></ul></div></div><div class="Para" id="Par59">Each task can be posed as <em class="EmphasisTypeItalic ">text classification</em> or <em class="EmphasisTypeItalic ">text pair classification</em> problem. The performance of a model is summarized in a single average value, which has the value 87.1 for human annotators [<span class="CitationRef"><a epub:type="biblioref" href="#CR145" role="doc-biblioref">145</a></span>]. Usually, there is an online leaderboard where the performance of the different models are recorded. A very large repository of leaderboards is on the PapersWithCode website [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>]. Table <span class="InternalRef"><a href="#Tab1">2.1</a></span> describes the tasks by examples and reports the performance of BERT<sub>LARGE</sub>. BERT was able to lift the <span class="EmphasisTypeSmallCaps ">Sota</span> of average accuracy from 75.2 to 82.1%. This is a remarkable increase, although the value is still far below the human performance of 87.1 with much room for improvement. Recent benchmark results for NLU are described in Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec1"><span class="RefSource">4.​1</span></a></span> for the more demanding SuperGLUE and other benchmarks. <div class="Table" id="Tab1"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 2.1</span><p class="SimplePara">GLUE language understanding tasks. BERT<sub>LARGE</sub> was trained for three epochs on the fine-tuning datasets [<span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>]. The performance of the resulting models is printed in the last column yielding an average value of 82.1</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/><col class="tcol5"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Task</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Description</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Example</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Metric</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BERT</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">CoLA</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Is the sentence grammatical or ungrammatical?</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">“This building is than that one.”</em><span class="InlineEquation" id="IEq32"><img alt="$$\rightarrow $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq32.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">Ungrammatical</em></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Matthews correlation</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">60.5</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">SST-2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Is the movie positive, negative, or neutral?</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">“The movie is funny, smart, visually inventive, and most of all, alive.”</em><span class="InlineEquation" id="IEq33"><img alt="$$\rightarrow $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq33.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">Positive</em></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Accuracy</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">94.9</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MRPC</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Is the sentence <em class="EmphasisTypeItalic ">B</em> a paraphrase of sentence <em class="EmphasisTypeItalic ">A</em>?</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">A</em>: <em class="EmphasisTypeItalic ">“Today, Taiwan reported 35 new infections.”</em><em class="EmphasisTypeItalic ">B</em>: <em class="EmphasisTypeItalic ">“Taiwan announced another 35 probable cases at noon.”</em><span class="InlineEquation" id="IEq34"><img alt="$$\rightarrow $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq34.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">Paraphrase</em></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Accuracy</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">89.3</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">STS-B</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">How similar are sentences <em class="EmphasisTypeItalic ">A</em> and <em class="EmphasisTypeItalic ">B</em>?</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">A</em>: <em class="EmphasisTypeItalic ">“Elephants are walking down a trail.”</em><em class="EmphasisTypeItalic ">B</em>: <em class="EmphasisTypeItalic ">“A herd of elephants is walking down a trail.”</em><span class="InlineEquation" id="IEq35"><img alt="$$\rightarrow $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq35.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">Similar</em></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Pearson/ Spearman correlation</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">86.5</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">QQP</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Are the two questions similar?</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">A</em>: <em class="EmphasisTypeItalic ">“How can I increase the speed of my Internet connection while using a VPN?”</em><em class="EmphasisTypeItalic ">B</em>: <em class="EmphasisTypeItalic ">“How can Internet speed be increased by hacking through DNS?”</em><span class="InlineEquation" id="IEq36"><img alt="$$\rightarrow $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq36.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">Not Similar</em></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Accuracy</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">72.1</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MNLI-mm</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Does sentence <em class="EmphasisTypeItalic ">A</em> entail or contradict sentence <em class="EmphasisTypeItalic ">B</em>?</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">A</em>: <em class="EmphasisTypeItalic ">“Tourist information offices can be very helpful.”</em><em class="EmphasisTypeItalic ">B</em>: <em class="EmphasisTypeItalic ">“Tourist information offices are never of any help.”</em><span class="InlineEquation" id="IEq37"><img alt="$$\rightarrow $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq37.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">Contradiction</em></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Accuracy</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">85.9</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">QNLI</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Does sentence <em class="EmphasisTypeItalic ">B</em> contain the answer to the question in sentence <em class="EmphasisTypeItalic ">A</em>?</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">A</em>: <em class="EmphasisTypeItalic ">“Which collection of minor poems are sometimes attributed to Virgil.”</em><em class="EmphasisTypeItalic ">B</em>: <em class="EmphasisTypeItalic ">“A number of minor poems, collected in the Appendix Vergiliana, are often attributed to him.”</em><span class="InlineEquation" id="IEq38"><img alt="$$\rightarrow $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq38.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">contains answer</em></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Accuracy</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">92.7</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">RTE</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Does sentence <em class="EmphasisTypeItalic ">A</em> entail sentence <em class="EmphasisTypeItalic ">B</em>?</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">A</em>: <em class="EmphasisTypeItalic ">“Yunus launched the microcredit revolution, funding 50,000 beggars, whom Grameen Bank respectfully calls ‘Struggling Members.”’</em><em class="EmphasisTypeItalic ">B</em>: <em class="EmphasisTypeItalic ">“Yunus supported more than 50,000 Struggling Members.”</em><span class="InlineEquation" id="IEq39"><img alt="$$\rightarrow $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq39.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">Entailed</em></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Accuracy</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">70.1</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">WNLI</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Sentence <em class="EmphasisTypeItalic ">B</em> replaces sentence <em class="EmphasisTypeItalic ">A</em>’s pronoun with a noun - is this the correct noun?</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">A</em>: <em class="EmphasisTypeItalic ">“Lily spoke to Donna, breaking her concentration.”</em><em class="EmphasisTypeItalic ">B</em>: <em class="EmphasisTypeItalic ">“Lily spoke to Donna, breaking Lily’s concentration.”</em><span class="InlineEquation" id="IEq40"><img alt="$$\rightarrow $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq40.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">Incorrect</em></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Accuracy</p></td><td style="text-align: left;"><p class="SimplePara">60.5</p></td></tr></tbody></table></div></div><section class="Section3 RenderAsSection3" id="Sec8"><h4 class="Heading">BERT’s Performance on Other Fine-Tuning Tasks</h4><p class="Para" id="Par60">The pre-training data is sufficient to adapt the large number of BERT parameters and learn very detailed peculiarities about language. The amount of training data for pre-training usually is much higher than for fine-tuning. Fine-tuning usually only requires two or three passes through the fine-tuning training data. Therefore, the stochastic gradient optimizer changes most parameters only slightly and sticks relatively close to the optimal pre-training parameters. Consequently, the model is usually capable to preserve its information about general language and to combine it with the information about the fine-tuning task.</p><div class="Para" id="Par61">Because BERT can reuse its general knowledge about language acquired during pre-training, it produces excellent results even with small fine-tuning training data [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>]. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par62"><strong class="EmphasisTypeBold ">CoNLL 2003</strong><span id="ITerm57"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR128" role="doc-biblioref">128</a></span>] is a benchmark dataset for <em class="EmphasisTypeItalic ">Named entity recognition</em><span id="ITerm58"/> (<em class="EmphasisTypeItalic ">NER</em>), where each token has to be marked with a named entity tag, e.g. PER (for person), LOC (for location), …, O (for no name) (Sect. <span class="ExternalRef"><a href="528393_1_En_5_Chapter.xhtml#Sec12"><span class="RefSource">5.​3</span></a></span>). The task involves text annotation, where a label is predicted for every input token. BERT increased <span class="EmphasisTypeSmallCaps ">Sota</span> from 92.6% to 92.8% F1-value on the test data.</p></li><li><p class="Para" id="Par63"><strong class="EmphasisTypeBold ">SQuAD 1.0</strong><span id="ITerm59"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR120" role="doc-biblioref">120</a></span>] is a collection of 100k triples of questions, contexts, and answers. The task is to mark the span of the answer tokens in the context. An example is the question <em class="EmphasisTypeItalic ">“When did Augustus die?”</em>, where the answer <em class="EmphasisTypeItalic ">“14 AD”</em> has to be marked in the context <em class="EmphasisTypeItalic ">“…the death of Augustus in AD 14 …”</em> (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec9"><span class="RefSource">6.​2</span></a></span>). Using span prediction BERT increased the <span class="EmphasisTypeSmallCaps ">Sota</span> of SQuAD from 91.7% to 93.2%, while the human performance was measured as 91.2%.</p></li></ul></div></div><p class="Para" id="Par64">From these experiments a large body of evidence has been collected demonstrating the strengths and weaknesses of BERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR124" role="doc-biblioref">124</a></span>]. This is discussed in Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec7"><span class="RefSource">4.​2</span></a></span>.</p><p class="Para" id="Par65">In summary, the advent of the BERT model marks a new era of NLP. It combines two pre-training tasks, i.e., predicting masked tokens and determining whether the second sentence matches the first sentence. Transfer learning with unsupervised pre-training and supervised fine-tuning becomes the new standard.</p></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec9"><h3 class="Heading"><span class="HeadingNumber">2.1.6 </span>Computational Complexity</h3><p class="Para" id="Par66">It is instructive to illustrate the computational effort required to train PLMs. Its growth determines the time needed to train larger models that can massively improve the quality of language representation. Assume <em class="EmphasisTypeItalic ">D</em> is the size of the hidden embeddings and the input sequence has length <em class="EmphasisTypeItalic ">T</em>, then the intermediate dimension of the fully connected layer <span class="EmphasisTypeSmallCaps ">Fcl</span> is set to 4<em class="EmphasisTypeItalic ">D</em> and the dimension of the keys and values are set to <em class="EmphasisTypeItalic ">D</em>∕<em class="EmphasisTypeItalic ">H</em> as in Vaswani et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>]. Then according to Lin et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR81" role="doc-biblioref">81</a></span>] we get the following computational complexities and parameters counts of self-attention and the position-wise <span class="EmphasisTypeSmallCaps ">Fcl</span> (<span class="InternalRef"><a href="#Equ7">2.7</a></span>):</p><div class="Para" id="Par67"><div class="Table" id="Tab2"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Module</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Complexity</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"># Parameters</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Self-attention</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">T</em><sup>2</sup> ∗ <em class="EmphasisTypeItalic ">D</em>)</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">4<em class="EmphasisTypeItalic ">D</em><sup>2</sup></p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Position-wise <span class="EmphasisTypeSmallCaps ">Fcl</span></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">T</em> ∗ <em class="EmphasisTypeItalic ">D</em><sup>2</sup>)</p></td><td style="text-align: left;"><p class="SimplePara">8<em class="EmphasisTypeItalic ">D</em><sup>2</sup></p></td></tr></tbody></table></div></div><p class="Para" id="Par68">As long as the input sequence length <em class="EmphasisTypeItalic ">T</em> is small, the hidden dimension <em class="EmphasisTypeItalic ">D</em> mainly determines the complexity of self-attention and position-wise <span class="EmphasisTypeSmallCaps ">Fcl</span>. The main limiting factor is the <span class="EmphasisTypeSmallCaps ">Fcl</span>. But when the input sequences become longer, the sequence length <em class="EmphasisTypeItalic ">T</em> gradually dominates the complexity of these modules, so that self-attention becomes the bottleneck of the PLM. Moreover, the computation of self-attention requires that an attention score matrix of size <em class="EmphasisTypeItalic ">T</em> × <em class="EmphasisTypeItalic ">T</em> is stored, which prevents the computation for long input sequences. Therefore, modifications reducing the computational effort for long input sequences are required.</p><p class="Para" id="Par69">To connect all input embeddings with each other, we could employ different modules. Fully connected layers require <em class="EmphasisTypeItalic ">T</em> ∗ <em class="EmphasisTypeItalic ">T</em> networks between the different embeddings. Convolutional layers with a kernel width <em class="EmphasisTypeItalic ">K</em> do not connect all pairs and therefore need <em class="EmphasisTypeItalic ">O</em>(log<sub><em class="EmphasisTypeItalic ">K</em></sub>(<em class="EmphasisTypeItalic ">T</em>)) layers in the case of dilated convolutions. RNNs have to apply a network <em class="EmphasisTypeItalic ">T</em> times. This leads to the following complexities per layer [<span class="CitationRef"><a epub:type="biblioref" href="#CR81" role="doc-biblioref">81</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>]</p><div class="Para" id="Par70"><div class="Table" id="Tab3"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Sequential</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Maximum</p></th></tr><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Layer type</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Complexity per layer</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">operations</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">path length</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Self-attention</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">T</em><sup>2</sup> ∗ <em class="EmphasisTypeItalic ">D</em>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(1)</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(1)</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Recurrent</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">T</em> ∗ <em class="EmphasisTypeItalic ">D</em><sup>2</sup>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">T</em>)</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">T</em>)</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Fully connected</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">T</em><sup>2</sup> ∗ <em class="EmphasisTypeItalic ">D</em><sup>2</sup>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(1)</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(1)</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Convolutional</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">K</em> ∗ <em class="EmphasisTypeItalic ">T</em> ∗ <em class="EmphasisTypeItalic ">D</em><sup>2</sup>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(1)</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(log<sub><em class="EmphasisTypeItalic ">K</em></sub>(<em class="EmphasisTypeItalic ">T</em>))</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Restricted self-attention</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">R</em> ∗ <em class="EmphasisTypeItalic ">T</em> ∗ <em class="EmphasisTypeItalic ">D</em>)</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(1)</p></td><td style="text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">T</em>∕<em class="EmphasisTypeItalic ">R</em>)</p></td></tr></tbody></table></div></div><p class="Para" id="Par71">The last line describes a restricted self-attention, where self-attention only considers a neighborhood of size <em class="EmphasisTypeItalic ">R</em> to reduce computational effort. Obviously the computational complexity per layer is a limiting factor. In addition, computation for recurrent layers need to be sequential and cannot be parallelized, as shown in the column for sequential operations. The last column shows the path length, i.e. the number of computations to communicate information between far-away positions. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies. Here self-attention has a definite advantage compared to all other layer types. Section <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec7"><span class="RefSource">3.​2</span></a></span> discusses advanced approaches to process input sequences of larger length. In conclusion, BERT requires less computational effort than alternative layer types.</p></section>
<section class="Section2 RenderAsSection2" id="Sec10"><h3 class="Heading"><span class="HeadingNumber">2.1.7 </span>Summary</h3><p class="Para" id="Par72"><em class="EmphasisTypeItalic ">BERT</em> is an autoencoder model whose main task is to derive context-sensitive embeddings for tokens. In a preliminary step, tokens are generated from the words and letters of the training data in such a way that most frequent words are tokens and arbitrary words can be composed of tokens. Each token is encoded by an input embedding. To mark the position of each input token, a position embedding is added to the input embedding.</p><p class="Para" id="Par73">In each layer of BERT, the lower layer embeddings are transformed by self-attention to a new embedding. Self-attention involves the computation of scalar products between linear transformations of embeddings. In this way, the embeddings in the next layer can adapt to tokens from the context, and the embeddings become context-sensitive. The operation is performed in parallel for several attention heads involving different linear projections. The heads can compute associations in parallel with respect to different semantic features. The resulting partial embeddings are concatenated to a new embedding. In addition to self-attention heads, each encoder block contains a fully connected layer as well as normalization operations.</p><p class="Para" id="Par74">The original BERT model consists of six encoder blocks and generates a final embedding for each input token. BERT is pre-trained on a very large document collection. The main pre-training task is to predict words from the input sequence, which have been replaced by a [MASK] token. This is done by using the last layer embedding of the token as input to a logistic classifier, which predicts the probabilities of tokens for this position. During pre-training the model parameters are optimized by stochastic gradient descent. This forces the model to collect all available information about that token in the output embedding. The first input token is the [CLS] token. During pre-training, it is used for next sentence prediction, where a logistic classifier with the [CLS]-embedding as input has to decide, if the first and second sentence of the input sequence belong together or not.</p><p class="Para" id="Par75">Typically, the pre-trained model is fine-tuned for a specific task using a small annotated training dataset. An example is the supervised classification task of whether the input text expresses a positive, negative or neutral sentiment. Again a logistic classifier with the [CLS]-embedding as input has to determine the probability of the three sentiments. During pre-training all parameters of the model are adjusted slightly. It turns out that this transfer learning approach has a much higher accuracy than supervised training only on the small training dataset, since the model can use knowledge about language acquired during pre-training.</p><p class="Para" id="Par76">Experiments show that BERT is able to raise the <span class="EmphasisTypeSmallCaps ">Sota</span> considerably in many language understanding tasks, e.g. the GLUE benchmark. Other applications are named entity recognition, where names of persons, locations, etc. have to be identified in a text, or question answering, where the answer to a question has to be extracted from a paragraph. An analysis of computational complexity shows that BERT requires less computational effort than alternative layer types. Overall, BERT is the workhorse of natural language processing and is used in different variants to solve language understanding problems. Its encoder blocks are reused in many other models.</p><p class="Para" id="Par77">Chapter <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml"><span class="RefSource">3</span></a></span> describes ways to improve the performance of BERT models, especially by designing new pre-training tasks (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>). In Chap. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml"><span class="RefSource">4</span></a></span> the knowledge acquired by BERT models is discussed. In the Chaps. <span class="ExternalRef"><a href="528393_1_En_5_Chapter.xhtml"><span class="RefSource">5</span></a></span>–<span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml"><span class="RefSource">7</span></a></span>, we describe a number of applications of BERT models such as relation extraction (Sect. <span class="ExternalRef"><a href="528393_1_En_5_Chapter.xhtml#Sec19"><span class="RefSource">5.​4</span></a></span>) or document retrieval (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec1"><span class="RefSource">6.​1</span></a></span>).</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec11"><h2 class="Heading"><span class="HeadingNumber">2.2 </span>GPT: Autoregressive Language Models</h2><section class="Section2 RenderAsSection2" id="Sec12"><h3 class="Heading"><span class="HeadingNumber">2.2.1 </span>The Task of Autoregressive Language Models</h3><div class="Para" id="Par78">To capture the information in natural language texts the conditional probability of tokens can be described by a language model. These <em class="EmphasisTypeItalic ">autoregressive language models</em><span id="ITerm60"/> aim to predict the probability of the next token in a text given the previous tokens. If <em class="EmphasisTypeItalic ">V</em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub> is a random variable whose values are the possible tokens <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub> at position <em class="EmphasisTypeItalic ">t</em> + 1, we have to calculate the conditional probability distribution <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">V</em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub>|<em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub>). According to the definition of conditional probability the probability of the complete text <em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">T</em></sub> can be computed as <div class="Equation NumberedEquation" id="Equ9"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} p(V_1\mkern1.5mu{=}\mkern1.5mu v_1,\ldots,V_T\mkern1.5mu{=}\mkern1.5mu v_T)= p(V_T\mkern1.5mu{=}\mkern1.5mu v_{T}|v_1,\ldots,v_{T-1})*\cdots*p(V_1\mkern1.5mu{=}\mkern1.5mu v_1) {}. \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ9.png" style="width:29.13em"/></div></div> <div class="EquationNumber">(2.9)</div></div></div> Therefore, the conditional probability can represent all information about valid sentences, including adequate and bad usage of language. Qudar et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR115" role="doc-biblioref">115</a></span>] provide a recent survey of language models.</div><p class="Para" id="Par79">In Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec6"><span class="RefSource">1.​6</span></a></span>, we used RNNs to build language models. However, these had problems determining long-range interactions between tokens. As an alternative, we can employ self-attention to infer contextual embeddings of the past tokens <em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub> and predict the next token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub> based on these embeddings.</p><div class="Para" id="Par80">Consequently, we need to restrict self-attention to the tokens <em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub>. This is the approach taken by the <strong class="EmphasisTypeBold ">Generative Pre-trained Transformer</strong><span id="ITerm61"/> (<strong class="EmphasisTypeBold ">GPT</strong>) [<span class="CitationRef"><a epub:type="biblioref" href="#CR116" role="doc-biblioref">116</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR118" role="doc-biblioref">118</a></span>]. Before training, the text is transformed to tokens, e.g. by byte-pair encoding (Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec2"><span class="RefSource">1.​2</span></a></span>). On input, these tokens are represented by token embeddings and position embeddings (Sect. <span class="InternalRef"><a href="#Sec2">2.1.1</a></span>). During training the GPT-model performs the self-attention computations described in Sect. <span class="InternalRef"><a href="#Sec3">2.1.1</a></span> in the same way as for BERT. For predicting the probabilities of different tokens at position <em class="EmphasisTypeItalic ">t</em> + 1, the self-attentions are restricted to previous tokens <em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub> and their embeddings. The probability of the possible next tokens at position <em class="EmphasisTypeItalic ">t</em> + 1 is computed by a logistic classifier <div class="Equation NumberedEquation" id="Equ10"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} p(V_{t+1}|v_1,\ldots,v_{t})=\operatorname{\mathrm{softmax}}(A\tilde{{\boldsymbol{x}}}_{k,t}+\boldsymbol{b}) {}, \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ10.png" style="width:17.74em"/></div></div> <div class="EquationNumber">(2.10)</div></div></div> which takes as input the embedding <span class="InlineEquation" id="IEq41"><img alt="$$\tilde {{\boldsymbol {x}}}_{k,t}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq41.png" style="width:1.82em"/></span> of the last layer <em class="EmphasisTypeItalic ">k</em> at position <em class="EmphasisTypeItalic ">t</em> to predict the random variable <em class="EmphasisTypeItalic ">V</em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub> of possible tokens at position <em class="EmphasisTypeItalic ">t</em> + 1 (Fig. <span class="InternalRef"><a href="#Fig8">2.8</a></span>). This approach is called <em class="EmphasisTypeItalic ">masked self-attention</em><span id="ITerm62"/> or <em class="EmphasisTypeItalic ">causal self-attention</em><span id="ITerm63"/> because the prediction depends only on past tokens. Since GPT generates the tokens by sequentially applying the same model, it is called an <em class="EmphasisTypeItalic ">autoregressive language model</em><span id="ITerm64"/>.<figure class="Figure" id="Fig8"><div class="MediaObject" id="MO8"><img alt="" aria-describedby="d64e5182" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig8_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e5182"><p class="Para" id="Par222">Two block diagrams of BERT encoder blocks and four different levels. The lowest level is the input block, followed by input embeddings, and output embeddings. The highest level is token probabilities.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.8</span><p class="SimplePara">The input of the GPT model are the embeddings of tokens <em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub> up to position <em class="EmphasisTypeItalic ">t</em>. GPT computes contextual self-embeddings of these tokens in different layers and uses the output embedding of the last token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub> = <em class="EmphasisTypeItalic ">“to”</em> in the highest layer to predict the probabilities of possible tokens at position <em class="EmphasisTypeItalic ">t</em> + 1 with a logistic classifier <em class="EmphasisTypeItalic ">L</em>. This probability should be high for the actually observed token <em class="EmphasisTypeItalic ">“new”</em> (left). Then the observed token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub> = <em class="EmphasisTypeItalic ">“new”</em> is appended to the input sequence and included in the self-attention computation for predicting the probabilities of possible tokens at position <em class="EmphasisTypeItalic ">t</em> + 2, which should be high for <em class="EmphasisTypeItalic ">“york”</em> (right)</p></div></figcaption></figure><figure class="Figure" id="Fig9"><div class="MediaObject" id="MO9"><img alt="" aria-describedby="d64e5197" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig9_HTML.png" style="width:34.48em"/><div class="TextObject" id="d64e5197"><p class="Para" id="Par223">2 block diagrams have various input tokens that go through B E R T encoder blocks and L classifiers to embed with other tokens. The diagrams have layers of input tokens, input embeddings, output embeddings, and token probabilities.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.9</span><p class="SimplePara">Visualization of embeddings with PCA together with the corresponding part-of speech tags. On the left side are GPT-2 embeddings of layer 0 of tokens of positions &gt; 0 which form ribbon-like structures for the different POS tags, with function words close to the top. On the right side the embeddings of BERT for layer 0 are shown. Image reprinted with kind permission of the author [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>]</p></div></figcaption></figure></div></section>
<section class="Section2 RenderAsSection2" id="Sec13"><h3 class="Heading"><span class="HeadingNumber">2.2.2 </span>Training GPT by Predicting the Next Token</h3><p class="Para" id="Par81">The training objective is adapted to the language modeling task of GPT. Figure <span class="InternalRef"><a href="#Fig8">2.8</a></span> shows the range of computations for two consecutive tokens. By <em class="EmphasisTypeItalic ">teacher forcing</em><span id="ITerm65"/> the model uses the observed tokens <em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub> up to position <em class="EmphasisTypeItalic ">t</em> to compute self-attentions and predict the token probabilities for the next token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub>. This is justified by the factorization (<span class="InternalRef"><a href="#Equ9">2.9</a></span>) of the full distribution. Note that the contextual embedding of a token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">s</em></sub>, <em class="EmphasisTypeItalic ">s</em> &lt; <em class="EmphasisTypeItalic ">t</em>, changes each time when a new token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub>, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em>+2</sub>, … is taken into account in the masked self-attention. As GPT considers only the tokens before the target token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub>, it is called an <em class="EmphasisTypeItalic ">unidirectional encoder</em><span id="ITerm66"/>. An intuitive high-level overview over GPT is given by Alammar [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>].</p><div class="Para" id="Par82">During training the model parameters have to be changed by optimization such that the probabilities of observed documents (<span class="InternalRef"><a href="#Equ9">2.9</a></span>) get maximal. By this <em class="EmphasisTypeItalic ">Maximum Likelihood estimation</em><span id="ITerm67"/> (<em class="EmphasisTypeItalic ">MLE</em><span id="ITerm68"/>) the parameters can be optimized for a large corpus of documents. To avoid numerical problems this is solved by maximizing the <em class="EmphasisTypeItalic ">log-likelihood</em><span id="ITerm69"/>, sum of logarithms of (<span class="InternalRef"><a href="#Equ9">2.9</a></span>) <div class="Equation NumberedEquation" id="Equ11"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \log p(v_1,\ldots,v_T)= \log p(v_{T}|v_1,\ldots,v_{T-1})+\cdots+\log p(v_{2}|v_1) +\log p(v_1) {}. \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ11.png" style="width:31.76em"/></div></div> <div class="EquationNumber">(2.11)</div></div></div> Alternatively we can minimize the negative log-likelihood <span class="InlineEquation" id="IEq42"><img alt="$$-\log p(v_1,\ldots ,v_T)$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq42.png" style="width:8.25em"/></span>.</div><p class="Para" id="Par83">GPT-2 can process an input sequence of 1024 tokens with an embedding size of 1024. In its medium version it has 345M parameters and contains 24 layers, each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized. The model was trained on 40 GB of text crawled from Reddit, a social media platform. Only texts that were well rated by other users were included, resulting in a higher quality data set. The larger model was trained on 256 cloud TPU v3 cores. The training duration was not disclosed, nor the exact details of training.</p><div class="Para" id="Par84">The quality of a language model may be measured by the probability <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">T</em></sub>) of a given text collection <em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">T</em></sub>. If we normalize its inverse by the number <em class="EmphasisTypeItalic ">T</em> of tokens we get the <em class="EmphasisTypeItalic ">perplexity</em><span id="ITerm70"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR28" role="doc-biblioref">28</a></span>] <div class="Equation NumberedEquation" id="Equ12"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} ppl(v_1,\ldots,v_T):=p(v_1,\ldots,v_T)^{-\frac 1T} {}. \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ12.png" style="width:15.44em"/></div></div> <div class="EquationNumber">(2.12)</div></div></div> A low perplexity indicates a high probability of the text. If we assume that the conditional probabilities <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub>|<em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em>−1</sub>) are identical for all <em class="EmphasisTypeItalic ">t</em>, we get <em class="EmphasisTypeItalic ">ppl</em>(<em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">T</em></sub>) = 1∕<em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub>|<em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em>−1</sub>), i.e. the inverse probability of the next token. GPT-2 was able to substantially reduce the perplexity on a number of benchmark data sets, e.g. from 46.5 to 35.8 for the <em class="EmphasisTypeItalic ">Penn Treebank corpus</em><span id="ITerm71"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR117" role="doc-biblioref">117</a></span>] meaning that the actual words in the texts were predicted with higher probability.</div><section class="Section3 RenderAsSection3" id="Sec14"><h4 class="Heading">Visualizing GPT Embeddings</h4><p class="Para" id="Par85">Kehlbeck et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>] investigated the relative location of embeddings in multivariate space for both BERT and GPT-2, each with 12 layers. They calculated 3-D projections using both <em class="EmphasisTypeItalic ">principal component analysis</em><span id="ITerm72"/><em class="EmphasisTypeItalic ">(PCA)</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR111" role="doc-biblioref">111</a></span>] and UMAP [<span class="CitationRef"><a epub:type="biblioref" href="#CR89" role="doc-biblioref">89</a></span>]. The latter can preserve the local structure of neighbors, but—differently to PCA—is unable to correctly maintain the global structure of the data. These 3d-scatterplots can be interactively manipulated on the website [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>]. It turns out that GPT-2 forms two separate clusters: There is a small cluster containing just all tokens at position 0, while the embeddings at other positions form ribbon-like structures in the second cluster.</p><div class="Para" id="Par86">Careful investigations have indicated that most embedding vectors are located in a narrow cone, leading to high cosine similarities between them [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>]. The authors identify isolated clusters and low dimensional manifolds in the contextual embedding space. Kehlbeck et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>] show that tokens with the same part-of-speech tag form ribbon-like structures in the projections (Fig. <span class="InternalRef"><a href="#Fig9">2.9</a></span> left). Function words are all located on a tight circular structure, whereas content words like nouns and verbs are located in other elongated structures and have overlap with other POS-tags. The embeddings generated by BERT form one or more clusters (Fig. <span class="InternalRef"><a href="#Fig9">2.9</a></span> right). They are quite separated for function words, but show some overlap for content words like nouns, verbs, or adjectives.<figure class="Figure" id="Fig10"><div class="MediaObject" id="MO10"><img alt="" aria-describedby="d64e5760" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig10_HTML.png" style="width:34.5em"/><div class="TextObject" id="d64e5760"><p class="Para" id="Par224">Two scatterplots. The left one plots a financial institution, sloping land, a bank building, a long ridge, arrangement of objects, and a flight maneuver. A financial institution has the highest concentration. The second one plots P C A projections of embeddings such as banks and material.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.10</span><p class="SimplePara">Plot of BERT-embeddings of different senses of <em class="EmphasisTypeItalic ">“bank”</em> projected to two dimensions by T-SNE (left). The legend contains a short description of the respective WordNet sense and the frequency of occurrence in the training data. Image[<span class="CitationRef"><a epub:type="biblioref" href="#CR153" role="doc-biblioref">153</a></span>]. The right side shows PCA projections of the embeddings of <em class="EmphasisTypeItalic ">“banks”</em> (lower strip) and <em class="EmphasisTypeItalic ">“material”</em> (middle strip) as well as other words computed for different contexts. Image interactively generated, printed with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par87">The GPT-2 embeddings of content words like <em class="EmphasisTypeItalic ">“banks”</em> and <em class="EmphasisTypeItalic ">“material”</em> at positions &gt; 0 form elongated band-structures, as shown in the right part of Fig. <span class="InternalRef"><a href="#Fig10">2.10</a></span>. For higher layers the PCA projections get more diffuse. The user can read the token context by pointing to each dot.</p><p class="Para" id="Par88">Token-based <em class="EmphasisTypeItalic ">self-similarity</em><span id="ITerm73"/> is the mean cosine similarity of the same token found in different sentences. In BERT as well as GPT-2, the self-similarity is higher for content than function words [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>]. This may indicate that function words have more diverse semantic roles in different contexts. It is interesting to evaluate the 10 nearest neighbors of a token with respect to cosine similarity. In the lower layers, for both models the nearest tokens were in most cases the same tokens, except for a few content words. In the higher layers this changed and different tokens were the nearest tokens. This shows that more and more context is included in the embeddings of higher layers.</p><p class="Para" id="Par89">The authors also investigated the embeddings generated by a number of other PLM types. They find that their structure is very different as they form different clusters and manifolds. They argue that this structure has to be taken into account for new applications of the models.</p></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec15"><h3 class="Heading"><span class="HeadingNumber">2.2.3 </span>Generating a Sequence of Words</h3><div class="Para" id="Par90">After training the GPT model can predict the probabilities of the tokens at the next position <em class="EmphasisTypeItalic ">t</em> + 1 given the previous tokens <em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub>. To generate a text we have to select a sequence of tokens according to these probabilities. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par91"><em class="EmphasisTypeItalic ">Random sampling</em><span id="ITerm74"/> selects the next token according to the predicted probabilities. This approach sometimes can select very improbable tokens such that the probability of the whole sentence gets too low. Although the individual probabilities are tiny, the probability of selecting an element of the group of improbable tokens is quite high. In addition, the estimates of small probability are often affected by errors.</p></li><li><p class="Para" id="Par92"><em class="EmphasisTypeItalic ">Top-k</em><em class="EmphasisTypeItalic ">sampling</em><span id="ITerm75"/> takes into account only the <em class="EmphasisTypeItalic ">k</em> tokens with the highest probability to generate the next token. The probability mass is redistributed among them [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>] and used for randomly selecting a token.</p></li><li><p class="Para" id="Par93"><em class="EmphasisTypeItalic ">Top-p</em><em class="EmphasisTypeItalic ">sampling</em><span id="ITerm76"/> considers the smallest set of top candidates with the cumulative probability above a threshold (e.g. <em class="EmphasisTypeItalic ">p</em> = 0.95) and then selects the next token according to the redistributed probabilities [<span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>]. This approach limits the probability mass of rare tokens which are ignored.</p></li></ul></div> There are also strategies which explicitly avoid previously generated tokens by reducing the corresponding scores in the update formula [<span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>]. Both top-<em class="EmphasisTypeItalic ">k</em> and top-<em class="EmphasisTypeItalic ">p</em> sampling usually generate plausible token sequences and are actually employed to generate texts.</div><p class="Para" id="Par94">There are a number of approaches to improve token selection. Meister et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>] found that human-produced text tends to have evenly distribution of “surprise”. This means that the next token should on average not be too rare and not be too frequent. They propose a number of sampling criteria, e.g. a variance regularizer.</p><p class="Para" id="Par95">Martins et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR86" role="doc-biblioref">86</a></span>] argue that softmax-generated output distributions are unrealistic, as they assign a positive probability to every output token. They propose the <em class="EmphasisTypeItalic ">Entmax transformation</em><span id="ITerm77"/> which generates a sparse probability distribution from the computed scores, where part of the probabilities are exactly zero. The Entmax transformation can be controlled by a parameter <em class="EmphasisTypeItalic ">α</em> ≥ 1. For <em class="EmphasisTypeItalic ">α</em> = 1 we get softmax and <em class="EmphasisTypeItalic ">α</em> = <em class="EmphasisTypeItalic ">∞</em> recovers <span class="InlineEquation" id="IEq43"><img alt="$$\arg \max $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq43.png" style="width:3.49em"/></span>. For intermediate values <em class="EmphasisTypeItalic ">∞</em> &gt; <em class="EmphasisTypeItalic ">α</em> &gt; 1.0 some tokens get exactly zero probability. Entmax losses are convex and differentiable and therefore may be trained by backpropagation. As in top-<em class="EmphasisTypeItalic ">p</em> sampling and in opposition to top-<em class="EmphasisTypeItalic ">k</em> sampling, Entmax sampling considers a varying number of tokens depending on the context. Experiments show that Entmax leads to better perplexities and less repetitions than other approaches. Compared with top-<em class="EmphasisTypeItalic ">p</em> sampling it has a higher variation in the number of tokens considered.</p><p class="Para" id="Par96">Khandelwal et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>] try to improve the estimated probabilities of the language model by statistics of token <em class="EmphasisTypeItalic ">n</em>-grams. They perform a nearest neighbor search on the last tokens already processed. As distance measure they use the distances of the pre-trained embedding space. From the retrieved nearest neighbors they get additional evidence on the probable next token, which is merged with the token probabilities of the language model. In this way, they are able to improve the perplexity of language models. The approach is particularly helpful in predicting rare patterns, e.g. factual knowledge.</p><p class="Para" id="Par97">Yang et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR157" role="doc-biblioref">157</a></span>] analyze the properties of the softmax function. They find that the standard softmax does not have enough capacity to model natural language, as it restricts the rank of the mapping to probabilities. They propose to predict probabilities by a <em class="EmphasisTypeItalic ">Mixture of Softmaxes</em><span id="ITerm78"/>, a convex combination of different logistic classifiers, which is more expressive than a single softmax. The authors show that this modification yields better perplexities in language modeling and also improves the performance of other transformer architectures [<span class="CitationRef"><a epub:type="biblioref" href="#CR101" role="doc-biblioref">101</a></span>].</p></section>
<section class="Section2 RenderAsSection2" id="Sec16"><h3 class="Heading"><span class="HeadingNumber">2.2.4 </span>The Advanced Language Model GPT-2</h3><p class="Para" id="Par98"><strong class="EmphasisTypeBold ">GPT-2</strong><span id="ITerm79"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR118" role="doc-biblioref">118</a></span>] is the first language model, which is able to generate documents of grammatically correct and semantically plausible text. Its largest version has 48 encoder blocks with 1.5B parameters and covers sequences of 1600 tokens. Given an initial text the model adapts to the style and content of this text and generates an answer, which often cannot be distinguished from human-generated continuations. Longer generated texts, however, sometimes tend to be repetitive and less coherent.</p><div class="Para" id="Par99">For GPT-2 top-<em class="EmphasisTypeItalic ">k</em> truncated sampling was used to generate the example text [<span class="CitationRef"><a epub:type="biblioref" href="#CR117" role="doc-biblioref">117</a></span>] shown in Fig. <span class="InternalRef"><a href="#Fig11">2.11</a></span>. As can be seen there are no syntax errors and the generated content is plausible. The authors remark that one in two trials were of high quality. The model adapts to the style and content of the input text. This allows the user to generate realistic and coherent continuations about a topic they like. Obviously the topic has to be mentioned in the Reddit training data, which covers a broad spectrum of themes such as news, music, games, sports, science, cooking, and pets.<figure class="Figure" id="Fig11"><div class="MediaObject" id="MO11"><img alt="" aria-describedby="d64e5982" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig11_HTML.png" style="width:34.38em"/><div class="TextObject" id="d64e5982"><p class="Para" id="Par232">A screenshot of the input and generated by G P T 2 input texts.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.11</span><p class="SimplePara">Given the input text, GPT-2 generates a continuation by top-<em class="EmphasisTypeItalic ">k</em> sampling [<span class="CitationRef"><a epub:type="biblioref" href="#CR117" role="doc-biblioref">117</a></span>]. Quoted with kind permission of the authors</p></div></figcaption></figure></div><p class="Para" id="Par100">The model was able to solve many tasks better than previous models without being trained on the specific task. This type of learning is called <em class="EmphasisTypeItalic ">zero-shot learning</em><span id="ITerm80"/>. For example, GPT-2 had a perplexity of 35.8 on the test set of the Penn Treebank compared to the inferior prior <span class="EmphasisTypeSmallCaps ">Sota</span> of 46.5 [<span class="CitationRef"><a epub:type="biblioref" href="#CR117" role="doc-biblioref">117</a></span>]. This was achieved without training GPT-2 on the <em class="EmphasisTypeItalic ">Penn Treebank corpus</em><span id="ITerm81"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR135" role="doc-biblioref">135</a></span>].</p></section>
<section class="Section2 RenderAsSection2" id="Sec17"><h3 class="Heading"><span class="HeadingNumber">2.2.5 </span>Fine-Tuning GPT</h3><p class="Para" id="Par101">By fine-tuning, GPT-2 may be adapted to new types of text, for example new genres of text. To create song lyrics, for example, St-Amant [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>] uses a dataset of 12,500 English rock song lyrics and fine-tunes GPT-2 for 5 epochs. Then the model is able to continue the lyrics of pop songs, which had not been seen by the model during training. The model had a high <span class="EmphasisTypeSmallCaps ">Bleu</span> score of 68 when applied to song lyrics. Another experiment describes the generation of poetry [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>].</p><p class="Para" id="Par102">Similar to BERT, a pre-trained GPT-2 can also be modified to perform a classification task. An example is fine-tuning to the classification of the sentiment of a document as positive or negative. Radford et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR116" role="doc-biblioref">116</a></span>] encode the classification task as a text with specific tokens and a final end token <em class="EmphasisTypeItalic ">[END]</em>. Then the model has to predict the sequence. The embedding of <em class="EmphasisTypeItalic ">[END]</em> in the highest layer is used as input to a logistic classifier, which is trained to predict the probability of classes. The authors found that including language modeling (<span class="InternalRef"><a href="#Equ11">2.11</a></span>) of the fine-tuning data as an auxiliary objective to fine-tuning improved generalization and accelerated convergence. They were able to improve the score on GLUE (Sect. <span class="InternalRef"><a href="#Sec7">2.1.5</a></span>) from 68.9 to 72.8 and achieved <span class="EmphasisTypeSmallCaps ">Sota</span> in 7 out of 8 GLUE tasks for natural language understanding. The results show that language models capture relevant information about syntax and semantics.</p><p class="Para" id="Par103">However, GPT operates from left to right when predicting the next token. In the sentences <em class="EmphasisTypeItalic ">“I went to the bank to deposit cash”</em> and <em class="EmphasisTypeItalic ">“I went to the bank to sit down”</em>, it will create the same context-sensitive embedding for <em class="EmphasisTypeItalic ">“bank”</em> when predicting <em class="EmphasisTypeItalic ">“sit”</em> or <em class="EmphasisTypeItalic ">“deposit”</em>, although the meaning of the token <em class="EmphasisTypeItalic ">“bank”</em> is different in both contexts. In contrast, BERT is bidirectional and takes into account all tokens of the text when predicting masked tokens. This fact explains why BERT for some tasks shows a better performance.</p></section>
<section class="Section2 RenderAsSection2" id="Sec18"><h3 class="Heading"><span class="HeadingNumber">2.2.6 </span>Summary</h3><p class="Para" id="Par104">GPT has an architecture similar to a BERT model that generates the tokens of a sentence one by one. It starts with an input sequence of tokens, which can be empty. Tokens are encoded as a sum of token embeddings and position embeddings. GPT uses the same encoder blocks as BERT, but the computations are masked, i.e. restricted to the already generated tokens. For these tokens the model produces contextual embeddings in several layers. The embedding of the last token in the top layer is entered into a logistic classifier and this calculates the probability of the tokens for the next position. Subsequently, the observed token is appended to the input at the next position and the computations are repeated for the next but one position. Therefore, GPT is called an autoregressive language model.</p><p class="Para" id="Par105">During training the parameters are changed by stochastic gradient descent in such a way that the model predicts high probabilities of the observed tokens in the training data. The maximum likelihood criterion is used, which optimizes the probability of the input data. When the model has been trained on a large text dataset it can be applied. Conditional to a start text it can sequentially compute the probability of the next token. Then a new token can be selected according to the probabilities.</p><p class="Para" id="Par106">If all alternative tokens are taken into account, rare tokens are often selected. Usually, the number of eligible tokens is restricted to <em class="EmphasisTypeItalic ">k</em> high-probability tokens (top-<em class="EmphasisTypeItalic ">k</em> sampling) or only high-probability tokens are included up to a prescribed probability sum <em class="EmphasisTypeItalic ">p</em> (top-<em class="EmphasisTypeItalic ">p</em> sampling). In this way, much better texts are generated. Advanced language models like GPT-2 have billions of parameters and are able to generate plausible stories without syntactic errors.</p><p class="Para" id="Par107">GPT models can also be fine-tuned. A first type of fine-tuning adapts the model to a specific text genre, e.g. poetry. Alternatively, GPT can be used as a classifier, where the output embedding of the most recently generated token for an input text is input to a logistic classifier. With this approach, GPT-2 was able to improve <span class="EmphasisTypeSmallCaps ">Sota</span> for most natural language understanding task in the GLUE benchmark. This shows that GPT-2 has acquired a comprehensive knowledge about language. However, since self-attention is only aware of past tokens, models like BERT are potentially better as they can take into account all input tokens during computations.</p><p class="Para" id="Par108">Chapter <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml"><span class="RefSource">3</span></a></span> discusses how to improve the performance of GPT models, in particular by using more parameters (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec3"><span class="RefSource">3.​1.​2</span></a></span>). These large models with billions of parameters can be instructed to perform a number of tasks without fine-tuning (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec41"><span class="RefSource">3.​6.​3</span></a></span>). In the Chaps. <span class="ExternalRef"><a href="528393_1_En_5_Chapter.xhtml"><span class="RefSource">5</span></a></span>–<span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml"><span class="RefSource">7</span></a></span>, we describe a number of applications of GPT-models such as question-answering (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec14"><span class="RefSource">6.​2.​3</span></a></span>), story generation (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec31"><span class="RefSource">6.​5</span></a></span>), or image generation from text (Sect. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml#Sec18"><span class="RefSource">7.​2.​6</span></a></span>).</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec19"><h2 class="Heading"><span class="HeadingNumber">2.3 </span>Transformer: Sequence-to-Sequence Translation</h2><section class="Section2 RenderAsSection2" id="Sec20"><h3 class="Heading"><span class="HeadingNumber">2.3.1 </span>The Transformer Architecture</h3><p class="Para" id="Par109">Translation models based on Recurrent Neural Networks (Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec6"><span class="RefSource">1.​6</span></a></span>) have a major limitation caused by the sequential nature of RNNs. The number of operations required to determine the relation between tokens <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">s</em></sub> and <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub> grows with the distance <em class="EmphasisTypeItalic ">t</em> − <em class="EmphasisTypeItalic ">s</em> between positions. The model has to store the relations between all tokens simultaneously in a vector, making it difficult to learn complex dependencies between distant positions.</p><p class="Para" id="Par110">The <em class="EmphasisTypeItalic ">Transformer</em><span id="ITerm82"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>]—similar to RNN-translation models—is based on an encoder and a decoder module (Fig. <span class="InternalRef"><a href="#Fig13">2.13</a></span>). The encoder is very similar to BERT, while the decoder resembles GPT. It is a <em class="EmphasisTypeItalic ">sequence-to-sequence model</em><span id="ITerm83"/> (<em class="EmphasisTypeItalic ">Seq2seq</em>), which translates a source text of the input language to a target text in the target language. Instead of relating distant tokens by a large number of computation steps, it directly computes the self-attention between these token in parallel in one step.</p><p class="Para" id="Par111">The <em class="EmphasisTypeItalic ">encoder</em><span id="ITerm84"/> generates contextual embeddings <span class="InlineEquation" id="IEq44"><img alt="$$\tilde {{\boldsymbol {x}}}_1,\ldots ,\tilde {{\boldsymbol {x}}}_{T_{\text{src}}}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq44.png" style="width:5.5em"/></span> of the source text tokens <span class="InlineEquation" id="IEq45"><img alt="$$v_1, \ldots , v_{T_{\text{src}}}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq45.png" style="width:5.07em"/></span> with exactly the same architecture as the BERT model (Fig. <span class="InternalRef"><a href="#Fig4">2.4</a></span>). The original transformer [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>] uses 6 encoder blocks. The generated embeddings of the last layer are denoted as <span class="InlineEquation" id="IEq46"><img alt="$$\breve {\boldsymbol {x}}_1,\ldots ,\breve {\boldsymbol {x}}_{T_{\text{src}}}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq46.png" style="width:5.5em"/></span>.</p><div class="Para" id="Par112">The transformer <em class="EmphasisTypeItalic ">decoder</em><span id="ITerm85"/> step by step computes the probability distributions <span class="InlineEquation" id="IEq47"><img alt="$$p(S_{t}|s_1,\ldots ,s_{t-1},v_1,\ldots ,v_{T_{\text{src}}})$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq47.png" style="width:13.18em"/></span> of target tokens <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em></sub> similar to the Recurrent Neural Network. Note that the source tokens <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">i</em></sub> as well as observed target tokens <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">j</em></sub> are taken as conditions. By the definition of conditional probability this yields the total probability of the output distribution <div class="Equation NumberedEquation" id="Equ13"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \begin{array}{rcl} &amp;amp; &amp;amp;\displaystyle {p(S_{1}\mkern1.5mu{=}\mkern1.5mu s_1,\ldots,S_{T}\mkern1.5mu{=}\mkern1.5mu s_T|v_1,\ldots,v_{T_{\text{src}}}) }\\ ~\qquad &amp;amp; =&amp;amp;\displaystyle p(S_T\mkern1.5mu{=}\mkern1.5mu s_T|s_1,\ldots,s_{T-1},v_1,\ldots,v_{T_{\text{src}}}) \cdots p(S_{1}\mkern1.5mu{=}\mkern1.5mu s_1|v_1,\ldots,v_{T_{\text{src}}}) , {} \end{array} \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ13.png" style="width:28.56em"/></div></div> <div class="EquationNumber">(2.13)</div></div></div> where <em class="EmphasisTypeItalic ">S</em><sub><em class="EmphasisTypeItalic ">t</em></sub> is a random variable with the possible target tokens <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em></sub> at position <em class="EmphasisTypeItalic ">t</em> as its values. This probability is maximized during training.</div><p class="Para" id="Par113">We denote the already translated tokens by <em class="EmphasisTypeItalic ">s</em><sub>0</sub>, <em class="EmphasisTypeItalic ">s</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em>−1</sub> were <em class="EmphasisTypeItalic ">s</em><sub>0</sub> is the token <em class="EmphasisTypeItalic ">“[BOS]”</em> indicating the beginning of the output text. The decoder first computes a self-attention for these tokens using the formula (<span class="InternalRef"><a href="#Equ4">2.4</a></span>) as for BERT. As only part of the target tokens are covered and the rest is ‘masked’, this layer is called <em class="EmphasisTypeItalic ">masked multi-head self-attention</em><span id="ITerm86"/> yielding intermediate contextual embeddings <span class="InlineEquation" id="IEq48"><img alt="$$\tilde {\boldsymbol {s}}_0,\tilde {\boldsymbol {s}}_1,\ldots ,\tilde {\boldsymbol {s}}_{t-1}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq48.png" style="width:6.69em"/></span> for the target tokens <em class="EmphasisTypeItalic ">s</em><sub>0</sub>, <em class="EmphasisTypeItalic ">s</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em>−1</sub>.</p><section class="Section3 RenderAsSection3" id="Sec21"><h4 class="Heading">Cross-Attention</h4><div class="Para" id="Par114">Then the decoder performs a <em class="EmphasisTypeItalic ">cross-attention</em><span id="ITerm87"/><span class="InlineEquation" id="IEq49"><img alt="$$\text{CATL}(\tilde {{\boldsymbol {V}}},\breve {{\boldsymbol {X}}})$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq49.png" style="width:6.2em"/></span> with the input text embeddings of the highest encoder block (Fig. <span class="InternalRef"><a href="#Fig12">2.12</a></span>). Here the query-vectors are computed for the embeddings of the target tokens <span class="InlineEquation" id="IEq50"><img alt="$$\tilde {\boldsymbol {S}}_t=(\tilde {\boldsymbol {s}}_0,\tilde {\boldsymbol {s}}_1,\ldots ,\tilde {\boldsymbol {s}}_{t-1}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq50.png" style="width:9.5em"/></span>) provided by the respective decoder block. The key and value vectors are computed for the embeddings <span class="InlineEquation" id="IEq51"><img alt="$$\breve {{\boldsymbol {X}}}=\breve {\boldsymbol {x}}_1,\ldots ,\breve {\boldsymbol {x}}_{T_{\text{src}}}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq51.png" style="width:7.94em"/></span> of the last encoder block. Note that cross attention employs the same Eq. (<span class="InternalRef"><a href="#Equ4">2.4</a></span>) with matrices <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sup>(<em class="EmphasisTypeItalic ">q</em>)</sup>, <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sup>(<em class="EmphasisTypeItalic ">k</em>)</sup>, <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sup>(<em class="EmphasisTypeItalic ">v</em>)</sup> as the BERT self-attentions. This is done in parallel and called <em class="EmphasisTypeItalic ">multi-head cross-attention</em><span id="ITerm88"/>. In this way, information from the source text is taken into account. Subsequently, the embeddings computed by different heads are concatenated (<span class="InternalRef"><a href="#Equ6">2.6</a></span>) and the result is transformed by a fully connected layer with ReLU activation (<span class="InternalRef"><a href="#Equ7">2.7</a></span>). In addition, residual “bypass” connections are used as well as layer normalization [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>] for regularization. The output of the fully connected layer yields a new ‘output’ embedding <span class="InlineEquation" id="IEq52"><img alt="$$\tilde {\boldsymbol {s}}_0,\ldots ,\tilde {\boldsymbol {s}}_{t-1}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq52.png" style="width:5.25em"/></span> for the target tokens <em class="EmphasisTypeItalic ">s</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em>−1</sub>. Together these layers are called a <em class="EmphasisTypeItalic ">decoder block</em><span id="ITerm89"/> (Fig. <span class="InternalRef"><a href="#Fig13">2.13</a></span>).<figure class="Figure" id="Fig12"><div class="MediaObject" id="MO12"><img alt="" aria-describedby="d64e7073" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig12_HTML.png" style="width:34.07em"/><div class="TextObject" id="d64e7073"><p class="Para" id="Par233">A flow diagram of a series of input tokens gives a new embedding via multi-head self-attention, k encoder blocks, cross-attention layer, decoder block, weighted value vectors, and fully connected layer, among others.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.12</span><p class="SimplePara">The transformer [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>] uses <em class="EmphasisTypeItalic ">k</em> encoder blocks with the same architecture as in BERT (Fig. <span class="InternalRef"><a href="#Fig4">2.4</a></span>) to generate contextual embeddings of all tokens of the input text. The decoder block is an autoregressive language model (Fig. <span class="InternalRef"><a href="#Fig8">2.8</a></span>) and sequentially predicts the next token in the target language. Each encoder block contains a multi-head self-attention for the current sequence of output tokens. By cross-attention the information from the input sequence is included. The calculations are repeated for all current input tokens and are very similar to the self-attention computations. The resulting vector is transformed by a fully connected layer yielding the embeddings of that layer</p></div></figcaption></figure><figure class="Figure" id="Fig13"><div class="MediaObject" id="MO13"><img alt="" aria-describedby="d64e7091" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig13_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e7091"><p class="Para" id="Par234">A flow diagram of a series of input tokens gives the target tokens via multi-head self-attention, multi-head cross-attention, encoder and decoder blocks, fully connected layers, further encoder blocks, final embedding vectors, logistic regression, and token probabilities.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.13</span><p class="SimplePara">The transformer [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>] uses an encoder with the same architecture as BERT to generate embeddings of all tokens of the input sentence. Each encoder block performs multi-head self-attention of the input sequence followed by a fully connected layer (FCL) . The decoder is similar to a GPT model and sequentially predicts the next token in the target language. Each encoder block contains a multi-head cross-attention including the final embeddings of the encoder. Using the last output embedding of the final decoder block, a logistic classifier <em class="EmphasisTypeItalic ">L</em> predicts probabilities of the next token of the output sentence</p></div></figcaption></figure></div><p class="Para" id="Par115">The next decoder block gets the computed token output embeddings of the previous block as input and computes a new embedding of the target tokens <em class="EmphasisTypeItalic ">s</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em>−1</sub>. The decoder consists of several decoder blocks (6 in the original model). Using the output embedding <em><strong class="EmphasisTypeBoldItalic ">s</strong></em>̆<sub><em class="EmphasisTypeItalic ">t</em>−1</sub> of the righmost token <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em>−1</sub> in the last decoder block, the token probabilities <span class="InlineEquation" id="IEq53"><img alt="$$p(S_{t}=s_t|s_1,\ldots ,s_{t-1},v_1,\ldots ,v_{T_{\text{src}}})$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq53.png" style="width:15.38em"/></span> of the next token <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em></sub> of the target text at position <em class="EmphasisTypeItalic ">t</em> are predicted by a logistic classifier, e.g. for the token <em class="EmphasisTypeItalic ">“Maus”</em> in Fig. <span class="InternalRef"><a href="#Fig13">2.13</a></span>.</p><p class="Para" id="Par116">Note that for the prediction of a further token at position <em class="EmphasisTypeItalic ">t</em> + 1 the observed token <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em></sub> is added to the computation (<span class="InternalRef"><a href="#Equ13">2.13</a></span>) of the self-attentions in the decoder. Hence, the decoder embeddings change and all decoder computations have to be repeated. In this respect the model still works in a recursive way. Nevertheless, all self-attentions and cross-attentions in each layer are computed in parallel. However, the computations for the encoder are only performed once.</p><p class="Para" id="Par117">Sequences of variable length are padded with a special token up to the maximal length. This is done for the input and the output sequence. If a sequence is very short, a lot of space is wasted. Therefore, the sequence length may be varied in different mini-batches called buckets in the training data.</p><p class="Para" id="Par118">The transformer has a large set of parameters. First it requires embeddings of the input and target token vocabularies. Then there are the <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sup>(<em class="EmphasisTypeItalic ">q</em>)</sup>, <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sup>(<em class="EmphasisTypeItalic ">k</em>)</sup>, <em><strong class="EmphasisTypeBoldItalic ">W</strong></em><sup>(<em class="EmphasisTypeItalic ">v</em>)</sup> matrices for the multi-head self-attention, the masked multi-head self-attention and the multi-head cross-attention of the different heads and layers. In addition, the parameters of the fully connected networks and the final logistic classifier have to be specified. While the base model had an input sequence length of 512 and 65M parameters, the big model had an input sequence length of 1024 and 213M parameters [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>]. The values of all these parameters are optimized during training.</p><p class="Para" id="Par119">The training data consists of pairs of an input sentence and the corresponding target sentence. Training aims to generate the target tokens with maximal probability for the given input tokens to maximize the joint conditional probability (<span class="InternalRef"><a href="#Equ13">2.13</a></span>) of the output sequence by stochastic gradient descent. In our example in Fig. <span class="InternalRef"><a href="#Fig13">2.13</a></span> for the given input text <em class="EmphasisTypeItalic ">“The mouse likes cheese”</em> the product of conditional probabilities of the output tokens <em class="EmphasisTypeItalic ">“Die Maus mag Käse”</em> has to be maximized. The original model [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>], for instance, used 36M sentences of the WMT English-French benchmark data encoded as 32,000 wordpiece tokens. Both the encoder and decoder are trained simultaneously by stochastic gradient descent end-to-end, requiring 3.5 days with 8 GPUs.</p><div class="Para" id="Par120">Cross-attention is the central part of the transformer, where the information from the input sentence is related to the translated output sentence. In Fig. <span class="InternalRef"><a href="#Fig14">2.14</a></span> a German input sentence is displayed together with its English translation. Both sentences are tokenized by byte-pair encoding, where the beginning of a word is indicated by <em class="EmphasisTypeItalic ">“_”</em>. Below the strength of cross-attentions between the input tokens and output tokens is depicted for two different heads. Obviously the first input token <em class="EmphasisTypeItalic ">“_The”</em> has a special role.<figure class="Figure" id="Fig14"><div class="MediaObject" id="MO14"><img alt="" aria-describedby="d64e7314" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig14_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e7314"><p class="Para" id="Par235">Two color gradient cross-attention graphs for the English input sentence and the German translation.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.14</span><p class="SimplePara">An English input sentence tokenized by Byte-Pair encoding and the translated tokenized German output sentence. Below are two cross-attention graphs from different heads of the 4-th decoder layer [<span class="CitationRef"><a epub:type="biblioref" href="#CR126" role="doc-biblioref">126</a></span>]. Dark values indicate a low cross-attention score. Image source: [<span class="CitationRef"><a epub:type="biblioref" href="#CR126" role="doc-biblioref">126</a></span>]</p></div></figcaption></figure></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec22"><h3 class="Heading"><span class="HeadingNumber">2.3.2 </span>Decoding a Translation to Generate the Words</h3><p class="Para" id="Par121">After training, the Transformer is able to predict the probabilities of output tokens for an input sentence. For a practical translation, however, it is necessary to generate an explicit sequence of output tokens. Computing the output sequence with maximal probability is computationally hard, as then all output possible sequences have to be considered. Therefore, an approximate solution is obtained using greedy decoding or beam search.</p><p class="Para" id="Par122"><strong class="EmphasisTypeBold ">Greedy decoding</strong><span id="ITerm90"/> simply picks the most likely token with the highest probability at each decoding step until the end-of-sentence token is generated. The problem with this approach is that once the output is chosen at any time step <em class="EmphasisTypeItalic ">t</em>, it is impossible to go back and change the selection. In practice there are often problems with greedy decoding, as the available probable continuation tokens may not fit to a previously assigned token. As the decision cannot be revised, this may lead to suboptimal generated translations.</p><div class="Para" id="Par123"><strong class="EmphasisTypeBold ">Beam search</strong><span id="ITerm91"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>] keeps a fixed number <em class="EmphasisTypeItalic ">k</em> of possible translations <em class="EmphasisTypeItalic ">s</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em></sub> of growing length (Fig. <span class="InternalRef"><a href="#Fig15">2.15</a></span>). At each step each translation of length <em class="EmphasisTypeItalic ">t</em> is enlarged by <em class="EmphasisTypeItalic ">k</em> different tokens at position <em class="EmphasisTypeItalic ">t</em> + 1 with the highest conditional probabilities <span class="InlineEquation" id="IEq54"><img alt="$$p(S_{t+1}=s_{t+1}|s_1,\ldots ,s_{t},v_1,\ldots ,v_{T_{\text{src}}})$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq54.png" style="width:16.32em"/></span>. From these <em class="EmphasisTypeItalic ">k</em> ∗ <em class="EmphasisTypeItalic ">k</em> token sequences only the <em class="EmphasisTypeItalic ">k</em> sequences with largest total probabilities <span class="InlineEquation" id="IEq55"><img alt="$$p(s_1,\ldots ,s_{t+1}|v_1,\ldots ,v_{T_{\text{src}}})$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq55.png" style="width:11.74em"/></span> are retained. A complete translation (containing the end-of-sentence token) is added to the final candidate list. The algorithm then picks the translation with the highest probability (normalized by the number of target words) from this list. For <em class="EmphasisTypeItalic ">k</em> = 1 beam search reduces to greedy decoding. In practice, the translation quality obtained via beam search (size of 4) is significantly better than that obtained via greedy decoding. Larger beam sizes often lead to suboptimal solutions [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>]. However, beam search is computationally very expensive (25%–50% slower depending on the base architecture and the beam size) in comparison to greedy decoding [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>].<figure class="Figure" id="Fig15"><div class="MediaObject" id="MO15"><img alt="" aria-describedby="d64e7562" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig15_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e7562"><p class="Para" id="Par236">A tree diagram of the beam search technique. It includes the B O S within square brackets block and the subsequent words with their scores.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.15</span><p class="SimplePara">Beam search is a technique for decoding a language model and producing text. At every step, the algorithm keeps track of the <em class="EmphasisTypeItalic ">k</em> most probable partial translations (bold margin). The score of each translation is equal to its log probability. The beam search continues until it reaches the end token for every branch [<span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>]</p></div></figcaption></figure></div></section>
<section class="Section2 RenderAsSection2" id="Sec23"><h3 class="Heading"><span class="HeadingNumber">2.3.3 </span>Evaluation of a Translation</h3><p class="Para" id="Par124">Traditionally, evaluation is done by comparing one or more reference translations to the generated translation, as described in the survey [<span class="CitationRef"><a epub:type="biblioref" href="#CR127" role="doc-biblioref">127</a></span>]. There are a number of automatic evaluation metrics:</p><p class="Para" id="Par125"><strong class="EmphasisTypeBoldSmallCaps ">Bleu</strong><span id="ITerm92"/> compares counts of 1-grams to 4-grams of tokens. The <span class="EmphasisTypeSmallCaps ">Bleu</span> metric ranges from 0 to 1, where 1 means an identical output with the reference. Although <span class="EmphasisTypeSmallCaps ">Bleu</span> correlates well with human judgment [<span class="CitationRef"><a epub:type="biblioref" href="#CR110" role="doc-biblioref">110</a></span>], it relies on precision alone and does not take into account recall—the proportion of the matched <em class="EmphasisTypeItalic ">n</em>-grams out of the total number of <em class="EmphasisTypeItalic ">n</em>-grams in the reference translation.</p><p class="Para" id="Par126"><strong class="EmphasisTypeBoldSmallCaps ">Rouge</strong><span id="ITerm93"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR80" role="doc-biblioref">80</a></span>] unlike <span class="EmphasisTypeSmallCaps ">Bleu</span> is a recall-based measure and determines which fraction of the words or n-grams in the reference text appear in the generated text. It determines, among other things, the overlap of unigrams or bigrams as well as the longest common subsequence between a pair of texts. Different versions are used: <span class="EmphasisTypeSmallCaps ">Rouge-1</span> measures the overlap of unigram (single words) between the pair of texts. <span class="EmphasisTypeSmallCaps ">Rouge-2</span> determines the overlap of bigrams (two-words sequences) between the pair of texts. <span class="EmphasisTypeSmallCaps ">Rouge-L</span>: measures the length of the longest sequence of words (not necessarily consecutive, but still in order) that is shared between both texts. This length is divided by the number of words in the reference text.</p><p class="Para" id="Par127"><strong class="EmphasisTypeBoldSmallCaps ">Meteor</strong><span id="ITerm94"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR75" role="doc-biblioref">75</a></span>] was proposed to address the deficits of <span class="EmphasisTypeSmallCaps ">Bleu</span>. It performs a word-to-word alignment between the translation output and a given reference translation. The alignments are produced via a sequence of word-mapping modules. These check, if the words are exactly the same, same after they are stemmed using the Porter stemmer, and if they are synonyms of each other. After obtaining the final alignment, <span class="EmphasisTypeSmallCaps ">Meteor</span> computes an F-value, which is a parameterized harmonic mean of unigram precision and recall. <span class="EmphasisTypeSmallCaps ">Meteor</span> has also demonstrated to have a high level of correlation with human judgment, often even better than <span class="EmphasisTypeSmallCaps ">Bleu</span>.</p><p class="Para" id="Par128"><strong class="EmphasisTypeBold ">BERTscore</strong><span id="ITerm95"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR164" role="doc-biblioref">164</a></span>] takes into account synonyms and measures the similarity of embeddings between the translation and the reference. It computes the cosine similarity between all token embeddings of both texts. Then a greedy matching approach is used to determine assignments of tokens. The maximum assignment similarity is used as BERTscore.</p><p class="Para" id="Par129">For high-quality translations, however, there is a noticeable difference between human judgment and automatic evaluation. Therefore, most high-end comparisons today use human experts to assess the quality of translation and other text generation methods. Since the transformer was proposed by Vaswani et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>] in 2017, its variants were able to raise the <span class="EmphasisTypeSmallCaps ">Sota</span> in language translation performance, e.g. for translation on WMT2014 English-French from 37.5 to 46.4 <span class="EmphasisTypeSmallCaps ">Bleu</span> score.</p><p class="Para" id="Par130">The transformer architecture was analyzed theoretically. Yun et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR160" role="doc-biblioref">160</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR161" role="doc-biblioref">161</a></span>] showed that transformers are expressive enough to capture all continuous sequence to sequence functions with a compact domain. Pérez et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR112" role="doc-biblioref">112</a></span>] derived that the full transformer is Turing complete, i.e. can simulate a full Turing machine.</p></section>
<section class="Section2 RenderAsSection2" id="Sec24"><h3 class="Heading"><span class="HeadingNumber">2.3.4 </span>Pre-trained Language Models and Foundation Models</h3><div class="Para" id="Par131">A model <em class="EmphasisTypeItalic ">language model</em> either computes the joint probability or the conditional probability of natural language texts and potentially includes all information about the language. BERT is an <em class="EmphasisTypeItalic ">autoencoder</em><span id="ITerm96"/> language models containing encoder blocks to generate contextual embeddings of tokens. GPT is an <em class="EmphasisTypeItalic ">autoregressive language models</em><span id="ITerm97"/> which predicts the next token of a sequence and restricts self-attention to tokens which already have been generated. <em class="EmphasisTypeItalic ">Transformers</em><span id="ITerm98"/> (or <em class="EmphasisTypeItalic ">Transformer encoder-decoders</em><span id="ITerm99"/>) use a transformer encoder to convert the input text to contextual embeddings and generate the translated text with an autoregressive transformer decoder utilizing the encoder embeddings as inputs (Fig. <span class="InternalRef"><a href="#Fig16">2.16</a></span>). These models are the backbone of modern NLP and are collectively called <em class="EmphasisTypeItalic ">Pre-trained Language Models</em><span id="ITerm100"/> (<em class="EmphasisTypeItalic ">PLM</em>).<figure class="Figure" id="Fig16"><div class="MediaObject" id="MO16"><img alt="" aria-describedby="d64e7739" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig16_HTML.png" style="width:34.07em"/><div class="TextObject" id="d64e7739"><p class="Para" id="Par237">3 flow diagrams of the B E R T autoencoder, G P T language model, and transformer encoder-decoder from left to right. They include input tokens, transformer encoder and decoder blocks, L classifiers, and target tokens.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.16</span><p class="SimplePara">Autoencoders like BERT (left) and autoregressive LMs like GPT-2 (middle) use transformer blocks to generate contextual embeddings of tokens. The transformer (right) combines a transformer encoder and an autoregressive transformer decoder to produce a translation. All models predict the probability of tokens with a logistic classifier <em class="EmphasisTypeItalic ">L</em>. Collectively these models are called Pre-trained Language Models (PLMs)</p></div></figcaption></figure></div><p class="Para" id="Par132">All these models, especially BERT and GPT, are initialized via pre-training on a large corpus of text documents. During pre-training, parts of the input are hidden from the model, and the model is trained to reconstruct these parts. This has proven to be extremely effective in building strong representations of language and in finding parameter initializations for highly expressive NLP models that can be adapted to specific tasks. Finally, these models provide probability distributions over language that we can sample from.</p><p class="Para" id="Par133">Most network types have some built-in assumptions called <em class="EmphasisTypeItalic ">inductive bias</em><span id="ITerm101"/>. Convolutional networks have local kernel functions that are shifted over the input matrix and therefore have an inductive bias of translation invariance and locality. Recurrent networks apply the same network to each input position and have a temporal invariance and locality. The BERT architecture makes only few assumptions about the structural dependency in data. The GPT model is similar to the RNN as it assumes a Markovian structure of dependencies to the next token. As a consequence, PLMs often require more training data to learn the interactions between different data points, but can later represent these interactions more accurately than other model types.</p><div class="Para" id="Par134">Historically, learned embedding vectors were used as representations of words for downstream tasks (Fig. <span class="InternalRef"><a href="#Fig17">2.17</a></span>). As early as 2003 Bengio et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR15" role="doc-biblioref">15</a></span>] proposed a distributed vector representation of words to predict the next word by a recurrent model. In 2011 Collobert et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>] successfully employed word embeddings for part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. In 2013 Mikolov et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR93" role="doc-biblioref">93</a></span>] derived their word embeddings using a logistic classifier. In 2015 Dai et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>] trained embeddings with an RNN language model in a self-supervised way and later applied it to text classification. In 2017 McCann et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR87" role="doc-biblioref">87</a></span>] pre-trained multilayer LSTMs for translation computing contextualized word vectors, which are later used for various classification tasks.<figure class="Figure" id="Fig17"><div class="MediaObject" id="MO17"><img alt="" aria-describedby="d64e7782" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig17_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e7782"><p class="Para" id="Par238">A chronological timeline of the developments from 2011 to 2022. They include word embeddings for N E R, word embeddings by logistic regression, and embeddings by R N N, among others.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.17</span><p class="SimplePara">Timeline for the development of embeddings, pre-training and fine-tuning</p></div></figcaption></figure></div><p class="Para" id="Par135">In the same year Vaswani et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>] developed the attention-only transformer for language translation. In 2018 Howard et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR59" role="doc-biblioref">59</a></span>] pre-trained a language model (ULMFiT), and demonstrated the effectiveness of fine-tuning to different target tasks by updating the full (pre-trained) model for each task. In the same year Howard et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR116" role="doc-biblioref">116</a></span>] used a pre-trained autoregressive part of the transformer [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>] to solve a large number of text understanding problems by fine-tuned models. At the same time Devlin et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>] pre-trained the autoencoder using the masked language model objective and adapted this BERT model to many downstream tasks by fine-tuning. In 2019 Radford et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR118" role="doc-biblioref">118</a></span>] presented the GPT-2 language model, which was able to generate semantically convincing texts. Brown et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>] proposed the GPT-3 model, which could be instructed to solve NLP-tasks by a task description and some examples. In 2021 Ramesh et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR121" role="doc-biblioref">121</a></span>] applied language modeling to text and pictures and were able to create impressive pictures from textual descriptions. Borgeaud et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>] presented the Retro model that answers questions by retrieving information from a text collection of 2 trillion tokens and composes an answer in natural language.</p><p class="Para" id="Par136">Almost all state-of-the-art NLP models are now adapted from one of a few Pre-trained Language Models, such as BERT, GPT-2, T5, etc. PLMs are becoming larger and more powerful, leading to new breakthroughs and attracting more and more research attention. Due to the huge increase in performance, some research groups have suggested that large-scale PLMs should be called <em class="EmphasisTypeItalic ">Foundation Models</em><span id="ITerm102"/>, as they constitute a ‘foundational’ breakthrough technology that can potentially impact many types of applications [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>, p. 3]. In this book, we reserve the term ‘Foundation Models’ for large Pre-trained Language Models with more than a billion parameters, since these models are able of generating fluent text, can potentially handle different media, and can usually be instructed by prompts to perform specific tasks.</p><div class="Para" id="Par137">If one of these models is improved, this high degree of homogeneity can lead to immediate benefits for many NLP applications. On the other hand all systems could share the same problematic biases present in a few basic models. As we will see in later chapters PLM-based sequence modeling approaches are now applied to text (Sect. <span class="InternalRef"><a href="#Sec11">2.2</a></span>), speech (Sect. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml#Sec1"><span class="RefSource">7.​1</span></a></span>), images (Sect. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml#Sec12"><span class="RefSource">7.​2</span></a></span>), videos (Sect. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml#Sec23"><span class="RefSource">7.​3</span></a></span>), computer code (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec46"><span class="RefSource">6.​5.​6</span></a></span>), and control (Sect. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml#Sec30"><span class="RefSource">7.​4</span></a></span>). These overarching capabilities of Foundation Models are depicted in Fig. <span class="InternalRef"><a href="#Fig18">2.18</a></span>.<figure class="Figure" id="Fig18"><div class="MediaObject" id="MO18"><img alt="" aria-describedby="d64e7876" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig18_HTML.png" style="width:33.85em"/><div class="TextObject" id="d64e7876"><p class="Para" id="Par239">An illustrated process flow of the foundation model. It includes 5 types of data, training, a foundation model, and a series of tasks via adaption.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.18</span><p class="SimplePara">A Foundation Model can integrate the information in the data from different modalities. Subsequently it can be adapted, e.g. by fine-tuning, to a wide range of downstream tasks [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>, p. 6]. Credits for image parts in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab1"><span class="RefSource">A.​1</span></a></span></p></div></figcaption></figure></div><p class="Para" id="Par138">The next Sect. <span class="InternalRef"><a href="#Sec27">2.4</a></span> discusses some common techniques for optimizing and regularizing pre-trained language models. In addition, some approaches to modify the architecture of these networks are presented. In Chap. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml"><span class="RefSource">3</span></a></span> we present a number of approaches to improve the capabilities of PLMs, especially by modifying the training tasks (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec4"><span class="RefSource">3.​1.​3</span></a></span>). In the Chaps. <span class="ExternalRef"><a href="528393_1_En_5_Chapter.xhtml"><span class="RefSource">5</span></a></span>–<span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml"><span class="RefSource">7</span></a></span> we discuss a number of applications of PLMs. Chapter <span class="ExternalRef"><a href="528393_1_En_5_Chapter.xhtml"><span class="RefSource">5</span></a></span> covers traditional NLP tasks like named entity recognition and relation extraction, where PLMs currently perform best. Most important applications of Foundation Models are on the one hand text generation and related tasks like question-answering and dialog systems, which are introduced in Chap. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml"><span class="RefSource">6</span></a></span>. On the other hand Foundation Models can simultaneously process different media and perform tasks like image captioning, object detection in images, image generation following a text description, video interpretation, or computer game control, which are discussed in Chap. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml"><span class="RefSource">7</span></a></span>. Because of the potential social and societal consequences of such Foundation Models, it is particularly important that researchers in this field keep society’s values and human rights in mind when developing and applying these models. These aspects are summarized in Sect. <span class="ExternalRef"><a href="528393_1_En_8_Chapter.xhtml#Sec10"><span class="RefSource">8.​2</span></a></span>.</p><section class="Section3 RenderAsSection3" id="Sec25"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par139"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par140">The source code for many pre-trained language models (BERT, GPT, Transformers) as well as pre-trained models for different languages and text corpora can be downloaded from Hugging Face <span class="ExternalRef"><a href="https://huggingface.co/transformers/"><span class="RefSource">https://​huggingface.​co/​transformers/​</span></a></span>, Fairseq <span class="ExternalRef"><a href="https://github.com/pytorch/fairseq"><span class="RefSource">https://​github.​com/​pytorch/​fairseq</span></a></span>, TensorFlow <span class="ExternalRef"><a href="https://www.tensorflow.org/"><span class="RefSource">https://​www.​tensorflow.​org/​</span></a></span> and PyTorch <span class="ExternalRef"><a href="https://pytorch.org/"><span class="RefSource">https://​pytorch.​org/​</span></a></span>. These toolkits also allow the flexible formulation of Deep Neural Networks and provide the automatic computation of gradients as well as optimization methods. All are able to execute computations in parallel and distribute them to different CPUs and Graphical Processing Units (GPUs).</p></li><li><p class="Para" id="Par141">PLMs are getting larger than the memory of a single GPU and require to distribute training code among several GPUs. This is supported by libraries like FastSeq <span class="ExternalRef"><a href="https://github.com/microsoft/fastseq"><span class="RefSource">https://​github.​com/​microsoft/​fastseq</span></a></span>, LightSeq <span class="ExternalRef"><a href="https://github.com/bytedance/lightseq"><span class="RefSource">https://​github.​com/​bytedance/​lightseq</span></a></span>, and FastT5 <span class="ExternalRef"><a href="https://github.com/Ki6an/fastT5"><span class="RefSource">https://​github.​com/​Ki6an/​fastT5</span></a></span>.</p></li><li><p class="Para" id="Par142">DeepSpeed [<span class="CitationRef"><a epub:type="biblioref" href="#CR122" role="doc-biblioref">122</a></span>]<span id="ITerm103"/> was used to train the MT-NLG autoregressive LM with 530B parameters (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec3"><span class="RefSource">3.​1.​2</span></a></span>) <span class="ExternalRef"><a href="https://github.com/microsoft/DeepSpeed"><span class="RefSource">https://​github.​com/​microsoft/​DeepSpeed</span></a></span>.</p></li><li><p class="Para" id="Par143">Ecco [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>] <span class="ExternalRef"><a href="https://github.com/jalammar/ecco"><span class="RefSource">https://​github.​com/​jalammar/​ecco</span></a></span> and BertViz [<span class="CitationRef"><a epub:type="biblioref" href="#CR144" role="doc-biblioref">144</a></span>] <span class="ExternalRef"><a href="https://github.com/jessevig/bertviz"><span class="RefSource">https://​github.​com/​jessevig/​bertviz</span></a></span> are tools to visualize the attentions and embeddings of PLMs.</p></li><li><p class="Para" id="Par144">Transformers-interpret <span class="ExternalRef"><a href="https://github.com/cdpierse/transformers-interpret"><span class="RefSource">https://​github.​com/​cdpierse/​transformers-interpret</span></a></span> is a model explainability tool designed for the Hugging Face package.</p></li><li><p class="Para" id="Par145">Captum [<span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>] is a library <span class="ExternalRef"><a href="https://captum.ai/"><span class="RefSource">https://​captum.​ai/​</span></a></span> to generate interpretations and explanations for the predictions of PyTorch models.</p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec26"><h3 class="Heading"><span class="HeadingNumber">2.3.5 </span>Summary</h3><p class="Para" id="Par146">A transformer is a sequence-to-sequence model, which translates a source text of the input language into a target text in the target language. It consists of an encoder with the same architecture as an autoencoder BERT model that computes contextual embeddings of tokens of the source text. The decoder resembles an autoregressive GPT model and sequentially generates the tokens of the target text. Internally, contextual embeddings of the target tokens are computed in the different layers. Each decoder block has an additional cross-attention module in which the query vectors are taken from the embeddings of the target tokens and the key and value vectors are computed for the embeddings of the source tokens of the last layer. In this way, the information from the source text is communicated to the decoder. The embedding of the last token in the top layer is entered into a logistic classifier and this calculates the probability of the tokens for the next position. Subsequently, the observed token at the next position is appended to the target input and the computations are repeated for the next but one position.</p><p class="Para" id="Par147">During training the parameters of the transformer are adapted by stochastic gradient descent in such a way that the model assigns high probabilities to the observed target tokens of the translation in the training data. When the model has been trained on a large text dataset it can be applied for translation. Conditional on an input text, it can sequentially compute the probability of the next token of the translation.</p><p class="Para" id="Par148">During application of a trained model either the token with the maximal probability is selected or several alternatives are generated by beam search and the final output sequence with maximal probability is chosen. The evaluation of the translations quality is difficult as different translations may be correct. A number of metrics, e.g. <span class="EmphasisTypeSmallCaps ">Bleu</span>, have been developed, which compare the machine translation to one or more reference translations by comparing the number of common word <em class="EmphasisTypeItalic ">n</em>-grams with <em class="EmphasisTypeItalic ">n</em> = 1, …, 4. Often the results are assessed by human raters. The transformer was able to generate better translation than prior models. In the meantime the translation quality for a number of language pairs is on par with human translators.</p><p class="Para" id="Par149">In the previous sections, we discussed <em class="EmphasisTypeItalic ">autoencoder BERT</em> models, <em class="EmphasisTypeItalic ">autoregressive GPT</em> models and the <em class="EmphasisTypeItalic ">encoder-decoder Transformers</em>. Collectively these models are called <em class="EmphasisTypeItalic ">pre-trained language models</em>, as transfer learning with a pre-training step using a large training set and a subsequent fine-tuning step is a core approach for all three variants. The self-attention and cross-attention modules are central building blocks used by all three models. Despite the development of many variations in recent years, the original architecture developed by Vaswani et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>] is still commonly employed.</p><p class="Para" id="Par150">It turns out that these models can be applied not only to text, but to various types of sequences, such as images, speech, and videos. In addition, they may be instructed to perform various tasks by simple prompts. Therefore, large PLMs are also called <em class="EmphasisTypeItalic ">Foundation Models</em>, as they are expected to play a crucial role in the future development of text and multimedia systems.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec27"><h2 class="Heading"><span class="HeadingNumber">2.4 </span>Training and Assessment of Pre-trained Language Models</h2><div class="Para" id="Par151">This section describes some techniques required to train and apply PLMs. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par152">We need <em class="EmphasisTypeItalic ">optimization techniques</em> which can process millions and billions of parameters and training examples.</p></li><li><p class="Para" id="Par153">Specific <em class="EmphasisTypeItalic ">regularization</em> methods are required to train the models and to avoid overfitting.</p></li><li><p class="Para" id="Par154">The <em class="EmphasisTypeItalic ">uncertainty</em> of model predictions has to be estimated to asses the performance of models.</p></li><li><p class="Para" id="Par155">The <em class="EmphasisTypeItalic ">explanation</em> of model predictions can be very helpful for the acceptance of models.</p></li></ul></div> Approaches to solving these problems are discussed in this section. PLMs are usually specified in one of the current Deep Learning frameworks. Most popular are <em class="EmphasisTypeItalic ">TensorFlow</em><span id="ITerm104"/> provided from Google [<span class="CitationRef"><a epub:type="biblioref" href="#CR137" role="doc-biblioref">137</a></span>] and <em class="EmphasisTypeItalic ">PyTorch</em><span id="ITerm105"/> from Meta [<span class="CitationRef"><a epub:type="biblioref" href="#CR114" role="doc-biblioref">114</a></span>]. Both are based on the Python programming language and include language elements to specify a network, train it in parallel on dedicated hardware, and to deploy it to different environments. A newcomer is the <em class="EmphasisTypeItalic ">JAX</em><span id="ITerm106"/> framework [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>], which is especially flexible for rapid experimentation. It has a compiler for linear algebra to accelerate computations for machine learning research.</div><section class="Section2 RenderAsSection2" id="Sec28"><h3 class="Heading"><span class="HeadingNumber">2.4.1 </span>Optimization of PLMs</h3><section class="Section3 RenderAsSection3" id="Sec29"><h4 class="Heading">Basics of PLM Optimization</h4><div class="Para" id="Par156">For the i.i.d. training sample <em class="EmphasisTypeItalic ">Tr</em> = {(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[1]</sup>, <em class="EmphasisTypeItalic ">y</em><sup>[1]</sup>), …, (<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">N</em>]</sup>, <em class="EmphasisTypeItalic ">y</em><sup>[<em class="EmphasisTypeItalic ">N</em>]</sup>)} parameter optimization for Deep Neural Networks aims to find a model that minimizes the loss function <em class="EmphasisTypeItalic ">L</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[<em class="EmphasisTypeItalic ">i</em>]</sup>, <em class="EmphasisTypeItalic ">y</em><sup>[<em class="EmphasisTypeItalic ">i</em>]</sup>;<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>) <div class="Equation NumberedEquation" id="Equ14"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \min_{\boldsymbol{w}} L({\boldsymbol{w}})=L({\boldsymbol{x}}^{[1]},y^{[1]};{\boldsymbol{w}}) +\cdots+L({\boldsymbol{x}}^{[N]},y^{[N]};{\boldsymbol{w}}). {} \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ14.png" style="width:22.76em"/></div></div> <div class="EquationNumber">(2.14)</div></div></div> First-order optimization methods, also known as gradient-based optimization, are based on first-order derivatives. A requirement is that the loss function <em class="EmphasisTypeItalic ">L</em>(<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>) is smooth, i.e. is continuous and in addition differentiable at almost all parameter values <em><strong class="EmphasisTypeBoldItalic ">w</strong></em> = (<em class="EmphasisTypeItalic ">w</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">w</em><sub><em class="EmphasisTypeItalic ">k</em></sub>). Then the partial derivatives <span class="InlineEquation" id="IEq56"><img alt="$$\frac {\partial L({\boldsymbol {w}})}{\partial w_j}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq56.png" style="width:2.25em"/></span> of <em class="EmphasisTypeItalic ">L</em>(<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>) with respect to any component <em class="EmphasisTypeItalic ">w</em><sub><em class="EmphasisTypeItalic ">j</em></sub> of <em><strong class="EmphasisTypeBoldItalic ">w</strong></em> can be computed at almost all points. The <em class="EmphasisTypeItalic ">gradient</em><span id="ITerm107"/> of <em class="EmphasisTypeItalic ">L</em>(<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>) in a specific point <em><strong class="EmphasisTypeBoldItalic ">w</strong></em> is the vector <div class="Equation NumberedEquation" id="Equ15"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \frac{\partial L({\boldsymbol{w}})}{\partial {\boldsymbol{w}}} = \left( \frac{\partial L({\boldsymbol{w}})}{\partial w_1},\ldots,\frac{\partial L({\boldsymbol{w}})}{\partial w_k}\right)^\intercal . {} \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ15.png" style="width:15.81em"/></div></div> <div class="EquationNumber">(2.15)</div></div></div> The gradient points into the direction, where <em class="EmphasisTypeItalic ">L</em>(<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>) in point <em><strong class="EmphasisTypeBoldItalic ">w</strong></em> has its steepest ascent. Consequently, the direction of the steepest descent is in the opposite direction <span class="InlineEquation" id="IEq57"><img alt="$$-\frac {\partial L({\boldsymbol {w}})}{\partial {\boldsymbol {w}}}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq57.png" style="width:3.2em"/></span>. The batch <em class="EmphasisTypeItalic ">gradient descent</em><span id="ITerm108"/> algorithm therefore changes the current parameter <em><strong class="EmphasisTypeBoldItalic ">w</strong></em><sub>(<em class="EmphasisTypeItalic ">t</em>)</sub> in the direction of the negative gradient to get closer to the minimum <div class="Equation NumberedEquation" id="Equ16"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} {\boldsymbol{w}}_{(t+1)} = {\boldsymbol{w}}_{(t)} - \lambda\frac{\partial L({\boldsymbol{w}})}{\partial {\boldsymbol{w}}} {}. \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ16.png" style="width:11.44em"/></div></div> <div class="EquationNumber">(2.16)</div></div></div> The <em class="EmphasisTypeItalic ">learning rate</em><span id="ITerm109"/><em class="EmphasisTypeItalic ">λ</em> determines the step-size or how much to move in each iteration until an optimal value is reached. As the gradient is usually different for each parameter <em><strong class="EmphasisTypeBoldItalic ">w</strong></em><sub>(<em class="EmphasisTypeItalic ">t</em>)</sub> it has to be recomputed for every new parameter vector (Fig. <span class="InternalRef"><a href="#Fig19">2.19</a></span>). The iteration process is repeated until the derivative becomes close to zero. A zero gradient indicates a <em class="EmphasisTypeItalic ">local minimum</em><span id="ITerm110"/> or a <em class="EmphasisTypeItalic ">saddle point</em><span id="ITerm111"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>, p. 79]. In practical applications it is sufficient to repeat the optimization beginning with different <em><strong class="EmphasisTypeBoldItalic ">w</strong></em>-values and stop, if the derivative is close to zero.<figure class="Figure" id="Fig19"><div class="MediaObject" id="MO19"><img alt="" aria-describedby="d64e8648" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig19_HTML.png" style="width:34.25em"/><div class="TextObject" id="d64e8648"><p class="Para" id="Par240">2 illustrations of color gradient grids with light to dark shades from top to bottom.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.19</span><p class="SimplePara">On all points of a grid the negative gradients are computed for this two-dimensional function <em class="EmphasisTypeItalic ">L</em>(<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>) (left). The gradient descent algorithm follows the negative gradients and approaches the local minima (right). The blue lines are the paths taken during minimization. Image credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab1"><span class="RefSource">A.​1</span></a></span></p></div></figcaption></figure></div><p class="Para" id="Par157">Deep Neural Networks often require many millions of training examples. The repeated computation of the gradient for all these examples is extremely costly. The <strong class="EmphasisTypeBold ">Stochastic Gradient Descent</strong><span id="ITerm112"/> (<em class="EmphasisTypeItalic ">SGD</em>) algorithm does not use the entire dataset but rather computes the gradient only for a small <em class="EmphasisTypeItalic ">mini-batch</em><span id="ITerm113"/> of <em class="EmphasisTypeItalic ">m</em> training examples at a time. In general, a mini-batch has sizes <em class="EmphasisTypeItalic ">m</em> ranging from 32 up to 1024, with even higher values for recent extremely large models. Subsequently, the parameters of the model are changed according to (<span class="InternalRef"><a href="#Equ16">2.16</a></span>).</p><p class="Para" id="Par158">For each iteration a new mini-batch is selected randomly from the training data. According to the law of large numbers the gradients computed from these mini-batches fluctuate around the true gradient for the whole training set. Therefore, the mini-batch gradient on average indicates an adequate direction for changing the parameters. Mertikopoulos et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>] show that by iteratively reducing the learning rate to 0, the SGD exhibits almost sure convergence, avoids spurious critical points such as saddle points (with probability 1), and stabilizes quickly at local minima. There are a number of variations of the SGD algorithm, which are described below [<span class="CitationRef"><a epub:type="biblioref" href="#CR65" role="doc-biblioref">65</a></span>].</p><p class="Para" id="Par159">An important step of optimization is the <em class="EmphasisTypeItalic ">initialization of parameters</em><span id="ITerm114"/>. Their initial values can determine whether the algorithm converges at all and how fast the optimization approaches the optimum. To break symmetry, the initial parameters must be random. Furthermore, the mean and variance of the parameters in each layer are set such that the resulting outputs of the layer have a well-behaved distribution, e.g. expectation 0.0 and variance 1.0. In addition, all gradients also should have such a benign distribution to avoid exploding or vanishing gradients. All Deep Learning software frameworks contain suitable initialization routines. A thorough introduction is given by Goodfellow et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>, p. 292].</p></section>
<section class="Section3 RenderAsSection3" id="Sec30"><h4 class="Heading">Variants of Stochastic Gradient Descent</h4><div class="Para" id="Par160"><strong class="EmphasisTypeBold ">Momentum</strong><span id="ITerm115"/> is a method that helps SGD to increase the rate of convergence in the relevant direction and reduce oscillations. Basically a moving average <em><strong class="EmphasisTypeBoldItalic ">u</strong></em><sub>(<em class="EmphasisTypeItalic ">t</em>)</sub> of recent gradients with a parameter <em class="EmphasisTypeItalic ">γ</em> ≈ 0.9 is computed and the parameter update is performed with this average by <div class="Equation NumberedEquation" id="Equ17"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \boldsymbol{u}_{(t)} = \gamma \boldsymbol{u}_{(t-1)}- \lambda\frac{\partial L({\boldsymbol{w}})}{\partial {\boldsymbol{w}}} \qquad  \text{where}\qquad  {\boldsymbol{w}}_{(t)} = {\boldsymbol{w}}_{(t-1)} - \boldsymbol{u}_{(t)}. {} \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ17.png" style="width:27.06em"/></div></div> <div class="EquationNumber">(2.17)</div></div></div> Note that in addition to the parameter vector <em><strong class="EmphasisTypeBoldItalic ">w</strong></em><sub>(<em class="EmphasisTypeItalic ">t</em>)</sub> the moving average <em><strong class="EmphasisTypeBoldItalic ">u</strong></em><sub>(<em class="EmphasisTypeItalic ">t</em>)</sub> of the same length has to be stored requiring the same memory as the parameter vector <em><strong class="EmphasisTypeBoldItalic ">w</strong></em>. This can consume a large additional memory size if the number of parameters approaches the billions. In recent years a number of further optimizers were developed [<span class="CitationRef"><a epub:type="biblioref" href="#CR65" role="doc-biblioref">65</a></span>]: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par161"><strong class="EmphasisTypeBold ">AdaGrad</strong><span id="ITerm116"/> adapts the learning rate dynamically based on the previous gradients. It uses smaller learning rates for features occurring often, and higher learning rates for features occurring rarely.</p></li><li><p class="Para" id="Par162"><strong class="EmphasisTypeBold ">AdaDelta</strong><span id="ITerm117"/> modifies AdaGrad. Instead of accumulating all past gradients, it restricts the accumulation window of the past gradients to some fixed size <em class="EmphasisTypeItalic ">k</em>.</p></li><li><p class="Para" id="Par163"><strong class="EmphasisTypeBold ">RMSProp</strong><span id="ITerm118"/> is also a method in which the learning rate is adapted for each of the parameters. The idea is to divide the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight.</p></li><li><p class="Para" id="Par164"><strong class="EmphasisTypeBold ">Adam</strong><span id="ITerm119"/> combines the advantages of both AdaGrad and RMSProp. Adam is based on adaptive estimates of lower-order moments. It uses running averages of both the gradients and the second moments of the gradients.</p></li></ul></div> Due to the extremely large number of parameters of PLMs second order optimization methods like <em class="EmphasisTypeItalic ">Conjugate Gradient</em><span id="ITerm120"/> or <em class="EmphasisTypeItalic ">Quasi-Newton</em><span id="ITerm121"/> are rarely employed. As the number of second order derivatives grows quadratically, only crude approximations may be used. An example is Adam, as described before.</div><p class="Para" id="Par165">An important architectural addition to PLMs to improve training are <em class="EmphasisTypeItalic ">residual connections</em><span id="ITerm122"/>, which were proposed by Vaswani et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>] for the Transformer. Residual connections have been shown to be very successful for image classification networks such as ResNet [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>] and allowed training networks with several hundred layers. The identity shortcuts skip blocks of layers to preserve features. Zhang et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR163" role="doc-biblioref">163</a></span>] analyze the representational power of networks containing residual connections.</p></section>
<section class="Section3 RenderAsSection3" id="Sec31"><h4 class="Heading">Parallel Training for Large Models</h4><p class="Para" id="Par166">Recently, there have been suggestions to reduce the optimization effort by employing larger mini-batches. You et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR159" role="doc-biblioref">159</a></span>] propose the <strong class="EmphasisTypeBold ">LAMB optimizer</strong><span id="ITerm123"/> with layerwise adaptive learning rates to accelerate training of PLMs using large mini-batches. They prove the convergence of their approach to a stationary point in a general nonconvex setting. Their empirical results demonstrate the superior performance of LAMB. It is possible to reduce the BERT training time from 3 days to just 76 min with very little hyperparameter tuning and batch sizes of 32,868 without any degradation of performance. The LAMB program code is available online [<span class="CitationRef"><a epub:type="biblioref" href="#CR97" role="doc-biblioref">97</a></span>]. In addition, the memory requirements of the optimization may be reduced [<span class="CitationRef"><a epub:type="biblioref" href="#CR119" role="doc-biblioref">119</a></span>] to enable parallelization of models resulting in a higher training speed.</p><div class="Para" id="Par167">Large models such as GPT-3 have many billion parameters that no longer fit into the memory of a single computational device, e.g. a GPU. Therefore, the computations have to be distributed among several GPUs. There are different parallelization techniques [<span class="CitationRef"><a epub:type="biblioref" href="#CR156" role="doc-biblioref">156</a></span>]: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par168"><em class="EmphasisTypeItalic ">Data parallelism</em> assigns the same model code and parameters to each GPU but different training examples [<span class="CitationRef"><a epub:type="biblioref" href="#CR72" role="doc-biblioref">72</a></span>]. Gradients are computed in parallel and finally summarized.</p></li><li><p class="Para" id="Par169"><em class="EmphasisTypeItalic ">Pipeline parallelism</em> partitions the model into different parts (e.g. layers) that are executed on different GPUs. If a part is computed it sends its results to the next GPU. This sequence is reversed in the backward pass of training.</p></li><li><p class="Para" id="Par170"><em class="EmphasisTypeItalic ">Within-layer model parallelism</em> distributes the weights of a single layer across multiple GPUs.</p></li></ul></div> The implementation of a parallelization strategy for a model is a tedious process. Support is given by the <strong class="EmphasisTypeBold ">DeepSpeed</strong><span id="ITerm124"/> library [<span class="CitationRef"><a epub:type="biblioref" href="#CR122" role="doc-biblioref">122</a></span>] that makes distributed training easy, efficient, and effective. Recently the <strong class="EmphasisTypeBold ">GSPMD</strong><span id="ITerm125"/> system [<span class="CitationRef"><a epub:type="biblioref" href="#CR156" role="doc-biblioref">156</a></span>] was developed which automates this process and is able to combine different parallelism paradigms in a unified way. GSPMD infers the distribution of computations to a network of GPUs based on limited user annotations to the model definition. It was, for instance, applied to distribute models with 1 trillion parameters on 2048 GPUs.</div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec32"><h3 class="Heading"><span class="HeadingNumber">2.4.2 </span>Regularization of Pre-trained Language Models</h3><p class="Para" id="Par171">If a model contains too many parameters it can nearly perfectly adapt to the training data by optimization, reflecting nearly all details of the training data. During this <em class="EmphasisTypeItalic ">overfitting</em><span id="ITerm126"/> the model learns the random variations expressed in the training data and deviates from the mean underlying distribution. Consequently, it has usually a lower performance on test data and a larger <em class="EmphasisTypeItalic ">generalization error</em><span id="ITerm127"/>. To avoid this phenomenon, the representational capacity of the model has to be reduced by <em class="EmphasisTypeItalic ">regularization methods</em><span id="ITerm128"/>, which often have the same effect as reducing the number of parameters. Well known approaches for Deep Learning models are the <em class="EmphasisTypeItalic ">L</em><sub>2</sub><span id="ITerm129"/> regularization and <em class="EmphasisTypeItalic ">L</em><sub>1</sub><span id="ITerm130"/> regularization penalizing large parameter values, or <em class="EmphasisTypeItalic ">Dropout</em><span id="ITerm131"/> temporarily setting randomly selected hidden variables to 0. A survey of regularization strategies for Deep Neural Networks is given by Moradi et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR96" role="doc-biblioref">96</a></span>].</p><p class="Para" id="Par172">The training of PLMs is often non-trivial. One problem is the occurrence of vanishing or exploding gradients, which is connected to the problem of the vanishing or exploding variance of input values of different layers [<span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>]. <em class="EmphasisTypeItalic ">Batch normalization</em><span id="ITerm132"/> normalizes the values of the components of hidden units to mean 0.0 and variance 1.0 and thus reduces the variation of input values. For a mini-batch of training cases the component values are aggregated to compute a mean and variance, which are then used to normalize the input of that component on each training case [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>]. It can be shown that batch normalization makes hidden representations increasingly orthogonal across layers of a Deep Neural Network [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>].</p><p class="Para" id="Par173">In their paper on the Transformer, Vaswani et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>] use a variant called <em class="EmphasisTypeItalic ">layer normalization</em><span id="ITerm133"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>] for regularization. The authors compute the mean and variance of the different components of hidden units for each training example and use this to normalize the input to mean 0.0 and variance 1.0. In addition, they apply dropout to the output of self-attention. Finally, they use <em class="EmphasisTypeItalic ">label smoothing</em><span id="ITerm134"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR133" role="doc-biblioref">133</a></span>] where the loss function is reformulated such that the observed tokens are not certain but alternative tokens may be possible with a small probability. This is a form of regularization which makes optimization easier. The RMSNorm [<span class="CitationRef"><a epub:type="biblioref" href="#CR162" role="doc-biblioref">162</a></span>] is a variant of the layer normalization, which only normalizes the input by division with the root-mean-square error without shifting the mean. In experiments, it compares favorably with the layer normalization [<span class="CitationRef"><a epub:type="biblioref" href="#CR101" role="doc-biblioref">101</a></span>].</p></section>
<section class="Section2 RenderAsSection2" id="Sec33"><h3 class="Heading"><span class="HeadingNumber">2.4.3 </span>Neural Architecture Search</h3><p class="Para" id="Par174">The structure of the self-attention block was manually designed, and it is not clear, whether it is optimal in all cases. Therefore, there are some approaches to generate the architecture of PLMs in an automatic way called <em class="EmphasisTypeItalic ">Neural Architecture Search</em><span id="ITerm135"/> (<em class="EmphasisTypeItalic ">NAS</em>). A survey is provided by He et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>], who argue that currently the contributions of architecture search to NLP tasks are minor. Zöller [<span class="CitationRef"><a epub:type="biblioref" href="#CR166" role="doc-biblioref">166</a></span>] evaluate architecture search for machine learning models.</p><p class="Para" id="Par175">Wang et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR149" role="doc-biblioref">149</a></span>] propose an architecture search space with flexible encoder-decoder attentions and heterogeneous layers. The architecture search produces several transformer versions and finally concentrates on hardware restrictions to adapt the computations to processors at hand. The authors report a speedup of 3 and a size reduction factor of 3.7 with no performance loss. For relation classification Zhu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR165" role="doc-biblioref">165</a></span>] design a comprehensive search space. They explore the search space by reinforcement learning strategy and yield models which have a better performance.</p><p class="Para" id="Par176">Architecture search may also be formulated as a ranking task. <strong class="EmphasisTypeBold ">RankNAS</strong><span id="ITerm136"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>] solves this by a series of binary classification problems. The authors investigate translation and language models. For translation the usual encoder-decoder is included in a super-net, where each of the 10<sup>23</sup> subnetworks is a unique architecture. The importance of an architectural feature (e.g., the number of layers) is measured by the increase in the model error after permuting the feature. The authors use an evolutionary optimization strategy and evaluate their approach on translation (WMT2014 En-De). They get increases in <span class="EmphasisTypeSmallCaps ">Bleu</span>-values at a fraction of cost of other approaches.</p><p class="Para" id="Par177">Recently differentiable architecture search has been developed, which embeds architecture search in a continuous search space and finds the optimal architecture by gradient descent. This leads to an efficient search process that is orders of magnitude faster than the discrete counterparts. This idea is applied by Fan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>], who propose a gradient-based NAS algorithm for machine translation. They explore attention modules and recurrent units, automatically discovering architectures with better performances. The topology of the connection among different units is learned in an end-to-end manner. On a number of benchmarks they were able to improve the performance of the Transformer, e.g. from 28.8 to 30.1 <span class="EmphasisTypeSmallCaps ">Bleu</span> scores for the WMT2014 English-to-German translation. There are other successful architecture search approaches for neural translation [<span class="CitationRef"><a epub:type="biblioref" href="#CR130" role="doc-biblioref">130</a></span>], named entity recognition [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>], and image classification models [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR147" role="doc-biblioref">147</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR148" role="doc-biblioref">148</a></span>], which may possibly be applied to other NLP tasks.</p></section>
<section class="Section2 RenderAsSection2" id="Sec34"><h3 class="Heading"><span class="HeadingNumber">2.4.4 </span>The Uncertainty of Model Predictions</h3><div class="Para" id="Par178">Variations in the outcome of a PLM can have two main sources: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par179"><em class="EmphasisTypeItalic ">Epistemic uncertainty</em><span id="ITerm137"/> reflects our limited knowledge about the real world. The real world situation corresponding to the training set can change causing a distribution shift. Moreover, the collected documents can have biases or errors and cover unwanted types of content. It is clear that the structure of the real world and the PLM differ. Therefore, a PLM can only approximate the correct conditional probabilities of language. This type of uncertainty is often called <em class="EmphasisTypeItalic ">structural uncertainty</em><span id="ITerm138"/> and is difficult to estimate.</p></li><li><p class="Para" id="Par180"><em class="EmphasisTypeItalic ">Aleatoric uncertainty</em><span id="ITerm139"/> is caused by random variations which can be assessed more easily. The training data is usually a sample of the underlying data in the population and therefore affected by the sampling variation. If a model is randomly re-initialized, it generates a completely different set of parameter values which leads to different predictions. Finally, language models predict probabilities of tokens and the generation of new tokens is also affected by uncertainty. The Bayesian framework offers a well-founded tool to assess this type of uncertainty in Deep Learning [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>].</p></li></ul></div> A recent survey of methods for estimating the model uncertainty is provided by Gawlikowski et al.[<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>]. We will describe three approaches to capture model uncertainty: Bayesian statistics, a Dirichlet distributions, and ensemble distributions.</div><section class="Section3 RenderAsSection3" id="Sec35"><h4 class="Heading">Bayesian Neural Networks</h4><div class="Para" id="Par181"><em class="EmphasisTypeItalic ">Bayesian Neural Networks</em><span id="ITerm140"/> directly represent the uncertainty of the estimated parameters <span class="InlineEquation" id="IEq58"><img alt="$${\boldsymbol {w}}=(w_1,\ldots ,w_{d_w})$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq58.png" style="width:8.31em"/></span> by the <em class="EmphasisTypeItalic ">posterior distribution</em><span id="ITerm141"/><div class="Equation NumberedEquation" id="Equ18"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} p({\boldsymbol{w}}|\boldsymbol{X},\boldsymbol{Y})\propto p({\boldsymbol{y}}|\boldsymbol{X},{\boldsymbol{w}})p({\boldsymbol{w}}) {}. \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ18.png" style="width:14em"/></div></div> <div class="EquationNumber">(2.18)</div></div></div> Here <em><strong class="EmphasisTypeBoldItalic ">X</strong></em> and <em><strong class="EmphasisTypeBoldItalic ">Y</strong></em> are the observed inputs and outputs in the training set and <em class="EmphasisTypeItalic ">p</em>(<em><strong class="EmphasisTypeBoldItalic ">Y</strong></em> |<em><strong class="EmphasisTypeBoldItalic ">X</strong></em>, <em><strong class="EmphasisTypeBoldItalic ">w</strong></em>) is the <em class="EmphasisTypeItalic ">likelihood</em><span id="ITerm142"/>, i.e. the probability of the outputs given <em><strong class="EmphasisTypeBoldItalic ">X</strong></em> and a parameter vector <em><strong class="EmphasisTypeBoldItalic ">w</strong></em>. The <em class="EmphasisTypeItalic ">prior distribution</em><span id="ITerm143"/><em class="EmphasisTypeItalic ">p</em>(<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>) describes the distribution of parameters before data is available. The distribution of predictions for a new input <span class="InlineEquation" id="IEq59"><img alt="$$\tilde {{\boldsymbol {x}}}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq59.png" style="width:0.94em"/></span> is given by <div class="Equation NumberedEquation" id="Equ19"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} p(\tilde{{\boldsymbol{y}}}|\tilde{{\boldsymbol{x}}},\boldsymbol{X},\boldsymbol{Y}) = \int p(\tilde{{\boldsymbol{y}}}|\tilde{{\boldsymbol{x}}},{\boldsymbol{w}}) p({\boldsymbol{w}}|\boldsymbol{X},\boldsymbol{Y}) d{\boldsymbol{w}} {}. \end{aligned} $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ19.png" style="width:20.07em"/></div></div> <div class="EquationNumber">(2.19)</div></div></div> The integral usually cannot be solved analytically and has to be approximated. Often a <em class="EmphasisTypeItalic ">Monte Carlo</em><span id="ITerm144"/> approximation is used, which infers the integral by a sum over different parameter values <em><strong class="EmphasisTypeBoldItalic ">w</strong></em><sup>[<em class="EmphasisTypeItalic ">i</em>]</sup> distributed according to the posterior distribution <em class="EmphasisTypeItalic ">p</em>(<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>|<em><strong class="EmphasisTypeBoldItalic ">X</strong></em>, <em><strong class="EmphasisTypeBoldItalic ">Y</strong></em> ). If <span class="InlineEquation" id="IEq60"><img alt="$$\tilde {{\boldsymbol {y}}}^{[i]}=f(\tilde {{\boldsymbol {x}}},{\boldsymbol {w}}^{[i]})$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq60.png" style="width:7.12em"/></span> is a deterministic network predicting the output for a parameter <em><strong class="EmphasisTypeBoldItalic ">w</strong></em><sup>[<em class="EmphasisTypeItalic ">i</em>]</sup> and input <span class="InlineEquation" id="IEq61"><img alt="$$\tilde {{\boldsymbol {x}}}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq61.png" style="width:0.94em"/></span>, the resulting sample <span class="InlineEquation" id="IEq62"><img alt="$$\tilde {{\boldsymbol {y}}}^{[1]},\ldots ,\tilde {{\boldsymbol {y}}}^{[k]}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq62.png" style="width:5.5em"/></span> can be considered as a sample of the output distribution <span class="InlineEquation" id="IEq63"><img alt="$$p(\tilde {{\boldsymbol {y}}}|\tilde {{\boldsymbol {x}}},\boldsymbol {X},\boldsymbol {Y})$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq63.png" style="width:6.2em"/></span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR108" role="doc-biblioref">108</a></span>].</div><div class="Para" id="Par182">Bayesian predictive distributions can be approximated in different ways: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par183"><em class="EmphasisTypeItalic ">Sampling approaches</em><span id="ITerm145"/> use a <em class="EmphasisTypeItalic ">Markov Chain Monte Carlo</em><span id="ITerm146"/> algorithm to generate parameter values distributed according to the posterior distributions, from which realizations can be sampled [<span class="CitationRef"><a epub:type="biblioref" href="#CR102" role="doc-biblioref">102</a></span>]. Markov Chain Monte Carlo defines a sampling strategy, where first a new parameter value <em><strong class="EmphasisTypeBoldItalic ">w</strong></em> is randomly generated and then the algorithm computes the probability to accept <em><strong class="EmphasisTypeBoldItalic ">w</strong></em>, or to keep the previous parameter value. Welling et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR150" role="doc-biblioref">150</a></span>] combined this approach with stochastic gradient descent and demonstrated that Bayesian inference on Deep Neural Networks can be done by a noisy SGD. A review of the favorable convergence properties has been given by Nemeth et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR103" role="doc-biblioref">103</a></span>]. Practical evaluations of this technique are performed by Wenzel et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR152" role="doc-biblioref">152</a></span>].</p></li><li><p class="Para" id="Par184"><em class="EmphasisTypeItalic ">Variational inference</em><span id="ITerm147"/> approximates the posterior distribution by a product <em class="EmphasisTypeItalic ">q</em>(<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>) of simpler distributions, which are easier to evaluate [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>]. Using multiple GPUs and practical tricks, such as data augmentation, momentum initialization and learning rate scheduling, and learning rate scheduling, Osawa et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR105" role="doc-biblioref">105</a></span>] demonstrated that variational inference can be scaled up to ImageNet size data-sets and architectures.</p><p class="Para" id="Par185">It can be shown [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>] that dropout regularization (Sect. <span class="InternalRef"><a href="#Sec32">2.4.2</a></span>) can be considered as approximate variational inference. Hence, the predictive uncertainty can be estimated by employing dropout not only during training, but also at test time. A variant called <em class="EmphasisTypeItalic ">Drop connect</em><span id="ITerm148"/> randomly removes incoming activations of a node, instead of dropping an activation for all following nodes. This approach yields a more reliable uncertainty estimate and can even be combined with the original dropout technique [<span class="CitationRef"><a epub:type="biblioref" href="#CR88" role="doc-biblioref">88</a></span>].</p></li><li><p class="Para" id="Par186"><em class="EmphasisTypeItalic ">Laplace approximation</em><span id="ITerm149"/> considers the logarithm of the posterior distribution around a local mode <span class="InlineEquation" id="IEq64"><img alt="$$\hat {{\boldsymbol {w}}}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq64.png" style="width:1.12em"/></span> and approximate it by a normal distribution <span class="InlineEquation" id="IEq65"><img alt="$$N(\hat {{\boldsymbol {w}}},[H+\beta I]^{-1})$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq65.png" style="width:8.13em"/></span> over the network weights [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>]. <em class="EmphasisTypeItalic ">H</em> is the Hessian, the matrix of second derivatives, of <span class="InlineEquation" id="IEq66"><img alt="$$\log p({\boldsymbol {w}}|\boldsymbol {X},\boldsymbol {Y})$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq66.png" style="width:6.63em"/></span>. This approximation may be computed for already trained networks and can be applied to Deep Neural Networks [<span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>]. A problem is the large number of coefficients of <em class="EmphasisTypeItalic ">H</em>, which limits the computations to elements on the diagonal. Extensions have been proposed by George et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>].</p></li></ul></div></div></section>
<section class="Section3 RenderAsSection3" id="Sec36"><h4 class="Heading">Estimating Uncertainty by a Single Deterministic Model</h4><p class="Para" id="Par187">Most PLMs predict tokens by a discrete probability distribution. If the softmax function is used to compute these probabilities, the optimization over the training set usually leads to very extreme probabilities close to 0 or 1. The network is often overconfident and generates inaccurate uncertainty estimates. To assess uncertainty, the difference between the estimated distribution and the actual distribution has to be described. If <span class="InlineEquation" id="IEq67"><img alt="$$v_1,\ldots ,v_{d_v}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq67.png" style="width:4.62em"/></span> is the vocabulary of tokens and <em><strong class="EmphasisTypeBoldItalic ">π</strong></em> a discrete distribution over these tokens, then we can use the <em class="EmphasisTypeItalic ">Dirichlet distribution</em><span id="ITerm150"/><em class="EmphasisTypeItalic ">p</em>(<em><strong class="EmphasisTypeBoldItalic ">π</strong></em>|<em><strong class="EmphasisTypeBoldItalic ">α</strong></em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>)) to characterize a distribution over these discrete distributions. The vector <em><strong class="EmphasisTypeBoldItalic ">α</strong></em> depends on the input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> and has a component <em class="EmphasisTypeItalic ">α</em><sub><em class="EmphasisTypeItalic ">i</em></sub> for each <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">i</em></sub>. The sum ∑<sub><em class="EmphasisTypeItalic ">i</em></sub><em class="EmphasisTypeItalic ">α</em><sub><em class="EmphasisTypeItalic ">i</em></sub> characterizes the variance. If it gets larger, the estimate for the probability of <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">i</em></sub> has a lower variance.</p><p class="Para" id="Par188">Malinin et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR85" role="doc-biblioref">85</a></span>] use the expected divergence between the empirical distribution and the predicted distribution to estimate the <em class="EmphasisTypeItalic ">p</em>(<em><strong class="EmphasisTypeBoldItalic ">π</strong></em>|<em><strong class="EmphasisTypeBoldItalic ">α</strong></em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>)) for a given input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>. In the region of the training data the network is trained to minimize the expected <em class="EmphasisTypeItalic ">Kullback-Leibler</em><span id="ITerm151"/> (<em class="EmphasisTypeItalic ">KL</em>) divergence between the predictions of in-distribution data and a low-variance Dirichlet distribution. In the region of out-of-distribution data a Dirichlet distribution with a higher variance is estimated. The distribution over the outputs can be interpreted as a quantification of the model uncertainty, trying to emulate the behavior of a Bayesian modeling of the network parameters [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>].</p><p class="Para" id="Par189">Liu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR83" role="doc-biblioref">83</a></span>] argue that the distance between training data elements is relevant for prediction uncertainty. To avoid that the layers of a network cause a high distortion of the distances of the input space, the authors propose a spectral normalization. This <strong class="EmphasisTypeBold ">SNGP</strong><span id="ITerm152"/> approach limits the distance <span class="InlineEquation" id="IEq68"><img alt="$$\lVert h({\boldsymbol {x}}^{[1]}) - h({\boldsymbol {x}}^{[2]}) \rVert $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq68.png" style="width:8.5em"/></span> compared to <span class="InlineEquation" id="IEq69"><img alt="$$\lVert {\boldsymbol {x}}^{[1]} - {\boldsymbol {x}}^{[2]} \rVert $$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq69.png" style="width:5.82em"/></span>, where <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[1]</sup> and <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[2]</sup> are two inputs and <em class="EmphasisTypeItalic ">h</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>) is a deep feature extractor. Then they pass <em class="EmphasisTypeItalic ">h</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>) into a distance-aware <em class="EmphasisTypeItalic ">Gaussian Process</em><span id="ITerm153"/> output layer. The Gaussian Process posterior is approximated by a Laplace approximation, which can be predicted by a deterministic Deep Neural Network.</p><p class="Para" id="Par190">The authors evaluate SNGP on BERT<sub>BASE</sub> to decide, if a natural utterance input is covered by the training data (so that it can be handled by the model) or outside. The model is only trained on in-domain data, and their predictive accuracy is evaluated on in-domain and out-of-domain data. While ensemble techniques have a slightly higher prediction accuracy, SNGP has a better calibration of probabilities and out-of-distribution detection. An implementation of the approach is available [<span class="CitationRef"><a epub:type="biblioref" href="#CR138" role="doc-biblioref">138</a></span>].</p><p class="Para" id="Par191">A number of alternative approaches are described in [<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>, p. 10f], which also discuss mixtures of Dirichlet distributions to characterize predictive uncertainty. In general single deterministic methods are computational less demanding in training and evaluation compared to other approaches. However, they rely on a single network configuration and may be very sensitive to the underlying network structure and the training data.</p></section>
<section class="Section3 RenderAsSection3" id="Sec37"><h4 class="Heading">Representing the Predictive Distribution by Ensembles</h4><p class="Para" id="Par192">It is possible to emulate the sampling variability of a training set by resampling methods. A well-founded approach is <em class="EmphasisTypeItalic ">bagging</em><span id="ITerm154"/>, where <em class="EmphasisTypeItalic ">n</em><sub><em class="EmphasisTypeItalic ">b</em></sub> samples of size <em class="EmphasisTypeItalic ">n</em> are drawn with replacement from a training set of <em class="EmphasisTypeItalic ">n</em> elements [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR107" role="doc-biblioref">107</a></span>]. For the <em class="EmphasisTypeItalic ">i</em>-th sample a model may be trained yielding a parameter <span class="InlineEquation" id="IEq70"><img alt="$$\hat {{\boldsymbol {w}}}^{[i]}$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq70.png" style="width:1.69em"/></span>. Then the distribution of predictions <span class="InlineEquation" id="IEq71"><img alt="$$f({\boldsymbol {x}},\hat {{\boldsymbol {w}}}^{[i]})$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq71.png" style="width:4.38em"/></span> represent the uncertainty in the model prediction for an input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>, and it can be shown that their mean value <span class="InlineEquation" id="IEq72"><img alt="$$\frac {1}{n_b}\sum _i f({\boldsymbol {x}},\hat {{\boldsymbol {w}}}^{[i]})$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq72.png" style="width:6.87em"/></span> has a lower variance than the original model prediction [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>]. In contrast to many approximate methods, ensemble approaches may take into account different local maxima of the likelihood function and may cover different network architectures. There are other methods to introduce data variation, e.g. random parameter initialization or random data augmentation. A survey on ensemble methods is provided by Dong et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>].</p><p class="Para" id="Par193">Besides the improvement in the accuracy, ensembles are widely used for representing prediction uncertainty of Deep Neural Networks [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>]. In empirical investigations, the approach was at least as reliable as Bayesian approaches (Monte Carlo Dropout, Probabilistic Backpropagation) [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>]. Reordering the training data and a random parameter initialization induces enough variability in the models for the prediction of uncertainty, while bagging may reduce the reliability of uncertainty estimation [<span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>]. Compared to Monte Carlo Dropout, ensembles yield more reliable and better calibrated prediction uncertainties and are applicable to real-world training data [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>]. Already for a relatively small ensemble size of five, deep ensembles seem to perform best and are more robust to data set shifts than the compared methods [<span class="CitationRef"><a epub:type="biblioref" href="#CR106" role="doc-biblioref">106</a></span>].</p><p class="Para" id="Par194">Although PLMs have been adapted as a standard solution for most NLP tasks, the majority of existing models is unable to estimate the uncertainty associated with their predictions. This seems to be mainly caused by the high computational effort of uncertainty estimation approaches. In addition, the concept of uncertainty of a predicted probability distribution is difficult to communicate. However, it is extremely important to get a diagnosis, when a PLM is given an input outside the support of its training data, as then the predictions get unreliable.</p><p class="Para" id="Par195">Among the discussed approaches the ensemble methods seem to be most reliable. However, they require a very high computational effort. New algorithms like SNGP are very promising. More research is needed to reduce this effort or develop alternative approaches. Recently benchmark repositories and datasets have been developed to provide high-quality implementations of standard and <span class="EmphasisTypeSmallCaps ">Sota</span> methods and describe best practices for uncertainty and robustness benchmarking [<span class="CitationRef"><a epub:type="biblioref" href="#CR99" role="doc-biblioref">99</a></span>].</p><div class="FormalPara FormalParaRenderingStyle3"><div class="Heading">Implementations</div><p class="Para FirstParaInFormalPara" id="Par196">Uncertainty Baselines [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR98" role="doc-biblioref">98</a></span>] provide a collection high-quality implementations of standard and state-of-the-art methods for uncertainty assessment.</p></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec38"><h3 class="Heading"><span class="HeadingNumber">2.4.5 </span>Explaining Model Predictions</h3><p class="Para" id="Par197">PLMs such as BERT are considered as black box models, as it is hard to understand, what they really learn and what determines their outputs. Hence, a lot of research goes into investigating the behavior of these models. There are three main reasons to explain the model predictions. <em class="EmphasisTypeItalic ">Trust</em> in the model predictions is needed, i.e. that the model generates reliable answers for the problem at hand and can be deployed in real-world applications. <em class="EmphasisTypeItalic ">Causality</em> asserts that the change of input attributes leads to sensible changes in the model predictions. <em class="EmphasisTypeItalic ">Understanding</em> of the model enables domain experts to compare the model prediction to the existing domain knowledge. This is a prerequisite for the ability to adjust the prediction model by incorporating domain knowledge.</p><p class="Para" id="Par198">Explanations can also be used to debug a model. A striking example was an image classification, where a horse was not detected by its shape, but by a label in the image [<span class="CitationRef"><a epub:type="biblioref" href="#CR74" role="doc-biblioref">74</a></span>]. Explanations are most important for critical decisions that involve humans or can cause high damage. Examples are health care, the judicial system, banking, or self-driving cars.</p><div class="Para" id="Par199">Explanation methods roughly can be grouped into local explanations or global explanations. A local explanation provides information or justification for the model’s prediction for a specific input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>, whereas global explanations cover the model in general. A large majority of models aims at local explanations, as these may be used to justify specific predictions. Surveys on methods for the explanation of PLMs are provided by Danilevsky et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>], Burkart and Huber [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>], Xu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR155" role="doc-biblioref">155</a></span>], Bauckhage et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>], Tjoa and Guan [<span class="CitationRef"><a epub:type="biblioref" href="#CR139" role="doc-biblioref">139</a></span>], and Belle and Papantonis [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>]. Molnar [<span class="CitationRef"><a epub:type="biblioref" href="#CR95" role="doc-biblioref">95</a></span>] devotes a whole book to this topic and Bommasani et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>, p. 125] provide a recent overview. For language models different types of explanation can be used: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par200"><strong class="EmphasisTypeBold ">Feature importance</strong> measures the influence of single input features, e.g. tokens, on the prediction. It often corresponds to the first derivative of a feature with respect to the output [<span class="CitationRef"><a epub:type="biblioref" href="#CR79" role="doc-biblioref">79</a></span>]. As the meaning of input tokens is easily understood, this type of explanation is readily interpretable by humans.</p></li><li><p class="Para" id="Par201"><strong class="EmphasisTypeBold ">Counterfactual explanations</strong> investigate, how an input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> has to be modified, to generate a different target output.</p></li><li><p class="Para" id="Par202"><strong class="EmphasisTypeBold ">Surrogate models</strong> explain model predictions by a second, simpler model. One well-known example is <em class="EmphasisTypeItalic ">LIME</em><span id="ITerm155"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR123" role="doc-biblioref">123</a></span>], which trains a local linear model around a single input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> of interest.</p></li><li><p class="Para" id="Par203"><strong class="EmphasisTypeBold ">Example-driven</strong> explanations illustrate the prediction of an input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> by selecting other labeled instances that are semantically similar to <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>. This is close to the nearest neighbor approach to prediction and has, for instance, been used for text classification [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>].</p></li><li><p class="Para" id="Par204"><strong class="EmphasisTypeBold ">Source citation</strong> is a general practice of scientific work in which a claim is supported by citing respectable scientific sources. The same can be done for a text generated by language models with a retrieval component [<span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>].</p></li></ul></div> Other approaches like a sequence of reasoning steps or rule invocations are unusable for PLMs with many millions of parameters.</div><p class="Para" id="Par205">The self-attention mechanism is the central function unit of PLMs. <strong class="EmphasisTypeBold ">BertViz</strong><span id="ITerm156"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR144" role="doc-biblioref">144</a></span>] is a visualization tool that allows users to explore the strength of attention between different tokens for the heads and layers in a PLM and allows users to get a quick overview of relevant attention heads. However, Jain et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>] demonstrate that attention does not correlate with feature importance methods and counterfactual changes of attention do not lead to corresponding changes in prediction. This may, for instance, be caused by the concatenation of head outputs and their subsequent processing by a fully connected nonlinear layer. Attentions are noisy predictors of the overall importance of components, but are not good at identifying the importance of features [<span class="CitationRef"><a epub:type="biblioref" href="#CR129" role="doc-biblioref">129</a></span>].</p><section class="Section3 RenderAsSection3" id="Sec39"><h4 class="Heading">Linear Local Approximations</h4><p class="Para" id="Par206">An important concept is the contribution of input <em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">i</em></sub> towards an output <em class="EmphasisTypeItalic ">y</em><sub><em class="EmphasisTypeItalic ">j</em></sub>, e.g. a class probability. Gradient-based explanations estimate the contribution of input <em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">i</em></sub> towards an output <em class="EmphasisTypeItalic ">y</em><sub><em class="EmphasisTypeItalic ">j</em></sub>, e.g. a class probability, by computing the partial derivative <em class="EmphasisTypeItalic ">∂y</em><sub><em class="EmphasisTypeItalic ">j</em></sub>∕<em class="EmphasisTypeItalic ">∂x</em><sub><em class="EmphasisTypeItalic ">i</em></sub>. This derivative is often called <em class="EmphasisTypeItalic ">saliency</em><span id="ITerm157"/> and can be interpreted as linear approximation to the prediction function at input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>. <strong class="EmphasisTypeBold ">LIME</strong><span id="ITerm158"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR123" role="doc-biblioref">123</a></span>] defines a local linear regression model around a single input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>. Because of correlation of features, the coefficients of the input features depend on the presence or absence of the other input features. The <strong class="EmphasisTypeBold ">SHAP</strong><span id="ITerm159"/> approach therefore determines the influence of a feature by the average influence of the feature for all combinations of other features [<span class="CitationRef"><a epub:type="biblioref" href="#CR84" role="doc-biblioref">84</a></span>]. The authors show the favorable theoretical properties of this approach and derive several efficient computation strategies.</p></section>
<section class="Section3 RenderAsSection3" id="Sec40"><h4 class="Heading">Nonlinear Local Approximations</h4><p class="Para" id="Par207">Sundararajan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR132" role="doc-biblioref">132</a></span>] formulate two basic requirements for this type of explanation. <em class="EmphasisTypeItalic ">Sensitivity</em>: if the inputs <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[1]</sup> and <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[2]</sup> differ in just one feature and lead to different predictions, then the differing feature should be given a non-zero contribution. <em class="EmphasisTypeItalic ">Implementation invariance</em>: i.e., the attributions are always identical for two functionally equivalent networks. As the prediction functions are usually nonlinear, gradient-based methods violate both requirements and may focus on irrelevant attributes.</p><div class="Para" id="Par208"><strong class="EmphasisTypeBold ">Integrated Gradients</strong><span id="ITerm160"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR132" role="doc-biblioref">132</a></span>] generates an approximation to the prediction function <span class="InlineEquation" id="IEq73"><img alt="$$F:\mathbb {R}^n\to [0,1]$$" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq73.png" style="width:6.57em"/></span>, which captures nonlinear dependencies. To assess the difference from baseline input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[1]</sup> to another input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[2]</sup>, the authors compute the mean value of gradients <em class="EmphasisTypeItalic ">∂F</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>)∕<em class="EmphasisTypeItalic ">∂</em><em><strong class="EmphasisTypeBoldItalic ">x</strong></em> of the output with respect to inputs along the line from <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[1]</sup> to <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[2]</sup> by an integral. It can be shown that this approach meets the above requirements. The authors apply the approach to question classification according to the type of the answer (Fig. <span class="InternalRef"><a href="#Fig20">2.20</a></span>). The baseline input is the all zero embedding vector. Another application considers neural machine translation. Here the output probability of every output token is attributed to the input tokens. As baseline all tokens were zeroed except the start and end markers. A similar analysis is based on a Taylor expansion of the prediction function [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>] .<figure class="Figure" id="Fig20"><div class="MediaObject" id="MO20"><img alt="" aria-describedby="d64e10603" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig20_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e10603"><p class="Para" id="Par241">A questionnaire on the left illustrates the classification task. On the right, a confusion matrix illustrates the task of translating an English input sentence into German.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.20</span><p class="SimplePara">Contributions for the question classification task (left). Red marks positive influence, blue negative, and black tokens are neutral. Contributions for the task of translating <em class="EmphasisTypeItalic ">“good morning ladies and gentlemen”</em> to the German <em class="EmphasisTypeItalic ">“Guten Morgen Damen und Herren”</em> are shown on the right side [<span class="CitationRef"><a epub:type="biblioref" href="#CR132" role="doc-biblioref">132</a></span>]. Words are tokenized to word pieces</p></div></figcaption></figure></div><p class="Para" id="Par209">Liu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR82" role="doc-biblioref">82</a></span>] propose a generative explanation framework which simultaneously learns to make classification decisions and generate fine-grained explanations for them. In order to reach a good connection between classification and explanation they introduce a classifier that is trained on their explanation. For product reviews they, for instance, generate the following positive explanations <em class="EmphasisTypeItalic ">“excellent picture, attractive glass-backed screen, hdr10 and dolby vision”</em> and negative reasons <em class="EmphasisTypeItalic ">“very expensive”</em>. The authors introduce an explanation factor, which represents the distance between the probabilities of the classifier trained on the explanations vs. the classifier trained on the original input and the gold labels. They optimize their models with minimum risk training.</p></section>
<section class="Section3 RenderAsSection3" id="Sec41"><h4 class="Heading">Explanation by Retrieval</h4><p class="Para" id="Par210">Recently, Deep Learning models have been playing an increasingly important role in science and technology. The algorithms developed by Facebook are able to predict user preferences better than any psychologist [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR71" role="doc-biblioref">71</a></span>]. AlphaFold, developed by DeepMind, makes the most accurate predictions of protein structures based on their amino acids [<span class="CitationRef"><a epub:type="biblioref" href="#CR131" role="doc-biblioref">131</a></span>]. And the PaLM and Retro models are capable of generating stories in fluent English, the latter with the knowledge of the Internet as background. However, none of the programs were actually able to justify their decisions and cannot indicate why a particular sequence was generated or on what information a decision was based on.</p><p class="Para" id="Par211">In 2008, Anderson [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>] predicted the end of theory-based science. In his view, theories are an oversimplification of reality, and the vast amount of accumulated data contains knowledge in a much more detailed form, so theories are no longer necessary. This is also the problem of <em class="EmphasisTypeItalic ">Explainable AI</em><span id="ITerm161"/>, which aims to explain the decisions of Deep Learning models. It is always faced with a trade-off where predictive accuracy must be sacrificed in order to interpret the model output.</p><p class="Para" id="Par212">As large autoregressive language models are combined with retrieval components, document retrieval can be used not only to incorporate more accurate knowledge into the language generation process, but also to support the generated answers by authoritative citations. Metzler et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>] argues that future PLMs should justify created text by referring to supporting documents in the training data or background document collection. To implement this approach Nakano et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR100" role="doc-biblioref">100</a></span>] combine <em class="EmphasisTypeItalic ">GPT-3</em><span id="ITerm162"/> with the search engine <em class="EmphasisTypeItalic ">BING</em><span id="ITerm163"/> to enhance language generation for question-answering by retrieved documents. Their <strong class="EmphasisTypeBold ">WebGPT</strong><span id="ITerm164"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR100" role="doc-biblioref">100</a></span>] first creates a text in natural language (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec16"><span class="RefSource">6.​2.​3</span></a></span>). After that, it enhances the generated sentences by different references to the found documents, similar to the way a scientist expands his texts by references. By this procedure WebGPT is able to justify and explain the created answer. This could be a way to make the generated text more trustworthy. Note that the advanced dialog model <strong class="EmphasisTypeBold ">LaMDA</strong><span id="ITerm165"/> can include links to external documents supporting an answer (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec52"><span class="RefSource">6.​6.​3</span></a></span>).</p></section>
<section class="Section3 RenderAsSection3" id="Sec42"><h4 class="Heading">Explanation by Generating a Chain of Thought</h4><p class="Para" id="Par213">Large autoregressive PLMs like GPT-3 are able to produce a very convincing continuation of a start text, and, for instance, generate the answer for a question. It turned out that their ability to generate the correct answer could drastically be improved by giving a few examples with a chain of thought (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec42"><span class="RefSource">3.​6.​4</span></a></span>) for deriving the correct answer. This has been demonstrated for the PaLM language model [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>].</p><p class="Para" id="Par214">A generated <em class="EmphasisTypeItalic ">thought chain</em><span id="ITerm166"/> can be used for other purposes. First, it can be checked whether the model produces the correct answer for the “right reasons”, rather than just exploiting superficial statistical correlations. In addition, the explanation can potentially be shown to an end-user of the system to increase or decrease their confidence in a given prediction. Finally, for some queries (e.g., explaining a joke), the explanation itself is the desired output [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>].</p><div class="Para" id="Par215">Figure <span class="InternalRef"><a href="#Fig21">2.21</a></span> contains a few-shot query and the resulting answer. For application only a few example chains of thought are necessary, which can be reused. To generate the best answer for the question greedy decoding has to be used, yielding the optimal prediction. As PaLM shows, the enumeration of argument steps works empirically. However, a sound theory of how models actually use such arguments internally is still lacking. Further, it is not known under which circumstances the derivation of such a chain of thoughts succeeds. It should be investigated to what extent the reasoning of a model corresponds to the reasoning steps performed by humans.<figure class="Figure" id="Fig21"><div class="MediaObject" id="MO21"><img alt="" aria-describedby="d64e10729" src="../images/528393_1_En_2_Chapter/528393_1_En_2_Fig21_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e10729"><p class="Para" id="Par242">A screenshot of 2 boxes with titles and descriptions. The first box includes example chain of thoughts and input query. The second one has the model output.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.21</span><p class="SimplePara">Explaining by a chain of thoughts. The first box contains two examples of thought chains, which are used for every query. This chain-of-thought prompt was input to the PaLM model together with the input query, and the model output was generated by PaLM [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>, p. 38]</p></div></figcaption></figure></div><div class="FormalPara FormalParaRenderingStyle3"><div class="Heading">Implementations</div><p class="Para FirstParaInFormalPara" id="Par216">Ecco [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>] and BertViz [<span class="CitationRef"><a epub:type="biblioref" href="#CR143" role="doc-biblioref">143</a></span>] are tools to visualize the attentions and embeddings of PLMs. An implementation and a tutorial on integrated gradients is available for TensorFlow [<span class="CitationRef"><a epub:type="biblioref" href="#CR136" role="doc-biblioref">136</a></span>]. Captum [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>] is an open-source library to generate interpretations and explanations for the predictions of PyTorch models containing most of the approaches discussed above. Transformers-interpret [<span class="CitationRef"><a epub:type="biblioref" href="#CR113" role="doc-biblioref">113</a></span>] is an alternative open-source model explainability tool for the Hugging Face package.</p></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec43"><h3 class="Heading"><span class="HeadingNumber">2.4.6 </span>Summary</h3><p class="Para" id="Par217">Similar to other large neural networks, PLMs are optimized with simple stochastic gradient descent optimizers that are able to approach the region of minimal cost even for huge models with billions of parameters and terabytes of training data. This requires parallel training on computing networks which can be controlled by suitable software libraries. There are many recipes in the literature for setting hyperparameters such as batch size and learning rate schedules. Important ingredients are residual connections to be able to optimize networks with many layers and regularization modules to keep parameters in a manageable range.</p><p class="Para" id="Par218">Neural architecture search is a way to improve performance and reduce memory requirements of networks. A number of approaches have been proposed that significantly speed up training. Some methods provide models with better performance and lower memory footprint. There are new differential methods that have the potential to derive better architectures with little effort.</p><p class="Para" id="Par219">PLMs aim to capture relations between language concepts and can only do so approximately. Therefore, it is important to evaluate their inherent uncertainty. Three different approaches to analyze the uncertainty are described. Among these, ensemble methods appear to be the most reliable, but involve a high computational cost. New algorithms such as SNGP, which are based on a single model, are very promising.</p><p class="Para" id="Par220">To enable a user to decide whether a model result makes sense, it is necessary to explain how the result was obtained. Explanations can be provided by showing the importance of features for a result, by exploring the PLM by related examples or by approximating the PLM with a simple model. Some libraries are available that allow routine use of these methods. A new way of explaining texts generated by PLMs is to enhance the texts with appropriate citations of relevant supporting documents. Finally, a PLM can be instructed by chain-of-thought prompts to provide an explanation for the model response. This type of explanation is particularly easy to understand and can reflect the essential parts of a chain of arguments.</p><p class="Para" id="Par221">The next chapter discusses approaches to improve the three basic PLM types by new pre-training tasks or architectural changes. The fourth chapter examines the knowledge, which can be acquired by PLMs and that can be used to interpret text and to generate new texts.</p></section>
</section>
<div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a><p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p><p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p></div><aside aria-labelledby="Bib1Heading" class="Bibliography" id="Bib1"><div epub:type="bibliography" role="doc-bibliography"><div class="Heading" id="Bib1Heading">References</div><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">A. Abujabal, R. S. Roy, M. Yahya, and G. Weikum. “Quint: Interpretable Question Answering over Knowledge Bases”. In: <em class="EmphasisTypeItalic ">Proc. 2017 Conf. Empir. Methods Nat. Lang. Process. Syst. Demonstr</em>. 2017, pp. 61–66.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">J. Alammar. “Ecco: An Open Source Library for the Explainability of Transformer Language Models”. In: <em class="EmphasisTypeItalic ">Proc. 59th Annu. Meet. Assoc. Comput. Linguist. 11th Int. Jt. Conf. Nat. Lang. Process. Syst. Demonstr</em>. 2021, pp. 249–257. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/jalammar/ecco"><span class="RefSource">https://​github.​com/​jalammar/​ecco</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">J. Alammar. <em class="EmphasisTypeItalic ">The Illustrated GPT-2 (Visualizing Transformer Language Models)</em>. Oct. 12, 2019. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="http://jalammar.github.io/illustrated-gpt2/"><span class="RefSource">http://​jalammar.​github.​io/​illustrated-gpt2/​</span></a></span> (visited on 01/24/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">F. St-Amant. <em class="EmphasisTypeItalic ">How to Fine-Tune GPT-2 for Text Generation</em>. Medium. May 8, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272"><span class="RefSource">https://​towardsdatascien​ce.​com/​how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272</span></a></span> (visited on 07/29/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">C. Anderson. “The End of Theory: The Data Deluge Makes the Scientific Method Obsolete”. In: <em class="EmphasisTypeItalic ">Wired</em> (June 23, 2008). <span class="EmphasisTypeSmallCaps ">issn</span>: 1059–1028. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.wired.com/2008/06/pb-theory/"><span class="RefSource">https://​www.​wired.​com/​2008/​06/​pb-theory/​</span></a></span> (visited on 01/11/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">J. L. Ba, J. R. Kiros, and G. E. Hinton. “Layer Normalization”. 2016. arXiv: <span class="EmphasisFontCategoryNonProportional ">1607.06450</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. Müller, and W. Samek. “On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation”. In: <em class="EmphasisTypeItalic ">PloS one</em> 10.7 (2015), e0130140.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">D. Bahdanau, K. Cho, and Y. Bengio. “Neural Machine Translation by Jointly Learning to Align and Translate”. 2014. arXiv: 1409.0473.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">D. Barber and C. M. Bishop. “Ensemble Learning in Bayesian Neural Networks”. In: <em class="EmphasisTypeItalic ">Nato ASI Ser. F Comput. Syst. Sci</em>. 168 (1998), pp. 215–238.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">baselines. <em class="EmphasisTypeItalic ">Uncertainty Baselines</em>. Google, Dec. 5, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/google/uncertainty-baselines"><span class="RefSource">https://​github.​com/​google/​uncertainty-baselines</span></a></span> (visited on 12/06/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">C. Bauckhage, J. Fürnkranz, and G. Paass. “Vertrauenswürdiges, Transparentes Und Robustes Maschinelles Lernen”. In: <em class="EmphasisTypeItalic ">Handbuch Der Künstlichen Intelligenz</em>. de Gruyter, 2021. <span class="EmphasisTypeSmallCaps ">isbn</span>: 978-3-11-065984-9.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">V. Belle and I. Papantonis. “Principles and Practice of Explainable Machine Learning”. In: <em class="EmphasisTypeItalic ">Front. Big Data</em> 4 (2021), p. 39. <span class="EmphasisTypeSmallCaps ">issn</span>: 2624-909X. <span class="ExternalRef"><a href="https://doi.org/10.3389/fdata.2021.688969"><span class="RefSource">https://​doi.​org/​10.​3389/​fdata.​2021.​688969</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">W. H. Beluch, T. Genewein, A. Nürnberger, and J. M. Köhler. “The Power of Ensembles for Active Learning in Image Classification”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</em>. 2018, pp. 9368–9377.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">Y. Bengio, A. Courville, and P. Vincent. “Representation Learning: A Review and New Perspectives”. In: <em class="EmphasisTypeItalic ">IEEE Trans. Pattern Anal. Mach. Intell</em>. 35.8 (2013), pp. 1798–1828.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. “A Neural Probabilistic Language Model”. In: <em class="EmphasisTypeItalic ">J. Mach. Learn. Res</em>. 3 (Feb 2003), pp. 1137–1155.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. “Greedy Layer-Wise Training of Deep Networks”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 19 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">R. Bommasani et al. “On the Opportunities and Risks of Foundation Models”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2108.07258</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">S. Borgeaud et al. “Improving Language Models by Retrieving from Trillions of Tokens”. Dec. 8, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2112.04426 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">G. Branwen. “GPT-2 Neural Network Poetry”. In: (Mar. 3, 2019). <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.gwern.net/GPT-2"><span class="RefSource">https://​www.​gwern.​net/​GPT-2</span></a></span> (visited on 01/27/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">L. Breiman. “Bagging Predictors”. In: <em class="EmphasisTypeItalic ">Mach. Learn</em>. 24.2 (1996), pp. 123–140.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">T. B. Brown et al. “Language Models Are Few-Shot Learners”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2005.14165</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">D. Budden and M. Hessel. <em class="EmphasisTypeItalic ">Using JAX to Accelerate Our Research</em>. Dec. 4, 2020. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.deepmind.com/blog/using-jax-to-accelerate-our-research"><span class="RefSource">https://​www.​deepmind.​com/​blog/​using-jax-to-accelerate-our-research</span></a></span> (visited on 06/21/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">N. Burkart and M. F. Huber. “A Survey on the Explainability of Supervised Machine Learning”. In: <em class="EmphasisTypeItalic ">J. Artif. Intell. Res</em>. 70 (2021), pp. 245–317.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">C. Cadwalladr and E. Graham-Harrison. “How Cambridge Analytica Turned Facebook ‘Likes’ into a Lucrative Political Tool”. In: <em class="EmphasisTypeItalic ">Guard. 17032018</em> (2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">X. Cai, J. Huang, Y. Bian, and K. Church. “Isotropy in the Contextual Embedding Space: Clusters and Manifolds”. In: <em class="EmphasisTypeItalic ">Int. Conf. Learn. Represent</em>. 2020.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">Captum. <em class="EmphasisTypeItalic ">Captum</em> ⋅ <em class="EmphasisTypeItalic ">Model Interpretability for PyTorch</em>. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://captum.ai/"><span class="RefSource">https://​captum.​ai/​</span></a></span> (visited on 12/06/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">S. Chaudhari, V. Mithal, G. Polatkan, and R. Ramanath. “An Attentive Survey of Attention Models”. In: <em class="EmphasisTypeItalic ">ACM Trans. Intell. Syst. Technol. TIST</em> 12.5 (2021), pp. 1–32.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">S. F. Chen, D. Beeferman, and R. Rosenfeld. “Evaluation Metrics for Language Models”. In: (1998). <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://kilthub.cmu.edu/articles/EvaluationMetricsForLanguageModels/6605324/files/12095765.pdf"><span class="RefSource">https://​kilthub.​cmu.​edu/​articles/​EvaluationMetric​sForLanguageMode​ls/​6605324/​files/​12095765.​pdf</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">Y. Chen, V. O. Li, K. Cho, and S. R. Bowman. “A Stable and Effective Learning Strategy for Trainable Greedy Decoding”. 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1804.07915</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">A. Chowdhery et al. “PaLM: Scaling Language Modeling with Pathways”. Apr. 5, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2204.02311 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">E. Cohen and C. Beck. “Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn</em>. PMLR, 2019, pp. 1290–1299.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. “Natural Language Processing (Almost) from Scratch”. In: <em class="EmphasisTypeItalic ">J. Mach. Learn. Res</em>. 12 (2011), pp. 2493–2537.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">A. M. Dai and Q. V. Le. “Semi-Supervised Sequence Learning”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2015, pp. 3079–3087.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">Z. Dai, H. Liu, Q. V. Le, and M. Tan. “CoAtNet: Marrying Convolution and Attention for All Data Sizes”. Sept. 15, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2106.04803 [cs].</span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">H. Daneshmand, A. Joudaki, and F. Bach. “Batch Normalization Orthogonalizes Representations in Deep Random Networks”. June 7, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2106.03970 [cs, stat]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">M. Danilevsky, K. Qian, R. Aharonov, Y. Katsis, B. Kawas, and P. Sen. “A Survey of the State of Explainable AI for Natural Language Processing”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.00711</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">A. de Santana Correia and E. L. Colombini. “Attention, Please! A Survey of Neural Attention Models in Deep Learning”. In: <em class="EmphasisTypeItalic ">Artif. Intell. Rev</em>. (2022), pp. 1–88.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. “Annotated BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. In: <em class="EmphasisTypeItalic ">Proc. 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. Vol. 1 Long Short Pap</em>. NAACL-HLT 2019. Minneapolis, Minnesota: Association for Computational Linguistics, June 2019, pp. 4171–4186. <span class="ExternalRef"><a href="https://doi.org/10.18653/v1/N19-1423"><span class="RefSource">https://​doi.​org/​10.​18653/​v1/​N19-1423</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. “Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding”. 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1810.04805</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">X. Dong, Z. Yu, W. Cao, Y. Shi, and Q. Ma. “A Survey on Ensemble Learning”. In: <em class="EmphasisTypeItalic ">Front. Comput. Sci</em>. 14.2 (2020), pp. 241–258.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">K. Doshi. <em class="EmphasisTypeItalic ">Transformers Explained Visually (Part 3): Multi-head Attention, Deep Dive</em>. Medium. June 3, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://towardsdatascience.com/transformers-explained-visuallypart-3-multi-head-attention-deep-dive-1c1ff1024853"><span class="RefSource">https://​towardsdatascien​ce.​com/​transformers-explained-visuallypart-3-multi-head-attention-deep-dive-1c1ff1024853</span></a></span> (visited on 11/19/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">A. Fan, M. Lewis, and Y. Dauphin. “Hierarchical Neural Story Generation”. 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1805.04833</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">Y. Fan, F. Tian, Y. Xia, T. Qin, X.-Y. Li, and T.-Y. Liu. “Searching Better Architectures for Neural Machine Translation”. In: <em class="EmphasisTypeItalic ">IEEEACM Trans. Audio Speech Lang. Process</em>. 28 (2020), pp. 1574–1585.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">Y. Gal and Z. Ghahramani. “Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference”. 2015. arXiv: <span class="EmphasisFontCategoryNonProportional ">1506.02158</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">Y. Gal, J. Hron, and A. Kendall. “Concrete Dropout”. 2017. arXiv: <span class="EmphasisFontCategoryNonProportional ">1705.07832</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">A. Galassi, M. Lippi, and P. Torroni. “Attention in Natural Language Processing”. In: <em class="EmphasisTypeItalic ">IEEE Transactions on Neural Networks and Learning Systems</em> 32 (Oct. 1, 2021), pp. 4291–4308. <span class="ExternalRef"><a href="https://doi.org/10.1109/TNNLS.2020.3019893"><span class="RefSource">https://​doi.​org/​10.​1109/​TNNLS.​2020.​3019893</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">J. Gawlikowski et al. “A Survey of Uncertainty in Deep Neural Networks”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2107.03342</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">T. George, C. Laurent, X. Bouthillier, N. Ballas, and P. Vincent. “Fast Approximate Natural Gradient Descent in a Kronecker-Factored Eigenbasis”. 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1806.03884</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">M. Geva, R. Schuster, J. Berant, and O. Levy. “Transformer Feed-Forward Layers Are Key-Value Memories”. In: (Dec. 29, 2020). <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://arxiv.org/abs/2012.14913v2"><span class="RefSource">https://​arxiv.​org/​abs/​2012.​14913v2</span></a></span> (visited on 11/08/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">B. Ghojogh and A. Ghodsi. “Attention Mechanism, Transformers, BERT, and GPT: Tutorial and Survey”. In: (2020). <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://osf.io/m6gcn/download"><span class="RefSource">https://​osf.​io/​m6gcn/​download</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">I. Goodfellow, Y. Bengio, and A. Courville. <em class="EmphasisTypeItalic ">Deep Learning</em>. Vol. 1. MIT press Cambridge, 2016. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.deeplearningbook.org/"><span class="RefSource">https://​www.​deeplearningbook​.​org/​</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">A. Graves. “Sequence Transduction with Recurrent Neural Networks”. 2012. arXiv: <span class="EmphasisFontCategoryNonProportional ">1211.3711</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">F. K. Gustafsson, M. Danelljan, and T. B. Schon. “Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision”. In: <em class="EmphasisTypeItalic ">Proc. IEEECVF Conf. Comput. Vis. Pattern Recognit. Workshop</em>. 2020, pp. 318–319.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">K. He, X. Zhang, S. Ren, and J. Sun. “Deep Residual Learning for Image Recognition”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</em>. 2016, pp. 770–778.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">K. He, X. Zhang, S. Ren, and J. Sun. “Delving Deep into Rectifiers: Surpassing Human-Level Performance on Imagenet Classification”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Int. Conf. Comput. Vis</em>. 2015, pp. 1026–1034.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">X. He, K. Zhao, and X. Chu. “AutoML: A Survey of the State-of-the-Art”. In: <em class="EmphasisTypeItalic ">Knowl.-Based Syst</em>. 212 (2021), p. 106622.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">J. Hilton. WebGPT: <em class="EmphasisTypeItalic ">Improving the Factual Accuracy of Language Models through Web Browsing</em>. OpenAI. Dec. 16, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://openai.com/blog/improving-factual-accuracy/"><span class="RefSource">https://​openai.​com/​blog/​improving-factual-accuracy/​</span></a></span> (visited on 01/12/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. “The Curious Case of Neural Text Degeneration”. Feb. 14, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">1904.09751 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">J. Howard and S. Ruder. “Universal Language Model Fine-tuning for Text Classification”. In: <em class="EmphasisTypeItalic ">Proc. 56th Annu. Meet. Assoc. Comput. Linguist. Vol. 1 Long Pap</em>. ACL 2018. Melbourne, Australia: Association for Computational Linguistics, July 2018, pp. 328–339. <span class="ExternalRef"><a href="https://doi.org/10.18653/v1/P18-1031"><span class="RefSource">https://​doi.​org/​10.​18653/​v1/​P18-1031</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">C. Hu et al. “RankNAS: Efficient Neural Architecture Search by Pairwise Ranking”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2109.07383</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">61.</div><div class="CitationContent" id="CR61">D. Hu. “An Introductory Survey on Attention Mechanisms in NLP Problems”. In: <em class="EmphasisTypeItalic ">Proc. SAI Intell. Syst. Conf</em>. Springer, 2019, pp. 432–448.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">62.</div><div class="CitationContent" id="CR62">S. Ioffe and C. Szegedy. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn</em>. PMLR, 2015, pp. 448–456.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">63.</div><div class="CitationContent" id="CR63">S. Jain and B. C. Wallace. “Attention Is Not Explanation”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1902.10186</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">64.</div><div class="CitationContent" id="CR64">Y. Jiang, C. Hu, T. Xiao, C. Zhang, and J. Zhu. “Improved Differentiable Architecture Search for Language Modeling and Named Entity Recognition”. In: <em class="EmphasisTypeItalic ">Proc. 2019 Conf. Empir. Methods Nat. Lang. Process. 9th Int. Jt. Conf. Nat. Lang. Process. EMNLP-IJCNLP</em>. 2019, pp. 3576–3581.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">65.</div><div class="CitationContent" id="CR65">M. Kastrati and M. Biba. “A State-of-the-Art Survey of Advanced Optimization Methods in Machine Learning”. In: <em class="EmphasisTypeItalic ">RTA-CSIT</em> (May 1, 2021), pp. 1–10.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">66.</div><div class="CitationContent" id="CR66">R. Kehlbeck, R. Sevastjanova, T. Spinner, T. Stähle, and M. El-Assady. <em class="EmphasisTypeItalic ">Demystifying the Embedding Space of Language Models</em>. July 31, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://bert-vs-gpt2.dbvis.de/"><span class="RefSource">https://​bert-vs-gpt2.​dbvis.​de/​</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">67.</div><div class="CitationContent" id="CR67">N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher. “CTRL: A Conditional Transformer Language Model for Controllable Generation”. Sept. 20, 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1909.05858</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">68.</div><div class="CitationContent" id="CR68">U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. “Generalization through Memorization: Nearest Neighbor Language Models”. Feb. 14, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">1911.00172</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">69.</div><div class="CitationContent" id="CR69">D. P. Kingma and J. Ba. “Adam: A Method for Stochastic Optimization”. 2014. arXiv: <span class="EmphasisFontCategoryNonProportional ">1412.6980</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">70.</div><div class="CitationContent" id="CR70">N. Kokhlikyan et al. “Captum: A Unified and Generic Model Interpretability Library for PyTorch”. Sept. 16, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2009.07896</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">71.</div><div class="CitationContent" id="CR71">M. Kosinski, D. Stillwell, and T. Graepel. “Private Traits and Attributes Are Predictable from Digital Records of Human Behavior”. In: <em class="EmphasisTypeItalic ">Proc. Natl. Acad. Sci</em>. 110.15 (2013), pp. 5802–5805.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">72.</div><div class="CitationContent" id="CR72">A. Krizhevsky, I. Sutskever, and G. E. Hinton. “Imagenet Classification with Deep Convolutional Neural Networks”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2012, pp. 1097–1105.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">73.</div><div class="CitationContent" id="CR73">B. Lakshminarayanan, A. Pritzel, and C. Blundell. “Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 30 (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">74.</div><div class="CitationContent" id="CR74">S. Lapuschkin, A. Binder, G. Montavon, K.-R. Muller, and W. Samek. “Analyzing Classifiers: Fisher Vectors and Deep Neural Networks”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</em>. 2016, pp. 2912–2920.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">75.</div><div class="CitationContent" id="CR75">A. Lavie and A. Agarwal. “METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments”. In: <em class="EmphasisTypeItalic ">Proc. Second Workshop Stat. Mach. Transl</em>. 2007, pp. 228–231.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">76.</div><div class="CitationContent" id="CR76">J. Lee, M. Humt, J. Feng, and R. Triebel. “Estimating Model Uncertainty of Neural Networks in Sparse Information Form”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn. PMLR</em>, 2020, pp. 5702–5713.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">77.</div><div class="CitationContent" id="CR77">S. Lee, S. Purushwalkam, M. Cogswell, D. Crandall, and D. Batra. “Why M Heads Are Better than One: Training a Diverse Ensemble of Deep Networks”. 2015. arXiv: <span class="EmphasisFontCategoryNonProportional ">1511.06314</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">78.</div><div class="CitationContent" id="CR78">M. Lewis. <em class="EmphasisTypeItalic ">Decoding Language Models</em> ⋅ <em class="EmphasisTypeItalic ">Deep Learning. Apr</em>. 20, 2020. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-2/"><span class="RefSource">https://​atcold.​github.​io/​pytorch-Deep-Learning/​en/​week12/​12-2/​</span></a></span> (visited on 07/30/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">79.</div><div class="CitationContent" id="CR79">J. Li, X. Chen, E. Hovy, and D. Jurafsky. “Visualizing and Understanding Neural Models in Nlp”. 2015. arXiv: <span class="EmphasisFontCategoryNonProportional ">1506.01066</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">80.</div><div class="CitationContent" id="CR80">C.-Y. Lin. “Rouge: A Package for Automatic Evaluation of Summaries”. In: <em class="EmphasisTypeItalic ">Text Summ. Branches Out</em>. 2004, pp. 74–81.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">81.</div><div class="CitationContent" id="CR81">T. Lin, Y. Wang, X. Liu, and X. Qiu. “A Survey of Transformers”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2106.04554</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">82.</div><div class="CitationContent" id="CR82">H. Liu, Q. Yin, and W. Y. Wang. “Towards Explainable NLP: A Generative Explanation Framework for Text Classification”. June 11, 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1811.00196</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">83.</div><div class="CitationContent" id="CR83">J. Z. Liu, Z. Lin, S. Padhy, D. Tran, T. Bedrax-Weiss, and B. Lakshminarayanan. “Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness”. Oct. 25, 2020. arXiv: 2006.10108.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">84.</div><div class="CitationContent" id="CR84">S. M. Lundberg and S.-I. Lee. “A Unified Approach to Interpreting Model Predictions”. In: <em class="EmphasisTypeItalic ">Proc. 31st Int. Conf. Neural Inf. Process. Syst</em>. 2017, pp. 4768–4777.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">85.</div><div class="CitationContent" id="CR85">A. Malinin and M. Gales. “Reverse Kl-Divergence Training of Prior Networks: Improved Uncertainty and Adversarial Robustness”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1905.13472</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">86.</div><div class="CitationContent" id="CR86">P. H. Martins, Z. Marinho, and A. F. Martins. “Sparse Text Generation”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2004.02644</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">87.</div><div class="CitationContent" id="CR87">B. McCann, J. Bradbury, C. Xiong, and R. Socher. “Learned in Translation: Contextualized Word Vectors”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2017, pp. 6294–6305.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">88.</div><div class="CitationContent" id="CR88">P. McClure and N. Kriegeskorte. “Robustly Representing Uncertainty through Sampling in Deep Neural Networks”. 2016. arXiv: <span class="EmphasisFontCategoryNonProportional ">1611.01639</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">89.</div><div class="CitationContent" id="CR89">L. McInnes, J. Healy, and J. Melville. “Umap: Uniform Manifold Approximation and Projection for Dimension Reduction”. 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1802.03426</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">90.</div><div class="CitationContent" id="CR90">C. Meister, T. Vieira, and R. Cotterell. “If Beam Search Is the Answer, What Was the Question?” Jan. 17, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.02650 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">91.</div><div class="CitationContent" id="CR91">P. Mertikopoulos, N. Hallak, A. Kavis, and V. Cevher. “On the Almost Sure Convergence of Stochastic Gradient Descent in Non-Convex Problems”. June 19, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2006.11144</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">92.</div><div class="CitationContent" id="CR92">D. Metzler, Y. Tay, D. Bahri, and M. Najork. “Rethinking Search: Making Experts out of Dilettantes”. May 5, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2105.02274 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">93.</div><div class="CitationContent" id="CR93">T. Mikolov, K. Chen, G. Corrado, and J. Dean. “Efficient Estimation of Word Representations in Vector Space”. 2013. arXiv: <span class="EmphasisFontCategoryNonProportional ">1301.3781</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">94.</div><div class="CitationContent" id="CR94">G. A. Miller. “WordNet: A Lexical Database for English”. In: <em class="EmphasisTypeItalic ">Commun. ACM</em> 38.11 (1995), pp. 39–41.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">95.</div><div class="CitationContent" id="CR95">C. Molnar. <em class="EmphasisTypeItalic ">Interpretable Machine Learning</em>. Jan. 21, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://christophm.github.io/interpretable-ml-book/"><span class="RefSource">https://​christophm.​github.​io/​interpretable-ml-book/​</span></a></span> (visited on 01/26/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">96.</div><div class="CitationContent" id="CR96">R. Moradi, R. Berangi, and B. Minaei. “A Survey of Regularization Strategies for Deep Models”. In: <em class="EmphasisTypeItalic ">Artif. Intell. Rev</em>. 53.6 (2020), pp. 3947–3986.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">97.</div><div class="CitationContent" id="CR97">S. Morgan. <em class="EmphasisTypeItalic ">Tensorflow/Addons</em>. tensorflow, Dec. 1, 2020. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/tensorflow/addons/blob/0c0fd8dfb4427df6b824c88f700ba5c7efd43bec/tensorflowaddons/optimizers/lamb.py"><span class="RefSource">https://​github.​com/​tensorflow/​addons/​blob/​0c0fd8dfb4427df6​b824c88f700ba5c7​efd43bec/​tensorflowaddons​/​optimizers/​lamb.​py</span></a></span> (visited on 11/08/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">98.</div><div class="CitationContent" id="CR98">Z. Nado. <em class="EmphasisTypeItalic ">Baselines for Uncertainty and Robustness in Deep Learning</em>. Google AI Blog. Oct. 14, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="http://ai.googleblog.com/2021/10/baselines-for-uncertainty-and.html"><span class="RefSource">http://​ai.​googleblog.​com/​2021/​10/​baselines-for-uncertainty-and.​html</span></a></span> (visited on 10/25/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">99.</div><div class="CitationContent" id="CR99">Z. Nado et al. “Uncertainty Baselines: Benchmarks for Uncertainty &amp; Robustness in Deep Learning”. June 7, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2106.04015</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">100.</div><div class="CitationContent" id="CR100">R. Nakano et al. “WebGPT: Browser-assisted Question-Answering with Human Feedback”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2112.09332</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">101.</div><div class="CitationContent" id="CR101">S. Narang et al. “Do Transformer Modifications Transfer Across Implementations and Applications?” Sept. 10, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2102.11972 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">102.</div><div class="CitationContent" id="CR102">R. M. Neal. <em class="EmphasisTypeItalic ">Bayesian Training of Backpropagation Networks by the Hybrid Monte Carlo Method</em>. Technical Report CRG-TR-92-1, Dept. of Computer Science, University of Toronto. Citeseer, 1992.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">103.</div><div class="CitationContent" id="CR103">C. Nemeth and P. Fearnhead. “Stochastic Gradient Markov Chain Monte Carlo”. In: <em class="EmphasisTypeItalic ">J. Am. Stat. Assoc</em>. 116.533 (2021), pp. 433–450.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">104.</div><div class="CitationContent" id="CR104">Z. Niu, G. Zhong, and H. Yu. “A Review on the Attention Mechanism of Deep Learning”. In: <em class="EmphasisTypeItalic ">Neurocomputing</em> 452 (2021), pp. 48–62.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">105.</div><div class="CitationContent" id="CR105">K. Osawa, S. Swaroop, A. Jain, R. Eschenhagen, R. E. Turner, R. Yokota, and M. E. Khan. “Practical Deep Learning with Bayesian Principles”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1906.02506</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">106.</div><div class="CitationContent" id="CR106">Y. Ovadia et al. “Can You Trust Your Model’s Uncertainty? Evaluating Predictive Uncertainty under Dataset Shift”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1906.02530</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">107.</div><div class="CitationContent" id="CR107">G. Paass. “Assessing and Improving Neural Network Predictions by the Bootstrap Algorithm”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. Citeseer, 1993, pp. 196–203.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">108.</div><div class="CitationContent" id="CR108">G. Paass and J. Kindermann. “Bayesian Classification Trees with Overlapping Leaves Applied to Credit-Scoring”. In: <em class="EmphasisTypeItalic ">Res. Dev. Knowl. Discov. Data Min</em>. Ed. by X. Wu, R. Kotagiri, and K. B. Korb. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer, 1998, pp. 234–245. <span class="EmphasisTypeSmallCaps ">isbn</span>: 978-3-540-69768-8. <span class="ExternalRef"><a href="https://doi.org/10.1007/3-540-64383-4_20"><span class="RefSource">https://​doi.​org/​10.​1007/​3-540-64383-4_​20</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">109.</div><div class="CitationContent" id="CR109">Paperswithcode. <em class="EmphasisTypeItalic ">Browse State-of-the-Art in AI</em>. 2019. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://paperswithcode.com/sota"><span class="RefSource">https://​paperswithcode.​com/​sota</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">110.</div><div class="CitationContent" id="CR110">K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. “Bleu: A Method for Automatic Evaluation of Machine Translation”. In: <em class="EmphasisTypeItalic ">Proc. 40th Annu. Meet. Assoc. Comput. Linguist</em>. 2002, pp. 311–318.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">111.</div><div class="CitationContent" id="CR111">K. Pearson. “On Lines and Planes of Closest Fit to Systems of Points in Space”. In: <em class="EmphasisTypeItalic ">Lond. Edinb. Dublin Philos. Mag. J. Sci</em>. 2.11 (1901), pp. 559–572.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">112.</div><div class="CitationContent" id="CR112">J. Pérez, J. Marinkoviæ, and P. Barceló. “On the Turing Completeness of Modern Neural Network Architectures”. 2019. arXiv: 1901.03429.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">113.</div><div class="CitationContent" id="CR113">C. Pierse. <em class="EmphasisTypeItalic ">Transformers Interpret</em>. Version 0.5.2. Feb. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/cdpierse/transformers-interpret"><span class="RefSource">https://​github.​com/​cdpierse/​transformers-interpret</span></a></span> (visited on 11/23/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">114.</div><div class="CitationContent" id="CR114">Pytorch. <em class="EmphasisTypeItalic ">PyTorch.</em> 2019. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://pytorch.org/"><span class="RefSource">https://​pytorch.​org/​</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">115.</div><div class="CitationContent" id="CR115">M. Qudar and V. Mago. <em class="EmphasisTypeItalic ">A Survey on Language Models</em>. Sept. 7, 2020. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.researchgate.net/publication/344158120ASurveyonLanguage_Models/"><span class="RefSource">https://​www.​researchgate.​net/​publication/​344158120ASurvey​onLanguage_​Models/​</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">116.</div><div class="CitationContent" id="CR116">A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. “Improving Language Understanding by Generative Pre-Training”. In: (2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">117.</div><div class="CitationContent" id="CR117">A. Radford, J. Wu, D. Amodei, D. Amodei, J. Clark, M. Brundage, and I. Sutskever. “Better Language Models and Their Implications”. In: <em class="EmphasisTypeItalic ">OpenAI Blog</em> (2019). <span class="EmphasisTypeSmallCaps ">url</span>: https://​openai.​%20com/blog/better-language-models.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">118.</div><div class="CitationContent" id="CR118">A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. “Language Models Are Unsupervised Multitask Learners”. In: <em class="EmphasisTypeItalic ">OpenAI blog</em> 1.8 (2019), p. 9.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">119.</div><div class="CitationContent" id="CR119">S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. “ZeRO: Memory Optimizations Toward Training Trillion Parameter Models”. May 13, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">1910.02054v3</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">120.</div><div class="CitationContent" id="CR120">P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. “Squad: 100,000+ Questions for Machine Comprehension of Text”. 2016. arXiv: <span class="EmphasisFontCategoryNonProportional ">1606.05250</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">121.</div><div class="CitationContent" id="CR121">A. Ramesh, M. Pavlov, G. Goh, and S. Gray. {DALL⋅E}: <em class="EmphasisTypeItalic ">Creating Images from Text</em>. Jan. 5, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://openai.com/blog/dall-e/"><span class="RefSource">https://​openai.​com/​blog/​dall-e/​</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">122.</div><div class="CitationContent" id="CR122">J. Rasley. <em class="EmphasisTypeItalic ">DeepSpeed</em>. Microsoft, Dec. 20, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/microsoft/DeepSpeed"><span class="RefSource">https://​github.​com/​microsoft/​DeepSpeed</span></a></span> (visited on 12/20/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">123.</div><div class="CitationContent" id="CR123">M. T. Ribeiro, S. Singh, and C. Guestrin. “Model-Agnostic Interpretability of Machine Learning”. 2016. arXiv: <span class="EmphasisFontCategoryNonProportional ">1606.05386</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">124.</div><div class="CitationContent" id="CR124">A. Rogers, O. Kovaleva, and A. Rumshisky. “A Primer in {Bertology}: What We Know about How {BERT} Works”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 8 (2021), pp. 842–866.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">125.</div><div class="CitationContent" id="CR125">S. Rönnqvist, J. Kanerva, T. Salakoski, and F. Ginter. “Is Multilingual BERT Fluent in Language Generation?” 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1910.03806</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">126.</div><div class="CitationContent" id="CR126">A. Rush. “The Annotated Transformer”. In: <em class="EmphasisTypeItalic ">Proc. Workshop NLP Open Source Softw. NLP-OSS</em> Melbourne, Australia: Association for Computational Linguistics, July 2018, pp. 52–60. <span class="ExternalRef"><a href="https://doi.org/10.18653/v1/W18-2509"><span class="RefSource">https://​doi.​org/​10.​18653/​v1/​W18-2509</span></a></span>.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.18653/v1/W18-2509"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">127.</div><div class="CitationContent" id="CR127">A. B. Sai, A. K. Mohankumar, and M. M. Khapra. “A Survey of Evaluation Metrics Used for NLG Systems”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2008.12009</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">128.</div><div class="CitationContent" id="CR128">E. F. Sang and F. De Meulder. “Introduction to the CoNLL-2003 Shared Task: Languageindependent Named Entity Recognition”. 2003. arXiv: <span class="EmphasisFontCategoryNonProportional ">cs/0306050</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">129.</div><div class="CitationContent" id="CR129">S. Serrano and N. A. Smith. “Is Attention Interpretable?” 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1906.03731</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">130.</div><div class="CitationContent" id="CR130">D. So, Q. Le, and C. Liang. “The Evolved Transformer”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn. PMLR</em>, 2019, pp. 5877–5886.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">131.</div><div class="CitationContent" id="CR131">L. Spinney. “Are We Witnessing the Dawn of Post-Theory Science?” In: <em class="EmphasisTypeItalic ">The Guardian. Technology</em> (Jan. 9, 2022). <span class="EmphasisTypeSmallCaps ">issn</span>: 0261-3077. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.theguardian.com/technology/2022/jan/09/are-we-witnessing-the-dawn-of-post-theory-science"><span class="RefSource">https://​www.​theguardian.​com/​technology/​2022/​jan/​09/​are-we-witnessing-the-dawn-of-post-theory-science</span></a></span> (visited on 01/11/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">132.</div><div class="CitationContent" id="CR132">M. Sundararajan, A. Taly, and Q. Yan. “Axiomatic Attribution for Deep Networks”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn</em>. PMLR, 2017, pp. 3319–3328.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">133.</div><div class="CitationContent" id="CR133">C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. “Rethinking the Inception Architecture for Computer Vision”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</em>. 2016, pp. 2818–2826.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">134.</div><div class="CitationContent" id="CR134">Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and C. Zheng. “Synthesizer: Rethinking Self-Attention in Transformer Models”. May 24, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2005.00743 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">135.</div><div class="CitationContent" id="CR135">A. Taylor, M. Marcus, and B. Santorini. “The Penn Treebank: An Overview”. In: <em class="EmphasisTypeItalic ">Treebanks</em> (2003), pp. 5–22.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">136.</div><div class="CitationContent" id="CR136">Tensorflow. <em class="EmphasisTypeItalic ">Integrated Gradients — TensorFlow Core. TensorFlow</em>. Nov. 25, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.tensorflow.org/tutorials/interpretability/integratedgradients"><span class="RefSource">https://​www.​tensorflow.​org/​tutorials/​interpretability​/​integratedgradie​nts</span></a></span> (visited on 12/06/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">137.</div><div class="CitationContent" id="CR137">Tensorflow. <em class="EmphasisTypeItalic ">Tensorflow Webseite</em>. 2019. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.tensorflow.org/"><span class="RefSource">https://​www.​tensorflow.​org/​</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">138.</div><div class="CitationContent" id="CR138">tensorflow. <em class="EmphasisTypeItalic ">Uncertainty-Aware Deep Learning with SNGP — TensorFlow Core</em>. Tensor-Flow. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.tensorflow.org/tutorials/understanding/sngp"><span class="RefSource">https://​www.​tensorflow.​org/​tutorials/​understanding/​sngp</span></a></span> (visited on 07/25/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">139.</div><div class="CitationContent" id="CR139">E. Tjoa and C. Guan. “A Survey on Explainable Artificial Intelligence (Xai): Toward Medical Xai”. In: <em class="EmphasisTypeItalic ">IEEE Trans. Neural Netw. Learn. Syst</em>. (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">140.</div><div class="CitationContent" id="CR140">L. van der Maaten and G. Hinton. “Visualizing Data Using T-SNE”. In: <em class="EmphasisTypeItalic ">J. Mach. Learn. Res</em>. 9 (Nov 2008), pp. 2579–2605.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">141.</div><div class="CitationContent" id="CR141">A. Vaswani et al. “Attention Is All You Need”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2017, pp. 5998–6008.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">142.</div><div class="CitationContent" id="CR142">J. Vig. “A Multiscale Visualization of Attention in the Transformer Model”. 2019. arXiv: <em class="EmphasisTypeItalic ">1906.05714</em>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">143.</div><div class="CitationContent" id="CR143">J. Vig. <em class="EmphasisTypeItalic ">BertViz</em>. Nov. 23, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/jessevig/bertviz"><span class="RefSource">https://​github.​com/​jessevig/​bertviz</span></a></span> (visited on 11/23/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">144.</div><div class="CitationContent" id="CR144">J. Vig. <em class="EmphasisTypeItalic ">BERTVIZ: A Tool for Visualizing Multihead Self-Attention in the BERT Model</em>. 2019. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://debug-ml-iclr2019.github.io/cameraready/DebugML-19paper2.pdf"><span class="RefSource">https://​debug-ml-iclr2019.​github.​io/​cameraready/​DebugML-19paper2.​pdf</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">145.</div><div class="CitationContent" id="CR145">Wang. <em class="EmphasisTypeItalic ">SuperGLUE Benchmark</em>. SuperGLUE Benchmark. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://super.gluebenchmark.com/"><span class="RefSource">https://​super.​gluebenchmark.​com/​</span></a></span> (visited on 02/23/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">146.</div><div class="CitationContent" id="CR146">A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. “Glue: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding”. Feb. 22, 2019. arXiv: <em class="EmphasisTypeItalic ">1804.07461</em>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">147.</div><div class="CitationContent" id="CR147">D. Wang, C. Gong, M. Li, Q. Liu, and V. Chandra. “AlphaNet: Improved Training of Supernet with Alpha-Divergence”. 2021. arXiv: <em class="EmphasisTypeItalic ">2102.07954</em>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">148.</div><div class="CitationContent" id="CR148">D. Wang, M. Li, C. Gong, and V. Chandra. “Attentivenas: Improving Neural Architecture Search via Attentive Sampling”. In: <em class="EmphasisTypeItalic ">Proc. IEEECVF Conf. Comput. Vis. Pattern Recognit</em>. 2021, pp. 6418–6427.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">149.</div><div class="CitationContent" id="CR149">H. Wang, Z. Wu, Z. Liu, H. Cai, L. Zhu, C. Gan, and S. Han. “Hat: Hardware-aware Transformers for Efficient Natural Language Processing”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2005.14187</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">150.</div><div class="CitationContent" id="CR150">M. Welling and Y. W. Teh. “Bayesian Learning via Stochastic Gradient Langevin Dynamics”. In: <em class="EmphasisTypeItalic ">Proc. 28th Int. Conf. Mach. Learn. ICML-11</em>. 2011, pp. 681–688.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">151.</div><div class="CitationContent" id="CR151">L. Weng. <em class="EmphasisTypeItalic ">Attention? Attention!</em> Lil’Log. June 24, 2018. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://lilianweng.github.io/2018/06/24/attention-attention.html"><span class="RefSource">https://​lilianweng.​github.​io/​2018/​06/​24/​attention-attention.​html</span></a></span> (visited on 11/19/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">152.</div><div class="CitationContent" id="CR152">F. Wenzel et al. “How Good Is the Bayes Posterior in Deep Neural Networks Really?” 2020. arXiv: 2002.02405.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">153.</div><div class="CitationContent" id="CR153">G. Wiedemann, S. Remus, A. Chawla, and C. Biemann. “Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1909.10430</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">154.</div><div class="CitationContent" id="CR154">Y. Wu et al. “Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation”. 2016. arXiv: <span class="EmphasisFontCategoryNonProportional ">1609.08144</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">155.</div><div class="CitationContent" id="CR155">F. Xu, H. Uszkoreit, Y. Du, W. Fan, D. Zhao, and J. Zhu. “Explainable AI: A Brief Survey on History, Research Areas, Approaches and Challenges”. In: <em class="EmphasisTypeItalic ">CCF Int. Conf. Nat. Lang. Process. Chin. Comput. Springer</em>, 2019, pp. 563–574.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">156.</div><div class="CitationContent" id="CR156">Y. Xu et al. “GSPMD: General and Scalable Parallelization for ML Computation Graphs”. Dec. 23, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2105.04663 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">157.</div><div class="CitationContent" id="CR157">Z. Yang, Z. Dai, R. Salakhutdinov, and W. W. Cohen. “Breaking the Softmax Bottleneck: A High-Rank RNN Language Model”. 2017. arXiv: <span class="EmphasisFontCategoryNonProportional ">1711.03953</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">158.</div><div class="CitationContent" id="CR158">Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le. “Xlnet: Generalized Autoregressive Pretraining for Language Understanding”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2019, pp. 5753–5763.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">159.</div><div class="CitationContent" id="CR159">Y. You et al. “Large Batch Optimization for Deep Learning: Training Bert in 76 Minutes”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1904.00962</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">160.</div><div class="CitationContent" id="CR160">C. Yun, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. “Are Transformers Universal Approximators of Sequence-to-Sequence Functions?” 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1912.10077</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">161.</div><div class="CitationContent" id="CR161">C. Yun, Y.-W. Chang, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. “<em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">n</em>) Connections Are Expressive Enough: Universal Approximability of Sparse Transformers”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2006.04862</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">162.</div><div class="CitationContent" id="CR162">B. Zhang and R. Sennrich. “Root Mean Square Layer Normalization”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1910.07467</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">163.</div><div class="CitationContent" id="CR163">C. Zhang et al. “Resnet or Densenet? Introducing Dense Shortcuts to Resnet”. In: <em class="EmphasisTypeItalic ">Proc. IEEECVF Winter Conf. Appl. Comput. Vis</em>. 2021, pp. 3550–3559.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">164.</div><div class="CitationContent" id="CR164">T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi. “BERTScore: Evaluating Text Generation with BERT”. Feb. 24, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">1904.09675</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">165.</div><div class="CitationContent" id="CR165">W. Zhu, X. Wang, X. Qiu, Y. Ni, and G. Xie. “AutoRC: Improving BERT Based Relation Classification Models via Architecture Search”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2009.10680</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">166.</div><div class="CitationContent" id="CR166">M.-A. Zöller and M. F. Huber. “Benchmark and Survey of Automated Machine Learning Frameworks”. In: <em class="EmphasisTypeItalic ">J. Artif. Intell. Res</em>. 70 (2021), pp. 409–472.</div></li></ol></div></aside></div></div></body></html>