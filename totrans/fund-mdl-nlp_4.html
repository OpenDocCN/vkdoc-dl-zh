<html><head></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="b978-3-031-23190-2_5"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">G. Paaß, S. Giesselbach</span><span class="ContextInformationBookTitles"><span class="BookTitle">Foundation Models for Natural Language Processing</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" lang="en">Artificial Intelligence: Foundations, Theory, and Algorithms</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-3-031-23190-2_5">https://doi.org/10.1007/978-3-031-23190-2_5</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">5. Foundation Models for Information Extraction</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Gerhard Paaß</span><sup><a href="#Aff5">1</a> <span class="ContactIcon"> </span></sup> and </span><span class="Author"><span class="AuthorName">Sven Giesselbach</span><sup><a href="#Aff5">1</a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff5"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Knowledge Discovery Department, Team NLU, Fraunhofer Institute for Intelligent Analysis and Information Systems (IAIS), Sankt Augustin, Nordrhein-Westfalen, Germany</div></div><div class="ClearBoth"> </div></div></div><section class="Abstract" id="Abs1" lang="en" role="doc-abstract"><h2 class="Heading">Abstract</h2><p class="Para" id="Par1">In the chapter we consider Information Extraction approaches that automatically identify structured information in text documents and comprise a set of tasks. The Text Classification task assigns a document to one or more pre-defined content categories or classes. This includes many subtasks such as language identification, sentiment analysis, etc. The Word Sense Disambiguation task attaches a predefined meaning to each word in a document. The Named Entity Recognition task identifies named entities in a document. An entity is any object or concept mentioned in the text and a named entity is an entity that is referred to by a proper name. The Relation Extraction task aims to identify the relationship between entities extracted from a text. This covers many subtasks such as coreference resolution, entity linking, and event extraction. Most demanding is the joint extraction of entities and relations from a text. Traditionally, relatively small Pre-trained Language Models have been fine-tuned to these task and yield high performance, while larger Foundation Models achieve high scores with few-shot prompts, but usually have not been benchmarked.</p></section><div class="KeywordGroup" lang="en"><div class="Heading">Keywords</div><span class="Keyword" epub:type="keyword">Text classification</span><span class="Keyword" epub:type="keyword">Named entity recognition</span><span class="Keyword" epub:type="keyword">Relation extraction</span><span class="Keyword" epub:type="keyword">Sentiment analysis</span><span class="Keyword" epub:type="keyword">Language understanding</span></div><!--End Abstract--><div class="Fulltext"><div class="Para" id="Par2">There are a large number of NLP applications of Pre-trained Language Models (PLMs), which can be roughly divided into three areas <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par3"><em class="EmphasisTypeItalic ">Information Extraction</em><span id="ITerm1"/> (<em class="EmphasisTypeItalic ">IE</em>) automatically identifies structured information in textual documents and analyzes language features (Chap. <span class="ExternalRef"><a href="528393_1_En_5_Chapter.xhtml"><span class="RefSource">5</span></a></span>).</p></li><li><p class="Para" id="Par4"><em class="EmphasisTypeItalic ">Natural Language Generation</em><span id="ITerm2"/> (<em class="EmphasisTypeItalic ">NLG</em>) automatically generates new natural language text, often in response to some prompt (Chap. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml"><span class="RefSource">6</span></a></span>).</p></li><li><p class="Para" id="Par5"><em class="EmphasisTypeItalic ">Multimodal Content Analysis</em><span id="ITerm3"/> and generation integrates the understanding and production of content across two or more modalities like text, speech, image, video, etc (Chap. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml"><span class="RefSource">7</span></a></span>).</p></li></ul></div> These applications are described in the three following chapters.</div><div class="Para" id="Par6">In the present chapter we focus on <strong class="EmphasisTypeBold ">information extraction</strong> with PLMs. Information extraction includes the following tasks: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><div class="Para" id="Par7"><em class="EmphasisTypeItalic ">Text classification</em><span id="ITerm4"/> assigns a document to one or more pre-defined content <em class="EmphasisTypeItalic ">categories</em><span id="ITerm5"/> or classes (Sect. <span class="InternalRef"><a href="#Sec1">5.1</a></span>). Note that many subtasks can be formulated as classification problems, e.g. language identification, sentiment analysis, etc. (Table <span class="InternalRef"><a href="#Tab1">5.1</a></span>). <div class="Table" id="Tab1"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 5.1</span><p class="SimplePara">Language analysis tasks based on text classification illustrated by examples</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Task</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Description</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Example</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Language identification</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Determine the language of a text, Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec2"><span class="RefSource">1.​2</span></a></span>.</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Shakespeare lived 400 years ago</em><span class="InlineEquation" id="IEq1"><img alt="$$\rightarrow $$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq1.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">English</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Document classification</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Assign a content category (class), e.g. economy, to a document or text, Sect. <span class="InternalRef"><a href="#Sec1">5.1</a></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">The Dow-Jones is up 50 points</em><span class="InlineEquation" id="IEq2"><img alt="$$\rightarrow $$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq2.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">economy</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Sentiment analysis</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Classification of a text according to the sentiment expressed in it (e.g. positive, negative, neutral), Sect. <span class="InternalRef"><a href="#Sec1">5.1</a></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Today I feel really lousy.</em><span class="InlineEquation" id="IEq3"><img alt="$$\rightarrow $$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq3.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">negative</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Hate speech detection</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Recognize if a text contains hate speech, Sect. <span class="InternalRef"><a href="#Sec2">5.1.1</a></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Immigrants infest our country</em><span class="InlineEquation" id="IEq4"><img alt="$$\rightarrow $$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq4.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">hate speech</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Fake news detection</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Detect a text that contains fake news, Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec45"><span class="RefSource">6.​5.​5</span></a></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Measles vaccination causes meningitis.</em><span class="InlineEquation" id="IEq5"><img alt="$$\rightarrow $$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq5.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">fake news</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Logical relation</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Determine whether the second text contains a logical consequence, a contradiction, or a neutral statement relative to the first text, Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec7"><span class="RefSource">2.​1.​5</span></a></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">John has a flat.</em> ↔<sub><em class="EmphasisTypeItalic ">contradiction</em></sub><em class="EmphasisTypeItalic ">John is a homeless person.</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Text entailment</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Does the first text imply the truth of the second text? Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec7"><span class="RefSource">2.​1.​5</span></a></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Exercising improves health.</em> →<sub><em class="EmphasisTypeItalic ">entails</em></sub><em class="EmphasisTypeItalic ">Physical activity has good consequences.</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Paraphrase detection</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Determine if two texts are semantically equivalent, Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec7"><span class="RefSource">2.​1.​5</span></a></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Fred is tired. /Fred wants to sleep.</em><span class="InlineEquation" id="IEq6"><img alt="$$\rightarrow $$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq6.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">equivalent</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Dialog act classification</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Determine the type of an utterance in a dialog (question, statement, request for action, etc.)</p></td><td style="text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Where is the dog?</em><span class="InlineEquation" id="IEq7"><img alt="$$\rightarrow $$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq7.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">question</em></p></td></tr></tbody></table></div></div></li><li><p class="Para" id="Par8"><em class="EmphasisTypeItalic ">Word Sense Disambiguation</em><span id="ITerm6"/> (<em class="EmphasisTypeItalic ">WSD</em>) connects a predefined meaning to each word in a document. This is especially important for the interpretation of <em class="EmphasisTypeItalic ">homonyms</em><span id="ITerm7"/>, i.e. words that have several meanings depending on the context (Sect. <span class="InternalRef"><a href="#Sec7">5.2</a></span>).</p></li><li><p class="Para" id="Par9"><em class="EmphasisTypeItalic ">Named Entity Recognition</em><span id="ITerm8"/> (<em class="EmphasisTypeItalic ">NER</em>) identifies <em class="EmphasisTypeItalic ">named entities</em><span id="ITerm9"/> in a document. An <em class="EmphasisTypeItalic ">entity</em><span id="ITerm10"/> is an any object or concept mentioned in the text. A <em class="EmphasisTypeItalic ">named entity</em><span id="ITerm11"/> is an entity that is referred to by a proper name. NER also associates a type with each entity, e.g. person, location, organization, etc. (Sect. <span class="InternalRef"><a href="#Sec12">5.3</a></span>).</p></li><li><p class="Para" id="Par10"><em class="EmphasisTypeItalic ">Relation Extraction</em><span id="ITerm12"/> aims to identify the relationship between <em class="EmphasisTypeItalic ">entities</em><span id="ITerm13"/> extracted from a text (Sect. <span class="InternalRef"><a href="#Sec19">5.4</a></span>). This covers many subtasks such as coreference resolution, entity linking, and event extraction (Table <span class="InternalRef"><a href="#Tab3">5.3</a></span>).</p></li></ul></div></div><p class="Para" id="Par11">Due to the large number of different approaches, we focus on representative models which exhibit a high performance at the time of writing. Traditionally relatively small PLMs have been fine-tuned to these task and yield high performance, while larger Foundation Models achieve high scores with few-shot prompts, but usually have not been benchmarked.</p><p class="Para" id="Par12">We outline the inner logic and main features of the methods, taking into account necessary resources, e.g. computational and memory requirements. For standard models a link to the description in earlier chapters is provided. Under the heading “Available Implementations” you will find links to available code and pre-trained models for a task. Good general sources for code are the websites [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR74" role="doc-biblioref">74</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR79" role="doc-biblioref">79</a></span>].</p><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading"><span class="HeadingNumber">5.1 </span>Text Classification</h2><p class="Para" id="Par13">Automatic <em class="EmphasisTypeItalic ">text classification</em><span id="ITerm14"/> is a common task in natural language processing where a <em class="EmphasisTypeItalic ">class</em><span id="ITerm15"/>, (also called <em class="EmphasisTypeItalic ">category</em><span id="ITerm16"/> or <em class="EmphasisTypeItalic ">label</em><span id="ITerm17"/>) is assigned to a short text or a document. The set of classes is predefined and may contain just two classes (<em class="EmphasisTypeItalic ">binary classification</em><span id="ITerm18"/>), or more classes (<em class="EmphasisTypeItalic ">multiclass classification</em><span id="ITerm19"/>). Each text must be assigned a single class, which means that the classes are exclusive. Typical tasks include spam detection <span id="ITerm20"/> in emails, sentiment analysis <span id="ITerm21"/>, categorization of news articles <span id="ITerm22"/>, hate speech detection<span id="ITerm23"/>, dialog act classification, and many more. Some examples are listed in Table <span class="InternalRef"><a href="#Tab1">5.1</a></span>. Kowsari et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>], Li et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>] and Minaee et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>] provide surveys on text classification.</p><p class="Para" id="Par14">Often a document covers several topics simultaneously, e.g. a news article on the construction cost of a soccer stadium. In this case it is necessary to assign multiple classes to a document, in our example <em class="EmphasisTypeItalic ">“soccer”</em> and <em class="EmphasisTypeItalic ">“finance”</em>. This type of classification is called <em class="EmphasisTypeItalic ">multilabel classification</em><span id="ITerm24"/>. <em class="EmphasisTypeItalic ">Extreme multilabel classification</em><span id="ITerm25"/> is a variant containing a very large label set with thousands of labels.</p><div class="Para" id="Par15">There are a number of popular benchmarks to assess the performance of document classification approaches covering two or more classes. Typically, the benchmarks contain many thousand training and test examples. Table <span class="InternalRef"><a href="#Tab2">5.2</a></span> describes the properties of some popular text classification benchmarks. Often documents are categorized according to the subjective opinions of users. An example are reviews of movies or restaurants, which can be classified as positive, negative, or neutral. Then the classification corresponds to a <em class="EmphasisTypeItalic ">sentiment analysis</em><span id="ITerm26"/> task. <div class="Table" id="Tab2"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 5.2</span><p class="SimplePara">Popular text classification benchmarks</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Task</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Description</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Classes</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">IMDB</em><span id="ITerm27"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Reviews from the movie rating page IMDB. 25k training, 25k test and 50k unlabeled reviews</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Two classes: positive and negative</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Yelp</em><span id="ITerm28"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR131" role="doc-biblioref">131</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Yelp reviews of stores and restaurants. 560k training and 38k text reviews.</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Binary: positive/negative multiclass: five star classes</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">DBpedia</em><span id="ITerm29"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">14 non-overlapping classes from the DBpedia ontology. Each class is represented by 40k training samples and 5k test samples,</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">14 different classes: company, artist, athlete, animal, album, film, etc.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">ArXiv</em><span id="ITerm30"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">33k scientific articles from arXiv with documents of average length 6300 and length &gt; 5000</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">11 classes: artificial intelligence, computer vision, group theory, etc.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">SemEval-20 Task 12</em><span id="ITerm31"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR128" role="doc-biblioref">128</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">14k Twitter tweets available for five languages: English, Arabic, Danish, Greek, Turkish</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Two classes: offensive or not offensive.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">EURLex-4K</em><span id="ITerm32"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Benchmark of law documents containing 45, 000 training examples with an average length of 727 words and an average of five correct classes per example</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">4271 non-exclusive classes</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Amazon670k dataset</em><span id="ITerm33"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>]</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Descriptions of amazon products. 490k training and 153k test samples. About 5.5 classes per document.</p></td><td style="text-align: left;"><p class="SimplePara">679k non-exclusive categories: products in the Amazon catalog, about 4 samples per category</p></td></tr></tbody></table></div><div class="Table" id="Tab3"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 5.3</span><p class="SimplePara">Language analysis tasks based on relation extraction [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>, p. 10]. Underlining indicates phrases annotated by the model</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Task</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Description</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Example</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Coreference resolution</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Group phrases which refer to the same object.</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalicUnderline ">Betty</em><sub>(1)</sub><em class="EmphasisTypeItalic ">loves</em><em class="EmphasisTypeItalicUnderline ">her</em><sub>(1)</sub><em class="EmphasisTypeItalicUnderline ">cute dog</em><sub>(2)</sub>.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Aspect-based sentiment analysis</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Extract phrases (aspects) from a text and determine sentiments for them (positive, negative, neutral).</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalicUnderline ">The steak</em><sub><em class="EmphasisTypeItalic ">aspect</em></sub><em class="EmphasisTypeItalic ">was</em><em class="EmphasisTypeItalicUnderline ">horrible</em><sub><em class="EmphasisTypeItalic ">negative</em></sub>.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Entity relation extraction</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Extract relations among entities or concepts in a text.</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Peter works as a lawyer.</em><span class="InlineEquation" id="IEq8"><img alt="$$\rightarrow $$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq8.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">profession(Peter, lawyer)</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Event extraction</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Extract events, i.e. n-ary relations among entities or nouns in a text.</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">At</em><em class="EmphasisTypeItalicUnderline ">noon</em><sub><em class="EmphasisTypeItalic ">time</em></sub><em class="EmphasisTypeItalicUnderline ">terrorists</em><sub><em class="EmphasisTypeItalic ">attacker</em></sub><em class="EmphasisTypeItalic ">detonated a</em><em class="EmphasisTypeItalicUnderline ">bomb</em><sub><em class="EmphasisTypeItalic ">instrument</em></sub><em class="EmphasisTypeItalic ">in</em><em class="EmphasisTypeItalicUnderline ">Paris</em><sub><em class="EmphasisTypeItalic ">place</em></sub>. <span class="InlineEquation" id="IEq9"><img alt="$$\rightarrow $$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq9.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">conflict-attack</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Semantic role labeling</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">For each verb determine the role of phrases w.r. to the verb.</p></td><td style="text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalicUnderline ">Mary</em><sub><em class="EmphasisTypeItalic ">agent</em></sub><em class="EmphasisTypeItalicUnderline ">sold</em><sub><em class="EmphasisTypeItalic ">verb</em></sub><em class="EmphasisTypeItalicUnderline ">the book</em><sub><em class="EmphasisTypeItalic ">theme</em></sub><em class="EmphasisTypeItalic ">to</em><em class="EmphasisTypeItalicUnderline ">John</em><sub><em class="EmphasisTypeItalic ">recipient</em></sub>.</p></td></tr></tbody></table></div></div><p class="Para" id="Par16">Early methods for document classification in the 1990s used classical machine learning approaches [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>]. In the first preprocessing step, manually created features were extracted from the documents. In the second step, a classifier was trained with these features to reconstruct the manually assigned class labels (Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec3"><span class="RefSource">1.​3</span></a></span>). Finally, this classifier was applied to new documents. Usually, <em class="EmphasisTypeItalic ">bag-of-words</em><span id="ITerm34"/> representations were used to represent the input documents. Popular classification methods included <em class="EmphasisTypeItalic ">naive Bayes</em><span id="ITerm35"/>, <em class="EmphasisTypeItalic ">logistic classifier</em><span id="ITerm36"/>, the <em class="EmphasisTypeItalic ">support vector machine</em><span id="ITerm37"/>, and tree-based methods like <em class="EmphasisTypeItalic ">random forests</em><span id="ITerm38"/>. However, all these methods were hampered by the shortcomings of the bag-of-words representation (Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec3"><span class="RefSource">1.​3</span></a></span>), which ignores the sequence of words in a document.</p><p class="Para" id="Par17">In the next sections, we consider current classification models for mutually exclusive as well as “overlapping” classes. It turns out that most of the current best approaches are based on PLMs.</p><section class="Section2 RenderAsSection2" id="Sec2"><h3 class="Heading"><span class="HeadingNumber">5.1.1 </span>Multiclass Classification with Exclusive Classes</h3><p class="Para" id="Par18">A prominent application of <strong class="EmphasisTypeBold ">BERT</strong><span id="ITerm39"/> is fine-tuning for a classification task (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec4"><span class="RefSource">2.​1.​2</span></a></span>). Here, a pre-trained BERT is adapted to this task by supervised fine-tuning, using the contextual embedding of the <em class="EmphasisTypeItalic ">“[CLS]”</em> token in the highest layer as input for a logistic classifier. This classifier is extremely successful for natural language understanding tasks (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec7"><span class="RefSource">2.​1.​5</span></a></span>).</p><p class="Para" id="Par19"><strong class="EmphasisTypeBold ">XLNet</strong><span id="ITerm40"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR120" role="doc-biblioref">120</a></span>] is trained by reconstructing a permuted token sequence (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>), and is therefore able to capture a lot of knowledge about the language. It achieves 96.2% accuracy on the binary IMDB classification task. This performance is surpassed by <strong class="EmphasisTypeBold ">ERNIE-Doc</strong><span id="ITerm41"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>] with 97.1%. ERNIE-Doc is a transformer with an enhanced recurrence mechanism capable of considering many previous segments of a text in the same layer. The model aims to mitigate problems of other transformer-based models for long contexts such as the Longformer, which do not provide the contextual information of whole documents to each segment. The <span class="EmphasisTypeSmallCaps ">Sota</span> is currently held by a simpler model [<span class="CitationRef"><a epub:type="biblioref" href="#CR101" role="doc-biblioref">101</a></span>], which modifies the well known paragraph vector [<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>] and Naive Bayes weighted bag of <em class="EmphasisTypeItalic ">n</em>-grams. It achieves an accuracy of 97.4%.</p><p class="Para" id="Par20">The current best model on the IMDB dataset with 10 classes is <strong class="EmphasisTypeBold ">ALBERT-SEN</strong><span id="ITerm42"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>]. The authors propose an approach which evaluates the overall importance of sentences to the whole document, with the motivation that different sentences can contain different polarities but that the overall polarity depends on a few important sentences. Their model uses ALBERT (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>) to encode sentences via the <em class="EmphasisTypeItalic ">[SEP]</em> and <em class="EmphasisTypeItalic ">[CLS]</em> token representations. They concatenate these representations with class-weighted representations. Then they have a document encoder that calculates importance weights for every sentence and creates a weighted representation of the sentences as document representation. Finally, they calculate a sentiment score by utilizing the document representation and the class representations, which were also used in the sentence encoder. The model achieves an accuracy of 54.8%. It should be noted that subtle nuances in language expressions must be taken into account in this classification task with 10 classes.</p><p class="Para" id="Par21">For the Yelp benchmark, <strong class="EmphasisTypeBold ">XLNet</strong><span id="ITerm43"/> performs best for the binary version with an accuracy of 98.4% and achieves the second-best accuracy of 73.0% for the fine-granular version with 5 classes. The leading model for this task is <strong class="EmphasisTypeBold ">HAHNN</strong><span id="ITerm44"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>] with an accuracy of 73.3%. HAHNN combines convolutional layers, gated recurrent units and attention mechanisms. It builds on non-contextual FastText [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>] embeddings as word representations and uses a stack of convolutional layers to obtain contextual information. This is followed by a word encoder which applies recurrent GRU cells to obtain word representations, and an attention mechanism to create weights for the input words. Sentence representations are then formed as an attention-weighted average of the words. Another GRU layer is employed to create sentence representations, which are then combined via attention to generate a document level representation. This establishes the input to a fully connected layer with softmax activation for classification.</p><p class="Para" id="Par22"><strong class="EmphasisTypeBold ">BigBird</strong><span id="ITerm45"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR127" role="doc-biblioref">127</a></span>] is especially valuable for classification tasks with long documents, as it can process input sequences of length 4096 (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec8"><span class="RefSource">3.​2.​1</span></a></span>). Following BERT, the output embedding of the first <em class="EmphasisTypeItalic ">[CLS]</em> is input for the classifier. For the IMDB data with shorter documents there is no performance gain compared to simpler models. On the <em class="EmphasisTypeItalic ">ArXiv benchmark</em><span id="ITerm46"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>] with documents of average length 6300 and 11 classes BigBird improves <span class="EmphasisTypeSmallCaps ">Sota</span> by about 5% points.</p><p class="Para" id="Par23">With the advent of Web 2.0 and the ability for users to create and share their own content with the world, the proliferation of harmful content such as hate speech, has increased. This is now fueled by bots and machine learning models that automatically create such content at a scale that humans can barely manage. <em class="EmphasisTypeItalic ">Hate speech</em><span id="ITerm47"/> is often defined as a hostile or disparaging communication by a person or group referring to characteristics such as race, color, national origin, gender, disability, religion, or sexual orientation [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>]. According to European law, hate speech is a punishable criminal offense.</p><p class="Para" id="Par24">Hate speech detection can be solved as a text classification task. Recognizing such a text is difficult because the line between hate speech, irony, free speech, and art is blurred. Jahan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>] and Yin et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR123" role="doc-biblioref">123</a></span>] give a systematic review on automatic hate speech detection. Because of the importance of the task, let’s take a closer look at current approaches.</p><p class="Para" id="Par25">Roy et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR88" role="doc-biblioref">88</a></span>] follow a multilingual approach. They preprocess the text from Twitter by using a special tokenization of tweets. The cleaned text, emojis and segmented hashtags are encoded by different transformers and concatenated. A final multilayer perceptron generates the classification. The results for the <em class="EmphasisTypeItalic ">HASOC 2019 tweet dataset</em><span id="ITerm48"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>] show that the additional signal from the emojis and the hashtags yield a performance boost for hate speech detection as well as for classifying the type of hate speech. They achieve F1-values of 90.3%, 81.9% and 75.5% on the English, German, and Hindi test sets.</p><p class="Para" id="Par26">Mathew et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR59" role="doc-biblioref">59</a></span>] argue that the decisions of hate speech classifiers should be explained. They present the <em class="EmphasisTypeItalic ">HateXplain</em><span id="ITerm49"/> dataset with about 20k posts. The annotation contains class labels (hateful, offensive, or normal), the target group being vilified, and span annotations of words causing the classification. Overall a BERT model yields the best results in explaining the hate speech classification decisions.</p><p class="Para" id="Par27">A recent competition was the SemEval-20 Task 12 [<span class="CitationRef"><a epub:type="biblioref" href="#CR128" role="doc-biblioref">128</a></span>], where 14,100 Twitter tweets were manually labeled as either offensive or not offensive. Using a <strong class="EmphasisTypeBold ">RoBERTa</strong><span id="ITerm50"/> classifier (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>) Wiedemann et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR110" role="doc-biblioref">110</a></span>] achieved 92.0% F1-value and won the competition. In a later experiment an ensemble of <em class="EmphasisTypeItalic ">ALBERT</em><span id="ITerm51"/> models (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>) increased this score to 92.6%. In summary, the automatic classification of hate speech can be solved by PLMs with high quality.</p></section>
<section class="Section2 RenderAsSection2" id="Sec3"><h3 class="Heading"><span class="HeadingNumber">5.1.2 </span>Multilabel Classification</h3><div class="Para" id="Par28">Multilabel classification is required whenever a text can belong to multiple classes simultaneously. When a very large number of classes is available, this is sometimes called <em class="EmphasisTypeItalic ">extreme multilabel classification</em><span id="ITerm52"/>. An example problem is the assignment of tags to Wikipedia articles, where Wikipedia has almost 500k tags. In multilabel classification usually a score or probability for each class is returned. This can be used to rank the classes. Traditional metrics such as accuracy, which assume that only one class is correct, cannot be applied. An alternative is to measure the quality of ranking induced by the score (c.f. Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec3"><span class="RefSource">6.​1.​2</span></a></span>). A popular measure for a predicted score vector <span class="InlineEquation" id="IEq10"><img alt="$$\hat {y}_i\in [0,1]$$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq10.png" style="width:4.24em"/></span> and a ground truth label vector <em class="EmphasisTypeItalic ">y</em><sub><em class="EmphasisTypeItalic ">i</em></sub> ∈{0, 1} is the <em class="EmphasisTypeItalic ">precision at</em><em class="EmphasisTypeItalic ">k</em><span id="ITerm53"/>, which counts, how many correct classes are among the <em class="EmphasisTypeItalic ">k</em> classes with the highest score: <div class="Equation NumberedEquation" id="Equ1"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} prec@k = \frac 1k \sum_{l\in \text{rank}_k(\hat{y})} y_l \qquad  DCG@k = \frac 1k \sum_{l\in rank_k(\hat{y})} \frac{y_l}{\log(l+1)} , \end{aligned} $$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_Equ1.png" style="width:26.88em"/></div></div> <div class="EquationNumber">(5.1)</div></div></div></div><p class="Para" id="Par29">where <span class="InlineEquation" id="IEq11"><img alt="$$\text{rank}(\hat {y})=(i_1,\ldots ,i_k)$$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq11.png" style="width:9.44em"/></span> is the vector of the indices of the <em class="EmphasisTypeItalic ">k</em> largest values of <span class="InlineEquation" id="IEq12"><img alt="$$\hat {y}_i$$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq12.png" style="width:1.06em"/></span> sorted in descending order <span class="InlineEquation" id="IEq13"><img alt="$$\hat {y}_{i_1}\ge \cdots \ge \hat {y}_{i_k}$$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq13.png" style="width:6.63em"/></span> . The second measure <em class="EmphasisTypeItalic ">DCG@k</em> is the <em class="EmphasisTypeItalic ">discounted cumulative gain</em><span id="ITerm54"/>, where the correct assignments <em class="EmphasisTypeItalic ">y</em><sub><em class="EmphasisTypeItalic ">l</em></sub> are weighted by their rank <em class="EmphasisTypeItalic ">l</em> transformed with <span class="InlineEquation" id="IEq14"><img alt="$$1/\log (l+1)$$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq14.png" style="width:5.44em"/></span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>]. This reflects that correct assignments with a lower rank should get a lower weight. In addition, there is a normalized version <em class="EmphasisTypeItalic ">nDCG@k</em>, where <em class="EmphasisTypeItalic ">DCG@k</em> is divided by its maximal possible value.</p><p class="Para" id="Par30">Separate classifiers for each class often yield a very good accuracy, but suffer from very bad training and prediction time. In the worst case these classifiers have to be trained per label with all positive instances of a label and all instances of the other labels as negative samples. To mitigate this effect <strong class="EmphasisTypeBold ">Parabel</strong><span id="ITerm55"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR83" role="doc-biblioref">83</a></span>] is based on a tree ensemble. First Parabel creates label representations by averaging all the instances that belong to a label and normalizing this averaged vector to 1. Then balanced 2-means clustering is applied on the label space recursively until all leaf nodes in the clustered label tree contain fewer than <em class="EmphasisTypeItalic ">M</em> labels, e.g. <em class="EmphasisTypeItalic ">M</em> = 100. For each internal node of the tree and for the leaf nodes, classifiers are trained to decide which path of the tree an instance follows. Thus, a balanced label hierarchy is generated efficiently based on a label representation such that labels with similar inputs end up together at the leaves. Up to 3 such trees are used as an ensemble.</p><p class="Para" id="Par31">Finally, for each label, 1-vs-All classifiers are trained as a MAP estimate of the joint probability distribution over labels. The negative examples used for training these classifiers are drawn from the other labels in the same leaf, so the most similar or confusing counterexamples are employed. For prediction a beam search is performed in the tree and only for the <em class="EmphasisTypeItalic ">k</em> most probable labels a classification is actually performed. Parabel has been applied to problems with 7M labels and can make predictions in logarithmic time. Parabel is significantly faster at training and prediction than state-of-the-art extreme classifiers while having almost the same precision. On the EURLex-4K it achieves a prec@1 value of 81.5 and on the Amazon-670k a prec@1 value of 43.9, which is worse than the 45.4 of the best approach, but its time for prediction is only 1/1000.</p><p class="Para" id="Par32"><strong class="EmphasisTypeBold ">AttentionXML</strong><span id="ITerm56"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR124" role="doc-biblioref">124</a></span>] is a tree-based classifier, which uses contextual embeddings as input features. With an attention between the many labels and the tokens, AttentionXML represents a given text differently for each label. The architecture of AttentionXML consists of a word representation layer, a bidirectional LSTM layer, an attention layer with attention from all labels to the BiLSTM (Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec6"><span class="RefSource">1.​6</span></a></span>) encoded input and lastly a fully connected layer and an output layer.</p><p class="Para" id="Par33">AttentionXML first builds a deep tree similar to Parabel. Then the tree is compressed to a shallow and wide tree, which allows to handle millions of categories, especially for “tail labels”, i.e. classes with only a few examples in the training set [<span class="CitationRef"><a epub:type="biblioref" href="#CR37" role="doc-biblioref">37</a></span>]. The model uses the binary cross-entropy loss function. For each level of the tree this model is trained, being initialized with the model of the prior tree level. AttentionXML trains label ranking with negative labels sampled by fine-tuned label recalling models. For prediction the tree is used for a beam search, so only tree branches where the parent nodes have highest scores are considered.</p><p class="Para" id="Par34">On the <em class="EmphasisTypeItalic ">EURLex-4K benchmark</em><span id="ITerm57"/> AttentionXML achieves <em class="EmphasisTypeItalic ">prec@</em>1 = 87.1% and <em class="EmphasisTypeItalic ">prec@</em>5 = 61.9%. This means that the highest scoring prediction of the model is correct for 87.1% of the test predictions and 61.9% of the five highest scoring predictions are correct. Note that the choice of <em class="EmphasisTypeItalic ">k</em> should be made according to the average number of labels per document in the training set. On the <em class="EmphasisTypeItalic ">Amazon670k dataset</em><span id="ITerm58"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>] with 679k categories AttentionXML achieves <em class="EmphasisTypeItalic ">prec@</em>1 = 47.6% and <em class="EmphasisTypeItalic ">prec@</em>5 = 38.9%. This means that about 40% of the alternative products are correctly identified.</p><p class="Para" id="Par35"><strong class="EmphasisTypeBold ">LightXML</strong><span id="ITerm59"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>] employs a transformer encoder to generate contextual word features and generates negative examples for each category in a dynamic way. First, a set of label clusters is created based on the input features so that each label belongs to one cluster. Then a pre-trained model like RoBERTa (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>) is employed to encode the input text of an instance into contextual embeddings. To represent the input text of a training example, the embeddings of the <em class="EmphasisTypeItalic ">[CLS]</em> token in the last five layers are concatenated.</p><p class="Para" id="Par36">A specific <em class="EmphasisTypeItalic ">label recalling</em> model aims to predict the label clusters using the <em class="EmphasisTypeItalic ">[CLS]</em> embeddings as input. In addition, the <em class="EmphasisTypeItalic ">label ranking model</em> receives the <em class="EmphasisTypeItalic ">[CLS]</em> embeddings of a training instance as well as the corresponding label . Negative examples with other labels are dynamically generated with the label recalling model. The loss terms of both the generator and the discriminator are combined in a joint loss function allowing end-to-end training. On the EURLex-4K benchmark LightXML achieves a <em class="EmphasisTypeItalic ">prec@</em>1 = 87.6% and <em class="EmphasisTypeItalic ">prec@</em>5 = 63.4%. On the Amazon670k benchmark it reaches a <em class="EmphasisTypeItalic ">prec@</em>1 = 49.1% and <em class="EmphasisTypeItalic ">prec@</em>5 = 39.6%. Both values are slightly better than those of AttentionXML. The approach also demonstrates <span class="EmphasisTypeSmallCaps ">Sota</span> performance compared to 7 alternative model on three other multilabel datasets.</p><p class="Para" id="Par37"><strong class="EmphasisTypeBold ">Overlap</strong><span id="ITerm60"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>] groups labels into overlapping clusters. In product categorization, for example, the tag <em class="EmphasisTypeItalic ">“belt”</em> can be related to a vehicle belt (in the <em class="EmphasisTypeItalic ">“vehicle accessories”</em> category), or a man’s belt (under <em class="EmphasisTypeItalic ">“clothing”</em> category). Each label can now occur at most <em class="EmphasisTypeItalic ">λ</em>-times, where <em class="EmphasisTypeItalic ">λ</em> is a hyperparameter of the approach. The authors initialize their partitioning with a balanced <em class="EmphasisTypeItalic ">k</em>-means clustering and then proceed with an optimization method to reassign labels in a way that maximizes the precision rate. On the Amazon670k benchmark the model reaches <span class="EmphasisTypeSmallCaps ">Sota</span> values of <em class="EmphasisTypeItalic ">prec@</em>1 = 50.7% and <em class="EmphasisTypeItalic ">prec@</em>5 = 41.6%. There are also alternative models with a tree-based search, which are able to increase recall rates and reduce effort [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>].</p><p class="Para" id="Par38">There is a great similarity of extreme multilabel classification with text retrieval, which is covered in Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec1"><span class="RefSource">6.​1</span></a></span>. This group of text applications has seen a large progress in recent years. For dense retrieval the query and the document representations are encoded by a BERT model, and the documents with largest cosine similarity are returned. Probably many approaches from this field may be used for text classification.</p></section>
<section class="Section2 RenderAsSection2" id="Sec4"><h3 class="Heading"><span class="HeadingNumber">5.1.3 </span>Few- and Zero-Shot Classification</h3><div class="Para" id="Par39">Large autoregressive language models like GPT-2, GPT-3, Gopher and PaLM have acquired an enormous amount of information about facts and language by pre-training. They can be instructed to classify a text by a few examples [<span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>], as described in Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec41"><span class="RefSource">3.​6.​3</span></a></span>. Figure <span class="InternalRef"><a href="#Fig1">5.1</a></span> provides an example prompt for the classification of a text by sentiment [<span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>]. This means that no additional fine-tuning dataset is required, but only a prompt with a few examples. In the same way the pre-trained Gopher model [<span class="CitationRef"><a epub:type="biblioref" href="#CR85" role="doc-biblioref">85</a></span>] was applied to a comprehensive set of about 150 benchmark tasks, which require the generation of answers using few-shot instructions. Similar to other autoregressive models it may predict class labels for documents (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec17"><span class="RefSource">2.​2.​5</span></a></span>). As the results show [<span class="CitationRef"><a epub:type="biblioref" href="#CR85" role="doc-biblioref">85</a></span>, p. 56], Gopher is often able to outperform conventional PLMs fine-tuned on the domain. Therefore, classification by instruction seems to be a viable alternative, if a large autoregressive PLM such as GPT-3, Gopher or GPT-Neo is available.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><img alt="" aria-describedby="d64e1972" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Fig1_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e1972"><p class="Para" id="Par156">A screenshot contains two sections titled prompt and G P T Neo. It has text inputs for sentiment analysis of respective tweets with G P T Neo as positive.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.1</span><p class="SimplePara">A query for few-shot learning for sentiment analysis with GPT-Neo, a free version of GPT with 2.7B parameters. The query can be evaluated on the API [<span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par40">Recently, the <em class="EmphasisTypeItalic ">RAFT</em><span id="ITerm61"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>] benchmark was released. RAFT is specifically designed for evaluating few-shot performance in text classification tasks. It covers 11 real-world datasets, 8 of which are binary classification, two contain three classes, and one contains 77 classes. Each task comes with natural language instructions and 50 labeled training examples. An example benchmark is <em class="EmphasisTypeItalic ">“Label the sentence based on whether it is related to an adverse drug effect. Sentence: No regional side effects were noted. Label: not related. …”</em>. A prompt contained less than 50 examples. The performance is measured by an average F1 over all 11 tasks. On these RAFT benchmarks BART yields an F1 average of 38.2%, GPT-Neo (2.7B) achieves 48.1%, AdaBoost decision trees 51.4%, and GPT-3 (175B) scores 62.7%. Humans achieve an average F1 of 73.5%.</p><p class="Para" id="Par41"><strong class="EmphasisTypeBold ">PET</strong><span id="ITerm62"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>] asks users to specify one or more patterns that convert an input example <em class="EmphasisTypeItalic ">x</em> into a <em class="EmphasisTypeItalic ">cloze prompt</em><span id="ITerm63"/> (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec4"><span class="RefSource">2.​1.​2</span></a></span>) so that it can be processed by a masked language model like BERT. In addition, users must describe the meaning of all output classes. This is done with a “verbalizer” that assigns a natural language expression to each output <em class="EmphasisTypeItalic ">y</em>. Multiple verbalizers may be specified for the same data. An example is <em class="EmphasisTypeItalic ">“I really enjoyed this movie. It was [MASK].”</em> and <em class="EmphasisTypeItalic ">“I really enjoyed this movie. Question: Is this a positive movie review? Answer: [MASK].”</em> for the text <em class="EmphasisTypeItalic ">“I really enjoyed this movie”</em>. The PLM is then trained to maximize <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">y</em>|<em class="EmphasisTypeItalic ">x</em>) for observed pairs. PET achieves a new state of the art on RAFT with an average F1 of 82.2% and performs close to nonexpert humans for 7 out of 11 tasks.</p><p class="Para" id="Par42">Foundation Models can also be used to generate new data for a text classification task. If, for example, input for a restaurant classification task is required, the model can be prompted to generate a new restaurant review for a specific label Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec46"><span class="RefSource">3.​6.​6</span></a></span>. In this way training data for fine-tuning a model can be created.</p><section class="Section3 RenderAsSection3" id="Sec5"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par43"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par44">The code and trained parameters of many classical models like BigBird, XLNET, T5 are available at Hugging Face <span class="ExternalRef"><a href="https://huggingface.co/transformers/"><span class="RefSource">https://​huggingface.​co/​transformers/​</span></a></span>.</p></li><li><p class="Para" id="Par45">The LightXML model code is here <span class="ExternalRef"><a href="https://github.com/kongds/LightXML"><span class="RefSource">https://​github.​com/​kongds/​LightXML</span></a></span>.</p></li><li><p class="Para" id="Par46">The code of PET can be found here <span class="ExternalRef"><a href="https://github.com/timoschick/pet"><span class="RefSource">https://​github.​com/​timoschick/​pet</span></a></span>.</p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec6"><h3 class="Heading"><span class="HeadingNumber">5.1.4 </span>Summary</h3><p class="Para" id="Par47">For document classification, a PLM that has been pre-trained with a large set of documents is usually fine-tuned to solve a specific classification task. Typically, the embedding of a particular token such as <em class="EmphasisTypeItalic ">[CLS]</em> is used as input to a logistic classifier. This setup has outperformed all previous bag-of-word classifiers such as the SVM. Specialized PLM variants like XLNET or ALBERT show a higher performance because of their more effective pre-training. For longer documents, suitable models like BigBird yield good results. Identifying hate speech can be considered as a classification task, where good results are achieved with standard models such as BERT and RoBERTa.</p><p class="Para" id="Par48">The situation is different for multi-label classification, where several categories can be correct for one document. Here, tree-like classifiers in combination with contextual embeddings show good results. By the tree a small number of candidate classes can be selected reducing the training and execution times. Extreme multi-label classifications, such as matching product descriptions to related product descriptions, are close to a document retrieval tasks and can benefit from techniques developed in this area, e.g. dense retrieval by DPR.</p><p class="Para" id="Par49">Large pre-trained autoregressive language models like GPT-3, Gopher and PaLM may be instructed by few-shot learning to solve classification tasks. Recent approaches achieve a performance close to humans. Not long ago an API has been released which allows to pre-train GPT-3 and adapt it to specific data and specific classification tasks (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec40"><span class="RefSource">3.​6.​2</span></a></span>). A simpler alternative is InstructGPT, which can be easily directed to perform a classification, e.g. a sentiment analysis (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec43"><span class="RefSource">3.​6.​5</span></a></span>). However, a formal evaluation of the performance of this approach is not yet available, as the model would have to process the training data.</p><p class="Para" id="Par50">While PLMs have achieved promising results on demanding benchmarks, most of these models are not interpretable. For example, why does a model arrive at a particular classification? Why does a model outperform another model on one dataset, but performs worse on other datasets? Although the mechanisms of attention and self-attention provide some insight into the associations that lead to a particular outcome, detailed investigation of the underlying behavior and dynamics of these models is still lacking (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec38"><span class="RefSource">2.​4.​5</span></a></span>). A thorough understanding of the theoretical aspects of these models would lead to a better acceptance of the results.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec7"><h2 class="Heading"><span class="HeadingNumber">5.2 </span>Word Sense Disambiguation</h2><p class="Para" id="Par51">In nearly all languages the same word may express different concepts. An example is the word <em class="EmphasisTypeItalic ">“set”</em>, which may be a verb, an adjective, or a noun and can be interpreted as ‘a group of things’, a ‘scenery’, a mathematical concept, a sports term, etc. The WordNet [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>] lexical database lists 45 different senses for this word. <em class="EmphasisTypeItalic ">Word sense disambiguation</em><span id="ITerm64"/> (<em class="EmphasisTypeItalic ">WSD</em>) aims to distinguish these different meanings and annotate each word with its sense. It can be treated as a classification task, where each word is assigned to a sense of a sense inventory such as WordNet. The contextual embeddings generated by PLMs offer a way to identify these meanings. Bevilacqua et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>] provide a recent survey of WSD approaches.</p><p class="Para" id="Par52">WSD can be used for a number of purposes. A traditional application is search, where the different senses of the same word are distinguished in the query. <em class="EmphasisTypeItalic ">Lexical substitution</em><span id="ITerm65"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>] aims to replace a word or phrase in a text with another with nearly identical meaning.</p><section class="Section2 RenderAsSection2" id="Sec8"><h3 class="Heading"><span class="HeadingNumber">5.2.1 </span>Sense Inventories</h3><p class="Para" id="Par53">WSD obviously depends on the definition of senses, which have to be assigned to the words. The main sense inventory for WSD in English is <em class="EmphasisTypeItalic ">WordNet</em><span id="ITerm66"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>]. It consist of expert-made <em class="EmphasisTypeItalic ">synsets</em><span id="ITerm67"/>, which are sets of synonymous words that represent a unique concept. A word can belong to multiple synsets denoting its different meanings. Version 3.0 of WordNet covers 147,306 words (or phrases) and 117,659 synsets. WordNet is also available for languages other than English through the <em class="EmphasisTypeItalic ">Open Multilingual WordNet</em><span id="ITerm68"/> project [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>]. <em class="EmphasisTypeItalic ">Wikipedia</em><span id="ITerm69"/> is another sense inventory often used for <em class="EmphasisTypeItalic ">Entity Linking</em><span id="ITerm70"/> (Sect. <span class="InternalRef"><a href="#Sec16">5.3.3</a></span>), where a person, a concept or an entity represented by a Wikipedia page has to be linked to a given <em class="EmphasisTypeItalic ">mention</em><span id="ITerm71"/> of the entity in a text. <em class="EmphasisTypeItalic ">BabelNet</em><span id="ITerm72"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR71" role="doc-biblioref">71</a></span>] is a mixture of WordNet, Wikipedia and several other lexical resources, such as Wiktionary [<span class="CitationRef"><a epub:type="biblioref" href="#CR111" role="doc-biblioref">111</a></span>] and OmegaWiki [<span class="CitationRef"><a epub:type="biblioref" href="#CR75" role="doc-biblioref">75</a></span>]. It is highly multilingual covering more than 500 languages.</p><p class="Para" id="Par54">WordNet’s sense inventory is often too fine-grained. For example, the noun <em class="EmphasisTypeItalic ">“star”</em> has eight meanings in WordNet. The two meanings referring to a <em class="EmphasisTypeItalic ">“celestial body”</em> distinguish only whether the star is visible from earth or not. Both meanings are translated in Spanish as <em class="EmphasisTypeItalic ">“estrella”</em>, so this sense distinction is useless for this translation. It has been shown that for many tasks more coarse-grained sense inventories are better [<span class="CitationRef"><a epub:type="biblioref" href="#CR81" role="doc-biblioref">81</a></span>].</p><p class="Para" id="Par55">The best WSD algorithms use PLMs pre-trained on large document corpora. Through fine-tuning, they are trained to assign senses from the available sense inventory. In some cases, nearest neighbor operations are employed to measure the distance between embeddings and determine the most appropriate sense.</p></section>
<section class="Section2 RenderAsSection2" id="Sec9"><h3 class="Heading"><span class="HeadingNumber">5.2.2 </span>Models</h3><p class="Para" id="Par56"><strong class="EmphasisTypeBold ">GlossBERT</strong><span id="ITerm73"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>] employs a pre-trained BERT encoder. Its fine-tuning input is both the context sentence (where the word is used in the specific sense) and the <em class="EmphasisTypeItalic ">gloss</em><span id="ITerm74"/> (a sentence defining the meaning of the word). GlossBERT is trained to predict whether the gloss correctly describes the use of the target word. The <em class="EmphasisTypeItalic ">SemCor3.0</em><span id="ITerm75"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>] benchmark is annotated with WordNet senses. GlossBERT achieves a new <span class="EmphasisTypeSmallCaps ">Sota</span> of 77.0% F1 on this data.</p><p class="Para" id="Par57"><strong class="EmphasisTypeBold ">EWISER</strong><span id="ITerm76"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>] expresses WSD as a simple <em class="EmphasisTypeItalic ">Word annotation</em><span id="ITerm77"/> task (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec5"><span class="RefSource">2.​1.​3</span></a></span>), where a sense label is assigned to each word. It starts with an average of BERT embeddings for each word <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub> from different contexts and transforms them with a linear layer and the <em class="EmphasisTypeItalic ">Swish</em><span id="ITerm78"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR86" role="doc-biblioref">86</a></span>] activation function <span class="InlineEquation" id="IEq15"><img alt="$$f(x)=x\cdot  \operatorname {\mathrm {sigmoid}}(\beta x)$$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq15.png" style="width:10.25em"/></span>. For each combination of a word and a part-of-speech a set <em class="EmphasisTypeItalic ">S</em>(<em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub>) of possible word senses and hypernyms is determined similar to [<span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>]. Then the approach computes probabilities that a word belongs to a synset in <em class="EmphasisTypeItalic ">S</em>(<em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub>). By this approach the prediction takes into account which WordNet senses are possible for a word. It achieves a new <span class="EmphasisTypeSmallCaps ">Sota</span> of 80.1% on a combination of WSD benchmarks. This value is also an estimated upper bound on human inter-annotator agreement [<span class="CitationRef"><a epub:type="biblioref" href="#CR69" role="doc-biblioref">69</a></span>], showing that WSD is on par with humans. The paper lists the results for a number of alternative approaches. The <strong class="EmphasisTypeBold ">BEM</strong><span id="ITerm79"/> model [<span class="CitationRef"><a epub:type="biblioref" href="#CR15" role="doc-biblioref">15</a></span>] is a similar system yielding comparable accuracy. A detailed analysis of how PLMs (especially BERT) capture lexical ambiguity can be found in [<span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>]. The authors show that the embedding space of BERT covers enough detail to distinguish word senses.</p><div class="Para" id="Par58"><strong class="EmphasisTypeBold ">MuLaN</strong><span id="ITerm80"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>] is based on a multilingual list <span class="InlineEquation" id="IEq16"><img alt="$$\mathcal {D}$$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq16.png" style="width:1.12em"/></span> of <em class="EmphasisTypeItalic ">synsets</em><span id="ITerm81"/> in different languages. For example, <span class="InlineEquation" id="IEq17"><img alt="$${\mathcal {D}}$$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq17.png" style="width:1.12em"/></span> may contain the synset corresponding to the <em class="EmphasisTypeItalic ">“fountain”</em> meaning of <em class="EmphasisTypeItalic ">“spring”</em>, which is expressed in different languages as <em class="EmphasisTypeItalic ">“Quelle</em><sub><em class="EmphasisTypeItalic ">DE</em></sub><em class="EmphasisTypeItalic ">”</em>, <em class="EmphasisTypeItalic ">“spring</em><sub><em class="EmphasisTypeItalic ">EN</em></sub><em class="EmphasisTypeItalic ">”</em>, <em class="EmphasisTypeItalic ">“fountain</em><sub><em class="EmphasisTypeItalic ">EN</em></sub><em class="EmphasisTypeItalic ">”</em>, <em class="EmphasisTypeItalic ">“manantial</em><sub><em class="EmphasisTypeItalic ">ES</em></sub><em class="EmphasisTypeItalic ">”</em>, <em class="EmphasisTypeItalic ">“brollador</em><sub><em class="EmphasisTypeItalic ">CAT</em></sub><em class="EmphasisTypeItalic ">”</em>, <em class="EmphasisTypeItalic ">“source</em><sub><em class="EmphasisTypeItalic ">FR</em></sub><em class="EmphasisTypeItalic ">”</em>, <em class="EmphasisTypeItalic ">“fonte</em><sub><em class="EmphasisTypeItalic ">IT</em></sub><em class="EmphasisTypeItalic ">”</em>, and <em class="EmphasisTypeItalic ">“sorgente</em><sub><em class="EmphasisTypeItalic ">IT</em></sub><em class="EmphasisTypeItalic ">”</em>. The semantic repositories <em class="EmphasisTypeItalic ">WordNet</em><span id="ITerm82"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>] and <em class="EmphasisTypeItalic ">BabelNet</em><span id="ITerm83"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR71" role="doc-biblioref">71</a></span>] are employed to create <span class="InlineEquation" id="IEq18"><img alt="$${\mathcal {D}}$$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq18.png" style="width:1.12em"/></span>. MuLaN has the task to annotate an unlabeled corpus <em class="EmphasisTypeItalic ">U</em> in the target language with senses using a corpus <em class="EmphasisTypeItalic ">L</em><sub>lab</sub> in the source language (e.g. English) as input, which is annotated with senses from <span class="InlineEquation" id="IEq19"><img alt="$${\mathcal {D}}$$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq19.png" style="width:1.12em"/></span> . This is done in the following steps: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par59"><em class="EmphasisTypeItalic ">Creating embeddings</em>: The multilingual mBERT (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec13"><span class="RefSource">3.​3.​1</span></a></span>) trained on 104 languages is used to compute the embedding <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">σ</em>, <em class="EmphasisTypeItalic ">w</em>) of every word <em class="EmphasisTypeItalic ">w</em> in context <em class="EmphasisTypeItalic ">σ</em> in <em class="EmphasisTypeItalic ">L</em><sub>lab</sub>. If <em class="EmphasisTypeItalic ">w</em> is split into multiple tokens, their average is used. If <em class="EmphasisTypeItalic ">w</em> is a compound, first the tokens of each word within the compound are averaged and then the average over words is taken as representation for <em class="EmphasisTypeItalic ">w</em>.</p></li><li><p class="Para" id="Par60"><em class="EmphasisTypeItalic ">Candidate production</em>: Then for each word <em class="EmphasisTypeItalic ">w</em> with embedding <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">σ</em>, <em class="EmphasisTypeItalic ">w</em>) in context <em class="EmphasisTypeItalic ">σ</em> from <em class="EmphasisTypeItalic ">L</em><sub>lab</sub> the nearest 1000 neighbors from the unlabeled corpus <em class="EmphasisTypeItalic ">U</em> are determined by <em class="EmphasisTypeItalic ">FAISS</em><span id="ITerm84"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>]. As an example we select the text span <em class="EmphasisTypeItalic ">v</em> = <em class="EmphasisTypeItalic ">“correre”</em> from the context <em class="EmphasisTypeItalic ">τ</em> = <em class="EmphasisTypeItalic ">“Mi hanno consigliato di andare a correre.”</em> in <em class="EmphasisTypeItalic ">L</em><sub>lab</sub> as the closest candidate <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">τ</em>, <em class="EmphasisTypeItalic ">v</em>) for the instance <em class="EmphasisTypeItalic ">w</em> = <em class="EmphasisTypeItalic ">“running”</em> from the sentence <em class="EmphasisTypeItalic ">σ</em> = <em class="EmphasisTypeItalic ">“I’ve seen her go running in the park.”</em>.</p></li><li><p class="Para" id="Par61"><em class="EmphasisTypeItalic ">Synset compatibility</em>: Subsequently, it is checked if the closest candidate word <em class="EmphasisTypeItalic ">v</em> is contained in a synset of <em class="EmphasisTypeItalic ">w</em> in <span class="InlineEquation" id="IEq20"><img alt="$${\mathcal {D}}$$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq20.png" style="width:1.12em"/></span>. Otherwise it is discarded.</p></li><li><p class="Para" id="Par62"><em class="EmphasisTypeItalic ">Backward compatibility</em>: Finally, the nearest neighbors of <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">τ</em>, <em class="EmphasisTypeItalic ">v</em>) in context <em class="EmphasisTypeItalic ">τ</em> in <em class="EmphasisTypeItalic ">L</em><sub>lab</sub> are determined. (<em class="EmphasisTypeItalic ">τ</em>, <em class="EmphasisTypeItalic ">v</em>) is only retained, if its nearest neighbor list contains <em class="EmphasisTypeItalic ">w</em>.</p></li><li><p class="Para" id="Par63"><em class="EmphasisTypeItalic ">Dataset generation</em>: After a number of additional filtering steps the final annotation of words in the target corpus <em class="EmphasisTypeItalic ">U</em> is performed.</p></li></ul></div></div><p class="Para" id="Par64">As a labeled corpus <em class="EmphasisTypeItalic ">L</em><sub>lab</sub> a union of <em class="EmphasisTypeItalic ">SemCor</em><span id="ITerm85"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>] and the <em class="EmphasisTypeItalic ">WordNet Glos Corpus</em><span id="ITerm86"/> (<em class="EmphasisTypeItalic ">WNG</em><span id="ITerm87"/>) [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>] is used, which are annotated with senses. As unlabeled corpus <em class="EmphasisTypeItalic ">U</em> the Wikipedia is used for Italian, French, Spanish and German. When tested on <em class="EmphasisTypeItalic ">SemEval-13</em><span id="ITerm88"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>] and <em class="EmphasisTypeItalic ">SemEval-15</em><span id="ITerm89"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>], MuLaN is the best system to annotate words with senses in the four languages with F1-values above 80%. An important advantage of MuLaN is that it is able to transfer sense annotations from high-resource to low-resource languages.</p><div class="Para" id="Par65"><strong class="EmphasisTypeBold ">Escher</strong><span id="ITerm90"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>] reformulates WSD as a span prediction problem. The input to the model is a sentence with a target word and all its possible sense definitions. The output is a text span identifying the gloss expressing the target words most suitable meaning. As an example consider Fig. <span class="InternalRef"><a href="#Fig2">5.2</a></span> with the input sentence <em class="EmphasisTypeItalic ">“&lt;s&gt; The bully had to &lt;t&gt; back down &lt;/t&gt;. &lt;/s&gt;”</em> where the target word is enclosed in <em class="EmphasisTypeItalic ">“&lt;t&gt;”</em> and <em class="EmphasisTypeItalic ">“&lt;/t&gt;”</em>. Subsequently, two glosses are appended.<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO2"><img alt="" aria-describedby="d64e2803" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Fig2_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e2803"><p class="Para" id="Par157">A flow diagram of various target words, transformer, 2 logistic regression models for start and end of span, and span blocks.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.2</span><p class="SimplePara">Escher [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>] takes as input a sentence, where the target word <em class="EmphasisTypeItalic ">“back down”</em> is enclosed by <em class="EmphasisTypeItalic ">“&lt;t&gt;”</em> and <em class="EmphasisTypeItalic ">“&lt;/t&gt;”</em>. The most probable sense of the target word is indicated by the sentence selected by span prediction. A high probability of a span start is indicated by <em class="EmphasisTypeItalic ">“[”</em> and a high probability of the span end is indicated by <em class="EmphasisTypeItalic ">“]”</em></p></div></figcaption></figure></div><p class="Para" id="Par66">The span is predicted similar to Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec5"><span class="RefSource">2.​1.​3</span></a></span> by separately computing the probability for the first and last token of the span covering the correct gloss. In the example the sentence <em class="EmphasisTypeItalic ">“Move backwards from a certain position.”</em> is selected as span, which describes the correct sense. By lowering the prior probability of the most frequent sense for a word the approach is able to reduce the most frequent sense bias. Escher uses BART<sub>LARGE</sub> (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec4"><span class="RefSource">3.​1.​3</span></a></span>) as PLM architecture, as it is effective for reading comprehension. The output of its last decoder layer is used to represent the input tokens and to compute the start and end token distributions. On a number of SemEval datasets [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>] Escher has higher F1-scores compared to its competitors and this difference is statistically highly significant. Best results are achieved for nouns and adjectives with F1-values &gt; 83%, while for verbs the F1-value is only 69.3%.</p><p class="Para" id="Par67"><strong class="EmphasisTypeBold ">ConSec</strong><span id="ITerm91"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>] determines the sense of a token by considering not only the context words, but also the senses assigned to the neighboring words. It is based on an extension of DeBERTa, a BERT variant with superior performance (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>). ConSec uses WordNet example sentences with annotated meanings (glosses) as additional training data. The approach yields a <span class="EmphasisTypeSmallCaps ">Sota</span> of 83.2% F1 when applied to the <em class="EmphasisTypeItalic ">SemCor3.0</em><span id="ITerm92"/> benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>].</p><section class="Section3 RenderAsSection3" id="Sec10"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par68"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par69">The codes of GlossBERT and EWISER and trained models are available for a number of different languages <span class="ExternalRef"><a href="https://github.com/HSLCY/GlossBERT"><span class="RefSource">https://​github.​com/​HSLCY/​GlossBERT</span></a></span><span class="ExternalRef"><a href="https://github.com/SapienzaNLP/ewiser"><span class="RefSource">https://​github.​com/​SapienzaNLP/​ewiser</span></a></span>.</p></li><li><p class="Para" id="Par70">Escher along with the necessary training data is available at <span class="ExternalRef"><a href="https://github.com/SapienzaNLP/esc"><span class="RefSource">https://​github.​com/​SapienzaNLP/​esc</span></a></span>.</p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec11"><h3 class="Heading"><span class="HeadingNumber">5.2.3 </span>Summary</h3><p class="Para" id="Par71">WSD can be handled as a classification task, where each word is assigned to a number of possible meaning classes. Often WordNet is used as the sense inventory. GlossBERT compares the contextual embedding of a word with the embedding of a word in an example sentence (gloss) of WordNet. EWISER and MULAN directly work on the synsets of WordNet and capture the sets of possible senses and hypernyms. They are able to annotate senses in four languages with an F1-value above 80%. Escher reformulates WSD as a span prediction problem increasing F1 to 83%. ConSec takes into account the senses of nearby tokens and achieves a similar performance.</p><p class="Para" id="Par72">As WSD models get better, there is a need for more demanding benchmark datasets, which possibly may be generated by adversarial techniques. Moreover, there is a trend to WSD models which are more robust to domain shift and can cope with text from social media documents. To advance WSD it is necessary to extend sense-annotated data, especially for rare senses. In addition, multilingual WSD systems may be constructed which require large-scale multilingual WSD benchmarks. There are tendencies in WSD to do away with the fixed sense inventory and to distinguish the senses in other ways, e.g., in a lexical substitution task or by generating the definition of a word in a particular context.</p><p class="Para" id="Par73">An opportunity is the integration of WSD with entity linking (Sect. <span class="InternalRef"><a href="#Sec16">5.3.3</a></span>), where the model is required to associate mentions with entries in a knowledge base such as Wikipedia. As WSD systems work fairly well now, it would be possible to combine them with other applications like question answering or dialog systems. It has to be tested, whether an explicit inclusion of WSD is able to generate better results. For retrieval tasks, WSD has been superseded by embedding-based methods (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec1"><span class="RefSource">6.​1</span></a></span>), which provide a better hit rate.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec12"><h2 class="Heading"><span class="HeadingNumber">5.3 </span>Named Entity Recognition</h2><p class="Para" id="Par74"><em class="EmphasisTypeItalic ">Named entity recognition</em><span id="ITerm93"/> (<em class="EmphasisTypeItalic ">NER</em>) refers to the task of tagging <em class="EmphasisTypeItalic ">mentions</em><span id="ITerm94"/> of <em class="EmphasisTypeItalic ">named entities</em><span id="ITerm95"/>, such as persons, organizations and locations in texts. Labeled datasets for NER exist across many domains, e.g. news, science and medicine [<span class="CitationRef"><a epub:type="biblioref" href="#CR72" role="doc-biblioref">72</a></span>]. Typically these datasets are annotated in the <em class="EmphasisTypeItalic ">IOB2 format</em><span id="ITerm96"/>, which, for instance annotates the first token of a person with B-per and all other tokens of that entity with I-per. The O-tag is used for all tokens outside of entity mentions. An example is <em class="EmphasisTypeItalic ">“U.N.</em><sub><em class="EmphasisTypeItalic ">B-org</em></sub><em class="EmphasisTypeItalic ">official</em><sub><em class="EmphasisTypeItalic ">O</em></sub><em class="EmphasisTypeItalic ">Peter</em><sub><em class="EmphasisTypeItalic ">B-per</em></sub><em class="EmphasisTypeItalic ">Ekeus</em><sub><em class="EmphasisTypeItalic ">I-per</em></sub><em class="EmphasisTypeItalic ">heads</em><sub><em class="EmphasisTypeItalic ">O</em></sub><em class="EmphasisTypeItalic ">for</em><sub><em class="EmphasisTypeItalic ">O</em></sub><em class="EmphasisTypeItalic ">Bagdad</em><sub><em class="EmphasisTypeItalic ">B-loc</em></sub>.<em class="EmphasisTypeItalic ">”</em> NER involves the prediction of these tags for each token, i.e. the suffixes in the prior example. Therefore, it can be considered as a classification task, where a tag is assigned to each token. A standard dataset for NER is the CoNLL-2003 dataset [<span class="CitationRef"><a epub:type="biblioref" href="#CR89" role="doc-biblioref">89</a></span>], which contains English resp. German news texts with annotations for persons, organizations, locations, and miscellaneous names. Surveys on NER are provided by Li et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>], Nasar et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>] and Bose et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>].</p><p class="Para" id="Par75">NER is particularly useful in areas with a highly specialized vocabulary. Examples include the fields of healthcare or electromobility, where many thousands of publications are released each year. Since few experts understand the terminology, NER systems are particularly valuable for identifying publications on specialized topics. Of course, the NER types must be adapted to each area.</p><p class="Para" id="Par76">In the following section, we present approaches to ordinary NER where each word can have a single entity type. Named entities can also be nested, e.g. <em class="EmphasisTypeItalic ">“[[UK]</em><sub><em class="EmphasisTypeItalic ">gpe</em></sub><em class="EmphasisTypeItalic ">Embassy in [France]</em><sub><em class="EmphasisTypeItalic ">gpe</em></sub><em class="EmphasisTypeItalic ">]</em><sub><em class="EmphasisTypeItalic ">facility</em></sub><em class="EmphasisTypeItalic ">”</em>. This case is discussed in the second section. Even more challenging is the mapping of a named-entity phrase to the underlying unique entity in a knowledge base or ontology, e.g., a person. This is called entity linking and is discussed in the third section.</p><section class="Section2 RenderAsSection2" id="Sec13"><h3 class="Heading"><span class="HeadingNumber">5.3.1 </span>Flat Named Entity Recognition</h3><p class="Para" id="Par77">In <em class="EmphasisTypeItalic ">flat named entity recognition</em><span id="ITerm97"/> each token corresponds to at most one named entity. <strong class="EmphasisTypeBold ">BERT</strong><span id="ITerm98"/> can be fine-tuned to NER by predicting tags for each token using a logistic classifier (Fig. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Fig5"><span class="RefSource">2.​5</span></a></span>) as a final layer. For this setup BERT<sub>LARGE</sub> yielded 92.8% F1-value on the CoNLL-2003 test data. While the F1-values for persons and locations were higher (≈ 95%), the F1-value for miscellaneous names (78%) was much lower, as these entities form a vaguely defined class.</p><p class="Para" id="Par78"><strong class="EmphasisTypeBold ">LUKE</strong><span id="ITerm99"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR117" role="doc-biblioref">117</a></span>] treats words and entities in a given text as independent objects, and outputs contextual embeddings of tokens and entities. The model is based on RoBERTa and trained to predict randomly masked words and entities in a large entity-annotated corpus derived from Wikipedia. In this way, it obtains a lot of information on the relation between entities in the text. It contains an entity-aware self-attention mechanism that is an extension of BERT’s self-attention mechanism and takes into account embeddings, which indicate if a token represents text or an entity. It yields an F1-value of 94.3-F1 for CoNLL-2003, which is near-<span class="EmphasisTypeSmallCaps ">Sota</span>.</p><p class="Para" id="Par79"><strong class="EmphasisTypeBold ">ACE</strong><span id="ITerm100"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR106" role="doc-biblioref">106</a></span>] builds on the assumption that weighted sums ∑<sub><em class="EmphasisTypeItalic ">i</em> ∈ <em class="EmphasisTypeItalic ">I</em></sub><em class="EmphasisTypeItalic ">A</em><sub><em class="EmphasisTypeItalic ">i</em></sub> ∗ <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">i</em></sub>) of different embeddings <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">i</em></sub>) of tokens <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">i</em></sub> yield better results than single embeddings. A controller samples a subset <em class="EmphasisTypeItalic ">I</em> from a set of eight embeddings (e.g. BERT<sub>BASE</sub>, GloVe, fastText, etc.) and a NER model is trained and returns an accuracy score. The accuracy is treated as a reward signal in a reinforcement setting using the policy gradient algorithm ([<span class="CitationRef"><a epub:type="biblioref" href="#CR112" role="doc-biblioref">112</a></span>]) to select an optimal subset <em class="EmphasisTypeItalic ">I</em>. As NER model a BiLSTM model (Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec6"><span class="RefSource">1.​6</span></a></span>) with a final CRF-layer was chosen. A CRF (Conditional Random Field)<span id="ITerm101"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR100" role="doc-biblioref">100</a></span>] is able to model the probabilistic relation between the tags in detail. The fine-tuned model reaches a <span class="EmphasisTypeSmallCaps ">Sota</span> F1-score of 94.6% for CoNLL-2003.</p><p class="Para" id="Par80"><strong class="EmphasisTypeBold ">KeBioLM</strong><span id="ITerm102"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR126" role="doc-biblioref">126</a></span>] is a biomedical pre-trained language model aiming to improve NER by including additional knowledge. The authors extract 660M entities from the <em class="EmphasisTypeItalic ">PubMed corpus</em><span id="ITerm103"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>] with abstracts of biomedical literature and link them to the UMLS knowledge base that contains more than 4M entities and their synonyms as well as relations. They train a variant of BERT on the PubMed data and explicitly generate embeddings for entities. Relation information is included by the TransE-mechanism (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec18"><span class="RefSource">3.​4.​1</span></a></span>). The joint loss function is a mixture of loss functions for masked language modeling, entity detection, and entity linking. The <em class="EmphasisTypeItalic ">JNLPBA benchmark</em><span id="ITerm104"/> contains 2000 PubMed abstracts with molecular biology-related entities. KeBioLM reaches a <span class="EmphasisTypeSmallCaps ">Sota</span> of 82.0% F1 on JNLPBA. This shows that pre-training on domain texts and the inclusion of additional knowledge can improve NER results.</p><p class="Para" id="Par81"><em class="EmphasisTypeItalic ">Retrieval</em><span id="ITerm105"/> is a way to enhance the context a PLM may use for NER. Wang et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR107" role="doc-biblioref">107</a></span>] query a search engine with the input text that should be tagged. They rank (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec22"><span class="RefSource">3.​4.​5</span></a></span>) the returned results by the similarity of RoBERTa embeddings and concatenate the top ranked results and the input text. This is fed into a variant of RoBERTa to generate token embeddings. As the model can exploit the attention to the retrieved texts, the generated embeddings are potentially more expressive. The results on CoNLL 2003 indicate that retrieval can increase the F1-value about 0.5% and could be combined with current <span class="EmphasisTypeSmallCaps ">Sota</span>-models.</p></section>
<section class="Section2 RenderAsSection2" id="Sec14"><h3 class="Heading"><span class="HeadingNumber">5.3.2 </span>Nested Named Entity Recognition</h3><p class="Para" id="Par82">Often named entities have an internal structure. An example for such <em class="EmphasisTypeItalic ">nested entities</em><span id="ITerm106"/> is the sentence <em class="EmphasisTypeItalic ">“Last night, the [[Chinese]</em><sub><em class="EmphasisTypeItalic ">gpe</em></sub><em class="EmphasisTypeItalic ">embassy in [France]</em><sub><em class="EmphasisTypeItalic ">gpe</em></sub><em class="EmphasisTypeItalic ">]</em><sub><em class="EmphasisTypeItalic ">facility</em></sub><em class="EmphasisTypeItalic ">was closed.”</em> In this case a single token may have several entity tags and the NER task has to be formulated differently.</p><p class="Para" id="Par83"><strong class="EmphasisTypeBold ">MRC</strong><span id="ITerm107"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>] treats nested NER as a question-answering task. For example, the extraction of entities with a “location” label is formalized as the question: <em class="EmphasisTypeItalic ">“Which locations are mentioned in the text?”</em> The questions are formulated using templates that reflect the annotation guidelines. When these questions are answered for each entity type, overlapping named entities can be detected. MRC uses BERT’s span prediction approach (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec5"><span class="RefSource">2.​1.​3</span></a></span>) to mark the beginning and end of spans in the token sequence for an entity type. In addition, MRC predicts the start and the end of each entity to allow that there are overlapping entities of the same type.</p><p class="Para" id="Par84">Nested entities are common in the medical domain. The <em class="EmphasisTypeItalic ">Genia Corpus</em><span id="ITerm108"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>] contains entity annotations for proteins, viruses, DNA, RNA and many more, with 17% of the entities being nested. MRC achieves a <span class="EmphasisTypeSmallCaps ">Sota</span> of 83.8% F1 on the Genia benchmark. The ACE-2005 benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR104" role="doc-biblioref">104</a></span>] contain diverse nested entities like persons, facilities, or vehicles with an overlap of 22%. MRC reached an F1-value of 86.9% for ACE-2005. A similar approach [<span class="CitationRef"><a epub:type="biblioref" href="#CR125" role="doc-biblioref">125</a></span>] also predicts spans of different entities and yields 85.4% for ACE-2005. A two-stage algorithm called Locate and Label is proposed by Shen et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR93" role="doc-biblioref">93</a></span>], who first extract candidate entities and then categorize them in a second step. They yield 86.7% for the nested NER on ACE-2005 using BERT or one of its variants.</p><p class="Para" id="Par85">Instead of using a BERT model pre-trained on general documents, <strong class="EmphasisTypeBold ">PubMedBERT</strong><span id="ITerm109"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR102" role="doc-biblioref">102</a></span>] pre-trains its BERT model with 100M parameters exclusively on 21 GB medical texts from PubMed. PubMedBERT achieves 86.3% F1 for NER on the <em class="EmphasisTypeItalic ">BLURB benchmark</em><span id="ITerm110"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>]. The model also yields <span class="EmphasisTypeSmallCaps ">Sota</span> scores for other task like classification and relation extraction summarized in an average score of 82.9%. This result strongly supports pre-training on domain-specific data. <strong class="EmphasisTypeBold ">BioELECTRA</strong><span id="ITerm111"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>] is a biomedical domain-specific language encoder model that adapts ELECTRA (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>) for the Biomedical domain. ELECTRA employs a sample-efficient ‘replaced token detection’ technique for pre-training, which causes the model to include an enormous amount of information from the training data. BioELECTRA is pre-trained on PubMed and PubMed Central full-text medical articles. For NER, it arrives at the best score with 86.7% F1-value on the BLURB benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>]. The model also yields a similar score of 82.6% as PubMedBERT for the other BLURB tasks.</p><section class="Section3 RenderAsSection3" id="Sec15"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par86"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par87">BERT<sub>LARGE</sub> for token classification <span class="ExternalRef"><a href="https://huggingface.co/transformers/model_doc/model_doc/bert.html"><span class="RefSource">https://​huggingface.​co/​transformers/​model_​doc/​model_​doc/​bert.​html</span></a></span>,</p></li><li><p class="Para" id="Par88">Luke <span class="ExternalRef"><a href="https://huggingface.co/transformers/model_doc/model_doc/luke.html"><span class="RefSource">https://​huggingface.​co/​transformers/​model_​doc/​model_​doc/​luke.​html</span></a></span></p></li><li><p class="Para" id="Par89">ACE <span class="ExternalRef"><a href="https://github.com/Alibaba-NLP/ACE"><span class="RefSource">https://​github.​com/​Alibaba-NLP/​ACE</span></a></span>,</p></li><li><p class="Para" id="Par90">MRC <span class="ExternalRef"><a href="https://github.com/ShannonAI/mrc-for-flat-nested-ner"><span class="RefSource">https://​github.​com/​ShannonAI/​mrc-for-flat-nested-ner</span></a></span></p></li><li><p class="Para" id="Par91">Locate and Label [<span class="CitationRef"><a epub:type="biblioref" href="#CR93" role="doc-biblioref">93</a></span>] <span class="ExternalRef"><a href="https://github.com/tricktreat/locate-and-label"><span class="RefSource">https://​github.​com/​tricktreat/​locate-and-label</span></a></span></p></li><li><p class="Para" id="Par92">Bioelectra for nested NER <span class="ExternalRef"><a href="https://github.com/kamalkraj/BioELECTRA"><span class="RefSource">https://​github.​com/​kamalkraj/​BioELECTRA</span></a></span></p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec16"><h3 class="Heading"><span class="HeadingNumber">5.3.3 </span>Entity Linking</h3><p class="Para" id="Par93">After identifying a named entity in a text (<em class="EmphasisTypeItalic ">entity mention</em><span id="ITerm112"/>), one often wants to disambiguate it, i.e. assign the mention to a unique entity in a KB or ontology. This involves unifying different writings of an entity name. To attach the corresponding facts and relation to the same entity, it is important to link the different writings of a name, e.g. <em class="EmphasisTypeItalic ">“Joe Biden was elected as 46th president of the United States of America”</em> and <em class="EmphasisTypeItalic ">“President Biden was born in Scranton Pennsylvania”</em>. Note that there exist about 35 writings for the name <em class="EmphasisTypeItalic ">“Muammar Muhammad Abu Minyar al-Gaddafi”</em>, e.g. <em class="EmphasisTypeItalic ">“Qadhafi”</em>, <em class="EmphasisTypeItalic ">“Gaddafi”</em> and <em class="EmphasisTypeItalic ">“Gadhafi”</em> in addition to versions with the different first names. <em class="EmphasisTypeItalic ">Entity Linking</em><span id="ITerm113"/> approaches aim to solve this problem.</p><div class="Para" id="Par94">Entity linking is useful for tasks such as knowledge base population, chatbots, recommender systems, and question answering to identify the correct object or entity referred to. It is also required as a preprocessing step for models that need the entity identity, such as KnowBERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR80" role="doc-biblioref">80</a></span>] or ERNIE [<span class="CitationRef"><a epub:type="biblioref" href="#CR99" role="doc-biblioref">99</a></span>] (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec18"><span class="RefSource">3.​4.​1</span></a></span>). Early approaches rely on semantic embeddings to match entity mentions belonging together [<span class="CitationRef"><a epub:type="biblioref" href="#CR82" role="doc-biblioref">82</a></span>]. Modern procedures use contextual embeddings to characterize the entity mentions. Sevgili et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>] provide a comprehensive survey of Deep Learning based entity linking approaches. They sketch the general solution architecture of entity linking approaches as shown in Fig. <span class="InternalRef"><a href="#Fig3">5.3</a></span> and compare different methods.<figure class="Figure" id="Fig3"><div class="MediaObject" id="MO3"><img alt="" aria-describedby="d64e3411" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Fig3_HTML.png" style="width:34.38em"/><div class="TextObject" id="d64e3411"><p class="Para" id="Par158">A block diagram of various entities includes the following steps. Input text, named entity recognition, entity mention, candidate generation from K B, candidates, embeddings, entity ranking model, and entity with the highest score.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.3</span><p class="SimplePara">Entity Linking includes the three steps entity recognition, which identifies entity mentions in a text, candidate generation generating possible entities for the mention using the KB, and entity ranking, computing a similarity score between the candidates and the mention. Image adapted from [<span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>], reprinted with kind permission of authors</p></div></figcaption></figure></div><p class="Para" id="Par95"><strong class="EmphasisTypeBold ">BLINK</strong><span id="ITerm114"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR113" role="doc-biblioref">113</a></span>] follows the scheme of Fig. <span class="InternalRef"><a href="#Fig3">5.3</a></span>. First entity mentions together with their types are extracted from a text by NER. Then it uses a BERT model to compute embeddings for mention contexts and the entity descriptions in the KB. This also involves the normalization of entity names. Using an efficient approximate <em class="EmphasisTypeItalic ">k</em>-nearest-neighbor indexing scheme FAISS [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>] for embeddings (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec5"><span class="RefSource">6.​1.​4</span></a></span>). FAISS is able to retrieve the best matching entity candidates from the KB with little computational effort. This approach is identical to dense retrieval by DPR (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec22"><span class="RefSource">3.​4.​5</span></a></span>). Each retrieved candidate is then examined more carefully with a cross-encoder that concatenates the input context, the mention and entity text and assigns a score to each candidate entity. Finally, the candidate with the highest score is selected. Although no explicit entity embeddings are computed, the approach achieves <span class="EmphasisTypeSmallCaps ">Sota</span> on the <em class="EmphasisTypeItalic ">TACKBP-2010 benchmark</em><span id="ITerm115"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>] with an accuracy of 94.5%. A very similar approach is chosen by <strong class="EmphasisTypeBold ">EntQA</strong><span id="ITerm116"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR130" role="doc-biblioref">130</a></span>], which also exploits a retriever-reader architecture and yields competitive results on several benchmarks.</p><p class="Para" id="Par96"><strong class="EmphasisTypeBold ">GENRE</strong><span id="ITerm117"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>] departs from the common solution architecture to most entity linking approaches and uses the encoder-decoder model BART (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec4"><span class="RefSource">3.​1.​3</span></a></span>) to disambiguate entities. This model has to recover text corrupted by a number of different approaches during pre-training and therefore gathers a lot of knowledge about language. The model is fine-tuned to generate disambiguated named entities. For example, the sentence <em class="EmphasisTypeItalic ">“In 1503, Leonardo began painting the Mona Lisa.”</em> is translated to <em class="EmphasisTypeItalic ">“In 1503, [Leonardo](Leonardo da Vinci) began painting the [Mona Lisa](Mona Lisa).”</em>, where <em class="EmphasisTypeItalic ">“[Leonardo](Leonardo da Vinci)”</em> and <em class="EmphasisTypeItalic ">“[Mona Lisa](Mona Lisa)”</em> are the unique headings of the corresponding articles in Wikipedia. GENRE uses a constrained BEAM search for decoding, which either copies the input text or generates a unique Wikipedia entity name. In addition, GENRE can perform mention detection and end-to-end entity linking by associating a mention with the corresponding KB entity (e.g. the Wikipedia article). On six different benchmarks, GENRE achieves an average F1-value of 88.8% and outperforming BLINK, which scores 77.0%. In addition, GENRE has a smaller memory footprint (2.1 GB) than BLINK (30.1 GB). Finally, the model has a tendency to copy the mention exactly, which is helpful for new, unseen named entities.</p><div class="Para" id="Par97"><strong class="EmphasisTypeBold ">EntMask</strong><span id="ITerm118"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR118" role="doc-biblioref">118</a></span>] is similar to LUKE (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec21"><span class="RefSource">3.​4.​4</span></a></span>) and learns to predict masked entities. To disambiguate new mentions, the authors use local contextual information based on words, and global contextual information based on already disambiguated entities. Their model is trained to jointly produce embeddings of words and entities and is also based on BERT<sub>LARGE</sub>. For fine-tuning 30% entities corresponding to Wikipedia hyperlinks are masked randomly and have to be predicted as shown in Fig. <span class="InternalRef"><a href="#Fig4">5.4</a></span>. During application the model predicts an entity for each mention, and from the unresolved mentions actually assigns the mention with the highest probability as ‘observed’. In this way, this assignment can influence the prediction for the remaining mentions, introducing a global perspective. On a number of benchmarks the approach yields roughly similar results to GENRE, with a small advantage on a few benchmarks.<figure class="Figure" id="Fig4"><div class="MediaObject" id="MO4"><img alt="" aria-describedby="d64e3531" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Fig4_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e3531"><p class="Para" id="Par159">A flow diagram has an input of text tokens and unique K B entities with token types, linked to the input embeddings, autoencoder with self-attentions, output embeddings, 2-layer logistic classifier, and entity probabilities to give the correct entity.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.4</span><p class="SimplePara">BERT<sub>LARGE</sub> can be fine-tuned to predict masked ‘entity tokens’ taking into account the corresponding text. During application successively the entities with highest probability are assigned. In this way, the joint probability of entities can be exploited [<span class="CitationRef"><a epub:type="biblioref" href="#CR118" role="doc-biblioref">118</a></span>]</p></div></figcaption></figure></div><section class="Section3 RenderAsSection3" id="Sec17"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par98"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par99">GENRE: model source code and datasets from Facebook <span class="ExternalRef"><a href="https://github.com/facebookresearch/GENRE"><span class="RefSource">https://​github.​com/​facebookresearch​/​GENRE</span></a></span></p></li><li><p class="Para" id="Par100">BLINK available at <span class="ExternalRef"><a href="https://github.com/facebookresearch/BLINK"><span class="RefSource">https://​github.​com/​facebookresearch​/​BLINK</span></a></span></p></li><li><p class="Para" id="Par101">EntMask code: <span class="ExternalRef"><a href="https://github.com/studio-ousia/luke"><span class="RefSource">https://​github.​com/​studio-ousia/​luke</span></a></span>.</p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec18"><h3 class="Heading"><span class="HeadingNumber">5.3.4 </span>Summary</h3><p class="Para" id="Par102">It is well known that named entities play a crucial role in understanding the meaning of a text. Thousands of new named entities appear every day, requiring special effort to interpret their sense. Due to the availability of contextual embeddings in PLMs Named Entity Recognition (NER) could increase F1-value on the CoNLL 2003 benchmark from 85% to 94.6%, dramatically reducing errors. The standard approach is token annotation by BERT, which marks each token with its corresponding entity type. Higher performance can be achieved by treating named entities as special tokens (LUKE), combining different kinds of embeddings (ACE), or using retrieval approaches based on embeddings. Empirical evaluations demonstrate that it is extremely important to train the underlying PLM on domain texts, e.g. from the medical domain. Single tokens or compounds can belong to multiple entity types at the same time. For this, nested NER question-answering approaches can be used to mark token spans as belonging to an entity type. Again training on domain texts is essential.</p><p class="Para" id="Par103">In Sect. <span class="InternalRef"><a href="#Sec24">5.4.4</a></span> approaches for joint entity and relation extraction are presented. The approaches described there can also be used for NER alone and promise high performance. An example is REBEL, which uses the BART encoder-decoder to translate the input sentence to a unique representation of the covered entities and relations.</p><p class="Para" id="Par104">Entity linking aims to map an entity mention to the underlying unique entity in a KB. One approach exploits the retriever-reader architecture to find entity candidates from a knowledge base (BLINK, EntQA). Subsequently, a reader module scrutinizes candidates and the mention to arrive at a final assignment. An alternative is GENRE’s encoder-decoder architecture, which translates entity mentions to unique entity names. Finally, a BERT model can determine self-attentions between token embeddings and entity embeddings and exploit this to predict unique entities contained in a text.</p><p class="Para" id="Par105">The majority of entity linking models still rely on external knowledge like Wikipedia for the candidate generation step. However, this is not sufficient when identifying a person who is not a celebrity. In this case we have to perform a search in the web or social media to find information. As retrieval-reader approaches gain popularity, this may be possible in the future. It turns out that NER and entity linking should be performed jointly, i.e. assignments should take into account each other to increase accuracy.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec19"><h2 class="Heading"><span class="HeadingNumber">5.4 </span>Relation Extraction</h2><p class="Para" id="Par106">After identifying relevant entities in a sentence, a crucial part of information extraction is often the extraction and classification of relations between these entities. This is useful, for example, when we automatically want to populate databases or knowledge graphs with linked information. Table <span class="InternalRef"><a href="#Tab3">5.3</a></span> contains examples of language analysis tasks based on relation extraction that are discussed in this section. Instances include coreference resolution, i.e. finding different mentions of an entity in the same text, aspect-based sentiment analysis, which links phrases in a text to opinions about them, or semantic role labeling, which identifies the function of a phrase for a predicate in a sentence. Because entity linking associates mentions of entities with the underlying unique object or person in an ontology, it differs from relation extraction. A survey on prior work in relation extraction is given by Nasar et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>].</p><section class="Section2 RenderAsSection2" id="Sec20"><h3 class="Heading"><span class="HeadingNumber">5.4.1 </span>Coreference Resolution</h3><p class="Para" id="Par107">A first type of relation extraction is <em class="EmphasisTypeItalic ">coreference resolution</em><span id="ITerm119"/>, whose goal is to establish a relation between all entity mentions in a text that refer to the same real-world entities. As an example, consider the sentence <em class="EmphasisTypeItalic ">“I voted for Biden because he was most aligned with my values”, she said.</em> where <em class="EmphasisTypeItalic ">“I”</em>, <em class="EmphasisTypeItalic ">“my”</em>, and <em class="EmphasisTypeItalic ">“she”</em> refer to the speaker, and <em class="EmphasisTypeItalic ">“Biden”</em> and <em class="EmphasisTypeItalic ">“he”</em> pertain to Joe Biden. Due to the combinatorial number of subsets of related phrases, coreference analysis is one of the most challenging tasks of NLP. A survey of coreference resolution is provided by Stylianou et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR98" role="doc-biblioref">98</a></span>].</p><p class="Para" id="Par108"><strong class="EmphasisTypeBold ">SpanBERT</strong><span id="ITerm120"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>] is a version of BERT, which predicts contiguous subsequences of masked tokens during pre-training, and therefore accumulates knowledge about spans of words (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>). The authors consider all possible spans of text and identify relevant mentions spans. In parallel, for each span <em class="EmphasisTypeItalic ">x</em>, the preceding spans <em class="EmphasisTypeItalic ">y</em> are examined, and a scoring function estimates whether the spans refer to the same entity.</p><p class="Para" id="Par109">This scoring function is defined as <em class="EmphasisTypeItalic ">s</em>(<em class="EmphasisTypeItalic ">x</em>, <em class="EmphasisTypeItalic ">y</em>) = <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">m</em></sub>(<em class="EmphasisTypeItalic ">x</em>) + <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">m</em></sub>(<em class="EmphasisTypeItalic ">y</em>) + <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">c</em></sub>(<em class="EmphasisTypeItalic ">x</em>, <em class="EmphasisTypeItalic ">y</em>). Here <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">m</em></sub>(<em class="EmphasisTypeItalic ">x</em>) and <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">m</em></sub>(<em class="EmphasisTypeItalic ">y</em>) measure how likely <em class="EmphasisTypeItalic ">x</em> and <em class="EmphasisTypeItalic ">y</em> are entity mentions. <em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">c</em></sub>(<em class="EmphasisTypeItalic ">x</em>, <em class="EmphasisTypeItalic ">y</em>) determines how likely <em class="EmphasisTypeItalic ">x</em> and <em class="EmphasisTypeItalic ">y</em> refer to the same entity. As input from a span, the scoring function gets the output embeddings of the two span endpoints and a summary of the tokens embeddings of the span. The probability that <em class="EmphasisTypeItalic ">y</em> is coreferent to <em class="EmphasisTypeItalic ">x</em> is computed as <span class="InlineEquation" id="IEq21"><img alt="$$p(y)=\exp (s(x,y))/\sum _{y'\in Y} \exp (s(x,y'))$$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq21.png" style="width:17.62em"/></span>. In this way, subsets of spans mentioning the same entity are formed. During the iterations of the approach, the span definitions may be refined, and an antecedent pruning mechanism is applied to reduce the number of spans to be considered. <em class="EmphasisTypeItalic ">OntoNotes</em><span id="ITerm121"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>] is a corpus of 1.5M words comprising various genres of text with structural information, e.g. coreference. After fine-tuning on OntoNotes, Span-BERT achieves a <span class="EmphasisTypeSmallCaps ">Sota</span> result of 79.6% F1-value on the test set. Dobrovolskii [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>] propose a variant which performs its analysis on the word level thus reducing the complexity of the task. It raises the <span class="EmphasisTypeSmallCaps ">Sota</span> on OntoNotes to 81.0%.</p><p class="Para" id="Par110"><strong class="EmphasisTypeBold ">CorefQA</strong><span id="ITerm122"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR114" role="doc-biblioref">114</a></span>] solves coreference resolution as a question-answering problem. A first stage considers all spans up to a maximum length as potential mentions. The authors use a SpanBERT model to compute embeddings for all tokens. To reduce the number of mentions, a proposal module combining the start and end embeddings of spans is pre-trained to predict relevant mentions. Subsequently, each mention is in turn surrounded by special tokens and the network is trained to mark all coreferent spans similar to the question-answering fine-tuning of BERT (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec5"><span class="RefSource">2.​1.​3</span></a></span>). To reduce the number of computations only a limited number of candidates in one direction is considered. The mention proposal and mention clustering can be trained end-to-end. On the coreference benchmark CoNLL 2012 [<span class="CitationRef"><a epub:type="biblioref" href="#CR84" role="doc-biblioref">84</a></span>] the approach improves <span class="EmphasisTypeSmallCaps ">Sota</span> significantly to 83.1% F1-value. Toshniwal et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR103" role="doc-biblioref">103</a></span>] extend this approach by tracking only a small bounded number of entities at a time. This approach can reach a high accuracy in coreference resolution even for long documents.</p><section class="Section3 RenderAsSection3" id="Sec21"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par111"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par112">SpanBERT for relation extraction and coreference resolution at GitHub <span class="ExternalRef"><a href="https://github.com/facebookresearch/SpanBERT"><span class="RefSource">https://​github.​com/​facebookresearch​/​SpanBERT</span></a></span></p></li><li><p class="Para" id="Par113">CorefQA at GitHub <span class="ExternalRef"><a href="https://github.com/ShannonAI/CorefQA"><span class="RefSource">https://​github.​com/​ShannonAI/​CorefQA</span></a></span></p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec22"><h3 class="Heading"><span class="HeadingNumber">5.4.2 </span>Sentence-Level Relation Extraction</h3><p class="Para" id="Par114">There are various types of relations which can be extracted, e.g. in the sentence <em class="EmphasisTypeItalic ">“Goethe succumbed to his suffering in Weimar”</em> the <em class="EmphasisTypeItalic ">“died-in”</em> relation relates a person (<em class="EmphasisTypeItalic ">“Goethe”</em>) to a location (<em class="EmphasisTypeItalic ">“Weimar”</em>). In this section we assume that entities have already been extracted from a sentence by NER (Sect. <span class="InternalRef"><a href="#Sec12">5.3</a></span>). Therefore, NER errors will increase the errors for relation extraction.</p><p class="Para" id="Par115"><strong class="EmphasisTypeBold ">SpanBERT</strong><span id="ITerm123"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>] is particularly suitable for relation extraction, since entity mentions often span over multiple tokens, and are masked by SpanBERT during pre-training (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>). For fine-tuning the model gets one sentence and two spans with possible relation arguments as input, which are replaced by their NER tags. An example is <em class="EmphasisTypeItalic ">“[CLS] [SUBJ-PER] was born in [OBJ-LOC] , Michigan, . . .”</em>. The final [CLS] embedding is input to a logistic classifier, which predicts one of the 42 predefined relation types, including “no relation”. <em class="EmphasisTypeItalic ">Re-TACRED</em><span id="ITerm124"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR97" role="doc-biblioref">97</a></span>] is a large-scale relation extraction dataset with 120k examples covering 41 relation types (e.g., per:schools-attended and org:members) and carefully checked relation annotations. SpanBERT showed good performance on Re-TACRED with 85.3% F1-value [<span class="CitationRef"><a epub:type="biblioref" href="#CR95" role="doc-biblioref">95</a></span>].</p><p class="Para" id="Par116"><strong class="EmphasisTypeBold ">RoBERTa</strong><span id="ITerm125"/> (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>) can be used to generate token embeddings for relation extraction. Zhou et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR135" role="doc-biblioref">135</a></span>] evaluate various entity representation techniques. They use RoBERTa<sub>LARGE</sub> to encode the input text by embeddings of the last layer. The embeddings of the first token in each span of relation argument mentions are used to represent these arguments. These are concatenated and adopted as input for a softmax classifier. It turns out that enclosing an entity and adding its type with special tokens yields the best results on the Re-TACRED dataset with 91.1% F1-value.</p><p class="Para" id="Par117"><strong class="EmphasisTypeBold ">Relation-QA</strong><span id="ITerm126"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>] rephrase the relation classification problem into a question answering problem. Consider the sentence <em class="EmphasisTypeItalic ">s</em> =  <em class="EmphasisTypeItalic ">“Sam Brown was born in 1991.”</em> with the extracted entities <em class="EmphasisTypeItalic ">“Sam Brown”</em> and <em class="EmphasisTypeItalic ">“1991”</em>. Then the authors create two queries, such as <em class="EmphasisTypeItalic ">“When was Sam Brown born?”</em> and <em class="EmphasisTypeItalic ">“Who was born in 1991?”</em>. They fine-tune ALBERT (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>) to answer these queries by marking the spans containing the desired entity. If no span is returned the relation does not hold. The approach achieves an F1-value of 74.8% for TACRED, an older version of ReTACRED with many annotation problems. <strong class="EmphasisTypeBold ">RECENT</strong><span id="ITerm127"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>] extends SpanBERT and trains more than one relation classification model, i.e. one classifier for each different pair of entity types. This restricts the possible output relation types and helps to increase performance. On TACRED the approach yields a <span class="EmphasisTypeSmallCaps ">Sota</span> F1-value of 75.2%.</p></section>
<section class="Section2 RenderAsSection2" id="Sec23"><h3 class="Heading"><span class="HeadingNumber">5.4.3 </span>Document-Level Relation Extraction</h3><p class="Para" id="Par118">Especially for larger documents, the assumption that relations occur only inside a sentence is too restrictive. Therefore, some models check for relations on the document level. When relation arguments are in different sentences the corresponding entities are often only referred to via coreferent mentions. Therefore, we assume in this section that entities have been extracted and grouped into clusters denoting the same entity by coreference resolution (Sect. <span class="InternalRef"><a href="#Sec20">5.4.1</a></span>). Obviously the errors of coreference resolution will increase the final relation extraction errors.</p><p class="Para" id="Par119"><strong class="EmphasisTypeBold ">SSAN</strong><span id="ITerm128"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR115" role="doc-biblioref">115</a></span>] (Structured Self-Attention Network) directly takes into account structural information such as coreference and cooccurrence of entity mentions for PLMs such as RoBERTa. The authors modify the self-attention computations in encoder blocks by adding specific biases, if two mentions refer to the same entity and/or are located in the same sentence. These biases are computed from the query and key vectors by a “transformation model” trained during fine-tuning. Therefore, the scalar products between keys and queries are modified depending on whether the corresponding tokens are coreferent, in the same sentence, or not. Entity embeddings are obtained via average pooling of token embeddings of the entity mention. For each pair <em class="EmphasisTypeItalic ">emb</em><sub><em class="EmphasisTypeItalic ">i</em></sub>, <em class="EmphasisTypeItalic ">emb</em><sub><em class="EmphasisTypeItalic ">j</em></sub> of entity embeddings the probability of a relation <em class="EmphasisTypeItalic ">r</em> is computed by a bilinear transformation <span class="InlineEquation" id="IEq22"><img alt="$$ \operatorname {\mathrm {sigmoid}}({emb}_i^\intercal W_r {emb}_j)$$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq22.png" style="width:10.07em"/></span> with a trainable parameter matrix <em class="EmphasisTypeItalic ">W</em><sub><em class="EmphasisTypeItalic ">r</em></sub>.</p><p class="Para" id="Par120"><em class="EmphasisTypeItalic ">DocRED</em><span id="ITerm129"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR121" role="doc-biblioref">121</a></span>] is a large benchmark of documents annotated with named entities, coreferences, and relations whose arguments may be located in different sentences. Using RoBERTa<sub>LARGE</sub> as base network, the authors achieve a <span class="EmphasisTypeSmallCaps ">Sota</span> of 65.9% F1 on DocRED. Using a special BERT version SciBERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>] trained on scientific papers from Semantic Scholar, the algorithm also yields <span class="EmphasisTypeSmallCaps ">Sota</span> results for benchmarks with chemical as well as biological texts.</p><p class="Para" id="Par121"><strong class="EmphasisTypeBold ">ATLOP</strong><span id="ITerm130"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR136" role="doc-biblioref">136</a></span>] marks the start and end of a mentions by a special token and encodes a document by BERT resulting in embeddings for each token. The embedding of token at the mention start is used as the mention embeddings. An entity embedding is computed by pooling coreferent mentions. The first and the second argument entity embedding of a relation are transformed by different fully connected layers to <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub>1</sub> and <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub>2</sub>. Subsequently, the probability of a relation <em class="EmphasisTypeItalic ">r</em> for an entity pair is estimated by a sparse bilinear transformation <span class="InlineEquation" id="IEq23"><img alt="$$ \operatorname {\mathrm {sigmoid}}({\boldsymbol {x}}_1^\intercal W {\boldsymbol {x}}_2)$$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq23.png" style="width:7.56em"/></span>. Trainable probability thresholds are used to decide if a relation holds. On the DocRED benchmark the model achieves an F1-value of 63.4%.</p></section>
<section class="Section2 RenderAsSection2" id="Sec24"><h3 class="Heading"><span class="HeadingNumber">5.4.4 </span>Joint Entity and Relation Extraction</h3><p class="Para" id="Par122">Since NER and relation extraction are closely related tasks and relation extraction depends on the results of NER, it is a natural choice to model these tasks jointly.</p><p class="Para" id="Par123"><strong class="EmphasisTypeBold ">UniRE</strong><span id="ITerm131"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR108" role="doc-biblioref">108</a></span>] encodes entity and relation properties in a joint matrix, which has a row and a column for each text token. While named entities, e.g. PER, are marked on the diagonal, relations are matrix entries off-diagonal. If, for example, <em class="EmphasisTypeItalic ">“David Perkins”</em> lives in <em class="EmphasisTypeItalic ">“California”</em> the matrix entries in the rows of the <em class="EmphasisTypeItalic ">“David Perkins”</em> tokens and the columns of the <em class="EmphasisTypeItalic ">“California”</em> tokens are marked with the <em class="EmphasisTypeItalic ">PHY</em> <em class="EmphasisTypeItalic ">S</em> relation. Note that in this way asymmetric relations may be specified.</p><div class="Para" id="Par124">All words in the input are encoded using a BERT encoder and then a biaffine model is used to create a scoring vector for a pair <em class="EmphasisTypeItalic ">h</em><sub><em class="EmphasisTypeItalic ">i</em></sub> and <em class="EmphasisTypeItalic ">h</em><sub><em class="EmphasisTypeItalic ">j</em></sub> of embeddings <div class="Equation NumberedEquation" id="Equ2"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} p(y_{i,j}|s) = \operatorname{\mathrm{softmax}}\left( ({\boldsymbol{h}}_i^{first})^\intercal U_1 {\boldsymbol{h}}_j^{sec} + U_2 \lbrack{\boldsymbol{h}}_i^{first},{\boldsymbol{h}}_j^{sec}\rbrack +b \right), \end{aligned} $$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_Equ2.png" style="width:25.99em"/></div></div> <div class="EquationNumber">(5.2)</div></div></div> where <span class="InlineEquation" id="IEq24"><img alt="$${\boldsymbol {h}}_i^{first}=\text{FCL}_{first}({\boldsymbol {h}}_i)$$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq24.png" style="width:9.32em"/></span> and <span class="InlineEquation" id="IEq25"><img alt="$${\boldsymbol {h}}_i^{sec}=\text{FCL}_{sec}({\boldsymbol {h}}_i)$$" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq25.png" style="width:8em"/></span> are fully connected layer transformations of the first and second relation argument respectively. The softmax function obtains a probability distribution over the entity and relation labels for all matrix cells. The model minimizes three losses, one based on the actual labels of each cell, one based on the knowledge that diagonal of entity labels should be symmetrical and one based on the fact that a relation label implies that respective entity labels must be present. <em class="EmphasisTypeItalic ">ACE 2005</em><span id="ITerm132"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR104" role="doc-biblioref">104</a></span>] consists of text of various types annotated for entities, relations and events. On ACE 2005 UniRE yields an F1-value of 66.0% for joint entity and relation extraction, which is less than the current <span class="EmphasisTypeSmallCaps ">Sota</span> of 70.5%.</div><div class="Para" id="Par125"><strong class="EmphasisTypeBold ">PL-Marker</strong><span id="ITerm133"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR122" role="doc-biblioref">122</a></span>] investigate different types of mention encodings. For a possible relation it surrounds the first argument span (subject) by solid marker tokens. The possible second argument spans (objects) are marked by <em class="EmphasisTypeItalic ">leviated tokens</em><span id="ITerm134"/><em class="EmphasisTypeItalic ">Oi</em> and ∕<em class="EmphasisTypeItalic ">Oi</em> outside the text (Fig. <span class="InternalRef"><a href="#Fig5">5.5</a></span>). These get the same position embeddings as the corresponding object spans in the text. Their attention connections are restricted, i.e they are visible to each other, but not to the text token and other pairs of markers. Therefore, depending on the subject span the object token embeddings can capture different aspects. For each pair of subject-object arguments, the corresponding embeddings are concatenated and used as input to a logistic classifier to estimate the probability of the possible relations (or ‘no relation’). Pre-trained variants of BERT are fine-tuned with ACE 2005 to predict the relations. With a BERT<sub>BASE</sub> model of 105M parameters the approach yields an F1-value of 68.8% on the ACE05 benchmark. If ALBERT<sub>XXLARGE</sub> [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>] with 235M parameters is used to compute the embeddings, the F1-score grows to 72.3%.<figure class="Figure" id="Fig5"><div class="MediaObject" id="MO5"><img alt="" aria-describedby="d64e4503" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Fig5_HTML.png" style="width:34.38em"/><div class="TextObject" id="d64e4503"><p class="Para" id="Par160">A flow diagram of input tokens and their positions, linked to the input embeddings, autoencoder with self-attentions, output embeddings, 2-layer logistic classifier, and relation probabilities to give the observed relation.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.5</span><p class="SimplePara">For a possible relation the PL-marker model marks the first relation argument by special ‘solid’ markers and the possible second arguments by ‘leviated’ markers outside the text. The latter get the same positions as the corresponding tokens, and do not influence the embeddings of normal tokens during attention computation. The marker embeddings are concatenated to compute the probability of the corresponding relation [<span class="CitationRef"><a epub:type="biblioref" href="#CR122" role="doc-biblioref">122</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par126">For NER, the PL-Marker model uses a similar approach. For each possible span in the input starting at token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">i</em></sub> and ending at token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">j</em>,<em class="EmphasisTypeItalic ">j</em>≥<em class="EmphasisTypeItalic ">i</em></sub>, leviated markers are created, which do not affect the embeddings of the normal tokens. Again the embeddings of the start and end tokens of a span as well as the embeddings of leviated markers are input for a logistic classifier computing the probability of the different NE-types. The model uses an efficient ‘packing’ to reduce computational effort. On the CoNLL03 named entity benchmark, PL-markers with a pre-trained RoBERTa<sub>LARGE</sub> achieve an F1-value of 94.0, which is well below the current <span class="EmphasisTypeSmallCaps ">Sota</span> of 96.1% held by DeBERTa [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>]. When the relation extraction employs the entity types and spans predicted by the PL-MARKER NER, the F1-value of the joint approach drops to 70.5%, which is <span class="EmphasisTypeSmallCaps ">Sota</span> for the ACE05 benchmark on joint NER and relation extraction.</p><div class="Para" id="Par127"><strong class="EmphasisTypeBold ">REBEL</strong><span id="ITerm135"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>] uses the encoder-decoder transformer BART<sub>LARGE</sub> (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec4"><span class="RefSource">3.​1.​3</span></a></span>) for joint entity and relation extraction that outputs each relation (<em class="EmphasisTypeItalic ">h</em>, <em class="EmphasisTypeItalic ">r</em>, <em class="EmphasisTypeItalic ">t</em>) triplet present in the input text. It translates a raw input sentence containing entities, together with implicit relations between them, into a set of triplets that explicitly refer to those relations. An example is shown in Fig. <span class="InternalRef"><a href="#Fig6">5.6</a></span>. Each relation in the text appears in the output according to the position of its first argument. An entity may be part of different relations, which are ordered according to the position of the second argument. This defines the order of relations in the linearized representation.<figure class="Figure" id="Fig6"><div class="MediaObject" id="MO6"><img alt="" aria-describedby="d64e4582" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Fig6_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e4582"><p class="Para" id="Par161">A block diagram of the input text and relation triples gives the linearized representation for the song titled this must be the place by the new wave band talking heads from their album speaking in tongues.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.6</span><p class="SimplePara">For the training set the relation information on the left side is linearized to the representation on the right side. The REBEL model thus learns to translate the input text to this linearized representation [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par128">The pre-trained BART<sub>LARGE</sub> with 400M parameters is first fine-tuned on a Wikipedia and WikiData training set with 220 relation types. Then it is fine-tuned a second time on varying benchmark datasets. On the <em class="EmphasisTypeItalic ">DocRED benchmark</em><span id="ITerm136"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR121" role="doc-biblioref">121</a></span>] it achieves <span class="EmphasisTypeSmallCaps ">Sota</span> with an F-value of 47.1%. On the <em class="EmphasisTypeItalic ">New York Times dataset</em><span id="ITerm137"/> it has a <span class="EmphasisTypeSmallCaps ">Sota</span> performance with 93.4% F1. On the ReTACRED benchmark it yields 90.4% F1 without the inclusion of entity type markers used by other approaches.</p><section class="Section3 RenderAsSection3" id="Sec25"><h4 class="Heading">Aspect-Based Sentiment Analysis</h4><p class="Para" id="Par129"><em class="EmphasisTypeItalic ">Aspect-based sentiment analysis</em><span id="ITerm138"/>, also known as aspect-level sentiment analysis, feature-based sentiment analysis, or simply, aspect sentiment analysis, allows organizations to perform a detailed analysis of their member or customer feedback data. This ranges from analyzing customer reactions for a restaurant to evaluating the attitude to political statements made by a politician. An example is <em class="EmphasisTypeItalic ">“The</em><em class="EmphasisTypeItalicUnderline ">waiter</em><sub><em class="EmphasisTypeItalic ">1-aspect</em></sub><em class="EmphasisTypeItalic ">was</em><em class="EmphasisTypeItalicUnderline ">very friendly</em><sub><em class="EmphasisTypeItalic ">1-positive</em></sub>,<em class="EmphasisTypeItalic ">but the</em><em class="EmphasisTypeItalicUnderline ">steak mignon</em><sub><em class="EmphasisTypeItalic ">2-aspect</em></sub><em class="EmphasisTypeItalic ">was</em><em class="EmphasisTypeItalicUnderline ">extremely burnt</em><sub><em class="EmphasisTypeItalic ">2-negative</em></sub>.<em class="EmphasisTypeItalic ">”</em> Note that a sentence may contain different aspects and each sentiment has to be assigned to one aspect. A recent survey of aspect-based sentiment analysis is given by Zhang et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR129" role="doc-biblioref">129</a></span>].</p><p class="Para" id="Par130"><strong class="EmphasisTypeBold ">DeBERTa</strong><span id="ITerm139"/> (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>) is a powerful BERT-like model, which assumes that the aspects are already known. It employs a disentangled attention mechanism for computing separate attention scores between words and positions disentangling semantic (content) and syntactic (position) representation of the textual data. The objective is to determine the sentiment of each aspect of a given entity. The input consist of a text and an aspect, e.g. <em class="EmphasisTypeItalic ">x</em> = <em class="EmphasisTypeItalic ">“[CLS] …nice video camera and keyboard …[SEP] keyboard [SEP]”</em>, where <em class="EmphasisTypeItalic ">“keyboard”</em> is a possible aspect span from the text [<span class="CitationRef"><a epub:type="biblioref" href="#CR94" role="doc-biblioref">94</a></span>]. The output embedding of <em class="EmphasisTypeItalic ">[CLS]</em> is used as input to a logistic classifier which generates the probabilities of three possible labels positive, negative, neutral. The model is fine-tuned on the <em class="EmphasisTypeItalic ">SemEval 2014 Task 4.2 benchmark</em><span id="ITerm140"/>. It yields a mean accuracy for the Restaurant and Laptop data of 86.1%. There are much more complex approaches like <strong class="EmphasisTypeBold ">LSA</strong><span id="ITerm141"/> (local sentiment aggregation) [<span class="CitationRef"><a epub:type="biblioref" href="#CR119" role="doc-biblioref">119</a></span>] achieving a <span class="EmphasisTypeSmallCaps ">Sota</span> of 88.6% on this benchmark.</p><p class="Para" id="Par131"><strong class="EmphasisTypeBold ">GRACE</strong><span id="ITerm142"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>] aims at extracting aspects and labels simultaneously. It consists of a first BERT<sub>BASE</sub> module generating token embeddings of the input text, which are fine-tuned to mark aspects by IOB2 tags for each token. The resulting information is fed into a Transformer decoder to predict the sentiments (positive, negative, neural) for each token. This decoder uses a multi-head cross attention to include the information from the first aspect module. Again for each token embedding in the last layer a logistic classifier is used to compute the probabilities of sentiments. To make the model more robust, small perturbations for input token embeddings are used during training. Note that no masked cross-attention is necessary as the decoder is not autoregressive. In this way, the model is able to take into account the interactions between aspect terms when labeling sentiments. The model achieves 87.9% F1 score for aspect extraction for the laptop reviews from SemEval 2014 and a <span class="EmphasisTypeSmallCaps ">Sota</span> of 70.7% F1-value for the joint extraction of aspects and sentiments. On the restaurant reviews it yields an F1 of 78.1% and on a tweet benchmark 58.3% for joint sentiment extraction, again outperforming a number of other models.</p></section>
<section class="Section3 RenderAsSection3" id="Sec26"><h4 class="Heading">Semantic Role Labeling</h4><p class="Para" id="Par132">Semantic role labeling considers a predicate (e.g. verb) of a sentence and word phrases are classified according to their syntactic roles, such as agent, goal, or result. It can be used to determine the meaning of the sentence. As an example consider the sentence <em class="EmphasisTypeItalic ">“They want to do more .”</em> where <em class="EmphasisTypeItalic ">“want”</em> is the predicate, <em class="EmphasisTypeItalic ">“They”</em> is the agent and <em class="EmphasisTypeItalic ">“to do more”</em> is the object (thing wanted).</p><p class="Para" id="Par133"><strong class="EmphasisTypeBold ">Crf2o</strong><span id="ITerm143"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR133" role="doc-biblioref">133</a></span>] is a tree-structured conditional random field (treecrf) [<span class="CitationRef"><a epub:type="biblioref" href="#CR28" role="doc-biblioref">28</a></span>] using contextual embeddings of the input tokens computed by RoBERTa as input. The sequence <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> = (<em class="EmphasisTypeItalic ">x</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">T</em></sub>) of inputs can be arranged in a tree <em><strong class="EmphasisTypeBoldItalic ">y</strong></em> and gets a score, which is the sum of all scores of its subtrees <em class="EmphasisTypeItalic ">s</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>, <em><strong class="EmphasisTypeBoldItalic ">y</strong></em>) =∑<sub><em class="EmphasisTypeItalic ">t</em> ∈<em><strong class="EmphasisTypeBoldItalic ">y</strong></em></sub><em class="EmphasisTypeItalic ">s</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>, <em class="EmphasisTypeItalic ">t</em>). Similar to dependency parsing, this can be used to model the dependency of phrases from the predicate in semantic role labeling [<span class="CitationRef"><a epub:type="biblioref" href="#CR87" role="doc-biblioref">87</a></span>]. To generate all possible subtrees requires <em class="EmphasisTypeItalic ">T</em><sup>3</sup> operations, which is very inefficient. The authors were able to reduce this effort using structural constraints. In addition, they could take into account the dependency between two branches of the tree, which generated a second order tree. During training the models maximize the probability of the provided tree structure of the training data for an input. <em class="EmphasisTypeItalic ">CoNLL05</em><span id="ITerm144"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>] and <em class="EmphasisTypeItalic ">OntoNotes</em><span id="ITerm145"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR84" role="doc-biblioref">84</a></span>] are two widely used benchmarks for semantic role labeling. For CoNLL05 the Crf2o yields an F1-value of 89.6% and for OntoNotes it achieves an F1-value of 88.3%, which both constitute a new <span class="EmphasisTypeSmallCaps ">Sota</span>. Note that this technique may also be used for <em class="EmphasisTypeItalic ">dependency parsing</em><span id="ITerm146"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR132" role="doc-biblioref">132</a></span>], which describes the syntactic structure of a sentence by a tree structure.</p></section>
<section class="Section3 RenderAsSection3" id="Sec27"><h4 class="Heading">Extracting Knowledge Graphs from Pre-trained PLMs</h4><p class="Para" id="Par134">A systematic way to extract knowledge from big language models has been demonstrated by Wang et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR105" role="doc-biblioref">105</a></span>]. Their <strong class="EmphasisTypeBold ">MaMa</strong><span id="ITerm147"/> approach consist of a match stage and a map stage. The match stage generates a set of candidate facts from the text collection exploiting the internal knowledge of a language model. Similar to TransE (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec18"><span class="RefSource">3.​4.​1</span></a></span>) each fact is represented as a relation triple (head, relation, tail), or (<em class="EmphasisTypeItalic ">h</em>, <em class="EmphasisTypeItalic ">r</em>, <em class="EmphasisTypeItalic ">t</em>). A language model is used to generate tokens corresponding to <em class="EmphasisTypeItalic ">r</em> or <em class="EmphasisTypeItalic ">t</em>. As a condition, the <em class="EmphasisTypeItalic ">r</em> values should be contiguous text sequences and express frequent relations.</p><p class="Para" id="Par135">In the map stage the triples are mapped to related triples with appropriate relations. As an example (Dylan, is, songwriter) is mapped to (Bob Dylan.Q392, occupation.P106, Songwriter.Q753110) according to the Wikidata schema. This stage is related to entity linking discussed in Sect. <span class="InternalRef"><a href="#Sec16">5.3.3</a></span>. The reason for mapping to an existing KG schema is to make use of the high-quality schema designed by experts.</p><div class="Para" id="Par136">A subgraph of the generated relations is shown in Fig. <span class="InternalRef"><a href="#Fig7">5.7</a></span>. Compared to the <span class="EmphasisTypeSmallCaps ">Sota</span> information extraction system Stanford OpenIE [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>] with 27.1% F1-value the approach yields 29.7% F1-value. The authors report that performance increases with model size because larger models can store more knowledge.<figure class="Figure" id="Fig7"><div class="MediaObject" id="MO7"><img alt="" aria-describedby="d64e4910" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Fig7_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e4910"><p class="Para" id="Par162">An interconnected network connects the central node of Bob underscore Dylan dot Q 392 with various music genres, places, awards, and other artists.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.7</span><p class="SimplePara">A snapshot subgraph of the open KG generated by MAMA [<span class="CitationRef"><a epub:type="biblioref" href="#CR105" role="doc-biblioref">105</a></span>] using BERT<sub>LARGE</sub> from Wikipedia pages neighboring <em class="EmphasisTypeItalic ">“Bob Dylan”</em>. The blue node and arrow represent the mapped facts in the Wikidata schema, while the yellow node and arrow denote the unmapped facts in the open schema. The correct facts that are new in Wikidata are visualized in yellow. Image source: [<span class="CitationRef"><a epub:type="biblioref" href="#CR105" role="doc-biblioref">105</a></span>, p. 6], with kind permission of the authors</p></div></figcaption></figure></div><section class="Section4 RenderAsSection4" id="Sec28"><h5 class="Heading"><em><strong class="EmphasisTypeBoldItalic ">Available Implementations</strong></em></h5><div class="Para" id="Par137"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par138">PL-Marker Code and models are publicly available at <span class="ExternalRef"><a href="https://github.com/thunlp/PL-Marker"><span class="RefSource">https://​github.​com/​thunlp/​PL-Marker</span></a></span>.</p></li><li><p class="Para" id="Par139">REBEL on GitHub <span class="ExternalRef"><a href="https://github.com/babelscape/rebel"><span class="RefSource">https://​github.​com/​babelscape/​rebel</span></a></span> and Hugging Face <span class="ExternalRef"><a href="https://huggingface.co/Babelscape/rebel-large"><span class="RefSource">https://​huggingface.​co/​Babelscape/​rebel-large</span></a></span></p></li><li><p class="Para" id="Par140">MaMa: Source code and pre-trained models at <span class="ExternalRef"><a href="https://github.com/theblackcat102/language-models-are-knowledge-graphs-pytorch"><span class="RefSource">https://​github.​com/​theblackcat102/​language-models-are-knowledge-graphs-pytorch</span></a></span></p></li></ul></div></div></section>
</section>
</section>
<section class="Section2 RenderAsSection2" id="Sec29"><h3 class="Heading"><span class="HeadingNumber">5.4.5 </span>Distant Supervision</h3><p class="Para" id="Par141">Obtaining a large annotated dataset for relation extraction is a tedious task and often difficult due to privacy issues. Since much relational knowledge is stored in knowledge bases, Mintz et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR65" role="doc-biblioref">65</a></span>] proposed the <em class="EmphasisTypeItalic ">distant supervision</em><span id="ITerm148"/> paradigm. The idea behind it is to collect all text mentions where two entities co-occur, which are in a relation in the knowledge base. Then it is assumed that for this mention pair the relation holds. Since this is not correct for all such mention pairs, many approaches aim to combat this ‘noise’. One approach is <em class="EmphasisTypeItalic ">multi-instance learning</em><span id="ITerm149"/>, which relaxes the original assumption that all text mention pairs represent the relation to the assumption that the relation holds for at least one pair [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR137" role="doc-biblioref">137</a></span>], or a specified fraction like 10% or depending on a score value. Take for example the entities <em class="EmphasisTypeItalic ">“Barack Obama”</em> and <em class="EmphasisTypeItalic ">“Hawaii”</em>, which might be in a relation <em class="EmphasisTypeItalic ">“born_in”</em> in a KB. Sentences obtained by searching for occurrences of these two entities could be <em class="EmphasisTypeItalic ">“Obama was born in Hawaii”</em> as well as <em class="EmphasisTypeItalic ">“Obama was on family vacation in Hawaii”</em>, where only the former represents the relation and should be used for training.</p><p class="Para" id="Par142"><strong class="EmphasisTypeBold ">KGPool</strong><span id="ITerm150"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>] uses entity pairs obtained from a KB, but also attributes associated with them. The idea is to create representations of the entity nodes, the sentence in which they occur, and the attributes of the entity nodes in a knowledge base, such as their description, instance-of and alias attribute. All this information is embedded using word and character embeddings and bidirectional LSTMs and connected as a heterogeneous information graph. Next three layers of graph convolutional networks are used with readout layers. Only relevant attribute nodes are picked by using self-attention on the readout representations, calculating a softmax score and then filtering via a hyperparameter according to the scores. A dynamic mask is created which pools out the less essential entity attribute nodes. Finally, all intermediate representations of both entities, the sentence and the readouts are each concatenated to form the final entity, sentence and readout representation. These representations together with relation representations are then passed through a fully connected layer with softmax activation to calculate the scores per relation. The <em class="EmphasisTypeItalic ">New York Times dataset</em><span id="ITerm151"/> is a standard benchmark for relation extraction with distant supervision. KGPool achieves a <span class="EmphasisTypeSmallCaps ">Sota</span> precision@10 of 92.3%, which is the fraction of relevant results if the ‘best’ 10 of the matches are used.</p></section>
<section class="Section2 RenderAsSection2" id="Sec30"><h3 class="Heading"><span class="HeadingNumber">5.4.6 </span>Relation Extraction Using Layout Information</h3><p class="Para" id="Par143">To understand a formal text, often the document layout has be taken into account in addition to its text. Especially in form-like texts, the positions of words and filled-in values are important. In Sect. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml#Sec12"><span class="RefSource">7.​2</span></a></span> we will describe, how text and images can be simultaneously processed by one or more transformers to extract meaning from both media. In anticipation, we will use this ability of transformers to process multimodal inputs and additionally include layout information via 2-dimensional positional features. A comprehensive overview of progress in layout analysis is provided by Stanisławek [<span class="CitationRef"><a epub:type="biblioref" href="#CR96" role="doc-biblioref">96</a></span>]. We will focus on methods for key-value extraction in this subchapter. In the task of key-value extraction, documents are analyzed to extract printed values to written keys of interest. Sample applications are the automatic processing of invoices, in which keys are attributes such as invoice date or the total amount to be paid.</p><p class="Para" id="Par144"><strong class="EmphasisTypeBold ">ReLIE</strong><span id="ITerm152"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>] is a framework for key-value extraction from form-like documents. The candidate generation step has the purpose of finding all possible value candidates for a certain key, e.g. the value <em class="EmphasisTypeItalic ">“1/16/2018”</em> for the key <em class="EmphasisTypeItalic ">“Date”</em>. Often these value candidates correspond to basic types such as numbers, amounts, dates, etc. and can be found via rule based matchers. Then a transformer-based scoring model is trained, to identify valid values among the extracted value candidates. To this end, embeddings are learned for the keys, the position of the value candidate and for neighboring tokens and their positions. Positions of a value candidate and each of its neighbors are described using the 2-D Cartesian coordinates of the centroids of their respective bounding boxes. Note that the text of the candidate value is not encoded to avoid overfitting. All embeddings are related to each other by self-attention in an autoencoder. The field embedding and the candidate embedding are then compared via cosine similarity and the resulting score is scaled into a range of [0, 1]. The model achieves an f1-score of 87.8% on key-value extraction for invoices and 83.3% for receipts.</p><p class="Para" id="Par145"><strong class="EmphasisTypeBold ">DocFormer</strong><span id="ITerm153"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>] consists of a CNN visual backbone and an encoder-only transformer architecture. Visual embeddings of the document are produced via a ResNet50 model and projected to the appropriate embedding size via a linear layer. Text tokens are contained in a bounding box and the top-left and lower-right position of each token bounding box are transformed to embeddings by two different matrices. In addition, the height, width and distances between neighboring bounding boxes are encoded. The 2D-positional embeddings are enriched with absolute positions via 1D-positional embeddings. Separate spatial embeddings are trained for visual and textual features. The attention mechanism of the DocFormer is a modified version of the original attention mechanism. Separate attention scores are calculated for the visual and the textual representation of tokens. In addition to the key-query attention, the relative position embeddings of both query and key tokens are used to add relative position attentions as well as a spatial attention for both the visual and the textual embeddings. The spatial attention weights are shared between the visual and the textual representations.</p><p class="Para" id="Par146">DocFormer is pre-trained with three different pre-training tasks: multi-modal masked language modeling (MM-MLM), learn to reconstruct (LTR) and text describes image (TDI). In the MM-MLM task, tokens are masked and should be reconstructed by the model. In LTR, the model is tasked to reconstruct the image of a document, given the multi-modal representation. A smooth-L1 loss is used to calculate differences between the original and the reconstructed image. TDI requires a text-image matching task, in which the model has to predict for random samples whether the image and the text are aligned or not. The <em class="EmphasisTypeItalic ">FUNSD benchmark</em><span id="ITerm154"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>] considers forms in 199 scanned documents, where tokens have to be assigned to a semantic key, such as ‘question’ or ‘answer’. On FUNSD DocFormer reaches an F1-value of 84.6%, which was <span class="EmphasisTypeSmallCaps ">Sota</span> at publication time.</p><div class="Para" id="Par147"><strong class="EmphasisTypeBold ">LayoutLM3</strong><span id="ITerm155"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>] uses an image embedding method inspired by the Vision Transformer (Sect. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml#Sec14"><span class="RefSource">7.​2.​2</span></a></span>). Each image is partitioned into 16 × 16 image patches similar to the Vision Transformer and linearly transformed to embeddings. As shown in Fig. <span class="InternalRef"><a href="#Fig8">5.8</a></span> words and image patches are processed by the same autoregressive Transformer. For pre-training the model uses the masked language modeling task, masked image patches and word-patch alignment pre-training task. In the masked image patches task, image patches have to be reconstructed by the model. The word-patch alignment task has to enable the model to learn alignments between textual and visual representations. The model should classify whether text and image patch of a token are aligned, i.e. both are unmasked, or unaligned, i.e. the image patch is masked. The <em class="EmphasisTypeItalic ">PubLayNet benchmark</em><span id="ITerm156"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR134" role="doc-biblioref">134</a></span>] contains the document layout of more than 1 million pdf documents matched against the correct document structure. Here LayoutLM3 achieves <span class="EmphasisTypeSmallCaps ">Sota</span> with 94.5% mean average precision of bounding boxes. It outperforms DocFormer on the FUNSD key-value extraction tasks and other benchmarks. <em class="EmphasisTypeItalic ">LayoutXLM</em><span id="ITerm157"/> is a recent multilingual version of LayoutLM2 [<span class="CitationRef"><a epub:type="biblioref" href="#CR116" role="doc-biblioref">116</a></span>].<figure class="Figure" id="Fig8"><div class="MediaObject" id="MO8"><img alt="" aria-describedby="d64e5112" src="../images/528393_1_En_5_Chapter/528393_1_En_5_Fig8_HTML.png" style="width:34.07em"/><div class="TextObject" id="d64e5112"><p class="Para" id="Par163">A flow diagram has an input of word tokens and image patches via word and linear embedding, linked to 1 D and 2 D position embeddings, input embeddings, autoencoder with self-attentions, output embeddings, and logistic classifiers to give probabilities for M L M, W P A, and M I M.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.8</span><p class="SimplePara">LayoutLMv3 takes the linear projection of image patches and word tokens as inputs and encodes them into contextualized vector representations. LayoutLMv3 is pre-trained with discrete token reconstructive objectives of Masked Language Modeling (MLM) and Masked Image Modeling (MIM). Additionally, LayoutLMv3 is pre-trained with a Word-Patch Alignment (WPA) objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. “Seg” denotes segment-level positions. Image source: [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>, p. 3], printed with kind permission of the authors</p></div></figcaption></figure></div><section class="Section3 RenderAsSection3" id="Sec31"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par148"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par149">KGPool at <span class="ExternalRef"><a href="https://github.com/nadgeri14/KGPool"><span class="RefSource">https://​github.​com/​nadgeri14/​KGPool</span></a></span></p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec32"><h3 class="Heading"><span class="HeadingNumber">5.4.7 </span>Summary</h3><p class="Para" id="Par150">Relation extraction has the task to evaluate the expressed relationship in the text with respect to specific entities. An example is the assessment of certain product characteristics by customers, which can help to improve the product or service. Given the massive amount of textual content, it is intractable to manually process the opinion information.</p><p class="Para" id="Par151">For simple cases, the relation arguments are know and relation extraction can be solved as a simple classification task using some BERT variant like RoBERTa, DeBERTa, or SpanBERT. However, to actually use these models we have to extract the relation arguments in a prior step, which leads to an increased total error.</p><p class="Para" id="Par152">More challenging is the simultaneous extraction of relation arguments and the corresponding relation type, as these task depend on each other. UniRE annotates entities and relations in a joint matrix and introduces a corresponding bias into the self-attention computations. PL-marker marks the first relation arguments with special tokens and the second argument with so-called leviated tokens. These tokens have specific attention properties and are able to improve the performance on popular benchmarks. GRACE employs a specific encoder-decoder architecture where the encoder labels the relation arguments (aspects) and the decoder assigns relation tags to each token. REBEL uses the BART encoder-decoder to translate the input sentence to a unique representation of the covered relations.</p><p class="Para" id="Par153">Relation extraction models have been adapted to specific applications. GRACE has been tuned for aspect-based sentiment analysis and Crf2o to semantic role labeling. The latter uses contextual embeddings and determines the relation between predicate and corresponding phrases by an efficient TreeCRF. Finally, MaMa can be used to build a knowledge graph from extracted relations between entities.</p><p class="Para" id="Par154">Often the spatial layout of documents and web pages contains relevant information for the extraction of relation arguments. In this case, visual information from the document image can be exploited to arrive at a valid interpretation. This visual information can be included via the position of bounding boxes for keys and values, but also in the form of image patches, which are explored later with the image transformer.</p><p class="Para" id="Par155">All recent relation extraction approaches are based on PLMs. Most models use small BERT variants for their experiments. Therefore, it can be assumed that larger models will directly increase performance. In addition, Foundation Models like GPT-3 may be fine-tuned (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec40"><span class="RefSource">3.​6.​2</span></a></span>) and probably will result in a higher accuracy. A related alternative is InstructGPT (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec43"><span class="RefSource">3.​6.​5</span></a></span>), which can be easily directed to perform a relation extraction via question answering, e.g. <em class="EmphasisTypeItalic ">“Who built the statue of liberty?”</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>, p. 29]. However, it seems to be difficult to evaluate the performance of this approach with respect to some test data.</p></section>
</section>
<div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a><p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p><p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p></div><aside aria-labelledby="Bib1Heading" class="Bibliography" id="Bib1"><div epub:type="bibliography" role="doc-bibliography"><div class="Heading" id="Bib1Heading">References</div><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">J. Abreu, L. Fred, D. Macêdo, and C. Zanchettin. “Hierarchical Attentional Hybrid Neural Networks for Document Classification”. In: <em class="EmphasisTypeItalic ">Int. Conf. Artif. Neural Netw</em>. Springer, 2019, pp. 396–402.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">L. Adilova, S. Giesselbach, and S. Rüping. “Making Efficient Use of a Domain Expert’s Time in Relation Extraction”. 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1807.04687</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">N. Alex et al. “RAFT: A Real-World Few-Shot Text Classification Benchmark”. Jan. 18, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2109.14076</span> [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">Z. Alyafeai, M. S. AlShaibani, and I. Ahmad. “A Survey on Transfer Learning in Natural Language Processing”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2007.04239</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">G. Angeli, M. J. J. Premkumar, and C. D. Manning. “Leveraging Linguistic Structure for Open Domain Information Extraction”. In: <em class="EmphasisTypeItalic ">Proc. 53rd Annu. Meet. Assoc. Comput. Linguist. 7th Int. Jt. Conf. Nat. Lang. Process. Vol. 1 Long Pap</em>. 2015, pp. 344–354.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">S. Appalaraju, B. Jasani, B. U. Kota, Y. Xie, and R. Manmatha. “Docformer: End-to-end Transformer for Document Understanding”. In: <em class="EmphasisTypeItalic ">Proc. IEEECVF Int. Conf. Comput. Vis</em>. 2021, pp. 993–1003.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives. “DBpedia: A Nucleus for a Web of Open Data”. In: <em class="EmphasisTypeItalic ">Semantic Web</em>. Ed. by K. Aberer et al. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer, 2007, pp. 722–735. <span class="EmphasisTypeSmallCaps ">isbn</span>: 978-3-540-76298-0. <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-540-76298-0_52"><span class="RefSource">https://​doi.​org/​10.​1007/​978-3-540-76298-0_​52</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">E. Barba, T. Pasini, and R. Navigli. “ESC: Redesigning WSD with Extractive Sense Comprehension”. In: <em class="EmphasisTypeItalic ">Proc. 2021 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol</em>. 2021, pp. 4661–4672.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">E. Barba, L. Procopio, N. Campolungo, T. Pasini, and R. Navigli. “MuLaN: Multilingual Label propagatioN for Word Sense Disambiguation”. In: <em class="EmphasisTypeItalic ">Proc IJCAI</em>. 2020, pp. 3837–3844.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">E. Barba, L. Procopio, and R. Navigli. “ConSeC: Word Sense Disambiguation as Continuous Sense Comprehension”. In: <em class="EmphasisTypeItalic ">Proc. 2021 Conf. Empir. Methods Nat. Lang. Process</em>. 2021, pp. 1492–1503.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">I. Beltagy, K. Lo, and A. Cohan. “SciBERT: A Pretrained Language Model for Scientific Text”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1903.10676</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">M. Bevilacqua and R. Navigli. “Breaking through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information”. In: <em class="EmphasisTypeItalic ">Proc Assoc. Comput. Linguist</em>. 2020, pp. 2854–2864.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">M. Bevilacqua, T. Pasini, A. Raganato, and R. Navigli. “Recent Trends in Word Sense Disambiguation: A Survey”. In: <em class="EmphasisTypeItalic ">Proc. Thirtieth Int. Jt. Conf. Artif. Intell. IJCAI-21</em>. International Joint Conference on Artificial Intelligence, Inc, 2021.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">K. Bhatia, K. Dahiya, H. Jain, P. Kar, A. Mittal, Y. Prabhu, and M. Varma. <em class="EmphasisTypeItalic ">The Extreme Classification Repository</em>. June 7, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="http://manikvarma.org/downloads/XC/XMLRepository.html"><span class="RefSource">http://​manikvarma.​org/​downloads/​XC/​XMLRepository.​html</span></a></span> (visited on 06/07/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">T. Blevins and L. Zettlemoyer. “Moving down the Long Tail of Word Sense Disambiguation with Gloss-Informed Biencoders”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2005.02590</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">P. Bojanowski. fastText. 2016. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://fasttext.cc/index.html"><span class="RefSource">https://​fasttext.​cc/​index.​html</span></a></span> (visited on 02/21/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">F. Bond and R. Foster. “Linking and Extending an Open Multilingual Wordnet”. In: <em class="EmphasisTypeItalic ">Proc. 51st Annu. Meet. Assoc. Comput. Linguist. Vol. 1 Long Pap</em>. 2013, pp. 1352–1362.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">P. Bose, S. Srinivasan, W. C. Sleeman, J. Palta, R. Kapoor, and P. Ghosh. “A Survey on Recent Named Entity Recognition and Relationship Extraction Techniques on Clinical Texts”. In: <em class="EmphasisTypeItalic ">Appl. Sci</em>. 11.18 (2021), p. 8319.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">F. Brandon. Brandon25/Deberta-Base-Finetuned-Ner ⋅ Hugging Face. Oct. 12, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://huggingface.co/brandon25/deberta-base-finetuned-ner"><span class="RefSource">https://​huggingface.​co/​brandon25/​deberta-base-finetuned-ner</span></a></span> (visited on 02/15/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">P.-L. H. Cabot and R. Navigli. “REBEL: Relation Extraction By End-to-end Language Generation”. In: <em class="EmphasisTypeItalic ">Find. Assoc. Comput. Linguist. EMNLP 2021</em>. 2021, pp. 2370–2381.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">X. Carreras and L. Màrquez. “Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling”. In: <em class="EmphasisTypeItalic ">Proc. Ninth Conf. Comput. Nat. Lang. Learn. CoNLL-2005</em>. 2005, pp. 152–164.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">W.-C. Chang et al. “Extreme Multi-label Learning for Semantic Matching in Product Search”. June 23, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2106.12657</span> [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">G. Choi, S. Oh, and H. Kim. “Improving Document-Level Sentiment Classification Using Importance of Sentences”. In: <em class="EmphasisTypeItalic ">Entropy</em> 22.12 (2020), p. 1336.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">A. D. Cohen, S. Rosenman, and Y. Goldberg. “Relation Extraction as Two-way Span- Prediction”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.04829</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">N. De Cao, G. Izacard, S. Riedel, and F. Petroni. “Autoregressive Entity Retrieval”. Mar. 24, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.00904</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">S. Ding, J. Shang, S. Wang, Y. Sun, H. Tian, H. Wu, and H. Wang. “ERNIE-DOC: The Retrospective Long-Document Modeling Transformer”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2012.15688</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">V. Dobrovolskii. “Word-Level Coreference Resolution”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2109.04127</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">J. Eisner. “Bilexical Grammars and Their Cubic-Time Parsing Algorithms”. In: <em class="EmphasisTypeItalic ">Advances in Probabilistic and Other Parsing Technologies</em>. Springer, 2000, pp. 29–61.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">D. Gillick, S. Kulkarni, L. Lansing, A. Presta, J. Baldridge, E. Ie, and D. Garcia-Olano. “Learning Dense Representations for Entity Retrieval”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1909.10506</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">GitHub. <em class="EmphasisTypeItalic ">GitHub</em>. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/"><span class="RefSource">https://​github.​com/​</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">Gu. <em class="EmphasisTypeItalic ">BLURB Leaderboard</em>. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://microsoft.github.io/BLURB/"><span class="RefSource">https://​microsoft.​github.​io/​BLURB/​</span></a></span> (visited on 02/13/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">J. He, L. Wang, L. Liu, J. Feng, and H. Wu. “Long Document Classification from Local Word Glimpses via Recurrent Attention Learning”. In: <em class="EmphasisTypeItalic ">IEEE Access</em> 7 (2019), pp. 40707–40718.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">L. Huang, C. Sun, X. Qiu, and X. Huang. “GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1908.07245</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">Y. Huang, T. Lv, L. Cui, Y. Lu, and F. Wei. “LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking”. 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2204.08387</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">huggingface. <em class="EmphasisTypeItalic ">Transformers – Transformers 4.3.0 Documentation</em>. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://huggingface.co/transformers/"><span class="RefSource">https://​huggingface.​co/​transformers/​</span></a></span> (visited on 02/21/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">M. S. Jahan and M. Oussalah. “A Systematic Review of Hate Speech Automatic Detection Using Natural Language Processing”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2106.00742</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">K. Jasinska, K. Dembczynski, R. Busa-Fekete, K. Pfannschmidt, T. Klerx, and E. Hullermeier. “Extreme F-Measure Maximization Using Sparse Probability Estimates”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn</em>. PMLR, 2016, pp. 1435–1444.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">G. Jaume, H. K. Ekenel, and J.-P. Thiran. “Funsd: A Dataset for Form Understanding in Noisy Scanned Documents”. In: <em class="EmphasisTypeItalic ">2019 Int. Conf. Doc. Anal. Recognit. Workshop ICDARW</em>. Vol. 2. IEEE, 2019, pp. 1–6.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">T. Jiang, D. Wang, L. Sun, H. Yang, Z. Zhao, and F. Zhuang. “Lightxml: Transformer with Dynamic Negative Sampling for High-Performance Extreme Multi-Label Text Classification”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2101.03305</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">J. Johnson, M. Douze, and H. Jégou. “Billion-Scale Similarity Search with Gpus”. In: <em class="EmphasisTypeItalic ">IEEE Trans. Big Data</em> (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy. “Spanbert: Improving Pre-Training by Representing and Predicting Spans”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 8 (2020), pp. 64–77.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">K. raj Kanakarajan, B. Kundumani, and M. Sankarasubbu. “BioELECTRA:Pretrained Biomedical Text Encoder Using Discriminators”. In: <em class="EmphasisTypeItalic ">Proc. 20th Workshop Biomed. Lang. Process</em>. BioNLP-NAACL 2021. Online: Association for Computational Linguistics, June 2021, pp. 143–154. <span class="ExternalRef"><a href="https://doi.org/10.18653/v1/2021.bionlp-1.16"><span class="RefSource">https://​doi.​org/​10.​18653/​v1/​2021.​bionlp-1.​16</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. “GENIA Corpus-a Semantically Annotated Corpus for Bio-Textmining”. In: <em class="EmphasisTypeItalic ">Bioinformatics</em> 19 (suppl_1 2003), pp. i180–i182.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">K. Kowsari, K. Jafari Meimandi, M. Heidarysafa, S. Mendu, L. Barnes, and D. Brown. “Text Classification Algorithms: A Survey”. In: <em class="EmphasisTypeItalic ">Information</em> 10.4 (2019), p. 150.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. “Albert: A Lite BERT for Self-Supervised Learning of Language Representations”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">1909.11942</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">H. Langone, B. R. Haskell, and G. A. Miller. Annotating Wordnet. PRINCETON UNIV NJ COGNITIVE SCIENCE LAB, 2004.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">Q. V. Le and T. Mikolov. “Distributed Representations of Sentences and Documents”. May 22, 2014. arXiv: <span class="EmphasisFontCategoryNonProportional ">1405.4053 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">J. Li, A. Sun, J. Han, and C. Li. “A Survey on Deep Learning for Named Entity Recognition”. In: <em class="EmphasisTypeItalic ">IEEE Trans. Knowl. Data Eng</em>. (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">Q. Li et al. “A Survey on Text Classification: From Shallow to Deep Learning”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2008.00364</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">X. Li, J. Feng, Y. Meng, Q. Han, F. Wu, and J. Li. “A Unified MRC Framework for Named Entity Recognition”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1910.11476</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">X. Liu, W.-C. Chang, H.-F. Yu, C.-J. Hsieh, and I. S. Dhillon. “Label Disentanglement in Partition-based Extreme Multilabel Classification”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2106.12751</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">D. Loureiro, K. Rezaee, M. T. Pilehvar, and J. Camacho-Collados. “Analysis and Evaluation of Language Models for Word Sense Disambiguation”. In: <em class="EmphasisTypeItalic ">Comput. Linguist. 2021 47 2 387–443</em> (Mar. 17, 2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">E. Loza Mencía and J. Fürnkranz. “Efficient Pairwise Multilabel Classification for Large- Scale Problems in the Legal Domain”. In: <em class="EmphasisTypeItalic ">Jt. Eur. Conf. Mach. Learn. Knowl. Discov. Databases</em>. Springer, 2008, pp. 50–65.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">H. Luo, L. Ji, T. Li, N. Duan, and D. Jiang. “Grace: Gradient Harmonized and Cascaded Labeling for Aspect-Based Sentiment Analysis”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2009.10557</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">S. Lyu and H. Chen. “Relation Classification with Entity Type Restriction”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2105.08393</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">A. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. “Learning Word Vectors for Sentiment Analysis”. In: <em class="EmphasisTypeItalic ">Proc. 49th Annu. Meet. Assoc. Comput. Linguist. Hum. Lang. Technol</em>. 2011, pp. 142–150.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">B. P. Majumder, N. Potti, S. Tata, J. B. Wendt, Q. Zhao, and M. Najork. “Representation Learning for Information Extraction from Form-like Documents”. In: <em class="EmphasisTypeItalic ">Proc. 58th Annu. Meet. Assoc. Comput. Linguist</em>. 2020, pp. 6495–6504.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">T. Mandl, S. Modha, P. Majumder, D. Patel, M. Dave, C. Mandlia, and A. Patel. “Overview of the HASOC Track at FIRE 2019: Hate Speech and Offensive Content Identification in Indo-European Languages”. In: <em class="EmphasisTypeItalic ">Proc. 11th Forum Inf. Retr. Eval</em>. FIRE ’19: Forum for Information Retrieval Evaluation. Kolkata India: ACM, Dec. 12, 2019, pp. 14–17. <span class="EmphasisTypeSmallCaps ">isbn</span>: 978-1-4503-7750-8. <span class="ExternalRef"><a href="https://doi.org/10.1145/3368567.3368584"><span class="RefSource">https://​doi.​org/​10.​1145/​3368567.​3368584</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">B. Mathew, P. Saha, S. M. Yimam, C. Biemann, P. Goyal, and A. Mukherjee. “HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2012.10289 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">J. McAuley and J. Leskovec. “Hidden Factors and Hidden Topics: Understanding Rating Dimensions with Review Text”. In: <em class="EmphasisTypeItalic ">Proc. 7th ACM Conf. Recomm. Syst</em>. 2013, pp. 165–172.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">61.</div><div class="CitationContent" id="CR61">R. Mihalcea. <em class="EmphasisTypeItalic ">SemCor Corpus</em>. June 13, 2008. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://kaggle.com/nltkdata/semcorcorpus"><span class="RefSource">https://​kaggle.​com/​nltkdata/​semcorcorpus</span></a></span> (visited on 01/04/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">62.</div><div class="CitationContent" id="CR62">G. A. Miller. “WordNet: A Lexical Database for English”. In: <em class="EmphasisTypeItalic ">Commun. ACM</em> 38.11 (1995), pp. 39–41.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">63.</div><div class="CitationContent" id="CR63">G. A. Miller, C. Leacock, R. Tengi, and R. T. Bunker. “A Semantic Concordance”. In: <em class="EmphasisTypeItalic ">Hum. Lang. Technol. Proc. Workshop Held Plainsboro N. J. March 21–24 1993</em>. 1993.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">64.</div><div class="CitationContent" id="CR64">S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu, and J. Gao. “Deep Learning-Based Text Classification: A Comprehensive Review”. In: <em class="EmphasisTypeItalic ">ACM Comput. Surv. CSUR</em> 54.3 (2021), pp. 1–40.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">65.</div><div class="CitationContent" id="CR65">M. Mintz, S. Bills, R. Snow, and D. Jurafsky. “Distant Supervision for Relation Extraction without Labeled Data”. In: <em class="EmphasisTypeItalic ">Proc. Jt. Conf. 47th Annu. Meet. ACL 4th Int. Jt. Conf. Nat. Lang. Process. AFNLP</em>. 2009, pp. 1003–1011.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">66.</div><div class="CitationContent" id="CR66">A. Moro and R. Navigli. “Semeval-2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking”. In: <em class="EmphasisTypeItalic ">Proc. 9th Int. Workshop Semantic Eval. SemEval 2015</em>. 2015, pp. 288–297.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">67.</div><div class="CitationContent" id="CR67">A. Nadgeri, A. Bastos, K. Singh, I. O. Mulang, J. Hoffart, S. Shekarpour, and V. Saraswat. “Kgpool: Dynamic Knowledge Graph Context Selection for Relation Extraction”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2106.00459</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">68.</div><div class="CitationContent" id="CR68">Z. Nasar, S. W. Jaffry, and M. K. Malik. “Named Entity Recognition and Relation Extraction: State-of-the-art”. In: <em class="EmphasisTypeItalic ">ACM Comput. Surv. CSUR</em> 54.1 (2021), pp. 1–39.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">69.</div><div class="CitationContent" id="CR69">R. Navigli. “Word Sense Disambiguation: A Survey”. In: <em class="EmphasisTypeItalic ">ACM Comput. Surv. CSUR</em> 41.2 (2009), pp. 1–69.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">70.</div><div class="CitationContent" id="CR70">R. Navigli, D. Jurgens, and D. Vannella. “Semeval-2013 Task 12: Multilingual Word Sense Disambiguation”. In: <em class="EmphasisTypeItalic ">Second Jt. Conf. Lex. Comput. Semant. SEM Vol. 2 Proc. Seventh Int. Workshop Semantic Eval. SemEval 2013</em>. 2013, pp. 222–231.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">71.</div><div class="CitationContent" id="CR71">R. Navigli and S. P. Ponzetto. “BabelNet: The Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network”. In: <em class="EmphasisTypeItalic ">Artif. Intell</em>. 193 (2012), pp. 217–250.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">72.</div><div class="CitationContent" id="CR72">ner. <em class="EmphasisTypeItalic ">Papers with Code - Named Entity Recognition</em>. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://paperswithcode.com/task/named-entity-recognition-ner"><span class="RefSource">https://​paperswithcode.​com/​task/​named-entity-recognition-ner</span></a></span> (visited on 07/09/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">73.</div><div class="CitationContent" id="CR73">NIH. <em class="EmphasisTypeItalic ">Download Data</em>. PubMed. 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://pubmed.ncbi.nlm.nih.gov/download/"><span class="RefSource">https://​pubmed.​ncbi.​nlm.​nih.​gov/​download/​</span></a></span> (visited on 06/15/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">74.</div><div class="CitationContent" id="CR74">NLP. <em class="EmphasisTypeItalic ">The NLP Index</em>. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://index.quantumstat.com/"><span class="RefSource">https://​index.​quantumstat.​com/​</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">75.</div><div class="CitationContent" id="CR75">Omegawiki. <em class="EmphasisTypeItalic ">OmegaWiki</em>. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="http://www.omegawiki.org/"><span class="RefSource">http://​www.​omegawiki.​org/​</span></a></span> (visited on 01/03/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">76.</div><div class="CitationContent" id="CR76">OpenAi. <em class="EmphasisTypeItalic ">OpenAI API</em>. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://beta.openai.com"><span class="RefSource">https://​beta.​openai.​com</span></a></span> (visited on 11/14/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">77.</div><div class="CitationContent" id="CR77">L. Ouyang et al. “Training Language Models to Follow Instructions with Human Feedback”. Jan. 31, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2203.02155</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">78.</div><div class="CitationContent" id="CR78">G. Paaß and F. Reichartz. “Exploiting Semantic Constraints for Estimating Supersenses with CRFs”. In: <em class="EmphasisTypeItalic ">Proc. 2009 SIAM Int. Conf. Data Min</em>. SIAM, 2009, pp. 485–496.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">79.</div><div class="CitationContent" id="CR79">Papers-with-code. <em class="EmphasisTypeItalic ">Papers with Code</em>. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://paperswithcode.com/"><span class="RefSource">https://​paperswithcode.​com/​</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">80.</div><div class="CitationContent" id="CR80">M. E. Peters, M. Neumann, R. L. Logan IV, R. Schwartz, V. Joshi, S. Singh, and N. A. Smith. “Knowledge Enhanced Contextual Word Representations”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1909.04164</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">81.</div><div class="CitationContent" id="CR81">M. T. Pilehvar, J. Camacho-Collados, R. Navigli, and N. Collier. “Towards a Seamless Integration of Word Senses into Downstream Nlp Applications”. 2017. arXiv: <span class="EmphasisFontCategoryNonProportional ">1710.06632</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">82.</div><div class="CitationContent" id="CR82">A. Pilz and G. Paaß. “From Names to Entities Using Thematic Context Distance”. In: <em class="EmphasisTypeItalic ">Proc. 20th ACM Int. Conf. Inf. Knowl. Manag</em>. 2011, pp. 857–866.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">83.</div><div class="CitationContent" id="CR83">Y. Prabhu, A. Kag, S. Harsola, R. Agrawal, and M. Varma. “Parabel: Partitioned Label Trees for Extreme Classification with Application to Dynamic Search Advertising”. In: <em class="EmphasisTypeItalic ">Proc. 2018 World Wide Web Conf</em>. 2018, pp. 993–1002.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">84.</div><div class="CitationContent" id="CR84">S. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and Y. Zhang. “CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes”. In: <em class="EmphasisTypeItalic ">Jt. Conf. EMNLP CoNLL-Shar. Task</em>. 2012, pp. 1–40.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">85.</div><div class="CitationContent" id="CR85">J. W. Rae et al. “Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher”. In: <em class="EmphasisTypeItalic ">ArXiv Prepr. ArXiv211211446</em> (Dec. 8, 2021), p. 118.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">86.</div><div class="CitationContent" id="CR86">P. Ramachandran, B. Zoph, and Q. V. Le. “Searching for Activation Functions”. 2017. arXiv: <span class="EmphasisFontCategoryNonProportional ">1710.05941</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">87.</div><div class="CitationContent" id="CR87">F. Reichartz, H. Korte, and G. Paass. “Semantic Relation Extraction with Kernels over Typed Dependency Trees”. In: <em class="EmphasisTypeItalic ">Proc. 16th ACM SIGKDD Int. Conf. Knowl. Discov. Data Min</em>. 2010, pp. 773–782.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">88.</div><div class="CitationContent" id="CR88">S. G. Roy, U. Narayan, T. Raha, Z. Abid, and V. Varma. “Leveraging Multilingual Transformers for Hate Speech Detection”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2101.03207</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">89.</div><div class="CitationContent" id="CR89">E. F. Sang and F. De Meulder. “Introduction to the CoNLL-2003 Shared Task: Languageindependent Named Entity Recognition”. 2003. arXiv: <span class="EmphasisFontCategoryNonProportional ">cs/0306050</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">90.</div><div class="CitationContent" id="CR90">T. Schick and H. Schütze. “True Few-Shot Learning with Prompts – A Real-World Perspective”. Nov. 26, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2111.13440 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">91.</div><div class="CitationContent" id="CR91">P. Schmid. <em class="EmphasisTypeItalic ">Few-Shot Learning in Practice: GPT-Neo and the .. Accelerated Inference API</em>. June 3, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api"><span class="RefSource">https://​huggingface.​co/​blog/​few-shot-learning-gpt-neo-and-inference-api</span></a></span> (visited on 05/23/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">92.</div><div class="CitationContent" id="CR92">O. Sevgili, A. Shelmanov, M. Arkhipov, A. Panchenko, and C. Biemann. “Neural Entity Linking: A Survey of Models Based on Deep Learning”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2006.00575</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">93.</div><div class="CitationContent" id="CR93">Y. Shen, X. Ma, Z. Tan, S. Zhang, W. Wang, and W. Lu. “Locate and Label: A Two-stage Identifier for Nested Named Entity Recognition”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2105.06804</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">94.</div><div class="CitationContent" id="CR94">E. H. Silva and R. M. Marcacini. “Aspect-Based Sentiment Analysis Using BERT with Disentangled Attention”. In: (2021). <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://repositorio.usp.br/bitstreams/701d2a63-e3f4-450d-8617-ad80de4345ed.2185FoundationModelsforInformationExtraction"><span class="RefSource">https://​repositorio.​usp.​br/​bitstreams/​701d2a63-e3f4-450d-8617-ad80de4345ed.​2185FoundationMo​delsforInformati​onExtraction</span></a></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">95.</div><div class="CitationContent" id="CR95">Spanbert. <em class="EmphasisTypeItalic ">Papers with Code - The Latest in Machine Learning</em>. July 17, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://paperswithcode.com/paper/spanbert-improving-pre-training-by/review/?hl=28781"><span class="RefSource">https://​paperswithcode.​com/​paper/​spanbert-improving-pre-training-by/​review/​?​hl=​28781</span></a></span> (visited on 07/17/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">96.</div><div class="CitationContent" id="CR96">T. Stanisławek. <em class="EmphasisTypeItalic ">Awesome Document Understanding</em>. July 2, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/tstanislawek/awesome-document-understanding"><span class="RefSource">https://​github.​com/​tstanislawek/​awesome-document-understanding</span></a></span> (visited on 07/08/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">97.</div><div class="CitationContent" id="CR97">G. Stoica, E. A. Platanios, and B. Póczos. “Re-Tacred: Addressing Shortcomings of the Tacred Dataset”. In: <em class="EmphasisTypeItalic ">Proc. AAAI Conf. Artif. Intell</em>. Vol. 35. 15. 2021, pp. 13843–13850.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">98.</div><div class="CitationContent" id="CR98">N. Stylianou and I. Vlahavas. “A Neural Entity Coreference Resolution Review”. In: <em class="EmphasisTypeItalic ">Expert Syst. Appl</em>. 168 (2021), p. 114466.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">99.</div><div class="CitationContent" id="CR99">Y. Sun et al. “Ernie: Enhanced Representation through Knowledge Integration”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1904.09223</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">100.</div><div class="CitationContent" id="CR100">C. Sutton and A. McCallum. “An Introduction to Conditional Random Fields for Relational Learning”. In: <em class="EmphasisTypeItalic ">Introd. Stat. Relational Learn</em>. 2 (2006), pp. 93–128.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">101.</div><div class="CitationContent" id="CR101">T. Thongtan and T. Phienthrakul. “Sentiment Classification Using Document Embeddings Trained with Cosine Similarity”. In: <em class="EmphasisTypeItalic ">Proc. 57th Annu. Meet. Assoc. Comput. Linguist. Stud. Res. Workshop</em>. Florence, Italy: Association for Computational Linguistics, July 2019, pp. 407–414. <span class="ExternalRef"><a href="https://doi.org/10.18653/v1/P19-2057"><span class="RefSource">https://​doi.​org/​10.​18653/​v1/​P19-2057</span></a></span>.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.18653/v1/P19-2057"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">102.</div><div class="CitationContent" id="CR102">R. Tinn et al. “Fine-Tuning Large Neural Language Models for Biomedical Natural Language Processing”. Dec. 14, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2112.07869 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">103.</div><div class="CitationContent" id="CR103">S. Toshniwal, S. Wiseman, A. Ettinger, K. Livescu, and K. Gimpel. “Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.02807</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">104.</div><div class="CitationContent" id="CR104">C. Walker, S. Strassel, J. Medero, and K. Maeda. <em class="EmphasisTypeItalic ">ACE 2005 Multilingual Training Corpus</em>. Linguistic Data Consortium, Feb. 15, 2006. <span class="ExternalRef"><a href="https://doi.org/10.35111/MWXC-VH88"><span class="RefSource">https://​doi.​org/​10.​35111/​MWXC-VH88</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">105.</div><div class="CitationContent" id="CR105">C. Wang, X. Liu, and D. Song. “Language Models Are Open Knowledge Graphs”. Oct. 22, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.11967</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">106.</div><div class="CitationContent" id="CR106">X. Wang, Y. Jiang, N. Bach, T. Wang, Z. Huang, F. Huang, and K. Tu. “Automated Concatenation of Embeddings for Structured Prediction”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.05006</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">107.</div><div class="CitationContent" id="CR107">X. Wang, Y. Jiang, N. Bach, T. Wang, Z. Huang, F. Huang, and K. Tu. “Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2105.03654</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">108.</div><div class="CitationContent" id="CR108">Y. Wang, C. Sun, Y. Wu, H. Zhou, L. Li, and J. Yan. “UniRE: A Unified Label Space for Entity Relation Extraction”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2107.04292</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">109.</div><div class="CitationContent" id="CR109">R. Weischedel, M. Palmer, R. B. S. P. L. Ramshaw, N. Xue, and E. Hovy. “Ontonotes: A Large Training Corpus for Enhanced Processing”. In: <em class="EmphasisTypeItalic ">Joseph Olive Caitlin Christ. And- John McCary Ed. Handb. Nat. Lang. Mach. Transl. DARPA Glob. Lang. Exploit</em>. (2011).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">110.</div><div class="CitationContent" id="CR110">G. Wiedemann, S. M. Yimam, and C. Biemann. “UHH-LT at SemEval-2020 Task 12: Fine-Tuning of Pre-Trained Transformer Networks for Offensive Language Detection”. June 10, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2004.11493 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">111.</div><div class="CitationContent" id="CR111">wiktionary. <em class="EmphasisTypeItalic ">Wiktionary</em>. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.wiktionary.org/"><span class="RefSource">https://​www.​wiktionary.​org/​</span></a></span> (visited on 01/03/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">112.</div><div class="CitationContent" id="CR112">R. J. Williams. “Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning”. In: <em class="EmphasisTypeItalic ">Mach. Learn</em>. 8.3 (1992), pp. 229–256.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">113.</div><div class="CitationContent" id="CR113">L. Wu, F. Petroni, M. Josifoski, S. Riedel, and L. Zettlemoyer. “Scalable Zero-shot Entity Linking with Dense Entity Retrieval”. In: <em class="EmphasisTypeItalic ">Proc. 2020 Conf. Empir. Methods Nat. Lang. Process. EMNLP</em>. 2020, pp. 6397–6407.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">114.</div><div class="CitationContent" id="CR114">W. Wu, F. Wang, A. Yuan, F. Wu, and J. Li. “Coreference Resolution as Query-Based Span Prediction”. July 18, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">1911.01746</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">115.</div><div class="CitationContent" id="CR115">B. Xu, Q. Wang, Y. Lyu, Y. Zhu, and Z. Mao. “Entity Structure Within and Throughout: Modeling Mention Dependencies for Document-Level Relation Extraction”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2102.10249</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">116.</div><div class="CitationContent" id="CR116">Y. Xu et al. “Layoutxlm: Multimodal Pre-Training for Multilingual Visually-Rich Document Understanding”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2104.08836</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">117.</div><div class="CitationContent" id="CR117">I. Yamada, A. Asai, H. Shindo, H. Takeda, and Y. Matsumoto. “LUKE: Deep Contextualized Entity Representations with Entity-Aware Self-Attention”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.01057</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">118.</div><div class="CitationContent" id="CR118">I. Yamada, K. Washio, H. Shindo, and Y. Matsumoto. “Global Entity Disambiguation with Pretrained Contextualized Embeddings of Words and Entities”. Nov. 24, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">1909.00426 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">119.</div><div class="CitationContent" id="CR119">H. Yang, B. Zeng, M. Xu, and T. Wang. “Back to Reality: Leveraging Pattern-driven Modeling to Enable Affordable Sentiment Dependency Learning”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2110.08604</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">120.</div><div class="CitationContent" id="CR120">Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le. “Xlnet: Generalized Autoregressive Pretraining for Language Understanding”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2019, pp. 5753–5763.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">121.</div><div class="CitationContent" id="CR121">Y. Yao et al. “DocRED: A Large-Scale Document-Level Relation Extraction Dataset”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1906.06127</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">122.</div><div class="CitationContent" id="CR122">D. Ye, Y. Lin, and M. Sun. “Pack Together: Entity and Relation Extraction with Levitated Marker”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2109.06067</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">123.</div><div class="CitationContent" id="CR123">W. Yin and A. Zubiaga. “Towards Generalisable Hate Speech Detection: A Review on Obstacles and Solutions”. In: <em class="EmphasisTypeItalic ">PeerJ Comput. Sci</em>. 7 (2021), e598.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">124.</div><div class="CitationContent" id="CR124">R. You, Z. Zhang, Z. Wang, S. Dai, H. Mamitsuka, and S. Zhu. “Attentionxml: Label Tree-Based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification”. 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1811.01727</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">125.</div><div class="CitationContent" id="CR125">J. Yu, B. Bohnet, and M. Poesio. “Named Entity Recognition as Dependency Parsing”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2005.07150</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">126.</div><div class="CitationContent" id="CR126">Z. Yuan, Y. Liu, C. Tan, S. Huang, and F. Huang. “Improving Biomedical Pretrained Language Models with Knowledge”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2104.10344</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">127.</div><div class="CitationContent" id="CR127">M. Zaheer et al. “Big Bird: Transformers for Longer Sequences”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 33 (Jan. 8, 2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">128.</div><div class="CitationContent" id="CR128">M. Zampieri et al. “SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media (OffensEval 2020)”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2006.07235</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">129.</div><div class="CitationContent" id="CR129">W. Zhang, X. Li, Y. Deng, L. Bing, and W. Lam. <em class="EmphasisTypeItalic ">A Survey on Aspect-Based Sentiment Analysis: Tasks, Methods, and Challenges</em>. Mar. 2, 2022. <span class="ExternalRef"><a href="https://doi.org/10.48550/2203.01054"><span class="RefSource">https://​doi.​org/​10.​48550/​2203.​01054</span></a></span>. arXiv: <span class="EmphasisFontCategoryNonProportional ">2203.01054 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">130.</div><div class="CitationContent" id="CR130">W. Zhang, W. Hua, and K. Stratos. “EntQA: Entity Linking as Question Answering”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2110.02369</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">131.</div><div class="CitationContent" id="CR131">X. Zhang, J. Zhao, and Y. LeCun. “Character-Level Convolutional Networks for Text Classification”. 2015. arXiv: <span class="EmphasisFontCategoryNonProportional ">1509.01626</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">132.</div><div class="CitationContent" id="CR132">Y. Zhang, Z. Li, and M. Zhang. “Efficient Second-Order TreeCRF for Neural Dependency Parsing”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2005.00975</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">133.</div><div class="CitationContent" id="CR133">Y. Zhang, Q. Xia, S. Zhou, Y. Jiang, Z. Li, G. Fu, and M. Zhang. “Semantic Role Labeling as Dependency Parsing: Exploring Latent Tree Structures Inside Arguments”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2110.06865</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">134.</div><div class="CitationContent" id="CR134">X. Zhong, J. Tang, and A. J. Yepes. <em class="EmphasisTypeItalic ">PubLayNet: Largest Dataset Ever for Document Layout Analysis</em>. Aug. 15, 2019. <span class="ExternalRef"><a href="https://doi.org/10.48550/1908.07836"><span class="RefSource">https://​doi.​org/​10.​48550/​1908.​07836</span></a></span>. arXiv: <span class="EmphasisFontCategoryNonProportional ">1908.07836 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">135.</div><div class="CitationContent" id="CR135">W. Zhou and M. Chen. “An Improved Baseline for Sentence-level Relation Extraction”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2102.01373</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">136.</div><div class="CitationContent" id="CR136">W. Zhou, K. Huang, T. Ma, and J. Huang. “Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.11304</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">137.</div><div class="CitationContent" id="CR137">Z.-H. Zhou. “Multi-Instance Learning: A Survey”. In: <em class="EmphasisTypeItalic ">Dep. Comput. Sci. Technol. Nanjing Univ. Tech Rep</em> 1 (2004).</div></li></ol></div></aside></div></div></body></html>