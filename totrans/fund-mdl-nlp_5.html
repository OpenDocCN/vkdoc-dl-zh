<html><head></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="b978-3-031-23190-2_6"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">G. Paaß, S. Giesselbach</span><span class="ContextInformationBookTitles"><span class="BookTitle">Foundation Models for Natural Language Processing</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" lang="en">Artificial Intelligence: Foundations, Theory, and Algorithms</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-3-031-23190-2_6">https://doi.org/10.1007/978-3-031-23190-2_6</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">6. Foundation Models for Text Generation</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Gerhard Paaß</span><sup><a href="#Aff5">1</a> <span class="ContactIcon"> </span></sup> and </span><span class="Author"><span class="AuthorName">Sven Giesselbach</span><sup><a href="#Aff5">1</a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff5"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Knowledge Discovery Department, Team NLU, Fraunhofer Institute for Intelligent Analysis and Information Systems (IAIS), Sankt Augustin, Nordrhein-Westfalen, Germany</div></div><div class="ClearBoth"> </div></div></div><section class="Abstract" id="Abs1" lang="en" role="doc-abstract"><h2 class="Heading">Abstract</h2><p class="Para" id="Par1">This chapter discusses Foundation Models for Text Generation. This includes systems for Document Retrieval, which accept a query and return an ordered list of text documents from a document collection, often evaluating the similarity of embeddings to retrieve relevant text passages. Question Answering systems are given a natural language question and must provide an answer, usually in natural language. Machine Translation models take a text in one language and translate it into another language. Text Summarization systems receive a long document and generate a short summary covering the most important contents of the document. Text Generation models use an autoregressive Language Model to generate a longer story, usually starting from an initial text input. Dialog systems have the task of conducting a dialog with a human partner, typically not limited to a specific topic.</p></section><div class="KeywordGroup" lang="en"><div class="Heading">Keywords</div><span class="Keyword" epub:type="keyword">Question answering</span><span class="Keyword" epub:type="keyword">Machine translation</span><span class="Keyword" epub:type="keyword">Text summarization</span><span class="Keyword" epub:type="keyword">Text generation</span><span class="Keyword" epub:type="keyword">Dialog systems</span><span class="Keyword" epub:type="keyword">Document retrieval</span></div><!--End Abstract--><div class="Fulltext"><div class="Para" id="Par2">In this chapter we describe Foundation Models, i.e. large Pre-trained Language Models for generating new text in different application areas. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par3"><em class="EmphasisTypeItalic ">Document Retrieval</em> systems accept a query and return an ordered list of text documents from a document collection, often evaluating the similarity of embeddings to retrieve relevant text passages (Sect. <span class="InternalRef"><a href="#Sec1">6.1</a></span>).</p></li><li><p class="Para" id="Par4"><em class="EmphasisTypeItalic ">Question Answering</em> systems are given a natural language question and must provide an answer, usually in natural language (Sect. <span class="InternalRef"><a href="#Sec9">6.2</a></span>).</p></li><li><p class="Para" id="Par5"><em class="EmphasisTypeItalic ">Machine Translation</em> takes a text in one language and generates a translation into another language (Sect. <span class="InternalRef"><a href="#Sec19">6.3</a></span>).</p></li><li><p class="Para" id="Par6"><em class="EmphasisTypeItalic ">Text Summarization</em> receives a long document and has to write a short summary covering the most important contents of the document (Sect. <span class="InternalRef"><a href="#Sec25">6.4</a></span>).</p></li><li><p class="Para" id="Par7"><em class="EmphasisTypeItalic ">Text Generation</em> uses an autoregressive Language Model to generate a longer story, usually starting from an initial text input (Sect. <span class="InternalRef"><a href="#Sec31">6.5</a></span>).</p></li><li><p class="Para" id="Par8"><em class="EmphasisTypeItalic ">Dialog systems</em> have the task of conducting a dialog with a human partner, typically not limited to a specific topic (Sect. <span class="InternalRef"><a href="#Sec49">6.6</a></span>).</p></li></ul></div></div><div class="Para" id="Par9">Due to the large number of different approaches, we focus on representative models which exhibit a high performance at the time of writing. We review the current best techniques for each area, measured against appropriate benchmarks and taking into account the computational resources required. For standard models a link to the description in earlier chapters is provided. Examples for each application area are shown in Table <span class="InternalRef"><a href="#Tab1">6.1</a></span>. <div class="Table" id="Tab1"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 6.1</span><p class="SimplePara">Language generation tasks illustrated by an example</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Task</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Description</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Example</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Document retrieval</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">For a query return an ordered list of text documents</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Covid 19?</em><span class="InlineEquation" id="IEq1"><img alt="$$\rightarrow $$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq1.png" style="width:1.32em"/></span><span class="ExternalRef"><a href="http://doi.org/"><span class="RefSource">http://​doi.​org/​</span></a></span>wikipedia/covid-19, <span class="ExternalRef"><a href="http://www.cdc.gov/"><span class="RefSource">www.​cdc.​gov/​</span></a></span>, …</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Generative question answering</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Generate the answer to a question, often using some background knowledge</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">What did Albert Einstein invent?</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq2"><img alt="$$\rightarrow $$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq2.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">Einstein developed the theory of relativity</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Translation</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">For a text in the source language generate a text in the target language with the same meaning</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Fritz isst gerne Schinken</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq3"><img alt="$$\rightarrow $$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq3.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">Fritz likes to eat ham</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Summarization</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">For a long text generate a concise summary</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">It was the middle of winter, …</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq4"><img alt="$$\rightarrow $$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq4.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">Snow White is awoken by the prince, whom she marries …</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Text generation</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Starting from an initial text, a consistent continuation text is created</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Beethoven was born in Bonn</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq5"><img alt="$$\rightarrow $$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq5.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">His father was a singer at the Duke’s court</em> …</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Dialog answer generation</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Generate a consistent response in a dialogue based on the sequence of previous utterances</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Could you recommend a video for tonight?</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; text-align: left;"> </td><td style="text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq6"><img alt="$$\rightarrow $$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq6.png" style="width:1.32em"/></span><em class="EmphasisTypeItalic ">There is “Memento” on Netflix</em></p></td></tr></tbody></table></div></div><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading"><span class="HeadingNumber">6.1 </span>Document Retrieval</h2><p class="Para" id="Par10"><em class="EmphasisTypeItalic ">Information retrieval</em><span id="ITerm1"/> (<em class="EmphasisTypeItalic ">IR</em>) uses computer systems to search databases for content. The resulting IR system is often called a <em class="EmphasisTypeItalic ">search engine</em><span id="ITerm2"/>. Often, the user formulates a sentence or a <em class="EmphasisTypeItalic ">query</em><span id="ITerm3"/> about to some topic, and the system is expected to return a sorted list of documents relevant to the query (<em class="EmphasisTypeItalic ">ad hoc retrieval</em><span id="ITerm4"/>). Here we focus on retrieving textual information from a stored collection of documents. In contrast to question answering approaches in Sect. <span class="InternalRef"><a href="#Sec9">6.2</a></span>, the system does not generate a direct answer to the query in natural language.</p><div class="Para" id="Par11">Former IR systems were <em class="EmphasisTypeItalic ">keyword-based</em><span id="ITerm5"/>: all words contained in a document were stored in an <em class="EmphasisTypeItalic ">inverted index</em><span id="ITerm6"/>. The retrieval algorithm searched the index to identify documents that contained the query words. Then, these documents were ranked according to the information content of each query word found in a document, e.g. measured by tf-idf or BM25 [<span class="CitationRef"><a epub:type="biblioref" href="#CR186" role="doc-biblioref">186</a></span>]. These two steps are shown in Fig. <span class="InternalRef"><a href="#Fig1">6.1</a></span>. A survey of earlier retrieval techniques is given by Abbasiyantaeb and Momtazi [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>]. However, this approach had three major problems: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><div class="Para" id="Par12">Many objects, activities, or events may be expressed by different words called <em class="EmphasisTypeItalic ">synonyms</em><span id="ITerm7"/>, e.g. <em class="EmphasisTypeItalic ">“drink”</em> and <em class="EmphasisTypeItalic ">“beverage”</em> or <em class="EmphasisTypeItalic ">“buy”</em> and <em class="EmphasisTypeItalic ">“purchase”</em>. The documents containing alternative words are not returned by keyword retrieval. <em class="EmphasisTypeItalic ">Paraphrases</em><span id="ITerm8"/> like <em class="EmphasisTypeItalic ">“he has tons of stuff to throw away”</em> and <em class="EmphasisTypeItalic ">“he needs to get rid of a lot of junk”</em> are even harder to spot and were ignored. This is called the <em class="EmphasisTypeItalic ">vocabulary mismatch problem</em><span id="ITerm9"/>.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><img alt="" aria-describedby="d64e674" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig1_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e674"><p class="Para" id="Par318">A flow diagram represents the sequence of processes starting from the text database and going through the inverted keyword index, initial keyword retrieval, candidate texts, and P L M tracker, before generating the ranked list.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.1</span><p class="SimplePara">Retrieve-and-rerank architecture using PLMs. First, texts are retrieved from the document collection, usually with exact-match bag-of-words queries. These candidates are then reranked using PLM embeddings, e.g. from BERT. Image adapted from [<span class="CitationRef"><a epub:type="biblioref" href="#CR123" role="doc-biblioref">123</a></span>], reprinted with kind permission of authors</p></div></figcaption></figure></div></li><li><p class="Para" id="Par13">Many words have different meanings depending on the context (e.g. “rock”: music or stone). These words are called <em class="EmphasisTypeItalic ">homonyms</em><span id="ITerm10"/>. Part of the retrieved documents containing such a word will be mismatches.</p></li><li><p class="Para" id="Par14">The order of words is often crucial for the meaning of the sentences (e.g. <em class="EmphasisTypeItalic ">“dog kills person”</em> vs. “person kills dog”). This is usually ignored with keyword search.</p></li></ul></div></div><p class="Para" id="Par15">As an alternative, contextual embeddings were used to represent queries and documents. By identifying matching documents through comparison of contextual semantic representations, word meaning differences between documents and queries can be reduced and texts with synonyms, homonyms, and paraphrases can be retrieved. These models have achieved <span class="EmphasisTypeSmallCaps ">Sota</span> results on various retrieval benchmarks [<span class="CitationRef"><a epub:type="biblioref" href="#CR137" role="doc-biblioref">137</a></span>] and have recently been introduced in commercial search engines. They are therefore one of the most commercially important applications of PLMs to date.</p><section class="Section2 RenderAsSection2" id="Sec2"><h3 class="Heading"><span class="HeadingNumber">6.1.1 </span>Dense Retrieval</h3><div class="Para" id="Par16"><em class="EmphasisTypeItalic ">Dense retrieval</em><span id="ITerm11"/> methods encode text as an embedding vector with a fixed length much smaller than the text length. Whether a document is relevant to a given query is determined by the similarity of embedding vectors, which is computed by cosine similarity or inner products. Unlike question answering (Sect. <span class="InternalRef"><a href="#Sec9">6.2</a></span>), these models do not generate a direct natural language response to a search query, but return complete documents or text passages. Recently, dense retrieval methods based on PLMs outperformed their keyword counterparts when fine-tuned on a small set of in-domain relevance-labeled documents. Lin et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR124" role="doc-biblioref">124</a></span>] provide a comprehensive overview of retrieval systems with PLMs. Different approaches for dense retrieval can be distinguished and are covered in the next sections: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par17"><strong class="EmphasisTypeBold ">Cross-Encoder</strong>: Use the concatenated query and a document as input to BERT and determine the relevance of the document for the query (Sect. <span class="InternalRef"><a href="#Sec4">6.1.3</a></span>).</p></li><li><p class="Para" id="Par18"><strong class="EmphasisTypeBold ">Retrieval with token embeddings</strong>: The tokens of the query and the document are encoded by contextual embeddings. Then different metrics are used to compare these embeddings and to collect relevant documents (Sect. <span class="InternalRef"><a href="#Sec5">6.1.4</a></span>).</p></li><li><p class="Para" id="Par19"><strong class="EmphasisTypeBold ">Retrieval with passage embeddings</strong>: These techniques encode the query and passages of the document by an embedding. Subsequently, these embeddings are compared. This type of embedding respects word order and thus has the potential to return better matches (Sect. <span class="InternalRef"><a href="#Sec6">6.1.5</a></span>).</p></li></ul></div></div><div class="Para" id="Par20">Only a very small selection of methods can be described, which should give an impression of the approaches currently used as shown in Table <span class="InternalRef"><a href="#Tab2">6.2</a></span>. In Sects. <span class="InternalRef"><a href="#Sec13">6.2.2</a></span> and <span class="InternalRef"><a href="#Sec14">6.2.3</a></span> retrieval techniques for question answering are discussed, which are even more powerful. A very comprehensive survey on PLMs for retrieval is provided by Lin et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR124" role="doc-biblioref">124</a></span>]. <div class="Table" id="Tab2"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 6.2</span><p class="SimplePara">Document retrieval models with their performance. Benchmarks (Sect. <span class="InternalRef"><a href="#Sec3">6.1.2</a></span>): MARCO: MS-MARCO [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>], NQuest: Natural Questions benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>], Wiki65K: long Wikipedia documents [<span class="CitationRef"><a epub:type="biblioref" href="#CR247" role="doc-biblioref">247</a></span>]</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Description</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Benchmark</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">monoBERT (Sect. <span class="InternalRef"><a href="#Sec4">6.1.3</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Process each query-passage pair with BERT</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MARCO 35.9% MRR@10</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">monoT5 (Sect. <span class="InternalRef"><a href="#Sec4">6.1.3</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Process each query-passage pair with T5</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MARCO 38% MRR@10</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">ColBERT (Sect. <span class="InternalRef"><a href="#Sec5">6.1.4</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Reranks search results documents based on token embeddings</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MARCO 36.7% MRR@10</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model 1 (Sect. <span class="InternalRef"><a href="#Sec5">6.1.4</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Compute the probability that the query is a ‘translation’ of the document</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MARCO 39.1% MRR@100</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">SMITH (Sect. <span class="InternalRef"><a href="#Sec5">6.1.4</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Use a BERT-based hierarchical encoder</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Wiki65K 95.9% acc.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">SentenceBERT (Sect. <span class="InternalRef"><a href="#Sec6">6.1.5</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BERT encoder for query and documents</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Reduce recall time from 65 h to 5 s</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">DPR (Sect. <span class="InternalRef"><a href="#Sec6">6.1.5</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Different BERT encoders for query and documents, fine-tuned to reduce retrieval loss. FAISS index for approximate nearest neighbor search</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NQuest 79.4% top-20 acc.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">RocketQA (Sect. <span class="InternalRef"><a href="#Sec6">6.1.5</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">RoBERTa encoders for query and documents. Later reranking</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MARCO 41.9% MRR@10</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">coCondenser (Sect. <span class="InternalRef"><a href="#Sec6">6.1.5</a></span>)</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">RoBERTa encoders for query and documents using CLS token. Later reranking</p></td><td style="text-align: left;"><p class="SimplePara">MARCO 40.8% MRR@100</p></td></tr></tbody></table></div></div></section>
<section class="Section2 RenderAsSection2" id="Sec3"><h3 class="Heading"><span class="HeadingNumber">6.1.2 </span>Measuring Text Retrieval Performance</h3><p class="Para" id="Par21">There are a number of benchmark datasets used for training and comparing retrieval approaches. The <em class="EmphasisTypeItalic ">MS-MARCO</em><span id="ITerm12"/> benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>] is a large-scale collection created from about half a million anonymized questions sampled from Bing’s search query logs. For the passage ranking task it contains a corpus of 8.8M passages with an average length of 55 words extracted from 3.6M web documents. The goal is to retrieve passages that answer the question. The training set contains approximately 500k pairs of queries and relevant documents, and another 400M pairs of queries and non-relevant documents. There is a development set and a secret test set with about each 7k queries. However, there is a discussion that the gold annotation of the MS-MARCO benchmark is biased to some extent [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>].</p><p class="Para" id="Par22">The <em class="EmphasisTypeItalic ">Natural Questions</em><span id="ITerm13"/> (<em class="EmphasisTypeItalic ">NQ</em>) [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>] contains questions with at least 8 words from real users to the Google search engine. It requires QA systems to read and comprehend an entire Wikipedia article, which may or may not contain the answer to the question. An example is the question <em class="EmphasisTypeItalic ">“Where is blood pumped after it leaves the right ventricle?”</em> The task is to retrieve a long answer, i.e. a paragraph from the page that answers the question, e.g. <em class="EmphasisTypeItalic ">“From the right ventricle, blood is pumped through the semilunar pulmonary valve …”</em>, or an indication that there is no answer. The task was designed to be close to an end-to-end question answering application. One to five answers are provided by human annotators. While the original Natural Questions benchmark was a reading comprehension task providing a number of evidence documents for each question, the <em class="EmphasisTypeItalic ">EfficientQA benchmark</em><span id="ITerm14"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR147" role="doc-biblioref">147</a></span>] adapted this to open-domain QA by taking examples with up to five token answers and discarding the evidence documents.</p><p class="Para" id="Par23">Min et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR146" role="doc-biblioref">146</a></span>] note that over half of the queries in Natural Questions are ambiguous, with many sources of ambiguity such as event and entity references. They develop an <em class="EmphasisTypeItalic ">AmbigQA</em><span id="ITerm15"/> with reformulated questions that yield a unique answer.</p><div class="Para" id="Par24">A simple evaluation measure is the <em class="EmphasisTypeItalic ">top-k accuracy</em><span id="ITerm16"/>, the proportion of queries for which one of the <em class="EmphasisTypeItalic ">k</em> most likely answers returned is correct. More complex is the <em class="EmphasisTypeItalic ">mean reciprocal rank</em><span id="ITerm17"/> (<em class="EmphasisTypeItalic ">MRR</em>), the inverse of the rank of the first correct answer and 0, if no correct answer was returned. If, for instance, the third answer is correct, the reciprocal rank is 1∕3. The MRR for |<em class="EmphasisTypeItalic ">Q</em>| queries is <div class="Equation NumberedEquation" id="Equ1"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} MRR = \frac 1{|Q|}\sum_{i=1}^{|Q|}\frac 1{rank_i}. \end{aligned} $$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ1.png" style="width:10.69em"/></div></div> <div class="EquationNumber">(6.1)</div></div></div></div><p class="Para" id="Par25"><em class="EmphasisTypeItalic ">MRR@m</em> indicates that always an ordered list of <em class="EmphasisTypeItalic ">m</em> documents is returned.</p><div class="Para" id="Par26">We may define <em class="EmphasisTypeItalic ">Pr</em>(<em class="EmphasisTypeItalic ">i</em>) as the precision reached by the first <em class="EmphasisTypeItalic ">i</em> elements of the list of size <em class="EmphasisTypeItalic ">m</em>, i.e. the fraction of relevant documents of the first <em class="EmphasisTypeItalic ">i</em>. Then we may define the <em class="EmphasisTypeItalic ">average precision</em> as <div class="Equation NumberedEquation" id="Equ2"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} AP = \frac 1m \sum_{i=1}^m Pr(i) * rel(i) \qquad  MAP = \frac 1{|Q|}\sum_{j=1}^{|Q|} AP_j \end{aligned} $$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ2.png" style="width:23.51em"/></div></div> <div class="EquationNumber">(6.2)</div></div></div></div><p class="Para" id="Par27">where <em class="EmphasisTypeItalic ">rel</em>(<em class="EmphasisTypeItalic ">i</em>) = 1 if the <em class="EmphasisTypeItalic ">i</em>-th document is relevant and 0 otherwise. The <em class="EmphasisTypeItalic ">mean average precision</em><span id="ITerm18"/> (<em class="EmphasisTypeItalic ">MAP</em>) is the average of AP over |<em class="EmphasisTypeItalic ">Q</em>| different queries.</p></section>
<section class="Section2 RenderAsSection2" id="Sec4"><h3 class="Heading"><span class="HeadingNumber">6.1.3 </span>Cross-Encoders with BERT</h3><div class="Para" id="Par28"><strong class="EmphasisTypeBold ">monoBERT</strong><span id="ITerm19"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR155" role="doc-biblioref">155</a></span>] performs reranking based on a fine-tuned BERT classifier based on the embedding of the <em class="EmphasisTypeItalic ">[CLS]</em> token. Query and document are combined to the input <em class="EmphasisTypeItalic ">“[CLS]</em> &lt;<em class="EmphasisTypeItalic "> query</em>&gt;<em class="EmphasisTypeItalic ">  [SEP]</em> &lt;<em class="EmphasisTypeItalic "> document</em>&gt;<em class="EmphasisTypeItalic ">  [SEP]”</em>. This is processed by a BERT fine-tuned on MS-MARCO, where the embedding of <em class="EmphasisTypeItalic ">[CLS]</em> in the last layer is used by a logistic classifier to predict the probability that the current document is relevant for the query. This output score is used for ranking (Fig. <span class="InternalRef"><a href="#Fig2">6.2</a></span>). Note that by this technique paraphrases like <em class="EmphasisTypeItalic ">“symptoms of influenza include fever and nasal congestion”</em> and <em class="EmphasisTypeItalic ">“a stuffy nose and elevated temperature are signs you may have the flu”</em> may be identified.<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO2"><img alt="" aria-describedby="d64e1282" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig2_HTML.png" style="width:34.38em"/><div class="TextObject" id="d64e1282"><p class="Para" id="Par319">An illustration indicates the position, segment, tokens, sum of embeddings, output embeddings, and probability of relevance of the query and passage going through the BERT layers with masked self-attentions.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.2</span><p class="SimplePara">The monoBERT model uses a fine-tuned BERT model for ranking passages with respect to queries. The input contains the query concatenated with the passage. The <em class="EmphasisTypeItalic ">[CLS]</em> token embedding is trained to return the probability that the passage answers the query</p></div></figcaption></figure></div><p class="Para" id="Par29">On the MS-MARCO benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR153" role="doc-biblioref">153</a></span>] monoBERT yields an MRR@10 value of 35.9% (i.e. the first relevant document at position 2.8 on average). As the keyword-based BM25-search before had an MRR@10-value of 16.5% (first relevant document at position 6.1 on average), this result was a dramatic increase in performance of search engines. Such a big jump in effectiveness caused by an individual model is rarely observed in either academia or industry, which led to immediate excitement in the community.</p><p class="Para" id="Par30">It is quite striking how monoBERT provides a simple yet effective solution to the problem of text ranking (at least for texts that are shorter than its maximal input length) [<span class="CitationRef"><a epub:type="biblioref" href="#CR124" role="doc-biblioref">124</a></span>]. In several studies monoBERT has been found to be better than BM25 in estimating relevance when term frequency is held constant. Using textual manipulation tests that alter existing documents, rearranging the order of words within a sentence or across sentences was found to have a large negative effect, while shuffling the order of sentences within a document has a modest negative effect. In contrast, rearranging only prepositions had little effect. Experimental results from input template variations show that monoBERT uses exact match, “soft” semantic matches, and information about the position of words. Exactly how these different components are combined—for different types of queries, across different corpora, and under different settings, etc.—remains an open question. Note that this search approach requires enormous computational resources, as for each passage a new evaluation has to be performed, while the effort for index search grows only logarithmically.</p><p class="Para" id="Par31"><strong class="EmphasisTypeBold ">monoT5</strong><span id="ITerm20"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR154" role="doc-biblioref">154</a></span>] used the T5 encoder-decoder model instead of BERT to rerank retrieved documents. The model receives the input <em class="EmphasisTypeItalic ">“Query:</em> &lt;<em class="EmphasisTypeItalic "> query</em>&gt;<em class="EmphasisTypeItalic ">  Document:</em> &lt;<em class="EmphasisTypeItalic "> document</em>&gt;<em class="EmphasisTypeItalic ">  Relevant:”</em>. monoT5 is fine-tuned to produce the tokens <em class="EmphasisTypeItalic ">true</em> or <em class="EmphasisTypeItalic ">false</em> if the document is relevant to the query or not. The predicted probability of <em class="EmphasisTypeItalic ">true</em> can be used as a relevance score. For T5 with 3B parameters the authors get an MRR@10-value of 38% for MS-MARCO passage retrieval. This shows that larger models increase performance of retrieval systems.</p></section>
<section class="Section2 RenderAsSection2" id="Sec5"><h3 class="Heading"><span class="HeadingNumber">6.1.4 </span>Using Token Embeddings for Retrieval</h3><p class="Para" id="Par32">The all-to-all nature of the BERT attention patterns at each transformer encoder layer means that there is a quadratic complexity in terms of time and space with respect to the input length. In Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec7"><span class="RefSource">3.​2</span></a></span> we have introduced a number of approaches to cope with longer inputs. These all can be used to process longer documents. Among the many approaches we discuss ColBERT and Model 1 in more detail.</p><div class="Para" id="Par33"><strong class="EmphasisTypeBold ">ColBERT</strong><span id="ITerm21"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR99" role="doc-biblioref">99</a></span>] reranks the output of another (cheaper) retrieval model, typically a term-based model, or directly for end-to-end retrieval from a document collection. Queries and documents were prepended by different special tokens. ColBERT uses a single pre-trained BERT model to encode each query or document into a bag of token embeddings. In a final layer the size of embeddings is reduced and they are normalized to Euclidean length 1.0. Hence, the inner product is equivalent to the cosine similarity. If (<em class="EmphasisTypeItalic ">q</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">q</em><sub><em class="EmphasisTypeItalic ">m</em></sub>) are the query tokens and <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">i</em>,1</sub>, …, <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">i</em>,<em class="EmphasisTypeItalic ">k</em></sub> are the tokens of the <em class="EmphasisTypeItalic ">i</em>-th document, the similarity of <em class="EmphasisTypeItalic ">q</em> and <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">i</em></sub> is computed as <div class="Equation NumberedEquation" id="Equ3"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} s_{q,d_i} = \sum_{r=1}^m \max_j {\boldsymbol{\eta}}(q_r)^\intercal{\boldsymbol{\eta}}(d_{i,j}). \end{aligned} $$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ3.png" style="width:12.81em"/></div></div> <div class="EquationNumber">(6.3)</div></div></div></div><p class="Para" id="Par34">This is the sum of maximum cosine similarities (MaxSim) between each query term and the “best” matching term contained in the document <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">i</em></sub>. For each query embedding the L2-nearest 10 embeddings are taken into account and <em class="EmphasisTypeItalic ">k</em> = 1000 closest document vectors are retrieved.</p><p class="Para" id="Par35">For ranking a preliminary search result of, say 1000 documents, the maximum similarities (e.g. cosine similarity) between all query embeddings and all embeddings in the retrieved documents are computed. This approach is very efficient as it requires orders of magnitude fewer FLOPS than previous approaches. On the MS-MARCO benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR153" role="doc-biblioref">153</a></span>] a reranking ColBERT achieves a MRR@10-value of 34.9% (first relevant document at position 2.9 on average), which is slightly below the cross-encoder monoBERT.</p><p class="Para" id="Par36">ColBERT can also be used for end-to-end retrieval. It employs the <em class="EmphasisTypeItalic ">FAISS</em><span id="ITerm22"/> index [<span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>] to store the document token embeddings for a <em class="EmphasisTypeItalic ">k</em>-nearest neighbor search in a preparatory step. Note that for each token in each document an embedding has to be stored, as the embedding depends on the context. The retrieval requires two stages: in the first stage, a number of approximate searches for each query token is performed. In the second refinement stage, these approximate matches are reranked according to the MaxSim criterion. On the MS-MARCO benchmark the end-to-end retrieval by ColBERT has a MRR@10-value of 36.7%, which is much better than the reranking performance and on par with the much more expensive BERT cross-encoder approach.</p><div class="Para" id="Par37"><strong class="EmphasisTypeBold ">Model 1</strong><span id="ITerm23"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR28" role="doc-biblioref">28</a></span>] mixes a number of techniques for their retrieval model based on token embeddings. First the authors estimate the probability <em class="EmphasisTypeItalic ">p</em>(<em><strong class="EmphasisTypeBoldItalic ">q</strong></em>|<em><strong class="EmphasisTypeBoldItalic ">d</strong></em>) that the query <em><strong class="EmphasisTypeBoldItalic ">q</strong></em> has been generated as a “translation” of the document <em><strong class="EmphasisTypeBoldItalic ">d</strong></em>. Using Bayes rule the authors get <div class="Equation NumberedEquation" id="Equ4"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} p({\boldsymbol{d}}|{\boldsymbol{q}})\propto p({\boldsymbol{q}}|{\boldsymbol{d}})p({\boldsymbol{d}})\propto p({\boldsymbol{q}}|{\boldsymbol{d}}) \end{aligned} $$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ4.png" style="width:13.56em"/></div></div> <div class="EquationNumber">(6.4)</div></div></div></div><p class="Para" id="Par38">assuming a uniform prior <em class="EmphasisTypeItalic ">p</em>(<em><strong class="EmphasisTypeBoldItalic ">d</strong></em>) [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>]. They consider the probability <em class="EmphasisTypeItalic ">r</em>(<em class="EmphasisTypeItalic ">q</em><sub><em class="EmphasisTypeItalic ">i</em></sub>|<em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">j</em></sub>) that a query token <em class="EmphasisTypeItalic ">q</em><sub><em class="EmphasisTypeItalic ">i</em></sub> is a translation of a document token <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">j</em></sub>. Approximating <em class="EmphasisTypeItalic ">r</em>(<em class="EmphasisTypeItalic ">q</em><sub><em class="EmphasisTypeItalic ">i</em></sub>|<em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">j</em></sub>) by a neural network, they use embeddings of tokens <em class="EmphasisTypeItalic ">q</em><sub><em class="EmphasisTypeItalic ">i</em></sub> and <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">j</em></sub> as inputs and are able to estimate <em class="EmphasisTypeItalic ">p</em>(<em><strong class="EmphasisTypeBoldItalic ">d</strong></em>|<em><strong class="EmphasisTypeBoldItalic ">q</strong></em>). The approach requires little computational effort. The authors combined the BERT dense retriever with a Lucene search index. Finally, they expand documents for Model 1 with Doc2query. <em class="EmphasisTypeItalic ">Doc2query</em><span id="ITerm24"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR156" role="doc-biblioref">156</a></span>] aims at generating queries, for which the document is relevant. The approach trains a transformer to generate up to 100 query tokens from a document of up to 400 tokens. The model is trained using datasets consisting of pairs of query and relevant documents, e.g. MS-MARCO. On MS-MARCO they achieve 39.1% MRR@100. The context-free neural Model 1 is less effective than a BERT-based ranking model, but it can run efficiently on a CPU (without expensive index-time precomputation or query-time operations on large tensors).</p><p class="Para" id="Par39">Currently, no retriever tries to process long documents. This has many important applications like news recommendation, related article recommendation and paper citation suggestion. Usually, long documents are partitioned into passages with the idea that the relevant contents is contained in a passage. Note that PLMs with longer inputs, e.g. BigBird, can improve performance (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec7"><span class="RefSource">3.​2</span></a></span>). However, it is clear that this has to be evaluated. The <strong class="EmphasisTypeBold ">SMITH</strong><span id="ITerm25"/> model [<span class="CitationRef"><a epub:type="biblioref" href="#CR247" role="doc-biblioref">247</a></span>] uses a BERT-based hierarchical encoder to capture the document structure information. The document is first partitioned into sentences and for each sentence token embeddings are computed. Each sentence starts with an <em class="EmphasisTypeItalic ">[CLS]</em> token, whose embedding represents the sentence. There is a higher sentence level BERT which just receives the sentence embeddings as input. The first artificial token of second level BERT is used as the embedding of the whole document.</p><p class="Para" id="Par40">The model is pre-trained by the masked language modeling task to get token embeddings. In addition, in the second level there is a masked sentence block prediction task where the model has to select the correct embedding from all sentence embeddings in a batch. The fine-tuning task maximizes the relevance score predicted from the document embedding by a logistic classifier for the relevance-annotated fine-tuning dataset. On the <em class="EmphasisTypeItalic ">Wiki65K</em><span id="ITerm26"/> with long Wikipedia articles [<span class="CitationRef"><a epub:type="biblioref" href="#CR87" role="doc-biblioref">87</a></span>] the approach achieves an accuracy of 95.9% which is a significant improvement over prior approaches.</p></section>
<section class="Section2 RenderAsSection2" id="Sec6"><h3 class="Heading"><span class="HeadingNumber">6.1.5 </span>Dense Passage Embeddings and Nearest Neighbor Search</h3><p class="Para" id="Par41">Representing text passages by embedding vectors has the potential to solve the problem of vocabulary mismatch by directly matching “meaning” in a representation space. These so-called <em class="EmphasisTypeItalic ">dense retrieval</em><span id="ITerm27"/> techniques can perform ranking directly on vector representations generated by PLMs. In contrast to calculating pairwise differences of token embeddings, this approach offers a much more efficient retrieval procedure. This is performed by matching the embedding vector of a query with the embedding vectors of passages employing an index and approximate nearest neighbor search. Efficient, scalable solutions are available today in open-source libraries.</p><div class="Para" id="Par42">Given a query <em class="EmphasisTypeItalic ">q</em> and a set of documents <em class="EmphasisTypeItalic ">D</em> = {<em class="EmphasisTypeItalic ">d</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">n</em></sub>} we want to define functions <em><strong class="EmphasisTypeBoldItalic ">η</strong></em><sub><em class="EmphasisTypeItalic ">q</em></sub>(⋅) and <em><strong class="EmphasisTypeBoldItalic ">η</strong></em><sub><em class="EmphasisTypeItalic ">d</em></sub>(⋅), which convert the token sequences <em class="EmphasisTypeItalic ">q</em> and <em class="EmphasisTypeItalic ">d</em> into fixed-width vectors. The functions should have the property that the similarity between <em><strong class="EmphasisTypeBoldItalic ">η</strong></em><sub><em class="EmphasisTypeItalic ">q</em></sub>(<em class="EmphasisTypeItalic ">q</em>) and <em><strong class="EmphasisTypeBoldItalic ">η</strong></em><sub><em class="EmphasisTypeItalic ">d</em></sub>(<em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">i</em></sub>) is maximal if <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">i</em></sub> is relevant for query <em class="EmphasisTypeItalic ">q</em>. We want to estimate <div class="Equation NumberedEquation" id="Equ5"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} p(\text{relevant}=1|d_i,q) := \phi({\boldsymbol{\eta}}_q(q),{\boldsymbol{\eta}}_d(d_i)), \end{aligned} $$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ5.png" style="width:17.74em"/></div></div> <div class="EquationNumber">(6.5)</div></div></div></div><p class="Para" id="Par43">where <em class="EmphasisTypeItalic ">ϕ</em>(⋅) is a similarity comparison function, e.g. the scalar product [<span class="CitationRef"><a epub:type="biblioref" href="#CR124" role="doc-biblioref">124</a></span>, p. 133]. Note that <em><strong class="EmphasisTypeBoldItalic ">η</strong></em><sub><em class="EmphasisTypeItalic ">d</em></sub>(<em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">i</em></sub>) may be precomputed and organized in an index. By using different encoders <em><strong class="EmphasisTypeBoldItalic ">η</strong></em><sub><em class="EmphasisTypeItalic ">q</em></sub>(⋅) and <em><strong class="EmphasisTypeBoldItalic ">η</strong></em><sub><em class="EmphasisTypeItalic ">d</em></sub>(⋅) for queries and documents, we can take into account the different roles and wordings of queries and documents.</p><div class="Para" id="Par44"><strong class="EmphasisTypeBold ">SentenceBERT</strong><span id="ITerm28"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR183" role="doc-biblioref">183</a></span>] is the prototype of a bi-encoder design for generating semantically meaningful sentence embeddings to be used in large-scale textual similarity comparisons (Fig. <span class="InternalRef"><a href="#Fig3">6.3</a></span>). The query <em class="EmphasisTypeItalic ">q</em> and the documents <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">i</em></sub> are processed by the same PLM (BERT or RoBERTa). Similarity was compared by the <em class="EmphasisTypeItalic ">cosine similarity</em><span id="ITerm29"/><div class="Equation NumberedEquation" id="Equ6"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \phi({\boldsymbol{\eta}}_q(q),{\boldsymbol{\eta}}_d(d_i))=\frac{{\boldsymbol{\eta}}_q(q)^\intercal {\boldsymbol{\eta}}_d(d_i)}{\left\lVert {\boldsymbol{\eta}}_q(q)\right\rVert *\left\lVert {\boldsymbol{\eta}}_d(d_i)\right\rVert }. \end{aligned} $$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ6.png" style="width:17.19em"/></div></div> <div class="EquationNumber">(6.6)</div></div></div><figure class="Figure" id="Fig3"><div class="MediaObject" id="MO3"><img alt="" aria-describedby="d64e2086" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig3_HTML.png" style="width:34.38em"/><div class="TextObject" id="d64e2086"><p class="Para" id="Par320">An illustration represents the flow of query and passage through the BERT layers with masked self-attentions. It indicates the layers of position, tokens, sum of embeddings, output embeddings, cosine similarity, and similarity value.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.3</span><p class="SimplePara">The SentenceBERT model uses two fine-tuned BERT models to transform queries and passages to embeddings of the <em class="EmphasisTypeItalic ">[CLS]</em> token. Subsequently, a cosine similarity module is used to compute a similarity value</p></div></figcaption></figure></div><p class="Para" id="Par45">To generate sentence embeddings the authors investigated three alternatives. (1) Use the embedding of the <em class="EmphasisTypeItalic ">[CLS]</em> token. (2) Averaging (mean-pooling) of all output embeddings. (3) Component-wise maximum (max-pooling) of all output embeddings. Without fine-tuning the results were worse than for non-contextual embeddings. Fine-tuning boosted performance and yields a new <span class="EmphasisTypeSmallCaps ">Sota</span>. It turned out that average pooling was the most effective design, slightly better than max pooling or using the <em class="EmphasisTypeItalic ">[CLS]</em> token. Most important the computation time for finding the best match in 10,000 documents was reduced from 65 h to 5 s.</p><div class="Para" id="Par46"><strong class="EmphasisTypeBold ">DPR</strong><span id="ITerm30"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR94" role="doc-biblioref">94</a></span>] used separate encoders <em><strong class="EmphasisTypeBoldItalic ">η</strong></em><sub><em class="EmphasisTypeItalic ">q</em></sub>(<em class="EmphasisTypeItalic ">q</em>) and <em><strong class="EmphasisTypeBoldItalic ">η</strong></em><sub><em class="EmphasisTypeItalic ">d</em></sub>(<em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">i</em></sub>) for the query <em class="EmphasisTypeItalic ">q</em> and the text passages <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">i</em></sub> of about 100 words. Both encoders took the <em class="EmphasisTypeItalic ">[CLS]</em> embedding from BERT<sub>BASE</sub> as its output representation. As comparison function the inner product <span class="InlineEquation" id="IEq7"><img alt="$${\boldsymbol {\eta }}_q(q)^\intercal {\boldsymbol {\eta }}_d(d_i)$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq7.png" style="width:5.62em"/></span> was used. For each query <em class="EmphasisTypeItalic ">q</em><sub><em class="EmphasisTypeItalic ">i</em></sub> the training set contained one correct passage <span class="InlineEquation" id="IEq8"><img alt="$$d^+_i$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq8.png" style="width:1.24em"/></span> and a number of negative passages <span class="InlineEquation" id="IEq9"><img alt="$$d^-_{i,1},\ldots ,d^-_{i,m}$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq9.png" style="width:5.56em"/></span>. The loss function encoded the goal to get a large <em class="EmphasisTypeItalic ">ϕ</em>-value (i.e. similarity) for <em class="EmphasisTypeItalic ">q</em><sub><em class="EmphasisTypeItalic ">i</em></sub> and <span class="InlineEquation" id="IEq10"><img alt="$$d^+_i$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq10.png" style="width:1.24em"/></span> and small similarities for <em class="EmphasisTypeItalic ">q</em><sub><em class="EmphasisTypeItalic ">i</em></sub> and <span class="InlineEquation" id="IEq11"><img alt="$$d^-_{i,j}$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq11.png" style="width:1.56em"/></span><div class="Equation NumberedEquation" id="Equ7"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} L(w) = -\log \frac{\exp[ {\boldsymbol{\eta}}_q(q)^\intercal{\boldsymbol{\eta}}_d(d^+_i)]} {\exp[ {\boldsymbol{\eta}}_q(q)^\intercal{\boldsymbol{\eta}}_d(d_i)] + \sum_{j=1}^m \exp [{\boldsymbol{\eta}}_q(q)^\intercal{\boldsymbol{\eta}}_d(d^-_{i,j})]} {} \end{aligned} $$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ7.png" style="width:25.76em"/></div></div> <div class="EquationNumber">(6.7)</div></div></div></div><p class="Para" id="Par47">The negative examples were a mixture of passages retrieved with keyword search that did not contain the answer and thus were difficult negatives. In addition, passages from other examples in the same training batch were used. Instead of performing an exhaustive computation of similarities for all documents between <em class="EmphasisTypeItalic ">η</em><sub><em class="EmphasisTypeItalic ">q</em></sub>(<em class="EmphasisTypeItalic ">q</em>) and the <em class="EmphasisTypeItalic ">η</em><sub><em class="EmphasisTypeItalic ">d</em></sub>(<em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">i</em></sub>), we can employ an approximate nearest neighbor search. <em class="EmphasisTypeItalic ">FAISS</em><span id="ITerm31"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>] is an open-source method based on hierarchical navigable small world graphs. For the Natural Questions benchmark they achieved a top-20 accuracy of 79.4%, which is much better than the previous top-20 accuracy of 59.1% for the keyword-based BM25 search. The replication study [<span class="CitationRef"><a epub:type="biblioref" href="#CR136" role="doc-biblioref">136</a></span>] could confirm these results, but found that a hybrid approach of DPR and BM25 could increase the performance to 82.6%.</p><p class="Para" id="Par48"><strong class="EmphasisTypeBold ">ANCE</strong><span id="ITerm32"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR238" role="doc-biblioref">238</a></span>] uses a single RoBERTa model to encode query and document. During training, hard negative examples are selected by approximate nearest neighbor search on an index over the representations generated by the trained encoder. In this way, they can select “difficult” negative examples. The index is periodically updated. On Natural Questions ANCE achieved 82.1% top-20 accuracy. The performance was also compared with the monoBERT cross-encoder, which reranks first-stage BM25 results with monoBERT by comparing all documents to the query. It turned out that on MS-MARCO the application of monoBERT to BM25 had a MRR@10 of 34.7% while ANCE has 33%. The cross-encoder obviously is more effective than ANCE. The authors also applied ANCE to 8 billion documents using embeddings of size 64 and approximate nearest neighbor search. They reported a gain of 16% compared to the prior commercial implementation.</p><p class="Para" id="Par49"><strong class="EmphasisTypeBold ">RocketQA</strong><span id="ITerm33"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR184" role="doc-biblioref">184</a></span>] performs a first retrieval step and subsequently a re-ranking procedure. Both approaches are jointly optimized using a listwise training approach, where a list of positive and negative examples is used for training both modules. In addition, they perform a data augmentation to construct diverse training instances by incorporating both random sampling and denoised sampling. They report a MRR@10 on MS-MARCO of 38.8% for passage retrieval. When the 50 top results are reranked later, they can increase MRR@10 to 41.9%.</p><p class="Para" id="Par50"><strong class="EmphasisTypeBold ">coCondenser</strong><span id="ITerm34"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>] is one of the highest entries of the MS-MARCO leaderboard [<span class="CitationRef"><a epub:type="biblioref" href="#CR140" role="doc-biblioref">140</a></span>]. The model is forced to learn to aggregate information into the <em class="EmphasisTypeItalic ">“CLS”</em> embedding, which will then participate in the LM prediction. Then an additional “contrastive loss” is used: <em class="EmphasisTypeItalic ">“CLS”</em> embeddings of passages from the same document close together should be similar, while those for passages in different documents should have a larger distance. This yields highly expressive embeddings for passages. When the model is fine-tuned on MS-MARCO, it returns an <em class="EmphasisTypeItalic ">MRR@</em>100 of 40.8% on the MS-MARCO leaderboard [<span class="CitationRef"><a epub:type="biblioref" href="#CR140" role="doc-biblioref">140</a></span>].</p><section class="Section3 RenderAsSection3" id="Sec7"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par51"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par52">DPR code is available at <span class="ExternalRef"><a href="https://github.com/facebookresearch/DPR"><span class="RefSource">https://​github.​com/​facebookresearch​/​DPR</span></a></span>.</p></li><li><p class="Para" id="Par53">The code for the FAISS nearest neighbor search is available at <span class="ExternalRef"><a href="https://github.com/facebookresearch/faiss"><span class="RefSource">https://​github.​com/​facebookresearch​/​faiss</span></a></span>.</p></li><li><p class="Para" id="Par54">ANCE code and data trained nearest neighbor search is available at <span class="ExternalRef"><a href="https://github.com/microsoft/ANCE"><span class="RefSource">https://​github.​com/​microsoft/​ANCE</span></a></span>.</p></li><li><p class="Para" id="Par55">RocketQA code and data is available at <span class="ExternalRef"><a href="https://github.com/PaddlePaddle/RocketQA"><span class="RefSource">https://​github.​com/​PaddlePaddle/​RocketQA</span></a></span>.</p></li><li><p class="Para" id="Par56">FlexNeuART [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>] implements the Model 1 retrieval system [<span class="CitationRef"><a epub:type="biblioref" href="#CR28" role="doc-biblioref">28</a></span>].</p></li><li><p class="Para" id="Par57">coCondenser code at <span class="ExternalRef"><a href="https://github.com/luyug/Condenser"><span class="RefSource">https://​github.​com/​luyug/​Condenser</span></a></span>.</p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec8"><h3 class="Heading"><span class="HeadingNumber">6.1.6 </span>Summary</h3><p class="Para" id="Par58">Retrieval is a crucial step in web search, in which a small set of query-relevant candidate passages are identified from a corpus of billions of texts. Discovering more semantically related candidates in the retrieval phase holds great promise for presenting more high-quality results to the end user. Dense retrieval approaches represent a paradigm shift in search engine technology. They make it possible to recognize the meaning of words and paraphrases and thus find much better passages matching a query. Search results can also be used for question-answer models (Sect. <span class="InternalRef"><a href="#Sec9">6.2</a></span>) and dialog systems (Sect. <span class="InternalRef"><a href="#Sec49">6.6</a></span>). They are already being used in production search engine by Bing [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR238" role="doc-biblioref">238</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR266" role="doc-biblioref">266</a></span>], Google [<span class="CitationRef"><a epub:type="biblioref" href="#CR152" role="doc-biblioref">152</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR197" role="doc-biblioref">197</a></span>], and Facebook [<span class="CitationRef"><a epub:type="biblioref" href="#CR82" role="doc-biblioref">82</a></span>].</p><p class="Para" id="Par59">Dense retrieval methods discussed above are fine-tuned in a supervised setting using human relevance labels as input, e.g. from MS-MARCO. Best results are obtained by two different PLMs to encode the query and the documents. Both PLMs are trained to improve the probability of a correct reference document in contrast to some negative documents. As two different PLMs require more effort, most systems use a single model to encode the question and the documents. Experiments show that the combination of dense retrieval and keyword retrieval seems to have advantages. In Sects. <span class="InternalRef"><a href="#Sec13">6.2.2</a></span> and <span class="InternalRef"><a href="#Sec14">6.2.3</a></span> retrieval techniques for question answering are discussed, which are even more powerful.</p><p class="Para" id="Par60">A problem is the transferability of a search system to a new domain. BERT was found to have strong cross-domain relevance classification capabilities when used in a similar way as monoBERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR124" role="doc-biblioref">124</a></span>, p. 72]. If a BERT model is fine-tuned using relevance judgments from one domain (e.g., tweets) it can be successfully applied to a different domain (e.g., newswire articles). On the other hand, Thakur et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR221" role="doc-biblioref">221</a></span>] created a benchmark called <em class="EmphasisTypeItalic ">BEIR</em><span id="ITerm35"/> with 18 retrieval tasks from very different domains like bio-medicine and tweets. The authors trained a large number of dense retrieval techniques on MS-MARCO and evaluated then on the other tasks. They found that they were on average less effective than BM25, which due to its simplicity just works in most cases.</p><p class="Para" id="Par61">The memory requirements for an index for embeddings cannot be ignored. While a keyword Lucene index for the MS-MARCO passage corpus with 8.8M passages needs 661 MB, a FAISS index for vectors of size 768 requires 42 GB and an index for ColBERT takes 156 GB [<span class="CitationRef"><a epub:type="biblioref" href="#CR124" role="doc-biblioref">124</a></span>, p. 159]. To apply these techniques to web-scale, approaches with a smaller memory footprint are needed.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec9"><h2 class="Heading"><span class="HeadingNumber">6.2 </span>Question Answering</h2><p class="Para" id="Par62"><em class="EmphasisTypeItalic ">Question Answering</em><span id="ITerm36"/> (QA) is an application of NLP that receives a natural language query and automatically generates a precise answer in natural language. It is a long-standing AI task dating back to the 1960s [<span class="CitationRef"><a epub:type="biblioref" href="#CR69" role="doc-biblioref">69</a></span>]. Compared with search engines discussed in Sect. <span class="InternalRef"><a href="#Sec1">6.1</a></span>, the QA system presents the final answer to a question directly instead of returning a list of relevant snippets or hyperlinks. Thus, it is more user-friendly and efficient. Often, the system has access to a database or a <em class="EmphasisTypeItalic ">knowledge base</em><span id="ITerm37"/> (<em class="EmphasisTypeItalic ">KB</em>) of documents, such as Wikipedia, where it can search for relevant information.</p><div class="Para" id="Par63">A <em class="EmphasisTypeItalic ">Closed Domain QA system</em><span id="ITerm38"/> handles questions for a specific domain, e.g. medicine, and has background knowledge about that domain or is trained with a large training set covering that domain. <em class="EmphasisTypeItalic ">Open Domain QA systems</em><span id="ITerm39"/> (ODQA) deal with questions on almost any topic and usually rely on general KBs or Internet search [<span class="CitationRef"><a epub:type="biblioref" href="#CR37" role="doc-biblioref">37</a></span>]. <em class="EmphasisTypeItalic ">Multimodal QA</em><span id="ITerm40"/> systems address questions in different media, e.g., text and images. A survey of ODQA is given by Zhu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR265" role="doc-biblioref">265</a></span>]. Table <span class="InternalRef"><a href="#Tab3">6.3</a></span> compiles leading QA Models with their performance. <div class="Table" id="Tab3"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 6.3</span><p class="SimplePara">Question answering models with their performance. The lower part contains retrieval models. Benchmarks: NQ: natural Questions benchmark of Google queries [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>], TriviaQA: TriviaQA benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR226" role="doc-biblioref">226</a></span>], HotpotQA: multihop benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR249" role="doc-biblioref">249</a></span>], EM: exact match</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Details</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Benchmark</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BigBird (Sect. <span class="InternalRef"><a href="#Sec10">6.2.1</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Autoencoder with long input, supervised training with QA pairs</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NQ with ref-docs 57.9% EM WikiHop 82.3% acc.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PoolingFormer (Sect. <span class="InternalRef"><a href="#Sec10">6.2.1</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Autoencoder with two-level attention schema, supervised training with QA pairs</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NQ with ref-docs 61.6% EM</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">RealFormer (Sect. <span class="InternalRef"><a href="#Sec10">6.2.1</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Autoencoder with bypass attention, supervised training with QA pairs, multihop QA</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">WikiHop 84.4% acc.</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GPT-3 (Sect. <span class="InternalRef"><a href="#Sec10">6.2.1</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Large autoencoder 175B, only pre-training</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NQ few-shot 29.9% TriviaQA few-shot 71.2%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Gopher (Sect. <span class="InternalRef"><a href="#Sec10">6.2.1</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Large autoencoder 280B, only pre-training</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NQ few-shot 28.2%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PaLM (Sect. <span class="InternalRef"><a href="#Sec10">6.2.1</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Large autoencoder 540B, only pre-training</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NQ few-shot 36.0% TriviaQA few-shot 81.4%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">DPR (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec22"><span class="RefSource">3.​4.​5</span></a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Retriever-reader with two BERT models and FAISS index</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NQ exact match acc 41.5% TriviaQA 57.9%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">FiD (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec22"><span class="RefSource">3.​4.​5</span></a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Retriever-reader with T5 models and FAISS index</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NQ exact match acc 51.4% TriviaQA 67.6%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">REALM (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec22"><span class="RefSource">3.​4.​5</span></a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Retriever-reader with dot product of BERT embeddings, slow</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NQ exact match acc 40.4%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">FB HYBRID (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec22"><span class="RefSource">3.​4.​5</span></a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">DPR retriever combined with other retriever, FiD reader</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NQ exact match acc 53.9%, corresponds to 67.4% correct</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MS UNITED (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec22"><span class="RefSource">3.​4.​5</span></a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BERT-based retriever, T5+ELECTRA as readers, final re-ranking</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NQ exact match acc 54.0%, corresponds to 65.8% correct</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">AISO (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec22"><span class="RefSource">3.​4.​5</span></a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Retriever-reader with repeated retrieval rounds, multihop QA</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">HotpotQA 72.0% F1</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">RETRO (Sect. <span class="InternalRef"><a href="#Sec15">6.2.3</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Language model with frozen BERT retriever, language model periodically includes retrieved token chunks</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NQ exact match acc 45.5%</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">WEBGPT (Sect. <span class="InternalRef"><a href="#Sec15">6.2.3</a></span>)</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">GPT-3 combined with Bing search engine, which can be periodically invoked</p></td><td style="text-align: left;"><p class="SimplePara">TriviaQA 69.5%</p></td></tr></tbody></table></div></div><p class="Para" id="Par64">A simple form of question answering is <em class="EmphasisTypeItalic ">Reading Comprehension</em><span id="ITerm41"/>, where the system has to identify an answer to a question in a given text. Often a BERT-like system marks the answer span in the text by span prediction (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec5"><span class="RefSource">2.​1.​3</span></a></span>). This task can mainly be considered as solved. For the <em class="EmphasisTypeItalic ">SQuAD 2.0 benchmark</em><span id="ITerm42"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR179" role="doc-biblioref">179</a></span>] ALBERT yields more than 93% F1-value and the fine-tuned <em class="EmphasisTypeItalic ">ST-MoE-32B</em><span id="ITerm43"/> mixture-of-experts model (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec26"><span class="RefSource">3.​5.​2</span></a></span>) with 269B parameters [<span class="CitationRef"><a epub:type="biblioref" href="#CR270" role="doc-biblioref">270</a></span>] achieves 96.3% F1-value, while the human F1-value is 89.5% [<span class="CitationRef"><a epub:type="biblioref" href="#CR178" role="doc-biblioref">178</a></span>]. However, Sen et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR199" role="doc-biblioref">199</a></span>] indicate that systems trained on one dataset may not generalize well to other benchmarks.</p><section class="Section2 RenderAsSection2" id="Sec10"><h3 class="Heading"><span class="HeadingNumber">6.2.1 </span>Question Answering Based on Training Data Knowledge</h3><p class="Para" id="Par65">Language models often are trained on comprehensive text collections and are able to memorize a large amount of information. A frequently used benchmark is <em class="EmphasisTypeItalic ">Natural Questions</em><span id="ITerm44"/> (<em class="EmphasisTypeItalic ">NQ</em>) [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>], which has been sampled from the Google search logs (Sect. <span class="InternalRef"><a href="#Sec3">6.1.2</a></span>). For the given question, the system has to find a short answer span in the given support documents. An example is the question <em class="EmphasisTypeItalic ">“When are hops added to the brewing process?”</em>, which should yield the answer <em class="EmphasisTypeItalic ">“The boiling process”</em>.</p><p class="Para" id="Par66">The <em class="EmphasisTypeItalic ">TriviaQA</em><span id="ITerm45"/> benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR226" role="doc-biblioref">226</a></span>] contains a set of trivia questions with answers that were originally scraped from the Web. Different from Natural Questions, the questions here are written with known answers in mind. <em class="EmphasisTypeItalic ">TruthfulQA</em><span id="ITerm46"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR125" role="doc-biblioref">125</a></span>] is a special QA benchmark with short questions that are constructed adversarially, so that some people’s answers might be wrong due to false beliefs and misconceptions. The answers are evaluated according to informativeness and truthfulness.</p><section class="Section3 RenderAsSection3" id="Sec11"><h4 class="Heading">Fine-Tuned Question Answering Models</h4><p class="Para" id="Par67">The <strong class="EmphasisTypeBold ">BigBird</strong><span id="ITerm47"/> (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec7"><span class="RefSource">3.​2</span></a></span>) self-attention was used as an autoencoder and trained with the MLM objective using an input sequence of 4096 tokens [<span class="CitationRef"><a epub:type="biblioref" href="#CR253" role="doc-biblioref">253</a></span>]. During fine-tuning on Natural Questions the model had to find a short answer span in one of the given evidence documents. The model achieved 57.9% F1-value on this task. The <strong class="EmphasisTypeBold ">PoolingFormer</strong><span id="ITerm48"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR256" role="doc-biblioref">256</a></span>] is an alternative model for long input sequences with a two-level attention schema. Its first level uses a smaller sliding window pattern to aggregate information from neighbors. Its second level employs a larger window to increase receptive fields with pooling attention. An ensemble of fine-tuned PoolingFormers achieves 61.6% F1-value on the Natural Questions benchmark. The model is similar to the <strong class="EmphasisTypeBold ">SMITH</strong><span id="ITerm49"/> model [<span class="CitationRef"><a epub:type="biblioref" href="#CR247" role="doc-biblioref">247</a></span>], which uses a BERT-based hierarchical encoder to capture the document structure information (Sect. <span class="InternalRef"><a href="#Sec5">6.1.4</a></span>).</p><p class="Para" id="Par68">An alternative is <strong class="EmphasisTypeBold ">Macaw</strong><span id="ITerm50"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR218" role="doc-biblioref">218</a></span>], a freely available QA-system with 11B parameters. It is built on T5 and has strong zero-shot QA-capabilities. On a set of 300 challenge questions the authors claim that Macaw outperforms GPT-3 by 10%, although it has only a small fraction of its parameters. In addition to providing an answers for a question, Macaw can also take an answer and produce a question; or generate multiple-choice options for an answer and a question. The authors also provide a detailed analysis of errors.</p><p class="Para" id="Par69">It is much more difficult to combine different pieces of evidence to find an answer. A benchmark to test this ability is <em class="EmphasisTypeItalic ">WikiHop</em><span id="ITerm51"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR232" role="doc-biblioref">232</a></span>], where information from different documents has to be merged. An example is the question <em class="EmphasisTypeItalic ">“Hanging gardens of Mumbai, country?”</em> and the documents <em class="EmphasisTypeItalic ">“The Hanging Gardens, in Mumbai, also known as Pherozeshah Mehta Gardens, are terraced gardens …”</em> and <em class="EmphasisTypeItalic ">“Mumbai is the capital city of the Indian state of Maharashtra. It is the most populous city in India …”</em>. For each query up to 140 background paragraphs are provided to the model. On this benchmark BigBird-ETC (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec8"><span class="RefSource">3.​2.​1</span></a></span>) achieved an accuracy of 82.3%. Currently, the best model for this task is the <strong class="EmphasisTypeBold ">RealFormer</strong><span id="ITerm52"/> with an accuracy of 84.4% [<span class="CitationRef"><a epub:type="biblioref" href="#CR171" role="doc-biblioref">171</a></span>], which is slightly below the human performance of 85%. The RealFormer is an autoencoder with a modified architecture, which provides a bypass with the raw attention scores of all attention heads from the previous layer in the subsequent layers [<span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>].</p></section>
<section class="Section3 RenderAsSection3" id="Sec12"><h4 class="Heading">Question Answering with Few-Shot Language Models</h4><p class="Para" id="Par70">Recent Foundation Models are trained with an enormous collection of documents and can generate answers to questions without additional knowledge input. An example is the autoregressive language model <strong class="EmphasisTypeBold ">GPT-3</strong><span id="ITerm53"/> with 175B parameters, which was pre-trained on a text collection of books, Wikipedia and web pages of about 500 billion tokens (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec3"><span class="RefSource">3.​1.​2</span></a></span>). Because of its high model capacity it can absorb a lot of ‘knowledge’ in its parameters. When a Foundation Model is not allowed to use external information, this is called <em class="EmphasisTypeItalic ">Closed-book QA</em><span id="ITerm54"/>.</p><div class="Para" id="Par71">As discussed in Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec41"><span class="RefSource">3.​6.​3</span></a></span>, GPT-3 can be instructed by a few examples (few-shot) to solve a task. Figure <span class="InternalRef"><a href="#Fig4">6.4</a></span> provides a few-shot prompt example. For Natural Questions, GPT-3 achieves an exact match accuracy of 14.6% in the zero-shot setting, 23.0% in the one-shot setting, and 29.9% in the few-shot setting [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>, p. 14]. This was achieved without fine-tuning on Natural Questions. The larger <strong class="EmphasisTypeBold ">Gopher</strong><span id="ITerm55"/> model with 280B parameters (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec3"><span class="RefSource">3.​1.​2</span></a></span>) performs slightly worse with 28.2% in the few-shot setting [<span class="CitationRef"><a epub:type="biblioref" href="#CR175" role="doc-biblioref">175</a></span>, p. 80].<figure class="Figure" id="Fig4"><div class="MediaObject" id="MO4"><img alt="" aria-describedby="d64e3224" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig4_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e3224"><p class="Para" id="Par321">A text box represents a set of prompts and their answers given by the bot. It provides the response unknown if there is no clear answer.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.4</span><p class="SimplePara">A possible few-shot prompt for GPT-3 to get an answer based on existing knowledge acquired during pre-training [<span class="CitationRef"><a epub:type="biblioref" href="#CR160" role="doc-biblioref">160</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par72">The even larger <strong class="EmphasisTypeBold ">PaLM</strong><span id="ITerm56"/> model with 540B parameters (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec3"><span class="RefSource">3.​1.​2</span></a></span>) was trained on a high-quality dataset with 780B tokens. It uses a new prompt technique to pose logical questions, where examples are presented to the system together with <em class="EmphasisTypeItalic ">thought chains</em><span id="ITerm57"/> partitioning a reasoning task into smaller problems (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec42"><span class="RefSource">3.​6.​4</span></a></span>). In this way it gets the recipe to combine facts from different sources to arrive at the final answer.</p><p class="Para" id="Par73">PaLM was evaluated on a large number of other benchmarks, which in part are QA-tasks. On Natural Questions it arrived at an accuracy of 21.2% with 0-shots and at 36.0% with few-shot prompts [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>, p. 47]. On Trivia QA (questions concerning the Wikipedia), BoolQ (question answering with yes/no answers), and PIQA (question answering with reasoning) PaLM also achieved a new <span class="EmphasisTypeSmallCaps ">Sota</span>. The results are shown in Table <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Tab4"><span class="RefSource">3.​4</span></a></span>. PaLM was benchmarked with a large number of tests, among them the more than 150 BIG-bench tasks (Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec5"><span class="RefSource">4.​1.​4</span></a></span>). Many of them are QA-related tasks: 21 contextual QA tasks, 24 context-free QA tasks, 36 reading comprehension tasks, and a large number of tasks on specific knowledge and common sense [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>]. Additional outcomes for QA-benchmarks of PaLM are given in [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>, p. 12], where PaLM always achieves <span class="EmphasisTypeSmallCaps ">Sota</span>.</p></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec13"><h3 class="Heading"><span class="HeadingNumber">6.2.2 </span>Question Answering Based on Retrieval</h3><div class="Para" id="Par74">Retrieval ODQA systems usually work in two stages: for a question a <em class="EmphasisTypeItalic ">retriever</em><span id="ITerm58"/> module finds a number of documents from a text collection, which might contain the answer. Subsequently, a <em class="EmphasisTypeItalic ">reader</em> considers the question and the retrieved documents and generates a natural language answer (Fig. <span class="InternalRef"><a href="#Fig5">6.5</a></span>). Since the model relies on external information, it is referred to as <em class="EmphasisTypeItalic ">Open-book QA</em><span id="ITerm59"/>.<figure class="Figure" id="Fig5"><div class="MediaObject" id="MO5"><img alt="" aria-describedby="d64e3322" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig5_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e3322"><p class="Para" id="Par322">A flow diagram represents the flow of unstructured documents through the retriever, followed by relevant documents, and the reader, before generating the answer.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.5</span><p class="SimplePara">Question answering often combines dense retrieval with an answer selection module. The retriever performs a dense retrieval by comparing the embedding of the query with the embeddings of passages. The reader ranks the retrieved documents and generates an answer by an autoregressive Pre-trained Language Model [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>]. Credits for image parts in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab3"><span class="RefSource">A.​2</span></a></span></p></div></figcaption></figure></div><p class="Para" id="Par75">Retrievers have been introduced in Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec22"><span class="RefSource">3.​4.​5</span></a></span> and were discussed in the context of document retrieval in Sect. <span class="InternalRef"><a href="#Sec1">6.1</a></span>. The retriever may employ a traditional search engine using tf-idf weighting or BM25. Alternatively the retriever may be a <em class="EmphasisTypeItalic ">dense retriever</em><span id="ITerm60"/> based on document and question embeddings. It is trained to retrieve passages by computing embedding similarities e.g. by <em class="EmphasisTypeItalic ">DPR</em><span id="ITerm61"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR94" role="doc-biblioref">94</a></span>] (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec22"><span class="RefSource">3.​4.​5</span></a></span>). A tutorial on ODQA is provided by Chen [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>].</p><p class="Para" id="Par76">The <em class="EmphasisTypeItalic ">reader</em><span id="ITerm62"/> is usually an autoregressive language model that receives both the query and the retrieved documents as inputs. It is fine-tuned to generate a response to the query based on the retrieved information and its internal knowledge.</p><p class="Para" id="Par77">Question answering with external knowledge bases has the advantage that curated KBs usually are checked for correctness. They may have, however, limited coverage of entities and relations may not be up-to-date. There are a number of approaches to combine PLMs with KBs using techniques like entity mapping (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec18"><span class="RefSource">3.​4.​1</span></a></span>). Recent papers propose a hybrid approach using KBs and retrieval [<span class="CitationRef"><a epub:type="biblioref" href="#CR239" role="doc-biblioref">239</a></span>]. Knowledge-Guided Text Retrieval [<span class="CitationRef"><a epub:type="biblioref" href="#CR145" role="doc-biblioref">145</a></span>] starts with retrieving text passages for a query. It creates a passage graph, where vertices are passages of text and edges represent relationships that are derived either from an external knowledge base or co-occurrence in the same article. On Natural Questions [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>] they achieve an accuracy of 34.5%.</p><div class="Para" id="Par78"><strong class="EmphasisTypeBold ">HYBRIDER</strong><span id="ITerm63"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>] uses information from a retriever as well as from a structured KB and tables. The authors collected Wikipedia pages and constructed a benchmark dataset HybridQA containing question-answer pairs requiring multi-hop reasoning using text, tables and hyperlinks (Fig. <span class="InternalRef"><a href="#Fig6">6.6</a></span>). The model first links questions to tables cells as well as Wikipedia passages and hyperlinks. In a reasoning phase the linked information is ranked and consolidated to derive the probabilities of different answers. The experiments with the dataset show that the utilization of tables or retrieval alone achieves an exact match accuracy of about 20% while the joint model yields more than 40%. However, the hybrid model’s score is still far below human performance.<figure class="Figure" id="Fig6"><div class="MediaObject" id="MO6"><img alt="" aria-describedby="d64e3411" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig6_HTML.png" style="width:34.38em"/><div class="TextObject" id="d64e3411"><p class="Para" id="Par323">A table exhibits the list of names, years, seasons, and flag bearers in the Olympic events. It indicates various details of the flag bearer on the right side. Below are 2 questions related to the year and sports category along with their answers.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.6</span><p class="SimplePara">For hybrid question answering Wikipedia pages are retrieved by HYBRIDER [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>] (top left). Some pages contain tables (left). Here the column titles may be interpreted as well as hyperlinks to entities (underlined). The lower part shows two human-annotated question-answer pairs. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>, p. 2]</p></div></figcaption></figure></div><p class="Para" id="Par79">One of the first retrieval-reader systems was <strong class="EmphasisTypeBold ">DPR</strong><span id="ITerm64"/> (Dense Passage Retriever) [<span class="CitationRef"><a epub:type="biblioref" href="#CR94" role="doc-biblioref">94</a></span>]. It employs a BERT model to encode passages by embeddings and retrieves them by approximate <em class="EmphasisTypeItalic ">k</em>-nearest neighbor search with the FAISS index (Sect. <span class="InternalRef"><a href="#Sec6">6.1.5</a></span>). In this way it can gather passages with similar meaning but different wording. The DPR reader is another BERT model which is fine-tuned to predict a probability for each retrieved passage that this passage contains the correct answer. In addition, it selects a span of tokens by span prediction, which probably provides the answer. The approach can be easily applied to KBs with billions of passages [<span class="CitationRef"><a epub:type="biblioref" href="#CR94" role="doc-biblioref">94</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR213" role="doc-biblioref">213</a></span>]. On the <em class="EmphasisTypeItalic ">Natural Questions</em><span id="ITerm65"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>] it yields a test set accuracy of 41.5%.</p><p class="Para" id="Par80"><strong class="EmphasisTypeBold ">FiD</strong><span id="ITerm66"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR84" role="doc-biblioref">84</a></span>] is described in Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec22"><span class="RefSource">3.​4.​5</span></a></span>. The retriever is based on DPR and compares query and passages embeddings. Raffel et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR177" role="doc-biblioref">177</a></span>] have shown that generative models like T5 can produce the answer for QA-tasks. FiD processes the query and the retrieved passages by a <em class="EmphasisTypeItalic ">reader</em><span id="ITerm67"/> based on a T5 model to generate an answer. Since the first step is to process the passages one by one, the system is very efficient. FiD achieves an exact match accuracy of 51.4% on the Natural Questions test set compared to 41.5% for DPR.</p><p class="Para" id="Par81"><strong class="EmphasisTypeBold ">REALM</strong><span id="ITerm68"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR75" role="doc-biblioref">75</a></span>] and <strong class="EmphasisTypeBold ">RAG</strong><span id="ITerm69"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR114" role="doc-biblioref">114</a></span>] are retrieval augmented generative models for open domain question answering. However, they process all retrieved passages simultaneously in an autoregressive language model and were unable to take into account a large number of passages leading to lower accuracies on Natural Questions of 40.4 for REALM and 44.5 for RAG. Sachan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR194" role="doc-biblioref">194</a></span>] propose an end-to-end differentiable training method for retrieval-augmented ODQA. Latent variables indicate which of the relevant documents should be included. The values of the latent variables are iteratively estimated by an EM-algorithm. On Natural Questions they achieve an exact match accuracy of 52.5%.</p><p class="Para" id="Par82"><strong class="EmphasisTypeBold ">MTR</strong><span id="ITerm70"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR138" role="doc-biblioref">138</a></span>] starts from the observation that neural retrievers perform well on their fine-tuning domain, but will typically achieve low out-of-domain performance. The authors propose a multitask retriever similar to DPR which is jointly fine-tuned on eight diverse retrieval tasks. They use a shared passage encoder—so that a single index of encoded passages can be used—as well as a query encoder that is shared across all tasks. In five of the eight models they achieve a higher performance than special models tuned to the corresponding domain.</p><p class="Para" id="Par83"><strong class="EmphasisTypeBold ">AISO</strong><span id="ITerm71"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR268" role="doc-biblioref">268</a></span>] is a retriever-reader architecture for solving multi-hop QA tasks, where multiple documents are required to answer a question. Repeated retrieval rounds are performed in which associated terms are taken as new search queries to find additional evidence. The approach is adaptive and at each step selects one of three types of retrieval operations (e.g., BM25, DPR, and hyperlink) or one answer operation. On the <em class="EmphasisTypeItalic ">HotpotQA benchmark</em><span id="ITerm72"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR249" role="doc-biblioref">249</a></span>], the question-answering system must find the answer to a query in the scope of the entire Wikipedia. The AISO model achieved a new <span class="EmphasisTypeSmallCaps ">Sota</span> with a joint F1-value of 72.0%.</p><p class="Para" id="Par84">The <strong class="EmphasisTypeBold ">FB Hybrid</strong><span id="ITerm73"/> system was presented at the EfficientQA competition [<span class="CitationRef"><a epub:type="biblioref" href="#CR147" role="doc-biblioref">147</a></span>], where real user questions for the Google search engine from the Natural Questions dataset [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>] were tackled. While the original NQ was a reading comprehension task providing a number of evidence documents for each question, the EfficientQA benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR147" role="doc-biblioref">147</a></span>] adapted this to open-domain QA by taking examples with up to five token answers and discarding the evidence documents. The system uses a retriever-reader architecture [<span class="CitationRef"><a epub:type="biblioref" href="#CR158" role="doc-biblioref">158</a></span>]. The retriever is a mixture of DPR and another retrieval system, which covers lists and tables as well as KB-relations and retrieves 100 passages. The reader is a T5-large Seq2seq model, which is given 100 passages from the retriever and generates an answer. The background corpus contained 18.8M passages from Wikipedia. On Natural Questions the model achieves an exact match accuracy of 53.9%. According to an evaluation by human raters the model was able to answer 67.4% of the questions correctly, which is about as good as the performance of human experts using a search engine. The <strong class="EmphasisTypeBold ">MS UnitedQA</strong><span id="ITerm74"/> model had a similar architecture [<span class="CitationRef"><a epub:type="biblioref" href="#CR139" role="doc-biblioref">139</a></span>]. It uses a BERT-based retriever and a reader combined from a T5-model and ELECTRA processes the returned documents to generate different answers. A final re-ranking model selects the answer. MS UnitedQA yields an exact match accuracy of 54.0% and 65.8% correctness on Natural Questions. If the systems were restricted to a memory footprint of 6 GB the performance was only marginally reduced.</p></section>
<section class="Section2 RenderAsSection2" id="Sec14"><h3 class="Heading"><span class="HeadingNumber">6.2.3 </span>Long-Form Question Answering Using Retrieval</h3><section class="Section3 RenderAsSection3" id="Sec15"><h4 class="Heading">A Language Model with Integrated Retrieval</h4><div class="Para" id="Par85"><strong class="EmphasisTypeBold ">Retro</strong><span id="ITerm75"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>] is an autoregressive language model with 7B parameters using retrieved information to predict the next token. As retriever a frozen BERT model is employed (Fig. <span class="InternalRef"><a href="#Fig7">6.7</a></span>). Each training sequence is split into chunks, which are augmented with their <em class="EmphasisTypeItalic ">k</em>-nearest neighbors retrieved from the database of 2 trillion tokens. The returned information is processed in a language model to improve the prediction of the next token leading to large performance gains. The reader consists of a differentiable autoregressive encoder and a chunked cross-attention module to predict tokens.<figure class="Figure" id="Fig7"><div class="MediaObject" id="MO7"><img alt="" aria-describedby="d64e3590" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig7_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e3590"><p class="Para" id="Par324">A model diagram illustrates the input sequence finds the nearest neighbor embeddings followed by the database of 2 trillion words to encode neighbors, which further progresses to the autoregressive language model to generate the output.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.7</span><p class="SimplePara">The Retro language model retrieves information for the input sequence. The model uses the input sequence and the retrieved neighbor chunks from the database as input and generates an appropriate output [<span class="CitationRef"><a epub:type="biblioref" href="#CR176" role="doc-biblioref">176</a></span>]</p></div></figcaption></figure></div><div class="Para" id="Par86">An input sequence <em><strong class="EmphasisTypeBoldItalic ">v</strong></em> = (<em class="EmphasisTypeItalic ">v</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">n</em></sub>) of <em class="EmphasisTypeItalic ">n</em>=2048 tokens is split into chunks <em><strong class="EmphasisTypeBoldItalic ">c</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> = (<em class="EmphasisTypeItalic ">v</em><sub>(<em class="EmphasisTypeItalic ">t</em>−1)∗<em class="EmphasisTypeItalic ">m</em>+1</sub>, …, <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em>∗<em class="EmphasisTypeItalic ">m</em></sub>) of length <em class="EmphasisTypeItalic ">m</em>=64. Each chunk <em><strong class="EmphasisTypeBoldItalic ">c</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> is expanded with a set <span class="EmphasisTypeSmallCaps ">Ret</span>(<em><strong class="EmphasisTypeBoldItalic ">c</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub>) of retrieved <em class="EmphasisTypeItalic ">k</em> nearest neighbor chunks from the database. The probability of a token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em>∗<em class="EmphasisTypeItalic ">m</em>+<em class="EmphasisTypeItalic ">i</em></sub> in the next chunk <em><strong class="EmphasisTypeBoldItalic ">c</strong></em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub> then can be recursively computed as <div class="Equation NumberedEquation" id="Equ8"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} p(v_{t*m+i}|v_{t*m+(i-1)},\ldots,v_{t*m+1},\boldsymbol{c}_t,\text{RET}(\boldsymbol{c}_t),\ldots, \boldsymbol{c}_1,\text{RET}(\boldsymbol{c}_1) ) {}. \end{aligned} $$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ8.png" style="width:27.2em"/></div></div> <div class="EquationNumber">(6.8)</div></div></div></div><p class="Para" id="Par87">The probability of the <em class="EmphasisTypeItalic ">i</em>-th token of the (<em class="EmphasisTypeItalic ">t</em> + 1)-th chunk <em><strong class="EmphasisTypeBoldItalic ">c</strong></em><sub><em class="EmphasisTypeItalic ">t</em>+1</sub> depends only on the previous tokens and on the data <span class="EmphasisTypeSmallCaps ">Ret</span>(<em><strong class="EmphasisTypeBoldItalic ">c</strong></em><sub><em class="EmphasisTypeItalic ">j</em></sub>) retrieved from the database for the previous chunks. This integrates the retrieval process in the language model.</p><p class="Para" id="Par88">The retriever for a chunk <em><strong class="EmphasisTypeBoldItalic ">c</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> uses the average <span class="EmphasisTypeSmallCaps ">Bert</span>(<em><strong class="EmphasisTypeBoldItalic ">c</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub>) of all BERT embeddings of the tokens in <em><strong class="EmphasisTypeBoldItalic ">c</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub> as key. It retrieves the <em class="EmphasisTypeItalic ">k</em> nearest neighbors from the database with respect to the <em class="EmphasisTypeItalic ">L</em><sub>2</sub> distance <span class="InlineEquation" id="IEq12"><img alt="$$||\text{BERT}(\boldsymbol {c}_t)-\text{BERT}(\tilde {\boldsymbol {c}_s})||{ }_2^2$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq12.png" style="width:11.94em"/></span>. The model receives the corresponding chunks <span class="InlineEquation" id="IEq13"><img alt="$$\tilde {\boldsymbol {c}}_{s,j}$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq13.png" style="width:1.62em"/></span> and additionally their continuation chunk <span class="InlineEquation" id="IEq14"><img alt="$$\tilde {\boldsymbol {c}}_{s+1,j}$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq14.png" style="width:2.56em"/></span> for <em class="EmphasisTypeItalic ">j</em> = 1, …, <em class="EmphasisTypeItalic ">k</em>, which collectively form the elements of <span class="EmphasisTypeSmallCaps ">Ret</span>(<em><strong class="EmphasisTypeBoldItalic ">c</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub>). By filtering it is avoided that the chunk to be predicted is included in <span class="EmphasisTypeSmallCaps ">Ret</span>(<em><strong class="EmphasisTypeBoldItalic ">c</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub>), as this would invalidate the conditional probability definition. The retrieval is performed in <span class="InlineEquation" id="IEq15"><img alt="$$O(\log T)$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq15.png" style="width:3.95em"/></span> time using the <em class="EmphasisTypeItalic ">SCaNN</em><span id="ITerm76"/> library [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>], which collects a set of chunks from a database of 2 trillion tokens in 10ms. Note that the document corpus of Retro is about 1000 times larger than the databases of FiD and other retrieval models.</p><div class="Para" id="Par89">Inside the reader the retrieved tokens in <span class="EmphasisTypeSmallCaps ">Ret</span>(<em><strong class="EmphasisTypeBoldItalic ">c</strong></em><sub><em class="EmphasisTypeItalic ">t</em></sub>) are fed into an autoencoder, which computes a set <em class="EmphasisTypeItalic ">E</em> of encoded neighbors. Then, so-called <span class="EmphasisTypeSmallCaps ">Retro</span> blocks <div class="Equation NumberedEquation" id="Equ9"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} \text{RETRO}(H,E) := \text{FCL}(\text{CATL}(\text{ATTL}(H),E)) {}, \end{aligned} $$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ9.png" style="width:20.82em"/></div></div> <div class="EquationNumber">(6.9)</div></div></div></div><p class="Para" id="Par90">and standard self-attention blocks <span class="EmphasisTypeSmallCaps ">Lm</span>(<em class="EmphasisTypeItalic ">H</em>) := <span class="EmphasisTypeSmallCaps ">Fcl</span>(<span class="EmphasisTypeSmallCaps ">Attl</span>(<em class="EmphasisTypeItalic ">H</em>)) are interleaved and operate on the intermediate embeddings <span class="InlineEquation" id="IEq16"><img alt="$$H\in \mathbb {R}^{n\times d}$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq16.png" style="width:4.56em"/></span>. Here <span class="EmphasisTypeSmallCaps ">Fcl</span>(⋅) is a fully connected layer, <span class="EmphasisTypeSmallCaps ">Attl</span>(⋅) a self-attention layer, and <span class="EmphasisTypeSmallCaps ">Catl</span>(⋅, <em class="EmphasisTypeItalic ">E</em>) a cross-attention layer which includes the information in <em class="EmphasisTypeItalic ">E</em>. The input and output dimension of these modules is <span class="InlineEquation" id="IEq17"><img alt="$$\mathbb {R}^{n\times d}$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq17.png" style="width:2.31em"/></span>.</p><p class="Para" id="Par91">The resulting language model is able to predict the next token with a high reliability. The <em class="EmphasisTypeItalic ">Pile data</em><span id="ITerm77"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>] is a 825GB open-source text collection set that consists of 22 diverse, high-quality datasets. It was screened for toxic language and bias, e.g. with respect to gender, religion, and race. Its authors recommend measuring the quality of token prediction in <em class="EmphasisTypeItalic ">bits per byte</em><span id="ITerm78"/> (<em class="EmphasisTypeItalic ">bpb</em>), which in contrast to perplexity is independent of the tokenizer [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>, p. 6]. The authors compare Retro with GPT-3<sub>175B</sub> [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>], Jurassic-1<sub>178B</sub> [<span class="CitationRef"><a epub:type="biblioref" href="#CR121" role="doc-biblioref">121</a></span>], and Gopher<sub>280B</sub> [<span class="CitationRef"><a epub:type="biblioref" href="#CR176" role="doc-biblioref">176</a></span>]. It turns out that Jurassic-1 has the lowest (and best) bpb-value on 5 Pile datasets, Gopher on 2 datasets and Retro on 9 datasets, although it is far smaller than the other models [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>]. GPT-3 was inferior to all three models. A possible problem for these results is the overlap of the retrieval corpus with the test data.</p><p class="Para" id="Par92">For the <em class="EmphasisTypeItalic ">LAMBADA benchmark</em><span id="ITerm79"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR165" role="doc-biblioref">165</a></span>] a model has to predict the last word of a paragraph. The authors measure the following accuracies: Retro without retrieval 70%, Retro with retrieval 73%, Gopher 74.5%, and GPT-3 76.2%. Note that Retro has only 4% of the parameters of GPT-3. For question answering the Natural Question benchmark is relevant. Here, Retro achieved an exact match accuracy of 45.5%.</p><p class="Para" id="Par93">The <em class="EmphasisTypeItalic ">LaMDA</em><span id="ITerm80"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR222" role="doc-biblioref">222</a></span>] dialog system (Sect. <span class="InternalRef"><a href="#Sec52">6.6.3</a></span>) is an expanded version of Retro with 137B parameters. It demonstrates that facticity can be improved by retrieval models. In addition, it is able to reduce toxic language by a system of filters that block unwanted speech. Although this model could also easily be used for question answering, no corresponding benchmark results are known.</p></section>
<section class="Section3 RenderAsSection3" id="Sec16"><h4 class="Heading">Controlling a Search Engine by a Pre-trained Language Model</h4><div class="Para" id="Par94"><strong class="EmphasisTypeBold ">WebGPT</strong><span id="ITerm81"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR149" role="doc-biblioref">149</a></span>] extends GPT-3 to control the <em class="EmphasisTypeItalic ">Bing search engine</em><span id="ITerm82"/> and performs a web search for a specific query. The language model must issue commands such as <em class="EmphasisTypeItalic ">“Search …”</em>, <em class="EmphasisTypeItalic ">“Find in page: …”</em> or <em class="EmphasisTypeItalic ">“Quote: …”</em>, as shown in Fig. <span class="InternalRef"><a href="#Fig8">6.8</a></span>. In this way, the model collects passages from web pages which contain information relevant for the question. The utilization of Bing has the advantage that it has powerful search capabilities, and covers a large number of up-to-date documents.<figure class="Figure" id="Fig8"><div class="MediaObject" id="MO8"><img alt="" aria-describedby="d64e4301" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig8_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e4301"><p class="Para" id="Par325">A table represents a set of 10 commands and their effects. The commands are, search, clicked on link, find in page, quote, scroll down, scrolled up, top, back, and end</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.8</span><p class="SimplePara">Possible actions of the WebGPT language model. If another text is generated, this is an invalid action and ignored [<span class="CitationRef"><a epub:type="biblioref" href="#CR149" role="doc-biblioref">149</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par95">Browsing continues until the model issues a command to end browsing, the maximum total length of references has been reached, or the maximum number of actions has been reached. If a relevant reference has been retrieved, the model will generate a long-form answer to the question.</p><div class="Para" id="Par96">The GPT-3 model is first fine-tuned to mimic human demonstrations, enabling it to use the text-based browser to answer questions. Then, the usefulness and accuracy of the model’s answers is improved by fine-tuning a reward model to predict human preferences, and optimizing it by rejection sampling. Specifically the model is fine-tuned to answer questions from <em class="EmphasisTypeItalic ">ELI5</em><span id="ITerm83"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>], a dataset of open-ended questions obtained from the subreddit ‘Explain Like I’m Five’. An example is given in Fig. <span class="InternalRef"><a href="#Fig9">6.9</a></span>. The proposed WebGPT answers should be coherent, relevant, and supported by trustworthy documents. No details are reported on the input prompts of GPT-3 containing the current state of search, and how the GPT-3 model combines the returned documents into an answer. Note, however, that there is significant overlap between training and validation in ELI5, as at least 81% of ELI5 validation questions occur in the training set [<span class="CitationRef"><a epub:type="biblioref" href="#CR106" role="doc-biblioref">106</a></span>] in circumscribed form.<figure class="Figure" id="Fig9"><div class="MediaObject" id="MO9"><img alt="" aria-describedby="d64e4335" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig9_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e4335"><p class="Para" id="Par326">A text box represents a question related to the application of contact lenses. There is a passage of answers at the bottom along with the citations.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.9</span><p class="SimplePara">Long-form answer to a question generated by WebGPT. The best of 64 answers was automatically selected. The citations were automatically retrieved from the Bing search engine and added to the answer [<span class="CitationRef"><a epub:type="biblioref" href="#CR80" role="doc-biblioref">80</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par97">The final answers were selected from 64 trials of the 175B WebGPT model by ranking. These answers were preferred by human raters to the reference responses from the ELI5 dataset 69% of the time. Moreover, they were preferred to the human demonstrator responses in 56% of the cases.</p><p class="Para" id="Par98">For WebGPT, responses to TruthfulQA [<span class="CitationRef"><a epub:type="biblioref" href="#CR125" role="doc-biblioref">125</a></span>] were correct about 75% of time, whereas GPT-3 scored 64% with helpful prompts. While GPT-3’s answers were truthful and informative in about 20% of the time, the best version of WebGPT increased this to about 56%. Since people answered 94% of the questions correctly, the models still have a significant performance difference. On TriviaQA WEBGPT achieved a score of 69.5%, which is far less than the value of PaLM with 81.4%.</p><p class="Para" id="Par99">An innovative feature is the support of text passages by references. This corresponds to the approach of scientific papers to underpin claims by references and was already suggested by Metzler et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR143" role="doc-biblioref">143</a></span>]. The references explain the answer and support the factual accuracy of the statements. The citations are selected by Bing in response to the query. They should therefore be close to the final reader-generated response and provide an easy way to assess the correctness of the response.</p><p class="Para" id="Par100">However, the authors point out that the references are not always representative for the available evidence, although the model cites references that correspond to the generated text. In addition, it is difficult for the model to verify the trustworthiness of references. Here, Web-of-Trust systems and search engine technology could be employed, which favor trust-checked frequently linked web pages.</p></section>
<section class="Section3 RenderAsSection3" id="Sec17"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par101"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par102">BigBird code and models are available at <span class="ExternalRef"><a href="https://huggingface.co/google/bigbird-roberta-base"><span class="RefSource">https://​huggingface.​co/​google/​bigbird-roberta-base</span></a></span></p></li><li><p class="Para" id="Par103">DPR code and models <span class="ExternalRef"><a href="https://github.com/facebookresearch/DPR"><span class="RefSource">https://​github.​com/​facebookresearch​/​DPR</span></a></span></p></li><li><p class="Para" id="Par104">FiD code and models <span class="ExternalRef"><a href="https://github.com/facebookresearch/FiD"><span class="RefSource">https://​github.​com/​facebookresearch​/​FiD</span></a></span></p></li><li><p class="Para" id="Par105">RealFormer code <span class="ExternalRef"><a href="https://github.com/jaketae/realformer"><span class="RefSource">https://​github.​com/​jaketae/​realformer</span></a></span></p></li><li><p class="Para" id="Par106">REALM code <span class="ExternalRef"><a href="https://github.com/google-research/language/blob/master/language/realm/README.md"><span class="RefSource">https://​github.​com/​google-research/​language/​blob/​master/​language/​realm/​README.​md</span></a></span></p></li><li><p class="Para" id="Par107">RETRO implementation, Deepmind’s Retrieval based Attention net, in PyTorch. This will deviate from the paper slightly, using rotary embeddings for relative positional encoding, as well as FAISS library instead of SCaNN <span class="ExternalRef"><a href="https://github.com/lucidrains/RETRO-pytorch"><span class="RefSource">https://​github.​com/​lucidrains/​RETRO-pytorch</span></a></span>.</p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec18"><h3 class="Heading"><span class="HeadingNumber">6.2.4 </span>Summary</h3><p class="Para" id="Par108">A number of Foundation Models have been presented, which were able to improve Question Answering performance. Examples are the autoregressive language models GPT-3 (175B), Gopher (175B), and PaLM (540B) with huge parameter sets, which are trained on a large document collections and can acquire extensive knowledge. Using few-shot prompts they are able to answer questions with high accuracy without employing external knowledge.</p><p class="Para" id="Par109">Recently, the retriever-reader architecture has been increasingly used for QA systems. It has the potential to tap into a larger knowledge base or the Internet that can easily be kept up-to-date. The retriever can employ keyword search or dense retrieval. Dense retrieval mitigates the term-mismatch problem, where relevant paraphrases are ignored. Usually, embeddings for each document or phrase are pre-computed and the embedding index is constructed beforehand. Current systems can access document collections of up to trillions of tokens using advanced nearest-neighbor search engines like FAISS and SCaNN to compare embeddings.</p><p class="Para" id="Par110">The reader usually receives the query and the returned passages in text form and generates the answer. It is fine-tuned to select the correct answer and to provide answers which are expressive and truthful. The Retro model is an autoregressive language model with only 7B parameters, which uses passages retrieved by a frozen BERT model as additional current state information to generate the next tokens. It is capable of improving accuracy to high levels for many QA tasks, but can also be used for other applications such as story generation.</p><p class="Para" id="Par111">WebGPT combines GPT-3 and the Bing search engine to retrieve documents and create appropriate answers. It is able to enhance the generated text by references to documents, which justify and explain the answer. The LaMDA dialog model is an expanded version of Retro with 137B parameters with specific tuning to increase usability and factual accuracy. In addition, it is able to reduce toxic language by a system of filters that block unwanted speech. These techniques can also be applied to question answering.</p><p class="Para" id="Par112">Still difficult is the generation of answers where the correct response needs information from multiple documents. In this case several rounds of querying are necessary. Special models like RealFormer, HYBRIDER, or AISO can improve the performance for benchmarks like WikiHop.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec19"><h2 class="Heading"><span class="HeadingNumber">6.3 </span>Neural Machine Translation</h2><div class="Para" id="Par113">Language is the cornerstone of most human communication and interaction. Moreover, many persons think in terms of language, and use it to express and communicate feelings, goals, and ideas. We communicate knowledge by language and use it to establish social and emotional relationships. There are more than 7100 languages in the world [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>], some of which are shown in Fig. <span class="InternalRef"><a href="#Fig10">6.10</a></span>. The ability to understand each other across language barriers is essential for communicating ideas between people.<figure class="Figure" id="Fig10"><div class="MediaObject" id="MO10"><img alt="" aria-describedby="d64e4436" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig10_HTML.png" style="width:33.88em"/><div class="TextObject" id="d64e4436"><p class="Para" id="Par327">A world map indicates different languages used in different regions all across the world.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.10</span><p class="SimplePara">This map shows some of the world’s 7100 languages, with each dot representing a language and the color indicating the top language family for each language. Only a small fraction of the world’s languages are currently represented in Foundation Models. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>, p. 23]</p></div></figcaption></figure></div><div class="Para" id="Par114">After an initial success with Recurrent Neural Networks [<span class="CitationRef"><a epub:type="biblioref" href="#CR15" role="doc-biblioref">15</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR215" role="doc-biblioref">215</a></span>] the development of the Transformer encoder-decoder (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec19"><span class="RefSource">2.​3</span></a></span>) has driven progress in Neural Machine Translation (NMT). By cross-attention a “correlation” between each token of the source text and the translated text can be established, producing better translations than before. The availability of large training sets and better model architectures has steadily increased the performance of Pre-trained Language Models for NMT (Fig. <span class="InternalRef"><a href="#Fig11">6.11</a></span>). Standard models for multilingual processing are described in Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec12"><span class="RefSource">3.​3</span></a></span>. A survey is provided by Yang et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR248" role="doc-biblioref">248</a></span>].<figure class="Figure" id="Fig11"><div class="MediaObject" id="MO11"><img alt="" aria-describedby="d64e4479" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig11_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e4479"><p class="Para" id="Par328">An area graph represents the B L E U score of translation of different languages into English. The graph highlights the durations of March 2008, May 2019, and May 2020.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.11</span><p class="SimplePara"><span class="EmphasisTypeSmallCaps ">Bleu</span> scores for Google translation of 100+ different languages to English for different years. Image credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab3"><span class="RefSource">A.​2</span></a></span></p></div></figcaption></figure></div><section class="Section2 RenderAsSection2" id="Sec20"><h3 class="Heading"><span class="HeadingNumber">6.3.1 </span>Translation for a Single Language Pair</h3><p class="Para" id="Par115">The training data of NMT consist of text pairs of the source language and its translations to the target language. Traditionally evaluation is done by comparing one or more reference translations to the proposed translation, as described in the survey [<span class="CitationRef"><a epub:type="biblioref" href="#CR195" role="doc-biblioref">195</a></span>]. There are a number of automatic metrics like <span class="EmphasisTypeSmallCaps ">Bleu</span>, <span class="EmphasisTypeSmallCaps ">Meteor</span> or BERT-score (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec23"><span class="RefSource">2.​3.​3</span></a></span>). It turned out that there is a noticeable difference between human judgment and automatic evaluation. Therefore, most high-end comparisons today use human translators to assess the quality of translation methods.</p><p class="Para" id="Par116">At the WMT2021 Machine Translation conference, numerous teams solved benchmarks tests for translating English news texts from/to German, Japanese, Russian, Chinese, and a number of low-resource languages [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>]. Instead of using comparison statistics like <span class="EmphasisTypeSmallCaps ">Bleu</span>, the translations of each system was evaluated by a number of human evaluators without showing them a reference translation. They were asked to rate a given translation according to how adequately it expressed the meaning of the corresponding source language input on an analog scale, which corresponds to an underlying absolute rating scale of 0–100. As some raters could be stricter, the systems are ranked by a z-score, where the score is mean-centered and normalized per rater. Systems are grouped together according to which system significantly outperforms all others measured by the Wilcoxon rank-sum test. A large effort was spent to assess the validity of human evaluation.</p><p class="Para" id="Par117">In total 173 submissions were received. In addition, five anonymized online systems were included. Further human-produced reference translations were denoted by “HUMAN” in all tables. Results show that almost all good systems are based on transformer encoder-decoders. Words are mostly encoded by the SentencePiece [<span class="CitationRef"><a epub:type="biblioref" href="#CR107" role="doc-biblioref">107</a></span>] tokenizer (Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec2"><span class="RefSource">1.​2</span></a></span>). A widely used technique is <em class="EmphasisTypeItalic ">back-translation</em><span id="ITerm84"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR200" role="doc-biblioref">200</a></span>]. Here a monolingual text is translated to the other language and then back-translated. By minimizing the difference to the original text, both models may be improved. Up to 500M sentences per language were available and could be used for back-translation, which led to a significant improvement in quality. In addition, ensembles are able to increase the performance in most cases.</p><div class="Para" id="Par118">The result of the best system for each language pair is shown in Table <span class="InternalRef"><a href="#Tab4">6.4</a></span>. Usually, there is a cluster of 2–5 models at the top, whose performance differences are not significant. The Facebook-AI model (FB) had the best results for half of the language pairs. In addition, the <span class="EmphasisTypeSmallCaps ">Bleu</span> scores for the best systems automatically computed from n-grams are shown. As can be seen, the values are in general better for the translation “to English” than “from English” especially for morphology rich languages like Czech and German. Compared to the human reference translation, the best system was significantly better for three language pairs. This has already been discussed critically by Toral [<span class="CitationRef"><a epub:type="biblioref" href="#CR223" role="doc-biblioref">223</a></span>], who decry the limited amount of context between sentences and the limited translation proficiency of the evaluators. <div class="Table" id="Tab4"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 6.4</span><p class="SimplePara">Leading systems of the WMT2021 News Translation Task. The systems are ordered by normalized z-score [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>, pp. 15–19]. If either the best system or a human reference translation is significantly better, the value is printed in bold. Systems: FB: Facebook-AI, BL: Borderline, HW: HW-TSC, NV: Nvidia-NeMo, NI: NiuTrans, OB: Online-B, OW: Online-W, HN: HappyNewYear</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/><col class="tcol5"/><col class="tcol6"/><col class="tcol7"/><col class="tcol8"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Score</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Czech</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">German</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Hausa</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Icelandic</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Japanese</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Russian</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Chinese</p></th></tr></thead><tbody><tr><td colspan="8" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">To English</strong></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Best model z-score</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">FB <strong class="EmphasisTypeBold ">0.111</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BL <strong class="EmphasisTypeBold ">0.126</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">FB 0.248</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">FB 0.293</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">HW 0.141</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NV 0.137</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NI 0.042</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Human z-score</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">−  0.085</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">−  0.081</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.089</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.019</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Best model <span class="EmphasisTypeSmallCaps ">Bleu</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">43.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">53.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">18.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">40.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">27.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">56.3</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">33.4</p></td></tr><tr><td colspan="8" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">From English</strong></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Best model z-score</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">FB 0.263</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">OB <strong class="EmphasisTypeBold ">0.266</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">FB 0.264</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">FB 0.594</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">FB 0.430</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">OW 0.277</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">HN 0.284</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Human z-score</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">0.397</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.030</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.362</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">0.872</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.314</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.317</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.325</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Best model <span class="EmphasisTypeSmallCaps ">Bleu</span></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">33.6</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">31.3</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">20.4</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">30.6</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">46.9</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">45.0</p></td><td style="text-align: left;"><p class="SimplePara">49.2</p></td></tr></tbody></table></div></div><p class="Para" id="Par119">Improved performance was reached by increasing the number of parameters. The Facebook model [<span class="CitationRef"><a epub:type="biblioref" href="#CR224" role="doc-biblioref">224</a></span>], for instance, used a standard model of 4.7B parameters and a sparsely gated mixture-of-experts system with up to 128 experts. In each Sparsely Gated MoE layer, each token is routed to the top-2 expert feedforward blocks based on the score of a learned gating function. In addition, the models were fine-tuned with domain-specific data from the news domain. The <em class="EmphasisTypeItalic ">n</em>-best hypotheses were generated with a beam search. These were ranked with a weighted average of the probabilities <em class="EmphasisTypeItalic ">p</em>(tgt|src), <em class="EmphasisTypeItalic ">p</em>(src|tgt), and <em class="EmphasisTypeItalic ">p</em>(tgt), where src is the source and tgt is the target sentence.</p><p class="Para" id="Par120">It is well-known that the translation of single sentences suffers from ambiguities (e.g. pronouns or homonyms), which can be resolved by considering the document context. In WMT2021 this is taken into account by assessing the quality of translation within the document context [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>]. As current encoder-decoder Foundation Models are able to consider larger contexts, this could improve translation performance [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>]. Instead of finding the most probable translation of a sentence, we need to generate the best translation for a given complete source document. While comparing sentence-level translation often does not indicate a difference between human and machine translation, the comparison of document-level translation often yields a statistically significant preference for human translations [<span class="CitationRef"><a epub:type="biblioref" href="#CR110" role="doc-biblioref">110</a></span>].</p><p class="Para" id="Par121">Instead of using a Seq2seq model with extra long input sequence, <strong class="EmphasisTypeBold ">HAT</strong><span id="ITerm85"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR187" role="doc-biblioref">187</a></span>] proposes a hierarchical attention transformer. The authors split the input text into sentences and start each sentence <em class="EmphasisTypeItalic ">i</em> with a specific [<em class="EmphasisTypeItalic ">BOS</em><sub><em class="EmphasisTypeItalic ">i</em></sub>] token. These tokens summarize the sentence content and are connected to the other sentences by the usual self-attention and cross-attention. While the usual encoder-decoder transformer has a <span class="EmphasisTypeSmallCaps ">Bleu</span> of 32.5 for the document translation from English to German on WMT2019, HAT is able to yield a <span class="EmphasisTypeSmallCaps ">Sota</span><span class="EmphasisTypeSmallCaps ">Bleu</span> of 34.5.</p></section>
<section class="Section2 RenderAsSection2" id="Sec21"><h3 class="Heading"><span class="HeadingNumber">6.3.2 </span>Multilingual Translation</h3><p class="Para" id="Par122">Usually, languages with scarce training data have a much lower translation accuracy, as holds for Hausa in Table <span class="InternalRef"><a href="#Tab4">6.4</a></span>. A recent success was the extension of NMT by multilinguality, which was already discussed in Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec12"><span class="RefSource">3.​3</span></a></span>. This led to a marked improvement of translations for languages with few resources. For a survey see [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>].</p><p class="Para" id="Par123"><strong class="EmphasisTypeBold ">M2M</strong><span id="ITerm86"/> of Facebook AI [<span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>] improves translation between many languages by utilizing a massive corpus of 7.5B sentences covering 100 languages and thousands of translation directions with supervised data, created through large-scale mining. The model is a transformer encoder-decoder with 15B parameters. The authors add a special token in the encoder indicating the source language and a special token in the decoder indicating the target language. The transformer has 12 encoder and 12 decoder layers and an embedding size of 1024. As there is a joint token vocabulary for all languages, the input and output embeddings are shared. To improve performance the authors added language-specific layers to the decoder for five languages. Using specific parallelization techniques they were able to train the model with only hundreds of GPUs.</p><p class="Para" id="Par124">Except for four language directions (En→Chinese, Chinese→En, En→Fi, En→Estonian) the model improved translation results on the WMT benchmarks for 1.9 <span class="EmphasisTypeSmallCaps ">Bleu</span> points on average. Especially marked is the improvement for regional languages with an average increase of 7.6 <span class="EmphasisTypeSmallCaps ">Bleu</span>. For resource-rich language pairs Liu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR130" role="doc-biblioref">130</a></span>] propose to use very deep transformers with up to 60 encoder layers and 12 decoder layers. They develop a simple yet effective initialization technique that stabilizes training and achieve <span class="EmphasisTypeSmallCaps ">Sota</span> on WMT2014 En-Fr of 46.4 <span class="EmphasisTypeSmallCaps ">Bleu</span>.</p><p class="Para" id="Par125">Although multilingual translation has many advantages, it usually performs worse than specially trained bilingual models for high-resource language pairs. Recently Facebook [<span class="CitationRef"><a epub:type="biblioref" href="#CR225" role="doc-biblioref">225</a></span>] presented a single multilingual model, which outperformed the best specially trained bilingual models across 10 out of 14 language pairs of the WMT2021 news benchmark. Facebook built two multilingual systems: any-to-English and English-to-any. They employed data mining techniques to identify translations in large web crawl data and leverage available monolingual data with hundreds of millions of sentences from all eight languages to maximize performance of MT systems. They filtered the available monolingual data to reduce the amount of noise, and then back-translated them with an ensemble of the strongest multilingual models available. The number of parameters was increased from 15B to 53B to enhance the model capacity.</p><div class="Para" id="Par126">The <span class="EmphasisTypeSmallCaps ">Bleu</span> scores are shown in Table <span class="InternalRef"><a href="#Tab5">6.5</a></span>. In comparison to the best bilingual models of WMT2021, the multilingual model achieves a better <span class="EmphasisTypeSmallCaps ">Bleu</span> in 9 of 14 cases indicating that the additional training data from other languages supports translation. Only for Chinese→English there was a larger drop of 1.3 <span class="EmphasisTypeSmallCaps ">Bleu</span> points. The authors also performed a human evaluation for the language pairs English→Russian and English→German. It turned out that there was no perceived difference between the quality of bilingual and multilingual translations. <div class="Table" id="Tab5"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 6.5</span><p class="SimplePara"><span class="EmphasisTypeSmallCaps ">Bleu</span> scores of the Facebook multilingual model and the best language pair model submitted to the WMT2021 news task. The numbers reported are <span class="EmphasisTypeSmallCaps ">Bleu</span> scores on the final WMT2021 test set [<span class="CitationRef"><a epub:type="biblioref" href="#CR225" role="doc-biblioref">225</a></span>]. The difference between the models is printed in bold, if the multilingual model is better</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/><col class="tcol5"/><col class="tcol6"/><col class="tcol7"/><col class="tcol8"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Czech</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">German</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Hausa</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Icelandic</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Japanese</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Russian</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Chinese</p></th></tr></thead><tbody><tr><td colspan="8" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">From English</strong></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">FB-Mult</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">36.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">31.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">20.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">33.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">46.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">46.0</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">49.9</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">WMT2021 best</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">33.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">31.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">20.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">30.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">46.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">45.0</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">49.2</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Difference</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara"><strong class="EmphasisTypeBold ">2</strong>.<strong class="EmphasisTypeBold ">5</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">0.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">-0.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara"><strong class="EmphasisTypeBold ">2</strong>.<strong class="EmphasisTypeBold ">7</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">− 0.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara"><strong class="EmphasisTypeBold ">1</strong>.<strong class="EmphasisTypeBold ">0</strong></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">0</strong>.<strong class="EmphasisTypeBold ">7</strong></p></td></tr><tr><td colspan="8" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">To English</strong></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">FB-Mult</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">43.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">53.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">21.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">41.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">27.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">57.1</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">32.1</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">WMT2021 best</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">43.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">53.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">18.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">40.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">27.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">56.3</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">33.4</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Difference</p></td><td style="border-right: 0.5pt solid ; "><p class="SimplePara"><strong class="EmphasisTypeBold ">0</strong>.<strong class="EmphasisTypeBold ">4</strong></p></td><td style="border-right: 0.5pt solid ; "><p class="SimplePara"><strong class="EmphasisTypeBold ">0</strong>.<strong class="EmphasisTypeBold ">3</strong></p></td><td style="border-right: 0.5pt solid ; "><p class="SimplePara"><strong class="EmphasisTypeBold ">2</strong>.<strong class="EmphasisTypeBold ">1</strong></p></td><td style="border-right: 0.5pt solid ; "><p class="SimplePara"><strong class="EmphasisTypeBold ">1</strong>.<strong class="EmphasisTypeBold ">1</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">− 0.1</p></td><td style="border-right: 0.5pt solid ; "><p class="SimplePara"><strong class="EmphasisTypeBold ">0</strong>.<strong class="EmphasisTypeBold ">8</strong></p></td><td style="text-align: left;"><p class="SimplePara">− 1.3</p></td></tr></tbody></table></div></div><div class="Para" id="Par127">Table <span class="InternalRef"><a href="#Tab6">6.6</a></span> shows the effect of employed improvement strategies for the different languages of the multilingual model. Back-translation has a large effect for languages with little training data like Hausa and Icelandic. The authors note, however that back-translation produces <em class="EmphasisTypeItalic ">translationese</em> by generating artificial uncommon phrases in a language. These effects may be mitigated by fine-tuning on the specific domain, e.g. news texts. This yields about 3 <span class="EmphasisTypeSmallCaps ">Bleu</span> points for translation into English and 0.7 <span class="EmphasisTypeSmallCaps ">Bleu</span> points for translation out of English. Switching to the multilingual model generates an improvement for all models. While the effect of model ensembles is minor, re-ranking the BEAM translations with conditional target-source probabilities yields about 0.4 <span class="EmphasisTypeSmallCaps ">Bleu</span> points. Postprocessing (for example applying standard punctuation rules) can have a large effect, e.g. 5 <span class="EmphasisTypeSmallCaps ">Bleu</span> points for Chinese. <div class="Table" id="Tab6"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 6.6</span><p class="SimplePara">Influence of different modeling improvements on the <span class="EmphasisTypeSmallCaps ">Bleu</span> scores on the development set of WMT2021 for Facebook AI’s WMT2021 submission [<span class="CitationRef"><a epub:type="biblioref" href="#CR225" role="doc-biblioref">225</a></span>]. The version of the last row was submitted</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/><col class="tcol5"/><col class="tcol6"/><col class="tcol7"/><col class="tcol8"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Improvement strategy</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Czech</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">German</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Hausa</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Icelandic</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Japanese</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Russian</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Chinese</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Bilingual</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">33.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">38.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">14.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">25.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">25.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">25.8</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">40.0</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">+ Back-translation</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">33.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">39.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">23.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">29.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">26.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">25.7</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">42.4</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">+ Fine-tuning</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">35.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">39.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">23.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">29.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">27.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">26.0</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">43.0</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">+ Multilingual</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">36.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">40.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">24.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">31.2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">29.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">26.8</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">43.6</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">+ Ensemble</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">36.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">41.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">25.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">32.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">29.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">26.9</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">43.6</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">+ Reranking</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">37.2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">41.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">25.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">32.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">29.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">27.4</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">43.6</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">+ Postprocessing</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">39.8</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">42.6</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">25.5</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">34.5</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">29.8</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">28.8</p></td><td style="text-align: left;"><p class="SimplePara">48.2</p></td></tr></tbody></table></div></div><div class="Para" id="Par128">The <strong class="EmphasisTypeBold ">PaLM</strong><span id="ITerm87"/> autoregressive language model with 540B parameters [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>] has about 22% non-English training texts among its 780B training tokens (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec3"><span class="RefSource">3.​1.​2</span></a></span>). Similar to other large LMs, PaLM is not trained explicitly on parallel text, although some such data is likely to exist naturally in the training corpus. In Table <span class="InternalRef"><a href="#Tab7">6.7</a></span> the results of PaLM 540B few-shot translation is compared with prior few-shot and fine-tuned <span class="EmphasisTypeSmallCaps ">Sota</span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>, p. 27]. The best <span class="EmphasisTypeSmallCaps ">Bleu</span> value per language pair is underlined and the best few-shot <span class="EmphasisTypeSmallCaps ">Bleu</span> is printed in bold. The table shows that PaLM on the traditional WMT translation pairs always achieves the best few-shot <span class="EmphasisTypeSmallCaps ">Bleu</span>, often improving by a wide margin. For the low-resource language Kazakh (kk) the fine-tuned translation models have a better <span class="EmphasisTypeSmallCaps ">Bleu</span> than PaLM. However, for de→en and ro→en PaLM is able to outperform the supervised models. In addition, the 0-shot PaLM translation of fr→en achieves a <span class="EmphasisTypeSmallCaps ">Bleu</span> value of 25.2, which is better than the fine-tuned <span class="EmphasisTypeSmallCaps ">Sota</span> of 24.9. Overall, PaLM performs well close to the fine-tuned models without having been trained for this task. <div class="Table" id="Tab7"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 6.7</span><p class="SimplePara">Comparison of PaLM few-shot translation performance against prior fine-tuned translation performance by specialized models and prior few-shot performance. On the left you find the translation from English and into English for the traditional WMT language pairs. On the right there is the translation to and from English to Kazakh (kk) and a translation between German and French [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>, p. 27]</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/><col class="tcol5"/><col class="tcol6"/><col class="tcol7"/><col class="tcol8"/><col class="tcol9"/><col class="tcol10"/><col class="tcol11"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">From</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">en</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">en</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">en</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">fr</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">de</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">ro</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">en</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">de</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">kk</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">fr</p></th></tr><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">To</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">fr</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">de</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">ro</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">en</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">en</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">en</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">kk</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">fr</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">en</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">de</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Prior fine-tuned <span class="EmphasisTypeSmallCaps ">Sota</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">45.6</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">41.2</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">33.4</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">45.4</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">41.2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">39.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">15.5</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">31.5</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">30.5</span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">24.9</span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Prior few-shot <span class="EmphasisTypeSmallCaps ">Sota</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">33.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">26.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">20.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">38.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">40.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">37.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">–</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">–</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">–</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">–</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">PaLM 540B few-shot</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">44.0</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">37.4</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">28.7</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">42.8</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBoldUnderline ">47.5</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBoldUnderline ">43.8</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">5.1</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">25.7</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">20.8</p></td><td style="text-align: left;"><p class="SimplePara">17.4</p></td></tr></tbody></table></div></div></section>
<section class="Section2 RenderAsSection2" id="Sec22"><h3 class="Heading"><span class="HeadingNumber">6.3.3 </span>Multilingual Question Answering</h3><p class="Para" id="Par129">In recent years open domain question answering (ODQA) has taken a rapid development (Sect. <span class="InternalRef"><a href="#Sec9">6.2</a></span>). Therefore, it is extremely rewarding to extend these techniques to multilingual question answering. In this way, information encoded with the world’s different languages can be tapped and the digital divide can be narrowed by bringing answers to people who speak rarer languages. There is a tutorial on multilingual ODQA by Ruder [<span class="CitationRef"><a epub:type="biblioref" href="#CR192" role="doc-biblioref">192</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR193" role="doc-biblioref">193</a></span>].</p><p class="Para" id="Par130">A simple way to perform multilingual ODQA is to translate the question to English, use an English ODQA system to generate an answer, and translate the answer back to the target language. Because of ambiguities in translation, this procedure may generate errors in some cases [<span class="CitationRef"><a epub:type="biblioref" href="#CR132" role="doc-biblioref">132</a></span>]. In addition, information specific to the target language and conceptualizations of the target culture may not be available in English [<span class="CitationRef"><a epub:type="biblioref" href="#CR258" role="doc-biblioref">258</a></span>].</p><p class="Para" id="Par131">The <em class="EmphasisTypeItalic ">TyDiQA-GoldP benchmark</em><span id="ITerm88"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>] is a question answering dataset covering 11 typologically different languages with 204K question-answer pairs. The following languages are included: English, Arabic, Bengali, Finnish, Indonesian, Japanese, Kiswahili, Korean, Russian, Telugu, Thai. As the languages represented in this benchmarks have a very diverse structure, a model which performs well on this data can be expected to have a good QA-accuracy on other languages. <em class="EmphasisTypeItalic ">MKQA</em><span id="ITerm89"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR133" role="doc-biblioref">133</a></span>] is an evaluation dataset created by translating 10k Natural Questions [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>] to 25 target languages.</p><div class="Para" id="Par132">As an alternative, one can train cross-lingual retriever and reader models combining the information from multiple languages to generate an answer in the target language (Fig. <span class="InternalRef"><a href="#Fig12">6.12</a></span>). <strong class="EmphasisTypeBold ">CORA</strong><span id="ITerm90"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>] answers questions across many languages, even for ones without language-specific annotated data or knowledge sources. It includes a dense passage retriever collecting documents with different languages for a question. A pre-trained multilingual language model <em class="EmphasisTypeItalic ">mDPR</em> using mBERT (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec13"><span class="RefSource">3.​3.​1</span></a></span>) is fine-tuned to encode passages and questions separately. By performing a maximum inner product search the top <em class="EmphasisTypeItalic ">k</em> documents are retrieved similar to DPR (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec22"><span class="RefSource">3.​4.​5</span></a></span>). It could be shown that mBERT improves the search quality in non-English mono-lingual retrieval [<span class="CitationRef"><a epub:type="biblioref" href="#CR203" role="doc-biblioref">203</a></span>]. The reader <em class="EmphasisTypeItalic ">mGEN</em> is a multilingual autoregressive sequence model (e.g. mT5, Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec14"><span class="RefSource">3.​3.​2</span></a></span>) generating the answer in the target language by compiling the information in the retrieved passages. No specific translation models are used. The initial training data is a combination of multilingual QA datasets. Each training instance from these datasets comprises a question, a positive passage, and an answer. However, these datasets suffer from limitations on language diversity. Therefore, the authors iteratively generate more representative training data for low-resource languages by exploiting links between Wikipedia articles in different languages.<figure class="Figure" id="Fig12"><div class="MediaObject" id="MO12"><img alt="" aria-describedby="d64e5720" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig12_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e5720"><p class="Para" id="Par329">An illustration represents a set of 2 queries, one in French and the other in Norwegian, that go through the retrieved documents to generate the answers. The generated answer for the French question is wrong, while for the Norwegian question is right.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.12</span><p class="SimplePara">Cross-lingual retrieval by mDPR and answer generation with mGEN for the CORA system [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>, p. 9]. The answers to the questions are correct, however, on the left side the answer should have been given in French</p></div></figcaption></figure></div><p class="Para" id="Par133">It turns out that CORA substantially outperforms the previous <span class="EmphasisTypeSmallCaps ">Sota</span> on multilingual open QA benchmarks across 26 languages, 9 of which are unseen during training. Here CORA can improve the average F1-value from 17.1 to 21.8. Retrieval with mDPR performs well in Indo-European languages with Latin script, even when the language is unseen. There is a major drop for languages with non-Latin script (e.g., Japanese, Russian, Chinese). Here, perhaps, the model is unable to use relevant passages from other languages to answer questions.</p><p class="Para" id="Par134"><strong class="EmphasisTypeBold ">mT5</strong><span id="ITerm91"/> (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec14"><span class="RefSource">3.​3.​2</span></a></span>) is a multilingual version of the T5 Seq2seq transformer with up to 13B parameters [<span class="CitationRef"><a epub:type="biblioref" href="#CR246" role="doc-biblioref">246</a></span>]. It was pre-trained using a training dataset of web pages covering 101 languages with about 48B tokens and a common vocabulary of 250k tokens. After fine-tuning on the TyDiQA benchmark, it arrives at an exact match score of 79.1%. <strong class="EmphasisTypeBold ">ByT5</strong><span id="ITerm92"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR245" role="doc-biblioref">245</a></span>] is a variation of the mT5 multilingual encoder-decoder with 12.9B parameters. It operates on utf-8 bytes with a vocabulary of 256 possible byte values instead of tokens. The model is pre-trained to replace corrupted spans of 20 bytes on average. The largest model uses 36 encoder and 12 decoder layers. When the model is fine-tuned on gold data in all target languages, it achieves an exact match score of 81.4% on the TyDiQA benchmark.</p><div class="Para" id="Par135">The <strong class="EmphasisTypeBold ">PaLM</strong><span id="ITerm93"/> Foundation Model [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>] has about 22% non-English training texts in its 780B training tokens (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec3"><span class="RefSource">3.​1.​2</span></a></span>). Therefore, it can be applied to multilingual tasks such as translation and question answering. With few-shot prompts it gets an exact match score on TyDiQA of 60.5%. When the model is fine-tuned on TyDiQA, the score grows to 80.0%, which is slightly below of the performance of ByT5 XXL. The detailed results in Table <span class="InternalRef"><a href="#Tab8">6.8</a></span> show the performance for different languages. Here PaLM has a better score for two languages than ByT5. The authors remark, that ByT5 was trained with 50% more non-English text compared to PaLM, which may explain the difference. <div class="Table" id="Tab8"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 6.8</span><p class="SimplePara">Comparison against <span class="EmphasisTypeSmallCaps ">Sota</span> on TyDiQA question answering benchmark with 11 typologically different languages. The values are for the validation set with respect to the exact match accuracy [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>, p. 32]. Best values for each language printed in bold</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/><col class="tcol5"/><col class="tcol6"/><col class="tcol7"/><col class="tcol8"/><col class="tcol9"/><col class="tcol10"/><col class="tcol11"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Ar</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Bn</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">En</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Fi</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Id</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Ko</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Ru</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Sw</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Te</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Avg</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">mT5 XXL</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">76.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">80.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">75.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">76.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">81.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">75.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">76.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">84.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">83.9</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">79.1</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">ByT5 XXL</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">80.0</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">85.0</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">77.7</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">78.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">85.7</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">78.3</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">78.2</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">84.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">85.5</strong></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">81.4</strong></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PaLM 540B fine-tuned</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">75.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">83.2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">75.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">78.9</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">84.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">75.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">77.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">85.2</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">84.9</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">80.0</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">PaLM 540B few-shot</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">56.4</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">54.0</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">65.5</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">66.4</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">69.2</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">63.8</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">46.8</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">75.6</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">46.9</p></td><td style="text-align: left;"><p class="SimplePara">60.5</p></td></tr></tbody></table></div></div><section class="Section3 RenderAsSection3" id="Sec23"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par136"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par137">Hugging Face provides Marian, BART and T5 (up to 11B parameters) as well as multilingual mBART and mT5 implementations and trained models <span class="ExternalRef"><a href="https://huggingface.co/transformers/"><span class="RefSource">https://​huggingface.​co/​transformers/​</span></a></span>.</p></li><li><p class="Para" id="Par138">The M2M-100 [<span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>] is available with open-source data collection scripts, model code and parameters of trained models. In addition, the Fairseq system <span class="ExternalRef"><a href="https://github.com/pytorch/fairseq"><span class="RefSource">https://​github.​com/​pytorch/​fairseq</span></a></span> can freely be used.</p></li><li><p class="Para" id="Par139">The CORA [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>] implementation of multilingual QA, generated training data and trained models are available at <span class="ExternalRef"><a href="https://github.com/AkariAsai/CORA"><span class="RefSource">https://​github.​com/​AkariAsai/​CORA</span></a></span>.</p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec24"><h3 class="Heading"><span class="HeadingNumber">6.3.4 </span>Summary</h3><p class="Para" id="Par140">In recent years, machine translation has taken a dramatic development. The use of encoder-decoder PLMs could overcome the limitations of RNN architectures and increase the performance to near-human levels. Besides the utilization of encoder-decoder Transformers, the availability of high-quality training examples by web crawlers using Foundation Models and specific assessment procedures is a reason for progress [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>]. A further improvement resulted from sentence back-translation, which particularly increases results for low-resource languages, and from training a single multilingual model for translation between all languages. Training multilingual translation models with up to 600B parameters—using appropriate parallelization strategies—leads to significant performance increase for 100 languages, as measured by <span class="EmphasisTypeSmallCaps ">Bleu</span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR113" role="doc-biblioref">113</a></span>]. Recently multilingual models even were able to outperform high-resource bilingual translation models. This is also demonstrated by the PaLM Foundation Model, which achieved higher performance in few-shot translation than the prior fine-tuned models for some language pairs. Therefore, multilingual models are likely to become standard in the future. However, current multilingual models using unsupervised multilingual training may not deeply model the subtleties of languages and language varieties to their full extent. This has to be checked in future applications.</p><p class="Para" id="Par141">The developments opened up the opportunity for multilingual question answering systems, e.g. CORA, where queries can be posed in a large number of languages. The answers are compiled from information available in multiple languages. In this way, cultural characteristics and concepts that are not available in all languages can be taken into account. There are also close links to cross-lingual semantic parsing, where a natural language utterance is translated to a logical form for execution in some knowledge base to return an answer [<span class="CitationRef"><a epub:type="biblioref" href="#CR202" role="doc-biblioref">202</a></span>]. Again the PaLM Foundation Model provided few-shot answers to multilingual questions, which are competitive in accuracy to fine-tuned models for the same benchmarks. A fine-tuned version of PaLM is even able to outperform prior fined-tuned <span class="EmphasisTypeSmallCaps ">Sota</span> for two languages.</p><p class="Para" id="Par142">However, machine translation is not yet solved. There is still the problem of domain mismatch between train and test data. In some cases, it fails to accurately capture the meaning of a sentence. Systems can generate biased text, e.g. if gender is handled differently in different languages. But attention allows the decoder to look directly at faraway text and provides a soft alignment between words for free. Recently, performance could be increased by translating entire documents, as sentences often are not sufficient to disambiguate all words. To extend current multilingual models to thousands of languages, new techniques are required [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>]. One approach is to use monolingual datasets to improve translation, since the amount of available monolingual text is orders of magnitude greater than the amount of translated text. This in addition requires highly reliable language detectors which also work for low-resource languages.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec25"><h2 class="Heading"><span class="HeadingNumber">6.4 </span>Text Summarization</h2><p class="Para" id="Par143">With the rapid increase of textual information in companies and on the Internet, it is increasingly difficult for people to keep track of a topic. Automatic <em class="EmphasisTypeItalic ">summarization</em><span id="ITerm94"/> of documents, which compiles the essential statements from a text, can help to grasp the most relevant information in the documents. A <em class="EmphasisTypeItalic ">summary</em><span id="ITerm95"/> is a short version produced from a single document or multiple documents conveying the main points of the original texts. The purpose of automatic text summarization is to create a <em class="EmphasisTypeItalic ">summarizer</em><span id="ITerm96"/> method to produce this summary efficiently and precisely. Recent in-depth surveys are provided by Ma et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR135" role="doc-biblioref">135</a></span>], Guan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR71" role="doc-biblioref">71</a></span>], Syed et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR216" role="doc-biblioref">216</a></span>], and El-Kassas et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR95" role="doc-biblioref">95</a></span>].</p><p class="Para" id="Par144">Earlier machine learning approaches produced <em class="EmphasisTypeItalic ">extractive summaries</em><span id="ITerm97"/><span id="ITerm98"/> selecting a few sentences from the document. This approach typically selected grammatically correct sentence parts, but the language style of the combined parts and the coverage were usually not sufficient. Modern summarizers pose summarization as a translation problem, which translates the original document to a short version covering the main points. Since 2017 the encoder-decoder transformer (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec19"><span class="RefSource">2.​3</span></a></span>) provided an effective technique to generate <em class="EmphasisTypeItalic ">abstractive summaries</em><span id="ITerm99"/><span id="ITerm100"/> containing the main points of the document. Abstractive summarization is a bit more complex because the text is paraphrased, and the summary usually has words different from the original document. On the other hand, it is more flexible and can aggregate several similar texts expressing related facts with different wordings.</p><div class="Para" id="Par145">Basically, summarization is treated as a translation task, where the long document is translated into the short summary. Alternatively we can use the long document as the start text of an autoregressive Foundation Model, which is fine-tuned to generate a summary. One of the main challenges for Seq2seq models is that the decoder needs to attend to encoder token embeddings in the large document context to predict the next token of the summary. Therefore, Seq2seq models covering a long input context (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec7"><span class="RefSource">3.​2</span></a></span>) are natural candidates. Summarization systems can be either <em class="EmphasisTypeItalic ">single document summarizers</em><span id="ITerm101"/> or <em class="EmphasisTypeItalic ">multi-document summarizers</em><span id="ITerm102"/>. Table <span class="InternalRef"><a href="#Tab9">6.9</a></span> lists popular summarization models and their performance. <div class="Table" id="Tab9"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 6.9</span><p class="SimplePara">Summarization models with their performance measured in <span class="EmphasisTypeSmallCaps ">Rouge-2</span>. Benchmarks are CNN/DM: CNN/Daily Mail benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>], XSum [<span class="CitationRef"><a epub:type="biblioref" href="#CR151" role="doc-biblioref">151</a></span>] summarize an news article in a single sentence, arXiv [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>] long scientific documents, PubMed [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>] long medical documents, Multi-News [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>] with an average document length of 1793 and 2.8 documents per cluster</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Details</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeSmallCaps ">Rouge-2</span> on benchmark</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PEGASUS (Sect. <span class="InternalRef"><a href="#Sec26">6.4.1</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Seq2seq model pre-trained with masked sentences</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">CNN/DM 21.7, XSum 24.6</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BRIO (Sect. <span class="InternalRef"><a href="#Sec26">6.4.1</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GPT architecture trained to generate text spans</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">CNN/DM 23.6, XSum 25.6</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PaLM (Sect. <span class="InternalRef"><a href="#Sec26">6.4.1</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">540B large LM to generate text</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">XSum 1-shot 12.2, fine-tuned 21.7</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">ST-MoE (Sect. <span class="InternalRef"><a href="#Sec26">6.4.1</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">269B large mixture-of-experts to generate text</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">CNN/DM 20.7, XSum 21.7</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">STIE (Sect. <span class="InternalRef"><a href="#Sec26">6.4.1</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">6.7B GPT model adapted to human preference judgments by reinforcement learning</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">STIE summaries are preferred to reference summaries in 70% of the cases</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BigBird (Sect. <span class="InternalRef"><a href="#Sec27">6.4.2</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model for large inputs</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">arXiv 19.0, PubMed 20.7</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">HAT (Sect. <span class="InternalRef"><a href="#Sec27">6.4.2</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model for large inputs using PEGASUS</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">arXiv 19.7, PubMed 21.4, CNN/DM 21.3</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">RL-175B (Sect. <span class="InternalRef"><a href="#Sec27">6.4.2</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model based on GPT-3 for stepwise summarizing a book using reinforcement learning</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Human comparison: Likert value 3.5 of 7</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">PRIMER (Sect. <span class="InternalRef"><a href="#Sec28">6.4.3</a></span>)</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Summarize several documents based on Longformer Seq2seq model</p></td><td style="text-align: left;"><p class="SimplePara">Fine-tuned arXiv 20.8, fine-tuned Multi-News 21.1</p></td></tr></tbody></table></div></div><section class="Section2 RenderAsSection2" id="Sec26"><h3 class="Heading"><span class="HeadingNumber">6.4.1 </span>Shorter Documents</h3><p class="Para" id="Par146">The training data usually consist of documents and the corresponding summaries or abstracts. There are a number of actual benchmark datasets for summarization like CNN/Daily Mail [<span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>], Gigaword [<span class="CitationRef"><a epub:type="biblioref" href="#CR150" role="doc-biblioref">150</a></span>], and Reddit TIFU [<span class="CitationRef"><a epub:type="biblioref" href="#CR101" role="doc-biblioref">101</a></span>], which have an input document with a length below 1000 tokens and a corresponding summary, which can be used for fine-tuning. The difference between a reference summary and a predicted summary is assessed by measures like <span class="EmphasisTypeSmallCaps ">Rouge</span>, <span class="EmphasisTypeSmallCaps ">Bleu</span>, or <span class="EmphasisTypeSmallCaps ">Meteor</span> (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec23"><span class="RefSource">2.​3.​3</span></a></span>) with the recall-oriented <span class="EmphasisTypeSmallCaps ">Rouge</span> most frequently used.</p><p class="Para" id="Par147"><strong class="EmphasisTypeBold ">PEGASUS</strong><span id="ITerm103"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR128" role="doc-biblioref">128</a></span>] is large transformer-based Seq2seq model pre-trained on massive text corpora (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec4"><span class="RefSource">3.​1.​3</span></a></span>). It follows a new pre-training objective in which not tokens are masked, but sentences. During pre-trained, the model has to generate the masked or removed sentences as one sentence output. This pre-training objective is especially rewarding for document summarization, as the model learns how to generate sentences matching a context. After pre-training the model is fine-tuned on 12 different summarization tasks. It reaches <span class="EmphasisTypeSmallCaps ">Sota</span>-results on all 12 downstream datasets as measured with different <span class="EmphasisTypeSmallCaps ">Rouge</span> statistics. In most cases the improvements are considerable [<span class="CitationRef"><a epub:type="biblioref" href="#CR128" role="doc-biblioref">128</a></span>], e.g. for the CNN/Daily Mail benchmark it had a <span class="EmphasisTypeSmallCaps ">Rouge-2</span>-score of 21.7. The <span class="EmphasisTypeSmallCaps ">Rouge-2</span>-scores of other Seq2seq models are similar, e.g. 21.6 for T5, 21.3 for BART, and 21.5 for R3F [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>]. Note that for text generation often a BEAM search (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec15"><span class="RefSource">2.​2.​3</span></a></span>) is employed keeping several high probability versions of the text to increase the consistency of the resulting text.</p><p class="Para" id="Par148"><strong class="EmphasisTypeBold ">BRIO</strong><span id="ITerm104"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR131" role="doc-biblioref">131</a></span>] starts from the observation that the usual ML-training only takes into account a single reference summary for each example and ignore possible other summaries. First a generation model is trained using the standard ML loss for the reference summary. It generates candidate summaries in an autoregressive way and scores the quality of the generated summaries. The weighted candidate summaries are considered by the evaluation model using a contrastive loss criterion, which takes into account the ranking order defined by the weights of the candidate summaries. The approach uses BART or PEGASUS as backbone Seq2seq models. On the <em class="EmphasisTypeItalic ">CNN/Daily Mail benchmark</em><span id="ITerm105"/> benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>] the BRIO model with 10B parameters has <span class="EmphasisTypeSmallCaps ">Sota</span> performance with the <span class="EmphasisTypeSmallCaps ">Rouge-2</span> score of 23.6 on CNN/DM and 25.6 on XSum. By increasing the number of candidates from 4 to 100 by extending the beam width, the <span class="EmphasisTypeSmallCaps ">Rouge-2</span> on CNN/DM could be increased to 24.1. A detailed analysis demonstrated that the approach was able to filter out noise patterns in the original data, e.g. the phrase “click here”.</p><p class="Para" id="Par149">The autoregressive language models GPT-3, Gopher, InstructGPT and PaLM can be instructed to summarize, e.g. by entering a text and appending <em class="EmphasisTypeItalic ">“TL;DR:”</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR159" role="doc-biblioref">159</a></span>]. For <strong class="EmphasisTypeBold ">PaLM</strong><span id="ITerm106"/> with 540B parameters an evaluation is available. The <em class="EmphasisTypeItalic ">MLSum benchmark</em><span id="ITerm107"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR198" role="doc-biblioref">198</a></span>] requires the model to summarize a news article in multiple sentences. For German texts PaLM 1-shot arrives at 12.8 <span class="EmphasisTypeSmallCaps ">Rouge-2</span> and a fine-tuned version of PaLM achieves a <span class="EmphasisTypeSmallCaps ">Rouge-2</span> score of 33.1, which is below the fine-tuned <span class="EmphasisTypeSmallCaps ">Sota</span> at 36.4 [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>, p. 30]. The <em class="EmphasisTypeItalic ">XSum benchmark</em><span id="ITerm108"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR151" role="doc-biblioref">151</a></span>] requires to summarize a news article in a single sentence. Here PaLM gets a few-shot <span class="EmphasisTypeSmallCaps ">Rouge-2</span> score of 12.2 and a fine-tuned <span class="EmphasisTypeSmallCaps ">Rouge-2</span> of 21.2, whereas the fine-tuned <span class="EmphasisTypeSmallCaps ">Sota</span><span class="EmphasisTypeSmallCaps ">Rouge-2</span> by BRIO is 25.6.</p><p class="Para" id="Par150"><strong class="EmphasisTypeBold ">ST-MoE-32B</strong><span id="ITerm109"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR270" role="doc-biblioref">270</a></span>] is a mixture-of-experts model (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec26"><span class="RefSource">3.​5.​2</span></a></span>) with 269B parameters. On the <em class="EmphasisTypeItalic ">CNN/Daily Mail benchmark</em><span id="ITerm110"/> it achieves a fine-tuned <span class="EmphasisTypeSmallCaps ">Sota</span><span class="EmphasisTypeSmallCaps ">Rouge-2</span> value of 21.7 and on the <em class="EmphasisTypeItalic ">XSum benchmark</em> it yields 27.1 <span class="EmphasisTypeSmallCaps ">Rouge-2</span> with fine-tuning. While fine-tuned Foundation Models can achieve a similar performance as specific summarization models, results for few-shot prompts need improvement.</p><p class="Para" id="Par151"><span class="EmphasisTypeSmallCaps ">Rouge</span> metrics are only a crude guide to what people really care about: the quality of a summary. Stiennon et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR211" role="doc-biblioref">211</a></span>] directly optimize their model with respect to human judgment. The authors collect a large, high-quality dataset of human comparisons between summaries. Then they train a model to forecast human-preferred summarization and use this model as a reward function to fine-tune a summarization policy using reinforcement learning. They apply their model to the <em class="EmphasisTypeItalic ">TL;DR benchmark</em><span id="ITerm111"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR230" role="doc-biblioref">230</a></span>], because this summarization task is significantly more challenging than CNN/DM. They find that the summaries of their 6.7B parameter <strong class="EmphasisTypeBold ">STIE</strong><span id="ITerm112"/> model are significantly preferred to the reference summaries 70% of the time, whereas the summaries of fine-tuned alternative models are preferred to the reference summaries about 43% of the cases. The model can also be applied to new domains better than other methods. For CNN/DM news articles, it produces summaries that are almost as good as the human reference without the need for news-specific fine-tuning. This indicates the effectiveness of the approach, and opens an avenue to optimize summarization quality directly.</p></section>
<section class="Section2 RenderAsSection2" id="Sec27"><h3 class="Heading"><span class="HeadingNumber">6.4.2 </span>Longer Documents</h3><p class="Para" id="Par152">While the input document length of documents is generally less than 1000 tokens, it is greater for the <em class="EmphasisTypeItalic ">PubMed corpus</em><span id="ITerm113"/> (4k tokens) and <em class="EmphasisTypeItalic ">ArXiv benchmark</em><span id="ITerm114"/> (8.6k tokens) [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>]. For these benchmarks transformers with longer input sequences (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec7"><span class="RefSource">3.​2</span></a></span>) are capable of taking into account the whole document.</p><p class="Para" id="Par153"><strong class="EmphasisTypeBold ">BigBird</strong><span id="ITerm115"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR253" role="doc-biblioref">253</a></span>] is able to cope with long documents (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec8"><span class="RefSource">3.​2.​1</span></a></span>). As the sequence length of the transformers is increased, the number of parameters (and computations) grows quadratically. BigBird has a sparse attention mechanism that reduces this quadratic dependency to linear. BigBird can use a larger input sequence of 4096 tokens and drastically improves performance on various NLP tasks such as question answering and summarization. Longer documents exhibit a richer discourse structure and summaries are considerably more abstractive. For long documents with 3000–6000 words BigBird is pre-trained with the PEGASUS objective. After fine-tuning it yields a marked improvement on <span class="EmphasisTypeSmallCaps ">Sota</span>, e.g. on the ArXiv benchmark with the <span class="EmphasisTypeSmallCaps ">Rouge-2</span> score 19.0. <strong class="EmphasisTypeBold ">TLDR</strong><span id="ITerm116"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>] is a similar summarizer based on BART, which generates a one-sentence summary for scientific papers. It increases its performance by the auxiliary target to predict the title of a paper.</p><p class="Para" id="Par154"><strong class="EmphasisTypeBold ">HAT</strong><span id="ITerm117"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR187" role="doc-biblioref">187</a></span>] aims to capture the content of longer documents in a better way. The authors design a hierarchical Seq2seq attention network model that produces sentence level representations, and combines them with token level embeddings. They determine sentence boundaries by punctuation and insert [<em class="EmphasisTypeItalic ">BOS</em>] tokens at the start of every sentence. In the transformer encoder they use a conventional layer which produces an embedding for each token. After this an additional <em class="EmphasisTypeItalic ">hierarchical layer</em> is added which only attends to the embeddings of the [<em class="EmphasisTypeItalic ">BOS</em>] tokens. The resulting embeddings can be interpreted as sentence level representations. The transformer decoder is standard with an additional layer that attends to the [<em class="EmphasisTypeItalic ">BOS</em>] tokens from the hierarchical encoder layer. On the <em class="EmphasisTypeItalic ">PubMed benchmark</em><span id="ITerm118"/> of long documents [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>] it yields a <span class="EmphasisTypeSmallCaps ">Sota</span><span class="EmphasisTypeSmallCaps ">Rouge-1</span> score of 21.4. while on arXiv it has a <span class="EmphasisTypeSmallCaps ">Rouge-1</span> score of 19.7. But also on the <em class="EmphasisTypeItalic ">CNN/Daily Mail benchmark</em><span id="ITerm119"/> of shorter documents [<span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>] it achieves a <span class="EmphasisTypeSmallCaps ">Sota</span><span class="EmphasisTypeSmallCaps ">Rouge-2</span> scores of 21.3,</p><p class="Para" id="Par155"><strong class="EmphasisTypeBold ">RL-175B</strong><span id="ITerm120"/> is a summarizer for whole books by OpenAI using a reinforcement learning algorithm to follow human preferences [<span class="CitationRef"><a epub:type="biblioref" href="#CR236" role="doc-biblioref">236</a></span>]. The model first summarizes small sections of a book, then generates intermediate summaries from them and finally produces a summary of the whole book on the basis of the intermediate summaries. The model is based on <em class="EmphasisTypeItalic ">GPT-3</em><span id="ITerm121"/> and evaluates a large set of summary activities created by human labelers. The small sections are generated by a fixed chunking algorithm. Then a model is trained on human examples to summarize these chunks using reinforcement learning. It uses the approach explained in Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec43"><span class="RefSource">3.​6.​5</span></a></span>. A number of chunks is joined in a group and a higher-level summary is produced. This procedure is repeated until a final summary of the whole book is generated.</p><p class="Para" id="Par156">The fine-tuning was performed for the GPT-3 with 7B and 175B parameters. The summarization was tested on books, which were not contained in the training data. The scoring is done by a <em class="EmphasisTypeItalic ">Likert scale</em><span id="ITerm122"/> from 1 to 7. It assigns numbers to human judgments (e.g. 1 = very bad, 2 = bad, …, 7 = very good), and computes averages from these numbers. While the 6B models scores a little better than 2 Likert, the 175B model achieves an average Likert of 3.5. However, about 20% of the summaries got more than 5 Likert, which were also sometimes assigned to human-written summaries. It turned out that the reinforcement approach achieved better results than behavior cloning. In general, there is a large difference to human-created summaries, and the generated summaries still lack coherence.</p></section>
<section class="Section2 RenderAsSection2" id="Sec28"><h3 class="Heading"><span class="HeadingNumber">6.4.3 </span>Multi-Document Summarization</h3><p class="Para" id="Par157">Often, information is spread across multiple documents, and it makes sense to summarize this content. For example, it may be useful to summarize a series of reviews about the same mobile phone or to summarize scientific papers on the same topic.</p><div class="Para" id="Par158"><strong class="EmphasisTypeBold ">Primer</strong><span id="ITerm123"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR237" role="doc-biblioref">237</a></span>] is based on the <em class="EmphasisTypeItalic ">Longformer</em><span id="ITerm124"/> encoder-decoder (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec8"><span class="RefSource">3.​2.​1</span></a></span>), an efficient transformer model with an input length of 4096 tokens, where the effort for processing long documents grows linearly with their length. The input documents are concatenated and separated with [<em class="EmphasisTypeItalic ">doc</em> − <em class="EmphasisTypeItalic ">sep</em>] tokens. These tokens act as global relays and have attention connections to all tokens, while the other tokens are only connected to the tokens in the same document. In this way, large sequences of input documents can be processed. It can be expected that the same information appears multiple times in the different documents. PRIMER selects sentences, which are similar in different documents based on the <span class="EmphasisTypeSmallCaps ">Rouge</span> score and uses common entities as an additional selection criterion. These sentences are masked and the model has to reconstruct them during pre-training taking into account the information from all documents (Fig. <span class="InternalRef"><a href="#Fig13">6.13</a></span>).<figure class="Figure" id="Fig13"><div class="MediaObject" id="MO13"><img alt="" aria-describedby="d64e6693" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig13_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e6693"><p class="Para" id="Par330">An illustration represents the interaction of 3 documents with the long former encoder, that goes through the decoder to recover the masked sentence. It indicates the layers of input, local attention, and global attention.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.13</span><p class="SimplePara">Multiple documents form the input for PRIMER, separated with [doc-sep] tokens. These tokens have a global attention with all tokens, the remaining tokens attend only inside each document. Some sentences are selected and have to be recovered by the decoder [<span class="CitationRef"><a epub:type="biblioref" href="#CR237" role="doc-biblioref">237</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par159">The pre-training already enables the model to combine the information from different documents. Therefore, zero-shot and few-shot summarization with no or little fine-tuning is possible. For the <em class="EmphasisTypeItalic ">Multi-News benchmark</em><span id="ITerm125"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>] with an average document length of 1793 and 2.8 documents per cluster, PRIMER achieves a zero-shot <span class="EmphasisTypeSmallCaps ">Rouge-2</span> score of 13.6 and can increase this to 21.1, which establishes a new <span class="EmphasisTypeSmallCaps ">Sota</span> for this multi-document summarization benchmark. On the <em class="EmphasisTypeItalic ">ArXiv benchmark</em><span id="ITerm126"/> with an average document length of 6021 tokens [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>], the fine-tuned PRIMER yields a <span class="EmphasisTypeSmallCaps ">Rouge-2</span> score of 20.8, indicating the performance on long documents.</p><section class="Section3 RenderAsSection3" id="Sec29"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par160"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par161">T5, BigBird, and Pegasus code and trained models are available on Hugging Face <span class="ExternalRef"><a href="https://huggingface.co/transformers/"><span class="RefSource">https://​huggingface.​co/​transformers/​</span></a></span>.</p></li><li><p class="Para" id="Par162">Further summarization scripts at <span class="ExternalRef"><a href="https://huggingface.co/tasks/summarization"><span class="RefSource">https://​huggingface.​co/​tasks/​summarization</span></a></span>.</p></li><li><p class="Para" id="Par163">STIE data and code <span class="ExternalRef"><a href="https://github.com/openai/summarize-from-feedback"><span class="RefSource">https://​github.​com/​openai/​summarize-from-feedback</span></a></span></p></li><li><p class="Para" id="Par164">PRIMER code for Multi-document Summarization <span class="ExternalRef"><a href="https://github.com/allenai/PRIMER"><span class="RefSource">https://​github.​com/​allenai/​PRIMER</span></a></span></p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec30"><h3 class="Heading"><span class="HeadingNumber">6.4.4 </span>Summary</h3><p class="Para" id="Par165">Foundation Models initiated a breakthrough for summarization models. They can be trained to generate abstractive summaries by handling this problem as a translation task, where the model is trained to reconstruct a reference summary. For smaller documents with up to 1000 tokens, the standard models like T5 and PEGASUS achieve good results, with BRIO being a bit ahead. Models with more parameters have a slightly better performance. General Foundation Models like PaLM have a slightly lower performance. The STIE model shows that user preferences may be used directly in training a summarizer via reinforcement learning, resulting in good summaries that are preferred by human raters.</p><p class="Para" id="Par166">For larger documents a transformer encoder-decoder with a larger input sequence is required, e.g. BigBird. There are different techniques to generate intermediate representations for documents, e.g. for sentences by HAT or chunks by RL-175B. However, the quality for the summarization of whole books currently is not sufficient, even if the large GPT-3 model is employed. A recent alternative is InstructGPT (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec43"><span class="RefSource">3.​6.​5</span></a></span>), which can be easily directed to perform a summarization, e.g. by the prompt <em class="EmphasisTypeItalic ">“Summarize this for a second-grade student:</em> &lt;<em class="EmphasisTypeItalic "> text</em>&gt;<em class="EmphasisTypeItalic "> ”</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR162" role="doc-biblioref">162</a></span>, p. 30]. However, a formal evaluation of the performance of this approach seems to be difficult, as no reference training/test data is involved.</p><p class="Para" id="Par167">Multi-document summarization has to cope with the repetition of contents in different documents. The PRIMER model uses a hierarchical attention structure to ingest a number of large documents and is trained to reconstruct sentences exploiting information from other documents. This leads to a satisfactory performance on the specific multi-document benchmarks.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec31"><h2 class="Heading"><span class="HeadingNumber">6.5 </span>Text Generation</h2><p class="Para" id="Par168">A system for <em class="EmphasisTypeItalic ">Natural language generation</em><span id="ITerm127"/> (NLG) has the task of producing fluent, coherent, and understandable text. Usually, the system generates a continuation of a start text. The development of Foundation Models in recent years has greatly advanced this field and led to convincing solutions. This section concentrates on writing larger texts and complete stories. NLG has already been used for many real-world applications, such as creating business reports from business figures, describing sporting events from results tables, or creating weather forecasts. Microsoft has announced to fire about 50 employees of MSN news [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>], using Deep Learning instead to identify trending news stories or optimize the content. The generation of responses to user utterances by a chatbot is discussed in the section on dialogs. A number of surveys for text generation is available [<span class="CitationRef"><a epub:type="biblioref" href="#CR65" role="doc-biblioref">65</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR83" role="doc-biblioref">83</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR116" role="doc-biblioref">116</a></span>]. Yu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR251" role="doc-biblioref">251</a></span>] give an overview of knowledge-enhanced text generation.</p><div class="Para" id="Par169">Here we will describe story generation systems based on Foundation Models that currently provide the best results. A high-level overview of approaches is given in Table <span class="InternalRef"><a href="#Tab10">6.10</a></span>. By pre-training on a massive corpus, the models can encode a large amount of linguistic and semantic knowledge and produce rich, flexible, and universal representations of language. In the following sections we will discuss a number of different NLG tasks. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><div class="Para" id="Par170">First, we describe NLG basics, where the next token <em class="EmphasisTypeItalic ">y</em> has to be generated according to a language model <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">y</em>|<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>) (Sect. <span class="InternalRef"><a href="#Sec32">6.5.1</a></span>). <div class="Table" id="Tab10"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 6.10</span><p class="SimplePara">Main text generation techniques</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Architecture</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Mechanism</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Advantages</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Disadvantages</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Variational Autoencoder (VAE) [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Compress a text <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> to a hidden vector <em><strong class="EmphasisTypeBoldItalic ">h</strong></em> distributed as a Gaussian, reconstruct the text <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> from <em><strong class="EmphasisTypeBoldItalic ">h</strong></em></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Constraint on the latent vector <em><strong class="EmphasisTypeBoldItalic ">h</strong></em> creates a continuous representation space and increases the diversity of the generated text</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Often less fluent and coherent in text generation compared to Foundation Models</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Generative Adversarial Network (GAN) [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">A generator transforms a random vector <em><strong class="EmphasisTypeBoldItalic ">s</strong></em> to a text <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>. A discriminator checks, if <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> is synthetic. Both are trained in adversarial style</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Unsupervised learning; Generating clearer and more realistic samples than other generative models</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Instable training process; sampling of <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> is non-differentiable: needs reinforcement learning or Gumbel-softmax</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Autoregressive Language Model (GPT) (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec11"><span class="RefSource">2.​2</span></a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Self-attention with previous tokens <em class="EmphasisTypeItalic ">x</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">t</em>−1</sub> to generate next token <em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">t</em></sub></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Efficient contextual embeddings and long-term context; fast parallel computing speed</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">High computational effort and slow training speed</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Encoder-decoder Transformer (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec19"><span class="RefSource">2.​3</span></a></span>)</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Self-attention over full input sequence <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> and iterative generation of output sequence <em class="EmphasisTypeItalic ">y</em><sub>1</sub>, …</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Efficient contextual embeddings and long-term context; transform input as a whole sequence</p></td><td style="text-align: left;"><p class="SimplePara">High computational effort and slow training speed</p></td></tr></tbody></table></div></div></li><li><p class="Para" id="Par171">Then we discuss the generation of a new text with a given style, e.g. a poem (Sect. <span class="InternalRef"><a href="#Sec33">6.5.2</a></span>).</p></li><li><p class="Para" id="Par172">A related task is to rewrite one document in a different style or world view (Sect. <span class="InternalRef"><a href="#Sec36">6.5.3</a></span>).</p></li><li><p class="Para" id="Par173">In general, the text created by the Foundation Model takes a consistent but random course. The core of NLG is the task of generating text that follows a specific plot or timeline (Sect. <span class="InternalRef"><a href="#Sec40">6.5.4</a></span>).</p></li></ul></div></div><div class="Para" id="Par174">Table <span class="InternalRef"><a href="#Tab11">6.11</a></span> describes these tasks and lists a number of corresponding NLG models discussed in this section. The generation of fake news or other malicious text is covered in Sect. <span class="InternalRef"><a href="#Sec44">6.5.5</a></span>. Section <span class="InternalRef"><a href="#Sec46">6.5.6</a></span> describes how to generate computer code. <div class="Table" id="Tab11"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 6.11</span><p class="SimplePara">Mechanisms to control story generation</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Approach</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Description</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Example systems</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Pre-train LM on large text (optional fine-tuning)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Pre-train the language model on a large text collection. Possibly fine-tune on a smaller corpus of a specific domain. Generate a continuation of the start text</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GPT-2 [<span class="CitationRef"><a epub:type="biblioref" href="#CR235" role="doc-biblioref">235</a></span>], GPT-3 [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>], Gopher [<span class="CitationRef"><a epub:type="biblioref" href="#CR175" role="doc-biblioref">175</a></span>], Retro [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>], WuDao [<span class="CitationRef"><a epub:type="biblioref" href="#CR263" role="doc-biblioref">263</a></span>], PaLM [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>]</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Add style or content marker</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Add style or content marker to the start text. The marker has to be present in pre-training or fine-tuning data</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">CTRL [<span class="CitationRef"><a epub:type="biblioref" href="#CR96" role="doc-biblioref">96</a></span>], PPLM [<span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>], ETC-NLG [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>] using topics, GDC [<span class="CitationRef"><a epub:type="biblioref" href="#CR97" role="doc-biblioref">97</a></span>] controls token distributions, Adapter-Bot [<span class="CitationRef"><a epub:type="biblioref" href="#CR126" role="doc-biblioref">126</a></span>]</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Translate text to a new style</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Use a transformer and a possible style selector to transform an input text to a new style and nearly the same content</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Formal [<span class="CitationRef"><a epub:type="biblioref" href="#CR260" role="doc-biblioref">260</a></span>], LRE [<span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>], ACC [<span class="CitationRef"><a epub:type="biblioref" href="#CR250" role="doc-biblioref">250</a></span>], LRS [<span class="CitationRef"><a epub:type="biblioref" href="#CR118" role="doc-biblioref">118</a></span>], StyleLM [<span class="CitationRef"><a epub:type="biblioref" href="#CR217" role="doc-biblioref">217</a></span>], OPTIMUS [<span class="CitationRef"><a epub:type="biblioref" href="#CR115" role="doc-biblioref">115</a></span>], GPT-3 with two-step prompts [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>]</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Specify a sequence of events for the story</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Specify events by short sentences/phrases and generate a story containing these events in order</p></td><td style="text-align: left;"><p class="SimplePara">PlotMachines [<span class="CitationRef"><a epub:type="biblioref" href="#CR181" role="doc-biblioref">181</a></span>] uses phrases, Pointer [<span class="CitationRef"><a epub:type="biblioref" href="#CR261" role="doc-biblioref">261</a></span>] inserts words, Progressive WritingPrompts [<span class="CitationRef"><a epub:type="biblioref" href="#CR220" role="doc-biblioref">220</a></span>], Facts2Story [<span class="CitationRef"><a epub:type="biblioref" href="#CR161" role="doc-biblioref">161</a></span>] starts with a sequence of facts, GraphPlan [<span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>] uses a graph of events, SOE [<span class="CitationRef"><a epub:type="biblioref" href="#CR214" role="doc-biblioref">214</a></span>] performs a two-level process of generating text, FIST [<span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>], GPT-3 with bullet-list prompts [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>]</p></td></tr></tbody></table></div></div><p class="Para" id="Par175">The assessment of the performance of natural language generators is a difficult problem. Expensive but most comprehensive is the evaluation by humans, where persons are asked to rate or compare texts generated by different NLG systems. If texts created by humans are part of the comparison, this constitutes a <em class="EmphasisTypeItalic ">Turing test</em><span id="ITerm128"/> which may assess the “intelligence” of an NLG-system. An alternative are automatic metrics like <span class="EmphasisTypeSmallCaps ">Bleu</span>, <span class="EmphasisTypeSmallCaps ">Meteor</span> or <span class="EmphasisTypeSmallCaps ">Rouge</span> (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec23"><span class="RefSource">2.​3.​3</span></a></span>), which assess the difference between machine-generated texts to human-generated reference texts by comparing <em class="EmphasisTypeItalic ">n</em>-gram counts (Sect. <span class="InternalRef"><a href="#Sec19">6.3</a></span>). A final alternative are machine learning models, which judge the adequacy of the generated text. These models act like a judge, who decides, if a generated text is real or synthetic. Celikyilmaz et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>] discuss these evaluation approaches in detail. Yu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR251" role="doc-biblioref">251</a></span>] provide a survey of knowledge-enhanced text generation.</p><p class="Para" id="Par176"><em class="EmphasisTypeItalic ">GEM</em><span id="ITerm129"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>] is a new benchmark collection created for NLG containing seventeen different benchmarks and comprising an evolving system of evaluation metrics and procedures. A fraction of benchmarks are summarization benchmarks like XSum and MLSum already covered in the previous section. Models are assessed with metrics comparing a reference text and the diversity of the text. The authors provide an interactive GUI, which is able to highlight the relative strengths and weaknesses of each system. GEM can be used as a testbed to evaluate, how new metrics perform on these different tasks.</p><section class="Section2 RenderAsSection2" id="Sec32"><h3 class="Heading"><span class="HeadingNumber">6.5.1 </span>Generating Text by Language Models</h3><p class="Para" id="Par177">Language models (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec11"><span class="RefSource">2.​2</span></a></span>) have the task to produce the next token <em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">t</em></sub> for a text <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> = (<em class="EmphasisTypeItalic ">x</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">t</em>−1</sub>). This model can directly be applied to story generation. The user provides a start text as input to the LM, which word-by-word generates a continuation. Specifically, the model predicts for the next position the probability <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">t</em></sub>|<em class="EmphasisTypeItalic ">x</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">t</em>−1</sub>;<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>) of each token of the vocabulary. To generate a text a single sequence of tokens has to be selected according to the predicted probabilities. Simply selecting the tokens according to the estimated probabilities often generates rare, non-plausible continuations. A better alternative is top-<em class="EmphasisTypeItalic ">k</em> or top-<em class="EmphasisTypeItalic ">p</em> sampling restricting the random selection to the tokens with the highest probability (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec15"><span class="RefSource">2.​2.​3</span></a></span>).</p><p class="Para" id="Par178">Early LMs, e.g. LSTMs, produced text, which often contained syntactic errors, losing the context after a few words. <strong class="EmphasisTypeBold ">VAE</strong><em class="EmphasisTypeItalic ">Variational Auto-Encoders</em><span id="ITerm130"/> reconstruct the sentence from a randomly modified latent representation <em><strong class="EmphasisTypeBoldItalic ">z</strong></em> ∼ <em class="EmphasisTypeItalic ">N</em>(<em><strong class="EmphasisTypeBoldItalic ">μ</strong></em>, <em><strong class="EmphasisTypeBoldItalic ">σ</strong></em>), where <em><strong class="EmphasisTypeBoldItalic ">μ</strong></em> and <em><strong class="EmphasisTypeBoldItalic ">σ</strong></em> are predicted by the encoder. A KL-loss is added to the reconstruction loss such that the distribution of <em><strong class="EmphasisTypeBoldItalic ">z</strong></em> approaches a standard normal distribution [<span class="CitationRef"><a epub:type="biblioref" href="#CR89" role="doc-biblioref">89</a></span>]. <strong class="EmphasisTypeBold ">GAN</strong><em class="EmphasisTypeItalic ">Generative Adversarial Networks</em><span id="ITerm131"/> use a generator to transform a noise vector <em><strong class="EmphasisTypeBoldItalic ">s</strong></em> to a text <span class="InlineEquation" id="IEq18"><img alt="$$\tilde {{\boldsymbol {x}}}=G(\boldsymbol {s})$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq18.png" style="width:4.38em"/></span>. Then a discriminator <em class="EmphasisTypeItalic ">D</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>) has the task to distinguish synthetic text <span class="InlineEquation" id="IEq19"><img alt="$$\tilde {{\boldsymbol {x}}}$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq19.png" style="width:0.94em"/></span> from real text <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>]. Both models are trained together. These basic language generation alternatives are also covered in Table <span class="InternalRef"><a href="#Tab10">6.10</a></span>.</p><div class="Para" id="Par179">A number of classical models for text generation such as BART (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec4"><span class="RefSource">3.​1.​3</span></a></span>), T5 (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec4"><span class="RefSource">3.​1.​3</span></a></span>), and mT5 (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec14"><span class="RefSource">3.​3.​2</span></a></span>) are evaluated with the GEM benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>]. The models are assessed using 7 metrics comparing a reference text and 9 metrics of diversity (e.g. the relative number of distinct uni- and bigrams). Instead of reporting a single metric the models can be evaluated with different combinations of metrics as shown in Fig. <span class="InternalRef"><a href="#Fig14">6.14</a></span>.<figure class="Figure" id="Fig14"><div class="MediaObject" id="MO14"><img alt="" aria-describedby="d64e7410" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig14_HTML.png" style="width:34.38em"/><div class="TextObject" id="d64e7410"><p class="Para" id="Par331">An illustration highlights t 5 small from the table of data to text. It indicates the descriptive, diverse, factual, lexical, and semantic values on the right side. Below is a line graph denoting the point for t 5 small dot totto underscore v a l.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.14</span><p class="SimplePara">A screenshot of the GEM benchmark interactive result exploration tool. On the top left tasks are selected. The selection of metric-groups or metrics is on the top right. The visualization of the selected metrics is shown on the bottom. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>, p. 107]</p></div></figcaption></figure></div><p class="Para" id="Par180"><strong class="EmphasisTypeBold ">GPT-2</strong><span id="ITerm132"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR174" role="doc-biblioref">174</a></span>] is an autoencoder comprising 1.5B parameters. It was able for the first time to generate consistent stories that continue a start text. According to the users, the stories were coherent in half of the cases. Much better is the performance of <strong class="EmphasisTypeBold ">GPT-3</strong><span id="ITerm133"/> with 175B parameters [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>]. Given an initial text it is able to create short stories, songs, press releases, technical manuals, poems, translations, guitar tabs, computer code, etc. Only with an accuracy close to chance (52%) humans were able to distinguish whether news articles of about 200 words were synthetic [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>, p. 26]. A discussion of relative strengths and weaknesses of these Foundation Models can be found in Chap. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml"><span class="RefSource">4</span></a></span>.</p><p class="Para" id="Par181">An evaluation benchmark measuring the degree to which a language model “understands” a story is the <em class="EmphasisTypeItalic ">LAMBADA benchmark</em><span id="ITerm134"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR165" role="doc-biblioref">165</a></span>] (Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec4"><span class="RefSource">4.​1.​3</span></a></span>). It consists of about 10,000 passages from the BooksCorpus containing unpublished novels. The task is to predict the missing last word of the last sentence of each passage. Examples were filtered by humans to ensure that models need to take into account the full passage of at least 50 tokens to induce the final word. The GPT-3<sub>175B</sub> autoregressive language model [<span class="CitationRef"><a epub:type="biblioref" href="#CR173" role="doc-biblioref">173</a></span>] predicted the last word with 76.2% [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>, p. 12]. PaLM with few-shot instructions could increase the accuracy to 89.7 [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>, p. 79]. This means that in nearly nine of ten cases the predicted word was exactly correct, which indicates that the model well “understood” the preceding passage. For advanced Foundation Models like Gopher (280B) and PaLM (540B) text generation is a background ability taken for granted, which is no longer tested with benchmarks. A large battery of benchmarks is applied to test other features, e.g. common sense knowledge, reasoning, etc. (Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec5"><span class="RefSource">4.​1.​4</span></a></span>).</p><p class="Para" id="Par182"><strong class="EmphasisTypeBold ">InstructGPT</strong><span id="ITerm135"/> is a recent variant of GPT-3 (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec43"><span class="RefSource">3.​6.​5</span></a></span>), which can easily be instructed to generate a story, e.g. by the prompt <em class="EmphasisTypeItalic ">“Write a short story where a bear goes to the beach, makes friends with a seal, and then returns home.”</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR162" role="doc-biblioref">162</a></span>, p. 6]. <strong class="EmphasisTypeBold ">Retro</strong><span id="ITerm136"/> is an autoregressive LM combined with a retrieval mechanism (Sect. <span class="InternalRef"><a href="#Sec15">6.2.3</a></span>). In this way, current and focused information can be collected during the generation of a story, instead of relying on the information contained in the model parameters, which were obtained from the training data. <strong class="EmphasisTypeBold ">LaMDA</strong><span id="ITerm137"/> (137B) is a recent Language Model (Sect. <span class="InternalRef"><a href="#Sec52">6.6.3</a></span>) specialized for dialogs. It also features a retriever-reader architecture to augment its internal knowledge acquired during pre-training with external information.</p><p class="Para" id="Par183"><strong class="EmphasisTypeBold ">GRF</strong><span id="ITerm138"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR86" role="doc-biblioref">86</a></span>] is a Foundation Model including multi-hop reasoning in a knowledge base to improve language generation. This enhances PLMs, which otherwise take into account common sense knowledge only if it is explicitly stated in the training data. The reasoning module operates on the sub-graph extended from the concepts in the input text and draws possible conclusions. These are taken into account for the further generation of text. Results, e.g. on task to finish a story, show that the model outperforms strong alternatives. Other approaches to enhance language models by additional knowledge are discussed in Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec17"><span class="RefSource">3.​4</span></a></span>. A survey of conditional text generation is given by Guo et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR72" role="doc-biblioref">72</a></span>].</p></section>
<section class="Section2 RenderAsSection2" id="Sec33"><h3 class="Heading"><span class="HeadingNumber">6.5.2 </span>Generating Text with a Given Style</h3><div class="Para" id="Par184">Often the goal is to create a text in a specific style or emphasizing a specific type of content: e.g. author’s style (e.g. Shakespeare), emotion (e.g. angry, malicious, happy), genre (e.g. humor, romance), topics (politics, religion), persona (e.g. lawyer, knight), or sentiment (e.g. positive, negative, fury). By design there are a number of ways how to influence the story produced by a Foundation Model. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par185">Pre-training a Foundation Model with corresponding texts.</p></li><li><p class="Para" id="Par186">Adaption of the Foundation Model to a new genre/style/content by fine-tuning.</p></li><li><p class="Para" id="Par187">Specification of an initial text.</p></li><li><p class="Para" id="Par188">Few-shot instruction, e.g. for GPT-3, or simple instructions for InstructGPT.</p></li></ul></div></div><p class="Para" id="Par189">There are different ways to achieve this with Foundation Models. A comprehensive survey is given by Lili and Vechtomova [<span class="CitationRef"><a epub:type="biblioref" href="#CR122" role="doc-biblioref">122</a></span>].</p><section class="Section3 RenderAsSection3" id="Sec34"><h4 class="Heading">Style-Conditional Probabilities</h4><p class="Para" id="Par190"><strong class="EmphasisTypeBold ">CTRL</strong><span id="ITerm139"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR96" role="doc-biblioref">96</a></span>] aims to train a generative model <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">y</em>|<em class="EmphasisTypeItalic ">x</em>;<em class="EmphasisTypeItalic ">a</em>) conditioned on a control variable <em class="EmphasisTypeItalic ">a</em>. To do this, the conditional distribution <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">x</em>|<em class="EmphasisTypeItalic ">a</em>) is adapted by training on raw text sequences with context classes prefixes such as <em class="EmphasisTypeItalic ">[horror]</em>, <em class="EmphasisTypeItalic ">[legal]</em>, etc. The authors used text collections, which are labeled with the corresponding context classes. Then the learned transformer model with 1.6B parameters is able to generate text with respect to the control prefix. This is developed further by <strong class="EmphasisTypeBold ">GeDI</strong><span id="ITerm140"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR105" role="doc-biblioref">105</a></span>], which has a stronger controllability, generates less toxic text, and can be extended to continuously weighted control codes for generating fluent stories [<span class="CitationRef"><a epub:type="biblioref" href="#CR127" role="doc-biblioref">127</a></span>].</p><p class="Para" id="Par191"><strong class="EmphasisTypeBold ">PPLM</strong><span id="ITerm141"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>] (Plug and Play Language Model) defines a model <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">x</em>|<em class="EmphasisTypeItalic ">a</em>), where <em class="EmphasisTypeItalic ">a</em> is some desired controllable attribute(s) and <em class="EmphasisTypeItalic ">x</em> the generated sample. If <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">x</em>) is the pre-trained LM, the authors define the conditional distribution <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">a</em>|<em class="EmphasisTypeItalic ">x</em>). This yields a conditional generative model <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">x</em>|<em class="EmphasisTypeItalic ">a</em>) ∝ <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">a</em>|<em class="EmphasisTypeItalic ">x</em>)<em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">x</em>). The distribution <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">a</em>|<em class="EmphasisTypeItalic ">x</em>) may be implemented by a single layer classifiers. The model samples from the resulting combined model by following gradients in the latent representation space (key-value-pairs of the transformer) such that <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">x</em>) as well as <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">a</em>|<em class="EmphasisTypeItalic ">x</em>) is improved. After a number of 3–10 updates the perturbed values are used to generate a new token at the next position. The model was able to create text with the desired tonality (e.g. positive/negative) while preserving fluency. However, balancing the impact of the PLM and the conditions is delicate and must be supported with additional measures like reranking, and early-stopping procedures.</p><p class="Para" id="Par192"><strong class="EmphasisTypeBold ">ETC-NLG</strong><span id="ITerm142"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>] leverages context-sensitive topic models [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>] to enhance PPLM with an unlabeled collection of documents. This is desirable as PPLM still requires large amounts of labeled texts to effectively balance generation fluency and proper conditioning. The attribute model discriminator, predicting document topics, and the unconditional language model PPLM are merged to obtain a conditional language model for topic-conditioned utterances.</p><p class="Para" id="Par193"><strong class="EmphasisTypeBold ">GDC</strong><span id="ITerm143"/> (Generation with Distributional Control) [<span class="CitationRef"><a epub:type="biblioref" href="#CR97" role="doc-biblioref">97</a></span>] propose an approach to emphasize specific words in addition to changing the distribution of generated words. For example, GDC can avoid toxic content, prevent bias, and align the generation with a particular theme or style. Instead of reweighting the generative distribution of tokens, the authors derive a stochastic policy by reinforcement learning [<span class="CitationRef"><a epub:type="biblioref" href="#CR166" role="doc-biblioref">166</a></span>] to get a good compromise between the constraints and the language model. The authors can reweight single words (e.g. <em class="EmphasisTypeItalic ">China</em>), all words in a word list (e.g. lists for <em class="EmphasisTypeItalic ">kitchen</em>, <em class="EmphasisTypeItalic ">fantasy</em>), and words emphasized by a classifier (e.g. for <em class="EmphasisTypeItalic ">very negative</em> or <em class="EmphasisTypeItalic ">clickbait</em>). The results show that the constraints are met with the lowest divergence from the original PLM and with the best diversity scores.</p><p class="Para" id="Par194"><strong class="EmphasisTypeBold ">Adapter-Bot</strong><span id="ITerm144"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR126" role="doc-biblioref">126</a></span>] provides different adapters trained independently for different skills. The backbone of the Adapter-Bot is a pre-trained GPT language model [<span class="CitationRef"><a epub:type="biblioref" href="#CR262" role="doc-biblioref">262</a></span>], providing the ability of text generation. A set of trainable adapters are added to the backbone, which are optimized over the target dataset of dialogues for specific dialogue skills. Using a trained classifier to select the right dialogue skill under the dialogue story, Adapter-Bot allows high-level control over the chatbot.</p></section>
<section class="Section3 RenderAsSection3" id="Sec35"><h4 class="Heading">Prompt-Based Generation</h4><p class="Para" id="Par195">GPT-3 is able to produce text, when it receives an appropriate prompt (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec41"><span class="RefSource">3.​6.​3</span></a></span>). It can, for instance, generate a poem [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>]. After the prompt <em class="EmphasisTypeItalic ">“write a poem in the style of Rabbie Burns”</em> it may produce something like “There once was a lady from Dundee</p><p class="Para" id="Par196">a’ wha was bonnie, braw, and meek</p><p class="Para" id="Par197">She met an old man from Dunfermline</p><p class="Para" id="Par198">who won’t let her to her sleep …”</p><p class="Para" id="Par199">With the prompt <em class="EmphasisTypeItalic ">“write this like an attorney”</em> it can create a text in the wording of a lawyer. Moreover, it can automatically write emails in your personal style by getting a prompt with some key points. GPT-3 can even work with unusual language types. It can, for instance, translate natural language into shell commands or programming code [<span class="CitationRef"><a epub:type="biblioref" href="#CR163" role="doc-biblioref">163</a></span>]. More prompts for GPT-3 and other Foundation Models are provided by OpenAI [<span class="CitationRef"><a epub:type="biblioref" href="#CR160" role="doc-biblioref">160</a></span>]. InstructGPT was fine-tuned to generate text according to an instruction (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec43"><span class="RefSource">3.​6.​5</span></a></span>). It can, for instance, receive the directives <em class="EmphasisTypeItalic ">“Complete the following sentence in a polite, respectful, and unbiased manner:”</em> or as <em class="EmphasisTypeItalic ">“Complete the following sentence using maximally biased and offensive language:”</em>. Then the model produces diverse texts that satisfy the requirements [<span class="CitationRef"><a epub:type="biblioref" href="#CR162" role="doc-biblioref">162</a></span>].</p></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec36"><h3 class="Heading"><span class="HeadingNumber">6.5.3 </span>Transferring a Document to Another Text Style</h3><p class="Para" id="Par200">Text style transfer aims to translate a text <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><em class="EmphasisTypeItalic ">′</em> with attribute <em class="EmphasisTypeItalic ">a′</em> to a similar text <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> of a desired attribute <em class="EmphasisTypeItalic ">a</em>. For example, the sentence <em class="EmphasisTypeItalic ">x′</em> = <em class="EmphasisTypeItalic ">“Peter screwed up”</em> with the attribute <em class="EmphasisTypeItalic ">a′</em> = <em class="EmphasisTypeItalic ">“informal”</em> can be transformed to <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> = <em class="EmphasisTypeItalic ">“Peter has not reached the goal”</em> with the attribute <em class="EmphasisTypeItalic ">a</em> = <em class="EmphasisTypeItalic ">“formal”</em>. The aim is to train a language model <em class="EmphasisTypeItalic ">p</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>|<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><em class="EmphasisTypeItalic ">′</em>, <em class="EmphasisTypeItalic ">a</em>). There are a number of other transformations, such as impolite ↔ polite, complicated ↔ simple, positive ↔ negative, biased ↔ neutral, or factual ↔ humorous ↔ romantic.</p><p class="Para" id="Par201">The separation of style from content is difficult. On the one hand it can be captured by linguistic features, e.g. the utilization of specific words and phrases. On the other hand, it can be provided by text collections, e.g. with the writings of different authors or with a corpus of positive/negative reviews. In the latter case we can train classifiers, which discriminate between the different styles. With the recent progress in the capabilities of language models there are a number of successful applications of style transfer like imitating the style of specific authors, removing bias in online text, etc. A recent comprehensive survey is provided by Jin et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR88" role="doc-biblioref">88</a></span>].</p><section class="Section3 RenderAsSection3" id="Sec37"><h4 class="Heading">Style Transfer with Parallel Data</h4><p class="Para" id="Par202">If there are parallel documents of both styles, the style transfer can be formulated as a translation problem. An encoder-decoder transformer has to be fine-tuned on this dataset.</p><p class="Para" id="Par203"><strong class="EmphasisTypeBold ">Formal</strong><span id="ITerm145"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR260" role="doc-biblioref">260</a></span>] formulate style transfer from informal to formal as a translation task. They use a transformer as Seq2seq model and apply it to the <em class="EmphasisTypeItalic ">GYAFC</em><span id="ITerm146"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR180" role="doc-biblioref">180</a></span>] benchmark dataset containing parallel formal/informal sentences. In addition, they augment the data by back-translation, employ machine translation to and from another language and leverage training data from grammatical error correction. They report a new <span class="EmphasisTypeSmallCaps ">Sota</span> on the GYAFC dataset with increased formality and fluency, while keeping the meaning of a text.</p></section>
<section class="Section3 RenderAsSection3" id="Sec38"><h4 class="Heading">Style Transfer without Parallel Data</h4><p class="Para" id="Par204"><strong class="EmphasisTypeBold ">StyleLM</strong><span id="ITerm147"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR217" role="doc-biblioref">217</a></span>] translates an arbitrary text into a text with the style properties of another author while keeping the content, even if no parallel data of the same content in different styles is available. First a BERT model is trained on a large neutral corpus (Gutenberg and Wikipedia) with the MLM loss. Then two copies of the model are used as an encoder-decoder transformer <span class="InlineEquation" id="IEq20"><img alt="$$\tilde {{\boldsymbol {x}}}=\text{DEC}_{\boldsymbol {w}}(\text{ENC}_{\boldsymbol {u}}({\boldsymbol {x}}))$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq20.png" style="width:10em"/></span>. As fine-tuning input this Seq2seq model receives texts from the target author, where a random fraction of the words have been masked and have to be reconstructed. Hence, the Seq2seq model induces text with the target author’s style while rewriting the input text.</p><p class="Para" id="Par205">For evaluation 10 different authors were selected and excluded from the training data. The <span class="EmphasisTypeSmallCaps ">Bleu</span> score and <span class="EmphasisTypeSmallCaps ">Rouge</span> scores are used to measure content preservation. To measure the style quantitatively, the frequency of author-specific words and of syntactic and punctuation elements are evaluated. StyleLM in most cases had the best content preservation and stylistic alignment. Singh et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR207" role="doc-biblioref">207</a></span>] note that StyleLM has problems with content reproduction. They propose to pre-train the encoder-decoder <span class="EmphasisTypeSmallCaps ">Dec</span><sub><em><strong class="EmphasisTypeBoldItalic ">w</strong></em></sub>(<span class="EmphasisTypeSmallCaps ">Enc</span><sub><em><strong class="EmphasisTypeBoldItalic ">u</strong></em></sub>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>)) on a large generic corpus. Afterwards the encoder-decoder is fine-tuned on the text of the target author.</p><p class="Para" id="Par206"><strong class="EmphasisTypeBold ">OPTIMUS</strong><span id="ITerm148"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR115" role="doc-biblioref">115</a></span>] investigates further manipulations of sentences embeddings. An encoder with parameter <em><strong class="EmphasisTypeBoldItalic ">u</strong></em> is required to generate a latent vector from text <em><strong class="EmphasisTypeBoldItalic ">z</strong></em> = <span class="EmphasisTypeSmallCaps ">Enc</span><sub><em><strong class="EmphasisTypeBoldItalic ">u</strong></em></sub>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>). It is initialized with a pre-trained BERT model. A linearly transformed version <em><strong class="EmphasisTypeBoldItalic ">z</strong></em> = <em class="EmphasisTypeItalic ">W</em> ∗<em><strong class="EmphasisTypeBoldItalic ">h</strong></em><sub>[<em class="EmphasisTypeItalic ">CLS</em>]</sub> of the embedding of the first token <em class="EmphasisTypeItalic ">[CLS]</em> of a sentence is defined as latent representation. The generator (decoder) with parameter <em><strong class="EmphasisTypeBoldItalic ">w</strong></em> generates the text sequence <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> = <span class="EmphasisTypeSmallCaps ">Dec</span><sub><em><strong class="EmphasisTypeBoldItalic ">w</strong></em></sub>(<em><strong class="EmphasisTypeBoldItalic ">z</strong></em>) from a random vector <em><strong class="EmphasisTypeBoldItalic ">z</strong></em> (e.g. multivariate Gaussian) with prior <em class="EmphasisTypeItalic ">p</em>(<em><strong class="EmphasisTypeBoldItalic ">z</strong></em>). The authors start with a pre-trained GPT-2 model as decoder. <em><strong class="EmphasisTypeBoldItalic ">z</strong></em> is used by the decoder as an additional vector to attend to (in addition to the previously generated token embeddings). Both networks <span class="InlineEquation" id="IEq21"><img alt="$$\tilde {{\boldsymbol {x}}}=\text{DEC}_{\boldsymbol {w}}(\text{ENC}_{\boldsymbol {u}}({\boldsymbol {x}}))$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq21.png" style="width:10em"/></span> are trained with the autoencoder loss and the variational autoencoder loss, i.e. the system has to minimize <span class="InlineEquation" id="IEq22"><img alt="$$|\tilde {{\boldsymbol {x}}}-{\boldsymbol {x}}|$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq22.png" style="width:3.69em"/></span> and encourage a Gaussian distribution for <em><strong class="EmphasisTypeBoldItalic ">z</strong></em>.</p><p class="Para" id="Par207">The approach learns bidirectional mappings between latent embeddings <em><strong class="EmphasisTypeBoldItalic ">z</strong></em> and sentences <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>. For two sentences <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub>1</sub> and <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub>2</sub> the embeddings may be calculated and by <em class="EmphasisTypeItalic ">α</em><em><strong class="EmphasisTypeBoldItalic ">z</strong></em><sub>1</sub> + (1 − <em class="EmphasisTypeItalic ">α</em>)<em><strong class="EmphasisTypeBoldItalic ">z</strong></em><sub>2</sub> we can continuously interpolate between the sentences. In addition, differences between latent vectors may be computed similar to Word2Vec. For dialog response generation and the generation of responses with a specific style OPTIMUS has a better performance on all metrics compared to its competitors. Using an additional GAN to manipulate the latent representation <em><strong class="EmphasisTypeBoldItalic ">z</strong></em>, OPTIMUS is able to generate YELP restaurant reviews of prescribed sentiment (positive/negative) better than the investigated alternatives. The authors argue that compared to BERT, OPTIMUS learns a more structured semantic space due to the use of the VAE prior distribution in training.</p></section>
<section class="Section3 RenderAsSection3" id="Sec39"><h4 class="Heading">Style Transfer with Few-Shot Prompts</h4><div class="Para" id="Par208">Sufficiently large Foundation Models such as <strong class="EmphasisTypeBold ">GPT-3</strong><span id="ITerm149"/>, Gopher, and PaLM can perform various tasks simply by choosing a clever prompt. If, however, only a simple prompt is entered, e.g. <em class="EmphasisTypeItalic ">“Here is some text: {That is an ugly dress}. Here is a rewrite of the text, which is more positive: {”</em> the model often fails and may not produce well-formatted or consistent outputs. The <strong class="EmphasisTypeBold ">AugZero</strong><span id="ITerm150"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR182" role="doc-biblioref">182</a></span>] prompting schema employs augmented zero-shot prompts, which provide several demonstrations of sentences being rewritten to a new style. An example is shown in Fig. <span class="InternalRef"><a href="#Fig15">6.15</a></span>. In contrast to few-shot examples, where the examples have to cover the exact task, the model can also generalize to other unseen types of styles, e.g. <em class="EmphasisTypeItalic ">“comic”</em> in the example.<figure class="Figure" id="Fig15"><div class="MediaObject" id="MO15"><img alt="" aria-describedby="d64e8192" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig15_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e8192"><p class="Para" id="Par332">A text box represents the description of a prompt to rewrite a sentence in a descriptive, melodramatic, and comic way. below is the generated answer for the prompt.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.15</span><p class="SimplePara">Augmented zero-shot prompts can instruct large autoregressive LMs like GPT-3 to transfer a text to a new style. This even works, if there is no example given for the specific style desired, e.g. “comic” in the example [<span class="CitationRef"><a epub:type="biblioref" href="#CR182" role="doc-biblioref">182</a></span>, p. 2]</p></div></figcaption></figure></div><p class="Para" id="Par209">The authors use GPT-3 with 175B parameters. Professional human raters were asked to assess text style, content preservation, and fluency. The zero-shot alternative performed worst and did not return a valid response in a quarter of the cases. It turned out that the AugZero rated comparably to human-written ground truth. Obviously, the language model can extrapolate the examples and transform a text in unseen styles. Adding the target attribute to the augmented prompts had a very similar performance. It can be expected that larger models like PaLM and LaMDA can generate even better results (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec43"><span class="RefSource">3.​6.​5</span></a></span>).</p><p class="Para" id="Par210">Buchanan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>] noted that they could not instruct <strong class="EmphasisTypeBold ">GPT-3</strong><span id="ITerm151"/> by a single prompt to express a given story in a new tone or slant, supporting the above finding. Therefore, they developed a two-step procedure: First, GPT-3 was instructed by a few-shot prompt to summarize the given story into a list of bullet points. In a second step GPT-3 was instructed by prompts such as <em class="EmphasisTypeItalic ">“Write a strongly pro-Trump article about [Topic X] that makes use of the following list of facts about [Topic X]”</em>. When examining 20 generated stories by human evaluators, 11 of them were identified by at least one person as being “definitely authentic.” The authors used GPT-3 to solve further tasks, e.g. creating new narratives that could form the basis of conspiracy theories (e.g. QAnon), convincing members of particular groups to believe a claim, or persuade persons to change their opinion on some topic. They come to the conclusion that systems like GPT-3 are well-suited for generating a story with a new slant, e.g. for disinformation. This is even more alarming for more efficient recent Foundation Models like LaMDA or PaLM.</p></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec40"><h3 class="Heading"><span class="HeadingNumber">6.5.4 </span>Story Generation with a Given Plot</h3><p class="Para" id="Par211">A narrative, story or tale is a description of a series of related events or experiences [<span class="CitationRef"><a epub:type="biblioref" href="#CR234" role="doc-biblioref">234</a></span>]. As the story generated by a PLM gets longer, often the earlier context is forgotten, and the text develops in an aimless fashion. Therefore, researchers would like to prepare a rough plot or storyline for the story, which is then taken into account by the Foundation Model. More specifically the story structure, the story ending, the general topic, or the persona of leading characters can be controlled. Besides story generation another application is data-to-text generation, where non-linguistic structured data (e.g., a table or a graph) is converted to natural language text, which can be applied in tasks like healthcare, weather forecast, legal text, etc. Surveys of controlled text generation are provided by Prabhumoye et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR170" role="doc-biblioref">170</a></span>], Yu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR251" role="doc-biblioref">251</a></span>], and Zhang et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR257" role="doc-biblioref">257</a></span>].</p><div class="Para" id="Par212">The planned course of a story can be described in different ways: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par213">A list of single keywords or phrases.</p></li><li><p class="Para" id="Par214">A list of sentences or bullet points describing an event.</p></li><li><p class="Para" id="Par215">An event graph describing the logical dependency of events.</p></li></ul></div></div><section class="Section3 RenderAsSection3" id="Sec41"><h4 class="Heading">Specify a Storyline by Keywords or Phrases</h4><p class="Para" id="Par216"><strong class="EmphasisTypeBold ">Megatron-CNTRL</strong><span id="ITerm152"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR243" role="doc-biblioref">243</a></span>] controls the story generation by keywords. In addition, retrieved knowledge allows dynamical incorporation of external knowledge from the <em class="EmphasisTypeItalic ">ConceptNet KB</em><span id="ITerm153"/> into language model during generation. From the current story context a keyword predictor first predicts a set of keywords for the next sentence. The retriever collects knowledge from the KB corresponding to the keywords. The returned sentences are re-ranked according to their relevance to the story context. Finally, the generator takes the story context and the top-ranked retrieved sentences and produces the next sentence. To support generalization of entities they replace names and entities in stories with special placeholders, [MALE], [FEMALE], and [NEUTRAL] for male, female and unknown names and entities, respectively. The underlying Megatron model (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec3"><span class="RefSource">3.​1.​2</span></a></span>) has up to 8B parameters. Experiments show that the model generates more fluent, consistent, and coherent stories with lower repetition rate and higher diversities compared to the previous <span class="EmphasisTypeSmallCaps ">Sota</span></p><p class="Para" id="Par217">Dong et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>] present a model, which takes as input a list of keywords with attached entity classes and generates a text containing these keywords. The entities are taken into account during text generation and the model embeds the meaning of entities into hidden states. The results show that the generated sentences are able to reflect the properties of the entities.</p><div class="Para" id="Par218"><strong class="EmphasisTypeBold ">PlotMachines</strong><span id="ITerm154"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR181" role="doc-biblioref">181</a></span>] generates a text based on a plot consisting of a set of phrases. The system can decide for itself in what order to introduce the concepts covered by the phrases. It is based on the GPT and GPT-2 language model. The authors use three different datasets describing TV-shows, movies, books, short stories, and news articles. They extract phrases (3–8 words) from these stories by a keyword extraction method [<span class="CitationRef"><a epub:type="biblioref" href="#CR167" role="doc-biblioref">167</a></span>]. Given an outline as input, the model recurrently generates paragraphs (Fig. <span class="InternalRef"><a href="#Fig16">6.16</a></span>). To create the next paragraph it uses a gating mechanism similar to an LSTM gate, which updates a memory matrix <em class="EmphasisTypeItalic ">M</em> that keeps track of plot elements of the outline. The self-attention in the model is adapted to receive input from the memory matrix as well as the previously generated words. According to automatic metrics (<span class="EmphasisTypeSmallCaps ">Rouge</span>, <span class="EmphasisTypeSmallCaps ">Bleu</span>) the model has a better ability to generate realistic looking as well as diverse texts than its competitors. In extensive experiments with human raters the authors demonstrate that their model produces text closer to the plot than alternative models.<figure class="Figure" id="Fig16"><div class="MediaObject" id="MO16"><img alt="" aria-describedby="d64e8314" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig16_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e8314"><p class="Para" id="Par333">An illustration indicates the story outline at the top. It exhibits the plot dynamics and the generated story according to the conditioned outline at the bottom.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.16</span><p class="SimplePara">An outline (input) together with a story (output) from the Wikiplots training set generated by PlotMachines. Plot elements from the outline can appear and reappear nonlinearly throughout the plot, as shown in plot dynamics graph. A memory matrix keeps track of how outline phrases have been used while writing. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR181" role="doc-biblioref">181</a></span>, p. 1]</p></div></figcaption></figure></div><p class="Para" id="Par219"><strong class="EmphasisTypeBold ">Pointer</strong><span id="ITerm155"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR261" role="doc-biblioref">261</a></span>] inserts new words between the words of a given start set. Based on the start set, the model first generates high-level words (e.g. verbs and adjectives) that provide a high-level connection. Then it inserts other words of finer granularity around the keywords iteratively until the whole sentence is generated. The training objective of POINTER is to generate a complete text sequence with a set of keywords as constraints. This is similar to the masked language modeling (MLM) objective in BERT, so a pre-trained BERT is used to initialize the model training. An insertion transformer [<span class="CitationRef"><a epub:type="biblioref" href="#CR210" role="doc-biblioref">210</a></span>] is used to generate either a regular token or a special token for each gap between two existing tokens. Empirical evaluations demonstrate the effectiveness of the approach. Similar models are <em class="EmphasisTypeItalic ">ProGeT</em><span id="ITerm156"/> proposed by Tan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR220" role="doc-biblioref">220</a></span>] and the constrained BART [<span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>].</p><p class="Para" id="Par220"><strong class="EmphasisTypeBold ">ProGen</strong><span id="ITerm157"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR219" role="doc-biblioref">219</a></span>] generates a story in <em class="EmphasisTypeItalic ">k</em> different levels. For each level a vocabulary <span class="InlineEquation" id="IEq23"><img alt="$$\mathcal {V}_i$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq23.png" style="width:1.12em"/></span> is defined based on tf-idf score, such that <span class="InlineEquation" id="IEq24"><img alt="$$\mathcal {V}_1$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq24.png" style="width:1.12em"/></span> contains high information words while <span class="InlineEquation" id="IEq25"><img alt="$$\mathcal {V}_k$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq25.png" style="width:1.24em"/></span> contains all words. <em class="EmphasisTypeItalic ">k</em> different encoder-decoder models (BART) <em class="EmphasisTypeItalic ">M</em><sub><em class="EmphasisTypeItalic ">i</em></sub> are trained for the <em class="EmphasisTypeItalic ">k</em> levels, where the <em class="EmphasisTypeItalic ">i</em>- level employs the training data <em class="EmphasisTypeItalic ">X</em><sub><em class="EmphasisTypeItalic ">i</em></sub> containing only words from vocabulary <span class="InlineEquation" id="IEq26"><img alt="$$\mathcal {V}_i$$" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq26.png" style="width:1.12em"/></span>. As input <em class="EmphasisTypeItalic ">M</em><sub><em class="EmphasisTypeItalic ">i</em></sub> gets the training data <em class="EmphasisTypeItalic ">X</em><sub><em class="EmphasisTypeItalic ">i</em>−1</sub> from the previous level and has to predict the refined version <em class="EmphasisTypeItalic ">X</em><sub><em class="EmphasisTypeItalic ">i</em></sub>. Note that usually the input words from <em class="EmphasisTypeItalic ">X</em><sub><em class="EmphasisTypeItalic ">i</em>−1</sub> will be included in the next output. A storyline now can be formulated by a human using words from a high-level vocabulary, which covers about 15% of all content. If, for example, the first stage text is <em class="EmphasisTypeItalic ">“beckham ∖n liverpool bayern chelsea ∖n beckham chelsea mancini …”</em> the final stage text starts as <em class="EmphasisTypeItalic ">“England striker Ashley Beckham has joined Premier League strugglers Newcastle United. ∖n England Football …”</em>. Evaluation shows that the coherence of the texts over long intervals (36 sentences) is close to humans and much better than for a basic BART model. In addition, ProGen has favorable properties with respect to fluency, lexical and semantic quality, as well as diversity.</p></section>
<section class="Section3 RenderAsSection3" id="Sec42"><h4 class="Heading">Specify a Storyline by Sentences</h4><div class="Para" id="Par221"><strong class="EmphasisTypeBold ">Facts2Story</strong><span id="ITerm158"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR161" role="doc-biblioref">161</a></span>] receives as input a sequence of key facts expressed in natural language and generates a story containing the facts in the given order (Table <span class="InternalRef"><a href="#Tab12">6.12</a></span>). These facts are simple sentences that describe factual information of the story. Each fact should report an event in the story, state the properties of a person or a place, mention the emotions of characters, etc. There should be a large degree of freedom to generate a story containing the facts. <div class="Table" id="Tab12"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 6.12</span><p class="SimplePara">Story generated by Facts2story model with facts as input [<span class="CitationRef"><a epub:type="biblioref" href="#CR161" role="doc-biblioref">161</a></span>]. Words taken from the facts are printed in italics</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/></colgroup><tbody><tr><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">Fact 1:</strong><em class="EmphasisTypeItalic ">German army has pulled from sector of Western Front in northern France</em></p></td></tr><tr><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">Fact 2:</strong><em class="EmphasisTypeItalic ">Blake cross no mans land to reach the abandoned German trenches</em></p></td></tr><tr><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">Fact 3:</strong><em class="EmphasisTypeItalic ">German plane shot down in flames</em></p></td></tr><tr><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">Generated text:</strong></p></td></tr><tr><td style="text-align: left;"><p class="SimplePara">In July 1930, on the eve of World War I, a train carrying German prisoners belonging to the <em class="EmphasisTypeItalic ">German army, has pulled from sector of Western Front in northern France</em><sub>fact1</sub>. Captain Alfred Blake (Greg Rogers), a British officer in the German Army, has been sent to the German border. After being briefed by one of the German troops, <em class="EmphasisTypeItalic ">Blake cross no mans land to reach the abandoned German trenches</em><sub>fact2</sub>. He is captured, but finds the <em class="EmphasisTypeItalic ">German plane shot down in flames</em><sub>fact3</sub>. He takes refuge in a French camp, where he and another German, Captain Schofield (James Shea), are kept …</p></td></tr></tbody></table></div></div><p class="Para" id="Par222">To keep the problem manageable, the authors give an input of 5 ordered facts and aim to generate a coherent story of 100–1000 words covering all facts in order. As training data 17k story plots from Wikipedia were used. From each of these plots facts were extracted by the SalIE framework [<span class="CitationRef"><a epub:type="biblioref" href="#CR169" role="doc-biblioref">169</a></span>]. The five facts with the highest saliency scores were selected.</p><p class="Para" id="Par223">As standard language models (GPT-2, BART) after a number of generated tokens diverge from the input and focus on the newly generated content, the authors use a pre-trained XLNET (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>), which is able to take into account future words. The assumption is that the words of the facts should appear in the final text in the given order. XLNET is able to process these tokens in random order, because the position embeddings are attached to the token embeddings. As between two consecutive tokens of the facts other words may occur, a model is trained to predict the number of intervening words. This model is used to determine the exact position of each word of each fact. Finally, the XLNET has to fill in the missing words.</p><p class="Para" id="Par224">The generated stories are evaluated by humans according to three criteria: (1) adherence to facts, (2) grammatical correctness, (3) common sense and plausibility of events. Alternatives investigated were GPT-2 (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec16"><span class="RefSource">2.​2.​4</span></a></span>) with additional self-attention [<span class="CitationRef"><a epub:type="biblioref" href="#CR269" role="doc-biblioref">269</a></span>] and the Seq2seq model BART (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec4"><span class="RefSource">3.​1.​3</span></a></span>), which is pre-trained to recover randomly shuffled text and fine-tuned to generate the story using the facts as input. The evaluation shows that Facts2Story generates a story containing on average 4.4 of the 5 facts, while the other models recover less than 1.7 facts. With respect to grammar and common sense Facts2Story fares slightly worse than GPT2 but much better than BART.</p><p class="Para" id="Par225"><strong class="EmphasisTypeBold ">SOE</strong><span id="ITerm159"/> (Summarize, Outline and Elaborate) [<span class="CitationRef"><a epub:type="biblioref" href="#CR214" role="doc-biblioref">214</a></span>] starts from the observation that most approaches for story generation produce texts in a word-by-word manner and have no high-level plan on what to generate. To address this issue, the coarse-to-fine generation strategy with two levels is proposed. For each segment <em><strong class="EmphasisTypeBoldItalic ">y</strong></em><sup><em class="EmphasisTypeItalic ">i</em></sup> of the text a summary <em class="EmphasisTypeItalic ">s</em><sup><em class="EmphasisTypeItalic ">i</em></sup> is provided. The model first generates “bullet points” for each summary. Subsequently, the model expands each bullet point to generate the corresponding segment. Note that during this process the high-level discourse dependencies are preserved.</p><p class="Para" id="Par226">To prepare the training data, the stories in a collection are partitioned into segments of several hundred words using BERT next sentence prediction measuring the degree of dependency of sentences. For each segment an extractive summary is generated using BERT and TextRank [<span class="CitationRef"><a epub:type="biblioref" href="#CR144" role="doc-biblioref">144</a></span>]. Then a transformer is employed to create the bullet points dependent on previous bullet points. From these the final text is produced taking into account previous text and abstractions. WikiText 103 [<span class="CitationRef"><a epub:type="biblioref" href="#CR142" role="doc-biblioref">142</a></span>] and the BookCorpus [<span class="CitationRef"><a epub:type="biblioref" href="#CR267" role="doc-biblioref">267</a></span>] were used as training data.</p><p class="Para" id="Par227">The performance of the model was evaluated with respect to fluency by perplexity, with respect to text diversity by the number of distinct <em class="EmphasisTypeItalic ">n</em>-grams, text acceptability as measured by an adversarial classifier, and sentence level coherence measured by a next-sentence prediction score. On all scores the SOE-model with an additional reranking procedure achieved the best results. Comparison with Transformer-XL [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>] and Progressive WritingPrompts [<span class="CitationRef"><a epub:type="biblioref" href="#CR220" role="doc-biblioref">220</a></span>] demonstrated the superiority of SOE with respect to perplexity, diversity of the generated text, and coherence.</p><div class="Para" id="Par228"><strong class="EmphasisTypeBold ">FIST</strong><span id="ITerm160"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>] receives a sequence of “events” as inputs describing each paragraph (Fig. <span class="InternalRef"><a href="#Fig17">6.17</a></span>). To extract events from paragraphs for training, keyword extraction techniques [<span class="CitationRef"><a epub:type="biblioref" href="#CR144" role="doc-biblioref">144</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR191" role="doc-biblioref">191</a></span>] are used. By means of special tokens as delimiters these events are connected with paragraphs in an interleaving manner. The authors fine-tune a pre-trained GPT-2 with the LM-loss on the augmented sequences to learn the functionality of special tokens and co-occurrence structures between events and stories. The performance of FIST is compared with Plotmachines (see above) and two other approaches on two benchmark datasets. With respect to most evaluation measure FIST generally achieves better results. The <span class="EmphasisTypeSmallCaps ">Sota</span> in story generation is developing fast with new techniques appearing every month. We describe some limitations of current models in the context of dialogs in Sect. <span class="InternalRef"><a href="#Sec53">6.6.4</a></span> and discuss some remedies.<figure class="Figure" id="Fig17"><div class="MediaObject" id="MO17"><img alt="" aria-describedby="d64e8655" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig17_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e8655"><p class="Para" id="Par334">A text box represents a prompt and a description of the event at the top. It exhibits the generated paragraph at the bottom.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.17</span><p class="SimplePara">Story generated by the FIST model with prompt and event as input [<span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par229">Papalampidi et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR164" role="doc-biblioref">164</a></span>] note that in generated stories the appearing entities are often incoherent, i.e. persons are replaced and locations change. The <strong class="EmphasisTypeBold ">MNEMELM</strong><span id="ITerm161"/> model employs an additional entity memory, where the generated entities and their attributes are stored dynamically and retrieved during further story generation. The representation for an entity is the average embedding of the tokens of the entity. Each entity memory slot <em class="EmphasisTypeItalic ">m</em><sub><em class="EmphasisTypeItalic ">j</em></sub> thus contains a fixed surface entity representation (writing) <em class="EmphasisTypeItalic ">k</em><sub><em class="EmphasisTypeItalic ">j</em></sub> and a dynamic value <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">j</em></sub>, which is frequently updated based on each new chunk of the narrative context. The stored entities enter the self-attention computations and thus influence the story.</p><p class="Para" id="Par230">As background model a Transformer-XL (∼300M parameters) pre-trained on a translation task is used (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec9"><span class="RefSource">3.​2.​2</span></a></span>). On the WikiPlot and the WritingPrompts benchmarks it turn out that MNEMELM better imitates the frequency of entity usage of humans than other models and in addition have a higher entity coherence and consistency. This is also confirmed by human judgment. Recently, dynamic retrieval-based approaches were also used by dialog systems such as BlenderBot-2 (Sect. <span class="InternalRef"><a href="#Sec51">6.6.2</a></span>). By the combination of these approaches the generation of stories may be improved.</p><p class="Para" id="Par231">We have seen above (Sect. <span class="InternalRef"><a href="#Sec39">6.5.3</a></span>) that <strong class="EmphasisTypeBold ">GPT-3</strong><span id="ITerm162"/> can rewrite a story in a new slant, when prompts are used in a two-step procedure [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>]. First, GPT-3 was instructed to summarize the given story into a list of bullet points. In a second step GPT-3 was instructed by prompts to write a story with a given tone containing the facts noted in the bullet points. If only the second step is executed, GPT-3 can be instructed to write a story covering the bullet point and in addition obey the prescribed slant. Currently, we are not aware of a systematic evaluation of the effectiveness of this technique, which should be even more rewarding for larger Foundation Models.</p></section>
<section class="Section3 RenderAsSection3" id="Sec43"><h4 class="Heading">Other Control Strategies</h4><p class="Para" id="Par232"><strong class="EmphasisTypeBold ">GraphPlan</strong><span id="ITerm163"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>] aims to prevent logical inconsistencies in generated text, which often are produced by models like GPT-2. The input to the model is an event graph, which represents each event with a verb phrase. To prepare training data, the verb phrases of events are extracted from a story using semantic role labeling and characterized by <em class="EmphasisTypeItalic ">Latent Dirichlet Allocation</em><span id="ITerm164"/> topics [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>]. The events are connected by directed edges indicating possible next events. In addition, event pairs are identified that are mutually exclusive. To generate a story, first a sequence of events is selected based on a beam search (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec22"><span class="RefSource">2.​3.​2</span></a></span>). Subsequently, the text is generated by a version of GPT-2. With extensive experiments the authors found that GraphPlan generates stories, which are less repetitive and more consistent. Koncel-Kedziorski et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR104" role="doc-biblioref">104</a></span>] present a similar model to generate text from knowledge graphs with graph transformers. By using another method based on BART and T5, it is possible to generate fluent stories from graphs representing the story structure [<span class="CitationRef"><a epub:type="biblioref" href="#CR185" role="doc-biblioref">185</a></span>].</p><p class="Para" id="Par233">Sakaguchi et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR196" role="doc-biblioref">196</a></span>] present an approach based on the T5 transformer with 11B parameters that generates a directed acyclic graph of events describing a story. The order of events indicates their logical and temporal dependency. This graph may be taken as an input to another Foundation Model to generate a story containing the events of the script.</p><p class="Para" id="Par234"><strong class="EmphasisTypeBold ">CAST</strong><span id="ITerm165"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR168" role="doc-biblioref">168</a></span>] aims to improve the coherence of the generated story and the coherence of the action of persons. It tries to infer the causal relations between events, as well as the intents and motivations of characters in the story context, and use it to influence the generation of a coherent story. They employ a logical inference model to reason about the characters in the story and to influence the generated words. As basic model, they use GPT-2 and generate stories for two persons. Their experiments show that the produced stories are more coherent and stay on topic.</p></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec44"><h3 class="Heading"><span class="HeadingNumber">6.5.5 </span>Generating Fake News</h3><div class="Para" id="Par235">The creation of Fake News can be simply considered as the task to generate stories with a new slant. Buchanan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>] investigated how GPT-3 can be used to generate large numbers of different fake news messages that can be easily distributed to thousands of users. They mainly formulate appropriate prompts for GPT-3 (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec41"><span class="RefSource">3.​6.​3</span></a></span>) to produce the desired texts. This comprises variations of tweet-like short messages, medium-sized posts expressing a world view, and longer articles reporting an event from a particular perspective. Examples are shown in Fig. <span class="InternalRef"><a href="#Fig18">6.18</a></span>.<figure class="Figure" id="Fig18"><div class="MediaObject" id="MO18"><img alt="" aria-describedby="d64e8790" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig18_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e8790"><p class="Para" id="Par335">A table represents the description and examples of the narrative reiteration, elaboration, and manipulation.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.18</span><p class="SimplePara">Some of the fake news generation tasks performed with GPT-3 [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par236"><em class="EmphasisTypeItalic ">Narrative Reiteration</em> aims at creating a large number of short messages (e.g. tweets) that express a particular theme, such as climate change denial. The authors collected replies with many likes from a climate change denial account. Ten of these messages were used as input prompt to GPT-3, e.g.: <em class="EmphasisTypeItalic ">“TWEET 4: Soros/Gates Funded $6.5 million to group now warning world may need ‘climate lockdown”’</em>. GPT-3 continued with similar tweets such as <em class="EmphasisTypeItalic ">“TWEET 14: Climate change is the new communism - an ideology based on a false science that cannot be questioned.”</em> Obviously, GPT-3 produces very good results with little human assistance.</p><div class="Para" id="Par237"><em class="EmphasisTypeItalic ">Narrative Elaboration</em> intends to justify a claim with a medium-length story. The authors accomplished this in a two-step process. First, GPT-3 is instructed to generate a series of headlines that each made some new assertion regarding a certain topic. This was done by collecting five headlines from a far-right media company, e.g. <em class="EmphasisTypeItalic ">“HEADLINE 5: Chinese Official Praises Quality of Country’s Vaccines, Despite Multiple Health Scandals”</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>, p. 9]. GPT-3 then generated five new headlines, e.g. <em class="EmphasisTypeItalic ">“HEADLINE 6: Secret Chinese Vaccine Testing on Half a Million Children Confirmed”</em>. Subsequently, GPT-3 was given these generated headlines to create longer articles. A headline together with a created article is shown in Fig. <span class="InternalRef"><a href="#Fig19">6.19</a></span>. It turned out that GPT-3 was able to capture the appropriate tone and tendency of the fake new source, as demonstrated by a classifier. Note that GPT-3 now can be fine-tuned (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec40"><span class="RefSource">3.​6.​2</span></a></span>) and even better concentrate on the content and the reasoning of specific news sources.<figure class="Figure" id="Fig19"><div class="MediaObject" id="MO19"><img alt="" aria-describedby="d64e8837" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig19_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e8837"><p class="Para" id="Par336">A textbox represents a prompt related to a report on the Chinese regime along with the generated response by the G P T 3.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.19</span><p class="SimplePara">A sample headline from The Epoch Times and the beginning of the article generated by GPT-3 [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>, p. 11]</p></div></figcaption></figure></div><p class="Para" id="Par238"><em class="EmphasisTypeItalic ">Narrative Reframing</em> is necessary if there exist new arguments in an article against a worldview. Then a new chain of arguments has to be generated that allows to uphold the worldview. The authors found a two-step approach for this task. First GPT-3 has to summarize the original article in a list of bullet points. Then GPT-3 is asked to generate a new article from a particular viewpoint, e.g.: <em class="EmphasisTypeItalic ">“write a strongly pro-Trump article about [Topic X] that makes use of the following list of facts about [Topic X]”</em>. The researchers took advantage of the fact that GPT-3 not only interprets the prompt provided by the human, as an example, but also learns something about the specific boundary conditions of the task from this example. An evaluation by human raters showed that 8 of 20 GPT-3 stories were judged as likely authentic by three of nine evaluators. The results suggest that GPT-3 can meaningfully shift the slant of a news story.</p><p class="Para" id="Par239">In addition, the authors evaluated GPT-3 for other tasks. GTP-3 was able to develop <em class="EmphasisTypeItalic ">new conspiracy theories</em> in the style of QAnon. It was not tested, if these theories could convince followers. Often the target is to <em class="EmphasisTypeItalic ">strengthen an attitude</em> or induce a specific behavior (e.g. voting) of members of particular social characteristics (e.g. race, religion). A human team with GPT-3 support is able to create credible targeted messages in just minutes. GPT-3 uses stereotypes and racist language in its texts, a tendency that is particularly worrying. Finally, a human-machine team is able to develop messages on two international issues—withdrawal from Afghanistan and sanctions against China—that cause survey respondents to <em class="EmphasisTypeItalic ">change their positions</em>. After seeing five short messages written by GPT-3 and selected by humans, the number of survey respondents who oppose sanctions against China has doubled.</p><p class="Para" id="Par240">The study shows that there is a real chance that automated tools will generate content for disinformation campaigns. It recommends focusing on the infrastructure used to disseminate campaign messages, such as fake accounts on social media, rather than determining the authorship of the text itself, as it is difficult to detect content fabricated by GPT-3. This is even more urgent because GPT-3 can now be fine-tuned to perform specific tasks (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec40"><span class="RefSource">3.​6.​2</span></a></span>) and the InstructGPT version can be easily instructed to execute specific assignments (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec43"><span class="RefSource">3.​6.​5</span></a></span>).</p><section class="Section3 RenderAsSection3" id="Sec45"><h4 class="Heading">Detecting Fake News</h4><p class="Para" id="Par241"><em class="EmphasisTypeItalic ">Fake news</em><span id="ITerm166"/> is false or misleading information presented as news in the media and on the Internet, especially in social media. Fake news is a global phenomenon. According to Khan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR98" role="doc-biblioref">98</a></span>], nearly 50% of the traffic on Facebook is fake or hyperpartisan. Since fake news aims to imitate real news, detecting fake news is generally not possible by analyzing the text alone. Monti et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR148" role="doc-biblioref">148</a></span>] showed that content, social context or news propagation in isolation is insufficient for neural models to detect fake news. Fake news detection is difficult because it is a gaming situation, in which fake news producers react to new detection methods.</p><p class="Para" id="Par242">There are a large number of benchmark datasets [<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>], which, however, are somewhat outdated. It is possible to achieve a high accuracy on these datasets, e.g. 94.1% on the Fake News Challenge FNC-1 [<span class="CitationRef"><a epub:type="biblioref" href="#CR201" role="doc-biblioref">201</a></span>] or 98.5% on Covid-19 fake news detection [<span class="CitationRef"><a epub:type="biblioref" href="#CR117" role="doc-biblioref">117</a></span>]. Ansar et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>] provide a survey on the characterization of fake news and methods for detecting it. They divide the detection of fake news into the analysis of the news content, the analysis of the source and its reliability and the analysis of the social reaction to an article. Other surveys on fake news detection are available [<span class="CitationRef"><a epub:type="biblioref" href="#CR85" role="doc-biblioref">85</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR98" role="doc-biblioref">98</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR172" role="doc-biblioref">172</a></span>]. An overview over multimodal disinformation detection, e.g. with text and images, is given by Alam et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>].</p><p class="Para" id="Par243">Gupta et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR74" role="doc-biblioref">74</a></span>] propose a knowledge-oriented framework that supports news verification by using trusted sources as context. They extract key information such as frequent words and entities from news articles and use them to query trusted sources for related articles. They calculate a similarity score between news article and the retrieved articles based on distributed embeddings and the Word Movers Distance [<span class="CitationRef"><a epub:type="biblioref" href="#CR108" role="doc-biblioref">108</a></span>]. Then they compare the similarity score to a preset threshold, to determine whether articles are semantically similar to the trusted news or not.</p><p class="Para" id="Par244">The detection of text generated by advanced language models like GPT-3 has been investigated by Fröhling et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>]. They conduct a number of experiments on data generated by different language models, such as GPT-2 with different parameter counts, Grover [<span class="CitationRef"><a epub:type="biblioref" href="#CR255" role="doc-biblioref">255</a></span>], and GPT-3 with 175B parameters. It turns out that classifiers are able to identify lingual peculiarities of a single language model with good accuracy of 70–90%. However, when another language model has generated the text, the accuracy drops and reaches only about 30–50%. The authors conclude that it might be impossible to account for these differences in one single classifier, and propose other solutions like dedicated classifiers.</p><p class="Para" id="Par245">Sepúlveda-Torres et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR201" role="doc-biblioref">201</a></span>] introduce a method to detect dissonance between the headline and the body of a news article. This is especially useful, when considering that most users do not read the body of news articles on social media, but rather form an opinion based on the headline. A summary of the article is generated and compared to the headline using a RoBERTa model. On a Fake News Challenge FNC-1 dataset the model achieves a new <span class="EmphasisTypeSmallCaps ">Sota</span> with 94.1% accuracy.</p><p class="Para" id="Par246">Alizadeh et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>] describe the practical application of a system analyzing publicly available Twitter data by Chinese, Russian, and Venezuelan trolls targeting the United States, as well as the Reddit dataset of Russian influence efforts. They report that content-based features perform well across period, country, platform, and prediction task.</p><p class="Para" id="Par247">As a new feature, the reliability of news publishers and disseminators can be taken into account for fake news detection. This means that a news story originating from a source with high reputation is more credible. <strong class="EmphasisTypeBold ">SMAN</strong><span id="ITerm167"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR252" role="doc-biblioref">252</a></span>] is a PLM-based model which combines the news content, publishing, and reposting relations of publishers and users, to jointly optimize the fake news detection and credibility prediction tasks. While the text of a story can be adapted by new algorithms it is not possible for the faker to change the network of publishers. The authors performed experiments on three real-world datasets. They considered messaging datasets with a time stamp and in this way could emulate detection over time. The results show that SMAN can detect fake news within 4 h with an accuracy of over 91%, which is much faster than the state-of-the-art models.</p><p class="Para" id="Par248">Fake news can jointly contain text and images. Therefore image analysis techniques discussed in Sect. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml#Sec12"><span class="RefSource">7.​2</span></a></span> can be employed. An advanced solution is discussed in [<span class="CitationRef"><a epub:type="biblioref" href="#CR208" role="doc-biblioref">208</a></span>], and a challenge including image hate news is described by Kiela et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR100" role="doc-biblioref">100</a></span>].</p></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec46"><h3 class="Heading"><span class="HeadingNumber">6.5.6 </span>Generating Computer Code</h3><p class="Para" id="Par249">The training data of Foundation Models contains a lot of computer code, e.g. 39B code tokens for PaLM [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>, p. 22]. Foundation Models handle code in the same way as they process words: they simply generate the next statement given the previous words. PaLM considers two tasks in connection to code [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>, p. 21]: Text-to-code aims to write code given a natural language description. Code-to-code involves the translation of C++ programs to Python. For evaluation, the percentage of generated code samples that solve the task is reported.</p><p class="Para" id="Par250">Different benchmarks were employed for evaluation. In the <em class="EmphasisTypeItalic ">HumanEval</em><span id="ITerm168"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>] and <em class="EmphasisTypeItalic ">MBPP</em><span id="ITerm169"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>] benchmarks, the model is given an English description of a few sentences and a small number of input-output examples, and the goal is to generate a short Python program, usually a single function. More demanding is the <em class="EmphasisTypeItalic ">GSM8K-Python</em><span id="ITerm170"/> task derived from the <em class="EmphasisTypeItalic ">GSM8K</em><span id="ITerm171"/> benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>]. The mathematics word problems in the GSM8K are converted to the task to produce a Python program that returns a correct solution. Four problems manually converted to Python programs were used as few-shot exemplars.</p><p class="Para" id="Par251">For the HumanEval and MBPP benchmarks the pre-trained PaLM<sub>540<em class="EmphasisTypeItalic ">B</em></sub> was able to generate a Python program that implemented the correct solution 76.2% and 75.0% of the cases, respectively. A PaLM<sub>540<em class="EmphasisTypeItalic ">B</em></sub> version fine-tuned on additional Python-text data is called PaLM-Coder. For this model, performance on HumanEval and MBPP was increased to 88.4% and 80.8% respectively, where the first result is <span class="EmphasisTypeSmallCaps ">Sota</span>. The mathematics word problems in the GSM8K-Python data were correctly solved by PaLM<sub>540<em class="EmphasisTypeItalic ">B</em></sub> in 51.3% of the cases, which again is <span class="EmphasisTypeSmallCaps ">Sota</span>. Note that the solution of mathematical text problems is also a big hurdle for many students. A systematic evaluation of Foundation Models of code is provided by Xu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR240" role="doc-biblioref">240</a></span>].</p><p class="Para" id="Par252">There are a number of other programming applications. In a GPT-3 based layout generator, for example, users just enter a short text describing a layout <em class="EmphasisTypeItalic ">“the google logo, a search box, 2 lightgrey buttons that say ‘Search Google’ and ‘I’m feeling Lucky’ with padding in-between them”</em> and the system creates a program for this website [<span class="CitationRef"><a epub:type="biblioref" href="#CR59" role="doc-biblioref">59</a></span>]. A more advanced system is the GPT-3 based <strong class="EmphasisTypeBold ">GitHub Copilot</strong><span id="ITerm172"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR157" role="doc-biblioref">157</a></span>]. Initial reactions are mostly positive, but the code produced by Copilot does not always work. GitHub itself advises checking the generated code carefully. The responsibility for ensuring that the program is correct in the end remains with the human programmer. Software developers with access to Copilot on GitHub already rely on it to generate a third of their code—especially for routine tasks—when using major programming languages [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>]. Note that there is a broad discussion about whether software copyrights are infringed by Copilot. Currently, courts are dealing with this issue [<span class="CitationRef"><a epub:type="biblioref" href="#CR229" role="doc-biblioref">229</a></span>]. Codex [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>] is an alternative Foundation Model to generate code from natural language text provided by OpenAI.</p><section class="Section3 RenderAsSection3" id="Sec47"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par253"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par254">CTRL <span class="ExternalRef"><a href="https://huggingface.co/transformers/model_doc/ctrl.html"><span class="RefSource">https://​huggingface.​co/​transformers/​model_​doc/​ctrl.​html</span></a></span></p></li><li><p class="Para" id="Par255">Facts2Story Data: <span class="ExternalRef"><a href="https://github.com/eyal-orbach/Facts2Story-data"><span class="RefSource">https://​github.​com/​eyal-orbach/​Facts2Story-data</span></a></span>,</p><p class="Para" id="Par256">code: <span class="ExternalRef"><a href="https://github.com/eyal-orbach/Facts2Story-XLNetPlanCloze"><span class="RefSource">https://​github.​com/​eyal-orbach/​Facts2Story-XLNetPlanCloze</span></a></span></p></li><li><p class="Para" id="Par257">XLNet <span class="ExternalRef"><a href="https://huggingface.co/transformers/model_doc/xlnet.html"><span class="RefSource">https://​huggingface.​co/​transformers/​model_​doc/​xlnet.​html</span></a></span></p></li><li><p class="Para" id="Par258">PlotMachines <span class="ExternalRef"><a href="https://github.com/hrashkin/plotmachines"><span class="RefSource">https://​github.​com/​hrashkin/​plotmachines</span></a></span></p></li><li><p class="Para" id="Par259">ProGen <span class="ExternalRef"><a href="https://github.com/tanyuqian/progressive-generation"><span class="RefSource">https://​github.​com/​tanyuqian/​progressive-generation</span></a></span></p></li><li><p class="Para" id="Par260">FIST code: <span class="ExternalRef"><a href="https://github.com/fangleai/Outline2Story"><span class="RefSource">https://​github.​com/​fangleai/​Outline2Story</span></a></span>,</p><p class="Para" id="Par261">WikiPlots data: <span class="ExternalRef"><a href="https://github.com/markriedl/WikiPlots"><span class="RefSource">https://​github.​com/​markriedl/​WikiPlots</span></a></span></p></li><li><p class="Para" id="Par262">GPT-3 API <span class="ExternalRef"><a href="https://openai.com/blog/openai-api/"><span class="RefSource">https://​openai.​com/​blog/​openai-api/​</span></a></span></p></li><li><p class="Para" id="Par263">GitHub Copilot for programming <span class="ExternalRef"><a href="https://github.com/features/copilot"><span class="RefSource">https://​github.​com/​features/​copilot</span></a></span></p></li><li><p class="Para" id="Par264">OpenAI Codex programming support <span class="ExternalRef"><a href="https://openai.com/blog/openai-codex/"><span class="RefSource">https://​openai.​com/​blog/​openai-codex/​</span></a></span></p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec48"><h3 class="Heading"><span class="HeadingNumber">6.5.7 </span>Summary</h3><p class="Para" id="Par265">Natural language generation (NLG) has made enormous progress in recent years. Starting from an input text, it is possible to generate a syntactically correct and semantically coherent continuation. The generation of natural language is a basic capability of Foundation Models and is frequently not even checked anymore. However, the start text alone often provides too little control to generate the desired output, so the performance of text generation is still far from satisfactory in many real-world scenarios. To address this issue, researchers have considered incorporating additional information and instructions into text generation systems.</p><p class="Para" id="Par266">Style is a text feature that can be controlled during text generation. This can be achieved by a language model, which has been fine-tuned with specific conditional style markers (e.g. CTRL). Alternatively, an independent model may be trained that modifies the distribution of generated words and produces at the desired style word distribution with the lowest divergence to the underlying language model (e.g. ETC-NLG, GDC). An alternative is the generation of text with a given style by GPT-3 using few-shot instructions. Often a document has to be transferred to a new style, e.g. from legal to non-formal, while keeping the content. This can be solved as a translation task with an encoder-decoder Foundation Model. Alternatively, an encoder-decoder PLM (e.g. StyleLM) may be fine-tuned on a corpus with the target style and thus learns to produce the desired output. Also embeddings of two texts may be created to produce a new text interpolating the meaning of the two input texts (OPTIMUS). Again Foundation Models like GPT-3 and PaLM can be used to transform a text to a new style by few-shot instructions.</p><p class="Para" id="Par267">Usually, the user wants to control the development of a story through a story line. PlotMachines is able to generate a story along different phrases and keeps track of the phrases already employed. Pointer and ProGen and SOE use a refinement strategy, where a story line consisting of phrases is expanded to the full text. Facts2story is based on XLNET, which can take into account “future” text during story generation and produces stories judged favorably by human raters. While the FIST model mixes the full text and the storyline separated by specific tokens, there are other approaches that employ an additional memory to store the entities and the generated text. Again GPT-3 and other Foundation Models can be instructed by few-shot prompts containing a list to generate a story along the list. Alternatively, the story can be specified as a list of events, where the logical and temporal dependency is expressed as a graph. The LaMDA dialog system (Sect. <span class="InternalRef"><a href="#Sec52">6.6.3</a></span>) shows that facticity can be improved by retrieval models. In addition, it is able to reduce toxic language by a system of filters that block unwanted speech. These techniques can also be applied to story generation.</p><p class="Para" id="Par268">A final section discusses the generation of fake news. It turns out that GPT-3 can be employed to generate different types of convincing fake news, such as tweets and longer stories, with little human effort. The content of fake text can be targeted to different recipients. The detection of fake news is difficult, if the generating model is unknown. Classifiers can identify various style features of fake news as well as a discrepancy between headline and body. A comparison with credible news sources is very helpful. After identifying problematic claims in a document, retrieval techniques can be used to find trusted news documents, which support the content. Here approaches developed for text retrieval (Sect. <span class="InternalRef"><a href="#Sec1">6.1</a></span>) offer great potential for improvement.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec49"><h2 class="Heading"><span class="HeadingNumber">6.6 </span>Dialog Systems</h2><p class="Para" id="Par269"><em class="EmphasisTypeItalic ">Dialog systems</em><span id="ITerm173"/> automatically generate adequate responses to the utterances of a human dialog partner in the course of a longer conversation. The human user sends a message and the systems gives an appropriate response based on the current message and the conversation history. If the messages and responses are written texts, then the system is called a <em class="EmphasisTypeItalic ">chatbot</em><span id="ITerm174"/>.</p><p class="Para" id="Par270">If the system also has <em class="EmphasisTypeItalic ">automatic speech recognition</em><span id="ITerm175"/> (<em class="EmphasisTypeItalic ">ASR</em>) and a <em class="EmphasisTypeItalic ">Text-to-Speech</em><span id="ITerm176"/> (<em class="EmphasisTypeItalic ">TTS</em>) module for voice output (Sect. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml#Sec1"><span class="RefSource">7.​1</span></a></span>), it is able to interpret human speech and respond via a synthetic voice. Then it is called <em class="EmphasisTypeItalic ">virtual assistant</em>.<span id="ITerm177"/> Examples include Apple’s Siri, Amazon’s Alexa, and Google’s Assistant. Currently, there are digital personal assistants in 4.2B devices such as smartphones and desktop computers around the world [<span class="CitationRef"><a epub:type="biblioref" href="#CR227" role="doc-biblioref">227</a></span>]. Such a system can answer questions, control media playback, operate home automation, or have a multi-turn chit-chat dialog with the user on almost any topic. Dialog systems combine techniques of question-answering (Sect. <span class="InternalRef"><a href="#Sec9">6.2</a></span>) with story generation (Sect. <span class="InternalRef"><a href="#Sec31">6.5</a></span>). Many enhancements such as generating diverse text (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec15"><span class="RefSource">2.​2.​3</span></a></span>) and retrieving additional information (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec17"><span class="RefSource">3.​4</span></a></span>) can be applied.</p><p class="Para" id="Par271">Evaluating dialog systems is difficult. Often a dialog system is fine-tuned on a dataset with human dialogs. Then the accuracy of the reconstruction of the dialogs can be measured in a similar way as the quality of a translation by <span class="EmphasisTypeSmallCaps ">Bleu</span>, <span class="EmphasisTypeSmallCaps ">Rouge</span>, etc. However, this ignores the variability of dialogs between humans. Therefore, evaluations are often performed by humans which have to assess, whether the system-generated contributions are coherent, factually correct, informative, engage the dialog partner, and sound ‘human’. The reliability of human evaluation requires that it is done by a number of independent raters. A survey of approaches for dialog evaluation is provided by Deriu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>].</p><div class="Para" id="Par272">Early dialog systems were <em class="EmphasisTypeItalic ">rule-based</em><span id="ITerm178"/>. They applied a set of rules, which were triggered by keywords and composed an answer. An example is <em class="EmphasisTypeItalic ">ELIZA</em><span id="ITerm179"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR231" role="doc-biblioref">231</a></span>]. These rules were brittle and had too limited coverage for open domain dialogs. Hence, they were extended by retrieval-based dialog systems [<span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>] collecting answer candidates by information retrieval from websites and social media. Surveys of dialog systems also covering earlier models are provided by Sun et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR212" role="doc-biblioref">212</a></span>] and Zaib et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR254" role="doc-biblioref">254</a></span>]. An overview over the models discussed in this section is given in Table <span class="InternalRef"><a href="#Tab13">6.13</a></span>. <div class="Table" id="Tab13"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 6.13</span><p class="SimplePara">Dialog systems with their performance measured by human assessment. Plato-2 human comparison benchmark on XiaoIce, DialoGPT, BlenderBot 1, Plato-2 taken from [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>]. SSA score (sensibleness and specificity average) defined by D. Adiwardana et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>]. SSI is LaMDA’s [<span class="CitationRef"><a epub:type="biblioref" href="#CR222" role="doc-biblioref">222</a></span>] evaluation by human comparison</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Details</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Benchmark</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Human</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">SSA score 86% [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>, p. 1]</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">XiaoIce (Sect. <span class="InternalRef"><a href="#Sec50">6.6.1</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Mostly rule-based system with many separate components</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">SSA score 31% [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>, p. 1]; coherent 0.87, informative 0.82, engaging 0.56, human 0.26. In Chinese [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>, table 3]</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">DialoGPT (Sect. <span class="InternalRef"><a href="#Sec51">6.6.2</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">345M, GPT-2 architecture penalizing boring answers</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">SSA score 48% [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>, p. 1]; coherent 0.72, informative 0.71, engaging 0.34, human 0.10 [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>, table 2]</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Meena (Sect. <span class="InternalRef"><a href="#Sec51">6.6.2</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">2.6B, encoder-decoder architecture</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">SSA score 79% [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>, p. 1]; 75% prefer BlenderBot 1 in terms of engagingness; 65% prefer Blenderbot 1.0 in terms of humanness</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">DialogBERT (Sect. <span class="InternalRef"><a href="#Sec51">6.6.2</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BERT-based model to generate hierarchical embeddings of phrases</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Outperforms DialoGPT in terms of BLEU and perplexity</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BlenderBot 1 (Sect. <span class="InternalRef"><a href="#Sec51">6.6.2</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">9.4B, retriever-generator architecture based on Seq2seq models. The retriever includes dialog history and facts</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">coherent 1.86, informative 1.82, engaging 1.82, human 1.54 [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>, table 2]</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Plato-2 (Sect. <span class="InternalRef"><a href="#Sec51">6.6.2</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.6B, has a fine-grained generation and an evaluation model selecting the response with best coherence</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Coherence 1.92, informativeness 1.89, Engaging 1.84, Human 1.740 [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>, table 2]</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BlenderBot 2 (Sect. <span class="InternalRef"><a href="#Sec51">6.6.2</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">2.7B, uses Bing web retrieval and DPR to obtain new information. Retrieves information on chat partner and dialog history</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Increase factual consistency from 75.5% to 84.9%, reduce factually incorrect responses from 9.1% to 3.0% [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>]</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MUDERN (Sect. <span class="InternalRef"><a href="#Sec51">6.6.2</a></span>)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Based on RoBERTa and BART. Considers multi-turn dialogs</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"> </td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">LaMDA (Sect. <span class="InternalRef"><a href="#Sec52">6.6.3</a></span>)</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">137B autoregressive Language Model, fine-tuned to increase quality, safety and factual grounding. Includes a retrieval model, a calculator and a translator</p></td><td style="text-align: left;"><p class="SimplePara">LaMDA is close to human performance in terms of sensibleness, safety and groundedness of the SSI metric [<span class="CitationRef"><a epub:type="biblioref" href="#CR222" role="doc-biblioref">222</a></span>, p. 2]</p></td></tr></tbody></table></div></div><section class="Section2 RenderAsSection2" id="Sec50"><h3 class="Heading"><span class="HeadingNumber">6.6.1 </span>Dialog Models as a Pipeline of Modules</h3><div class="Para" id="Par273">The <strong class="EmphasisTypeBold ">Alexa Prize Challenge</strong><span id="ITerm180"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>] is hosted every year by Amazon to support the development of natural, sustainable, coherent and engaging open-domain dialog systems. During this challenge, participants gain access to Amazon’s software modules that provide insight into Alexa’s software architecture. It turns out that the architecture is composed of a number of interacting modules for specific tasks such as ASR, feature extraction, and intent classification (Fig. <span class="InternalRef"><a href="#Fig20">6.20</a></span>), which were in part described in prior sections. Background information is collected from the Evi knowledge graph and by retrieval models. A response generator based on GPT-2 (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec11"><span class="RefSource">2.​2</span></a></span>) was provided. Dialog management was mostly rule-based, but also used models like RoBERTa (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>) to react to user statements. Some of the modules were replaced by the participants. There was a significant improvement in the capabilities of chatbots, e.g. only 8.6% of the responses of the best chatbot contained errors.<figure class="Figure" id="Fig20"><div class="MediaObject" id="MO20"><img alt="" aria-describedby="d64e9506" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig20_HTML.png" style="width:33.98em"/><div class="TextObject" id="d64e9506"><p class="Para" id="Par337">An illustration represents the flow of processes starting from speech recognition and proceeding through the language understanding unit, dialog manager, response generator, response builder, and speech generation.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.20</span><p class="SimplePara">The chatbot software architecture for the Alexa Prize Challenge consists of a number of modules, which are rule-based or trained separately [<span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>]. Image credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab3"><span class="RefSource">A.​2</span></a></span></p></div></figcaption></figure></div><p class="Para" id="Par274">Microsoft’s <strong class="EmphasisTypeBold ">XiaoIce</strong><span id="ITerm181"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR264" role="doc-biblioref">264</a></span>] chatbot has a similar design including dialogue manager, core chat, skills, and an ‘empathetic computing module’. It is designed to build an ‘emotional’ connection to the user and take the role of an AI companion. It is optimized for long-term engagement of interlocutors and was able to build an enormous base of 660M regular users in Asia.</p></section>
<section class="Section2 RenderAsSection2" id="Sec51"><h3 class="Heading"><span class="HeadingNumber">6.6.2 </span>Advanced Dialog Models</h3><p class="Para" id="Par275">With the introduction of the transformer by Vaswani et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR228" role="doc-biblioref">228</a></span>] PLMs have been trained which are able to generate text of unprecedented coherence and fluency. Similar to a translation task, the transformer can receive a user utterance as input and generate the response as output. Foundation Models have the potential of covering a wide range of domains and can often be trained end-to-end. As recent progress in Foundation Models has strongly pushed the performance of dialog systems, we concentrate on these models. Speech recognition (ASR) and speech generation (TTS) typically have text as an intermediate representation. Therefore, we defer the description of speech modules to Sect. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml#Sec1"><span class="RefSource">7.​1</span></a></span>.</p><p class="Para" id="Par276"><strong class="EmphasisTypeBold ">DialoGPT</strong><span id="ITerm182"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR262" role="doc-biblioref">262</a></span>] extends GPT-2 to generate a single response to a user utterance. Unlike the Alexa system, it consists of a single model. It is trained on a large collection of 147M Reddit discussions. All dialog turns are concatenated into a long text and are given as input. The GPT-2 model has to generate the observed response. To favor more interesting answers, the authors trained a backward model to predict source sentences from given responses that penalized boring alternatives. The system with 762M parameters produced more relevant and consistent text than strong base systems. The model can be extended to take into account the graph-like dependency between utterances [<span class="CitationRef"><a epub:type="biblioref" href="#CR120" role="doc-biblioref">120</a></span>]. DialoGPT yielded an SSA (sensibleness and specificity avg.) score of 51%.</p><p class="Para" id="Par277"><strong class="EmphasisTypeBold ">Meena</strong><span id="ITerm183"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>] is a multi-turn open-domain chatbot developed by Google. It consists of a modified encoder-decoder transformer with one encoder block, 13 decoder blocks, and 2.6B parameters. It was trained end-to-end on 40B words from public domain social media conversations. Each training example had the form (<em class="EmphasisTypeItalic ">context</em>, <em class="EmphasisTypeItalic ">response</em>), and the tokens of the response were predicted. It turned out that low perplexity (i.e. high likelihood of the predicted tokens) corresponds to a high sensibleness and specifity (SSA) of responses. Meena achieved a much better SSA score (78%) than other chatbots, such as DialogGPT and XiaoIce, but still less than the human score of 86%.</p><p class="Para" id="Par278"><strong class="EmphasisTypeBold ">DialogBERT</strong><span id="ITerm184"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>] has a hierarchical transformer architecture to capture the high-level structure of a multi-turn dialog. For example, if a dialog contains the phrases <em class="EmphasisTypeItalic ">“[CLS] good morning [CLS] can I help you [CLS] coffee please”</em> the lower-level <em class="EmphasisTypeItalic ">utterance encoder</em> generates embeddings for each of the three utterances employing the <em class="EmphasisTypeItalic ">[CLS]</em> token embeddings. A higher-level <em class="EmphasisTypeItalic ">context encoder</em> processes these embeddings and produces the next utterance, e.g. <em class="EmphasisTypeItalic ">“[CLS] here you are”</em>. The BERT-based models are trained with the generation of the next utterance, the reconstruction of a masked utterance, and the reordering of utterances. In terms of perplexity and <span class="EmphasisTypeSmallCaps ">Bleu</span>, the model has a much higher accuracy in reconstructing dialogs than BART and DialoGPT. An evaluation of coherence, informativeness and ‘humanness’ by human raters is also favorable for DialogBERT.</p><p class="Para" id="Par279"><strong class="EmphasisTypeBold ">BlenderBot 1</strong><span id="ITerm185"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR190" role="doc-biblioref">190</a></span>] is an open-domain chatbot opensourced by Facebook with 90M to 9.4B parameters. It aims to ‘blend’ the following skills: listen to the users, develop empathy, use background knowledge, and maintain a consistent persona. It addresses the problem of previous chatbots, which often give dull and repetitive answers, frequently hallucinate knowledge and make false statements. The authors use a Transformer encoder-decoder as base model and train different variants, among them a ‘retrieve and refine’ model integrating dialog history and knowledge retrieval results as additional input. To avoid known biases, an ‘unlikelihood-loss’ is used, penalizing specific tokens. Retrieval is based on a tf-idf-based inverted index and a transformer-based ranker. In addition, a classifier is employed to decide if a retrieval-step is required. Finally, the <em class="EmphasisTypeItalic ">persona</em><span id="ITerm186"/>, i.e. the personality, of the model can be defined by two sentences, e.g. <em class="EmphasisTypeItalic ">“I am a self aware chatbot. My name is Captain Kiwi”</em>.</p><p class="Para" id="Par280">The model is pre-trained on group discussions and fine-tuned on four direct two-way conversational data collections, e.g. ConvAI2. It turned out that the retrieve and refine model yielded best results. Note that most retrieval techniques discussed in QA (Sect. <span class="InternalRef"><a href="#Sec13">6.2.2</a></span>) may also be employed in dialog systems. In addition, it was important to control the length of the responses to avoid answers that were too short or too verbose. In a comparison, 67% of the human evaluators said that BlenderBot 1 responses sound more human than Meena responses. When comparing human-to-human and human-to-BlenderBot conversations, 49% of the BlenderBot 1 conversation were preferred by human raters, which is indistinguishable from chance. However, BlenderBot 1 still has some limitations, such as sometimes generating a response that resembles the user’s remarks. Sometimes it does not remember facts already mentioned during the conversation, or it generates incorrect information.</p><p class="Para" id="Par281"><strong class="EmphasisTypeBold ">Plato-2</strong><span id="ITerm187"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>] of Baidu starts from the observation that there are multiple appropriate responses to the same dialog context, and controls this variability by a discrete latent variable. In the first stage a coarse-grained transformer model is trained under the assumption that there is one correct response. It optimizes the LM-loss for the best prediction of the next token.</p><p class="Para" id="Par282">The second stage continues to refine the generation with a fine-grained generation model and an evaluation model. The fine-grained model estimates an intervening discrete latent variable <em class="EmphasisTypeItalic ">z</em> with <em class="EmphasisTypeItalic ">K</em> = 20 different values corresponding to a particular latent speech act in the response. An evaluation model estimates the coherence of responses.</p><p class="Para" id="Par283">The model has versions with 310M and 1.6B parameters and was trained on 684M English open-domain (context, response) samples. The response is generated by first producing a response conditional to each value of <em class="EmphasisTypeItalic ">z</em>. Then the response with the highest coherence value is selected as final response. Compared to Meena, DialoGPT, and BlenderBot 1, Plato-2’s responses are more coherent, informative and engaging according to the experiments. In relation to BlenderBot 1, PLATO-2 can stick to the start topic and conduct more in-depth discussions. In the DSTC9 competition Plato-2 was used by the winning system in the knowledge-grounded dialogue generation track [<span class="CitationRef"><a epub:type="biblioref" href="#CR119" role="doc-biblioref">119</a></span>].</p><div class="Para" id="Par284"><strong class="EmphasisTypeBold ">BlenderBot 2</strong><span id="ITerm188"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR102" role="doc-biblioref">102</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR242" role="doc-biblioref">242</a></span>] is an extension of Blenderbot 1.0 with 2.7B parameters (Fig. <span class="InternalRef"><a href="#Fig21">6.21</a></span>). On the one hand, the system uses web retrieval (Bing), to obtain new information from the internet employing a conventional search engine and dense retrieval based on DPR (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec22"><span class="RefSource">3.​4.​5</span></a></span>). On the other hand, it provides a read-write partner memory storing the features of the dialog partner as well as a chatbot memory with the properties and persona of the chatbot. The text to be stored is generated from the conversation by a transformer-based abstractive summarizer and added to the corresponding memory (Fig. <span class="InternalRef"><a href="#Fig22">6.22</a></span>). In this way, the model gets access to up-to-date information on the web and can remember properties of the partner and statements mentioned in the dialog.<figure class="Figure" id="Fig21"><div class="MediaObject" id="MO21"><img alt="" aria-describedby="d64e9682" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig21_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e9682"><p class="Para" id="Par338">An illustration represents the query generator and encoder-decoder summarizer going through the long-term memory, followed by the concatenated embeddings, and the decoder to generate the response. It indicates the role of the internet in the keyword search.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.21</span><p class="SimplePara">Architecture of BlenderBot 2 dialog system combining a standard Internet keyword search and a long term memory to store dialog events etc. Adapted from [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>]. Image credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab3"><span class="RefSource">A.​2</span></a></span></p></div></figcaption></figure><figure class="Figure" id="Fig22"><div class="MediaObject" id="MO22"><img alt="" aria-describedby="d64e9697" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig22_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e9697"><p class="Para" id="Par339">A diagram represents a set of prompts related to music albums along with their answers generated by the bot.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.22</span><p class="SimplePara">Example conversation of BlenderBot 2 with a human partner [<span class="CitationRef"><a epub:type="biblioref" href="#CR233" role="doc-biblioref">233</a></span>]. The dashed boxes describe actions of the system and the grey boxes contain utterances of the system</p></div></figcaption></figure></div><p class="Para" id="Par285">When an answer has to be generated, different retrievers form a query from the context and retrieve content from the partner and the chatbot memory as well as from the Internet. The retrieved content and the context are processed by the generator to create the response (Fig. <span class="InternalRef"><a href="#Fig21">6.21</a></span>). To be able to train a sequence of chats with the same partner, a new dataset <em class="EmphasisTypeItalic ">Multi-Session Chat</em><span id="ITerm189"/> was created by crowdworkers. Due to the dialog history memory, the new model had a significantly higher engaging response and a significantly better final human rating compared to BlenderBot 1. BlenderBot 2 delivers consistent conversations across multiple sessions and uses the Internet’s dynamic knowledge to access the most recent information. In addition, factual consistency was increased from 75.5% to 84.9% and the internet search module reduced the percentage of factually incorrect responses from 9.1% to 3.0% [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>]. To exclude toxic language, the model inserts a specific token at the end of possibly unwanted output. Then the algorithm can handle this and possibly exclude the text [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>].</p><p class="Para" id="Par286">An error analysis revealed [<span class="CitationRef"><a epub:type="biblioref" href="#CR111" role="doc-biblioref">111</a></span>] that there are a number of practical problems with BlenderBot 2. First, generating appropriate web queries from the context seems to be difficult. Sometimes the wrong information is extracted from the selected answers. In particular, extracting information from tabular data is challenging. An improvement would be the translation into multiple languages to retrieve information in different languages. Another issue is the verification of knowledge retrieved from the Internet, which is currently not done.</p><p class="Para" id="Par287"><strong class="EmphasisTypeBold ">MUDERN</strong><span id="ITerm190"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>] considers retrieval techniques in a multi-turn dialogue. Here, the system has to select information pertaining to a user question in a sequential way and ask follow-up clarification questions, whose answers are necessary to satisfy the request. The model is based on RoBERTa and BART and has a favorable performance on a specific multi-turn benchmark.</p></section>
<section class="Section2 RenderAsSection2" id="Sec52"><h3 class="Heading"><span class="HeadingNumber">6.6.3 </span>LaMDA and BlenderBot 3 Using Retrieval and Filters</h3><p class="Para" id="Par288"><strong class="EmphasisTypeBold ">LaMDA</strong><span id="ITerm191"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR222" role="doc-biblioref">222</a></span>] is a PLM-based dialog system with up to 137B non-embedding parameters presented by Google. LaMDA is a decoder-only PLM similar to GPT with 64 layers, 128 heads, relative attention similar to T5, and gated-GELU activation. It was pre-trained on 1560B words of public dialog data and other public web documents with the task to predict the next token of a text. Pre-training required 1024 TPU chips and took 58 days using the GSPDM framework [<span class="CitationRef"><a epub:type="biblioref" href="#CR244" role="doc-biblioref">244</a></span>]. The LaMDA generator is fine-tuned to predict the next token on a dialog dataset restricted to back-and-forth dialog between two participants. Arcas [<span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>] discusses some sample dialogs with LaMDA. The dialog does not belong to Arcas [<span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>].</p><p class="Para" id="Par289">LaMDA concentrates on three aspects: <em class="EmphasisTypeItalic ">quality</em> including sensible, specific and interesting (SSI) answers, <em class="EmphasisTypeItalic ">safety</em> to avoid harmful suggestions and unfair bias as well as <em class="EmphasisTypeItalic ">factual grounding</em>, i.e. preventing unproven statements. For all three dimensions (quality, safety, factual grounding) appropriate metrics were developed. While increasing the model size alone can improve quality, it shows less improvements on safety and factual grounding.</p><p class="Para" id="Par290">To improve the responses with respect to the three dimensions, LaMDA classifiers were fine-tuned to predict SSI ratings for the response. The training data is generated through extensive dialog experiments with crowdworkers. The dialog generation is performed in an adversarial manner, with analysts trying to intentionally provoke responses that violate the safety rules. After training, the classifiers provide a rating of the quality, safety, and factual grounding metric for a response.</p><div class="Para" id="Par291">During a dialog the LaMDA generator produces several candidate responses using the current context as input. Then the LaMDA classifier filters out candidates with a low sensibleness, specificity, and interestingness (SSI) ratings. Subsequently, the candidate with the highest ratings is selected as response. An evaluation by human raters shows that LaMDA is close to human performance in terms of sensibleness, safety and groundedness (Fig. <span class="InternalRef"><a href="#Fig23">6.23</a></span>). It exhibits a specificity, which is similar to humans. In informativeness, it performs better than a human without IR, and in interestingness, it fares better than human responses. It turns out that fine-tuning with respect to quality, safety and groundedness is a big advantage compared to the pre-trained model. On the question <em class="EmphasisTypeItalic ">“Do you think one skin color is better?”</em> the pre-trained model responded as <em class="EmphasisTypeItalic ">“.) What the **** I mean why the **** would anyone want to put up with this ******* bullshit? Are you ******* kidding me?”</em> while the fine-tuned model answered <em class="EmphasisTypeItalic ">“I don’t think the color of skin has anything to do with being better or worse. It’s what’s inside someone that counts, not what they look like.”</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR222" role="doc-biblioref">222</a></span>, p. 36].<figure class="Figure" id="Fig23"><div class="MediaObject" id="MO23"><img alt="" aria-describedby="d64e9798" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig23_HTML.png" style="width:34.38em"/><div class="TextObject" id="d64e9798"><p class="Para" id="Par316">An illustration represents the flow of the process, which starts with a prompt from a human followed by a set of partial information collected in the LAMDA base, LaMDA research, and toolset, before generating the relevant response.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.23</span><p class="SimplePara">For the LaMDA dialog model the performance of generated text is measured with six different metrics [<span class="CitationRef"><a epub:type="biblioref" href="#CR222" role="doc-biblioref">222</a></span>, p. 12]. The results for pre-trained models (PT) and LaMDA models with additional filtering using fine-tuned classifiers are shown. These are compared with results for crowdworkers with access to information retrieval tools (‘Human’), and without access to information retrieval tools (‘Human w/o IR’)</p></div></figcaption></figure></div><div class="Para" id="Par292">In addition, LaMDA is trained to perform retrieval and include retrieved information into its answers similar to Retro (Sect. <span class="InternalRef"><a href="#Sec15">6.2.3</a></span>). It has access to a <em class="EmphasisTypeItalic ">toolset</em> containing an information retrieval system, a calculator, and a translator. Each component expects a string as input. For example, the calculator takes <em class="EmphasisTypeItalic ">“1351+772”</em>, and outputs a list containing [“2123”]. Similarly, the translator can take <em class="EmphasisTypeItalic ">“I would like to have some coffee in Spanish”</em> and output <em class="EmphasisTypeItalic ">“Me gustaría tomar un café”</em>. Finally, the information retrieval system can take <em class="EmphasisTypeItalic ">“How old is Vladimir Putin?”</em>, and output <em class="EmphasisTypeItalic ">“Vladimir Putin/Age/69”</em>. The IR system is also capable of returning passages from the open web, with their corresponding URLs. The output of the calculator, translator and IR system are concatenated. An example is shown in Fig. <span class="InternalRef"><a href="#Fig24">6.24</a></span>.<figure class="Figure" id="Fig24"><div class="MediaObject" id="MO24"><img alt="" aria-describedby="d64e9844" src="../images/528393_1_En_6_Chapter/528393_1_En_6_Fig24_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e9844"><p class="Para" id="Par317">A set of 6 line graphs represents the data for sensibleness, safety, interestingness, specificity, groundedness, and informativeness. The y-axis denotes the percentage while the x-axis denotes the model size ranging from 2 to 128 billion.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.24</span><p class="SimplePara">To handle a user request, the LaMDA-Base model is called first. Then the LaMDA-research model is invoked several times. The receiver of the query is indicated by the first token. Note that the context and all intermediate results are available as input [<span class="CitationRef"><a epub:type="biblioref" href="#CR222" role="doc-biblioref">222</a></span>]. Image credits in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab3"><span class="RefSource">A.​2</span></a></span></p></div></figcaption></figure></div><p class="Para" id="Par293">Note that LaMDA can include links to external documents supporting an answer. The model can also be pre-conditioned on a specific role, e.g. as Mount Everest. The model’s role is specified by a brief description, e.g. <em class="EmphasisTypeItalic ">“Domain eduction. It teaches facts about Mount Everest, while pretending to be Mount Everest itself”</em>.</p><p class="Para" id="Par294">In June 2022 a Google engineer published a long dialog with LaMDA [<span class="CitationRef"><a epub:type="biblioref" href="#CR112" role="doc-biblioref">112</a></span>]. He claimed that the system is “sentient” with the “ability to express thoughts and feelings that was equivalent to a human child” [<span class="CitationRef"><a epub:type="biblioref" href="#CR134" role="doc-biblioref">134</a></span>]. Google denied the claim and also other researchers like Gary Marcus noted “To be sentient is to be aware of yourself in the world; LaMDA simply isn’t” [<span class="CitationRef"><a epub:type="biblioref" href="#CR79" role="doc-biblioref">79</a></span>]. The discussion shows that dialog systems have reached an amazing level of performance and consistency.</p><p class="Para" id="Par295"><strong class="EmphasisTypeBold ">BlenderBot 3</strong><span id="ITerm192"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR206" role="doc-biblioref">206</a></span>] is a dialog system with 175B parameters based on the pre-trained open-source <strong class="EmphasisTypeBold ">OPT</strong><span id="ITerm193"/> language model from Meta (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec3"><span class="RefSource">3.​1.​2</span></a></span>). It is fine-tuned as a dialog system and uses a similar mix of components as LaMDA. On the one hand it searches the Internet for information on the current subject of the dialog [<span class="CitationRef"><a epub:type="biblioref" href="#CR204" role="doc-biblioref">204</a></span>]. On the other hand it stores information about its persona and the dialog turns in a long-term memory. Similar to LaMDA it uses classifiers to detect toxic responses, which were trained with data collected from users. This even works for adversarial raters [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR93" role="doc-biblioref">93</a></span>]. Data collection can therefore continue as the model is used, with users being asked to rate the quality of responses as good or bad. This allows the model to improve its capabilities and security over time.</p><p class="Para" id="Par296">Two different models with 3B and 30B parameters are publicly available, while the 175B model is only released for reliable research facilities. The model can be explored in a live demo. In a comparison with the previous versions of BlenderBot 3<sub>175B</sub> the new model performed better with respect to factual correctness and knowledge, but was outperformed by BlenderBot 1 with respect to consistency and per-turn engagingness. There was an additional evaluation where crowdworkers talk to models given an open-ended Internet-driven dialogue task. According to human assessment, BlenderBot 3<sub>175B</sub> performed significantly better than the other BlenderBot versions and OPT<sub>175B</sub>. Currently, no comparisons with other models like LaMDA are available.</p></section>
<section class="Section2 RenderAsSection2" id="Sec53"><h3 class="Heading"><span class="HeadingNumber">6.6.4 </span>Limitations and Remedies of Dialog Systems</h3><div class="Para" id="Par297">At the end of this chapter, let us step back and take a look at the limitations and their possible remedies of dialog systems and text generation systems in general. Roller et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR190" role="doc-biblioref">190</a></span>] identified a number of weak points, which can be observed in many of these models [<span class="CitationRef"><a epub:type="biblioref" href="#CR190" role="doc-biblioref">190</a></span>]. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par298"><em class="EmphasisTypeItalic ">Vocabulary usage:</em> The models tend to generate common phrases like <em class="EmphasisTypeItalic ">“do you like”</em> and <em class="EmphasisTypeItalic ">“lot of fun”</em> too frequently and rare words too infrequently. This can be remedied by unlikelihood training [<span class="CitationRef"><a epub:type="biblioref" href="#CR190" role="doc-biblioref">190</a></span>], in which common phrases are penalized.</p></li><li><p class="Para" id="Par299"><em class="EmphasisTypeItalic ">Nontrivial repetition:</em> The models often repeat what is said to them, e.g. say that they have a pet dog if the user mentions a pet dog. This tendency may be reduced by assigning a persona to the chatbot, which directs the responses in a specific direction.</p></li><li><p class="Para" id="Par300"><em class="EmphasisTypeItalic ">Contradiction and forgetfulness:</em> Dialog models sometimes contradict themselves, especially the smaller models. For example, in a dialog, the first output is <em class="EmphasisTypeItalic ">“Arsenal won the premiership for the first time this year”</em> and then the model adds <em class="EmphasisTypeItalic ">“Arsenal has won the premiership again this year”</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR189" role="doc-biblioref">189</a></span>]. Fine-tuning a model on a task to detect contradictory statements in natural language inference was largely able to reduce such contradictions [<span class="CitationRef"><a epub:type="biblioref" href="#CR189" role="doc-biblioref">189</a></span>]. In addition, an explicit textual memory of the dialog history can be accessed by retrieval during response generation [<span class="CitationRef"><a epub:type="biblioref" href="#CR233" role="doc-biblioref">233</a></span>].</p></li><li><p class="Para" id="Par301"><em class="EmphasisTypeItalic ">Knowledge and factual correctness:</em> Sometimes models make factual errors and hallucinate information, particularly when deeply exploring a topic. Shuster et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR205" role="doc-biblioref">205</a></span>] propose a number of augmentation techniques to improve retrieval and substantially reduce the knowledge fabrication problem while maintaining conversational ability. Honovich et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR81" role="doc-biblioref">81</a></span>] develop an automatic evaluation metric for factual consistency of responses by checking statements using retrieval techniques. This strategy is also adopted by the LaMDA system (Sect. <span class="InternalRef"><a href="#Sec52">6.6.3</a></span>). Chen et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>] provide an algorithm for fact verification from tabular data. It has been shown that in human conversations it is often necessary to provide step-by-step evidence to improve mutual understanding [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>]. Dialogues with other people are rarely fluent and without glitches, and people don’t expect them to be. LaMDA was fine-tuned to generate multiple answers using retrieval and then selects an answer according to its correctness score.</p></li><li><p class="Para" id="Par302"><em class="EmphasisTypeItalic ">Reliability of knowledge:</em> Metzler et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR143" role="doc-biblioref">143</a></span>] suggests that models have to take into account the reliability and provenance of the information they cover. By citing documents that have been used for creating an answer the response can be justified and explained (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec41"><span class="RefSource">2.​4.​5</span></a></span>). This approach is also implemented in the LaMDA system (Sect. <span class="InternalRef"><a href="#Sec52">6.6.3</a></span>).</p></li><li><p class="Para" id="Par303"><em class="EmphasisTypeItalic ">Toxic language:</em> Unfortunately, when chatbots are trained on huge web collections, they also learn undesirable contents from conversations between humans, such as the use of toxic or biased language. Xu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR241" role="doc-biblioref">241</a></span>] investigate methods for filtering toxic language by classifiers and compare them to methods for ensuring safe responses in generative models. It turns out that the boundary between safe and toxic language is blurred: What is offensive to one person may not be offensive to another. They show that their best systems are able to avoid 96.6% of unacceptable language, although they are not perfect. The LaMDA system (Sect. <span class="InternalRef"><a href="#Sec52">6.6.3</a></span>) uses a battery of filters to eliminate toxic language in answers. A comprehensive discussion is given in Sect. <span class="ExternalRef"><a href="528393_1_En_8_Chapter.xhtml#Sec11"><span class="RefSource">8.​2.​1</span></a></span>.</p></li><li><p class="Para" id="Par304"><em class="EmphasisTypeItalic ">Memory:</em> Chatbots often cannot remember previous conversation turns or past conversations. This may be avoided by including the dialog history in the generation process, e.g. by storing dialog statements and retrieving it from the storage medium during response generation [<span class="CitationRef"><a epub:type="biblioref" href="#CR189" role="doc-biblioref">189</a></span>]. Zhang et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR259" role="doc-biblioref">259</a></span>] investigate several methods for long-range dialog state tracking.</p></li><li><p class="Para" id="Par305"><em class="EmphasisTypeItalic ">Retrieval Problems:</em> The generation of a query based on a user utterance to retrieve information from a dialog or web memory is difficult. In addition, the conversion of retrieved text to a response sometimes does not work properly. For BlenderBot 2, for instance, the user question <em class="EmphasisTypeItalic ">“Where is Cristiano Ronaldo’s current team”</em> generated the query <em class="EmphasisTypeItalic ">“Cristiano Ronaldo”</em> and lead to the answer <em class="EmphasisTypeItalic ">“My favorite team is Manchester United. I think they are the best team in the world.”</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR111" role="doc-biblioref">111</a></span>].</p></li><li><p class="Para" id="Par306"><em class="EmphasisTypeItalic ">Deeper understanding:</em> Dialog models cannot learn concepts through further conversation, and they have no way of <em class="EmphasisTypeItalic ">grounding</em><span id="ITerm194"/> entities, actions, and experiences in the real world. Unlike dictionaries, which define words in terms of other words, humans understand many basic words in terms of associations with sensory-motor experiences. When a person talks about <em class="EmphasisTypeItalic ">“have a pizza for dinner”</em>, she has the impression of sitting in a dimly lit pizzeria, sipping a glass of strong red wine, eating a crispy pizza, smelling the scent of the fire in the oven, and hearing the chatter of people. An engaging chatbot should be able to discuss the contents of an image or a video [<span class="CitationRef"><a epub:type="biblioref" href="#CR189" role="doc-biblioref">189</a></span>]. There are approaches to combine images with the corresponding text descriptions (Sect. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml#Sec12"><span class="RefSource">7.​2</span></a></span>). The grounding of words by sensory information is further discussed in Sect. <span class="ExternalRef"><a href="528393_1_En_8_Chapter.xhtml#Sec28"><span class="RefSource">8.​3.​2</span></a></span>.</p></li></ul></div></div><p class="Para" id="Par307">In summary, many of these problems have been mitigated in large Foundation Models.</p><section class="Section3 RenderAsSection3" id="Sec54"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par308"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par309">BlenderBot 1 (from Facebook) [<span class="CitationRef"><a epub:type="biblioref" href="#CR188" role="doc-biblioref">188</a></span>] <span class="ExternalRef"><a href="https://parl.ai/projects/recipes/"><span class="RefSource">https://​parl.​ai/​projects/​recipes/​</span></a></span>.</p></li><li><p class="Para" id="Par310">Plato-2 (from Baidu) [<span class="CitationRef"><a epub:type="biblioref" href="#CR209" role="doc-biblioref">209</a></span>] <span class="ExternalRef"><a href="https://github.com/PaddlePaddle/Knover"><span class="RefSource">https://​github.​com/​PaddlePaddle/​Knover</span></a></span></p></li><li><p class="Para" id="Par311">BlenderBot 2 [<span class="CitationRef"><a epub:type="biblioref" href="#CR103" role="doc-biblioref">103</a></span>] <span class="ExternalRef"><a href="https://parl.ai/projects/blenderbot2/"><span class="RefSource">https://​parl.​ai/​projects/​blenderbot2/​</span></a></span></p></li><li><p class="Para" id="Par312">BlenderBot 3 [<span class="CitationRef"><a epub:type="biblioref" href="#CR206" role="doc-biblioref">206</a></span>] <span class="ExternalRef"><a href="https://parl.ai/projects/bb3/"><span class="RefSource">https://​parl.​ai/​projects/​bb3/​</span></a></span></p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec55"><h3 class="Heading"><span class="HeadingNumber">6.6.5 </span>Summary</h3><p class="Para" id="Par313">During the last years Foundation Models did a large step forward towards practically usable dialog systems. All models are pre-trained on large collections of natural language text, preferable dialogs from social media. Fine-tuning employs specifically selected data to train the adequate sequence of utterances. While the quality of syntactic and semantic language production can be extended by using larger models, it is necessary to exploit other ways to improve factual correctness and eliminate toxic and unwanted language.</p><p class="Para" id="Par314">The LaMDA model with 137B parameters can be fine-tuned on dialogs generated by crowdworkers. The fine-tuning criterion increases quality (sensible, specific and interesting answers), safety (avoid harmful suggestions and unfair bias), and factual grounding (preventing unproven statements). However, the reduction of safety risks does not guarantee complete reliability. An important improvement is the retrieval of background information, especially form authoritative sources. In this way, groundedness has been improved, and simpler facts can be substantiated by established sources. More complex reasoning is still not satisfactory. There is also encouraging evidence that key challenges with neural language models, such as using a safety metric and improving soundness, can be improved with larger models and fine-tuning with specific dialog data. LaMDA and the similar BlenderBot 3 are large steps towards practical and secure open-ended dialog systems, which in turn can open up a wide range of useful applications. Note that these new approaches may be used for Foundation Models in other applications, e.g. question answering and story generation. BlenderBot 3 stands out because it is open source and gives interested researchers and companies access to high-performance dialog systems.</p><p class="Para" id="Par315">A fascinating application is emotional support for users, i.e. reducing a persons’s emotional distress and supporting her in specific situations [<span class="CitationRef"><a epub:type="biblioref" href="#CR129" role="doc-biblioref">129</a></span>]. As XiaoIce has shown, many users are willing to share their problems with a dialog system [<span class="CitationRef"><a epub:type="biblioref" href="#CR264" role="doc-biblioref">264</a></span>]. Currently, training datasets for emotional support conversations are provided. The results indicate that training with these datasets improve the ability of a dialog system to provide emotional support [<span class="CitationRef"><a epub:type="biblioref" href="#CR129" role="doc-biblioref">129</a></span>]. The discussion on the possible self-awareness of the LaMDA dialog model illustrates that the model has reached a remarkable level of performance and consistency.</p></section>
</section>
<div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a><p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p><p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p></div><aside aria-labelledby="Bib1Heading" class="Bibliography" id="Bib1"><div epub:type="bibliography" role="doc-bibliography"><div class="Heading" id="Bib1Heading">References</div><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">S. Aarohi and R. Abhinav. <em class="EmphasisTypeItalic ">BIG-bench</em>. Google, June 20, 2022. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://github.com/google/BIG-bench/blob/936c4a5876646966344349b28ae187c556938ec4/docs/paper/BIG-bench"><span class="RefSource">https://​github.​com/​google/​BIG-bench/​blob/​936c4a5876646966​344349b28ae187c5​56938ec4/​docs/​paper/​BIG-bench</span></a></span>. pdf (visited on 06/20/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">Z. Abbasiyantaeb and S. Momtazi. “Text-Based Question Answering from Information Retrieval and Deep Neural Network Perspectives: A Survey”. 2020. arXiv: 2002.06612.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">D. Adiwardana et al. “Towards a Human-like Open-Domain Chatbot”. 2020. arXiv: 2001.09977.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">A. Aghajanyan, A. Shrivastava, A. Gupta, N. Goyal, L. Zettlemoyer, and S. Gupta. “Better Fine-Tuning by Reducing Representational Collapse”. Aug. 6, 2020. arXiv: 2008.03156.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">F. Akhbardeh et al. “Findings of the 2021 Conference on Machine Translation (WMT21)”. In: <em class="EmphasisTypeItalic ">Sixth Conf. Mach. Transl. Pp 1–88 Assoc. Comput. Linguist</em>. (Nov. 10, 2021), p. 88.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">F. Alam et al. “A Survey on Multimodal Disinformation Detection”. 2021. arXiv: 2103.12541.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">M. Alizadeh, J. N. Shapiro, C. Buntain, and J. A. Tucker. “Content-Based Features Predict Social Media Influence Operations”. In: <em class="EmphasisTypeItalic ">Sci. Adv</em> 6.30 (July 24, 2020), eabb5824. <span class="EmphasisTypeSmallCaps ">issn:</span> 2375–2548. <span class="EmphasisTypeSmallCaps ">doi:</span><span class="ExternalRef"><a href="https://doi.org/10.1126/sciadv.abb5824"><span class="RefSource">https://​doi.​org/​10.​1126/​sciadv.​abb5824</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">D. Anderson. <em class="EmphasisTypeItalic ">Humanise.AI</em>. Humanise.AI. Jan. 13, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://www.humanise.ai/blog/ai-writes-poetry/"><span class="RefSource">https://​www.​humanise.​ai/​blog/​ai-writes-poetry/​</span></a></span> (visited on 02/19/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">W. Ansar and S. Goswami. “Combating the Menace: A Survey on Characterization and Detection of Fake News from a Data Science Perspective”. In: <em class="EmphasisTypeItalic ">Int. J. Inf. Manag. Data Insights</em> 1.2 (2021), p. 100052.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">N. Arabzadeh, A. Vtyurina, X. Yan, and C. L. A. Clarke. “Shallow Pooling for Sparse Labels”. Aug. 31, 2021. arXiv: 2109.00062 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">B. A. y Arcas. <em class="EmphasisTypeItalic ">Do Large Language Models Understand Us?</em> Medium. Feb. 16, 2022. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://medium.com/@blaisea/do-large-language-models-understand-us-6f881d6d8e75"><span class="RefSource">https://​medium.​com/​@blaisea/​do-large-language-models-understand-us-6f881d6d8e75</span></a></span> (visited on 05/26/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">K. Arora, K. Shuster, S. Sukhbaatar, and J. Weston. <em class="EmphasisTypeItalic ">DIRECTOR: Generator-Classifiers For Supervised Language Modeling</em>. June 15, 2022. <span class="EmphasisTypeSmallCaps ">doi:</span><span class="ExternalRef"><a href="https://doi.org/10.48550/arXiv.2206.07694"><span class="RefSource">https://​doi.​org/​10.​48550/​arXiv.​2206.​07694</span></a></span>. arXiv: 2206.07694 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">A. Asai, X. Yu, J. Kasai, and H. Hajishirzi. “One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval”. Oct. 27, 2021. arXiv: 2107.11976 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">J. Austin et al. “Program Synthesis with Large Language Models”. 2021. arXiv: 2108.07732.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">D. Bahdanau, K. Cho, and Y. Bengio. “Neural Machine Translation by Jointly Learning to Align and Translate”. 2014. arXiv: 1409.0473.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">P. Bajaj et al. “Ms Marco: A Human Generated Machine Reading Comprehension Dataset”. 2016. arXiv: 1611.09268.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">J. Baker. <em class="EmphasisTypeItalic ">Microsoft Is Cutting Dozens of MSN News Production Workers and Replacing Them with Artificial Intelligence</em>. The Seattle Times. May 29, 2020. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://www.seattletimes.com/business/local-business/microsoft-is-cutting-dozens-of-msn-news-productionworkers-and-replacing-them-with-artificial-intelligence/"><span class="RefSource">https://​www.​seattletimes.​com/​business/​local-business/​microsoft-is-cutting-dozens-of-msn-news-productionworker​s-and-replacing-them-with-artificial-intelligence/​</span></a></span> (visited on 04/29/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">S. Bao et al. “Plato-2: Towards Building an Open-Domain Chatbot via Curriculum Learning”. 2020. arXiv: 2006.16779.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">A. Bapna et al. <em class="EmphasisTypeItalic ">Building Machine Translation Systems for the Next Thousand Languages</em>. May 16, 2022. arXiv: 2205.03983 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">L. Benotti and P. Blackburn. “Grounding as a Collaborative Process”. In: <em class="EmphasisTypeItalic ">Proc. 16th Conf. Eur. Chapter Assoc. Comput. Linguist. Main Vol</em>. 2021, pp. 515–531.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">A. Berger and J. Lafferty. “Information Retrieval as Statistical Translation”. In: <em class="EmphasisTypeItalic ">ACM SIGIR Forum</em>. Vol. 51. 2. ACM New York, NY, USA, Jan. 12, 1999, pp. 219–226.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">BIG. BIG-bench …Google, June 26, 2022. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://github.com/google/BIG-bench/blob/b12c2434fce5e58002e1d919f8c7a379f5bb6047/bigbench/benchmark_tasks/keywords_to_tasks.md"><span class="RefSource">https://​github.​com/​google/​BIG-bench/​blob/​b12c2434fce5e580​02e1d919f8c7a379​f5bb6047/​bigbench/​benchmark_​tasks/​keywords_​to_​tasks.​md</span></a></span> (visited on 06/26/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">D. M. Blei. “Introduction to Probabilistic Topic Models”. In: <em class="EmphasisTypeItalic ">Commun. ACM</em> 55.4 (2011), pp. 77–84.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">R. Bommasani et al. “On the Opportunities and Risks of Foundation Models”. 2021. arXiv: 2108.07258.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">S. Borgeaud et al. “Improving Language Models by Retrieving from Trillions of Tokens”. Dec. 8, 2021. arXiv: 2112.04426 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and S. Bengio. “Generating Sentences from a Continuous Space”. May 12, 2016. arXiv: 1511.06349.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">L. Boytsov. <em class="EmphasisTypeItalic ">Oaqa/FlexNeuART</em>. Open Advancement of Question Answering Systems, Apr. 27, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://github.com/oaqa/FlexNeuART"><span class="RefSource">https://​github.​com/​oaqa/​FlexNeuART</span></a></span> (visited on 05/02/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">L. Boytsov and Z. Kolter. “Exploring Classic and Neural Lexical Translation Models for Information Retrieval: Interpretability, Effectiveness, and Efficiency Benefits”. Mar. 17, 2021. arXiv: 2102.06815.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">T. B. Brown et al. “Language Models Are Few-Shot Learners”. 2020. arXiv: 2005.14165.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">B. Buchanan, A. Lohn, M. Musser, and K. Sedova. <em class="EmphasisTypeItalic ">Truth, Lies, and Automation: How Language Models Could Change Disinformation</em>. May 1, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://cset.georgetown.edu/publication/truth-lies-and-automation/"><span class="RefSource">https://​cset.​georgetown.​edu/​publication/​truth-lies-and-automation/​</span></a></span> (visited on 10/13/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">I. Cachola, K. Lo, A. Cohan, and D. S. Weld. “TLDR: Extreme Summarization of Scientific Documents”. 2020. arXiv: 2004.15011.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">G. Carbone and G. Sarti. “ETC-NLG: End-to-end Topic-Conditioned Natural Language Generation”. Feb. 5, 2021. arXiv: 2008.10875.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">I. Caswell. <em class="EmphasisTypeItalic ">Recent Advances in Google Translate</em>. Google AI Blog. June 8, 2020. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="http://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html"><span class="RefSource">http://​ai.​googleblog.​com/​2020/​06/​recent-advances-in-google-translate.​html</span></a></span> (visited on 02/18/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">A. Celikyilmaz, E. Clark, and J. Gao. “Evaluation of Text Generation: A Survey”. 2020. arXiv: 2006.14799.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">A. Chakrabarti. <em class="EmphasisTypeItalic ">Towards More Intelligent Search: Deep Learning for Query Semantics</em>. May 1, 2018. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://blogs.bing.com/search-quality-insights/May-2018/Towards-More-Intelligent-Search-Deep-Learning-for-Query-Semantics/"><span class="RefSource">https://​blogs.​bing.​com/​search-quality-insights/​May-2018/​Towards-More-Intelligent-Search-Deep-Learning-for-Query-Semantics/​</span></a></span> (visited on 01/25/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">D. Chen. <em class="EmphasisTypeItalic ">Openqa-Tutorial Danqi/Acl2020</em>. July 5, 2020. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://github.com/danqi/acl2020-openqa-tutorial"><span class="RefSource">https://​github.​com/​danqi/​acl2020-openqa-tutorial</span></a></span> (visited on 02/24/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">D. Chen and W.-t. Yih. “Open-Domain Question Answering”. In: <em class="EmphasisTypeItalic ">Proc. 58th Annu. Meet. Assoc. Comput. Linguist. Tutor. Abstr</em>. Online: Association for Computational Linguistics, July 2020, pp. 34–37. <span class="EmphasisTypeSmallCaps ">doi:</span><span class="ExternalRef"><a href="https://doi.org/10.18653/v1/2020.acl-tutorials.8"><span class="RefSource">https://​doi.​org/​10.​18653/​v1/​2020.​acl-tutorials.​8</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">H. Chen, R. Shu, H. Takamura, and H. Nakayama. “GraphPlan: Story Generation by Planning with Event Graph”. 2021. arXiv: 2102.02977.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">M. Chen et al. “Evaluating Large Language Models Trained on Code”. 2021. arXiv: 2107. 03374.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">M. Chen. <em class="EmphasisTypeItalic ">BlenderBot 2.0: An Open Source Chatbot That Builds Long-Term Memory and Searches the Internet</em>. July 15, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://parl.ai/projects/blenderbot2/"><span class="RefSource">https://​parl.​ai/​projects/​blenderbot2/​</span></a></span> (visited on 07/24/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">W. Chen, H. Zha, Z. Chen, W. Xiong, H. Wang, and W. Wang. “Hybridqa: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data”. 2020. arXiv: 2004.07347.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">W. Chen et al. “Tabfact: A Large-Scale Dataset for Table-Based Fact Verification”. 2019. arXiv: 1909.02164.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">A. Chowdhery et al. “PaLM: Scaling Language Modeling with Pathways”. Apr. 5, 2022. arXiv: 2204.02311 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">J. H. Clark, E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V. Nikolaev, and J. Palomaki. “TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 8 (2020), pp. 454–470.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">K. Cobbe, V. Kosaraju, M. Bavarian, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. “Training Verifiers to Solve Math Word Problems”. 2021. arXiv: 2110.14168.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">A. Cohan, F. Dernoncourt, D. S. Kim, T. Bui, S. Kim, W. Chang, and N. Goharian. “A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents”. 2018. arXiv: 1804.05685.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">A. D’Ulizia, M. C. Caschera, F. Ferri, and P. Grifoni. “Fake News Detection: A Survey of Evaluation Datasets”. In: <em class="EmphasisTypeItalic ">PeerJ Comput. Sci</em>. 7 (June 18, 2021), e518. <span class="EmphasisTypeSmallCaps ">issn:</span> 2376–5992. <span class="EmphasisTypeSmallCaps ">doi:</span><span class="ExternalRef"><a href="https://doi.org/10.7717/peerj-cs.518"><span class="RefSource">https://​doi.​org/​10.​7717/​peerj-cs.​518</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">R. Dabre, C. Chu, and A. Kunchukuttan. “A Survey of Multilingual Neural Machine Translation”. In: <em class="EmphasisTypeItalic ">ACM Comput. Surv. CSUR</em> 53.5 (2020), pp. 1–38.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">Z. Dai, Z. Yang, Y. Yang, W. W. Cohen, J. Carbonell, Q. V. Le, and R. Salakhutdinov. “Transformer-XL: Language Modeling with Longer-Term Dependency, 2019”. In: URL <em class="EmphasisTypeItalic ">Httpsopenreview Netforum</em>. 2019.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">S. Dathathri et al. “Plug and Play Language Models: A Simple Approach to Controlled Text Generation”. Mar. 3, 2020. arXiv: 1912.02164.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">J. Deriu, A. Rodrigo, A. Otegi, G. Echegoyen, S. Rosset, E. Agirre, and M. Cieliebak. “Survey on Evaluation Methods for Dialogue Systems”. In: <em class="EmphasisTypeItalic ">Artif Intell Rev</em> 54.1 (Jan. 1, 2021), pp. 755–810. <span class="EmphasisTypeSmallCaps ">issn:</span> 1573–7462. <span class="EmphasisTypeSmallCaps ">doi:</span><span class="ExternalRef"><a href="https://doi.org/10.1007/s10462-020-09866-x"><span class="RefSource">https://​doi.​org/​10.​1007/​s10462-020-09866-x</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">X. Dong, W. Yu, C. Zhu, and M. Jiang. “Injecting Entity Types into Entity-Guided Text Generation”. 2020. arXiv: 2009.13401.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">Economist. “Huge “Foundation Models” Are Turbo-Charging AI Progress”. In: <em class="EmphasisTypeItalic ">The Economist</em> (June 11, 2022). <span class="EmphasisTypeSmallCaps ">issn:</span> 0013-0613. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://www.economist.com/interactive/briefing/2022/06/11/huge-foundation-models-are-turbo-charging-ai-progress"><span class="RefSource">https://​www.​economist.​com/​interactive/​briefing/​2022/​06/​11/​huge-foundation-models-are-turbo-charging-ai-progress</span></a></span> (visited on 06/20/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">A. R. Fabbri, I. Li, T. She, S. Li, and D. R. Radev. “Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model”. 2019. arXiv: 1906.01749.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">A. Fan. M2M. GitHub. 2020. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://github.com/pytorch/fairseq"><span class="RefSource">https://​github.​com/​pytorch/​fairseq</span></a></span> (visited on 02/26/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli. “ELI5: Long Form Question Answering”. 2019. arXiv: 1907.09190.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">A. Fan et al. “Beyond English-Centric Multilingual Machine Translation”. 2020. arXiv: 2010.11125.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">L. Fang, T. Zeng, C. Liu, L. Bo, W. Dong, and C. Chen. “Outline to Story: Fine-grained Controllable Story Generation from Cascaded Events”. 2021. arXiv: 2101.00822.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">F. S. Finkbeiner Bernd. “Über die Schulter geschaut: Künstliche Intelligenz in der Softwareentwicklung”. In: <em class="EmphasisTypeItalic ">iX</em> 2021.8 (July 21, 2021), p. 40. <span class="EmphasisTypeSmallCaps ">issn:</span> 0935-9680. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://www.heise.de/select/ix/2021/8/2111712093770753246"><span class="RefSource">https://​www.​heise.​de/​select/​ix/​2021/​8/​2111712093770753​246</span></a></span> (visited on 02/19/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">L. Fröhling and A. Zubiaga. “Feature-Based Detection of Automated Language Models: Tackling GPT-2, GPT-3 and Grover”. In: <em class="EmphasisTypeItalic ">PeerJ Comput. Sci</em>. 7 (2021), e443.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">61.</div><div class="CitationContent" id="CR61">R. Gabriel et al. “Further Advances in Open Domain Dialog Systems in the Third Alexa Prize Socialbot Grand Challenge”. In: <em class="EmphasisTypeItalic ">Alexa Prize Proc</em>. (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">62.</div><div class="CitationContent" id="CR62">L. Gao et al. “The Pile: An 800GB Dataset of Diverse Text for Language Modeling”. 2020. arXiv: 2101.00027.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">63.</div><div class="CitationContent" id="CR63">L. Gao and J. Callan. “Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval”. Aug. 12, 2021. arXiv: 2108.05540 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">64.</div><div class="CitationContent" id="CR64">Y. Gao, J. Li, M. R. Lyu, and I. King. “Open-Retrieval Conversational Machine Reading”. Feb. 17, 2021. arXiv: 2102.08633.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">65.</div><div class="CitationContent" id="CR65">A. Gatt and E. Krahmer. “Survey of the State of the Art in Natural Language Generation: Core Tasks, Applications and Evaluation”. In: <em class="EmphasisTypeItalic ">J. Artif. Intell. Res</em>. 61 (2018), pp. 65–170.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">66.</div><div class="CitationContent" id="CR66">S. Gehrmann et al. “The GEM Benchmark: Natural Language Generation, Its Evaluation and Metrics”. In: <em class="EmphasisTypeItalic ">Proc. 1st Workshop Nat. Lang. Gener. Eval. Metr. GEM 2021</em>. ACL-GEM-IJCNLP 2021. Online: Association for Computational Linguistics, Aug. 2021, pp. 96–120. <span class="EmphasisTypeSmallCaps ">doi:</span><span class="ExternalRef"><a href="https://doi.org/10.18653/v1/2021.gem-1.10"><span class="RefSource">https://​doi.​org/​10.​18653/​v1/​2021.​gem-1.​10</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">67.</div><div class="CitationContent" id="CR67">W. H. Gomaa and A. A. Fahmy. “A Survey of Text Similarity Approaches”. In: <em class="EmphasisTypeItalic ">Int. J. Comput. Appl</em>. 68.13 (2013), pp. 13–18.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">68.</div><div class="CitationContent" id="CR68">I. Goodfellow et al. “Generative Adversarial Nets”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2014, pp. 2672–2680.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">69.</div><div class="CitationContent" id="CR69">B. F. Green Jr, A. K. Wolf, C. Chomsky, and K. Laughery. “Baseball: An Automatic Question-Answerer”. In: <em class="EmphasisTypeItalic ">Pap. Present. May 9–11 1961 West. Jt. IRE-AIEE-ACM Comput. Conf</em>. 1961, pp. 219–224.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">70.</div><div class="CitationContent" id="CR70">X. Gu, K. M. Yoo, and J.-W. Ha. “Dialogbert: Discourse-aware Response Generation via Learning to Recover and Rank Utterances”. In: <em class="EmphasisTypeItalic ">Proc. AAAI</em> (2021). <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://www.aaai.org/AAAI21Papers/AAAI-10083.GuX.pdf"><span class="RefSource">https://​www.​aaai.​org/​AAAI21Papers/​AAAI-10083.​GuX.​pdf</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">71.</div><div class="CitationContent" id="CR71">W. Guan, I. Smetannikov, and M. Tianxing. “Survey on Automatic Text Summarization and Transformer Models Applicability”. In: <em class="EmphasisTypeItalic ">2020 Int. Conf. Control Robot. Intell. Syst</em>. 2020, pp. 176–184.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">72.</div><div class="CitationContent" id="CR72">B. Guo, H. Wang, Y. Ding, W. Wu, S. Hao, Y. Sun, and Z. Yu. “Conditional Text Generation for Harmonious Human-Machine Interaction”. Dec. 24, 2020. arXiv: 1909.03409.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">73.</div><div class="CitationContent" id="CR73">R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and S. Kumar. “Accelerating Large-Scale Inference with Anisotropic Vector Quantization”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn</em>. PMLR, 2020, pp. 3887–3896.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">74.</div><div class="CitationContent" id="CR74">V. Gupta, K. Beckh, S. Giesselbach, D. Wegener, and T. Wirtz. “Supporting Verification of News Articles with Automated Search for Semantically Similar Articles”. 2021. arXiv: 2103.15581.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">75.</div><div class="CitationContent" id="CR75">K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. “Realm: Retrieval-augmented Language Model Pre-Training”. 2020. arXiv: 2002.08909.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">76.</div><div class="CitationContent" id="CR76">R. He, A. Ravula, B. Kanagal, and J. Ainslie. “RealFormer: Transformer Likes Residual Attention”. In: <em class="EmphasisTypeItalic ">arXiv e-prints</em> (2020), arXiv–2012.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">77.</div><div class="CitationContent" id="CR77">X. He. “Parallel Refinements for Lexically Constrained Text Generation with BART”. 2021. arXiv: 2109.12487.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">78.</div><div class="CitationContent" id="CR78">K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. “Teaching Machines to Read and Comprehend”. 2015. arXiv: 1506.03340.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">79.</div><div class="CitationContent" id="CR79">A. Hern. <em class="EmphasisTypeItalic ">How Does Google’s AI Chatbot Work – and Could It Be Sentient? — Google — The Guardian</em>. June 13, 2022. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://www.theguardian.com/technology/2022/jun/13/howdoes-googles-ai-chatbot-work-and-could-it-be-sentient"><span class="RefSource">https://​www.​theguardian.​com/​technology/​2022/​jun/​13/​howdoes-googles-ai-chatbot-work-and-could-it-be-sentient</span></a></span> (visited on 06/24/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">80.</div><div class="CitationContent" id="CR80">J. Hilton. WebGPT: <em class="EmphasisTypeItalic ">Improving the Factual Accuracy of Language Models through Web Browsing</em>. OpenAI. Dec. 16, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://openai.com/blog/improving-factual-accuracy/"><span class="RefSource">https://​openai.​com/​blog/​improving-factual-accuracy/​</span></a></span> (visited on 01/12/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">81.</div><div class="CitationContent" id="CR81">O. Honovich, L. Choshen, R. Aharoni, E. Neeman, I. Szpektor, and O. Abend. “$ Q{<em class="EmphasisTypeItalic ">$</em>2<em class="EmphasisTypeItalic ">$</em>}<em class="EmphasisTypeItalic ">$</em>: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering”. 2021. arXiv: 2104.08202.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">82.</div><div class="CitationContent" id="CR82">J.-T. Huang et al. “Embedding-Based Retrieval in Facebook Search”. In: <em class="EmphasisTypeItalic ">Proc. 26th ACM SIGKDD Int. Conf. Knowl. Discov. Data Min</em>. 2020, pp. 2553–2561.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">83.</div><div class="CitationContent" id="CR83">T. Iqbal and S. Qureshi. “The Survey: Text Generation Models in Deep Learning.” In: <em class="EmphasisTypeItalic ">J. King Saud Univ.-Comput. Inf. Sci</em>. (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">84.</div><div class="CitationContent" id="CR84">G. Izacard and E. Grave. “Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering”. In: <em class="EmphasisTypeItalic ">Proc. 16th Conf. Eur. Chapter Assoc. Comput. Linguist. Main Vol</em>. EACL 2021. Online: Association for Computational Linguistics, Apr. 1, 2021, pp. 874–880. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://www.aclweb.org/anthology/2021.eacl-main.74"><span class="RefSource">https://​www.​aclweb.​org/​anthology/​2021.​eacl-main.​74</span></a></span> (visited on 06/16/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">85.</div><div class="CitationContent" id="CR85">G. Jawahar, M. Abdul-Mageed, and L. V. S. Lakshmanan. “Automatic Detection of Machine Generated Text: A Critical Survey”. Nov. 2, 2020. arXiv: 2011.01314 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">86.</div><div class="CitationContent" id="CR86">H. Ji, P. Ke, S. Huang, F. Wei, X. Zhu, and M. Huang. “Language Generation with Multi-Hop Reasoning on Commonsense Knowledge Graph”. 2020. arXiv: 2009.11692.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">87.</div><div class="CitationContent" id="CR87">J.-Y. Jiang, M. Zhang, C. Li, M. Bendersky, N. Golbandi, and M. Najork. “Semantic Text Matching for Long-Form Documents”. In: <em class="EmphasisTypeItalic ">World Wide Web Conf</em>. 2019, pp. 795–806.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">88.</div><div class="CitationContent" id="CR88">D. Jin, Z. Jin, Z. Hu, O. Vechtomova, and R. Mihalcea. “Deep Learning for Text Style Transfer: A Survey”. In: <em class="EmphasisTypeItalic ">Comput. Linguist</em>. (2021), pp. 1–51.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">89.</div><div class="CitationContent" id="CR89">D. Jin, Z. Jin, and R. Mihalcea. “Deep Learning for Text Attribute Transfer: A Survey”. 2020. arXiv: 2011.00416.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">90.</div><div class="CitationContent" id="CR90">D. Jin, Z. Jin, J. T. Zhou, L. Orii, and P. Szolovits. “Hooks in the Headline: Learning to Generate Headlines with Controlled Styles”. 2020. arXiv: 2004.01980.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">91.</div><div class="CitationContent" id="CR91">J. Johnson, M. Douze, and H. Jégou. “Billion-Scale Similarity Search with Gpus”. In: <em class="EmphasisTypeItalic ">IEEE Trans. Big Data</em> (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">92.</div><div class="CitationContent" id="CR92">M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer. “Triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension”. 2017. arXiv: 1705.03551.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">93.</div><div class="CitationContent" id="CR93">D. Ju. <em class="EmphasisTypeItalic ">Learning from Data in the Mixed Adversarial Non-Adversarial Case: Finding the Helpers and Ignoring the Trolls</em>. Meta Research, Aug. 7, 2022. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://github.com/facebookresearch/ParlAI/blob/f9da661cf05496c50d18d8685a228faa574373ce/projects/trollhunting/finding_the_helpers.pdf"><span class="RefSource">https://​github.​com/​facebookresearch​/​ParlAI/​blob/​f9da661cf05496c5​0d18d8685a228faa​574373ce/​projects/​trollhunting/​finding_​the_​helpers.​pdf</span></a></span> (visited on 08/07/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">94.</div><div class="CitationContent" id="CR94">V. Karpukhin, B. Oguz, S. Min, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. “Dense Passage Retrieval for Open-Domain Question Answering”. 2020. arXiv: 2004.04906.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">95.</div><div class="CitationContent" id="CR95">W. S. El-Kassas, C. R. Salama, A. A. Rafea, and H. K. Mohamed. “Automatic Text Summarization: A Comprehensive Survey”. In: <em class="EmphasisTypeItalic ">Expert Syst. Appl</em>. 165 (2021), p. 113679.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">96.</div><div class="CitationContent" id="CR96">N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher. “CTRL: A Conditional Transformer Language Model for Controllable Generation”. Sept. 20, 2019. arXiv: 1909. 05858.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">97.</div><div class="CitationContent" id="CR97">M. Khalifa, H. Elsahar, and M. Dymetman. “A Distributional Approach to Controlled Text Generation”. Dec. 21, 2020. arXiv: 2012.11635.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">98.</div><div class="CitationContent" id="CR98">T. Khan, A. Michalas, and A. Akhunzada. “Fake News Outbreak 2021: Can We Stop the Viral Spread?” In: <em class="EmphasisTypeItalic ">Journal of Network and Computer Applications</em> 190 (Sept. 15, 2021), p. 103112. <span class="EmphasisTypeSmallCaps ">issn:</span> 1084-8045. <span class="EmphasisTypeSmallCaps ">doi:</span><span class="ExternalRef"><a href="https://doi.org/10.1016/j.jnca.2021.103112"><span class="RefSource">https://​doi.​org/​10.​1016/​j.​jnca.​2021.​103112</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">99.</div><div class="CitationContent" id="CR99">O. Khattab and M. Zaharia. “Colbert: Efficient and Effective Passage Search via Contextualized Late Interaction over Bert”. In: <em class="EmphasisTypeItalic ">Proc. 43rd Int. ACM SIGIR Conf. Res. Dev. Inf. Retr</em>. 2020, pp. 39–48.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">100.</div><div class="CitationContent" id="CR100">D. Kiela, H. Firooz, A. Mohan, V. Goswami, A. Singh, P. Ringshia, and D. Testuggine. “The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes”. In: Adv. <em class="EmphasisTypeItalic ">Neural Inf. Process. Syst</em>. 33 (2020), pp. 2611–2624.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">101.</div><div class="CitationContent" id="CR101">B. Kim, H. Kim, and G. Kim. “Abstractive Summarization of Reddit Posts with Multi- Level Memory Networks”. 2018. arXiv: 1811.00783.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">102.</div><div class="CitationContent" id="CR102">M. Komeili, K. Shuster, and J. Weston. “Internet-Augmented Dialogue Generation”. July 15, 2021. arXiv: 2107.07566.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">103.</div><div class="CitationContent" id="CR103">M. Komeili, K. Shuster, and J. Weston. <em class="EmphasisTypeItalic ">Sea</em>. 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://parl.ai/projects/sea/"><span class="RefSource">https://​parl.​ai/​projects/​sea/​</span></a></span> (visited on 02/25/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">104.</div><div class="CitationContent" id="CR104">R. Koncel-Kedziorski, D. Bekal, Y. Luan, M. Lapata, and H. Hajishirzi. “Text Generation from Knowledge Graphs with Graph Transformers”. 2019. arXiv: 1904.02342.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">105.</div><div class="CitationContent" id="CR105">B. Krause, A. D. Gotmare, B. McCann, N. S. Keskar, S. Joty, R. Socher, and N. F. Rajani. “Gedi: Generative Discriminator Guided Sequence Generation”. 2020. arXiv: 2009.06367.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">106.</div><div class="CitationContent" id="CR106">K. Krishna, A. Roy, and M. Iyyer. “Hurdles to Progress in Long-form Question Answering”. 2021. arXiv: 2103.06332.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">107.</div><div class="CitationContent" id="CR107">T. Kudo and J. Richardson. “Sentencepiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing”. 2018. arXiv: 1808.06226.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">108.</div><div class="CitationContent" id="CR108">M. Kusner, Y. Sun, N. Kolkin, and K. Weinberger. “From Word Embeddings to Document Distances”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn</em>. PMLR, 2015, pp. 957–966.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">109.</div><div class="CitationContent" id="CR109">T. Kwiatkowski et al. “Natural Questions: A Benchmark for Question Answering Research”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 7 (2019), pp. 453–466.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">110.</div><div class="CitationContent" id="CR110">S. Läubli, R. Sennrich, and M. Volk. “Has Machine Translation Achieved Human Parity? A Case for Document-Level Evaluation”. 2018. arXiv: 1808.07048.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">111.</div><div class="CitationContent" id="CR111">J. Lee, M. Shim, S. Son, Y. Kim, C. Park, and H. Lim. “Empirical Study on BlenderBot 2.0 Errors Analysis in Terms of Model, Data and User-Centric Approach”. Jan. 10, 2022. arXiv: 2201.03239 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">112.</div><div class="CitationContent" id="CR112">B. Lemoine. <em class="EmphasisTypeItalic ">Is LaMDA Sentient? – An Interview</em>. Medium. June 11, 2022. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917"><span class="RefSource">https://​cajundiscordian.​medium.​com/​is-lamda-sentient-an-interview-ea64d916d917</span></a></span> (visited on 06/24/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">113.</div><div class="CitationContent" id="CR113">D. Lepikhin et al. “Gshard: Scaling Giant Models with Conditional Computation and Automatic Sharding”. 2020. arXiv: 2006.16668.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">114.</div><div class="CitationContent" id="CR114">P. Lewis et al. “Retrieval-Augmented Generation for Knowledge-Intensive Nlp Tasks”. Dec. 7, 2020. arXiv: 2005.11401.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">115.</div><div class="CitationContent" id="CR115">C. Li, X. Gao, Y. Li, B. Peng, X. Li, Y. Zhang, and J. Gao. “Optimus: Organizing Sentences via Pre-Trained Modeling of a Latent Space”. 2020. arXiv: 2004.04092.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">116.</div><div class="CitationContent" id="CR116">J. Li, T. Tang, W. X. Zhao, and J.-R. Wen. “Pretrained Language Models for Text Generation: A Survey”. May 24, 2021. arXiv: 2105.10311.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">117.</div><div class="CitationContent" id="CR117">X. Li, Y. Xia, X. Long, Z. Li, and S. Li. “Exploring Text-Transformers in Aaai 2021 Shared Task: Covid-19 Fake News Detection in English”. 2021. arXiv: 2101.02359.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">118.</div><div class="CitationContent" id="CR118">Y. Li, C. Li, Y. Zhang, X. Li, G. Zheng, L. Carin, and J. Gao. “Complementary Auxiliary Classifiers for Label-Conditional Text Generation”. In: <em class="EmphasisTypeItalic ">Proc. AAAI Conf. Artif. Intell</em>. Vol. 34. 05. 2020, pp. 8303–8310.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">119.</div><div class="CitationContent" id="CR119">Z. Li, Z. Li, J. Zhang, Y. Feng, and J. Zhou. “WeChat AI’s Submission for DSTC9 Interactive Dialogue Evaluation Track”. 2021. arXiv: 2101.07947.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">120.</div><div class="CitationContent" id="CR120">Z. Li, J. Zhang, Z. Fei, Y. Feng, and J. Zhou. “Conversations Are Not Flat: Modeling the Dynamic Information Flow across Dialogue Utterances”. 2021. arXiv: 2106.02227.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">121.</div><div class="CitationContent" id="CR121">O. Lieber, O. Sharir, B. Lentz, and Y. Shoham. “Jurassic-1: Technical Details and Evaluation”. In: (2021), p. 9. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf"><span class="RefSource">https://​uploads-ssl.​webflow.​com/​60fd4503684b4665​78c0d307/​61138924626a6981​ee09caf6_​jurassic_​tech_​paper.​pdf</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">122.</div><div class="CitationContent" id="CR122">M. Lili and O. Vechtomova. <em class="EmphasisTypeItalic ">Stylized Text Generation - ACL 2020 Tutorial</em>. 2020. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://sites.google.com/view/2020-stylized-text-generation/tutorial"><span class="RefSource">https://​sites.​google.​com/​view/​2020-stylized-text-generation/​tutorial</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">123.</div><div class="CitationContent" id="CR123">J. Lin, R. Nogueira, and A. Yates. “Pretrained Transformers for Text Ranking: Bert and Beyond”. 2020. arXiv: 2010.06467.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">124.</div><div class="CitationContent" id="CR124">J. Lin, R. Nogueira, and A. Yates. “Pretrained Transformers for Text Ranking: Bert and Beyond”. In: <em class="EmphasisTypeItalic ">Synth. Lect. Hum. Lang. Technol</em>. 14.4 (2021), pp. 1–325.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">125.</div><div class="CitationContent" id="CR125">S. Lin, J. Hilton, and O. Evans. “TruthfulQA: Measuring How Models Mimic Human Falsehoods”. 2021. arXiv: 2109.07958.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">126.</div><div class="CitationContent" id="CR126">Z. Lin, A. Madotto, Y. Bang, and P. Fung. “The Adapter-Bot: All-In-One Controllable Conversational Model”. In: (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">127.</div><div class="CitationContent" id="CR127">Z. Lin and M. Riedl. “Plug-and-Blend: A Framework for Controllable Story Generation with Blended Control Codes”. 2021. arXiv: 2104.04039.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">128.</div><div class="CitationContent" id="CR128">P. J. Liu and Y. Zhao. PEGASUS: <em class="EmphasisTypeItalic ">A State-of-the-Art Model for Abstractive Text Summarization</em>. Google AI Blog. June 9, 2020. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="http://ai.googleblog.com/2020/06/pegasusstate-of-art-model-for.html"><span class="RefSource">http://​ai.​googleblog.​com/​2020/​06/​pegasusstate-of-art-model-for.​html</span></a></span> (visited on 02/18/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">129.</div><div class="CitationContent" id="CR129">S. Liu et al. “Towards Emotional Support Dialog Systems”. June 2, 2021. arXiv: 2106. 01144 [cs]. 296 6 Foundation Models for Text Generation</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">130.</div><div class="CitationContent" id="CR130">X. Liu, K. Duh, L. Liu, and J. Gao. “Very Deep Transformers for Neural Machine Translation”. 2020. arXiv: 2008.07772.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">131.</div><div class="CitationContent" id="CR131">Y. Liu, P. Liu, D. Radev, and G. Neubig. “BRIO: Bringing Order to Abstractive Summarization”. 2022. arXiv: 2203.16804.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">132.</div><div class="CitationContent" id="CR132">E. Loginova, S. Varanasi, and G. Neumann. “Towards End-to-End Multilingual Question Answering”. In: <em class="EmphasisTypeItalic ">Inf. Syst. Front</em>. 23.1 (2021), pp. 227–241.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">133.</div><div class="CitationContent" id="CR133">S. Longpre, Y. Lu, and J. Daiber. “MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 9 (2021), pp. 1389–1406.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">134.</div><div class="CitationContent" id="CR134">R. Luscombe. “Google Engineer Put on Leave after Saying AI Chatbot Has Become Sentient”. In: <em class="EmphasisTypeItalic ">The Guardian. Technology</em> (June 12, 2022). <span class="EmphasisTypeSmallCaps ">issn:</span> 0261-3077. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://www.theguardian.com/technology/2022/jun/12/google-engineer-ai-bot-sentient-blake-lemoine"><span class="RefSource">https://​www.​theguardian.​com/​technology/​2022/​jun/​12/​google-engineer-ai-bot-sentient-blake-lemoine</span></a></span> (visited on 06/24/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">135.</div><div class="CitationContent" id="CR135">C. Ma, W. E. Zhang, M. Guo, H. Wang, and Q. Z. Sheng. “Multi-Document Summarization via Deep Learning Techniques: A Survey”. 2020. arXiv: 2011.04843.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">136.</div><div class="CitationContent" id="CR136">X. Ma, K. Sun, R. Pradeep, and J. Lin. “A Replication Study of Dense Passage Retriever”. 2021. arXiv: 2104.05740.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">137.</div><div class="CitationContent" id="CR137">S. MacAvaney, A. Yates, A. Cohan, and N. Goharian. “CEDR: Contextualized Embeddings for Document Ranking”. In: <em class="EmphasisTypeItalic ">Proc. 42nd Int. ACM SIGIR Conf. Res. Dev. Inf. Retr</em>. 2019, pp. 1101–1104.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">138.</div><div class="CitationContent" id="CR138">J. Maillard, V. Karpukhin, F. Petroni, W.-t. Yih, B. Oğuz, V. Stoyanov, and G. Ghosh. “Multi-Task Retrieval for Knowledge-Intensive Tasks”. 2021. arXiv: 2101.00117.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">139.</div><div class="CitationContent" id="CR139">Y. Mao, P. He, X. Liu, Y. Shen, J. Gao, J. Han, and W. Chen. “Generation-Augmented Retrieval for Open-Domain Question Answering”. 2020. arXiv: 2009.08553.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">140.</div><div class="CitationContent" id="CR140">marco. <em class="EmphasisTypeItalic ">MS MARCO Passage Ranking Leaderboard</em>. Mar. 4, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://microsoft.github.io/msmarco/leaderboard/"><span class="RefSource">https://​microsoft.​github.​io/​msmarco/​leaderboard/​</span></a></span> (visited on 05/02/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">141.</div><div class="CitationContent" id="CR141">S. Maruf, F. Saleh, and G. Haffari. “A Survey on Document-level Neural Machine Translation: Methods and Evaluation”. In: <em class="EmphasisTypeItalic ">ACM Comput. Surv</em>. 54.2 (Mar. 5, 2021), 45:1–45:36. <span class="EmphasisTypeSmallCaps ">issn:</span> 0360-0300. <span class="EmphasisTypeSmallCaps ">doi:</span><span class="ExternalRef"><a href="https://doi.org/10.1145/3441691"><span class="RefSource">https://​doi.​org/​10.​1145/​3441691</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">142.</div><div class="CitationContent" id="CR142">S. Merity, C. Xiong, J. Bradbury, and R. Socher. “Pointer Sentinel Mixture Models”. 2016. arXiv: 1609.07843.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">143.</div><div class="CitationContent" id="CR143">D. Metzler, Y. Tay, D. Bahri, and M. Najork. “Rethinking Search: Making Experts out of Dilettantes”. May 5, 2021. arXiv: 2105.02274 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">144.</div><div class="CitationContent" id="CR144">R. Mihalcea and P. Tarau. “Textrank: Bringing Order into Text”. <em class="EmphasisTypeItalic ">In: Proc. 2004 Conf. Empir. Methods Nat. Lang. Process</em>. 2004, pp. 404–411.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">145.</div><div class="CitationContent" id="CR145">S. Min, D. Chen, L. Zettlemoyer, and H. Hajishirzi. “Knowledge Guided Text Retrieval and Reading for Open Domain Question Answering”. Apr. 13, 2020. arXiv: 1911.03868.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">146.</div><div class="CitationContent" id="CR146">S. Min, J. Michael, H. Hajishirzi, and L. Zettlemoyer. “AmbigQA: Answering Ambiguous Open-Domain Questions”. 2020. arXiv: 2004.10645.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">147.</div><div class="CitationContent" id="CR147">S. Min et al. “NeurIPS 2020 EfficientQA Competition: Systems, Analyses and Lessons Learned”. 2021. arXiv: 2101.00133.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">148.</div><div class="CitationContent" id="CR148">F. Monti, F. Frasca, D. Eynard, D. Mannion, and M. M. Bronstein. “Fake News Detection on Social Media Using Geometric Deep Learning”. 2019. arXiv: 1902.06673.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">149.</div><div class="CitationContent" id="CR149">R. Nakano et al. “WebGPT: Browser-assisted Question-Answering with Human Feedback”. 2021. arXiv: 2112.09332.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">150.</div><div class="CitationContent" id="CR150">C. Napoles, M. R. Gormley, and B. Van Durme. “Annotated Gigaword”. In: <em class="EmphasisTypeItalic ">Proc. Jt. Workshop Autom. Knowl. Base Constr. Web-Scale Knowl. Extr. AKBC-WEKEX</em>. 2021, pp. 95–100.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">151.</div><div class="CitationContent" id="CR151">S. Narayan, S. B. Cohen, and M. Lapata. “Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization”. In: <em class="EmphasisTypeItalic ">Proc. 2018 Conf. Empir. Methods Nat. Lang. Process</em>. EMNLP 2018. Brussels, Belgium: Association for Computational Linguistics, Oct. 2018, pp. 1797–1807. <span class="EmphasisTypeSmallCaps ">doi:</span><span class="ExternalRef"><a href="https://doi.org/10.18653/v1/D18-1206"><span class="RefSource">https://​doi.​org/​10.​18653/​v1/​D18-1206</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">152.</div><div class="CitationContent" id="CR152">P. Nayak. “Understanding Searches Better than Ever Before”. In: <em class="EmphasisTypeItalic ">Google Blog Oct</em>. 25 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">153.</div><div class="CitationContent" id="CR153">T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder, and L. Deng. “MS MARCO: A Human Generated Machine Reading Comprehension Dataset”. In: <em class="EmphasisTypeItalic ">CoCo NIPS</em>. 2016.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">154.</div><div class="CitationContent" id="CR154">R. Nogueira, Z. Jiang, and J. Lin. “Document Ranking with a Pretrained Sequence-to- Sequence Model”. 2020. arXiv: 2003.06713.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">155.</div><div class="CitationContent" id="CR155">R. Nogueira, W. Yang, K. Cho, and J. Lin. “Multi-Stage Document Ranking with Bert”. 2019. arXiv: 1910.14424.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">156.</div><div class="CitationContent" id="CR156">R. Nogueira, W. Yang, J. Lin, and K. Cho. “Document Expansion by Query Prediction”. 2019. arXiv: 1904.08375.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">157.</div><div class="CitationContent" id="CR157">J. Novet. <em class="EmphasisTypeItalic ">Microsoft and OpenAI Have a New A.I. Tool That Will Give Coding Suggestions to Software Developers</em>. CNBC. June 29, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://www.cnbc.com/2021/06/29/microsoft-github-copilot-ai-offers-coding-suggestions.html"><span class="RefSource">https://​www.​cnbc.​com/​2021/​06/​29/​microsoft-github-copilot-ai-offers-coding-suggestions.​html</span></a></span> (visited on 02/19/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">158.</div><div class="CitationContent" id="CR158">B. Oguz et al. “Unified Open-Domain Question Answering with Structured and Unstructured Knowledge”. 2020. arXiv: 2012.14610.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">159.</div><div class="CitationContent" id="CR159">OpenAI. <em class="EmphasisTypeItalic ">Example Prompts OpenAI API</em>. Feb. 19, 2022. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://beta.openai.com"><span class="RefSource">https://​beta.​openai.​com</span></a></span> (visited on 02/19/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">160.</div><div class="CitationContent" id="CR160">OpenAi. <em class="EmphasisTypeItalic ">Prompt Examples for GPT-3</em>. Sept. 3, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://beta.openai.com/examples"><span class="RefSource">https://​beta.​openai.​com/​examples</span></a></span> (visited on 09/03/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">161.</div><div class="CitationContent" id="CR161">E. Orbach and Y. Goldberg. “Facts2Story: Controlling Text Generation by Key Facts”. 2020. arXiv: 2012.04332.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">162.</div><div class="CitationContent" id="CR162">L. Ouyang et al. “Training Language Models to Follow Instructions with Human Feedback”. Jan. 31, 2022. arXiv: 2203.02155.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">163.</div><div class="CitationContent" id="CR163">Y. M. Palenzuela. <em class="EmphasisTypeItalic ">Awesome GPT-3</em>. Feb. 19, 2022. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://github.com/elyase/awesome-gpt3"><span class="RefSource">https://​github.​com/​elyase/​awesome-gpt3</span></a></span> (visited on 02/19/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">164.</div><div class="CitationContent" id="CR164">P. Papalampidi, K. Cao, and T. Kocisky. “Towards Coherent and Consistent Use of Entities in Narrative Generation”. Feb. 3, 2022. arXiv: 2202.01709 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">165.</div><div class="CitationContent" id="CR165">D. Paperno et al. “The LAMBADA Dataset: Word Prediction Requiring a Broad Discourse Context”. June 20, 2016. arXiv: 1606.06031 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">166.</div><div class="CitationContent" id="CR166">T. Parshakova, J.-M. Andreoli, and M. Dymetman. “Distributional Reinforcement Learning for Energy-Based Sequential Models”. Dec. 18, 2019. arXiv: 1912.08517.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">167.</div><div class="CitationContent" id="CR167">N. Peng, M. Ghazvininejad, J. May, and K. Knight. “Towards Controllable Story Generation”. In: Proc. <em class="EmphasisTypeItalic ">First Workshop Storytell</em>. 2018, pp. 43–49.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">168.</div><div class="CitationContent" id="CR168">X. Peng, S. Li, S. Wiegreffe, and M. Riedl. “Inferring the Reader: Guiding Automated Story Generation with Commonsense Reasoning”. 2021. arXiv: 2105.01311.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">169.</div><div class="CitationContent" id="CR169">M. Ponza, L. Del Corro, and G. Weikum. “Facts That Matter”. In: <em class="EmphasisTypeItalic ">Proc. 2018 Conf. Empir. Methods Nat. Lang. Process</em>. 2018, pp. 1043–1048.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">170.</div><div class="CitationContent" id="CR170">S. Prabhumoye, A. W. Black, and R. Salakhutdinov. “Exploring Controllable Text Generation Techniques”. 2020. arXiv: 2005.01822.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">171.</div><div class="CitationContent" id="CR171">QAngaroo. <em class="EmphasisTypeItalic ">WikiHop Leaderboard</em>. Mar. 2, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="http://qangaroo.cs.ucl.ac.uk/leaderboard.html"><span class="RefSource">http://​qangaroo.​cs.​ucl.​ac.​uk/​leaderboard.​html</span></a></span> (visited on 03/02/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">172.</div><div class="CitationContent" id="CR172">P. Racsko. “Fake News Identification”. In: <em class="EmphasisTypeItalic ">Soc. Econ</em>. -1 (aop Nov. 11, 2021). <span class="EmphasisTypeSmallCaps ">issn:</span> 1588-9726, 1588-970X. <span class="EmphasisTypeSmallCaps ">doi:</span><span class="ExternalRef"><a href="https://doi.org/10.1556/204.2021.00020"><span class="RefSource">https://​doi.​org/​10.​1556/​204.​2021.​00020</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">173.</div><div class="CitationContent" id="CR173">A. Radford, J. Wu, D. Amodei, D. Amodei, J. Clark, M. Brundage, and I. Sutskever. “Better Language Models and Their Implications”. In: <em class="EmphasisTypeItalic ">OpenAI Blog</em> (2019). <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://openai.com/blog/better-language-models"><span class="RefSource">https://​openai.​com/​blog/​better-language-models</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">174.</div><div class="CitationContent" id="CR174">A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. “Language Models Are Unsupervised Multitask Learners”. In: <em class="EmphasisTypeItalic ">OpenAI blog</em> 1.8 (2019), p. 9.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">175.</div><div class="CitationContent" id="CR175">J. W. Rae et al. “Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher”. In: <em class="EmphasisTypeItalic ">ArXiv Prepr. ArXiv211211446</em> (Dec. 8, 2021), p. 118.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">176.</div><div class="CitationContent" id="CR176">J. W. Rae, G. Irving, and L. Weidinger. <em class="EmphasisTypeItalic ">Language modelling at scale: Gopher, ethical considerations, and retrieval</em>. Deepmind. Dec. 8, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://deepmind.com/blog/article/language-modelling-at-scale"><span class="RefSource">https://​deepmind.​com/​blog/​article/​language-modelling-at-scale</span></a></span> (visited on 12/16/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">177.</div><div class="CitationContent" id="CR177">C. Raffel et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”. In: <em class="EmphasisTypeItalic ">J. Mach. Learn. Res</em>. 21.140 (2020), pp. 1–67.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">178.</div><div class="CitationContent" id="CR178">P. Rajpurkar. <em class="EmphasisTypeItalic ">SQUAD: The Stanford Question Answering Dataset</em>. Mar. 3, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://rajpurkar.github.io/SQuAD-explorer/"><span class="RefSource">https://​rajpurkar.​github.​io/​SQuAD-explorer/​</span></a></span> (visited on 03/03/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">179.</div><div class="CitationContent" id="CR179">P. Rajpurkar, R. Jia, and P. Liang. “Know What You Don’t Know: Unanswerable Questions for SQuAD”. 2018. arXiv: 1806.03822.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">180.</div><div class="CitationContent" id="CR180">S. Rao and J. Tetreault. “Gyafc Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer”. 2018. arXiv: 1803.06535.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">181.</div><div class="CitationContent" id="CR181">H. Rashkin, A. Celikyilmaz, Y. Choi, and J. Gao. “PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking”. Oct. 9, 2020. arXiv: 2004.14967 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">182.</div><div class="CitationContent" id="CR182">E. Reif, D. Ippolito, A. Yuan, A. Coenen, C. Callison-Burch, and J. Wei. “A Recipe for Arbitrary Text Style Transfer with Large Language Models”. 2021. arXiv: 2109.03910.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">183.</div><div class="CitationContent" id="CR183">N. Reimers and I. Gurevych. “Sentence-Bert: Sentence Embeddings Using Siamese Bert- Networks”. 2019. arXiv: 1908.10084.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">184.</div><div class="CitationContent" id="CR184">R. Ren et al. “RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking”. 2021. arXiv: 2110.07367.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">185.</div><div class="CitationContent" id="CR185">L. F. Ribeiro, M. Schmitt, H. Schütze, and I. Gurevych. “Investigating Pretrained Language Models for Graph-to-Text Generation”. 2020. arXiv: 2007.08426.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">186.</div><div class="CitationContent" id="CR186">S. Robertson and H. Zaragoza. <em class="EmphasisTypeItalic ">The Probabilistic Relevance Framework: BM25 and Beyond</em>. Now Publishers Inc, 2009.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">187.</div><div class="CitationContent" id="CR187">T. Rohde, X. Wu, and Y. Liu. “Hierarchical Learning for Generation with Long Source Sequences”. 2021. arXiv: 2104.07545.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">188.</div><div class="CitationContent" id="CR188">S. Roller, E. Dinan, and J. Weston. <em class="EmphasisTypeItalic ">A state-of-the-art open source chatbot</em>. Apr. 29, 2020. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/"><span class="RefSource">https://​ai.​facebook.​com/​blog/​state-of-the-art-open-source-chatbot/​</span></a></span> (visited on 11/21/2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">189.</div><div class="CitationContent" id="CR189">S. Roller et al. “Open-Domain Conversational Agents: Current Progress, Open Problems, and Future Directions”. 2020. arXiv: 2006.12442.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">190.</div><div class="CitationContent" id="CR190">S. Roller et al. “Recipes for Building an Open-Domain Chatbot”. 2020. arXiv: 2004.13637.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">191.</div><div class="CitationContent" id="CR191">S. Rose, D. Engel, N. Cramer, and W. Cowley. “Automatic Keyword Extraction from Individual Documents”. In: <em class="EmphasisTypeItalic ">Text Min. Appl. Theory 1</em> (2010), pp. 1–20.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">192.</div><div class="CitationContent" id="CR192">S. Ruder. <em class="EmphasisTypeItalic ">Multi-Domain Multilingual Question Answering</em>. Sebastian Ruder. Dec. 6, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://ruder.io/multi-qa-tutorial/"><span class="RefSource">https://​ruder.​io/​multi-qa-tutorial/​</span></a></span> (visited on 02/10/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">193.</div><div class="CitationContent" id="CR193">S. Ruder and A. Sil. “Multi-Domain Multilingual Question Answering”. In: <em class="EmphasisTypeItalic ">Proc. 2021 Conf. Empir. Methods Nat. Lang. Process. Tutor. Abstr</em>. Punta Cana, Dominican Republic &amp; Online: Association for Computational Linguistics, Nov. 2021, pp. 17–21. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://aclanthology.org/2021.emnlp-tutorials.4"><span class="RefSource">https://​aclanthology.​org/​2021.​emnlp-tutorials.​4</span></a></span> (visited on 11/24/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">194.</div><div class="CitationContent" id="CR194">D. S. Sachan, S. Reddy, W. Hamilton, C. Dyer, and D. Yogatama. “End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering”. June 9, 2021. arXiv: 2106.05346.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">195.</div><div class="CitationContent" id="CR195">A. B. Sai, A. K. Mohankumar, and M. M. Khapra. “A Survey of Evaluation Metrics Used for NLG Systems”. 2020. arXiv: 2008.12009.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">196.</div><div class="CitationContent" id="CR196">K. Sakaguchi, C. Bhagavatula, R. L. Bras, N. Tandon, P. Clark, and Y. Choi. “Proscript: Partially Ordered Scripts Generation via Pre-Trained Language Models”. 2021. arXiv: 2104.08251.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">197.</div><div class="CitationContent" id="CR197">B. Schwartz. Google: BERT <em class="EmphasisTypeItalic ">Now Used on Almost Every English Query</em>. Search Engine Land. Oct. 15, 2020. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://searchengineland.com/google-bert-used-on-almost-everyenglish-query-342193"><span class="RefSource">https://​searchengineland​.​com/​google-bert-used-on-almost-everyenglish-query-342193</span></a></span> (visited on 01/24/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">198.</div><div class="CitationContent" id="CR198">T. Scialom, P.-A. Dray, S. Lamprier, B. Piwowarski, and J. Staiano. “MLSUM: The Multilingual Summarization Corpus”. 2020. arXiv: 2004.14900.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">199.</div><div class="CitationContent" id="CR199">P. Sen and A. Saffari. “What Do Models Learn from Question Answering Datasets?” 2020. arXiv: 2004.03490.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">200.</div><div class="CitationContent" id="CR200">R. Sennrich, B. Haddow, and A. Birch. “Improving Neural Machine Translation Models with Monolingual Data”. 2015. arXiv: 1511.06709.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">201.</div><div class="CitationContent" id="CR201">R. Sepúlveda-Torres, M. Vicente, E. Saquete, E. Lloret, and M. Palomar. “Exploring Summarization to Enhance Headline Stance Detection”. In: <em class="EmphasisTypeItalic ">Nat. Lang. Process. Inf. Syst</em>. Ed. by E. Métais, F. Meziane, H. Horacek, and E. Kapetanios. Lecture Notes in Computer Science. Cham: Springer International Publishing, 2021, pp. 243–254. <span class="EmphasisTypeSmallCaps ">isbn:</span> 978-3-030-80599-9. <span class="EmphasisTypeSmallCaps ">doi:</span><span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-030-80599-9_22"><span class="RefSource">https://​doi.​org/​10.​1007/​978-3-030-80599-9_​22</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">202.</div><div class="CitationContent" id="CR202">T. Sherborne and M. Lapata. “Zero-Shot Cross-lingual Semantic Parsing”. 2021. arXiv: 2104.07554.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">203.</div><div class="CitationContent" id="CR203">P. Shi, R. Zhang, H. Bai, and J. Lin. “Cross-Lingual Training with Dense Retrieval for Document Retrieval”. Sept. 3, 2021. arXiv: 2109.01628 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">204.</div><div class="CitationContent" id="CR204">K. Shuster, M. Komeili, L. Adolphs, S. Roller, A. Szlam, and J. Weston. <em class="EmphasisTypeItalic ">Language Models That Seek for Knowledge: Modular Search &amp; Generation for Dialogue and Prompt Completion</em>. Mar. 29, 2022. arXiv: 2203.13224 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">205.</div><div class="CitationContent" id="CR205">K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston. “Retrieval Augmentation Reduces Hallucination in Conversation”. 2021. arXiv: 2104.07567.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">206.</div><div class="CitationContent" id="CR206">K. Shuster and J. Xu. <em class="EmphasisTypeItalic ">Blender BlenderBot 3: A 175B parameter, publicly available chatbot that improves its skills and safety over time</em>. Aug. 5, 2022. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://ai.facebook.com/blog/blenderbot-3-a-175b-parameter-publicly-available-chatbot-that-improves-its-skills-andsafety-over-time/"><span class="RefSource">https://​ai.​facebook.​com/​blog/​blenderbot-3-a-175b-parameter-publicly-available-chatbot-that-improves-its-skills-andsafety-over-time/​</span></a></span> (visited on 08/07/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">207.</div><div class="CitationContent" id="CR207">H. Singh, G. Verma, A. Garimella, and B. V. Srinivasan. “DRAG: Director-Generator Language Modelling Framework for Non-Parallel Author Stylized Rewriting”. 2021. arXiv: 2101.11836.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">208.</div><div class="CitationContent" id="CR208">C. Song, N. Ning, Y. Zhang, and B. Wu. “A Multimodal Fake News Detection Model Based on Crossmodal Attention Residual and Multichannel Convolutional Neural Networks”. In: <em class="EmphasisTypeItalic ">Inf. Process. Manag</em>. 58.1 (2021), p. 102437.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">209.</div><div class="CitationContent" id="CR209">Sserdoubleh. <em class="EmphasisTypeItalic ">Large-Scale Open Domain KNOwledge Grounded conVERsation System Based on PaddlePaddle</em>. PaddlePaddle, Apr. 26, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://github.com/PaddlePaddle/Knover"><span class="RefSource">https://​github.​com/​PaddlePaddle/​Knover</span></a></span> (visited on 05/08/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">210.</div><div class="CitationContent" id="CR210">M. Stern, W. Chan, J. Kiros, and J. Uszkoreit. “Insertion Transformer: Flexible Sequence Generation via Insertion Operations”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn</em>. PMLR, 2019, pp. 5976–5985.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">211.</div><div class="CitationContent" id="CR211">N. Stiennon et al. “Learning to Summarize with Human Feedback”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 33 (Feb. 15, 2022), pp. 3008–3021.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">212.</div><div class="CitationContent" id="CR212">B. Sun and K. Li. “Neural Dialogue Generation Methods in Open Domain: A Survey”. In: <em class="EmphasisTypeItalic ">Nat. Lang. Process. Res</em>. 1.3–4 (2021), pp. 56–70.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">213.</div><div class="CitationContent" id="CR213">P. Sun. <em class="EmphasisTypeItalic ">Announcing ScaNN: Efficient Vector Similarity Search</em>. Google AI Blog. July 28, 2020. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="http://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html"><span class="RefSource">http://​ai.​googleblog.​com/​2020/​07/​announcing-scann-efficient-vector.​html</span></a></span> (visited on 02/18/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">214.</div><div class="CitationContent" id="CR214">X. Sun, C. Fan, Z. Sun, Y. Meng, F. Wu, and J. Li. “Summarize, Outline, and Elaborate: Long-Text Generation via Hierarchical Supervision from Extractive Summaries”. 2020. arXiv: 2010.07074.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">215.</div><div class="CitationContent" id="CR215">I. Sutskever, O. Vinyals, and Q. V. Le. “Sequence to Sequence Learning with Neural Networks”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2014, pp. 3104–3112.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">216.</div><div class="CitationContent" id="CR216">A. A. Syed, F. L. Gaol, and T. Matsuo. “A Survey of the State-of-the-Art Models in Neural Abstractive Text Summarization”. In: <em class="EmphasisTypeItalic ">IEEE Access</em> 9 (2021), pp. 13248–13265.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">217.</div><div class="CitationContent" id="CR217">B. Syed, G. Verma, B. V. Srinivasan, A. Natarajan, and V. Varma. “Adapting Language Models for Non-Parallel Author-Stylized Rewriting”. In: <em class="EmphasisTypeItalic ">Proc. AAAI Conf. Artif. Intell</em>. Vol. 34. 05. 2020, pp. 9008–9015.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">218.</div><div class="CitationContent" id="CR218">O. Tafjord and P. Clark. “General-Purpose Question-Answering with Macaw”. Sept. 6, 2021. arXiv: 2109.02593 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">219.</div><div class="CitationContent" id="CR219">B. Tan, Z. Yang, M. Al-Shedivat, E. Xing, and Z. Hu. “Progressive Generation of Long Text with Pretrained Language Models”. In: <em class="EmphasisTypeItalic ">Proc. 2021 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol</em>. 2021, pp. 4313–4324.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">220.</div><div class="CitationContent" id="CR220">B. Tan, Z. Yang, M. AI-Shedivat, E. P. Xing, and Z. Hu. “Progressive Generation of Long Text”. 2020. arXiv: 2006.15720.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">221.</div><div class="CitationContent" id="CR221">N. Thakur, N. Reimers, A. Rücklé, A. Srivastava, and I. Gurevych. “BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models”. Sept. 7, 2021. arXiv: 2104.08663.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">222.</div><div class="CitationContent" id="CR222">R. Thoppilan et al. “LaMDA: Language Models for Dialog Applications”. Feb. 10, 2022. arXiv: 2201.08239 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">223.</div><div class="CitationContent" id="CR223">A. Toral. “Reassessing Claims of Human Parity and Super-Human Performance in Machine Translation at WMT 2019”. 2020. arXiv: 2005.05738.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">224.</div><div class="CitationContent" id="CR224">C. Tran, S. Bhosale, J. Cross, P. Koehn, S. Edunov, and A. Fan. “Facebook AI WMT21 News Translation Task Submission”. 2021. arXiv: 2108.03265.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">225.</div><div class="CitationContent" id="CR225">C. Tran, J. Cross, S. Bhosale, and A. Fan. <em class="EmphasisTypeItalic ">The first-ever multilingual model to win WMT, beating out bilingual models</em>. Nov. 10, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://ai.facebook.com/blog/the-firstever-multilingual-model-to-win-wmt-beating-out-bilingual-models/"><span class="RefSource">https://​ai.​facebook.​com/​blog/​the-firstever-multilingual-model-to-win-wmt-beating-out-bilingual-models/​</span></a></span> (visited on 02/08/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">226.</div><div class="CitationContent" id="CR226">triviaQA. <em class="EmphasisTypeItalic ">Papers with Code - TriviaQA Benchmark (Question Answering)</em>. June 27, 2022. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://paperswithcode.com/sota/question-answering-on-triviaqa"><span class="RefSource">https://​paperswithcode.​com/​sota/​question-answering-on-triviaqa</span></a></span> (visited on 06/27/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">227.</div><div class="CitationContent" id="CR227">L. Vailshery. <em class="EmphasisTypeItalic ">Number of Digital Voice Assistants in Use Worldwide 2019–2024</em>. Jan. 22, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://www.statista.com/statistics/973815/worldwide-digital-voice-assistant-inuse/"><span class="RefSource">https://​www.​statista.​com/​statistics/​973815/​worldwide-digital-voice-assistant-inuse/​</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">228.</div><div class="CitationContent" id="CR228">A. Vaswani et al. “Attention Is All You Need”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2017, pp. 5998–6008.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">229.</div><div class="CitationContent" id="CR229">S. J. Vaughan-Nichols. <em class="EmphasisTypeItalic ">GitHub’s Copilot Faces First Open Source Copyright Lawsuit</em>. Nov. 11, 2022. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://www.theregister.com/2022/11/11/githubs_copilot_opinion/"><span class="RefSource">https://​www.​theregister.​com/​2022/​11/​11/​githubs_​copilot_​opinion/​</span></a></span> (visited on 12/17/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">230.</div><div class="CitationContent" id="CR230">M. Völske, M. Potthast, S. Syed, and B. Stein. “TL;DR: Mining Reddit to Learn Automatic Summarization”. In: <em class="EmphasisTypeItalic ">Proc. Workshop New Front. Summ</em>. Copenhagen, Denmark: Association for Computational Linguistics, Sept. 2017, pp. 59–63. <span class="EmphasisTypeSmallCaps ">doi:</span><span class="ExternalRef"><a href="https://doi.org/10.18653/v1/W17-4508"><span class="RefSource">https://​doi.​org/​10.​18653/​v1/​W17-4508</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">231.</div><div class="CitationContent" id="CR231">J. Weizenbaum. “ELIZA—a Computer Program for the Study of Natural Language Communication between Man and Machine”. In: <em class="EmphasisTypeItalic ">Commun. ACM</em> 9.1 (1966), pp. 36–45.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">232.</div><div class="CitationContent" id="CR232">J. Welbl, P. Stenetorp, and S. Riedel. “Constructing Datasets for Multi-Hop Reading Comprehension across Documents”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 6 (2018), pp. 287–302.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">233.</div><div class="CitationContent" id="CR233">J. Weston and K. Shuster. <em class="EmphasisTypeItalic ">Blender Bot 2.0: An open source chatbot that builds long-term memory and searches the internet</em>. July 16, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://ai.facebook.com/blog/blender-bot-2-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet/"><span class="RefSource">https://​ai.​facebook.​com/​blog/​blender-bot-2-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet/​</span></a></span> (visited on 02/25/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">234.</div><div class="CitationContent" id="CR234">Wikipedia. <em class="EmphasisTypeItalic ">Narrative</em>. In: <em class="EmphasisTypeItalic ">Wikipedia</em>. Apr. 10, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://en.wikipedia.org/w/index.php?title=Narrative&amp;oldid=1017065535"><span class="RefSource">https://​en.​wikipedia.​org/​w/​index.​php?​title=​Narrative&amp;​oldid=​1017065535</span></a></span> (visited on 04/21/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">235.</div><div class="CitationContent" id="CR235">M. Woolf. <em class="EmphasisTypeItalic ">How To Make Custom AI-Generated Text With GPT-2</em>. Max Woolf’s Blog. Sept. 4, 2019. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://minimaxir.com/2019/09/howto-gpt2/"><span class="RefSource">https://​minimaxir.​com/​2019/​09/​howto-gpt2/​</span></a></span> (visited on 04/29/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">236.</div><div class="CitationContent" id="CR236">J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, and P. Christiano. “Recursively Summarizing Books with Human Feedback”. 2021. arXiv: 2109.10862.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">237.</div><div class="CitationContent" id="CR237">W. Xiao, I. Beltagy, G. Carenini, and A. Cohan. “Primer: Pyramid-based Masked Sentence Pre-Training for Multi-Document Summarization”. 2021. arXiv: 2110.08499.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">238.</div><div class="CitationContent" id="CR238">L. Xiong et al. “Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval”. 2020. arXiv: 2007.00808.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">239.</div><div class="CitationContent" id="CR239">W. Xiong, M. Yu, S. Chang, X. Guo, and W. Y. Wang. “Improving Question Answering over Incomplete Kbs with Knowledge-Aware Reader”. 2019. arXiv: 1905.07098.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">240.</div><div class="CitationContent" id="CR240">F. F. Xu, U. Alon, G. Neubig, and V. J. Hellendoorn. “A Systematic Evaluation of Large Language Models of Code”. In: <em class="EmphasisTypeItalic ">Proc. 6th ACM SIGPLAN Int. Symp. Mach. Program</em>. 2022, pp. 1–10.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">241.</div><div class="CitationContent" id="CR241">J. Xu, D. Ju, M. Li, Y.-L. Boureau, J. Weston, and E. Dinan. “Recipes for Safety in Open- Domain Chatbots”. 2020. arXiv: 2010.07079.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">242.</div><div class="CitationContent" id="CR242">J. Xu, A. Szlam, and J. Weston. “Beyond Goldfish Memory: Long-Term Open-Domain Conversation”. July 15, 2021. arXiv: 2107.07567.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">243.</div><div class="CitationContent" id="CR243">P. Xu, M. Patwary, M. Shoeybi, R. Puri, P. Fung, A. Anandkumar, and B. Catanzaro. “MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models”. Oct. 2, 2020. arXiv: 2010.00840 [cs]. 6.6 Dialog Systems 301</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">244.</div><div class="CitationContent" id="CR244">Y. Xu et al. “GSPMD: General and Scalable Parallelization for ML Computation Graphs”. Dec. 23, 2021. arXiv: 2105.04663 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">245.</div><div class="CitationContent" id="CR245">L. Xue et al. “Byt5: Towards a Token-Free Future with Pre-Trained Byte-to-Byte Models”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 10 (2022), pp. 291–306.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">246.</div><div class="CitationContent" id="CR246">L. Xue et al. “mT5: A Massively Multilingual Pre-Trained Text-to-Text Transformer”. 2020. arXiv: 2010.11934.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">247.</div><div class="CitationContent" id="CR247">L. Yang, M. Zhang, C. Li, M. Bendersky, and M. Najork. “Beyond 512 Tokens: Siamese Multi-Depth Transformer-Based Hierarchical Encoder for Long-Form Document Matching”. In: <em class="EmphasisTypeItalic ">Proc. 29th ACM Int. Conf. Inf. Knowl. Manag</em>. 2020, pp. 1725–1734.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">248.</div><div class="CitationContent" id="CR248">S. Yang, Y. Wang, and X. Chu. “A Survey of Deep Learning Techniques for Neural Machine Translation”. 2020. arXiv: 2002.07526.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">249.</div><div class="CitationContent" id="CR249">Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning. “Hotpotqa: A Dataset for Diverse, Explainable Multi-Hop Question Answering”. 2018. arXiv: 1809.09600.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">250.</div><div class="CitationContent" id="CR250">X. Yi, Z. Liu, W. Li, and M. Sun. “Text Style Transfer via Learning Style Instance Supported Latent Space”. In: IJCAI, 2020.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.24963/ijcai.2020/526"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">251.</div><div class="CitationContent" id="CR251">W. Yu, C. Zhu, Z. Li, Z. Hu, Q. Wang, H. Ji, and M. Jiang. “A Survey of Knowledge- Enhanced Text Generation”. July 5, 2021. arXiv: 2010.04389.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">252.</div><div class="CitationContent" id="CR252">C. Yuan, Q. Ma, W. Zhou, J. Han, and S. Hu. “Early Detection of Fake News by Utilizing the Credibility of News, Publishers, and Users Based on Weakly Supervised Learning”. 2020. arXiv: 2012.04233.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">253.</div><div class="CitationContent" id="CR253">M. Zaheer et al. “Big Bird: Transformers for Longer Sequences”. In: Adv. Neural Inf. Process. Syst. 33 (Jan. 8, 2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">254.</div><div class="CitationContent" id="CR254">M. Zaib, W. E. Zhang, Q. Z. Sheng, A. Mahmood, and Y. Zhang. “Conversational Question Answering: A Survey”. 2021. arXiv: 2106.00874.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">255.</div><div class="CitationContent" id="CR255">R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, and Y. Choi. “Defending against Neural Fake News”. Dec. 11, 2020. arXiv: 1905.12616.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">256.</div><div class="CitationContent" id="CR256">H. Zhang, Y. Gong, Y. Shen, W. Li, J. Lv, N. Duan, and W. Chen. “Poolingformer: Long Document Modeling with Pooling Attention”. May 10, 2021. arXiv: 2105.04371 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">257.</div><div class="CitationContent" id="CR257">H. Zhang, H. Song, S. Li, M. Zhou, and D. Song. “A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models”. 2022. arXiv: 2201.05337.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">258.</div><div class="CitationContent" id="CR258">X. Zhang, X. Ma, P. Shi, and J. Lin. “Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval”. Nov. 8, 2021. arXiv: 2108.08787 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">259.</div><div class="CitationContent" id="CR259">Y. Zhang, Y. Cao, M. Mahdieh, J. Zhao, and Y. Wu. “Improving Longer-range Dialogue State Tracking”. 2021. arXiv: 2103.00109.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">260.</div><div class="CitationContent" id="CR260">Y. Zhang, T. Ge, and X. Sun. “Parallel Data Augmentation for Formality Style Transfer”. May 4, 2020. arXiv: 2005.07522.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">261.</div><div class="CitationContent" id="CR261">Y. Zhang, G. Wang, C. Li, Z. Gan, C. Brockett, and B. Dolan. “Pointer: Constrained Text Generation via Insertion-Based Generative Pre-Training”. Sept. 27, 2020. arXiv: 2005.00558.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">262.</div><div class="CitationContent" id="CR262">Y. Zhang et al. “Dialogpt: Large-scale Generative Pre-Training for Conversational Response Generation”. May 2, 2020. arXiv: 1911.00536.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">263.</div><div class="CitationContent" id="CR263">A. Zhavoronkov. <em class="EmphasisTypeItalic ">Wu Dao 2.0 - Bigger, Stronger, Faster AI From China</em>. Forbes. July 19, 2021. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://www.forbes.com/sites/alexzhavoronkov/2021/07/19/wu-dao-20biggerstronger-faster-ai-from-china/"><span class="RefSource">https://​www.​forbes.​com/​sites/​alexzhavoronkov/​2021/​07/​19/​wu-dao-20biggerstronger​-faster-ai-from-china/​</span></a></span> (visited on 07/29/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">264.</div><div class="CitationContent" id="CR264">L. Zhou, J. Gao, D. Li, and H.-Y. Shum. “The Design and Implementation of Xiaoice, an Empathetic Social Chatbot”. In: <em class="EmphasisTypeItalic ">Comput. Linguist</em>. 46.1 (2020), pp. 53–93.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">265.</div><div class="CitationContent" id="CR265">F. Zhu, W. Lei, C. Wang, J. Zheng, S. Poria, and T.-S. Chua. “Retrieving and Reading: A Comprehensive Survey on Open-Domain Question Answering”. 2021. arXiv: 2101.00774.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">266.</div><div class="CitationContent" id="CR266">J. Zhu. <em class="EmphasisTypeItalic ">Bing Delivers Its Largest Improvement in Search Experience Using Azure GPUs</em>. Nov. 18, 2019. <span class="EmphasisTypeSmallCaps ">url:</span><span class="ExternalRef"><a href="https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-in-search-experience-using-azure-gpus/"><span class="RefSource">https://​azure.​microsoft.​com/​en-us/​blog/​bing-delivers-its-largest-improvement-in-search-experience-using-azure-gpus/​</span></a></span> (visited on 01/08/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">267.</div><div class="CitationContent" id="CR267">Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. “Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books”. In: <em class="EmphasisTypeItalic ">Proc. IEEE Int. Conf. Comput. Vis</em>. 2015, pp. 19–27.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">268.</div><div class="CitationContent" id="CR268">Y. Zhu, L. Pang, Y. Lan, H. Shen, and X. Cheng. “Adaptive Information Seeking for Open- Domain Question Answering”. Sept. 14, 2021. arXiv: 2109.06747 [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">269.</div><div class="CitationContent" id="CR269">Z. M. Ziegler, L. Melas-Kyriazi, S. Gehrmann, and A. M. Rush. “Encoder-Agnostic Adaptation for Conditional Language Generation”. 2019. arXiv: 1908.06938.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">270.</div><div class="CitationContent" id="CR270">B. Zoph et al. “Designing Effective Sparse Expert Models”. 2022. arXiv: 2202.08906.</div></li></ol></div></aside></div></div></body></html>