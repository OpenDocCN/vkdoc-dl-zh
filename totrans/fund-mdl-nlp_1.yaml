- en: '© The Author(s) 2023G. Paaß, S. GiesselbachFoundation Models for Natural Language
    ProcessingArtificial Intelligence: Foundations, Theory, and Algorithms[https://doi.org/10.1007/978-3-031-23190-2_2](https://doi.org/10.1007/978-3-031-23190-2_2)'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: © 作者(们) 2023G. Paaß, S. Giesselbach自然语言处理基础模型人工智能：基础、理论和算法[https://doi.org/10.1007/978-3-031-23190-2_2](https://doi.org/10.1007/978-3-031-23190-2_2)
- en: 2. Pre-trained Language Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 预训练语言模型
- en: Gerhard Paaß^([1](#Aff5)  ) and Sven Giesselbach^([1](#Aff5))(1)Knowledge Discovery
    Department, Team NLU, Fraunhofer Institute for Intelligent Analysis and Information
    Systems (IAIS), Sankt Augustin, Nordrhein-Westfalen, Germany
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Gerhard Paaß^([1](#Aff5)  ) 和 Sven Giesselbach^([1](#Aff5))(1)知识发现部门，NLU团队，弗劳恩霍夫智能分析和信息系统研究所（IAIS），圣奥古斯丁，北莱茵-威斯特法伦，德国
- en: Abstract
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This chapter presents the main architecture types of attention-based language
    models, which describe the distribution of tokens in texts: Autoencoders similar
    to BERT receive an input text and produce a contextual embedding for each token.
    Autoregressive language models similar to GPT receive a subsequence of tokens
    as input. They produce a contextual embedding for each token and predict the next
    token. In this way, all tokens of a text can successively be generated. Transformer
    Encoder-Decoders have the task to translate an input sequence to another sequence,
    e.g. for language translation. First they generate a contextual embedding for
    each input token by an autoencoder. Then these embeddings are used as input to
    an autoregressive language model, which sequentially generates the output sequence
    tokens. These models are usually pre-trained on a large general training set and
    often fine-tuned for a specific task. Therefore, they are collectively called
    Pre-trained Language Models (PLM). When the number of parameters of these models
    gets large, they often can be instructed by prompts and are called Foundation
    Models. In further sections we described details on optimization and regularization
    methods used for training. Finally, we analyze the uncertainty of model predictions
    and how predictions may be explained.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了基于注意力的语言模型的主要架构类型，这些模型描述了文本中标记的分布：类似于BERT的自动编码器接收输入文本并为每个标记生成上下文嵌入。类似于GPT的自回归语言模型接收输入文本的子序列作为输入。它们为每个标记生成上下文嵌入并预测下一个标记。这样，可以依次生成文本中的所有标记。这些模型通常在大规模通用训练集上预训练，并且经常针对特定任务进行微调。因此，它们被统称为预训练语言模型（PLM）。当这些模型的参数数量很大时，它们通常可以通过提示来指导，被称为基础模型。在接下来的章节中，我们描述了用于训练的优化和正则化方法的细节。最后，我们分析了模型预测的不确定性以及预测可能如何被解释。
- en: 'KeywordsBERTLanguage modelGPT-2TransformerPre-trainingFine-tuningSequence-to-sequence
    modelA model that either computes the joint probability or the conditional probability
    of natural language texts is called a *language model* as it potentially covers
    all information about the language. In this chapter, we present the main architecture
    types of attention-based *language models* (*LMs*), which process texts consisting
    of sequences of *tokens*, i.e. words, numbers, punctuation, etc.:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词BERT语言模型GPT-2Transformer预训练微调序列到序列模型一个计算自然语言文本的联合概率或条件概率的模型被称为*语言模型*，因为它可能涵盖了关于语言的所有信息。在本章中，我们介绍了基于注意力的*语言模型*（*LMs*）的主要架构类型，这些模型处理由*标记*（即单词、数字、标点符号等）组成的文本序列：
- en: '*Autoencoders* (*AE*) receive an input text and produce a contextual embedding
    for each token. These models are also called *BERT models* and are described in
    Sect. [2.1](#Sec1).'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动编码器* (*AE*) 接收输入文本并为每个标记生成上下文嵌入。这些模型也被称为*BERT模型*，在[2.1](#Sec1)节中进行了描述。'
- en: '*Autoregressive**language models* (*AR*) receive a subsequence *v*[1], …, *v*[*t*−1]
    of tokens of the input text. They generate contextual embeddings for each token
    and use them to predict the next token *v*[*t*]. In this way, they can successively
    predict all tokens of the sequence. These models are also called *GPT models*
    and are outlined in Sect. [2.2](#Sec11).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自回归*语言模型 (*AR*) 接收输入文本的子序列 *v*[1]，…，*v*[*t*−1]。它们为每个标记生成上下文嵌入并使用它们来预测下一个标记
    *v*[*t*]。这样，它们可以依次预测序列中的所有标记。这些模型也被称为*GPT模型*，在[2.2](#Sec11)节中进行了概述。'
- en: '*Transformer Encoder-Decoders* have the task to translate an input sequence
    in to another sequence, e.g. for language translation. First they generate a contextual
    embedding for each input token by an autoencoder. Then these embeddings are used
    as input to an autoregressive language model, which sequentially generates the
    output sequence tokens. These models are also called *Transformers* and are defined
    in Sect. [2.3](#Sec19).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformer编码器-解码器**的任务是将输入序列转换为另一个序列，例如进行语言翻译。首先，它们通过自动编码器为每个输入标记生成上下文嵌入。然后，这些嵌入被用作自回归语言模型的输入，该模型按顺序生成输出序列标记。这些模型也被称为**Transformers**，并在Sect.
    [2.3](#Sec19)中定义。'
- en: In this chapter, we focus on NLP, where we consider sequences of text tokens.
    Historically, the transformer encoder-decoder was developed in 2017 by Vaswani
    et al. [[141](#CR141)] to perform translation of text into another language. The
    *autoencoder* [[39](#CR39)] and the *autoregressive language model* [[118](#CR118)]
    are the encoder-part and the decoder-part of this transformer encoder-decoder
    and were proposed later. As they are conceptually simpler, they are introduced
    in preceding sections. A final section (Sect. [2.4](#Sec27)) describes methods
    for optimizing models during training, determining a model architecture, and estimating
    the uncertainty of model predictions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于NLP，其中我们考虑文本标记的序列。历史上，Transformer编码器-解码器是由Vaswani等人于2017年[[141](#CR141)]开发的，用于将文本翻译成另一种语言。**自动编码器**[[39](#CR39)]和**自回归语言模型**[[118](#CR118)]是此Transformer编码器-解码器的编码部分和解码部分，后来被提出。由于它们在概念上更简单，它们在前面的章节中介绍。最后一节（Sect.
    [2.4](#Sec27)）描述了在训练过程中优化模型、确定模型架构和估计模型预测不确定性的方法。
- en: It turned out that the models can first be trained on a large training set of
    general text documents and are able to acquire the distribution of tokens in correct
    and fluent language. Subsequently, they can be adapted to a specific task, e.g.
    by fine-tuning with a small supervised classification task. Therefore, the models
    are called *Pre-trained Language models*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，模型可以先在大规模通用文本文档训练集上训练，并能够获得正确流畅语言中标记的分布。随后，它们可以被调整到特定任务，例如通过使用小规模监督分类任务进行微调。因此，这些模型被称为**预训练语言模型**。
- en: As we will see later, all models can be applied to arbitrary sequences, e.g.
    musical notes, sound, speech, images, or even videos. When the number of parameters
    of these models gets large, they often can be instructed by prompts and are called
    *Foundation Models*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们稍后所见，所有模型都可以应用于任意序列，例如音乐音符、声音、语音、图像，甚至视频。当这些模型的参数数量变得很大时，它们通常可以通过提示来指导，被称为**基础模型**。
- en: '2.1 BERT: Self-Attention and Contextual Embeddings'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 BERT：自注意力与上下文嵌入
- en: Common words often have a large number of different meanings. For the word *“bank”*,
    for instance, the lexical database WordNet [[94](#CR94)] lists 18 different senses
    from *“sloping land”* to *“financial institution”*. In a simple embedding of the
    word *“bank”* introduced in Sect. [1.​5](528393_1_En_1_Chapter.xhtml#Sec5) all
    these meanings are conflated. As a consequence, the interpretation of text based
    on these embeddings is flawed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 常用词汇通常有多种不同的含义。例如，对于单词**“bank”**，词汇数据库WordNet [[94](#CR94)] 列出了从**“斜坡土地”**到**“金融机构”**的18种不同含义。在Sect.
    [1.5](528393_1_En_1_Chapter.xhtml#Sec5)中引入的单词**“bank”**的简单嵌入中，所有这些含义都被合并了。因此，基于这些嵌入的文本解释是有缺陷的。
- en: As an alternative, *contextual embeddings* or contextualized embeddings were
    developed, where the details of a word embedding depend on the word itself as
    well as on the neighboring words occurring in the specific document. Consequently,
    each occurrence of the same word in the text has a different embedding depending
    on the context. Starting with the Transformer [[141](#CR141)], a number of approaches
    have been designed to generate these contextual embeddings, which are generally
    trained in an unsupervised manner using a large corpus of documents.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种替代方案，开发了**上下文嵌入**或上下文化嵌入，其中单词嵌入的细节取决于单词本身以及特定文档中出现的邻近单词。因此，文本中相同单词的每次出现都有不同的嵌入，这取决于上下文。从Transformer
    [[141](#CR141)] 开始，已经设计了许多方法来生成这些上下文嵌入，这些嵌入通常使用大量文档语料库以无监督的方式进行训练。
- en: '**BERT** (Bidirectional Encoder Representations from Transformers) was proposed
    by Devlin et al. [[39](#CR39)] and is the most important approach for generating
    contextual embeddings. BERT is based on the concept of attention [[8](#CR8)] and
    on prior work by Vaswani et al. [[141](#CR141)]. The notion of **attention** is
    inspired by a brain mechanism that tends to focus on distinctive parts of memory
    when processing large amounts of information. The details of the computations
    are explained by Rush [[126](#CR126)].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT**（来自 Transformer 的双向编码器表示）由 Devlin 等人提出 [[39](#CR39)]，是生成上下文嵌入最重要的方法。BERT
    基于注意力的概念 [[8](#CR8)] 和 Vaswani 等人的先前工作 [[141](#CR141)]。**注意力**的概念受到大脑机制的影响，当处理大量信息时，大脑机制倾向于关注记忆中的独特部分。计算细节由
    Rush [[126](#CR126)] 解释。'
- en: 2.1.1 BERT Input Embeddings and Self-Attention
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 BERT 输入嵌入和自注意力
- en: As input BERT takes some text which is converted to tokens, e.g. by the Wordpiece
    tokenizer (Sect. [1.​2](528393_1_En_1_Chapter.xhtml#Sec2)) with a vocabulary of
    a selected size, e.g. 30,000\. This means that frequent words like *“dog”* are
    represented by a token of their own, but more rare words like *“playing”* are
    split into several tokens, e.g. *“play”* and *“##ing”*, where *“##”* indicates
    that the token is part of a word. As all characters are retained as tokens, arbitrary
    words may be represented by a few tokens. In addition, there are special tokens
    like *[CLS]* at the first position of the input text and two *“[SEP]”* tokens
    marking the end of text segments. Finally, during training, there are *[MASK]*
    tokens as explained later. Each token is represented by a *token embedding*, a
    vector of fixed length *d*[*emb*], e.g. *d*[*emb*] = 768\. Input sequences of
    variable length are padded to the maximal length with a special padding token.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为输入，BERT 接收一些文本，这些文本被转换为标记，例如通过 Wordpiece 标记化器（第 [1.2](528393_1_En_1_Chapter.xhtml#Sec2)
    节）进行转换，具有选定大小的词汇表，例如 30,000。这意味着像 *“狗”* 这样频繁的单词由它们自己的标记表示，但像 *“玩耍”* 这样更罕见的单词被拆分为几个标记，例如
    *“play”* 和 *“##ing”*，其中 *“##”* 表示该标记是单词的一部分。由于所有字符都保留为标记，任意单词可能由几个标记表示。此外，还有一些特殊标记，如输入文本的第一个位置的
    *[CLS]* 标记和两个 *“[SEP]”* 标记，用于标记文本段的结束。最后，在训练过程中，有 *（此处应有内容）* 标记，如后文所述。每个标记由一个
    *标记嵌入* 表示，这是一个长度为 *d*[*emb*] 的向量，例如 *d*[*emb*] = 768。可变长度的输入序列通过特殊填充标记填充到最大长度。
- en: Since all token embeddings are processed simultaneously, the tokens need an
    indication of their position in the input text. Therefore, each position is marked
    with *position embeddings* of the same length as the token embeddings, which encode
    the position index. The BERT paper encodes the position number by trainable embeddings,
    which are added to the input token embeddings [[39](#CR39)]. Finally, BERT compares
    the first and second input segment. Therefore, the algorithm needs the information,
    which token belongs to the first and second segment. This is also encoded by a
    trainable segment embedding added to the token and position embedding. The sum
    of all embeddings is used as *input embedding* for BERT. An example is shown in
    Fig. [2.1](#Fig1).![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig1_HTML.png)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有标记嵌入都是同时处理的，因此标记需要指示它们在输入文本中的位置。因此，每个位置都标记有与标记嵌入相同长度的 *位置嵌入*，它编码位置索引。BERT
    论文通过可训练嵌入编码位置编号，这些嵌入被添加到输入标记嵌入 [[39](#CR39)]。最后，BERT 比较第一个和第二个输入段。因此，算法需要知道哪个标记属于第一个和第二个段。这也通过添加到标记和位置嵌入的可训练段嵌入进行编码。所有嵌入的总和被用作
    BERT 的 *输入嵌入*。一个示例在图 [2.1](#Fig1) 中显示。![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig1_HTML.png)
- en: A table has 3 rows of position embeddings, segment embeddings, and token embeddings
    x subscript t for the input tokens v subscript t with their respective rows of
    data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个表格有 3 行位置嵌入、段嵌入和输入标记的标记嵌入 x 下标 t，以及它们各自的数据行。
- en: Fig. 2.1
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1
- en: The input of the BERT model consist of a sequence of embeddings corresponding
    to the input tokens. Each token is represented by a sum consisting of the embedding
    of the token text, the embedding of its segment indicator and an embedding of
    its position [[39](#CR39)]
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 模型的输入由对应于输入标记的嵌入序列组成。每个标记由一个总和表示，包括标记文本的嵌入、其段指示符的嵌入以及其位置的嵌入 [[39](#CR39)]
- en: Self-Attention to Generate Contextual Embeddings
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自注意力生成上下文嵌入
- en: BERT starts with input embeddings ***x***[*t*] of length *d*[*emb*] for each
    token *v*[*t*] of the input sequence *v*[1], …, *v*[*T*]. These embeddings are
    transformed by linear mappings to so-called *query-vectors****q***[*t*], *key-vectors****k***[*t*]
    and *value-vectors****v***[*t*]. These are computed by multiplying ***x***[*t*]
    with the matrices ***W***^((*q*)), ***W***^((*k*)), and ***W***^((*v*)) with dimensions
    *d*[*emb*] × *d*[*q*], *d*[*emb*] × *d*[*q*] and *d*[*emb*] × *d*[*v*] respectively![$$\displaystyle
    \begin{aligned} \boldsymbol{q}_t^\intercal={\boldsymbol{x}}_t^\intercal {\boldsymbol{W}}^{(q)}
    \qquad  \boldsymbol{k}_t^\intercal = {\boldsymbol{x}}_t^\intercal {\boldsymbol{W}}^{(k)}
    \qquad  {\boldsymbol{v}}_t^\intercal={\boldsymbol{x}}_t^\intercal {\boldsymbol{W}}^{(v)}.
    {} \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ1.png)(2.1)Note
    that the query- and key-vectors have the same length. Then scalar products ![$$\boldsymbol
    {q}^\intercal _r\boldsymbol {k}_t$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq1.png)
    between the query-vector ***q***[*r*] of a target token *v*[*r*] and the key-vectors
    ***k***[*t*] of all tokens of the sequence are computed:![$$\displaystyle \begin{aligned}
    (\alpha_{r,1},\ldots,\alpha_{r,T})=\operatorname{\mathrm{softmax}}\left( \frac{\boldsymbol{q}^\intercal_r\boldsymbol{k}_1}{\sqrt{d_k}},\ldots,
    \frac{\boldsymbol{q}^\intercal_r\boldsymbol{k}_T}{\sqrt{d_k}}\right). {} \end{aligned}
    $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ2.png)(2.2)Each
    scalar product yields a real-valued *association score*![$$(\boldsymbol {q}^\intercal
    _r\boldsymbol {k}_t)/\sqrt {d_k}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq2.png)
    between the tokens, which depends on the matrices ***W***^((*q*)) and ***W***^((*k*)).
    This association score is called *scaled dot-product attention*. It is normalized
    to a probability score *α*[*r*,*t*] by the softmax function. The factor ![$$1/\sqrt
    {d_k}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq3.png) avoids
    large values, where the softmax function has only tiny gradients. With these weights
    a weighted average of the value vectors ***v***[*t*] of all sequence elements
    is formed yielding the new embedding ***x***̆[*r*] of length *d*[*v*] for the
    target token *v*[*r*]:![$$\displaystyle \begin{aligned} \breve{{\boldsymbol{x}}}_r
    = \alpha_{r,1}*{\boldsymbol{v}}_1+\cdots+\alpha_{r,T}*{\boldsymbol{v}}_T {}. \end{aligned}
    $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ3.png)(2.3)This
    algorithm is called *self-attention* and was first proposed by Vaswani et al.
    [[141](#CR141)]. Figure [2.2](#Fig2) shows the computations for the *r*-th token
    *“mouse”*. Note that the resulting embedding is a *contextual embedding* as it
    includes information about all words in the input text. A component of ***v***[*t*]
    gets a high weight whenever the scalar product ![$$\boldsymbol {q}^\intercal _r\boldsymbol
    {k}_t$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq4.png) is
    large. It measures a specific form of a correlation between ***x***[*r*] and ***x***[*t*]
    and is maximal if the vector ![$${\boldsymbol {x}}_r^\intercal {\boldsymbol {W}}^{(q)}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq5.png)
    points in the same direction as ![$${\boldsymbol {x}}_t^\intercal {\boldsymbol
    {W}}^{(k)}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq6.png).![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig2_HTML.png)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 以输入序列 *v*[1], …, *v*[*T*] 中每个标记 *v*[*t*] 的长度为 *d*[*emb*] 的输入嵌入 ***x***[*t*]
    开始。这些嵌入通过线性映射转换为所谓的 *查询向量***q***[*t*]，*键向量***k***[*t*] 和 *值向量***v***[*t*]。这些向量通过将
    ***x***[*t*] 与矩阵 ***W***^((*q*)), ***W***^((*k*)), 和 ***W***^((*v*)) 相乘得到，这些矩阵的维度分别为
    *d*[*emb*] × *d*[*q*]，*d*[*emb*] × *d*[*q*] 和 *d*[*emb*] × *d*[*v*]！[$$\displaystyle
    \begin{aligned} \boldsymbol{q}_t^\intercal={\boldsymbol{x}}_t^\intercal {\boldsymbol{W}}^{(q)}
    \qquad  \boldsymbol{k}_t^\intercal = {\boldsymbol{x}}_t^\intercal {\boldsymbol{W}}^{(k)}
    \qquad  {\boldsymbol{v}}_t^\intercal={\boldsymbol{x}}_t^\intercal {\boldsymbol{W}}^{(v)}.
    {} \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ1.png)(2.1)注意查询向量和键向量具有相同的长度。然后计算目标标记
    *v*[*r*] 的查询向量 ***q***[*r*] 与序列中所有标记的键向量 ***k***[*t*] 之间的标量积 ![$$\boldsymbol {q}^\intercal
    _r\boldsymbol {k}_t$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq1.png)！[$$\displaystyle
    \begin{aligned} (\alpha_{r,1},\ldots,\alpha_{r,T})=\operatorname{\mathrm{softmax}}\left(
    \frac{\boldsymbol{q}^\intercal_r\boldsymbol{k}_1}{\sqrt{d_k}},\ldots, \frac{\boldsymbol{q}^\intercal_r\boldsymbol{k}_T}{\sqrt{d_k}}\right).
    {} \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ2.png)(2.2)每个标量积产生一个实值
    *关联分数*![$$(\boldsymbol {q}^\intercal _r\boldsymbol {k}_t)/\sqrt {d_k}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq2.png)，它取决于矩阵
    ***W***^((*q*)) 和 ***W***^((*k*))。这个关联分数被称为 *缩放点积注意力*。通过 softmax 函数将其归一化到概率分数
    *α*[*r*,*t*]。因子 ![$$1/\sqrt {d_k}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq3.png)
    避免了 softmax 函数只有微小梯度的较大值。使用这些权重，对所有序列元素的价值向量 ***v***[*t*] 进行加权平均，得到目标标记 *v*[*r*]
    的新嵌入 ***x***̆[*r*]，长度为 *d*[*v*]！[$$\displaystyle \begin{aligned} \breve{{\boldsymbol{x}}}_r
    = \alpha_{r,1}*{\boldsymbol{v}}_1+\cdots+\alpha_{r,T}*{\boldsymbol{v}}_T {}. \end{aligned}
    $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ3.png)(2.3)这个算法被称为
    *自注意力*，最初由 Vaswani 等人提出 [[141](#CR141)]。图 [2.2](#Fig2) 展示了 *r*-th 标记 *“mouse”*
    的计算。注意，生成的嵌入是一个 *上下文嵌入*，因为它包含了关于输入文本中所有单词的信息。当标量积 ![$$\boldsymbol {q}^\intercal
    _r\boldsymbol {k}_t$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq4.png)
    较大时，***v***[*t*] 的一个分量会获得高权重。它衡量了 ***x***[*r*] 和 ***x***[*t*] 之间的特定相关形式，如果向量 ![$${\boldsymbol
    {x}}_r^\intercal {\boldsymbol {W}}^{(q)}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq5.png)
    和 ![$${\boldsymbol {x}}_t^\intercal {\boldsymbol {W}}^{(k)}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq6.png)
    指向同一方向，则该相关形式达到最大值。![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig2_HTML.png)
- en: A flow diagram of the input tokens the, mouse, MASK within square brackets,
    and cheese have an interconnected system of embedding. They are divided into sections
    of embedding vector, query and key and value vectors, association and probability
    scores, weighted value vectors, and new embedding.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 输入标记the、mouse、MASK（方括号内）和cheese之间有一个相互关联的嵌入系统。它们被分为嵌入向量、查询和键值向量、关联和概率分数、加权值向量和新的嵌入部分。
- en: Fig. 2.2
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2
- en: Computation of a contextual embedding for a single token *“mouse”* by self-attention.
    By including the embedding of “cheese”, the embedding of mouse can be shifted
    to the meaning of “rodent” and away from “computer pointing device”. Such an embedding
    is computed for every word of the input sequence
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自注意力计算单个标记“mouse”的上下文嵌入。通过包含“cheese”的嵌入，鼠标的嵌入可以转移到“rodent”的意义上，并远离“计算机指针”。这种嵌入为输入序列中的每个单词都进行计算。
- en: The self-attention mechanism in general is non-symmetric, as the matrices ***W***^((*q*))
    and ***W***^((*k*)) are different. If token *v*[*i*] has a high attention to token
    *v*[*j*] (i.e. ![$$\boldsymbol {q}^\intercal _i\boldsymbol {k}_j$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq7.png)
    is large), this does not necessarily mean that *v*[*j*] will highly attend to
    token *v*[*i*] (i.e. ![$$\boldsymbol {q}^\intercal _j\boldsymbol {k}_i$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq8.png)
    also is large). The influence of *v*[*i*] on the contextual embedding of *v*[*j*]
    therefore is different from the influence of *v*[*j*] on the contextual embedding
    of *v*[*i*]. Consider the following example text *“Fred gave roses to Mary”*.
    Here the word *“gave”* has different relations to the remaining words. *“Fred”*
    is the person who is performing the giving, *“roses”* are the objects been given,
    and *“Mary”* is the recipient of the given objects. Obviously these semantic role
    relations are non-symmetric. Therefore, they can be captured with the different
    matrices ***W***^((*q*)) and ***W***^((*k*)) and can be encoded in the embeddings.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制通常是非对称的，因为矩阵 ***W***^((*q*)) 和 ***W***^((*k*)) 是不同的。如果标记 *v*[*i*] 对标记
    *v*[*j*] 有高注意力（即 ![$$\boldsymbol {q}^\intercal _i\boldsymbol {k}_j$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq7.png)
    很大），这并不一定意味着 *v*[*j*] 会对标记 *v*[*i*] 有高度注意力（即 ![$$\boldsymbol {q}^\intercal _j\boldsymbol
    {k}_i$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq8.png) 也很大）。因此，*v*[*i*]
    对 *v*[*j*] 的上下文嵌入的影响与 *v*[*j*] 对 *v*[*i*] 的上下文嵌入的影响是不同的。考虑以下示例文本“Fred gave roses
    to Mary”。在这里，单词“gave”与剩余单词有不同的关系。“Fred”是执行给予的人，“roses”是被给予的对象，“Mary”是接受这些对象的人。显然，这些语义角色关系是非对称的。因此，它们可以用不同的矩阵
    ***W***^((*q*)) 和 ***W***^((*k*)) 来捕捉，并可以编码在嵌入中。
- en: Self-attention allows for shorter computation paths and provides direct avenues
    to compare distant elements in the input sequence, such as a pronoun and its antecedent
    in a sentence. The multiplicative interaction involved in attention provides a
    more flexible alternative to the inflexible fixed-weight computation of MLPs and
    CNNs by dynamically adjusting the computation to the input at hand. This is especially
    useful for language modeling, where, for instance, the sentence *“She ate the
    ice-cream with the X”* is processed. While a feed-forward network would always
    process it in the same way, an attention-based model could adapt its computation
    to the input and update the contextual embedding of the word *“ate”* if *X* is
    *“spoon”*, or update the embedding of *“ice-cream”* if *X* refers to *“strawberries”*
    [[17](#CR17)].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力允许更短的计算路径，并为直接比较输入序列中的远程元素提供了途径，例如句子中的代词及其先行词。注意力中涉及到的乘性交互为MLPs和CNNs的固定权重计算提供了更灵活的替代方案，通过动态调整计算以适应当前输入。这在语言建模中特别有用，例如，处理句子“她用X吃了冰淇淋”。虽然前馈网络始终以相同的方式处理它，但基于注意力的模型可以调整其计算以适应输入，并在X是“勺子”时更新单词“ate”的上下文嵌入，或者在X指的是“草莓”时更新“ice-cream”的嵌入
    [[17](#CR17)]。
- en: In practice all query, key, and value vectors are computed in parallel by ***Q*** = ***XW***^((*q*)),
    ***K*** = ***XW***^((*k*)), ***V***  = ***XW***^((*v*)), where ***X*** is the
    *T* × *d*[*emb*] matrix of input embeddings [[141](#CR141)]. The query-vectors
    ***q***[*t*], key-vectors ***k***[*t*] and value vectors ***v***[*t*] are the
    rows of ***Q***, ***K***, ***V*** respectively. Then the self-attention output
    matrix ATTL(X) is calculated by one large matrix expression![$$\displaystyle \begin{aligned}
    \breve{{\boldsymbol{X}}}=\text{ATTL}({\boldsymbol{X}})=\text{ATTL}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})=\operatorname{\mathrm{softmax}}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\intercal}{\sqrt{d_k}}\right)\boldsymbol{V}
    {}, \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ4.png)(2.4)resulting
    in a *T* × *d*[*v*]-matrix ***X***̆. Its *r*-th row contains the new embedding
    ***x***̆[*r*] of the *r*-th token *v*[*r*].
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，所有查询、键和值向量都是通过 ***Q*** = ***XW***^((*q*)), ***K*** = ***XW***^((*k*)),
    ***V*** = ***XW***^((*v*)) 并行计算的，其中 ***X*** 是输入嵌入的 *T* × *d*[*emb*] 矩阵 [[141](#CR141)]。查询向量
    ***q***[*t*]，键向量 ***k***[*t*] 和值向量 ***v***[*t*] 分别是 ***Q***，***K***，***V*** 的行。然后，通过一个大的矩阵表达式计算自注意力输出矩阵
    ATTL(X)！[$$\displaystyle \begin{aligned} \breve{{\boldsymbol{X}}}=\text{ATTL}({\boldsymbol{X}})=\text{ATTL}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})=\operatorname{\mathrm{softmax}}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\intercal}{\sqrt{d_k}}\right)\boldsymbol{V}
    {}, \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ4.png)(2.4)，得到一个
    *T* × *d*[*v*] 的矩阵 ***X***̆。它的 *r*-th 行包含第 *r*- 个标记 *v*[*r*] 的新嵌入 ***x***̆[*r*]。
- en: A number of alternative compatibility measures instead of the scaled dot-product
    attention ([2.2](#Equ2)) have been proposed. They are, however, rarely used in
    PLMs, as described in the surveys [[27](#CR27), [46](#CR46)].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 除了缩放点积注意力（[2.2](#Equ2)）之外，还提出了许多替代的兼容性度量方法。然而，正如调查[[27](#CR27), [46](#CR46)]所描述的，它们在PLMs中很少被使用。
- en: It turns out that a single self-attention module is not sufficient to characterize
    the tokens. Therefore, in a layer *d*[head] parallel self-attentions are computed
    with different matrices ![$${\boldsymbol {W}}^{(q)}_m$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq9.png),
    ![$${\boldsymbol {W}}^{(k)}_m$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq10.png),
    and ![$${\boldsymbol {W}}^{(v)}_m$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq11.png),
    *m* = 1, …, *d*[head], yielding partial new embeddings![$$\displaystyle \begin{aligned}
    \breve{{\boldsymbol{X}}}_m = \text{ATTL}({\boldsymbol{X}}{\boldsymbol{W}}^{(q)}_m,
    {\boldsymbol{X}}{\boldsymbol{W}}^{(k)}_m, {\boldsymbol{X}}{\boldsymbol{W}}^{(v)}_m)
    {}. \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ5.png)(2.5)The
    emerging partial embeddings ***x***̆[*m*,*t*] for a token *v*[*t*] are able to
    concentrate on complementary semantic aspects, which develop during training.The
    BERT[BASE] model has *d*[head]=12 of these parallel *attention heads*. The lengths
    of these head embeddings are only a fraction *d*[*emb*]∕*d*[head] of the original
    length *d*[*emb*]. The resulting embeddings are concatenated and multiplied with
    a (*d*[head] ∗ *d*[*v*]) × *d*[*emb*]-matrix *W*^((*o*)) yielding the matrix of
    intermediate embeddings![$$\displaystyle \begin{aligned} \breve{{\boldsymbol{X}}}
    &amp;= \left[\breve{{\boldsymbol{X}}}_1,\ldots,\breve{{\boldsymbol{X}}}_{d_{\text{head}}}\right]
    {\boldsymbol{W}}_0 {}, \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ6.png)(2.6)where
    ***W***[0] is a parameter matrix. If the length of the input embeddings is *d*[*emb*],
    the length of the query, key, and value vector is chosen as *d*[*k*] = *d*[*v*] = *d*[*emb*]∕*d*[head].
    Therefore, the concatenation again creates a *T* × *d*[*emb*] matrix ***X***̆.
    This setup is called *multi-head self-attention*. Because of the reduced dimension
    of the individual heads, the total computational cost is similar to that of a
    single-head attention with full dimensionality.Subsequently, each row of ***X***̆,
    the intermediate embedding vectors ![$$\breve {{\boldsymbol {x}}}_t^\intercal
    $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq12.png), is converted
    by a *fully connected layer*Fcl with a ReLU activation followed by another linear
    transformation [[141](#CR141)]![$$\displaystyle \begin{aligned} \tilde{{\boldsymbol{x}}}_t^\intercal
    &amp;= \text{FCL}(\breve{{\boldsymbol{x}}}_t) =ReLU(\breve{{\boldsymbol{x}}}_t^\intercal*{\boldsymbol{W}}_1+\boldsymbol{b}_1^\intercal)*{\boldsymbol{W}}_2
    + \boldsymbol{b}_2^\intercal {}. \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ7.png)(2.7)The
    matrices ***W***[0], ***W***[1], ***W***[2] and the vectors ***b***[1], ***b***[2]
    are parameters. These transformations are the same for each token *v*[*t*] of
    the sequence yielding the embedding ![$$\tilde {{\boldsymbol {x}}}_t $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq13.png).To
    improve training speed, *residual connections* are added as a “bypass”, which
    simply copy the input. They were shown to be extremely helpful for the optimization
    of multi-layer image classifiers [[54](#CR54)]. In addition, *layer normalization*
    [[6](#CR6)] is used for regularization (Sect. [2.4.2](#Sec32)), as shown in Fig.
    [2.3](#Fig3). Together the multi-head self-attention ([2.5](#Equ5)), the concatenation
    ([2.6](#Equ6)), and the fully connected layer ([2.7](#Equ7)) form an *encoder
    block*.![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig3_HTML.png)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，单个自注意力模块不足以表征标记。因此，在一个层 *d*[head] 并行自注意力中，使用不同的矩阵 ![$${\boldsymbol {W}}^{(q)}_m$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq9.png),
    ![$${\boldsymbol {W}}^{(k)}_m$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq10.png),
    和 ![$${\boldsymbol {W}}^{(v)}_m$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq11.png),
    *m* = 1, …, *d*[head]，得到部分新的嵌入 ![$$\displaystyle \begin{aligned} \breve{{\boldsymbol{X}}}_m
    = \text{ATTL}({\boldsymbol{X}}{\boldsymbol{W}}^{(q)}_m, {\boldsymbol{X}}{\boldsymbol{W}}^{(k)}_m,
    {\boldsymbol{X}}{\boldsymbol{W}}^{(v)}_m) {}. \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ5.png)(2.5)
    对于一个标记 *v*[*t*] 的出现的部分嵌入 ***x***̆[*m*,*t*] 能够专注于互补的语义方面，这些方面在训练过程中发展。BERT[BASE]模型有
    *d*[head]=12 个这样的并行 *注意力头*。这些头嵌入的长度只是原始长度 *d*[*emb*] 的一个分数 *d*[*emb*]∕*d*[head]。得到的嵌入通过一个
    (*d*[head] ∗ *d*[*v*]) × *d*[*emb*]-矩阵 *W*^((*o*)) 连接并相乘，得到中间嵌入矩阵 ![$$\displaystyle
    \begin{aligned} \breve{{\boldsymbol{X}}} &amp;= \left[\breve{{\boldsymbol{X}}}_1,\ldots,\breve{{\boldsymbol{X}}}_{d_{\text{head}}}\right]
    {\boldsymbol{W}}_0 {}, \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ6.png)(2.6)
    其中 ***W***[0] 是一个参数矩阵。如果输入嵌入的长度是 *d*[*emb*]，则查询、键和值向量的长度选择为 *d*[*k*] = *d*[*v*] = *d*[*emb*]∕*d*[head]。因此，连接再次创建一个
    *T* × *d*[*emb*] 矩阵 ***X***̆。这种设置被称为 *多头自注意力*。由于单个头的维度减少，总的计算成本与全维度的单头注意力相似。随后，***X***̆
    的每一行，即中间嵌入向量 ![$$\breve {{\boldsymbol {x}}}_t^\intercal $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq12.png)，通过一个
    *全连接层*Fcl 转换，后跟一个 ReLU 激活，然后是另一个线性变换 [[141](#CR141)]![$$\displaystyle \begin{aligned}
    \tilde{{\boldsymbol{x}}}_t^\intercal &amp;= \text{FCL}(\breve{{\boldsymbol{x}}}_t)
    =ReLU(\breve{{\boldsymbol{x}}}_t^\intercal*{\boldsymbol{W}}_1+\boldsymbol{b}_1^\intercal)*{\boldsymbol{W}}_2
    + \boldsymbol{b}_2^\intercal {}. \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ7.png)(2.7)
    矩阵 ***W***[0]，***W***[1]，***W***[2] 和向量 ***b***[1]，***b***[2] 是参数。这些变换对于序列中的每个标记
    *v*[*t*] 都是相同的，从而得到嵌入 ![$$\tilde {{\boldsymbol {x}}}_t $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq13.png)。为了提高训练速度，添加了
    *残差连接* 作为“旁路”，它简单地复制输入。它们已被证明对于多层图像分类器的优化非常有帮助 [[54](#CR54)]。此外，使用 *层归一化* [[6](#CR6)]
    进行正则化（见第 [2.4.2](#Sec32) 节），如图 [2.3](#Fig3) 所示。多头自注意力 ([2.5](#Equ5))、连接 ([2.6](#Equ6))
    和全连接层 ([2.7](#Equ7)) 一起形成一个 *编码块*。![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig3_HTML.png)
- en: A flow diagram of the inputs the, mouse, MASK within square brackets, and cheese
    have an interconnected system of embedding divided into sections of input embeddings,
    self-attention, concatenation of partial embeddings, feed-forward and non-linearity,
    and output embedding via residual connections.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 输入the、mouse、方括号内的MASK和cheese的流程图显示了划分为输入嵌入、自注意力、部分嵌入的连接、前馈和非线性和通过残差连接的输出嵌入的相互连接的嵌入系统。
- en: Fig. 2.3
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3
- en: Multi-head self-attention computes self-attentions for each layer *l* and head
    *m* with different matrices ![$${ \boldsymbol {W}}^{(q)}_{l,m}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq14.png),
    ![$${ \boldsymbol {W}}^{(k)}_{l,m}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq15.png),
    and ![$${ \boldsymbol {W}}^{(v)}_{l,m}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq16.png).
    In this way, different aspects of the association between token pairs, e.g. “mouse”
    and “cheese”, can be computed. The resulting embeddings are concatenated and transformed
    by a feedforward network. In addition, residual connections and layer normalization
    improve training convergence [[39](#CR39)] This procedure is repeated for a number
    of *k* layers with different encoder blocks, using the output embeddings of one
    block as input embeddings of the next block. This setup is shown in Fig. [2.4](#Fig4).
    The embeddings ![$$\tilde {{\boldsymbol {x}}}_{k,t}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq17.png)
    of the last encoder block provides the desired contextual embeddings. The structure
    of an encoder block overcomes the limitations of RNNs (namely the sequential nature
    of RNNs) by allowing each token in the input sequence to directly determine associations
    with every other token in the sequence. BERT[BASE] has *k*=12 encoder blocks.
    It was developed at Google by Devlin et al. [[39](#CR39)]. More details on the
    implementation of self-attention can be found in these papers [[38](#CR38), [41](#CR41),
    [126](#CR126)].![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig4_HTML.png)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 多头自注意力计算每个层*l*和头*m*的自注意力，使用不同的矩阵 ![$${ \boldsymbol {W}}^{(q)}_{l,m}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq14.png),
    ![$${ \boldsymbol {W}}^{(k)}_{l,m}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq15.png),
    和 ![$${ \boldsymbol {W}}^{(v)}_{l,m}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq16.png)。这样，可以计算标记对之间的不同方面的关联，例如“mouse”和“cheese”。结果嵌入通过前馈网络连接和转换。此外，残差连接和层归一化提高了训练的收敛性
    [[39](#CR39)]。这个过程在多个*k*层中重复，使用一个块的输出嵌入作为下一个块的输入嵌入。这种设置如图[2.4](#Fig4)所示。最后一个编码块的嵌入![$$\tilde
    {{\boldsymbol {x}}}_{k,t}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq17.png)提供了所需的上下文嵌入。编码器块的结构通过允许输入序列中的每个标记直接确定与序列中其他标记的关联，克服了RNN（即RNN的顺序性质）的限制。BERT[BASE]有*k*=12个编码器块。它由Devlin等人开发于Google
    [[39](#CR39)]。关于自注意力的实现细节，可以在这些论文中找到 [[38](#CR38), [41](#CR41), [126](#CR126)]。![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig4_HTML.png)
- en: A flow diagram of the input tokens the, mouse, MASK within square brackets,
    and cheese have an interconnected system of embedding. They are divided into sections
    of input embedding, parallel self-attention, embedding vector, fully connected
    layer, and target word, among others via encoder blocks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 输入标记the、mouse、方括号内的MASK和cheese之间有一个相互连接的嵌入系统。它们被划分为输入嵌入、并行自注意力、嵌入向量、全连接层和目标词等部分，通过编码器块实现。
- en: Fig. 2.4
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4
- en: Parallel computation of contextual embeddings in each encoder block by BERT.
    The output embeddings of an encoder block are used as input embeddings of the
    next encoder block. Finally, masked tokens are predicted by a logistic classifier
    *L* using the corresponding contextual embedding of the last encoder block as
    input
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: BERT在每个编码器块中通过并行计算上下文嵌入。编码器块的输出嵌入被用作下一个编码器的输入嵌入。最后，使用最后一个编码块的对应上下文嵌入作为输入，由逻辑分类器*L*预测掩码标记。
- en: 2.1.2 Training BERT by Predicting Masked Tokens
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 通过预测掩码标记训练BERT
- en: The BERT model has a large number of unknown parameters. These parameters are
    trained in a two-step procedure.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型有大量的未知参数。这些参数通过两步程序进行训练。
- en: '*Pre-training* enables the model to acquire general knowledge about language
    in an unsupervised way. The model has the task to fill in missing words in a text.
    As no manual annotation is required, pre-training can use large text corpora.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预训练*使模型能够以无监督的方式获取关于语言的一般知识。模型的任务是填充文本中的缺失单词。由于不需要手动标注，预训练可以使用大量的文本语料库。'
- en: '*Fine-tuning* adjusts the pre-trained model to a specific task, e.g. sentiment
    analysis. Here, the model parameters are adapted to solve this task using a smaller
    labeled training dataset.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*微调* 将预训练模型调整到特定任务，例如情感分析。在这里，模型参数通过使用较小的标记训练数据集来适应解决这个任务。'
- en: 'The performance on the fine-tuning task is much better than without pre-training
    because the model can use the knowledge acquired during pre-training through *transfer
    learning*.To pre-train the model parameters, a training task is designed: the
    *masked language model* (*MLM*). Roughly 15% of the input tokens in the training
    documents are selected for prediction, which is performed by a logistic classifier
    (Sect. [1.​3](528393_1_En_1_Chapter.xhtml#Sec3))![$$\displaystyle \begin{aligned}
    p(V_t|v_1,\ldots,v_{t-1},v_{t+1}\ldots,v_T)=\operatorname{\mathrm{softmax}}(A\tilde{{\boldsymbol{x}}}_{k,t}+\boldsymbol{b})
    {}, \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ8.png)(2.8)receiving
    the embedding ![$$\tilde {{\boldsymbol {x}}}_{k,t}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq18.png)
    of the last layer at position *t* as input to predict the random variable *V*[*t*]
    of possible tokens at position *t*. This approach avoids cycles where words can
    indirectly “see themselves”.The tokens to be predicted have to be changed, as
    otherwise the prediction would be trivial. Therefore, a token selected for prediction
    is replaced by:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调任务上的表现比没有预训练的要好得多，因为模型可以通过**迁移学习**使用预训练期间获得的知识。为了预训练模型参数，设计了一个训练任务：**掩码语言模型**（MLM）。大约15%的训练文档中的输入标记被选中进行预测，这通过一个逻辑分类器（见第[1.3](528393_1_En_1_Chapter.xhtml#Sec3)节）来完成！
- en: a special *[MASK]* token for 80% of the time (e.g., *“the mouse likes cheese”*
    becomes *“the mouse [MASK] cheese”*);
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 80% 的时间里使用一个特殊的 *斜体* 标记（例如，“the mouse likes cheese” 变成 “the mouse *
- en: a random token for 10% of the time (e.g., *“the mouse likes cheese”* becomes
    *“the mouse absent cheese”*);
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 10% 的时间内使用随机令牌（例如，*“老鼠喜欢奶酪”* 变成 *“老鼠没有奶酪”*）；
- en: the unchanged label token for 10% of the time (e.g., *“the mouse likes cheese”*
    becomes *“the mouse likes cheese”*).
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 10% 的时间内保持标签标记不变（例如，*“the mouse likes cheese”* 变成 *“the mouse likes cheese”*）。
- en: The second and third variants were introduced, as there is a discrepancy between
    pre-training and the subsequent fine-tuning, were there is no *[MASK]* token.
    The authors mitigate this issue by occasionally replacing *[MASK]* with the original
    token, or by sampling from the vocabulary. Note that in 1.5% of the cases a random
    token is inserted. This occasional noise encourages BERT to be less biased towards
    the masked token (especially when the label token remains unchanged) in its bidirectional
    context encoding. To predict the masked token, BERT has to concentrate all knowledge
    about this token in the corresponding output embedding of the last layer, which
    is the input to the logistic classifier. Therefore, it is often called an *autoencoder*,
    which generates extremely rich output embeddings.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种和第三种变体被引入，因为预训练和后续微调之间存在差异，尤其是在没有 *
- en: In addition to predicting the masked tokens, BERT also has to predict, whether
    the next sentence is a randomly chosen sentence or the actual following sentence
    (*next sentence prediction*). This requires BERT to consider the relation between
    two consecutive pieces of text. Again a logistic classifier receiving the embedding
    of the first *[CLS]* token is used for this classification. However, this task
    did not have a major impact on BERT’s performance, as BERT simply learned if the
    topics of both sentences are similar [[158](#CR158)].
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了预测掩码标记外，BERT还必须预测下一个句子是随机选择的句子还是实际的后续句子（*下一个句子预测*）。这要求BERT考虑连续两段文本之间的关系。再次使用接收第一个*[CLS]*标记嵌入的逻辑分类器进行此分类。然而，这项任务对BERT的性能影响不大，因为BERT只是学会了两个句子的主题是否相似
    [[158](#CR158)]。
- en: In Fig. [2.4](#Fig4) the task is to predict a high probability of the token
    *“likes”* for the input text *“The mouse [MASK] cheese”*. At the beginning of
    the training this probability will be very small (≈ 1∕no. of tokens). By backpropagation
    for each unknown parameter the derivative can be determined, indicating how the
    parameters should be changed to increase the probability of *“likes”*. The unknown
    parameters of BERT comprise the input embeddings for each token of the vocabulary,
    the position embeddings for each position, matrices![$${\boldsymbol {W}}^{(q)}_{l,m}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq19.png),
    ![$${\boldsymbol {W}}^{(k)}_{l,m}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq20.png),
    ![$${\boldsymbol {W}}^{(v)}_{l,m}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq21.png)
    for each layer *l* and attention head *m* ([2.4](#Equ4)), the parameters of the
    fully connected layers ([2.7](#Equ7)) as well as *A*, ***b*** of the logistic
    classifier ([2.8](#Equ8)). BERT uses the Adam algorithm [[69](#CR69)] for stochastic
    gradient descent.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[2.4](#Fig4)中，任务是预测输入文本“鼠标奶酪”中标记“likes”的高概率。在训练开始时，这个概率将非常小（≈1/标记数量）。通过反向传播为每个未知参数确定导数，指示参数应该如何改变以增加“likes”的概率。BERT的未知参数包括词汇表中每个标记的输入嵌入，每个位置的嵌入，每个层*l*和注意力头*m*的矩阵![$${\boldsymbol
    {W}}^{(q)}_{l,m}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq19.png)，![$${\boldsymbol
    {W}}^{(k)}_{l,m}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq20.png)，![$${\boldsymbol
    {W}}^{(v)}_{l,m}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq21.png)（[2.4](#Equ4)），全连接层的参数
    ([2.7](#Equ7)) 以及逻辑分类器的*A*，***b***（[2.8](#Equ8)）。BERT使用Adam算法 [[69](#CR69)] 进行随机梯度下降。
- en: The BERT[BASE] model has a hidden size of *d*[*emb*]=768, *k*=12 encoder blocks
    each with *d*[head]=12 attention heads, and a total of 110 million parameters.
    The BERT[LARGE] model has a hidden size of *d*[*emb*]=1024, and *k*=24 encoder
    blocks each with *d*[head]=16 attention heads and a total of 340 million parameters
    [[39](#CR39)]. The English Wikipedia and a book corpus with 3.3 billion words
    were encoded by the WordPiece tokenizer [[154](#CR154)] with a vocabulary of 30,000
    tokens and used to pre-train BERT. No annotations of the texts by humans were
    required, so the training is self-supervised. The pre-training took 4 days on
    64 TPU chips, which are very fast GPU chips allowing parallel processing. Fine-tuning
    can be done on a single Graphical Processing Unit (GPU).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: BERT[BASE]模型具有*隐藏大小*[*emb*]=768，*k*=12个编码块，每个编码块包含*head*[*d*]=12个注意力头，总共有1100万个参数。BERT[LARGE]模型具有*隐藏大小*[*emb*]=1024，*k*=24个编码块，每个编码块包含*head*[*d*]=16个注意力头，总共有3.4亿个参数
    [[39](#CR39)]。英语维基百科和包含33亿个单词的书籍语料库由WordPiece分词器 [[154](#CR154)] 编码，词汇量为30,000个标记，并用于预训练BERT。不需要人类对文本进行标注，因此训练是自监督的。在64个TPU芯片上预训练耗时4天，这些TPU芯片是非常快速的GPU芯片，允许并行处理。微调可以在单个图形处理单元（GPU）上完成。
- en: 'To predict the masked tokens, the model has to learn many types of language
    understanding features: syntax (*[MASK]* is a good position for a verb), semantics
    (e.g. the mouse prefers cheese), pragmatics, coreference, etc. Note that the computations
    can be processed in parallel for each token of the input sequence, eliminating
    the sequential dependency in Recurrent Neural Networks. This parallelism enables
    BERT and related models to leverage the full power of modern SIMD (single instruction
    multiple data) hardware accelerators like GPUs/TPUs, thereby facilitating training
    of NLP models on datasets of unprecedented size. Reconstructing missing tokens
    in a sentence has long been used in psychology. Therefore, predicting masked tokens
    is also called a *cloze task* from ‘closure’ in Gestalt theory (a school of psychology).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测掩码标记，模型必须学习许多类型的语言理解特征：语法（*is* 是动词的好位置），语义（例如，老鼠喜欢奶酪），语用学，指代等。请注意，计算可以并行处理输入序列的每个标记，消除了循环神经网络中的序列依赖性。这种并行性使得
    BERT 和相关模型能够利用现代 SIMD（单指令多数据）硬件加速器（如 GPU/TPU）的全部能力，从而促进了在前所未有的数据集上训练 NLP 模型。在心理学中，重建句子中缺失的标记已经使用了很长时间。因此，预测掩码标记也被称为来自格式塔理论（一个心理学流派）中的“闭合”的
    *闭合任务*。
- en: It turns out that BERT achieves excellent results for the prediction of the
    masked tokens, and that additional encoder blocks markedly increase the accuracy.
    For example, BERT is able to predict the original words (or parts of words) with
    an accuracy of 45.9%, although in many cases several values are valid at the target
    position [[125](#CR125)]. In contrast to conventional language models, the MLM
    takes into account the tokens before and after the masked target token. Hence,
    it is called a *bidirectional encoder*. In addition, self-attention directly provides
    the relation to distant tokens without recurrent model application. Finally, self-attention
    is fast, as it can be computed in parallel for all input tokens of an encoder
    block.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，BERT 在预测掩码标记方面取得了优异的成绩，并且额外的编码器块显著提高了准确率。例如，BERT 能够以 45.9% 的准确率预测原始单词（或单词的部分），尽管在许多情况下，目标位置上有几个值是有效的
    [[125](#CR125)]。与传统的语言模型相比，MLM 考虑了掩码目标标记之前和之后的标记。因此，它被称为 *双向编码器*。此外，自注意力直接提供了与远程标记的关系，而不需要应用循环模型。最后，自注意力速度快，因为它可以并行计算编码器块的所有输入标记。
- en: 2.1.3 Fine-Tuning BERT to Downstream Tasks
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 将 BERT 微调到下游任务
- en: Neural networks have already been pre-trained many years ago [[16](#CR16)],
    but the success of pre-training has become more evident in recent years. During
    pre-training BERT learns general syntactic and semantic properties of the language.
    This can be exploited for a special training task during subsequent *fine-tuning*
    with a modified training task. This approach is also called *transfer learning*
    as the knowledge acquired during pre-training is transferred to a related application.
    In contrast to other models, BERT requires minimal architecture changes for a
    wide range of natural language processing tasks. At the time of its publication,
    BERT improved the Sota on various natural language processing tasks.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络已经在多年前进行了预训练 [[16](#CR16)]，但预训练的成功在近年来变得更加明显。在预训练期间，BERT 学习了语言的通用语法和语义属性。这可以在后续的
    *微调* 期间用于特殊训练任务。这种方法也被称为 *迁移学习*，因为预训练期间获得的知识被转移到相关应用中。与其它模型相比，BERT 对于广泛的自然语言处理任务只需要最小的架构更改。在其发布时，BERT
    在各种自然语言处理任务上提高了 Sota（最先进的技术）。
- en: Usually, a fine-tuning task requires a classification, solved by applying a
    logistic classifier *L* to the output embedding ![$$\tilde {{\boldsymbol {x}}}_{k,1}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq22.png)
    of the *[CLS]* token at position 1 of BERT’s last encoder block. There are different
    types of fine-tuning tasks, as shown in Fig. [2.5](#Fig5).![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig5_HTML.png)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，微调任务需要一个分类，通过将逻辑分类器 *L* 应用到 BERT 最后一个编码器块位置 1 的 *[CLS]* 标记的输出嵌入 ![$$\tilde
    {{\boldsymbol {x}}}_{k,1}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq22.png)
    来解决。如图 [2.5](#Fig5) 所示，存在不同类型的微调任务。![图 2.5](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig5_HTML.png)
- en: 4 block diagrams explain the text classification and text annotation on the
    left and text pair classification and span prediction on the right via B E R T
    encoder blocks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 4 个框图通过 BERT 编码器块解释了左侧的文本分类和文本标注，以及右侧的文本对分类和跨度预测。
- en: Fig. 2.5
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5
- en: For fine-tuning, BERT is enhanced with an additional layer containing one or
    more logistic classifiers *L* using the embeddings of the last layer as inputs.
    This setup may be employed for text classification and comparison of texts with
    the embedding of *[CLS]* as input of the logistic classifier. For sequence tagging,
    *L* predicts a class for each sequence token. For span prediction, two logistic
    classifiers *L*[1] and *L*[2] predict the start and end of the answer phrase [[39](#CR39)]
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于微调，BERT通过添加一个包含一个或多个逻辑分类器*L*的额外层得到增强，这些分类器以最后一层的嵌入作为输入。这种设置可以用于文本分类和将文本与*[CLS]*嵌入作为逻辑分类器输入进行比较。对于序列标注，*L*为每个序列标记预测一个类别。对于跨度预测，两个逻辑分类器*L*[1]和*L*[2]预测答案短语的起始和结束位置
    [[39](#CR39)]。
- en: '*Text classification* assigns a sentence to one of two or more classes. Examples
    are the classification of restaurant reviews as positive/negative or the categorization
    of sentences as good/bad English. Here the output embedding of the start token
    *[CLS]* is used as input to *L* to generate the final classification.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文本分类*将一个句子分配到两个或更多类别之一。例子包括将餐厅评论分类为正面/负面或对句子进行良好/差的英语分类。在这里，开始标记*[CLS]*的输出嵌入被用作*L*的输入以生成最终的分类。'
- en: '*Text pair classification* compares two sentences separated by *“[SEP]”*. Examples
    include classifying whether the second sentence implies, contradicts, or is neutral
    with respect to the first sentence, or whether the two sentences are semantically
    equivalent. Again the output embedding of the start token *[CLS]* is used as input
    to *L*. Sometimes more than one sentence is compared to the root sentence. Then
    outputs are computed for every sentence pair and jointly normalized to a probability.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文本对分类*比较由*“[SEP]”*分隔的两个句子。例子包括判断第二个句子是否对第一个句子有暗示、矛盾或中立，或者两个句子是否在语义上等价。同样，输出嵌入的开始标记*[CLS]*被用作*L*的输入。有时会比较多个句子与根句子。然后为每个句子对计算输出，并联合归一化到概率。'
- en: '*Word annotation* marks each word or token of the input text with a specific
    property. An example is *Named Entity Recognition* (*NER*) annotating the tokens
    with five name classes (e.g. “person”, “location”, …, “other”). Here the same
    logistic model *L* is applied to every token output embedding ![$$\tilde {{\boldsymbol
    {x}}}_{k,t}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq23.png)
    at position *t* and yields a probability vector of the different entity classes.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*词标注*为输入文本的每个单词或标记分配一个特定的属性。一个例子是*命名实体识别*（NER）将标记标注为五个名称类别（例如，“人”，“地点”，…，“其他”）。在这里，相同的逻辑模型*L*应用于每个位置*t*输出的嵌入![$$\tilde
    {{\boldsymbol {x}}}_{k,t}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq23.png)，并产生不同实体类别的概率向量。'
- en: '*Span prediction* tags a short sequence of tokens within a text. An example
    is *question answering*. The input to BERT consists of a question followed by
    *“[SEP]”* and a context text, which is assumed to contain the answer. Here two
    different logistic classifiers *L* and ![$$\tilde {L}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq24.png)
    are applied to every token output embedding ![$$\tilde {{\boldsymbol {x}}}_{k,t}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq25.png)
    of the context and generate the probability that the answer to the question starts/ends
    at the specific position. The valid span (i.e. the end is not before the start)
    with the highest sum of start/end scores is selected as the answer. An example
    is the input *“[CLS] When did Caesar die ? [SEP] … On the Ides of March, 44 BC,
    Caesar was assassinated by a group of rebellious senators …”*, where the answer
    to the question is the span *“Ides*[*start*]*of March, 44 BC*[*end*]*”*. Span
    prediction may be applied to a number of similar tasks.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*跨度预测*在文本中标记一个短序列的标记。一个例子是*问答*。BERT的输入是一个问题，后面跟着*“[SEP]”*和一个上下文文本，假设它包含答案。在这里，两个不同的逻辑分类器*L*和![$$\tilde
    {L}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq24.png)应用于上下文的每个标记输出嵌入![$$\tilde
    {{\boldsymbol {x}}}_{k,t}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq25.png)，并生成答案在特定位置开始/结束的概率。选择具有最高起始/结束分数总和的有效跨度（即结束不在开始之前），作为答案。一个例子是输入*“[CLS]
    当凯撒什么时候去世？ [SEP] … 在公元前44年3月15日，凯撒被一群反叛的参议员暗杀 …”*，其中问题的答案是跨度*“Ides*[*起始*]*of March,
    44 BC*[*结束*]*”。跨度预测可以应用于许多类似任务。'
- en: Therefore, BERT just needs an extra layer with one or more logistic classifiers
    for fine-tuning. During fine-tuning with a downstream application, parameters
    of the logistic models are learned from scratch and usually all parameters in
    the pre-trained BERT model are adapted. The parameters for the logistic classifiers
    of the masked language model and the next sentence prediction are not used during
    fine-tuning.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，BERT只需要添加一个或多个逻辑分类器层来进行微调。在下游应用中进行微调时，逻辑模型的参数是从头开始学习的，并且通常所有预训练的BERT模型中的参数都会进行适配。在微调过程中，掩码语言模型和下一个句子预测的逻辑分类器的参数没有被使用。
- en: 2.1.4 Visualizing Attentions and Embeddings
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.4 可视化注意力和嵌入
- en: According to Bengio et al. [[14](#CR14)], a good representation of language
    should capture the implicit linguistic rules and common sense knowledge contained
    in text data, such as lexical meanings, syntactic relations, semantic roles, and
    the pragmatics of language use. The contextual word embeddings of BERT can be
    seen as a big step in this direction. They may be used to disambiguate different
    meanings of the same word.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Bengio等人[[14](#CR14)]的研究，一种好的语言表示应该能够捕捉到文本数据中包含的隐含语言规则和常识知识，例如词汇意义、句法关系、语义角色以及语言使用的语用学。BERT的上下文词嵌入可以被视为在这一方向上的一大步。它们可以被用来消除同一词语的不同含义。
- en: The self-attention mechanism of BERT computes a large number of “associations”
    between tokens and merges embeddings according to the strengths of these associations.
    If ***x***[1], …, ***x***[*T*] are the embeddings of the input tokens *v*[1],
    …, *v*[*T*], the associations ![$$\boldsymbol {q}^\intercal _r\boldsymbol {k}_t$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq26.png)
    are determined between the query ![$$\boldsymbol {q}_r^\intercal ={\boldsymbol
    {x}}_r^\intercal {\boldsymbol {W}}^{(q)}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq27.png)
    and the key ![$$\boldsymbol {k}_t^\intercal = {\boldsymbol {x}}_t^\intercal {\boldsymbol
    {W}}^{(k)}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq28.png)
    vectors ([2.1](#Equ1)). Then a sum of value vectors ![$${\boldsymbol {v}}_t^\intercal
    ={\boldsymbol {x}}_t^\intercal {\boldsymbol {W}}^{(v)}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq29.png)
    weighted with the normalized associations is formed yielding the new embeddings
    ([2.3](#Equ3)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的自注意力机制计算了大量的“关联”关系，根据这些关联的强度合并嵌入。如果***x***[1], …, ***x***[*T*]是输入标记*v*[1],
    …, *v*[*T*]的嵌入，那么查询![$$\boldsymbol {q}^\intercal _r\boldsymbol {k}_t$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq26.png)和键![$$\boldsymbol
    {k}_t^\intercal = {\boldsymbol {x}}_t^\intercal {\boldsymbol {W}}^{(k)}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq28.png)向量之间的关联![$$\boldsymbol
    {q}_r^\intercal ={\boldsymbol {x}}_r^\intercal {\boldsymbol {W}}^{(q)}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq27.png)被确定([2.1](#Equ1))。然后，通过归一化关联的加权和形成值向量的总和![$${\boldsymbol
    {v}}_t^\intercal ={\boldsymbol {x}}_t^\intercal {\boldsymbol {W}}^{(v)}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq29.png)，从而得到新的嵌入([2.3](#Equ3))。
- en: This is repeated with different matrices ![$${\boldsymbol {W}}^{(q)}_{l,m},{\boldsymbol
    {W}}^{(k)}_{l,m},{\boldsymbol {W}}^{(v)}_{l,m}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq30.png)
    in *m* self-attention heads and *l* layers. Each layer and head the new embeddings
    thus captures different aspects of the relations between the embeddings of each
    layer. For BERT[BASE] we have *l* = 12 layers and *m* = 12 bidirectional self-attention
    heads in each layer yielding 144 different “associations” or self-attentions.
    For the input sentence *“The girl and the boy went home. She entered the door.”*
    Figure [2.6](#Fig6) shows on the left side the strength of associations for one
    of the 144 self-attention heads. Between every pair of tokens of the sentence
    an attention value is calculated and its strength is symbolized by lines of different
    widths. We see that the pronoun *“she”* is strongly associated with *“the girl”*.
    In the subsequent calculations (c.f. Fig. [2.2](#Fig2)) the word *“she”* is disambiguated
    by merging its embedding with the embeddings of *“the”* and *“girl”* generating
    a new *contextual embedding* of *“she”*, which includes its relation to *“girl”*.
    On the right side of the figure the input *“The girl and the boy went home. He
    entered the door.”* is processed. Then the model creates an association of *“boy”*
    with *“he”*.![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig6_HTML.png)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况在不同的矩阵中重复出现，这些矩阵是 ![$${\boldsymbol {W}}^{(q)}_{l,m},{\boldsymbol {W}}^{(k)}_{l,m},{\boldsymbol
    {W}}^{(v)}_{l,m}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq30.png)
    ，在 *m* 个自注意力头和 *l* 层中。每一层和每个头都捕捉到每个层嵌入之间不同方面的关系。对于 BERT[BASE]，我们每层有 *l* = 12 层和
    *m* = 12 个双向自注意力头，从而产生 144 个不同的“关联”或自注意力。对于输入句子 *“The girl and the boy went home.
    She entered the door.”*，图 [2.6](#Fig6) 左侧显示了 144 个自注意力头中的一个关联强度。在句子的每对标记之间计算一个注意力值，其强度由不同宽度的线条表示。我们看到代词
    *“she”* 与 *“the girl”* 有很强的关联。在随后的计算中（参见图 [2.2](#Fig2)），单词 *“she”* 通过将其嵌入与“the”和“girl”的嵌入合并来消除歧义，生成一个新的
    *上下文嵌入* 的 *“she”*，其中包含其与 *“girl”* 的关系。图象的右侧处理了输入 *“The girl and the boy went home.
    He entered the door.”*。然后模型创建了一个 *“boy”* 与 *“he”* 的关联。![图 2.6](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig6_HTML.png)
- en: 2 screenshots with layer spin boxes have a horizontal color gradient bar. A
    list of words read the, girl, and, the, boy, walked, home divided into 2 sections.
    In the first one, she is linked to the and girl. In the second one, he is linked
    to the and boy.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 2个带有层旋转框的截图具有水平颜色渐变条。单词列表“read the, girl, and, the, boy, walked, home”分为2部分。在第一部分中，她与“the”和“girl”相关联。在第二部分中，他与“the”和“boy”相关联。
- en: Fig. 2.6
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6
- en: Visualization of a specific self-attention in the fifth layer of a BERT model
    with BERTviz [[142](#CR142)]. If the next sentence contains the pronoun *“she”*
    this is associated with *“the girl”*. If this pronoun is changed to *“he”* it
    is related to *“the boy”*. Image created with BERTviz [[142](#CR142)], with kind
    permission of the author Figure [2.7](#Fig7) shows a subset of the self-attention
    patterns for the sentence *“[CLS] the cat sat on the mat [SEP] the cat lay on
    the rug [SEP]”*. The self-attention patterns are automatically optimized in such
    a way that they jointly lead to an optimal prediction of the masked tokens. It
    can be seen that the special tokens *[CLS]* and *[SEP]* often are prominent targets
    of attentions. They usually function as representatives of the whole sentence
    [[124](#CR124)]. Note, however, that in a multilayer PLM the embeddings generated
    by different heads are concatenated and transformed by a nonlinear transformation.
    Therefore, the attention patterns of a single head do not contain the complete
    information [[124](#CR124)]. Whenever the matrices are randomly initialized, the
    self-attention patterns will be completely different, if the training is restarted
    with new random parameter values. However, the overall pattern of attentions between
    tokens will be similar.![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig7_HTML.png)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 BERTviz 可视化 BERT 模型第五层的特定自注意力 [[142](#CR142)]。如果下一句包含代词 *“she”*，则与 *“the
    girl”* 相关。如果这个代词改为 *“he”*，则与 *“the boy”* 相关。图片由 BERTviz 创建 [[142](#CR142)]，经作者许可。图
    [2.7](#Fig7) 显示了句子 *“[CLS] the cat sat on the mat [SEP] the cat lay on the rug
    [SEP]”* 的自注意力模式的子集。自注意力模式被自动优化，以便它们共同导致对掩码令牌的最佳预测。可以看到，特殊令牌 *[CLS]* 和 *[SEP]*
    经常是注意力的重要目标。它们通常作为整个句子的代表 [[124](#CR124)]。请注意，然而，在多层 PLM 中，不同头部生成的嵌入被连接并经过非线性变换。因此，单个头部的注意力模式不包含完整的信息
    [[124](#CR124)]。每当矩阵随机初始化时，如果使用新的随机参数值重新开始训练，自注意力模式将完全不同。然而，令牌之间注意力的整体模式将是相似的。![图片](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig7_HTML.png)
- en: A matrix of 24 color gradient interconnected patterns divided into 0 by 3 and
    0 by 5 grids labeled heads.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一个由 24 个颜色渐变互联模式组成的矩阵，分为 0 到 3 和 0 到 5 的网格，并标记为头部。
- en: Fig. 2.7
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7
- en: Visualization of some of the 144 self-attention patterns computed for the sentence
    *“[CLS] the cat sat on the mat [SEP] the cat lay on the rug[SEP]”* with BERTviz.
    Image reprinted with kind permission of the author [[142](#CR142)]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 BERTviz 可视化句子 *“[CLS] the cat sat on the mat [SEP] the cat lay on the rug[SEP]”*
    的 144 个自注意力模式中的一些。图片经作者许可重新印刷 [[142](#CR142)]。
- en: Figure [2.10](#Fig10) shows on the left side a plot of six different senses
    of the token embeddings of *“bank”* in the *Senseval-3 dataset* projected to two
    dimensions by *T-SNE* [[140](#CR140)]. The different senses are identified by
    different colors and form well-separated clusters of their own. Senses which are
    difficult to distinguish, like *“bank building”* and *“financial institution”*
    show a strong overlap [[153](#CR153)]. The graphic demonstrates that BERT embeddings
    have the ability to distinguish different senses of words which are observed frequently
    enough.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2.10](#Fig10) 在左侧显示了 *“Senseval-3 数据集”* 中 *“bank”* 令牌的六个不同意义的投影，这些投影通过 *T-SNE*
    投影到二维空间 [[140](#CR140)]。不同的意义通过不同的颜色标识，并形成了各自分离的簇。难以区分的意义，如 *“bank building”*
    和 *“financial institution”*，显示出强烈的重叠 [[153](#CR153)]。该图形表明，BERT 嵌入具有区分频繁观察到的词语不同意义的能力。
- en: There is an ongoing discussion on the inner workings of self attention.Tay et
    al [[134](#CR134)] empirically evaluated the importance of the dot product ![$$\boldsymbol
    {q}^\intercal _r\boldsymbol {k}_s$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq31.png)
    on natural language processing tasks and concluded that query-key interaction
    is “useful but not that important”. Consequently they derived alternative formulae,
    which in some cases worked well and failed in others. A survey of attention approaches
    is provided by de Santana Correia et al. [[37](#CR37)]. There are a number of
    different attention mechanisms computing the association between embedding vectors
    [[50](#CR50), [61](#CR61), [104](#CR104), [151](#CR151)]. However, most current
    large-scale models still use the original scaled dot-product attention with minor
    variations, such as other activation functions and regularizers (c.f. Sect. [3.​1.​4](528393_1_En_3_Chapter.xhtml#Sec5)).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 关于自注意力机制的内部工作原理正在持续讨论。Tay 等人 [[134](#CR134)] 通过实证评估了点积 ![$$\boldsymbol {q}^\intercal
    _r\boldsymbol {k}_s$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq31.png)
    在自然语言处理任务中的重要性，并得出结论，查询-键交互是“有用但并非那么重要”。因此，他们推导出了一些替代公式，在某些情况下效果良好，在其他情况下则失败。de
    Santana Correia 等人提供了一份关于注意力方法的调查 [[37](#CR37)]。存在多种不同的注意力机制，用于计算嵌入向量之间的关联 [[50](#CR50),
    [61](#CR61), [104](#CR104), [151](#CR151)]。然而，大多数当前的大规模模型仍然使用原始的缩放点积注意力，并辅以一些小的变化，例如其他激活函数和正则化器（参见图
    [3.1.4](528393_1_En_3_Chapter.xhtml#Sec5)）。
- en: The fully connected layers Fcl(***x***̆[*t*]) in ([2.7](#Equ7)) contain 2/3
    of the parameters of BERT, but their role in the network has hardly been discussed.
    Geva et al. [[49](#CR49)] show that fully connected layers operate as key-value
    memories, where each key is correlated with text patterns in the training samples,
    and each value induces a distribution over the output vocabulary. For a key the
    authors retrieve the training inputs, which yield the highest activation of the
    key. Experts were able to assign one or more interpretations to each key. Usually
    lower fully connected layers were associated with shallow patterns often sharing
    the last word. The upper layers are characterized by more semantic patterns that
    describe similar contexts. The authors demonstrate that the output of a feed-forward
    layer is a composition of its memories.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ([2.7](#Equ7)) 中的全连接层 Fcl(***x***̆[*t*]) 包含了 BERT 参数的 2/3，但它们在网络中的作用几乎没有被讨论。Geva
    等人 [[49](#CR49)] 表明，全连接层作为键值存储器运行，其中每个键都与训练样本中的文本模式相关联，每个值在输出词汇表上诱导一个分布。对于某个键，作者检索出产生该键最高激活的训练输入。专家能够为每个键分配一个或多个解释。通常，较低的完全连接层与浅层模式相关联，这些模式通常共享最后一个单词。上层由更多语义模式描述，这些模式描述了相似上下文。作者证明了前馈层的输出是其记忆的组合。
- en: 2.1.5 Natural Language Understanding by BERT
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.5 BERT 的自然语言理解
- en: An outstanding goal of PLMs is *Natural Language Understanding* (*NLU*). This
    cannot be evaluated against a single task, but requires a set of benchmarks covering
    different areas to assess the ability of machines to understand natural language
    text and acquire linguistic, common sense, and world knowledge. Therefore, PLMs
    are fine-tuned to corresponding real-world downstream tasks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: PLM 的一个杰出目标是 *自然语言理解* (*NLU*)。这不能针对单个任务进行评估，但需要一套涵盖不同领域的基准测试，以评估机器理解自然语言文本和获取语言学、常识和世界知识的能力。因此，PLM
    被微调以适应相应的现实世界下游任务。
- en: '**GLUE** [[146](#CR146)] is a prominent benchmark for NLU. It is a collection
    of nine NLU tasks with public training data, and an evaluation server using private
    test data. Its benchmarks cover a number of different aspects, which can be formulated
    as classification problems:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**GLUE** [[146](#CR146)] 是一个突出的 NLU 基准测试。它包含九个具有公共训练数据的 NLU 任务，以及一个使用私有测试数据的评估服务器。其基准测试涵盖了多个不同方面，可以表述为分类问题：'
- en: Determine the sentiment (positive/negative) of a sentences (SST-2).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定句子（SST-2）的情感（正面/负面）。
- en: Classify a sentence as grammatically acceptable or unacceptable (CoLA).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将句子分类为语法可接受或不可接受（CoLA）。
- en: Check if two sentences are similar or are paraphrases (MPRC, STS-B, QQP).
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查两个句子是否相似或是否为释义（MPRC, STS-B, QQP）。
- en: Determine if the first sentence entails the second one (MNLI, RTE).
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判断第一句话是否蕴涵第二句话（MNLI, RTE）。
- en: Check if sentence *B* contains the answer to question *A* (QNLI).
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查句子 *B* 是否包含问题 *A* 的答案（QNLI）。
- en: Specify the target of a pronoun from a set of alternatives (WNLI). Each task
    can be posed as *text classification* or *text pair classification* problem. The
    performance of a model is summarized in a single average value, which has the
    value 87.1 for human annotators [[145](#CR145)]. Usually, there is an online leaderboard
    where the performance of the different models are recorded. A very large repository
    of leaderboards is on the PapersWithCode website [[109](#CR109)]. Table [2.1](#Tab1)
    describes the tasks by examples and reports the performance of BERT[LARGE]. BERT
    was able to lift the Sota of average accuracy from 75.2 to 82.1%. This is a remarkable
    increase, although the value is still far below the human performance of 87.1
    with much room for improvement. Recent benchmark results for NLU are described
    in Sect. [4.​1](528393_1_En_4_Chapter.xhtml#Sec1) for the more demanding SuperGLUE
    and other benchmarks.Table 2.1
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一组备选方案中指定代词的目标（WNLI）。每个任务都可以被表述为 *文本分类* 或 *文本对分类* 问题。模型的性能用一个平均值来总结，对于人工标注者的平均值是
    87.1 [[145](#CR145)]。通常，有一个在线排行榜记录不同模型的性能。PapersWithCode网站上有一个非常大的排行榜库 [[109](#CR109)]。表
    [2.1](#Tab1) 通过示例描述了任务并报告了 BERT[LARGE] 的性能。BERT 能够将平均准确率的 Sota 从 75.2 提高到 82.1%。这是一个显著的提升，尽管这个值仍然远低于
    87.1 的人工性能，仍有很大的提升空间。最近的自然语言理解基准测试结果在 Sect. [4.1](528393_1_En_4_Chapter.xhtml#Sec1)
    中描述，对于更具挑战性的 SuperGLUE 和其他基准测试。表 2.1
- en: GLUE language understanding tasks. BERT[LARGE] was trained for three epochs
    on the fine-tuning datasets [[38](#CR38)]. The performance of the resulting models
    is printed in the last column yielding an average value of 82.1
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: GLUE 语言理解任务。BERT[LARGE] 在微调数据集上训练了三个时期 [[38](#CR38)]。最后列中打印出的模型性能的平均值是 82.1
- en: '| Task | Description | Example | Metric | BERT |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 描述 | 示例 | 指标 | BERT |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| CoLA | Is the sentence grammatical or ungrammatical? | *“This building is
    than that one.”*![$$\rightarrow $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq32.png)*Ungrammatical*
    | Matthews correlation | 60.5 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| CoLA | 句子是语法正确还是语法错误？ | *“这座建筑比那座建筑大。”*![$$\rightarrow $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq32.png)*语法错误*
    | 马修斯相关系数 | 60.5 |'
- en: '| SST-2 | Is the movie positive, negative, or neutral? | *“The movie is funny,
    smart, visually inventive, and most of all, alive.”*![$$\rightarrow $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq33.png)*Positive*
    | Accuracy | 94.9 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 | 电影是积极的、消极的还是中性的？ | *“这部电影很有趣，聪明，视觉上富有创意，最重要的是，它充满活力。”*![$$\rightarrow
    $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq33.png)*积极* |
    准确率 | 94.9 |'
- en: '| MRPC | Is the sentence *B* a paraphrase of sentence *A*? | *A*: *“Today,
    Taiwan reported 35 new infections.”**B*: *“Taiwan announced another 35 probable
    cases at noon.”*![$$\rightarrow $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq34.png)*Paraphrase*
    | Accuracy | 89.3 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| MRPC | 句子 *B* 是否是句子 *A* 的释义？ | *A*: *“今天，台湾报告了35例新感染。”**B*: *“台湾在中午宣布了另外35例可能的病例。”*![$$\rightarrow
    $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq34.png)*释义* |
    准确率 | 89.3 |'
- en: '| STS-B | How similar are sentences *A* and *B*? | *A*: *“Elephants are walking
    down a trail.”**B*: *“A herd of elephants is walking down a trail.”*![$$\rightarrow
    $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq35.png)*Similar*
    | Pearson/ Spearman correlation | 86.5 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| STS-B | 句子 *A* 和 *B* 有多相似？ | *A*: *“大象正在沿着一条小径行走。”**B*: *“一群大象正在沿着一条小径行走。”*![$$\rightarrow
    $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq35.png)*相似* |
    皮尔逊/斯皮尔曼相关系数 | 86.5 |'
- en: '| QQP | Are the two questions similar? | *A*: *“How can I increase the speed
    of my Internet connection while using a VPN?”**B*: *“How can Internet speed be
    increased by hacking through DNS?”*![$$\rightarrow $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq36.png)*Not
    Similar* | Accuracy | 72.1 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| QQP | 两个问题是否相似？ | *A*: *“我在使用VPN的同时如何提高我的互联网连接速度？”**B*: *“通过DNS黑客攻击如何提高互联网速度？”*![$$\rightarrow
    $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq36.png)*不相似*
    | 准确率 | 72.1 |'
- en: '| MNLI-mm | Does sentence *A* entail or contradict sentence *B*? | *A*: *“Tourist
    information offices can be very helpful.”**B*: *“Tourist information offices are
    never of any help.”*![$$\rightarrow $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq37.png)*Contradiction*
    | Accuracy | 85.9 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| MNLI-mm | 句子 *A* 是否蕴涵或矛盾于句子 *B*？ | *A*: *“旅游信息办公室可以非常有帮助。”**B*: *“旅游信息办公室永远不会有任何帮助。”*![$$\rightarrow
    $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq37.png)*矛盾* |
    准确率 | 85.9 |'
- en: '| QNLI | Does sentence *B* contain the answer to the question in sentence *A*?
    | *A*: *“Which collection of minor poems are sometimes attributed to Virgil.”**B*:
    *“A number of minor poems, collected in the Appendix Vergiliana, are often attributed
    to him.”*![$$\rightarrow $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq38.png)*contains
    answer* | Accuracy | 92.7 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| QNLI | 句子 *B* 是否包含句子 *A* 中问题的答案？ | *A*: *“哪些小诗集有时被归功于维吉尔。”**B*: *“许多小诗集，收集在《维吉尔附录》中，通常被认为是他所作。”*![$$\rightarrow$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq38.png)*包含答案*
    | 准确率 | 92.7 |'
- en: '| RTE | Does sentence *A* entail sentence *B*? | *A*: *“Yunus launched the
    microcredit revolution, funding 50,000 beggars, whom Grameen Bank respectfully
    calls ‘Struggling Members.”’**B*: *“Yunus supported more than 50,000 Struggling
    Members.”*![$$\rightarrow $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq39.png)*Entailed*
    | Accuracy | 70.1 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| RTE | 句子 *A* 是否蕴涵句子 *B*？ | *A*: *“尤努斯发起了小额信贷革命，资助了50,000名乞丐，格莱姆银行尊重地称他们为‘奋斗者。’”**B*:
    *“尤努斯支持了超过50,000名奋斗者。”*![$$\rightarrow$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq39.png)*蕴涵*
    | 准确率 | 70.1 |'
- en: '| WNLI | Sentence *B* replaces sentence *A*’s pronoun with a noun - is this
    the correct noun? | *A*: *“Lily spoke to Donna, breaking her concentration.”**B*:
    *“Lily spoke to Donna, breaking Lily’s concentration.”*![$$\rightarrow $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq40.png)*Incorrect*
    | Accuracy | 60.5 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| WNLI | 句子 *B* 将句子 *A* 中的代词替换为名词 - 这个名词是否正确？ | *A*: *“莉莉和唐娜说话，打断了她的注意力。”**B*:
    *“莉莉和唐娜说话，打断了莉莉的注意力。”*![$$\rightarrow$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq40.png)*不正确*
    | 准确率 | 60.5 |'
- en: BERT’s Performance on Other Fine-Tuning Tasks
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BERT在其它微调任务上的表现
- en: The pre-training data is sufficient to adapt the large number of BERT parameters
    and learn very detailed peculiarities about language. The amount of training data
    for pre-training usually is much higher than for fine-tuning. Fine-tuning usually
    only requires two or three passes through the fine-tuning training data. Therefore,
    the stochastic gradient optimizer changes most parameters only slightly and sticks
    relatively close to the optimal pre-training parameters. Consequently, the model
    is usually capable to preserve its information about general language and to combine
    it with the information about the fine-tuning task.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练数据足以适应大量BERT参数，并学习关于语言的非常详细的特点。通常，预训练的训练数据量远高于微调。微调通常只需要对微调训练数据进行两到三次遍历。因此，随机梯度优化器只对大多数参数进行轻微的调整，并相对接近最优的预训练参数。因此，模型通常能够保留其关于通用语言的信息，并将其与微调任务的信息相结合。
- en: Because BERT can reuse its general knowledge about language acquired during
    pre-training, it produces excellent results even with small fine-tuning training
    data [[39](#CR39)].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因为BERT可以在预训练期间重用其关于语言的一般知识，即使是在小的微调训练数据下也能产生优秀的结果 [[39](#CR39)]。
- en: '**CoNLL 2003** [[128](#CR128)] is a benchmark dataset for *Named entity recognition*
    (*NER*), where each token has to be marked with a named entity tag, e.g. PER (for
    person), LOC (for location), …, O (for no name) (Sect. [5.​3](528393_1_En_5_Chapter.xhtml#Sec12)).
    The task involves text annotation, where a label is predicted for every input
    token. BERT increased Sota from 92.6% to 92.8% F1-value on the test data.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CoNLL 2003** [[128](#CR128)] 是一个用于 *命名实体识别* (*NER*) 的基准数据集，其中每个标记都必须标记为命名实体标签，例如PER（表示人名），LOC（表示地理位置），…，O（表示无名称）(Sect.
    [5.3](528393_1_En_5_Chapter.xhtml#Sec12))。该任务涉及文本标注，为每个输入标记预测一个标签。BERT在测试数据上提高了Sota从92.6%到92.8%的F1值。'
- en: '**SQuAD 1.0** [[120](#CR120)] is a collection of 100k triples of questions,
    contexts, and answers. The task is to mark the span of the answer tokens in the
    context. An example is the question *“When did Augustus die?”*, where the answer
    *“14 AD”* has to be marked in the context *“…the death of Augustus in AD 14 …”*
    (Sect. [6.​2](528393_1_En_6_Chapter.xhtml#Sec9)). Using span prediction BERT increased
    the Sota of SQuAD from 91.7% to 93.2%, while the human performance was measured
    as 91.2%.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SQuAD 1.0** [[120](#CR120)] 是一个包含100k个问题、上下文和答案的三元组集合。任务是标记上下文中答案标记的范围。例如，问题是
    *“奥古斯都什么时候去世？”*，其中答案 *“14 AD”* 必须在上下文 *“…奥古斯都在公元14年去世…”* 中标记(Sect. [6.2](528393_1_En_6_Chapter.xhtml#Sec9))。使用范围预测，BERT将SQuAD的Sota从91.7%提高到93.2%，而人类的表现被测量为91.2%。'
- en: From these experiments a large body of evidence has been collected demonstrating
    the strengths and weaknesses of BERT [[124](#CR124)]. This is discussed in Sect.
    [4.​2](528393_1_En_4_Chapter.xhtml#Sec7).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些实验中收集了大量证据，证明了 BERT 的优势和劣势 [[124](#CR124)]。这在第 [4.2](528393_1_En_4_Chapter.xhtml#Sec7)
    节中进行了讨论。
- en: In summary, the advent of the BERT model marks a new era of NLP. It combines
    two pre-training tasks, i.e., predicting masked tokens and determining whether
    the second sentence matches the first sentence. Transfer learning with unsupervised
    pre-training and supervised fine-tuning becomes the new standard.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，BERT 模型的出现标志着 NLP 的新时代。它结合了两个预训练任务，即预测掩码标记和确定第二个句子是否与第一个句子匹配。使用无监督预训练和监督微调的迁移学习成为新的标准。
- en: 2.1.6 Computational Complexity
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.6 计算复杂度
- en: 'It is instructive to illustrate the computational effort required to train
    PLMs. Its growth determines the time needed to train larger models that can massively
    improve the quality of language representation. Assume *D* is the size of the
    hidden embeddings and the input sequence has length *T*, then the intermediate
    dimension of the fully connected layer Fcl is set to 4*D* and the dimension of
    the keys and values are set to *D*∕*H* as in Vaswani et al. [[141](#CR141)]. Then
    according to Lin et al. [[81](#CR81)] we get the following computational complexities
    and parameters counts of self-attention and the position-wise Fcl ([2.7](#Equ7)):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 举例说明训练 PLM 所需的计算工作量是有教育意义的。其增长决定了训练更大模型所需的时间，这些模型可以大幅提高语言表示的质量。假设 *D* 是隐藏嵌入的大小，输入序列长度为
    *T*，那么全连接层 Fcl 的中间维度设置为 4*D*，键和值的维度设置为 *D*∕*H*，如 Vaswani 等人所述 [[141](#CR141)]。然后根据
    Lin 等人 [[81](#CR81)]，我们得到以下自注意力和位置性 Fcl 的计算复杂度和参数计数 ([2.7](#Equ7))：
- en: '| Module | Complexity | # Parameters |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 模块 | 复杂度 | # 参数 |'
- en: '| --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Self-attention | *O*(*T*² ∗ *D*) | 4*D*² |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 自注意力 | *O*(*T*²∗*D*) | 4*D*² |'
- en: '| Position-wise Fcl | *O*(*T* ∗ *D*²) | 8*D*² |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 位置性 Fcl | *O*(*T*∗*D*²) | 8*D*² |'
- en: As long as the input sequence length *T* is small, the hidden dimension *D*
    mainly determines the complexity of self-attention and position-wise Fcl. The
    main limiting factor is the Fcl. But when the input sequences become longer, the
    sequence length *T* gradually dominates the complexity of these modules, so that
    self-attention becomes the bottleneck of the PLM. Moreover, the computation of
    self-attention requires that an attention score matrix of size *T* × *T* is stored,
    which prevents the computation for long input sequences. Therefore, modifications
    reducing the computational effort for long input sequences are required.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 只要输入序列长度 *T* 较小，隐藏维度 *D* 主要决定了自注意力和位置性 Fcl 的复杂度。主要的限制因素是 Fcl。但是当输入序列变长时，序列长度
    *T* 逐渐主导了这些模块的复杂度，因此自注意力成为 PLM 的瓶颈。此外，自注意力的计算需要存储大小为 *T*×*T* 的注意力分数矩阵，这阻止了对长输入序列的计算。因此，需要修改以减少长输入序列的计算工作量。
- en: To connect all input embeddings with each other, we could employ different modules.
    Fully connected layers require *T* ∗ *T* networks between the different embeddings.
    Convolutional layers with a kernel width *K* do not connect all pairs and therefore
    need *O*(log[*K*](*T*)) layers in the case of dilated convolutions. RNNs have
    to apply a network *T* times. This leads to the following complexities per layer
    [[81](#CR81), [141](#CR141)]
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将所有输入嵌入相互连接，我们可以采用不同的模块。全连接层需要在不同的嵌入之间使用 *T*∗*T* 网络层。具有核宽度 *K* 的卷积层不会连接所有对，因此在扩张卷积的情况下需要
    *O*(log[*K*](*T*)) 层。RNN 必须应用 *T* 次网络。这导致每层的以下复杂度 [[81](#CR81), [141](#CR141)]
- en: '|   |   | Sequential | Maximum |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|   |   | 序列 | 最大 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Layer type | Complexity per layer | operations | path length |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 层类型 | 每层复杂度 | 操作 | 路径长度 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Self-attention | *O*(*T*² ∗ *D*) | *O*(1) | *O*(1) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 自注意力 | *O*(*T*²∗*D*) | *O*(1) | *O*(1) |'
- en: '| Recurrent | *O*(*T* ∗ *D*²) | *O*(*T*) | *O*(*T*) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 循环 | *O*(*T*∗*D*²) | *O*(*T*) | *O*(*T*) |'
- en: '| Fully connected | *O*(*T*² ∗ *D*²) | *O*(1) | *O*(1) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 全连接 | *O*(*T*²∗*D*²) | *O*(1) | *O*(1) |'
- en: '| Convolutional | *O*(*K* ∗ *T* ∗ *D*²) | *O*(1) | *O*(log[*K*](*T*)) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 卷积 | *O*(*K*∗*T*∗*D*²) | *O*(1) | *O*(log[*K*](*T*)) |'
- en: '| Restricted self-attention | *O*(*R* ∗ *T* ∗ *D*) | *O*(1) | *O*(*T*∕*R*)
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 限制性自注意力 | *O*(*R*∗*T*∗*D*) | *O*(1) | *O*(*T*∕*R*) |'
- en: The last line describes a restricted self-attention, where self-attention only
    considers a neighborhood of size *R* to reduce computational effort. Obviously
    the computational complexity per layer is a limiting factor. In addition, computation
    for recurrent layers need to be sequential and cannot be parallelized, as shown
    in the column for sequential operations. The last column shows the path length,
    i.e. the number of computations to communicate information between far-away positions.
    The shorter these paths between any combination of positions in the input and
    output sequences, the easier it is to learn long-range dependencies. Here self-attention
    has a definite advantage compared to all other layer types. Section [3.​2](528393_1_En_3_Chapter.xhtml#Sec7)
    discusses advanced approaches to process input sequences of larger length. In
    conclusion, BERT requires less computational effort than alternative layer types.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行描述了一种受限的自注意力，其中自注意力只考虑大小为 *R* 的邻域以减少计算工作量。显然，每层的计算复杂度是一个限制因素。此外，循环层的计算需要顺序执行，不能并行化，如顺序操作列所示。最后一列显示了路径长度，即在不同位置之间传递信息所需的计算次数。这些路径越短，学习长距离依赖关系就越容易。在这里，自注意力与所有其他层类型相比具有明显的优势。第
    [3.2](528393_1_En_3_Chapter.xhtml#Sec7) 节讨论了处理较长输入序列的高级方法。总之，BERT 比其他层类型需要更少的计算工作量。
- en: 2.1.7 Summary
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.7 摘要
- en: '*BERT* is an autoencoder model whose main task is to derive context-sensitive
    embeddings for tokens. In a preliminary step, tokens are generated from the words
    and letters of the training data in such a way that most frequent words are tokens
    and arbitrary words can be composed of tokens. Each token is encoded by an input
    embedding. To mark the position of each input token, a position embedding is added
    to the input embedding.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*BERT* 是一个自动编码器模型，其主要任务是推导出针对标记的上下文相关嵌入。在初步步骤中，从训练数据的单词和字母生成标记，使得最常见的单词是标记，任意单词可以由标记组成。每个标记由一个输入嵌入编码。为了标记每个输入标记的位置，向输入嵌入添加一个位置嵌入。'
- en: In each layer of BERT, the lower layer embeddings are transformed by self-attention
    to a new embedding. Self-attention involves the computation of scalar products
    between linear transformations of embeddings. In this way, the embeddings in the
    next layer can adapt to tokens from the context, and the embeddings become context-sensitive.
    The operation is performed in parallel for several attention heads involving different
    linear projections. The heads can compute associations in parallel with respect
    to different semantic features. The resulting partial embeddings are concatenated
    to a new embedding. In addition to self-attention heads, each encoder block contains
    a fully connected layer as well as normalization operations.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BERT 的每一层中，较低层的嵌入通过自注意力转换成新的嵌入。自注意力涉及嵌入的线性变换之间的标量积计算。通过这种方式，下一层的嵌入可以适应上下文中的标记，并且嵌入变得上下文相关。该操作在涉及不同线性投影的多个注意力头之间并行执行。头部可以并行计算与不同语义特征相关的关联。结果的部分嵌入被连接成一个新的嵌入。除了自注意力头部之外，每个编码器块还包含一个全连接层以及归一化操作。
- en: The original BERT model consists of six encoder blocks and generates a final
    embedding for each input token. BERT is pre-trained on a very large document collection.
    The main pre-training task is to predict words from the input sequence, which
    have been replaced by a [MASK] token. This is done by using the last layer embedding
    of the token as input to a logistic classifier, which predicts the probabilities
    of tokens for this position. During pre-training the model parameters are optimized
    by stochastic gradient descent. This forces the model to collect all available
    information about that token in the output embedding. The first input token is
    the [CLS] token. During pre-training, it is used for next sentence prediction,
    where a logistic classifier with the [CLS]-embedding as input has to decide, if
    the first and second sentence of the input sequence belong together or not.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 BERT 模型由六个编码器块组成，并为每个输入标记生成一个最终的嵌入。BERT 在一个非常大的文档集合上进行了预训练。主要的预训练任务是预测输入序列中已被替换为
    `[token]` 标记的单词。这是通过使用标记的最后层嵌入作为逻辑分类器的输入来完成的，该分类器预测该位置标记的概率。在预训练期间，通过随机梯度下降优化模型参数。这迫使模型在输出嵌入中收集有关该标记的所有可用信息。第一个输入标记是
    `[CLS]` 标记。在预训练期间，它用于预测下一句，其中以 `[CLS]` 嵌入作为输入的逻辑分类器必须决定输入序列的第一句和第二句是否属于一起。
- en: Typically, the pre-trained model is fine-tuned for a specific task using a small
    annotated training dataset. An example is the supervised classification task of
    whether the input text expresses a positive, negative or neutral sentiment. Again
    a logistic classifier with the [CLS]-embedding as input has to determine the probability
    of the three sentiments. During pre-training all parameters of the model are adjusted
    slightly. It turns out that this transfer learning approach has a much higher
    accuracy than supervised training only on the small training dataset, since the
    model can use knowledge about language acquired during pre-training.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，预训练模型会使用一个小型的标注训练数据集进行微调以适应特定任务。一个例子是监督分类任务，即判断输入文本表达的是积极、消极还是中立情绪。再次使用以
    [CLS]-embedding 作为输入的逻辑分类器来确定三种情绪的概率。在预训练期间，模型的全部参数都会进行轻微调整。结果表明，这种迁移学习方法比仅在小型训练数据集上进行监督训练具有更高的准确率，因为模型可以使用在预训练期间获得的语言知识。
- en: Experiments show that BERT is able to raise the Sota considerably in many language
    understanding tasks, e.g. the GLUE benchmark. Other applications are named entity
    recognition, where names of persons, locations, etc. have to be identified in
    a text, or question answering, where the answer to a question has to be extracted
    from a paragraph. An analysis of computational complexity shows that BERT requires
    less computational effort than alternative layer types. Overall, BERT is the workhorse
    of natural language processing and is used in different variants to solve language
    understanding problems. Its encoder blocks are reused in many other models.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明，BERT 能够在许多语言理解任务中显著提高 Sota（即“最先进的技术”），例如 GLUE 基准测试。其他应用包括命名实体识别，其中需要在文本中识别人名、地点等名称，或者问答任务，其中需要从段落中提取问题的答案。对计算复杂性的分析显示，BERT
    比替代层类型需要更少的计算资源。总的来说，BERT 是自然语言处理中的工作马，被用于不同变体来解决语言理解问题。其编码器块被许多其他模型重用。
- en: Chapter [3](528393_1_En_3_Chapter.xhtml) describes ways to improve the performance
    of BERT models, especially by designing new pre-training tasks (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2)).
    In Chap. [4](528393_1_En_4_Chapter.xhtml) the knowledge acquired by BERT models
    is discussed. In the Chaps. [5](528393_1_En_5_Chapter.xhtml)–[7](528393_1_En_7_Chapter.xhtml),
    we describe a number of applications of BERT models such as relation extraction
    (Sect. [5.​4](528393_1_En_5_Chapter.xhtml#Sec19)) or document retrieval (Sect.
    [6.​1](528393_1_En_6_Chapter.xhtml#Sec1)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 第 [3](528393_1_En_3_Chapter.xhtml) 章描述了提高 BERT 模型性能的方法，特别是通过设计新的预训练任务（第 [3.1.1](528393_1_En_3_Chapter.xhtml#Sec2)
    节）。在第 [4](528393_1_En_4_Chapter.xhtml) 章中，讨论了 BERT 模型获得的知识。在第 [5](528393_1_En_5_Chapter.xhtml)–[7](528393_1_En_7_Chapter.xhtml)
    章中，我们描述了 BERT 模型的多种应用，例如关系抽取（第 [5.4](528393_1_En_5_Chapter.xhtml#Sec19) 节）或文档检索（第
    [6.1](528393_1_En_6_Chapter.xhtml#Sec1) 节）。
- en: '2.2 GPT: Autoregressive Language Models'
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 GPT：自回归语言模型
- en: 2.2.1 The Task of Autoregressive Language Models
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 自回归语言模型的任务
- en: To capture the information in natural language texts the conditional probability
    of tokens can be described by a language model. These *autoregressive language
    models* aim to predict the probability of the next token in a text given the previous
    tokens. If *V*[*t*+1] is a random variable whose values are the possible tokens
    *v*[*t*+1] at position *t* + 1, we have to calculate the conditional probability
    distribution *p*(*V*[*t*+1]|*v*[1], …, *v*[*t*]). According to the definition
    of conditional probability the probability of the complete text *v*[1], …, *v*[*T*]
    can be computed as![$$\displaystyle \begin{aligned} p(V_1\mkern1.5mu{=}\mkern1.5mu
    v_1,\ldots,V_T\mkern1.5mu{=}\mkern1.5mu v_T)= p(V_T\mkern1.5mu{=}\mkern1.5mu v_{T}|v_1,\ldots,v_{T-1})*\cdots*p(V_1\mkern1.5mu{=}\mkern1.5mu
    v_1) {}. \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ9.png)(2.9)Therefore,
    the conditional probability can represent all information about valid sentences,
    including adequate and bad usage of language. Qudar et al. [[115](#CR115)] provide
    a recent survey of language models.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉自然语言文本中的信息，标记的条件概率可以通过语言模型来描述。这些 *自回归语言模型* 旨在根据前面的标记预测文本中下一个标记的概率。如果 *V*[*t*+1]
    是一个随机变量，其值是位置 *t*+1 处的可能标记 *v*[*t*+1]，我们必须计算条件概率分布 *p*(*V*[*t*+1]|*v*[1]，…，*v*[*t*])。根据条件概率的定义，完整文本
    *v*[1]，…，*v*[*T*] 的概率可以计算为！[$$\displaystyle \begin{aligned} p(V_1\mkern1.5mu{=}\mkern1.5mu
    v_1,\ldots,V_T\mkern1.5mu{=}\mkern1.5mu v_T)= p(V_T\mkern1.5mu{=}\mkern1.5mu v_{T}|v_1,\ldots,v_{T-1})*\cdots*p(V_1\mkern1.5mu{=}\mkern1.5mu
    v_1) {}. \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ9.png)(2.9)因此，条件概率可以代表关于有效句子所有信息，包括语言使用的恰当与否。Qudar
    等人 [[115](#CR115)] 提供了关于语言模型的最新综述。
- en: In Sect. [1.​6](528393_1_En_1_Chapter.xhtml#Sec6), we used RNNs to build language
    models. However, these had problems determining long-range interactions between
    tokens. As an alternative, we can employ self-attention to infer contextual embeddings
    of the past tokens *v*[1], …, *v*[*t*] and predict the next token *v*[*t*+1] based
    on these embeddings.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [1.6](528393_1_En_1_Chapter.xhtml#Sec6) 节中，我们使用了 RNNs 来构建语言模型。然而，这些模型在确定标记之间的长距离交互时存在问题。作为替代方案，我们可以使用自注意力来推断过去标记
    *v*[1]，…，*v*[*t*] 的上下文嵌入，并根据这些嵌入预测下一个标记 *v*[*t*+1]。
- en: Consequently, we need to restrict self-attention to the tokens *v*[1], …, *v*[*t*].
    This is the approach taken by the **Generative Pre-trained Transformer** (**GPT**)
    [[116](#CR116), [118](#CR118)]. Before training, the text is transformed to tokens,
    e.g. by byte-pair encoding (Sect. [1.​2](528393_1_En_1_Chapter.xhtml#Sec2)). On
    input, these tokens are represented by token embeddings and position embeddings
    (Sect. [2.1.1](#Sec2)). During training the GPT-model performs the self-attention
    computations described in Sect. [2.1.1](#Sec3) in the same way as for BERT. For
    predicting the probabilities of different tokens at position *t* + 1, the self-attentions
    are restricted to previous tokens *v*[1], …, *v*[*t*] and their embeddings. The
    probability of the possible next tokens at position *t* + 1 is computed by a logistic
    classifier![$$\displaystyle \begin{aligned} p(V_{t+1}|v_1,\ldots,v_{t})=\operatorname{\mathrm{softmax}}(A\tilde{{\boldsymbol{x}}}_{k,t}+\boldsymbol{b})
    {}, \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ10.png)(2.10)which
    takes as input the embedding ![$$\tilde {{\boldsymbol {x}}}_{k,t}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq41.png)
    of the last layer *k* at position *t* to predict the random variable *V*[*t*+1]
    of possible tokens at position *t* + 1 (Fig. [2.8](#Fig8)). This approach is called
    *masked self-attention* or *causal self-attention* because the prediction depends
    only on past tokens. Since GPT generates the tokens by sequentially applying the
    same model, it is called an *autoregressive language model*.![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig8_HTML.png)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要将自注意力限制在标记 *v*[1]，…，*v*[*t*] 上。这是**生成预训练Transformer**（**GPT**）[[116](#CR116)，[118](#CR118)]所采取的方法。在训练之前，文本被转换为标记，例如通过字节对编码（第[1.2](528393_1_En_1_Chapter.xhtml#Sec2)节）。在输入时，这些标记由标记嵌入和位置嵌入表示（第[2.1.1](#Sec2)节）。在训练过程中，GPT模型以与BERT相同的方式进行自注意力计算（第[2.1.1](#Sec3)节）。为了预测位置
    *t*+1 处不同标记的概率，自注意力被限制在之前的标记 *v*[1]，…，*v*[*t*] 及其嵌入上。位置 *t*+1 处可能出现的下一个标记的概率是通过逻辑分类器计算的![$$\displaystyle
    \begin{aligned} p(V_{t+1}|v_1,\ldots,v_{t})=\operatorname{\mathrm{softmax}}(A\tilde{{\boldsymbol{x}}}_{k,t}+\boldsymbol{b})
    {}, \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ10.png)(2.10)，它将最后一层
    *k* 在位置 *t* 的嵌入 ![$$\tilde {{\boldsymbol {x}}}_{k,t}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq41.png)
    作为输入来预测位置 *t*+1 处可能标记的随机变量 *V*[*t*+1]（图[2.8](#Fig8)）。这种方法被称为*掩码自注意力*或*因果自注意力*，因为预测只依赖于过去的标记。由于GPT通过顺序应用相同的模型生成标记，因此它被称为*自回归语言模型*。[图片](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig8_HTML.png)
- en: Two block diagrams of BERT encoder blocks and four different levels. The lowest
    level is the input block, followed by input embeddings, and output embeddings.
    The highest level is token probabilities.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: BERT编码块的两个框图和四个不同级别。最低级别是输入块，然后是输入嵌入和输出嵌入。最高级别是标记概率。
- en: Fig. 2.8
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8
- en: The input of the GPT model are the embeddings of tokens *v*[1], …, *v*[*t*]
    up to position *t*. GPT computes contextual self-embeddings of these tokens in
    different layers and uses the output embedding of the last token *v*[*t*] = *“to”*
    in the highest layer to predict the probabilities of possible tokens at position
    *t* + 1 with a logistic classifier *L*. This probability should be high for the
    actually observed token *“new”* (left). Then the observed token *v*[*t*+1] = *“new”*
    is appended to the input sequence and included in the self-attention computation
    for predicting the probabilities of possible tokens at position *t* + 2, which
    should be high for *“york”* (right)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型的输入是直到位置 *t* 的标记 *v*[1]，…，*v*[*t*] 的嵌入。GPT计算这些标记在不同层的上下文自嵌入，并使用最高层中最后一个标记
    *v*[*t*] = *“to”* 的输出嵌入来预测位置 *t*+1 处可能标记的概率，该概率通过逻辑分类器 *L* 计算。对于实际观察到的标记 *“new”*（左侧），这个概率应该很高。然后，观察到的标记
    *v*[*t*+1] = *“new”* 被追加到输入序列中，并包含在自注意力计算中，以预测位置 *t*+2 处可能标记的概率，该概率对于 *“york”*（右侧）应该很高。
- en: '![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig9_HTML.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig9_HTML.png)'
- en: 2 block diagrams have various input tokens that go through B E R T encoder blocks
    and L classifiers to embed with other tokens. The diagrams have layers of input
    tokens, input embeddings, output embeddings, and token probabilities.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 两个框图有各种输入标记，这些标记通过BERT编码块和L分类器进行嵌入。这些框图有输入标记层、输入嵌入层、输出嵌入层和标记概率层。
- en: Fig. 2.9
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9
- en: Visualization of embeddings with PCA together with the corresponding part-of
    speech tags. On the left side are GPT-2 embeddings of layer 0 of tokens of positions
    > 0 which form ribbon-like structures for the different POS tags, with function
    words close to the top. On the right side the embeddings of BERT for layer 0 are
    shown. Image reprinted with kind permission of the author [[66](#CR66)]
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PCA进行嵌入可视化，并显示相应的词性标签。左侧是位置>0的标记的层0的GPT-2嵌入，这些嵌入形成了不同词性标签的带状结构，功能词靠近顶部。右侧显示了层0的BERT嵌入。图片经作者许可重新印刷
    [[66](#CR66)]。
- en: 2.2.2 Training GPT by Predicting the Next Token
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 通过预测下一个标记来训练GPT
- en: The training objective is adapted to the language modeling task of GPT. Figure
    [2.8](#Fig8) shows the range of computations for two consecutive tokens. By *teacher
    forcing* the model uses the observed tokens *v*[1], …, *v*[*t*] up to position
    *t* to compute self-attentions and predict the token probabilities for the next
    token *v*[*t*+1]. This is justified by the factorization ([2.9](#Equ9)) of the
    full distribution. Note that the contextual embedding of a token *v*[*s*], *s* < *t*,
    changes each time when a new token *v*[*t*+1], *v*[*t*+2], … is taken into account
    in the masked self-attention. As GPT considers only the tokens before the target
    token *v*[*t*+1], it is called an *unidirectional encoder*. An intuitive high-level
    overview over GPT is given by Alammar [[3](#CR3)].
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 训练目标被调整为GPT的语言模型任务。图[2.8](#Fig8)显示了连续两个标记的计算范围。通过*教师强制*，模型使用观察到的标记*v*[1]，…，*v*[*t*]直到位置*t*来计算自注意力并预测下一个标记*v*[*t*+1]的概率。这是由全分布的分解([2.9](#Equ9))所证明的。请注意，标记*v*[*s*]的上下文嵌入，*s*<*t*，每次在考虑新的标记*v*[*t*+1]，*v*[*t*+2]，…时都会改变。由于GPT只考虑目标标记*v*[*t*+1]之前的标记，因此它被称为*单向编码器*。Alammár
    [[3](#CR3)]给出了GPT的直观高级概述。
- en: During training the model parameters have to be changed by optimization such
    that the probabilities of observed documents ([2.9](#Equ9)) get maximal. By this
    *Maximum Likelihood estimation* (*MLE*) the parameters can be optimized for a
    large corpus of documents. To avoid numerical problems this is solved by maximizing
    the *log-likelihood*, sum of logarithms of ([2.9](#Equ9))![$$\displaystyle \begin{aligned}
    \log p(v_1,\ldots,v_T)= \log p(v_{T}|v_1,\ldots,v_{T-1})+\cdots+\log p(v_{2}|v_1)
    +\log p(v_1) {}. \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ11.png)(2.11)Alternatively
    we can minimize the negative log-likelihood ![$$-\log p(v_1,\ldots ,v_T)$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq42.png).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，必须通过优化来改变模型参数，以便使观察到的文档的概率（[2.9](#Equ9)）最大化。通过这种*最大似然估计*（*MLE*），可以针对大量文档的语料库优化参数。为了避免数值问题，这是通过最大化*对数似然*，即([2.9](#Equ9))的对数之和来解决的！[$$\displaystyle
    \begin{aligned} \log p(v_1,\ldots,v_T)= \log p(v_{T}|v_1,\ldots,v_{T-1})+\cdots+\log
    p(v_{2}|v_1) +\log p(v_1) {}. \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ11.png)(2.11)或者我们可以通过最小化负对数似然
    ![$$-\log p(v_1,\ldots ,v_T)$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq42.png)。
- en: GPT-2 can process an input sequence of 1024 tokens with an embedding size of
    1024\. In its medium version it has 345M parameters and contains 24 layers, each
    with 12 attention heads. For the training with gradient descent a batch size of
    512 was utilized. The model was trained on 40 GB of text crawled from Reddit,
    a social media platform. Only texts that were well rated by other users were included,
    resulting in a higher quality data set. The larger model was trained on 256 cloud
    TPU v3 cores. The training duration was not disclosed, nor the exact details of
    training.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2可以处理一个包含1024个标记的输入序列，嵌入大小为1024。在其中等版本中，它有345M个参数，包含24层，每层有12个注意力头。在梯度下降训练中，使用了512个批次的尺寸。该模型在从Reddit，一个社交媒体平台上爬取的40
    GB文本上进行了训练。只包括其他用户评价良好的文本，从而得到一个质量更高的数据集。较大的模型在256个云TPU v3核心上进行了训练。训练持续时间没有公开，也没有训练的详细情况。
- en: The quality of a language model may be measured by the probability *p*(*v*[1],
    …, *v*[*T*]) of a given text collection *v*[1], …, *v*[*T*]. If we normalize its
    inverse by the number *T* of tokens we get the *perplexity* [[28](#CR28)]![$$\displaystyle
    \begin{aligned} ppl(v_1,\ldots,v_T):=p(v_1,\ldots,v_T)^{-\frac 1T} {}. \end{aligned}
    $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ12.png)(2.12)A
    low perplexity indicates a high probability of the text. If we assume that the
    conditional probabilities *p*(*v*[*t*]|*v*[1], …, *v*[*t*−1]) are identical for
    all *t*, we get *ppl*(*v*[1], …, *v*[*T*]) = 1∕*p*(*v*[*t*]|*v*[1], …, *v*[*t*−1]),
    i.e. the inverse probability of the next token. GPT-2 was able to substantially
    reduce the perplexity on a number of benchmark data sets, e.g. from 46.5 to 35.8
    for the *Penn Treebank corpus* [[117](#CR117)] meaning that the actual words in
    the texts were predicted with higher probability.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的质量可以通过给定文本集合*v*[1]，…，*v*[*T*]的概率*p*(*v*[1]，…，*v*[*T*])来衡量。如果我们通过标记数*T*来归一化其倒数，我们得到*puzzle*
    [[28](#CR28)]![$$\displaystyle \begin{aligned} ppl(v_1,\ldots,v_T):=p(v_1,\ldots,v_T)^{-\frac
    1T} {}. \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ12.png)(2.12)低*puzzle*表示文本的高概率。如果我们假设条件概率*p*(*v*[*t*]|*v*[1]，…，*v*[*t*−1])对所有*t*都是相同的，我们得到*puzzle*(*v*[1]，…，*v*[*T*])
    = 1 / *p*(*v*[*t*]|*v*[1]，…，*v*[*t*−1])，即下一个标记的逆概率。GPT-2能够在多个基准数据集上显著降低*puzzle*，例如，对于*Penn
    Treebank语料库* [[117](#CR117)]从46.5降低到35.8，这意味着文本中的实际单词被以更高的概率预测。
- en: Visualizing GPT Embeddings
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可视化GPT嵌入
- en: 'Kehlbeck et al. [[66](#CR66)] investigated the relative location of embeddings
    in multivariate space for both BERT and GPT-2, each with 12 layers. They calculated
    3-D projections using both *principal component analysis**(PCA)* [[111](#CR111)]
    and UMAP [[89](#CR89)]. The latter can preserve the local structure of neighbors,
    but—differently to PCA—is unable to correctly maintain the global structure of
    the data. These 3d-scatterplots can be interactively manipulated on the website
    [[66](#CR66)]. It turns out that GPT-2 forms two separate clusters: There is a
    small cluster containing just all tokens at position 0, while the embeddings at
    other positions form ribbon-like structures in the second cluster.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Kehlbeck等人[[66](#CR66)]研究了BERT和GPT-2在多变量空间中嵌入的相对位置，每个模型都有12层。他们使用**主成分分析**(PCA)
    [[111](#CR111)]和UMAP [[89](#CR89)]进行了3D投影。后者可以保留邻居的局部结构，但与PCA不同，无法正确维护数据的全局结构。这些3D散点图可以在网站上交互式操作[[66](#CR66)]。结果显示，GPT-2形成了两个独立的簇：有一个小簇仅包含位置0的所有标记，而其他位置的嵌入在第二个簇中形成带状结构。
- en: Careful investigations have indicated that most embedding vectors are located
    in a narrow cone, leading to high cosine similarities between them [[25](#CR25)].
    The authors identify isolated clusters and low dimensional manifolds in the contextual
    embedding space. Kehlbeck et al. [[66](#CR66)] show that tokens with the same
    part-of-speech tag form ribbon-like structures in the projections (Fig. [2.9](#Fig9)
    left). Function words are all located on a tight circular structure, whereas content
    words like nouns and verbs are located in other elongated structures and have
    overlap with other POS-tags. The embeddings generated by BERT form one or more
    clusters (Fig. [2.9](#Fig9) right). They are quite separated for function words,
    but show some overlap for content words like nouns, verbs, or adjectives.![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig10_HTML.png)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细的调查表明，大多数嵌入向量位于一个狭窄的圆锥体内，导致它们之间的高余弦相似度[[25](#CR25)]。作者在上下文嵌入空间中识别出孤立的簇和低维流形。Kehlbeck等人[[66](#CR66)]表明，具有相同词性标记的标记在投影中形成带状结构（图2.9左）。功能词都位于一个紧密的圆形结构上，而像名词和动词这样的内容词则位于其他拉长的结构中，并且与其他词性标记有重叠。BERT生成的嵌入形成了一个或多个簇（图2.9右）。对于功能词，它们相当分离，但对于像名词、动词或形容词这样的内容词，则显示出一些重叠。![图片](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig10_HTML.png)
- en: Two scatterplots. The left one plots a financial institution, sloping land,
    a bank building, a long ridge, arrangement of objects, and a flight maneuver.
    A financial institution has the highest concentration. The second one plots P
    C A projections of embeddings such as banks and material.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 两个散点图。左边的散点图描绘了一个金融机构、倾斜的土地、一座银行大楼、一条长脊、物体的排列和飞行机动。金融机构的浓度最高。第二个散点图描绘了银行和材料等嵌入的PCA投影。
- en: Fig. 2.10
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10
- en: Plot of BERT-embeddings of different senses of *“bank”* projected to two dimensions
    by T-SNE (left). The legend contains a short description of the respective WordNet
    sense and the frequency of occurrence in the training data. Image[[153](#CR153)].
    The right side shows PCA projections of the embeddings of *“banks”* (lower strip)
    and *“material”* (middle strip) as well as other words computed for different
    contexts. Image interactively generated, printed with kind permission of the authors
    [[66](#CR66)]
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 将*“bank”*的不同含义的BERT嵌入通过T-SNE投影到二维图（左）。图例包含各自WordNet含义的简短描述和训练数据中的出现频率。图像[153](#CR153)]。右侧显示了*“banks”*（下条带）和*“material”*（中间条带）以及其他在不同上下文中计算出的嵌入的PCA投影。图像由作者友好许可打印生成
    [[66](#CR66)]。
- en: The GPT-2 embeddings of content words like *“banks”* and *“material”* at positions
    > 0 form elongated band-structures, as shown in the right part of Fig. [2.10](#Fig10).
    For higher layers the PCA projections get more diffuse. The user can read the
    token context by pointing to each dot.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在位置>0处，内容词如*“banks”*和*“material”*的GPT-2嵌入形成了延伸的带状结构，如图[2.10](#Fig10)的右侧所示。对于较高层，PCA投影变得更加分散。用户可以通过指向每个点来读取标记上下文。
- en: Token-based *self-similarity* is the mean cosine similarity of the same token
    found in different sentences. In BERT as well as GPT-2, the self-similarity is
    higher for content than function words [[66](#CR66)]. This may indicate that function
    words have more diverse semantic roles in different contexts. It is interesting
    to evaluate the 10 nearest neighbors of a token with respect to cosine similarity.
    In the lower layers, for both models the nearest tokens were in most cases the
    same tokens, except for a few content words. In the higher layers this changed
    and different tokens were the nearest tokens. This shows that more and more context
    is included in the embeddings of higher layers.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 基于标记的*自相似性*是不同句子中找到的相同标记的平均余弦相似度。在BERT以及GPT-2中，内容词的自相似性高于功能词 [[66](#CR66)]。这可能表明，在不同的上下文中，功能词具有更多样化的语义角色。评估一个标记的10个最近的邻居（根据余弦相似度）是很有趣的。在较低层，对于这两个模型，最近的标记在大多数情况下是相同的标记，除了少数内容词。在较高层，这种变化发生了，最近的标记是不同的标记。这表明，随着层级的提高，越来越多的上下文被包含在嵌入中。
- en: The authors also investigated the embeddings generated by a number of other
    PLM types. They find that their structure is very different as they form different
    clusters and manifolds. They argue that this structure has to be taken into account
    for new applications of the models.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们还调查了由多种其他PLM类型生成的嵌入。他们发现，由于形成了不同的簇和流形，它们的结构非常不同。他们认为，这种结构必须考虑用于模型的新应用。
- en: 2.2.3 Generating a Sequence of Words
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 生成单词序列
- en: After training the GPT model can predict the probabilities of the tokens at
    the next position *t* + 1 given the previous tokens *v*[1], …, *v*[*t*]. To generate
    a text we have to select a sequence of tokens according to these probabilities.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 训练GPT模型后，可以预测给定之前的标记*v*[1]，…，*v*[*t*]的下一个位置*t* + 1的标记概率。为了生成文本，我们必须根据这些概率选择一系列标记。
- en: '*Random sampling* selects the next token according to the predicted probabilities.
    This approach sometimes can select very improbable tokens such that the probability
    of the whole sentence gets too low. Although the individual probabilities are
    tiny, the probability of selecting an element of the group of improbable tokens
    is quite high. In addition, the estimates of small probability are often affected
    by errors.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机采样*根据预测的概率选择下一个标记。这种方法有时可以选中非常不可能的标记，从而使整个句子的概率变得太低。尽管个别概率很小，但选择不可能标记组中元素的概率相当高。此外，小概率的估计通常会受到误差的影响。'
- en: '*Top-k**sampling* takes into account only the *k* tokens with the highest probability
    to generate the next token. The probability mass is redistributed among them [[42](#CR42)]
    and used for randomly selecting a token.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Top-k采样*只考虑生成下一个标记的最高概率的*k*个标记。将这些概率质量重新分配给它们 [[42](#CR42)]，并用于随机选择一个标记。'
- en: '*Top-p**sampling* considers the smallest set of top candidates with the cumulative
    probability above a threshold (e.g. *p* = 0.95) and then selects the next token
    according to the redistributed probabilities [[58](#CR58)]. This approach limits
    the probability mass of rare tokens which are ignored.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Top-p采样*考虑了累积概率超过阈值（例如*p* = 0.95）的最小候选集，然后根据重新分配的概率选择下一个标记 [[58](#CR58)]。这种方法限制了罕见标记的概率质量，这些标记被忽略。'
- en: There are also strategies which explicitly avoid previously generated tokens
    by reducing the corresponding scores in the update formula [[67](#CR67)]. Both
    top-*k* and top-*p* sampling usually generate plausible token sequences and are
    actually employed to generate texts.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些策略明确避免之前生成的token，通过在更新公式中减少相应的分数[[67](#CR67)]。top-*k*和top-*p*采样通常生成合理的token序列，并且实际上被用于生成文本。
- en: There are a number of approaches to improve token selection. Meister et al.
    [[90](#CR90)] found that human-produced text tends to have evenly distribution
    of “surprise”. This means that the next token should on average not be too rare
    and not be too frequent. They propose a number of sampling criteria, e.g. a variance
    regularizer.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以改进token选择。Meister等人[[90](#CR90)]发现，人类产生的文本往往具有“惊喜”的均匀分布。这意味着下一个token的平均值不应该太稀少，也不应该太频繁。他们提出了一系列采样标准，例如方差正则化器。
- en: Martins et al. [[86](#CR86)] argue that softmax-generated output distributions
    are unrealistic, as they assign a positive probability to every output token.
    They propose the *Entmax transformation* which generates a sparse probability
    distribution from the computed scores, where part of the probabilities are exactly
    zero. The Entmax transformation can be controlled by a parameter *α* ≥ 1\. For
    *α* = 1 we get softmax and *α* = *∞* recovers ![$$\arg \max $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq43.png).
    For intermediate values *∞* > *α* > 1.0 some tokens get exactly zero probability.
    Entmax losses are convex and differentiable and therefore may be trained by backpropagation.
    As in top-*p* sampling and in opposition to top-*k* sampling, Entmax sampling
    considers a varying number of tokens depending on the context. Experiments show
    that Entmax leads to better perplexities and less repetitions than other approaches.
    Compared with top-*p* sampling it has a higher variation in the number of tokens
    considered.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Martins等人[[86](#CR86)]认为，由softmax生成的输出分布是不现实的，因为它们给每个输出token分配了正概率。他们提出了*Entmax变换*，它从计算的分数生成稀疏概率分布，其中部分概率正好为零。Entmax变换可以通过参数*α*≥1来控制。对于*α*＝1，我们得到softmax，而对于*α*＝*∞*，则恢复![$$\arg
    \max $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq43.png)。对于中间值*∞*＞*α*＞1.0，一些token将获得正好为零的概率。Entmax损失是凸的和可微的，因此可以通过反向传播进行训练。与top-*p*采样类似，但与top-*k*采样相反，Entmax采样根据上下文考虑不同数量的token。实验表明，Entmax比其他方法导致更好的困惑度和更少的重复。
- en: Khandelwal et al. [[68](#CR68)] try to improve the estimated probabilities of
    the language model by statistics of token *n*-grams. They perform a nearest neighbor
    search on the last tokens already processed. As distance measure they use the
    distances of the pre-trained embedding space. From the retrieved nearest neighbors
    they get additional evidence on the probable next token, which is merged with
    the token probabilities of the language model. In this way, they are able to improve
    the perplexity of language models. The approach is particularly helpful in predicting
    rare patterns, e.g. factual knowledge.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Khandelwal等人[[68](#CR68)]试图通过统计token *n*-grams来提高语言模型估计概率。他们在已处理的最后token上执行最近邻搜索。作为距离度量，他们使用预训练嵌入空间的距离。从检索到的最近邻中，他们获得关于可能下一个token的额外证据，这些证据与语言模型的token概率合并。这样，他们能够提高语言模型的困惑度。这种方法在预测罕见模式，例如事实知识方面特别有帮助。
- en: Yang et al. [[157](#CR157)] analyze the properties of the softmax function.
    They find that the standard softmax does not have enough capacity to model natural
    language, as it restricts the rank of the mapping to probabilities. They propose
    to predict probabilities by a *Mixture of Softmaxes*, a convex combination of
    different logistic classifiers, which is more expressive than a single softmax.
    The authors show that this modification yields better perplexities in language
    modeling and also improves the performance of other transformer architectures
    [[101](#CR101)].
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Yang等人[[157](#CR157)]分析了softmax函数的性质。他们发现，标准的softmax没有足够的容量来模拟自然语言，因为它限制了映射到概率的秩。他们提出使用*混合softmax*来预测概率，这是一种不同逻辑分类器的凸组合，比单个softmax更具表达性。作者表明，这种修改在语言建模中产生了更好的困惑度，并且也提高了其他transformer架构的性能[[101](#CR101)]。
- en: 2.2.4 The Advanced Language Model GPT-2
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.4 高级语言模型GPT-2
- en: '**GPT-2** [[118](#CR118)] is the first language model, which is able to generate
    documents of grammatically correct and semantically plausible text. Its largest
    version has 48 encoder blocks with 1.5B parameters and covers sequences of 1600
    tokens. Given an initial text the model adapts to the style and content of this
    text and generates an answer, which often cannot be distinguished from human-generated
    continuations. Longer generated texts, however, sometimes tend to be repetitive
    and less coherent.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-2** [[118](#CR118)] 是第一个能够生成语法正确且语义合理的文本的语言模型。其最大版本包含 48 个编码器块和 1.5B
    个参数，覆盖 1600 个标记的序列。给定一个初始文本，该模型会适应文本的风格和内容，并生成一个答案，这个答案通常无法与人类生成的续写区分开来。然而，生成的长文本有时会显得重复且不够连贯。'
- en: For GPT-2 top-*k* truncated sampling was used to generate the example text [[117](#CR117)]
    shown in Fig. [2.11](#Fig11). As can be seen there are no syntax errors and the
    generated content is plausible. The authors remark that one in two trials were
    of high quality. The model adapts to the style and content of the input text.
    This allows the user to generate realistic and coherent continuations about a
    topic they like. Obviously the topic has to be mentioned in the Reddit training
    data, which covers a broad spectrum of themes such as news, music, games, sports,
    science, cooking, and pets.![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig11_HTML.png)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GPT-2，使用了顶 *k* 截断采样来生成图 [2.11](#Fig11) 中所示的示例文本 [[117](#CR117)]。如图所示，没有语法错误，生成的内容是合理的。作者指出，一半的试验结果是高质量的。该模型适应输入文本的风格和内容。这使得用户能够生成关于他们喜欢的话题的逼真且连贯的续写。显然，该话题必须在
    Reddit 训练数据中提及，这些数据涵盖了新闻、音乐、游戏、体育、科学、烹饪和宠物等广泛的主题。![图 2.11](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig11_HTML.png)
- en: A screenshot of the input and generated by G P T 2 input texts.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 输入和生成的截图。
- en: Fig. 2.11
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.11
- en: Given the input text, GPT-2 generates a continuation by top-*k* sampling [[117](#CR117)].
    Quoted with kind permission of the authors
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入文本，GPT-2 通过顶 *k* 样本生成续写 [[117](#CR117)]。经作者同意引用
- en: The model was able to solve many tasks better than previous models without being
    trained on the specific task. This type of learning is called *zero-shot learning*.
    For example, GPT-2 had a perplexity of 35.8 on the test set of the Penn Treebank
    compared to the inferior prior Sota of 46.5 [[117](#CR117)]. This was achieved
    without training GPT-2 on the *Penn Treebank corpus* [[135](#CR135)].
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型能够在没有针对特定任务进行训练的情况下，解决许多任务并优于之前的模型。这种学习方式被称为 *零样本学习*。例如，GPT-2 在宾夕法尼亚树库测试集上的困惑度为
    35.8，相比之下，之前的 Sota 水平为 46.5 [[117](#CR117)]。这是在没有对 GPT-2 进行 *宾夕法尼亚树库语料库* [[135](#CR135)]
    训练的情况下实现的。
- en: 2.2.5 Fine-Tuning GPT
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.5 微调 GPT
- en: By fine-tuning, GPT-2 may be adapted to new types of text, for example new genres
    of text. To create song lyrics, for example, St-Amant [[4](#CR4)] uses a dataset
    of 12,500 English rock song lyrics and fine-tunes GPT-2 for 5 epochs. Then the
    model is able to continue the lyrics of pop songs, which had not been seen by
    the model during training. The model had a high Bleu score of 68 when applied
    to song lyrics. Another experiment describes the generation of poetry [[19](#CR19)].
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 通过微调，GPT-2 可以适应新的文本类型，例如新的文本流派。例如，为了创建歌词，St-Amant [[4](#CR4)] 使用了一个包含 12,500
    首英文摇滚歌曲歌词的数据集，并对 GPT-2 进行了 5 个周期的微调。然后，该模型能够继续流行歌曲的歌词，这些歌曲在训练期间并未被模型看到。当应用于歌词时，该模型获得了
    68 的高 Bleu 分数。另一个实验描述了诗歌的生成 [[19](#CR19)]。
- en: Similar to BERT, a pre-trained GPT-2 can also be modified to perform a classification
    task. An example is fine-tuning to the classification of the sentiment of a document
    as positive or negative. Radford et al. [[116](#CR116)] encode the classification
    task as a text with specific tokens and a final end token *[END]*. Then the model
    has to predict the sequence. The embedding of *[END]* in the highest layer is
    used as input to a logistic classifier, which is trained to predict the probability
    of classes. The authors found that including language modeling ([2.11](#Equ11))
    of the fine-tuning data as an auxiliary objective to fine-tuning improved generalization
    and accelerated convergence. They were able to improve the score on GLUE (Sect.
    [2.1.5](#Sec7)) from 68.9 to 72.8 and achieved Sota in 7 out of 8 GLUE tasks for
    natural language understanding. The results show that language models capture
    relevant information about syntax and semantics.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 与BERT类似，预训练的GPT-2也可以修改以执行分类任务。一个例子是将微调用于将文档的情感分类为正面或负面。Radford等人[[116](#CR116)]将分类任务编码为包含特定标记和最终结束标记*[END]*的文本。然后模型必须预测序列。*[END]*在最高层的嵌入被用作输入到一个逻辑分类器，该分类器被训练来预测类别的概率。作者发现，将语言模型([2.11](#Equ11))的微调数据作为辅助目标包括在内，可以改善泛化并加速收敛。他们能够将GLUE（第[2.1.5](#Sec7)节）的分数从68.9提高到72.8，并在8个GLUE任务中的7个任务上实现了Sota，用于自然语言理解。结果表明，语言模型捕捉了有关语法和语义的相关信息。
- en: However, GPT operates from left to right when predicting the next token. In
    the sentences *“I went to the bank to deposit cash”* and *“I went to the bank
    to sit down”*, it will create the same context-sensitive embedding for *“bank”*
    when predicting *“sit”* or *“deposit”*, although the meaning of the token *“bank”*
    is different in both contexts. In contrast, BERT is bidirectional and takes into
    account all tokens of the text when predicting masked tokens. This fact explains
    why BERT for some tasks shows a better performance.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GPT在预测下一个标记时是从左到右操作的。在句子“我去银行存现金”和“我去银行坐下”中，当预测“坐下”或“存现金”时，它将为“银行”创建相同的上下文敏感嵌入，尽管在这两种情况下标记“银行”的意义是不同的。相比之下，BERT是双向的，在预测掩码标记时考虑了文本中的所有标记。这一事实解释了为什么BERT在某些任务上表现出更好的性能。
- en: 2.2.6 Summary
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.6 摘要
- en: GPT has an architecture similar to a BERT model that generates the tokens of
    a sentence one by one. It starts with an input sequence of tokens, which can be
    empty. Tokens are encoded as a sum of token embeddings and position embeddings.
    GPT uses the same encoder blocks as BERT, but the computations are masked, i.e.
    restricted to the already generated tokens. For these tokens the model produces
    contextual embeddings in several layers. The embedding of the last token in the
    top layer is entered into a logistic classifier and this calculates the probability
    of the tokens for the next position. Subsequently, the observed token is appended
    to the input at the next position and the computations are repeated for the next
    but one position. Therefore, GPT is called an autoregressive language model.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: GPT具有与BERT模型类似的架构，逐个生成句子的标记。它从一个标记序列的输入开始，这可能为空。标记被编码为标记嵌入和位置嵌入的总和。GPT使用与BERT相同的编码器块，但计算是掩码的，即限制在已经生成的标记上。对于这些标记，模型在几层中产生上下文嵌入。顶层最后一个标记的嵌入被输入到一个逻辑分类器，并计算下一个位置的标记的概率。随后，观察到的标记被附加到下一个位置，并重复计算下一个但一个位置的计算。因此，GPT被称为自回归语言模型。
- en: During training the parameters are changed by stochastic gradient descent in
    such a way that the model predicts high probabilities of the observed tokens in
    the training data. The maximum likelihood criterion is used, which optimizes the
    probability of the input data. When the model has been trained on a large text
    dataset it can be applied. Conditional to a start text it can sequentially compute
    the probability of the next token. Then a new token can be selected according
    to the probabilities.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，参数通过随机梯度下降进行改变，使得模型预测训练数据中观察到的标记的高概率。使用最大似然准则，该准则优化输入数据的概率。当模型在大型文本数据集上训练后，它可以被应用。在给定起始文本的条件下，它可以顺序地计算下一个标记的概率。然后可以根据概率选择一个新的标记。
- en: If all alternative tokens are taken into account, rare tokens are often selected.
    Usually, the number of eligible tokens is restricted to *k* high-probability tokens
    (top-*k* sampling) or only high-probability tokens are included up to a prescribed
    probability sum *p* (top-*p* sampling). In this way, much better texts are generated.
    Advanced language models like GPT-2 have billions of parameters and are able to
    generate plausible stories without syntactic errors.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果考虑所有可能的标记，通常会选择稀有标记。通常，合格标记的数量限制为 *k* 个高概率标记（top-*k*采样）或仅包括高达规定的概率和 *p* 的高概率标记（top-*p*采样）。这样，可以生成更好的文本。像GPT-2这样的高级语言模型拥有数十亿参数，能够生成没有语法错误的合理故事。
- en: GPT models can also be fine-tuned. A first type of fine-tuning adapts the model
    to a specific text genre, e.g. poetry. Alternatively, GPT can be used as a classifier,
    where the output embedding of the most recently generated token for an input text
    is input to a logistic classifier. With this approach, GPT-2 was able to improve
    Sota for most natural language understanding task in the GLUE benchmark. This
    shows that GPT-2 has acquired a comprehensive knowledge about language. However,
    since self-attention is only aware of past tokens, models like BERT are potentially
    better as they can take into account all input tokens during computations.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型也可以进行微调。一种微调类型是使模型适应特定的文本类型，例如诗歌。或者，GPT可以用作分类器，其中最近生成的输入文本的标记的输出嵌入被输入到逻辑分类器中。通过这种方法，GPT-2能够在GLUE基准测试中的大多数自然语言理解任务上提高Sota。这表明GPT-2已经获得了关于语言的全面知识。然而，由于自注意力只关注过去标记，因此像BERT这样的模型可能更好，因为它们可以在计算时考虑所有输入标记。
- en: Chapter [3](528393_1_En_3_Chapter.xhtml) discusses how to improve the performance
    of GPT models, in particular by using more parameters (Sect. [3.​1.​2](528393_1_En_3_Chapter.xhtml#Sec3)).
    These large models with billions of parameters can be instructed to perform a
    number of tasks without fine-tuning (Sect. [3.​6.​3](528393_1_En_3_Chapter.xhtml#Sec41)).
    In the Chaps. [5](528393_1_En_5_Chapter.xhtml)–[7](528393_1_En_7_Chapter.xhtml),
    we describe a number of applications of GPT-models such as question-answering
    (Sect. [6.​2.​3](528393_1_En_6_Chapter.xhtml#Sec14)), story generation (Sect.
    [6.​5](528393_1_En_6_Chapter.xhtml#Sec31)), or image generation from text (Sect.
    [7.​2.​6](528393_1_En_7_Chapter.xhtml#Sec18)).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 第[3章](528393_1_En_3_Chapter.xhtml)讨论了如何提高GPT模型的表现，特别是通过使用更多的参数（见[3.1.2节](528393_1_En_3_Chapter.xhtml#Sec3)）。这些拥有数十亿参数的大型模型可以被指导执行多项任务而无需微调（见[3.6.3节](528393_1_En_3_Chapter.xhtml#Sec41)）。在第[5章](528393_1_En_5_Chapter.xhtml)至[7章](528393_1_En_7_Chapter.xhtml)中，我们描述了GPT模型的一些应用，例如问答（见[6.2.3节](528393_1_En_6_Chapter.xhtml#Sec14)）、故事生成（见[6.5节](528393_1_En_6_Chapter.xhtml#Sec31)）或从文本生成图像（见[7.2.6节](528393_1_En_7_Chapter.xhtml#Sec18)）。
- en: '2.3 Transformer: Sequence-to-Sequence Translation'
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 Transformer：序列到序列翻译
- en: 2.3.1 The Transformer Architecture
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 Transformer架构
- en: Translation models based on Recurrent Neural Networks (Sect. [1.​6](528393_1_En_1_Chapter.xhtml#Sec6))
    have a major limitation caused by the sequential nature of RNNs. The number of
    operations required to determine the relation between tokens *v*[*s*] and *v*[*t*]
    grows with the distance *t* − *s* between positions. The model has to store the
    relations between all tokens simultaneously in a vector, making it difficult to
    learn complex dependencies between distant positions.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 基于循环神经网络（RNN）的翻译模型（见[1.6节](528393_1_En_1_Chapter.xhtml#Sec6)）由于RNN的序列性质存在一个主要限制。确定标记
    *v*[*s*] 和 *v*[*t*] 之间关系所需的操作数量随着位置之间的距离 *t*− *s* 的增长而增加。模型必须同时在一个向量中存储所有标记之间的关系，这使得学习远程位置之间的复杂依赖关系变得困难。
- en: The *Transformer* [[141](#CR141)]—similar to RNN-translation models—is based
    on an encoder and a decoder module (Fig. [2.13](#Fig13)). The encoder is very
    similar to BERT, while the decoder resembles GPT. It is a *sequence-to-sequence
    model* (*Seq2seq*), which translates a source text of the input language to a
    target text in the target language. Instead of relating distant tokens by a large
    number of computation steps, it directly computes the self-attention between these
    token in parallel in one step.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*Transformer* [[141](#CR141)]——类似于RNN翻译模型——基于编码器和解码器模块（见图[2.13](#Fig13)）。编码器非常类似于BERT，而解码器类似于GPT。它是一个
    *序列到序列模型* (*Seq2seq*)，将输入语言的源文本翻译成目标语言的文本。它不是通过大量计算步骤来关联远程标记，而是在一步中并行直接计算这些标记之间的自注意力。'
- en: The *encoder* generates contextual embeddings ![$$\tilde {{\boldsymbol {x}}}_1,\ldots
    ,\tilde {{\boldsymbol {x}}}_{T_{\text{src}}}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq44.png)
    of the source text tokens ![$$v_1, \ldots , v_{T_{\text{src}}}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq45.png)
    with exactly the same architecture as the BERT model (Fig. [2.4](#Fig4)). The
    original transformer [[141](#CR141)] uses 6 encoder blocks. The generated embeddings
    of the last layer are denoted as ![$$\breve {\boldsymbol {x}}_1,\ldots ,\breve
    {\boldsymbol {x}}_{T_{\text{src}}}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq46.png).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码器**使用与 BERT 模型（图 [2.4](#Fig4)）完全相同的架构生成源文本标记 ![$$v_1, \ldots , v_{T_{\text{src}}}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq45.png)
    的上下文嵌入 ![$$\tilde {{\boldsymbol {x}}}_1,\ldots ,\tilde {{\boldsymbol {x}}}_{T_{\text{src}}}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq44.png)。原始变换器
    [[141](#CR141)] 使用 6 个编码器块。最后一层的生成嵌入表示为 ![$$\breve {\boldsymbol {x}}_1,\ldots
    ,\breve {\boldsymbol {x}}_{T_{\text{src}}}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq46.png)。'
- en: The transformer *decoder* step by step computes the probability distributions
    ![$$p(S_{t}|s_1,\ldots ,s_{t-1},v_1,\ldots ,v_{T_{\text{src}}})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq47.png)
    of target tokens *s*[*t*] similar to the Recurrent Neural Network. Note that the
    source tokens *v*[*i*] as well as observed target tokens *s*[*j*] are taken as
    conditions. By the definition of conditional probability this yields the total
    probability of the output distribution![$$\displaystyle \begin{aligned} \begin{array}{rcl}
    &amp; &amp;\displaystyle {p(S_{1}\mkern1.5mu{=}\mkern1.5mu s_1,\ldots,S_{T}\mkern1.5mu{=}\mkern1.5mu
    s_T|v_1,\ldots,v_{T_{\text{src}}}) }\\ ~\qquad &amp; =&amp;\displaystyle p(S_T\mkern1.5mu{=}\mkern1.5mu
    s_T|s_1,\ldots,s_{T-1},v_1,\ldots,v_{T_{\text{src}}}) \cdots p(S_{1}\mkern1.5mu{=}\mkern1.5mu
    s_1|v_1,\ldots,v_{T_{\text{src}}}) , {} \end{array} \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ13.png)(2.13)where
    *S*[*t*] is a random variable with the possible target tokens *s*[*t*] at position
    *t* as its values. This probability is maximized during training.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器**解码器**逐步计算目标标记 *s*[*t*] 的概率分布 ![$$p(S_{t}|s_1,\ldots ,s_{t-1},v_1,\ldots
    ,v_{T_{\text{src}}})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq47.png)，类似于循环神经网络。请注意，源标记
    *v*[*i*] 以及观察到的目标标记 *s*[*j*] 都被作为条件。根据条件概率的定义，这产生了输出分布的总概率![$$\displaystyle \begin{aligned}
    \begin{array}{rcl} &amp; &amp;\displaystyle {p(S_{1}\mkern1.5mu{=}\mkern1.5mu
    s_1,\ldots,S_{T}\mkern1.5mu{=}\mkern1.5mu s_T|v_1,\ldots,v_{T_{\text{src}}}) }\\
    ~\qquad &amp; =&amp;\displaystyle p(S_T\mkern1.5mu{=}\mkern1.5mu s_T|s_1,\ldots,s_{T-1},v_1,\ldots,v_{T_{\text{src}}})
    \cdots p(S_{1}\mkern1.5mu{=}\mkern1.5mu s_1|v_1,\ldots,v_{T_{\text{src}}}) , {}
    \end{array} \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ13.png)(2.13)，其中
    *S*[*t*] 是一个随机变量，其值在位置 *t* 处为可能的目标标记 *s*[*t*]。这个概率在训练过程中被最大化。
- en: We denote the already translated tokens by *s*[0], *s*[1], …, *s*[*t*−1] were
    *s*[0] is the token *“[BOS]”* indicating the beginning of the output text. The
    decoder first computes a self-attention for these tokens using the formula ([2.4](#Equ4))
    as for BERT. As only part of the target tokens are covered and the rest is ‘masked’,
    this layer is called *masked multi-head self-attention* yielding intermediate
    contextual embeddings ![$$\tilde {\boldsymbol {s}}_0,\tilde {\boldsymbol {s}}_1,\ldots
    ,\tilde {\boldsymbol {s}}_{t-1}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq48.png)
    for the target tokens *s*[0], *s*[1], …, *s*[*t*−1].
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用 *s*[0]，*s*[1]，…，*s*[*t*−1] 表示已经翻译的标记，其中 *s*[0] 是标记 *“[BOS]”*，表示输出文本的开始。解码器首先使用公式
    ([2.4](#Equ4)) 计算这些标记的自注意力，就像 BERT 一样。由于只有部分目标标记被覆盖，其余部分被“掩码”，因此这一层被称为**掩码多头自注意力**，为目标标记
    *s*[0]，*s*[1]，…，*s*[*t*−1] 生成中间上下文嵌入 ![$$\tilde {\boldsymbol {s}}_0,\tilde {\boldsymbol
    {s}}_1,\ldots ,\tilde {\boldsymbol {s}}_{t-1}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq48.png)。
- en: Cross-Attention
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 交叉注意力
- en: Then the decoder performs a *cross-attention*![$$\text{CATL}(\tilde {{\boldsymbol
    {V}}},\breve {{\boldsymbol {X}}})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq49.png)
    with the input text embeddings of the highest encoder block (Fig. [2.12](#Fig12)).
    Here the query-vectors are computed for the embeddings of the target tokens ![$$\tilde
    {\boldsymbol {S}}_t=(\tilde {\boldsymbol {s}}_0,\tilde {\boldsymbol {s}}_1,\ldots
    ,\tilde {\boldsymbol {s}}_{t-1}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq50.png))
    provided by the respective decoder block. The key and value vectors are computed
    for the embeddings ![$$\breve {{\boldsymbol {X}}}=\breve {\boldsymbol {x}}_1,\ldots
    ,\breve {\boldsymbol {x}}_{T_{\text{src}}}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq51.png)
    of the last encoder block. Note that cross attention employs the same Eq. ([2.4](#Equ4))
    with matrices ***W***^((*q*)), ***W***^((*k*)), ***W***^((*v*)) as the BERT self-attentions.
    This is done in parallel and called *multi-head cross-attention*. In this way,
    information from the source text is taken into account. Subsequently, the embeddings
    computed by different heads are concatenated ([2.6](#Equ6)) and the result is
    transformed by a fully connected layer with ReLU activation ([2.7](#Equ7)). In
    addition, residual “bypass” connections are used as well as layer normalization
    [[6](#CR6)] for regularization. The output of the fully connected layer yields
    a new ‘output’ embedding ![$$\tilde {\boldsymbol {s}}_0,\ldots ,\tilde {\boldsymbol
    {s}}_{t-1}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq52.png)
    for the target tokens *s*[1], …, *s*[*t*−1]. Together these layers are called
    a *decoder block* (Fig. [2.13](#Fig13)).![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig12_HTML.png)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然后解码器与最高编码器块的输入文本嵌入执行*交叉注意力*![$$\text{CATL}(\tilde {{\boldsymbol {V}}},\breve
    {{\boldsymbol {X}}})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq49.png)。在这里，查询向量是为由相应解码器块提供的目标标记嵌入![$$\tilde
    {\boldsymbol {S}}_t=(\tilde {\boldsymbol {s}}_0,\tilde {\boldsymbol {s}}_1,\ldots
    ,\tilde {\boldsymbol {s}}_{t-1}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq50.png))计算的。键值向量是为最后一个编码器块的嵌入![$$\breve
    {{\boldsymbol {X}}}=\breve {\boldsymbol {x}}_1,\ldots ,\breve {\boldsymbol {x}}_{T_{\text{src}}}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq51.png)计算的。注意，交叉注意力使用与BERT自注意力相同的方程式([2.4](#Equ4))，即矩阵***W***^((*q*)),
    ***W***^((*k*)), ***W***^((*v*)*)。这是并行进行的，称为*多头交叉注意力*。通过这种方式，考虑了源文本的信息。随后，不同头计算出的嵌入被连接([2.6](#Equ6))，并通过具有ReLU激活([2.7](#Equ7))的全连接层进行转换。此外，还使用了残差“旁路”连接以及层归一化[[6](#CR6)]进行正则化。全连接层的输出产生了一个新的“输出”嵌入![$$\tilde
    {\boldsymbol {s}}_0,\ldots ,\tilde {\boldsymbol {s}}_{t-1}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq52.png)用于目标标记*s*[1]，…，*s*[*t*−1]。这些层共同被称为*解码器块*（图[2.13](#Fig13)）.![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig12_HTML.png)
- en: A flow diagram of a series of input tokens gives a new embedding via multi-head
    self-attention, k encoder blocks, cross-attention layer, decoder block, weighted
    value vectors, and fully connected layer, among others.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列输入标记的流程图通过多头自注意力、k个编码器块、交叉注意力层、解码器块、加权值向量和全连接层等，提供了新的嵌入。
- en: Fig. 2.12
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12
- en: The transformer [[141](#CR141)] uses *k* encoder blocks with the same architecture
    as in BERT (Fig. [2.4](#Fig4)) to generate contextual embeddings of all tokens
    of the input text. The decoder block is an autoregressive language model (Fig.
    [2.8](#Fig8)) and sequentially predicts the next token in the target language.
    Each encoder block contains a multi-head self-attention for the current sequence
    of output tokens. By cross-attention the information from the input sequence is
    included. The calculations are repeated for all current input tokens and are very
    similar to the self-attention computations. The resulting vector is transformed
    by a fully connected layer yielding the embeddings of that layer
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer [[141](#CR141)] 使用与BERT（图[2.4](#Fig4)）相同的架构的*k*个编码器块来生成输入文本中所有标记的上下文嵌入。解码器块是一个自回归语言模型（图[2.8](#Fig8)），并按顺序预测目标语言中的下一个标记。每个编码器块包含一个针对当前输出标记序列的多头自注意力。通过交叉注意力将输入序列的信息包含在内。对所有当前输入标记重复计算，与自注意力计算非常相似。通过全连接层转换得到的向量产生了该层的嵌入。
- en: '![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig13_HTML.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig13_HTML.png)'
- en: A flow diagram of a series of input tokens gives the target tokens via multi-head
    self-attention, multi-head cross-attention, encoder and decoder blocks, fully
    connected layers, further encoder blocks, final embedding vectors, logistic regression,
    and token probabilities.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列输入标记的流程图通过多头自注意力、多头交叉注意力、编码器和解码器块、全连接层、进一步的编码器块、最终嵌入向量、逻辑回归和标记概率来给出目标标记。
- en: Fig. 2.13
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13
- en: The transformer [[141](#CR141)] uses an encoder with the same architecture as
    BERT to generate embeddings of all tokens of the input sentence. Each encoder
    block performs multi-head self-attention of the input sequence followed by a fully
    connected layer (FCL) . The decoder is similar to a GPT model and sequentially
    predicts the next token in the target language. Each encoder block contains a
    multi-head cross-attention including the final embeddings of the encoder. Using
    the last output embedding of the final decoder block, a logistic classifier *L*
    predicts probabilities of the next token of the output sentence
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer [[141](#CR141)] 使用与 BERT 相同架构的编码器来生成输入句子中所有标记的嵌入。每个编码器块执行输入序列的多头自注意力，随后是一个全连接层（FCL）。解码器类似于
    GPT 模型，并按顺序预测目标语言中的下一个标记。每个编码器块包含一个多头交叉注意力，包括编码器的最终嵌入。使用最终解码器块的最后一个输出嵌入，逻辑分类器
    *L* 预测输出句子下一个标记的概率。
- en: The next decoder block gets the computed token output embeddings of the previous
    block as input and computes a new embedding of the target tokens *s*[1], …, *s*[*t*−1].
    The decoder consists of several decoder blocks (6 in the original model). Using
    the output embedding ***s***̆[*t*−1] of the righmost token *s*[*t*−1] in the last
    decoder block, the token probabilities ![$$p(S_{t}=s_t|s_1,\ldots ,s_{t-1},v_1,\ldots
    ,v_{T_{\text{src}}})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq53.png)
    of the next token *s*[*t*] of the target text at position *t* are predicted by
    a logistic classifier, e.g. for the token *“Maus”* in Fig. [2.13](#Fig13).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个解码器块接收前一个块的计算出的标记输出嵌入作为输入，并计算目标标记 *s*[1]，…，*s*[*t*−1] 的新嵌入。解码器由多个解码器块组成（原始模型中有6个）。使用最后一个解码器块中右侧标记
    *s*[*t*−1] 的输出嵌入 ***s***̆[*t*−1]，通过逻辑分类器预测目标文本在位置 *t* 的下一个标记 *s*[*t*] 的概率 ![$$p(S_{t}=s_t|s_1,\ldots
    ,s_{t-1},v_1,\ldots ,v_{T_{\text{src}}})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq53.png)，例如图
    [2.13](#Fig13) 中的标记 *“Maus”*。
- en: Note that for the prediction of a further token at position *t* + 1 the observed
    token *s*[*t*] is added to the computation ([2.13](#Equ13)) of the self-attentions
    in the decoder. Hence, the decoder embeddings change and all decoder computations
    have to be repeated. In this respect the model still works in a recursive way.
    Nevertheless, all self-attentions and cross-attentions in each layer are computed
    in parallel. However, the computations for the encoder are only performed once.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了预测位置 *t*+1 的下一个标记，观察到的标记 *s*[*t*] 被添加到解码器中自注意力的计算 ([2.13](#Equ13))。因此，解码器嵌入会改变，所有解码器计算都需要重新进行。在这方面，模型仍然以递归方式工作。然而，每一层的所有自注意力和交叉注意力都是并行计算的。然而，编码器的计算只进行一次。
- en: Sequences of variable length are padded with a special token up to the maximal
    length. This is done for the input and the output sequence. If a sequence is very
    short, a lot of space is wasted. Therefore, the sequence length may be varied
    in different mini-batches called buckets in the training data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 可变长度的序列通过添加一个特殊标记填充到最大长度。这适用于输入和输出序列。如果序列非常短，会浪费很多空间。因此，序列长度可能在不同的训练数据的小批量（称为桶）中变化。
- en: The transformer has a large set of parameters. First it requires embeddings
    of the input and target token vocabularies. Then there are the ***W***^((*q*)),
    ***W***^((*k*)), ***W***^((*v*)) matrices for the multi-head self-attention, the
    masked multi-head self-attention and the multi-head cross-attention of the different
    heads and layers. In addition, the parameters of the fully connected networks
    and the final logistic classifier have to be specified. While the base model had
    an input sequence length of 512 and 65M parameters, the big model had an input
    sequence length of 1024 and 213M parameters [[141](#CR141)]. The values of all
    these parameters are optimized during training.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器拥有大量参数。首先，它需要输入和目标标记词汇表的嵌入。然后，对于多头自注意力、掩码多头自注意力和不同头和层的多头交叉注意力，存在***W***^((*q*)),
    ***W***^((*k*)), ***W***^((*v*))矩阵。此外，全连接网络和最终逻辑分类器的参数也必须指定。而基础模型具有512个输入序列长度和65M个参数，大模型则具有1024个输入序列长度和213M个参数
    [[141](#CR141)]。所有这些参数的值在训练过程中都会被优化。
- en: The training data consists of pairs of an input sentence and the corresponding
    target sentence. Training aims to generate the target tokens with maximal probability
    for the given input tokens to maximize the joint conditional probability ([2.13](#Equ13))
    of the output sequence by stochastic gradient descent. In our example in Fig.
    [2.13](#Fig13) for the given input text *“The mouse likes cheese”* the product
    of conditional probabilities of the output tokens *“Die Maus mag Käse”* has to
    be maximized. The original model [[141](#CR141)], for instance, used 36M sentences
    of the WMT English-French benchmark data encoded as 32,000 wordpiece tokens. Both
    the encoder and decoder are trained simultaneously by stochastic gradient descent
    end-to-end, requiring 3.5 days with 8 GPUs.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据由一对输入句子和相应的目标句子组成。训练的目标是使用最大概率生成目标标记，以最大化输出序列的联合条件概率 ([2.13](#Equ13))，通过随机梯度下降实现。在我们的示例中（图[2.13](#Fig13)），对于给定的输入文本
    *“The mouse likes cheese”*，需要最大化输出标记 *“Die Maus mag Käse”* 的条件概率乘积。例如，原始模型 [[141](#CR141)]
    使用了WMT英语-法语基准数据集中的3600万个句子，编码为32,000个wordpiece标记。编码器和解码器通过端到端的随机梯度下降同时训练，需要3.5天和8个GPU。
- en: Cross-attention is the central part of the transformer, where the information
    from the input sentence is related to the translated output sentence. In Fig.
    [2.14](#Fig14) a German input sentence is displayed together with its English
    translation. Both sentences are tokenized by byte-pair encoding, where the beginning
    of a word is indicated by *“_”*. Below the strength of cross-attentions between
    the input tokens and output tokens is depicted for two different heads. Obviously
    the first input token *“_The”* has a special role.![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig14_HTML.png)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉注意力是变换器的核心部分，其中输入句子的信息与翻译后的输出句子相关联。在图[2.14](#Fig14)中，显示了一个德语输入句子及其英语翻译。这两个句子都通过字节对编码进行标记化，其中单词的开始由
    *“_”* 表示。下面描绘了两个不同头之间的输入标记和输出标记之间的交叉注意力强度。显然，第一个输入标记 *“_The”* 具有特殊的作用。![图2.14](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig14_HTML.png)
- en: Two color gradient cross-attention graphs for the English input sentence and
    the German translation.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 两个颜色渐变交叉注意力图，用于英语输入句子和德语翻译。
- en: Fig. 2.14
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14
- en: 'An English input sentence tokenized by Byte-Pair encoding and the translated
    tokenized German output sentence. Below are two cross-attention graphs from different
    heads of the 4-th decoder layer [[126](#CR126)]. Dark values indicate a low cross-attention
    score. Image source: [[126](#CR126)]'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 通过字节对编码标记化的英语输入句子和翻译后的标记化德语输出句子。下面是来自第四个解码层不同头的两个交叉注意力图 [[126](#CR126)]。暗色值表示交叉注意力分数低。图片来源：[[126](#CR126)]
- en: 2.3.2 Decoding a Translation to Generate the Words
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 解码翻译以生成单词
- en: After training, the Transformer is able to predict the probabilities of output
    tokens for an input sentence. For a practical translation, however, it is necessary
    to generate an explicit sequence of output tokens. Computing the output sequence
    with maximal probability is computationally hard, as then all output possible
    sequences have to be considered. Therefore, an approximate solution is obtained
    using greedy decoding or beam search.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，变换器能够预测输入句子的输出标记的概率。然而，对于实际的翻译，有必要生成一个显式的输出标记序列。计算具有最大概率的输出序列在计算上是困难的，因为那时必须考虑所有可能的输出序列。因此，使用贪婪解码或束搜索获得一个近似解。
- en: '**Greedy decoding** simply picks the most likely token with the highest probability
    at each decoding step until the end-of-sentence token is generated. The problem
    with this approach is that once the output is chosen at any time step *t*, it
    is impossible to go back and change the selection. In practice there are often
    problems with greedy decoding, as the available probable continuation tokens may
    not fit to a previously assigned token. As the decision cannot be revised, this
    may lead to suboptimal generated translations.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**贪婪解码**简单地在每个解码步骤中选择概率最高的标记，直到生成句子结束标记。这种方法的缺点是一旦在任何时间步 *t* 选择了输出，就无法回过头来更改选择。在实践中，贪婪解码经常出现问题，因为可用的可能后续标记可能不适合之前分配的标记。由于决策不能修改，这可能导致生成的翻译次优。'
- en: '**Beam search** [[52](#CR52)] keeps a fixed number *k* of possible translations
    *s*[1], …, *s*[*t*] of growing length (Fig. [2.15](#Fig15)). At each step each
    translation of length *t* is enlarged by *k* different tokens at position *t* + 1
    with the highest conditional probabilities ![$$p(S_{t+1}=s_{t+1}|s_1,\ldots ,s_{t},v_1,\ldots
    ,v_{T_{\text{src}}})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq54.png).
    From these *k* ∗ *k* token sequences only the *k* sequences with largest total
    probabilities ![$$p(s_1,\ldots ,s_{t+1}|v_1,\ldots ,v_{T_{\text{src}}})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq55.png)
    are retained. A complete translation (containing the end-of-sentence token) is
    added to the final candidate list. The algorithm then picks the translation with
    the highest probability (normalized by the number of target words) from this list.
    For *k* = 1 beam search reduces to greedy decoding. In practice, the translation
    quality obtained via beam search (size of 4) is significantly better than that
    obtained via greedy decoding. Larger beam sizes often lead to suboptimal solutions
    [[31](#CR31)]. However, beam search is computationally very expensive (25%–50%
    slower depending on the base architecture and the beam size) in comparison to
    greedy decoding [[29](#CR29)].![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig15_HTML.png)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**车轮搜索** [[52](#CR52)] 维护固定数量的 *k* 个可能翻译 *s*[1]，…，*s*[*t*] 的增长长度（图 [2.15](#Fig15)）。在每一步，长度为
    *t* 的每个翻译都会通过位置 *t*+1 的 *k* 个不同标记进行扩展，这些标记具有最高的条件概率 ![$$p(S_{t+1}=s_{t+1}|s_1,\ldots
    ,s_{t},v_1,\ldots ,v_{T_{\text{src}}})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq54.png)。在这些
    *k* * *k* 个标记序列中，仅保留总概率最大的 *k* 个序列 ![$$p(s_1,\ldots ,s_{t+1}|v_1,\ldots ,v_{T_{\text{src}}})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq55.png)。将完整的翻译（包含句子结束标记）添加到最终候选列表中。然后，算法从这个列表中选择概率最高的翻译（按目标词的数量归一化）。对于
    *k* = 1，车轮搜索退化为贪婪解码。在实践中，通过车轮搜索（大小为 4）获得的翻译质量（与贪婪解码相比）显著更好。更大的车轮大小通常会导致次优解 [[31](#CR31)]。然而，与贪婪解码相比，车轮搜索在计算上非常昂贵（根据基本架构和车轮大小，速度慢
    25%–50%）[[29](#CR29)]。![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig15_HTML.png)'
- en: A tree diagram of the beam search technique. It includes the B O S within square
    brackets block and the subsequent words with their scores.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 车轮搜索技术的树形图。它包括方括号内的 B O S 块以及随后的带分数的单词。
- en: Fig. 2.15
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15
- en: Beam search is a technique for decoding a language model and producing text.
    At every step, the algorithm keeps track of the *k* most probable partial translations
    (bold margin). The score of each translation is equal to its log probability.
    The beam search continues until it reaches the end token for every branch [[78](#CR78)]
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 车轮搜索是一种解码语言模型并生成文本的技术。在每一步，算法都会跟踪最可能的 *k* 个部分翻译（粗体边缘）。每个翻译的分数等于其对数概率。车轮搜索会持续进行，直到每个分支都达到结束标记
    [[78](#CR78)]
- en: 2.3.3 Evaluation of a Translation
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 翻译评估
- en: 'Traditionally, evaluation is done by comparing one or more reference translations
    to the generated translation, as described in the survey [[127](#CR127)]. There
    are a number of automatic evaluation metrics:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，评估是通过将一个或多个参考翻译与生成的翻译进行比较来完成的，如调查所述 [[127](#CR127)]。存在许多自动评估指标：
- en: '**Bleu** compares counts of 1-grams to 4-grams of tokens. The Bleu metric ranges
    from 0 to 1, where 1 means an identical output with the reference. Although Bleu
    correlates well with human judgment [[110](#CR110)], it relies on precision alone
    and does not take into account recall—the proportion of the matched *n*-grams
    out of the total number of *n*-grams in the reference translation.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**Bleu** 比较了标记的 1-gram 到 4-gram 的计数。Bleu 度量范围从 0 到 1，其中 1 表示与参考相同的输出。尽管 Bleu
    与人工判断有很好的相关性 [[110](#CR110)]，但它仅依赖于精确度，并且不考虑召回率——匹配的 *n*-gram 在参考翻译中总 *n*-gram
    数中的比例。'
- en: '**Rouge** [[80](#CR80)] unlike Bleu is a recall-based measure and determines
    which fraction of the words or n-grams in the reference text appear in the generated
    text. It determines, among other things, the overlap of unigrams or bigrams as
    well as the longest common subsequence between a pair of texts. Different versions
    are used: Rouge-1 measures the overlap of unigram (single words) between the pair
    of texts. Rouge-2 determines the overlap of bigrams (two-words sequences) between
    the pair of texts. Rouge-L: measures the length of the longest sequence of words
    (not necessarily consecutive, but still in order) that is shared between both
    texts. This length is divided by the number of words in the reference text.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**Rouge** [[80](#CR80)] 与 Bleu 不同，是一个基于召回率的度量，它确定参考文本中哪些比例的单词或 n-gram 出现在生成的文本中。它确定了许多其他因素，包括单语元或双语元的重叠以及一对文本之间的最长公共子序列。使用不同的版本：Rouge-1
    测量文本对之间单语元（单个单词）的重叠。Rouge-2 确定文本对之间双语元（两个单词序列）的重叠。Rouge-L：测量两个文本之间共享的最长序列的长度（不一定是连续的，但仍然是有序的）。这个长度除以参考文本中的单词数。'
- en: '**Meteor** [[75](#CR75)] was proposed to address the deficits of Bleu. It performs
    a word-to-word alignment between the translation output and a given reference
    translation. The alignments are produced via a sequence of word-mapping modules.
    These check, if the words are exactly the same, same after they are stemmed using
    the Porter stemmer, and if they are synonyms of each other. After obtaining the
    final alignment, Meteor computes an F-value, which is a parameterized harmonic
    mean of unigram precision and recall. Meteor has also demonstrated to have a high
    level of correlation with human judgment, often even better than Bleu.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**Meteor** [[75](#CR75)] 被提出以解决 Bleu 的不足。它在翻译输出和给定的参考翻译之间执行词对词的对齐。这些对齐是通过一系列词映射模块产生的。这些模块检查单词是否完全相同，或者在使用
    Porter 词干提取器之后是否相同，以及它们是否是彼此的同义词。在获得最终对齐后，Meteor 计算一个 F 值，这是一个单语元精确度和召回率的参数化调和平均值。Meteor
    还表明与人工判断有很高的相关性，通常甚至优于 Bleu。'
- en: '**BERTscore** [[164](#CR164)] takes into account synonyms and measures the
    similarity of embeddings between the translation and the reference. It computes
    the cosine similarity between all token embeddings of both texts. Then a greedy
    matching approach is used to determine assignments of tokens. The maximum assignment
    similarity is used as BERTscore.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERTscore** [[164](#CR164)] 考虑了同义词并测量了翻译和参考之间嵌入的相似性。它计算了两个文本中所有标记嵌入之间的余弦相似度。然后使用贪婪匹配方法来确定标记的分配。最大分配相似度被用作
    BERTscore。'
- en: For high-quality translations, however, there is a noticeable difference between
    human judgment and automatic evaluation. Therefore, most high-end comparisons
    today use human experts to assess the quality of translation and other text generation
    methods. Since the transformer was proposed by Vaswani et al. [[141](#CR141)]
    in 2017, its variants were able to raise the Sota in language translation performance,
    e.g. for translation on WMT2014 English-French from 37.5 to 46.4 Bleu score.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高质量的翻译，然而，人工判断和自动评估之间存在明显的差异。因此，今天大多数高端比较都使用人工专家来评估翻译质量和其他文本生成方法的质量。自从 Vaswani
    等人在 2017 年提出了 transformer [[141](#CR141)]，其变体能够将语言翻译性能的 Sota 提高到新的水平，例如，对于 WMT2014
    英语-法语翻译，从 37.5 提高到 46.4 的 Bleu 分数。
- en: The transformer architecture was analyzed theoretically. Yun et al. [[160](#CR160),
    [161](#CR161)] showed that transformers are expressive enough to capture all continuous
    sequence to sequence functions with a compact domain. Pérez et al. [[112](#CR112)]
    derived that the full transformer is Turing complete, i.e. can simulate a full
    Turing machine.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: transformer 架构从理论上进行了分析。Yun 等人 [[160](#CR160), [161](#CR161)] 表明，transformers
    足够表达，可以捕获所有具有紧凑域的连续序列到序列函数。Pérez 等人 [[112](#CR112)] 推导出完整的 transformer 是图灵完备的，即可以模拟完整的图灵机。
- en: 2.3.4 Pre-trained Language Models and Foundation Models
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.4 预训练语言模型和基础模型
- en: A model *language model* either computes the joint probability or the conditional
    probability of natural language texts and potentially includes all information
    about the language. BERT is an *autoencoder* language models containing encoder
    blocks to generate contextual embeddings of tokens. GPT is an *autoregressive
    language models* which predicts the next token of a sequence and restricts self-attention
    to tokens which already have been generated. *Transformers* (or *Transformer encoder-decoders*)
    use a transformer encoder to convert the input text to contextual embeddings and
    generate the translated text with an autoregressive transformer decoder utilizing
    the encoder embeddings as inputs (Fig. [2.16](#Fig16)). These models are the backbone
    of modern NLP and are collectively called *Pre-trained Language Models* (*PLM*).![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig16_HTML.png)
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一种模型 *语言模型* 要么计算自然语言文本的联合概率，要么计算条件概率，并且可能包含有关语言的所有信息。BERT 是一个包含编码器块以生成标记上下文嵌入的
    *自动编码器* 语言模型。GPT 是一个 *自回归语言模型*，它预测序列的下一个标记，并将自注意力限制在已经生成的标记上。*转换器*（或 *转换器编码器-解码器*）使用转换器编码器将输入文本转换为上下文嵌入，并使用自回归转换器解码器（利用编码器嵌入作为输入）生成翻译文本（图
    [2.16](#Fig16)）。这些模型是现代 NLP 的基础，统称为 *预训练语言模型* (*PLM*)。![图 2.16](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig16_HTML.png)
- en: 3 flow diagrams of the B E R T autoencoder, G P T language model, and transformer
    encoder-decoder from left to right. They include input tokens, transformer encoder
    and decoder blocks, L classifiers, and target tokens.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 从左到右分别是 BERT 自动编码器、GPT 语言模型和转换器编码器-解码器的 3 个流程图。它们包括输入标记、转换器编码器和解码器块、L 个分类器和目标标记。
- en: Fig. 2.16
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.16
- en: Autoencoders like BERT (left) and autoregressive LMs like GPT-2 (middle) use
    transformer blocks to generate contextual embeddings of tokens. The transformer
    (right) combines a transformer encoder and an autoregressive transformer decoder
    to produce a translation. All models predict the probability of tokens with a
    logistic classifier *L*. Collectively these models are called Pre-trained Language
    Models (PLMs)
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 BERT（左侧）和自回归语言模型 GPT-2（中间）的自动编码器使用转换器块生成标记的上下文嵌入。转换器（右侧）结合转换器编码器和自回归转换器解码器来生成翻译。所有模型都使用逻辑分类器
    *L* 预测标记的概率。这些模型统称为预训练语言模型 (PLM)。
- en: All these models, especially BERT and GPT, are initialized via pre-training
    on a large corpus of text documents. During pre-training, parts of the input are
    hidden from the model, and the model is trained to reconstruct these parts. This
    has proven to be extremely effective in building strong representations of language
    and in finding parameter initializations for highly expressive NLP models that
    can be adapted to specific tasks. Finally, these models provide probability distributions
    over language that we can sample from.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些模型，尤其是 BERT 和 GPT，都是通过在大规模文本语料库上进行预训练来初始化的。在预训练期间，输入的部分被隐藏起来，模型被训练来重建这些部分。这已被证明在构建强大的语言表示以及在寻找高度表达力的
    NLP 模型的参数初始化方面非常有效，这些模型可以适应特定任务。最后，这些模型提供了关于语言的概率分布，我们可以从中采样。
- en: Most network types have some built-in assumptions called *inductive bias*. Convolutional
    networks have local kernel functions that are shifted over the input matrix and
    therefore have an inductive bias of translation invariance and locality. Recurrent
    networks apply the same network to each input position and have a temporal invariance
    and locality. The BERT architecture makes only few assumptions about the structural
    dependency in data. The GPT model is similar to the RNN as it assumes a Markovian
    structure of dependencies to the next token. As a consequence, PLMs often require
    more training data to learn the interactions between different data points, but
    can later represent these interactions more accurately than other model types.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数网络类型都有一些内置的假设，称为 *归纳偏差*。卷积网络具有在输入矩阵上移动的局部核函数，因此具有平移不变性和局部性的归纳偏差。循环网络对每个输入位置应用相同的网络，具有时间不变性和局部性。BERT
    架构对数据中的结构依赖性只有少数假设。GPT 模型与 RNN 类似，因为它假设下一个标记的依赖关系具有马尔可夫结构。因此，PLM 通常需要更多的训练数据来学习不同数据点之间的交互，但可以比其他模型类型更准确地表示这些交互。
- en: Historically, learned embedding vectors were used as representations of words
    for downstream tasks (Fig. [2.17](#Fig17)). As early as 2003 Bengio et al. [[15](#CR15)]
    proposed a distributed vector representation of words to predict the next word
    by a recurrent model. In 2011 Collobert et al. [[32](#CR32)] successfully employed
    word embeddings for part-of-speech tagging, chunking, named entity recognition,
    and semantic role labeling. In 2013 Mikolov et al. [[93](#CR93)] derived their
    word embeddings using a logistic classifier. In 2015 Dai et al. [[33](#CR33)]
    trained embeddings with an RNN language model in a self-supervised way and later
    applied it to text classification. In 2017 McCann et al. [[87](#CR87)] pre-trained
    multilayer LSTMs for translation computing contextualized word vectors, which
    are later used for various classification tasks.![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig17_HTML.png)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，学习到的嵌入向量被用作下游任务的词表示（图[2.17](#Fig17)）。早在2003年，Bengio等人[[15](#CR15)]就提出了一个通过循环模型预测下一个词的词的分布式向量表示。2011年，Collobert等人[[32](#CR32)]成功地将词嵌入应用于词性标注、分块、命名实体识别和语义角色标注。2013年，Mikolov等人[[93](#CR93)]使用逻辑分类器推导出他们的词嵌入。2015年，Dai等人[[33](#CR33)]以自监督的方式训练了RNN语言模型中的嵌入，后来将其应用于文本分类。2017年，McCann等人[[87](#CR87)]预训练了多层LSTM用于翻译计算上下文化的词向量，这些词向量后来被用于各种分类任务。![图2.17](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig17_HTML.png)
- en: A chronological timeline of the developments from 2011 to 2022\. They include
    word embeddings for N E R, word embeddings by logistic regression, and embeddings
    by R N N, among others.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 从2011年到2022年的发展时间线。它们包括用于N E R的词嵌入、通过逻辑回归的词嵌入以及通过R N N的嵌入等。
- en: Fig. 2.17
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17
- en: Timeline for the development of embeddings, pre-training and fine-tuning
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入、预训练和微调的发展时间线
- en: In the same year Vaswani et al. [[141](#CR141)] developed the attention-only
    transformer for language translation. In 2018 Howard et al. [[59](#CR59)] pre-trained
    a language model (ULMFiT), and demonstrated the effectiveness of fine-tuning to
    different target tasks by updating the full (pre-trained) model for each task.
    In the same year Howard et al. [[116](#CR116)] used a pre-trained autoregressive
    part of the transformer [[141](#CR141)] to solve a large number of text understanding
    problems by fine-tuned models. At the same time Devlin et al. [[39](#CR39)] pre-trained
    the autoencoder using the masked language model objective and adapted this BERT
    model to many downstream tasks by fine-tuning. In 2019 Radford et al. [[118](#CR118)]
    presented the GPT-2 language model, which was able to generate semantically convincing
    texts. Brown et al. [[21](#CR21)] proposed the GPT-3 model, which could be instructed
    to solve NLP-tasks by a task description and some examples. In 2021 Ramesh et
    al. [[121](#CR121)] applied language modeling to text and pictures and were able
    to create impressive pictures from textual descriptions. Borgeaud et al. [[18](#CR18)]
    presented the Retro model that answers questions by retrieving information from
    a text collection of 2 trillion tokens and composes an answer in natural language.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在同年，Vaswani等人[[141](#CR141)]开发了仅关注注意力的翻译变压器。2018年，Howard等人[[59](#CR59)]预训练了语言模型（ULMFiT），并通过更新每个任务的完整（预训练）模型来展示微调对不同目标任务的有效性。同年，Howard等人[[116](#CR116)]使用预训练的变压器自回归部分[[141](#CR141)]通过微调模型解决了大量文本理解问题。同时，Devlin等人[[39](#CR39)]使用掩码语言模型目标预训练了自动编码器，并通过微调将此BERT模型应用于许多下游任务。2019年，Radford等人[[118](#CR118)]提出了GPT-2语言模型，能够生成语义上令人信服的文本。Brown等人[[21](#CR21)]提出了GPT-3模型，可以通过任务描述和一些示例来指导解决NLP任务。2021年，Ramesh等人[[121](#CR121)]将语言建模应用于文本和图片，并能够从文本描述中创建令人印象深刻的图片。Borgeaud等人[[18](#CR18)]提出了Retro模型，该模型通过从包含2000亿个标记的文本集合中检索信息来回答问题，并以自然语言组成答案。
- en: Almost all state-of-the-art NLP models are now adapted from one of a few Pre-trained
    Language Models, such as BERT, GPT-2, T5, etc. PLMs are becoming larger and more
    powerful, leading to new breakthroughs and attracting more and more research attention.
    Due to the huge increase in performance, some research groups have suggested that
    large-scale PLMs should be called *Foundation Models*, as they constitute a ‘foundational’
    breakthrough technology that can potentially impact many types of applications
    [[17](#CR17), p. 3]. In this book, we reserve the term ‘Foundation Models’ for
    large Pre-trained Language Models with more than a billion parameters, since these
    models are able of generating fluent text, can potentially handle different media,
    and can usually be instructed by prompts to perform specific tasks.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有最先进的自然语言处理模型现在都是从少数几个预训练语言模型中改编而来，例如 BERT、GPT-2、T5 等。PLM 正在变得更大、更强大，从而带来新的突破，并吸引越来越多的研究关注。由于性能的巨大提升，一些研究小组建议将大规模
    PLM 称为 *基础模型*，因为它们构成了一种“基础性”的突破性技术，可能影响许多类型的应用 [[17](#CR17), p. 3]。在本书中，我们将“基础模型”一词保留给具有超过十亿参数的大型预训练语言模型，因为这些模型能够生成流畅的文本，能够处理不同的媒体，并且通常可以通过提示来执行特定任务。
- en: If one of these models is improved, this high degree of homogeneity can lead
    to immediate benefits for many NLP applications. On the other hand all systems
    could share the same problematic biases present in a few basic models. As we will
    see in later chapters PLM-based sequence modeling approaches are now applied to
    text (Sect. [2.2](#Sec11)), speech (Sect. [7.​1](528393_1_En_7_Chapter.xhtml#Sec1)),
    images (Sect. [7.​2](528393_1_En_7_Chapter.xhtml#Sec12)), videos (Sect. [7.​3](528393_1_En_7_Chapter.xhtml#Sec23)),
    computer code (Sect. [6.​5.​6](528393_1_En_6_Chapter.xhtml#Sec46)), and control
    (Sect. [7.​4](528393_1_En_7_Chapter.xhtml#Sec30)). These overarching capabilities
    of Foundation Models are depicted in Fig. [2.18](#Fig18).![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig18_HTML.png)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些模型中的任何一个得到改进，这种高度的同质性将立即为许多自然语言处理（NLP）应用带来好处。另一方面，所有系统都可能共享一些基本模型中存在的相同问题偏见。正如我们将在后续章节中看到的那样，基于预训练语言模型（PLM）的序列建模方法现在已应用于文本（第
    [2.2](#Sec11) 节）、语音（第 [7.1](528393_1_En_7_Chapter.xhtml#Sec1) 节）、图像（第 [7.2](528393_1_En_7_Chapter.xhtml#Sec12)
    节）、视频（第 [7.3](528393_1_En_7_Chapter.xhtml#Sec23) 节）、计算机代码（第 [6.5.6](528393_1_En_6_Chapter.xhtml#Sec46)
    节）和控制（第 [7.4](528393_1_En_7_Chapter.xhtml#Sec30) 节）。这些基础模型的整体能力在图 [2.18](#Fig18)
    中展示。![图 2.18](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig18_HTML.png)
- en: An illustrated process flow of the foundation model. It includes 5 types of
    data, training, a foundation model, and a series of tasks via adaption.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型的流程图示例。它包括 5 种类型的数据、训练、一个基础模型以及通过适配的一系列任务。
- en: Fig. 2.18
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.18
- en: A Foundation Model can integrate the information in the data from different
    modalities. Subsequently it can be adapted, e.g. by fine-tuning, to a wide range
    of downstream tasks [[17](#CR17), p. 6]. Credits for image parts in Table [A.​1](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab1)
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型可以整合来自不同模态的数据信息。随后，它可以通过微调等方式适应广泛的下游任务 [[17](#CR17), p. 6]。图像部分表格 [A.1](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab1)
    的版权归属。
- en: The next Sect. [2.4](#Sec27) discusses some common techniques for optimizing
    and regularizing pre-trained language models. In addition, some approaches to
    modify the architecture of these networks are presented. In Chap. [3](528393_1_En_3_Chapter.xhtml)
    we present a number of approaches to improve the capabilities of PLMs, especially
    by modifying the training tasks (Sect. [3.​1.​3](528393_1_En_3_Chapter.xhtml#Sec4)).
    In the Chaps. [5](528393_1_En_5_Chapter.xhtml)–[7](528393_1_En_7_Chapter.xhtml)
    we discuss a number of applications of PLMs. Chapter [5](528393_1_En_5_Chapter.xhtml)
    covers traditional NLP tasks like named entity recognition and relation extraction,
    where PLMs currently perform best. Most important applications of Foundation Models
    are on the one hand text generation and related tasks like question-answering
    and dialog systems, which are introduced in Chap. [6](528393_1_En_6_Chapter.xhtml).
    On the other hand Foundation Models can simultaneously process different media
    and perform tasks like image captioning, object detection in images, image generation
    following a text description, video interpretation, or computer game control,
    which are discussed in Chap. [7](528393_1_En_7_Chapter.xhtml). Because of the
    potential social and societal consequences of such Foundation Models, it is particularly
    important that researchers in this field keep society’s values and human rights
    in mind when developing and applying these models. These aspects are summarized
    in Sect. [8.​2](528393_1_En_8_Chapter.xhtml#Sec10).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个章节[2.4](#Sec27)讨论了一些优化和正则化预训练语言模型的常见技术。此外，还介绍了一些修改这些网络架构的方法。在第[3](528393_1_En_3_Chapter.xhtml)章中，我们提出了许多提高PLM能力的方法，特别是通过修改训练任务（第[3.1.3](528393_1_En_3_Chapter.xhtml#Sec4)节）。在第[5](528393_1_En_5_Chapter.xhtml)章至[7](528393_1_En_7_Chapter.xhtml)章中，我们讨论了PLM的许多应用。第[5](528393_1_En_5_Chapter.xhtml)章涵盖了传统的NLP任务，如命名实体识别和关系抽取，在这些任务中PLM目前表现最佳。基础模型最重要的应用一方面是文本生成和相关任务，如问答系统和对话系统，这些在[6](528393_1_En_6_Chapter.xhtml)章中介绍。另一方面，基础模型可以同时处理不同媒体，执行图像标题、图像中的对象检测、根据文本描述生成图像、视频解释或计算机游戏控制等任务，这些在[7](528393_1_En_7_Chapter.xhtml)章中讨论。由于这种基础模型可能带来的社会和伦理后果，研究人员在开发和应用这些模型时，特别重要的是要牢记社会的价值观和人权。这些方面在第[8.2](528393_1_En_8_Chapter.xhtml#Sec10)节中进行了总结。
- en: Available Implementations
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: The source code for many pre-trained language models (BERT, GPT, Transformers)
    as well as pre-trained models for different languages and text corpora can be
    downloaded from Hugging Face [https://​huggingface.​co/​transformers/​](https://huggingface.co/transformers/),
    Fairseq [https://​github.​com/​pytorch/​fairseq](https://github.com/pytorch/fairseq),
    TensorFlow [https://​www.​tensorflow.​org/​](https://www.tensorflow.org/) and
    PyTorch [https://​pytorch.​org/​](https://pytorch.org/). These toolkits also allow
    the flexible formulation of Deep Neural Networks and provide the automatic computation
    of gradients as well as optimization methods. All are able to execute computations
    in parallel and distribute them to different CPUs and Graphical Processing Units
    (GPUs).
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多预训练语言模型（BERT、GPT、Transformers）以及不同语言和文本语料库的预训练模型可以从Hugging Face [https://huggingface.co/transformers/](https://huggingface.co/transformers/)、Fairseq
    [https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq)、TensorFlow
    [https://www.tensorflow.org/](https://www.tensorflow.org/) 和 PyTorch [https://pytorch.org/](https://pytorch.org/)下载。这些工具包还允许灵活地制定深度神经网络，并提供自动计算梯度以及优化方法。所有这些都能并行执行计算并将它们分布到不同的CPU和图形处理单元（GPU）。
- en: PLMs are getting larger than the memory of a single GPU and require to distribute
    training code among several GPUs. This is supported by libraries like FastSeq
    [https://​github.​com/​microsoft/​fastseq](https://github.com/microsoft/fastseq),
    LightSeq [https://​github.​com/​bytedance/​lightseq](https://github.com/bytedance/lightseq),
    and FastT5 [https://​github.​com/​Ki6an/​fastT5](https://github.com/Ki6an/fastT5).
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PLM的规模已经超过了单个GPU的内存，需要将训练代码分布在多个GPU上。这由FastSeq [https://github.com/microsoft/fastseq](https://github.com/microsoft/fastseq)、LightSeq
    [https://github.com/bytedance/lightseq](https://github.com/bytedance/lightseq)和FastT5
    [https://github.com/Ki6an/fastT5](https://github.com/Ki6an/fastT5)等库支持。
- en: DeepSpeed [[122](#CR122)] was used to train the MT-NLG autoregressive LM with
    530B parameters (Sect. [3.​1.​2](528393_1_En_3_Chapter.xhtml#Sec3)) [https://​github.​com/​microsoft/​DeepSpeed](https://github.com/microsoft/DeepSpeed).
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DeepSpeed [[122](#CR122)]训练了具有530B参数的MT-NLG自回归语言模型（第[3.1.2](528393_1_En_3_Chapter.xhtml#Sec3)节）[https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)。
- en: Ecco [[2](#CR2)] [https://​github.​com/​jalammar/​ecco](https://github.com/jalammar/ecco)
    and BertViz [[144](#CR144)] [https://​github.​com/​jessevig/​bertviz](https://github.com/jessevig/bertviz)
    are tools to visualize the attentions and embeddings of PLMs.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ecco [[2](#CR2)] [https://github.com/jalammar/ecco](https://github.com/jalammar/ecco)
    和 BertViz [[144](#CR144)] [https://github.com/jessevig/bertviz](https://github.com/jessevig/bertviz)
    是用于可视化PLM的注意力和嵌入的工具。
- en: Transformers-interpret [https://​github.​com/​cdpierse/​transformers-interpret](https://github.com/cdpierse/transformers-interpret)
    is a model explainability tool designed for the Hugging Face package.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers-interpret [https://github.com/cdpierse/transformers-interpret](https://github.com/cdpierse/transformers-interpret)
    是为Hugging Face包设计的模型可解释性工具。
- en: Captum [[70](#CR70)] is a library [https://​captum.​ai/​](https://captum.ai/)
    to generate interpretations and explanations for the predictions of PyTorch models.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Captum [[70](#CR70)] 是一个库 [https://captum.ai/](https://captum.ai/)，用于生成PyTorch模型预测的解释和说明。
- en: 2.3.5 Summary
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.5 摘要
- en: A transformer is a sequence-to-sequence model, which translates a source text
    of the input language into a target text in the target language. It consists of
    an encoder with the same architecture as an autoencoder BERT model that computes
    contextual embeddings of tokens of the source text. The decoder resembles an autoregressive
    GPT model and sequentially generates the tokens of the target text. Internally,
    contextual embeddings of the target tokens are computed in the different layers.
    Each decoder block has an additional cross-attention module in which the query
    vectors are taken from the embeddings of the target tokens and the key and value
    vectors are computed for the embeddings of the source tokens of the last layer.
    In this way, the information from the source text is communicated to the decoder.
    The embedding of the last token in the top layer is entered into a logistic classifier
    and this calculates the probability of the tokens for the next position. Subsequently,
    the observed token at the next position is appended to the target input and the
    computations are repeated for the next but one position.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer是一种序列到序列模型，它将输入语言的源文本翻译成目标语言的文本。它由一个编码器组成，其架构与自编码器BERT模型相同，用于计算源文本标记的上下文嵌入。解码器类似于自回归GPT模型，并依次生成目标文本的标记。在内部，不同层中计算目标标记的上下文嵌入。每个解码器块都有一个额外的交叉注意力模块，其中查询向量来自目标标记的嵌入，而键和值向量是为最后一层的源标记嵌入计算的。这样，源文本的信息就被传递给解码器。顶层最后一个标记的嵌入被输入到逻辑分类器中，并计算下一个位置的标记的概率。随后，下一个位置的观察到的标记被附加到目标输入上，并重复计算下一个但一个位置的计算。
- en: During training the parameters of the transformer are adapted by stochastic
    gradient descent in such a way that the model assigns high probabilities to the
    observed target tokens of the translation in the training data. When the model
    has been trained on a large text dataset it can be applied for translation. Conditional
    on an input text, it can sequentially compute the probability of the next token
    of the translation.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，通过随机梯度下降调整transformer的参数，使得模型在训练数据中对翻译的观察目标标记赋予高概率。当模型在大型文本数据集上训练完成后，它可以用于翻译。基于输入文本，模型可以依次计算翻译的下一个标记的概率。
- en: During application of a trained model either the token with the maximal probability
    is selected or several alternatives are generated by beam search and the final
    output sequence with maximal probability is chosen. The evaluation of the translations
    quality is difficult as different translations may be correct. A number of metrics,
    e.g. Bleu, have been developed, which compare the machine translation to one or
    more reference translations by comparing the number of common word *n*-grams with
    *n* = 1, …, 4\. Often the results are assessed by human raters. The transformer
    was able to generate better translation than prior models. In the meantime the
    translation quality for a number of language pairs is on par with human translators.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用训练好的模型时，要么选择概率最大的标记，要么通过束搜索生成几个替代方案，并选择概率最大的最终输出序列。由于不同的翻译可能都是正确的，因此翻译质量的评估很困难。已经开发出了一些指标，例如Bleu，这些指标通过比较具有n=1,
    …, 4的n-gram的公共数量来将机器翻译与一个或多个参考翻译进行比较。通常，结果由人工评分员进行评估。Transformer能够生成比先前模型更好的翻译。与此同时，对于许多语言对来说，翻译质量与人类翻译者相当。
- en: In the previous sections, we discussed *autoencoder BERT* models, *autoregressive
    GPT* models and the *encoder-decoder Transformers*. Collectively these models
    are called *pre-trained language models*, as transfer learning with a pre-training
    step using a large training set and a subsequent fine-tuning step is a core approach
    for all three variants. The self-attention and cross-attention modules are central
    building blocks used by all three models. Despite the development of many variations
    in recent years, the original architecture developed by Vaswani et al. [[141](#CR141)]
    is still commonly employed.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了*自动编码BERT*模型、*自回归GPT*模型和*编码器-解码器Transformer*。这些模型统称为*预训练语言模型*，因为使用大型训练集进行预训练步骤和随后的微调步骤的迁移学习是三种变体的核心方法。自注意力和交叉注意力模块是所有三种模型使用的核心构建块。尽管近年来发展了许多变体，但Vaswani等人[[141](#CR141)]开发的原始架构仍然被广泛采用。
- en: It turns out that these models can be applied not only to text, but to various
    types of sequences, such as images, speech, and videos. In addition, they may
    be instructed to perform various tasks by simple prompts. Therefore, large PLMs
    are also called *Foundation Models*, as they are expected to play a crucial role
    in the future development of text and multimedia systems.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，这些模型不仅可以应用于文本，还可以应用于各种类型的序列，如图像、语音和视频。此外，它们可以通过简单的提示执行各种任务。因此，大型PLM也被称为*基础模型*，因为它们预计将在文本和多媒体系统的未来发展中发挥关键作用。
- en: 2.4 Training and Assessment of Pre-trained Language Models
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 预训练语言模型的训练和评估
- en: This section describes some techniques required to train and apply PLMs.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了训练和应用PLMs所需的一些技术。
- en: We need *optimization techniques* which can process millions and billions of
    parameters and training examples.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要能够处理数百万和数十亿参数和训练示例的*优化技术*。
- en: Specific *regularization* methods are required to train the models and to avoid
    overfitting.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型并避免过拟合需要特定的*正则化*方法。
- en: The *uncertainty* of model predictions has to be estimated to asses the performance
    of models.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型预测的*不确定性*必须被估计以评估模型性能。
- en: The *explanation* of model predictions can be very helpful for the acceptance
    of models.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型预测的*解释*对于模型的接受度非常有帮助。
- en: Approaches to solving these problems are discussed in this section. PLMs are
    usually specified in one of the current Deep Learning frameworks. Most popular
    are *TensorFlow* provided from Google [[137](#CR137)] and *PyTorch* from Meta
    [[114](#CR114)]. Both are based on the Python programming language and include
    language elements to specify a network, train it in parallel on dedicated hardware,
    and to deploy it to different environments. A newcomer is the *JAX* framework
    [[22](#CR22)], which is especially flexible for rapid experimentation. It has
    a compiler for linear algebra to accelerate computations for machine learning
    research.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了解决这些问题的方法。预训练语言模型（PLMs）通常在当前的深度学习框架中指定。最流行的是来自谷歌的*TensorFlow* [[137](#CR137)]
    和来自Meta的*PyTorch* [[114](#CR114)]。两者都基于Python编程语言，并包含指定网络、在专用硬件上并行训练以及部署到不同环境中的语言元素。新出现的框架是*JAX*
    [[22](#CR22)]，它特别适合快速实验。它有一个线性代数的编译器，可以加速机器学习研究的计算。
- en: 2.4.1 Optimization of PLMs
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 PLM优化
- en: Basics of PLM Optimization
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PLM优化基础
- en: For the i.i.d. training sample *Tr* = {(***x***^([1]), *y*^([1])), …, (***x***^([*N*]),
    *y*^([*N*]))} parameter optimization for Deep Neural Networks aims to find a model
    that minimizes the loss function *L*(***x***^([*i*]), *y*^([*i*]);***w***)![$$\displaystyle
    \begin{aligned} \min_{\boldsymbol{w}} L({\boldsymbol{w}})=L({\boldsymbol{x}}^{[1]},y^{[1]};{\boldsymbol{w}})
    +\cdots+L({\boldsymbol{x}}^{[N]},y^{[N]};{\boldsymbol{w}}). {} \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ14.png)(2.14)First-order
    optimization methods, also known as gradient-based optimization, are based on
    first-order derivatives. A requirement is that the loss function *L*(***w***)
    is smooth, i.e. is continuous and in addition differentiable at almost all parameter
    values ***w*** = (*w*[1], …, *w*[*k*]). Then the partial derivatives ![$$\frac
    {\partial L({\boldsymbol {w}})}{\partial w_j}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq56.png)
    of *L*(***w***) with respect to any component *w*[*j*] of ***w*** can be computed
    at almost all points. The *gradient* of *L*(***w***) in a specific point ***w***
    is the vector![$$\displaystyle \begin{aligned} \frac{\partial L({\boldsymbol{w}})}{\partial
    {\boldsymbol{w}}} = \left( \frac{\partial L({\boldsymbol{w}})}{\partial w_1},\ldots,\frac{\partial
    L({\boldsymbol{w}})}{\partial w_k}\right)^\intercal . {} \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ15.png)(2.15)The
    gradient points into the direction, where *L*(***w***) in point ***w*** has its
    steepest ascent. Consequently, the direction of the steepest descent is in the
    opposite direction ![$$-\frac {\partial L({\boldsymbol {w}})}{\partial {\boldsymbol
    {w}}}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq57.png).
    The batch *gradient descent* algorithm therefore changes the current parameter
    ***w***[(*t*)] in the direction of the negative gradient to get closer to the
    minimum![$$\displaystyle \begin{aligned} {\boldsymbol{w}}_{(t+1)} = {\boldsymbol{w}}_{(t)}
    - \lambda\frac{\partial L({\boldsymbol{w}})}{\partial {\boldsymbol{w}}} {}. \end{aligned}
    $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ16.png)(2.16)The
    *learning rate**λ* determines the step-size or how much to move in each iteration
    until an optimal value is reached. As the gradient is usually different for each
    parameter ***w***[(*t*)] it has to be recomputed for every new parameter vector
    (Fig. [2.19](#Fig19)). The iteration process is repeated until the derivative
    becomes close to zero. A zero gradient indicates a *local minimum* or a *saddle
    point* [[51](#CR51), p. 79]. In practical applications it is sufficient to repeat
    the optimization beginning with different ***w***-values and stop, if the derivative
    is close to zero.![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig19_HTML.png)
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 对于独立同分布的训练样本 *Tr* = {(***x***^([1]), *y*^([1])), …, (***x***^([*N*]), *y*^([*N*]))}
    的深度神经网络参数优化旨在找到一个最小化损失函数 *L*(***x***^([*i*]), *y*^([*i*]);***w***) 的模型![公式](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ14.png)(2.14)一阶优化方法，也称为基于梯度的优化，基于一阶导数。一个要求是损失函数
    *L*(***w***) 是平滑的，即连续且在几乎所有的参数值 ***w*** = (*w*[1], …, *w*[*k*]) 上可导。然后，*L*(***w***)
    关于 ***w*** 的任意分量 *w*[*j*] 的偏导数 ![$$\frac {\partial L({\boldsymbol {w}})}{\partial
    w_j}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq56.png) 可以在几乎所有的点上计算。*L*(***w***)
    在特定点 ***w*** 的梯度是一个向量![公式](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ15.png)(2.15)梯度指向
    *L*(***w***) 在点 ***w*** 的最陡上升方向。因此，最陡下降的方向是相反的方向 ![$$-\frac {\partial L({\boldsymbol
    {w}})}{\partial {\boldsymbol {w}}}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq57.png)。因此，批量
    *梯度下降* 算法因此改变当前参数 ***w***[(*t*)] 的方向，以负梯度方向移动，以接近最小值![公式](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ16.png)(2.16)*学习率**λ*
    决定了步长或每次迭代中移动多少，直到达到最优值。由于梯度通常对每个参数 ***w***[(*t*)] 都不同，因此必须为每个新的参数向量重新计算（图[2.19](#Fig19)）。迭代过程重复进行，直到导数接近零。零梯度表示
    *局部最小值* 或 *鞍点* [[51](#CR51)，第79页]。在实际应用中，从不同的 ***w***-值开始重复优化，如果导数接近零则停止。![图片](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig19_HTML.png)
- en: 2 illustrations of color gradient grids with light to dark shades from top to
    bottom.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 2个从上到下由浅入深颜色渐变的网格示例。
- en: Fig. 2.19
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19
- en: On all points of a grid the negative gradients are computed for this two-dimensional
    function *L*(***w***) (left). The gradient descent algorithm follows the negative
    gradients and approaches the local minima (right). The blue lines are the paths
    taken during minimization. Image credits in Table [A.​1](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab1)
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在网格的所有点上计算这个二维函数*L*(***w***)（左）的负梯度。梯度下降算法遵循负梯度并接近局部最小值（右）。蓝色线条是最小化过程中所采取的路径。图像归功于表[A.1](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab1)。
- en: Deep Neural Networks often require many millions of training examples. The repeated
    computation of the gradient for all these examples is extremely costly. The **Stochastic
    Gradient Descent** (*SGD*) algorithm does not use the entire dataset but rather
    computes the gradient only for a small *mini-batch* of *m* training examples at
    a time. In general, a mini-batch has sizes *m* ranging from 32 up to 1024, with
    even higher values for recent extremely large models. Subsequently, the parameters
    of the model are changed according to ([2.16](#Equ16)).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络通常需要数百万个训练示例。对所有这些示例重复计算梯度是非常昂贵的。**随机梯度下降法**（*SGD*）算法不使用整个数据集，而是每次只计算一个小型*迷你批次*的*m*个训练示例的梯度。一般来说，迷你批次的大小*m*从32到1024不等，对于最近的一些极其大的模型，甚至更高。随后，根据([2.16](#Equ16))改变模型的参数。
- en: For each iteration a new mini-batch is selected randomly from the training data.
    According to the law of large numbers the gradients computed from these mini-batches
    fluctuate around the true gradient for the whole training set. Therefore, the
    mini-batch gradient on average indicates an adequate direction for changing the
    parameters. Mertikopoulos et al. [[91](#CR91)] show that by iteratively reducing
    the learning rate to 0, the SGD exhibits almost sure convergence, avoids spurious
    critical points such as saddle points (with probability 1), and stabilizes quickly
    at local minima. There are a number of variations of the SGD algorithm, which
    are described below [[65](#CR65)].
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一次迭代，都会从训练数据中随机选择一个新的迷你批次。根据大数定律，从这些迷你批次计算出的梯度会在整个训练集的真实梯度周围波动。因此，迷你批次梯度在平均意义上指示了改变参数的适当方向。Mertikopoulos等人[[91](#CR91)]表明，通过迭代地将学习率降低到0，随机梯度下降法几乎必然收敛，避免了诸如鞍点（概率为1）等虚假的临界点，并且快速稳定在局部最小值。随机梯度下降法算法有许多变体，下面将进行描述[[65](#CR65)]。
- en: An important step of optimization is the *initialization of parameters*. Their
    initial values can determine whether the algorithm converges at all and how fast
    the optimization approaches the optimum. To break symmetry, the initial parameters
    must be random. Furthermore, the mean and variance of the parameters in each layer
    are set such that the resulting outputs of the layer have a well-behaved distribution,
    e.g. expectation 0.0 and variance 1.0\. In addition, all gradients also should
    have such a benign distribution to avoid exploding or vanishing gradients. All
    Deep Learning software frameworks contain suitable initialization routines. A
    thorough introduction is given by Goodfellow et al. [[51](#CR51), p. 292].
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 优化过程中的一个重要步骤是**参数的初始化**。它们的初始值可以决定算法是否收敛以及优化接近最优值的速度。为了打破对称性，初始参数必须是随机的。此外，每一层的参数的均值和方差被设置为使得该层的输出结果具有良好行为的分布，例如期望值为0.0和方差为1.0。此外，所有梯度也应具有这种良性的分布，以避免梯度爆炸或消失。所有深度学习软件框架都包含合适的初始化例程。Goodfellow等人[[51](#CR51)，第292页]给出了详细的介绍。
- en: Variants of Stochastic Gradient Descent
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机梯度下降法的变体
- en: '**Momentum** is a method that helps SGD to increase the rate of convergence
    in the relevant direction and reduce oscillations. Basically a moving average
    ***u***[(*t*)] of recent gradients with a parameter *γ* ≈ 0.9 is computed and
    the parameter update is performed with this average by![$$\displaystyle \begin{aligned}
    \boldsymbol{u}_{(t)} = \gamma \boldsymbol{u}_{(t-1)}- \lambda\frac{\partial L({\boldsymbol{w}})}{\partial
    {\boldsymbol{w}}} \qquad  \text{where}\qquad  {\boldsymbol{w}}_{(t)} = {\boldsymbol{w}}_{(t-1)}
    - \boldsymbol{u}_{(t)}. {} \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ17.png)(2.17)Note
    that in addition to the parameter vector ***w***[(*t*)] the moving average ***u***[(*t*)]
    of the same length has to be stored requiring the same memory as the parameter
    vector ***w***. This can consume a large additional memory size if the number
    of parameters approaches the billions. In recent years a number of further optimizers
    were developed [[65](#CR65)]:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '**动量**是一种帮助SGD在相关方向上增加收敛速度并减少振荡的方法。基本上，通过参数*γ*≈0.9计算最近梯度的移动平均***u***[(*t*)]，并通过以下公式进行参数更新![公式](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ17.png)(2.17)其中，\(\boldsymbol{u}_{(t)}
    = \gamma \boldsymbol{u}_{(t-1)}- \lambda\frac{\partial L({\boldsymbol{w}})}{\partial
    {\boldsymbol{w}}}\)，\(\text{where}\quad {\boldsymbol{w}}_{(t)} = {\boldsymbol{w}}_{(t-1)}
    - \boldsymbol{u}_{(t)}\)。注意，除了参数向量***w***[(*t*)]外，还需要存储相同长度的移动平均***u***[(*t*)]，这需要与参数向量***w***相同的内存。如果参数数量接近数十亿，这可能会消耗大量的额外内存空间。近年来，开发了许多其他优化器
    [[65](#CR65)]：'
- en: '**AdaGrad** adapts the learning rate dynamically based on the previous gradients.
    It uses smaller learning rates for features occurring often, and higher learning
    rates for features occurring rarely.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AdaGrad**根据之前的梯度动态调整学习率。它对经常出现的特征使用较小的学习率，对很少出现的特征使用较高的学习率。'
- en: '**AdaDelta** modifies AdaGrad. Instead of accumulating all past gradients,
    it restricts the accumulation window of the past gradients to some fixed size
    *k*.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AdaDelta**修改了AdaGrad。它不是累积所有过去的梯度，而是将过去梯度的累积窗口限制为某个固定的尺寸*k*。'
- en: '**RMSProp** is also a method in which the learning rate is adapted for each
    of the parameters. The idea is to divide the learning rate for a weight by a running
    average of the magnitudes of recent gradients for that weight.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RMSProp**也是一种为每个参数调整学习率的方法。其想法是将权重的学习率除以该权重最近梯度的幅度的运行平均值。'
- en: '**Adam** combines the advantages of both AdaGrad and RMSProp. Adam is based
    on adaptive estimates of lower-order moments. It uses running averages of both
    the gradients and the second moments of the gradients.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Adam**结合了AdaGrad和RMSProp的优点。Adam基于低阶矩的自适应估计。它使用梯度和梯度的二阶矩的运行平均值。'
- en: Due to the extremely large number of parameters of PLMs second order optimization
    methods like *Conjugate Gradient* or *Quasi-Newton* are rarely employed. As the
    number of second order derivatives grows quadratically, only crude approximations
    may be used. An example is Adam, as described before.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 由于PLM的参数数量极其庞大，因此很少使用像*共轭梯度*或*拟牛顿*这样的二阶优化方法。因为二阶导数的数量呈二次增长，所以只能使用粗略的近似。例如，之前描述的Adam。
- en: An important architectural addition to PLMs to improve training are *residual
    connections*, which were proposed by Vaswani et al. [[141](#CR141)] for the Transformer.
    Residual connections have been shown to be very successful for image classification
    networks such as ResNet [[54](#CR54)] and allowed training networks with several
    hundred layers. The identity shortcuts skip blocks of layers to preserve features.
    Zhang et al. [[163](#CR163)] analyze the representational power of networks containing
    residual connections.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高PLM的训练，一个重要的架构补充是*残差连接*，这是Vaswani等人[[141](#CR141)]为Transformer提出的。残差连接已被证明对于图像分类网络如ResNet[[54](#CR54)]非常成功，并允许训练具有数百层的网络。恒等快捷方式跳过层块以保留特征。张等人[[163](#CR163)]分析了包含残差连接的网络的表示能力。
- en: Parallel Training for Large Models
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大型模型的并行训练
- en: Recently, there have been suggestions to reduce the optimization effort by employing
    larger mini-batches. You et al. [[159](#CR159)] propose the **LAMB optimizer**
    with layerwise adaptive learning rates to accelerate training of PLMs using large
    mini-batches. They prove the convergence of their approach to a stationary point
    in a general nonconvex setting. Their empirical results demonstrate the superior
    performance of LAMB. It is possible to reduce the BERT training time from 3 days
    to just 76 min with very little hyperparameter tuning and batch sizes of 32,868
    without any degradation of performance. The LAMB program code is available online
    [[97](#CR97)]. In addition, the memory requirements of the optimization may be
    reduced [[119](#CR119)] to enable parallelization of models resulting in a higher
    training speed.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，有人建议通过使用更大的mini-batch来减少优化工作量。You et al. [[159](#CR159)] 提出了**LAMB优化器**，具有层自适应学习率，以加速使用大mini-batch的PLM训练。他们在一般非凸设置中证明了他们方法收敛到平稳点的收敛性。他们的经验结果表明LAMB的性能优越。通过非常少的超参数调整和32,868个批大小，可以将BERT的训练时间从3天减少到仅76分钟，而不会降低性能。LAMB程序代码可在网上找到
    [[97](#CR97)]。此外，优化器的内存需求可能被减少 [[119](#CR119)]，以便实现导致更高训练速度的模型并行化。
- en: 'Large models such as GPT-3 have many billion parameters that no longer fit
    into the memory of a single computational device, e.g. a GPU. Therefore, the computations
    have to be distributed among several GPUs. There are different parallelization
    techniques [[156](#CR156)]:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 大型模型如GPT-3具有数十亿参数，这些参数不再适合单个计算设备（例如GPU）的内存。因此，计算必须分布在多个GPU上。存在不同的并行化技术 [[156](#CR156)]：
- en: '*Data parallelism* assigns the same model code and parameters to each GPU but
    different training examples [[72](#CR72)]. Gradients are computed in parallel
    and finally summarized.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据并行*将相同的模型代码和参数分配给每个GPU，但不同的训练示例 [[72](#CR72)]。梯度是并行计算的，最后汇总。'
- en: '*Pipeline parallelism* partitions the model into different parts (e.g. layers)
    that are executed on different GPUs. If a part is computed it sends its results
    to the next GPU. This sequence is reversed in the backward pass of training.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*流水线并行*将模型分成不同的部分（例如层），在不同的GPU上执行。如果一个部分被计算，它将结果发送到下一个GPU。在训练的逆传播过程中，这个顺序被反转。'
- en: '*Within-layer model parallelism* distributes the weights of a single layer
    across multiple GPUs.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*层内模型并行*将单个层的权重分布到多个GPU上。'
- en: The implementation of a parallelization strategy for a model is a tedious process.
    Support is given by the **DeepSpeed** library [[122](#CR122)] that makes distributed
    training easy, efficient, and effective. Recently the **GSPMD** system [[156](#CR156)]
    was developed which automates this process and is able to combine different parallelism
    paradigms in a unified way. GSPMD infers the distribution of computations to a
    network of GPUs based on limited user annotations to the model definition. It
    was, for instance, applied to distribute models with 1 trillion parameters on
    2048 GPUs.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 实现模型并行化策略是一个繁琐的过程。**DeepSpeed**库 [[122](#CR122)] 提供了支持，使得分布式训练变得简单、高效和有效。最近开发了**GSPMD**系统
    [[156](#CR156)]，它自动化了这个过程，并能以统一的方式结合不同的并行化范式。GSPMD根据对模型定义的有限用户注释，推断计算分布到GPU网络中。例如，它被应用于在2048个GPU上分配具有1万亿参数的模型。
- en: 2.4.2 Regularization of Pre-trained Language Models
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 预训练语言模型的正则化
- en: If a model contains too many parameters it can nearly perfectly adapt to the
    training data by optimization, reflecting nearly all details of the training data.
    During this *overfitting* the model learns the random variations expressed in
    the training data and deviates from the mean underlying distribution. Consequently,
    it has usually a lower performance on test data and a larger *generalization error*.
    To avoid this phenomenon, the representational capacity of the model has to be
    reduced by *regularization methods*, which often have the same effect as reducing
    the number of parameters. Well known approaches for Deep Learning models are the
    *L*[2] regularization and *L*[1] regularization penalizing large parameter values,
    or *Dropout* temporarily setting randomly selected hidden variables to 0\. A survey
    of regularization strategies for Deep Neural Networks is given by Moradi et al.
    [[96](#CR96)].
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个模型包含太多的参数，它可以通过优化几乎完美地适应训练数据，反映了训练数据几乎所有的细节。在*过拟合*过程中，模型学习训练数据中表达的随机变化，并偏离了潜在的均值分布。因此，它在测试数据上通常表现较差，并且具有更大的*泛化误差*。为了避免这种现象，必须通过*正则化方法*减少模型的表示能力，这些方法通常与减少参数数量有相同的效果。深度学习模型中众所周知的方法是*L*[2]正则化和*L*[1]正则化，惩罚大的参数值，或者*Dropout*临时将随机选择的隐藏变量设置为0。Moradi等人
    [[96](#CR96)] 提供了深度神经网络正则化策略的概述。
- en: The training of PLMs is often non-trivial. One problem is the occurrence of
    vanishing or exploding gradients, which is connected to the problem of the vanishing
    or exploding variance of input values of different layers [[55](#CR55)]. *Batch
    normalization* normalizes the values of the components of hidden units to mean
    0.0 and variance 1.0 and thus reduces the variation of input values. For a mini-batch
    of training cases the component values are aggregated to compute a mean and variance,
    which are then used to normalize the input of that component on each training
    case [[62](#CR62)]. It can be shown that batch normalization makes hidden representations
    increasingly orthogonal across layers of a Deep Neural Network [[35](#CR35)].
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: PLM的训练通常并不简单。一个问题是在不同层的输入值中出现的梯度消失或爆炸，这与输入值方差消失或爆炸的问题相关 [[55](#CR55)]。*批归一化*将隐藏单元组件的值归一化到均值为0.0和方差1.0，从而减少了输入值的变异。对于训练案例的迷你批次，组件值被聚合以计算均值和方差，然后这些值被用来在每个训练案例中归一化该组件的输入
    [[62](#CR62)]。可以证明，批归一化使得深度神经网络各层的隐藏表示越来越正交 [[35](#CR35)]。
- en: In their paper on the Transformer, Vaswani et al. [[141](#CR141)] use a variant
    called *layer normalization* [[6](#CR6)] for regularization. The authors compute
    the mean and variance of the different components of hidden units for each training
    example and use this to normalize the input to mean 0.0 and variance 1.0\. In
    addition, they apply dropout to the output of self-attention. Finally, they use
    *label smoothing* [[133](#CR133)] where the loss function is reformulated such
    that the observed tokens are not certain but alternative tokens may be possible
    with a small probability. This is a form of regularization which makes optimization
    easier. The RMSNorm [[162](#CR162)] is a variant of the layer normalization, which
    only normalizes the input by division with the root-mean-square error without
    shifting the mean. In experiments, it compares favorably with the layer normalization
    [[101](#CR101)].
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的关于Transformer的论文中，Vaswani等人 [[141](#CR141)] 使用了一种称为*层归一化*的变体 [[6](#CR6)]
    进行正则化。作者为每个训练示例计算隐藏单元不同组件的均值和方差，并使用这些值将输入归一化到均值为0.0和方差1.0。此外，他们对自注意力机制的输出应用了dropout。最后，他们使用了*标签平滑*
    [[133](#CR133)]，其中损失函数被重新表述，使得观察到的标记不是确定的，但以小概率可能存在其他标记。这是一种正则化形式，使得优化更容易。RMSNorm
    [[162](#CR162)] 是层归一化的一个变体，它只通过除以均方根误差来归一化输入，而不移动均值。在实验中，它与层归一化 [[101](#CR101)]
    相比表现良好。
- en: 2.4.3 Neural Architecture Search
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 神经架构搜索
- en: The structure of the self-attention block was manually designed, and it is not
    clear, whether it is optimal in all cases. Therefore, there are some approaches
    to generate the architecture of PLMs in an automatic way called *Neural Architecture
    Search* (*NAS*). A survey is provided by He et al. [[56](#CR56)], who argue that
    currently the contributions of architecture search to NLP tasks are minor. Zöller
    [[166](#CR166)] evaluate architecture search for machine learning models.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力块的结构是手动设计的，并不清楚它在所有情况下是否是最优的。因此，有一些方法以自动方式生成 PLM 的架构，称为 *神经架构搜索* (*NAS*)。He
    等人 [[56](#CR56)] 提供了一份调查，他们认为目前架构搜索对 NLP 任务的贡献是微小的。Zöller [[166](#CR166)] 评估了架构搜索在机器学习模型中的应用。
- en: Wang et al. [[149](#CR149)] propose an architecture search space with flexible
    encoder-decoder attentions and heterogeneous layers. The architecture search produces
    several transformer versions and finally concentrates on hardware restrictions
    to adapt the computations to processors at hand. The authors report a speedup
    of 3 and a size reduction factor of 3.7 with no performance loss. For relation
    classification Zhu et al. [[165](#CR165)] design a comprehensive search space.
    They explore the search space by reinforcement learning strategy and yield models
    which have a better performance.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 王等人 [[149](#CR149)] 提出了一种具有灵活编码器-解码器注意力和异构层的架构搜索空间。架构搜索产生了几个 Transformer 版本，并最终专注于硬件限制，以适应手头的处理器。作者报告了速度提高了
    3 倍，尺寸减少了 3.7 倍，且没有性能损失。对于关系分类，Zhu 等人 [[165](#CR165)] 设计了一个全面的搜索空间。他们通过强化学习策略探索搜索空间，并产生了性能更好的模型。
- en: Architecture search may also be formulated as a ranking task. **RankNAS** [[60](#CR60)]
    solves this by a series of binary classification problems. The authors investigate
    translation and language models. For translation the usual encoder-decoder is
    included in a super-net, where each of the 10^(23) subnetworks is a unique architecture.
    The importance of an architectural feature (e.g., the number of layers) is measured
    by the increase in the model error after permuting the feature. The authors use
    an evolutionary optimization strategy and evaluate their approach on translation
    (WMT2014 En-De). They get increases in Bleu-values at a fraction of cost of other
    approaches.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 架构搜索也可以被表述为一个排序任务。**RankNAS** [[60](#CR60)] 通过一系列二分类问题来解决这一问题。作者研究了翻译和语言模型。对于翻译，通常的编码器-解码器被包含在一个超级网络中，其中每个
    10^(23) 个子网络都是独特的架构。一个架构特征（例如，层数）的重要性通过在特征重新排列后模型误差的增加来衡量。作者使用进化优化策略，并在翻译（WMT2014
    En-De）上评估了他们的方法。他们以其他方法成本的一小部分获得了 Bleu 值的增加。
- en: Recently differentiable architecture search has been developed, which embeds
    architecture search in a continuous search space and finds the optimal architecture
    by gradient descent. This leads to an efficient search process that is orders
    of magnitude faster than the discrete counterparts. This idea is applied by Fan
    et al. [[43](#CR43)], who propose a gradient-based NAS algorithm for machine translation.
    They explore attention modules and recurrent units, automatically discovering
    architectures with better performances. The topology of the connection among different
    units is learned in an end-to-end manner. On a number of benchmarks they were
    able to improve the performance of the Transformer, e.g. from 28.8 to 30.1 Bleu
    scores for the WMT2014 English-to-German translation. There are other successful
    architecture search approaches for neural translation [[130](#CR130)], named entity
    recognition [[64](#CR64)], and image classification models [[34](#CR34), [147](#CR147),
    [148](#CR148)], which may possibly be applied to other NLP tasks.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，可微架构搜索（differentiable architecture search）得到了发展，它将架构搜索嵌入到连续的搜索空间中，并通过梯度下降找到最优架构。这导致了一个高效的搜索过程，其速度比离散对应物快几个数量级。这一想法被
    Fan 等人 [[43](#CR43)] 所应用，他们提出了一种基于梯度的机器翻译 NAS 算法。他们探索了注意力模块和循环单元，自动发现性能更好的架构。不同单元之间的连接拓扑以端到端的方式进行学习。在多个基准测试中，他们能够提高
    Transformer 的性能，例如，将 WMT2014 英语到德语的翻译的 Bleu 分数从 28.8 提高到 30.1。还有其他成功的架构搜索方法适用于神经翻译
    [[130](#CR130)]、命名实体识别 [[64](#CR64)] 和图像分类模型 [[34](#CR34), [147](#CR147), [148](#CR148)]，这些方法可能也适用于其他
    NLP 任务。
- en: 2.4.4 The Uncertainty of Model Predictions
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.4 模型预测的不确定性
- en: 'Variations in the outcome of a PLM can have two main sources:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: PLM 的结果变化可能有两个主要来源：
- en: '*Epistemic uncertainty* reflects our limited knowledge about the real world.
    The real world situation corresponding to the training set can change causing
    a distribution shift. Moreover, the collected documents can have biases or errors
    and cover unwanted types of content. It is clear that the structure of the real
    world and the PLM differ. Therefore, a PLM can only approximate the correct conditional
    probabilities of language. This type of uncertainty is often called *structural
    uncertainty* and is difficult to estimate.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*认知不确定性*反映了我们对现实世界的有限了解。与训练集相对应的现实世界情况可能会发生变化，导致分布偏移。此外，收集到的文档可能存在偏见或错误，并包含不希望的内容。很明显，现实世界的结构和PLM是不同的。因此，PLM只能近似正确条件概率的语言。这种类型的不确定性通常被称为*结构不确定性*，难以估计。'
- en: '*Aleatoric uncertainty* is caused by random variations which can be assessed
    more easily. The training data is usually a sample of the underlying data in the
    population and therefore affected by the sampling variation. If a model is randomly
    re-initialized, it generates a completely different set of parameter values which
    leads to different predictions. Finally, language models predict probabilities
    of tokens and the generation of new tokens is also affected by uncertainty. The
    Bayesian framework offers a well-founded tool to assess this type of uncertainty
    in Deep Learning [[44](#CR44)].'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机不确定性*是由随机变化引起的，这种变化可以更容易地评估。训练数据通常是总体中潜在数据的样本，因此受到抽样变化的影响。如果一个模型随机重新初始化，它将生成一组完全不同的参数值，从而导致不同的预测。最后，语言模型预测标记的概率，新标记的生成也受到不确定性的影响。贝叶斯框架提供了一个有充分依据的工具来评估这种深度学习中的不确定性[[44](#CR44)]。'
- en: 'A recent survey of methods for estimating the model uncertainty is provided
    by Gawlikowski et al.[[47](#CR47)]. We will describe three approaches to capture
    model uncertainty: Bayesian statistics, a Dirichlet distributions, and ensemble
    distributions.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: Gawlikowski等人提供了一种估计模型不确定性的方法综述[[47](#CR47)]。我们将描述三种捕捉模型不确定性的方法：贝叶斯统计、狄利克雷分布和集成分布。
- en: Bayesian Neural Networks
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贝叶斯神经网络
- en: '*Bayesian Neural Networks* directly represent the uncertainty of the estimated
    parameters ![$${\boldsymbol {w}}=(w_1,\ldots ,w_{d_w})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq58.png)
    by the *posterior distribution*![$$\displaystyle \begin{aligned} p({\boldsymbol{w}}|\boldsymbol{X},\boldsymbol{Y})\propto
    p({\boldsymbol{y}}|\boldsymbol{X},{\boldsymbol{w}})p({\boldsymbol{w}}) {}. \end{aligned}
    $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ18.png)(2.18)Here
    ***X*** and ***Y*** are the observed inputs and outputs in the training set and
    *p*(***Y*** |***X***, ***w***) is the *likelihood*, i.e. the probability of the
    outputs given ***X*** and a parameter vector ***w***. The *prior distribution**p*(***w***)
    describes the distribution of parameters before data is available. The distribution
    of predictions for a new input ![$$\tilde {{\boldsymbol {x}}}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq59.png)
    is given by![$$\displaystyle \begin{aligned} p(\tilde{{\boldsymbol{y}}}|\tilde{{\boldsymbol{x}}},\boldsymbol{X},\boldsymbol{Y})
    = \int p(\tilde{{\boldsymbol{y}}}|\tilde{{\boldsymbol{x}}},{\boldsymbol{w}}) p({\boldsymbol{w}}|\boldsymbol{X},\boldsymbol{Y})
    d{\boldsymbol{w}} {}. \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ19.png)(2.19)The
    integral usually cannot be solved analytically and has to be approximated. Often
    a *Monte Carlo* approximation is used, which infers the integral by a sum over
    different parameter values ***w***^([*i*]) distributed according to the posterior
    distribution *p*(***w***|***X***, ***Y*** ). If ![$$\tilde {{\boldsymbol {y}}}^{[i]}=f(\tilde
    {{\boldsymbol {x}}},{\boldsymbol {w}}^{[i]})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq60.png)
    is a deterministic network predicting the output for a parameter ***w***^([*i*])
    and input ![$$\tilde {{\boldsymbol {x}}}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq61.png),
    the resulting sample ![$$\tilde {{\boldsymbol {y}}}^{[1]},\ldots ,\tilde {{\boldsymbol
    {y}}}^{[k]}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq62.png)
    can be considered as a sample of the output distribution ![$$p(\tilde {{\boldsymbol
    {y}}}|\tilde {{\boldsymbol {x}}},\boldsymbol {X},\boldsymbol {Y})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq63.png)
    [[108](#CR108)].Bayesian predictive distributions can be approximated in different
    ways:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '*贝叶斯神经网络*直接通过*后验分布*表示估计参数的不确定性 ![$${\boldsymbol {w}}=(w_1,\ldots ,w_{d_w})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq58.png)
    ![$$\displaystyle \begin{aligned} p({\boldsymbol{w}}|\boldsymbol{X},\boldsymbol{Y})\propto
    p({\boldsymbol{y}}|\boldsymbol{X},{\boldsymbol{w}})p({\boldsymbol{w}}) {}. \end{aligned}
    $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ18.png)(2.18)Here
    ***X*** and ***Y*** are the observed inputs and outputs in the training set and
    *p*(***Y*** |***X***, ***w***) is the *likelihood*, i.e. the probability of the
    outputs given ***X*** and a parameter vector ***w***. The *prior distribution**p*(***w***)
    describes the distribution of parameters before data is available. The distribution
    of predictions for a new input ![$$\tilde {{\boldsymbol {x}}}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq59.png)
    is given by![$$\displaystyle \begin{aligned} p(\tilde{{\boldsymbol{y}}}|\tilde{{\boldsymbol{x}}},\boldsymbol{X},\boldsymbol{Y})
    = \int p(\tilde{{\boldsymbol{y}}}|\tilde{{\boldsymbol{x}}},{\boldsymbol{w}}) p({\boldsymbol{w}}|\boldsymbol{X},\boldsymbol{Y})
    d{\boldsymbol{w}} {}. \end{aligned} $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_Equ19.png)(2.19)The
    integral usually cannot be solved analytically and has to be approximated. Often
    a *Monte Carlo* approximation is used, which infers the integral by a sum over
    different parameter values ***w***^([*i*]) distributed according to the posterior
    distribution *p*(***w***|***X***, ***Y*** ). If ![$$\tilde {{\boldsymbol {y}}}^{[i]}=f(\tilde
    {{\boldsymbol {x}}},{\boldsymbol {w}}^{[i]})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq60.png)
    is a deterministic network predicting the output for a parameter ***w***^([*i*])
    and input ![$$\tilde {{\boldsymbol {x}}}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq61.png),
    the resulting sample ![$$\tilde {{\boldsymbol {y}}}^{[1]},\ldots ,\tilde {{\boldsymbol
    {y}}}^{[k]}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq62.png)
    can be considered as a sample of the output distribution ![$$p(\tilde {{\boldsymbol
    {y}}}|\tilde {{\boldsymbol {x}}},\boldsymbol {X},\boldsymbol {Y})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq63.png)
    [[108](#CR108)].贝叶斯预测分布可以通过不同的方式进行近似：'
- en: '*Sampling approaches* use a *Markov Chain Monte Carlo* algorithm to generate
    parameter values distributed according to the posterior distributions, from which
    realizations can be sampled [[102](#CR102)]. Markov Chain Monte Carlo defines
    a sampling strategy, where first a new parameter value ***w*** is randomly generated
    and then the algorithm computes the probability to accept ***w***, or to keep
    the previous parameter value. Welling et al. [[150](#CR150)] combined this approach
    with stochastic gradient descent and demonstrated that Bayesian inference on Deep
    Neural Networks can be done by a noisy SGD. A review of the favorable convergence
    properties has been given by Nemeth et al. [[103](#CR103)]. Practical evaluations
    of this technique are performed by Wenzel et al. [[152](#CR152)].'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*采样方法*使用马尔可夫链蒙特卡洛算法生成根据后验分布分布的参数值，从中可以抽取实现值 [[102](#CR102)]。马尔可夫链蒙特卡洛定义了一种采样策略，首先随机生成一个新的参数值
    ***w***，然后算法计算接受 ***w*** 或保持先前参数值的概率。Welling等人 [[150](#CR150)] 将这种方法与随机梯度下降相结合，证明了可以通过噪声SGD在深度神经网络上进行贝叶斯推断。Nemeth等人
    [[103](#CR103)] 给出了有利的收敛性质的综述。Wenzel等人 [[152](#CR152)] 对此技术进行了实际评估。'
- en: '*Variational inference* approximates the posterior distribution by a product
    *q*(***w***) of simpler distributions, which are easier to evaluate [[9](#CR9)].
    Using multiple GPUs and practical tricks, such as data augmentation, momentum
    initialization and learning rate scheduling, and learning rate scheduling, Osawa
    et al. [[105](#CR105)] demonstrated that variational inference can be scaled up
    to ImageNet size data-sets and architectures.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*变分推断*通过简单分布的乘积 *q*(***w***) 来近似后验分布，这些简单分布更容易评估 [[9](#CR9)]。通过使用多个GPU和实用技巧，例如数据增强、动量初始化和学习率调度，Osawa等人
    [[105](#CR105)] 证明了变分推断可以扩展到ImageNet规模的数据集和架构。'
- en: It can be shown [[45](#CR45)] that dropout regularization (Sect. [2.4.2](#Sec32))
    can be considered as approximate variational inference. Hence, the predictive
    uncertainty can be estimated by employing dropout not only during training, but
    also at test time. A variant called *Drop connect* randomly removes incoming activations
    of a node, instead of dropping an activation for all following nodes. This approach
    yields a more reliable uncertainty estimate and can even be combined with the
    original dropout technique [[88](#CR88)].
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以证明 [[45](#CR45)]，dropout正则化（第[2.4.2](#Sec32)节）可以被视为近似变分推断。因此，预测不确定性可以通过在训练期间以及测试时使用dropout来估计。一种称为*Drop
    connect*的变体随机移除节点的输入激活，而不是对所有后续节点进行激活丢弃。这种方法提供了更可靠的不确定性估计，甚至可以与原始dropout技术 [[88](#CR88)]
    结合使用。
- en: '*Laplace approximation* considers the logarithm of the posterior distribution
    around a local mode ![$$\hat {{\boldsymbol {w}}}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq64.png)
    and approximate it by a normal distribution ![$$N(\hat {{\boldsymbol {w}}},[H+\beta
    I]^{-1})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq65.png)
    over the network weights [[9](#CR9)]. *H* is the Hessian, the matrix of second
    derivatives, of ![$$\log p({\boldsymbol {w}}|\boldsymbol {X},\boldsymbol {Y})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq66.png).
    This approximation may be computed for already trained networks and can be applied
    to Deep Neural Networks [[76](#CR76)]. A problem is the large number of coefficients
    of *H*, which limits the computations to elements on the diagonal. Extensions
    have been proposed by George et al. [[48](#CR48)].'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*拉普拉斯近似*考虑后验分布围绕局部模态 ![$$\hat {{\boldsymbol {w}}}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq64.png)
    的对数，并通过网络权重上的正态分布 ![$$N(\hat {{\boldsymbol {w}}},[H+\beta I]^{-1})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq65.png)
    来近似它 [[9](#CR9)]。*H* 是 ![$$\log p({\boldsymbol {w}}|\boldsymbol {X},\boldsymbol
    {Y})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq66.png) 的Hessian，即二阶导数的矩阵。这种近似可以用于已经训练好的网络，并且可以应用于深度神经网络
    [[76](#CR76)]。一个问题是要计算*H*的系数数量很大，这限制了计算仅限于对角线元素。George等人 [[48](#CR48)] 提出了扩展。'
- en: Estimating Uncertainty by a Single Deterministic Model
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过单一确定性模型估计不确定性
- en: Most PLMs predict tokens by a discrete probability distribution. If the softmax
    function is used to compute these probabilities, the optimization over the training
    set usually leads to very extreme probabilities close to 0 or 1\. The network
    is often overconfident and generates inaccurate uncertainty estimates. To assess
    uncertainty, the difference between the estimated distribution and the actual
    distribution has to be described. If ![$$v_1,\ldots ,v_{d_v}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq67.png)
    is the vocabulary of tokens and ***π*** a discrete distribution over these tokens,
    then we can use the *Dirichlet distribution**p*(***π***|***α***(***x***)) to characterize
    a distribution over these discrete distributions. The vector ***α*** depends on
    the input ***x*** and has a component *α*[*i*] for each *v*[*i*]. The sum ∑[*i*]*α*[*i*]
    characterizes the variance. If it gets larger, the estimate for the probability
    of *v*[*i*] has a lower variance.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数PLM通过离散概率分布来预测标记。如果使用softmax函数来计算这些概率，那么在训练集上的优化通常会导致非常极端的概率，接近0或1。网络往往过于自信，并生成不准确的不确定性估计。为了评估不确定性，必须描述估计分布与实际分布之间的差异。如果![$$v_1,\ldots
    ,v_{d_v}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq67.png)是标记的词汇表，而***π***是这些标记上的离散分布，那么我们可以使用*Dirichlet分布**p*(***π***|***α***(***x***))来表征这些离散分布上的分布。向量***α***依赖于输入***x***，并为每个*v*[*i*]有一个组件*α*[*i*]。求和∑[*i*]*α*[*i*]表征了方差。如果它变大，那么对*v*[*i*]的概率估计的方差就较低。
- en: Malinin et al. [[85](#CR85)] use the expected divergence between the empirical
    distribution and the predicted distribution to estimate the *p*(***π***|***α***(***x***))
    for a given input ***x***. In the region of the training data the network is trained
    to minimize the expected *Kullback-Leibler* (*KL*) divergence between the predictions
    of in-distribution data and a low-variance Dirichlet distribution. In the region
    of out-of-distribution data a Dirichlet distribution with a higher variance is
    estimated. The distribution over the outputs can be interpreted as a quantification
    of the model uncertainty, trying to emulate the behavior of a Bayesian modeling
    of the network parameters [[44](#CR44)].
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: Malinin等人[[85](#CR85)]使用经验分布和预测分布之间的期望散度来估计给定输入***x***的*p*(***π***|***α***(***x***))。在训练数据区域，网络被训练以最小化预测分布与低方差Dirichlet分布之间的期望*Kullback-Leibler*
    (*KL*)散度。在分布外数据区域，估计一个方差更高的Dirichlet分布。输出分布可以解释为模型不确定性的量化，试图模拟网络参数的贝叶斯建模行为[[44](#CR44)]。
- en: Liu et al. [[83](#CR83)] argue that the distance between training data elements
    is relevant for prediction uncertainty. To avoid that the layers of a network
    cause a high distortion of the distances of the input space, the authors propose
    a spectral normalization. This **SNGP** approach limits the distance ![$$\lVert
    h({\boldsymbol {x}}^{[1]}) - h({\boldsymbol {x}}^{[2]}) \rVert $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq68.png)
    compared to ![$$\lVert {\boldsymbol {x}}^{[1]} - {\boldsymbol {x}}^{[2]} \rVert
    $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq69.png), where
    ***x***^([1]) and ***x***^([2]) are two inputs and *h*(***x***) is a deep feature
    extractor. Then they pass *h*(***x***) into a distance-aware *Gaussian Process*
    output layer. The Gaussian Process posterior is approximated by a Laplace approximation,
    which can be predicted by a deterministic Deep Neural Network.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Liu等人[[83](#CR83)]认为，训练数据元素之间的距离与预测不确定性相关。为了避免网络的层导致输入空间距离的高扭曲，作者提出了谱归一化。这种**SNGP**方法限制了![$$\lVert
    h({\boldsymbol {x}}^{[1]}) - h({\boldsymbol {x}}^{[2]}) \rVert $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq68.png)与![$$\lVert
    {\boldsymbol {x}}^{[1]} - {\boldsymbol {x}}^{[2]} \rVert $$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq69.png)相比的距离，其中***x***^([1])和***x***^([2])是两个输入，*h*(***x***)是一个深度特征提取器。然后他们将*h*(***x***)传递到一个距离感知的*高斯过程*输出层。高斯过程后验通过拉普拉斯近似来近似，这可以通过一个确定的深度神经网络来预测。
- en: The authors evaluate SNGP on BERT[BASE] to decide, if a natural utterance input
    is covered by the training data (so that it can be handled by the model) or outside.
    The model is only trained on in-domain data, and their predictive accuracy is
    evaluated on in-domain and out-of-domain data. While ensemble techniques have
    a slightly higher prediction accuracy, SNGP has a better calibration of probabilities
    and out-of-distribution detection. An implementation of the approach is available
    [[138](#CR138)].
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在 BERT[BASE] 上评估了 SNGP，以决定一个自然语言输入是否包含在训练数据中（因此可以被模型处理）或在其外。模型仅在领域内数据上训练，并且他们的预测准确性在领域内和领域外数据上评估。虽然集成技术具有略高的预测准确性，但
    SNGP 在概率校准和分布外检测方面表现更好。该方法的实现可用 [[138](#CR138)]。
- en: A number of alternative approaches are described in [[47](#CR47), p. 10f], which
    also discuss mixtures of Dirichlet distributions to characterize predictive uncertainty.
    In general single deterministic methods are computational less demanding in training
    and evaluation compared to other approaches. However, they rely on a single network
    configuration and may be very sensitive to the underlying network structure and
    the training data.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[47](#CR47), p. 10f] 中描述了多种替代方法，这些方法还讨论了使用狄利克雷分布的混合来表征预测不确定性。一般来说，与其它方法相比，单一定义方法在训练和评估时的计算需求较低。然而，它们依赖于单个网络配置，并且可能对基础网络结构和训练数据非常敏感。
- en: Representing the Predictive Distribution by Ensembles
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过集成表示预测分布
- en: It is possible to emulate the sampling variability of a training set by resampling
    methods. A well-founded approach is *bagging*, where *n*[*b*] samples of size
    *n* are drawn with replacement from a training set of *n* elements [[20](#CR20),
    [107](#CR107)]. For the *i*-th sample a model may be trained yielding a parameter
    ![$$\hat {{\boldsymbol {w}}}^{[i]}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq70.png).
    Then the distribution of predictions ![$$f({\boldsymbol {x}},\hat {{\boldsymbol
    {w}}}^{[i]})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq71.png)
    represent the uncertainty in the model prediction for an input ***x***, and it
    can be shown that their mean value ![$$\frac {1}{n_b}\sum _i f({\boldsymbol {x}},\hat
    {{\boldsymbol {w}}}^{[i]})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq72.png)
    has a lower variance than the original model prediction [[73](#CR73)]. In contrast
    to many approximate methods, ensemble approaches may take into account different
    local maxima of the likelihood function and may cover different network architectures.
    There are other methods to introduce data variation, e.g. random parameter initialization
    or random data augmentation. A survey on ensemble methods is provided by Dong
    et al. [[40](#CR40)].
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重采样方法可以模拟训练集的采样变异性。一种有根据的方法是**bagging**，其中从包含*n*个元素的训练集中有放回地抽取大小为*n*的*n*个样本
    [[20](#CR20), [107](#CR107)]。对于第*i*个样本，可以训练一个模型，得到参数 ![$$\hat {{\boldsymbol {w}}}^{[i]}$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq70.png)。然后，预测分布
    ![$$f({\boldsymbol {x}},\hat {{\boldsymbol {w}}}^{[i]})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq71.png)
    代表了对于输入 ***x*** 的模型预测的不确定性，并且可以证明它们的平均值 ![$$\frac {1}{n_b}\sum _i f({\boldsymbol
    {x}},\hat {{\boldsymbol {w}}}^{[i]})$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq72.png)
    比原始模型预测的方差要低 [[73](#CR73)]。与许多近似方法不同，集成方法可以考虑到似然函数的不同局部极大值，并且可能覆盖不同的网络架构。还有其他方法可以引入数据变异性，例如随机参数初始化或随机数据增强。Dong等人提供了一份关于集成方法的调查
    [[40](#CR40)]。
- en: Besides the improvement in the accuracy, ensembles are widely used for representing
    prediction uncertainty of Deep Neural Networks [[73](#CR73)]. In empirical investigations,
    the approach was at least as reliable as Bayesian approaches (Monte Carlo Dropout,
    Probabilistic Backpropagation) [[73](#CR73)]. Reordering the training data and
    a random parameter initialization induces enough variability in the models for
    the prediction of uncertainty, while bagging may reduce the reliability of uncertainty
    estimation [[77](#CR77)]. Compared to Monte Carlo Dropout, ensembles yield more
    reliable and better calibrated prediction uncertainties and are applicable to
    real-world training data [[13](#CR13), [53](#CR53)]. Already for a relatively
    small ensemble size of five, deep ensembles seem to perform best and are more
    robust to data set shifts than the compared methods [[106](#CR106)].
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提高准确性之外，集成方法被广泛用于表示深度神经网络的预测不确定性 [[73](#CR73)]。在经验研究中，这种方法至少与贝叶斯方法（蒙特卡洛Dropout，概率反向传播）[[73](#CR73)]一样可靠。重新排序训练数据和随机参数初始化可以在模型中诱导足够的变异性以预测不确定性，而袋装方法可能会降低不确定性估计的可靠性
    [[77](#CR77)]。与蒙特卡洛Dropout相比，集成方法产生更可靠和更好校准的预测不确定性，并且适用于现实世界的训练数据 [[13](#CR13),
    [53](#CR53)]。即使对于相对较小的五个样本的集成大小，深度集成似乎表现最佳，并且比比较方法更能抵抗数据集的变化 [[106](#CR106)]。
- en: Although PLMs have been adapted as a standard solution for most NLP tasks, the
    majority of existing models is unable to estimate the uncertainty associated with
    their predictions. This seems to be mainly caused by the high computational effort
    of uncertainty estimation approaches. In addition, the concept of uncertainty
    of a predicted probability distribution is difficult to communicate. However,
    it is extremely important to get a diagnosis, when a PLM is given an input outside
    the support of its training data, as then the predictions get unreliable.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管PLM已被作为大多数NLP任务的标准解决方案，但大多数现有模型无法估计其预测相关的不确定性。这似乎主要是由于不确定性估计方法的高计算工作量造成的。此外，预测概率分布不确定性的概念很难传达。然而，当PLM被给予其训练数据支持之外的输入时，获得诊断至关重要，因为那时预测变得不可靠。
- en: Among the discussed approaches the ensemble methods seem to be most reliable.
    However, they require a very high computational effort. New algorithms like SNGP
    are very promising. More research is needed to reduce this effort or develop alternative
    approaches. Recently benchmark repositories and datasets have been developed to
    provide high-quality implementations of standard and Sota methods and describe
    best practices for uncertainty and robustness benchmarking [[99](#CR99)].
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论的方法中，集成方法似乎最可靠。然而，它们需要非常高的计算工作量。新的算法，如SNGP，非常有前景。需要更多的研究来减少这种努力或开发替代方法。最近已经开发了基准存储库和数据集，以提供标准和方法的高质量实现，并描述了不确定性和鲁棒性基准测试的最佳实践
    [[99](#CR99)]。
- en: Implementations
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 实现
- en: Uncertainty Baselines [[10](#CR10), [98](#CR98)] provide a collection high-quality
    implementations of standard and state-of-the-art methods for uncertainty assessment.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性基线 [[10](#CR10), [98](#CR98)] 提供了一组高质量的标准和最先进的不确定性评估方法的实现。
- en: 2.4.5 Explaining Model Predictions
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.5 解释模型预测
- en: PLMs such as BERT are considered as black box models, as it is hard to understand,
    what they really learn and what determines their outputs. Hence, a lot of research
    goes into investigating the behavior of these models. There are three main reasons
    to explain the model predictions. *Trust* in the model predictions is needed,
    i.e. that the model generates reliable answers for the problem at hand and can
    be deployed in real-world applications. *Causality* asserts that the change of
    input attributes leads to sensible changes in the model predictions. *Understanding*
    of the model enables domain experts to compare the model prediction to the existing
    domain knowledge. This is a prerequisite for the ability to adjust the prediction
    model by incorporating domain knowledge.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: PLM，如BERT，被视为黑盒模型，因为很难理解它们真正学习的内容以及什么决定了它们的输出。因此，大量研究致力于调查这些模型的行为。解释模型预测有三个主要原因。*信任*模型预测是必要的，即模型为手头的问题生成可靠的答案，并且可以在现实世界应用中部署。*因果性*断言输入属性的变化会导致模型预测的合理变化。*理解*模型使领域专家能够将模型预测与现有领域知识进行比较。这是通过结合领域知识调整预测模型能力的前提。
- en: Explanations can also be used to debug a model. A striking example was an image
    classification, where a horse was not detected by its shape, but by a label in
    the image [[74](#CR74)]. Explanations are most important for critical decisions
    that involve humans or can cause high damage. Examples are health care, the judicial
    system, banking, or self-driving cars.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 解释也可以用来调试模型。一个引人注目的例子是图像分类，其中一匹马不是通过其形状被检测到，而是通过图像中的一个标签 [[74](#CR74)]。对于涉及人类或可能造成重大损害的关键决策，解释尤为重要。例如，医疗保健、司法系统、银行或自动驾驶汽车。
- en: 'Explanation methods roughly can be grouped into local explanations or global
    explanations. A local explanation provides information or justification for the
    model’s prediction for a specific input ***x***, whereas global explanations cover
    the model in general. A large majority of models aims at local explanations, as
    these may be used to justify specific predictions. Surveys on methods for the
    explanation of PLMs are provided by Danilevsky et al. [[36](#CR36)], Burkart and
    Huber [[23](#CR23)], Xu et al. [[155](#CR155)], Bauckhage et al. [[11](#CR11)],
    Tjoa and Guan [[139](#CR139)], and Belle and Papantonis [[12](#CR12)]. Molnar
    [[95](#CR95)] devotes a whole book to this topic and Bommasani et al. [[17](#CR17),
    p. 125] provide a recent overview. For language models different types of explanation
    can be used:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 解释方法大致可以分为局部解释或全局解释。局部解释为特定输入 ***x*** 的模型预测提供信息或理由，而全局解释涵盖模型的整体。大多数模型的目标是局部解释，因为这些可以用来证明特定预测的合理性。关于PLM解释方法的调查由Danilevsky等人
    [[36](#CR36)]、Burkart和Huber [[23](#CR23)]、Xu等人 [[155](#CR155)]、Bauckhage等人 [[11](#CR11)]、Tjoa和Guan
    [[139](#CR139)]以及Belle和Papantonis [[12](#CR12)]提供。Molnar [[95](#CR95)] 将整本书都奉献给了这个主题，而Bommasani等人
    [[17](#CR17), p. 125] 提供了最近的概述。对于语言模型，可以使用不同类型的解释：
- en: '**Feature importance** measures the influence of single input features, e.g.
    tokens, on the prediction. It often corresponds to the first derivative of a feature
    with respect to the output [[79](#CR79)]. As the meaning of input tokens is easily
    understood, this type of explanation is readily interpretable by humans.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征重要性**衡量单个输入特征（例如，标记）对预测的影响。它通常对应于特征相对于输出的第一导数 [[79](#CR79)]。由于输入标记的含义容易理解，这种类型的解释可以很容易地由人类解释。'
- en: '**Counterfactual explanations** investigate, how an input ***x*** has to be
    modified, to generate a different target output.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反事实解释**研究如何修改输入 ***x*** 以生成不同的目标输出。'
- en: '**Surrogate models** explain model predictions by a second, simpler model.
    One well-known example is *LIME* [[123](#CR123)], which trains a local linear
    model around a single input ***x*** of interest.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理模型**通过第二个更简单的模型来解释模型预测。一个著名的例子是 *LIME* [[123](#CR123)]，它在一个感兴趣的单一输入 ***x***
    附近训练一个局部线性模型。'
- en: '**Example-driven** explanations illustrate the prediction of an input ***x***
    by selecting other labeled instances that are semantically similar to ***x***.
    This is close to the nearest neighbor approach to prediction and has, for instance,
    been used for text classification [[1](#CR1)].'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以示例驱动**的解释通过选择与 ***x*** 语义相似的标记实例来展示输入 ***x*** 的预测。这接近于预测的最近邻方法，例如，已被用于文本分类
    [[1](#CR1)]。'
- en: '**Source citation** is a general practice of scientific work in which a claim
    is supported by citing respectable scientific sources. The same can be done for
    a text generated by language models with a retrieval component [[57](#CR57)].'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来源引用**是科学工作的普遍做法，其中一项主张通过引用值得尊敬的科学来源来支持。对于具有检索组件的语言模型生成的文本也可以这样做 [[57](#CR57)]。'
- en: Other approaches like a sequence of reasoning steps or rule invocations are
    unusable for PLMs with many millions of parameters.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法，如一系列推理步骤或规则调用，对于具有数百万参数的PLM来说是不可用的。
- en: The self-attention mechanism is the central function unit of PLMs. **BertViz**
    [[144](#CR144)] is a visualization tool that allows users to explore the strength
    of attention between different tokens for the heads and layers in a PLM and allows
    users to get a quick overview of relevant attention heads. However, Jain et al.
    [[63](#CR63)] demonstrate that attention does not correlate with feature importance
    methods and counterfactual changes of attention do not lead to corresponding changes
    in prediction. This may, for instance, be caused by the concatenation of head
    outputs and their subsequent processing by a fully connected nonlinear layer.
    Attentions are noisy predictors of the overall importance of components, but are
    not good at identifying the importance of features [[129](#CR129)].
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制是PLMs（预训练语言模型）的核心功能单元。**BertViz** [[144](#CR144)] 是一个可视化工具，允许用户探索PLM中不同标记的头和层之间的注意力强度，并允许用户快速了解相关的注意力头。然而，Jain等人
    [[63](#CR63)] 证明，注意力与特征重要性方法不相关，并且注意力变化的反事实变化不会导致预测的相应变化。这可能是由头输出的连接及其随后通过全连接非线性层的处理引起的。注意力是组件整体重要性的噪声预测器，但并不擅长识别特征的重要性
    [[129](#CR129)]。
- en: Linear Local Approximations
  id: totrans-328
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 线性局部近似
- en: An important concept is the contribution of input *x*[*i*] towards an output
    *y*[*j*], e.g. a class probability. Gradient-based explanations estimate the contribution
    of input *x*[*i*] towards an output *y*[*j*], e.g. a class probability, by computing
    the partial derivative *∂y*[*j*]∕*∂x*[*i*]. This derivative is often called *saliency*
    and can be interpreted as linear approximation to the prediction function at input
    ***x***. **LIME** [[123](#CR123)] defines a local linear regression model around
    a single input ***x***. Because of correlation of features, the coefficients of
    the input features depend on the presence or absence of the other input features.
    The **SHAP** approach therefore determines the influence of a feature by the average
    influence of the feature for all combinations of other features [[84](#CR84)].
    The authors show the favorable theoretical properties of this approach and derive
    several efficient computation strategies.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的概念是输入 *x*[*i*] 对输出 *y*[*j*]（例如，一个类概率）的贡献。基于梯度的解释通过计算偏导数 *∂y*[*j*]∕*∂x*[*i*]
    来估计输入 *x*[*i*] 对输出 *y*[*j*]（例如，一个类概率）的贡献。这个导数通常被称为 *显著性*，可以解释为输入 ***x*** 处预测函数的线性近似。**LIME**
    [[123](#CR123)] 在单个输入 ***x*** 附近定义了一个局部线性回归模型。由于特征之间的相关性，输入特征的系数依赖于其他输入特征的存在或不存在。因此，**SHAP**
    方法通过其他特征所有组合的平均影响来确定特征的影响 [[84](#CR84)]。作者展示了这种方法的有利理论特性，并推导出几种有效的计算策略。
- en: Nonlinear Local Approximations
  id: totrans-330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非线性局部近似
- en: 'Sundararajan et al. [[132](#CR132)] formulate two basic requirements for this
    type of explanation. *Sensitivity*: if the inputs ***x***^([1]) and ***x***^([2])
    differ in just one feature and lead to different predictions, then the differing
    feature should be given a non-zero contribution. *Implementation invariance*:
    i.e., the attributions are always identical for two functionally equivalent networks.
    As the prediction functions are usually nonlinear, gradient-based methods violate
    both requirements and may focus on irrelevant attributes.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: Sundararajan等人 [[132](#CR132)] 为此类解释提出了两个基本要求。*敏感性*：如果输入 ***x***^([1]) 和 ***x***^([2])
    在仅一个特征上不同并导致不同的预测，那么不同的特征应该被赋予非零的贡献。*实现不变性*：即，对于功能等效的两个网络，归因总是相同的。由于预测函数通常是非线性的，基于梯度的方法违反了这两个要求，并可能关注不相关的属性。
- en: '**Integrated Gradients** [[132](#CR132)] generates an approximation to the
    prediction function ![$$F:\mathbb {R}^n\to [0,1]$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq73.png),
    which captures nonlinear dependencies. To assess the difference from baseline
    input ***x***^([1]) to another input ***x***^([2]), the authors compute the mean
    value of gradients *∂F*(***x***)∕*∂****x*** of the output with respect to inputs
    along the line from ***x***^([1]) to ***x***^([2]) by an integral. It can be shown
    that this approach meets the above requirements. The authors apply the approach
    to question classification according to the type of the answer (Fig. [2.20](#Fig20)).
    The baseline input is the all zero embedding vector. Another application considers
    neural machine translation. Here the output probability of every output token
    is attributed to the input tokens. As baseline all tokens were zeroed except the
    start and end markers. A similar analysis is based on a Taylor expansion of the
    prediction function [[7](#CR7)] .![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig20_HTML.png)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成梯度** [[132](#CR132)] 生成预测函数 ![$$F:\mathbb {R}^n\to [0,1]$$](../images/528393_1_En_2_Chapter/528393_1_En_2_Chapter_TeX_IEq73.png)
    的近似，该函数捕捉非线性依赖关系。为了评估基准输入 ***x***^([1]) 与另一个输入 ***x***^([2]) 之间的差异，作者通过积分计算了输出相对于输入沿从
    ***x***^([1]) 到 ***x***^([2]) 的线的梯度 *∂F*(***x***)∕*∂****x*** 的平均值。可以证明这种方法满足上述要求。作者根据答案类型将这种方法应用于问答分类（图[2.20](#Fig20)）。基准输入是全零嵌入向量。另一个应用考虑了神经机器翻译。在这里，每个输出标记的输出概率都被归因于输入标记。作为基准，所有标记都被置零，除了起始和结束标记。基于预测函数的泰勒展开也有类似的分析
    [[7](#CR7)] 。![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig20_HTML.png)'
- en: A questionnaire on the left illustrates the classification task. On the right,
    a confusion matrix illustrates the task of translating an English input sentence
    into German.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的问卷展示了分类任务。右侧，一个混淆矩阵展示了将英语输入句子翻译成德语的翻译任务。
- en: Fig. 2.20
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.20
- en: Contributions for the question classification task (left). Red marks positive
    influence, blue negative, and black tokens are neutral. Contributions for the
    task of translating *“good morning ladies and gentlemen”* to the German *“Guten
    Morgen Damen und Herren”* are shown on the right side [[132](#CR132)]. Words are
    tokenized to word pieces
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 问答分类任务的贡献（左侧）。红色标记表示正面影响，蓝色表示负面影响，黑色标记表示中性。将“早上好，女士们先生们”翻译成德语“Guten Morgen Damen
    und Herren”的贡献显示在右侧 [[132](#CR132)]。单词被标记为词元
- en: Liu et al. [[82](#CR82)] propose a generative explanation framework which simultaneously
    learns to make classification decisions and generate fine-grained explanations
    for them. In order to reach a good connection between classification and explanation
    they introduce a classifier that is trained on their explanation. For product
    reviews they, for instance, generate the following positive explanations *“excellent
    picture, attractive glass-backed screen, hdr10 and dolby vision”* and negative
    reasons *“very expensive”*. The authors introduce an explanation factor, which
    represents the distance between the probabilities of the classifier trained on
    the explanations vs. the classifier trained on the original input and the gold
    labels. They optimize their models with minimum risk training.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: Liu等人 [[82](#CR82)] 提出了一种生成性解释框架，该框架同时学习做出分类决策并为它们生成细粒度解释。为了在分类和解释之间建立良好的联系，他们引入了一个在他们的解释上训练的分类器。例如，对于产品评论，他们生成了以下正面解释*“优秀的图片，吸引人的玻璃背板屏幕，hdr10和dolby
    vision”*和负面原因*“非常昂贵”*。作者引入了一个解释因子，它表示在解释上训练的分类器与在原始输入和金标签上训练的分类器之间的概率距离。他们通过最小风险训练优化他们的模型。
- en: Explanation by Retrieval
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过检索进行解释
- en: Recently, Deep Learning models have been playing an increasingly important role
    in science and technology. The algorithms developed by Facebook are able to predict
    user preferences better than any psychologist [[24](#CR24), [71](#CR71)]. AlphaFold,
    developed by DeepMind, makes the most accurate predictions of protein structures
    based on their amino acids [[131](#CR131)]. And the PaLM and Retro models are
    capable of generating stories in fluent English, the latter with the knowledge
    of the Internet as background. However, none of the programs were actually able
    to justify their decisions and cannot indicate why a particular sequence was generated
    or on what information a decision was based on.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习模型在科学技术领域扮演着越来越重要的角色。Facebook 开发的算法能够比任何心理学家更好地预测用户偏好 [[24](#CR24), [71](#CR71)]。DeepMind
    开发的 AlphaFold 基于氨基酸最准确地预测蛋白质结构 [[131](#CR131)]。而 PaLM 和 Retro 模型能够用流畅的英语生成故事，后者以互联网知识为背景。然而，这些程序实际上都无法证明它们的决策，也无法指出为什么生成特定的序列，或者决策基于什么信息。
- en: In 2008, Anderson [[5](#CR5)] predicted the end of theory-based science. In
    his view, theories are an oversimplification of reality, and the vast amount of
    accumulated data contains knowledge in a much more detailed form, so theories
    are no longer necessary. This is also the problem of *Explainable AI*, which aims
    to explain the decisions of Deep Learning models. It is always faced with a trade-off
    where predictive accuracy must be sacrificed in order to interpret the model output.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 2008 年，安德森 [[5](#CR5)] 预测了基于理论的科学的终结。在他看来，理论是对现实的过度简化，而大量积累的数据以更详细的形式包含了知识，因此理论不再是必要的。这也是
    *可解释人工智能* 的问题，其目标是解释深度学习模型的决策。它始终面临着一种权衡，即为了解释模型输出，必须牺牲预测准确性。
- en: As large autoregressive language models are combined with retrieval components,
    document retrieval can be used not only to incorporate more accurate knowledge
    into the language generation process, but also to support the generated answers
    by authoritative citations. Metzler et al. [[92](#CR92)] argues that future PLMs
    should justify created text by referring to supporting documents in the training
    data or background document collection. To implement this approach Nakano et al.
    [[100](#CR100)] combine *GPT-3* with the search engine *BING* to enhance language
    generation for question-answering by retrieved documents. Their **WebGPT** [[100](#CR100)]
    first creates a text in natural language (Sect. [6.​2.​3](528393_1_En_6_Chapter.xhtml#Sec16)).
    After that, it enhances the generated sentences by different references to the
    found documents, similar to the way a scientist expands his texts by references.
    By this procedure WebGPT is able to justify and explain the created answer. This
    could be a way to make the generated text more trustworthy. Note that the advanced
    dialog model **LaMDA** can include links to external documents supporting an answer
    (Sect. [6.​6.​3](528393_1_En_6_Chapter.xhtml#Sec52)).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型自回归语言模型与检索组件的结合，文档检索不仅可以用于将更准确的知识纳入语言生成过程，还可以通过权威引用支持生成的答案。Metzler 等人 [[92](#CR92)]
    认为，未来的 PLM 应该通过引用训练数据中的支持文档或背景文档集合中的文档来证明创建的文本。为了实施这种方法，Nakano 等人 [[100](#CR100)]
    将 *GPT-3* 与搜索引擎 *BING* 结合，通过检索文档增强问答的语言生成。他们的 **WebGPT** [[100](#CR100)] 首先以自然语言创建文本（第
    [6.2.3](528393_1_En_6_Chapter.xhtml#Sec16) 节）。之后，它通过引用找到的文档的不同参考来增强生成的句子，类似于科学家通过引用扩展他的文本的方式。通过这一程序，WebGPT
    能够证明和解释创建的答案。这可能是一种使生成的文本更具可信度的方法。注意，高级对话模型 **LaMDA** 可以包含指向支持答案的外部文档的链接（第 [6.6.3](528393_1_En_6_Chapter.xhtml#Sec52)
    节）。
- en: Explanation by Generating a Chain of Thought
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过生成思维链进行解释
- en: Large autoregressive PLMs like GPT-3 are able to produce a very convincing continuation
    of a start text, and, for instance, generate the answer for a question. It turned
    out that their ability to generate the correct answer could drastically be improved
    by giving a few examples with a chain of thought (Sect. [3.​6.​4](528393_1_En_3_Chapter.xhtml#Sec42))
    for deriving the correct answer. This has been demonstrated for the PaLM language
    model [[30](#CR30)].
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 大型自回归语言模型如 GPT-3 能够生成非常令人信服的起始文本的延续，例如，生成问题的答案。结果证明，通过给出一系列推导正确答案的示例（第 [3.6.4](528393_1_En_3_Chapter.xhtml#Sec42)
    节），可以极大地提高生成正确答案的能力。这一点在 PaLM 语言模型 [[30](#CR30)] 中得到了证明。
- en: A generated *thought chain* can be used for other purposes. First, it can be
    checked whether the model produces the correct answer for the “right reasons”,
    rather than just exploiting superficial statistical correlations. In addition,
    the explanation can potentially be shown to an end-user of the system to increase
    or decrease their confidence in a given prediction. Finally, for some queries
    (e.g., explaining a joke), the explanation itself is the desired output [[30](#CR30)].
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的 *思维链* 可以用于其他目的。首先，可以检查模型是否因为“正确的原因”产生正确答案，而不仅仅是利用表面的统计相关性。此外，解释可以潜在地向系统最终用户展示，以增加或减少他们对给定预测的信心。最后，对于某些查询（例如，解释一个笑话），解释本身就是期望的输出
    [[30](#CR30)]。
- en: Figure [2.21](#Fig21) contains a few-shot query and the resulting answer. For
    application only a few example chains of thought are necessary, which can be reused.
    To generate the best answer for the question greedy decoding has to be used, yielding
    the optimal prediction. As PaLM shows, the enumeration of argument steps works
    empirically. However, a sound theory of how models actually use such arguments
    internally is still lacking. Further, it is not known under which circumstances
    the derivation of such a chain of thoughts succeeds. It should be investigated
    to what extent the reasoning of a model corresponds to the reasoning steps performed
    by humans.![](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig21_HTML.png)
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2.21](#Fig21) 包含了几个样本查询及其结果答案。对于应用，只需要几个示例思维链即可重复使用。为了生成最佳答案，必须使用贪婪解码，从而得到最优预测。正如
    PaLM 所示，枚举论证步骤在经验上是有效的。然而，关于模型如何实际内部使用此类论证的理论仍然缺乏。此外，尚不清楚在何种情况下，此类思维链的推导是成功的。应该研究模型推理与人类执行推理步骤的程度对应到何种程度。![图
    2.21](../images/528393_1_En_2_Chapter/528393_1_En_2_Fig21_HTML.png)
- en: A screenshot of 2 boxes with titles and descriptions. The first box includes
    example chain of thoughts and input query. The second one has the model output.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 2 个带有标题和描述的截图。第一个框包含示例思维链和输入查询。第二个框包含模型输出。
- en: Fig. 2.21
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.21
- en: Explaining by a chain of thoughts. The first box contains two examples of thought
    chains, which are used for every query. This chain-of-thought prompt was input
    to the PaLM model together with the input query, and the model output was generated
    by PaLM [[30](#CR30), p. 38]
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 通过思维链进行解释。第一个框包含两个思维链示例，这些示例用于每个查询。这个思维链提示与输入查询一起输入到 PaLM 模型中，并由 PaLM 生成输出 [[30](#CR30)，第
    38 页]
- en: Implementations
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 实现
- en: Ecco [[2](#CR2)] and BertViz [[143](#CR143)] are tools to visualize the attentions
    and embeddings of PLMs. An implementation and a tutorial on integrated gradients
    is available for TensorFlow [[136](#CR136)]. Captum [[26](#CR26), [70](#CR70)]
    is an open-source library to generate interpretations and explanations for the
    predictions of PyTorch models containing most of the approaches discussed above.
    Transformers-interpret [[113](#CR113)] is an alternative open-source model explainability
    tool for the Hugging Face package.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: Ecco [[2](#CR2)] 和 BertViz [[143](#CR143)] 是用于可视化 PLM 的注意力和嵌入的工具。TensorFlow
    [[136](#CR136)] 上有集成梯度的实现和教程。Captum [[26](#CR26)，[70](#CR70)] 是一个开源库，用于为包含上述大多数方法的
    PyTorch 模型的预测生成解释和说明。Transformers-interpret [[113](#CR113)] 是 Hugging Face 包的替代开源模型可解释性工具。
- en: 2.4.6 Summary
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.6 摘要
- en: Similar to other large neural networks, PLMs are optimized with simple stochastic
    gradient descent optimizers that are able to approach the region of minimal cost
    even for huge models with billions of parameters and terabytes of training data.
    This requires parallel training on computing networks which can be controlled
    by suitable software libraries. There are many recipes in the literature for setting
    hyperparameters such as batch size and learning rate schedules. Important ingredients
    are residual connections to be able to optimize networks with many layers and
    regularization modules to keep parameters in a manageable range.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他大型神经网络类似，PLMs 使用简单的随机梯度下降优化器进行优化，即使对于具有数十亿参数和数TB训练数据的巨大模型也能接近最小成本区域。这需要计算网络上的并行训练，这可以通过合适的软件库进行控制。文献中有很多设置超参数（如批量大小和学习率计划）的配方。重要的组成部分包括残差连接，以便能够优化多层网络，以及正则化模块，以保持参数在可管理的范围内。
- en: Neural architecture search is a way to improve performance and reduce memory
    requirements of networks. A number of approaches have been proposed that significantly
    speed up training. Some methods provide models with better performance and lower
    memory footprint. There are new differential methods that have the potential to
    derive better architectures with little effort.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 神经架构搜索是一种提高网络性能和降低内存需求的方法。已经提出了多种方法，这些方法显著加快了训练速度。一些方法提供了性能更好且内存占用更低的模型。有一些新的微分方法，只需付出很少的努力就能推导出更好的架构。
- en: PLMs aim to capture relations between language concepts and can only do so approximately.
    Therefore, it is important to evaluate their inherent uncertainty. Three different
    approaches to analyze the uncertainty are described. Among these, ensemble methods
    appear to be the most reliable, but involve a high computational cost. New algorithms
    such as SNGP, which are based on a single model, are very promising.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: PLM旨在捕捉语言概念之间的关系，但只能近似地做到这一点。因此，评估其固有的不确定性很重要。描述了三种不同的分析方法。在这些方法中，集成方法似乎最可靠，但涉及很高的计算成本。基于单个模型的新算法，如SNGP，非常有前景。
- en: To enable a user to decide whether a model result makes sense, it is necessary
    to explain how the result was obtained. Explanations can be provided by showing
    the importance of features for a result, by exploring the PLM by related examples
    or by approximating the PLM with a simple model. Some libraries are available
    that allow routine use of these methods. A new way of explaining texts generated
    by PLMs is to enhance the texts with appropriate citations of relevant supporting
    documents. Finally, a PLM can be instructed by chain-of-thought prompts to provide
    an explanation for the model response. This type of explanation is particularly
    easy to understand and can reflect the essential parts of a chain of arguments.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让用户决定模型结果是否有意义，有必要解释结果是如何获得的。可以通过展示特征对结果的重要性、通过探索相关示例或通过用简单模型近似PLM来提供解释。一些库允许常规使用这些方法。解释由PLM生成的文本的新方法是通过增强文本，引用相关的支持文档。最后，可以通过思维链提示来指导PLM提供模型响应的解释。这种解释特别容易理解，并能反映论点的关键部分。
- en: The next chapter discusses approaches to improve the three basic PLM types by
    new pre-training tasks or architectural changes. The fourth chapter examines the
    knowledge, which can be acquired by PLMs and that can be used to interpret text
    and to generate new texts.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章讨论了通过新的预训练任务或架构变化来改进三种基本PLM类型的方法。第四章考察了PLM可以获取并用于解释文本和生成新文本的知识。
- en: '[![Creative Commons](../css/cc-by.png)](https://creativecommons.org/licenses/by/4.0)'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '![Creative Commons](../css/cc-by.png)](https://creativecommons.org/licenses/by/4.0)'
- en: '**Open Access** This chapter is licensed under the terms of the Creative Commons
    Attribution 4.0 International License ([http://​creativecommons.​org/​licenses/​by/​4.​0/​](http://creativecommons.org/licenses/by/4.0/)),
    which permits use, sharing, adaptation, distribution and reproduction in any medium
    or format, as long as you give appropriate credit to the original author(s) and
    the source, provide a link to the Creative Commons license and indicate if changes
    were made.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '**开放获取** 本章节根据Creative Commons Attribution 4.0国际许可协议（[http://creativecommons.org/licenses/by/4.0/](http://creativecommons.org/licenses/by/4.0/)）授权，允许在任何媒介或格式下使用、分享、改编、分发和复制，只要您适当引用原始作者和来源，提供Creative
    Commons许可的链接，并指出是否进行了修改。'
- en: The images or other third party material in this chapter are included in the
    chapter's Creative Commons license, unless indicated otherwise in a credit line
    to the material. If material is not included in the chapter's Creative Commons
    license and your intended use is not permitted by statutory regulation or exceeds
    the permitted use, you will need to obtain permission directly from the copyright
    holder.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的图像或其他第三方材料包含在本章节的Creative Commons许可中，除非在材料引用行中另有说明。如果材料未包含在本章节的Creative
    Commons许可中，且您的使用未获得法定规定的许可或超出了许可的使用范围，您需要直接从版权所有者处获得许可。
