<html><head></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="b978-3-031-23190-2_3"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">G. Paaß, S. Giesselbach</span><span class="ContextInformationBookTitles"><span class="BookTitle">Foundation Models for Natural Language Processing</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" lang="en">Artificial Intelligence: Foundations, Theory, and Algorithms</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-3-031-23190-2_3">https://doi.org/10.1007/978-3-031-23190-2_3</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">3. Improving Pre-trained Language Models</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Gerhard Paaß</span><sup><a href="#Aff5">1</a> <span class="ContactIcon"> </span></sup> and </span><span class="Author"><span class="AuthorName">Sven Giesselbach</span><sup><a href="#Aff5">1</a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff5"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Knowledge Discovery Department, Team NLU, Fraunhofer Institute for Intelligent Analysis and Information Systems (IAIS), Sankt Augustin, Nordrhein-Westfalen, Germany</div></div><div class="ClearBoth"> </div></div></div><section class="Abstract" id="Abs1" lang="en" role="doc-abstract"><h2 class="Heading">Abstract</h2><p class="Para" id="Par1">This chapter describes a number of different approaches to improve the performance of Pre-trained Language Models (PLMs), i.e. variants of BERT, autoregressive language models similar to GPT, and sequence-to-sequence models like Transformers. First we may modify the pre-training tasks to learn as much as possible about the syntax and semantics of language. Then we can extend the length of the input sequence to be able to process longer inputs. Multilingual models are simultaneously trained with text in different languages. Most important is the inclusion of further knowledge into the PLM to produce better predictions. It turns out that by increasing the number of parameters, the size of the training data and the computing effort the performance of the models can always be increased. There are a number of different fine-tuning strategies which allow the model to be adapted to special tasks. In addition, models may be instructed by few-shot prompts to solve specific tasks. This is especially rewarding for larger PLMs, which therefore are called Foundation Models.</p></section><div class="KeywordGroup" lang="en"><div class="Heading">Keywords</div><span class="Keyword" epub:type="keyword">Pre-training objective</span><span class="Keyword" epub:type="keyword">Input size</span><span class="Keyword" epub:type="keyword">Multilingual model</span><span class="Keyword" epub:type="keyword">Long dependencies</span><span class="Keyword" epub:type="keyword">Additional knowledge</span><span class="Keyword" epub:type="keyword">Fine-tuning</span></div><!--End Abstract--><div class="Fulltext"><div class="Para" id="Par2">This chapter describes a number of different approaches to improve the performance of <em class="EmphasisTypeItalic ">Pre-trained Language Models</em> (PLMs), i.e. variants of BERT, autoregressive language models similar to GPT, and sequence-to-sequence models like Transformers. When these models have a large number of parameters, they can be instructed by input prompts to solve new tasks and are called <em class="EmphasisTypeItalic ">Foundation Models</em>. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par3"><strong class="EmphasisTypeBold ">Modification of the pre-training tasks</strong>. During pre-training with a large corpus the PLM should learn as much as possible about the syntax and semantics of language. By adapting and enhancing the pre-training objectives the performance of PLMs can be improved markedly, as shown in Sect. <span class="InternalRef"><a href="#Sec1">3.1</a></span>.</p></li><li><p class="Para" id="Par4"><strong class="EmphasisTypeBold ">Increase of the input size</strong>. The length of the input sequence restricts the context, which can be taken into account by a PLM. This is especially important for applications like story generation. Simply increasing input length does not work, as then the number of parameters grows quadratically. In Sect. <span class="InternalRef"><a href="#Sec7">3.2</a></span>, alternatives for establishing sparse attention patterns for remote tokens are explored.</p></li><li><p class="Para" id="Par5"><strong class="EmphasisTypeBold ">Multilingual training</strong> simultaneously trains the same model in different languages. By appropriate pre-training targets the models can generate a joint meaning representation in all languages. Especially for languages with little training data better results can be achieved Sect. <span class="InternalRef"><a href="#Sec12">3.3</a></span>.</p></li><li><p class="Para" id="Par6"><strong class="EmphasisTypeBold ">Adding extra knowledge</strong>. PLMs can be enhanced by including additional information not covered by the training data. This is important as due to the restricted number of parameters PLMs cannot memorize all details included in the training data. Moreover, strict rules are usually represented only as weak associations and need to be reinforced. By incorporating facts and rules from an outside <em class="EmphasisTypeItalic ">knowledge base</em><span id="ITerm1"/> (<em class="EmphasisTypeItalic ">KB</em>) or an additional text collection PLMs can obtain necessary information and keep the content up-to-date, as shown in Sect. <span class="InternalRef"><a href="#Sec17">3.4</a></span>.</p></li><li><p class="Para" id="Par7"><strong class="EmphasisTypeBold ">Changing the model size</strong>. Theoretical results show that model performance improves when the PLMs become larger (Foundation Models). Hence, there is a general trend to increase model size, e.g. by forming mixture-of-experts. On the other hand, it may be necessary to reduce the computation effort and the memory footprint of a PLM. There are a number of techniques to achieve this without sacrificing much performance, as described in Sect. <span class="InternalRef"><a href="#Sec24">3.5</a></span>.</p></li><li><p class="Para" id="Par8"><strong class="EmphasisTypeBold ">Fine-tuning for specific applications</strong>. This can be performed according to different strategies, e.g. with several fine-tuning steps or multiple fine-tuning tasks. Larger PLMs usually can be instructed by prompts to perform specific tasks and are called Foundation Models. In addition, few-shot prompts may be optimized to achieve a more adequate model reaction. This is described in Sect. <span class="InternalRef"><a href="#Sec31">3.6</a></span>.</p></li></ul></div> Note that nearly all proposals may be combined for most model types, resulting in the vast number of model variants that is currently discussed.</div><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading"><span class="HeadingNumber">3.1 </span>Modifying Pre-training Objectives</h2><p class="Para" id="Par9"><span id="ITerm2"/></p><p class="Para" id="Par10">The basic BERT model [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>] has two pre-training tasks: the prediction of masked tokens with the masked language model (MLM) and next sentence prediction (NSP) (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec1"><span class="RefSource">2.​1</span></a></span>). These tasks were chosen heuristically and there are many plausible loss functions and architectures. Researchers have investigated many alternative training objectives, model structures, and attention mechanisms. In this section, the most promising of these variations of the BERT and Transformer architecture are discussed and their relative merits are compared.</p><p class="Para" id="Par11">An important question is the level of aggregation of the input sequence. Here subword tokens are standard. One option is to use raw letters as input. However, this may lead to a high computational burden, as the computational cost of self-attention grows quadratically with the size of the input. Another option is the use of domain-adapted knowledge to model the input sequence by learned tokenizations or patch embeddings (e.g. for image representation, Sect. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml#Sec12"><span class="RefSource">7.​2</span></a></span>). These methods reduce the input complexity, but may potentially ignore useful information in the input [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>].</p><section class="Section2 RenderAsSection2" id="Sec2"><h3 class="Heading"><span class="HeadingNumber">3.1.1 </span>Autoencoders Similar to BERT</h3><div class="Para" id="Par12">To improve BERT’s performance a number of alternatives to capture knowledge from the unlabeled data were proposed: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par13">RoBERTa dynamically changes masks during training.</p></li><li><p class="Para" id="Par14">ALBERT replaces the matrices for self-attention by a matrix product and shares parameters across all layers.</p></li><li><p class="Para" id="Par15">Predicting single masked tokens can be generalized. SpanBERT masks spans of tokens and predicts them. ELECTRA detects randomly replaced tokens at arbitrary positions. XLNet permutes the order of tokens in a sentence and predicts tokens left to right similar to a language model.</p></li><li><p class="Para" id="Par16">DeBERTa disentangles the embeddings for content and position.</p></li></ul></div> The details are given in the following paragraphs. Popular loss functions are defined in Table <span class="InternalRef"><a href="#Tab1">3.1</a></span>. A list of prominent autoencoders is provided in Table <span class="InternalRef"><a href="#Tab2">3.2</a></span>. They can be compared by their performance on natural language understanding tasks (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec7"><span class="RefSource">2.​1.​5</span></a></span>) like GLUE [<span class="CitationRef"><a epub:type="biblioref" href="#CR218" role="doc-biblioref">218</a></span>]. <div class="Table" id="Tab1"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 3.1</span><p class="SimplePara">Loss functions for PLMs. A sequence is denoted by <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> = (<em class="EmphasisTypeItalic ">x</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">T</em></sub>) and <em><strong class="EmphasisTypeBoldItalic ">z</strong></em> = (<em class="EmphasisTypeItalic ">z</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">z</em><sub><em class="EmphasisTypeItalic ">R</em></sub>) is a related sequence, e.g. a translation</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Name</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Loss function</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Description</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC multivariate classification</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq1"><img alt="$$L_{MC}= - \log p(y|{\boldsymbol {x}})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq1.png" style="width:8.94em"/></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">For each training instance (<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>, <em class="EmphasisTypeItalic ">y</em>), e.g. logistic classifier, Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec3"><span class="RefSource">1.​3</span></a></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NM neighborhood model</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq2"><img alt="$$L_{NM}= - \sum _{t=1}^T\sum _{i\in N(t)}\log p(x_i|x_{t})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq2.png" style="width:15.63em"/></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">For neighborhood <em class="EmphasisTypeItalic ">N</em>(<em class="EmphasisTypeItalic ">t</em>) =  {<em class="EmphasisTypeItalic ">t</em>−<em class="EmphasisTypeItalic ">k</em>, …, <em class="EmphasisTypeItalic ">t</em>−1, <em class="EmphasisTypeItalic ">t</em>+1, …, <em class="EmphasisTypeItalic ">t</em>+<em class="EmphasisTypeItalic ">k</em>}, e.g. word2vec, Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec5"><span class="RefSource">1.​5</span></a></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">LM language model</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq3"><img alt="$$L_{LM}= - \sum _{t=1}^T\log p(x_t|{\boldsymbol {x}}_{&amp;lt;t})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq3.png" style="width:12.57em"/></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">e.g. RNN Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec6"><span class="RefSource">1.​6</span></a></span>, GPT Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec13"><span class="RefSource">2.​2.​2</span></a></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">S2S sequence-to-sequence model</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq4"><img alt="$$L_{S2S}= - \sum _{t=1}^{n_z}\log p(z_t|{\boldsymbol {z}}_{&amp;lt;t},{\boldsymbol {x}})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq4.png" style="width:13.62em"/></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">For input sequence <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> = (<em class="EmphasisTypeItalic ">x</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">T</em></sub>) and translation <em><strong class="EmphasisTypeBoldItalic ">z</strong></em> = (<em class="EmphasisTypeItalic ">z</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">z</em><sub><em class="EmphasisTypeItalic ">R</em></sub>) Sects. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec6"><span class="RefSource">1.​6</span></a></span> and <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec19"><span class="RefSource">2.​3</span></a></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM masked language model</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq5"><img alt="$$L_{MLM}= - \sum _{t\in m({\boldsymbol {x}})}\log p(x_t|\tilde {{\boldsymbol {x}}})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq5.png" style="width:13.75em"/></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">m</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em> ) contains the indices of masked tokens in <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>. In <span class="InlineEquation" id="IEq6"><img alt="$$\tilde {{\boldsymbol {x}}}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq6.png" style="width:0.94em"/></span> the masked tokens are replaced by <em class="EmphasisTypeItalic ">MASK</em>, e.g. BERT, Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec1"><span class="RefSource">2.​1</span></a></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">TLM translation masked language model</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq7"><img alt="$$L_{TLM}= - \sum _{t\in m(x)}\log p(x_t|\tilde {{\boldsymbol {x}}})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq7.png" style="width:13.44em"/></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">m</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em> ) contains the indices of masked tokens. <span class="InlineEquation" id="IEq8"><img alt="$$\tilde {{\boldsymbol {x}}}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq8.png" style="width:0.94em"/></span> contains a sentence and its translation. Masked tokens are replaced by <em class="EmphasisTypeItalic ">MASK</em>, e.g. mBERT, Sect. <span class="InternalRef"><a href="#Sec12">3.3</a></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">SBO span boundary objective</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq9"><img alt="$$L_{SMLM}= - \sum _{(i:j)\in m({\boldsymbol {x}})}\log p({\boldsymbol {x}}_{i:j}|\tilde {{\boldsymbol {x}}})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq9.png" style="width:16em"/></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">m</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em> ) contains the spans (<em class="EmphasisTypeItalic ">i</em> : <em class="EmphasisTypeItalic ">j</em>) of masked tokens in <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>. In <span class="InlineEquation" id="IEq10"><img alt="$$\tilde {{\boldsymbol {x}}}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq10.png" style="width:0.94em"/></span> the masked tokens are replaced by other tokens, e.g. SpanBERT, Sect. <span class="InternalRef"><a href="#Sec2">3.1.1</a></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PLM permutation language model</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq11"><img alt="$$L_{PLM}= - \sum _{t=1}^{T}\log p(z_t|{\boldsymbol {z}}_{&amp;lt;t})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq11.png" style="width:13em"/></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em><strong class="EmphasisTypeBoldItalic ">z</strong></em>=<em class="EmphasisTypeItalic ">perm</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>) is a permutation of <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>, e.g. XLNet, Sect. <span class="InternalRef"><a href="#Sec2">3.1.1</a></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">NSP next sentence prediction</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq12"><img alt="$$L_{NSP}= - \log p(\xi |{\boldsymbol {x}},{\boldsymbol {z}})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq12.png" style="width:10.38em"/></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">ξ</em>=1 if text <em><strong class="EmphasisTypeBoldItalic ">z</strong></em> after <em class="EmphasisTypeItalic ">x</em> (else <em><strong class="EmphasisTypeBoldItalic ">z</strong></em> is randomly selected), e.g. BERT, Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec1"><span class="RefSource">2.​1</span></a></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">SOP sentence order prediction</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq13"><img alt="$$L_{SOP}= - \log p(\xi |{\boldsymbol {x}},{\boldsymbol {z}})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq13.png" style="width:10.25em"/></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">ξ</em>=1 if text <em><strong class="EmphasisTypeBoldItalic ">z</strong></em> after <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> (else <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> after <em><strong class="EmphasisTypeBoldItalic ">z</strong></em>), e.g. ALBERT, Sect. <span class="InternalRef"><a href="#Sec2">3.1.1</a></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">RTD replaced token detection</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq14"><img alt="$$L_{RTD}= -\log \sum _{t=1}^{T} p(x_t{=}\tilde {x}_t|\tilde {{\boldsymbol {x}}}) $$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq14.png" style="width:13.82em"/></span></p></td><td style="text-align: left;"><p class="SimplePara">In <span class="InlineEquation" id="IEq15"><img alt="$$\tilde {{\boldsymbol {x}}}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq15.png" style="width:0.94em"/></span> randomly selected elements of <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> were replaced, e.g. ELECTRA, Sect. <span class="InternalRef"><a href="#Sec2">3.1.1</a></span></p></td></tr></tbody></table></div><div class="Table" id="Tab2"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 3.2</span><p class="SimplePara">Autoencoders similar to BERT. The pre-training and fine-tuning loss functions are defined in Table <span class="InternalRef"><a href="#Tab1">3.1</a></span>. The benchmark figures are only a hint, as they depend on the number of parameters and the computing effort</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/><col class="tcol5"/><col class="tcol6"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Section</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Pre-training</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Fine-tuning</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Extra</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Benchmark</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">ELMo [<span class="CitationRef"><a epub:type="biblioref" href="#CR156" role="doc-biblioref">156</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec6"><span class="RefSource">1.​6</span></a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BiLM</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Use bidirectional LSTM</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE 71.0</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec1"><span class="RefSource">2.​1</span></a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM + NSP</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Predict masked tokens</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE 80.5</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">RoBERTa [<span class="CitationRef"><a epub:type="biblioref" href="#CR127" role="doc-biblioref">127</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec2">3.1.1</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Train longer, new mask in new epoch</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE 88.5</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">SpanBERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR98" role="doc-biblioref">98</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec2">3.1.1</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PLM, SBO</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Predict spans of tokens</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE 82.8</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">ELECTRA [<span class="CitationRef"><a epub:type="biblioref" href="#CR223" role="doc-biblioref">223</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec2">3.1.1</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">RTD</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Replaced token detection</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE 89.4</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">StructBERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec2">3.1.1</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">RTD</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Reorder shuffled tokens</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE 89.0</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">ALBERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR113" role="doc-biblioref">113</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec2">3.1.1</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM + SOP</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Factorized embeddings, parameter sharing</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE 89.4</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">XLNET [<span class="CitationRef"><a epub:type="biblioref" href="#CR240" role="doc-biblioref">240</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec2">3.1.1</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PLM</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Predict permuted tokens</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE 90.5</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">DeBERTa [<span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec2">3.1.1</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC, S2S</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Disentangled attention</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE 90.0</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Prod. Key [<span class="CitationRef"><a epub:type="biblioref" href="#CR112" role="doc-biblioref">112</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec2">3.1.1</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Nearest neighbor</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">–</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">UniLM [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec4">3.1.3</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM, LM</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC, LM</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Uni- and bidirectional</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE 87.3</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">BigBird [<span class="CitationRef"><a epub:type="biblioref" href="#CR247" role="doc-biblioref">247</a></span>]</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec8">3.2.1</a></span></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC, S2S</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Sparse attention mechanism</p></td><td style="text-align: left;"><p class="SimplePara">TriviaQA 84.5</p></td></tr></tbody></table></div></div><p class="Para" id="Par17"><strong class="EmphasisTypeBold ">RoBERTa</strong><span id="ITerm3"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR127" role="doc-biblioref">127</a></span>] is an enhanced BERT model boosted by tweaking parts of the pre-training process. The authors improved the BERT<sub>BASE</sub> architecture by the following changes: (1) Instead of using the same mask for all epochs, they replicate training sequences with different masks. (2) They remove the Next-Sentence-Prediction objective and found that performance is best, when all sentences in a batch are from the same document. (3) Larger batches with larger step sizes increase perplexity for both the masked language model task and downstream task performance. (4) A 10-fold increase of training data to 160 GB, which is used in large batches. The resulting model achieves an impressive <span class="EmphasisTypeSmallCaps ">Sota</span> result of 88.5 on <em class="EmphasisTypeItalic ">GLUE</em><span id="ITerm4"/> (language understanding [<span class="CitationRef"><a epub:type="biblioref" href="#CR217" role="doc-biblioref">217</a></span>]), and the reading comprehension tasks <em class="EmphasisTypeItalic ">RACE</em><span id="ITerm5"/> and <em class="EmphasisTypeItalic ">SQuAD</em><span id="ITerm6"/><span id="ITerm7"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR173" role="doc-biblioref">173</a></span>].</p><div class="Para" id="Par18"><strong class="EmphasisTypeBold ">SpanBERT</strong><span id="ITerm8"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR98" role="doc-biblioref">98</a></span>] introduces a span-level pre-training approach. Rather than masking single tokens during pre-training, spans of one or more complete words are masked covering about 15% of the tokens. A new span-boundary objective (SBO) is introduced, where tokens inside of the masked span are predicted, using only representations of the tokens just outside the boundaries of the span combined with positional information. The details are shown in Fig. <span class="InternalRef"><a href="#Fig1">3.1</a></span>. SBO is used together with the usual MLM objective. Finally, the authors omit the next sentence prediction task as in [<span class="CitationRef"><a epub:type="biblioref" href="#CR127" role="doc-biblioref">127</a></span>] and only use single text fragments/sentences for training. The authors find that masking random spans is more effective than masking linguistic units. SpanBERT has the same configuration as BERT<sub>LARGE</sub> and is pre-trained on the BooksCorpus and the English Wikipedia. SpanBERT achieves a new <span class="EmphasisTypeSmallCaps ">Sota</span> of 79.6% F1 on the <em class="EmphasisTypeItalic ">OntoNotes coreference task</em><span id="ITerm9"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR164" role="doc-biblioref">164</a></span>], which requires identifying pronouns and the corresponding nouns or two phrases referring to the same thing (Sect. <span class="ExternalRef"><a href="528393_1_En_5_Chapter.xhtml#Sec20"><span class="RefSource">5.​4.​1</span></a></span>).<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><img alt="" aria-describedby="d64e1893" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig1_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e1893"><p class="Para" id="Par301">An illustration represents the approach of span B E R T. It indicates the layers of input, Input and output embeddings through transformer encoder, left and right boundary embeddings, position embeddings, 2-layer network, token probabilities, and predicted tokens.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.1</span><p class="SimplePara">SpanBERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR98" role="doc-biblioref">98</a></span>] concatenates the embeddings outside the border of a span with a position embedding. With this input a 2-layer model predicts the probabilities of masked tokens</p></div></figcaption></figure></div><p class="Para" id="Par19"><strong class="EmphasisTypeBold ">StructBERT</strong><span id="ITerm10"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR223" role="doc-biblioref">223</a></span>] enhances the original BERT MLM objective by the task to predict the order of shuffled token triples. In addition, the order of three sentences has to be detected. Using models with the same number of parameters, StructBERT can increase the <span class="EmphasisTypeSmallCaps ">Sota</span> on GLUE in comparison to BERT and RoBERTa to 83.9 and 89.0, respectively.</p><p class="Para" id="Par20"><strong class="EmphasisTypeBold ">Electra</strong><span id="ITerm11"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>] proposes a new pre-training task called <em class="EmphasisTypeItalic ">replaced token detection</em><span id="ITerm12"/> (RTD). In the paper a generator network, trained with a masked language model loss, is combined with a discriminator network. Some tokens in the input sequence are replaced with plausible alternatives which are generated by a small language model (about 1∕4 of the size of the discriminator). The discriminator network has to predict for every token, whether it is a replacement or not. This corruption procedure solves a mismatch in BERT, where <em class="EmphasisTypeItalic ">MASK</em> tokens appear in pre-training but not in fine-tuning. The model learns from all input tokens instead of just the small masked subset, making it more computationally efficient than e.g. BERT and RoBERTa, while performing better on several tasks, e.g. 89.4% on the GLUE language understanding task.</p><p class="Para" id="Par21"><strong class="EmphasisTypeBold ">ALBERT</strong><span id="ITerm13"/> (a lite BERT) [<span class="CitationRef"><a epub:type="biblioref" href="#CR113" role="doc-biblioref">113</a></span>] uses two parameter-reduction techniques to tackle the huge memory consumption of BERT and its slow training speed. The first tweak is untying the dimensionality of the WordPiece embeddings from the hidden layer size of BERT. Instead of using a single embedding matrix <em class="EmphasisTypeItalic ">M</em>, the authors factorize <em class="EmphasisTypeItalic ">M</em> = <em class="EmphasisTypeItalic ">A</em> ∗ <em class="EmphasisTypeItalic ">B</em>, such that the joint number of parameters in <em class="EmphasisTypeItalic ">A</em> and <em class="EmphasisTypeItalic ">B</em> is much lower than the number of parameters in <em class="EmphasisTypeItalic ">M</em>. The second tweak is sharing all parameters across all layers of BERT, which is shown to stabilize training and keep the number of parameters fixed even if more layers are added. In addition to the two tweaks, a new sentence order prediction (SOP) is introduced. Specifically, the model has to predict if the order of two sentences is correct or reversed. The authors report that this task improves accuracy compared to BERT’s NSP task, which could be solved by comparing the topics of the two sentences. It is still unclear, however, if this is the best way to incorporate text structure in training. ALBERT achieved new <span class="EmphasisTypeSmallCaps ">Sota</span> results on GLUE and SQuAD.</p><p class="Para" id="Par22"><strong class="EmphasisTypeBold ">XLNet</strong><span id="ITerm14"/> solves an autoregressive pre-training task instead of predicting masked words [<span class="CitationRef"><a epub:type="biblioref" href="#CR240" role="doc-biblioref">240</a></span>]. This addresses the problem that BERT’s <em class="EmphasisTypeItalic ">[MASK]</em> token only appears during pre-training and not in fine-tuning. The words in a sequence, e.g. <em class="EmphasisTypeItalic ">“The</em><sub>1</sub><em class="EmphasisTypeItalic ">mouse</em><sub>2</sub><em class="EmphasisTypeItalic ">likes</em><sub>3</sub><em class="EmphasisTypeItalic ">cheese</em><sub>4</sub>”, are reordered together with their position information (indices) by a random permutation, e.g. <em class="EmphasisTypeItalic ">“cheese</em><sub>4</sub><em class="EmphasisTypeItalic ">The</em><sub>1</sub><em class="EmphasisTypeItalic ">likes</em><sub>3</sub><em class="EmphasisTypeItalic ">mouse</em><sub>2</sub>”. The task is to successively predict the tokens in the permuted sequence similarly to a GPT language model. The model has to predict, e.g. <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">mouse</em>—2, <em class="EmphasisTypeItalic ">cheese</em><sub>4</sub>,<em class="EmphasisTypeItalic ">The</em><sub>1</sub>,<em class="EmphasisTypeItalic ">likes</em><sub>3</sub>). Note that the model must additionally know the position, here 2, of the word to be predicted. The transformer, however, mixes the position information with the content information by forming a sum. Hence, the position information is inseparable from the token embedding.</p><p class="Para" id="Par23">Therefore, the authors decided to compute an additional self-attention embedding called <em class="EmphasisTypeItalic ">query stream</em><span id="ITerm15"/>, which as query only receives the target position and then can compute the attention with the key and value vectors (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec3"><span class="RefSource">2.​1.​1</span></a></span>). The resulting embedding encodes the position of the token to be predicted and correlations to other tokens, but has no information on the content of that token. This information can be added as input to the model. The normal self-attention and the query stream have the same parameter matrices <em class="EmphasisTypeItalic ">Q</em> (query),<em class="EmphasisTypeItalic ">K</em> (key), <em class="EmphasisTypeItalic ">V</em>  (value). To save training effort, XLNet only predicts a few tokens at the end of the permuted sequence. In addition, XLNet integrates the segment recurrence mechanism and relative encoding scheme of Transformer-XL (Sect. <span class="InternalRef"><a href="#Sec9">3.2.2</a></span>) into pre-training, which empirically improves the performance especially for tasks involving a longer text sequence.</p><p class="Para" id="Par24">When a token is predicted information about tokens before and after it may be used. Therefore, the model is a bidirectional encoder. With BERT, if the two tokens <em class="EmphasisTypeItalic ">“New”</em> and <em class="EmphasisTypeItalic ">“York”</em> are masked, both words are predicted independently, ignoring valuable information. In contrast, XLNet properly handles the dependence of masked tokens. XLNet was able to outperform BERT and RoBERTa on many tasks, e.g. the GLUE language understanding tasks, reading comprehension tasks like SQuAD (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec8"><span class="RefSource">2.​1.​5</span></a></span>), text classification tasks such as <em class="EmphasisTypeItalic ">IMDB</em><span id="ITerm16"/> (movie review classification) [<span class="CitationRef"><a epub:type="biblioref" href="#CR130" role="doc-biblioref">130</a></span>].</p><p class="Para" id="Par25"><strong class="EmphasisTypeBold ">Product Keys</strong><span id="ITerm17"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR112" role="doc-biblioref">112</a></span>] replace the dot-product attention by a nearest neighbor search. A query <em><strong class="EmphasisTypeBoldItalic ">q</strong></em><sub><em class="EmphasisTypeItalic ">r</em></sub> is split into two sub-queries <span class="InlineEquation" id="IEq16"><img alt="$${\boldsymbol {q}}_{r}^{[1]}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq16.png" style="width:1.56em"/></span> and <span class="InlineEquation" id="IEq17"><img alt="$${\boldsymbol {q}}_{r}^{[2]}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq17.png" style="width:1.56em"/></span>. For each sub-query the <em class="EmphasisTypeItalic ">k</em> closest sub-keys <span class="InlineEquation" id="IEq18"><img alt="$$\boldsymbol {k}_i^{[1]}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq18.png" style="width:1.56em"/></span> and <span class="InlineEquation" id="IEq19"><img alt="$$\boldsymbol {k}_j^{[2]}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq19.png" style="width:1.56em"/></span> are selected. From the <em class="EmphasisTypeItalic ">k</em><sup>2</sup> combinations of sub-keys the highest dot products can be efficiently computed and the <em class="EmphasisTypeItalic ">k</em> highest combinations are selected. The results are normalized with the softmax function and used for the computation of a weighted sum of value vectors. During optimization only the <em class="EmphasisTypeItalic ">k</em> optimal keys are affected reducing the training effort. The approach allows very large transformers to be defined with only a minimal computational overhead. With 12 layers the authors achieve the same performance as a 24 layer BERT model using only half of the computation time. In a comprehensive comparison of transformer architectures [<span class="CitationRef"><a epub:type="biblioref" href="#CR142" role="doc-biblioref">142</a></span>] the approach yields an increase for SuperGLUE NLU task (Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec3"><span class="RefSource">4.​1.​2</span></a></span>) from 71.7% for the standard T5 model to 75.2%.</p><p class="Para" id="Par26"><strong class="EmphasisTypeBold ">DeBERTa</strong><span id="ITerm18"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>] uses a <em class="EmphasisTypeItalic ">disentangled attention</em><span id="ITerm19"/> mechanism, where each word is represented by two different types of vectors encoding content and position. The attention weights between tokens are computed using different matrices for content and relative position. In addition, DeBERTa includes absolute word positions in the last layer to capture different syntactic roles in the sentence. During fine-tuning the model employs an “adversarial” training approach, where embeddings are normalized to probability vectors. Then the model is trained to be robust against small perturbations of embeddings. According to the authors, this improves the performance of fine-tuned models. The large version of the model with 1.5B parameters has superior performance in several application areas, e.g. in natural language understanding (Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec3"><span class="RefSource">4.​1.​2</span></a></span>), where DeBERTa surpasses the human performance on the <em class="EmphasisTypeItalic ">SuperGLUE benchmark</em><span id="ITerm20"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR219" role="doc-biblioref">219</a></span>] for the first time, increasing the macro-average score to 89.9%.</p><p class="Para" id="Par27">Bengio et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>] argue that representations, e.g. embeddings, should be <em class="EmphasisTypeItalic ">disentangled</em><span id="ITerm21"/> and should represent different content aspects, e.g. syntax, style, semantics, in different parts of the embedding vector. Locatello et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR129" role="doc-biblioref">129</a></span>] have proven that this is not possible in an unsupervised way. Hence, some explicit supervision or prior information has to be used to generate interpretable subvectors of embeddings.</p><p class="Para" id="Par28"><strong class="EmphasisTypeBold ">DeBERTaV3</strong><span id="ITerm22"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR75" role="doc-biblioref">75</a></span>] substitutes the MLM loss of DeBERTa with the replaced token detection (RTD) of Electra (Sect. <span class="InternalRef"><a href="#Sec2">3.1.1</a></span>). In addition, a new gradient-disentangled embedding sharing method is employed that improves both training efficiency and the quality of the pre-trained model. Its largest version has a 128k-token vocabulary, 24 layers, and 304M parameters. For the GLUE benchmark with fine-tuning, the model increases the score by 1.4% to a new <span class="EmphasisTypeSmallCaps ">Sota</span> of 91.4%. The multi-language version of the model mDeBERTa<sub>BASE</sub> outperforms XLM-R<sub>BASE</sub> by 3.6% in terms of the cross lingual transfer accuracy on the <em class="EmphasisTypeItalic ">XNLI</em><span id="ITerm23"/> task (Sect. <span class="InternalRef"><a href="#Sec13">3.3.1</a></span>).</p></section>
<section class="Section2 RenderAsSection2" id="Sec3"><h3 class="Heading"><span class="HeadingNumber">3.1.2 </span>Autoregressive Language Models Similar to GPT</h3><div class="Para" id="Par29">By increasing the number of parameters and the training set size the capabilities of GPT models can be markedly improved. An overview is given in Table <span class="InternalRef"><a href="#Tab3">3.3</a></span>. <div class="Table" id="Tab3"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 3.3</span><p class="SimplePara">Autoregressive language models (LM) similar to GPT. ‘Details’ provides the number of parameters and specific features. The ‘benchmark’ figures are only a hint, as they depend on the selected number of parameters and the computing effort. Best benchmark value printed in bold</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Section</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Details</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Benchmark</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GPT-2 [<span class="CitationRef"><a epub:type="biblioref" href="#CR167" role="doc-biblioref">167</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec11"><span class="RefSource">2.​2</span></a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.6B LM to generate text</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Lambada 0-shot 63.2%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Retro [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec15"><span class="RefSource">6.​2.​3</span></a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">7B LM with retrieval to generate text</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Lambada 73.0%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Megatron-LM [<span class="CitationRef"><a epub:type="biblioref" href="#CR193" role="doc-biblioref">193</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec3">3.1.2</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.3B LM to generate text</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Lambada 66.5%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Turing-NLG [<span class="CitationRef"><a epub:type="biblioref" href="#CR179" role="doc-biblioref">179</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec3">3.1.2</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">17B LM to generate text</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Lambada 68.0%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Chinchilla [<span class="CitationRef"><a epub:type="biblioref" href="#CR83" role="doc-biblioref">83</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec3">3.1.2</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">70B LM to generate text</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Lambada 0-shot 77.4%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GPT-3 [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec3">3.1.2</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">175B long sequence LM to generate text</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Lambada 0-shot 76.2%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">WebGPT [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec16"><span class="RefSource">6.​2.​3</span></a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">175B GPT-3 + Bing search engine</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Same as GPT-3</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">InstructGPT [<span class="CitationRef"><a epub:type="biblioref" href="#CR151" role="doc-biblioref">151</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec43">3.6.5</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">175B GPT-3 fine-tuned for instructions</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Same as GPT-3</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">OPT [<span class="CitationRef"><a epub:type="biblioref" href="#CR151" role="doc-biblioref">151</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec3">3.1.2</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">free 175B LM similar to GPT-3</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Lambada 0-shot 74.7%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BLOOM [<span class="CitationRef"><a epub:type="biblioref" href="#CR151" role="doc-biblioref">151</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec3">3.1.2</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">176B LM for European languages</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Lambada 0-shot 67.2%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PanGu-<em class="EmphasisTypeItalic ">α</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR248" role="doc-biblioref">248</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec3">3.1.2</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">200B long sequence LM to generate text</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Chinese benchmarks</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Gopher [<span class="CitationRef"><a epub:type="biblioref" href="#CR168" role="doc-biblioref">168</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec3">3.1.2</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">280B LM to generate text</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Lambada 0-shot 74.5%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MT-NLG [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec3">3.1.2</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">530B Megatron variant</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Lambada 76.6%</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PaLM [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec3">3.1.2</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">540B shared key-value projections</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Lambada 0-shot <strong class="EmphasisTypeBold ">77.9%</strong></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLaM [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec26">3.5.2</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1200B mixture-of-experts LM</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Lambada 0-shot 73.7%</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">WuDao-2.0 [<span class="CitationRef"><a epub:type="biblioref" href="#CR178" role="doc-biblioref">178</a></span>]</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec26">3.5.2</a></span></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">1750B mixture-of-experts LM</p></td><td style="text-align: left;"><p class="SimplePara">Lambada: better than Turing-NLG</p></td></tr></tbody></table></div></div><div class="Para" id="Par30"><strong class="EmphasisTypeBold ">GPT-3</strong><span id="ITerm24"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>] is a language model with extreme dimensions. Its largest version has 96 layers, 96 attention heads, 175 billion parameters and covers sequences of length 2048. It was trained on a text collection of books, Wikipedia and web pages of about 500 billion tokens. The details of the architecture are not known yet. GPT-3 is structurally similar to GPT-2, and therefore its higher level of accuracy is attributed to its increased capacity and higher number of parameters. The model achieved an unprecedented performance in language modeling, question answering, etc. Some results are compiled in Table <span class="InternalRef"><a href="#Tab4">3.4</a></span> and many more in the paper [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>]. <div class="Table" id="Tab4"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 3.4</span><p class="SimplePara">Comparing different versions of PaLM, GPT-3, Chinchilla, Gopher, OPT, GLaM, and BLOOM on a number of popular benchmarks covering text completion, pronoun coreference, common sense reasoning and question answering (QA) [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>]. FLOPS measures the computational effort in floating point operations per second. Best benchmark values printed in bold</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/><col class="tcol5"/><col class="tcol6"/><col class="tcol7"/><col class="tcol8"/><col class="tcol9"/><col class="tcol10"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PaLM</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PaLM</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PaLM</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GPT-3</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Chinchilla</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Gopher</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">OPT</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLaM</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BLOOM</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model size (billion parameters)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">62</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">540</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">175</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">70</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">280</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">175</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1200</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">176</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Num. training Tokens (billion)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">780</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">795</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">780</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">400</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1400</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">300</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">180</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1600</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">350</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Training effort (10<sup>21</sup> FLOPS)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">37.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">295.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">2527</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">314.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">588.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">504.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">≈ 50</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">≈ 105</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"> </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Lambada 0-shot (text compl.)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">69.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">75.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">77.9</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">76.2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">77.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">74.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">73.7</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">67.2</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">HellaSWAG 0-shot (text compl.)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">68.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">79.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">83.4</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">78.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">80.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">79.2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">79.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">77.1</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">73.0</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PIQA 0-shot (common sense)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">77.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">80.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">82.3</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">80.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">81.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">81.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">78.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">80.4</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"> </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Winogrande 0-shot (coreference)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">66.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">77.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">81.1</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">70.2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">74.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">70.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">74.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">73.4</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">70.1</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BoolQ 0-shot (QA)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">68.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">84.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">88.0</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">60.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">83.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">79.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">64.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">83.0</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"> </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Natural questions 0-shot (QA)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">18.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">21.2</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">14.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">16.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">10.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">21.5</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"> </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Natural questions few-shot (QA)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">14.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">27.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">36.0</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">29.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">31.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">24.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-bottom: 0.5pt solid ; text-align: left;"> </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Trivia QA 0-shot (QA)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">39.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">67.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">76.9</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">64.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">67.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">52.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">68.0</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"> </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Trivia QA few-shot (QA)</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">48.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">72.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">81.4</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">71.2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">73.2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">63.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-bottom: 0.5pt solid ; text-align: left;"> </td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Average task metric</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">51.2</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">64.8</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">69.8</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">60.7</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">65.2</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">59.5</p></td><td style="border-right: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; text-align: left;"> </td><td style="text-align: left;"> </td></tr></tbody></table></div></div><div class="Para" id="Par31">GPT-3 is able to generate fluent texts and covers a huge amount of world knowledge, as the example in Fig. <span class="InternalRef"><a href="#Fig2">3.2</a></span> shows. Examples of generated texts can be found in many locations [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR149" role="doc-biblioref">149</a></span>]. The amount and quality of knowledge captured by PLMs is discussed in Chap. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml"><span class="RefSource">4</span></a></span>. In contrast to other language models, GPT-3 can be instructed by a few sentences to perform quite arbitrary tasks (few-shot learning). This is a very simple way to use GPT-3 to solve quite specific tasks such as translating into another language, summarizing a document, correcting grammar, writing an essay on a given topic, etc. Details are discussed in Sect. <span class="InternalRef"><a href="#Sec41">3.6.3</a></span>.<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO2"><img alt="" aria-describedby="d64e3100" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig2_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e3100"><p class="Para" id="Par302">A set of 2 text boxes represents the input of an article requirement and the output of G P T 3. The input title reads united methodists agree to historic split.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.2</span><p class="SimplePara">Text generated by GPT-3 in response to an input. Quoted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>, p. 28]</p></div></figcaption></figure></div><p class="Para" id="Par32">At the end of 2021 OpenAI provided an API to fine-tune GPT-3 with user-specific data [<span class="CitationRef"><a epub:type="biblioref" href="#CR123" role="doc-biblioref">123</a></span>]. In this way, the model can be adapted to a specific domain language and, in addition, be prepared to perform specific classification tasks. In general, this yields higher quality results than prompt design. In addition, no few-shot examples are necessary anymore. Details of fine-tuning GPT-3 are discussed in Sect. <span class="InternalRef"><a href="#Sec40">3.6.2</a></span>. Table <span class="InternalRef"><a href="#Tab4">3.4</a></span> compares GPT-3 with other more recent language models on a number of popular benchmarks. There is a clear advantage of the new PaLM model.</p><p class="Para" id="Par33"><strong class="EmphasisTypeBold ">GPT-J-6B</strong><span id="ITerm25"/> is an open-source GPT model with 28 layers, 16 heads, a context size of 2048, and 6B parameters [<span class="CitationRef"><a epub:type="biblioref" href="#CR221" role="doc-biblioref">221</a></span>]. It has a similar performance as the GPT-3 version with 6.7B parameters. There is an interactive web demo where users can enter their prompts and a continuation text is generated [<span class="CitationRef"><a epub:type="biblioref" href="#CR220" role="doc-biblioref">220</a></span>]. <strong class="EmphasisTypeBold ">GPT-Neo</strong><span id="ITerm26"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>] is another free version of GPT with 2.7B parameters. It was trained on the <em class="EmphasisTypeItalic ">Pile</em><span id="ITerm27"/>, a 825 GB data set containing data from 22 diverse sources, including academic sources (e.g. ArXiv), Internet webpages (e.g. StackExchange), dialogs from subtitles, GitHub, etc. It outperforms the GPT-3 version with the same parameter size on some natural language understanding tasks [<span class="CitationRef"><a epub:type="biblioref" href="#CR89" role="doc-biblioref">89</a></span>]. Recently, <strong class="EmphasisTypeBold ">GPT-NeoX-20B</strong><span id="ITerm28"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR215" role="doc-biblioref">215</a></span>] was released. It has 44 layers, an internal vector dimension of 6144, 64 heads and uses batches of size 3.1M for training. In the LAMBADA benchmark (Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec4"><span class="RefSource">4.​1.​3</span></a></span>) with the task of predicting the missing last word of the last sentence of each passage, it achieves an accuracy of 72.0%. This value is close to GPT-3 with 75.2%.</p><p class="Para" id="Par34"><strong class="EmphasisTypeBold ">Megatron-LM</strong><span id="ITerm29"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR193" role="doc-biblioref">193</a></span>] scale language models such as GPT-2 and BERT efficiently by introducing intra-layer model parallelism. The authors place self-attention heads as well as feed-forward layers on different GPUs, reducing the memory burden of a single GPU. They present a GPT-variant with 8.3B parameters and a 3.9B parameter model similar to BERT. Highlights of the approach include 76% scaling efficiency when using 512 GPUs. Their GPT model reduces the <em class="EmphasisTypeItalic ">WikiText-103</em><span id="ITerm30"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR134" role="doc-biblioref">134</a></span>] <span class="EmphasisTypeSmallCaps ">Sota</span> perplexity from 15.8 to 10.8 and their BERT model increases RACE (reading comprehension) [<span class="CitationRef"><a epub:type="biblioref" href="#CR110" role="doc-biblioref">110</a></span>] accuracy to 90.9%.</p><p class="Para" id="Par35"><strong class="EmphasisTypeBold ">Jurassic-1</strong><span id="ITerm31"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR122" role="doc-biblioref">122</a></span>] is an autoregressive language model similar to GPT-3 with 178B parameters. The authors chose a token vocabulary of 256k instead of 50k for GPT-3, which also included frequent multi-word expressions such as named entities and common phrases. The training text could be represented with 28% fewer tokens than GPT-3. Hence, the model can process queries up to 1.4× faster when using the same architecture. The model used a maximal sequence length of 2048 tokens. In spite of the larger vocabulary only 2% of all parameters were required for the input embeddings. The model was trained on 300B tokens drawn from public text corpora using a final batch size of 3.2M tokens.</p><p class="Para" id="Par36"><strong class="EmphasisTypeBold ">PanGu-</strong><em class="EmphasisTypeItalic ">α</em><span id="ITerm32"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR248" role="doc-biblioref">248</a></span>] is a model of Huawei similar to GPT-3 with up to 200B parameters. It was trained on 1.1TB Chinese text, and was applied to a large number of tasks in zero-shot, one-shot, and few-shot settings without any fine-tuning. The model has a performance comparable to GPT-3.</p><p class="Para" id="Par37"><strong class="EmphasisTypeBold ">OPT-175B</strong><span id="ITerm33"/> (Open Pre-trained Transformer) [<span class="CitationRef"><a epub:type="biblioref" href="#CR253" role="doc-biblioref">253</a></span>] is a suite of 8 GPT models with 125M to 175B parameters developed by Meta. It was trained on publicly available datasets with 180B tokens. The largest models has 96 layers, each with 96 heads. Although OPT-175B has the same parameter count as GPT-3, its training required only 1/7th of computing effort of GPT-3. The model was evaluated on 16 NLP tasks and showed approximately the same performance as GPT-3 (Table <span class="InternalRef"><a href="#Tab4">3.4</a></span>). All trained models up to 30B parameters are freely available. The large 175B parameter model is only available to academic researchers upon request to discourage the production of fake news. The model can be trained and deployed on only 16 NVIDIA V100 GPUs. Some benchmark results are provided in Table <span class="InternalRef"><a href="#Tab4">3.4</a></span>.</p><p class="Para" id="Par38"><strong class="EmphasisTypeBold ">BLOOM</strong><span id="ITerm34"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR139" role="doc-biblioref">139</a></span>] is an autoregressive large language model with 176B parameters. It has 70 layers with 112 attention-heads per layer and 2048 token sequence length. It was developed by the BigScience initiative<span id="ITerm35"/> of over 1000 AI researchers to provide a free large language model for everyone who wants to try. Its training data covers 46 natural languages (English 30%, Chinese 16%, French 12%, Spanish 11%, …) and 11% code (java, php, …) with 350B tokens. The 176B BLOOM model has been trained using the Megatron-DeepSpeed library [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>] <span id="ITerm36"/> offering different types of parallelism. The model can be evaluated on 8 large GPUs. Hence, BLOOM is one of the largest trained model available for research purposes. Some benchmark results are provided in Table <span class="InternalRef"><a href="#Tab4">3.4</a></span>.</p><p class="Para" id="Par39"><strong class="EmphasisTypeBold ">Gopher</strong><span id="ITerm37"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR168" role="doc-biblioref">168</a></span>] employed the GPT-2 architecture with two modifications. For regularization the authors used RMSNorm (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec32"><span class="RefSource">2.​4.​2</span></a></span>) instead of LayerNorm and they employed the relative positional encoding scheme [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>] instead of absolute positional encoding. Gopher has 80 layers with 128 attention heads and 280B parameters. All models were trained on 300B tokens with a context window of 2048 tokens and a batch size of up to 6M tokens. For the large models a 16 bit float numbers was used to reduce memory and increase training throughput.</p><p class="Para" id="Par40">Six model versions with different numbers of parameters were trained to assess the effect of model size. The authors present a comprehensive evaluation on 152 tasks described in Table <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Tab3"><span class="RefSource">4.​3</span></a></span>. Gopher shows an improvement on 100 of 124 tasks. One of these is the <em class="EmphasisTypeItalic ">LAMBADA benchmark</em><span id="ITerm38"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR154" role="doc-biblioref">154</a></span>] where Gopher generates a zero-shot score of 74.5, which is only slightly below the value 76.6 of <em class="EmphasisTypeItalic ">MT-NLG</em> model with 530B parameters [<span class="CitationRef"><a epub:type="biblioref" href="#CR106" role="doc-biblioref">106</a></span>]. For instance Gopher achieves <span class="EmphasisTypeSmallCaps ">Sota</span> for all 12 benchmarks on humanities covering areas like econometrics and psychology surpassing the best supervised results for 11 benchmarks. Some results are provided in Table <span class="InternalRef"><a href="#Tab4">3.4</a></span> while Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec5"><span class="RefSource">4.​1.​4</span></a></span> describes more details.</p><p class="Para" id="Par41"><strong class="EmphasisTypeBold ">Chinchilla</strong><span id="ITerm39"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR83" role="doc-biblioref">83</a></span>] is a mid-size encoder model with 70B parameters, which has the same compute budget as the larger Gopher model, but four times as much data. Chinchilla consistently has a better performance than Gopher (Table <span class="InternalRef"><a href="#Tab4">3.4</a></span>) and significantly outperforms GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large set of downstream evaluation tasks. For every doubling of model size the number of training tokens should also be doubled. This is a much larger scaling rate than that predicted by Kaplan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR102" role="doc-biblioref">102</a></span>] in Sect. <span class="InternalRef"><a href="#Sec25">3.5.1</a></span>.</p><p class="Para" id="Par42"><strong class="EmphasisTypeBold ">Turing-NLG</strong><span id="ITerm40"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR179" role="doc-biblioref">179</a></span>] introduces an autoregressive language model with 78 transformer layers, a hidden vector-size of 4256, 28 attention heads and 17B parameters. As a model with more than 1.3B parameters cannot fit into a single GPU with 32 GB memory it must be parallelized, or broken into pieces, across multiple GPUs. Turing-NLG leverages a <span class="EmphasisTypeSmallCaps ">Sota</span> Deep Learning hardware with high communication bandwidth, the Megatron-LM framework, and the DeepSpeed library<span id="ITerm41"/>, which further optimizes the training speed and reduces the resources needed. The model achieved <span class="EmphasisTypeSmallCaps ">Sota</span> performance on language modeling tasks and also proved to be effective for zero-shot question answering and abstractive summarization.</p><p class="Para" id="Par43">Its successor <strong class="EmphasisTypeBold ">MT-NLG</strong><span id="ITerm42"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>] is a 105-layer encoder model with 530B parameters and was trained across 280 GPUs with a huge batch size of 1920. Similar to GPT-3 it improves performance on zero-, one- and few-shot tasks. For the <em class="EmphasisTypeItalic ">LAMBADA benchmark</em><span id="ITerm43"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR154" role="doc-biblioref">154</a></span>], for example, the model has to predict the last word of paragraph (Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec4"><span class="RefSource">4.​1.​3</span></a></span>). On this benchmark MT-NLG improves the few-shot accuracy of GPT-3 (86.4%) to the <span class="EmphasisTypeSmallCaps ">Sota</span> 87.2%.</p><p class="Para" id="Par44"><strong class="EmphasisTypeBold ">PaLM</strong><span id="ITerm44"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>] is an autoregressive language model developed by Google with 540B parameters. It has 118 layers, 48 heads and an input sequence length of 2048. There are also smaller versions with 8B and 62B parameters. It uses a standard autoregressive decoder with SwiGLU activation function and shared query-value projections for the heads of a layer, which improves autoregressive decoding speed. The model is trained on a high-quality dataset with 780B tokens, where sloppy and toxic language have been filtered. Each training example is used only once. The training set contains social media conversation (50%), multilingual web pages (27%), books (13%), source code files (5%), multilingual Wikipedia articles (4%), and news articles (1%). Training required 3072 TPU chips for 1368 h, resulting in a total emission that is 50% higher than the emissions for a direct round-trip flight in an aircraft between San Francisco and New York [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>, p. 18].</p><div class="Para" id="Par45">PaLM was evaluated on hundreds of natural language inference, mathematical, reasoning and knowledge intensive tasks and achieved <span class="EmphasisTypeSmallCaps ">Sota</span> accuracy in the large majority of benchmarks, e.g. in 28 of 29 most widely evaluated English language understanding benchmarks (cf. Table <span class="InternalRef"><a href="#Tab4">3.4</a></span>). This demonstrates that the scaling effects continue to hold for large Foundation Models. Figure <span class="InternalRef"><a href="#Fig3">3.3</a></span> shows the results on BIG-bench data compared to prior models. PaLM 540B 5-shot outperforms the prior <span class="EmphasisTypeSmallCaps ">Sota</span> on 44 out of the 58 common tasks, and on average is significantly better than the other models (Gopher, Chinchilla, GPT-3). Moreover, PaLM 540B 5-shot achieves a higher score than the average score of the humans asked to solve the same tasks. When fine-tuned on SuperGLUE, the model outperforms the best decoder-only model and is competitive with encoder-decoder models, which in general perform better for fine-tuning. A significant number of tasks showed discontinuous improvements from model scale, meaning that the performance improvement from the smaller version to the largest model was higher than expected.<figure class="Figure" id="Fig3"><div class="MediaObject" id="MO3"><img alt="" aria-describedby="d64e3414" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig3_HTML.png" style="width:34.38em"/><div class="TextObject" id="d64e3414"><p class="Para" id="Par303">5 line graphs. Graph 1 of normalized preferred metric versus model parameters plots the performance on 58 tasks. The other 4 graphs plot represent the percentages of negative, English proverbs, mathematic induction, and logical sequence, each versus model scale. All graphs have increasing trends.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.3</span><p class="SimplePara">Evaluation of PaLM, GPT-3, Gopher, and Chinchilla (left). Previous models were only evaluated on a subset of tasks, so this graph shows the aggregated results on the 58 tasks where all three models have been evaluated [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>]. The medium accuracy of PaLM is better than the average performance of humans. The right side shows the results for four specific BIG-tasks. A detailed comparison between the performance of three PaLM models of different size as well as human levels is presented in [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>, p. 15f]</p></div></figcaption></figure></div><p class="Para" id="Par46">PaLM has been fine-tuned on program code documents. The resulting model is called <em class="EmphasisTypeItalic ">PaLM-Coder</em><span id="ITerm45"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>, p.23]. The quality of the code is measured by the pass@<em class="EmphasisTypeItalic ">k</em> metric, in which for each problem in the test set, <em class="EmphasisTypeItalic ">k</em> samples of source code are generated by PaLM-Coder, and a problem is counted as solved if any sample solves the problem. PaLM-Coder is able to solve a number of benchmark tasks with about a pass@1-value of about 50. There is an elaborate evaluation of the properties of the PaLM-Coder model.</p><p class="Para" id="Par47">For about a quarter of tasks the authors observe a discontinuous jump in accuracy, if the model is increased from 58B to 540B parameters, far exceeding the ‘power law’ postulated by Kaplan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR102" role="doc-biblioref">102</a></span>] (Sect. <span class="InternalRef"><a href="#Sec25">3.5.1</a></span>). Examples are ‘english proverbs’ and ‘logical sequence’ shown in Fig. <span class="InternalRef"><a href="#Fig3">3.3</a></span>. This suggests that new abilities of PLMs can evolve when the model reaches a sufficient size, and that these abilities also develop beyond the model sizes studied so far.</p><p class="Para" id="Par48">The training data contains 22% multilingual documents. For translation between different languages, the few-shot PaLM model comes close to or even exceeds the fine-tuned <span class="EmphasisTypeSmallCaps ">Sota</span>. For English-French translation, Palm 540B few-shot achieves 44.0 <span class="EmphasisTypeSmallCaps ">Bleu</span> compared to a <span class="EmphasisTypeSmallCaps ">Sota</span> of 45.6. For German-English, PaLM 540B few-shot reaches 47.5 <span class="EmphasisTypeSmallCaps ">Bleu</span> vs. a 45.6 <span class="EmphasisTypeSmallCaps ">Bleu</span><span class="EmphasisTypeSmallCaps ">Sota</span>. For other tasks like summarization and question answering, Palm 540B few-shot comes close to the fine-tuned models, and can outperform them in a few cases.</p><div class="Para" id="Par49">Reasoning with a number of intermediate steps was always difficult for language models. Recently chain-of-thought prompting (Sect. <span class="InternalRef"><a href="#Sec42">3.6.4</a></span>) was proposed which adds intermediate reasoning steps [<span class="CitationRef"><a epub:type="biblioref" href="#CR226" role="doc-biblioref">226</a></span>] into the few-shot prompts (Fig. <span class="InternalRef"><a href="#Fig4">3.4</a></span>). Following this recipe, the PaLM model similarly produces its own intermediate steps for a multistep problem before giving the final answer. This leads to a boost in performance for a number of benchmark tasks. Using this technique PaLM is even able to explain jokes, as Fig. <span class="InternalRef"><a href="#Fig5">3.5</a></span> demonstrates.<figure class="Figure" id="Fig4"><div class="MediaObject" id="MO4"><img alt="" aria-describedby="d64e3490" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig4_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e3490"><p class="Para" id="Par304">2 text boxes represent a set of 2 prompts and their model outputs.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.4</span><p class="SimplePara">Few-shot example of a chain-of-thought prompt for a common sense question-answering task [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>, p. 38]. The same two example chains of thought were combined with different prompts requiring an answer</p></div></figcaption></figure><figure class="Figure" id="Fig5"><div class="MediaObject" id="MO5"><img alt="" aria-describedby="d64e3505" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig5_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e3505"><p class="Para" id="Par305">A text box represents an input prompt and its model output.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.5</span><p class="SimplePara">By using thought-chain-prompts PaLM can explain jokes [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>]</p></div></figcaption></figure></div></section>
<section class="Section2 RenderAsSection2" id="Sec4"><h3 class="Heading"><span class="HeadingNumber">3.1.3 </span>Transformer Encoder-Decoders</h3><div class="Para" id="Par50">The Transformer encoder-decoder [<span class="CitationRef"><a epub:type="biblioref" href="#CR212" role="doc-biblioref">212</a></span>] was pre-trained with a translation task (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec19"><span class="RefSource">2.​3</span></a></span>). To improve performance a number of alternatives were proposed: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par51">Different targets to restore corrupted pre-training data are proposed by MASS, BART and PEGASUS. Examples are predicting masked spans, ordering permuted sentences, or inserting omitted tokens.</p></li><li><p class="Para" id="Par52">T5 formulates many language understanding and language generation tasks as text translations and handles them with the same model.</p></li><li><p class="Para" id="Par53">Longformer, Reformer and Transformerl-XL extend the size of the input text without increasing the number of parameters. They are discussed in Sect. <span class="InternalRef"><a href="#Sec7">3.2</a></span>.</p></li></ul></div></div><div class="Para" id="Par54">The details are given in the following paragraphs. A representative list of transformer encoder-decoders is provided in Table <span class="InternalRef"><a href="#Tab5">3.5</a></span>. <div class="Table" id="Tab5"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 3.5</span><p class="SimplePara">Transformer encoder-decoders. The pre-training and fine-tuning loss functions are defined in Table <span class="InternalRef"><a href="#Tab1">3.1</a></span>. Benchmarks: En-De WMT2014 English-to-German BLEU, GLUE Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec2"><span class="RefSource">4.​1.​1</span></a></span> accuracy, SuperGLUE Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec3"><span class="RefSource">4.​1.​2</span></a></span> accuracy, TriviaQA [<span class="CitationRef"><a epub:type="biblioref" href="#CR99" role="doc-biblioref">99</a></span>] Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec10"><span class="RefSource">6.​2.​1</span></a></span> accuracy, Penn Treebank [<span class="CitationRef"><a epub:type="biblioref" href="#CR136" role="doc-biblioref">136</a></span>] perplexity. The benchmark figures are only a hint, as they depend on the number of parameters and the computing effort</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/><col class="tcol5"/><col class="tcol6"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Section</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Pre-training</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Fine-tuning</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Extra</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Benchmark</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Transformer [<span class="CitationRef"><a epub:type="biblioref" href="#CR212" role="doc-biblioref">212</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec19"><span class="RefSource">2.​3</span></a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">S2S</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">S2S</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Predict translated tokens</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">En-De 26.4</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">UniLM [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec4">3.1.3</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM, LM</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC, LM</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Uni- and bidirectional</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE 87.3</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MASS [<span class="CitationRef"><a epub:type="biblioref" href="#CR196" role="doc-biblioref">196</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec4">3.1.3</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">S2S</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">S2S</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Predict masked tokens</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">En-De 28.3</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BART [<span class="CitationRef"><a epub:type="biblioref" href="#CR119" role="doc-biblioref">119</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec4">3.1.3</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">DAE</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC, LM, S2S</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Restore corrupted text</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE 88.4</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">T5 [<span class="CitationRef"><a epub:type="biblioref" href="#CR170" role="doc-biblioref">170</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec4">3.1.3</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">S2S</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC, LM, S2S</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Solve many NLP tasks as S2S problems</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE 89.7</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLM [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec4">3.1.3</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">LM</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">LM</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Solve all task by autoregressive prediction</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">SuperGLUE 82.9</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Longformer [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec8">3.2.1</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM, S2S</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">LM, MC, S2S</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Sparse attention mechanism</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">TriviaQA 77.3</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Reformer [<span class="CitationRef"><a epub:type="biblioref" href="#CR108" role="doc-biblioref">108</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec9">3.2.2</a></span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">LM, S2S</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">LM, MC, S2S</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Locality-sensitive hashing, reversible residual layers</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">En-De 29.1</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Transformer-XL [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>]</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InternalRef"><a href="#Sec9">3.2.2</a></span></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM, S2S</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">MC, S2S</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Sparse attention mechanism</p></td><td style="text-align: left;"><p class="SimplePara">Penn-Tree Bank 54.5</p></td></tr></tbody></table></div></div><div class="Para" id="Par55"><strong class="EmphasisTypeBold ">MASS</strong><span id="ITerm46"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR196" role="doc-biblioref">196</a></span>] is based on the transformer architecture. In contrast to the original transformer, a sequence of consecutive tokens in the encoder is masked and the decoder’s task is to predict the masked tokens recursively (Fig. <span class="InternalRef"><a href="#Fig6">3.6</a></span>). Therefore, MASS can jointly train the encoder and decoder to develop the capability of extracting embeddings and language modeling. MASS is fine-tuned on language generation tasks such as neural machine translation, summarization and conversational response generation. It shows significant performance improvements compared to prior transformer architectures.<figure class="Figure" id="Fig6"><div class="MediaObject" id="MO6"><img alt="" aria-describedby="d64e3836" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig6_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e3836"><p class="Para" id="Par306">An illustration lists 6 pre-training tasks in a sentence reads, I love vanilla ice cream. John did not have any. The tasks are span masking, token masking, token deletion, text filling, sentence permutation, and document rotation from the original input.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.6</span><p class="SimplePara">Different pre-training tasks to restore corrupted text by the transformer. Span masking is the task for MASS [<span class="CitationRef"><a epub:type="biblioref" href="#CR196" role="doc-biblioref">196</a></span>]. BART uses all tasks from token masking to document rotation [<span class="CitationRef"><a epub:type="biblioref" href="#CR119" role="doc-biblioref">119</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par56"><strong class="EmphasisTypeBold ">BART</strong><span id="ITerm47"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR119" role="doc-biblioref">119</a></span>] uses a standard Transformer-based encoder-decoder architecture. The pre-training task is to recover text corrupted by a number of different approaches (Fig. <span class="InternalRef"><a href="#Fig6">3.6</a></span>): predict masked tokens as with BERT; predict deleted tokens and their positions, predict the missing tokens replaced by a single mask, reconstruct a permuted sentence as with XLNet, and find the beginning of a rotated document. BART was fine-tuned on a number of tasks like GLUE, SQuAD, summarization, and machine translation. BART achieved the best performance with the prediction of missing tokens replaced by a single mask. A large version of BART was trained with a hidden size of 1024 and 12 encoder and decoder layers with a similar dataset as used by RoBERTa. The resulting performance was similar to that of RoBERTa. For abstractive summarization, e.g. on the <em class="EmphasisTypeItalic ">CNN/Daily Mail benchmark</em><span id="ITerm48"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>], BART achieves <span class="EmphasisTypeSmallCaps ">Sota</span>.</p><p class="Para" id="Par57"><strong class="EmphasisTypeBold ">PEGASUS</strong><span id="ITerm49"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR251" role="doc-biblioref">251</a></span>] proposed pre-training large Transformer-based Seq2seq models on massive text corpora with a new objective: <em class="EmphasisTypeItalic ">gap-sentences generation</em><span id="ITerm50"/>, where sentences instead of tokens are masked or removed. The model has to generate these modified parts as a one sentence output. On 12 document summarization tasks the model achieves <span class="EmphasisTypeSmallCaps ">Sota</span> performance.</p><p class="Para" id="Par58"><strong class="EmphasisTypeBold ">T5</strong><span id="ITerm51"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR170" role="doc-biblioref">170</a></span>] is based on the standard transformer architecture. Pre-training is performed on a huge training set by restoring corrupted texts, which is formulated as a sequence-to-sequence tasks. The comparison of different pre-training tasks listed in Fig. <span class="InternalRef"><a href="#Fig6">3.6</a></span> found that, similar to BART, text infilling achieves the best results. If the original text is <em class="EmphasisTypeItalic ">“Thank you for inviting me to your party last week .”</em> the model receives the input <em class="EmphasisTypeItalic ">“Thank you [X] me to your party [Y] week .”</em> with masked phrases and has to generate the output <em class="EmphasisTypeItalic ">“[X] for inviting [Y] last [Z]”</em> to reconstruct the masked phrases.</p><p class="Para" id="Par59"><em class="EmphasisTypeItalic ">Salient span masking</em><span id="ITerm52"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR72" role="doc-biblioref">72</a></span>] was especially effective. To focus on relevant phrases a BERT-tagger was trained to recognize named entities (person names, locations, etc. Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec5"><span class="RefSource">2.​1.​3</span></a></span>), and dates were identified by regular expressions. If the model had to recreate these spans the model performance was significantly increased. By predicting the omitted tokens, the model is able to collect an enormous amount of information on syntactic and semantic knowledge. Extensive comparisons show that the sequence-to-sequence architecture yields better results than other architectures, e.g. autoregressive language models.</p><div class="Para" id="Par60">T5 is pre-trained on a multitask mixture of unsupervised and supervised tasks using a training dataset of 750 GB of cleaned English web text. Its largest version has 24 layers, 128 attention heads, and 11B parameters. For each task the data is converted into a text-to-text format (Fig. <span class="InternalRef"><a href="#Fig7">3.7</a></span>). The model achieves <span class="EmphasisTypeSmallCaps ">Sota</span> results on many benchmarks, for example summarization, question answering, text classification, and more. The results for GLUE is 90.3% [<span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>].<figure class="Figure" id="Fig7"><div class="MediaObject" id="MO7"><img alt="" aria-describedby="d64e3945" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig7_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e3945"><p class="Para" id="Par307">A block diagram represents a set of prompts translated through T 5. There are 5 instructions on the left, and their outputs on the right.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.7</span><p class="SimplePara">Every task in T5 is expressed as a translation task, where the type of the task is a prefix to the input text (on the left) and the model produces the corresponding output (right) . Adapted from [<span class="CitationRef"><a epub:type="biblioref" href="#CR170" role="doc-biblioref">170</a></span>, p.3] with kind permission of the authors</p></div></figcaption></figure></div><p class="Para" id="Par61"><strong class="EmphasisTypeBold ">Primer</strong><span id="ITerm53"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR195" role="doc-biblioref">195</a></span>] proposes two modifications of the original self-attention architecture. First the ReLU activation function is squared. In addition, a convolution layer is added after each of the multi-head projections for query <em class="EmphasisTypeItalic ">Q</em>, key <em class="EmphasisTypeItalic ">K</em>, and value <em class="EmphasisTypeItalic ">V</em> . For the original T5 architecture this reduces the training cost by a factor 4.</p><p class="Para" id="Par62"><strong class="EmphasisTypeBold ">UniLM2</strong><span id="ITerm54"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>] simultaneously pre-trains a bidirectional language models and a sequence-to-sequence model for language generation. The model parameters are shared between the two tasks, and the encoding results of the context tokens are reused. The model uses two mask types, one for bidirectional masking similar to BERT and pseudo masks for language modeling. With special self-attention masks and position embeddings, the model can perform both language modeling tasks in one forward pass without redundant computation of context. The model beats BART<sub>BASE</sub> for reading comprehension on SQuAD 1.1 and T5<sub>BASE</sub> for abstractive summarization on CNN/Daily Mail.</p><div class="Para" id="Par63"><strong class="EmphasisTypeBold ">GLM</strong><span id="ITerm55"/> (General Language Model) [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>] is a successor of UniLM2 aiming to combine the different learning paradigms of BERT, GPT and the transformer. For pre-training GLM has the task to generate multiple text spans in an autoregressive way basically using the GPT architecture. From the input text <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> = (<em class="EmphasisTypeItalic ">x</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">T</em></sub>) a number <em class="EmphasisTypeItalic ">m</em> spans <span class="InlineEquation" id="IEq20"><img alt="$$x_{i_1},\ldots , x_{i_1+l_i}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq20.png" style="width:6em"/></span> are sampled. Each span is replaced with a single <em class="EmphasisTypeItalic ">[MASK]</em> token yielding the corrupted input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub>corrupt</sub>. The model then successively generates the tokens of the spans having access to the corrupted input and the already generated tokens of the spans (Fig. <span class="InternalRef"><a href="#Fig8">3.8</a></span>). Within the input text all tokens are connected by self attention while in the output section a masked self-attention is used. Each span is finished by an <em class="EmphasisTypeItalic ">[END]</em> token. To identify the positions of generated tokens two positions are encoded by embeddings: the input position and the position within a span. Note that the mask prediction can be done in arbitrary sequence and the model has to predict the length of the spans during reconstruction.<figure class="Figure" id="Fig8"><div class="MediaObject" id="MO8"><img alt="" aria-describedby="d64e4093" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig8_HTML.png" style="width:34.38em"/><div class="TextObject" id="d64e4093"><p class="Para" id="Par308">An illustration of a set of full self-attention and masked self-attention going through the transformer with self-attention. It indicates the layers of input position, mask position, embeddings, output embeddings, and token probabilities.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.8</span><p class="SimplePara">During pre-training GLM has the task to reconstruct masked single words or multi-word phrases. The position of generated words in the text and in the masks are indicated by position embeddings, which are added to the token embeddings. The generated answers are terminated by an <em class="EmphasisTypeItalic ">[END]</em> token [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par64">For fine-tuning, text classification tasks are converted to word predictions. To assess the sentence <em class="EmphasisTypeItalic ">“The waiters were friendly.”</em> in a sentiment classification task the input is extended to <em class="EmphasisTypeItalic ">“The waiters were friendly. It’s really [MASK].”</em> where <em class="EmphasisTypeItalic ">[MASK]</em> has to be replaced by <em class="EmphasisTypeItalic ">“good”</em> or <em class="EmphasisTypeItalic ">“bad”</em>. For a text generation task a <em class="EmphasisTypeItalic ">[MASK]</em> token is appended to the input text. Then the model generates the continuation as the output text in an autoregressive way. In contrast to BERT the model observes the dependency between masked tokens yielding more consistent predictions. In comparison to XLNet no additional attention for position encoding is needed reducing the computational requirements. Compared to T5, GLM predicts the spans in arbitrary order and requires fewer extra tokens.</p><p class="Para" id="Par65">To evaluate the model performance, Du et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>] train GLM<sub>BASE</sub> and GLM<sub>LARGE</sub> with the same training data and parameter counts (110M and 340M) as BERT<sub>BASE</sub> and BERT<sub>LARGE</sub>. For both model configurations, GLM outperforms BERT on SuperGLUE (Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec3"><span class="RefSource">4.​1.​2</span></a></span>), e.g. GLM<sub>LARGE</sub> has an average score of 77.0 compared to 72.0 for BERT<sub>LARGE</sub>. On a larger pre-training dataset for a model with the same size as RoBERTa they yield an average SuperGLUE score of 82.9 compared to 81.5 for RoBERTa. They show that by multitask learning, a single model with the same parameters can simultaneously achieve higher accuracy in NLU, generating text given an input, and solve other tasks such as summarization [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>].</p><p class="Para" id="Par66">Larger models like <strong class="EmphasisTypeBold ">GLaM</strong> [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>] and <strong class="EmphasisTypeBold ">WuDao-2.0</strong> [<span class="CitationRef"><a epub:type="biblioref" href="#CR257" role="doc-biblioref">257</a></span>] have a mixture-of-experts architecture and are described in Sect. <span class="InternalRef"><a href="#Sec26">3.5.2</a></span>.</p></section>
<section class="Section2 RenderAsSection2" id="Sec5"><h3 class="Heading"><span class="HeadingNumber">3.1.4 </span>Systematic Comparison of Transformer Variants</h3><div class="Para" id="Par67">As an example of a fair comparison of architectural features, we report the following experimental analysis of PLMs, where Narang et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR142" role="doc-biblioref">142</a></span>] evaluated the effect of a number of transformer modifications. The following transformer features were investigated: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par68"><em class="EmphasisTypeItalic ">Activation functions:</em> In addition to the ReLU-activation in the feedforward layers 11 different activations functions were assessed.</p></li><li><p class="Para" id="Par69"><em class="EmphasisTypeItalic ">Normalization:</em> Together with the original layer normalization, five different regularization techniques were explored.</p></li><li><p class="Para" id="Par70"><em class="EmphasisTypeItalic ">Number of layers:</em> The number <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">L</em></sub> of layers was varied between 6 and 24. To keep the comparison fair, the number of parameters was held constant by varying the number <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">H</em></sub> of heads and the widths <em class="EmphasisTypeItalic ">d</em><sub>ff</sub> of internal embeddings.</p></li><li><p class="Para" id="Par71"><em class="EmphasisTypeItalic ">Token embeddings:</em> The original transformer embeddings were compared to five variants of factored embeddings. In addition, the sharing of transformer blocks was investigated.</p></li><li><p class="Para" id="Par72"><em class="EmphasisTypeItalic ">Softmax:</em> The standard softmax to compute token probabilities was contrasted to three softmax variants.</p></li><li><p class="Para" id="Par73"><em class="EmphasisTypeItalic ">Architecture:</em> The authors compared the base transformer with 17 other architectures. In most cases, the number of parameters was kept about the same.</p></li></ul></div></div><p class="Para" id="Par74">The authors evaluated the variants in two settings: Transfer learning based on the T5 transformer (Sect. <span class="InternalRef"><a href="#Sec4">3.1.3</a></span>) and supervised machine translation on the <em class="EmphasisTypeItalic ">WMT2014 En-De</em><span id="ITerm56"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>]. With some caution, the results can also be applied to other types of PLMs like BERT and GPT.</p><p class="Para" id="Par75">Each architecture variant of T5 was pre-trained on the <em class="EmphasisTypeItalic ">C4 dataset</em><span id="ITerm57"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR171" role="doc-biblioref">171</a></span>] of 806 GB using the “span corruption” masked language modeling objective. Subsequently, T5 was fine-tuned on three tasks: the <em class="EmphasisTypeItalic ">SuperGLUE</em><span id="ITerm58"/> language understanding task [<span class="CitationRef"><a epub:type="biblioref" href="#CR219" role="doc-biblioref">219</a></span>], the <em class="EmphasisTypeItalic ">XSum</em><span id="ITerm59"/> abstractive summarization dataset [<span class="CitationRef"><a epub:type="biblioref" href="#CR143" role="doc-biblioref">143</a></span>], and the <em class="EmphasisTypeItalic ">WebQuestions benchmark</em><span id="ITerm60"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>], where no additional knowledge was provided as background information. The computing effort and the number of parameters for each model was fixed to the same level. An exception was an architecture with significantly fewer parameters, which was trained for longer.</p><p class="Para" id="Par76">Several <em class="EmphasisTypeItalic ">activation functions</em><span id="ITerm61"/> achieve a better performance compared to the ReLU activation, especially <em class="EmphasisTypeItalic ">SwiGLU</em><span id="ITerm62"/> and <em class="EmphasisTypeItalic ">GEGLU</em><span id="ITerm63"/>, which are <em class="EmphasisTypeItalic ">gated linear units</em><span id="ITerm64"/> (GLU) forming a product with another activation [<span class="CitationRef"><a epub:type="biblioref" href="#CR189" role="doc-biblioref">189</a></span>]. The improvement can be observed for pre-training, fine-tuning, and supervised training without affecting the computation time. For SuperGLUE, for instance, an increase from 71.7% to about 76.0% can be observed. Replacing <em class="EmphasisTypeItalic ">layer normalization</em><span id="ITerm65"/> with <em class="EmphasisTypeItalic ">RMS normalization</em><span id="ITerm66"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR249" role="doc-biblioref">249</a></span>] causes performance gains for all tasks. The SuperGLUE score, for example, was improved from 71.7% to 75.5%. In addition, the training speed was higher.</p><p class="Para" id="Par77">As expected, increasing the depth of a models usually led to a better performance even if the number of parameters is kept constant. On SuperGLUE the model with 18 layers achieved a score of 76.5% compared to 71.7% for the base model. Similar improvements can be observed for WebQuestions and translation, while there were no improvements for the summarization task. This is in line with theoretical results (Sect. <span class="InternalRef"><a href="#Sec25">3.5.1</a></span>). A drawback is that deeper models require more computation time.</p><p class="Para" id="Par78">Architectures, which share parameters in different layers, usually lead to a decreased performance. The effect of using the same embeddings for encoders and decoders is mixed. Factorization of embeddings into a matrix product usually cause inferior results. If a <em class="EmphasisTypeItalic ">Mixture of Softmaxes</em><span id="ITerm67"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR239" role="doc-biblioref">239</a></span>] is used to predict the output probabilities, the performance usually is better, e.g. an increase to 76.8% for SuperGLUE. However, this approach requires up to 40% more computation effort.</p><p class="Para" id="Par79">Of the architectural variants evaluated, two combinations of the <em class="EmphasisTypeItalic ">Synthesizers</em><span id="ITerm68"/> with dot-product attention (Sect. <span class="InternalRef"><a href="#Sec9">3.2.2</a></span>) perform better than the standard Transformer. The Synthesizers do not compute a “correlation” of embeddings but determine the attention weights from a single embedding or randomly. Switch Transformer, Mixture-of-experts, and Product key memories all have significantly more parameters than the baseline transformer but are able to improve performance. The <em class="EmphasisTypeItalic ">Switch</em><span id="ITerm69"/> transformer ([<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>] Sect. <span class="InternalRef"><a href="#Sec26">3.5.2</a></span>) has many more parameters than the base T5 model. To reach the same performance as Switch, T5 needs seven times more training FLOPS<span id="ITerm70"/> (floating point operations per second). The <em class="EmphasisTypeItalic ">Mixture-of-experts</em><span id="ITerm71"/> model [<span class="CitationRef"><a epub:type="biblioref" href="#CR116" role="doc-biblioref">116</a></span>] distributes computations to 2 expert models in both the encoder and the decoder. <em class="EmphasisTypeItalic ">Product key memory</em><span id="ITerm72"/> ([<span class="CitationRef"><a epub:type="biblioref" href="#CR112" role="doc-biblioref">112</a></span>] Sect. <span class="InternalRef"><a href="#Sec2">3.1.1</a></span>) replaces the dot-product attention by a nearest neighbor search.</p><p class="Para" id="Par80">For all other 12 architectures, there were no improvements over the standard transformer [<span class="CitationRef"><a epub:type="biblioref" href="#CR142" role="doc-biblioref">142</a></span>]. This is different to the findings of the papers proposing the models. A reason seems to be that changes of the transformer architecture are difficult to transfer to other code bases and applications. Therefore, the authors propose to try out new modifications on different low-level implementations. In addition, a new approach should be evaluated on a variety of downstream applications including transfer learning, supervised learning, and language modeling. <em class="EmphasisTypeItalic ">Hyperparameter</em><span id="ITerm73"/> optimization should be kept fixed to assure the robustness of the approach. Finally, the mean and standard deviation of results should be reported to avoid the selection of a single best result.</p></section>
<section class="Section2 RenderAsSection2" id="Sec6"><h3 class="Heading"><span class="HeadingNumber">3.1.5 </span>Summary</h3><p class="Para" id="Par81">The modification of pre-training tasks has a profound influence on the performance of PLMs. Many different types of pre-training losses have been evaluated, such as masked phrase prediction, replaced token detection, or sentence order recognition. According to the benchmarks, the prediction of permuted tokens by XLNET is especially rewarding because XLNET takes into account the dependency between masked tokens. In addition, DeBERTa’s disentangled token and position embeddings are able to boost the performance in downstream classifiers. With respect to applications, autoencoders like BERT are particular important for information extraction in Chap. <span class="ExternalRef"><a href="528393_1_En_5_Chapter.xhtml"><span class="RefSource">5</span></a></span>.</p><p class="Para" id="Par82">For autoregressive PLMs like GPT, a number of variants with larger model size and larger training data have been presented. However, in most cases, the pre-training tasks were not changed. The training of the larger models required improvements in the parallel computing infrastructure and resulted in an unprecedented performance in text generation. By creating custom start texts (prompting), the models can solve a large number of specific tasks with very high accuracy without further fine-tuning (Sect. <span class="InternalRef"><a href="#Sec41">3.6.3</a></span>). The amount and quality of knowledge captured by PLMs is surprisingly high and is discussed in Chap. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml"><span class="RefSource">4</span></a></span>. In terms of applications, autoregressive PLMs are used in particular for text (Chap. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml"><span class="RefSource">6</span></a></span>) and image generation (Sect. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml#Sec12"><span class="RefSource">7.​2</span></a></span>). Because of their versatility and the tremendous increase in performance, recent large-scale PLMs are called <em class="EmphasisTypeItalic ">Foundation Models</em><span id="ITerm74"/>.</p><p class="Para" id="Par83">Encoder-decoder transformers were introduced for translating a text from one language to another. A number of new pre-training tasks were evaluated for these models. Some of them are similar to the tasks for autoencoders, such as predicting masked spans or inserting omitted tokens. Others were adapted to the input-output architecture, e.g. the reconstruction of sentence permutations and document rotations. Here BART and T5 achieved the best performances in the GLUE and SuperGLUE natural language understanding tasks. By creating additional synthetic training examples, the performance of T5 and other models can be increased (Sect. <span class="InternalRef"><a href="#Sec46">3.6.6</a></span>).</p><p class="Para" id="Par84">A systematic comparison of transformer architectures demonstrated that several architectural changes increased performance. The SwiGLU and GEGLU activation function instead of ReLU increased accuracy for SuperGLUE by more than 4%. Similar gains were observed when using RMS normalization instead of layer normalization. Increasing the model depth resulted in better performance even when the number of parameters was held constant. Synthesizers, mixtures-of-experts, and Product keys replacing scalar products by <em class="EmphasisTypeItalic ">k</em>-means clustering also performed better than the standard transformer.</p><p class="Para" id="Par85">T5 and GLM demonstrate that transformers, controlled by instructive prompts, can be used to solve arbitrary problems of text classification, text generation, and text translation. They thus combine the capabilities of BERT, GPT, and translation models. Transformers are used extensively in complex text generation tasks, e.g. machine translation (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec19"><span class="RefSource">6.​3</span></a></span>), dialog (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec49"><span class="RefSource">6.​6</span></a></span>), and image generation (Sect. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml#Sec12"><span class="RefSource">7.​2</span></a></span>).</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec7"><h2 class="Heading"><span class="HeadingNumber">3.2 </span>Capturing Longer Dependencies</h2><div class="Para" id="Par86">A well-known concern with self-attention is the quadratic time and memory complexity, which can hinder the scalability of the model in many settings (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec9"><span class="RefSource">2.​1.​6</span></a></span>). If the sequence length <em class="EmphasisTypeItalic ">T</em> is increased to 2<em class="EmphasisTypeItalic ">T</em> then four times as many associations (attentions) between tokens have to be computed. This limits the direct applicability of models when a task requires larger contexts, such as answering questions or summarizing a document. Moreover, a larger memory is required to store the attentions for training. Therefore, a number of concepts have been proposed to cover long sequences without excessive computational and memory demands. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par87">Sparse attention matrices are employed by BigBird, the Sparse Transformer, Longformer, and GPT-3 to reduce the number of parameters.</p></li><li><p class="Para" id="Par88">Clustering tokens by locality-sensitive hashing reduces the number of attentions computed by the Reformer.</p></li><li><p class="Para" id="Par89">Low-rank-approximation of attention matrices or by a kernel-based formulation of self-attention decreases the number of parameters of the Performer and the Linear Transformer.</p></li><li><p class="Para" id="Par90">Transformer-XL and the Linear Transformer reuse computations from previous text segments in an autoregressive manner to lower computational overhead.</p></li></ul></div></div><p class="Para" id="Par91">Surveys of techniques for enlarging the input sequence are provided by Tay et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR207" role="doc-biblioref">207</a></span>] and Fournier et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR59" role="doc-biblioref">59</a></span>].</p><section class="Section2 RenderAsSection2" id="Sec8"><h3 class="Heading"><span class="HeadingNumber">3.2.1 </span>Sparse Attention Matrices</h3><div class="Para" id="Par92"><strong class="EmphasisTypeBold ">BigBird</strong><span id="ITerm75"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR247" role="doc-biblioref">247</a></span>] reduces the number of attention computations by omitting entries according to some pre-determined pattern from the matrix of attention relations. BigBird extends transformer-based models, e.g. BERT, and uses a set of <em class="EmphasisTypeItalic ">g</em><em class="EmphasisTypeItalic ">global tokens</em><span id="ITerm76"/> attending on all tokens of the sequence. In addition, each token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub> attends to a set of <em class="EmphasisTypeItalic ">n</em><sub><em class="EmphasisTypeItalic ">l</em></sub> local <em class="EmphasisTypeItalic ">neighboring tokens</em> and to a set of <em class="EmphasisTypeItalic ">n</em><sub><em class="EmphasisTypeItalic ">r</em></sub><em class="EmphasisTypeItalic ">random tokens</em>. The resulting association matrices are shown in Fig. <span class="InternalRef"><a href="#Fig9">3.9</a></span>. If the numbers <em class="EmphasisTypeItalic ">g</em>, <em class="EmphasisTypeItalic ">n</em><sub><em class="EmphasisTypeItalic ">l</em></sub>, and <em class="EmphasisTypeItalic ">n</em><sub><em class="EmphasisTypeItalic ">r</em></sub> do not increase with sequence length <em class="EmphasisTypeItalic ">T</em> the number of attentions grows linearly with <em class="EmphasisTypeItalic ">T</em>.<figure class="Figure" id="Fig9"><div class="MediaObject" id="MO9"><img alt="" aria-describedby="d64e4580" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig9_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e4580"><p class="Para" id="Par309">4 illustrations exhibit different arrangements of a cluster of squared blocks. It denotes window attention, global attention, random attention, and combined attention.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.9</span><p class="SimplePara">Attention mechanism used in BigBird [<span class="CitationRef"><a epub:type="biblioref" href="#CR247" role="doc-biblioref">247</a></span>] to compute the association between input tokens. Matrix indicating attention between pairs of tokens: attentions between sequence neighbors (left), global attentions to a few tokens (second left), random attentions (third from left), the combined BigBird attentions (right). White blocks indicate omitted attention pairs</p></div></figcaption></figure></div><div class="Para" id="Par93">The model is constructed in such a way that the length of the path between arbitrary token pairs along intermediate tokens is kept small, as in a small-world graph. The authors prove that their model allows to express all continuous sequence-to-sequence functions with only <em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">T</em>) inner products (Table <span class="InternalRef"><a href="#Tab6">3.6</a></span>). In addition, they show that under standard assumptions BigBird is Turing complete, i.e. can perform arbitrary computations (see also [<span class="CitationRef"><a epub:type="biblioref" href="#CR246" role="doc-biblioref">246</a></span>]). The BigBird attention module can be used in BERT, autoregressive language models, and Transformer architectures. In a number of applications BigBird using a sequence length of 4096 is able to improve the <span class="EmphasisTypeSmallCaps ">Sota</span>, e.g. for question answering requiring multi-hop reasoning from the given evidences. Note that BigBird without random attention performed better than BigBird with random attention in a set of experiments. <div class="Table" id="Tab6"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 3.6</span><p class="SimplePara">Important models with sparse self-attention for long dependencies. <em class="EmphasisTypeItalic ">T</em> is the sequence length, <em class="EmphasisTypeItalic ">g</em> number of global tokens, <em class="EmphasisTypeItalic ">k</em> is window size. (cf. [<span class="CitationRef"><a epub:type="biblioref" href="#CR207" role="doc-biblioref">207</a></span>])</p></div></div><div class="MediaObject" id="MO10"><img alt="" aria-describedby="d64e4622" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Figaaa_HTML.png" style="width:34.6em"/><div class="TextObject" id="d64e4622"><p class="Para" id="Par310">A table represents the details of the complexity, low rank, recurrence, memory, random patterns, and learnable patterns of 10 different models.</p></div></div></div></div><p class="Para" id="Par94">Prior models using these concepts were the <em class="EmphasisTypeItalic ">Sparse Transformer</em><span id="ITerm77"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>] and the <em class="EmphasisTypeItalic ">Longformer</em><span id="ITerm78"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>], which similarly to WaveNet [<span class="CitationRef"><a epub:type="biblioref" href="#CR148" role="doc-biblioref">148</a></span>] employ strided or “dilated” neighborhoods. Here not all adjacent neighbors are attended by a token, but only every <em class="EmphasisTypeItalic ">d</em>-th neighbor with <em class="EmphasisTypeItalic ">d</em> &gt; 1. If <em class="EmphasisTypeItalic ">k</em> layers are used, this construction covers <em class="EmphasisTypeItalic ">d</em><sup><em class="EmphasisTypeItalic ">k</em></sup> neighbors and thus allows associations over large distances. The <strong class="EmphasisTypeBold ">Extended Transformer Construction</strong><span id="ITerm79"/> (ETC) model [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>] generalizes the idea of global tokens, which can communicate associations between far-away tokens of the whole sequence.</p><p class="Para" id="Par95"><strong class="EmphasisTypeBold ">GPT-3</strong><span id="ITerm80"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>] (Sect. <span class="InternalRef"><a href="#Sec3">3.1.2</a></span>) is a recent language model with 96 layers, 96 attention heads, 175 billion parameters covering sequences of length 2048. To cope with the excessive sequence length the authors used “alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer” [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>]. The details of the architecture are not yet known. The model achieved an unprecedented performance in language modeling, question answering, etc., which is discussed in Sect. <span class="InternalRef"><a href="#Sec41">3.6.3</a></span>.</p></section>
<section class="Section2 RenderAsSection2" id="Sec9"><h3 class="Heading"><span class="HeadingNumber">3.2.2 </span>Hashing and Low-Rank Approximations</h3><p class="Para" id="Par96">The <strong class="EmphasisTypeBold ">Reformer</strong><span id="ITerm81"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR108" role="doc-biblioref">108</a></span>] introduces locality-sensitive hashing to cluster tokens with similar key/query vectors. This approach hashes similar input items into the same “buckets” with high probability. For each cluster the same query/key parameters are used. In this way, tokens are aggregated in a data-driven fashion. In a similar way, the <em class="EmphasisTypeItalic ">Routing Transformer</em><span id="ITerm82"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR180" role="doc-biblioref">180</a></span>] clusters tokens by <em class="EmphasisTypeItalic ">k</em>-means clustering.</p><p class="Para" id="Par97"><strong class="EmphasisTypeBold ">Transformer-XL</strong><span id="ITerm83"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>] reuses computation results from prior segments of a sequence. With this recurrence mechanism applied to every two consecutive segments of a corpus, it essentially creates a segment-level recurrence in the hidden states. With multiple layers, the effective context being utilized can go way beyond just two segments. A similar approach is used by the <em class="EmphasisTypeItalic ">Compressive Transformer</em><span id="ITerm84"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR169" role="doc-biblioref">169</a></span>]. <em class="EmphasisTypeItalic ">Segatron</em><span id="ITerm85"/> is a variant that encodes a paragraph index in a document, a sentence index in a paragraph, and token index in a sentence as embeddings to be added to the token embedding. This modification leads to a better perplexity in language modeling.</p><p class="Para" id="Par98">The <strong class="EmphasisTypeBold ">Performer</strong><span id="ITerm86"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>] reduces the computational load by employing low rank approximations of the self-attention matrix. It uses a random kernel with positive orthogonal random features to compute the self-attention. By orthogonality, the authors avoid computing the full square matrix of products, since the dot product of orthogonal features is 0. Hence, computation requirements grow linearly with sequence length. The authors are able to prove that their model allows nearly-unbiased estimation of the full attention matrix as well as uniform convergence and lower variance of the approximation.</p><p class="Para" id="Par99">The <strong class="EmphasisTypeBold ">Linear Transformer</strong><span id="ITerm87"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR105" role="doc-biblioref">105</a></span>] also uses a kernel-based formulation of self-attention reducing complexity to linear. For predicting the future elements from past inputs, the authors are able to construct an iterative algorithm similar to RNNs that is dramatically faster than standard transformers. The model has been shown to improve inference speeds up to three orders of magnitude without much loss in predictive performance.</p><p class="Para" id="Par100">The <strong class="EmphasisTypeBold ">Transformer-LS</strong><span id="ITerm88"/> (Long-Short Transformer) [<span class="CitationRef"><a epub:type="biblioref" href="#CR258" role="doc-biblioref">258</a></span>] has a local sliding window attention between neighboring tokens and a long-range attention with dynamic projections to represent relationships between distant tokens. The dynamic low-rank projections depends on the content of the input sequence. The authors claim that the approach is more robust against insertion, deletion, paraphrasing, etc. The scheme achieves <span class="EmphasisTypeSmallCaps ">Sota</span> perplexities in language modeling for different benchmarks, e.g. 0.99 for enwik8 and <span class="EmphasisTypeSmallCaps ">Sota</span> results as vision transformer on ImageNet.</p><p class="Para" id="Par101">The <strong class="EmphasisTypeBold ">Combiner</strong><span id="ITerm89"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR174" role="doc-biblioref">174</a></span>] represents groups of embeddings by key vectors. The probability that a given token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub> attends to a token <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">s</em></sub> is described by a product, where <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">t</em></sub> first attends to the key vector that represents a group of locations containing <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">s</em></sub> multiplied by the probability of choosing <em class="EmphasisTypeItalic ">v</em><sub><em class="EmphasisTypeItalic ">s</em></sub> within that group. In this way, the Combiner can be applied to sequences of length up to 12,000. The approach is able to achieve <span class="EmphasisTypeSmallCaps ">Sota</span> perplexity on large benchmarks. In addition, it improves the average performance on the <em class="EmphasisTypeItalic ">Long Range Arena benchmark</em><span id="ITerm90"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR209" role="doc-biblioref">209</a></span>] specifically focused on evaluating model quality for long documents.</p><p class="Para" id="Par102">The <strong class="EmphasisTypeBold ">Synthesizer</strong><span id="ITerm91"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR206" role="doc-biblioref">206</a></span>] replaces the pairwise dot products of attention with “synthesizing functions” that learn attention matrices, which may or may not depend on the input tokens (cf. Sect. <span class="InternalRef"><a href="#Sec5">3.1.4</a></span>). In the Dense Synthesizer, each token embedding <em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">i</em></sub>, <em class="EmphasisTypeItalic ">i</em> = 1, …, <em class="EmphasisTypeItalic ">T</em>, in a layer is projected to a vector of the length <em class="EmphasisTypeItalic ">T</em> using a two-layered nonlinear feed-forward network with a ReLU activation. The values of this vector are used as weights to determine the mixture of values to form the output embedding. Hence, no “correlations” between embeddings are computed to determine their similarity, as it is done for the standard self-attention. There is an extreme variant, where the mixing proportions are set randomly. Nevertheless, on multiple tasks such as machine translation, language modeling, dialogue generation, masked language modeling and document classification, this “synthetic” attention demonstrates competitive performance compared to vanilla self-attention. The combination of Random Synthesizers with normal dot-product attention is able to beat T5 on several benchmarks.</p><div class="Para" id="Par103">The <strong class="EmphasisTypeBold ">Perceiver</strong><span id="ITerm92"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR93" role="doc-biblioref">93</a></span>] defines an asymmetric attention mechanism iteratively converting the long input sequence <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub>1</sub>, …, <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em class="EmphasisTypeItalic ">T</em></sub> (e.g. the 50k pixels of an image) into a shorter sequence of latent units <em><strong class="EmphasisTypeBoldItalic ">u</strong></em><sub>1</sub>, …, <em><strong class="EmphasisTypeBoldItalic ">u</strong></em><sub><em class="EmphasisTypeItalic ">n</em></sub> (e.g. <em class="EmphasisTypeItalic ">n</em> = 512) that form a bottleneck through which the inputs must pass (Fig. <span class="InternalRef"><a href="#Fig10">3.10</a></span>). With cross-attention (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec21"><span class="RefSource">2.​3.​1</span></a></span>) the <em class="EmphasisTypeItalic ">Q</em>-transformed latent sequence embeddings <em class="EmphasisTypeItalic ">Q</em><em><strong class="EmphasisTypeBoldItalic ">u</strong></em><sub><em class="EmphasisTypeItalic ">i</em></sub> and the <em class="EmphasisTypeItalic ">K</em>-transformed long input sequence embeddings <em class="EmphasisTypeItalic ">K</em><em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em class="EmphasisTypeItalic ">j</em></sub> form a scalar product <span class="InlineEquation" id="IEq21"><img alt="$$(Q\boldsymbol {u}_i)^\intercal (K{\boldsymbol {x}}_j)$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq21.png" style="width:6em"/></span>. It is used as a weight for the <em class="EmphasisTypeItalic ">V</em> -transformed long sequence embedding <em class="EmphasisTypeItalic ">V</em><em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em class="EmphasisTypeItalic ">j</em></sub> to generate the new short embeddings. The Perceiver is basically a BERT model with a sequence length of <em class="EmphasisTypeItalic ">n</em> instead of <em class="EmphasisTypeItalic ">T</em>, which avoids that the computing effort scales quadratically with the input length. The iterative approach enables the model to devote its limited capacity to the most relevant inputs. In experiments the Perceiver was able to beat the leading ResNet-50 CNN with respect to image classification [<span class="CitationRef"><a epub:type="biblioref" href="#CR93" role="doc-biblioref">93</a></span>]. <em class="EmphasisTypeItalic ">Perceiver IO</em><span id="ITerm93"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>] projects the resulting <em class="EmphasisTypeItalic ">n</em> output embeddings of a Perceiver to a larger sequence of output embeddings by another cross-attention operation, which, for instance, gets the position embeddings of output elements as query vectors. The <em class="EmphasisTypeItalic ">Perceiver AR</em><span id="ITerm94"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>] extends the Perceiver to generate an output sequentially similar to the encoder-decoder transformer.<figure class="Figure" id="Fig10"><div class="MediaObject" id="MO11"><img alt="" aria-describedby="d64e5033" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig10_HTML.png" style="width:23.75em"/><div class="TextObject" id="d64e5033"><p class="Para" id="Par311">A diagram represents the sequence of actions through the latent sequence embeddings, cross-attention, and latent transformer in an interchangeable manner. It indicates the layers of embeddings, logistic classifier, and class probabilities.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.10</span><p class="SimplePara">If the input sequence is too long, a short latent sequence is defined by the Perceiver. By cross-attention between the long sequence and the latent sequence the information is compressed. A standard transformer block computes the self-attentions between the latent sequence elements, which in the end generates a classification [<span class="CitationRef"><a epub:type="biblioref" href="#CR93" role="doc-biblioref">93</a></span>]</p></div></figcaption></figure></div><div class="Para" id="Par104"><strong class="EmphasisTypeBold ">S4</strong><span id="ITerm95"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>] is a Structured State Space Sequence model based on the Kalman filter for the observation of a state model with errors [<span class="CitationRef"><a epub:type="biblioref" href="#CR101" role="doc-biblioref">101</a></span>]. A continuous state space model is defined by <div class="Equation NumberedEquation" id="Equ1"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} {\boldsymbol{x}}'(t) = \boldsymbol{A}{\boldsymbol{x}}(t) + \boldsymbol{B} \boldsymbol{u}(t) \qquad  {\boldsymbol{y}}(t) = \boldsymbol{C} {\boldsymbol{x}}_t + \boldsymbol{D}\boldsymbol{u}(t), \end{aligned} $$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_Equ1.png" style="width:22.12em"/></div></div> <div class="EquationNumber">(3.1)</div></div></div></div><p class="Para" id="Par105">which maps an input signal <em><strong class="EmphasisTypeBoldItalic ">u</strong></em>(<em class="EmphasisTypeItalic ">t</em>) to output <em><strong class="EmphasisTypeBoldItalic ">y</strong></em>(<em class="EmphasisTypeItalic ">t</em>) through a latent state <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>(<em class="EmphasisTypeItalic ">t</em>). The authors reparametrize the matrices <em><strong class="EmphasisTypeBoldItalic ">A</strong></em> and decompose them as the sum of a low-rank and skew-symmetric term. Moreover, they compute its generating function of the associated infinite sequence truncated to some length <em class="EmphasisTypeItalic ">L</em> in frequency space. The low-rank term can be corrected by the Woodbury identity for matrix inversion. The skew-symmetric term can be diagonalized and can be reduced to a Cauchy kernel [<span class="CitationRef"><a epub:type="biblioref" href="#CR153" role="doc-biblioref">153</a></span>].</p><p class="Para" id="Par106">The <em><strong class="EmphasisTypeBoldItalic ">A</strong></em> matrix is initialized with an special upper-triangular “HIPPO” matrix that allows the state <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>(<em class="EmphasisTypeItalic ">t</em>) to memorize the history of the input <em><strong class="EmphasisTypeBoldItalic ">u</strong></em>(<em class="EmphasisTypeItalic ">t</em>). The authors prove that in complex space <span class="InlineEquation" id="IEq22"><img alt="$$\mathbb {C}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq22.png" style="width:0.81em"/></span> the corresponding state-space model can be expressed by matrices ( <strong class="EmphasisTypeBold ">Λ</strong> −<em><strong class="EmphasisTypeBoldItalic ">PQ</strong></em><sup>∗</sup>, <em><strong class="EmphasisTypeBoldItalic ">B</strong></em>, <em><strong class="EmphasisTypeBoldItalic ">C</strong></em>) for some diagonal matrix <strong class="EmphasisTypeBold ">Λ</strong> and vectors <span class="InlineEquation" id="IEq23"><img alt="$$\boldsymbol {P},\boldsymbol {Q},\boldsymbol {B},\boldsymbol {C}\in \mathbb {C}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq23.png" style="width:7.32em"/></span>. These are the 5<em class="EmphasisTypeItalic ">N</em> trainable parameters of S4, where <em class="EmphasisTypeItalic ">N</em> is the state dimension. Overall, S4 defines a sequence-to-sequence map of shape (batch size, sequence length, hidden dimension), in the same way as related sequence models such as Transformers, RNNs, and CNNs. For sequence length <em class="EmphasisTypeItalic ">L</em> this requires a computing effort of <sup>∼</sup><em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">N</em> + <em class="EmphasisTypeItalic ">L</em>) and <em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">N</em> + <em class="EmphasisTypeItalic ">L</em>) memory space, which is close to the lowest value for sequence models. Gu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR69" role="doc-biblioref">69</a></span>] provide a detailed exposition and implementation of the S4 model.</p><div class="Para" id="Par107">In empirical evaluations it turned out that S4 for an input length of 1024 is 1.6 times faster than the standard transformer and requires only 43% of its memory. For an input length of 4096, S4 is 5 times faster and requires just 9% of the memory of the standard transformer. For the benchmarks of the <em class="EmphasisTypeItalic ">Long Range Arena benchmark</em><span id="ITerm96"/> S4 increased <span class="EmphasisTypeSmallCaps ">Sota</span> average accuracy from 59.4% to 80.5% (Table <span class="InternalRef"><a href="#Tab7">3.7</a></span>). Moreover, S4 was able to solve the extremely challenging Path-X task that involves reasoning over sequences of length 16k where all previous models have failed. Finally, S4 was able to perform raw speech signal classification on sequences of length 16k and achieves a new <span class="EmphasisTypeSmallCaps ">Sota</span> of 98.3% accuracy. S4 involves a genuine breakthrough in long range sequence processing. In addition, S4 is better in long-range <em class="EmphasisTypeItalic ">time-series forecasting</em><span id="ITerm97"/>, e.g. reducing Mean Square Error by 37% when forecasting 30 days of weather data. <em class="EmphasisTypeItalic ">DSS</em><span id="ITerm98"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>] is a variant of S4 that is simpler to formulate and achieves a slightly lower performance. <div class="Table" id="Tab7"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 3.7</span><p class="SimplePara">Accuracy results for the Long-Range Arena Benchmark. The best score is printed in bold, results improving the standard transformer are underlined (cf. [<span class="CitationRef"><a epub:type="biblioref" href="#CR209" role="doc-biblioref">209</a></span>])</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/><col class="tcol5"/><col class="tcol6"/><col class="tcol7"/><col class="tcol8"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">ListOps</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Text classif.</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Retrieval</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Image classif.</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Pathfinder</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Path-X</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Average</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Transformer</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">36.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">64.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">57.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">42.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">71.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">×</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">54.4</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Reformer</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">37.3</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">56.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">53.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">38.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">68.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">×</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">50.7</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Synthesizer</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">37.0</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">61.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">54.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">41.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">69.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">×</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">52.9</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BigBird</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">36.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">64.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">59.3</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">40.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">74.9</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">×</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">55.0</span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Linear transf.</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">16.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">65.9</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">53.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">42.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">75.3</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">×</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">50.6</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Performer</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">18.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">65.4</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">53.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">42.8</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">77.0</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">×</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">51.4</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">S4</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">58.4</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">76.0</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">87.1</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">87.3</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">86.1</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">88.1</strong></p></td><td style="text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">80.5</strong></p></td></tr></tbody></table></div></div></section>
<section class="Section2 RenderAsSection2" id="Sec10"><h3 class="Heading"><span class="HeadingNumber">3.2.3 </span>Comparisons of Transformers with Long Input Sequences</h3><p class="Para" id="Par108">The <em class="EmphasisTypeItalic ">Long Range Arena</em><span id="ITerm99"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR209" role="doc-biblioref">209</a></span>] aims to evaluate the performance on tasks with long input sequences from 1k to 16k tokens. It contains six different benchmark datasets covering text, images, mathematical expressions, and visual spatial reasoning. The tasks include ListOps (computations in a list-notation), text classification (classify IMDB reviews using character sequences), document retrieval (based on document embeddings), image classification (based on a sequence of pixels), and pathfinder (detection of circles) in two versions. The authors evaluate nine transformer architectures with the ability to process long inputs.</p><p class="Para" id="Par109">The results are shown in Table <span class="InternalRef"><a href="#Tab7">3.7</a></span>. For the hierarchically structured data of ListOps, it turns out that kernel-based approaches, for instance the Performer and the Linear Transformer, are not appropriate. For text classification, kernel-based methods perform particularly well. For image classification most models do well, except for the Reformer. The pathfinder task is solved by all models with an acceptable performance, with the Performer doing best. However, all models except S4 fail on the extended Pathfinder task and are not able to find a solution. In terms of all benchmarks, S4 is the best model by a wide margin.</p><p class="Para" id="Par110">With respect to speed, the Performer was best, being 5.7 times faster than the standard transformer on sequences of length 4k. Memory consumption ranged from 9.5 GB for the standard transformer to about 1.1 GB for the Linear Transformer. All other models except the Synthesizer require less than 3 GB with S4 doing well in both aspects.</p></section>
<section class="Section2 RenderAsSection2" id="Sec11"><h3 class="Heading"><span class="HeadingNumber">3.2.4 </span>Summary</h3><p class="Para" id="Par111">There are a variety of proposals for PLMs to efficiently process long input sequences. Often a sparse attention matrix is employed, where only a part of the possible attentions is used to establish the connection between far-away positions. Usually, full attention is computed for near positions. Some tokens have a global attention to communicate information between positions not connected directly. A prominent example is BigBird, which adds random attentions. Its computational effort only grows linearly with input size and it still can perform arbitrary sequence computations. There are other architectures like the Performer and the Linear Transformer, which also exhibit linear growth.</p><p class="Para" id="Par112">Some architectures either approximate the attention matrices by low-rank factorizations or aggregate tokens, which express similar content (Reformer, Combiner). Another approach is to use a recurrence mechanism such that computations are reduced for far-away tokens (Transformer-XL, Linear Transformer, Transformer-LS, Perceiver). An alternative is the factorization of the self-attention matrix (Performer) or its replacement with simpler computations (Synthesizer). Recently, the S4 model has been proposed that applies a state-space model to long-range prediction. It uses an architecture based on complex number computations, which is completely different from the usual transformer setup. It outperforms all prior models by a large margin and is efficient in terms of computation time and memory.</p><p class="Para" id="Par113">The performance of these approaches was evaluated with six different benchmarks of the Long Range Arena. It turned out that S4 beats the other models with respect to all benchmarks. All approaches were able to reduce memory consumption compared to the standard transformer. The larger input length allow new applications, e.g. in raw speech processing, image processing or genomics [<span class="CitationRef"><a epub:type="biblioref" href="#CR247" role="doc-biblioref">247</a></span>].</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec12"><h2 class="Heading"><span class="HeadingNumber">3.3 </span>Multilingual Pre-trained Language Models</h2><p class="Para" id="Par114">There are more than 7100 languages in the world [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>], and each language can express almost all facts and concepts. Therefore, PLMs should also be able to generate consistent representations for concepts in different languages. Languages differ to some extent in the basic word order of verbs, subjects, and objects in simple declarative sentences. English, German, French, and Mandarin, for example, are SVO languages (subject-verb-object) [<span class="CitationRef"><a epub:type="biblioref" href="#CR100" role="doc-biblioref">100</a></span>]. Here, the verb is usually placed between the subject and the object. Hindi and Japanese, on the other hand, are SOV languages, meaning that the verb is placed at the end of the main clause. Irish and Arabic, on the other hand, are VSO languages. Two languages that have the same basic word order often have other similarities. For example, VO languages generally have prepositions, while OV languages generally have postpositions. Also, there may be a lexical gap in one language, where no word or phrase can express the exact meaning of a word in the other language. An example is the word <em class="EmphasisTypeItalic ">“Schadenfreude”</em> in German, which roughly translates to <em class="EmphasisTypeItalic ">“have joy because some other person has bad luck”</em>. More such differences are discussed by Jurafsky and Martin [<span class="CitationRef"><a epub:type="biblioref" href="#CR100" role="doc-biblioref">100</a></span>].</p><div class="Para" id="Par115">To gain cross-lingual language understanding, a PLM has to be trained with more than one language and has to capture their structural differences. During training, PLMs can establish an alignment between concepts in different languages. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par116">Training large PLMs models, e.g. T5 or BERT, on multilingual data with a joint token vocabulary leads to models that transfer information between languages by exploiting their common structure.</p></li><li><p class="Para" id="Par117">BERT-like models can be trained to associate the words of a sentence in one language with the words of its translation to another language by masked language modeling. However, it has been shown that multilingual processing is possible, even when little or no parallel training data is available.</p></li><li><p class="Para" id="Par118">Transformer encoder-decoder models are explicitly trained to translate a text from one language to another language.</p></li></ul></div></div><p class="Para" id="Par119">Training a language model with several languages in parallel can improve the performance—especially for languages with little training data. This could already be demonstrated for static word embeddings [<span class="CitationRef"><a epub:type="biblioref" href="#CR194" role="doc-biblioref">194</a></span>].</p><section class="Section2 RenderAsSection2" id="Sec13"><h3 class="Heading"><span class="HeadingNumber">3.3.1 </span>Autoencoder Models</h3><p class="Para" id="Par120"><strong class="EmphasisTypeBold ">mBERT</strong><span id="ITerm100"/> (multilingual BERT) [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>] is a standard BERT model. It has been pre-trained with the MLM loss on non-parallel Wikipedia texts from 104 languages and has a shared token vocabulary of 110k WordPiece tokens for all languages. This implies that Chinese is effectively character-tokenized. Each training sample is a document in one language, and there are no cross-lingual dictionaries or training criteria. To demonstrate its properties the model was fine-tuned to a multilingual version <em class="EmphasisTypeItalic ">XNLI</em><span id="ITerm101"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>] of the Natural Language Inference (NLI) benchmark, i.e. the task to predict, whether the first sentence entails the second. It turns out that mBERT may be fine-tuned with a single language on NLI and still yields good test results on related languages [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR232" role="doc-biblioref">232</a></span>].</p><div class="Para" id="Par121">The results for 6 languages [<span class="CitationRef"><a epub:type="biblioref" href="#CR111" role="doc-biblioref">111</a></span>] are shown in Table <span class="InternalRef"><a href="#Tab8">3.8</a></span>. Compared to fine-tuning XNLI with all languages, there is only a small drop in accuracy for related languages, e.g. Spanish and German, if the fine-tuning is done with XNLI in English and the evaluation in the other language. For the other languages the reduction of performance is larger, but the results are still good. There is even a transfer of information between languages with different scripts, e.g. for Arabic and Urdu. The authors also consider the embeddings of a word and its translation. It turns out that the cosine similarity between a word and its translation is 0.55, although there is no alignment between languages. <div class="Table" id="Tab8"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 3.8</span><p class="SimplePara">Cross-lingual natural language inference (XNLI) [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>] test accuracy for 6 languages. Fine-tuning with XNLI for all languages is compared to fine-tuning with XNLI only for English. Results for mBERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>] and XLM [<span class="CitationRef"><a epub:type="biblioref" href="#CR111" role="doc-biblioref">111</a></span>]</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/><col class="tcol5"/><col class="tcol6"/><col class="tcol7"/><col class="tcol8"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Fine-tune with …</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">English</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Chinese</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Spanish</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">German</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Arabic</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Urdu</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">All languages</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">mBERT</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">81.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">76.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">77.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">75.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">70.7</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">61.6</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">English only</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">mBERT</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">81.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">63.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">74.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">70.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">62.1</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">58.3</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">All languages</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">XLM</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">85.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">78.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">80.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">80.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">76.5</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">63.2</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">English only</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">XLM</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">85.0</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">76.5</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">78.9</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">77.8</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">73.1</p></td><td style="text-align: left;"><p class="SimplePara">57.3</p></td></tr></tbody></table></div></div><p class="Para" id="Par122">Karthikeyan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR104" role="doc-biblioref">104</a></span>] investigate the factors for the success of mBERT. They find that mBERT has cross-lingual capabilities even if there is absolutely no overlap in the token vocabulary. Moreover, a higher number of identical tokens in both vocabularies contributes little to the performance improvements. Comparing different language pairs the authors show that a large network depth and a high total number of parameters of a bilingual BERT are crucial for both monolingual and cross-lingual performance, whereas the number of attention heads is not a significant factor. On the other hand, the structural similarity of the source and target language, i.e. word order and frequency of words, has a large influence on cross-lingual performance.</p><div class="Para" id="Par123"><strong class="EmphasisTypeBold ">XLM</strong><span id="ITerm102"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR111" role="doc-biblioref">111</a></span>] improves the transfer of knowledge between different languages by using translated sentences from different language pairs during pre-training. The authors concatenate a sentence with its translations to another language for training and introduce a new <em class="EmphasisTypeItalic ">translation language modeling</em><span id="ITerm103"/> (<em class="EmphasisTypeItalic ">TLM</em>) objective for improving cross-lingual pre-training. To predict masked words in the input sentence, the algorithm can attend to the words in the translated sentence. In this way, the model learns to correlate words from different languages. An example is shown in Fig. <span class="InternalRef"><a href="#Fig11">3.11</a></span>. As shown in Table <span class="InternalRef"><a href="#Tab8">3.8</a></span>, XLM has a much higher cross-lingual accuracy for XNLI compared to mBERT. The transfer from a model fine-tuned in English to other languages incurs only a small loss. The experiments show that TLM is able to increase the XNLI accuracy for 3.6% on average. The model was also evaluated for unsupervised machine translation from German and other languages to English, yielding a very good performance (cf. Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec19"><span class="RefSource">6.​3</span></a></span>).<figure class="Figure" id="Fig11"><div class="MediaObject" id="MO12"><img alt="" aria-describedby="d64e5846" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig11_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e5846"><p class="Para" id="Par312">A model diagram represents the approach of translation language modeling. It indicates the layers of language, position, and token embeddings go through the transformer encoder, followed by output embeddings, logistic classifiers, token probabilities, and masked tokens.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.11</span><p class="SimplePara">The translation language modeling (TLM) task is applied to pairs of translated sentences. To predict a masked English word, the model can attend to both the English sentence and its French translation, and is thus encouraged to align English and French representations [<span class="CitationRef"><a epub:type="biblioref" href="#CR111" role="doc-biblioref">111</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par124"><strong class="EmphasisTypeBold ">Unicoder</strong><span id="ITerm104"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR88" role="doc-biblioref">88</a></span>] is an improved XLM model with three additional training tasks. Cross-lingual word alignment learns to associate the corresponding words in translated sentences. Cross-lingual paraphrase detection takes two sentences from different languages as input and classifies whether they have the same meaning. The document-level cross-lingual masked language model applies the MLM task to documents where part of the sentences are replaced by their translations. On XNLI the authors report an average accuracy improvement of 1.8%.</p><p class="Para" id="Par125"><strong class="EmphasisTypeBold ">XLM-R</strong><span id="ITerm105"/> is an optimized version of XLM [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>]. It is based on RoBERTa and trained on a huge multilingual CommonCrawl dataset of 2.5TB covering 100 languages with a common vocabulary of 250k tokens. It increased the <span class="EmphasisTypeSmallCaps ">Sota</span> on the XNLI-score to 79.2%. For cross-lingual question answering, models are fine-tuned on the English SQuAD dataset and evaluated on 7 other languages. XLM-R improves the F1 score on this SQuAD version by 9.1%–70.7%. It outperforms mBERT on cross-lingual classification by up to 23% accuracy on low-resource languages. The performance of XLM-R is nearly as good as that of strong monolingual models.</p><p class="Para" id="Par126">These results support the observation that the performance of PLMs can be improved by training on large volumes of text [<span class="CitationRef"><a epub:type="biblioref" href="#CR102" role="doc-biblioref">102</a></span>]. More languages lead to better cross-lingual performance on low-resource languages under the condition that the model capacity is large enough. Combined with the approach of Aghajanyan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>], which avoids too large changes in representation during fine-tuning (Sect. <span class="InternalRef"><a href="#Sec31">3.6</a></span>), the XLM-R<sub>LARGE</sub> model increases the <span class="EmphasisTypeSmallCaps ">Sota</span> in XNLI to 81.4%. If an additional criterion of separating semantically-equivalent sentences in different languages from other sentences is added to XLM-R, the accuracy on semantic tasks is increased [<span class="CitationRef"><a epub:type="biblioref" href="#CR228" role="doc-biblioref">228</a></span>]. Even larger models like <em class="EmphasisTypeItalic ">XLM-R</em><sub>XXL</sub><span id="ITerm106"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>] with 10.7B parameters were pre-trained on CC-100, which consists of 167B tokens of non-parallel text also covering low-resource languages, and increased the XNLI performance by 2.4%.</p><p class="Para" id="Par127"><strong class="EmphasisTypeBold ">RemBERT</strong><span id="ITerm107"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR37" role="doc-biblioref">37</a></span>] redistributes the parameters of multilingual models. First the authors showed that using different input and output embeddings in state-of-the-art pre-trained language models improved model performance. Then they demonstrated that assigning more parameters to the output embeddings increased model accuracy, which was maintained during fine-tuning. As a consequence Transformer representations were more general and more transferable to other tasks and languages. The <em class="EmphasisTypeItalic ">Xtreme</em><span id="ITerm108"/> collection [<span class="CitationRef"><a epub:type="biblioref" href="#CR86" role="doc-biblioref">86</a></span>] is a multitask benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. RemBERT outperformed XLM-R on Xtreme, despite being trained only on a smaller subset of training data and ten additional languages.</p><p class="Para" id="Par128">PLMs like BERT generate contextual token embeddings. However, the user often needs contextual <em class="EmphasisTypeItalic ">embeddings for passage</em><span id="ITerm109"/> or sentences to compare their content. <strong class="EmphasisTypeBold ">LaBSE</strong><span id="ITerm110"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>] is a language-agnostic generator of passage embeddings, where source and target sentences are encoded separately using a shared BERT-based encoder. The representations of <em class="EmphasisTypeItalic ">[CLS]</em> in the final layer were taken as the <em class="EmphasisTypeItalic ">sentence embeddings</em><span id="ITerm111"/> for each input. LaBSE combined a masked language model (MLM) and a translation language model (TLM) loss with a margin criterion. This criterion computes the cosine distance <span class="InlineEquation" id="IEq24"><img alt="$$\cos {}(x,y)$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq24.png" style="width:4em"/></span> between the passage embeddings <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> and the embedding <em><strong class="EmphasisTypeBoldItalic ">y</strong></em> of its correct translation. Then it is required that <em class="EmphasisTypeItalic ">cos</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>, <em><strong class="EmphasisTypeBoldItalic ">y</strong></em>) − <em class="EmphasisTypeItalic ">m</em> is larger than <span class="InlineEquation" id="IEq25"><img alt="$$\cos {}({\boldsymbol {x}},{\boldsymbol {y}}_i)$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq25.png" style="width:4.5em"/></span>, where <em class="EmphasisTypeItalic ">m</em> is a positive margin and the <em><strong class="EmphasisTypeBoldItalic ">y</strong></em><sub><em class="EmphasisTypeItalic ">i</em></sub> are embeddings of arbitrary other passages. LaBSE was trained using 17B monolingual sentences and 6B bilingual translated sentences. The resulting sentence embeddings markedly improve the retrieval accuracy <span class="EmphasisTypeSmallCaps ">Sota</span> of sentences in cross-lingual information retrieval (cf. Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec1"><span class="RefSource">6.​1</span></a></span>). The code and pre-trained models are available.</p></section>
<section class="Section2 RenderAsSection2" id="Sec14"><h3 class="Heading"><span class="HeadingNumber">3.3.2 </span>Seq2seq Transformer Models</h3><p class="Para" id="Par129"><strong class="EmphasisTypeBold ">mT5</strong><span id="ITerm112"/> is a multilingual version of the T5 Seq2seq transformer (Sect. <span class="InternalRef"><a href="#Sec4">3.1.3</a></span>) with up to 13B parameters [<span class="CitationRef"><a epub:type="biblioref" href="#CR236" role="doc-biblioref">236</a></span>]. It was pre-trained using a training dataset of web pages covering 101 languages with about 48B tokens and a common vocabulary of 250k tokens. For pre-training, the model had to predict masked phrases in monolingual documents in the same way as T5. Similar to T5 the model may be instructed to perform different tasks by a prefix, e.g. “summarize”. These tasks were trained by fine-tuning on the corresponding datasets.</p><p class="Para" id="Par130">For the <em class="EmphasisTypeItalic ">XNLI benchmark</em><span id="ITerm113"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>] the model has to decide, if the first sentence entails the second sentence. When the model is fine-tuned on XNLI with English data and performance is measured for 15 languages, accuracy is 84.8% compared to 65.4% for mBERT, 69.1% for XLM, and 79.2% for XLM-R. Although the texts in the different languages are not parallel, the model is able to exploit structural similarities between languages to solve the task. The code of this model is available at [<span class="CitationRef"><a epub:type="biblioref" href="#CR235" role="doc-biblioref">235</a></span>]. Similar models are used for multilingual translation (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec19"><span class="RefSource">6.​3</span></a></span>). <strong class="EmphasisTypeBold ">mT6</strong><span id="ITerm114"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>] enhances the training of mT5 with pairs of translated sentences and defines new training tasks. Experimental results show that mT6 has improved cross-lingual capabilities compared to mT5. A further improvement is <strong class="EmphasisTypeBold ">Switch</strong><span id="ITerm115"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>] with a <em class="EmphasisTypeItalic ">mixture-of-experts</em><span id="ITerm116"/> (<em class="EmphasisTypeItalic ">MoE</em>) architecture of mT5 requiring only one fifth of the training time of mT5 while yielding a performance gain across all 101 languages (Sect. <span class="InternalRef"><a href="#Sec26">3.5.2</a></span>).</p><p class="Para" id="Par131"><strong class="EmphasisTypeBold ">mBART</strong><span id="ITerm117"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR126" role="doc-biblioref">126</a></span>] is a multilingual encoder-decoder based on the BART model (Sect. <span class="InternalRef"><a href="#Sec4">3.1.3</a></span>). The input texts are corrupted by masking phrases and permuting sentences, and a single Transformer model is pre-trained to recover the corrupted text. This is performed for the training documents covering 25 languages. Subsequently, the pre-trained model is fine-tuned with a translation task between a single language pair. In addition, <em class="EmphasisTypeItalic ">back-translation</em><span id="ITerm118"/> may be used, where another model is trained to translate the target sentence back to the source language and an additional loss encourages to reconstruct the source sentence. mBART adds a language symbol both to the end of the encoder input and the beginning of the decoder input. This enables models to know the languages to be encoded and generated. It turns out that pre-training improves translation, especially for languages with little parallel training data. In addition, back-translation markedly ameliorates the translation results. Many experiments are performed to analyze the effect of different algorithmic features. Pre-training is especially important if complete documents are translated instead of single sentences.</p><p class="Para" id="Par132">mBART may also be used for <em class="EmphasisTypeItalic ">unsupervised machine translation</em><span id="ITerm119"/><span id="ITerm120"/>, where no parallel text of any kind is used. Here the authors initialize the model with pre-trained weights and then learn to predict the monolingual sentences from the source sentences generated by back-translation. The results for languages with similar structure are very good, e.g. for En-De mBART achieves a <span class="EmphasisTypeSmallCaps ">Bleu</span>-value of 29.8, which is close to the supervised value of 30.9. Note that mBART has a similar performance as MASS (Sect. <span class="InternalRef"><a href="#Sec4">3.1.3</a></span>). For dissimilar pairs of languages, e.g. English-Nepali, mBART has reasonable results where other approaches fail.</p><p class="Para" id="Par133"><strong class="EmphasisTypeBold ">MARGE</strong><span id="ITerm121"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR118" role="doc-biblioref">118</a></span>] is a multilingual Seq2seq model that is trained to reconstruct a document <em class="EmphasisTypeItalic ">x</em> in one language by retrieving documents <em class="EmphasisTypeItalic ">z</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">z</em><sub><em class="EmphasisTypeItalic ">k</em></sub> in other languages. It was trained with texts in 26 languages from Wikipedia and CC-News. A document was encoded by the output embedding of the first token of a Transformer [<span class="CitationRef"><a epub:type="biblioref" href="#CR212" role="doc-biblioref">212</a></span>]. A retrieval model scores the relevance <em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em>, <em class="EmphasisTypeItalic ">z</em><sub><em class="EmphasisTypeItalic ">j</em></sub>) of the target document <em class="EmphasisTypeItalic ">x</em> to each evidence document <em class="EmphasisTypeItalic ">z</em><sub><em class="EmphasisTypeItalic ">j</em></sub> by embedding each document and computing their cosine similarities. A transformer receives the embedded texts of <em class="EmphasisTypeItalic ">z</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">z</em><sub><em class="EmphasisTypeItalic ">k</em></sub> and auxiliary relevance scores <em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em>, <em class="EmphasisTypeItalic ">z</em><sub><em class="EmphasisTypeItalic ">j</em></sub>) from retrieval as input and is trained to generate the target document <em class="EmphasisTypeItalic ">x</em> as output. The similarity score is used to weight the cross-attention from the decoder to the encoder, so that the decoder will pay more attention to more relevant evidence documents. The models jointly learn to do retrieval and reconstruction, given only a random initialization. In a zero-shot setting the model can do document translation with <span class="EmphasisTypeSmallCaps ">Bleu</span> scores of up to 35.8 in the <em class="EmphasisTypeItalic ">WMT2019 De-En benchmark</em><span id="ITerm122"/>, as well as abstractive summarization, question answering and paraphrasing. Fine-tuning gives additional strong performance on a range of tasks in many languages, showing that MARGE is a generally applicable pre-training method.</p><p class="Para" id="Par134"><strong class="EmphasisTypeBold ">XLNG</strong><span id="ITerm123"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>] pre-trains the same Seq2seq model simultaneously using an MLM and a translation TLM loss (Table <span class="InternalRef"><a href="#Tab1">3.1</a></span>). The pre-training objective generates embeddings for different languages in a common space, enabling zero-shot cross-lingual transfer. In the fine-tuning stage monolingual data is used to train the pre-trained model on natural language generation tasks. In this way, the model trained in a single language can directly solve the corresponding task in other languages. The model outperforms methods based on machine translation for zero-shot cross-lingual question generation and abstractive summarization. In addition, this approach improves performance for languages with little training data by leveraging data from resource-rich languages.</p></section>
<section class="Section2 RenderAsSection2" id="Sec15"><h3 class="Heading"><span class="HeadingNumber">3.3.3 </span>Autoregressive Language Models</h3><p class="Para" id="Par135">Generative models like GPT-3 are trained on huge collections of documents which usually contain texts from different languages. By this training data, the model also acquires the knowledge about these languages and generates joint contextual representations of meanings. As described in Sect. <span class="InternalRef"><a href="#Sec41">3.6.3</a></span>, it is able to translate between languages if given an appropriate prompt and some examples (few-shot learning). On WMT2016 En→De, for instance, GPT-3 achieves a few-shot <span class="EmphasisTypeSmallCaps ">Bleu</span> of 29.7 compared to a supervised <span class="EmphasisTypeSmallCaps ">Sota</span> of 41.2, whereas in the De→En direction GPT-3 outperforms the current <span class="EmphasisTypeSmallCaps ">Sota</span> of 40.2 <span class="EmphasisTypeSmallCaps ">Bleu</span> with 40.6 <span class="EmphasisTypeSmallCaps ">Bleu</span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>].</p><p class="Para" id="Par136">Winata et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR231" role="doc-biblioref">231</a></span>] evaluate in detail the multilingual capabilities of GPT-2, GPT<sub>NEO</sub> and T5 with 1.6B, 6B, and 3B parameters respectively. The models are able to use the context from English to predict the answer in non-English languages. The authors find that the largest model GPT<sub>NEO</sub> always performs best on a set of multilingual benchmarks. The performance depends on the language pair. The models, for instance, achieve higher performance for En→Es than for the other two target languages (De and Fr). For the <em class="EmphasisTypeItalic ">MultiNLU benchmark</em><span id="ITerm124"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR187" role="doc-biblioref">187</a></span>] the error 12.1% of the <span class="EmphasisTypeSmallCaps ">Sota</span> model fully trained on the target language is not much lower than the error of 17.3% for few-shot prompts of GPT<sub>NEO</sub>.</p></section>
<section class="Section2 RenderAsSection2" id="Sec16"><h3 class="Heading"><span class="HeadingNumber">3.3.4 </span>Summary</h3><p class="Para" id="Par137">Machine translation is one of the most widely used applications of NLP. Languages have both structural and lexical differences that make translation difficult. The joint processing of multiple languages must take these differences into account.</p><p class="Para" id="Par138">When BERT is trained with documents from multiple languages, it is able to transfer knowledge between languages, e.g. solve language inference tasks, even if it has no access to parallel texts. Knowledge transfer is improved in XLM by using the translation language modeling loss, such that translated sentences are employed to reconstruct masked tokens. There are a number of improved versions of XLM that are able to increase the accuracy of cross-language inference.</p><p class="Para" id="Par139">Encoder-decoder models such as T5 can be generalized to multiple languages and induce powerful multilingual embeddings. mT5 can be controlled by a prefix and solves various task like translation, summarization, and language inference. mT6 and Switch are more effective variants of mT5. mBART is pre-trained by recovering corrupted text in different languages. It can even be used for unsupervised machine translation. XNLG generates joint embeddings in a multilingual space and MARGE leverages retrieval of background documents to reconstruct a target document. Both models are able to perform multiple tasks such as abstractive summarization, question answering, and paraphrasing. Note, however that specialized models are used for translating single language pairs (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec20"><span class="RefSource">6.​3.​1</span></a></span>).</p><p class="Para" id="Par140">Autoregressive language models such as GPT-3 are trained on huge corpora, which also contain multilingual documents. Therefore, these models can also be instructed by few-shot learning to perform multilingual tasks such as translations or question answering. However, performance is usually not as good as for dedicated, fine-tuned models.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec17"><h2 class="Heading"><span class="HeadingNumber">3.4 </span>Additional Knowledge for Pre-trained Language Models</h2><p class="Para" id="Par141">During unsupervised pre-training, PLMs like BERT and GPT2 are forced to predict missing words from the context. They are optimized to predict either the next word in a sequence or some masked words (e.g. <em class="EmphasisTypeItalic ">“Einstein was [MASK] in the city of Ulm.”</em>). Trained on this task, they obviously gather knowledge about real-world facts and relations from the training data. PLMs do surprisingly well in reproducing facts and relations based on unsupervised training. In Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec7"><span class="RefSource">4.​2</span></a></span> we discuss, what knowledge is covered by standard PLMs. It turns out, however that due to the still limited number of parameters only a fraction of knowledge contained in the training data can be remembered by a PLM. In addition, events that occurred after the training are missed.</p><div class="Para" id="Par142">This section presents methods for extending factual knowledge in PLMs, either during training or on the fly during actual model usage Fig. <span class="InternalRef"><a href="#Fig12">3.12</a></span>. A <em class="EmphasisTypeItalic ">Knowledge Base</em><span id="ITerm125"/> (<em class="EmphasisTypeItalic ">KB</em>) describes knowledge about the world, e.g. by entities and their relations. We outline a number of different approaches with which information in KBs or other knowledge sources such as text collections can be incorporated into PLMs (Table <span class="InternalRef"><a href="#Tab9">3.9</a></span>): <div class="DefinitionList"><dl><dt class="Term">Knowledge Base Embeddings:</dt><dd class="Description"><div class="Para" id="Par143">There are techniques to represent the entities and relations in a KB by embeddings. A number of approaches try to combine these embeddings with the token embeddings created by a PLM. In this way, the information in the KB can be injected into the PLM and used for downstream tasks.<figure class="Figure" id="Fig12"><div class="MediaObject" id="MO13"><img alt="" aria-describedby="d64e6355" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig12_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e6355"><p class="Para" id="Par313">An illustration represents the input and output through the pre-trained language model that interacts with the knowledge requirements of the knowledge graph, knowledge base, table, and text.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.12</span><p class="SimplePara">A PLM gets an input text and collects additional knowledge from different sources. This knowledge may be added beforehand or can be retrieved on demand. Subsequently, an output is generated using the additional knowledge</p></div></figcaption></figure><div class="Table" id="Tab9"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 3.9</span><p class="SimplePara">Models integrating additional knowledge (cf. [<span class="CitationRef"><a epub:type="biblioref" href="#CR166" role="doc-biblioref">166</a></span>, p. 10]). Benchmarks: GLUE natural language understanding Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec2"><span class="RefSource">4.​1.​1</span></a></span>, TACRED relation extraction Sect. <span class="ExternalRef"><a href="528393_1_En_5_Chapter.xhtml#Sec22"><span class="RefSource">5.​4.​2</span></a></span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR199" role="doc-biblioref">199</a></span>], TriviaQA question answering Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec10"><span class="RefSource">6.​2.​1</span></a></span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR99" role="doc-biblioref">99</a></span>], English all word WSD [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>], Nat. Quest question answering [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>] Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec3"><span class="RefSource">6.​1.​2</span></a></span></p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/><col class="tcol5"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Train task</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Fine-tuning</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Extra</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Benchmark</p></th></tr></thead><tbody><tr><td colspan="5" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Using knowledge base embeddings in pre-trained language models</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">ERNIE(THU) [<span class="CitationRef"><a epub:type="biblioref" href="#CR255" role="doc-biblioref">255</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM+NSP + masked NEs</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE, etc.</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">KB NE embeddings combined with token embeddings</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE 79.6</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">KnowBERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR157" role="doc-biblioref">157</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM+NSP +EL</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE, etc</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Translate token embeddings ↔ KB NE embeddings</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"> </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">KEPLER [<span class="CitationRef"><a epub:type="biblioref" href="#CR224" role="doc-biblioref">224</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM+KE</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE, etc</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Combine token embeddings with NE embeddings; use TransE loss</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">TACRED 71.5 F1</p></td></tr><tr><td colspan="5" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Using textual information from knowledge bases</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">K-Adapter [<span class="CitationRef"><a epub:type="biblioref" href="#CR222" role="doc-biblioref">222</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM + rel. extr.</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">–</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Add parallel adapter network to RoBERTa</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">TACRED 72.0 F1</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">WKLM [<span class="CitationRef"><a epub:type="biblioref" href="#CR234" role="doc-biblioref">234</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM+ERD</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">–</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Detect replaced NEs in text</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">TriviaQA 63.1 F1</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">CoLAKE [<span class="CitationRef"><a epub:type="biblioref" href="#CR202" role="doc-biblioref">202</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">–</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Create graph from textual relation triples and tokens</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GLUE 86.3</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">LUKE [<span class="CitationRef"><a epub:type="biblioref" href="#CR234" role="doc-biblioref">234</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM+ERD</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">–</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Masked language modeling for text and contained entities</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">TACRED 72.7% F1</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">EWISER [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Word sense classification</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Include wordnet supersense graph</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">English all word WSD 80.1% F1</p></td></tr><tr><td colspan="5" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Using text passages retrieved from text collections</em></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">FiD [<span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MLM, S2S</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">QA</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Encode query and KB by BERT; combine query and retrieved docs with Seq2seq</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Nat. Quest. 51.4% acc.</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Retro [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>]</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">LM</p></td><td style="border-right: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Language generation with periodical retrieval</p></td><td style="text-align: left;"><p class="SimplePara">Nat. Quest. 45.5% acc.</p></td></tr></tbody></table></div></div></dd><dt class="Term">Textual Encoding of Tables:</dt><dd class="Description"><p class="Para" id="Par144">Often additional knowledge is available in tables. The entries in these tables can be encoded in a special text format. A PLM can be trained with this text to acquire the knowledge in the rows and columns, in a similar way as the relation between the words of two languages can be learned.</p></dd><dt class="Term">Textual Encoding of KB Relations:</dt><dd class="Description"><p class="Para" id="Par145">An alternative way to use KB information starts with identifying entities or concepts in a text. The relations available for these entities and concepts can be extracted from the KB and can be included in the training process either as text or in another appropriate form.</p></dd><dt class="Term">Adding Retrieved Facts:</dt><dd class="Description"><p class="Para" id="Par146">When a PLM needs to answer a question or create a text, it can formulate a query on the topic and retrieve corresponding text content from a KB or the Internet. This textual information may be picked up by a transformer and enhance the output. In this way, the model can use comprehensive and up-to-date information on the fly.</p></dd><dt class="Term">Enhancing Logical Consistency:</dt><dd class="Description"><p class="Para" id="Par147">PLMs sometimes do not generate logically consistent content. By additional fine-tuning tasks a model can be trained to respect logical consistency.</p></dd></dl></div> Surveys of methods to incorporate domain knowledge into Deep Neural Networks are given by Dash et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>] and Yu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR243" role="doc-biblioref">243</a></span>].</div><section class="Section2 RenderAsSection2" id="Sec18"><h3 class="Heading"><span class="HeadingNumber">3.4.1 </span>Exploiting Knowledge Base Embeddings</h3><div class="Para" id="Par148">Typically, <em class="EmphasisTypeItalic ">Knowledge Bases</em><span id="ITerm126"/> are graph structures where the nodes correspond to entities and the edges represent <em class="EmphasisTypeItalic ">relations</em><span id="ITerm127"/> connecting the entities. Many large-scale KBs, such as <em class="EmphasisTypeItalic ">WordNet</em><span id="ITerm128"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR137" role="doc-biblioref">137</a></span>], <em class="EmphasisTypeItalic ">YAGO</em><span id="ITerm129"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR200" role="doc-biblioref">200</a></span>], <em class="EmphasisTypeItalic ">Freebase</em><span id="ITerm130"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>], <em class="EmphasisTypeItalic ">DBpedia</em><span id="ITerm131"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR15" role="doc-biblioref">15</a></span>], and <em class="EmphasisTypeItalic ">DiffBot</em><span id="ITerm132"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>] have been released in recent years with millions of entities. Figure <span class="InternalRef"><a href="#Fig13">3.13</a></span> shows a small subset of the WordNet hierarchy. In most cases a KB can be described by triples (<em class="EmphasisTypeItalic ">h</em>, <em class="EmphasisTypeItalic ">r</em>, <em class="EmphasisTypeItalic ">t</em>), where <em class="EmphasisTypeItalic ">h</em> and <em class="EmphasisTypeItalic ">t</em> are entities in a set <em class="EmphasisTypeItalic ">E</em>, and <em class="EmphasisTypeItalic ">r</em> is a relation holding between these entities. To assess the semantic contents of a KB, it was proposed to encode its entities as well as its relations as embeddings in a low-dimensional space, allowing to determine the similarity of entities and relations [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>]. Subsequently, these embeddings can be used to disambiguate entities (entity linking, Sect. <span class="ExternalRef"><a href="528393_1_En_5_Chapter.xhtml#Sec16"><span class="RefSource">5.​3.​3</span></a></span>), or predict new relations (Sect. <span class="ExternalRef"><a href="528393_1_En_5_Chapter.xhtml#Sec19"><span class="RefSource">5.​4</span></a></span>).<figure class="Figure" id="Fig13"><div class="MediaObject" id="MO14"><img alt="" aria-describedby="d64e6773" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig13_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e6773"><p class="Para" id="Par314">A model diagram explains that instrumentality, conveyance, and vehicles have nearly the same meaning. It indicates that motorized vehicles have engines and other parts, whereas trains and cars are the members of vehicles.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.13</span><p class="SimplePara">Small part of the WordNet knowledge base describing the relations between English words. It contains synsets of word with approximately the same meaning, which are related by the hypernym (is-a) meronym (has-part) and member-of relations [<span class="CitationRef"><a epub:type="biblioref" href="#CR137" role="doc-biblioref">137</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par149">For the embeddings <span class="InlineEquation" id="IEq26"><img alt="" aria-describedby="d64e6781" src="../images/528393_1_En_3_Chapter/528393_1_En_3_IEq26_HTML.gif" style="width:4.31em"/><span class="TextObject" id="d64e6781"><span class="Para" id="Par150">e m b left parenthesis word right parenthesis</span></span></span>) of words generated by Word2Vec [<span class="CitationRef"><a epub:type="biblioref" href="#CR135" role="doc-biblioref">135</a></span>] it turned out that relations between entities often are represented in the space of word embeddings as vector differences between entity embeddings (Sect. <span class="ExternalRef"><a href="528393_1_En_1_Chapter.xhtml#Sec5"><span class="RefSource">1.​5</span></a></span>). An example is the relation between a country and its capital, for which we have approximately <span class="InlineEquation" id="IEq27"><img alt="" aria-describedby="d64e6796" src="../images/528393_1_En_3_Chapter/528393_1_En_3_IEq27_HTML.gif" style="width:23.88em"/><span class="TextObject" id="d64e6796"><span class="Para" id="Par151">e m b left parenthesis Germany right parenthesis minus e m b left parenthesis Berlin right parenthesis almost equals e m b left parenthesis France right parenthesis minus e m b left parenthesis Paris right parenthesis</span></span></span>) .</p><p class="Para" id="Par152">The <strong class="EmphasisTypeBold ">TransE</strong><span id="ITerm133"/> model [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>] is built on this pattern. TransE adapts the embeddings in such a way that whenever (<em class="EmphasisTypeItalic ">h</em>, <em class="EmphasisTypeItalic ">r</em>, <em class="EmphasisTypeItalic ">t</em>) holds and <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">h</em>) and <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">t</em>) are the embeddings of <em class="EmphasisTypeItalic ">h</em> and <em class="EmphasisTypeItalic ">t</em>, then equation <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">h</em>) + <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">r</em>) ≈ <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">t</em>) should be approximately valid for some vector <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">r</em>), which is considered as the embedding of the relation <em class="EmphasisTypeItalic ">r</em>. Consequently, for all triples (<em class="EmphasisTypeItalic ">h</em>, <em class="EmphasisTypeItalic ">r</em>, <em class="EmphasisTypeItalic ">t</em>) in the set <em class="EmphasisTypeItalic ">S</em> of correct triples the <em class="EmphasisTypeItalic ">TransE-loss</em><span id="ITerm134"/><span class="InlineEquation" id="IEq28"><img alt="$$f_r(h,t)=\left \lVert {emb}(h)+{emb}(r)-{emb}(t)\right \rVert ^2_2$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq28.png" style="width:18em"/></span> should become 0. The TransE-model uses the hinge loss to approximate this goal, which modifies the embeddings in such a way that <em class="EmphasisTypeItalic ">f</em><sub><em class="EmphasisTypeItalic ">r</em></sub>(<em class="EmphasisTypeItalic ">h</em>, <em class="EmphasisTypeItalic ">t</em>) for correct relation triples gets lower than <span class="InlineEquation" id="IEq29"><img alt="$$f_r(\tilde {h},\tilde {t})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq29.png" style="width:3.32em"/></span> for randomly selected incorrect triples <span class="InlineEquation" id="IEq30"><img alt="$$(\tilde {h},r,\tilde {t})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq30.png" style="width:3.26em"/></span>. The models and embeddings are trained with relations from WordNet and Freebase.</p><p class="Para" id="Par153">There are a number of more elaborate models to encode relations from KBs, as described in the surveys [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR94" role="doc-biblioref">94</a></span>]. <em class="EmphasisTypeItalic ">TransH</em><span id="ITerm135"/> overcomes TransE’s inability to model complex relations, and <em class="EmphasisTypeItalic ">TransD</em><span id="ITerm136"/> aims to reduce the parameters by proposing two different mapping matrices for head and tail. But these alternatives are rarely used for contextual embeddings. Another method for KB representation is tensor factorization [<span class="CitationRef"><a epub:type="biblioref" href="#CR144" role="doc-biblioref">144</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR145" role="doc-biblioref">145</a></span>]. This approach, however, is not based on word embeddings and therefore mainly used for KB completion and not to enhance PLMs.</p><p class="Para" id="Par154">In the rest of the section we describe approaches, which merge KB-embeddings usually computed by TransE and token embeddings generated by language models. A difficulty is to establish a relation between the token embeddings and the entities, which usually contain several tokens.</p><div class="Para" id="Par155"><strong class="EmphasisTypeBold ">KEPLER</strong><span id="ITerm137"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR224" role="doc-biblioref">224</a></span>] consists of a BERT-like language model generating token embeddings by the MLM objective. In addition, it computes embeddings for entities from descriptive text in the KB using a special token “&lt;<em class="EmphasisTypeItalic ">S</em>&gt;” at the beginning of the input text. This token is trained to produce an embedding of the named entity argument of the relation, e.g. for the input “&lt;<em class="EmphasisTypeItalic ">S</em>&gt; <em class="EmphasisTypeItalic ">Johannes Kepler”</em> in Fig. <span class="InternalRef"><a href="#Fig14">3.14</a></span>. In this way, the arguments <em class="EmphasisTypeItalic ">h</em> and <em class="EmphasisTypeItalic ">t</em> of the relation are embedded. The embedding of the relation <em class="EmphasisTypeItalic ">r</em> is either a parameter to be trained, or it may be determined by the text verbalizing the relation. These embeddings are fed into the TransE loss and used as an extra training criterion in addition to MLM (Fig. <span class="InternalRef"><a href="#Fig14">3.14</a></span>). In a number of language understanding tasks the approach is able to achieve good results. On the relation extraction benchmark <em class="EmphasisTypeItalic ">TACRED</em><span id="ITerm138"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR254" role="doc-biblioref">254</a></span>] the approach reaches 71.5% F1-value.<figure class="Figure" id="Fig14"><div class="MediaObject" id="MO15"><img alt="" aria-describedby="d64e7129" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig14_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e7129"><p class="Para" id="Par315">A model diagram represents the input text that goes through the encoder and M L M loss. It also indicates the flow of the knowledge graph through the encoders and embeddings along with the K E loss.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.14</span><p class="SimplePara">KEPLER [<span class="CitationRef"><a epub:type="biblioref" href="#CR224" role="doc-biblioref">224</a></span>] trains a conventional BERT-like model by the MLM-loss. For a knowledge base with text entries it generates entity embeddings using the special &lt;<em class="EmphasisTypeItalic ">S</em>&gt; token and encodes relations by the TransE-loss. Both loss functions are added during training</p></div></figcaption></figure></div><p class="Para" id="Par156"><strong class="EmphasisTypeBold ">KnowBERT</strong><span id="ITerm139"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR157" role="doc-biblioref">157</a></span>] explicitly models entity spans in the input text and uses an entity linker to retrieve precomputed entity embeddings from a KB to form knowledge enhanced entity-span representations. The KB-embeddings are precomputed with a loss function similar to TransE. Projection mappings are used to transform LM-embeddings to KB-embeddings and vice versa. Information from the best matching KB-embeddings is averaged and retransformed to enhance the LM-embeddings. These computations form an additional layer of BERT. Wikipedia and WordNet were used as KBs. To test KnowBERT’s ability to retrieve facts from the KB, a relation was formulated and one argument of the relation was masked. KnowBERT reaches a <em class="EmphasisTypeItalic ">mean reciprocal rank</em><span id="ITerm140"/> (<em class="EmphasisTypeItalic ">MRR</em>) of 0.31, indicating that on average the correct entity appeared on rank 3, whereas for BERT it shows up on rank 9. Hence, the model generates better answers than BERT, but is only approximately able to reproduce the relations of the KB. However, it often leads to improvements in downstream tasks.</p><p class="Para" id="Par157"><strong class="EmphasisTypeBold ">ERNIE-THU</strong><span id="ITerm141"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR255" role="doc-biblioref">255</a></span>] relates named entities in a KB to the named entities in a document in a similar way, and transforms embeddings between these two spaces. <em class="EmphasisTypeItalic ">E-BERT</em><span id="ITerm142"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR162" role="doc-biblioref">162</a></span>] is similar in spirit to KnowBert, but it requires no expensive further pre-training of the BERT encoder. <em class="EmphasisTypeItalic ">Facts as Experts</em><span id="ITerm143"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR213" role="doc-biblioref">213</a></span>] also links factual information and entities using embeddings, and in this way can inject new information into the model.</p><p class="Para" id="Par158">In summary the methods presented in this section directly infuse domain-specific knowledge expressed by relation embeddings into token embeddings of PLMs. There are, however, a number of disadvantages. The KB entity embeddings are separately pre-trained with some knowledge embedding models (e.g., TransE [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>]) and fixed during training of the PLMs. Thus KB-embedding and token embeddings are not learned simultaneously. Moreover, the KB entity embeddings often cannot fully capture the rich contextual and relational information of an entity in the KB. Furthermore, they are static and do not depend on the context. In addition, they rely to a great extent on the performance of the linking algorithm and on the reliability of graph embeddings. This means that in general other approaches perform better, e.g. for relation extraction (Sect. <span class="ExternalRef"><a href="528393_1_En_5_Chapter.xhtml#Sec19"><span class="RefSource">5.​4</span></a></span>).</p></section>
<section class="Section2 RenderAsSection2" id="Sec19"><h3 class="Heading"><span class="HeadingNumber">3.4.2 </span>Pre-trained Language Models for Graph Learning</h3><p class="Para" id="Par159">Relations between objects and concepts can be joined in a graph and provide a uniform representation for the relatedness of many items. Using the structure of a graph many properties of nodes can be predicted. In recent years there was a great effort to design models which can capture the composition of a graph and predict its parts, e.g. <em class="EmphasisTypeItalic ">node2vec</em><span id="ITerm144"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>] or <em class="EmphasisTypeItalic ">graph convolutional networks</em><span id="ITerm145"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR107" role="doc-biblioref">107</a></span>]. However, the node representations obtained by such deep models tend to be over-smoothed and also become very vague. PLMs potentially are able to improve the representation by self-attention over long distances. Xia et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR233" role="doc-biblioref">233</a></span>] provide a survey on PLMs for graphs. Nodes and edges are characterized by different feature and position embeddings, and are processed with different types of PLMs. Prominent applications are <em class="EmphasisTypeItalic ">recommender systems</em><span id="ITerm146"/> exploiting user-product graphs and <em class="EmphasisTypeItalic ">drug discovery</em><span id="ITerm147"/> evaluating molecule structures.</p><p class="Para" id="Par160"><strong class="EmphasisTypeBold ">Graph-BERT</strong><span id="ITerm148"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR250" role="doc-biblioref">250</a></span>] is trained on sample nodes taken from a large graph together with their context. These samples are drawn using the closeness according to the PageRank algorithm [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>] and contain no direct link information. Nodes are characterized by feature embeddings, embeddings based on the PageRank information, and hop-based distance embeddings. These embeddings are summarized and form the input of a BERT model. The model is pre-trained to reconstruct the information of masked nodes and to predict the relation between two nodes by evaluating their cosine similarity. The model is fine-tuned for node classification and graph clustering. Graph-BERT achieves the second-best accuracies for node classification on three graph benchmarks [<span class="CitationRef"><a epub:type="biblioref" href="#CR128" role="doc-biblioref">128</a></span>, p. 16].</p><p class="Para" id="Par161"><strong class="EmphasisTypeBold ">GPT-GNN</strong><span id="ITerm149"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR87" role="doc-biblioref">87</a></span>] proposes an autoregressive PLM to perform an iterative reconstruction on given graphs. The method assumes a random order on the edges and nodes. Given the edges and nodes up to a specific position, it predicts the properties of the next nodes/edges. GPT-GNN generates one masked node and its edges at a time and optimizes the parameterized models via maximizing the likelihood of the node and edges generated in the current iteration. Then, it iteratively generates nodes and edges until all masked nodes are generated. The model is trained on a graph of 178M scientific papers with their features, the venue and the authors, and on a graph with 83M Amazon reviews, users and products. On both benchmarks the model has the best accuracies.</p><p class="Para" id="Par162"><strong class="EmphasisTypeBold ">MPG</strong><span id="ITerm150"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR120" role="doc-biblioref">120</a></span>] consists of a BERT model encoding node and edge features. As a pre-training task, the model has to learn whether two graphs divided into two halves actually belong together or whether the halves are a random pair. The model is applied to the modeling of molecules and achieves <span class="EmphasisTypeSmallCaps ">Sota</span> results on a range of 14 benchmarks, especially drug discovery.<span id="ITerm151"/></p><p class="Para" id="Par163"><strong class="EmphasisTypeBold ">GraphFormers</strong><span id="ITerm152"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR238" role="doc-biblioref">238</a></span>] jointly models a graph structure together with sequences of words. Each node of the graph contains a text. A center node and its neighbors are tokenized into sequences of tokens. The model has special transformer layers for computing the embeddings of text tokens and for the derivation of node embeddings by aggregating the corresponding text embeddings. The model is pre-trained with the task to predict, if two nodes are linked or not. GraphFormers is tested on three benchmark tasks, e.g. a graph with scientific papers characterized by their titles and their citation graph. The model consistently outperforms all prior approaches in the prediction of links.</p></section>
<section class="Section2 RenderAsSection2" id="Sec20"><h3 class="Heading"><span class="HeadingNumber">3.4.3 </span>Textual Encoding of Tables</h3><p class="Para" id="Par164">Tabular data probably makes up the majority of all business and administrative data today. Examples are retail transactions, official statistics, processing data from industrial applications, etc. A survey on the interpretation of tables on the web is provided by de Alwis et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>]. Previous work often relies on manually selected features, cannot handle the flexible schemas in web tables, and does not generalize well across tasks.</p><p class="Para" id="Par165"><strong class="EmphasisTypeBold ">TURL</strong><span id="ITerm153"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>] characterizes a relational table by the table caption <em class="EmphasisTypeItalic ">C</em> (a short text, may be enhanced by section title), column headers <em class="EmphasisTypeItalic ">h</em><sub><em class="EmphasisTypeItalic ">i</em></sub> (a sequence of tokens) describing the table scheme <em class="EmphasisTypeItalic ">H</em> = {<em class="EmphasisTypeItalic ">h</em><sub>1</sub>, …, <em class="EmphasisTypeItalic ">h</em><sub><em class="EmphasisTypeItalic ">m</em></sub>} and cell values, where each cell may represent an entity, e.g. a person. Cells in the same row share some relation, and cells in the same column share another relation. This requires a structure-aware attention mechanism implemented by a visibility matrix, which restricts the attention to specific columns and rows.</p><div class="Para" id="Par166">TURL is pre-trained according to the masked language model loss on a large unstructured dataset consisting of the table captions and headers. Subsequently, the relation between entities in the same row or column can be learned. Entities in a table are masked, and the model has the task to predict them based on the table context and the visibility matrix. By this target TURL can learn factual relations from the table and encode them into entity embeddings (Fig. <span class="InternalRef"><a href="#Fig15">3.15</a></span>).<figure class="Figure" id="Fig15"><div class="MediaObject" id="MO16"><img alt="" aria-describedby="d64e7340" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig15_HTML.png" style="width:34.38em"/><div class="TextObject" id="d64e7340"><p class="Para" id="Par316">A model diagram represents the input tokens and input entities going through self-attention. It indicates the layers of word embedding, type embedding, position embedding, contextualized representation entity embedding, and mention representation.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.15</span><p class="SimplePara">Learning table relations with TURL [<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>]. On the left side the table caption and the column headers are trained. On the right side the row markers together with input entities (cells in a specific row) are processed</p></div></figcaption></figure></div><p class="Para" id="Par167">The model is trained on 570k tables extracted from Wikipedia. All columns containing at least one linked cell are marked as entity columns. After fine-tuning, the model is able to predict the masked contents of table cells in the test set with precision of 54.8%, beating competing approaches. An ablation study shows that the visibility attention matrix is essential for achieving a high performance.</p><div class="Para" id="Par168"><strong class="EmphasisTypeBold ">TaBERT</strong><span id="ITerm154"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR241" role="doc-biblioref">241</a></span>] aims to include both, natural language text and structured table data. TaBERT is trained on 26.6M tables and surrounding text from English Wikipedia and the WDC WebTable Corpus [<span class="CitationRef"><a epub:type="biblioref" href="#CR115" role="doc-biblioref">115</a></span>]. Each table cell is described as (column header, column value type, value). Subsequently, the table rows are encoded as text, as shown in Fig. <span class="InternalRef"><a href="#Fig16">3.16</a></span>. For pre-training 20% of the columns of a table are randomly selected and the model has to predict the masked column names and types. In addition, the cell values are reconstructed according to a special scheme. The model is fine-tuned on the <em class="EmphasisTypeItalic ">WikiTableQuestions benchmark</em><span id="ITerm155"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR155" role="doc-biblioref">155</a></span>], which contains questions requiring compositional, multi-hop reasoning over a series of entries in the given table. To reduce effort only table rows containing query tokens are encoded. TaBERT is able to increase the <span class="EmphasisTypeSmallCaps ">Sota</span> accuracy on this benchmark to 51.8%. The authors show that their table cell encoding is more effective than alternatives. <strong class="EmphasisTypeBold ">RPT</strong><span id="ITerm156"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR205" role="doc-biblioref">205</a></span>] proposes a similar scheme for table encoding. <strong class="EmphasisTypeBold ">BRIDGE</strong><span id="ITerm157"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR124" role="doc-biblioref">124</a></span>] is a system for <em class="EmphasisTypeItalic ">semantic parsing</em><span id="ITerm158"/>, which converts information from text and tables to an SQL query extracting information from a database.<figure class="Figure" id="Fig16"><div class="MediaObject" id="MO17"><img alt="" aria-describedby="d64e7411" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig16_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e7411"><p class="Para" id="Par317">A table of 2 rows and 3 columns represents the list of the venues, positions, and events in the years 2005 and 2006. At the bottom, it indicates the types of data present in each column.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.16</span><p class="SimplePara">TaBERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR241" role="doc-biblioref">241</a></span>] encodes the rows of a table as text in a special format. The “context” contains corresponding text. Each table cell is represented as (column header, column value type, value). Here the first table row is encoded by the line starting with [CLS]</p></div></figcaption></figure></div><p class="Para" id="Par169"><strong class="EmphasisTypeBold ">Tapas</strong><span id="ITerm159"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR81" role="doc-biblioref">81</a></span>] is a variant of BERT optimized for table processing. The table is flattened row-by-row, tokenized and enhanced with position embeddings. Following embeddings are added: a row id embedding, a column id embedding, and a rank embedding indicating the rank in the sorted sequence, e.g. for numbers. The model is pre-trained on 6.2M table-text pairs from the English Wikipedia with the task to restore words in both table and text that have been replaced with a mask. The model can do this with relatively high accuracy (71.4% accuracy on a test set).</p><p class="Para" id="Par170">During fine-tuning the model learns to answer questions from a table, e.g. <em class="EmphasisTypeItalic ">“Which wrestler had the most number of reigns?”</em> for a table with wrestling results. <em class="EmphasisTypeItalic ">[CLS]</em> and a query are prepended to the flattened table and both parts are distinguished by an additional segment embedding. The model has two output types: (1) a score for each table cell with the probability that this cell will be part of the answer and (2) a probability of the result type (none, count, sum, average) for <em class="EmphasisTypeItalic ">[CLS]</em> to produce the final answer. Together the result indicates which operation should be performed over which table cells to generate the final answer. On several benchmarks Tapas reaches <span class="EmphasisTypeSmallCaps ">Sota</span> results, e.g. improving from 55.1% to 67.2% for <em class="EmphasisTypeItalic ">SQA</em><span id="ITerm160"/> benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>]. The source code and pre-trained models are available at <span class="ExternalRef"><a href="https://huggingface.co/transformers/model_doc/tapas.html"><span class="RefSource">Hugging Face</span></a></span>.</p><p class="Para" id="Par171">The results show that the models described above are able to extract information from tables and answer question about the table content. This makes it possible to use a large source of information, since tables are ubiquitous in text documents and web pages. In principle, the approach can also be used by large Foundation Models to include table information in the text they generate.</p><p class="Para" id="Par172"><strong class="EmphasisTypeBold ">TableGPT</strong><span id="ITerm161"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>] generate a text from a table using the GPT-2 language model. It enhances GPT-2 for table-to-text generation with two auxiliary tasks, table structure reconstruction and content matching, for improving text fidelity.</p></section>
<section class="Section2 RenderAsSection2" id="Sec21"><h3 class="Heading"><span class="HeadingNumber">3.4.4 </span>Textual Encoding of Knowledge Base Relations</h3><p class="Para" id="Par173">A number of proposals try to verbalize KB-relations as text. In this way, KB-relations may be directly incorporated in the training text of the language models.</p><p class="Para" id="Par174"><strong class="EmphasisTypeBold ">WKLM</strong><span id="ITerm162"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR234" role="doc-biblioref">234</a></span>] randomly replaces a fraction of the entity mentions in the original document with names of other entities of the same type. The model is trained to distinguish the correct entity mention from the randomly chosen ones. In addition, the model has to predict masked token. The types of entities are obtained from Wikidata [<span class="CitationRef"><a epub:type="biblioref" href="#CR214" role="doc-biblioref">214</a></span>]. In this way, the model can better capture entity information from natural language and yields better results for entity-related NLP tasks. WKLM is able to predict relation arguments much better than BERT. In question answering (SQuAD and open domain, Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec9"><span class="RefSource">6.​2</span></a></span>) the model is also able to reach <span class="EmphasisTypeSmallCaps ">Sota</span> results. Similar approaches [<span class="CitationRef"><a epub:type="biblioref" href="#CR191" role="doc-biblioref">191</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR203" role="doc-biblioref">203</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR234" role="doc-biblioref">234</a></span>] propose entity and phrase masking and replacement schemes.</p><div class="Para" id="Par175"><strong class="EmphasisTypeBold ">CoLAKE</strong><span id="ITerm163"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR202" role="doc-biblioref">202</a></span>] extracts the knowledge context of an entity from large-scale knowledge bases. The model links entity mentions to the underlying entities in a KB by an entity linker. The mention nodes are then replaced by their linked entities. The CoLAKE model is initialized with the RoBERTa<sub>BASE</sub> model. It is trained on Wikipedia with 3 million entity embeddings and 822 relation embeddings aligned to the <em class="EmphasisTypeItalic ">Wikidata5M</em><span id="ITerm164"/> KB [<span class="CitationRef"><a epub:type="biblioref" href="#CR224" role="doc-biblioref">224</a></span>] on 26M training samples. The example input <em class="EmphasisTypeItalic ">“[CLS] Harry Potter points his wand at Lord Voldemort [SEP]”</em> is shown in Fig. <span class="InternalRef"><a href="#Fig17">3.17</a></span>. The type of inputs (word, entity, relation) is encoded as type embeddings and added to the token and position embeddings. To introduce a relation from the KB, e.g. <em class="EmphasisTypeItalic ">“(Harry Potter, mother, Lily Potter)”</em>, the relation node <em class="EmphasisTypeItalic ">“mother”</em> and the entity node <em class="EmphasisTypeItalic ">“Lily Potter”</em> are introduced with the position embeddings 2 and 3, as the first relation argument <em class="EmphasisTypeItalic ">“Harry Potter”</em> is located at position 1. Self attention is computed between text inputs. There is a masking mechanism restricting the self-attention for relation elements, e.g. to the pairs <em class="EmphasisTypeItalic ">“(Harry Potter, mother)”</em> as well as <em class="EmphasisTypeItalic ">“(mother, Lily Potter)”</em> in our example.<figure class="Figure" id="Fig17"><div class="MediaObject" id="MO18"><img alt="" aria-describedby="d64e7564" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig17_HTML.png" style="width:34.38em"/><div class="TextObject" id="d64e7564"><p class="Para" id="Par318">A model diagram represents the input text with named entity mentions replaced and knowledge graph elements going through the autoencoder with masked self-attentions. It indicates the layers of position, type, input and output embeddings, and token probabilities.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.17</span><p class="SimplePara">CoLAKE [<span class="CitationRef"><a epub:type="biblioref" href="#CR202" role="doc-biblioref">202</a></span>] identifies entities and encodes them with specific embeddings. Type embeddings distinguish words, entities and relations. The input embeddings are the sum of token/entity, position, and type embeddings. For all entities in the input text relations are extracted from the Knowledge Base and appended after <em class="EmphasisTypeItalic ">“[SEP]”</em>, e.g. mother(Harry Potter, Lily Potter). A masking mechanism ensures that relation elements can attend only to their corresponding elements in the input text. During pre-training the model has to predict masked tokens and entities</p></div></figcaption></figure></div><p class="Para" id="Par176">During pre-training about 15% of the input elements (words, entities, relations) are masked and have to be predicted by the model. As entity nodes simultaneously appear in the input text and the knowledge base this helps to align the representations of language and relations. Masking relation nodes helps CoLAKE to learn contextualized representation for relations. On the language understanding tasks of GLUE the CoLAKE model achieves a similar average of 86.3 as RoBERTa. An alternative task consist of the completion of relation triplets (<em class="EmphasisTypeItalic ">h</em>, <em class="EmphasisTypeItalic ">r</em>, <em class="EmphasisTypeItalic ">t</em>) using a sentence describing the relation. It turns out that CoLAKE is much better than its competitors, e.g. the correct relation is inferred from two entities in 72.1% of the cases.</p><p class="Para" id="Par177"><strong class="EmphasisTypeBold ">LUKE</strong><span id="ITerm165"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR237" role="doc-biblioref">237</a></span>] treats words and entities in a given text as independent tokens, and outputs contextualized representations of both. The model is based on BERT and trained to predict randomly masked words and entities in a large entity-annotated corpus derived from Wikipedia. It contains an entity-aware self-attention mechanism that is an extension of BERT’s self-attention. It takes into account embeddings indicating if a token represents text or an entity. LUKE yields <span class="EmphasisTypeSmallCaps ">Sota</span> results in relation classification, entity typing and NER. <strong class="EmphasisTypeBold ">K-adapter</strong><span id="ITerm166"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR222" role="doc-biblioref">222</a></span>] is a related approach using RoBERTa (Sect. <span class="InternalRef"><a href="#Sec2">3.1.1</a></span>) as fixed background model and building several independent “Adapters” to include knowledge from different KBs.</p><p class="Para" id="Par178"><strong class="EmphasisTypeBold ">EWISER</strong><span id="ITerm167"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>] similarly targets word sense disambiguation (WSD). Starting with BERT embeddings, it computes scores for WordNet synsets (sets of words with similar meaning). Exploiting the interdependence of the synset graph the approach computes final scores that a word belongs to a synset. It achieves a new <span class="EmphasisTypeSmallCaps ">Sota</span> on a number of WSD benchmarks (Sect. <span class="ExternalRef"><a href="528393_1_En_5_Chapter.xhtml#Sec7"><span class="RefSource">5.​2</span></a></span>).</p><p class="Para" id="Par179"><strong class="EmphasisTypeBold ">PET</strong><span id="ITerm168"/> (Pattern-Exploiting Training) [<span class="CitationRef"><a epub:type="biblioref" href="#CR184" role="doc-biblioref">184</a></span>] as an alternative constructs an additional training set using only a few labeled examples. Consider a 5-star scale rating for a restaurant in the Yelp dataset [<span class="CitationRef"><a epub:type="biblioref" href="#CR185" role="doc-biblioref">185</a></span>]. The authors add text to the reviews to express the ratings, e.g. <em class="EmphasisTypeItalic ">“All in all it was great”</em>. Using this approach the authors convert the Yelp dataset to a task for predicting masked words, e.g. <em class="EmphasisTypeItalic ">“All in all it was [MASK]”</em>. However, they provide the verbalized labels only for a small number of examples. Subsequently, they predict the best class for the non-labeled examples and train the model with the predicted classes as well as the language modeling loss to avoid <em class="EmphasisTypeItalic ">catastrophic forgetting</em><span id="ITerm169"/>. This can be done in several iterations. Although only a few labels have been used, the model performs better on Yelp than standard supervised approaches. The SuperGLUE benchmark data covers eight challenging NLP tasks. With just 32 labeled examples the PET approach trained according to the above schema yields a better average (75.4%) than GPT-3 (71.8%) with the same number of few-shot examples. This shows that good results can be achieved with a small model (223M) and only few labeled examples. Note that the fine-trained <span class="EmphasisTypeSmallCaps ">Sota</span> for SuperGLUE is 90.4% using T5 and Meena.</p><p class="Para" id="Par180"><strong class="EmphasisTypeBold ">TeKGen</strong><span id="ITerm170"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>] is a data-to-text sequence-to-sequence model to verbalize a complete KB. It is applied to the English <em class="EmphasisTypeItalic ">Wikidata knowledge base</em><span id="ITerm171"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR214" role="doc-biblioref">214</a></span>] with ≈ 6M entities and about 1500 relations. The model starts with a large training corpus of heuristically aligned Wikipedia text and Wikidata triples. Relations sharing a common entity <em class="EmphasisTypeItalic ">subject</em> are converted to the input <em class="EmphasisTypeItalic ">subject relation</em><sub>1</sub><em class="EmphasisTypeItalic ">object</em><sub>1</sub><em class="EmphasisTypeItalic ">, …, relation</em><sub><em class="EmphasisTypeItalic ">n</em></sub><em class="EmphasisTypeItalic ">object</em><sub><em class="EmphasisTypeItalic ">n</em></sub> for the T5 transformer (Sect. <span class="InternalRef"><a href="#Sec4">3.1.3</a></span>). As an example <em class="EmphasisTypeItalic ">“To kill a Mockingbird, author: Harper Lee, publication date: 11 July 1960”</em> is translated to <em class="EmphasisTypeItalic ">“To Kill a Mockingbird is a novel by Harper Lee published in 1960.”</em> The T5 model is fine-tuned and subjected to an addition check to generate good verbalizations. The resulting dataset of verbalized triples was used in a question answering task. It was able to increase the accuracy in the<span id="ITerm172"/><em class="EmphasisTypeItalic ">Natural QuestionsNatural Questions (NQ) benchmark</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>] (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec3"><span class="RefSource">6.​1.​2</span></a></span>) from 38.8% to 41.5%. <strong class="EmphasisTypeBold ">KGPT</strong><span id="ITerm173"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>] in a similar way converts structural knowledge into the serialized text and lets model learn knowledge-text alignments.</p><p class="Para" id="Par181">In summary these methods transform KB relations into text, e.g. as complete sentences expressing relations or as concatenated triples (e.g., [head text, relation text, tail text]) into LMs for training or fine-tuning. This text is transformed into contextual embeddings and the model is trained to detect the underlying relation. The drawback is that focusing on knowledge base completion tends to over-adapt the models to this specific task, which comes at the cost of generalization.</p></section>
<section class="Section2 RenderAsSection2" id="Sec22"><h3 class="Heading"><span class="HeadingNumber">3.4.5 </span>Enhancing Pre-trained Language Models by Retrieved Texts</h3><p class="Para" id="Par182">An <em class="EmphasisTypeItalic ">open domain question answering</em><span id="ITerm174"/><span id="ITerm175"/> system has the task of answering questions not restricted to a specific domain [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>]. Consider the following example from the <em class="EmphasisTypeItalic ">TriviaQA benchmark</em><span id="ITerm176"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR99" role="doc-biblioref">99</a></span>]. <em class="EmphasisTypeItalic ">“Question: The Dodecanese Campaign of WWII that was an attempt by the Allied forces to capture islands in the Aegean Sea was the inspiration for which acclaimed 1961 commando film?”</em><em class="EmphasisTypeItalic ">“Answer: The Guns of Navarone”</em>. It is not plausible that the model can reproduce such a specific response from the knowledge stored in its parameters, even if it was present in the data before training. Therefore, it would be desirable for the system to be able to gather additional evidence by a <em class="EmphasisTypeItalic ">retriever</em><span id="ITerm177"/> collecting relevant documents from a large text repository. Subsequently, it has to align the retrieved information with the question and generate an answer by another PLM, a <em class="EmphasisTypeItalic ">reader</em><span id="ITerm178"/>. New web search techniques can be used for this approach. They are based on comparing embeddings for words or passages consisting of several sentences. There are numerous applications such as question answering, summarization, and dialog systems. In Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec1"><span class="RefSource">6.​1</span></a></span> this is discussed in more detail. Recent surveys are provided by Zhu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR259" role="doc-biblioref">259</a></span>] and Yu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR244" role="doc-biblioref">244</a></span>].</p><p class="Para" id="Par183"><strong class="EmphasisTypeBold ">DPR</strong><span id="ITerm179"/> (Dense Passage Retriever) [<span class="CitationRef"><a epub:type="biblioref" href="#CR103" role="doc-biblioref">103</a></span>] employs a PLM to encode KB-passages <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">i</em></sub>, e.g. from Wikipedia, as embeddings <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">i</em></sub>). This can be achieved by fine-tuning a BERT model to encode passages by the embedding of the token <em class="EmphasisTypeItalic ">[CLS]</em>. These embeddings can be stored in an index for fast access. Then the DPR <em class="EmphasisTypeItalic ">retriever</em> processes the query sequence <em class="EmphasisTypeItalic ">x</em> by another BERT model and generates the query embedding <em class="EmphasisTypeItalic ">emb</em>(<em class="EmphasisTypeItalic ">x</em>). A number of <em class="EmphasisTypeItalic ">k</em> = 100 passages <em class="EmphasisTypeItalic ">d</em><sub><em class="EmphasisTypeItalic ">j</em></sub> with maximal inner product <span class="InlineEquation" id="IEq31"><img alt="$${emb}(x)^\intercal {emb}(d_j)$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq31.png" style="width:7.32em"/></span> is retrieved by a <em class="EmphasisTypeItalic ">nearest-neighbor search</em><span id="ITerm180"/>. Both BERT encoders can be trained together to generate appropriate embeddings using weak supervision in the form of question-answer pairs (cf. Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec6"><span class="RefSource">6.​1.​5</span></a></span>). If, for instance, the query is <em class="EmphasisTypeItalic ">“Who is the bad guy in lord of the rings”</em>, the algorithm can retrieve <em class="EmphasisTypeItalic ">“Sala Baker is best known for portraying the villain Sauron in the Lord of the Rings trilogy”</em>, because <em class="EmphasisTypeItalic ">“bad guy”</em> and <em class="EmphasisTypeItalic ">“villain”</em> have similar embeddings. Therefore, DPR can find passages with similar meaning, expressed with different words. Karpukhin et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR103" role="doc-biblioref">103</a></span>], for instance, show that already with 1000 training examples the dense retriever is better than the classical keyword search. For 40k training examples the top-20 retrieved passages contain the correct answer in about 79% of the time, while this value is only 59% for the classical retrieval. An in-depth discussion is given in Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec6"><span class="RefSource">6.​1.​5</span></a></span>.</p><p class="Para" id="Par184">The DPR <em class="EmphasisTypeItalic ">reader</em><span id="ITerm181"/> is another BERT model. Similar to BERT’s text pair classification, it is fine-tuned to predict a probability for each retrieved passage that this passage contains the correct answer. In addition, it selects a span of tokens by span prediction, which probably provides the answer. In the example it selects <em class="EmphasisTypeItalic ">“Sala Baker”</em> as the answer. Together both components form a <em class="EmphasisTypeItalic ">retriever-reader architecture</em><span id="ITerm182"/>, which recently became popular. The approach can be easily applied to KBs with billions of passages [<span class="CitationRef"><a epub:type="biblioref" href="#CR103" role="doc-biblioref">103</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR201" role="doc-biblioref">201</a></span>]. On the <em class="EmphasisTypeItalic ">Natural Questions</em><span id="ITerm183"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>] it yields a test set accuracy of 41.5%.</p><p class="Para" id="Par185"><strong class="EmphasisTypeBold ">DensePhrases</strong><span id="ITerm184"/> is a different system creating embeddings for phrases of up to 20 words in the KB, which are computed without knowing the query [<span class="CitationRef"><a epub:type="biblioref" href="#CR114" role="doc-biblioref">114</a></span>]. The processing of the retrieved phrases directly yields the answer without much computational effort. Using careful workflow optimization the authors achieve near-<span class="EmphasisTypeSmallCaps ">Sota</span> results with a much lower processing time than dense passage retrieval systems, e.g. a test set accuracy of 40.9% on Natural Questions.</p><div class="Para" id="Par186"><strong class="EmphasisTypeBold ">FiD</strong><span id="ITerm185"/> (Fusion in Decoder) [<span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>] employs DPR as retriever. In the reader step it uses the special tokens <em class="EmphasisTypeItalic ">“question:”</em>, <em class="EmphasisTypeItalic ">“title:”</em>, and <em class="EmphasisTypeItalic ">“context:”</em>. These tokens mark the question, the retrieved passage title and the passage text and are concatenated forming the input. Subsequently, these <em class="EmphasisTypeItalic ">k</em> retrieved triples are fed one-by-one into a transformer encoder like T5 [<span class="CitationRef"><a epub:type="biblioref" href="#CR170" role="doc-biblioref">170</a></span>] (770M parameters), which independently processes each triples by the encoder. Only in the decoder the passages are handled jointly and the text of the answer is generated. This approach drastically reduces the computational effort. The transformer is fine-tuned on a QA-task. The architecture of the model is shown in Fig. <span class="InternalRef"><a href="#Fig18">3.18</a></span>. Raffel et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR170" role="doc-biblioref">170</a></span>] provided evidence that generative models like T5 are even competitive for QA-tasks such as SQuAD [<span class="CitationRef"><a epub:type="biblioref" href="#CR173" role="doc-biblioref">173</a></span>], where answers are spans in a given document.<figure class="Figure" id="Fig18"><div class="MediaObject" id="MO19"><img alt="" aria-describedby="d64e8005" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig18_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e8005"><p class="Para" id="Par319">A diagram illustrates the flow of an input question to the output answer through units of retriever and reader. The retriever unit consists of a BERT encoder and an inner product. The reader unit consists of t 5 encoder, encoded question, and t 5 decoder.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.18</span><p class="SimplePara">A retrieval enhanced language model [<span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>] encodes the query and the KB passages as embeddings and uses a pre-trained retriever to find passages corresponding to the query. The reader is a Seq2seq model (T5) combining the query and the passages to generate the answer. This model setup is fine-tuned with different benchmark datasets</p></div></figcaption></figure></div><p class="Para" id="Par187">The system achieves a test set exact match accuracy of 51.4% on the Natural Questions benchmark compared to 41.5% for DPR. The <em class="EmphasisTypeItalic ">TriviaQA</em><span id="ITerm186"/> benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR99" role="doc-biblioref">99</a></span>] contains a set of trivia questions with answers that were originally scraped from the Web. On this benchmark the model yields <span class="EmphasisTypeSmallCaps ">Sota</span> results with 80.1% exact match accuracy [<span class="CitationRef"><a epub:type="biblioref" href="#CR211" role="doc-biblioref">211</a></span>]. This is better than the accuracy of other much larger models, like GPT3 with 175B parameters (71.2% EM), or T5 without retrieval and 11B parameters (60.5% EM). It turns out that increasing the number of retrieved passages strongly enhances the answer quality.</p><p class="Para" id="Par188">There are a number of new approaches to augment PLMs with text from an external KB. In Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec1"><span class="RefSource">6.​1</span></a></span> we describe different PLMs for retrieval that can be used by web search engines. In Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec9"><span class="RefSource">6.​2</span></a></span> we investigate systems for question answering that often employ a PLM-based retrieval mechanism and an additional PLM to generate the answer text. It combines the query, the knowledge acquired during training, as well as the information in the retrieved documents.</p><p class="Para" id="Par189">In summary, combining language models with retrieval is currently the most efficient way to incorporate additional information into PLMs. The new information is focused on the current query and thus very informative. The retrieval model can access semantically related passages within fractions of a second using new approximate open-source nearest neighbor index structures. By relying on embeddings, synonyms and paraphrases can be found and the meaning of words can be disambiguated. In addition, the underlying knowledge bases can be updated on the fly to keep the information current.</p></section>
<section class="Section2 RenderAsSection2" id="Sec23"><h3 class="Heading"><span class="HeadingNumber">3.4.6 </span>Summary</h3><p class="Para" id="Par190">The knowledge covered by the textual training data can be leveraged in various ways to improve the performance of PLMs. Entities and relations from a knowledge base can be represented by embeddings, e.g. by TransE. However, the utilization of these embeddings for PLMs is not very efficient and error-prone. A more promising alternative is the direct use of table content or knowledge base relations by specialized PLMs, which capture relationships between entities and table cells by specific self-attention patterns. Similar to Graph-CNNs PLMs have been directly used to acquire the relationship between the nodes of a graph by encoding the features of links by embeddings in a BERT-like model. Along this line a promising way to transfer relational knowledge from a graph to a language model is proposed by GraphFormers.</p><p class="Para" id="Par191">A very simple and efficient approach of incorporating tables and knowledge bases in PLMs is the creation of text that expresses the information content. This can be used by the PLM either as conditioning text or during training. However, the most promising way to include knowledge is <em class="EmphasisTypeItalic ">retrieval</em>, since most information is stored in the form of unstructured text on the Web or databases. Here, the retriever-reader architecture emerged as an effective way to collect relevant passages. Subsequently, the PLM generates new text by combining the internal knowledge, the start text, and the retrieved passages.</p><p class="Para" id="Par192">Much effort was devoted to the extension of the length of input sequences (Sect. <span class="InternalRef"><a href="#Sec7">3.2</a></span>). This was mainly achieved by sparse attention patterns reducing the increase in computational effort from quadratic to linear with S4 as a leading approach. Nevertheless, larger input sequences still have limited range of context both within the same sample and outside of it.</p><p class="Para" id="Par193">In contrast, retrieval can cover an indefinite context within the same sample by gathering appropriate passages, even if there is no simultaneous attention over the whole context. In addition, retrieval can access relevant information in huge document collections. Either the highly developed traditional keyword search engines may be used. Alternatively dense retrieval may be employed which compares embeddings of the query and passages using approximate nearest neighbor search over an index. It turns out that relatively small retrieval-based models outperform large Foundation Models like GPT-3. FiD, for example, achieves an exact match accuracy of 51.4% on the Natural Questions benchmark compared to 29.9% for GPT-3. Retrieval is extensively used by recent models such as WebGPT and Retro.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec24"><h2 class="Heading"><span class="HeadingNumber">3.5 </span>Changing Model Size</h2><p class="Para" id="Par194">The size of a model, especially its number of parameters, has a marked influence on the performance of the model, its memory requirements and the computational resources required for training. In the first section we discuss that models with more parameters potentially have a better performance. This, however, requires a larger computational effort during training and model utilization. An alternative are mixture-of-experts models, which define a number of parallel model structures which selectively compute a solution. This is described in the second section.</p><p class="Para" id="Par195">As initial versions of successful models often are extremely large, a variety of model compression and acceleration techniques have been developed. They reduce memory requirements and training time without noticeable degradation of accuracy, and allow the models to be deployed on low resource computing devices, such as cell phones. There are three main techniques for model size reduction [<span class="CitationRef"><a epub:type="biblioref" href="#CR65" role="doc-biblioref">65</a></span>]—parameter compression and reduction, low-rank factorization, and knowledge distillation—which are outlined in the subsequent sections.</p><section class="Section2 RenderAsSection2" id="Sec25"><h3 class="Heading"><span class="HeadingNumber">3.5.1 </span>Larger Models Usually Have a better Performance</h3><div class="Para" id="Par196">As a rule for machine learning, the number of parameters of a model should be limited to avoid <em class="EmphasisTypeItalic ">overfitting</em><span id="ITerm187"/>, i.e. adapting to random fluctuations in the data. It turned out that this does not hold for PLMs if the amount of training data and the number of model parameters are increased simultaneously. Larger PLMs have been shown to have better performance on NLP tasks, which is underscored by theoretical work on PLMs [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>, p. 117]. The benefits of increasing the number of parameters come from two factors: additional computations at training and inference time, and increased memorization of the training data. Kaplan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR102" role="doc-biblioref">102</a></span>] empirically investigated in detail the dependency between the number of model parameters <em class="EmphasisTypeItalic ">R</em> (excluding embeddings), the size <em class="EmphasisTypeItalic ">N</em> of the training data, and the amount of computing effort <em class="EmphasisTypeItalic ">C</em> used for training. They evaluated a large number of models and draw the following conclusions: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par197">The performance of the models depends largely on the size quantities <em class="EmphasisTypeItalic ">R</em>, <em class="EmphasisTypeItalic ">N</em>, <em class="EmphasisTypeItalic ">C</em>. Other architectural features such as width or depth have only a weak influence.</p></li><li><p class="Para" id="Par198">The performance follows a smooth power-law dependency with each of <em class="EmphasisTypeItalic ">R</em>, <em class="EmphasisTypeItalic ">N</em>, <em class="EmphasisTypeItalic ">C</em>, if the other quantities are not too small. As an example the loss is approximately <em class="EmphasisTypeItalic ">L</em> ≈ (<em class="EmphasisTypeItalic ">N</em>∕(5.4 ∗ 10<sup>13</sup>))<sup>−0.095</sup>.</p></li><li><p class="Para" id="Par199">If <em class="EmphasisTypeItalic ">R</em> and <em class="EmphasisTypeItalic ">N</em> are increased at the same rate, the model accuracy grows reliably. If one of these factors is held constant the improvement gets lower. To get the best performance, the model size <em class="EmphasisTypeItalic ">R</em> should grow with the factor 8, if the data <em class="EmphasisTypeItalic ">N</em> is increased 5 times.</p></li><li><p class="Para" id="Par200">Training loss has a predictable dependency on computing effort and can be extrapolated.</p></li><li><p class="Para" id="Par201">The performance of fine-tuning of a pre-trained model on a different training task depends strongly on the loss for the pre-training validation set. Therefore, transfer to a different distribution induces a constant penalty, but roughly improves with the performance on the pre-training set.</p></li><li><div class="Para" id="Par202">Large models are better able to extract information from data than small models. They reach the same level of accuracy with fewer optimization steps and using fewer data points. If there is only a fixed amount of computation time, but no restrictions on size or data, one should use very large models and stop before convergence (Fig. <span class="InternalRef"><a href="#Fig19">3.19</a></span>). The optimal batch size depends on the <em class="EmphasisTypeItalic ">gradient noise</em><span id="ITerm188"/>, which is easy to measure during training [<span class="CitationRef"><a epub:type="biblioref" href="#CR132" role="doc-biblioref">132</a></span>] and is larger than assumed before.<figure class="Figure" id="Fig19"><div class="MediaObject" id="MO20"><img alt="" aria-describedby="d64e8182" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig19_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e8182"><p class="Para" id="Par320">2 line graphs plot test loss versus tokens processed and compute budget. Both graphs denote the decreasing trend of multiple lines indicating the number of parameters range from 10 power 3 10 power 6 and 10 power 9. It indicates that compute-efficient training stops far short of convergence.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.19</span><p class="SimplePara">A series of language model training runs with varying model sizes [<span class="CitationRef"><a epub:type="biblioref" href="#CR102" role="doc-biblioref">102</a></span>]. The left graph shows that larger models require fewer samples to reach a fixed test loss. The right graph demonstrates that the model size should grow with compute budget. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR102" role="doc-biblioref">102</a></span>, p. 4]</p></div></figcaption></figure></div></li></ul></div> These findings show that the success of larger PLMs is a systematic feature. A larger number of model parameters is much more sample efficient than thought before, when overfitting was a major problem for smaller training tasks. This also explains the success of large models like T5, BigBird, or GPT-3. Hernandez et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR80" role="doc-biblioref">80</a></span>] investigate empirical scaling laws for the transfer from pre-training to fine-tuning. Figure <span class="InternalRef"><a href="#Fig20">3.20</a></span> plots the training efforts of some Deep Learning models during the last two decades.<figure class="Figure" id="Fig20"><div class="MediaObject" id="MO21"><img alt="" aria-describedby="d64e8204" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig20_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e8204"><p class="Para" id="Par321">A scatter plot of parameters versus dates represents the distribution of 11 different domains between 2018 and 2022. It indicates the evolution of transformer, BERT, G P T 2, G P T 3, gopher, PaLM, and DALL E 2, across the durations.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.20</span><p class="SimplePara">Number of parameters for Deep Learning Models since 2017 [<span class="CitationRef"><a epub:type="biblioref" href="#CR188" role="doc-biblioref">188</a></span>]. Note that the parameter scale is logarithmic. The number of parameters roughly increased from 100M up to 1000B</p></div></figcaption></figure></div></section>
<section class="Section2 RenderAsSection2" id="Sec26"><h3 class="Heading"><span class="HeadingNumber">3.5.2 </span>Mixture-of-Experts Models</h3><p class="Para" id="Par203">As discussed above a model with more parameters usually can achieve a better performance. A simple way to increase the number of parameters without a higher training effort is a <strong class="EmphasisTypeBold ">mixture-of-experts</strong><span id="ITerm189"/> architecture. It was already proposed in the nineties by Nowlan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR147" role="doc-biblioref">147</a></span>] and has a strong resemblance to decision tree models [<span class="CitationRef"><a epub:type="biblioref" href="#CR152" role="doc-biblioref">152</a></span>]. It consists of a single gating module and a number of expert modules with identical architecture but different parameters. Each expert specializes in only a subset of the data, and the gating module assigns each input to the appropriate experts. Specifically, the gating network computes a probability distribution over the experts indicating how well each expert is able to process the incoming input. A reduction in computational effort can be achieved, if only a few expert modules are actually used. The model is trained by stochastic gradient descent, which can compute the parameter gradient despite the discontinuities if some expert is exchanged. Increasing the number of experts keeps the computational cost constant because the model always selects the same small number of experts for each input, regardless of the total number of experts. The architecture enables massive models and is particularly efficient for distributed systems where the experts are spread across different computational devices.</p><div class="Para" id="Par204">Clark et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>] analyze the theoretical properties of such <em class="EmphasisTypeItalic ">routing networks</em><span id="ITerm190"/>, where each input is processed only by subnetworks with a fraction of the network’s parameters.The authors analyze three different architectures and get the following results. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par205">Routing improves the performance of PLMs in all investigated sizes and variants.</p></li><li><p class="Para" id="Par206">Improvement follows a power-law in the number of experts <em class="EmphasisTypeItalic ">E</em> that diminishes with model size <em class="EmphasisTypeItalic ">N</em>, and can be further generalized across routing architectures.</p></li></ul></div> The analysis is based on the evaluation of several magnitudes of size, including models with hundreds of experts and hundreds of billions of parameters.</div><div class="Para" id="Par207"><strong class="EmphasisTypeBold ">GLaM</strong><span id="ITerm191"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>] is an autoregressive <em class="EmphasisTypeItalic ">mixture-of-experts</em><span id="ITerm192"/> (<em class="EmphasisTypeItalic ">MoE</em>) model with up to 1200B parameters. It replaces the fully connected layer of every second encoder block (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec2"><span class="RefSource">2.​1.​1</span></a></span>) with 64 copies having different parameters. For each embedding, a gating module selects two of these 64 fully connected layer for processing. The architecture is shown in Fig. <span class="InternalRef"><a href="#Fig21">3.21</a></span>. The model was trained on a huge collection of 1.6T tokens documents and quality-checked web pages. It has approximately 7 times more parameters than GPT-3 but requires only 1/3 of its training effort. In this way, the model has many more parameters increasing its representational capacity. As for a given input token, only two expert models are used, the computational effort for training and application is lower. The zero-shot and one-shot performance is better than for GPT-3 on 29 NLP tasks. Some results are compared to those of other models in Tables <span class="InternalRef"><a href="#Tab3">3.3</a></span> and <span class="InternalRef"><a href="#Tab4">3.4</a></span>. GLaM is remarkable as it requires only 1/3 of the training effort of GPT-3 but it achieves a similar or better performance than GPT-3 on NLP tasks.<figure class="Figure" id="Fig21"><div class="MediaObject" id="MO22"><img alt="" aria-describedby="d64e8298" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig21_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e8298"><p class="Para" id="Par322">An illustration of the architecture of G L A M. It indicates the layers of input and position embeddings, feed-forward, and output embedding, along with the residual connections.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.21</span><p class="SimplePara">Architecture of GLaM [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>]. For each input token, e.g., <em class="EmphasisTypeItalic ">“likes”</em>, the gating module dynamically selects two most relevant experts out of 64 available experts. This is indicated by the blue grid. The weighted average of the outputs from these two experts’ feedforward models is then passed to the next encoder block. For the other inputs different experts are selected. A mixture-of-experts layer is used in every second encoder block</p></div></figcaption></figure></div><p class="Para" id="Par208"><strong class="EmphasisTypeBold ">WuDao-2.0</strong><span id="ITerm193"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR175" role="doc-biblioref">175</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR178" role="doc-biblioref">178</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR257" role="doc-biblioref">257</a></span>] is a recent giant autoregressive language model with 1750B parameters, ten times larger than GPT-3. It has <em class="EmphasisTypeItalic ">mixture-of-experts</em><span id="ITerm194"/> layers, where a gating network selects a submodule for processing based on the input. WuDao-2.0 uses the <em class="EmphasisTypeItalic ">FastMoE</em><span id="ITerm195"/> library [<span class="CitationRef"><a epub:type="biblioref" href="#CR74" role="doc-biblioref">74</a></span>] and employs the GLM 2.0 architecture (Sect. <span class="InternalRef"><a href="#Sec4">3.1.3</a></span>) combining the different learning paradigms of BERT, GPT and the encoder-decoder transformer [<span class="CitationRef"><a epub:type="biblioref" href="#CR175" role="doc-biblioref">175</a></span>].</p><p class="Para" id="Par209">The training data consist of 1.2TB Chinese text, 2.5TB Chinese graphic data and 1.2TB English text data from the <em class="EmphasisTypeItalic ">Pile</em><span id="ITerm196"/> corpus [<span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>]. The <em class="EmphasisTypeItalic ">Cogview</em><span id="ITerm197"/> model is used for the joint processing of images Sect. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml#Sec12"><span class="RefSource">7.​2</span></a></span>. In addition, WuDao-2.0 can learn on the fly, draw pictures and compose poetry. These capabilities are a significant difference to GPT-3.</p><p class="Para" id="Par210">The published performance claims are impressive. On the LAMA benchmark for measuring world knowledge [<span class="CitationRef"><a epub:type="biblioref" href="#CR158" role="doc-biblioref">158</a></span>] it scores higher than AutoPrompt [<span class="CitationRef"><a epub:type="biblioref" href="#CR192" role="doc-biblioref">192</a></span>]. For the <em class="EmphasisTypeItalic ">SuperGLUE</em><span id="ITerm198"/> few-shot natural language understanding task [<span class="CitationRef"><a epub:type="biblioref" href="#CR219" role="doc-biblioref">219</a></span>] it achieves <span class="EmphasisTypeSmallCaps ">Sota</span> and surpasses GPT-3. For the Lambada benchmark (Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec4"><span class="RefSource">4.​1.​3</span></a></span>), where the last word of a paragraph has to be predicted, it yields better results than Microsoft Turing NLG. In addition, it increases <span class="EmphasisTypeSmallCaps ">Sota</span> for a number of text-graphics tasks (Sect. <span class="ExternalRef"><a href="528393_1_En_7_Chapter.xhtml#Sec20"><span class="RefSource">7.​2.​8</span></a></span>).</p><p class="Para" id="Par211"><strong class="EmphasisTypeBold ">Switch</strong><span id="ITerm199"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>] is a variant of the transformer encoder-decoder T5 (Sect. <span class="InternalRef"><a href="#Sec4">3.1.3</a></span>). It has a <em class="EmphasisTypeItalic ">mixture-of-experts</em><span id="ITerm200"/> architecture, which replaces the fully connected layer of each encoder block with <em class="EmphasisTypeItalic ">k</em> = 128 copies having different parameters. There is a simple linear gating network, which selects one of the 128 single fully connected layers (the experts) per token. Hence, the number of parameters is drastically increased with approximately constant computational effort. For this architecture a gradient can be computed and the model may be optimized using a number of specific strategies and a special TensorFlow version. It turns out that Switch achieves the same loss level compared to the standard T5 version with 1/7 of the computing time. On a number of fine-tuning tasks the large Switch model with 1600B parameters and 2048 experts yields better results than T5-large (Sect. <span class="InternalRef"><a href="#Sec4">3.1.3</a></span>) with 13B parameters requiring a quarter of the computational training effort.</p><p class="Para" id="Par212">As an alternative to the gating network in the mixtures-of-experts architecture, it is possible to use hash values to activate different parts of the network. <strong class="EmphasisTypeBold ">Token Switch</strong><span id="ITerm201"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR177" role="doc-biblioref">177</a></span>] computes a hash value for each input token and routes the generated embeddings of each token to different feedforward networks based on the hash values. The authors show that their approach compares favorable to Switch and works well on comprehensive language modeling tasks.</p><div class="Para" id="Par213"><strong class="EmphasisTypeBold ">ST-MoE-32B</strong><span id="ITerm202"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR261" role="doc-biblioref">261</a></span>] is a mixture-of-experts model with 269B parameters and a comparable training cost of a 32B dense model. The authors modify the routing algorithm which dispatches token embeddings to one or two experts, and resolve instability issues. The model is similar to a T5-Large encoder-decoder [<span class="CitationRef"><a epub:type="biblioref" href="#CR170" role="doc-biblioref">170</a></span>]. The ST-MoE-32B has 32 experts with an expert layer frequency of 1/4, such that every fourth feedforward layer of T5 is replaced by an MoE layer. The authors use the <em class="EmphasisTypeItalic ">GEGLU</em><span id="ITerm203"/> activation function, which contains multiplicative elements [<span class="CitationRef"><a epub:type="biblioref" href="#CR142" role="doc-biblioref">142</a></span>] <div class="Equation NumberedEquation" id="Equ2"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} FFN_{GEGLU}({\boldsymbol{x}},W,V,{\boldsymbol{b}},\boldsymbol{c}) = GELU({\boldsymbol{x}} W+{\boldsymbol{b}})\odot ({\boldsymbol{x}} V+\boldsymbol{c}). \end{aligned} $$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_Equ2.png" style="width:26.31em"/></div></div> <div class="EquationNumber">(3.2)</div></div></div> The authors compare a large number of variants and hyperparameters to improve training.</div><p class="Para" id="Par214">The model achieves <span class="EmphasisTypeSmallCaps ">Sota</span> in many transfer learning benchmarks, e.g. for SuperGLUE with an average accuracy of 93.2% beating the PaLM LM with 540B parameters. Other <span class="EmphasisTypeSmallCaps ">Sota</span> results were reached for summarization (XSum [<span class="CitationRef"><a epub:type="biblioref" href="#CR143" role="doc-biblioref">143</a></span>] with 27.1 <span class="EmphasisTypeSmallCaps ">Rouge-2</span>, CNN/Daily Mail [<span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>] with 21.7 <span class="EmphasisTypeSmallCaps ">Rouge-2</span>), closed book question answering (WebQA [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>] 47.4% exact match, Natural Questions [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>] 41.9% exact match), and adversarially constructed tasks for common sense reasoning (Winogrande [<span class="CitationRef"><a epub:type="biblioref" href="#CR182" role="doc-biblioref">182</a></span>] 96.6%, ANLI R3 [<span class="CitationRef"><a epub:type="biblioref" href="#CR146" role="doc-biblioref">146</a></span>] 74.4%).</p></section>
<section class="Section2 RenderAsSection2" id="Sec27"><h3 class="Heading"><span class="HeadingNumber">3.5.3 </span>Parameter Compression and Reduction</h3><p class="Para" id="Par215"><em class="EmphasisTypeItalic ">Model quantization</em><span id="ITerm204"/> is a parameter reduction technique, where parameters are stored in low precision and therefore the computations in PLMs are also less precise. Conventional models normally use parameters of 32 bits or 16 bits, while parameters after quantization can have 8 bits or even 1 or 2 bits. <strong class="EmphasisTypeBold ">Q-BERT</strong><span id="ITerm205"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR190" role="doc-biblioref">190</a></span>], for example, quantizes Transformer models to ultra-low precision. This reduces the model size 13-fold while only loosing 2.3% performance. The authors avoid the naive approach of simply reducing weight precision, but use additional training steps to adjust the quantized weights and allow higher precision for more “sensitive” parameters. Other authors propose to delete parameters with small values [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>]. ALBERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR113" role="doc-biblioref">113</a></span>] uses the same weights across all layers and achieves a significant parameter reduction. Nevertheless, ALBERT has the same or better performance compared to BERT.</p><p class="Para" id="Par216">Another approach aims to reduce the number of parameters, e.g. by removing attention heads. It was shown that most attention heads focus only on nearly identical positional relations and can be replaced with fixed attention patterns [<span class="CitationRef"><a epub:type="biblioref" href="#CR172" role="doc-biblioref">172</a></span>]. It turned out that high performance is possible with only 1–2 attention heads per encoder unit instead of the 16 attention heads of the original model. A detailed overview on parameter compression techniques is provided by Ganesh et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>] .</p><p class="Para" id="Par217">Another method to reduce model parameters is model pruning, which cuts off irrelevant parts in PLMs to achieve a smaller memory footprint and faster execution without compromising performance. It could be shown, for example that some attention heads of the transformer may be removed with little impact on the accuracy [<span class="CitationRef"><a epub:type="biblioref" href="#CR256" role="doc-biblioref">256</a></span>]. Other researchers prune the weights of attention layers and linear layers to reduce the number of parameters without reducing the accuracy [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>]. Note that model pruning does not always lead to speedups, as sparse computations may be hard to parallelize on GPUs.</p></section>
<section class="Section2 RenderAsSection2" id="Sec28"><h3 class="Heading"><span class="HeadingNumber">3.5.4 </span>Low-Rank Factorization</h3><p class="Para" id="Par218">This technique employs matrix and tensor decomposition to reduce the number of parameters of full rank parameter matrices and already has been discussed in Sect. <span class="InternalRef"><a href="#Sec9">3.2.2</a></span> for the extension of the input sequence length. Examples are the Performer [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>] and the Linear Transformer [<span class="CitationRef"><a epub:type="biblioref" href="#CR105" role="doc-biblioref">105</a></span>] (Sect. <span class="InternalRef"><a href="#Sec9">3.2.2</a></span>). As an alternative, ALBERT (Sect. <span class="InternalRef"><a href="#Sec2">3.1.1</a></span>) approximates the embedding matrix as a product of two smaller matrices.</p></section>
<section class="Section2 RenderAsSection2" id="Sec29"><h3 class="Heading"><span class="HeadingNumber">3.5.5 </span>Knowledge Distillation</h3><p class="Para" id="Par219">In machine learning the knowledge distillation approach [<span class="CitationRef"><a epub:type="biblioref" href="#CR82" role="doc-biblioref">82</a></span>] transfers knowledge from a large <em class="EmphasisTypeItalic ">teacher model</em><span id="ITerm206"/> to a smaller <em class="EmphasisTypeItalic ">student model</em><span id="ITerm207"/>. The large model can often be trained successfully to approximate a functional relation without using its full representational capacity. To reduce the high computational and memory requirements during application, a smaller model is trained to imitate the large model without sacrificing accuracy.</p><div class="Para" id="Par220">The advantage of this approach is that the student model may be trained to approximate <em class="EmphasisTypeItalic ">internal activations</em> of the teacher model. Often the target probabilities generated by the teacher model are used to train the student network . Typically the outputs of the teacher model for an input <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> is <em class="EmphasisTypeItalic ">z</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>), which can be translated to a probability by a scaled softmax <div class="Equation NumberedEquation" id="Equ3"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} {\boldsymbol{y}}(x|\tau) = \frac{[\exp(z_1({\boldsymbol{x}})/\tau),\ldots,\exp(z_k({\boldsymbol{x}}))/\tau]} {\exp(z_1({\boldsymbol{x}})/\tau)+\cdots+\exp(z_k({\boldsymbol{x}})/\tau)} , \end{aligned} $$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_Equ3.png" style="width:20.19em"/></div></div> <div class="EquationNumber">(3.3)</div></div></div> where <em><strong class="EmphasisTypeBoldItalic ">y</strong></em>(<em class="EmphasisTypeItalic ">x</em>|<em class="EmphasisTypeItalic ">τ</em>) is a probability vector and <em class="EmphasisTypeItalic ">τ</em> is a parameter called <em class="EmphasisTypeItalic ">temperature</em>, which for a standard softmax is normally set to 1.0. The student model is trained to imitate the probabilities <span class="InlineEquation" id="IEq32"><img alt="$$\hat {{\boldsymbol {y}}}(x|\tau )$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq32.png" style="width:3.06em"/></span> generated by the teacher model by minimizing <em class="EmphasisTypeItalic ">cross entropy</em><span id="ITerm208"/><div class="Equation NumberedEquation" id="Equ4"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\displaystyle \begin{aligned} E({\boldsymbol{y}}|\tau) = - \sum_{j=1}^k \hat{y}_j(x|\tau)\log y_j(x|\tau), \end{aligned} $$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_Equ4.png" style="width:15.57em"/></div></div> <div class="EquationNumber">(3.4)</div></div></div> where <em><strong class="EmphasisTypeBoldItalic ">y</strong></em>(<em class="EmphasisTypeItalic ">x</em>|<em class="EmphasisTypeItalic ">τ</em>) is the output probability vector of the student model. If observed values are available the probabilities of the teacher model <em class="EmphasisTypeItalic ">y</em><sub><em class="EmphasisTypeItalic ">j</em></sub>(<em class="EmphasisTypeItalic ">x</em>|<em class="EmphasisTypeItalic ">τ</em>) may be replaced by 1.0 for the observed class and 0.0 otherwise. During training the temperature may be varied. A high temperature avoids extreme probability values and reduces the gradients. This may lead to a faster convergence in the beginning of the optimization.</div><p class="Para" id="Par221"><strong class="EmphasisTypeBold ">DistilBERT</strong><span id="ITerm209"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR183" role="doc-biblioref">183</a></span>] uses MLM cross-entropy loss to predict token probabilities and in addition the cosine similarity between the embedding matrices of the teacher and student networks to train a smaller BERT model. It utilizes knowledge distillation during pre-training to reduce the size of BERT by 40% while retaining 99% of its original capabilities and making the inference 60% faster. <em class="EmphasisTypeItalic ">MobileBERT</em><span id="ITerm210"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR204" role="doc-biblioref">204</a></span>] is based on a specific large BERT model and transfers information about multi-head-attention as well as the resulting embeddings. Experiments show that MobileBERT is 4.3× smaller and 5.5× faster than BERT while achieving competitive results on well-known benchmarks.</p><p class="Para" id="Par222"><strong class="EmphasisTypeBold ">TinyBERT</strong><span id="ITerm211"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR97" role="doc-biblioref">97</a></span>] proposes distillation of a BERT model during pre-training and fine-tuning. The model is adapted to: (1) the output of the embedding of selected layers; (2) the hidden states and attention matrices derived from selected Transformer layers; (3) the logit outputs of the prediction layer. As distillation is also performed during fine-tuning the model can be better adapted to the fine-tuned BERT. On a number of benchmarks TinyBERT is on par with BERT<sub>BASE</sub> and outperforms DistilBERT.</p><p class="Para" id="Par223">Note that the knowledge distillation methods discussed above require the data used for pre-training the teacher model, which is often not released because of data copyright. It has not yet been evaluated whether distillation is also feasible with new data. The training time for knowledge distillation is high, because the teacher model needs to perform a forward prediction over the entire pre-training data to generate activation values or intermediate representations.</p><p class="Para" id="Par224">Rogers et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR176" role="doc-biblioref">176</a></span>] list a large number of size reduction studies for BERT and report parameter size and computing time reduction as well as the resulting performance. For a number of approaches there is a marked reduction in memory and computing effort with nearly identical performance.</p></section>
<section class="Section2 RenderAsSection2" id="Sec30"><h3 class="Heading"><span class="HeadingNumber">3.5.6 </span>Summary</h3><p class="Para" id="Par225">The number of model parameters, the size of the training data and the amount of computation effort for training are the determining factors for the performance of a model. Kaplan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR102" role="doc-biblioref">102</a></span>] show by experiments that increasing parameter count and training set size reliably lead to a better performance and provide a detailed formula for the dependency. If a fixed compute budget is available, one should use a very large model and much data.</p><p class="Para" id="Par226">Mixtures-of-experts follow this approach by increasing the number of parameters without requiring more computational effort. By routing inputs to specific subnetworks they are able to increase performance compared to monolithic networks. Examples are GLaM, WuDao-2.0, and Switch. However, these networks have hundreds of billions of parameters and require a specific parallel computational infrastructure.</p><p class="Para" id="Par227">Often the trained networks are too large and have to be reduced to fit to smaller computing devices. A viable approach is low-precision computation, which reduces memory requirements for parameter storing. Low-Rank factorization of matrices also has a lower memory footprint as a side effect. Finally, knowledge distillation may be employed to create a student model which imitates the inner working of a large trained teacher network. DistilBERT, for example, was able to reduce the memory size by 40%, kept 99% of the original performance and was 60% faster. There are a number of other size reduction approaches with similar results.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec31"><h2 class="Heading"><span class="HeadingNumber">3.6 </span>Fine-Tuning for Specific Applications</h2><p class="Para" id="Par228">Self-supervised pre-training of language models on large text collections and subsequent fine-tuning them to solve specific tasks has become the standard paradigm in natural language processing and understanding. It has been shown that pre-trained language models such as BERT are excellent for generalization and can easily be fine-tuned to multiple tasks. However, sometimes simple fine-tuning to a domain-specific task is not sufficient, and other transfer learning approaches have to be used to better adapt models to domain-shift in the data [<span class="CitationRef"><a epub:type="biblioref" href="#CR166" role="doc-biblioref">166</a></span>]. There are a number of surveys covering transfer learning in depth [<span class="CitationRef"><a epub:type="biblioref" href="#CR230" role="doc-biblioref">230</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR252" role="doc-biblioref">252</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR260" role="doc-biblioref">260</a></span>]</p><div class="Para" id="Par229">Fine-tuning updates all the model layers, including the embedding layer, but there are larger changes in the higher layers [<span class="CitationRef"><a epub:type="biblioref" href="#CR133" role="doc-biblioref">133</a></span>]. First, we discuss whether fine-tuning can destroy the knowledge gained during pre-training. <em class="EmphasisTypeItalic ">Standard fine-tuning</em> adapts a large pre-trained PLM with many parameters to a relatively small fine-tuning training data set with little computational effort. We investigate whether <em class="EmphasisTypeItalic ">overfitting</em><span id="ITerm212"/> occurs during this phase. Subsequent sections introduce different approaches for fine-tuning: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par230"><em class="EmphasisTypeItalic ">Intermediate Fine-Tuning</em> performs an in-between fine-tuning step with a larger training set before a final target fine-tuning takes place.</p></li><li><p class="Para" id="Par231"><em class="EmphasisTypeItalic ">Multitask fine-tuning</em> enhances the model capabilities by simultaneously fine-tuning on a number of tasks.</p></li><li><p class="Para" id="Par232"><em class="EmphasisTypeItalic ">Fine-tuning a frozen model</em> adapts a small additional layer to the fine-tuning task instead of changing all weights of the large pre-trained model.</p></li><li><p class="Para" id="Par233"><em class="EmphasisTypeItalic ">Creating Prompts for Few-Shot Instructions</em> aims to generate inputs for a large autoregressive PLM like GPT-3 to solve a task in a zero or few-shot approach.</p></li></ul></div></div><section class="Section2 RenderAsSection2" id="Sec32"><h3 class="Heading"><span class="HeadingNumber">3.6.1 </span>Properties of Fine-Tuning</h3><div class="Para" id="Par234">Fine-tuning of PLMs is commonly employed to adapt a pre-trained model to a specific task by supervised training. This adaption of the model from a source task to a related target task is also called <em class="EmphasisTypeItalic ">transfer learning</em><span id="ITerm213"/><span id="ITerm214"/>. Transfer learning is especially rewarding if we have abundant training data for self-supervised learning—as it is typical for non-annotated text—and only little annotated data for the target task. A survey of transfer learning is provided by Zhuang et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR260" role="doc-biblioref">260</a></span>]. Fine-tuning has a number of advantages: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par235">The model acquires detailed knowledge about the language, its syntax and semantics by exploiting the content provided in the pre-training data.</p></li><li><p class="Para" id="Par236">Pre-trained models can easily be adapted to new tasks, e.g. by an additional layer with a simple classifier. The language representations of the pre-trained model support fine-tuning and are only slightly changed during this process.</p></li><li><p class="Para" id="Par237">Fine-tuning even with a small data set yields a much better performance than direct training of a classifier on the limited data.</p></li></ul></div> Autoencoder models like BERT are typically fine-tuned for classification tasks, where the logistic classifiers for masked language modeling and next sentence prediction have to be removed. Using the <em class="EmphasisTypeItalic ">[CLS]</em> token or other tokens as input, new logistic classifier models as well as all model parameters are trained end-to-end with the new task for a few epochs (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec5"><span class="RefSource">2.​1.​3</span></a></span>). Compared to pre-training, fine-tuning is relatively inexpensive. Usually, only a small fraction of the pre-training effort is required to achieve good results.</div><p class="Para" id="Par238">Tripuraneni et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR210" role="doc-biblioref">210</a></span>] have theoretically proven that transfer learning requires far less data than learn tasks in isolation. They prove that transfer learning improves if the task diversity is enhanced. Bansal et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>] investigate the theoretical properties of fine-tuning a classifier using pre-trained embeddings. The authors prove that these classifiers have a smaller generalization gap between their train and test accuracy, than standard classifiers.</p><section class="Section3 RenderAsSection3" id="Sec33"><h4 class="Heading">Catastrophic Forgetting</h4><p class="Para" id="Par239">The question is whether fine-tuning can destroy the original capabilities of the model. This means, after fine-tuning a pre-trained model for a few epochs, it could lose predictive performance available after pre-training. A possible reason can be <em class="EmphasisTypeItalic ">catastrophic forgetting</em><span id="ITerm215"/>, where all parameters are adapted to a new learning task while forgetting learned content.</p><p class="Para" id="Par240">Merchant et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR133" role="doc-biblioref">133</a></span>] fine-tune BERT<sub>BASE</sub> with three different tasks: (1) MNLI sentence pair classification task [<span class="CitationRef"><a epub:type="biblioref" href="#CR229" role="doc-biblioref">229</a></span>] measuring if the first sentence entails the second; (2) SQuAD question answering [<span class="CitationRef"><a epub:type="biblioref" href="#CR173" role="doc-biblioref">173</a></span>], where the answer to a question has to be marked in a text; (3) Dependency Parsing [<span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>] to capture the syntactic structure of sentences. Then they investigate the performance of a number of probing classifiers before and after fine-tuning. The results demonstrate that the fine-tuned models only show a small decrease in the accuracy to detect linguistic concepts. The reduction cause by the MNLI task in most cases is less than 1%, while higher differences (less than 3%) are observed for SQuAD and dependency parsing. Therefore, catastrophic forgetting cannot be observed. The authors state that fine-tuning primarily changes the top layers of BERT, with dependency parsing also affecting deeper layers. More detailed results are provided by Wallat et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR216" role="doc-biblioref">216</a></span>].</p><p class="Para" id="Par241">Fine-tuning only benefits from the pre-training, if there are similarities between the two tasks. Hence, pre-training should have a loss function which enforces the learning of semantics at word, phrase and document level. In addition, its training documents should originate from a domain close to the fine-tuning task. Otherwise the vocabulary may not include many domain-specific words. As a result, domain-specific words are split into a number of tokens which hinders model learning and degrades its performance in downstream tasks. In the next sections we will discuss alternative training regimes which improve BERT’s capabilities.</p></section>
<section class="Section3 RenderAsSection3" id="Sec34"><h4 class="Heading">Fine-Tuning and Overfitting</h4><p class="Para" id="Par242">During pre-training BERT’s parameters are adapted to the pre-training data, acquiring universal language representations. As pre-training provides a good initialization, it avoids overfitting on the small fine-tuning datasets, if the fine-tuning error is not minimized too much.</p><p class="Para" id="Par243">Since PLMs have a very large number of parameters, there is the risk of overfitting on the fine-tuning data. As a result, generalization from unseen data can be poor and counterstrategies may be required. D’Amour [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>] present a comprehensive discussion of this <em class="EmphasisTypeItalic ">underspecification</em><span id="ITerm216"/> phenomenon. Jiang et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR95" role="doc-biblioref">95</a></span>] introduces a form of regularization, which makes the model invariant to small perturbations of the input, inducing smoothness in the local neighborhood. They develop a class of Bregman proximal point optimization methods, which penalize large updates of the model at each iteration. Aghajanyan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>] introduce the notion of representational collapse, stating that fine-tuned models lose their ability to generalize. They propose fine-tuning optimization based on trust-region theory, which alleviates representational collapse at a fraction of the cost of other recently proposed fine-tuning methods and, for instance, improves the best known results on fine-tuning RoBERTa on GLUE.</p><p class="Para" id="Par244">Fine-tuning the same model with multiple random seeds can lead to large variance in task performance. Most papers argue that this effect is caused by <em class="EmphasisTypeItalic ">catastrophic forgetting</em><span id="ITerm217"/> and the small size of the fine-tuning datasets. However, Mosbach et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR140" role="doc-biblioref">140</a></span>] show that often fine-tuning has an optimization problem due to vanishing gradients. In addition, it can often occur that a model does not generalize well, although it has the same fine-tuning loss as a successful model. This is an indication for the underspecification mention above. The authors recommend to use small learning rates with bias correction to avoid vanishing gradients early in training. In addition, they propose to use more iterations for fine-tuning. More recipes to improve fine-tuning are provided by Rogers et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR176" role="doc-biblioref">176</a></span>].</p></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec35"><h3 class="Heading"><span class="HeadingNumber">3.6.2 </span>Fine-Tuning Variants</h3><section class="Section3 RenderAsSection3" id="Sec36"><h4 class="Heading">Fine-Tuning in Two Stages</h4><p class="Para" id="Par245">The intermediate training set should be closer to the final task. Although this approach can increase performance in some cases, an experimental evaluation demonstrates a decrease in performance in 44% of the cases [<span class="CitationRef"><a epub:type="biblioref" href="#CR163" role="doc-biblioref">163</a></span>]. An intermediate training with a task requiring high-level inference and reasoning abilities tend to work best, as was shown in a large experiment [<span class="CitationRef"><a epub:type="biblioref" href="#CR165" role="doc-biblioref">165</a></span>]. However, the authors also observe catastrophic forgetting of the pre-trained abilities. Gururangan et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR71" role="doc-biblioref">71</a></span>] have shown that a second phase of pre-training, using domain-specific data, leads to significant performance gains, both in high- and low-resource settings. In addition, pre-training on tasks-specific unlabeled data improves performance on various tasks and domains.</p></section>
<section class="Section3 RenderAsSection3" id="Sec37"><h4 class="Heading">Fine-Tuning for Multiple Tasks</h4><p class="Para" id="Par246">For each task, a task-specific layer is added to the underlying pre-trained model. Then the model is simultaneously trained with all tasks. However, it sometimes happens that performance does not increase compared to standard fine-tuning [<span class="CitationRef"><a epub:type="biblioref" href="#CR141" role="doc-biblioref">141</a></span>], perhaps because of contradicting requirements of tasks. As an alternative, a subset of fine-tuning tasks from the available datasets may be selected based on similarity measures [<span class="CitationRef"><a epub:type="biblioref" href="#CR131" role="doc-biblioref">131</a></span>].</p><p class="Para" id="Par247"><strong class="EmphasisTypeBold ">HyperGrid</strong><span id="ITerm218"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR208" role="doc-biblioref">208</a></span>] is a multitask learning approach evaluated on the T5 model. It learns grid-wise projections that help to specialize regions in weight matrices for different tasks. As an example, a single model is simultaneously adapted to all GLUE and SuperGLUE tasks at once. In spite of the multitude of tasks, the model has a slightly better performance on SuperGLUE than the single models.</p></section>
<section class="Section3 RenderAsSection3" id="Sec38"><h4 class="Heading">Meta-Learning to Accelerate Fine-Tuning</h4><p class="Para" id="Par248">During fine-tuning a pre-trained PLM is adapted to a new NLP task. It is usually trained for two or three epochs on a labeled fine-tuning dataset. Although this is much faster than pre-training the model on a large training corpus it still requires a lot of effort. To reduce this effort researchers tried to prepare the pre-trained model to fine-tuning by <em class="EmphasisTypeItalic ">meta-learning</em><span id="ITerm219"/>. A survey of meta-learning is provided by Yin [<span class="CitationRef"><a epub:type="biblioref" href="#CR242" role="doc-biblioref">242</a></span>].</p><div class="Para" id="Par249">Usually, there is a set <span class="InlineEquation" id="IEq33"><img alt="$$\mathcal {T}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq33.png" style="width:0.87em"/></span> of related fine-tuning tasks <em class="EmphasisTypeItalic ">T</em><sub><em class="EmphasisTypeItalic ">i</em></sub>. During meta-training a task <em class="EmphasisTypeItalic ">T</em><sub><em class="EmphasisTypeItalic ">i</em></sub> is sampled from a distribution <span class="InlineEquation" id="IEq34"><img alt="$$p(\mathcal {T})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq34.png" style="width:2.31em"/></span>. Then the model is trained with <em class="EmphasisTypeItalic ">K</em> training samples from <span class="InlineEquation" id="IEq35"><img alt="$$T_i^{\text{train}}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq35.png" style="width:2.44em"/></span> and then tested on the validation set of <span class="InlineEquation" id="IEq36"><img alt="$$T_i^{\text{val}}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq36.png" style="width:1.82em"/></span>. The validation error of <em class="EmphasisTypeItalic ">T</em><sub><em class="EmphasisTypeItalic ">i</em></sub> is utilized as the training error of the meta-learning framework for the current iteration. The <strong class="EmphasisTypeBold ">MAML</strong><span id="ITerm220"/> algorithm [<span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>] follows this pattern: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par250">Copy <em><strong class="EmphasisTypeBoldItalic ">w</strong></em><sup>[<em class="EmphasisTypeItalic ">i</em>]</sup> of the initial model parameters <em><strong class="EmphasisTypeBoldItalic ">w</strong></em>.</p></li><li><p class="Para" id="Par251">Train the model on the training set <span class="InlineEquation" id="IEq37"><img alt="$$T_i^{\text{train}}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq37.png" style="width:2.44em"/></span> with a <em class="EmphasisTypeItalic ">K</em> gradient updates: <span class="InlineEquation" id="IEq38"><img alt="$$\hat {{\boldsymbol {w}}}^{[i]} \gets {\boldsymbol {w}}^{[i]} - \gamma \partial L_i({\boldsymbol {w}}^{[i]},T_i^{\text{train}})/\partial {\boldsymbol {w}}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq38.png" style="width:15.87em"/></span></p></li><li><p class="Para" id="Par252">Apply the model with the updated parameters <span class="InlineEquation" id="IEq39"><img alt="$$\hat {{\boldsymbol {w}}}^{[i]}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq39.png" style="width:1.69em"/></span> on the validation set <span class="InlineEquation" id="IEq40"><img alt="$$T_i^{\text{val}}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq40.png" style="width:1.82em"/></span>.</p></li><li><p class="Para" id="Par253">Update the initial model parameters <em><strong class="EmphasisTypeBoldItalic ">w</strong></em> using the loss on the validation set <span class="InlineEquation" id="IEq41"><img alt="$${\boldsymbol {w}} \gets {\boldsymbol {w}} - \beta \partial L_i(\hat {{\boldsymbol {w}}}^{[i]},T_i^{\text{val}})/\partial {\boldsymbol {w}}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq41.png" style="width:13.94em"/></span></p></li></ul></div> This scheme was applied to BERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>]. The authors generate a large, rich, meta-learning task distribution from unlabeled text by gathering tokens-to-be masked from a few vocabulary terms. On 17 NLP tasks, they show that this type of meta-training leads to better few-shot generalization than language-model pre-training followed by fine-tuning. Chen et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR28" role="doc-biblioref">28</a></span>] provide data-dependent generalization bounds for these approaches.</div></section>
<section class="Section3 RenderAsSection3" id="Sec39"><h4 class="Heading">Fine-Tuning a Frozen Model by Adapters</h4><p class="Para" id="Par254">A downside of fine-tuning for task-adoption is that new model parameters are needed for every task. <em class="EmphasisTypeItalic ">Task adapters</em><span id="ITerm221"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR84" role="doc-biblioref">84</a></span>] aim to mitigate this problem. The authors introduce adapter layers, which are inserted in a encoder block after the multi-head attention and the feedforward layer (<span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Equ7"><span class="RefSource">2.​7</span></a></span>). Now, to fine-tune transformer models to new tasks, instead of relearning all parameters, all weights of the network are frozen except for the adapter layers and the normalization layers. On tasks like GLUE this yields a significant reduction of parameters that need to be trained while preserving model quality.</p><p class="Para" id="Par255">Rather than having multiple adapters for different tasks, Stickland et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR197" role="doc-biblioref">197</a></span>] propose training a multitasking version of BERT that can be used for several tasks simultaneously. They add low-dimensional projected attention layers as bypass to BERT encoder blocks, which connect the input to layer-norm layers and the subsequent layer-norm layers. They sample data from the different tasks during training proportionally to the sizes of the respective training sets and use an annealing mechanism to converge towards equally distributed training samples by the end of the training. Their results surpass the results of a BERT<sub>BASE</sub> model.</p><p class="Para" id="Par256"><strong class="EmphasisTypeBold ">MAD-X</strong><span id="ITerm222"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR160" role="doc-biblioref">160</a></span>] is a framework to adapt multilingual models to arbitrary languages and tasks. The authors introduce language- and task-specific adapters, which consist of a linear down-projection to a small vector, a ReLU activation and a linear up-projection. The language specific adapters are trained with an MLM objective, while the rest of the model is frozen. The task-specific adapters are trained with the task-specific data, fixing the rest of the parameters. Finally, invertible adapters are added after the input embedding layer and before the output embedding layer to mitigate differences between the multilingual vocabulary and the target language vocabulary. MAD-X achieves <span class="EmphasisTypeSmallCaps ">Sota</span> for NER and common sense reasoning for a set of different languages.</p><p class="Para" id="Par257"><strong class="EmphasisTypeBold ">LoRA</strong><span id="ITerm223"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR85" role="doc-biblioref">85</a></span>] freezes the weights of the pre-trained model and adds trainable bypasses to the model, which consist of trainable matrix transformations to a short vector and to the full rank. This drastically reduces the number of trainable parameters (1/30 for GPT-3 and 1/100 for GPT-2) while achieving better results than with traditional fine-tuning on many NLP tasks. <em class="EmphasisTypeItalic ">AdapterHub</em><span id="ITerm224"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR161" role="doc-biblioref">161</a></span>] is a repository for adapters that as of writing contains around 380 adapters. AdapterHub is built on the Hugging Face transformer library for compatibility with existing transformer models.</p></section>
<section class="Section3 RenderAsSection3" id="Sec40"><h4 class="Heading">Fine-Tuning GPT-3</h4><div class="Para" id="Par258">GPT-3 is an extremely powerful Foundation Model, but it is not publicly available (Sect. <span class="InternalRef"><a href="#Sec3">3.1.2</a></span>). By using the API for fine-tuning GPT-3 with user-specific data [<span class="CitationRef"><a epub:type="biblioref" href="#CR123" role="doc-biblioref">123</a></span>], the model can be adapted to specific domain languages and particular tasks. This typically yields a higher quality than few-shot examples and prompt design described below. To fine-tune the 175B parameter model on a 1M token file for four epochs OpenAI charges about $120. The fine-tuning can be used in a number of ways [<span class="CitationRef"><a epub:type="biblioref" href="#CR123" role="doc-biblioref">123</a></span>]: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par259"><em class="EmphasisTypeItalic ">Completion</em>: Generate a completion for a prompt.</p></li><li><p class="Para" id="Par260"><em class="EmphasisTypeItalic ">Search</em>: Given a search query and a set of documents or labels, the model ranks each document with a score based on its semantic similarity to the query.</p></li><li><p class="Para" id="Par261"><em class="EmphasisTypeItalic ">Classification</em>: Input is a query and a set of labeled examples, e.g., <em class="EmphasisTypeItalic ">[“I am feeling awesome”, “Positive”]</em>. Then GPT-3 will predict the most probable label for the query. This can be used similar to BERT for any type of classification task.</p></li><li><p class="Para" id="Par262"><em class="EmphasisTypeItalic ">Answer</em>: Input is a question, a set of documents with background information, and some examples. Based on the information in the documents and the examples, an answer is generated. This is similar to the reading comprehension task of question answering (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec9"><span class="RefSource">6.​2</span></a></span>).</p></li><li><p class="Para" id="Par263"><em class="EmphasisTypeItalic ">Fine-tune</em>: Adapts GPT-3 to a specific domain text.</p></li><li><p class="Para" id="Par264"><em class="EmphasisTypeItalic ">Embeddings</em>: Get a vector of contextual embeddings for an input text for further processing or exploration.</p></li></ul></div> It can be assumed that GPT-3 and other Foundation Models like PaLM fine-tuned in this way will increase <span class="EmphasisTypeSmallCaps ">Sota</span> in many areas due to their comprehensive knowledge about language.</div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec41"><h3 class="Heading"><span class="HeadingNumber">3.6.3 </span>Creating Few-Shot Prompts</h3><p class="Para" id="Par265">For <em class="EmphasisTypeItalic ">zero-shot learning</em><span id="ITerm225"/><span id="ITerm226"/> the model just gets a task description or <em class="EmphasisTypeItalic ">prompt</em><span id="ITerm227"/>, e.g. <em class="EmphasisTypeItalic ">“Translate English to French: cheese =</em>&gt; ”, and directly generates the answer <em class="EmphasisTypeItalic ">“fromage”</em>. For <em class="EmphasisTypeItalic ">one-shot</em><span id="ITerm228"/><span id="ITerm229"/> or <em class="EmphasisTypeItalic ">few-shot learning</em><span id="ITerm230"/><span id="ITerm231"/> the model receives a task description as well as one or more examples, e.g. <em class="EmphasisTypeItalic ">“Translate English to French: sea otter =</em>&gt; <em class="EmphasisTypeItalic ">loutre de mer; cheese =</em>&gt; ”, which helps the model to find the answer <em class="EmphasisTypeItalic ">“fromage”</em>. This happens without training, the parameters of the model are not changed, and the model creates the answer based on the knowledge acquired during pre-training.</p><div class="Para" id="Par266">In this way, GPT-3 can be instructed by natural language prompts to generate short stories, songs, answers to questions, press releases, technical manuals, and more [<span class="CitationRef"><a epub:type="biblioref" href="#CR181" role="doc-biblioref">181</a></span>]. It can adapt its output texts to specific styles, personalities or ideologies. Here are some of the recommended prompts used for few-shot learning [<span class="CitationRef"><a epub:type="biblioref" href="#CR150" role="doc-biblioref">150</a></span>]: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par267">Summarization: the model receives a long story and the prompt <em class="EmphasisTypeItalic ">“tl;dr:”</em>.</p></li><li><p class="Para" id="Par268">Grammar correction <em class="EmphasisTypeItalic ">“Original: She no went to the market. Standard American English:”</em></p></li><li><p class="Para" id="Par269">Translation: <em class="EmphasisTypeItalic ">“English: I do not speak French. French: Je ne parle pas français. English: Where is the restroom?”</em> French:</p></li><li><p class="Para" id="Par270">Generate an outline for an essay: <em class="EmphasisTypeItalic ">“Create an outline for an essay about Walt Disney and his contributions to animation:</em></p><p class="Para ParaOneEmphasisChild" id="Par271"><em class="EmphasisTypeItalic ">I: Introduction”</em></p></li></ul></div> Figure <span class="InternalRef"><a href="#Fig22">3.22</a></span> shows the accuracy of “few-shot learning” for different GPT-3 model sizes and different numbers of given examples.<figure class="Figure" id="Fig22"><div class="MediaObject" id="MO23"><img alt="" aria-describedby="d64e9822" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig22_HTML.png" style="width:34.07em"/><div class="TextObject" id="d64e9822"><p class="Para" id="Par323">A line graph of accuracy percentage versus the number of examples in the prompt indicates the trends of natural language prompts and no prompts with 175, 13, and 1.3 billion parameters. All the lines have increasing trends.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.22</span><p class="SimplePara">The accuracy of few-shot learning of GPT-3 is increased by extending the model size as well as the number of presented examples [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>]. The task is to remove random symbols from a word. A natural language description of the task can support the model especially in the one-shot regime. Image reprinted with kind permission of the authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>, p. 4]</p></div></figcaption></figure></div><p class="Para" id="Par272">In a comprehensive survey Liu et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR125" role="doc-biblioref">125</a></span>] compile approaches to <span id="ITerm232">prompt design</span> to create prompts for language models that reliably generate the desired response. For example, when we want to recognize the sentiment of the text <em class="EmphasisTypeItalic ">“I missed the bus today.”</em>, we may insert the prompt <em class="EmphasisTypeItalic ">“I felt so</em> <em class="EmphasisTypeItalic ">”</em>, and use the language model to replace the blank. There are two types of prompts: <em class="EmphasisTypeItalic ">cloze prompts</em><span id="ITerm233"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR159" role="doc-biblioref">159</a></span>], which fill in the blanks of a textual string by an autoencoder model similar to BERT, and <em class="EmphasisTypeItalic ">prefix prompts</em><span id="ITerm234"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR117" role="doc-biblioref">117</a></span>], which continue a text by an autoregressive language model.</p><p class="Para" id="Par273">For prompt mining [<span class="CitationRef"><a epub:type="biblioref" href="#CR96" role="doc-biblioref">96</a></span>], for instance, a large number of sentences with phrases <em class="EmphasisTypeItalic ">x</em> and <em class="EmphasisTypeItalic ">y</em> are collected. Subsequently, prompts are generated using the words between <em class="EmphasisTypeItalic ">x</em> and <em class="EmphasisTypeItalic ">y</em>, or on the dependency path generated by parser. Another approach is based on paraphrasing existing prompts, for instance by translation to another language and back-translation. The probability of desired answers may be increased by gradient-based search [<span class="CitationRef"><a epub:type="biblioref" href="#CR192" role="doc-biblioref">192</a></span>] as demonstrated with the <em class="EmphasisTypeItalic ">AutoPrompt</em><span id="ITerm235"/> model. Alternative approaches are described in [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR245" role="doc-biblioref">245</a></span>]. It should be noted, however, that the output of a model instructed with few-shot prompts can be easily altered if an adversary adds some new prompts [<span class="CitationRef"><a epub:type="biblioref" href="#CR79" role="doc-biblioref">79</a></span>].</p><p class="Para" id="Par274">Instead of improving prompt tokens, which generate a desired output by the language model, one can optimize the input embeddings of some “virtual” tokens, such that the desired answer is created. The embeddings of this “continuous” prompt can be optimized by gradient descent while keeping the parameters of the language model fixed [<span class="CitationRef"><a epub:type="biblioref" href="#CR121" role="doc-biblioref">121</a></span>]. Lester et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR117" role="doc-biblioref">117</a></span>] apply this approach with a continuous prompt sequence of 100 tokens to the T5 transformer. On the <em class="EmphasisTypeItalic ">SuperGLUE</em><span id="ITerm236"/> benchmark they achieve the same performance of 90.5% as for fine-tuning T5. This demonstrates that prompt tuning becomes competitive with fine-tuning and is much better than few-shot instructions. Note that the effort for prompt tuning is much lower than for fine-tuning, as the number of parameters is much smaller. It would be interesting to see this technique applied to recent autoregressive models like GPT-3 or PaLM.</p></section>
<section class="Section2 RenderAsSection2" id="Sec42"><h3 class="Heading"><span class="HeadingNumber">3.6.4 </span>Thought Chains for Few-Shot Learning of Reasoning</h3><p class="Para" id="Par275">To improve the reasoning capabilities of language models, prompts can contain a <em class="EmphasisTypeItalic ">chain of thought</em><span id="ITerm237"/>, a sequence of short sentences that imitate the reasoning process a person might have when answering a question [<span class="CitationRef"><a epub:type="biblioref" href="#CR226" role="doc-biblioref">226</a></span>]. Two examples are shown in Fig. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Fig21"><span class="RefSource">2.​21</span></a></span>. The idea is that a chain of thought allows language models to split a multistep problem into intermediate steps that are solved one at a time, rather than solving an entire multistep problem in a single pass.</p><p class="Para" id="Par276">The approach has a number of advantages. First, the chain-of-thought approach enables a model to decompose complex reasoning tasks into simpler intermediate steps, which can be solved by the model. To solve an entire class of problems, only a few chains of thought need to be provided. Second, when a model performs the intermediate steps, it is easier to check where the model has introduced an error. This may give a clue how to improve the chain of thought. Chain of thought reasoning can be applied to symbolic manipulation, common sense reasoning and math tasks, and is potentially applicable to any task that humans can solve via language.</p><p class="Para" id="Par277">Prompts also do not need to be restricted to input-output pairs or explanations and can cover many arguments, including things to avoid, rules of thumb, reasoning chains, positive or negative examples. Mishra et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR138" role="doc-biblioref">138</a></span>] consider instructions for crowdworkers, which contain very detailed prescriptions how to solve a task. They compile a dataset of tasks, instructions and generated input-output pairs. Subsequently, they investigate how well models are able to generalize to similar tasks. The results show that PLMs benefit from instructions when evaluated in terms of generalization to unseen tasks (19% improvement). However, there is much room for improvement.</p><p class="Para" id="Par278">Du et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>] investigate few-shot learning theoretically. They investigate the case that a model is pre-trained on a number of tasks with a large training set and subsequently fine-tuned on a related task. They theoretically derive bounds on the required sample size for the fine-tuning task, which can be reduced when there is a good common representation.</p></section>
<section class="Section2 RenderAsSection2" id="Sec43"><h3 class="Heading"><span class="HeadingNumber">3.6.5 </span>Fine-Tuning Models to Execute Instructions</h3><p class="Para" id="Par279">Instead of querying autoregressive PLMs by few-shot instructions it is possible to fine-tune these models to execute instructions without additional examples.</p><p class="Para" id="Par280"><strong class="EmphasisTypeBold ">InstructGPT</strong><span id="ITerm238"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR151" role="doc-biblioref">151</a></span>] is a new version of GPT-3. It is optimized to follow instructions instead of predicting the probable next words. Instead of needing a series of examples, GPT-3 now directly executes an instruction, e.g. <em class="EmphasisTypeItalic ">“Write a short story about the moon and the stars:”</em>, and the model generates a plausible story. In a first trial a dataset of 13k pairs of instructions and completions was collected to adapt GPT-3. GPT-3 was fine-tuned using this data. However, the model did not adequately match the intended human preferences. Therefore, the model was modified using a different training approach.</p><p class="Para" id="Par281">To adjust GPT-3 a <em class="EmphasisTypeItalic ">reinforcement learning</em><span id="ITerm239"/> approach with human feedback was used<span id="ITerm240"/>. The <em class="EmphasisTypeItalic ">proximal policy optimization</em><span id="ITerm241"/> (PPO) [<span class="CitationRef"><a epub:type="biblioref" href="#CR186" role="doc-biblioref">186</a></span>] follows the policy gradient pattern. It approximates the conditional distribution <em class="EmphasisTypeItalic ">π</em>(<em class="EmphasisTypeItalic ">a</em><sub><em class="EmphasisTypeItalic ">t</em></sub>|<em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em></sub>;<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>) of actions <span class="InlineEquation" id="IEq42"><img alt="$$a_t\in \mathcal {A}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq42.png" style="width:3.32em"/></span> at step <em class="EmphasisTypeItalic ">t</em> conditional to the current observation <span class="InlineEquation" id="IEq43"><img alt="$$s_t\in \mathcal {S}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq43.png" style="width:3.06em"/></span> about the state of the environment and a vector <em><strong class="EmphasisTypeBoldItalic ">w</strong></em> of parameters. In usual reinforcement learning, the environment generates a reward and the algorithm tries to maximize the weighted sum of rewards. The gradient for this optimization (policy gradient) can be easily computed from the model. PPO computes an update at each step that minimizes the cost function while ensuring the deviation from the previous policy is relatively small [<span class="CitationRef"><a epub:type="biblioref" href="#CR186" role="doc-biblioref">186</a></span>].</p><div class="Para" id="Par282">The algorithm needs a numeric score to measure the quality of each generated sequence. To reduce the data necessary for optimization, a human can express preferences [<span class="CitationRef"><a epub:type="biblioref" href="#CR198" role="doc-biblioref">198</a></span>] between trajectories <em class="EmphasisTypeItalic ">τ</em> = (<em><strong class="EmphasisTypeBoldItalic ">y</strong></em>, <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>) for pairs of instructions <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> and generated text <em><strong class="EmphasisTypeBoldItalic ">y</strong></em>. Informally, the goal is to produce trajectories which are preferred by the human, while querying the human as little as possible. To achieve this goal, a reward function <span class="InlineEquation" id="IEq44"><img alt="$$r({\boldsymbol {y}},{\boldsymbol {x}})\in \mathbb {R}$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq44.png" style="width:5.19em"/></span> is postulated [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>] with the property that (<em><strong class="EmphasisTypeBoldItalic ">y</strong></em><sup>[1]</sup>, <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[1]</sup>) is preferred to (<em><strong class="EmphasisTypeBoldItalic ">y</strong></em><sup>[2]</sup>, <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[2]</sup>) if <em class="EmphasisTypeItalic ">r</em>(<em><strong class="EmphasisTypeBoldItalic ">y</strong></em><sup>[1]</sup>, <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[1]</sup>) &gt; <em class="EmphasisTypeItalic ">r</em>(<em><strong class="EmphasisTypeBoldItalic ">y</strong></em><sup>[2]</sup>, <em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>[2]</sup>). The original policy <em class="EmphasisTypeItalic ">π</em>(<em class="EmphasisTypeItalic ">a</em><sub><em class="EmphasisTypeItalic ">t</em></sub>|<em class="EmphasisTypeItalic ">s</em><sub><em class="EmphasisTypeItalic ">t</em></sub>;<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>) induces a conditional distribution <em class="EmphasisTypeItalic ">π</em>(<em><strong class="EmphasisTypeBoldItalic ">y</strong></em>|<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>;<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>). To construct this, the reward function <em class="EmphasisTypeItalic ">r</em>(<em><strong class="EmphasisTypeBoldItalic ">y</strong></em>, <em><strong class="EmphasisTypeBoldItalic ">x</strong></em>) is approximated by a deep neural network <span class="InlineEquation" id="IEq45"><img alt="$$\hat {r}({\boldsymbol {y}},{\boldsymbol {x}};\boldsymbol {u})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq45.png" style="width:4.44em"/></span> with parameter <em><strong class="EmphasisTypeBoldItalic ">u</strong></em>. The network is trained by three alternating steps (Fig. <span class="InternalRef"><a href="#Fig23">3.23</a></span>): <div class="OrderedList"><ol><li class="ListItem"><div class="ItemNumber">1.</div><div class="ItemContent"><div class="Para" id="Par283">The policy <em class="EmphasisTypeItalic ">π</em>(<em><strong class="EmphasisTypeBoldItalic ">y</strong></em>|<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>;<em><strong class="EmphasisTypeBoldItalic ">w</strong></em>) is used to generate set of trajectories {<em class="EmphasisTypeItalic ">τ</em><sup>1</sup>, …, <em class="EmphasisTypeItalic ">τ</em><sup><em class="EmphasisTypeItalic ">i</em></sup>}. The parameter <em><strong class="EmphasisTypeBoldItalic ">w</strong></em> is updated by reinforcement learning in order to maximize the reward <span class="InlineEquation" id="IEq46"><img alt="$$\hat {r}({\boldsymbol {y}},{\boldsymbol {x}};\boldsymbol {u})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq46.png" style="width:4.44em"/></span>.<figure class="Figure" id="Fig23"><div class="MediaObject" id="MO24"><img alt="" aria-describedby="d64e10302" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig23_HTML.png" style="width:28.75em"/><div class="TextObject" id="d64e10302"><p class="Para" id="Par324">An illustration lists 3 steps as follows. 1, train a supervised model. 2, train a reward model for ranking answers. 3, train a stepwise model by reinforcement learning to reproduce the ranking. Each step has multiple sub-steps.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.23</span><p class="SimplePara">InstructGPT is trained in three steps [<span class="CitationRef"><a epub:type="biblioref" href="#CR151" role="doc-biblioref">151</a></span>, p. 3]. First GPT-3 is fine-tuned on instructions and the corresponding completions. Then a reward model is generated by optimizing the selection of a completion for an instruction. Finally, a policy is trained to generate token by token of the answer with maximal reward. Credits for image parts in Table <span class="ExternalRef"><a href="https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab1"><span class="RefSource">A.​1</span></a></span></p></div></figcaption></figure></div></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">2.</div><div class="ItemContent"><p class="Para" id="Par284">Pairs of trajectories (<em class="EmphasisTypeItalic ">σ</em><sup>[1]</sup>, <em class="EmphasisTypeItalic ">σ</em><sup>[2]</sup>) from the {<em class="EmphasisTypeItalic ">τ</em><sup>1</sup>, …, <em class="EmphasisTypeItalic ">τ</em><sup><em class="EmphasisTypeItalic ">i</em></sup>} are selected and submitted to a human for comparison.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">3.</div><div class="ItemContent"><p class="Para" id="Par285">The parameters <em><strong class="EmphasisTypeBoldItalic ">u</strong></em> of the reward function <span class="InlineEquation" id="IEq47"><img alt="$$\hat {r}({\boldsymbol {y}},{\boldsymbol {x}};\boldsymbol {u})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq47.png" style="width:4.44em"/></span> are optimized to correspond to the comparisons collected from the human up to now.</p></div><div class="ClearBoth"> </div></li></ol></div> For a set of 33k instructions, a <em class="EmphasisTypeItalic ">reward model</em><span id="ITerm242"/><span class="InlineEquation" id="IEq48"><img alt="$$\hat {r}({\boldsymbol {y}},{\boldsymbol {x}};\boldsymbol {u})$$" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq48.png" style="width:4.44em"/></span> was built with 6B parameters, where <em><strong class="EmphasisTypeBoldItalic ">x</strong></em> is the instruction and <em><strong class="EmphasisTypeBoldItalic ">y</strong></em> a completion [<span class="CitationRef"><a epub:type="biblioref" href="#CR198" role="doc-biblioref">198</a></span>]. It selects the best completion from a small set of proposed completions. Proximal policy optimization (PPO) was used as reinforcement model [<span class="CitationRef"><a epub:type="biblioref" href="#CR151" role="doc-biblioref">151</a></span>, p. 41]. To avoid catastrophic forgetting (Sect. <span class="InternalRef"><a href="#Sec32">3.6.1</a></span>), pre-training samples were mixed into fine-tuning.</div><p class="Para" id="Par286">The reward model was then applied to create a final model by another reinforcement learning step. During this process, InstructGPT generates a completion for an instruction. The reward model calculates a reward and the policy is updated to approximate the preferences encoded in the reward model. By mimicking human utterances, the model implicitly learns human intentions and preferences. This process is called <em class="EmphasisTypeItalic ">alignment to human preferences</em><span id="ITerm243"/> and is extensively discussed by Askell et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>].</p><section class="Section3 RenderAsSection3" id="Sec44"><h4 class="Heading">InstructGPT Results</h4><p class="Para" id="Par287">The GPT-3 model with 175B parameters fined-tuned in a supervised way to the 13k instruction-completion examples was taken as the base model called SFT. The final completions were again scored by human raters [<span class="CitationRef"><a epub:type="biblioref" href="#CR151" role="doc-biblioref">151</a></span>]. The InstructGPT completions were preferred to the standard GPT-3 output in 85% of cases and to few-shot-GPT-3 in 71% of cases.</p><p class="Para" id="Par288">Specifically, raters found that InstructGPT attempts to follow the correct instruction in 92% of cases, compared to 85% for SFT and 75% for few-shot GPT-3 [<span class="CitationRef"><a epub:type="biblioref" href="#CR151" role="doc-biblioref">151</a></span>, p. 53]. In addition, InstructGPT follows explicit constraints in 50% of the cases, compared to 43% for SFT and 34% for SFT and 28% for few-shot GPT-3. Hallucinations were observed for 20% of the cases for InstructGPT compared to 16% for SFT and 50% for few-shot GPT-3. Finally, the raters found that the language use is appropriate for a customer assistant in 92% of the cases for InstructGPT, about 90% for SFT and about 85% for GPT-3 few-shot. InstructGPT was also evaluated on a few natural language benchmarks where it achieved very similar results to GPT-3 [<span class="CitationRef"><a epub:type="biblioref" href="#CR151" role="doc-biblioref">151</a></span>, p. 56].</p><p class="Para" id="Par289">It turned out that InstructGPT is able to generalize to unseen labeler preferences. Thus, InstructGPT does not simply adapt to the preferences of a few training labelers. In addition, InstructGPT produces slightly less toxic language than standard GPT-3. However, InstructGPT still makes simple mistakes, e.g., given an instruction with a false premise, the model sometimes incorrectly assumes the premise is true. Note that the results depend on the subjective preferences of the labelers.</p><p class="Para" id="Par290">Comparisons between alternatives are not necessarily the most effective approach to generate an improvement signal. For example, one could ask labelers to edit model responses to make them better, or generate critiques of model responses in natural language. There is also a vast space of options for designing interfaces for labelers to provide feedback to language models; this is an interesting human-computer interaction problem. The authors note that the cost of aligning GPT-3 to human preferences described above is just 1.6% of the cost spent to train GPT-3. Therefore, it seems to make sense to put more effort into alignment than into the mere enlargement of the models.</p><p class="Para" id="Par291">The results show that the InstructGPT techniques potentially make language models more helpful, truthful, and harmless. In a way InstructGPT works like an intelligent assistant for speech generation and information provision. However, the model is currently not fit for use in safety-critical applications, because failures cannot be ruled out. What is still missing is a comprehensive evaluation similar to Gopher or PaLM (Sect. <span class="InternalRef"><a href="#Sec3">3.1.2</a></span>) that shows the real utility of this approach. It can be expected that the combination of this approach with retrieval techniques as used for WebGPT (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec16"><span class="RefSource">6.​2.​3</span></a></span>) and Retro (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec16"><span class="RefSource">6.​2.​3</span></a></span>) will increase the performance, reliability, and correctness of InstructGPT.</p></section>
<section class="Section3 RenderAsSection3" id="Sec45"><h4 class="Heading">Instruction Tuning with FLAN</h4><div class="Para" id="Par292"><strong class="EmphasisTypeBold ">FLAN</strong><span id="ITerm244"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR227" role="doc-biblioref">227</a></span>] uses instruction tuning to improve the ability of the language model to respond to natural language prompts. The language model has to learn through supervision to perform tasks described by prompts, and to follow instructions, even for unfamiliar tasks (Fig. <span class="InternalRef"><a href="#Fig24">3.24</a></span>). The authors group 62 publicly available NLP datasets into twelve task clusters, e.g. “sentiment” “natural language inference”, “summarization”, etc. For each of the datasets they compose ten templates describing the task in natural language. Then an existing language model is fine-tuned to provide better answers to the prompts.<figure class="Figure" id="Fig24"><div class="MediaObject" id="MO25"><img alt="" aria-describedby="d64e10494" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig24_HTML.png" style="width:34.38em"/><div class="TextObject" id="d64e10494"><p class="Para" id="Par325">A block diagram exhibits a set of prompt inputs related to commonsense reasoning and translation. It indicates the translation input leads to the input of natural language inference. It also denotes the tasks of sentiment analysis and conference resolution and the F LAN response.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.24</span><p class="SimplePara">FLAN instruction tuning fine-tunes a pre-trained language models on a set of tasks with instructions of ten different templates (left). The trained model can be applied to unseen tasks by formulating prompts according to these templates (right). Image adapted from [<span class="CitationRef"><a epub:type="biblioref" href="#CR227" role="doc-biblioref">227</a></span>, p. 1] with kind permission of the authors</p></div></figcaption></figure></div><p class="Para" id="Par293">The approach was applied to a LaMDA-PT language model with 137B parameters using retrieval and filters (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec52"><span class="RefSource">6.​6.​3</span></a></span>). For 18 NLI tasks the FLAN model was compared to LaMDA-PT 137B, GPT-3 175B, and GLaM 64B. In 14 of 18 cases FLAN substantially improved the performance of its unmodified counterpart and achieved better results than the competitors, while in 4 cases it was surpassed by GLaM [<span class="CitationRef"><a epub:type="biblioref" href="#CR227" role="doc-biblioref">227</a></span>]. FLAN even outperforms few-shot GPT-3 by a large margin on a number of tasks.</p></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec46"><h3 class="Heading"><span class="HeadingNumber">3.6.6 </span>Generating Labeled Data by Foundation Models</h3><p class="Para" id="Par294">The performance of GPT-3 and other Foundation Models in few-shot learning enables the generation of new high-quality training data for other models. By <em class="EmphasisTypeItalic ">Unsupervised Data Generation</em><span id="ITerm245"/> (<em class="EmphasisTypeItalic ">UDG</em>) the creation of fine-tuning data for models of downstream tasks is possible that would otherwise be produced by manual human annotation. This approach is similar to Sect. <span class="ExternalRef"><a href="528393_1_En_4_Chapter.xhtml#Sec11"><span class="RefSource">4.​2.​3</span></a></span>.</p><div class="Para" id="Par295">The idea for data generation is to utilize the language model to learn the input-label relation based on the task description and a few sample input-label pairs [<span class="CitationRef"><a epub:type="biblioref" href="#CR225" role="doc-biblioref">225</a></span>]. Instead of generating and predicting a label for a classification task the language model has to create the input text using the output class and a task description as input. For a classification task like product reviews on Amazon, the approach is able to produce 10k new examples for each class, covering a much larger spectrum as the currently available labeled data. It turns out that up to 32 few-shot examples still increase the quality of the generated training data. Examples are shown in Fig. <span class="InternalRef"><a href="#Fig25">3.25</a></span>. The authors use an additional module to filter out noisy examples. In this approach, a given training example is removed if the trained classifier does not match its label with high probability.<figure class="Figure" id="Fig25"><div class="MediaObject" id="MO26"><img alt="" aria-describedby="d64e10546" src="../images/528393_1_En_3_Chapter/528393_1_En_3_Fig25_HTML.png" style="width:33.75em"/><div class="TextObject" id="d64e10546"><p class="Para" id="Par326">3 text boxes represents the prompts in amazon reviews on the left and the Copa common sense on the right. Each prompt exhibits its generated answer at the bottom.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.25</span><p class="SimplePara">New data can be generated by GPT-3 and other Foundation Models using the few-shot UDG strategy. Here the prompts for two examples, Amazon reviews and Copa common sense reasoning, and the generated answers are shown [<span class="CitationRef"><a epub:type="biblioref" href="#CR225" role="doc-biblioref">225</a></span>]</p></div></figcaption></figure></div><p class="Para" id="Par296">The T5-XXL encoder-decoder model fine-tuned on SuperGLUE data enhanced with UDG data is able to improve the overall accuracy on the SuperGLUE task for natural language understanding to 90.4% and is even able to beat DeBERTa with 90.3%. Moreover, the approach achieves very high performance scores on a list of text classification and sentiment analysis tasks [<span class="CitationRef"><a epub:type="biblioref" href="#CR225" role="doc-biblioref">225</a></span>].</p></section>
<section class="Section2 RenderAsSection2" id="Sec47"><h3 class="Heading"><span class="HeadingNumber">3.6.7 </span>Summary</h3><p class="Para" id="Par297">When pre-training Foundation Models on a big text collection and subsequent supervised fine-tuning on a small labeled dataset, PLMs achieved unprecedented performance on many NLP tasks. Fine-tuning has been shown to change model parameters only slightly and, in general, no catastrophic forgetting occurs. Usually, no overfitting is observed if fine-tuning is stopped after a few epochs. If necessary, there are some approaches to avoid overfitting.</p><p class="Para" id="Par298">Fine-tuning can be performed in different ways. It has been suggested to use an intermediate fine-tuning with a more related dataset before the final fine-tuning on the small dataset takes place. The results of such approaches have been mixed. Also, simultaneous fine-tuning to several tasks is possible. In some cases, it could improve performance. As an alternative, there are strategies to accelerate fine-tuning by meta-learning. To avoid that the full model is changed adapter layers can be defined, and only their parameters are adapted. This can drastically reduce the number of trainable parameters and nevertheless lead to good performance on the fine-tuning tasks. Finally, fine-tuning APIs have been recently provided for proprietary models like GPT-3.</p><p class="Para" id="Par299">Foundation Models like GPT-3 and PaLM can be instructed by prompts to solve specific tasks without training. A large number of different prompts has been collected to order the model to complete a task. InstructGPT is a new version of GPT-3 that directly takes instructions and provides the answers for a large spectrum of tasks. The model was customized to carry out the instructions by adapting to user judgments through reinforcement learning. Instruction tuning is a variant, where a Foundation Model is fine-tuned to provide improved answers to instructions for a number of tasks. It turns out that afterwards the model generates better answers even for unseen tasks.</p><p class="Para" id="Par300">Finally, big language models may be employed to generate high-quality training data for fine-tuning. Again, the few-shot learning technique is used to generate input texts for specific learning tasks. In this way, the scarce training data can be expanded and better fine-tuning results can be achieved.</p></section>
</section>
<div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a><p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p><p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p></div><aside aria-labelledby="Bib1Heading" class="Bibliography" id="Bib1"><div epub:type="bibliography" role="doc-bibliography"><div class="Heading" id="Bib1Heading">References</div><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">O. Agarwal, H. Ge, S. Shakeri, and R. Al-Rfou. “Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training”. Mar. 13, 2021. arXiv: <span class="EmphasisTypeSmallCaps ">2010.12688</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">A. Aghajanyan, A. Shrivastava, A. Gupta, N. Goyal, L. Zettlemoyer, and S. Gupta. “Better Fine-Tuning by Reducing Representational Collapse”. Aug. 6, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2008.03156</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">J. Ainslie, S. Ontanon, C. Alberti, P. Pham, A. Ravula, and S. Sanghai. “ETC: Encoding Long and Structured Data in Transformers”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2004.08483</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">A. Alvi. <em class="EmphasisTypeItalic ">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World fs Largest and Most Powerful Generative Language Model</em>. Microsoft Research. Oct. 11, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.microsoft.com/en-us/research/blog/using-deepspeed-andmegatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/"><span class="RefSource">https://​www.​microsoft.​com/​en-us/​research/​blog/​using-deepspeed-andmegatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/​</span></a></span> (visited on 11/12/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">A. Askell et al. “A General Language Assistant as a Laboratory for Alignment”. Dec. 9, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2112.00861 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">T. Bansal, R. Jha, T. Munkhdalai, and A. McCallum. “Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2009.08445</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">Y. Bansal, G. Kaplun, and B. Barak. “For Self-Supervised Learning, Rationality Implies Generalization, Provably”. 2020. arXiv: <span class="EmphasisTypeSmallCaps ">2010.08508</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">H. Bao et al. “Unilmv2: Pseudo-masked Language Models for Unified Language Model Pre-Training”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn</em>. PMLR, 2020, pp. 642–652.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">A. Bapna et al. <em class="EmphasisTypeItalic ">Building Machine Translation Systems for the Next Thousand Languages</em>. May 16, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2205.03983 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">I. Beltagy, M. E. Peters, and A. Cohan. “Longformer: The Long-Document Transformer”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2004.05150</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">benchmark. <em class="EmphasisTypeItalic ">GLUE Benchmark</em>. Aug. 5, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://gluebenchmark.com/"><span class="RefSource">https://​gluebenchmark.​com/​</span></a></span> (visited on 08/05/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">Y. Bengio, A. Courville, and P. Vincent. “Representation Learning: A Review and New Perspectives”. In: <em class="EmphasisTypeItalic ">IEEE Trans. Pattern Anal. Mach. Intell</em>. 35.8 (2013), pp. 1798–1828.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">J. Berant, A. Chou, R. Frostig, and P. Liang. “Semantic Parsing on Freebase from Question-Answer Pairs”. In: <em class="EmphasisTypeItalic ">Proc. 2013 Conf. Empir. Methods Nat. Lang. Process</em>. EMNLP 2013. Seattle, Washington, USA: Association for Computational Linguistics, Oct. 2013, pp. 1533–1544. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://aclanthology.org/D13-1160"><span class="RefSource">https://​aclanthology.​org/​D13-1160</span></a></span> (visited on 12/14/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">M. Bevilacqua and R. Navigli. “Breaking through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information”. In: <em class="EmphasisTypeItalic ">Proc Assoc. Comput. Linguist</em>. 2020, pp. 2854–2864.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak, and S. Hellmann. “DBpedia-A Crystallization Point for the Web of Data”. In: <em class="EmphasisTypeItalic ">J. Web Semant</em>. 7.3 (2009), pp. 154–165.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">S. Black, G. Leo, P. Wang, C. Leahy, and S. Biderman. <em class="EmphasisTypeItalic ">GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow</em>. Zenodo, Mar. 21, 2021. <span class="ExternalRef"><a href="https://doi.org/10.5281/zenodo.5297715"><span class="RefSource">https://​doi.​org/​10.​5281/​zenodo.​5297715</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">O. Bojar et al. “Findings of the 2014 Workshop on Statistical Machine Translation”. In: <em class="EmphasisTypeItalic ">Proc. Ninth Workshop Stat. Mach. Transl</em>. 2014, pp. 12–58.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. “Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge”. In: <em class="EmphasisTypeItalic ">Proc. 2008 ACM SIGMOD Int. Conf. Manag. Data</em>. 2008, pp. 1247–1250.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">R. Bommasani et al. “On the Opportunities and Risks of Foundation Models”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2108.07258</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko. “Translating Embeddings for Modeling Multi-Relational Data”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 26 (2013), pp. 2787–2795.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">S. Borgeaud et al. “Improving Language Models by Retrieving from Trillions of Tokens”. Dec. 8, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2112.04426 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">A. Borzunov et al. <em class="EmphasisTypeItalic ">Petals: Collaborative Inference and Fine-tuning of Large Models</em>. Sept. 2, 2022. <span class="ExternalRef"><a href="https://doi.org/10.48550/2209.01188"><span class="RefSource">https://​doi.​org/​10.​48550/​2209.​01188</span></a></span>. arXiv: <span class="EmphasisFontCategoryNonProportional ">2209.01188 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">G. Branwen. “GPT-3 Creative Fiction”. In: (June 19, 2020). <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.gwern.net/GPT-3"><span class="RefSource">https://​www.​gwern.​net/​GPT-3</span></a></span> (visited on 11/14/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">S. Brin and L. Page. “The Anatomy of a Large-Scale Hypertextual Web Search Engine”. In: <em class="EmphasisTypeItalic ">Comput. Netw. ISDN Syst</em>. 30.1-7 (1998), pp. 107–117.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">T. B. Brown et al. “Language Models Are Few-Shot Learners”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2005.14165</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">J. Casper. <em class="EmphasisTypeItalic ">What Is This Fork of Megatron-LM and Megatron-DeepSpeed</em>. BigScience Workshop, Oct. 25, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/bigscience-workshop/Megatron-DeepSpeed"><span class="RefSource">https://​github.​com/​bigscience-workshop/​Megatron-DeepSpeed</span></a></span> (visited on 10/25/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">D. Chen. <em class="EmphasisTypeItalic ">Openqa-Tutorial Danqi/Acl2020</em>. July 5, 2020. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/danqi/acl2020-openqa-tutorial"><span class="RefSource">https://​github.​com/​danqi/​acl2020-openqa-tutorial</span></a></span> (visited on 02/24/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">Q. Chen, C. Shui, and M. Marchand. “Generalization Bounds For Meta-Learning: An Information-Theoretic Analysis”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 34 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">T. Chen, J. Frankle, S. Chang, S. Liu, Y. Zhang, Z. Wang, and M. Carbin. “The Lottery Ticket Hypothesis for Pre-Trained Bert Networks”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2007.12223</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">W. Chen, Y. Su, X. Yan, and W. Y. Wang. “KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.02307</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">Z. Chi, L. Dong, S. Ma, S. H. X.-L. Mao, H. Huang, and F. Wei. “mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2104.08692</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">Z. Chi, L. Dong, F. Wei, W. Wang, X.-L. Mao, and H. Huang. “Cross-Lingual Natural Language Generation via Pre-Training.” In: <em class="EmphasisTypeItalic ">AAAI</em>. 2020, pp. 7570–7577.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">R. Child, S. Gray, A. Radford, and I. Sutskever. “Generating Long Sequences with Sparse Transformers”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1904.10509</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">K. Choromanski et al. “Rethinking Attention with Performers”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2009.14794</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">A. Chowdhery et al. “PaLM: Scaling Language Modeling with Pathways”. Apr. 5, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2204.02311 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. “Deep Reinforcement Learning from Human Preferences”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 30 (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">H. W. Chung, T. Févry, H. Tsai, M. Johnson, and S. Ruder. “Rethinking Embedding Coupling in Pre-Trained Language Models”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.12821</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">A. Clark et al. “Unified Scaling Laws for Routed Language Models”. Feb. 9, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2202.01169 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning. “Electra: Pre-training Text Encoders as Discriminators Rather than Generators”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2003.10555</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">A. Conneau, G. Lample, R. Rinott, A. Williams, S. R. Bowman, H. Schwenk, and V. Stoyanov. “XNLI: Evaluating Cross-lingual Sentence Representations”. Sept. 13, 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1809.05053</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">A. Conneau et al. “Unsupervised Cross-Lingual Representation Learning at Scale”. Apr. 8, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">1911.02116</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">A. D’Amour. <em class="EmphasisTypeItalic ">How Underspecification Presents Challenges for Machine Learning</em>. Google AI Blog. Oct. 18, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="http://ai.googleblog.com/2021/10/how-underspecificationpresents.html"><span class="RefSource">http://​ai.​googleblog.​com/​2021/​10/​how-underspecificati​onpresents.​html</span></a></span> (visited on 10/25/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">Y. Dai, S. Wang, N. N. Xiong, and W. Guo. “A Survey on Knowledge Graph Embedding: Approaches, Applications and Benchmarks”. In: <em class="EmphasisTypeItalic ">Electronics</em> 9.5 (2020), p. 750.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">Z. Dai, Z. Yang, Y. Yang, W. W. Cohen, J. Carbonell, Q. V. Le, and R. Salakhutdinov. “Transformer-XL: Language Modeling with Longer-Term Dependency, 2019”. In: <em class="EmphasisTypeItalic ">URL Httpsopenreview Netforum</em>. 2019.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">T. Dash, S. Chitlangia, A. Ahuja, and A. Srinivasan. “Incorporating Domain Knowledge into Deep Neural Networks”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2103.00180</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">L. de Alwis, A. Dissanayake, M. Pallewatte, K. Silva, and U. Thayasivam. “Survey on Semantic Table Interpretation”. In: (July 13, 2018). <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="http://semantic-web-journal.org/system/files/swj1946.pdf"><span class="RefSource">http://​semantic-web-journal.​org/​system/​files/​swj1946.​pdf</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">X. Deng, H. Sun, A. Lees, Y. Wu, and C. Yu. “Turl: Table Understanding through Representation Learning”. Dec. 3, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2006.14806</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">J. Devlin. <em class="EmphasisTypeItalic ">mBERT - Multilingual BERT</em>. GitHub. 2019. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/googleresearch/bert/blob/master/multilingual.md"><span class="RefSource">https://​github.​com/​googleresearch/​bert/​blob/​master/​multilingual.​md</span></a></span> (visited on 02/21/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. “Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding”. 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1810.04805</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">T. Dozat and C. D. Manning. “Deep Biaffine Attention for Neural Dependency Parsing”. 2016. arXiv: <span class="EmphasisFontCategoryNonProportional ">1611.01734</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">N. Du et al. “GLaM: Efficient Scaling of Language Models with Mixture-of-Experts”. Dec. 13, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2112.06905 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">S. S. Du, W. Hu, S. M. Kakade, J. D. Lee, and Q. Lei. “Few-Shot Learning via Learning the Representation, Provably”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2002.09434</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">Z. Du. <em class="EmphasisTypeItalic ">GLM. THUDM</em>, Dec. 14, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/THUDM/GLM"><span class="RefSource">https://​github.​com/​THUDM/​GLM</span></a></span> (visited on 12/17/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. “All NLP Tasks Are Generation Tasks: A General Pretraining Framework”. Mar. 18, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2103.10360 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. <em class="EmphasisTypeItalic ">GLM: General Language Model Pretraining with Autoregressive Blank Infilling</em>. Nov. 1, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://aclanthology.org/2022.acl-long.26/"><span class="RefSource">https://​aclanthology.​org/​2022.​acl-long.​26/​</span></a></span> (visited on 12/17/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">W. Fedus, B. Zoph, and N. Shazeer. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2101.03961</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">F. Feng, Y. Yang, D. Cer, N. Arivazhagan, and W. Wang. “Language-Agnostic BERT Sentence Embedding”. July 3, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2007.01852 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">C. Finn, P. Abbeel, and S. Levine. “Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn</em>. PMLR, 2017, pp. 1126–1135.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">Q. Fournier, G. M. Caron, and D. Aloise. “A Practical Survey on Faster and Lighter Transformers”. Mar. 26, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2103.14636 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">P. Ganesh et al. “Compressing Large-Scale Transformer-Based Models: A Case Study on Bert”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2002.11985</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">61.</div><div class="CitationContent" id="CR61">L. Gao et al. “The Pile: An 800GB Dataset of Diverse Text for Language Modeling”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2101.00027</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">62.</div><div class="CitationContent" id="CR62">T. Gao, A. Fisch, and D. Chen. “Making Pre-Trained Language Models Better Few-Shot Learners”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2012.15723</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">63.</div><div class="CitationContent" id="CR63">H. Gong, Y. Sun, X. Feng, B. Qin, W. Bi, X. Liu, and T. Liu. “Tablegpt: Few-shot Tableto-Text Generation with Table Structure Reconstruction and Content Matching”. In: <em class="EmphasisTypeItalic ">Proc. 28th Int. Conf. Comput. Linguist</em>. 2020, pp. 1978–1988.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">64.</div><div class="CitationContent" id="CR64">M. A. Gordon, K. Duh, and N. Andrews. “Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2002.08307</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">65.</div><div class="CitationContent" id="CR65">J. Gou, B. Yu, S. Maybank, and D. Tao. “Knowledge Distillation: A Survey”. Jan. 26, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2006.05525</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">66.</div><div class="CitationContent" id="CR66">N. Goyal, J. Du, M. Ott, G. Anantharaman, and A. Conneau. “Larger-Scale Transformers for Multilingual Masked Language Modeling”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2105.00572</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">67.</div><div class="CitationContent" id="CR67">A. Grover and J. Leskovec. “Node2vec: Scalable Feature Learning for Networks”. In: <em class="EmphasisTypeItalic ">Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discov. Data Min</em>. 2016, pp. 855–864.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">68.</div><div class="CitationContent" id="CR68">A. Gu, K. Goel, and C. Ré. “Efficiently Modeling Long Sequences with Structured State Spaces”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2111.00396</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">69.</div><div class="CitationContent" id="CR69">A. Gu, K. Goel, and C. Ré. <em class="EmphasisTypeItalic ">The Annotated S4</em>. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://srush.github.io/annotateds4/"><span class="RefSource">https://​srush.​github.​io/​annotateds4/​</span></a></span> (visited on 04/05/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">70.</div><div class="CitationContent" id="CR70">A. Gupta. “Diagonal State Spaces Are as Effective as Structured State Spaces”. 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2203.14343</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">71.</div><div class="CitationContent" id="CR71">S. Gururangan, A. Marasović, S. Swayamdipta, K. Lo, I. Beltagy, D. Downey, and N. A. Smith. “Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2004.10964</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">72.</div><div class="CitationContent" id="CR72">K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. “Realm: Retrieval-augmented Language Model Pre-Training”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2002.08909</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">73.</div><div class="CitationContent" id="CR73">C. Hawthorne et al. “General-Purpose, Long-Context Autoregressive Modeling with Perceiver AR”. 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2202.07765</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">74.</div><div class="CitationContent" id="CR74">J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, and J. Tang. “FastMoE: A Fast Mixture-of-Expert Training System”. Mar. 24, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2103.13262 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">75.</div><div class="CitationContent" id="CR75">P. He, J. Gao, and W. Chen. “Debertav3: Improving Deberta Using Electra-Style Pre-Training with Gradient-Disentangled Embedding Sharing”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2111.09543</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">76.</div><div class="CitationContent" id="CR76">P. He, X. Liu, J. Gao, and W. Chen. “DeBERTa: Decoding-enhanced BERT with Disentangled Attention”. Jan. 11, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2006.03654</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">77.</div><div class="CitationContent" id="CR77">W. D. Heaven. <em class="EmphasisTypeItalic ">This Know-It-All AI Learns by Reading the Entire Web Nonstop</em>. MIT Technology Review. Sept. 4, 2020. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.technologyreview.com/2020/09/04/1008156/knowledge-graph-ai-reads-web-machine-learning-natural-language-processing/"><span class="RefSource">https://​www.​technologyreview​.​com/​2020/​09/​04/​1008156/​knowledge-graph-ai-reads-web-machine-learning-natural-language-processing/​</span></a></span> (visited on 12/01/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">78.</div><div class="CitationContent" id="CR78">K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. “Teaching Machines to Read and Comprehend”. 2015. arXiv: <span class="EmphasisFontCategoryNonProportional ">1506.03340</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">79.</div><div class="CitationContent" id="CR79">A. Hern. “TechScape: AI’s Dark Arts Come into Their Own”. In: <em class="EmphasisTypeItalic ">The Guardian. Technology</em> (Sept. 21, 2022). <span class="EmphasisTypeSmallCaps ">issn</span>: 0261-3077. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.theguardian.com/technology/2022/sep/21/ais-dark-arts-come-into-their-own"><span class="RefSource">https://​www.​theguardian.​com/​technology/​2022/​sep/​21/​ais-dark-arts-come-into-their-own</span></a></span> (visited on 10/01/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">80.</div><div class="CitationContent" id="CR80">D. Hernandez, J. Kaplan, T. Henighan, and S. McCandlish. “Scaling Laws for Transfer”. Feb. 1, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2102.01293 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">81.</div><div class="CitationContent" id="CR81">J. Herzig, P. K. Nowak, T. Müller, F. Piccinno, and J. M. Eisenschlos. “Tapas: Weakly Supervised Table Parsing via Pre-Training”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2004.02349</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">82.</div><div class="CitationContent" id="CR82">G. Hinton, O. Vinyals, and J. Dean. “Distilling the Knowledge in a Neural Network”. 2015. arXiv: <span class="EmphasisFontCategoryNonProportional ">1503.02531</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">83.</div><div class="CitationContent" id="CR83">J. Hoffmann et al. “Training Compute-Optimal Large Language Models”. 2022. arXiv: 2203.15556.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">84.</div><div class="CitationContent" id="CR84">N. Houlsby et al. “Parameter-Efficient Transfer Learning for NLP”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn</em>. PMLR, 2019, pp. 2790–2799.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">85.</div><div class="CitationContent" id="CR85">E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, and W. Chen. “LoRA: Low- Rank Adaptation of Large Language Models”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2106.09685</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">86.</div><div class="CitationContent" id="CR86">J. Hu, S. Ruder, A. Siddhant, G. Neubig, O. Firat, and M. Johnson. “Xtreme: A Massively Multilingual Multi-Task Benchmark for Evaluating Cross-Lingual Generalisation”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn</em>. PMLR, 2020, pp. 4411–4421.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">87.</div><div class="CitationContent" id="CR87">Z. Hu, Y. Dong, K. Wang, K.-W. Chang, and Y. Sun. “Gpt-Gnn: Generative Pre-Training of Graph Neural Networks”. In: <em class="EmphasisTypeItalic ">Proc. 26th ACM SIGKDD Int. Conf. Knowl. Discov. Data Min</em>. 2020, pp. 1857–1867.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">88.</div><div class="CitationContent" id="CR88">H. Huang, Y. Liang, N. Duan, M. Gong, L. Shou, D. Jiang, and M. Zhou. “Unicoder: A Universal Language Encoder by Pre-Training with Multiple Cross-Lingual Tasks”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1909.00964</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">89.</div><div class="CitationContent" id="CR89">A. Iyer. <em class="EmphasisTypeItalic ">GPT-3’s Free Alternative GPT-Neo Is Something to Be Excited About</em>. Venture- Beat. May 15, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://venturebeat.com/2021/05/15/gpt-3s-free-alternative-gptneo-is-something-to-be-excited-about/"><span class="RefSource">https://​venturebeat.​com/​2021/​05/​15/​gpt-3s-free-alternative-gptneo-is-something-to-be-excited-about/​</span></a></span> (visited on 01/03/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">90.</div><div class="CitationContent" id="CR90">M. Iyyer, W.-t. Yih, and M.-W. Chang. “Search-Based Neural Structured Learning for Sequential Question Answering”. In: <em class="EmphasisTypeItalic ">Proc. 55th Annu. Meet. Assoc. Comput. Linguist. Vol. 1 Long Pap</em>. 2017, pp. 1821–1831.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">91.</div><div class="CitationContent" id="CR91">G. Izacard and E. Grave. “Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering”. In: <em class="EmphasisTypeItalic ">Proc. 16th Conf. Eur. Chapter Assoc. Comput. Linguist. Main Vol</em>. EACL 2021. Online: Association for Computational Linguistics, Apr. 1, 2021, pp. 874–880. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.aclweb.org/anthology/2021.eacl-main.74"><span class="RefSource">https://​www.​aclweb.​org/​anthology/​2021.​eacl-main.​74</span></a></span> (visited on 06/16/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">92.</div><div class="CitationContent" id="CR92">A. Jaegle, F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, and J. Carreira. “Perceiver: General Perception with Iterative Attention”. June 22, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2103.03206 [cs, eess]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">93.</div><div class="CitationContent" id="CR93">A. Jaegle et al. “Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs”. Aug. 2, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2107.14795</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">94.</div><div class="CitationContent" id="CR94">S. Ji, S. Pan, E. Cambria, P. Marttinen, and S. Y. Philip. “A Survey on Knowledge Graphs: Representation, Acquisition, and Applications”. In: <em class="EmphasisTypeItalic ">IEEE Trans. Neural Netw. Learn. Syst</em>. (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">95.</div><div class="CitationContent" id="CR95">H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and T. Zhao. “SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization”. In: <em class="EmphasisTypeItalic ">Proc. 58th Annu. Meet. Assoc. Comput. Linguist</em>. ACL 2020. Online: Association for Computational Linguistics, July 2020, pp. 2177–2190. <span class="ExternalRef"><a href="https://doi.org/10.18653/v1/2020.acl-main.197"><span class="RefSource">https://​doi.​org/​10.​18653/​v1/​2020.​acl-main.​197</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">96.</div><div class="CitationContent" id="CR96">Z. Jiang, F. F. Xu, J. Araki, and G. Neubig. “How Can We Know What Language Models Know?” In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 8 (2020), pp. 423–438.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">97.</div><div class="CitationContent" id="CR97">X. Jiao et al. “Tinybert: Distilling Bert for Natural Language Understanding”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1909.10351</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">98.</div><div class="CitationContent" id="CR98">M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy. “Spanbert: Improving Pre-Training by Representing and Predicting Spans”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 8 (2020), pp. 64–77.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">99.</div><div class="CitationContent" id="CR99">M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer. “Triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension”. 2017. arXiv: <span class="EmphasisFontCategoryNonProportional ">1705.03551</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">100.</div><div class="CitationContent" id="CR100">D. Jurafsky and J. H. Martin. <em class="EmphasisTypeItalic ">Speech and Language ProcessingAn Introduction to Natural Language Processing,Computational Linguistics, and Speech Recognition</em>. 3rd Draft. Jan. 12, 2022.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">101.</div><div class="CitationContent" id="CR101">R. E. Kalman. “A New Approach to Linear Filtering and Prediction Problems”. In: (1960).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">102.</div><div class="CitationContent" id="CR102">J. Kaplan et al. “Scaling Laws for Neural Language Models”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2001.08361</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">103.</div><div class="CitationContent" id="CR103">V. Karpukhin, B. Oğuz, S. Min, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. “Dense Passage Retrieval for Open-Domain Question Answering”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2004.04906</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">104.</div><div class="CitationContent" id="CR104">K. Karthikeyan, Z. Wang, S. Mayhew, and D. Roth. “Cross-Lingual Ability of Multilingual BERT: An Empirical Study”. Feb. 15, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">1912.07840</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">105.</div><div class="CitationContent" id="CR105">A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. “Transformers Are Rnns: Fast Autoregressive Transformers with Linear Attention”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn</em>. PMLR, 2020, pp. 5156–5165.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">106.</div><div class="CitationContent" id="CR106">P. Kharya and A. Alvi. <em class="EmphasisTypeItalic ">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model</em>. NVIDIA Developer Blog. Oct. 11, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://developer.nvidia.com/blog/using-deepspeed-andmegatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generativelanguage-model/"><span class="RefSource">https://​developer.​nvidia.​com/​blog/​using-deepspeed-andmegatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generativelangua​ge-model/​</span></a></span> (visited on 01/08/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">107.</div><div class="CitationContent" id="CR107">T. N. Kipf and M. Welling. “Semi-Supervised Classification with Graph Convolutional Networks”. 2016. arXiv: <span class="EmphasisFontCategoryNonProportional ">1609.02907</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">108.</div><div class="CitationContent" id="CR108">N. Kitaev, L. Kaiser, and A. Levskaya. “Reformer: The Efficient Transformer”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2001.04451</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">109.</div><div class="CitationContent" id="CR109">T. Kwiatkowski et al. “Natural Questions: A Benchmark for Question Answering Research”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 7 (2019), pp. 453–466.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">110.</div><div class="CitationContent" id="CR110">G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. “Race: Large-scale Reading Comprehension Dataset from Examinations”. 2017. arXiv: <span class="EmphasisFontCategoryNonProportional ">1704.04683</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">111.</div><div class="CitationContent" id="CR111">G. Lample and A. Conneau. “Cross-Lingual Language Model Pretraining”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1901.07291</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">112.</div><div class="CitationContent" id="CR112">G. Lample, A. Sablayrolles, M. Ranzato, L. Denoyer, and H. Jégou. “Large Memory Layers with Product Keys”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1907.05242</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">113.</div><div class="CitationContent" id="CR113">Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. “Albert: A Lite BERT for Self-Supervised Learning of Language Representations”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">1909.11942</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">114.</div><div class="CitationContent" id="CR114">J. Lee, M. Sung, J. Kang, and D. Chen. “Learning Dense Representations of Phrases at Scale”. Jan. 2, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2012.12624</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">115.</div><div class="CitationContent" id="CR115">O. Lehmberg, D. Ritze, R. Meusel, and C. Bizer. “A Large Public Corpus of Web Tables Containing Time and Context Metadata”. In: <em class="EmphasisTypeItalic ">Proc. 25th Int. Conf. Companion World Wide Web</em>. 2016, pp. 75–76.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">116.</div><div class="CitationContent" id="CR116">D. Lepikhin et al. “Gshard: Scaling Giant Models with Conditional Computation and Automatic Sharding”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2006.16668</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">117.</div><div class="CitationContent" id="CR117">B. Lester, R. Al-Rfou, and N. Constant. “The Power of Scale for Parameter-Efficient Prompt Tuning”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2104.08691</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">118.</div><div class="CitationContent" id="CR118">M. Lewis, M. Ghazvininejad, G. Ghosh, A. Aghajanyan, S. Wang, and L. Zettlemoyer. “Pre-Training via Paraphrasing”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2006.15020</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">119.</div><div class="CitationContent" id="CR119">M. Lewis et al. “Bart: Denoising Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation, and Comprehension”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">1910.13461</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">120.</div><div class="CitationContent" id="CR120">P. Li et al. “An Effective Self-Supervised Framework for Learning Expressive Molecular Global Representations to Drug Discovery”. In: <em class="EmphasisTypeItalic ">Brief Bioinform</em> 22.6 (Nov. 5, 2021), bbab109. <span class="EmphasisTypeSmallCaps ">issn</span>: 1477-4054. <span class="ExternalRef"><a href="https://doi.org/10.1093/bib/bbab109"><span class="RefSource">https://​doi.​org/​10.​1093/​bib/​bbab109</span></a></span>. pmid: <span class="EmphasisFontCategoryNonProportional ">33940598</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">121.</div><div class="CitationContent" id="CR121">X. L. Li and P. Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Generation”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2101.00190</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">122.</div><div class="CitationContent" id="CR122">O. Lieber, O. Sharir, B. Lentz, and Y. Shoham. “Jurassic-1: Technical Details and Evaluation”. In: (2021), p. 9. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf"><span class="RefSource">https://​uploads-ssl.​webflow.​com/​60fd4503684b4665​78c0d307/​61138924626a6981​ee09caf6_​jurassic_​tech_​paper.​pdf</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">123.</div><div class="CitationContent" id="CR123">R. Lim, M. Wu, and L. Miller. <em class="EmphasisTypeItalic ">Customizing GPT-3 for Your Application</em>. OpenAI. Dec. 14, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://openai.com/blog/customized-gpt-3/"><span class="RefSource">https://​openai.​com/​blog/​customized-gpt-3/​</span></a></span> (visited on 02/16/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">124.</div><div class="CitationContent" id="CR124">X. V. Lin, R. Socher, and C. Xiong. “Bridging Textual and Tabular Data for Cross-Domain Text-to-Sql Semantic Parsing”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2012.12627</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">125.</div><div class="CitationContent" id="CR125">P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. “Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2107.13586</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">126.</div><div class="CitationContent" id="CR126">Y. Liu et al. “Multilingual Denoising Pre-Training for Neural Machine Translation”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2001.08210</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">127.</div><div class="CitationContent" id="CR127">Y. Liu et al. “Roberta: A Robustly Optimized Bert Pretraining Approach”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1907.11692</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">128.</div><div class="CitationContent" id="CR128">Y. Liu, S. Pan, M. Jin, C. Zhou, F. Xia, and P. S. Yu. “Graph Self-Supervised Learning: A Survey”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2103.00111</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">129.</div><div class="CitationContent" id="CR129">F. Locatello, S. Bauer, M. Lucic, G. Raetsch, S. Gelly, B. Schölkopf, and O. Bachem. “Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn</em>. PMLR, 2019, pp. 4114–4124.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">130.</div><div class="CitationContent" id="CR130">A. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. “Learning Word Vectors for Sentiment Analysis”. In: <em class="EmphasisTypeItalic ">Proc. 49th Annu. Meet. Assoc. Comput. Linguist. Hum. Lang. Technol</em>. 2011, pp. 142–150.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">131.</div><div class="CitationContent" id="CR131">D. Mahajan et al. “Identification of Semantically Similar Sentences in Clinical Notes: Iterative Intermediate Training Using Multi-Task Learning”. In: <em class="EmphasisTypeItalic ">JMIR Med. Inform</em>. 8.11 (2020), e22508.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">132.</div><div class="CitationContent" id="CR132">S. McCandlish, J. Kaplan, D. Amodei, and O. D. Team. “An Empirical Model of Large-Batch Training”. 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1812.06162</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">133.</div><div class="CitationContent" id="CR133">A. Merchant, E. Rahimtoroghi, E. Pavlick, and I. Tenney. “What Happens To BERT Embeddings During Fine-tuning?” Apr. 29, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2004.14448</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">134.</div><div class="CitationContent" id="CR134">S. Merity, C. Xiong, J. Bradbury, and R. Socher. “Pointer Sentinel Mixture Models”. 2016. arXiv: <span class="EmphasisFontCategoryNonProportional ">1609.07843</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">135.</div><div class="CitationContent" id="CR135">T. Mikolov, K. Chen, G. Corrado, and J. Dean. “Efficient Estimation of Word Representations in Vector Space”. 2013. arXiv: <span class="EmphasisFontCategoryNonProportional ">1301.3781</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">136.</div><div class="CitationContent" id="CR136">T. Mikolov and G. Zweig. “Context Dependent Recurrent Neural Network Language Model”. In: <em class="EmphasisTypeItalic ">2012 IEEE Spok. Lang. Technol. Workshop SLT</em>. IEEE, 2012, pp. 234–239.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">137.</div><div class="CitationContent" id="CR137">G. A. Miller. “WordNet: A Lexical Database for English”. In: <em class="EmphasisTypeItalic ">Commun. ACM</em> 38.11 (1995), pp. 39–41.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">138.</div><div class="CitationContent" id="CR138">S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi. “Cross-Task Generalization via Natural Language Crowdsourcing Instructions”. Mar. 14, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2104.08773 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">139.</div><div class="CitationContent" id="CR139">M. Mitchell. <em class="EmphasisTypeItalic ">BigScience Large Open-science Open-access Multilingual Language Model</em>. July 6, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://huggingface.co/bigscience/bloom"><span class="RefSource">https://​huggingface.​co/​bigscience/​bloom</span></a></span> (visited on 10/25/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">140.</div><div class="CitationContent" id="CR140">M. Mosbach, M. Andriushchenko, and D. Klakow. “On the Stability of Fine-Tuning Bert: Misconceptions, Explanations, and Strong Baselines”. Mar. 25, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2006.04884</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">141.</div><div class="CitationContent" id="CR141">A. Mulyar, O. Uzuner, and B. McInnes. “MT-clinical BERT: Scaling Clinical Information Extraction with Multitask Learning”. In: <em class="EmphasisTypeItalic ">J. Am. Med. Inform. Assoc</em>. 28.10 (2021), pp. 2108–2115.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">142.</div><div class="CitationContent" id="CR142">S. Narang et al. “Do Transformer Modifications Transfer Across Implementations and Applications?” Sept. 10, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2102.11972 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">143.</div><div class="CitationContent" id="CR143">S. Narayan, S. B. Cohen, and M. Lapata. “Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization”. In: <em class="EmphasisTypeItalic ">Proc. 2018 Conf. Empir. Methods Nat. Lang. Process</em>. EMNLP 2018. Brussels, Belgium: Association for Computational Linguistics, Oct. 2018, pp. 1797–1807. <span class="ExternalRef"><a href="https://doi.org/10.18653/v1/D18-1206"><span class="RefSource">https://​doi.​org/​10.​18653/​v1/​D18-1206</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">144.</div><div class="CitationContent" id="CR144">M. Nayyeri, S. Vahdati, C. Aykul, and J. Lehmann. “5* Knowledge Graph Embeddings with Projective Transformations”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2006.04986</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">145.</div><div class="CitationContent" id="CR145">M. Nickel, V. Tresp, and H.-P. Kriegel. “A Three-Way Model for Collective Learning on Multi-Relational Data”. In: <em class="EmphasisTypeItalic ">Icml</em>. 2011.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">146.</div><div class="CitationContent" id="CR146">Y. Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, and D. Kiela. “Adversarial Nli: A New Benchmark for Natural Language Understanding”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1910.14599</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">147.</div><div class="CitationContent" id="CR147">S. J. Nowlan and G. E. Hinton. “Evaluation of Adaptive Mixtures of Competing Experts.” In: <em class="EmphasisTypeItalic ">NIPS</em>. Vol. 3. 1990, pp. 774–780.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">148.</div><div class="CitationContent" id="CR148">A. van den Oord et al. “Wavenet: A Generative Model for Raw Audio”. 2016. arXiv: <span class="EmphasisFontCategoryNonProportional ">1609.03499</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">149.</div><div class="CitationContent" id="CR149">OpenAi. <em class="EmphasisTypeItalic ">OpenAI API</em>. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://beta.openai.com"><span class="RefSource">https://​beta.​openai.​com</span></a></span> (visited on 11/14/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">150.</div><div class="CitationContent" id="CR150">OpenAi. <em class="EmphasisTypeItalic ">Prompt Examples for GPT-3</em>. Sept. 3, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://beta.openai.com/examples"><span class="RefSource">https://​beta.​openai.​com/​examples</span></a></span> (visited on 09/03/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">151.</div><div class="CitationContent" id="CR151">L. Ouyang et al. “Training Language Models to Follow Instructions with Human Feedback”. Jan. 31, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2203.02155</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">152.</div><div class="CitationContent" id="CR152">G. Paass and J. Kindermann. “Bayesian Classification Trees with Overlapping Leaves Applied to Credit-Scoring”. In: <em class="EmphasisTypeItalic ">Res. Dev. Knowl. Discov. Data Min</em>. Ed. by X. Wu, R. Ko tagiri, and K. B. Korb. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer, 1998, pp. 234–245. <span class="EmphasisTypeSmallCaps ">isbn</span>: 978-3-540-69768-8. <span class="ExternalRef"><a href="https://doi.org/10.1007/3-540-64383-4_20"><span class="RefSource">https://​doi.​org/​10.​1007/​3-540-64383-4_​20</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">153.</div><div class="CitationContent" id="CR153">V. Pan. “Fast Approximate Computations with Cauchy Matrices and Polynomials”. In: <em class="EmphasisTypeItalic ">Math. Comput</em>. 86.308 (2017), pp. 2799–2826.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">154.</div><div class="CitationContent" id="CR154">D. Paperno et al. “The LAMBADA Dataset: Word Prediction Requiring a Broad Discourse Context”. June 20, 2016. arXiv: <span class="EmphasisFontCategoryNonProportional ">1606.06031 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">155.</div><div class="CitationContent" id="CR155">P. Pasupat and P. Liang. “Compositional Semantic Parsing on Semi-Structured Tables”. 2015. arXiv: <span class="EmphasisFontCategoryNonProportional ">1508.00305</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">156.</div><div class="CitationContent" id="CR156">M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. “Deep Contextualized Word Representations”. In: <em class="EmphasisTypeItalic ">Proc. NAACL-HLT</em>. 2018, pp. 2227–2237.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">157.</div><div class="CitationContent" id="CR157">M. E. Peters, M. Neumann, R. L. Logan IV, R. Schwartz, V. Joshi, S. Singh, and N. A. Smith. “Knowledge Enhanced Contextual Word Representations”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1909.04164</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">158.</div><div class="CitationContent" id="CR158">F. Petroni. <em class="EmphasisTypeItalic ">LAMA: LAnguage Model Analysis</em>. Meta Research, 2020. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/facebookresearch/LAMA"><span class="RefSource">https://​github.​com/​facebookresearch​/​LAMA</span></a></span> (visited on 03/08/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">159.</div><div class="CitationContent" id="CR159">F. Petroni, T. Rocktäschel, P. Lewis, A. Bakhtin, Y. Wu, A. H. Miller, and S. Riedel. “Language Models as Knowledge Bases?” 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1909.01066</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">160.</div><div class="CitationContent" id="CR160">J. Pfeiffer, I. Vulic̀, I. Gurevych, and S. Ruder. “Mad-x: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2005.00052</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">161.</div><div class="CitationContent" id="CR161">J. Pfeiffer et al. “Adapterhub: A Framework for Adapting Transformers”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2007.07779</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">162.</div><div class="CitationContent" id="CR162">N. Poerner, U. Waltinger, and H. Schütze. “Bert Is Not a Knowledge Base (yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised Qa”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1911.03681</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">163.</div><div class="CitationContent" id="CR163">C. Poth, J. Pfeiffer, A. Rücklé, and I. Gurevych. “What to Pre-Train on? Efficient Intermediate Task Selection”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2104.08247</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">164.</div><div class="CitationContent" id="CR164">S. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and Y. Zhang. “CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes”. In: <em class="EmphasisTypeItalic ">Jt. Conf. EMNLP CoNLL-Shar. Task</em>. 2012, pp. 1–40.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">165.</div><div class="CitationContent" id="CR165">Y. Pruksachatkun et al. “Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding: When and Why Does It Work?” 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2005.00628</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">166.</div><div class="CitationContent" id="CR166">X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang. “Pre-Trained Models for Natural Language Processing: A Survey”. In: <em class="EmphasisTypeItalic ">Sci. China Technol. Sci</em>. 63.10 (June 23, 2021), pp. 1872–1897. <span class="EmphasisTypeSmallCaps ">issn</span>: 1674–7321, 1869–1900. <span class="ExternalRef"><a href="https://doi.org/10.1007/s11431-020-1647-3"><span class="RefSource">https://​doi.​org/​10.​1007/​s11431-020-1647-3</span></a></span>. arXiv: <span class="EmphasisFontCategoryNonProportional ">2003.08271</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">167.</div><div class="CitationContent" id="CR167">A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. “Language Models Are Unsupervised Multitask Learners”. In: <em class="EmphasisTypeItalic ">OpenAI blog</em> 1.8 (2019), p. 9.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">168.</div><div class="CitationContent" id="CR168">J. W. Rae et al. “Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher”. In: <em class="EmphasisTypeItalic ">ArXiv Prepr. ArXiv211211446</em> (Dec. 8, 2021), p. 118.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">169.</div><div class="CitationContent" id="CR169">J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap. “Compressive Transformers for Long-Range Sequence Modelling”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1911.05507</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">170.</div><div class="CitationContent" id="CR170">C. Raffel et al. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”. In: <em class="EmphasisTypeItalic ">J. Mach. Learn. Res</em>. 21.140 (2020), pp. 1–67.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">171.</div><div class="CitationContent" id="CR171">c. raffel. <em class="EmphasisTypeItalic ">C4 — TensorFlow Datasets</em>. TensorFlow. 2019. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.tensorflow.org/datasets/catalog/c4"><span class="RefSource">https://​www.​tensorflow.​org/​datasets/​catalog/​c4</span></a></span> (visited on 12/14/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">172.</div><div class="CitationContent" id="CR172">A. Raganato, Y. Scherrer, and J. Tiedemann. “Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2002.10260</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">173.</div><div class="CitationContent" id="CR173">P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. “Squad: 100,000+ Questions for Machine Comprehension of Text”. 2016. arXiv: <span class="EmphasisFontCategoryNonProportional ">1606.05250</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">174.</div><div class="CitationContent" id="CR174">H. Ren, H. Dai, Z. Dai, M. Yang, J. Leskovec, D. Schuurmans, and B. Dai. “Combiner: Full Attention Transformer with Sparse Computation Cost”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 34 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">175.</div><div class="CitationContent" id="CR175">J. Rodriguez. <em class="EmphasisTypeItalic ">Five Key Facts Wu Dao 2.0: The Largest Transformer Model Ever Built</em>. DataSeries. Sept. 21, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://medium.com/dataseries/five-key-facts-wu-dao-2-0-the-largest-transformer-model-ever-built-19316159796b"><span class="RefSource">https://​medium.​com/​dataseries/​five-key-facts-wu-dao-2-0-the-largest-transformer-model-ever-built-19316159796b</span></a></span> (visited on 12/12/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">176.</div><div class="CitationContent" id="CR176">A. Rogers, O. Kovaleva, and A. Rumshisky. “A Primer in {Bertology}: What We Know about How {BERT} Works”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 8 (2021), pp. 842–866.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">177.</div><div class="CitationContent" id="CR177">S. Roller, S. Sukhbaatar, A. Szlam, and J. Weston. “Hash Layers For Large Sparse Models”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2106.04426</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">178.</div><div class="CitationContent" id="CR178">A. Romero. <em class="EmphasisTypeItalic ">GPT-3 Scared You? Meet Wu Dao 2.0: A Monster of 1.75 Trillion Parameters</em>. Medium. June 8, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://towardsdatascience.com/gpt-3-scared-you-meet-wu-dao-2-0-a-monster-of-1-75-trillion-parameters-832cd83db484"><span class="RefSource">https://​towardsdatascien​ce.​com/​gpt-3-scared-you-meet-wu-dao-2-0-a-monster-of-1-75-trillion-parameters-832cd83db484</span></a></span> (visited on 07/29/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">179.</div><div class="CitationContent" id="CR179">C. Rosset. “Turing-Nlg: A 17-Billion-Parameter Language Model by Microsoft”. In: <em class="EmphasisTypeItalic ">Microsoft Blog — 13.02 2020</em> (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">180.</div><div class="CitationContent" id="CR180">A. Roy, M. Saffar, A. Vaswani, and D. Grangier. “Efficient Content-Based Sparse Attention with Routing Transformers”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2003.05997</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">181.</div><div class="CitationContent" id="CR181">A. Sabeti. <em class="EmphasisTypeItalic ">GPT-3: An AI That’s Eerily Good at Writing Almost Anything</em>. Arram Sabeti. July 9, 2020. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://arr.am/2020/07/09/gpt-3-an-ai-thats-eerily-good-at-writing-almostanything/"><span class="RefSource">https://​arr.​am/​2020/​07/​09/​gpt-3-an-ai-thats-eerily-good-at-writing-almostanything/​</span></a></span> (visited on 09/04/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">182.</div><div class="CitationContent" id="CR182">K. Sakaguchi, R. Le Bras, C. Bhagavatula, and Y. Choi. “Winogrande: An Adversarial Winograd Schema Challenge at Scale”. In: <em class="EmphasisTypeItalic ">Proc. AAAI Conf. Artif. Intell</em>. Vol. 34. 05. 2020, pp. 8732–8740.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">183.</div><div class="CitationContent" id="CR183">V. Sanh, L. Debut, J. Chaumond, and T. Wolf. “DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1910.01108</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">184.</div><div class="CitationContent" id="CR184">T. Schick and H. Schütze. “Exploiting Cloze Questions for Few-Shot Text Classification and Natural Language Inference”. Jan. 25, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2001.07676</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">185.</div><div class="CitationContent" id="CR185">T. Schick and H. Schütze. “It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners”. Apr. 12, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2009.07118</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">186.</div><div class="CitationContent" id="CR186">J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. “Proximal Policy Optimization Algorithms”. 2017. arXiv: <span class="EmphasisFontCategoryNonProportional ">1707.06347</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">187.</div><div class="CitationContent" id="CR187">S. Schuster, S. Gupta, R. Shah, and M. Lewis. “Cross-Lingual Transfer Learning for Multilingual Task Oriented Dialog”. 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1810.13327</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">188.</div><div class="CitationContent" id="CR188">J. Sevilla, L. Heim, A. Ho, T. Besiroglu, M. Hobbhahn, and P. Villalobos. <em class="EmphasisTypeItalic ">Compute Trends Across Three Eras of Machine Learning</em>. Mar. 9, 2022. <span class="ExternalRef"><a href="https://doi.org/10.48550/arXiv.2202.05924"><span class="RefSource">https://​doi.​org/​10.​48550/​arXiv.​2202.​05924</span></a></span>. arXiv: <span class="EmphasisFontCategoryNonProportional ">2202.05924 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">189.</div><div class="CitationContent" id="CR189">N. Shazeer. “GLU Variants Improve Transformer”. Feb. 12, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2002.05202 [cs, stat]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">190.</div><div class="CitationContent" id="CR190">S. Shen et al. “Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT.” In: <em class="EmphasisTypeItalic ">AAAI</em>. 2020, pp. 8815–8821.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">191.</div><div class="CitationContent" id="CR191">T. Shen, Y. Mao, P. He, G. Long, A. Trischler, and W. Chen. “Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2004.14224</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">192.</div><div class="CitationContent" id="CR192">T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh. “Autoprompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.15980</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">193.</div><div class="CitationContent" id="CR193">M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. “Megatron-Lm: Training Multi-Billion Parameter Language Models Using Model Parallelism”. In: <em class="EmphasisTypeItalic ">arXiv</em> (2019), arXiv—1909.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">194.</div><div class="CitationContent" id="CR194">K. Singla, D. Can, and S. Narayanan. “A Multi-Task Approach to Learning Multilingual Representations”. In: <em class="EmphasisTypeItalic ">Proc. 56th Annu. Meet. Assoc. Comput. Linguist. Vol. 2 Short Pap</em>. 2018, pp. 214–220.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">195.</div><div class="CitationContent" id="CR195">D. R. So, W. Mańke, H. Liu, Z. Dai, N. Shazeer, and Q. V. Le. “Primer: Searching for Efficient Transformers for Language Modeling”. Jan. 24, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2109.08668 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">196.</div><div class="CitationContent" id="CR196">K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu. “Mass: Masked Sequence to Sequence Pre-Training for Language Generation”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1905.02450</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">197.</div><div class="CitationContent" id="CR197">A. C. Stickland and I. Murray. “Bert and Pals: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn</em>. PMLR, 2019, pp. 5986–5995.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">198.</div><div class="CitationContent" id="CR198">N. Stiennon et al. “Learning to Summarize with Human Feedback”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 33 (2020), pp. 3008–3021.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">199.</div><div class="CitationContent" id="CR199">G. Stoica, E. A. Platanios, and B. Póczos. “Re-Tacred: Addressing Shortcomings of the Tacred Dataset”. In: <em class="EmphasisTypeItalic ">Proc. AAAI Conf. Artif. Intell</em>. Vol. 35. 15. 2021, pp. 13843–13850.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">200.</div><div class="CitationContent" id="CR200">F. M. Suchanek, G. Kasneci, and G. Weikum. “Yago: A Core of Semantic Knowledge”. In: <em class="EmphasisTypeItalic ">Proc. 16th Int. Conf. World Wide Web</em>. 2007, pp. 697–706.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">201.</div><div class="CitationContent" id="CR201">P. Sun. <em class="EmphasisTypeItalic ">Announcing ScaNN: Efficient Vector Similarity Search</em>. Google AI Blog. July 28, 2020. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="http://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html"><span class="RefSource">http://​ai.​googleblog.​com/​2020/​07/​announcing-scann-efficient-vector.​html</span></a></span> (visited on 02/18/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">202.</div><div class="CitationContent" id="CR202">T. Sun, Y. Shao, X. Qiu, Q. Guo, Y. Hu, X. Huang, and Z. Zhang. “CoLAKE: Contextualized Language and Knowledge Embedding”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.00309</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">203.</div><div class="CitationContent" id="CR203">Y. Sun et al. “Ernie: Enhanced Representation through Knowledge Integration”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1904.09223</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">204.</div><div class="CitationContent" id="CR204">Z. Sun, H. Yu, X. Song, R. Liu, Y. Yang, and D. Zhou. “MobileBERT: A Compact Task-Agnostic BERT for Resource-Limited Devices”. Apr. 14, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2004.02984</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">205.</div><div class="CitationContent" id="CR205">N. Tang et al. “RPT: Relational Pre-trained Transformer Is Almost All You Need towards Democratizing Data Preparation”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2012.02469</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">206.</div><div class="CitationContent" id="CR206">Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and C. Zheng. “Synthesizer: Rethinking Self-Attention in Transformer Models”. May 24, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2005.00743 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">207.</div><div class="CitationContent" id="CR207">Y. Tay, M. Dehghani, D. Bahri, and D. Metzler. “Efficient Transformers: A Survey”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2009.06732</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">208.</div><div class="CitationContent" id="CR208">Y. Tay, Z. Zhao, D. Bahri, D. Metzler, and D.-C. Juan. “HyperGrid Transformers: Towards A Single Model for Multiple Tasks”. In: <em class="EmphasisTypeItalic ">Int. Conf. Learn. Represent</em>. 2021.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">209.</div><div class="CitationContent" id="CR209">Y. Tay et al. “Long Range Arena: A Benchmark for Efficient Transformers”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2011.04006</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">210.</div><div class="CitationContent" id="CR210">N. Tripuraneni, M. Jordan, and C. Jin. “On the Theory of Transfer Learning: The Importance of Task Diversity”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 33 (2020), pp. 7852–7862.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">211.</div><div class="CitationContent" id="CR211">L. TriviaQA. <em class="EmphasisTypeItalic ">CodaLab - Competition</em>. Feb. 28, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://competitions.codalab.org/competitions/17208#results"><span class="RefSource">https://​competitions.​codalab.​org/​competitions/​17208#results</span></a></span> (visited on 02/28/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">212.</div><div class="CitationContent" id="CR212">A. Vaswani et al. “Attention Is All You Need”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2017, pp. 5998–6008.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">213.</div><div class="CitationContent" id="CR213">P. Verga, H. Sun, L. B. Soares, and W. W. Cohen. “Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2007.00849</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">214.</div><div class="CitationContent" id="CR214">D. Vrandečić and M. Krötzsch. “Wikidata: A Free Collaborative Knowledgebase”. In: <em class="EmphasisTypeItalic ">Commun. ACM</em> 57.10 (2014), pp. 78–85.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">215.</div><div class="CitationContent" id="CR215">K. Wali. <em class="EmphasisTypeItalic ">EleutherAI Launches GPT-NeoX-20B, the Biggest Public-Access Language Model</em>. Analytics India Magazine. Feb. 14, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://analyticsindiamag.com/eleutherailaunches-gpt-neox-20b-the-biggest-public-access-language-model/"><span class="RefSource">https://​analyticsindiama​g.​com/​eleutherailaunch​es-gpt-neox-20b-the-biggest-public-access-language-model/​</span></a></span> (visited on 02/23/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">216.</div><div class="CitationContent" id="CR216">J. Wallat, J. Singh, and A. Anand. “BERTnesia: Investigating the Capture and Forgetting of Knowledge in BERT”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.09313</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">217.</div><div class="CitationContent" id="CR217">A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding”. 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1804.07461</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">218.</div><div class="CitationContent" id="CR218">A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. “Glue: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding”. Feb. 22, 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1804.07461</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">219.</div><div class="CitationContent" id="CR219">A. Wang et al. “Superglue: A Stickier Benchmark for General-Purpose Language Understanding Systems”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2019, pp. 3266–3280.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">220.</div><div class="CitationContent" id="CR220">B. Wang. <em class="EmphasisTypeItalic ">EleutherAI - Text Generation Testing UI</em>. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://6b.eleuther.ai/"><span class="RefSource">https://​6b.​eleuther.​ai/​</span></a></span> (visited on 11/14/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">221.</div><div class="CitationContent" id="CR221">B. Wang. <em class="EmphasisTypeItalic ">Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX</em>. May 1, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/kingoflolz/mesh-transformerjax"><span class="RefSource">https://​github.​com/​kingoflolz/​mesh-transformerjax</span></a></span> (visited on 11/14/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">222.</div><div class="CitationContent" id="CR222">R. Wang et al. “K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters”. Dec. 28, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2002.01808</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">223.</div><div class="CitationContent" id="CR223">W. Wang et al. “Structbert: Incorporating Language Structures into Pre-Training for Deep Language Understanding”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1908.04577</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">224.</div><div class="CitationContent" id="CR224">X. Wang, T. Gao, Z. Zhu, Z. Liu, J. Li, and J. Tang. “KEPLER: A Unified Model for Knowledge Embedding and Pre-Trained Language Representation”. Nov. 23, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">1911.06136</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">225.</div><div class="CitationContent" id="CR225">Z. Wang, A. W. Yu, O. Firat, and Y. Cao. “Towards Zero-Label Language Learning”. Sept. 19, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2109.09193 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">226.</div><div class="CitationContent" id="CR226">J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. “Chain of Thought Prompting Elicits Reasoning in Large Language Models”. 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2201.11903</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">227.</div><div class="CitationContent" id="CR227">J. Wei et al. “Finetuned Language Models Are Zero-shot Learners”. In: <em class="EmphasisTypeItalic ">ICLR 2022</em> (2022), p. 46.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">228.</div><div class="CitationContent" id="CR228">X. Wei, Y. Hu, R. Weng, L. Xing, H. Yu, and W. Luo. “On Learning Universal Representations across Languages”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2007.15960</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">229.</div><div class="CitationContent" id="CR229">A. Williams, N. Nangia, and S. R. Bowman. “A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference”. 2017. arXiv: <span class="EmphasisFontCategoryNonProportional ">1704.05426</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">230.</div><div class="CitationContent" id="CR230">G. Wilson and D. J. Cook. “A Survey of Unsupervised Deep Domain Adaptation”. In: <em class="EmphasisTypeItalic ">ACM Trans. Intell. Syst. Technol. TIST</em> 11.5 (2020), pp. 1–46.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">231.</div><div class="CitationContent" id="CR231">G. I. Winata, A. Madotto, Z. Lin, R. Liu, J. Yosinski, and P. Fung. “Language Models Are Few-shot Multilingual Learners”. Sept. 15, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2109.07684</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">232.</div><div class="CitationContent" id="CR232">S. Wu and M. Dredze. “Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT”. In: <em class="EmphasisTypeItalic ">Proc. 2019 Conf. Empir. Methods Nat. Lang. Process. 9th Int. Jt. Conf. Nat. Lang. Process. EMNLP-IJCNLP</em>. EMNLP-IJCNLP 2019. Hong Kong, China: Association for Computational Linguistics, Nov. 2019, pp. 833–844. <span class="ExternalRef"><a href="https://doi.org/10.18653/v1/D19-1077"><span class="RefSource">https://​doi.​org/​10.​18653/​v1/​D19-1077</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">233.</div><div class="CitationContent" id="CR233">J. Xia, Y. Zhu, Y. Du, and S. Z. Li. “A Survey of Pretraining on Graphs: Taxonomy, Methods, and Applications”. 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2202.07893</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">234.</div><div class="CitationContent" id="CR234">W. Xiong, J. Du, W. Y. Wang, and V. Stoyanov. “Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1912.09637</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">235.</div><div class="CitationContent" id="CR235">L. Xue. <em class="EmphasisTypeItalic ">mT5-code: Multilingual T5</em>. Google Research, Feb. 25, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/google-research/multilingual-t5"><span class="RefSource">https://​github.​com/​google-research/​multilingual-t5</span></a></span> (visited on 02/26/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">236.</div><div class="CitationContent" id="CR236">L. Xue et al. “mT5: A Massively Multilingual Pre-Trained Text-to-Text Transformer”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.11934</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">237.</div><div class="CitationContent" id="CR237">I. Yamada, A. Asai, H. Shindo, H. Takeda, and Y. Matsumoto. “LUKE: Deep Contextualized Entity Representations with Entity-Aware Self-Attention”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.01057</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">238.</div><div class="CitationContent" id="CR238">J. Yang et al. “GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 34 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">239.</div><div class="CitationContent" id="CR239">Z. Yang, Z. Dai, R. Salakhutdinov, and W. W. Cohen. “Breaking the Softmax Bottleneck: A High-Rank RNN Language Model”. 2017. arXiv: <span class="EmphasisFontCategoryNonProportional ">1711.03953</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">240.</div><div class="CitationContent" id="CR240">Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le. “Xlnet: Generalized Autoregressive Pretraining for Language Understanding”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2019, pp. 5753–5763.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">241.</div><div class="CitationContent" id="CR241">P. Yin, G. Neubig, W.-t. Yih, and S. Riedel. “TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2005.08314</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">242.</div><div class="CitationContent" id="CR242">W. Yin. “Meta-Learning for Few-Shot Natural Language Processing: A Survey”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2007.09604</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">243.</div><div class="CitationContent" id="CR243">W. Yu, M. Jiang, Z. Hu, Q. Wang, H. Ji, and N. Rajani. “Knowledge-Enriched Natural Language Generation”. In: (Nov. 10, 2021), p. 6.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">244.</div><div class="CitationContent" id="CR244">W. Yu, C. Zhu, Z. Li, Z. Hu, Q. Wang, H. Ji, and M. Jiang. “A Survey of Knowledge-Enhanced Text Generation”. July 5, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2010.04389</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">245.</div><div class="CitationContent" id="CR245">W. Yuan, G. Neubig, and P. Liu. “Bartscore: Evaluating Generated Text as Text Generation”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 34 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">246.</div><div class="CitationContent" id="CR246">C. Yun, Y.-W. Chang, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. “<em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">n</em>) Connections Are Expressive Enough: Universal Approximability of Sparse Transformers”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2006.04862</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">247.</div><div class="CitationContent" id="CR247">M. Zaheer et al. “Big Bird: Transformers for Longer Sequences”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 33 (Jan. 8, 2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">248.</div><div class="CitationContent" id="CR248">W. Zeng et al. “PanGu-<em class="EmphasisTypeItalic ">α</em>: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2104.12369</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">249.</div><div class="CitationContent" id="CR249">B. Zhang and R. Sennrich. “Root Mean Square Layer Normalization”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1910.07467</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">250.</div><div class="CitationContent" id="CR250">J. Zhang, H. Zhang, C. Xia, and L. Sun. “Graph-Bert: Only Attention Is Needed for Learning Graph Representations”. Jan. 22, 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2001.05140 [cs, stat]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">251.</div><div class="CitationContent" id="CR251">J. Zhang, Y. Zhao, M. Saleh, and P. Liu. “Pegasus: Pre-training with Extracted Gap-Sentences for Abstractive Summarization”. In: <em class="EmphasisTypeItalic ">Int. Conf. Mach. Learn</em>. PMLR, 2020, pp. 11328–11339.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">252.</div><div class="CitationContent" id="CR252">L. Zhang. “Transfer Adaptation Learning: A Decade Survey”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1903.04687</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">253.</div><div class="CitationContent" id="CR253">S. Zhang et al. <em class="EmphasisTypeItalic ">OPT: Open Pre-trained Transformer Language Models</em>. May 5, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2205.01068 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">254.</div><div class="CitationContent" id="CR254">Y. Zhang, V. Zhong, D. Chen, G. Angeli, and C. D. Manning. “Position-Aware Attention and Supervised Data Improve Slot Filling”. In: <em class="EmphasisTypeItalic ">Proc. 2017 Conf. Empir. Methods Nat. Lang. Process</em>. 2017, pp. 35–45.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">255.</div><div class="CitationContent" id="CR255">Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu. “ERNIE: Enhanced Language Representation with Informative Entities”. June 4, 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1905.07129</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">256.</div><div class="CitationContent" id="CR256">Z. Zhang, F. Qi, Z. Liu, Q. Liu, and M. Sun. “Know What You Don’t Need: Single-Shot Meta-Pruning for Attention Heads”. In: <em class="EmphasisTypeItalic ">AI Open</em> 2 (2021), pp. 36–42.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">257.</div><div class="CitationContent" id="CR257">A. Zhavoronkov. <em class="EmphasisTypeItalic ">Wu Dao 2.0 - Bigger, Stronger, Faster AI From China</em>. Forbes. July 19, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.forbes.com/sites/alexzhavoronkov/2021/07/19/wu-dao-20biggerstronger-faster-ai-from-china/"><span class="RefSource">https://​www.​forbes.​com/​sites/​alexzhavoronkov/​2021/​07/​19/​wu-dao-20biggerstronger​-faster-ai-from-china/​</span></a></span> (visited on 07/29/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">258.</div><div class="CitationContent" id="CR258">C. Zhu, W. Ping, C. Xiao, M. Shoeybi, T. Goldstein, A. Anandkumar, and B. Catanzaro. “Long-Short Transformer: Efficient Transformers for Language and Vision”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 34 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">259.</div><div class="CitationContent" id="CR259">F. Zhu, W. Lei, C. Wang, J. Zheng, S. Poria, and T.-S. Chua. “Retrieving and Reading: A Comprehensive Survey on Open-Domain Question Answering”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2101.00774</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">260.</div><div class="CitationContent" id="CR260">F. Zhuang et al. “A Comprehensive Survey on Transfer Learning”. In: <em class="EmphasisTypeItalic ">Proc. IEEE</em> 109.1 (2020), pp. 43–76.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">261.</div><div class="CitationContent" id="CR261">B. Zoph et al. “Designing Effective Sparse Expert Models”. 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2202.08906</span>.</div></li></ol></div></aside></div></div></body></html>