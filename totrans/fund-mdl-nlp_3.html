<html><head></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="b978-3-031-23190-2_4"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">G. Paaß, S. Giesselbach</span><span class="ContextInformationBookTitles"><span class="BookTitle">Foundation Models for Natural Language Processing</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" lang="en">Artificial Intelligence: Foundations, Theory, and Algorithms</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-3-031-23190-2_4">https://doi.org/10.1007/978-3-031-23190-2_4</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">4. Knowledge Acquired by Foundation Models</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Gerhard Paaß</span><sup><a href="#Aff5">1</a> <span class="ContactIcon"> </span></sup> and </span><span class="Author"><span class="AuthorName">Sven Giesselbach</span><sup><a href="#Aff5">1</a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff5"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Knowledge Discovery Department, Team NLU, Fraunhofer Institute for Intelligent Analysis and Information Systems (IAIS), Sankt Augustin, Nordrhein-Westfalen, Germany</div></div><div class="ClearBoth"> </div></div></div><section class="Abstract" id="Abs1" lang="en" role="doc-abstract"><h2 class="Heading">Abstract</h2><p class="Para" id="Par1">During pre-training, a Foundation Model is trained on an extensive collection of documents and learns the distribution of words in correct and fluent language. In this chapter, we investigate the knowledge acquired by PLMs and the larger Foundation Models. We first discuss the application of Foundation Models to specific benchmarks to test knowledge in a large number of areas and examine if the models are able to derive correct conclusions from the content. Another group of tests assesses Foundation Models by completing text and by applying specific probing classifiers that consider syntactic knowledge, semantic knowledge, and logical reasoning separately. Finally, we investigate if the benchmarks are reliable and reproducible, i.e. whether they actually test the targeted properties and yield the same performance values when repeated by other researchers.</p></section><div class="KeywordGroup" lang="en"><div class="Heading">Keywords</div><span class="Keyword" epub:type="keyword">Knowledge in foundation models</span><span class="Keyword" epub:type="keyword">Common Sense knowledge</span><span class="Keyword" epub:type="keyword">Logical coherence</span><span class="Keyword" epub:type="keyword">Benchmark collections</span><span class="Keyword" epub:type="keyword">Reproducibility</span></div><!--End Abstract--><div class="Fulltext"><div class="Para" id="Par2">During pre-training, Pre-trained Language Models (PLMs) and the larger Foundation Models are trained on an extensive collection of documents and learn the distribution of words in correct and fluent language. During fine-tuning, the models are adapted to a specific task using the knowledge from the pre-training and requiring only a small set of manually labeled fine-tuning data. In this chapter, we investigate the knowledge acquired by these models by different types of tests: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par3">We first assess PLMs and Foundation Models by specific benchmarks to test knowledge in a large number of areas and examine if the models are able to derive correct conclusions from the content (Sect. <span class="InternalRef"><a href="#Sec1">4.1</a></span>). Usually these benchmark collections have an aggregated performance measure averaging over different tests. Benchmark tests can be accomplished by fine-tuning models to perform specific classification tasks or by few-shot querying Foundation Models.</p></li><li><p class="Para" id="Par4">Then we assess Foundation Models by completing text and by applying specific probing classifiers without adapting model parameters (Sect. <span class="InternalRef"><a href="#Sec7">4.2</a></span>). We separately consider syntactic knowledge, semantic knowledge and logical reasoning and demonstrate the achievements and deficits in different areas and for different model architectures.</p></li><li><p class="Para" id="Par5">Finally, we investigate if the benchmarks are reliable, i.e. actually test the targeted properties (Sect. <span class="InternalRef"><a href="#Sec13">4.3</a></span>). Moreover, we analyze if published benchmark results are reproducible and yield the same performance values if they are repeated by other researchers.</p></li></ul></div></div><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading"><span class="HeadingNumber">4.1 </span>Benchmark Collections</h2><p class="Para" id="Par6">In order to arrive at quantitative measures of common sense knowledge and commonsense reasoning, the community has compiled a number of benchmarks. These allow a standardized comparison of different aspects of natural language understanding and provide comparable scores for the strength and weaknesses of different PLMs. Benchmarks have been a key driver for the development of language models. A comprehensive collection of benchmarks and the corresponding leaderboards are provided by PapersWithCode [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>]. A survey of actual benchmarks is given by Storks et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>].</p><div class="Para" id="Par7">A fair comparison of model architectures requires that the number of parameters, the size of the training data, and the computing effort for training are similar. This has been extensively discussed in Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec25"><span class="RefSource">3.​5.​1</span></a></span>. Therefore, many authors conduct extensive ablation studies to adjust their training resources to a standard, e.g. to BERT as a “benchmark model”. This is really important, as it helps the reader to get an intuition for the impact of pre-training resources. Nevertheless, comparability is often hampered by two problems: <div class="OrderedList"><ol><li class="ListItem"><div class="ItemNumber">1.</div><div class="ItemContent"><p class="Para" id="Par8">Some training datasets, e.g. the BooksCorpus of BERT, are not publicly available.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">2.</div><div class="ItemContent"><p class="Para" id="Par9">These comparisons do not show the performance of a model when the size of data, the number of parameters, or the computing effort are increased.</p></div><div class="ClearBoth"> </div></li></ol></div> Therefore, statements like <em class="EmphasisTypeItalic ">“Model architecture A is superior to model architecture B on performing task X.”</em> in general are not valid, but have to be qualified [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>], e.g. “Model architecture <em class="EmphasisTypeItalic ">A</em> is superior to model architecture <em class="EmphasisTypeItalic ">B</em> on performing task <em class="EmphasisTypeItalic ">X</em>, when pre-trained on a small/large corpus of low/high quality data from domain <em class="EmphasisTypeItalic ">Y</em>  with computing effort <em class="EmphasisTypeItalic ">Z</em>.”</div><section class="Section2 RenderAsSection2" id="Sec2"><h3 class="Heading"><span class="HeadingNumber">4.1.1 </span>The GLUE Benchmark Collection</h3><div class="Para" id="Par10">To test the ability of PLMs to capture the content of a document, the GLUE (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec7"><span class="RefSource">2.​1.​5</span></a></span>) set of benchmarks has been developed. This is a collection of 9 benchmarks testing different aspects of <em class="EmphasisTypeItalic ">Natural Language Understanding</em><span id="ITerm1"/> (<em class="EmphasisTypeItalic ">NLU</em>). The joint performance is measured by a single score, which has the value 87.1 for human annotators. The tasks are described in detail by examples in Table <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Tab1"><span class="RefSource">2.​1</span></a></span>. It turns out that variants of BERT fine-tuned to the different GLUE-tasks can yield better results than people. The results are determined for the large variants of the models and shown in Table <span class="InternalRef"><a href="#Tab1">4.1</a></span>. <div class="Table" id="Tab1"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 4.1</span><p class="SimplePara">Results for the GLUE benchmark for four different models and human annotators. The best value of a PLM for each task is printed in bold [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>, p. 7]. Human scores better than all model scores are underlined</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/><col class="tcol5"/><col class="tcol6"/><col class="tcol7"/><col class="tcol8"/><col class="tcol9"/><col class="tcol10"/><col class="tcol11"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">CoLA</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">QQP</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MNLI m</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">SST-2</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">STS-B</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">QNLI</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">RTE</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">WNLI</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MRPC</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"> </th></tr><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Mcc</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Acc</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Acc</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Acc</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Corr</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Acc</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Acc</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Acc</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">F1</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"> </th></tr><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Grammar</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Paraphr.</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Entail</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Sentim.</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Similar</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Question</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Entail</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Coref</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Paraphr.</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Avg</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Human [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">66.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">80.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">92.0</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">97.8</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">92.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">91.2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">93.6</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">95.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">86.3</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">87.1</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BERT<sub>LARGE</sub></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">60.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">91.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">86.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">93.2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">90.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">92.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">70.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">65.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">88.0</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">84.1</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">RoBERTa<sub>LARGE</sub></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">68.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">92.2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">90.2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">96.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">92.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">93.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">86.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">89.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">90.9</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">88.8</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">XLNET<sub>LARGE</sub></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">69.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">92.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">90.8</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">97.0</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">92.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">94.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">85.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">92.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">90.8</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">89.2</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">DeBERTaV3<sub>LARGE</sub></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">75.3</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">93.0</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">91.8</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">96.9</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">93.0</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">96.0</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">92.7</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">–</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">92.2</strong></p></td><td style="text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">91.4</strong></p></td></tr></tbody></table></div></div><p class="Para" id="Par11">In the past years GLUE was routinely employed to demonstrate the NLU capabilities of PLMs. Currently, the best average value of 91.4 after fine-tuning was reached by DeBERTaV3 [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>] (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>). It uses separate embeddings for content and position and employs a corresponding disentangled attention mechanism. There are only three tasks where PLMs are worse than humans, but only by a small margin. Note that ensembles of several models often yield slightly better results. Nangia et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>] also measures the performance of human teams of 5 people. The numbers are not comparable as cases were excluded when the teams arrived at split judgment. Newer models such as PaLM use SuperGLUE instead of GLUE because GLUE is considered too simple.</p></section>
<section class="Section2 RenderAsSection2" id="Sec3"><h3 class="Heading"><span class="HeadingNumber">4.1.2 </span>SuperGLUE: An Advanced Version of GLUE</h3><div class="Para" id="Par12">Due to the progress in the last years, PLMs have reached human performance in most tasks and the GLUE is no longer able to discriminate between models. Therefore, the authors of GLUE proposed a more demanding test suite called <strong class="EmphasisTypeBold ">SuperGLUE</strong><span id="ITerm2"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>] as an advanced version of GLUE with eight challenging tasks. The tasks are similar to GLUE with longer contexts to consider. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par13"><em class="EmphasisTypeItalic ">BoolQ</em><span id="ITerm3"/> is a QA-task with questions collected from Google search and yes/no answers.</p></li><li><p class="Para" id="Par14"><em class="EmphasisTypeItalic ">CB</em><span id="ITerm4"/> is a textual entailment task.</p></li><li><p class="Para" id="Par15"><em class="EmphasisTypeItalic ">COPA</em><span id="ITerm5"/> is a causal reasoning task in which a system must determine either the cause or effect of a given premise from two possible choices.</p></li><li><p class="Para" id="Par16"><em class="EmphasisTypeItalic ">MultiRC</em><span id="ITerm6"/> is a QA task where each instance consists of a context passage, a question about that passage, and a list of possible answers.</p></li><li><p class="Para" id="Par17">In <em class="EmphasisTypeItalic ">ReCoRD</em><span id="ITerm7"/> each example consists of a news article and an article in which one entity is masked out. The system must predict the masked entity from a list of possible entities.</p></li><li><p class="Para" id="Par18"><em class="EmphasisTypeItalic ">RTE</em><span id="ITerm8"/> requires detecting whether a hypothesis is implied by a premise.</p></li><li><p class="Para" id="Par19"><em class="EmphasisTypeItalic ">WiC</em><span id="ITerm9"/> is a word sense disambiguation task, where for two given sentences the system has to determine if a polysemous word is used with the same sense in both sentences.</p></li><li><p class="Para" id="Par20"><em class="EmphasisTypeItalic ">WSC</em><span id="ITerm10"/> is the Winograd Schema Challenge, where the system has to determine the correct noun phrase represented by a pronoun.</p></li></ul></div></div><p class="Para" id="Par21">The performance again is measured by a single average score with a value of 89.8 for human annotators [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>].</p><div class="Para" id="Par22"><em class="EmphasisTypeItalic ">GPT-3</em><span id="ITerm11"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>] is a huge language model (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec3"><span class="RefSource">3.​1.​2</span></a></span>), which can be instructed to perform a task without fine-tuning (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec7"><span class="RefSource">3.​2</span></a></span>). With this few-shot learning GPT-3 achieved an average SuperGLUE score of only 71.8 as shown in Table <span class="InternalRef"><a href="#Tab2">4.2</a></span>. Obviously fine-tuning the specific tasks seems to be important. Recently a fine-tuned DeBERTa ensemble (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>) surpassed human performance on SuperGLUE with an average score of 90.3. The most difficult task is a comparison of word senses in two sentences (WiC), where an accuracy of about 77% can be reached. The autoregressive LM <em class="EmphasisTypeItalic ">PaLM</em><span id="ITerm12"/> 540B was fine-tuned on SuperGLUE and achieved an average of 90.4% on the test set [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>, p. 13]. The best average of 91.2% was obtained by the <em class="EmphasisTypeItalic ">ST-MoE</em><sub>32B</sub><span id="ITerm13"/> mixture-of-experts model (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec26"><span class="RefSource">3.​5.​2</span></a></span>) with 269B parameters [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>]. This shows that Foundation Models are able to analyze complex text semantics. <div class="Table" id="Tab2"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 4.2</span><p class="SimplePara">Results for the SuperGLUE benchmark on the test set for human annotators and five different models. The best value for each task is printed in bold and human values better than the model values are underlined. For GPT-3 few-shot values (FS) are reported, fine-tuned otherwise</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/><col class="tcol4"/><col class="tcol5"/><col class="tcol6"/><col class="tcol7"/><col class="tcol8"/><col class="tcol9"/><col class="tcol10"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BoolQ</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">CB</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">COPA</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">MultiRC</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">ReCoRD</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">RTE</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">WiC</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">WNLI</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"> </th></tr><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Acc</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Acc/F1</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Acc</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">F1a/EM</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">F1/EM</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">F1/EM</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Acc</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Acc</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"> </th></tr><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Model</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">QA y/n</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Entail</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Cause</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">QA mult.</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Entities</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Entail</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">WSD</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Coref</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Avg</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Human [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">89.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">95.8/98.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">100.0</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">81.8/51.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">91.7/91.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">93.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="EmphasisTypeUnderline ">80.0</span></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">100.0</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">89.8</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">BERT<sub>336M</sub> [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">77.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">83.6/75.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">70.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">70.0/24.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">72.0/71.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">71.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">69.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">64.3</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">69.0</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">GPT-3<sub>270B</sub> FS [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">76.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">75.6/52.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">92.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">75.4/30.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">91.1/90.2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">69.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">49.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">80.1</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">71.8</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">DeBERTA Ens. [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">90.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">94.9/97.2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">98.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">88.2/63.7</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">94.5/94.1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">93.2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">77.5</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">95.9</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">90.3</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PaLM<sub>540B</sub> [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">91.9</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">94.4/96.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">99.0</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">88.7/63.6</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">94.2/93.3</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">95.9</strong></p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">77.4</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">95.9</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">90.4</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">ST-MoE<sub>32B</sub> [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>]</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">92.4</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">96.9/98.0</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">99.2</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">89.6/65.8</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">95.1/94.4</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">93.5</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">77.7</strong></p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">96.6</strong></p></td><td style="text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">91.2</strong></p></td></tr></tbody></table></div></div><p class="Para" id="Par23">GLUE and SuperGLUE have been criticized, as the answers of the posed problems always can be reduced to a classification task and the systems do not have to formulate an answer in natural language. In addition, it turns out that the performance of PLMs is not very stable. It has been shown that the prediction of current models often change in an inconsistent way, if some words are replaced [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>]. If, for instance, in a sentiment analysis the input <em class="EmphasisTypeItalic ">“I love the flight”</em> is classified as <em class="EmphasisTypeItalic ">positive</em>, then <em class="EmphasisTypeItalic ">“I didn’t love the flight”</em> should not be classified as <em class="EmphasisTypeItalic ">neutral</em>. Ribeiro et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>] show that inconsistencies like this often occur. They developed the <strong class="EmphasisTypeBold ">CheckList</strong><span id="ITerm14"/> system (Sect. <span class="InternalRef"><a href="#Sec15">4.3.1</a></span>), which automatically generates test examples for probing a model.</p></section>
<section class="Section2 RenderAsSection2" id="Sec4"><h3 class="Heading"><span class="HeadingNumber">4.1.3 </span>Text Completion Benchmarks</h3><p class="Para" id="Par24">The task of an autoregressive language models is the reliable generation of the next word in a text. This has to obey grammatical correctness as well as semantic consistency. The <em class="EmphasisTypeItalic ">LAMBADA benchmark</em><span id="ITerm15"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>] is a good test to demonstrate this ability. It consists of about 10,000 passages from the BooksCorpus containing unpublished novels. The task is to predict the missing last word of the last sentence of each passage. Examples were filtered by humans to ensure that models need to take into account the full passage of at least 50 tokens to induce the final word.</p><p class="Para" id="Par25">An example is the passage <em class="EmphasisTypeItalic ">“Both its sun-speckled shade and the cool grass beneath were a welcome respite after the stifling kitchen, and I was glad to relax against the tree’s rough, brittle bark and begin my breakfast of buttery, toasted bread and fresh fruit. Even the water was tasty, it was so clean and cold. It almost made up for the lack of</em>  <em class="EmphasisTypeItalic ">.”</em>, where <em class="EmphasisTypeItalic ">“coffee”</em> is the missing target word to be predicted. Examples which could be easily predicted by simpler language models were omitted. Examples were only selected, if the target word could be predicted by humans from the full passage but not from the last sentence.</p><p class="Para" id="Par26">The GPT-3<sub>175B</sub> autoregressive language model [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>] predicted the last word with 76.2% [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>, p. 12]. PaLM<sub>540B</sub> with few-shot instructions could increase the accuracy to 89.7 [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>, p. 79]. This means that in nearly nine of ten cases, the predicted word was exactly the missing word in the test data.</p><p class="Para" id="Par27">Another relevant benchmark for language modeling is <em class="EmphasisTypeItalic ">WikiText-103</em><span id="ITerm16"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>] of 28k articles from Wikipedia with 103M tokens. If large Foundation Models are applied to this corpus the following perplexities result: GPT-2<sub>1.7B</sub> 17.5 [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>], Megatron-LM 10.8 [<span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>], Gopher<sub>280B</sub> 8.1 [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>, p. 61]. Recently a small Retro<sub>1.8B</sub> model with retrieval could reduce this perplexity to 3.9 [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>, p. 12]. Note that there might be a partial overlap of Wikitext 103 with Retro’s training data not caught by deduplication.</p></section>
<section class="Section2 RenderAsSection2" id="Sec5"><h3 class="Heading"><span class="HeadingNumber">4.1.4 </span>Large Benchmark Collections</h3><p class="Para" id="Par28">Recently large autoregressive language models like GPT-3, Gopher, and PaLM have been developed, which are trained on extremely large document collections with hundreds of billions of tokens. The models should perform well across a wide range of tasks. Therefore, instead of the limited GLUE benchmarks a large number of benchmarks covering many aspects of possible applications are used to evaluate their performance.</p><div class="Para" id="Par29">A frequent opinion is that current benchmarks are insufficient and “saturate”, “have artifacts”, and are “overfitted by researchers”. Bowman et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>] argue that “evaluation for many natural language understanding (NLU) tasks is broken”. They complain that there are systems at the top of the leaderboards which fail in simple test cases (cf. [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>]). As a consequence they formulate four requirements on new benchmarks: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par30">A model should only reach good performance on the benchmark if it also has a good performance on actual applications.</p></li><li><p class="Para" id="Par31">The annotation of benchmarks should be accurate and not ambiguous (e.g. 36% of the answers in Natural Questions are ambiguous).</p></li><li><p class="Para" id="Par32">The benchmarks should be large and challenging enough to detect relevant performance differences between models.</p></li><li><p class="Para" id="Par33">Benchmarks should reveal plausibly harmful social biases in systems, and should not encourage the creation of biases.</p></li></ul></div></div><p class="Para" id="Par34">They summarize some promising developments that could support these challenges, including data collection involving both crowdworkers and domain experts, and larger-scale data validation.</p><p class="Para" id="Par35">To address this criticism, two comprehensive collections of benchmarks have been defined. The <em class="EmphasisTypeItalic ">Massive Multitask Language Understanding</em><span id="ITerm17"/> (MMLU) benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>] emulates human exams with multiple choice questions, each with four responses. In addition to logical and mathematical reasoning it tests a model’s ability across a wide range of academic subjects from computer science to history and law. The other collection is the <em class="EmphasisTypeItalic ">BIG-bench</em><span id="ITerm18"/> collaborative benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>], designed to evaluate language interpretation aspects like reading comprehension, question answering, world understanding, etc. Both benchmark collections include more than a hundred tasks.</p><div class="Para" id="Par36">The <em class="EmphasisTypeItalic ">Gopher</em><span id="ITerm19"/> model with 280B parameters together with alternatives like GPT-3, Jurassic-1, and Megatron-Turing NLG (all discussed in Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec3"><span class="RefSource">3.​1.​2</span></a></span>) were tested on these and other benchmarks. Note that this was done with a total of 152 benchmarks described in Table <span class="InternalRef"><a href="#Tab3">4.3</a></span>. Gopher shows an improvement on 100 of 124 tasks (81%) compared to the previous <span class="EmphasisTypeSmallCaps ">Sota</span> scores. In language modeling (next word prediction) Gopher improves <span class="EmphasisTypeSmallCaps ">Sota</span> for 10 of 19 benchmarks. Note that all benchmark results were not obtained after fine-tuning but by zero-shot or few-shot learning. <div class="Table" id="Tab3"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 4.3</span><p class="SimplePara">Groups of evaluation benchmarks for Gopher and related models [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>, p. 8]</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1"/><col class="tcol2"/><col class="tcol3"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Task group</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"># Tasks</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Examples</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Language modeling</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">20</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">WikiText-103, The Pile: PG-19, arXiv, FreeLaw, …</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Reading comprehension</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">3</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">RACE-m, RACE-h, LAMBADA</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Fact checking</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">3</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">FEVER (2-way &amp; 3-way), MultiFC</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Question answering</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">3</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Natural questions, TriviaQA, TruthfulQA</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Common sense</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">4</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">HellaSwag, Winogrande, PIQA, SIQA</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Massive multitask language understanding (MMLU) [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>]</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "><p class="SimplePara">57</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">High school chemistry, astronomy, clinical knowledge, social science, math, …</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">BIG-bench [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>]</p></td><td style="border-right: 0.5pt solid ; "><p class="SimplePara">62</p></td><td style="text-align: left;"><p class="SimplePara">Causal judgement, epistemic reasoning, temporal sequences, logic, math, code, social reasoning, …</p></td></tr></tbody></table></div></div><div class="Para" id="Par37">The distribution Gopher accuracies for thematic groups are shown in Fig. <span class="InternalRef"><a href="#Fig1">4.1</a></span>. Gopher is able to increase <span class="EmphasisTypeSmallCaps ">Sota</span> for 4 out of 7 math tasks, 5 out of 9 common sense tasks, 9 out of 12 logical reasoning tasks, 22 out of 24 fact checking and general knowledge tasks, all 24 STEM (Science Technology Engineering Mathematics) and medicine tasks, all 15 humanities and ethics task, and 10 out of 11 reading comprehension tasks. The average accuracies for common sense and general knowledge are about 50%, indicating that some knowledge exists but can be improved. Among these tests were benchmarks on logical reasoning, which, for instance, include “Formal Fallacies Syllogisms Negation” or “Logical Fallacy Detection”. Only two of the 19 benchmarks achieved an accuracy of more than 60% [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>, p. 58], indicating that even for this large model correct reasoning is a major obstacle. Obviously this spectrum of evaluation gives a deep insight into the capabilities of the compared models. It can be expected that the new Retro model (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec15"><span class="RefSource">6.​2.​3</span></a></span>), which performs retrieval during language generation, will improve these results.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><img alt="" aria-describedby="d64e1503" src="../images/528393_1_En_4_Chapter/528393_1_En_4_Fig1_HTML.png" style="width:34.48em"/><div class="TextObject" id="d64e1503"><p class="Para" id="Par109">A box plot of accuracy versus the different groups. General knowledge ranges between 40 and 75, and has more than 50 % of the median value. Humanities and Medicine have less range and mark one outliner for humanities below lower whisker. Medicine marks one outliners for upper and lower whiskers.</p></div></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 4.1</span><p class="SimplePara">Accuracies in percent of different groups covering 152 different benchmarks evaluated for the Gopher model [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>, p. 57]. The 25% and 75% percentiles are given by the box, and the inner line is the median. The outside lines indicate variability outside the upper and lower quartiles</p></div></figcaption></figure></div><p class="Para" id="Par38">The <em class="EmphasisTypeItalic ">PaLM</em><span id="ITerm20"/> autoregressive language model with 580B parameters [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>, p. 15] recently was evaluated with the BIG-bench benchmark. On the 150 tasks, PaLM with 5-shot prompts achieved an normalized average score of 46%, which was better than the average human score of 39%. However, the best human experts have a score of about 77%. The detailed results for the different BIG benchmark areas are not yet available. On a subset of 58 BIG-tasks, which were also used by prior models, PaLM obtained a 5-shot normalized score of about 55%, again above the human average of 49%, outperforming Chinchilla (47%) and Gopher (30%). GPT-3 achieved a 1-shot performance of 16% on the 58 tasks. In general Foundation Models like Gopher and PaLM with several hundred billion parameters have ‘dramatically better’ results on BIG than smaller models, even if the model architecture is not fundamentally different [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>]. In this respect Foundation Models show a qualitatively new behavior.</p><p class="Para" id="Par39">Researchers at Google have proposed to use the BIG-bench benchmark with currently 200 tasks as a replacement for the Turing test for “intelligence” [<span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>]. In this way the knowledge of an AI-System can be checked at a large scale. Recently, a Google engineer published a dialog [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>] with the LaMDA language model (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec52"><span class="RefSource">6.​6.​3</span></a></span>). In his view this indicates that LaMDA is “sentient”. However, this aspect of human intelligence is not checked by knowledge and reasoning tests such as BIG and requires the development of new types of tests.</p></section>
<section class="Section2 RenderAsSection2" id="Sec6"><h3 class="Heading"><span class="HeadingNumber">4.1.5 </span>Summary</h3><p class="Para" id="Par40">Benchmark collections are a popular way to demonstrate the superiority of a Pre-trained Language Model for specific tasks. To show the merits of an architecture, however, also the number of parameters, the size of training data, and the computing effort has to be reported and compared, because these numbers also affect the model performance.</p><p class="Para" id="Par41">The GLUE benchmark collection of nine language understanding tasks has demonstrated the considerable progress of PLMs during the last years. It tests the ability of PLMs to detect paraphrases, coreference relations, logical entailments and grammatical correctness. Meanwhile, the average accuracy exceeds the average human performance. The similar, more challenging SuperGLUE benchmark suite has been introduced, where human performance is also surpassed. For autoregressive language models the LAMBADA benchmark requires an impressive ability to determine the most probable last word of a paragraph. Current models like PaLM are able to predict the last word with an accuracy of nearly 90% demonstrating its ability to capture the flow of arguments.</p><p class="Para" id="Par42">Foundation Models are usually tested by extensive standardized test collections covering many aspects like common sense knowledge, emotional intelligence, logical reasoning, or social sciences. Recent Foundation Models like Gopher and PaLM, with several hundred billion parameters, have been able to achieve performance better than that the human average and ‘dramatically better’ than smaller models. However, these models still have a lower accuracy than human experts. Although the benchmarks are very expressive, they do not take into account the societal impact of the models and are unable to detect features like self-awareness and sentience.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec7"><h2 class="Heading"><span class="HeadingNumber">4.2 </span>Evaluating Knowledge by Probing Classifiers</h2><p class="Para" id="Par43">In this section, we examine the extent to which PLMs acquire different types of knowledge. We discuss the covered knowledge for the small BERT model and later review the improvements for foundation models such as GPT-3 and PaLM. First, we consider their syntactic knowledge of correct language. Then, we investigate how much common sense knowledge is represented by PLMs. Finally, we explore whether the output produced by PLMs is logically consistent.</p><section class="Section2 RenderAsSection2" id="Sec8"><h3 class="Heading"><span class="HeadingNumber">4.2.1 </span>BERT’s Syntactic Knowledge</h3><p class="Para" id="Par44">We discuss the syntactic knowledge incorporated in PLMs using BERT as an example. In the course of pre-training BERT is able to capture <em class="EmphasisTypeItalic ">syntactic knowledge</em><span id="ITerm21"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>]. Embeddings can encode information about parts of speech, syntactic phrases and syntactic roles. Probing classifiers can predict part-of-speech tags and supersense information with an accuracy of 85% [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>]. Obviously, this information has to be encoded in BERT’s final embeddings. BERT also has knowledge of subject-verb agreement [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>] and semantic roles [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>]. It is also possible to extract dependency trees and syntactic constituency trees from BERT [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>]. While probing indicates that the information can be extracted from the representation, it can be shown [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>] that in some cases the features are not used for prediction. According to an empirical evaluation PLMs encode linguistic information with phrase features in the bottom layers, syntactic features in the middle layers and semantic features in the top layers [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>].</p><p class="Para" id="Par45">However, BERT’s syntactic knowledge is incomplete and there is, for example, evidence that BERT often does not capture <em class="EmphasisTypeItalic ">negations</em><span id="ITerm22"/>. For instance, BERT<sub>LARGE</sub> is able to determine the correct supersense, e.g. <em class="EmphasisTypeItalic ">“bird”</em> in the masked sentence <em class="EmphasisTypeItalic ">“A robin is a [MASK]”</em> with high probability [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>]. On the other hand, the model predicts <em class="EmphasisTypeItalic ">“robin”</em>, <em class="EmphasisTypeItalic ">“bird”</em>, <em class="EmphasisTypeItalic ">“penguin”</em>, <em class="EmphasisTypeItalic ">“man”</em>, <em class="EmphasisTypeItalic ">“fly”</em> with maximum probabilities for the mask in <em class="EmphasisTypeItalic ">“A robin is not a [MASK]”</em>, effectively ignoring the negation.</p><p class="Para" id="Par46">Some benchmarks described in Sect. <span class="InternalRef"><a href="#Sec1">4.1</a></span> check the syntactic knowledge of PLMs. An example is the GLUE’s CoLA task testing the grammatical correctness of sentences, which is the most difficult task of GLUE where the best models only yield about 75% correct answers (Table <span class="InternalRef"><a href="#Tab1">4.1</a></span>). SuperGLUE (Sect. <span class="InternalRef"><a href="#Sec3">4.1.2</a></span>) is a benchmark, which requires syntactic knowledge, e.g. for the textual entailment task COPA and the coreference resolution task WSC. While the fine-tuned BERT gets an average score of 69.0 the fine-tuned PaLM<sub>540B</sub> achieves an average of 91.4 (Table <span class="InternalRef"><a href="#Tab2">4.2</a></span>). Large foundation models such as PaLM, which has more than 1000 times as many parameters as BERT, are obviously able to capture syntactical knowledge much better than the ‘small’ BERT.</p></section>
<section class="Section2 RenderAsSection2" id="Sec9"><h3 class="Heading"><span class="HeadingNumber">4.2.2 </span>Common Sense Knowledge</h3><p class="Para" id="Par47"><em class="EmphasisTypeItalic ">World knowledge</em><span id="ITerm23"/>, also called <em class="EmphasisTypeItalic ">common sense knowledge</em><span id="ITerm24"/>, consists of facts about our every day world, such as <em class="EmphasisTypeItalic ">“fire is hot”</em>. A simple method of checking world knowledge is to query BERT with cloze statements, for example, <em class="EmphasisTypeItalic ">“Einstein was born in [MASK]”</em>. BERT acquires some <em class="EmphasisTypeItalic ">semantic knowledge</em><span id="ITerm25"/> about semantic roles and encodes information about entity types and relations [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>]. For instance, in the sentence <em class="EmphasisTypeItalic ">“to tip a [MASK]”</em> the token <em class="EmphasisTypeItalic ">“waiter”</em> gets a high probability for the position of <em class="EmphasisTypeItalic ">[MASK]</em>. Petroni et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>] and Zhou et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR72" role="doc-biblioref">72</a></span>] experimented with such queries and concluded that BERT contains world knowledge competitive with traditional supervised information extraction methods. It has been shown that BERT’s contextual embeddings make up clusters corresponding to <em class="EmphasisTypeItalic ">word senses</em><span id="ITerm26"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>]. This explains why BERT is quite capable of word sense disambiguation (Fig. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Fig10"><span class="RefSource">2.​10</span></a></span>).</p><p class="Para" id="Par48">Petroni et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>] remark that certain types of factual knowledge are learned much more easily than others by the standard language model pre-training approaches. They state that without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge. In addition, BERT also does remarkably well on open-domain question answering against a supervised baseline. These capabilities of BERT are a great achievement.</p><p class="Para" id="Par49">The language model GPT-3 has one hundred times more parameters than BERT and a dramatically better common sense knowledge. This, for example, can be seen from its answers (A) to the questions (Q): <em class="EmphasisTypeItalic ">“Q: Are there any animals with three legs?”</em><em class="EmphasisTypeItalic ">“A: No, there are no animals with three legs.”</em> or <em class="EmphasisTypeItalic ">“Q: Which is heavier, a football player or a car?”</em><em class="EmphasisTypeItalic ">“A: A car is heavier than a football player.”</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>]. In an initial experiment eighty persons were asked to assess, if short 200 word articles were written by humans or GPT-3. The persons judged incorrectly 48% of the time, doing only slightly better than random guessing [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>].</p><p class="Para" id="Par50">However, the semantic knowledge of PLMs is not perfect. BERT, for instance, has difficulties with the representation of numbers and often has problems with the replacement of <em class="EmphasisTypeItalic ">named entities</em><span id="ITerm27"/> (<em class="EmphasisTypeItalic ">NE</em>s), e.g. person names or location names. For example, replacing names in the coreference task changes 85% of coreference assignments of expressions that refer to the same entity [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>]. Obviously the pre-trained version of BERT struggles to generalize the relations involving one named entity to other named entities of the same type. Moreover, BERT has problems to transfer knowledge based on the roles or types of objects. In addition, it is possible to mislead BERT by adding some content to a cloze query. An example is the word <em class="EmphasisTypeItalic ">“Talk”</em> in <em class="EmphasisTypeItalic ">“Talk? Birds can [MASK]”</em>. A human would ignore <em class="EmphasisTypeItalic ">“Talk?”</em> and use his world knowledge to generate a result like <em class="EmphasisTypeItalic ">“fly”</em>. In contrast, PLMs can be misled and produce the wrong answer <em class="EmphasisTypeItalic ">“talk”</em> for the mask [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>].</p><p class="Para" id="Par51">A related phenomenon is the invariance to <em class="EmphasisTypeItalic ">paraphrases</em><span id="ITerm28"/>. Elazar et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>] generate a high-quality set of 328 paraphrases to express 38 relations. Examples are <em class="EmphasisTypeItalic ">“X originally aired on [MASK]”</em> and <em class="EmphasisTypeItalic ">“X premiered on [MASK]”</em>, which should give the same prediction for <em class="EmphasisTypeItalic ">[MASK]</em>, if <em class="EmphasisTypeItalic ">“X”</em> is replaced by some TV series like <em class="EmphasisTypeItalic ">“Seinfeld”</em>. Although the models in about 60% of the cases have access to the required knowledge to fill the mask correctly, BERT<sub>LARGE</sub> yields a consistency in paraphrases in only 48.7% of the cases. This indicates that not every fact present in the training data is encoded in the parameters and that the model does not always detect the equivalence of paraphrases. The model variants RoBERTa and ALBERT achieve a lower consistency, although they are superior to BERT in other tasks.</p><p class="Para" id="Par52">It is instructive to consider the influence of word order on the performance of BERT. Word order is taken into account by specific position embeddings, which are added to the token embeddings. It turns out, however that masked language models like BERT still achieve a high accuracy, if word positions are permuted. For pre-training Sinha et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR59" role="doc-biblioref">59</a></span>] perform sentence permutations, where each word in a sentence is randomly placed at a different position. The model was fine-tuned on GLUE, a set of classification tasks for natural language understanding (Sect. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Sec7"><span class="RefSource">2.​1.​5</span></a></span>). If we ignore the CoLA-task, which checks linguistic acceptability, the model on average only looses 3.4% accuracy if the word order is permuted compared to the original RoBERTa results (88.7% on average). The authors conclude that BERT-like models achieve high performance on downstream tasks almost entirely by exploiting higher-order word co-occurrence statistics.</p><p class="Para" id="Par53">Another aspect of common sense knowledge is time. When a PLM is applied to new documents it often does not know the meaning of new named entities and concepts [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>]. Often, the model cannot infer the time and region of a document and may not be able to correctly combine facts from documents that originate from different time periods or geographical regions. A benchmark for assessing the temporal reasoning capabilities of PLMs in dialogs shows that BERT and T5 have major deficits on this task [<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>]. In summary it can be expected that the new Retro (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec15"><span class="RefSource">6.​2.​3</span></a></span>) or WebGPT (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec16"><span class="RefSource">6.​2.​3</span></a></span>) models, which perform retrieval during language generation, will considerably mitigate the problems discussed in this section.</p><p class="Para" id="Par54">To be able to check a multitude of different knowledge types in a standardized way large benchmarks like BIG-bench have been developed (Sect. <span class="InternalRef"><a href="#Sec5">4.1.4</a></span>). It comprises benchmarks on common sense, emotional intelligence, ethics, fact checking, general knowledge, humanities, mathematics, medicine, reading comprehension, science and social sciences. Figure <span class="InternalRef"><a href="#Fig1">4.1</a></span> shows the performance of the Gopher model with 280B parameters on these benchmark groups. On most groups more than 50% accuracy was achieved. The PaLM model with 540B parameters was able to improve these performance figures. On about 2∕3 of these tasks PaLM using 5-shot prompts achieves a better performance than average humans [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>, p. 17]. This indicates that PaLM has a much better common sense knowledge than earlier models. Nevertheless, PaLM surpasses the performance of human experts only in a small fraction of cases suggesting further headroom for improvement.</p><p class="Para" id="Par55">An interesting idea is to use large pre-trained multilingual language models as a multilingual knowledge base [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>]. The authors evaluate this for mBERT (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec13"><span class="RefSource">3.​3.​1</span></a></span>), a standard BERT model, which has been pre-trained with the MLM loss on non-parallel Wikipedia texts from 104 languages. The authors find that correct entities can be retrieved for many languages. However, there is a clear performance gap between English and, e.g., Japanese and Thai. This suggests that mBERT does not store knowledge about entities in a language-independent way. It would be revealing if these experiments could be repeated with up-to-date language models like PaLM.</p></section>
<section class="Section2 RenderAsSection2" id="Sec10"><h3 class="Heading"><span class="HeadingNumber">4.2.3 </span>Logical Consistency</h3><p class="Para" id="Par56">A set of statements is logically inconsistent if they cannot all be true at the same time. As an example consider the statements “John is Tom’s father. Tom is the daughter of John.” Sometimes, BERT is unable to reason, i.e. logically connect different pieces of knowledge. It reproduces, for instance, the relations that persons can walk into houses, and that houses are big, but it cannot infer that houses are bigger than persons [<span class="CitationRef"><a epub:type="biblioref" href="#CR15" role="doc-biblioref">15</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>]. However, semantic knowledge problems tend to be smaller for models with more parameters.</p><p class="Para" id="Par57">Richardson et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>] formulated nine different types of simple sentence pairs containing e.g. negations, quantifiers, comparatives, etc. These sentences express logical entailment, contradiction or neutrality. In addition, they also employ chains of hypernomy, e.g. <em class="EmphasisTypeItalic ">poodle</em> ≤ <em class="EmphasisTypeItalic ">dog</em> ≤ <em class="EmphasisTypeItalic ">mammal</em> ≤ <em class="EmphasisTypeItalic ">animal</em>, and use these relations to generate new sentences expressing the corresponding logical properties. It turns out that BERT fine-tuned with the ‘logical tasks’ SNLI and MNLI predicts correct statements only with 47.3% accuracy of the cases.</p><p class="Para" id="Par58">Ribeiro et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>] propose to generate a large number of simple examples to test relations by a <em class="EmphasisTypeItalic ">CheckList procedure</em><span id="ITerm29"/> described in Sect. <span class="InternalRef"><a href="#Sec15">4.3.1</a></span>. It tests, for instance, whether negating a positive sentiment expression leads to a negative sentiment rating. For more than half of the tests with commercial and open-source models they observed failure rates of more than 50%.</p><p class="Para" id="Par59">Even the larger model GPT-3 is not perfect, e.g. it incorrectly answers some common sense physics questions like <em class="EmphasisTypeItalic ">“If I put cheese into the fridge, will it melt?”</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>]. In addition, it has difficulties with logical reasoning, e.g. to determine if one sentence implies another. If a question is not covered in its training material, GPT-3 compiles the most probable answer and sometimes this is wrong, e.g. <em class="EmphasisTypeItalic ">“Q: How many eyes does the sun have?”</em><em class="EmphasisTypeItalic ">“A: The sun has one eye.”</em> or <em class="EmphasisTypeItalic ">“Q: Who was president of the United States in 1600?”</em><em class="EmphasisTypeItalic ">“A: Queen Elizabeth I was president of the United States in 1600.”</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>]. As another example consider the following input <em class="EmphasisTypeItalic ">“You poured yourself a glass of cranberry, but then absentmindedly, you poured about a teaspoon of grape juice into it. It looks OK. You try sniffing it, but you have a bad cold, so you can’t smell anything. You are very thirsty. So you …”</em>. The continuation generated by GPT-3 is <em class="EmphasisTypeItalic ">“drink it. You are now dead.”</em>. GPT-3 assumes wrongly that <em class="EmphasisTypeItalic ">“grape juice”</em> is a poison and drinking it will kill you [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>].</p><section class="Section3 RenderAsSection3" id="Sec11"><h4 class="Heading">Improving Logical Consistency</h4><p class="Para" id="Par60">PLMs can improve logical reasoning capabilities if they are trained with appropriately generated textual expressions. By fine-tuning a BERT model with created sentences containing negations, hypernomy, etc., and testing with other generated sentences, Richardson et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>] achieve an accuracy of 98%. This approach is similar to the data generation strategy proposed in Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec46"><span class="RefSource">3.​6.​6</span></a></span>.</p><p class="Para" id="Par61">Similarly, Clark et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>] generate datasets of the form (context, statement, answer), where context contains different logical facts and rules, statement is a logical question to prove and answer is either T or F. Facts, rules, and the question statements are then expressed in (synthetic) English. The problems require simultaneous consideration of a number of different statements to reach a conclusion, from depth 0 (simple lookup) to depth 5. During fine-tuning on this data, RoBERTa was trained to answer these questions as true or false. On the test data RoBERTa is able to answer the questions with 99% accuracy. If the facts and rules are paraphrased the accuracy drops to 66%. However, by training on paraphrased rules the model again reaches an accuracy beyond 90%. Clark et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>] suggest that by this approach the transformer can be considered as a “soft theorem prover” able to work with statements in language.</p><p class="Para" id="Par62">It is possible to combine the implicit, pre-trained knowledge of an LM and explicit statements in natural language. Talmor et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>] show that models trained with such datasets can perform inferences involving implicit world knowledge and taxonomic knowledge (e.g. the WordNet hierarchy) . In addition, inference patterns provided by examples are used by the model to solve logical problems.</p><p class="Para" id="Par63">There were a number of prior approaches to combine logical reasoning with neural networks. If a neural network provides probabilities for logical facts, then we can use a probabilistic reasoning system to enforce additional constraints. Examples are <em class="EmphasisTypeItalic ">DeepProblog</em><span id="ITerm30"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>] that incorporates Deep Learning by means of neural predicates, i.e. statements whose probability is determined by a neural network. An alternative is <em class="EmphasisTypeItalic ">probabilistic soft logic</em><span id="ITerm31"/> (<em class="EmphasisTypeItalic ">PSL</em>) [<span class="CitationRef"><a epub:type="biblioref" href="#CR28" role="doc-biblioref">28</a></span>], which allows first order probabilistic reasoning in relational domains. However, PLMs do not directly provide probabilities for facts. There have been approaches to translate natural language sentences to logical statements and apply logical reasoning. However, this “semantic parsing” [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>] was not very successful.</p><p class="Para" id="Par64">A number of researchers have developed methods for neural theorem proving. This work combines symbolic and neural methods to reason about results derived from language. Examples are e.g. Minervini et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>], which jointly embed logical predicates and text in a shared space by using an end-to-end differentiable model, or Weber et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>] which combine a Prolog prover with a language model to apply rule-based reasoning to natural language. The <strong class="EmphasisTypeBold ">DeepCTRL</strong><span id="ITerm32"/> approach [<span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>] integrates rules with Deep Learning. It has a rule encoder which allows to control the strengths of the rules at inference. It can be applied to domains like healthcare, physical models or accounting, where obeying rules is essential.</p><p class="Para" id="Par65">A simple but effective way to improve logical consistency is to increase the number of model parameters creating a Foundation Model. A large fraction of the tasks in the BIG-bench benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>] is devoted to checking logical consistency, e.g. the benchmark groups with analogical reasoning and logical reasoning. <em class="EmphasisTypeItalic ">Gopher</em><span id="ITerm33"/> (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec3"><span class="RefSource">3.​1.​2</span></a></span>) is a language model with 280B parameters. It was applied to about 150 benchmarks, among them 19 logical reasoning tasks. In all but 4 benchmarks it could increase <span class="EmphasisTypeSmallCaps ">Sota</span> indicating that larger PLMs have better reasoning capabilities. Nevertheless, the average accuracy was only about 50%. It was not yet evaluated whether the recent <em class="EmphasisTypeItalic ">Retro</em><span id="ITerm34"/> (Sect. <span class="ExternalRef"><a href="528393_1_En_6_Chapter.xhtml#Sec15"><span class="RefSource">6.​2.​3</span></a></span>) language model with retrieval of additional text documents is able to improve these results.</p><p class="Para" id="Par66"><em class="EmphasisTypeItalic ">PaLM</em><span id="ITerm35"/> (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec3"><span class="RefSource">3.​1.​2</span></a></span>) is an even larger language model with 540B parameters. On the SuperGLUE logical tasks CB, COPA, RTE, it can drastically increase the scores compared to BERT, e.g. for COPA from 70.6 to 99.2 (Table <span class="InternalRef"><a href="#Tab2">4.2</a></span>). It has been evaluated on hundreds of benchmarks including those used for Gopher. It uses a new prompt technique to pose logical questions, where examples are presented to the system together with <em class="EmphasisTypeItalic ">thought chains</em><span id="ITerm36"/> partitioning a reasoning task into smaller problems (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec42"><span class="RefSource">3.​6.​4</span></a></span>). Two examples are shown in Fig. <span class="ExternalRef"><a href="528393_1_En_2_Chapter.xhtml#Fig21"><span class="RefSource">2.​21</span></a></span>. Note that <em class="EmphasisTypeItalic ">k</em>-shot reasoning only requires a single sequence of <em class="EmphasisTypeItalic ">k</em> thought chain prompts to be provided for the training examples. The model then generates a thought chain for each test example. This can be used for error analysis and explaining the model behavior.</p><p class="Para" id="Par67">Using this technique, PaLM is able to match or surpass the performance level of an average human asked to solve the task. As an example consider the <em class="EmphasisTypeItalic ">StrategyQA benchmark</em><span id="ITerm37"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>], which contains questions like <em class="EmphasisTypeItalic ">“Did Aristotle use a laptop?”</em>. For this question the model has to collect facts on the lifespan of Aristotle and the year, when the first laptop was invented to arrive at the answer <em class="EmphasisTypeItalic ">“No”</em>. Without thought chain prompts PaLM reached 69%, while the use of thought chain prompts could improve the prior <span class="EmphasisTypeSmallCaps ">Sota</span> from 70% to 73.9%. As a comparison, average humans achieve 62.9%, while expert humans have an accuracy of 90%.</p><p class="Para" id="Par68">There are other ways to improve learning with such intermediate outputs. Wang et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR69" role="doc-biblioref">69</a></span>] sample multiple chains of thought exploiting the diversity of reasoning paths and then return the most consistent final answer in the set. Since it is expensive to obtain chains-of-thought for a large number of examples, Zelikman et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR71" role="doc-biblioref">71</a></span>] generate explanations for a large dataset by bootstrapping a model in the few-shot setting and only retaining chains-of-thought that lead to correct answers.</p></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec12"><h3 class="Heading"><span class="HeadingNumber">4.2.4 </span>Summary</h3><p class="Para" id="Par69">Pre-trained PLMs have a huge number of parameters and are able to represent an enormous amount of syntactic and factual knowledge. This knowledge can be elicited by probing classifiers, the prediction of masked words, by generating answers to inputs, or by solving benchmark tasks.</p><p class="Para" id="Par70">As far as syntactic knowledge is concerned, Foundation Models like GPT-3 produce almost error-free text and ‘know’ a lot about syntactic rules. One problem is to adequately reflect the effect of negations.</p><p class="Para" id="Par71">Even smaller models like BERT are capable of producing a lot of commonsense knowledge. Here, the effect of substituting names or using paraphrases is problematic. Larger language models like GPT-3 are more robust, and the recently proposed language models with retrieval (WebGPT, Retro) are able to include relevant external documents for the current task. This information can reduce errors considerably. However, there is no comprehensive evaluation yet. One problem is the correct temporal and spatial location of information. Here, smaller models like BERT and T5 have large deficits. Foundation Models meanwhile surpass the average human score in 2/3 of the BIG-bench tests on common sense knowledge. They can even be used as a multilingual knowledge base, since models like PaLM cover many languages.</p><p class="Para" id="Par72">Logical consistency of inferences is a problem, and the PLMs often associate answers that are plausible but wrong. The models are only able to make logical inferences for relationships mentioned in the training text, and they are often incapable of making abstractions and generalizing an observed relationship to other objects or entities of the same type. Logical consistency can be improved by generating additional training texts containing assumptions and valid logical consequences resulting from them. The direct inclusion of logical reasoning systems in Foundation Models was not very successful. The PaLM language model with 540B parameters achieved a fundamental improvement of the accuracy of logical reasoning through the use of thought chain prompts. Here in a few-shot prompt a logical derivation is broken down into smaller logical substeps . At present, it is not clear, to what extent language models with retrieval can reduce the still existing deficits in logical reasoning.</p></section>
</section>
<section class="Section1 RenderAsSection1" id="Sec13"><h2 class="Heading"><span class="HeadingNumber">4.3 </span>Transferability and Reproducibility of Benchmarks</h2><p class="Para" id="Par73">In this section, we consider whether benchmarks actually evaluate the properties they are supposed to test. We also discuss the extent to which they are reproducible.</p><section class="Section2 RenderAsSection2" id="Sec14"><h3 class="Heading"><span class="HeadingNumber">4.3.1 </span>Transferability of Benchmark Results</h3><p class="Para" id="Par74">On a number of benchmarks, the performance of human annotators is exceeded by Foundation Models. This is an indication that the model has learned valuable contents about language. However, Ribeiro et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>] argue that this can be misleading, because the test sets often do not cover the right content. While performance on held-out test data is a useful measure, these datasets are often not comprehensive. Hence, there is the danger of overestimating the usability of the model in real applications.</p><section class="Section3 RenderAsSection3" id="Sec15"><h4 class="Heading">Benchmarks May Not Test All Aspects</h4><p class="Para" id="Par75">On the MRPC task of the GLUE benchmark for detecting paraphrases RoBERTa, BERT<sub>LARGE</sub>, and humans have F1 scores of 90.9% [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>], 89.3% [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>] and 86.3% respectively. Therefore, both models perform better than humans. To test whether the models respect basic logical relationships, Ribeiro et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>] propose to generate a large number of simple examples using a <strong class="EmphasisTypeBold ">CheckList procedure</strong><span id="ITerm38"/>. This approach is similar to testing software by systematically generating a large variety of inputs in unit tests.</p><p class="Para" id="Par76">The following scheme, for instance, can be used to check the effect of a negation in a sentiment classification task <em class="EmphasisTypeItalic ">“I</em> &lt;<em class="EmphasisTypeItalic "> negation</em>&gt;  &lt;<em class="EmphasisTypeItalic "> positive_verb</em>&gt;<em class="EmphasisTypeItalic ">  the</em> &lt;<em class="EmphasisTypeItalic "> thing</em>&gt;<em class="EmphasisTypeItalic "> ”</em>. It generates sentences like <em class="EmphasisTypeItalic ">“I didn’t love the food”</em> or <em class="EmphasisTypeItalic ">“I don’t enjoy sailing”</em>. The authors formulate <em class="EmphasisTypeItalic ">minimum functionality tests</em>, which are useful to check if the model actually detected the reason of an outcome or used some unjustified association. In addition, they utilize <em class="EmphasisTypeItalic ">invariance tests</em> to find out, if neutral perturbations or paraphrases change the result. Finally, they create <em class="EmphasisTypeItalic ">directional expectation tests</em>, where a modification is known to change the result in an expected way.</p><p class="Para" id="Par77">For MPRC it turned out that the failure rates of RoBERTa and BERT on these 23 test templates are larger than 50% for 11 and 14 of the templates respectively. Therefore, the “superhuman” performance of the two models should be taken with a grain of salt.</p><p class="Para" id="Par78">The authors also tested five current PLMs: BERT<sub>BASE</sub>, RoBERTa<sub>BASE</sub>, Microsoft’s Text Analytics, Google Cloud’s Natural Language, and Amazon’s Comprehend. They report the results of 17 tests for sentiment classification, where most problems occurred with negations. For instance, the following example <em class="EmphasisTypeItalic ">“I thought the plane would be awful, but it wasn’t.”</em> was misclassified by most models. Also very difficult is the detection of paraphrases with 23 tests templates. Here RoBERTa had for 11 and BERT for 14 of the test templates a failure rate of more than 50%. A similar failure rate was observed for reading comprehension when test cases were generated with logical templates. These results indicate that the examples in the original test sets of the benchmarks are too easy.</p><p class="Para" id="Par79">To increase robustness of PLMs it is possible to generate adversarial examples [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR65" role="doc-biblioref">65</a></span>]. The authors discuss methods that augment training data with adversarial examples as well as methods that produce certificates of robustness. They also investigate methods to avoid spurious correlations, i.e. predictive patterns that work well on a specific dataset but do not hold in general.</p><p class="Para" id="Par80">Talman et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>] checked, if the results for benchmarks may be transferred to similar datasets. They trained six PLMs on different benchmarks for <em class="EmphasisTypeItalic ">natural language inference</em><span id="ITerm39"/> (<em class="EmphasisTypeItalic ">NLI</em>) containing sentence pairs manually labeled with the labels entailment, contradiction, and neutral. While six models perform well when the test set matches the training set, accuracy is significantly lower when a test set from another benchmark is used. BERT<sub>BASE</sub>, for instance, yields a test accuracy of 90.4% for SNLI, which drops on average 21.2% for the test sets of the other benchmarks. The reason behind this drop is a slightly different definition of the task as well as small differences in the documents domains. Obviously, it cannot be expected that the performance of PLMs can simply be transferred to new data.</p></section>
<section class="Section3 RenderAsSection3" id="Sec16"><h4 class="Heading">Logical Reasoning by Correlation</h4><div class="Para" id="Par81">The <em class="EmphasisTypeItalic ">Winograd schema challenge</em><span id="ITerm40"/> (WNLI) was developed by Levesque et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>] and is part of the GLUE benchmark collection. The test consists of a pair of sentences differing by exactly one word, each followed by a question [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>], e.g. <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par82">The sports car passed the mail truck because it was going faster. Question: Which was going faster, the sports car or the mail truck?</p></li><li><p class="Para" id="Par83">The sports car passed the mail truck because it was going slower. Question: Which was going slower, the sports car or the mail truck?</p></li></ul></div> In this pair of sentences, the difference of one word changes which thing or person a pronoun refers to. Answering these questions correctly seems to require common sense reasoning and world knowledge. In addition, the authors have designed the questions to be “Google-proof”: The system should not be able to use a web search (or anything similar) to answer the questions. GPT-3 reaches a value of 88.6% using few-shot prompts without fine-tuning [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>] and DeBERTa managed an accuracy of 95.6% after fine-tuning [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>]. This accuracy roughly equals human performance.</div><p class="Para" id="Par84">As Mitchell [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>] argues, this does not necessarily mean that neural network language models have attained human-like understanding. For a number of question pairs it seems possible to answer the question by some sort of correlation instead of actual world knowledge. If pre-trained on a large corpus the model will learn the high correlation between <em class="EmphasisTypeItalic ">“sports car”</em> and <em class="EmphasisTypeItalic ">“fast”</em> and between <em class="EmphasisTypeItalic ">“mail truck”</em> and <em class="EmphasisTypeItalic ">“slow”</em> for the above example. Therefore, it can give the correct answer on the coreference of <em class="EmphasisTypeItalic ">“it”</em> based on those correlations alone and not by recourse to any understanding. It turns out that many of the Winograd schema challenge question follow this pattern. A similar argument states [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR37" role="doc-biblioref">37</a></span>] that a model might heuristically accept a hypothesis by assuming that the premise entails any hypothesis whose words all appear in the premise. This means that the model can give the right answer without ‘understanding’ the situation in question.</p><div class="Para" id="Par85">To reduce the deficits of the Winograd schema challenge a much larger <em class="EmphasisTypeItalic ">Winogrande</em><span id="ITerm41"/> benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>] was created using crowdsourcing. The researchers discarded sentences which could be answered by exploiting intuition and correlation. They used the embeddings created by RoBERTa (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec2"><span class="RefSource">3.​1.​1</span></a></span>) to determine if these embeddings strongly indicated the correct response option. In this case they discarded the question pair and finally ended up with 44k sentences. An example for a question pair without correlation problems is: <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par86">The trophy doesn’t fit into the brown suitcase because it’s too large. (it: trophy)</p></li><li><p class="Para" id="Par87">The trophy doesn’t fit into the brown suitcase because it’s too small. (it: suitcase)</p></li></ul></div> While humans reach an accuracy of 94%, the best PLMs, standard models like RoBERTa only reached 79.1% accuracy. Recently, <em class="EmphasisTypeItalic ">T5-XXL</em><span id="ITerm42"/> achieved an accuracy of about 91% [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>] and the <em class="EmphasisTypeItalic ">ST-MoE-32B</em><span id="ITerm43"/> mixture-of-experts model [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>] with 269B parameters (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec26"><span class="RefSource">3.​5.​2</span></a></span>) obtained 96.1%, drastically reducing the errors. It appears that in most cases the latter models are able to perform ‘reasoning’ without simply correlating statements.</div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec17"><h3 class="Heading"><span class="HeadingNumber">4.3.2 </span>Reproducibility of Published Results in Natural Language Processing</h3><p class="Para" id="Par88">Many publications in NLP claim that their model achieves <span class="EmphasisTypeSmallCaps ">Sota</span> for some benchmark. Examples are the GLUE benchmark [<span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>] for language understanding and the SQuAD data [<span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>] for reading comprehension. There are two main problems with this approach. First it is difficult to assess, if the results are reproducible and significant. As Crane [<span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>] demonstrates, there are usually a number of unreported conditions that affect the reproducibility of the result. An example is the random initialization of the network parameters. The resulting variance is often larger than the reported improvement in <span class="EmphasisTypeSmallCaps ">Sota</span> scores. However, the variance resulting from these phenomena is usually not reported. Other effects are the underlying programming frameworks and libraries, which change over time. Often the hyperparameters and the details of preprocessing and model configuration are not communicated.</p><div class="Para" id="Par89">To document the model architecture, the training and evaluation process of a model, Mitchell et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>] proposed the description of relevant facts and hyperparameters in a <strong class="EmphasisTypeBold ">model card</strong><span id="ITerm44"/>. After a short high-level description of the model and its purpose the model card should contain nine different sections [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>]: <div class="OrderedList"><ol><li class="ListItem"><div class="ItemNumber">1.</div><div class="ItemContent"><p class="Para" id="Par90">Basic information about the model,</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">2.</div><div class="ItemContent"><p class="Para" id="Par91">Intended uses and scope limitations,</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">3.</div><div class="ItemContent"><p class="Para" id="Par92">Model performance across a variety of relevant factors,</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">4.</div><div class="ItemContent"><p class="Para" id="Par93">Performance metrics,</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">5.</div><div class="ItemContent"><p class="Para" id="Par94">Evaluation data,</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">6.</div><div class="ItemContent"><p class="Para" id="Par95">Training data,</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">7.</div><div class="ItemContent"><p class="Para" id="Par96">Evaluation results according to the chosen metrics.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">8.</div><div class="ItemContent"><p class="Para" id="Par97">Ethical consideration, risks and harms.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">9.</div><div class="ItemContent"><p class="Para" id="Par98">Caveats and recommendations.</p></div><div class="ClearBoth"> </div></li></ol></div> More details are given by huggingface [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>]. Even if models still can be published without a model card, the explicit documentation of the model can only benefit future users. Therefore, model cards should be provided if possible. For most recent models, a model card is provided even if the model is not open-source.</div><p class="Para" id="Par99">A survey on <em class="EmphasisTypeItalic ">reproducibility in NLP</em><span id="ITerm45"/> is given by Belz et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>]. They note that the performance results often depend on seemingly small differences in model parameters and settings, for example minimum counts for rare word or the normalization of writing. The authors state in their study on repeated experiments that only 14% of the 513 reported scores were the same. An annoying fraction of 59% of the scores were worse than the published numbers. Therefore, the experimental results published in papers should be treated with caution.</p><p class="Para" id="Par100">Another issue is the question of what causes an increase in performance. As we have discussed above, a growth in the number of parameters and in the computing effort regularly leads to better results for PLMs (Sect. <span class="ExternalRef"><a href="528393_1_En_3_Chapter.xhtml#Sec25"><span class="RefSource">3.​5.​1</span></a></span>). As a consequence, it is often not clear, whether the architectural changes to a model yield the improved performance or just the number of additional parameters or the larger training set [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>].</p><p class="Para" id="Par101">Obviously a first place in a leaderboard can be achieved with a larger model and more computing effort. This, however, “is not research news” according to Rogers [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>]. In addition, these results are often not reproducible: Who can afford to retrain GPT-3 for 4.6 million dollars. As a consequence, the development of smaller but more innovative models is less rewarding, as it is difficult to beat the bigger model. Only if the authors of a new model can show that their architecture is better than the previous <span class="EmphasisTypeSmallCaps ">Sota</span> model with the same number of parameters and compute budget, they can claim to have made a valuable contribution. Rogers [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>] proposes to provide a standard training corpus for a leaderboard and limit the amount of computation effort to that of a strong baseline model. As an alternative the size of the training data and the computational effort should be reported and taken into account in the final score.</p><section class="Section3 RenderAsSection3" id="Sec18"><h4 class="Heading">Available Implementations</h4><div class="Para" id="Par102"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par103">There are model codes and trained models for RoBERTa and ELECTRA at Hugging Face <span class="ExternalRef"><a href="https://huggingface.co/transformers/"><span class="RefSource">https://​huggingface.​co/​transformers/​</span></a></span>.</p></li><li><p class="Para" id="Par104">The code for DeBERTa is available at <span class="ExternalRef"><a href="https://github.com/microsoft/DeBERTa"><span class="RefSource">https://​github.​com/​microsoft/​DeBERTa</span></a></span> and Hugging Face.</p></li><li><p class="Para" id="Par105">The Checklist code is at <span class="ExternalRef"><a href="https://github.com/marcotcr/checklist"><span class="RefSource">https://​github.​com/​marcotcr/​checklist</span></a></span>.</p></li></ul></div></div></section>
</section>
<section class="Section2 RenderAsSection2" id="Sec19"><h3 class="Heading"><span class="HeadingNumber">4.3.3 </span>Summary</h3><p class="Para" id="Par106">The transferability of benchmark results to real applications is not always granted. Even if a PLM is better than humans at logical reasoning on the test set, it may not be able to classify generated logical reasoning chains correctly. This indicates that the test set does not cover the full spectrum of possible examples. It is common for performance to be lower on related benchmarks because the domain or the definition of the task may deviate.</p><p class="Para" id="Par107">There are cases where a logical conclusion is obtained not by logical deduction, but by a simple correlation of antecedent and consequent. This could be demonstrated for the Winograd task of the GLUE benchmark. To avoid this type of ‘reasoning’ a new variant task called Winogrande was developed where correlations are unrelated to the reasoning task. Meanwhile, a Foundation Model with 269B parameters was also able to solve this task better than humans.</p><p class="Para" id="Par108">A survey on the reproducibility of results in NLP demonstrated that the published performance often depends on a number of unreported effects, such as random number initialization. Often the variability of such effects is larger than the reported improvement. Therefore, it is necessary to report the variance caused by these effects. In addition, the details of the model architecture, its training and evaluation should be documented in a model card. In about 500 repeated experiments, an irritating rate of about 60% of final scores were lower than reported. Note that improvements due to more parameters, more training data, or higher computational effort are not indicative of a better model architecture.</p></section>
</section>
<div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a><p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p><p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p></div><aside aria-labelledby="Bib1Heading" class="Bibliography" id="Bib1"><div epub:type="bibliography" role="doc-bibliography"><div class="Heading" id="Bib1Heading">References</div><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">S. Aarohi and R. Abhinav. <em class="EmphasisTypeItalic ">BIG-bench</em> ⋅ Google, June 20, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/google/BIG-bench/blob/936c4a5876646966344349b28ae187c556938ec4/docs/paper/BIGbench.pdf"><span class="RefSource">https://​github.​com/​google/​BIG-bench/​blob/​936c4a5876646966​344349b28ae187c5​56938ec4/​docs/​paper/​BIGbench.​pdf</span></a></span> (visited on 06/20/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">M. Aßenmacher and C. Heumann. “On the Comparability of Pre-Trained Language Models”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2001.00781</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">S. Balasubramanian, N. Jain, G. Jindal, A. Awasthi, and S. Sarawagi. “What’s in a Name? Are BERT Named Entity Representations Just as Good for Any Other Name?” 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2007.06897</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">A. Belz, S. Agarwal, A. Shimorina, and E. Reiter. “A Systematic Review of Reproducibility Research in Natural Language Processing”. Mar. 21, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2103.07929</span> [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">S. R. Bowman and G. E. Dahl. “What Will It Take to Fix Benchmarking in Natural Language Understanding?” 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2104.02145</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">R. Branco, A. Branco, J. António Rodrigues, and J. R. Silva. “Shortcutted Commonsense: Data Spuriousness in Deep Learning of Commonsense Reasoning”. In: <em class="EmphasisTypeItalic ">Proc. 2021 Conf. Empir. Methods Nat. Lang. Process</em>. EMNLP 2021. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 1504–1521. <span class="ExternalRef"><a href="https://doi.org/10.18653/v1/2021.emnlp-main.113"><span class="RefSource">https://​doi.​org/​10.​18653/​v1/​2021.​emnlp-main.​113</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">T. B. Brown et al. “Language Models Are Few-Shot Learners”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2005.14165</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">K.-W. Chang, H. He, R. Jia, and S. Singh. “Robustness and Adversarial Examples in Natural Language Processing”. In: <em class="EmphasisTypeItalic ">Proc. 2021 Conf. Empir. Methods Nat. Lang. Process. Tutor. Abstr</em>. Punta Cana, Dominican Republic &amp; Online: Association for Computational Linguistics, Nov. 2021, pp. 22–26. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://aclanthology.org/2021.emnlp-tutorials.5"><span class="RefSource">https://​aclanthology.​org/​2021.​emnlp-tutorials.​5</span></a></span> (visited on 11/24/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">A. Chowdhery et al. “PaLM: Scaling Language Modeling with Pathways”. Apr. 5, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2204.02311 [cs]</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">P. Clark, O. Tafjord, and K. Richardson. “Transformers as Soft Reasoners over Language”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2002.05867</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">M. Crane. “Questionable Answers in Question Answering Research: Reproducibility and Variability of Published Results”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 6 (2018), pp. 241–252.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">Y. Elazar, N. Kassner, S. Ravfogel, A. Ravichander, E. Hovy, H. Schütze, and Y. Goldberg. “Measuring and Improving Consistency in Pretrained Language Models”. May 29, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2102.01017</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">Y. Elazar, S. Ravfogel, A. Jacovi, and Y. Goldberg. “Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 9 (2021), pp. 160–175.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">A. Ettinger. “What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 8 (2020), pp. 34–48.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">M. Forbes, A. Holtzman, and Y. Choi. “Do Neural Language Representations Learn Physical Commonsense?” 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1908.02899</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant. “Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 9 (2021), pp. 346–361.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">Y. Goldberg. “Assessing BERT’s Syntactic Abilities”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1901.05287</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">P. He, J. Gao, and W. Chen. “Debertav3: Improving Deberta Using Electra-Style Pre- Training with Gradient-Disentangled Embedding Sharing”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2111.09543</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">P. He, X. Liu, J. Gao, and W. Chen. “DeBERTa: Decoding-enhanced BERT with Disentangled Attention”. Jan. 11, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2006.03654</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. “Measuring Massive Multitask Language Understanding”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2009.03300</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">J. Hewitt and C. D. Manning. “A Structural Probe for Finding Syntax in Word Representations”. In: <em class="EmphasisTypeItalic ">Proc. 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. Vol. 1 Long Short Pap</em>. 2019, pp. 4129–4138.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">huggingface. <em class="EmphasisTypeItalic ">Building a Model Card - Hugging Face Course</em>. 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://huggingface.co/course/chapter4/4"><span class="RefSource">https://​huggingface.​co/​course/​chapter4/​4</span></a></span> (visited on 08/07/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">G. Jawahar, B. Sagot, and D. Seddah. “What Does BERT Learn about the Structure of Language?” In: 2019.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">A. Kamath and R. Das. “A Survey on Semantic Parsing”. 2018. arXiv: <span class="EmphasisFontCategoryNonProportional ">1812.00978</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">N. Kassner, P. Dufter, and H. Schütze. “Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models”. 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2102.00894</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">N. Kassner and H. Schütze. “Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, but Cannot Fly”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1911.03343</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">T. Kim, J. Choi, D. Edmiston, and S.-g. Lee. “Are Pre-Trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2002.00737</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">B. Kirsch, S. Giesselbach, T. Schmude, M. Völkening, F. Rostalski, and S. Rüping. “Using Probabilistic Soft Logic to Improve Information Extraction in the Legal Domain”. In: (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">K. Lacker. <em class="EmphasisTypeItalic ">Giving GPT-3 a Turing Test</em>. July 6, 2020. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html"><span class="RefSource">https://​lacker.​io/​ai/​2020/​07/​06/​giving-gpt-3-a-turing-test.​html</span></a></span> (visited on 12/03/2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">A. Lazaridou et al. “Mind the Gap: Assessing Temporal Generalization in Neural Language Models”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 34 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">B. Lemoine. <em class="EmphasisTypeItalic ">Is LaMDA Sentient? – An Interview. Medium</em>. June 11, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917"><span class="RefSource">https://​cajundiscordian.​medium.​com/​is-lamda-sentient-an-interview-ea64d916d917</span></a></span> (visited on 06/24/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">H. Levesque, E. Davis, and L. Morgenstern. “The Winograd Schema Challenge”. In: <em class="EmphasisTypeItalic ">Thirteen. Int. Conf. Princ. Knowl. Represent. Reason</em>. 2012.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">N. F. Liu, M. Gardner, Y. Belinkov, M. E. Peters, and N. A. Smith. “Linguistic Knowledge and Transferability of Contextual Representations”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1903.08855</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">Y. Liu et al. “Roberta: A Robustly Optimized Bert Pretraining Approach”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1907.11692</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">R. Manhaeve, S. Dumancic, A. Kimmig, T. Demeester, and L. De Raedt. “Deepproblog: Neural Probabilistic Logic Programming”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2018, pp. 3749–3759.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">G. Marcus and E. Davis. <em class="EmphasisTypeItalic ">GPT-3: Commonsense Reasoning</em>. Aug. 1, 2020. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://cs.nyu.edu/faculty/davise/papers/GPT3CompleteTests.html"><span class="RefSource">https://​cs.​nyu.​edu/​faculty/​davise/​papers/​GPT3CompleteTest​s.​html</span></a></span> (visited on 02/15/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">R. T. McCoy, E. Pavlick, and T. Linzen. “Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference”. June 24, 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1902.01007</span> [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">S. Merity, C. Xiong, J. Bradbury, and R. Socher. “Pointer Sentinel Mixture Models”. 2016. arXiv: <span class="EmphasisFontCategoryNonProportional ">1609.07843</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">P. Minervini, M. Bošnjak, T. Rocktäschel, S. Riedel, and E. Grefenstette. “Differentiable Reasoning on Large Knowledge Bases and Natural Language”. In: <em class="EmphasisTypeItalic ">Proc. AAAI Conf. Artif. Intell</em>. Vol. 34. 04. 2020, pp. 5182–5190.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">M. Mitchell et al. “Model Cards for Model Reporting”. In: <em class="EmphasisTypeItalic ">Proc. Conf. Fairness Account. Transpar</em>. Jan. 29, 2019, pp. 220–229. <span class="ExternalRef"><a href="https://doi.org/10.1145/3287560.3287596"><span class="RefSource">https://​doi.​org/​10.​1145/​3287560.​3287596</span></a></span>. arXiv: <span class="EmphasisFontCategoryNonProportional ">1810.03993</span> [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">M. Mitchell. <em class="EmphasisTypeItalic ">What Does It Mean for AI to Understand?</em> Quanta Magazine. Dec. 16, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.quantamagazine.org/what-does-it-mean-for-ai-to-understand-20211216/"><span class="RefSource">https://​www.​quantamagazine.​org/​what-does-it-mean-for-ai-to-understand-20211216/​</span></a></span> (visited on 01/03/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">N. Nangia and S. R. Bowman. “Human vs. Muppet: A Conservative Estimate of Human Performance on the GLUE Benchmark”. June 1, 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1905.10425</span> [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">openai. <em class="EmphasisTypeItalic ">Submissions – WinoGrande: Adversarial Winograd Schema Challenge at Scale Leaderboard</em>. Jan. 5, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://leaderboard.allenai.org/winogrande/submissions/public"><span class="RefSource">https://​leaderboard.​allenai.​org/​winogrande/​submissions/​public</span></a></span> (visited on 01/05/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">D. Paperno et al. “The LAMBADA Dataset: Word Prediction Requiring a Broad Discourse Context”. June 20, 2016. arXiv: <span class="EmphasisFontCategoryNonProportional ">1606.06031</span> [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">Paperswithcode. Browse State-of-the-Art in AI. 2019. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://paperswithcode.com/sota"><span class="RefSource">https://​paperswithcode.​com/​sota</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">F. Petroni, T. Rocktäschel, P. Lewis, A. Bakhtin, Y. Wu, A. H. Miller, and S. Riedel. “Language Models as Knowledge Bases?” 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1909.01066</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">L. Qin, A. Gupta, S. Upadhyay, L. He, Y. Choi, and M. Faruqui. “TIMEDIAL: Temporal Commonsense Reasoning in Dialog”. In: <em class="EmphasisTypeItalic ">Proc. 59th Annu. Meet. Assoc. Comput. Linguist. 11th Int. Jt. Conf. Nat. Lang. Process. Vol. 1 Long Pap</em>. ACL-IJCNLP 2021. Online: Association for Computational Linguistics, Aug. 2021, pp. 7066–7076. <span class="ExternalRef"><a href="https://doi.org/10.18653/v1/2021.acl-long.549"><span class="RefSource">https://​doi.​org/​10.​18653/​v1/​2021.​acl-long.​549</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">A. Radford, J. Wu, D. Amodei, D. Amodei, J. Clark, M. Brundage, and I. Sutskever. “Better Language Models and Their Implications”. In: <em class="EmphasisTypeItalic ">OpenAI Blog</em> (2019). <span class="EmphasisTypeSmallCaps ">url</span>: https://​openai.​%20com/blog/better-language-models.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">J. W. Rae et al. “Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher”. In: <em class="EmphasisTypeItalic ">ArXiv Prepr. ArXiv211211446</em> (Dec. 8, 2021), p. 118.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. “Squad: 100,000+ Questions for Machine Comprehension of Text”. 2016. arXiv: <span class="EmphasisFontCategoryNonProportional ">1606.05250</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">M. T. Ribeiro, T. Wu, C. Guestrin, and S. Singh. “Beyond Accuracy: Behavioral Testing of NLP Models with CheckList”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2005.04118</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">K. Richardson, H. Hu, L. Moss, and A. Sabharwal. “Probing Natural Language Inference Models through Semantic Fragments”. In: <em class="EmphasisTypeItalic ">Proc. AAAI Conf. Artif. Intell</em>. Vol. 34. 05. 2020, pp. 8713–8721.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">A. Rogers. <em class="EmphasisTypeItalic ">How the Transformers Broke NLP Leaderboards</em>. Hacking semantics. June 30, 2019. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://hackingsemantics.xyz/2019/leaderboards/"><span class="RefSource">https://​hackingsemantics​.​xyz/​2019/​leaderboards/​</span></a></span> (visited on 12/15/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">A. Rogers, O. Kovaleva, and A. Rumshisky. “A Primer in {Bertology}: What We Know about How {BERT} Works”. In: <em class="EmphasisTypeItalic ">Trans. Assoc. Comput. Linguist</em>. 8 (2021), pp. 842–866.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. “WinoGrande: An Adversarial Winograd Schema Challenge at Scale”. In: <em class="EmphasisTypeItalic ">Commun. ACM</em> 64.9 (2021), pp. 99–106.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">F. Schmidt and T. Hofmann. “BERT as a Teacher: Contextual Embeddings for Sequence- Level Reward”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2003.02738</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">S. Seo, S. Arik, J. Yoon, X. Zhang, K. Sohn, and T. Pfister. “Controlling Neural Networks with Rule Representations”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 34 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. “Megatron- Lm: Training Multi-Billion Parameter Language Models Using Model Parallelism”. In: <em class="EmphasisTypeItalic ">arXiv</em> (2019), arXiv-<span class="EmphasisFontCategoryNonProportional ">1909</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">K. Sinha, R. Jia, D. Hupkes, J. Pineau, A. Williams, and D. Kiela. “Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little”. Apr. 14, 2021. arXiv: <span class="EmphasisFontCategoryNonProportional ">2104.06644</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">J. Sohl-Dickstein. BIG-bench. Google, Dec. 16, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://github.com/google/BIGbench"><span class="RefSource">https://​github.​com/​google/​BIGbench</span></a></span> (visited on 12/16/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">61.</div><div class="CitationContent" id="CR61">M. Sparkes. <em class="EmphasisTypeItalic ">Google Wants to Challenge AI with 200 Tasks to Replace the Turing Test</em>. New Scientist. June 14, 2022. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.newscientist.com/article/2323685-google-wantsto-challenge-ai-with-200-tasks-to-replace-the-turing-test/"><span class="RefSource">https://​www.​newscientist.​com/​article/​2323685-google-wantsto-challenge-ai-with-200-tasks-to-replace-the-turing-test/​</span></a></span> (visited on 06/26/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">62.</div><div class="CitationContent" id="CR62">S. Storks, Q. Gao, and J. Y. Chai. “Commonsense Reasoning for Natural Language Understanding: A Survey of Benchmarks, Resources, and Approaches”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1904.01172</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">63.</div><div class="CitationContent" id="CR63">A. Talman and S. Chatzikyriakidis. “Testing the Generalization Power of Neural Network Models Across NLI Benchmarks”. May 31, 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1810.09774</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">64.</div><div class="CitationContent" id="CR64">A. Talmor, O. Tafjord, P. Clark, Y. Goldberg, and J. Berant. “Teaching Pre-Trained Models to Systematically Reason over Implicit Knowledge”. 2020. arXiv: <span class="EmphasisFontCategoryNonProportional ">2006.06609</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">65.</div><div class="CitationContent" id="CR65">TrustworthyAI, director. <em class="EmphasisTypeItalic ">CVPR 2021 Tutorial on ”Practical Adversarial Robustness in Deep Learning: Problems and Solutions”</em>. June 28, 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://www.youtube.com/watch?v=ZmkU1YO4X7U"><span class="RefSource">https://​www.​youtube.​com/​watch?​v=​ZmkU1YO4X7U</span></a></span> (visited on 02/26/2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">66.</div><div class="CitationContent" id="CR66">Wang. <em class="EmphasisTypeItalic ">SuperGLUE Benchmark</em>. SuperGLUE Benchmark. 2021. <span class="EmphasisTypeSmallCaps ">url</span>: <span class="ExternalRef"><a href="https://super.gluebenchmark.com/"><span class="RefSource">https://​super.​gluebenchmark.​com/​</span></a></span> (visited on 02/23/2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">67.</div><div class="CitationContent" id="CR67">A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. “Glue: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding”. Feb. 22, 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1804.07461</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">68.</div><div class="CitationContent" id="CR68">A. Wang et al. “Superglue: A Stickier Benchmark for General-Purpose Language Understanding Systems”. In: <em class="EmphasisTypeItalic ">Adv. Neural Inf. Process. Syst</em>. 2019, pp. 3266–3280.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">69.</div><div class="CitationContent" id="CR69">X. Wang et al. “Self-Consistency Improves Chain of Thought Reasoning in Language Models”. Apr. 6, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2203.11171</span> [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">70.</div><div class="CitationContent" id="CR70">L. Weber, P. Minervini, J. Münchmeyer, U. Leser, and T. Rocktäschel. “Nlprolog: Reasoning with Weak Unification for Question Answering in Natural Language”. 2019. arXiv: <span class="EmphasisFontCategoryNonProportional ">1906.06187</span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">71.</div><div class="CitationContent" id="CR71">E. Zelikman, Y. Wu, and N. D. Goodman. “STaR: Bootstrapping Reasoning With Reasoning”. Mar. 27, 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2203.14465</span> [cs].</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">72.</div><div class="CitationContent" id="CR72">X. Zhou, Y. Zhang, L. Cui, and D. Huang. “Evaluating Commonsense in Pre-Trained Language Models.” In: <em class="EmphasisTypeItalic ">AAAI</em>. 2020, pp. 9733–9740.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">73.</div><div class="CitationContent" id="CR73">B. Zoph et al. “Designing Effective Sparse Expert Models”. 2022. arXiv: <span class="EmphasisFontCategoryNonProportional ">2202.08906</span>.</div></li></ol></div></aside></div></div></body></html>