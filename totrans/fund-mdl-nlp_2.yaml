- en: '© The Author(s) 2023G. Paaß, S. GiesselbachFoundation Models for Natural Language
    ProcessingArtificial Intelligence: Foundations, Theory, and Algorithms[https://doi.org/10.1007/978-3-031-23190-2_3](https://doi.org/10.1007/978-3-031-23190-2_3)'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: © 作者（2023）G. Paaß, S. Giesselbach自然语言处理基础模型人工智能：基础、理论与算法[https://doi.org/10.1007/978-3-031-23190-2_3](https://doi.org/10.1007/978-3-031-23190-2_3)
- en: 3. Improving Pre-trained Language Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 改进预训练语言模型
- en: Gerhard Paaß^([1](#Aff5)  ) and Sven Giesselbach^([1](#Aff5))(1)Knowledge Discovery
    Department, Team NLU, Fraunhofer Institute for Intelligent Analysis and Information
    Systems (IAIS), Sankt Augustin, Nordrhein-Westfalen, Germany
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Gerhard Paaß^([1](#Aff5)  ) 和 Sven Giesselbach^([1](#Aff5))(1)知识发现部门，NLU 团队，弗劳恩霍夫智能分析和信息系统研究所（IAIS），圣奥古斯丁，北莱茵-威斯特法伦，德国
- en: Abstract
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter describes a number of different approaches to improve the performance
    of Pre-trained Language Models (PLMs), i.e. variants of BERT, autoregressive language
    models similar to GPT, and sequence-to-sequence models like Transformers. First
    we may modify the pre-training tasks to learn as much as possible about the syntax
    and semantics of language. Then we can extend the length of the input sequence
    to be able to process longer inputs. Multilingual models are simultaneously trained
    with text in different languages. Most important is the inclusion of further knowledge
    into the PLM to produce better predictions. It turns out that by increasing the
    number of parameters, the size of the training data and the computing effort the
    performance of the models can always be increased. There are a number of different
    fine-tuning strategies which allow the model to be adapted to special tasks. In
    addition, models may be instructed by few-shot prompts to solve specific tasks.
    This is especially rewarding for larger PLMs, which therefore are called Foundation
    Models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了多种提高预训练语言模型（PLM）性能的方法，即 BERT 的变体、类似于 GPT 的自回归语言模型以及像 Transformers 这样的序列到序列模型。首先，我们可能需要修改预训练任务，以便尽可能多地了解语言的语法和语义。然后，我们可以扩展输入序列的长度，以便能够处理更长的输入。多语言模型同时使用不同语言的文本进行训练。最重要的是将更多知识纳入
    PLM 以产生更好的预测。结果表明，通过增加参数数量、训练数据大小和计算工作量，模型的性能可以始终得到提高。存在多种不同的微调策略，这些策略允许模型适应特殊任务。此外，模型可以通过少量提示来解决特定任务。这对于较大的
    PLM 特别有益，因此它们被称为基础模型。
- en: KeywordsPre-training objectiveInput sizeMultilingual modelLong dependenciesAdditional
    knowledgeFine-tuningThis chapter describes a number of different approaches to
    improve the performance of *Pre-trained Language Models* (PLMs), i.e. variants
    of BERT, autoregressive language models similar to GPT, and sequence-to-sequence
    models like Transformers. When these models have a large number of parameters,
    they can be instructed by input prompts to solve new tasks and are called *Foundation
    Models*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词：预训练目标、输入大小、多语言模型、长依赖、额外知识、微调本章介绍了多种提高预训练语言模型（PLM）性能的方法，即 BERT 的变体、类似于 GPT
    的自回归语言模型以及像 Transformers 这样的序列到序列模型。当这些模型具有大量参数时，它们可以通过输入提示来解决新任务，并被称为**基础模型**。
- en: '**Modification of the pre-training tasks**. During pre-training with a large
    corpus the PLM should learn as much as possible about the syntax and semantics
    of language. By adapting and enhancing the pre-training objectives the performance
    of PLMs can be improved markedly, as shown in Sect. [3.1](#Sec1).'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修改预训练任务**。在用大型语料库进行预训练期间，PLM 应尽可能多地了解语言的语法和语义。通过调整和增强预训练目标，PLM 的性能可以得到显著提高，如第
    [3.1](#Sec1) 节所示。'
- en: '**Increase of the input size**. The length of the input sequence restricts
    the context, which can be taken into account by a PLM. This is especially important
    for applications like story generation. Simply increasing input length does not
    work, as then the number of parameters grows quadratically. In Sect. [3.2](#Sec7),
    alternatives for establishing sparse attention patterns for remote tokens are
    explored.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加输入大小**。输入序列的长度限制了 PLM 可以考虑的上下文，这对于像故事生成这样的应用尤为重要。简单地增加输入长度是不行的，因为这样参数数量会呈平方增长。在第
    [3.2](#Sec7) 节中，探讨了为远程标记建立稀疏注意力模式的替代方案。'
- en: '**Multilingual training** simultaneously trains the same model in different
    languages. By appropriate pre-training targets the models can generate a joint
    meaning representation in all languages. Especially for languages with little
    training data better results can be achieved Sect. [3.3](#Sec12).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多语言训练**同时在不同语言中训练相同的模型。通过适当的预训练目标，模型可以在所有语言中生成联合意义表示。特别是对于训练数据较少的语言，可以取得更好的结果，如第[3.3](#Sec12)节所述。'
- en: '**Adding extra knowledge**. PLMs can be enhanced by including additional information
    not covered by the training data. This is important as due to the restricted number
    of parameters PLMs cannot memorize all details included in the training data.
    Moreover, strict rules are usually represented only as weak associations and need
    to be reinforced. By incorporating facts and rules from an outside *knowledge
    base* (*KB*) or an additional text collection PLMs can obtain necessary information
    and keep the content up-to-date, as shown in Sect. [3.4](#Sec17).'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加额外知识**. PLM可以通过包含训练数据未涵盖的额外信息来增强。这很重要，因为由于参数数量有限，PLM无法记住训练数据中包含的所有细节。此外，严格的规则通常仅表示为弱关联，需要加强。通过结合来自外部*知识库*（*KB*）或额外文本集合的事实和规则，PLM可以获得必要的信息并保持内容更新，如第[3.4](#Sec17)节所示。'
- en: '**Changing the model size**. Theoretical results show that model performance
    improves when the PLMs become larger (Foundation Models). Hence, there is a general
    trend to increase model size, e.g. by forming mixture-of-experts. On the other
    hand, it may be necessary to reduce the computation effort and the memory footprint
    of a PLM. There are a number of techniques to achieve this without sacrificing
    much performance, as described in Sect. [3.5](#Sec24).'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改变模型大小**。理论结果表明，当PLM变得更大时（基础模型），模型性能会提高。因此，普遍趋势是增加模型大小，例如通过形成专家混合。另一方面，可能需要减少PLM的计算努力和内存占用。有许多技术可以实现这一点，同时不会牺牲太多性能，如第[3.5](#Sec24)节所述。'
- en: '**Fine-tuning for specific applications**. This can be performed according
    to different strategies, e.g. with several fine-tuning steps or multiple fine-tuning
    tasks. Larger PLMs usually can be instructed by prompts to perform specific tasks
    and are called Foundation Models. In addition, few-shot prompts may be optimized
    to achieve a more adequate model reaction. This is described in Sect. [3.6](#Sec31).'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**针对特定应用的微调**。这可以根据不同的策略进行，例如通过几个微调步骤或多个微调任务。较大的PLM通常可以通过提示来执行特定任务，被称为基础模型。此外，可以优化少量样本提示以实现更合适的模型反应。这将在第[3.6](#Sec31)节中描述。'
- en: Note that nearly all proposals may be combined for most model types, resulting
    in the vast number of model variants that is currently discussed.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，几乎所有提议都可以用于大多数模型类型，从而产生了目前讨论的巨大数量的模型变体。
- en: 3.1 Modifying Pre-training Objectives
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 修改预训练目标
- en: 'The basic BERT model [[49](#CR49)] has two pre-training tasks: the prediction
    of masked tokens with the masked language model (MLM) and next sentence prediction
    (NSP) (Sect. [2.​1](528393_1_En_2_Chapter.xhtml#Sec1)). These tasks were chosen
    heuristically and there are many plausible loss functions and architectures. Researchers
    have investigated many alternative training objectives, model structures, and
    attention mechanisms. In this section, the most promising of these variations
    of the BERT and Transformer architecture are discussed and their relative merits
    are compared.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 基本BERT模型[[49](#CR49)]有两个预训练任务：使用掩码语言模型（MLM）预测掩码标记和下一句预测（NSP）（第[2.1](528393_1_En_2_Chapter.xhtml#Sec1)节）。这些任务是通过启发式方法选择的，并且有许多可能的损失函数和架构。研究人员已经调查了许多替代训练目标、模型结构和注意力机制。在本节中，讨论了BERT和Transformer架构这些变体中最有希望的，并比较了它们的相对优点。
- en: An important question is the level of aggregation of the input sequence. Here
    subword tokens are standard. One option is to use raw letters as input. However,
    this may lead to a high computational burden, as the computational cost of self-attention
    grows quadratically with the size of the input. Another option is the use of domain-adapted
    knowledge to model the input sequence by learned tokenizations or patch embeddings
    (e.g. for image representation, Sect. [7.​2](528393_1_En_7_Chapter.xhtml#Sec12)).
    These methods reduce the input complexity, but may potentially ignore useful information
    in the input [[19](#CR19)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的问题是输入序列的聚合级别。这里子词标记是标准的。一个选项是使用原始字母作为输入。然而，这可能会导致高计算负担，因为自注意力的计算成本随着输入大小的平方增长。另一个选项是使用领域自适应知识通过学习标记化或补丁嵌入（例如，用于图像表示，见第
    [7.2](528393_1_En_7_Chapter.xhtml#Sec12) 节）来建模输入序列。这些方法减少了输入复杂性，但可能潜在地忽略了输入中的有用信息
    [[19](#CR19)]。
- en: 3.1.1 Autoencoders Similar to BERT
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 与 BERT 类似的自编码器
- en: 'To improve BERT’s performance a number of alternatives to capture knowledge
    from the unlabeled data were proposed:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高 BERT 的性能，提出了许多从未标记数据中捕获知识的方法：
- en: RoBERTa dynamically changes masks during training.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RoBERTa 在训练过程中动态改变掩码。
- en: ALBERT replaces the matrices for self-attention by a matrix product and shares
    parameters across all layers.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ALBERT 通过矩阵乘法替换自注意力矩阵，并在所有层之间共享参数。
- en: Predicting single masked tokens can be generalized. SpanBERT masks spans of
    tokens and predicts them. ELECTRA detects randomly replaced tokens at arbitrary
    positions. XLNet permutes the order of tokens in a sentence and predicts tokens
    left to right similar to a language model.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测单个掩码标记可以推广。SpanBERT 掩码标记的跨度并预测它们。ELECTRA 在任意位置检测随机替换的标记。XLNet 重新排列句子中标记的顺序并从左到右预测标记，类似于语言模型。
- en: DeBERTa disentangles the embeddings for content and position.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeBERTa 将内容和位置的嵌入解耦。
- en: The details are given in the following paragraphs. Popular loss functions are
    defined in Table [3.1](#Tab1). A list of prominent autoencoders is provided in
    Table [3.2](#Tab2). They can be compared by their performance on natural language
    understanding tasks (Sect. [2.​1.​5](528393_1_En_2_Chapter.xhtml#Sec7)) like GLUE
    [[218](#CR218)].Table 3.1
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 详细内容见下文。常见的损失函数定义在表 [3.1](#Tab1) 中。表 [3.2](#Tab2) 提供了一系列突出的自编码器。它们可以通过在自然语言理解任务（见第
    [2.1.5](528393_1_En_2_Chapter.xhtml#Sec7) 节）上的性能进行比较，如 GLUE [[218](#CR218)]。表
    3.1
- en: Loss functions for PLMs. A sequence is denoted by ***x*** = (*x*[1], …, *x*[*T*])
    and ***z*** = (*z*[1], …, *z*[*R*]) is a related sequence, e.g. a translation
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: PLM 的损失函数。一个序列表示为 ***x*** = (*x*[1], …, *x*[*T*])，而 ***z*** = (*z*[1], …, *z*[*R*])
    是一个相关序列，例如翻译
- en: '| Name | Loss function | Description |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 损失函数 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| MC multivariate classification | ![$$L_{MC}= - \log p(y&#124;{\boldsymbol
    {x}})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq1.png) |
    For each training instance (***x***, *y*), e.g. logistic classifier, Sect. [1.​3](528393_1_En_1_Chapter.xhtml#Sec3)
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 多元分类 | ![多元分类损失函数](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq1.png)
    | 对于每个训练实例（***x***, *y*），例如逻辑分类器，见第 [1.3](528393_1_En_1_Chapter.xhtml#Sec3) 节
    |'
- en: '| NM neighborhood model | ![$$L_{NM}= - \sum _{t=1}^T\sum _{i\in N(t)}\log
    p(x_i&#124;x_{t})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq2.png)
    | For neighborhood *N*(*t*) =  {*t*−*k*, …, *t*−1, *t*+1, …, *t*+*k*}, e.g. word2vec,
    Sect. [1.​5](528393_1_En_1_Chapter.xhtml#Sec5) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 邻域模型 | ![邻域模型损失函数](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq2.png)
    | 对于邻域 *N*(*t*) = { *t*−*k*, …, *t*−1, *t*+1, …, *t*+*k* }，例如 word2vec，见第 [1.5](528393_1_En_1_Chapter.xhtml#Sec5)
    节 |'
- en: '| LM language model | ![$$L_{LM}= - \sum _{t=1}^T\log p(x_t&#124;{\boldsymbol
    {x}}_{&lt;t})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq3.png)
    | e.g. RNN Sect. [1.​6](528393_1_En_1_Chapter.xhtml#Sec6), GPT Sect. [2.​2.​2](528393_1_En_2_Chapter.xhtml#Sec13)
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 语言模型 | ![语言模型损失函数](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq3.png)
    | 例如 RNN 见第 [1.6](528393_1_En_1_Chapter.xhtml#Sec6) 节，GPT 见第 [2.2.2](528393_1_En_2_Chapter.xhtml#Sec13)
    节 |'
- en: '| S2S sequence-to-sequence model | ![$$L_{S2S}= - \sum _{t=1}^{n_z}\log p(z_t&#124;{\boldsymbol
    {z}}_{&lt;t},{\boldsymbol {x}})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq4.png)
    | For input sequence ***x*** = (*x*[1], …, *x*[*T*]) and translation ***z*** = (*z*[1],
    …, *z*[*R*]) Sects. [1.​6](528393_1_En_1_Chapter.xhtml#Sec6) and [2.​3](528393_1_En_2_Chapter.xhtml#Sec19)
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| S2S 序列到序列模型 | ![S2S 序列到序列模型的公式](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq4.png)
    | 对于输入序列 ***x*** = (*x*[1], …, *x*[*T*]) 和翻译 ***z*** = (*z*[1], …, *z*[*R*])，章节
    [1.6](528393_1_En_1_Chapter.xhtml#Sec6) 和 [2.3](528393_1_En_2_Chapter.xhtml#Sec19)
    |'
- en: '| MLM masked language model | ![$$L_{MLM}= - \sum _{t\in m({\boldsymbol {x}})}\log
    p(x_t&#124;\tilde {{\boldsymbol {x}}})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq5.png)
    | *m*(***x*** ) contains the indices of masked tokens in ***x***. In ![$$\tilde
    {{\boldsymbol {x}}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq6.png)
    the masked tokens are replaced by *MASK*, e.g. BERT, Sect. [2.​1](528393_1_En_2_Chapter.xhtml#Sec1)
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| MLM 掩码语言模型 | ![MLM 掩码语言模型的公式](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq5.png)
    | *m*(***x***) 包含 ***x*** 中掩码标记的索引。在 ![~x](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq6.png)
    中，掩码标记被替换为 *MASK*，例如 BERT，章节 [2.1](528393_1_En_2_Chapter.xhtml#Sec1) |'
- en: '| TLM translation masked language model | ![$$L_{TLM}= - \sum _{t\in m(x)}\log
    p(x_t&#124;\tilde {{\boldsymbol {x}}})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq7.png)
    | *m*(***x*** ) contains the indices of masked tokens. ![$$\tilde {{\boldsymbol
    {x}}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq8.png) contains
    a sentence and its translation. Masked tokens are replaced by *MASK*, e.g. mBERT,
    Sect. [3.3](#Sec12) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| TLM 翻译掩码语言模型 | ![TLM 翻译掩码语言模型的公式](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq7.png)
    | *m*(***x***) 包含掩码标记的索引。![~x](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq8.png)
    包含一个句子及其翻译。掩码标记被替换为 *MASK*，例如 mBERT，章节 [3.3](#Sec12) |'
- en: '| SBO span boundary objective | ![$$L_{SMLM}= - \sum _{(i:j)\in m({\boldsymbol
    {x}})}\log p({\boldsymbol {x}}_{i:j}&#124;\tilde {{\boldsymbol {x}}})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq9.png)
    | *m*(***x*** ) contains the spans (*i* : *j*) of masked tokens in ***x***. In
    ![$$\tilde {{\boldsymbol {x}}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq10.png)
    the masked tokens are replaced by other tokens, e.g. SpanBERT, Sect. [3.1.1](#Sec2)
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| SBO 跨度边界目标 | ![SBO 跨度边界目标的公式](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq9.png)
    | *m*(***x***) 包含 ***x*** 中掩码标记的跨度 (*i* : *j*)。在 ![~x](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq10.png)
    中，掩码标记被替换为其他标记，例如 SpanBERT，章节 [3.1.1](#Sec2) |'
- en: '| PLM permutation language model | ![$$L_{PLM}= - \sum _{t=1}^{T}\log p(z_t&#124;{\boldsymbol
    {z}}_{&lt;t})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq11.png)
    | ***z***=*perm*(***x***) is a permutation of ***x***, e.g. XLNet, Sect. [3.1.1](#Sec2)
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| PLM 排列语言模型 | ![PLM 排列语言模型的公式](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq11.png)
    | ***z*** = *perm*(***x***) 是 ***x*** 的排列，例如 XLNet，章节 [3.1.1](#Sec2) |'
- en: '| NSP next sentence prediction | ![$$L_{NSP}= - \log p(\xi &#124;{\boldsymbol
    {x}},{\boldsymbol {z}})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq12.png)
    | *ξ*=1 if text ***z*** after *x* (else ***z*** is randomly selected), e.g. BERT,
    Sect. [2.​1](528393_1_En_2_Chapter.xhtml#Sec1) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| NSP 下一个句子预测 | ![NSP 下一个句子预测的公式](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq12.png)
    | *ξ*=1 如果文本中 ***z*** 在 *x* 之后（否则 ***z*** 是随机选择的），例如 BERT，章节 [2.1](528393_1_En_2_Chapter.xhtml#Sec1)
    |'
- en: '| SOP sentence order prediction | ![$$L_{SOP}= - \log p(\xi &#124;{\boldsymbol
    {x}},{\boldsymbol {z}})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq13.png)
    | *ξ*=1 if text ***z*** after ***x*** (else ***x*** after ***z***), e.g. ALBERT,
    Sect. [3.1.1](#Sec2) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| SOP 句子顺序预测 | ![SOP 句子顺序预测的公式](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq13.png)
    | *ξ*=1 如果文本中 ***z*** 在 ***x*** 之后（否则 ***x*** 在 ***z*** 之后），例如 ALBERT，章节 [3.1.1](#Sec2)
    |'
- en: '| RTD replaced token detection | ![$$L_{RTD}= -\log \sum _{t=1}^{T} p(x_t{=}\tilde
    {x}_t&#124;\tilde {{\boldsymbol {x}}}) $$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq14.png)
    | In ![$$\tilde {{\boldsymbol {x}}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq15.png)
    randomly selected elements of ***x*** were replaced, e.g. ELECTRA, Sect. [3.1.1](#Sec2)
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| RTD 替换标记检测 | ![$$L_{RTD}= -\log \sum _{t=1}^{T} p(x_t{=}\tilde {x}_t&#124;\tilde
    {{\boldsymbol {x}}}) $$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq14.png)
    | 在 ![$$\tilde {{\boldsymbol {x}}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq15.png)
    中随机选择的 ***x*** 元素被替换，例如 ELECTRA，第 [3.1.1](#Sec2) 节 |'
- en: Table 3.2
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.2
- en: Autoencoders similar to BERT. The pre-training and fine-tuning loss functions
    are defined in Table [3.1](#Tab1). The benchmark figures are only a hint, as they
    depend on the number of parameters and the computing effort
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与 BERT 类似的自编码器。预训练和微调损失函数在表 [3.1](#Tab1) 中定义。基准数字只是一个提示，因为它们取决于参数数量和计算工作量。
- en: '| Model | Section | Pre-training | Fine-tuning | Extra | Benchmark |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 部分 | 预训练 | 微调 | 额外 | 基准 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| ELMo [[156](#CR156)] | [1.​6](528393_1_En_1_Chapter.xhtml#Sec6) | BiLM |
    MC | Use bidirectional LSTM | GLUE 71.0 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| ELMo [[156](#CR156)] | [1.​6](528393_1_En_1_Chapter.xhtml#Sec6) | BiLM |
    MC | 使用双向 LSTM | GLUE 71.0 |'
- en: '| BERT [[49](#CR49)] | [2.​1](528393_1_En_2_Chapter.xhtml#Sec1) | MLM + NSP
    | MC | Predict masked tokens | GLUE 80.5 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| BERT [[49](#CR49)] | [2.​1](528393_1_En_2_Chapter.xhtml#Sec1) | MLM + NSP
    | MC | 预测掩码标记 | GLUE 80.5 |'
- en: '| RoBERTa [[127](#CR127)] | [3.1.1](#Sec2) | MLM | MC | Train longer, new mask
    in new epoch | GLUE 88.5 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa [[127](#CR127)] | [3.1.1](#Sec2) | MLM | MC | 训练更长，新 epoch 中的新掩码
    | GLUE 88.5 |'
- en: '| SpanBERT [[98](#CR98)] | [3.1.1](#Sec2) | PLM, SBO | MC | Predict spans of
    tokens | GLUE 82.8 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| SpanBERT [[98](#CR98)] | [3.1.1](#Sec2) | PLM, SBO | MC | 预测标记跨度 | GLUE 82.8
    |'
- en: '| ELECTRA [[223](#CR223)] | [3.1.1](#Sec2) | RTD | MC | Replaced token detection
    | GLUE 89.4 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| ELECTRA [[223](#CR223)] | [3.1.1](#Sec2) | RTD | MC | 替换标记检测 | GLUE 89.4
    |'
- en: '| StructBERT [[39](#CR39)] | [3.1.1](#Sec2) | RTD | MC | Reorder shuffled tokens
    | GLUE 89.0 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| StructBERT [[39](#CR39)] | [3.1.1](#Sec2) | RTD | MC | 重新排序打乱后的标记 | GLUE
    89.0 |'
- en: '| ALBERT [[113](#CR113)] | [3.1.1](#Sec2) | MLM + SOP | MC | Factorized embeddings,
    parameter sharing | GLUE 89.4 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| ALBERT [[113](#CR113)] | [3.1.1](#Sec2) | MLM + SOP | MC | 因子嵌入，参数共享 | GLUE
    89.4 |'
- en: '| XLNET [[240](#CR240)] | [3.1.1](#Sec2) | PLM | MC | Predict permuted tokens
    | GLUE 90.5 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| XLNET [[240](#CR240)] | [3.1.1](#Sec2) | PLM | MC | 预测排列后的标记 | GLUE 90.5
    |'
- en: '| DeBERTa [[76](#CR76)] | [3.1.1](#Sec2) | MLM | MC, S2S | Disentangled attention
    | GLUE 90.0 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| DeBERTa [[76](#CR76)] | [3.1.1](#Sec2) | MLM | MC, S2S | 解耦注意力 | GLUE 90.0
    |'
- en: '| Prod. Key [[112](#CR112)] | [3.1.1](#Sec2) | MLM | MC | Nearest neighbor
    | – |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Prod. Key [[112](#CR112)] | [3.1.1](#Sec2) | MLM | MC | 最近邻 | – |'
- en: '| UniLM [[8](#CR8)] | [3.1.3](#Sec4) | MLM, LM | MC, LM | Uni- and bidirectional
    | GLUE 87.3 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| UniLM [[8](#CR8)] | [3.1.3](#Sec4) | MLM, LM | MC, LM | 单向和双向 | GLUE 87.3
    |'
- en: '| BigBird [[247](#CR247)] | [3.2.1](#Sec8) | MLM | MC, S2S | Sparse attention
    mechanism | TriviaQA 84.5 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| BigBird [[247](#CR247)] | [3.2.1](#Sec8) | MLM | MC, S2S | 稀疏注意力机制 | TriviaQA
    84.5 |'
- en: '**RoBERTa** [[127](#CR127)] is an enhanced BERT model boosted by tweaking parts
    of the pre-training process. The authors improved the BERT[BASE] architecture
    by the following changes: (1) Instead of using the same mask for all epochs, they
    replicate training sequences with different masks. (2) They remove the Next-Sentence-Prediction
    objective and found that performance is best, when all sentences in a batch are
    from the same document. (3) Larger batches with larger step sizes increase perplexity
    for both the masked language model task and downstream task performance. (4) A
    10-fold increase of training data to 160 GB, which is used in large batches. The
    resulting model achieves an impressive Sota result of 88.5 on *GLUE* (language
    understanding [[217](#CR217)]), and the reading comprehension tasks *RACE* and
    *SQuAD* [[173](#CR173)].'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**RoBERTa** [[127](#CR127)] 是一个通过调整预训练过程部分内容而增强的 BERT 模型。作者通过以下变更改进了 BERT[BASE]
    架构：(1) 他们不再使用所有 epoch 中相同的掩码，而是使用不同掩码复制训练序列。(2) 他们移除了 Next-Sentence-Prediction
    目标，并发现当一批中的所有句子都来自同一文档时，性能最佳。(3) 更大的批次和更大的步长增加了掩码语言模型任务和下游任务性能的困惑度。(4) 将训练数据增加到
    160 GB，用于大批次，这是通过 10 倍增加训练数据实现的。该模型在 *GLUE* (*语言理解 [[217](#CR217)]) 上实现了令人印象深刻的
    Sota 结果 88.5，以及阅读理解任务 *RACE* 和 *SQuAD* [[173](#CR173)]。'
- en: '**SpanBERT** [[98](#CR98)] introduces a span-level pre-training approach. Rather
    than masking single tokens during pre-training, spans of one or more complete
    words are masked covering about 15% of the tokens. A new span-boundary objective
    (SBO) is introduced, where tokens inside of the masked span are predicted, using
    only representations of the tokens just outside the boundaries of the span combined
    with positional information. The details are shown in Fig. [3.1](#Fig1). SBO is
    used together with the usual MLM objective. Finally, the authors omit the next
    sentence prediction task as in [[127](#CR127)] and only use single text fragments/sentences
    for training. The authors find that masking random spans is more effective than
    masking linguistic units. SpanBERT has the same configuration as BERT[LARGE] and
    is pre-trained on the BooksCorpus and the English Wikipedia. SpanBERT achieves
    a new Sota of 79.6% F1 on the *OntoNotes coreference task* [[164](#CR164)], which
    requires identifying pronouns and the corresponding nouns or two phrases referring
    to the same thing (Sect. [5.​4.​1](528393_1_En_5_Chapter.xhtml#Sec20)).![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig1_HTML.png)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**SpanBERT** [[98](#CR98)] 引入了一种跨度级别的预训练方法。在预训练期间，不是掩盖单个标记，而是掩盖一个或多个完整单词的跨度，覆盖大约15%的标记。引入了一个新的跨度边界目标（SBO），其中使用跨度边界外的标记表示，并结合位置信息来预测掩盖跨度内的标记。详细信息如图[3.1](#Fig1)所示。SBO与常用的MLM目标一起使用。最后，作者省略了下一句预测任务，如[[127](#CR127)]中所述，并且只使用单个文本片段/句子进行训练。作者发现，掩盖随机跨度比掩盖语言单位更有效。SpanBERT具有与BERT[LARGE]相同的配置，并在BooksCorpus和英语维基百科上进行预训练。SpanBERT在*OntoNotes核心ference任务*
    [[164](#CR164)]上实现了新的Sota 79.6% F1，该任务需要识别代词及其对应的名词或两个指代同一事物的短语（第[5.4.1](528393_1_En_5_Chapter.xhtml#Sec20)节）！[](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig1_HTML.png)'
- en: An illustration represents the approach of span B E R T. It indicates the layers
    of input, Input and output embeddings through transformer encoder, left and right
    boundary embeddings, position embeddings, 2-layer network, token probabilities,
    and predicted tokens.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一幅插图展示了Span B E R T的方法。它表示了输入层、输入和输出嵌入通过Transformer编码器、左右边界嵌入、位置嵌入、2层网络、标记概率和预测标记。
- en: Fig. 3.1
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1
- en: SpanBERT [[98](#CR98)] concatenates the embeddings outside the border of a span
    with a position embedding. With this input a 2-layer model predicts the probabilities
    of masked tokens
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: SpanBERT [[98](#CR98)] 将跨度边界外的嵌入与位置嵌入连接起来。使用这个输入，一个2层模型预测被掩盖的标记的概率
- en: '**StructBERT** [[223](#CR223)] enhances the original BERT MLM objective by
    the task to predict the order of shuffled token triples. In addition, the order
    of three sentences has to be detected. Using models with the same number of parameters,
    StructBERT can increase the Sota on GLUE in comparison to BERT and RoBERTa to
    83.9 and 89.0, respectively.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**StructBERT** [[223](#CR223)] 通过预测打乱标记三元组的顺序的任务增强了原始BERT MLM目标。此外，还需要检测三个句子的顺序。使用与BERT和RoBERTa相同数量的参数的模型，StructBERT可以将GLUE上的Sota分别提高到83.9和89.0。'
- en: '**Electra** [[39](#CR39)] proposes a new pre-training task called *replaced
    token detection* (RTD). In the paper a generator network, trained with a masked
    language model loss, is combined with a discriminator network. Some tokens in
    the input sequence are replaced with plausible alternatives which are generated
    by a small language model (about 1∕4 of the size of the discriminator). The discriminator
    network has to predict for every token, whether it is a replacement or not. This
    corruption procedure solves a mismatch in BERT, where *MASK* tokens appear in
    pre-training but not in fine-tuning. The model learns from all input tokens instead
    of just the small masked subset, making it more computationally efficient than
    e.g. BERT and RoBERTa, while performing better on several tasks, e.g. 89.4% on
    the GLUE language understanding task.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**Electra** [[39](#CR39)] 提出了一种新的预训练任务，称为*替换标记检测*（RTD）。在论文中，一个与掩盖语言模型损失一起训练的生成网络与一个判别网络相结合。输入序列中的某些标记被替换为由小型语言模型（判别网络大小的约1/4）生成的合理替代品。判别网络必须预测每个标记是否被替换。这种破坏过程解决了BERT中的不匹配，其中*MASK*标记出现在预训练中但不在微调中。该模型从所有输入标记中学习，而不是只从小的掩盖子集中学习，这使得它在计算效率上比BERT和RoBERTa等模型更高，同时在多个任务上表现更好，例如在GLUE语言理解任务上达到89.4%。'
- en: '**ALBERT** (a lite BERT) [[113](#CR113)] uses two parameter-reduction techniques
    to tackle the huge memory consumption of BERT and its slow training speed. The
    first tweak is untying the dimensionality of the WordPiece embeddings from the
    hidden layer size of BERT. Instead of using a single embedding matrix *M*, the
    authors factorize *M* = *A* ∗ *B*, such that the joint number of parameters in
    *A* and *B* is much lower than the number of parameters in *M*. The second tweak
    is sharing all parameters across all layers of BERT, which is shown to stabilize
    training and keep the number of parameters fixed even if more layers are added.
    In addition to the two tweaks, a new sentence order prediction (SOP) is introduced.
    Specifically, the model has to predict if the order of two sentences is correct
    or reversed. The authors report that this task improves accuracy compared to BERT’s
    NSP task, which could be solved by comparing the topics of the two sentences.
    It is still unclear, however, if this is the best way to incorporate text structure
    in training. ALBERT achieved new Sota results on GLUE and SQuAD.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**ALBERT**（轻量级的BERT）[[113](#CR113)] 使用两种参数减少技术来解决BERT及其缓慢的训练速度带来的巨大内存消耗问题。第一种调整是将WordPiece嵌入的维度与BERT隐藏层的大小解耦。作者将矩阵
    *M* 分解为 *A* 和 *B*，使得 *A* 和 *B* 的参数总数远低于 *M* 的参数总数。第二种调整是在BERT的所有层之间共享所有参数，这已被证明可以稳定训练并保持参数数量不变，即使添加了更多层。除了这两种调整外，还引入了一种新的句子顺序预测（SOP）。具体来说，模型必须预测两个句子的顺序是否正确或相反。作者报告说，与BERT的NSP任务相比，这项任务提高了准确性，该任务可以通过比较两个句子的主题来解决。然而，目前尚不清楚这是否是最佳地将文本结构纳入训练的方法。ALBERT在GLUE和SQuAD上实现了新的Sota结果。'
- en: '**XLNet** solves an autoregressive pre-training task instead of predicting
    masked words [[240](#CR240)]. This addresses the problem that BERT’s *[MASK]*
    token only appears during pre-training and not in fine-tuning. The words in a
    sequence, e.g. *“The*[1]*mouse*[2]*likes*[3]*cheese*[4]”, are reordered together
    with their position information (indices) by a random permutation, e.g. *“cheese*[4]*The*[1]*likes*[3]*mouse*[2]”.
    The task is to successively predict the tokens in the permuted sequence similarly
    to a GPT language model. The model has to predict, e.g. *p*(*mouse*—2, *cheese*[4],*The*[1],*likes*[3]).
    Note that the model must additionally know the position, here 2, of the word to
    be predicted. The transformer, however, mixes the position information with the
    content information by forming a sum. Hence, the position information is inseparable
    from the token embedding.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**XLNet** 解决的是自回归预训练任务，而不是预测掩码词 [[240](#CR240)]。这解决了BERT的 *token* 在预训练期间出现，但在微调期间不出现的问题。序列中的单词，例如
    *“The*[1]*mouse*[2]*likes*[3]*cheese*[4]*”，与其位置信息（索引）一起通过随机排列重新排序，例如 *“cheese*[4]*The*[1]*likes*[3]*mouse*[2]*”。任务是依次预测打乱序列中的标记，类似于GPT语言模型。模型必须预测，例如
    *p*(*mouse*—2, *cheese*[4],*The*[1],*likes*[3])。请注意，模型还必须知道要预测的单词的位置，即这里为2。然而，transformer通过形成总和将位置信息与内容信息混合，因此位置信息与标记嵌入不可分离。'
- en: Therefore, the authors decided to compute an additional self-attention embedding
    called *query stream*, which as query only receives the target position and then
    can compute the attention with the key and value vectors (Sect. [2.​1.​1](528393_1_En_2_Chapter.xhtml#Sec3)).
    The resulting embedding encodes the position of the token to be predicted and
    correlations to other tokens, but has no information on the content of that token.
    This information can be added as input to the model. The normal self-attention
    and the query stream have the same parameter matrices *Q* (query),*K* (key), *V* 
    (value). To save training effort, XLNet only predicts a few tokens at the end
    of the permuted sequence. In addition, XLNet integrates the segment recurrence
    mechanism and relative encoding scheme of Transformer-XL (Sect. [3.2.2](#Sec9))
    into pre-training, which empirically improves the performance especially for tasks
    involving a longer text sequence.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作者决定计算一个额外的自注意力嵌入，称为 *query stream*，它作为查询只接收目标位置，然后可以与键和值向量（Sect. [2.1.1](528393_1_En_2_Chapter.xhtml#Sec3)）计算注意力。生成的嵌入编码了要预测的标记的位置和其他标记的相关性，但没有关于该标记内容的信息。这些信息可以作为输入添加到模型中。正常自注意力和查询流具有相同的参数矩阵
    *Q*（查询），*K*（键），*V*（值）。为了节省训练努力，XLNet只在打乱序列的末尾预测几个标记。此外，XLNet将Transformer-XL的段递归机制和相对编码方案（Sect.
    [3.2.2](#Sec9)）集成到预训练中，这在经验上提高了性能，特别是对于涉及较长的文本序列的任务。
- en: When a token is predicted information about tokens before and after it may be
    used. Therefore, the model is a bidirectional encoder. With BERT, if the two tokens
    *“New”* and *“York”* are masked, both words are predicted independently, ignoring
    valuable information. In contrast, XLNet properly handles the dependence of masked
    tokens. XLNet was able to outperform BERT and RoBERTa on many tasks, e.g. the
    GLUE language understanding tasks, reading comprehension tasks like SQuAD (Sect.
    [2.​1.​5](528393_1_En_2_Chapter.xhtml#Sec8)), text classification tasks such as
    *IMDB* (movie review classification) [[130](#CR130)].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当预测一个标记时，可以使用关于它之前和之后的标记的信息。因此，该模型是一个双向编码器。在 BERT 中，如果两个标记 *“New”* 和 *“York”*
    被掩码，这两个词都是独立预测的，忽略了有价值的信息。相比之下，XLNet 正确处理了掩码标记的依赖性。XLNet 能够在许多任务上超越 BERT 和 RoBERTa，例如
    GLUE 语言理解任务、阅读理解任务如 SQuAD（第 [2.1.5](528393_1_En_2_Chapter.xhtml#Sec8) 节）、文本分类任务，如
    *IMDB*（电影评论分类）[[130](#CR130)]。
- en: '**Product Keys** [[112](#CR112)] replace the dot-product attention by a nearest
    neighbor search. A query ***q***[*r*] is split into two sub-queries ![$${\boldsymbol
    {q}}_{r}^{[1]}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq16.png)
    and ![$${\boldsymbol {q}}_{r}^{[2]}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq17.png).
    For each sub-query the *k* closest sub-keys ![$$\boldsymbol {k}_i^{[1]}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq18.png)
    and ![$$\boldsymbol {k}_j^{[2]}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq19.png)
    are selected. From the *k*² combinations of sub-keys the highest dot products
    can be efficiently computed and the *k* highest combinations are selected. The
    results are normalized with the softmax function and used for the computation
    of a weighted sum of value vectors. During optimization only the *k* optimal keys
    are affected reducing the training effort. The approach allows very large transformers
    to be defined with only a minimal computational overhead. With 12 layers the authors
    achieve the same performance as a 24 layer BERT model using only half of the computation
    time. In a comprehensive comparison of transformer architectures [[142](#CR142)]
    the approach yields an increase for SuperGLUE NLU task (Sect. [4.​1.​2](528393_1_En_4_Chapter.xhtml#Sec3))
    from 71.7% for the standard T5 model to 75.2%.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**产品密钥** [[112](#CR112)] 通过最近邻搜索替换了点积注意力。查询 ***q***[*r*] 被分割成两个子查询 ![$${\boldsymbol
    {q}}_{r}^{[1]}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq16.png)
    和 ![$${\boldsymbol {q}}_{r}^{[2]}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq17.png)。对于每个子查询，选择
    *k* 个最近的子密钥 ![$$\boldsymbol {k}_i^{[1]}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq18.png)
    和 ![$$\boldsymbol {k}_j^{[2]}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq19.png)。从子密钥的
    *k*² 组合中，可以高效地计算出最大的点积，并选择 *k* 个最高的组合。结果通过 softmax 函数进行归一化，并用于计算值向量的加权总和。在优化过程中，只有
    *k* 个最优密钥受到影响，从而减少了训练工作量。这种方法允许定义具有最小计算开销的非常大的转换器。使用 12 层，作者实现了与 24 层 BERT 模型相同的性能，但计算时间只有一半。在一个全面的转换器架构比较
    [[142](#CR142)] 中，该方法使 SuperGLUE NLU 任务（第 [4.1.2](528393_1_En_4_Chapter.xhtml#Sec3)
    节）的性能从标准 T5 模型的 71.7% 提高到 75.2%。'
- en: '**DeBERTa** [[76](#CR76)] uses a *disentangled attention* mechanism, where
    each word is represented by two different types of vectors encoding content and
    position. The attention weights between tokens are computed using different matrices
    for content and relative position. In addition, DeBERTa includes absolute word
    positions in the last layer to capture different syntactic roles in the sentence.
    During fine-tuning the model employs an “adversarial” training approach, where
    embeddings are normalized to probability vectors. Then the model is trained to
    be robust against small perturbations of embeddings. According to the authors,
    this improves the performance of fine-tuned models. The large version of the model
    with 1.5B parameters has superior performance in several application areas, e.g.
    in natural language understanding (Sect. [4.​1.​2](528393_1_En_4_Chapter.xhtml#Sec3)),
    where DeBERTa surpasses the human performance on the *SuperGLUE benchmark* [[219](#CR219)]
    for the first time, increasing the macro-average score to 89.9%.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**DeBERTa** [[76](#CR76)] 使用了一种*解耦注意力*机制，其中每个词由两种不同类型的向量表示，分别编码内容和位置。使用不同的矩阵计算标记之间的注意力权重，用于内容和相对位置。此外，DeBERTa在最后一层中包含绝对词位置，以捕捉句子中的不同句法角色。在微调过程中，模型采用了一种“对抗”的训练方法，其中嵌入被归一化为概率向量。然后模型被训练以对嵌入的小扰动具有鲁棒性。根据作者的说法，这提高了微调模型的表现。该模型的大版本拥有15亿个参数，在多个应用领域表现出色，例如在自然语言理解（第[4.1.2](528393_1_En_4_Chapter.xhtml#Sec3)节）中，DeBERTa首次在*SuperGLUE基准*
    [[219](#CR219)]上超越了人类表现，将宏平均分数提高到89.9%。'
- en: Bengio et al. [[12](#CR12)] argue that representations, e.g. embeddings, should
    be *disentangled* and should represent different content aspects, e.g. syntax,
    style, semantics, in different parts of the embedding vector. Locatello et al.
    [[129](#CR129)] have proven that this is not possible in an unsupervised way.
    Hence, some explicit supervision or prior information has to be used to generate
    interpretable subvectors of embeddings.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Bengio等人[[12](#CR12)]认为，表示，例如嵌入，应该是*解耦*的，并且应该在不同嵌入向量的不同部分表示不同的内容方面，例如语法、风格、语义。Locatello等人[[129](#CR129)]已经证明这不可能以无监督的方式进行。因此，必须使用一些显式的监督或先验信息来生成嵌入的可解释子向量。
- en: '**DeBERTaV3** [[75](#CR75)] substitutes the MLM loss of DeBERTa with the replaced
    token detection (RTD) of Electra (Sect. [3.1.1](#Sec2)). In addition, a new gradient-disentangled
    embedding sharing method is employed that improves both training efficiency and
    the quality of the pre-trained model. Its largest version has a 128k-token vocabulary,
    24 layers, and 304M parameters. For the GLUE benchmark with fine-tuning, the model
    increases the score by 1.4% to a new Sota of 91.4%. The multi-language version
    of the model mDeBERTa[BASE] outperforms XLM-R[BASE] by 3.6% in terms of the cross
    lingual transfer accuracy on the *XNLI* task (Sect. [3.3.1](#Sec13)).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**DeBERTaV3** [[75](#CR75)] 用Electra（第[3.1.1](#Sec2)节）的替换标记检测（RTD）替换了DeBERTa的MLM损失。此外，还采用了一种新的梯度解耦嵌入共享方法，该方法提高了训练效率和预训练模型的质量。其最大版本拥有128k个标记的词汇表，24层，和3.04亿个参数。对于经过微调的GLUE基准，该模型将分数提高了1.4%，达到新的Sota
    91.4%。该模型的多语言版本mDeBERTa[BASE]在*XNLI*任务（第[3.3.1](#Sec13)节）的跨语言迁移准确性方面比XLM-R[BASE]高出3.6%。'
- en: 3.1.2 Autoregressive Language Models Similar to GPT
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 与GPT类似的自回归语言模型
- en: By increasing the number of parameters and the training set size the capabilities
    of GPT models can be markedly improved. An overview is given in Table [3.3](#Tab3).Table
    3.3
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增加参数数量和训练集大小，可以显著提高GPT模型的能力。概述见表[3.3](#Tab3)。表3.3
- en: Autoregressive language models (LM) similar to GPT. ‘Details’ provides the number
    of parameters and specific features. The ‘benchmark’ figures are only a hint,
    as they depend on the selected number of parameters and the computing effort.
    Best benchmark value printed in bold
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 与GPT类似的自回归语言模型（LM）。“详细信息”提供了参数数量和特定功能。“基准”数字仅作为一个提示，因为它们取决于所选参数数量和计算工作量。最佳基准值以粗体显示
- en: '| Model | Section | Details | Benchmark |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 章节 | 详细信息 | 基准 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| GPT-2 [[167](#CR167)] | [2.​2](528393_1_En_2_Chapter.xhtml#Sec11) | 1.6B
    LM to generate text | Lambada 0-shot 63.2% |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 [[167](#CR167)] | [2.​2](528393_1_En_2_Chapter.xhtml#Sec11) | 1.6B
    LM用于生成文本 | Lambada 0-shot 63.2% |'
- en: '| Retro [[21](#CR21)] | [6.​2.​3](528393_1_En_6_Chapter.xhtml#Sec15) | 7B LM
    with retrieval to generate text | Lambada 73.0% |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Retro [[21](#CR21)] | [6.​2.​3](528393_1_En_6_Chapter.xhtml#Sec15) | 7B LM与检索生成文本
    | Lambada 73.0% |'
- en: '| Megatron-LM [[193](#CR193)] | [3.1.2](#Sec3) | 8.3B LM to generate text |
    Lambada 66.5% |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Megatron-LM [[193](#CR193)] | [3.1.2](#Sec3) | 8.3B 语言模型生成文本 | Lambada 66.5%
    |'
- en: '| Turing-NLG [[179](#CR179)] | [3.1.2](#Sec3) | 17B LM to generate text | Lambada
    68.0% |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Turing-NLG [[179](#CR179)] | [3.1.2](#Sec3) | 17B 语言模型生成文本 | Lambada 68.0%
    |'
- en: '| Chinchilla [[83](#CR83)] | [3.1.2](#Sec3) | 70B LM to generate text | Lambada
    0-shot 77.4% |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 马海毛 [[83](#CR83)] | [3.1.2](#Sec3) | 70B 语言模型生成文本 | Lambada 0-shot 77.4%
    |'
- en: '| GPT-3 [[25](#CR25)] | [3.1.2](#Sec3) | 175B long sequence LM to generate
    text | Lambada 0-shot 76.2% |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3 [[25](#CR25)] | [3.1.2](#Sec3) | 175B 长序列语言模型生成文本 | Lambada 0-shot
    76.2% |'
- en: '| WebGPT [[25](#CR25)] | [6.​2.​3](528393_1_En_6_Chapter.xhtml#Sec16) | 175B
    GPT-3 + Bing search engine | Same as GPT-3 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| WebGPT [[25](#CR25)] | [6.2.3](528393_1_En_6_Chapter.xhtml#Sec16) | 175B
    GPT-3 + Bing 搜索引擎 | 与 GPT-3 相同 |'
- en: '| InstructGPT [[151](#CR151)] | [3.6.5](#Sec43) | 175B GPT-3 fine-tuned for
    instructions | Same as GPT-3 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| InstructGPT [[151](#CR151)] | [3.6.5](#Sec43) | 175B GPT-3 优化指令 | 与 GPT-3
    相同 |'
- en: '| OPT [[151](#CR151)] | [3.1.2](#Sec3) | free 175B LM similar to GPT-3 | Lambada
    0-shot 74.7% |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| OPT [[151](#CR151)] | [3.1.2](#Sec3) | 免费类似 GPT-3 的 175B 语言模型 | Lambada 0-shot
    74.7% |'
- en: '| BLOOM [[151](#CR151)] | [3.1.2](#Sec3) | 176B LM for European languages |
    Lambada 0-shot 67.2% |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM [[151](#CR151)] | [3.1.2](#Sec3) | 176B 欧洲语言语言模型 | Lambada 0-shot 67.2%
    |'
- en: '| PanGu-*α* [[248](#CR248)] | [3.1.2](#Sec3) | 200B long sequence LM to generate
    text | Chinese benchmarks |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| PanGu-*α* [[248](#CR248)] | [3.1.2](#Sec3) | 200B 长序列语言模型生成文本 | 中国基准测试 |'
- en: '| Gopher [[168](#CR168)] | [3.1.2](#Sec3) | 280B LM to generate text | Lambada
    0-shot 74.5% |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Gopher [[168](#CR168)] | [3.1.2](#Sec3) | 280B 语言模型生成文本 | Lambada 0-shot
    74.5% |'
- en: '| MT-NLG [[4](#CR4)] | [3.1.2](#Sec3) | 530B Megatron variant | Lambada 76.6%
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| MT-NLG [[4](#CR4)] | [3.1.2](#Sec3) | 530B Megatron 变体 | Lambada 76.6% |'
- en: '| PaLM [[35](#CR35)] | [3.1.2](#Sec3) | 540B shared key-value projections |
    Lambada 0-shot **77.9%** |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| PaLM [[35](#CR35)] | [3.1.2](#Sec3) | 540B 共享键值投影 | Lambada 0-shot **77.9%**
    |'
- en: '| GLaM [[51](#CR51)] | [3.5.2](#Sec26) | 1200B mixture-of-experts LM | Lambada
    0-shot 73.7% |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| GLaM [[51](#CR51)] | [3.5.2](#Sec26) | 1200B 专家混合语言模型 | Lambada 0-shot 73.7%
    |'
- en: '| WuDao-2.0 [[178](#CR178)] | [3.5.2](#Sec26) | 1750B mixture-of-experts LM
    | Lambada: better than Turing-NLG |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| WuDao-2.0 [[178](#CR178)] | [3.5.2](#Sec26) | 1750B 专家混合语言模型 | Lambada：优于
    Turing-NLG |'
- en: '**GPT-3** [[25](#CR25)] is a language model with extreme dimensions. Its largest
    version has 96 layers, 96 attention heads, 175 billion parameters and covers sequences
    of length 2048\. It was trained on a text collection of books, Wikipedia and web
    pages of about 500 billion tokens. The details of the architecture are not known
    yet. GPT-3 is structurally similar to GPT-2, and therefore its higher level of
    accuracy is attributed to its increased capacity and higher number of parameters.
    The model achieved an unprecedented performance in language modeling, question
    answering, etc. Some results are compiled in Table [3.4](#Tab4) and many more
    in the paper [[25](#CR25)].Table 3.4'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-3** [[25](#CR25)] 是一个具有极端维度的语言模型。其最大版本有 96 层，96 个注意力头，1750 亿参数，覆盖长度为
    2048 的序列。它是在大约 5000 亿个标记的书籍、维基百科和网页文本集合上训练的。架构的细节尚不清楚。GPT-3 在语言建模、问答等方面取得了前所未有的性能。一些结果汇总在表
    [3.4](#Tab4) 中，更多内容在论文 [[25](#CR25)] 中。表 3.4'
- en: Comparing different versions of PaLM, GPT-3, Chinchilla, Gopher, OPT, GLaM,
    and BLOOM on a number of popular benchmarks covering text completion, pronoun
    coreference, common sense reasoning and question answering (QA) [[22](#CR22),
    [25](#CR25), [35](#CR35), [51](#CR51)]. FLOPS measures the computational effort
    in floating point operations per second. Best benchmark values printed in bold
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个流行的基准测试中比较 PaLM、GPT-3、Chinchilla、Gopher、OPT、GLaM 和 BLOOM 的不同版本，涵盖文本补全、代词指代、常识推理和问答（QA）[[22](#CR22),
    [25](#CR25), [35](#CR35), [51](#CR51)]。FLOPS 衡量每秒浮点运算的计算工作量。加粗的为最佳基准值
- en: '|   | PaLM | PaLM | PaLM | GPT-3 | Chinchilla | Gopher | OPT | GLaM | BLOOM
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | PaLM | PaLM | PaLM | GPT-3 | Chinchilla | Gopher | OPT | GLaM | BLOOM
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Model size (billion parameters) | 8 | 62 | 540 | 175 | 70 | 280 | 175 | 1200
    | 176 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 模型大小（十亿参数） | 8 | 62 | 540 | 175 | 70 | 280 | 175 | 1200 | 176 |'
- en: '| Num. training Tokens (billion) | 780 | 795 | 780 | 400 | 1400 | 300 | 180
    | 1600 | 350 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 数量训练标记（十亿） | 780 | 795 | 780 | 400 | 1400 | 300 | 180 | 1600 | 350 |'
- en: '| Training effort (10^(21) FLOPS) | 37.4 | 295.7 | 2527 | 314.0 | 588.0 | 504.0
    | ≈ 50 | ≈ 105 |   |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 训练工作量（10^(21) FLOPS） | 37.4 | 295.7 | 2527 | 314.0 | 588.0 | 504.0 | ≈ 50
    | ≈ 105 |  |'
- en: '| Lambada 0-shot (text compl.) | 69.5 | 75.4 | **77.9** | 76.2 | 77.4 | 74.5
    |   | 73.7 | 67.2 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Lambada 0-shot（文本补全） | 69.5 | 75.4 | **77.9** | 76.2 | 77.4 | 74.5 |  | 73.7
    | 67.2 |'
- en: '| HellaSWAG 0-shot (text compl.) | 68.7 | 79.7 | **83.4** | 78.9 | 80.8 | 79.2
    | 79.0 | 77.1 | 73.0 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| HellaSWAG 0-shot (文本补全) | 68.7 | 79.7 | **83.4** | 78.9 | 80.8 | 79.2 | 79.0
    | 77.1 | 73.0 |'
- en: '| PIQA 0-shot (common sense) | 77.1 | 80.5 | **82.3** | 80.5 | 81.8 | 81.8
    | 78.5 | 80.4 |   |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| PIQA 0-shot (常识) | 77.1 | 80.5 | **82.3** | 80.5 | 81.8 | 81.8 | 78.5 | 80.4
    |   |'
- en: '| Winogrande 0-shot (coreference) | 66.3 | 77.0 | **81.1** | 70.2 | 74.9 |
    70.1 | 74.0 | 73.4 | 70.1 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Winogrande 0-shot (指代消解) | 66.3 | 77.0 | **81.1** | 70.2 | 74.9 | 70.1 |
    74.0 | 73.4 | 70.1 |'
- en: '| BoolQ 0-shot (QA) | 68.3 | 84.8 | **88.0** | 60.5 | 83.7 | 79.3 | 64.0 |
    83.0 |   |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ 0-shot (QA) | 68.3 | 84.8 | **88.0** | 60.5 | 83.7 | 79.3 | 64.0 |
    83.0 |   |'
- en: '| Natural questions 0-shot (QA) | 8.4 | 18.1 | **21.2** | 14.6 | 16.6 | 10.1
    |   | 21.5 |   |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 自然问题 0-shot (QA) | 8.4 | 18.1 | **21.2** | 14.6 | 16.6 | 10.1 |   | 21.5
    |   |'
- en: '| Natural questions few-shot (QA) | 14.6 | 27.6 | **36.0** | 29.9 | 31.5 |
    24.5 |   |   |   |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 自然问题少量示例（QA） | 14.6 | 27.6 | **36.0** | 29.9 | 31.5 | 24.5 |   |   |   |'
- en: '| Trivia QA 0-shot (QA) | 39.5 | 67.3 | **76.9** | 64.3 | 67.0 | 52.8 |   |
    68.0 |   |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Trivia QA 0-shot (QA) | 39.5 | 67.3 | **76.9** | 64.3 | 67.0 | 52.8 |   |
    68.0 |   |'
- en: '| Trivia QA few-shot (QA) | 48.5 | 72.7 | **81.4** | 71.2 | 73.2 | 63.6 |  
    |   |   |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Trivia QA 少量示例 (QA) | 48.5 | 72.7 | **81.4** | 71.2 | 73.2 | 63.6 |   |  
    |   |'
- en: '| Average task metric | 51.2 | 64.8 | **69.8** | 60.7 | 65.2 | 59.5 |   |  
    |   | GPT-3 is able to generate fluent texts and covers a huge amount of world
    knowledge, as the example in Fig. [3.2](#Fig2) shows. Examples of generated texts
    can be found in many locations [[23](#CR23), [149](#CR149)]. The amount and quality
    of knowledge captured by PLMs is discussed in Chap. [4](528393_1_En_4_Chapter.xhtml).
    In contrast to other language models, GPT-3 can be instructed by a few sentences
    to perform quite arbitrary tasks (few-shot learning). This is a very simple way
    to use GPT-3 to solve quite specific tasks such as translating into another language,
    summarizing a document, correcting grammar, writing an essay on a given topic,
    etc. Details are discussed in Sect. [3.6.3](#Sec41).![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig2_HTML.png)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '| 平均任务指标 | 51.2 | 64.8 | **69.8** | 60.7 | 65.2 | 59.5 |   |   |   | GPT-3能够生成流畅的文本，并涵盖大量的世界知识，如图3.2[3.2](#Fig2)所示。生成的文本示例可以在许多地方找到
    [[23](#CR23)，[149](#CR149)]。PLM捕获的知识量和质量在第四章[4](528393_1_En_4_Chapter.xhtml)中讨论。与其它语言模型不同，GPT-3可以通过几句话的指令执行相当任意的任务（少量示例学习）。这是一种非常简单的方法来使用GPT-3解决相当具体的问题，如翻译成另一种语言、总结文档、纠正语法、就给定主题写文章等。具体细节在3.6.3节[3.6.3](#Sec41)中讨论。![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig2_HTML.png)'
- en: A set of 2 text boxes represents the input of an article requirement and the
    output of G P T 3\. The input title reads united methodists agree to historic
    split.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一组2个文本框代表文章要求的输入和GPT-3的输出。输入标题为“联合卫理公会同意历史性分裂”。
- en: Fig. 3.2
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2
- en: Text generated by GPT-3 in response to an input. Quoted with kind permission
    of the authors [[25](#CR25), p. 28]
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3根据输入生成的文本。经作者同意引用 [[25](#CR25)，第28页]
- en: At the end of 2021 OpenAI provided an API to fine-tune GPT-3 with user-specific
    data [[123](#CR123)]. In this way, the model can be adapted to a specific domain
    language and, in addition, be prepared to perform specific classification tasks.
    In general, this yields higher quality results than prompt design. In addition,
    no few-shot examples are necessary anymore. Details of fine-tuning GPT-3 are discussed
    in Sect. [3.6.2](#Sec40). Table [3.4](#Tab4) compares GPT-3 with other more recent
    language models on a number of popular benchmarks. There is a clear advantage
    of the new PaLM model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 到2021年底，OpenAI提供了一种API，使用户特定数据微调GPT-3 [[123](#CR123)]。这样，模型可以适应特定领域的语言，并且还可以准备执行特定的分类任务。通常，这比提示设计产生更高的质量结果。此外，不再需要少量示例。GPT-3微调的细节在3.6.2节[3.6.2](#Sec40)中讨论。表3.4[3.4](#Tab4)比较了GPT-3与其他更近期的语言模型在多个流行基准上的表现。新的PaLM模型具有明显的优势。
- en: '**GPT-J-6B** is an open-source GPT model with 28 layers, 16 heads, a context
    size of 2048, and 6B parameters [[221](#CR221)]. It has a similar performance
    as the GPT-3 version with 6.7B parameters. There is an interactive web demo where
    users can enter their prompts and a continuation text is generated [[220](#CR220)].
    **GPT-Neo** [[16](#CR16)] is another free version of GPT with 2.7B parameters.
    It was trained on the *Pile*, a 825 GB data set containing data from 22 diverse
    sources, including academic sources (e.g. ArXiv), Internet webpages (e.g. StackExchange),
    dialogs from subtitles, GitHub, etc. It outperforms the GPT-3 version with the
    same parameter size on some natural language understanding tasks [[89](#CR89)].
    Recently, **GPT-NeoX-20B** [[215](#CR215)] was released. It has 44 layers, an
    internal vector dimension of 6144, 64 heads and uses batches of size 3.1M for
    training. In the LAMBADA benchmark (Sect. [4.​1.​3](528393_1_En_4_Chapter.xhtml#Sec4))
    with the task of predicting the missing last word of the last sentence of each
    passage, it achieves an accuracy of 72.0%. This value is close to GPT-3 with 75.2%.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-J-6B** 是一个具有 28 层、16 个头、上下文大小为 2048 和 60 亿参数的开源 GPT 模型 [[221](#CR221)]。它的性能与具有
    67 亿参数的 GPT-3 版本相似。有一个交互式网络演示，用户可以输入他们的提示，并生成后续文本 [[220](#CR220)]。**GPT-Neo**
    [[16](#CR16)] 是另一个具有 27 亿参数的免费 GPT 版本。它是在 *Pile* 上训练的，这是一个包含来自 22 个不同来源的数据的 825
    GB 数据集，包括学术来源（例如 ArXiv）、互联网网页（例如 StackExchange）、字幕对话、GitHub 等。它在某些自然语言理解任务上优于具有相同参数大小的
    GPT-3 版本 [[89](#CR89)]。最近，**GPT-NeoX-20B** [[215](#CR215)] 发布。它有 44 层，内部向量维度为
    6144，64 个头，并使用 3.1M 大小的批次进行训练。在 LAMBADA 基准测试（第 [4.1.3](528393_1_En_4_Chapter.xhtml#Sec4)
    节）中，其任务是预测每段最后一句的缺失最后一个单词，它达到了 72.0% 的准确率。这个值接近具有 75.2% 准确率的 GPT-3。'
- en: '**Megatron-LM** [[193](#CR193)] scale language models such as GPT-2 and BERT
    efficiently by introducing intra-layer model parallelism. The authors place self-attention
    heads as well as feed-forward layers on different GPUs, reducing the memory burden
    of a single GPU. They present a GPT-variant with 8.3B parameters and a 3.9B parameter
    model similar to BERT. Highlights of the approach include 76% scaling efficiency
    when using 512 GPUs. Their GPT model reduces the *WikiText-103* [[134](#CR134)]
    Sota perplexity from 15.8 to 10.8 and their BERT model increases RACE (reading
    comprehension) [[110](#CR110)] accuracy to 90.9%.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**Megatron-LM** [[193](#CR193)] 之类的语言模型，如 GPT-2 和 BERT，通过引入层内模型并行化来高效地扩展到 **Megatron-LM**
    的规模。作者将自注意力头和前馈层放置在不同的 GPU 上，从而减轻了单个 GPU 的内存负担。他们提出了一种具有 83 亿参数的 GPT 变体和一种与 BERT
    相似的 39 亿参数模型。该方法的亮点包括使用 512 个 GPU 时的 76% 扩缩比效率。他们的 GPT 模型将 *WikiText-103* [[134](#CR134)]
    的 Sota 混淆度从 15.8 降低到 10.8，而他们的 BERT 模型将 RACE (阅读理解) [[110](#CR110)] 准确率提高到 90.9%。'
- en: '**Jurassic-1** [[122](#CR122)] is an autoregressive language model similar
    to GPT-3 with 178B parameters. The authors chose a token vocabulary of 256k instead
    of 50k for GPT-3, which also included frequent multi-word expressions such as
    named entities and common phrases. The training text could be represented with
    28% fewer tokens than GPT-3\. Hence, the model can process queries up to 1.4×
    faster when using the same architecture. The model used a maximal sequence length
    of 2048 tokens. In spite of the larger vocabulary only 2% of all parameters were
    required for the input embeddings. The model was trained on 300B tokens drawn
    from public text corpora using a final batch size of 3.2M tokens.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jurassic-1** [[122](#CR122)] 是一个类似于 GPT-3 的自回归语言模型，具有 1780 亿参数。作者选择了 256k
    的标记词汇量，而不是 GPT-3 的 5 万，这还包括了诸如命名实体和常用短语等频繁的多词表达式。与 GPT-3 相比，训练文本可以用 28% 更少的标记表示。因此，当使用相同的架构时，该模型可以处理查询的速度快
    1.4 倍。该模型使用了最大序列长度为 2048 个标记。尽管词汇量更大，但所有参数中只有 2% 用于输入嵌入。该模型是在使用 3.2M 个标记的最终批次大小从公共文本语料库中抽取的
    3000 亿个标记上训练的。'
- en: '**PanGu-***α* [[248](#CR248)] is a model of Huawei similar to GPT-3 with up
    to 200B parameters. It was trained on 1.1TB Chinese text, and was applied to a
    large number of tasks in zero-shot, one-shot, and few-shot settings without any
    fine-tuning. The model has a performance comparable to GPT-3.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**PanGu-***α* [[248](#CR248)] 是华为的一个类似于 GPT-3 的模型，参数量高达 2000 亿。它是在 1.1TB 的中文文本上训练的，并在零样本、单样本和少样本设置中应用于大量任务，而无需任何微调。该模型的表现与
    GPT-3 相当。'
- en: '**OPT-175B** (Open Pre-trained Transformer) [[253](#CR253)] is a suite of 8
    GPT models with 125M to 175B parameters developed by Meta. It was trained on publicly
    available datasets with 180B tokens. The largest models has 96 layers, each with
    96 heads. Although OPT-175B has the same parameter count as GPT-3, its training
    required only 1/7th of computing effort of GPT-3\. The model was evaluated on
    16 NLP tasks and showed approximately the same performance as GPT-3 (Table [3.4](#Tab4)).
    All trained models up to 30B parameters are freely available. The large 175B parameter
    model is only available to academic researchers upon request to discourage the
    production of fake news. The model can be trained and deployed on only 16 NVIDIA
    V100 GPUs. Some benchmark results are provided in Table [3.4](#Tab4).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**OPT-175B**（开放预训练Transformer）[[253](#CR253)] 是由Meta开发的包含8个GPT模型的一系列，参数范围从125M到175B。它在包含180B个标记的公开可用数据集上进行了训练。最大的模型有96层，每层有96个头。尽管OPT-175B与GPT-3具有相同的参数数量，但其训练所需的计算工作量仅为GPT-3的1/7。该模型在16个NLP任务上进行了评估，并显示出与GPT-3大致相同的性能（表
    [3.4](#Tab4)）。所有参数量高达30B的模型都是免费提供的。175B参数的大型模型仅对学术研究人员开放，需申请以防止虚假新闻的生产。该模型只能在16个NVIDIA
    V100 GPU上进行训练和部署。一些基准测试结果在表 [3.4](#Tab4) 中提供。'
- en: '**BLOOM** [[139](#CR139)] is an autoregressive large language model with 176B
    parameters. It has 70 layers with 112 attention-heads per layer and 2048 token
    sequence length. It was developed by the BigScience initiative of over 1000 AI
    researchers to provide a free large language model for everyone who wants to try.
    Its training data covers 46 natural languages (English 30%, Chinese 16%, French
    12%, Spanish 11%, …) and 11% code (java, php, …) with 350B tokens. The 176B BLOOM
    model has been trained using the Megatron-DeepSpeed library [[26](#CR26)] offering
    different types of parallelism. The model can be evaluated on 8 large GPUs. Hence,
    BLOOM is one of the largest trained model available for research purposes. Some
    benchmark results are provided in Table [3.4](#Tab4).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**BLOOM** [[139](#CR139)] 是一个具有176B参数的自回归大型语言模型。它有70层，每层有112个注意力头，序列长度为2048个标记。它是由超过1000名AI研究人员组成的BigScience倡议开发的，旨在为所有想要尝试的人提供免费的大型语言模型。其训练数据涵盖了46种自然语言（英语30%，中文16%，法语12%，西班牙语11%，……）和11%的代码（Java，PHP，……），总共有350B个标记。176B的BLOOM模型使用了提供不同类型并行性的Megatron-DeepSpeed库
    [[26](#CR26)] 进行训练。该模型可以在8个大型GPU上进行评估。因此，BLOOM是可用于研究目的的最大训练模型之一。一些基准测试结果在表 [3.4](#Tab4)
    中提供。'
- en: '**Gopher** [[168](#CR168)] employed the GPT-2 architecture with two modifications.
    For regularization the authors used RMSNorm (Sect. [2.​4.​2](528393_1_En_2_Chapter.xhtml#Sec32))
    instead of LayerNorm and they employed the relative positional encoding scheme
    [[44](#CR44)] instead of absolute positional encoding. Gopher has 80 layers with
    128 attention heads and 280B parameters. All models were trained on 300B tokens
    with a context window of 2048 tokens and a batch size of up to 6M tokens. For
    the large models a 16 bit float numbers was used to reduce memory and increase
    training throughput.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**Gopher** [[168](#CR168)] 使用了GPT-2架构，并进行了两项修改。为了正则化，作者使用了RMSNorm（第 [2.4.2](528393_1_En_2_Chapter.xhtml#Sec32)
    节）而不是LayerNorm，并且使用了相对位置编码方案 [[44](#CR44)] 而不是绝对位置编码。Gopher有80层，每层有128个注意力头和280B参数。所有模型都是在300B个标记上，上下文窗口为2048个标记，批大小高达6M个标记的情况下进行训练的。对于大型模型，使用了16位浮点数以减少内存和提高训练吞吐量。'
- en: Six model versions with different numbers of parameters were trained to assess
    the effect of model size. The authors present a comprehensive evaluation on 152
    tasks described in Table [4.​3](528393_1_En_4_Chapter.xhtml#Tab3). Gopher shows
    an improvement on 100 of 124 tasks. One of these is the *LAMBADA benchmark* [[154](#CR154)]
    where Gopher generates a zero-shot score of 74.5, which is only slightly below
    the value 76.6 of *MT-NLG* model with 530B parameters [[106](#CR106)]. For instance
    Gopher achieves Sota for all 12 benchmarks on humanities covering areas like econometrics
    and psychology surpassing the best supervised results for 11 benchmarks. Some
    results are provided in Table [3.4](#Tab4) while Sect. [4.​1.​4](528393_1_En_4_Chapter.xhtml#Sec5)
    describes more details.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 训练了六个不同参数数量的模型版本，以评估模型大小的影响。作者在表 [4.3](528393_1_En_4_Chapter.xhtml#Tab3) 中描述的152个任务上进行了全面的评估。Gopher在124个任务中的100个任务上有所提升。其中之一是*LAMBADA基准测试*
    [[154](#CR154)]，Gopher在该基准测试中生成了一个零样本得分74.5，仅略低于具有530B参数的*MT-NLG*模型的76.6分 [[106](#CR106)]。例如，Gopher在涵盖经济学计量学和心理学等人文领域的12个基准测试中均达到了Sota水平，超过了11个基准测试的最佳监督结果。一些结果在表
    [3.4](#Tab4) 中提供，而第 [4.1.4](528393_1_En_4_Chapter.xhtml#Sec5) 节描述了更多细节。
- en: '**Chinchilla** [[83](#CR83)] is a mid-size encoder model with 70B parameters,
    which has the same compute budget as the larger Gopher model, but four times as
    much data. Chinchilla consistently has a better performance than Gopher (Table
    [3.4](#Tab4)) and significantly outperforms GPT-3 (175B), Jurassic-1 (178B), and
    Megatron-Turing NLG (530B) on a large set of downstream evaluation tasks. For
    every doubling of model size the number of training tokens should also be doubled.
    This is a much larger scaling rate than that predicted by Kaplan et al. [[102](#CR102)]
    in Sect. [3.5.1](#Sec25).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**Chinchilla** [[83](#CR83)] 是一个中等规模的编码器模型，具有700亿参数，其计算预算与更大的Gopher模型相同，但数据量是其四倍。Chinchilla在性能上始终优于Gopher（见表[3.4](#Tab4)），并在大量下游评估任务上显著优于GPT-3（1750亿）、Jurassic-1（1780亿）和Megatron-Turing
    NLG（5300亿）。对于模型大小的每倍增加，训练标记的数量也应加倍。这比Kaplan等人[[102](#CR102)]在[3.5.1](#Sec25)节中预测的扩展率要大得多。'
- en: '**Turing-NLG** [[179](#CR179)] introduces an autoregressive language model
    with 78 transformer layers, a hidden vector-size of 4256, 28 attention heads and
    17B parameters. As a model with more than 1.3B parameters cannot fit into a single
    GPU with 32 GB memory it must be parallelized, or broken into pieces, across multiple
    GPUs. Turing-NLG leverages a Sota Deep Learning hardware with high communication
    bandwidth, the Megatron-LM framework, and the DeepSpeed library, which further
    optimizes the training speed and reduces the resources needed. The model achieved
    Sota performance on language modeling tasks and also proved to be effective for
    zero-shot question answering and abstractive summarization.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**Turing-NLG** [[179](#CR179)] 引入了一个具有78层transformer、隐藏向量大小为4256、28个注意力头和170亿参数的自回归语言模型。由于超过13亿参数的模型无法在一个32GB内存的单个GPU中拟合，它必须并行化或在多个GPU之间分割。Turing-NLG利用了具有高通信带宽的Sota深度学习硬件、Megatron-LM框架和DeepSpeed库，进一步优化了训练速度并减少了所需资源。该模型在语言建模任务上实现了Sota性能，并且也证明了在零样本问答和抽象摘要方面是有效的。'
- en: Its successor **MT-NLG** [[4](#CR4)] is a 105-layer encoder model with 530B
    parameters and was trained across 280 GPUs with a huge batch size of 1920\. Similar
    to GPT-3 it improves performance on zero-, one- and few-shot tasks. For the *LAMBADA
    benchmark* [[154](#CR154)], for example, the model has to predict the last word
    of paragraph (Sect. [4.​1.​3](528393_1_En_4_Chapter.xhtml#Sec4)). On this benchmark
    MT-NLG improves the few-shot accuracy of GPT-3 (86.4%) to the Sota 87.2%.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其继任者 **MT-NLG** [[4](#CR4)] 是一个具有105层编码器和5300亿参数的模型，在280个GPU上以巨大的批处理大小1920进行训练。与GPT-3类似，它在零样本、单样本和少量样本任务上提高了性能。例如，对于
    *LAMBADA基准测试* [[154](#CR154)]，模型需要预测段落（[4.1.3](528393_1_En_4_Chapter.xhtml#Sec4)节）的最后一个单词。在这个基准测试中，MT-NLG将GPT-3（86.4%）的少量样本准确性提高到了Sota
    87.2%。
- en: '**PaLM** [[35](#CR35)] is an autoregressive language model developed by Google
    with 540B parameters. It has 118 layers, 48 heads and an input sequence length
    of 2048\. There are also smaller versions with 8B and 62B parameters. It uses
    a standard autoregressive decoder with SwiGLU activation function and shared query-value
    projections for the heads of a layer, which improves autoregressive decoding speed.
    The model is trained on a high-quality dataset with 780B tokens, where sloppy
    and toxic language have been filtered. Each training example is used only once.
    The training set contains social media conversation (50%), multilingual web pages
    (27%), books (13%), source code files (5%), multilingual Wikipedia articles (4%),
    and news articles (1%). Training required 3072 TPU chips for 1368 h, resulting
    in a total emission that is 50% higher than the emissions for a direct round-trip
    flight in an aircraft between San Francisco and New York [[35](#CR35), p. 18].'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**PaLM** [[35](#CR35)] 是谷歌开发的一个具有5400亿参数的自回归语言模型。它有118层、48个头和2048个输入序列长度。还有较小的版本，参数为80亿和620亿。它使用标准的自回归解码器，具有SwiGLU激活函数，并为层的头部共享查询-值投影，这提高了自回归解码速度。该模型在一个包含7800亿标记的高质量数据集上进行训练，其中已经过滤掉了粗俗和有害的语言。每个训练示例只使用一次。训练集包含社交媒体对话（50%）、多语言网页（27%）、书籍（13%）、源代码文件（5%）、多语言维基百科文章（4%）和新闻文章（1%）。训练需要3072个TPU芯片运行1368小时，导致总排放量比旧金山和纽约之间飞机直接往返的排放量高出50%[[35](#CR35)，第18页]。'
- en: PaLM was evaluated on hundreds of natural language inference, mathematical,
    reasoning and knowledge intensive tasks and achieved Sota accuracy in the large
    majority of benchmarks, e.g. in 28 of 29 most widely evaluated English language
    understanding benchmarks (cf. Table [3.4](#Tab4)). This demonstrates that the
    scaling effects continue to hold for large Foundation Models. Figure [3.3](#Fig3)
    shows the results on BIG-bench data compared to prior models. PaLM 540B 5-shot
    outperforms the prior Sota on 44 out of the 58 common tasks, and on average is
    significantly better than the other models (Gopher, Chinchilla, GPT-3). Moreover,
    PaLM 540B 5-shot achieves a higher score than the average score of the humans
    asked to solve the same tasks. When fine-tuned on SuperGLUE, the model outperforms
    the best decoder-only model and is competitive with encoder-decoder models, which
    in general perform better for fine-tuning. A significant number of tasks showed
    discontinuous improvements from model scale, meaning that the performance improvement
    from the smaller version to the largest model was higher than expected.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig3_HTML.png)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: PaLM在数百个自然语言推理、数学、推理和知识密集型任务上进行了评估，并在大多数基准测试中达到了Sota准确率，例如在29个最广泛评估的英语语言理解基准测试中的28个（参见表[3.4](#Tab4)）。这表明，对于大型基础模型，缩放效应仍然持续存在。图[3.3](#Fig3)显示了与先前模型相比在BIG-bench数据上的结果。PaLM
    540B 5-shot在58个共同任务中有44个超过了先前的Sota，平均而言，显著优于其他模型（Gopher、Chinchilla、GPT-3）。此外，PaLM
    540B 5-shot的得分高于被要求解决相同任务的人类的平均得分。当在SuperGLUE上进行微调时，该模型优于最佳解码器模型，并且与编码器-解码器模型具有竞争力，编码器-解码器模型通常在微调方面表现更好。许多任务显示了从模型规模到性能的连续改进，这意味着从小型版本到最大模型的性能提升高于预期。![图3.3](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig3_HTML.png)
- en: 5 line graphs. Graph 1 of normalized preferred metric versus model parameters
    plots the performance on 58 tasks. The other 4 graphs plot represent the percentages
    of negative, English proverbs, mathematic induction, and logical sequence, each
    versus model scale. All graphs have increasing trends.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 5条线图。图1是标准化首选指标与模型参数的对比，显示了58个任务上的性能。其他4个图分别表示负数、英语谚语、数学归纳和逻辑序列的百分比，每个与模型规模对比。所有图表都显示出上升趋势。
- en: Fig. 3.3
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3
- en: Evaluation of PaLM, GPT-3, Gopher, and Chinchilla (left). Previous models were
    only evaluated on a subset of tasks, so this graph shows the aggregated results
    on the 58 tasks where all three models have been evaluated [[35](#CR35)]. The
    medium accuracy of PaLM is better than the average performance of humans. The
    right side shows the results for four specific BIG-tasks. A detailed comparison
    between the performance of three PaLM models of different size as well as human
    levels is presented in [[35](#CR35), p. 15f]
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: PaLM、GPT-3、Gopher和Chinchilla的评估（左侧）。之前的模型仅在一组子任务上进行了评估，因此此图显示了所有三个模型都评估过的58个任务的汇总结果
    [[35](#CR35)]。PaLM的中等准确率优于人类的平均表现。右侧显示了四个特定BIG任务的评估结果。在[[35](#CR35)，第15页及以下]中，详细比较了不同尺寸的三个PaLM模型以及人类水平的性能。
- en: PaLM has been fine-tuned on program code documents. The resulting model is called
    *PaLM-Coder* [[35](#CR35), p.23]. The quality of the code is measured by the pass@*k*
    metric, in which for each problem in the test set, *k* samples of source code
    are generated by PaLM-Coder, and a problem is counted as solved if any sample
    solves the problem. PaLM-Coder is able to solve a number of benchmark tasks with
    about a pass@1-value of about 50\. There is an elaborate evaluation of the properties
    of the PaLM-Coder model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: PaLM已在程序代码文档上进行了微调。得到的模型被称为*PaLM-Coder* [[35](#CR35)，第23页]。代码质量通过pass@*k*指标来衡量，其中对于测试集中的每个问题，PaLM-Coder生成*k*个源代码样本，如果任何样本解决了问题，则认为问题已解决。PaLM-Coder能够解决大约50%的基准任务，其pass@1值约为50。对PaLM-Coder模型的属性进行了详尽的评估。
- en: For about a quarter of tasks the authors observe a discontinuous jump in accuracy,
    if the model is increased from 58B to 540B parameters, far exceeding the ‘power
    law’ postulated by Kaplan et al. [[102](#CR102)] (Sect. [3.5.1](#Sec25)). Examples
    are ‘english proverbs’ and ‘logical sequence’ shown in Fig. [3.3](#Fig3). This
    suggests that new abilities of PLMs can evolve when the model reaches a sufficient
    size, and that these abilities also develop beyond the model sizes studied so
    far.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大约四分之一的任务，作者观察到，如果模型从58B参数增加到540B参数，准确率会出现不连续的跳跃，远超过Kaplan等人提出的“幂律”[[102](#CR102)]（Sect.
    [3.5.1](#Sec25)）。例如，图[3.3](#Fig3)中显示的“英语谚语”和“逻辑序列”。这表明，当模型达到足够大时，PLM的新能力可以进化，并且这些能力的发展也超出了迄今为止研究的模型大小。
- en: The training data contains 22% multilingual documents. For translation between
    different languages, the few-shot PaLM model comes close to or even exceeds the
    fine-tuned Sota. For English-French translation, Palm 540B few-shot achieves 44.0
    Bleu compared to a Sota of 45.6\. For German-English, PaLM 540B few-shot reaches
    47.5 Bleu vs. a 45.6 BleuSota. For other tasks like summarization and question
    answering, Palm 540B few-shot comes close to the fine-tuned models, and can outperform
    them in a few cases.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据包含22%的多语言文档。对于不同语言之间的翻译，少量PaLM模型接近甚至超过了微调的Sota。对于英语-法语翻译，PaLM 540B少量模型达到44.0
    Bleu，而Sota为45.6。对于德语-英语，PaLM 540B少量模型达到47.5 Bleu，而Sota为45.6 Bleu。对于其他任务，如摘要和问答，PaLM
    540B少量模型接近微调模型，并在某些情况下可以超越它们。
- en: Reasoning with a number of intermediate steps was always difficult for language
    models. Recently chain-of-thought prompting (Sect. [3.6.4](#Sec42)) was proposed
    which adds intermediate reasoning steps [[226](#CR226)] into the few-shot prompts
    (Fig. [3.4](#Fig4)). Following this recipe, the PaLM model similarly produces
    its own intermediate steps for a multistep problem before giving the final answer.
    This leads to a boost in performance for a number of benchmark tasks. Using this
    technique PaLM is even able to explain jokes, as Fig. [3.5](#Fig5) demonstrates.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig4_HTML.png)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言模型来说，通过多个中间步骤进行推理一直都很困难。最近提出了思维链提示（Sect. [3.6.4](#Sec42)），它将中间推理步骤 [[226](#CR226)]
    加入到少量提示（Fig. [3.4](#Fig4)）中。遵循这个方法，PaLM模型在给出最终答案之前，同样会为多步问题生成自己的中间步骤。这导致了许多基准任务的性能提升。使用这种技术，PaLM甚至能够解释笑话，如图[3.5](#Fig5)所示。![图3.4](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig4_HTML.png)
- en: 2 text boxes represent a set of 2 prompts and their model outputs.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 2个文本框代表一组2个提示及其模型输出。
- en: Fig. 3.4
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4
- en: Few-shot example of a chain-of-thought prompt for a common sense question-answering
    task [[35](#CR35), p. 38]. The same two example chains of thought were combined
    with different prompts requiring an answer
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 常识性问题回答任务的思维链提示的少量示例 [[35](#CR35)，p. 38]。相同的两个示例思维链与不同的需要答案的提示结合
- en: '![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig5_HTML.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图3.5](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig5_HTML.png)'
- en: A text box represents an input prompt and its model output.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 文本框代表一个输入提示及其模型输出。
- en: Fig. 3.5
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5
- en: By using thought-chain-prompts PaLM can explain jokes [[35](#CR35)]
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用思维链提示，PaLM可以解释笑话 [[35](#CR35)]
- en: 3.1.3 Transformer Encoder-Decoders
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.3 Transformer编码器-解码器
- en: 'The Transformer encoder-decoder [[212](#CR212)] was pre-trained with a translation
    task (Sect. [2.​3](528393_1_En_2_Chapter.xhtml#Sec19)). To improve performance
    a number of alternatives were proposed:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer编码器-解码器 [[212](#CR212)] 使用翻译任务（Sect. [2.3](528393_1_En_2_Chapter.xhtml#Sec19)）进行预训练。为了提高性能，提出了许多替代方案：
- en: Different targets to restore corrupted pre-training data are proposed by MASS,
    BART and PEGASUS. Examples are predicting masked spans, ordering permuted sentences,
    or inserting omitted tokens.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MASS、BART和PEGASUS提出了恢复损坏的预训练数据的不同目标。例如，预测掩码跨度、排列句子顺序或插入省略的标记。
- en: T5 formulates many language understanding and language generation tasks as text
    translations and handles them with the same model.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T5将许多语言理解和语言生成任务表述为文本翻译，并使用相同的模型处理它们。
- en: Longformer, Reformer and Transformerl-XL extend the size of the input text without
    increasing the number of parameters. They are discussed in Sect. [3.2](#Sec7).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Longformer、Reformer和Transformerl-XL扩展了输入文本的大小，而不增加参数数量。它们在Sect. [3.2](#Sec7)中讨论。
- en: The details are given in the following paragraphs. A representative list of
    transformer encoder-decoders is provided in Table [3.5](#Tab5).Table 3.5
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 详细内容见下文。在表[3.5](#Tab5)中提供了一个代表性的transformer编码器-解码器列表。表3.5
- en: 'Transformer encoder-decoders. The pre-training and fine-tuning loss functions
    are defined in Table [3.1](#Tab1). Benchmarks: En-De WMT2014 English-to-German
    BLEU, GLUE Sect. [4.​1.​1](528393_1_En_4_Chapter.xhtml#Sec2) accuracy, SuperGLUE
    Sect. [4.​1.​2](528393_1_En_4_Chapter.xhtml#Sec3) accuracy, TriviaQA [[99](#CR99)]
    Sect. [6.​2.​1](528393_1_En_6_Chapter.xhtml#Sec10) accuracy, Penn Treebank [[136](#CR136)]
    perplexity. The benchmark figures are only a hint, as they depend on the number
    of parameters and the computing effort'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 编码器-解码器。预训练和微调的损失函数定义在表 [3.1](#Tab1) 中。基准：En-De WMT2014 英语到德语的 BLEU，GLUE
    部分 [4.1.1](528393_1_En_4_Chapter.xhtml#Sec2) 准确率，SuperGLUE 部分 [4.1.2](528393_1_En_4_Chapter.xhtml#Sec3)
    准确率，TriviaQA [[99](#CR99)] 部分 [6.2.1](528393_1_En_6_Chapter.xhtml#Sec10) 准确率，Penn
    Treebank [[136](#CR136)] 混淆度。基准数据仅作为一个参考，因为它们取决于参数数量和计算工作量
- en: '| Model | Section | Pre-training | Fine-tuning | Extra | Benchmark |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 章节 | 预训练 | 微调 | 额外 | 基准 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Transformer [[212](#CR212)] | [2.​3](528393_1_En_2_Chapter.xhtml#Sec19) |
    S2S | S2S | Predict translated tokens | En-De 26.4 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Transformer [[212](#CR212)] | [2.3](528393_1_En_2_Chapter.xhtml#Sec19) |
    S2S | S2S | 预测翻译标记 | En-De 26.4 |'
- en: '| UniLM [[8](#CR8)] | [3.1.3](#Sec4) | MLM, LM | MC, LM | Uni- and bidirectional
    | GLUE 87.3 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| UniLM [[8](#CR8)] | [3.1.3](#Sec4) | MLM, LM | MC, LM | 单向和双向 | GLUE 87.3
    |'
- en: '| MASS [[196](#CR196)] | [3.1.3](#Sec4) | S2S | S2S | Predict masked tokens
    | En-De 28.3 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| MASS [[196](#CR196)] | [3.1.3](#Sec4) | S2S | S2S | 预测掩码标记 | En-De 28.3 |'
- en: '| BART [[119](#CR119)] | [3.1.3](#Sec4) | DAE | MC, LM, S2S | Restore corrupted
    text | GLUE 88.4 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| BART [[119](#CR119)] | [3.1.3](#Sec4) | DAE | MC, LM, S2S | 恢复损坏的文本 | GLUE
    88.4 |'
- en: '| T5 [[170](#CR170)] | [3.1.3](#Sec4) | S2S | MC, LM, S2S | Solve many NLP
    tasks as S2S problems | GLUE 89.7 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| T5 [[170](#CR170)] | [3.1.3](#Sec4) | S2S | MC, LM, S2S | 将许多 NLP 任务作为 S2S
    问题解决 | GLUE 89.7 |'
- en: '| GLM [[54](#CR54)] | [3.1.3](#Sec4) | LM | LM | Solve all task by autoregressive
    prediction | SuperGLUE 82.9 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| GLM [[54](#CR54)] | [3.1.3](#Sec4) | LM | LM | 通过自回归预测解决所有任务 | SuperGLUE
    82.9 |'
- en: '| Longformer [[10](#CR10)] | [3.2.1](#Sec8) | MLM, S2S | LM, MC, S2S | Sparse
    attention mechanism | TriviaQA 77.3 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Longformer [[10](#CR10)] | [3.2.1](#Sec8) | MLM, S2S | LM, MC, S2S | 稀疏注意力机制
    | TriviaQA 77.3 |'
- en: '| Reformer [[108](#CR108)] | [3.2.2](#Sec9) | LM, S2S | LM, MC, S2S | Locality-sensitive
    hashing, reversible residual layers | En-De 29.1 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Reformer [[108](#CR108)] | [3.2.2](#Sec9) | LM, S2S | LM, MC, S2S | 局部敏感哈希，可逆残差层
    | En-De 29.1 |'
- en: '| Transformer-XL [[44](#CR44)] | [3.2.2](#Sec9) | MLM, S2S | MC, S2S | Sparse
    attention mechanism | Penn-Tree Bank 54.5 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Transformer-XL [[44](#CR44)] | [3.2.2](#Sec9) | MLM, S2S | MC, S2S | 稀疏注意力机制
    | Penn-Tree Bank 54.5 |'
- en: '**MASS** [[196](#CR196)] is based on the transformer architecture. In contrast
    to the original transformer, a sequence of consecutive tokens in the encoder is
    masked and the decoder’s task is to predict the masked tokens recursively (Fig.
    [3.6](#Fig6)). Therefore, MASS can jointly train the encoder and decoder to develop
    the capability of extracting embeddings and language modeling. MASS is fine-tuned
    on language generation tasks such as neural machine translation, summarization
    and conversational response generation. It shows significant performance improvements
    compared to prior transformer architectures.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig6_HTML.png)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**MASS** [[196](#CR196)] 基于 Transformer 架构。与原始 Transformer 相比，编码器中连续的标记序列被掩码，解码器的任务是递归地预测掩码标记（图
    [3.6](#Fig6)）。因此，MASS 可以联合训练编码器和解码器，以开发提取嵌入和语言建模的能力。MASS 在神经机器翻译、摘要和对话响应生成等语言生成任务上进行微调。与之前的
    Transformer 架构相比，它显示出显著的性能提升！[](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig6_HTML.png)'
- en: An illustration lists 6 pre-training tasks in a sentence reads, I love vanilla
    ice cream. John did not have any. The tasks are span masking, token masking, token
    deletion, text filling, sentence permutation, and document rotation from the original
    input.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一幅插图列出了句子“我爱香草冰淇淋。约翰一个也没有。”中的 6 个预训练任务，这些任务是：跨度掩码、标记掩码、标记删除、文本填充、句子排列和文档旋转。
- en: Fig. 3.6
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6
- en: Different pre-training tasks to restore corrupted text by the transformer. Span
    masking is the task for MASS [[196](#CR196)]. BART uses all tasks from token masking
    to document rotation [[119](#CR119)]
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 通过不同的预训练任务恢复损坏的文本。Span masking 是 MASS [[196](#CR196)] 的任务。BART 使用从标记掩码到文档旋转的所有任务
    [[119](#CR119)]
- en: '**BART** [[119](#CR119)] uses a standard Transformer-based encoder-decoder
    architecture. The pre-training task is to recover text corrupted by a number of
    different approaches (Fig. [3.6](#Fig6)): predict masked tokens as with BERT;
    predict deleted tokens and their positions, predict the missing tokens replaced
    by a single mask, reconstruct a permuted sentence as with XLNet, and find the
    beginning of a rotated document. BART was fine-tuned on a number of tasks like
    GLUE, SQuAD, summarization, and machine translation. BART achieved the best performance
    with the prediction of missing tokens replaced by a single mask. A large version
    of BART was trained with a hidden size of 1024 and 12 encoder and decoder layers
    with a similar dataset as used by RoBERTa. The resulting performance was similar
    to that of RoBERTa. For abstractive summarization, e.g. on the *CNN/Daily Mail
    benchmark* [[78](#CR78)], BART achieves Sota.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**BART** [[119](#CR119)] 采用基于标准的 Transformer 编码器-解码器架构。预训练任务是通过多种不同的方法恢复被破坏的文本（图
    [3.6](#Fig6)）：预测被掩码的标记，类似于 BERT；预测删除的标记及其位置，预测由单个掩码替换的缺失标记，重构一个排列后的句子，类似于 XLNet，以及找到旋转文档的开始。BART
    在 GLUE、SQuAD、摘要和机器翻译等任务上进行了微调。BART 在预测由单个掩码替换的缺失标记方面取得了最佳性能。一个大型版本的 BART 使用了 1024
    的隐藏大小和 12 个编码器和解码器层，与 RoBERTa 所使用的相似数据集。结果性能与 RoBERTa 相似。对于抽象摘要，例如在 *CNN/Daily
    Mail 基准测试* [[78](#CR78)] 上，BART 实现了 Sota。'
- en: '**PEGASUS** [[251](#CR251)] proposed pre-training large Transformer-based Seq2seq
    models on massive text corpora with a new objective: *gap-sentences generation*,
    where sentences instead of tokens are masked or removed. The model has to generate
    these modified parts as a one sentence output. On 12 document summarization tasks
    the model achieves Sota performance.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**PEGASUS** [[251](#CR251)] 提出了一种新的目标：*gap-sentences generation*，在大量文本语料库上预训练大型基于
    Transformer 的 Seq2seq 模型，其中句子而不是标记被掩码或删除。模型必须将这些修改部分生成为一个单句输出。在 12 个文档摘要任务上，该模型实现了
    Sota 性能。'
- en: '**T5** [[170](#CR170)] is based on the standard transformer architecture. Pre-training
    is performed on a huge training set by restoring corrupted texts, which is formulated
    as a sequence-to-sequence tasks. The comparison of different pre-training tasks
    listed in Fig. [3.6](#Fig6) found that, similar to BART, text infilling achieves
    the best results. If the original text is *“Thank you for inviting me to your
    party last week .”* the model receives the input *“Thank you [X] me to your party
    [Y] week .”* with masked phrases and has to generate the output *“[X] for inviting
    [Y] last [Z]”* to reconstruct the masked phrases.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**T5** [[170](#CR170)] 基于标准的 Transformer 架构。通过恢复损坏的文本进行预训练，这被表述为一个序列到序列的任务。图
    [3.6](#Fig6) 中列出的不同预训练任务的比较发现，与 BART 类似，文本填充取得了最佳结果。如果原始文本是 *“Thank you for inviting
    me to your party last week .”*，则模型接收输入 *“Thank you [X] me to your party [Y] week
    .”*，其中包含掩码短语，并必须生成输出 *“[X] for inviting [Y] last [Z]”* 来重构掩码短语。'
- en: '*Salient span masking* [[72](#CR72)] was especially effective. To focus on
    relevant phrases a BERT-tagger was trained to recognize named entities (person
    names, locations, etc. Sect. [2.​1.​3](528393_1_En_2_Chapter.xhtml#Sec5)), and
    dates were identified by regular expressions. If the model had to recreate these
    spans the model performance was significantly increased. By predicting the omitted
    tokens, the model is able to collect an enormous amount of information on syntactic
    and semantic knowledge. Extensive comparisons show that the sequence-to-sequence
    architecture yields better results than other architectures, e.g. autoregressive
    language models.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*显著跨度掩码* [[72](#CR72)] 特别有效。为了关注相关短语，训练了一个 BERT 标记器来识别命名实体（人名、地点等，第 [2.1.3](528393_1_En_2_Chapter.xhtml#Sec5)
    节），日期通过正则表达式识别。如果模型需要重新创建这些跨度，则模型性能显著提高。通过预测省略的标记，模型能够收集大量关于句法和语义知识的信息。广泛的比较表明，序列到序列架构比其他架构，例如自回归语言模型，产生了更好的结果。'
- en: T5 is pre-trained on a multitask mixture of unsupervised and supervised tasks
    using a training dataset of 750 GB of cleaned English web text. Its largest version
    has 24 layers, 128 attention heads, and 11B parameters. For each task the data
    is converted into a text-to-text format (Fig. [3.7](#Fig7)). The model achieves
    Sota results on many benchmarks, for example summarization, question answering,
    text classification, and more. The results for GLUE is 90.3% [[11](#CR11)].![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig7_HTML.png)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: T5使用750 GB的清洁英文网页文本训练数据集，在无监督和监督任务的多任务混合上进行预训练。其最大版本有24层，128个注意力头和110亿个参数。对于每个任务，数据被转换为文本到文本格式（图[3.7](#Fig7)）。该模型在许多基准测试中实现了Sota结果，例如摘要、问答、文本分类等。GLUE的结果为90.3%
    [[11](#CR11)]。![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig7_HTML.png)
- en: A block diagram represents a set of prompts translated through T 5\. There are
    5 instructions on the left, and their outputs on the right.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 块图表示通过T5翻译的一系列提示。左侧有5条指令，右侧是它们的输出。
- en: Fig. 3.7
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7
- en: Every task in T5 is expressed as a translation task, where the type of the task
    is a prefix to the input text (on the left) and the model produces the corresponding
    output (right) . Adapted from [[170](#CR170), p.3] with kind permission of the
    authors
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: T5中的每个任务都表示为一个翻译任务，其中任务的类型是输入文本（左侧）的前缀，模型产生相应的输出（右侧）。改编自 [[170](#CR170)，第3页]，经作者友好许可。
- en: '**Primer** [[195](#CR195)] proposes two modifications of the original self-attention
    architecture. First the ReLU activation function is squared. In addition, a convolution
    layer is added after each of the multi-head projections for query *Q*, key *K*,
    and value *V* . For the original T5 architecture this reduces the training cost
    by a factor 4.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**Primer** [[195](#CR195)] 提出了对原始自注意力架构的两种修改。首先，ReLU激活函数被平方。此外，在每个多头投影之后添加了一个卷积层，用于查询
    *Q*、键 *K* 和值 *V*。对于原始的T5架构，这减少了训练成本的四倍。'
- en: '**UniLM2** [[8](#CR8)] simultaneously pre-trains a bidirectional language models
    and a sequence-to-sequence model for language generation. The model parameters
    are shared between the two tasks, and the encoding results of the context tokens
    are reused. The model uses two mask types, one for bidirectional masking similar
    to BERT and pseudo masks for language modeling. With special self-attention masks
    and position embeddings, the model can perform both language modeling tasks in
    one forward pass without redundant computation of context. The model beats BART[BASE]
    for reading comprehension on SQuAD 1.1 and T5[BASE] for abstractive summarization
    on CNN/Daily Mail.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**UniLM2** [[8](#CR8)] 同时预训练了一个双向语言模型和一个用于语言生成的序列到序列模型。这两个任务的模型参数是共享的，并且上下文标记的编码结果被重复使用。该模型使用两种掩码类型，一种类似于BERT的双向掩码，以及用于语言模型的伪掩码。通过特殊的自注意力掩码和位置嵌入，模型可以在一次前向传递中执行语言建模任务，而不需要冗余计算上下文。该模型在SQuAD
    1.1的阅读理解任务上击败了BART[BASE]，在CNN/Daily Mail的抽象摘要任务上击败了T5[BASE]。'
- en: '**GLM** (General Language Model) [[54](#CR54), [55](#CR55)] is a successor
    of UniLM2 aiming to combine the different learning paradigms of BERT, GPT and
    the transformer. For pre-training GLM has the task to generate multiple text spans
    in an autoregressive way basically using the GPT architecture. From the input
    text ***x*** = (*x*[1], …, *x*[*T*]) a number *m* spans ![$$x_{i_1},\ldots , x_{i_1+l_i}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq20.png)
    are sampled. Each span is replaced with a single *[MASK]* token yielding the corrupted
    input ***x***[corrupt]. The model then successively generates the tokens of the
    spans having access to the corrupted input and the already generated tokens of
    the spans (Fig. [3.8](#Fig8)). Within the input text all tokens are connected
    by self attention while in the output section a masked self-attention is used.
    Each span is finished by an *[END]* token. To identify the positions of generated
    tokens two positions are encoded by embeddings: the input position and the position
    within a span. Note that the mask prediction can be done in arbitrary sequence
    and the model has to predict the length of the spans during reconstruction.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig8_HTML.png)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**GLM** (通用语言模型) [[54](#CR54), [55](#CR55)] 是 UniLM2 的后继者，旨在结合 BERT、GPT 和 transformer
    的不同学习范式。对于预训练，GLM 的任务是使用 GPT 架构以自回归方式生成多个文本跨度。从输入文本 ***x*** = (*x*[1], …, *x*[*T*])
    中采样一个数量为 *m* 的跨度 ![$$x_{i_1},\ldots , x_{i_1+l_i}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq20.png)。每个跨度被替换为一个单独的
    *token*，从而得到损坏的输入 ***x***[corrupt]。然后模型依次生成跨度的标记，同时访问损坏的输入和已生成的跨度的标记（图 [3.8](#Fig8)）。在输入文本中，所有标记通过自注意力连接，而在输出部分使用掩码自注意力。每个跨度由一个
    *[END]* 标记结束。为了识别生成标记的位置，通过嵌入编码两个位置：输入位置和跨度内的位置。请注意，掩码预测可以按任意顺序进行，并且在重建过程中模型必须预测跨度的长度。[![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig8_HTML.png)]'
- en: An illustration of a set of full self-attention and masked self-attention going
    through the transformer with self-attention. It indicates the layers of input
    position, mask position, embeddings, output embeddings, and token probabilities.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一组完整自注意力和掩码自注意力通过具有自注意力的 transformer 的示意图。它表示输入位置、掩码位置、嵌入、输出嵌入和标记概率的层。
- en: Fig. 3.8
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8
- en: During pre-training GLM has the task to reconstruct masked single words or multi-word
    phrases. The position of generated words in the text and in the masks are indicated
    by position embeddings, which are added to the token embeddings. The generated
    answers are terminated by an *[END]* token [[54](#CR54)]
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练期间，GLM 的任务是重建掩码的单个单词或多词短语。通过位置嵌入指示生成单词在文本和掩码中的位置，这些嵌入被添加到标记嵌入中。生成的答案由一个
    *[END]* 标记结束 [[54](#CR54)]
- en: For fine-tuning, text classification tasks are converted to word predictions.
    To assess the sentence *“The waiters were friendly.”* in a sentiment classification
    task the input is extended to *“The waiters were friendly. It’s really [MASK].”*
    where *[MASK]* has to be replaced by *“good”* or *“bad”*. For a text generation
    task a *[MASK]* token is appended to the input text. Then the model generates
    the continuation as the output text in an autoregressive way. In contrast to BERT
    the model observes the dependency between masked tokens yielding more consistent
    predictions. In comparison to XLNet no additional attention for position encoding
    is needed reducing the computational requirements. Compared to T5, GLM predicts
    the spans in arbitrary order and requires fewer extra tokens.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于微调，文本分类任务被转换为单词预测。为了评估情感分类任务中的句子 *“The waiters were friendly.”*，输入被扩展为 *“The
    waiters were friendly. It’s really
- en: To evaluate the model performance, Du et al. [[54](#CR54)] train GLM[BASE] and
    GLM[LARGE] with the same training data and parameter counts (110M and 340M) as
    BERT[BASE] and BERT[LARGE]. For both model configurations, GLM outperforms BERT
    on SuperGLUE (Sect. [4.​1.​2](528393_1_En_4_Chapter.xhtml#Sec3)), e.g. GLM[LARGE]
    has an average score of 77.0 compared to 72.0 for BERT[LARGE]. On a larger pre-training
    dataset for a model with the same size as RoBERTa they yield an average SuperGLUE
    score of 82.9 compared to 81.5 for RoBERTa. They show that by multitask learning,
    a single model with the same parameters can simultaneously achieve higher accuracy
    in NLU, generating text given an input, and solve other tasks such as summarization
    [[53](#CR53)].
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型性能，Du 等人 [[54](#CR54)] 使用与 BERT[BASE] 和 BERT[LARGE] 相同的训练数据和参数数量（110M
    和 340M）训练 GLM[BASE] 和 GLM[LARGE]。对于这两种模型配置，GLM 在 SuperGLUE (第 [4.1.2](528393_1_En_4_Chapter.xhtml#Sec3))
    上优于 BERT，例如 GLM[LARGE] 的平均分数为 77.0，而 BERT[LARGE] 为 72.0。在大小与 RoBERTa 相同的更大预训练数据集上，他们得到的平均
    SuperGLUE 分数为 82.9，而 RoBERTa 为 81.5。他们表明，通过多任务学习，具有相同参数的单个模型可以同时在高精度 NLU、给定输入生成文本以及解决其他任务（如摘要
    [[53](#CR53)]）中取得更高的准确率。
- en: Larger models like **GLaM** [[51](#CR51)] and **WuDao-2.0** [[257](#CR257)]
    have a mixture-of-experts architecture and are described in Sect. [3.5.2](#Sec26).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的模型如 **GLaM** [[51](#CR51)] 和 **WuDao-2.0** [[257](#CR257)] 具有专家混合架构，并在第 [3.5.2](#Sec26)
    节中描述。
- en: 3.1.4 Systematic Comparison of Transformer Variants
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.4 Transformer 变体的系统比较
- en: 'As an example of a fair comparison of architectural features, we report the
    following experimental analysis of PLMs, where Narang et al. [[142](#CR142)] evaluated
    the effect of a number of transformer modifications. The following transformer
    features were investigated:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对架构特征进行公平比较的例子，我们报告了以下 PLM 的实验分析，其中 Narang 等人 [[142](#CR142)] 评估了多种 transformer
    修改的效果。以下 transformer 特性被研究：
- en: '*Activation functions:* In addition to the ReLU-activation in the feedforward
    layers 11 different activations functions were assessed.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*激活函数:* 除了前馈层中的 ReLU 激活函数外，还评估了 11 种不同的激活函数。'
- en: '*Normalization:* Together with the original layer normalization, five different
    regularization techniques were explored.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*归一化:* 与原始层归一化一起，探索了五种不同的正则化技术。'
- en: '*Number of layers:* The number *d*[*L*] of layers was varied between 6 and
    24\. To keep the comparison fair, the number of parameters was held constant by
    varying the number *d*[*H*] of heads and the widths *d*[ff] of internal embeddings.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*层数数量:* 层数的数量 *d*[*L*] 在 6 到 24 之间变化。为了保持比较的公平性，通过改变头数 *d*[*H*] 和内部嵌入宽度 *d*[ff]
    来保持参数数量不变。'
- en: '*Token embeddings:* The original transformer embeddings were compared to five
    variants of factored embeddings. In addition, the sharing of transformer blocks
    was investigated.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标记嵌入:* 将原始 transformer 嵌入与五种因式分解嵌入的变体进行了比较。此外，还研究了 transformer 块的共享。'
- en: '*Softmax:* The standard softmax to compute token probabilities was contrasted
    to three softmax variants.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Softmax:* 将计算标记概率的标准 softmax 与三种 softmax 变体进行了对比。'
- en: '*Architecture:* The authors compared the base transformer with 17 other architectures.
    In most cases, the number of parameters was kept about the same.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*架构:* 作者比较了基础 transformer 与 17 种其他架构。在大多数情况下，参数数量保持大致相同。'
- en: 'The authors evaluated the variants in two settings: Transfer learning based
    on the T5 transformer (Sect. [3.1.3](#Sec4)) and supervised machine translation
    on the *WMT2014 En-De* [[17](#CR17)]. With some caution, the results can also
    be applied to other types of PLMs like BERT and GPT.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在两种设置中评估了这些变体：基于 T5 transformer 的迁移学习（第 [3.1.3](#Sec4) 节）和 *WMT2014 En-De*
    上的监督机器翻译[*17](#CR17)]。谨慎起见，这些结果也可以应用于其他类型的 PLM，如 BERT 和 GPT。
- en: 'Each architecture variant of T5 was pre-trained on the *C4 dataset* [[171](#CR171)]
    of 806 GB using the “span corruption” masked language modeling objective. Subsequently,
    T5 was fine-tuned on three tasks: the *SuperGLUE* language understanding task
    [[219](#CR219)], the *XSum* abstractive summarization dataset [[143](#CR143)],
    and the *WebQuestions benchmark* [[13](#CR13)], where no additional knowledge
    was provided as background information. The computing effort and the number of
    parameters for each model was fixed to the same level. An exception was an architecture
    with significantly fewer parameters, which was trained for longer.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: T5的每个架构变体都在806 GB的*C4数据集*[[171](#CR171)]上使用“span corruption”掩码语言建模目标进行预训练。随后，T5在三个任务上进行微调：*SuperGLUE*语言理解任务[[219](#CR219)]，*XSum*抽象摘要数据集[[143](#CR143)]，以及*WebQuestions基准测试*[[13](#CR13)]，在这些任务中，没有提供额外的背景信息作为知识。每个模型的计算努力和参数数量都固定在同一水平。一个例外是参数显著减少的架构，它训练时间更长。
- en: Several *activation functions* achieve a better performance compared to the
    ReLU activation, especially *SwiGLU* and *GEGLU*, which are *gated linear units*
    (GLU) forming a product with another activation [[189](#CR189)]. The improvement
    can be observed for pre-training, fine-tuning, and supervised training without
    affecting the computation time. For SuperGLUE, for instance, an increase from
    71.7% to about 76.0% can be observed. Replacing *layer normalization* with *RMS
    normalization* [[249](#CR249)] causes performance gains for all tasks. The SuperGLUE
    score, for example, was improved from 71.7% to 75.5%. In addition, the training
    speed was higher.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 相比ReLU激活函数，一些*激活函数*的性能更好，特别是*SwiGLU*和*GEGLU*，它们是*门控线性单元*（GLU），与另一个激活函数形成乘积[[189](#CR189)]。这种改进可以在预训练、微调和监督训练中观察到，而不会影响计算时间。例如，对于SuperGLUE，分数从71.7%提高到约76.0%。将*层归一化*替换为*RMS归一化*[[249](#CR249)]会导致所有任务的性能提升。例如，SuperGLUE的分数从71.7%提高到75.5%。此外，训练速度也更快。
- en: As expected, increasing the depth of a models usually led to a better performance
    even if the number of parameters is kept constant. On SuperGLUE the model with
    18 layers achieved a score of 76.5% compared to 71.7% for the base model. Similar
    improvements can be observed for WebQuestions and translation, while there were
    no improvements for the summarization task. This is in line with theoretical results
    (Sect. [3.5.1](#Sec25)). A drawback is that deeper models require more computation
    time.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，增加模型的深度通常会导致性能提升，即使参数数量保持不变。在SuperGLUE上，具有18层的模型取得了76.5%的分数，而基础模型的分数为71.7%。对于WebQuestions和翻译任务，也可以观察到类似的改进，而在摘要任务上没有观察到改进。这与理论结果（第[3.5.1节](#Sec25)）一致。缺点是深度模型需要更多的计算时间。
- en: Architectures, which share parameters in different layers, usually lead to a
    decreased performance. The effect of using the same embeddings for encoders and
    decoders is mixed. Factorization of embeddings into a matrix product usually cause
    inferior results. If a *Mixture of Softmaxes* [[239](#CR239)] is used to predict
    the output probabilities, the performance usually is better, e.g. an increase
    to 76.8% for SuperGLUE. However, this approach requires up to 40% more computation
    effort.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同层之间共享参数的架构通常会导致性能下降。使用相同的嵌入表示编码器和解码器的影响是混合的。将嵌入分解为矩阵乘积通常会导致较差的结果。如果使用*Softmax混合*[[239](#CR239)]来预测输出概率，性能通常会更好，例如SuperGLUE的分数提高到76.8%。然而，这种方法需要高达40%更多的计算努力。
- en: Of the architectural variants evaluated, two combinations of the *Synthesizers*
    with dot-product attention (Sect. [3.2.2](#Sec9)) perform better than the standard
    Transformer. The Synthesizers do not compute a “correlation” of embeddings but
    determine the attention weights from a single embedding or randomly. Switch Transformer,
    Mixture-of-experts, and Product key memories all have significantly more parameters
    than the baseline transformer but are able to improve performance. The *Switch*
    transformer ([[56](#CR56)] Sect. [3.5.2](#Sec26)) has many more parameters than
    the base T5 model. To reach the same performance as Switch, T5 needs seven times
    more training FLOPS (floating point operations per second). The *Mixture-of-experts*
    model [[116](#CR116)] distributes computations to 2 expert models in both the
    encoder and the decoder. *Product key memory* ([[112](#CR112)] Sect. [3.1.1](#Sec2))
    replaces the dot-product attention by a nearest neighbor search.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估的架构变体中，两种与点积注意力（Sect. [3.2.2](#Sec9)）结合的 *Synthesizers* 表现优于标准Transformer。Synthesizers不计算嵌入的“相关性”，而是从单个嵌入或随机确定注意力权重。Switch
    Transformer、专家混合模型和乘积键内存都比基线Transformer有显著更多的参数，但能够提高性能。*Switch* Transformer ([[56](#CR56)]
    Sect. [3.5.2](#Sec26)) 比基础T5模型有更多的参数。为了达到与Switch相同的表现，T5需要七倍的训练FLOPS（每秒浮点运算数）。*专家混合模型*
    [[116](#CR116)] 在编码器和解码器中都分配计算到2个专家模型。*乘积键内存* ([[112](#CR112)] Sect. [3.1.1](#Sec2))
    用最近邻搜索替换了点积注意力。
- en: For all other 12 architectures, there were no improvements over the standard
    transformer [[142](#CR142)]. This is different to the findings of the papers proposing
    the models. A reason seems to be that changes of the transformer architecture
    are difficult to transfer to other code bases and applications. Therefore, the
    authors propose to try out new modifications on different low-level implementations.
    In addition, a new approach should be evaluated on a variety of downstream applications
    including transfer learning, supervised learning, and language modeling. *Hyperparameter*
    optimization should be kept fixed to assure the robustness of the approach. Finally,
    the mean and standard deviation of results should be reported to avoid the selection
    of a single best result.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有其他12种架构，它们在标准Transformer [[142](#CR142)] 上没有带来任何改进。这与提出这些模型的论文的发现不同。一个可能的原因是，Transformer架构的变化难以转移到其他代码库和应用中。因此，作者建议在不同的低级实现上尝试新的修改。此外，应该对包括迁移学习、监督学习和语言建模在内的各种下游应用进行新的方法评估。*超参数*优化应保持固定，以确保方法的鲁棒性。最后，应报告结果的平均值和标准差，以避免选择单个最佳结果。
- en: 3.1.5 Summary
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.5 摘要
- en: The modification of pre-training tasks has a profound influence on the performance
    of PLMs. Many different types of pre-training losses have been evaluated, such
    as masked phrase prediction, replaced token detection, or sentence order recognition.
    According to the benchmarks, the prediction of permuted tokens by XLNET is especially
    rewarding because XLNET takes into account the dependency between masked tokens.
    In addition, DeBERTa’s disentangled token and position embeddings are able to
    boost the performance in downstream classifiers. With respect to applications,
    autoencoders like BERT are particular important for information extraction in
    Chap. [5](528393_1_En_5_Chapter.xhtml).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练任务的修改对PLMs的性能有深远的影响。已经评估了许多不同类型的预训练损失，例如掩码短语预测、替换标记检测或句子顺序识别。根据基准测试，XLNET对置换标记的预测特别有益，因为XLNET考虑了掩码标记之间的依赖关系。此外，DeBERTa的解耦标记和位置嵌入能够提升下游分类器的性能。就应用而言，像BERT这样的自编码器在信息提取（第[5](528393_1_En_5_Chapter.xhtml)章）中特别重要。
- en: For autoregressive PLMs like GPT, a number of variants with larger model size
    and larger training data have been presented. However, in most cases, the pre-training
    tasks were not changed. The training of the larger models required improvements
    in the parallel computing infrastructure and resulted in an unprecedented performance
    in text generation. By creating custom start texts (prompting), the models can
    solve a large number of specific tasks with very high accuracy without further
    fine-tuning (Sect. [3.6.3](#Sec41)). The amount and quality of knowledge captured
    by PLMs is surprisingly high and is discussed in Chap. [4](528393_1_En_4_Chapter.xhtml).
    In terms of applications, autoregressive PLMs are used in particular for text
    (Chap. [6](528393_1_En_6_Chapter.xhtml)) and image generation (Sect. [7.​2](528393_1_En_7_Chapter.xhtml#Sec12)).
    Because of their versatility and the tremendous increase in performance, recent
    large-scale PLMs are called *Foundation Models*.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像GPT这样的自回归PLM，已经提出了具有更大模型规模和更大训练数据的多种变体。然而，在大多数情况下，预训练任务并未改变。更大模型的训练需要改进并行计算基础设施，并在文本生成中实现了前所未有的性能。通过创建自定义起始文本（提示），模型可以在不进行进一步微调的情况下解决大量特定任务，并达到非常高的准确率（见第[3.6.3](#Sec41)节）。PLM捕获的知识量和质量出人意料地高，这在第[4](528393_1_En_4_Chapter.xhtml)章中进行了讨论。在应用方面，自回归PLM特别用于文本（见第[6](528393_1_En_6_Chapter.xhtml)章）和图像生成（见第[7.2](528393_1_En_7_Chapter.xhtml#Sec12)节）。由于它们的通用性和性能的巨大提升，最近的大型PLM被称为*基础模型*。
- en: Encoder-decoder transformers were introduced for translating a text from one
    language to another. A number of new pre-training tasks were evaluated for these
    models. Some of them are similar to the tasks for autoencoders, such as predicting
    masked spans or inserting omitted tokens. Others were adapted to the input-output
    architecture, e.g. the reconstruction of sentence permutations and document rotations.
    Here BART and T5 achieved the best performances in the GLUE and SuperGLUE natural
    language understanding tasks. By creating additional synthetic training examples,
    the performance of T5 and other models can be increased (Sect. [3.6.6](#Sec46)).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器变压器被引入用于将一种语言的文本翻译成另一种语言。对这些模型评估了许多新的预训练任务。其中一些与自编码器的任务类似，例如预测掩码跨度或插入省略的标记。其他任务则被调整为输入-输出架构，例如句子排列和文档旋转的重构。在这里，BART和T5在GLUE和SuperGLUE自然语言理解任务中取得了最佳性能。通过创建额外的合成训练示例，T5和其他模型的性能可以得到提高（见第[3.6.6](#Sec46)节）。
- en: A systematic comparison of transformer architectures demonstrated that several
    architectural changes increased performance. The SwiGLU and GEGLU activation function
    instead of ReLU increased accuracy for SuperGLUE by more than 4%. Similar gains
    were observed when using RMS normalization instead of layer normalization. Increasing
    the model depth resulted in better performance even when the number of parameters
    was held constant. Synthesizers, mixtures-of-experts, and Product keys replacing
    scalar products by *k*-means clustering also performed better than the standard
    transformer.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对变压器架构的系统比较表明，一些架构变化提高了性能。使用SwiGLU和GEGLU激活函数代替ReLU，SuperGLUE的准确率提高了超过4%。使用RMS归一化代替层归一化时也观察到了类似的增益。即使保持参数数量不变，增加模型深度也能带来更好的性能。通过用k-means聚类替换标量乘积，合成器、专家混合和产品键也表现优于标准变压器。
- en: T5 and GLM demonstrate that transformers, controlled by instructive prompts,
    can be used to solve arbitrary problems of text classification, text generation,
    and text translation. They thus combine the capabilities of BERT, GPT, and translation
    models. Transformers are used extensively in complex text generation tasks, e.g.
    machine translation (Sect. [6.​3](528393_1_En_6_Chapter.xhtml#Sec19)), dialog
    (Sect. [6.​6](528393_1_En_6_Chapter.xhtml#Sec49)), and image generation (Sect.
    [7.​2](528393_1_En_7_Chapter.xhtml#Sec12)).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: T5和GLM表明，在指导性提示的控制下，变压器可以用于解决文本分类、文本生成和文本翻译的任意问题。因此，它们结合了BERT、GPT和翻译模型的能力。变压器在复杂的文本生成任务中被广泛使用，例如机器翻译（见第[6.3](528393_1_En_6_Chapter.xhtml#Sec19)节）、对话（见第[6.6](528393_1_En_6_Chapter.xhtml#Sec49)节）和图像生成（见第[7.2](528393_1_En_7_Chapter.xhtml#Sec12)节）。
- en: 3.2 Capturing Longer Dependencies
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 捕获更长的依赖关系
- en: A well-known concern with self-attention is the quadratic time and memory complexity,
    which can hinder the scalability of the model in many settings (Sect. [2.​1.​6](528393_1_En_2_Chapter.xhtml#Sec9)).
    If the sequence length *T* is increased to 2*T* then four times as many associations
    (attentions) between tokens have to be computed. This limits the direct applicability
    of models when a task requires larger contexts, such as answering questions or
    summarizing a document. Moreover, a larger memory is required to store the attentions
    for training. Therefore, a number of concepts have been proposed to cover long
    sequences without excessive computational and memory demands.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力存在的一个众所周知的问题是二次时间和内存复杂度，这可能会阻碍模型在许多设置中的可扩展性（见第 [2.1.6](528393_1_En_2_Chapter.xhtml#Sec9)
    节）。如果序列长度 *T* 增加到 2*T*，则需要在标记之间计算四倍的关联（注意力）。这限制了模型在需要更大上下文的任务中的直接应用，例如回答问题或总结文档。此外，还需要更大的内存来存储训练中的注意力。因此，已经提出了许多概念来覆盖长序列，同时不过度增加计算和内存需求。
- en: Sparse attention matrices are employed by BigBird, the Sparse Transformer, Longformer,
    and GPT-3 to reduce the number of parameters.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BigBird、稀疏 Transformer、Longformer 和 GPT-3 使用稀疏注意力矩阵来减少参数数量。
- en: Clustering tokens by locality-sensitive hashing reduces the number of attentions
    computed by the Reformer.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过局部敏感哈希对标记进行聚类可以减少 Reformer 计算的注意力数量。
- en: Low-rank-approximation of attention matrices or by a kernel-based formulation
    of self-attention decreases the number of parameters of the Performer and the
    Linear Transformer.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过注意力矩阵的低秩近似或基于核的自注意力公式可以减少 Performer 和线性 Transformer 的参数数量。
- en: Transformer-XL and the Linear Transformer reuse computations from previous text
    segments in an autoregressive manner to lower computational overhead.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer-XL 和线性 Transformer 以自回归方式重用先前文本段落的计算，以降低计算开销。
- en: Surveys of techniques for enlarging the input sequence are provided by Tay et
    al. [[207](#CR207)] and Fournier et al. [[59](#CR59)].
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Tay 等人 [[207](#CR207)] 和 Fournier 等人 [[59](#CR59)] 提供了扩大输入序列的技术综述。
- en: 3.2.1 Sparse Attention Matrices
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 稀疏注意力矩阵
- en: '**BigBird** [[247](#CR247)] reduces the number of attention computations by
    omitting entries according to some pre-determined pattern from the matrix of attention
    relations. BigBird extends transformer-based models, e.g. BERT, and uses a set
    of *g**global tokens* attending on all tokens of the sequence. In addition, each
    token *v*[*t*] attends to a set of *n*[*l*] local *neighboring tokens* and to
    a set of *n*[*r*]*random tokens*. The resulting association matrices are shown
    in Fig. [3.9](#Fig9). If the numbers *g*, *n*[*l*], and *n*[*r*] do not increase
    with sequence length *T* the number of attentions grows linearly with *T*.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig9_HTML.png)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**BigBird** [[247](#CR247)] 通过从注意力关系矩阵中根据某些预先确定的模式省略条目来减少注意力计算的数量。BigBird 扩展了基于
    Transformer 的模型，例如 BERT，并使用一组 *g* 全局标记来关注序列中的所有标记。此外，每个标记 *v*[*t*] 关注一组 *n*[*l*]
    本地 *邻近标记* 和一组 *n*[*r*]*随机标记*。结果关联矩阵如图 [3.9](#Fig9) 所示。如果 *g*、*n*[*l*] 和 *n*[*r*]
    的数量不随序列长度 *T* 增加而增加，则注意力数量将随 *T* 线性增长。![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig9_HTML.png)'
- en: 4 illustrations exhibit different arrangements of a cluster of squared blocks.
    It denotes window attention, global attention, random attention, and combined
    attention.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 4 个插图展示了平方块群的不同排列。它表示窗口注意力、全局注意力、随机注意力和组合注意力。
- en: Fig. 3.9
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9
- en: 'Attention mechanism used in BigBird [[247](#CR247)] to compute the association
    between input tokens. Matrix indicating attention between pairs of tokens: attentions
    between sequence neighbors (left), global attentions to a few tokens (second left),
    random attentions (third from left), the combined BigBird attentions (right).
    White blocks indicate omitted attention pairs The model is constructed in such
    a way that the length of the path between arbitrary token pairs along intermediate
    tokens is kept small, as in a small-world graph. The authors prove that their
    model allows to express all continuous sequence-to-sequence functions with only
    *O*(*T*) inner products (Table [3.6](#Tab6)). In addition, they show that under
    standard assumptions BigBird is Turing complete, i.e. can perform arbitrary computations
    (see also [[246](#CR246)]). The BigBird attention module can be used in BERT,
    autoregressive language models, and Transformer architectures. In a number of
    applications BigBird using a sequence length of 4096 is able to improve the Sota,
    e.g. for question answering requiring multi-hop reasoning from the given evidences.
    Note that BigBird without random attention performed better than BigBird with
    random attention in a set of experiments.Table 3.6'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: BigBird [[247](#CR247)] 中使用的注意力机制用于计算输入标记之间的关联。表示标记对之间注意力的矩阵：序列邻居之间的注意力（左侧），对少数标记的全局注意力（第二左侧），随机注意力（第三左侧），结合的
    BigBird 注意力（右侧）。白色块表示省略的注意力对。该模型以这种方式构建，使得任意标记对之间沿中间标记的路径长度保持较小，就像一个小世界图。作者们证明，他们的模型仅通过
    *O*(*T*) 个内积就可以表达所有连续的序列到序列函数（见表 [3.6](#Tab6)）。此外，他们还表明，在标准假设下，BigBird 是图灵完备的，即可以执行任意计算（参见
    [[246](#CR246)]）。BigBird 注意力模块可用于 BERT、自回归语言模型和 Transformer 架构。在许多应用中，使用序列长度为
    4096 的 BigBird 能够提高 Sota，例如，对于需要从给定证据中进行多跳推理的问答。请注意，在一系列实验中，没有随机注意力的 BigBird 比带有随机注意力的
    BigBird 表现更好。表 3.6
- en: Important models with sparse self-attention for long dependencies. *T* is the
    sequence length, *g* number of global tokens, *k* is window size. (cf. [[207](#CR207)])
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于长依赖关系的重要模型具有稀疏自注意力。*T* 是序列长度，*g* 是全局标记数，*k* 是窗口大小。（参看 [[207](#CR207)]）
- en: '![](../images/528393_1_En_3_Chapter/528393_1_En_3_Figaaa_HTML.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/528393_1_En_3_Chapter/528393_1_En_3_Figaaa_HTML.png)'
- en: A table represents the details of the complexity, low rank, recurrence, memory,
    random patterns, and learnable patterns of 10 different models.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 一个表格表示了 10 个不同模型的复杂度、低秩、递归、内存、随机模式和可学习模式的详细信息。
- en: Prior models using these concepts were the *Sparse Transformer* [[33](#CR33)]
    and the *Longformer* [[10](#CR10)], which similarly to WaveNet [[148](#CR148)]
    employ strided or “dilated” neighborhoods. Here not all adjacent neighbors are
    attended by a token, but only every *d*-th neighbor with *d* > 1\. If *k* layers
    are used, this construction covers *d*^(*k*) neighbors and thus allows associations
    over large distances. The **Extended Transformer Construction** (ETC) model [[3](#CR3)]
    generalizes the idea of global tokens, which can communicate associations between
    far-away tokens of the whole sequence.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 之前使用这些概念的模型是 *稀疏 Transformer* [[33](#CR33)] 和 *Longformer* [[10](#CR10)]，它们类似于
    WaveNet [[148](#CR148)]，使用步进或“膨胀”的邻域。在这里，并非所有相邻的邻居都会被一个标记所关注，而只有每隔 *d* 个邻居（*d*
    > 1）。如果使用 *k* 层，这种结构将覆盖 *d*^(*k*) 个邻居，从而允许跨越较大距离的关联。**扩展 Transformer 构造**（ETC）模型
    [[3](#CR3)] 将全局标记的概念进行了推广，它可以沟通整个序列中远离的标记之间的关联。
- en: '**GPT-3** [[25](#CR25)] (Sect. [3.1.2](#Sec3)) is a recent language model with
    96 layers, 96 attention heads, 175 billion parameters covering sequences of length
    2048\. To cope with the excessive sequence length the authors used “alternating
    dense and locally banded sparse attention patterns in the layers of the transformer,
    similar to the Sparse Transformer” [[33](#CR33)]. The details of the architecture
    are not yet known. The model achieved an unprecedented performance in language
    modeling, question answering, etc., which is discussed in Sect. [3.6.3](#Sec41).'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-3** [[25](#CR25)]（第 [3.1.2](#Sec3) 节）是一个最近的语言模型，有 96 层，96 个注意力头，1750
    亿个参数，覆盖长度为 2048 的序列。为了应对过长的序列长度，作者们在 Transformer 的层中使用了“交替密集和局部带状稀疏注意力模式”，类似于稀疏
    Transformer [[33](#CR33)]。架构的细节尚不清楚。该模型在语言建模、问答等方面取得了前所未有的性能，这在第 [3.6.3](#Sec41)
    节中进行了讨论。'
- en: 3.2.2 Hashing and Low-Rank Approximations
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 哈希和低秩近似
- en: The **Reformer** [[108](#CR108)] introduces locality-sensitive hashing to cluster
    tokens with similar key/query vectors. This approach hashes similar input items
    into the same “buckets” with high probability. For each cluster the same query/key
    parameters are used. In this way, tokens are aggregated in a data-driven fashion.
    In a similar way, the *Routing Transformer* [[180](#CR180)] clusters tokens by
    *k*-means clustering.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**Reformer** [[108](#CR108)] 引入了局部敏感哈希来聚类具有相似键/查询向量的标记。这种方法以高概率将相似输入项哈希到相同的“桶”中。对于每个聚类，使用相同的查询/键参数。这样，标记以数据驱动的方式聚合。以类似的方式，*路由变换器*
    [[180](#CR180)] 通过k-means聚类来聚类标记。'
- en: '**Transformer-XL** [[44](#CR44)] reuses computation results from prior segments
    of a sequence. With this recurrence mechanism applied to every two consecutive
    segments of a corpus, it essentially creates a segment-level recurrence in the
    hidden states. With multiple layers, the effective context being utilized can
    go way beyond just two segments. A similar approach is used by the *Compressive
    Transformer* [[169](#CR169)]. *Segatron* is a variant that encodes a paragraph
    index in a document, a sentence index in a paragraph, and token index in a sentence
    as embeddings to be added to the token embedding. This modification leads to a
    better perplexity in language modeling.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer-XL** [[44](#CR44)] 重新使用序列先前部分的计算结果。通过将这种递归机制应用于语料库的每两个连续部分，它实际上在隐藏状态中创建了一个段级递归。通过多层，有效利用的上下文可以远远超出仅仅两个段。*压缩变换器*
    [[169](#CR169)] 也采用了类似的方法。*Segatron* 是一种变体，它将文档中的段落索引、段落中的句子索引和句子中的标记索引编码为要添加到标记嵌入中的嵌入。这种修改导致语言模型中的困惑度更好。'
- en: The **Performer** [[34](#CR34)] reduces the computational load by employing
    low rank approximations of the self-attention matrix. It uses a random kernel
    with positive orthogonal random features to compute the self-attention. By orthogonality,
    the authors avoid computing the full square matrix of products, since the dot
    product of orthogonal features is 0\. Hence, computation requirements grow linearly
    with sequence length. The authors are able to prove that their model allows nearly-unbiased
    estimation of the full attention matrix as well as uniform convergence and lower
    variance of the approximation.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**Performer** [[34](#CR34)] 通过采用自注意力矩阵的低秩近似来减少计算负载。它使用具有正正交随机特征的随机核来计算自注意力。通过正交性，作者避免了计算完整平方矩阵的乘积，因为正交特征的点积为0。因此，计算需求与序列长度线性增长。作者能够证明，他们的模型允许几乎无偏估计完整的注意力矩阵，以及近似的一致收敛和较低的方差。'
- en: The **Linear Transformer** [[105](#CR105)] also uses a kernel-based formulation
    of self-attention reducing complexity to linear. For predicting the future elements
    from past inputs, the authors are able to construct an iterative algorithm similar
    to RNNs that is dramatically faster than standard transformers. The model has
    been shown to improve inference speeds up to three orders of magnitude without
    much loss in predictive performance.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性变换器** [[105](#CR105)] 也使用基于核的自注意力公式，将复杂度降低到线性。为了从过去输入预测未来元素，作者能够构建一个类似于RNN的迭代算法，该算法比标准变换器快得多。该模型已被证明可以提高推理速度高达三个数量级，而预测性能损失不大。'
- en: The **Transformer-LS** (Long-Short Transformer) [[258](#CR258)] has a local
    sliding window attention between neighboring tokens and a long-range attention
    with dynamic projections to represent relationships between distant tokens. The
    dynamic low-rank projections depends on the content of the input sequence. The
    authors claim that the approach is more robust against insertion, deletion, paraphrasing,
    etc. The scheme achieves Sota perplexities in language modeling for different
    benchmarks, e.g. 0.99 for enwik8 and Sota results as vision transformer on ImageNet.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer-LS**（长短变换器）[[258](#CR258)] 在相邻标记之间具有局部滑动窗口注意力，并通过动态投影来表示远程标记之间的关系。动态低秩投影依赖于输入序列的内容。作者声称，这种方法对插入、删除、释义等更具鲁棒性。该方案在语言模型的不同基准测试中实现了Sota困惑度，例如enwik8为0.99，以及在ImageNet上的视觉变换器Sota结果。'
- en: The **Combiner** [[174](#CR174)] represents groups of embeddings by key vectors.
    The probability that a given token *v*[*t*] attends to a token *v*[*s*] is described
    by a product, where *v*[*t*] first attends to the key vector that represents a
    group of locations containing *v*[*s*] multiplied by the probability of choosing
    *v*[*s*] within that group. In this way, the Combiner can be applied to sequences
    of length up to 12,000\. The approach is able to achieve Sota perplexity on large
    benchmarks. In addition, it improves the average performance on the *Long Range
    Arena benchmark* [[209](#CR209)] specifically focused on evaluating model quality
    for long documents.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**组合器** [[174](#CR174)] 通过键向量表示嵌入组。一个给定的标记 *v*[*t*] 关注到标记 *v*[*s*] 的概率由一个乘积描述，其中
    *v*[*t*] 首先关注代表包含 *v*[*s*] 的位置组的键向量，然后乘以在该组中选择 *v*[*s*] 的概率。这样，组合器可以应用于长度高达 12,000
    的序列。这种方法能够在大型基准测试上实现 Sota 混淆度。此外，它还提高了针对评估长文档模型质量的特定基准 *Long Range Arena benchmark*
    [[209](#CR209)] 上的平均性能。'
- en: The **Synthesizer** [[206](#CR206)] replaces the pairwise dot products of attention
    with “synthesizing functions” that learn attention matrices, which may or may
    not depend on the input tokens (cf. Sect. [3.1.4](#Sec5)). In the Dense Synthesizer,
    each token embedding *x*[*i*], *i* = 1, …, *T*, in a layer is projected to a vector
    of the length *T* using a two-layered nonlinear feed-forward network with a ReLU
    activation. The values of this vector are used as weights to determine the mixture
    of values to form the output embedding. Hence, no “correlations” between embeddings
    are computed to determine their similarity, as it is done for the standard self-attention.
    There is an extreme variant, where the mixing proportions are set randomly. Nevertheless,
    on multiple tasks such as machine translation, language modeling, dialogue generation,
    masked language modeling and document classification, this “synthetic” attention
    demonstrates competitive performance compared to vanilla self-attention. The combination
    of Random Synthesizers with normal dot-product attention is able to beat T5 on
    several benchmarks.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**合成器** [[206](#CR206)] 用“合成函数”替换了注意力中的成对点积，这些函数学习注意力矩阵，这些矩阵可能或可能不依赖于输入标记（参见图
    [3.1.4](#Sec5)）。在密集合成器中，一个层的每个标记嵌入 *x*[*i*]，其中 *i* = 1, …, *T*，被一个具有 ReLU 激活的二层非线性前馈网络投影到一个长度为
    *T* 的向量。这个向量的值被用作权重，以确定形成输出嵌入的值混合。因此，与标准自注意力不同，不需要计算嵌入之间的“相关性”来确定它们的相似性。有一个极端的变体，其中混合比例是随机设置的。尽管如此，在机器翻译、语言建模、对话生成、掩码语言建模和文档分类等多个任务中，这种“合成”注意力与标准自注意力相比表现出具有竞争力的性能。随机合成器与正常点积注意力的结合能够在多个基准测试上击败
    T5。'
- en: The **Perceiver** [[93](#CR93)] defines an asymmetric attention mechanism iteratively
    converting the long input sequence ***x***[1], …, ***x***[*T*] (e.g. the 50k pixels
    of an image) into a shorter sequence of latent units ***u***[1], …, ***u***[*n*]
    (e.g. *n* = 512) that form a bottleneck through which the inputs must pass (Fig.
    [3.10](#Fig10)). With cross-attention (Sect. [2.​3.​1](528393_1_En_2_Chapter.xhtml#Sec21))
    the *Q*-transformed latent sequence embeddings *Q****u***[*i*] and the *K*-transformed
    long input sequence embeddings *K****x***[*j*] form a scalar product ![$$(Q\boldsymbol
    {u}_i)^\intercal (K{\boldsymbol {x}}_j)$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq21.png).
    It is used as a weight for the *V* -transformed long sequence embedding *V****x***[*j*]
    to generate the new short embeddings. The Perceiver is basically a BERT model
    with a sequence length of *n* instead of *T*, which avoids that the computing
    effort scales quadratically with the input length. The iterative approach enables
    the model to devote its limited capacity to the most relevant inputs. In experiments
    the Perceiver was able to beat the leading ResNet-50 CNN with respect to image
    classification [[93](#CR93)]. *Perceiver IO* [[92](#CR92)] projects the resulting
    *n* output embeddings of a Perceiver to a larger sequence of output embeddings
    by another cross-attention operation, which, for instance, gets the position embeddings
    of output elements as query vectors. The *Perceiver AR* [[73](#CR73)] extends
    the Perceiver to generate an output sequentially similar to the encoder-decoder
    transformer.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig10_HTML.png)
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**感知器** [[93](#CR93)] 定义了一种非对称的注意力机制，通过迭代将长输入序列 ***x***[1]，…，***x***[*T*]（例如，图像的50k像素）转换为更短的潜在单元序列
    ***u***[1]，…，***u***[*n*]（例如，*n* = 512），这些单元形成一个瓶颈，输入必须通过这个瓶颈（图[3.10](#Fig10)）。通过交叉注意力（第[2.3.1](528393_1_En_2_Chapter.xhtml#Sec21)节），Q变换后的潜在序列嵌入
    *Q***u***[*i*] 和K变换后的长输入序列嵌入 *K***x***[*j*] 形成一个标量积 ![$$(Q\boldsymbol {u}_i)^\intercal
    (K{\boldsymbol {x}}_j)$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq21.png)。它被用作V变换后的长序列嵌入
    *V***x***[*j*] 的权重，以生成新的短嵌入。感知器基本上是一个BERT模型，其序列长度为 *n* 而不是 *T*，这避免了计算工作量与输入长度成二次方关系。迭代方法使模型能够将其有限的容量投入到最相关的输入中。在实验中，感知器能够在图像分类方面击败领先的ResNet-50
    CNN [[93](#CR93)]。*感知器IO* [[92](#CR92)] 通过另一个交叉注意力操作将感知器的结果 *n* 个输出嵌入投影到一个更大的输出嵌入序列中，例如，将输出元素的定位嵌入作为查询向量。*感知器AR*
    [[73](#CR73)] 将感知器扩展到生成与编码器-解码器转换器类似的输出序列。![图3.10](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig10_HTML.png)'
- en: A diagram represents the sequence of actions through the latent sequence embeddings,
    cross-attention, and latent transformer in an interchangeable manner. It indicates
    the layers of embeddings, logistic classifier, and class probabilities.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图表示了通过潜在序列嵌入、交叉注意力和潜在转换器进行交互式操作的行动序列。它指示了嵌入层、逻辑分类器和类概率层。
- en: Fig. 3.10
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10
- en: If the input sequence is too long, a short latent sequence is defined by the
    Perceiver. By cross-attention between the long sequence and the latent sequence
    the information is compressed. A standard transformer block computes the self-attentions
    between the latent sequence elements, which in the end generates a classification
    [[93](#CR93)] **S4** [[68](#CR68)] is a Structured State Space Sequence model
    based on the Kalman filter for the observation of a state model with errors [[101](#CR101)].
    A continuous state space model is defined by![$$\displaystyle \begin{aligned}
    {\boldsymbol{x}}'(t) = \boldsymbol{A}{\boldsymbol{x}}(t) + \boldsymbol{B} \boldsymbol{u}(t)
    \qquad  {\boldsymbol{y}}(t) = \boldsymbol{C} {\boldsymbol{x}}_t + \boldsymbol{D}\boldsymbol{u}(t),
    \end{aligned} $$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_Equ1.png)(3.1)
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入序列太长，感知器定义了一个短的潜在序列。通过长序列和潜在序列之间的交叉注意力，信息被压缩。一个标准的转换器块计算潜在序列元素之间的自注意力，最终生成一个分类
    [[93](#CR93)] **S4** [[68](#CR68)] 是一个基于卡尔曼滤波的有序状态空间序列模型，用于观察具有误差的状态模型 [[101](#CR101)]。一个连续状态空间模型由以下公式定义！[$$\displaystyle
    \begin{aligned} {\boldsymbol{x}}'(t) = \boldsymbol{A}{\boldsymbol{x}}(t) + \boldsymbol{B}
    \boldsymbol{u}(t) \qquad  {\boldsymbol{y}}(t) = \boldsymbol{C} {\boldsymbol{x}}_t
    + \boldsymbol{D}\boldsymbol{u}(t), \end{aligned} $$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_Equ1.png)(3.1)
- en: which maps an input signal ***u***(*t*) to output ***y***(*t*) through a latent
    state ***x***(*t*). The authors reparametrize the matrices ***A*** and decompose
    them as the sum of a low-rank and skew-symmetric term. Moreover, they compute
    its generating function of the associated infinite sequence truncated to some
    length *L* in frequency space. The low-rank term can be corrected by the Woodbury
    identity for matrix inversion. The skew-symmetric term can be diagonalized and
    can be reduced to a Cauchy kernel [[153](#CR153)].
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型将输入信号 ***u***(*t*) 通过一个潜在状态 ***x***(*t*) 映射到输出 ***y***(*t*)。作者重新参数化矩阵 ***A***
    并将它们分解为低秩和斜对称项之和。此外，他们在频率空间中计算了与之相关的无限序列的生成函数，该序列截断到某个长度 *L*。低秩项可以通过 Woodbury
    标识符来校正。斜对称项可以对角化，并可以减少到柯西核 [[153](#CR153)]。
- en: The ***A*** matrix is initialized with an special upper-triangular “HIPPO” matrix
    that allows the state ***x***(*t*) to memorize the history of the input ***u***(*t*).
    The authors prove that in complex space ![$$\mathbb {C}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq22.png)
    the corresponding state-space model can be expressed by matrices ( **Λ** −***PQ***^∗,
    ***B***, ***C***) for some diagonal matrix **Λ** and vectors ![$$\boldsymbol {P},\boldsymbol
    {Q},\boldsymbol {B},\boldsymbol {C}\in \mathbb {C}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq23.png).
    These are the 5*N* trainable parameters of S4, where *N* is the state dimension.
    Overall, S4 defines a sequence-to-sequence map of shape (batch size, sequence
    length, hidden dimension), in the same way as related sequence models such as
    Transformers, RNNs, and CNNs. For sequence length *L* this requires a computing
    effort of ^∼*O*(*N* + *L*) and *O*(*N* + *L*) memory space, which is close to
    the lowest value for sequence models. Gu et al. [[69](#CR69)] provide a detailed
    exposition and implementation of the S4 model.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**A** 矩阵使用一个特殊的上三角“HIPPO”矩阵初始化，允许状态 ***x***(*t*) 记忆输入 ***u***(*t*) 的历史。作者证明在复数空间
    ![$$\mathbb {C}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq22.png)
    中，相应的状态空间模型可以用矩阵 ( **Λ** −***PQ***^∗, ***B***, ***C***) 表示，其中 **Λ** 是一个对角矩阵，而
    ![$$\boldsymbol {P},\boldsymbol {Q},\boldsymbol {B},\boldsymbol {C}\in \mathbb
    {C}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq23.png) 是向量。这些是
    S4 的 5*N* 个可训练参数，其中 *N* 是状态维度。总的来说，S4 定义了一个形状为 (batch size, sequence length, hidden
    dimension) 的序列到序列映射，这与相关的序列模型（如 Transformers、RNNs 和 CNNs）类似。对于序列长度 *L*，这需要 ^∼*O*(*N* + *L*)
    的计算努力和 *O*(*N* + *L*) 的内存空间，这接近序列模型的最低值。Gu 等人 [[69](#CR69)] 提供了 S4 模型的详细阐述和实现。'
- en: In empirical evaluations it turned out that S4 for an input length of 1024 is
    1.6 times faster than the standard transformer and requires only 43% of its memory.
    For an input length of 4096, S4 is 5 times faster and requires just 9% of the
    memory of the standard transformer. For the benchmarks of the *Long Range Arena
    benchmark* S4 increased Sota average accuracy from 59.4% to 80.5% (Table [3.7](#Tab7)).
    Moreover, S4 was able to solve the extremely challenging Path-X task that involves
    reasoning over sequences of length 16k where all previous models have failed.
    Finally, S4 was able to perform raw speech signal classification on sequences
    of length 16k and achieves a new Sota of 98.3% accuracy. S4 involves a genuine
    breakthrough in long range sequence processing. In addition, S4 is better in long-range
    *time-series forecasting*, e.g. reducing Mean Square Error by 37% when forecasting
    30 days of weather data. *DSS* [[70](#CR70)] is a variant of S4 that is simpler
    to formulate and achieves a slightly lower performance.Table 3.7
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在经验评估中，对于长度为 1024 的输入，S4 比标准 Transformer 快 1.6 倍，并且只需要其 43% 的内存。对于长度为 4096 的输入，S4
    快 5 倍，并且只需要标准 Transformer 9% 的内存。对于 *Long Range Arena benchmark* 的基准测试，S4 将 Sota
    平均准确率从 59.4% 提高到 80.5%（见表 [3.7](#Tab7)）。此外，S4 能够解决涉及长度为 16k 的序列的极具挑战性的 Path-X
    任务，而所有之前的模型都失败了。最后，S4 能够对长度为 16k 的序列进行原始语音信号分类，并实现了 98.3% 的新 Sota 准确率。S4 在长距离序列处理方面取得了真正的突破。此外，S4
    在长距离 *时间序列预测* 方面表现更好，例如在预测 30 天的天气数据时，将均方误差降低了 37%。*DSS* [[70](#CR70)] 是 S4 的一个变体，它更容易表述，并且实现了略低的表现。表
    3.7
- en: Accuracy results for the Long-Range Arena Benchmark. The best score is printed
    in bold, results improving the standard transformer are underlined (cf. [[209](#CR209)])
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Long-Range Arena Benchmark 的准确率结果。最佳得分以粗体打印，改进标准 Transformer 的结果以下划线表示（参见图 [[209](#CR209)]）。
- en: '| Model | ListOps | Text classif. | Retrieval | Image classif. | Pathfinder
    | Path-X | Average |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 列操作 | 文本分类 | 检索 | 图像分类 | 寻路器 | Path-X | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Transformer | 36.3 | 64.3 | 57.5 | 42.4 | 71.4 | × | 54.4 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | 36.3 | 64.3 | 57.5 | 42.4 | 71.4 | × | 54.4 |'
- en: '| Reformer | 37.3 | 56.1 | 53.4 | 38.1 | 68.5 | × | 50.7 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Reformer | 37.3 | 56.1 | 53.4 | 38.1 | 68.5 | × | 50.7 |'
- en: '| Synthesizer | 37.0 | 61.9 | 54.7 | 41.6 | 69.5 | × | 52.9 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| Synthesizer | 37.0 | 61.9 | 54.7 | 41.6 | 69.5 | × | 52.9 |'
- en: '| BigBird | 36.0 | 64.0 | 59.3 | 40.8 | 74.9 | × | 55.0 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| BigBird | 36.0 | 64.0 | 59.3 | 40.8 | 74.9 | × | 55.0 |'
- en: '| Linear transf. | 16.1 | 65.9 | 53.1 | 42.3 | 75.3 | × | 50.6 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| Linear transf. | 16.1 | 65.9 | 53.1 | 42.3 | 75.3 | × | 50.6 |'
- en: '| Performer | 18.0 | 65.4 | 53.8 | 42.8 | 77.0 | × | 51.4 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| Performer | 18.0 | 65.4 | 53.8 | 42.8 | 77.0 | × | 51.4 |'
- en: '| S4 | **58.4** | **76.0** | **87.1** | **87.3** | **86.1** | **88.1** | **80.5**
    |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| S4 | **58.4** | **76.0** | **87.1** | **87.3** | **86.1** | **88.1** | **80.5**
    |'
- en: 3.2.3 Comparisons of Transformers with Long Input Sequences
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 长输入序列的Transformer比较
- en: The *Long Range Arena* [[209](#CR209)] aims to evaluate the performance on tasks
    with long input sequences from 1k to 16k tokens. It contains six different benchmark
    datasets covering text, images, mathematical expressions, and visual spatial reasoning.
    The tasks include ListOps (computations in a list-notation), text classification
    (classify IMDB reviews using character sequences), document retrieval (based on
    document embeddings), image classification (based on a sequence of pixels), and
    pathfinder (detection of circles) in two versions. The authors evaluate nine transformer
    architectures with the ability to process long inputs.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '*Long Range Arena* [[209](#CR209)]旨在评估从1k到16k标记的长输入序列任务上的性能。它包含六个不同的基准数据集，涵盖文本、图像、数学表达式和视觉空间推理。任务包括ListOps（列表符号中的计算）、文本分类（使用字符序列对IMDB评论进行分类）、文档检索（基于文档嵌入）、图像分类（基于像素序列）和路径查找（检测圆圈）的两个版本。作者评估了九种能够处理长输入的transformer架构。'
- en: The results are shown in Table [3.7](#Tab7). For the hierarchically structured
    data of ListOps, it turns out that kernel-based approaches, for instance the Performer
    and the Linear Transformer, are not appropriate. For text classification, kernel-based
    methods perform particularly well. For image classification most models do well,
    except for the Reformer. The pathfinder task is solved by all models with an acceptable
    performance, with the Performer doing best. However, all models except S4 fail
    on the extended Pathfinder task and are not able to find a solution. In terms
    of all benchmarks, S4 is the best model by a wide margin.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在表[3.7](#Tab7)中。对于层次结构化的ListOps数据，结果表明基于核的方法，例如Performer和Linear Transformer，是不合适的。对于文本分类，基于核的方法表现特别出色。对于图像分类，大多数模型表现良好，除了Reformer。路径查找任务由所有模型以可接受的性能解决，其中Performer表现最佳。然而，除了S4之外的所有模型在扩展的路径查找任务上失败，并且无法找到解决方案。在所有基准测试中，S4是最佳模型，差距很大。
- en: With respect to speed, the Performer was best, being 5.7 times faster than the
    standard transformer on sequences of length 4k. Memory consumption ranged from
    9.5 GB for the standard transformer to about 1.1 GB for the Linear Transformer.
    All other models except the Synthesizer require less than 3 GB with S4 doing well
    in both aspects.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在速度方面，Performer表现最佳，在长度为4k的序列上比标准Transformer快5.7倍。内存消耗从标准Transformer的9.5GB到Linear
    Transformer的大约1.1GB不等。除了Synthesizer之外的所有模型都小于3GB，其中S4在这两方面都表现良好。
- en: 3.2.4 Summary
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.4 总结
- en: There are a variety of proposals for PLMs to efficiently process long input
    sequences. Often a sparse attention matrix is employed, where only a part of the
    possible attentions is used to establish the connection between far-away positions.
    Usually, full attention is computed for near positions. Some tokens have a global
    attention to communicate information between positions not connected directly.
    A prominent example is BigBird, which adds random attentions. Its computational
    effort only grows linearly with input size and it still can perform arbitrary
    sequence computations. There are other architectures like the Performer and the
    Linear Transformer, which also exhibit linear growth.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PLMs高效处理长输入序列，有许多不同的建议。通常采用稀疏注意力矩阵，其中只使用可能注意力的一部分来建立远距离位置之间的连接。通常，对于近距离位置计算全注意力。一些标记具有全局注意力，用于在未直接连接的位置之间传递信息。一个突出的例子是BigBird，它添加了随机注意力。其计算工作量仅与输入大小呈线性增长，并且仍然可以执行任意序列计算。还有其他架构，如Performer和Linear
    Transformer，它们也表现出线性增长。
- en: Some architectures either approximate the attention matrices by low-rank factorizations
    or aggregate tokens, which express similar content (Reformer, Combiner). Another
    approach is to use a recurrence mechanism such that computations are reduced for
    far-away tokens (Transformer-XL, Linear Transformer, Transformer-LS, Perceiver).
    An alternative is the factorization of the self-attention matrix (Performer) or
    its replacement with simpler computations (Synthesizer). Recently, the S4 model
    has been proposed that applies a state-space model to long-range prediction. It
    uses an architecture based on complex number computations, which is completely
    different from the usual transformer setup. It outperforms all prior models by
    a large margin and is efficient in terms of computation time and memory.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 一些架构通过低秩分解来近似注意力矩阵，或者聚合表达类似内容的标记（Reformer，Combiner）。另一种方法是使用递归机制，从而减少远离标记的计算（Transformer-XL，线性Transformer，Transformer-LS，Perceiver）。另一种选择是分解自注意力矩阵（Performer）或用更简单的计算来替换它（Synthesizer）。最近，提出了S4模型，该模型将状态空间模型应用于长距离预测。它使用基于复数计算的架构，这与通常的Transformer设置完全不同。它在计算时间和内存效率方面都优于所有先前模型。
- en: The performance of these approaches was evaluated with six different benchmarks
    of the Long Range Arena. It turned out that S4 beats the other models with respect
    to all benchmarks. All approaches were able to reduce memory consumption compared
    to the standard transformer. The larger input length allow new applications, e.g.
    in raw speech processing, image processing or genomics [[247](#CR247)].
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的性能使用Long Range Arena的六个不同基准进行了评估。结果表明，S4在所有基准上击败了其他模型。所有方法都能与标准Transformer相比减少内存消耗。更长的输入长度允许新的应用，例如在原始语音处理、图像处理或基因组学[[247](#CR247)]。
- en: 3.3 Multilingual Pre-trained Language Models
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 多语言预训练语言模型
- en: There are more than 7100 languages in the world [[9](#CR9)], and each language
    can express almost all facts and concepts. Therefore, PLMs should also be able
    to generate consistent representations for concepts in different languages. Languages
    differ to some extent in the basic word order of verbs, subjects, and objects
    in simple declarative sentences. English, German, French, and Mandarin, for example,
    are SVO languages (subject-verb-object) [[100](#CR100)]. Here, the verb is usually
    placed between the subject and the object. Hindi and Japanese, on the other hand,
    are SOV languages, meaning that the verb is placed at the end of the main clause.
    Irish and Arabic, on the other hand, are VSO languages. Two languages that have
    the same basic word order often have other similarities. For example, VO languages
    generally have prepositions, while OV languages generally have postpositions.
    Also, there may be a lexical gap in one language, where no word or phrase can
    express the exact meaning of a word in the other language. An example is the word
    *“Schadenfreude”* in German, which roughly translates to *“have joy because some
    other person has bad luck”*. More such differences are discussed by Jurafsky and
    Martin [[100](#CR100)].
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 世界上有超过7100种语言[[9](#CR9)]，每种语言几乎都能表达几乎所有的事实和概念。因此，预训练语言模型（PLM）也应该能够为不同语言中的概念生成一致的表示。在简单的陈述句中，不同语言在动词、主语和宾语的基本语序上存在一定程度的不同。例如，英语、德语、法语和普通话是主谓宾（SVO）语言[[100](#CR100)]。在这里，动词通常位于主语和宾语之间。另一方面，印地语和日语是主宾谓（SOV）语言，这意味着动词位于主句的末尾。另一方面，爱尔兰语和阿拉伯语是谓语主语宾语（VSO）语言。具有相同基本语序的两种语言通常还有其他相似之处。例如，VO语言通常有介词，而OV语言通常有后置词。此外，一种语言可能存在词汇空缺，即没有单词或短语可以表达另一种语言中某个单词的确切含义。例如，德语中的单词*“Schadenfreude”*大致可以翻译为*“因他人不幸而感到高兴”*。Jurafsky和Martin讨论了更多这样的差异[[100](#CR100)]。
- en: To gain cross-lingual language understanding, a PLM has to be trained with more
    than one language and has to capture their structural differences. During training,
    PLMs can establish an alignment between concepts in different languages.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得跨语言的语言理解，PLM必须用多种语言进行训练，并捕捉它们的结构差异。在训练过程中，PLMs可以在不同语言的概念之间建立对齐。
- en: Training large PLMs models, e.g. T5 or BERT, on multilingual data with a joint
    token vocabulary leads to models that transfer information between languages by
    exploiting their common structure.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在具有联合标记词汇的多语言数据上训练大型PLM模型，例如T5或BERT，会导致模型通过利用它们的共同结构在语言之间传递信息。
- en: BERT-like models can be trained to associate the words of a sentence in one
    language with the words of its translation to another language by masked language
    modeling. However, it has been shown that multilingual processing is possible,
    even when little or no parallel training data is available.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似于BERT的模型可以通过掩码语言建模来训练，以将一种语言的句子中的单词与另一种语言的翻译单词关联起来。然而，已经证明，即使在很少或没有平行训练数据的情况下，也可以进行多语言处理。
- en: Transformer encoder-decoder models are explicitly trained to translate a text
    from one language to another language.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer编码器-解码器模型被明确训练以将一种语言的文本翻译成另一种语言。
- en: Training a language model with several languages in parallel can improve the
    performance—especially for languages with little training data. This could already
    be demonstrated for static word embeddings [[194](#CR194)].
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 并行训练多种语言的语言模型可以提高性能——特别是对于训练数据较少的语言。这已经在静态词嵌入 [[194](#CR194)] 中得到了证明。
- en: 3.3.1 Autoencoder Models
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 自动编码器模型
- en: '**mBERT** (multilingual BERT) [[48](#CR48)] is a standard BERT model. It has
    been pre-trained with the MLM loss on non-parallel Wikipedia texts from 104 languages
    and has a shared token vocabulary of 110k WordPiece tokens for all languages.
    This implies that Chinese is effectively character-tokenized. Each training sample
    is a document in one language, and there are no cross-lingual dictionaries or
    training criteria. To demonstrate its properties the model was fine-tuned to a
    multilingual version *XNLI* [[40](#CR40)] of the Natural Language Inference (NLI)
    benchmark, i.e. the task to predict, whether the first sentence entails the second.
    It turns out that mBERT may be fine-tuned with a single language on NLI and still
    yields good test results on related languages [[40](#CR40), [232](#CR232)].'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**mBERT** (多语言BERT) [[48](#CR48)] 是一个标准的BERT模型。它已经在104种语言的非平行维基百科文本上使用MLM损失进行预训练，并为所有语言共享了110k个WordPiece标记的词汇表。这意味着中文实际上是字符标记化的。每个训练样本都是一种语言的文档，没有跨语言词典或训练标准。为了展示其特性，该模型被微调到自然语言推理（NLI）基准的多语言版本
    *XNLI* [[40](#CR40)]，即预测第一个句子是否蕴涵第二个句子的任务。结果发现，mBERT可以在NLI上使用单一语言进行微调，并在相关语言上仍然获得良好的测试结果
    [[40](#CR40), [232](#CR232)]。'
- en: The results for 6 languages [[111](#CR111)] are shown in Table [3.8](#Tab8).
    Compared to fine-tuning XNLI with all languages, there is only a small drop in
    accuracy for related languages, e.g. Spanish and German, if the fine-tuning is
    done with XNLI in English and the evaluation in the other language. For the other
    languages the reduction of performance is larger, but the results are still good.
    There is even a transfer of information between languages with different scripts,
    e.g. for Arabic and Urdu. The authors also consider the embeddings of a word and
    its translation. It turns out that the cosine similarity between a word and its
    translation is 0.55, although there is no alignment between languages.Table 3.8
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 6种语言的测试结果 [[111](#CR111)] 如表 [3.8](#Tab8) 所示。与使用所有语言的XNLI进行微调相比，如果使用英语进行XNLI微调并在其他语言中进行评估，则相关语言（例如西班牙语和德语）的准确率仅略有下降。对于其他语言，性能的下降更大，但结果仍然不错。甚至在不同文字的语言之间也有信息传递，例如阿拉伯语和乌尔都语。作者还考虑了单词及其翻译的嵌入。结果显示，单词与其翻译之间的余弦相似度为0.55，尽管语言之间没有对齐。表
    3.8
- en: Cross-lingual natural language inference (XNLI) [[40](#CR40)] test accuracy
    for 6 languages. Fine-tuning with XNLI for all languages is compared to fine-tuning
    with XNLI only for English. Results for mBERT [[48](#CR48)] and XLM [[111](#CR111)]
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 6种语言的跨语言自然语言推理（XNLI）[[40](#CR40)] 测试准确率。与所有语言的XNLI微调相比，仅用英语进行XNLI微调的结果。
- en: '| Fine-tune with … | Model | English | Chinese | Spanish | German | Arabic
    | Urdu |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 微调方式 | 模型 | 英语 | 中文 | 西班牙语 | 德语 | 阿拉伯语 | 乌尔都语 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| All languages | mBERT | 81.9 | 76.6 | 77.8 | 75.9 | 70.7 | 61.6 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 所有语言 | mBERT | 81.9 | 76.6 | 77.8 | 75.9 | 70.7 | 61.6 |'
- en: '| English only | mBERT | 81.4 | 63.8 | 74.3 | 70.5 | 62.1 | 58.3 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 仅英语 | mBERT | 81.4 | 63.8 | 74.3 | 70.5 | 62.1 | 58.3 |'
- en: '| All languages | XLM | 85.0 | 78.6 | 80.8 | 80.3 | 76.5 | 63.2 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 所有语言 | XLM | 85.0 | 78.6 | 80.8 | 80.3 | 76.5 | 63.2 |'
- en: '| English only | XLM | 85.0 | 76.5 | 78.9 | 77.8 | 73.1 | 57.3 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 仅英语 | XLM | 85.0 | 76.5 | 78.9 | 77.8 | 73.1 | 57.3 |'
- en: Karthikeyan et al. [[104](#CR104)] investigate the factors for the success of
    mBERT. They find that mBERT has cross-lingual capabilities even if there is absolutely
    no overlap in the token vocabulary. Moreover, a higher number of identical tokens
    in both vocabularies contributes little to the performance improvements. Comparing
    different language pairs the authors show that a large network depth and a high
    total number of parameters of a bilingual BERT are crucial for both monolingual
    and cross-lingual performance, whereas the number of attention heads is not a
    significant factor. On the other hand, the structural similarity of the source
    and target language, i.e. word order and frequency of words, has a large influence
    on cross-lingual performance.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: Karthikeyan等人[[104](#CR104)]研究了mBERT成功的关键因素。他们发现，即使标记词汇完全没有重叠，mBERT也具有跨语言能力。此外，两个词汇库中相同标记的数量对性能提升的贡献很小。作者通过比较不同的语言对，表明大型网络深度和双语BERT的总参数数量对于单语和跨语言性能都至关重要，而注意力头数不是一个显著因素。另一方面，源语言和目标语言的相似性，即词序和词频，对跨语言性能有重大影响。
- en: '**XLM** [[111](#CR111)] improves the transfer of knowledge between different
    languages by using translated sentences from different language pairs during pre-training.
    The authors concatenate a sentence with its translations to another language for
    training and introduce a new *translation language modeling* (*TLM*) objective
    for improving cross-lingual pre-training. To predict masked words in the input
    sentence, the algorithm can attend to the words in the translated sentence. In
    this way, the model learns to correlate words from different languages. An example
    is shown in Fig. [3.11](#Fig11). As shown in Table [3.8](#Tab8), XLM has a much
    higher cross-lingual accuracy for XNLI compared to mBERT. The transfer from a
    model fine-tuned in English to other languages incurs only a small loss. The experiments
    show that TLM is able to increase the XNLI accuracy for 3.6% on average. The model
    was also evaluated for unsupervised machine translation from German and other
    languages to English, yielding a very good performance (cf. Sect. [6.​3](528393_1_En_6_Chapter.xhtml#Sec19)).![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig11_HTML.png)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**XLM** [[111](#CR111)]通过在预训练期间使用来自不同语言对的翻译句子来提高不同语言之间知识转移的能力。作者将一个句子及其翻译到另一种语言的句子连接起来进行训练，并引入一个新的*翻译语言模型*
    (*TLM*) 目标，以改善跨语言预训练。为了预测输入句子中的掩码词，算法可以关注翻译句子中的单词。这样，模型学会关联不同语言中的单词。一个例子如图[3.11](#Fig11)所示。如表[3.8](#Tab8)所示，XLM在XNLI上的跨语言准确率比mBERT高得多。从在英语上微调的模型转移到其他语言只会造成很小的损失。实验表明，TLM能够将XNLI的准确率平均提高3.6%。该模型还用于从德语和其他语言到英语的无监督机器翻译评估，取得了非常好的性能（参见第[6.3](528393_1_En_6_Chapter.xhtml#Sec19)节）。![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig11_HTML.png)'
- en: A model diagram represents the approach of translation language modeling. It
    indicates the layers of language, position, and token embeddings go through the
    transformer encoder, followed by output embeddings, logistic classifiers, token
    probabilities, and masked tokens.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 模型图表示了翻译语言模型的方法。它表明语言层、位置和标记嵌入经过Transformer编码器，随后是输出嵌入、逻辑分类器、标记概率和掩码标记。
- en: Fig. 3.11
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11
- en: The translation language modeling (TLM) task is applied to pairs of translated
    sentences. To predict a masked English word, the model can attend to both the
    English sentence and its French translation, and is thus encouraged to align English
    and French representations [[111](#CR111)]
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译语言模型（TLM）任务应用于翻译句子的对。为了预测一个掩码的英语单词，模型可以关注英语句子及其法语翻译，因此鼓励英语和法语表示的对齐[[111](#CR111)]
- en: '**Unicoder** [[88](#CR88)] is an improved XLM model with three additional training
    tasks. Cross-lingual word alignment learns to associate the corresponding words
    in translated sentences. Cross-lingual paraphrase detection takes two sentences
    from different languages as input and classifies whether they have the same meaning.
    The document-level cross-lingual masked language model applies the MLM task to
    documents where part of the sentences are replaced by their translations. On XNLI
    the authors report an average accuracy improvement of 1.8%.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '**Unicoder** [[88](#CR88)] 是一个改进的 XLM 模型，具有三个额外的训练任务。跨语言词对齐学习将翻译句子中的对应词语关联起来。跨语言释义检测将两种不同语言的句子作为输入，并分类它们是否具有相同的意义。文档级跨语言掩码语言模型将
    MLM 任务应用于部分句子被其翻译替换的文档。在 XNLI 上，作者报告了平均准确率提高了 1.8%。'
- en: '**XLM-R** is an optimized version of XLM [[41](#CR41)]. It is based on RoBERTa
    and trained on a huge multilingual CommonCrawl dataset of 2.5TB covering 100 languages
    with a common vocabulary of 250k tokens. It increased the Sota on the XNLI-score
    to 79.2%. For cross-lingual question answering, models are fine-tuned on the English
    SQuAD dataset and evaluated on 7 other languages. XLM-R improves the F1 score
    on this SQuAD version by 9.1%–70.7%. It outperforms mBERT on cross-lingual classification
    by up to 23% accuracy on low-resource languages. The performance of XLM-R is nearly
    as good as that of strong monolingual models.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '**XLM-R** 是 XLM 的优化版本 [[41](#CR41)]。它基于 RoBERTa，在包含 100 种语言、25k 个共同词汇的 2.5TB
    多语言 CommonCrawl 数据集上进行训练。它将 XNLI 上的 Sota 提高到了 79.2%。对于跨语言问答，模型在英语 SQuAD 数据集上进行微调，并在
    7 种其他语言上进行评估。XLM-R 在这个 SQuAD 版本上的 F1 分数提高了 9.1%–70.7%。在低资源语言上，XLM-R 在跨语言分类上的准确率比
    mBERT 高出高达 23%。XLM-R 的性能几乎与强大的单语模型相当。'
- en: These results support the observation that the performance of PLMs can be improved
    by training on large volumes of text [[102](#CR102)]. More languages lead to better
    cross-lingual performance on low-resource languages under the condition that the
    model capacity is large enough. Combined with the approach of Aghajanyan et al.
    [[2](#CR2)], which avoids too large changes in representation during fine-tuning
    (Sect. [3.6](#Sec31)), the XLM-R[LARGE] model increases the Sota in XNLI to 81.4%.
    If an additional criterion of separating semantically-equivalent sentences in
    different languages from other sentences is added to XLM-R, the accuracy on semantic
    tasks is increased [[228](#CR228)]. Even larger models like *XLM-R*[XXL] [[66](#CR66)]
    with 10.7B parameters were pre-trained on CC-100, which consists of 167B tokens
    of non-parallel text also covering low-resource languages, and increased the XNLI
    performance by 2.4%.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果支持了观察结果，即通过在大规模文本上进行训练，PLM 的性能可以得到提升 [[102](#CR102)]。更多语言在模型容量足够大的条件下，可以带来在低资源语言上的更好的跨语言性能。结合
    Aghajanyan 等人的方法 [[2](#CR2)]，该方法在微调过程中避免表示发生过大变化（见第 [3.6](#Sec31) 节），XLM-R[LARGE]
    模型将 XNLI 上的 Sota 提高到了 81.4%。如果向 XLM-R 添加一个额外的标准，即从不同语言中分离出语义等效句子与其他句子，那么在语义任务上的准确率会提高
    [[228](#CR228)]。更大的模型如 *XLM-R*[XXL] [[66](#CR66)]，拥有 10.7B 个参数，在包含 167B 个非平行文本的
    CC-100 数据集上进行了预训练，该数据集也涵盖了低资源语言，并将 XNLI 性能提高了 2.4%。
- en: '**RemBERT** [[37](#CR37)] redistributes the parameters of multilingual models.
    First the authors showed that using different input and output embeddings in state-of-the-art
    pre-trained language models improved model performance. Then they demonstrated
    that assigning more parameters to the output embeddings increased model accuracy,
    which was maintained during fine-tuning. As a consequence Transformer representations
    were more general and more transferable to other tasks and languages. The *Xtreme*
    collection [[86](#CR86)] is a multitask benchmark for evaluating the cross-lingual
    generalization capabilities of multilingual representations across 40 languages
    and 9 tasks. RemBERT outperformed XLM-R on Xtreme, despite being trained only
    on a smaller subset of training data and ten additional languages.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '**RemBERT** [[37](#CR37)] 重新分配了多语言模型的参数。首先，作者展示了使用最先进的预训练语言模型中的不同输入和输出嵌入可以提高模型性能。然后，他们证明了将更多参数分配给输出嵌入可以提高模型准确率，这一提高在微调过程中得到保持。因此，Transformer
    表示更加通用，并且更易于迁移到其他任务和语言。*Xtreme* 集合 [[86](#CR86)] 是一个多任务基准，用于评估 40 种语言和 9 个任务上多语言表示的跨语言泛化能力。尽管
    RemBERT 只在较小的训练数据子集和额外 10 种语言上进行了训练，但它还是在 Xtreme 上优于 XLM-R。'
- en: PLMs like BERT generate contextual token embeddings. However, the user often
    needs contextual *embeddings for passage* or sentences to compare their content.
    **LaBSE** [[57](#CR57)] is a language-agnostic generator of passage embeddings,
    where source and target sentences are encoded separately using a shared BERT-based
    encoder. The representations of *[CLS]* in the final layer were taken as the *sentence
    embeddings* for each input. LaBSE combined a masked language model (MLM) and a
    translation language model (TLM) loss with a margin criterion. This criterion
    computes the cosine distance ![$$\cos {}(x,y)$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq24.png)
    between the passage embeddings ***x*** and the embedding ***y*** of its correct
    translation. Then it is required that *cos*(***x***, ***y***) − *m* is larger
    than ![$$\cos {}({\boldsymbol {x}},{\boldsymbol {y}}_i)$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq25.png),
    where *m* is a positive margin and the ***y***[*i*] are embeddings of arbitrary
    other passages. LaBSE was trained using 17B monolingual sentences and 6B bilingual
    translated sentences. The resulting sentence embeddings markedly improve the retrieval
    accuracy Sota of sentences in cross-lingual information retrieval (cf. Sect. [6.​1](528393_1_En_6_Chapter.xhtml#Sec1)).
    The code and pre-trained models are available.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 BERT 的 PLM 生成上下文标记嵌入。然而，用户通常需要上下文**段落**或句子的嵌入来比较其内容。**LaBSE** [[57](#CR57)]
    是一个无语言限制的段落嵌入生成器，其中源句和目标句分别使用共享的基于 BERT 的编码器进行编码。最终层的 *CLS* 表示被用作每个输入的 *句子嵌入*。LaBSE
    结合了掩码语言模型 (MLM) 和翻译语言模型 (TLM) 损失以及一个边缘标准。该标准计算段落嵌入 ***x*** 和其正确翻译的嵌入 ***y*** 之间的余弦距离
    ![$$\cos {}(x,y)$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq24.png)。然后要求
    *cos*(***x***, ***y***)− *m* 大于 ![$$\cos {}({\boldsymbol {x}},{\boldsymbol {y}}_i)$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq25.png)，其中
    *m* 是一个正的边缘，而 ***y***[*i*] 是任意其他段落的嵌入。LaBSE 使用 17B 单语句子和 6B 双语翻译句子进行训练。生成的句子嵌入显著提高了跨语言信息检索中句子的检索准确率
    Sota（参看第 [6.1](528393_1_En_6_Chapter.xhtml#Sec1) 节）。代码和预训练模型是可用的。
- en: 3.3.2 Seq2seq Transformer Models
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 Seq2seq Transformer 模型
- en: '**mT5** is a multilingual version of the T5 Seq2seq transformer (Sect. [3.1.3](#Sec4))
    with up to 13B parameters [[236](#CR236)]. It was pre-trained using a training
    dataset of web pages covering 101 languages with about 48B tokens and a common
    vocabulary of 250k tokens. For pre-training, the model had to predict masked phrases
    in monolingual documents in the same way as T5\. Similar to T5 the model may be
    instructed to perform different tasks by a prefix, e.g. “summarize”. These tasks
    were trained by fine-tuning on the corresponding datasets.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '**mT5** 是 T5 Seq2seq Transformer（第 [3.1.3](#Sec4) 节）的多语言版本，参数量高达 13B [[236](#CR236)]。它使用覆盖
    101 种语言的网页训练数据集进行预训练，包含约 48B 个标记和一个 25k 个标记的通用词汇表。对于预训练，模型必须以与 T5 相同的方式预测单语文档中的掩码短语。与
    T5 类似，模型可以通过前缀来执行不同的任务，例如“总结”。这些任务通过在相应的数据集上进行微调进行训练。'
- en: For the *XNLI benchmark* [[40](#CR40)] the model has to decide, if the first
    sentence entails the second sentence. When the model is fine-tuned on XNLI with
    English data and performance is measured for 15 languages, accuracy is 84.8% compared
    to 65.4% for mBERT, 69.1% for XLM, and 79.2% for XLM-R. Although the texts in
    the different languages are not parallel, the model is able to exploit structural
    similarities between languages to solve the task. The code of this model is available
    at [[235](#CR235)]. Similar models are used for multilingual translation (Sect.
    [6.​3](528393_1_En_6_Chapter.xhtml#Sec19)). **mT6** [[31](#CR31)] enhances the
    training of mT5 with pairs of translated sentences and defines new training tasks.
    Experimental results show that mT6 has improved cross-lingual capabilities compared
    to mT5\. A further improvement is **Switch** [[56](#CR56)] with a *mixture-of-experts*
    (*MoE*) architecture of mT5 requiring only one fifth of the training time of mT5
    while yielding a performance gain across all 101 languages (Sect. [3.5.2](#Sec26)).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *XNLI 基准测试* [[40](#CR40)]，模型需要判断第一句话是否蕴含第二句话。当模型在 XNLI 上使用英语数据进行微调，并对 15
    种语言进行性能测量时，准确率为 84.8%，相比之下，mBERT 的准确率为 65.4%，XLM 的准确率为 69.1%，XLM-R 的准确率为 79.2%。尽管不同语言中的文本不是平行的，但模型能够利用语言之间的结构相似性来解决这个任务。该模型的代码可在
    [[235](#CR235)] 处找到。类似的模型用于多语言翻译（第 [6.3](528393_1_En_6_Chapter.xhtml#Sec19) 节）。**mT6**
    [[31](#CR31)] 通过翻译句子对增强 mT5 的训练，并定义了新的训练任务。实验结果表明，与 mT5 相比，mT6 在跨语言能力方面有所提升。进一步的改进是
    **Switch** [[56](#CR56)]，它采用了 mT5 的 *专家混合* (*MoE*) 架构，只需 mT5 五分之一的训练时间，同时在所有 101
    种语言上实现了性能提升（第 [3.5.2](#Sec26) 节）。
- en: '**mBART** [[126](#CR126)] is a multilingual encoder-decoder based on the BART
    model (Sect. [3.1.3](#Sec4)). The input texts are corrupted by masking phrases
    and permuting sentences, and a single Transformer model is pre-trained to recover
    the corrupted text. This is performed for the training documents covering 25 languages.
    Subsequently, the pre-trained model is fine-tuned with a translation task between
    a single language pair. In addition, *back-translation* may be used, where another
    model is trained to translate the target sentence back to the source language
    and an additional loss encourages to reconstruct the source sentence. mBART adds
    a language symbol both to the end of the encoder input and the beginning of the
    decoder input. This enables models to know the languages to be encoded and generated.
    It turns out that pre-training improves translation, especially for languages
    with little parallel training data. In addition, back-translation markedly ameliorates
    the translation results. Many experiments are performed to analyze the effect
    of different algorithmic features. Pre-training is especially important if complete
    documents are translated instead of single sentences.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**mBART** [[126](#CR126)] 是基于 BART 模型的多语言编码器-解码器（第 [3.1.3](#Sec4) 节）。输入文本通过掩码短语和句子排列进行损坏，并使用单个
    Transformer 模型进行预训练以恢复损坏的文本。这适用于涵盖 25 种语言的训练文档。随后，预训练模型通过单一语言对的翻译任务进行微调。此外，还可以使用
    *回译*，其中另一个模型被训练将目标句子翻译回源语言，并引入额外的损失来鼓励重建源句子。mBART 在编码器输入的末尾和解码器输入的开头添加了一个语言符号。这使得模型能够知道要编码和生成的语言。结果表明，预训练提高了翻译质量，特别是对于平行训练数据较少的语言。此外，回译显著改善了翻译结果。进行了许多实验来分析不同算法特征的影响。如果翻译的是完整的文档而不是单个句子，预训练尤为重要。'
- en: mBART may also be used for *unsupervised machine translation*, where no parallel
    text of any kind is used. Here the authors initialize the model with pre-trained
    weights and then learn to predict the monolingual sentences from the source sentences
    generated by back-translation. The results for languages with similar structure
    are very good, e.g. for En-De mBART achieves a Bleu-value of 29.8, which is close
    to the supervised value of 30.9\. Note that mBART has a similar performance as
    MASS (Sect. [3.1.3](#Sec4)). For dissimilar pairs of languages, e.g. English-Nepali,
    mBART has reasonable results where other approaches fail.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: mBART 也可以用于 *无监督机器翻译*，在这种情况下，不使用任何类型的平行文本。在这里，作者使用预训练的权重初始化模型，然后学习从由回译生成的源句子中预测单语句子。对于结构相似的语言，结果非常好，例如，对于
    En-De，mBART 实现了 29.8 的 Bleu 值，接近监督学习的 30.9。请注意，mBART 与 MASS 的性能相似（第 [3.1.3](#Sec4)
    节）。对于结构不相似的语言对，例如英语-尼泊尔语，mBART 在其他方法失败的情况下仍能获得合理的结果。
- en: '**MARGE** [[118](#CR118)] is a multilingual Seq2seq model that is trained to
    reconstruct a document *x* in one language by retrieving documents *z*[1], …,
    *z*[*k*] in other languages. It was trained with texts in 26 languages from Wikipedia
    and CC-News. A document was encoded by the output embedding of the first token
    of a Transformer [[212](#CR212)]. A retrieval model scores the relevance *f*(*x*,
    *z*[*j*]) of the target document *x* to each evidence document *z*[*j*] by embedding
    each document and computing their cosine similarities. A transformer receives
    the embedded texts of *z*[1], …, *z*[*k*] and auxiliary relevance scores *f*(*x*,
    *z*[*j*]) from retrieval as input and is trained to generate the target document
    *x* as output. The similarity score is used to weight the cross-attention from
    the decoder to the encoder, so that the decoder will pay more attention to more
    relevant evidence documents. The models jointly learn to do retrieval and reconstruction,
    given only a random initialization. In a zero-shot setting the model can do document
    translation with Bleu scores of up to 35.8 in the *WMT2019 De-En benchmark*, as
    well as abstractive summarization, question answering and paraphrasing. Fine-tuning
    gives additional strong performance on a range of tasks in many languages, showing
    that MARGE is a generally applicable pre-training method.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '**MARGE** [[118](#CR118)] 是一个多语言Seq2seq模型，它被训练通过检索其他语言中的文档 *z*[1]，…，*z*[*k*]
    来重建一种语言中的文档 *x*。它使用来自维基百科和CC-News的26种语言的文本进行训练。文档通过Transformer的第一个标记的输出嵌入进行编码
    [[212](#CR212)]。检索模型通过嵌入每个文档并计算它们的余弦相似度来评分目标文档 *x* 对每个证据文档 *z*[*j*] 的相关性 *f*(*x*,
    *z*[*j*])。一个Transformer接收来自检索的嵌入文本 *z*[1]，…，*z*[*k*] 和辅助相关性评分 *f*(*x*, *z*[*j*])
    作为输入，并训练生成目标文档 *x* 作为输出。相似度评分用于加权解码器到编码器的交叉注意力，这样解码器将更多地关注更相关的证据文档。模型在仅给定随机初始化的情况下共同学习检索和重建。在零样本设置中，该模型在WMT2019
    De-En基准测试中实现了高达35.8的Bleu分数进行文档翻译，以及抽象摘要、问答和释义。微调在许多语言的多种任务上提供了额外的强大性能，表明MARGE是一种通用的预训练方法。'
- en: '**XLNG** [[32](#CR32)] pre-trains the same Seq2seq model simultaneously using
    an MLM and a translation TLM loss (Table [3.1](#Tab1)). The pre-training objective
    generates embeddings for different languages in a common space, enabling zero-shot
    cross-lingual transfer. In the fine-tuning stage monolingual data is used to train
    the pre-trained model on natural language generation tasks. In this way, the model
    trained in a single language can directly solve the corresponding task in other
    languages. The model outperforms methods based on machine translation for zero-shot
    cross-lingual question generation and abstractive summarization. In addition,
    this approach improves performance for languages with little training data by
    leveraging data from resource-rich languages.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '**XLNG** [[32](#CR32)] 同时使用MLM和翻译TLM损失（表[3.1](#Tab1)）对相同的Seq2seq模型进行预训练。预训练目标在公共空间中为不同语言生成嵌入，从而实现零样本跨语言迁移。在微调阶段，使用单语数据训练预训练模型进行自然语言生成任务。这样，在单一语言中训练的模型可以直接解决其他语言中的相应任务。该模型在零样本跨语言问答生成和抽象摘要方面优于基于机器翻译的方法。此外，通过利用资源丰富的语言的数据，这种方法提高了训练数据较少的语言的性能。'
- en: 3.3.3 Autoregressive Language Models
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.3 自回归语言模型
- en: Generative models like GPT-3 are trained on huge collections of documents which
    usually contain texts from different languages. By this training data, the model
    also acquires the knowledge about these languages and generates joint contextual
    representations of meanings. As described in Sect. [3.6.3](#Sec41), it is able
    to translate between languages if given an appropriate prompt and some examples
    (few-shot learning). On WMT2016 En→De, for instance, GPT-3 achieves a few-shot
    Bleu of 29.7 compared to a supervised Sota of 41.2, whereas in the De→En direction
    GPT-3 outperforms the current Sota of 40.2 Bleu with 40.6 Bleu [[25](#CR25)].
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于GPT-3这样的生成模型是在包含不同语言文本的大量文档集合上训练的。通过这些训练数据，模型也获得了关于这些语言的知识，并生成意义的联合上下文表示。如第[3.6.3](#Sec41)节所述，如果提供适当的提示和一些示例（少样本学习），它能够进行语言之间的翻译。例如，在WMT2016
    En→De上，GPT-3实现了29.7的少样本Bleu，而监督Sota为41.2，而在De→En方向上，GPT-3以40.6的Bleu超过了当前Sota的40.2
    Bleu [[25](#CR25)]。
- en: Winata et al. [[231](#CR231)] evaluate in detail the multilingual capabilities
    of GPT-2, GPT[NEO] and T5 with 1.6B, 6B, and 3B parameters respectively. The models
    are able to use the context from English to predict the answer in non-English
    languages. The authors find that the largest model GPT[NEO] always performs best
    on a set of multilingual benchmarks. The performance depends on the language pair.
    The models, for instance, achieve higher performance for En→Es than for the other
    two target languages (De and Fr). For the *MultiNLU benchmark* [[187](#CR187)]
    the error 12.1% of the Sota model fully trained on the target language is not
    much lower than the error of 17.3% for few-shot prompts of GPT[NEO].
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Winata 等人 [[231](#CR231)] 使用 1.6B、6B 和 3B 参数分别评估了 GPT-2、GPT[NEO] 和 T5 的多语言能力。这些模型能够利用英语的上下文来预测非英语语言的答案。作者发现，最大的模型
    GPT[NEO] 在一组多语言基准测试中总是表现最佳。性能取决于语言对。例如，模型在 En→Es 上的性能高于其他两种目标语言（德语和法语）。对于 *MultiNLU
    基准测试* [[187](#CR187)]，在目标语言上完全训练的 Sota 模型的错误率为 12.1%，与 GPT[NEO] 少样本提示的 17.3% 错误率相差不大。
- en: 3.3.4 Summary
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.4 摘要
- en: Machine translation is one of the most widely used applications of NLP. Languages
    have both structural and lexical differences that make translation difficult.
    The joint processing of multiple languages must take these differences into account.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译是 NLP 最广泛使用的应用之一。语言既有结构上的差异，也有词汇上的差异，这使得翻译变得困难。处理多种语言的联合必须考虑这些差异。
- en: When BERT is trained with documents from multiple languages, it is able to transfer
    knowledge between languages, e.g. solve language inference tasks, even if it has
    no access to parallel texts. Knowledge transfer is improved in XLM by using the
    translation language modeling loss, such that translated sentences are employed
    to reconstruct masked tokens. There are a number of improved versions of XLM that
    are able to increase the accuracy of cross-language inference.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 当 BERT 使用来自多种语言的文档进行训练时，它能够在语言之间进行知识迁移，例如解决语言推理任务，即使它没有访问平行文本。通过使用翻译语言建模损失，XLM
    中的知识迁移得到了改善，这样翻译句子就被用来重建掩码标记。有几种 XLM 的改进版本能够提高跨语言推理的准确性。
- en: Encoder-decoder models such as T5 can be generalized to multiple languages and
    induce powerful multilingual embeddings. mT5 can be controlled by a prefix and
    solves various task like translation, summarization, and language inference. mT6
    and Switch are more effective variants of mT5\. mBART is pre-trained by recovering
    corrupted text in different languages. It can even be used for unsupervised machine
    translation. XNLG generates joint embeddings in a multilingual space and MARGE
    leverages retrieval of background documents to reconstruct a target document.
    Both models are able to perform multiple tasks such as abstractive summarization,
    question answering, and paraphrasing. Note, however that specialized models are
    used for translating single language pairs (Sect. [6.​3.​1](528393_1_En_6_Chapter.xhtml#Sec20)).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器模型，如 T5，可以推广到多种语言并诱导强大的多语言嵌入。mT5 可以通过前缀来控制，并解决各种任务，如翻译、摘要和语言推理。mT6 和
    Switch 是 mT5 的更有效变体。mBART 通过恢复不同语言的损坏文本进行预训练。它甚至可以用于无监督机器翻译。XNLG 在多语言空间中生成联合嵌入，而
    MARGE 利用背景文档的检索来重建目标文档。这两个模型都能够执行多种任务，如抽象摘要、问答和释义。然而，需要注意的是，用于翻译单一语言对的专用模型（第 [6.3.1](528393_1_En_6_Chapter.xhtml#Sec20)
    节）。
- en: Autoregressive language models such as GPT-3 are trained on huge corpora, which
    also contain multilingual documents. Therefore, these models can also be instructed
    by few-shot learning to perform multilingual tasks such as translations or question
    answering. However, performance is usually not as good as for dedicated, fine-tuned
    models.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归语言模型，如 GPT-3，是在庞大的语料库上训练的，这些语料库也包含多语言文档。因此，这些模型也可以通过少样本学习来执行多语言任务，如翻译或问答。然而，性能通常不如专门微调的模型好。
- en: 3.4 Additional Knowledge for Pre-trained Language Models
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 预训练语言模型的额外知识
- en: During unsupervised pre-training, PLMs like BERT and GPT2 are forced to predict
    missing words from the context. They are optimized to predict either the next
    word in a sequence or some masked words (e.g. *“Einstein was [MASK] in the city
    of Ulm.”*). Trained on this task, they obviously gather knowledge about real-world
    facts and relations from the training data. PLMs do surprisingly well in reproducing
    facts and relations based on unsupervised training. In Sect. [4.​2](528393_1_En_4_Chapter.xhtml#Sec7)
    we discuss, what knowledge is covered by standard PLMs. It turns out, however
    that due to the still limited number of parameters only a fraction of knowledge
    contained in the training data can be remembered by a PLM. In addition, events
    that occurred after the training are missed.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督预训练期间，PLM（如 BERT 和 GPT2）被迫从上下文中预测缺失的单词。它们被优化来预测序列中的下一个单词或一些被掩盖的单词（例如 *“爱因斯坦出生在乌尔姆市。”*）。在完成这项任务后，它们显然从训练数据中收集了关于现实世界事实和关系的知识。PLM
    在基于无监督训练重现事实和关系方面表现出惊人的效果。在第 [4.2](528393_1_En_4_Chapter.xhtml#Sec7) 节中，我们讨论了标准
    PLM 覆盖的知识。然而，由于参数数量仍然有限，PLM 只能记住训练数据中包含的知识的一小部分。此外，训练之后发生的事件被遗漏了。
- en: 'This section presents methods for extending factual knowledge in PLMs, either
    during training or on the fly during actual model usage Fig. [3.12](#Fig12). A
    *Knowledge Base* (*KB*) describes knowledge about the world, e.g. by entities
    and their relations. We outline a number of different approaches with which information
    in KBs or other knowledge sources such as text collections can be incorporated
    into PLMs (Table [3.9](#Tab9)):'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了在 PLM 中扩展事实知识的方法，无论是在训练期间还是在实际模型使用时的实时扩展。图 [3.12](#Fig12) 展示了这些方法。*知识库*
    (*KB*) 描述了关于世界的知识，例如通过实体及其关系。我们概述了将 KB 或其他知识源（如文本集合）中的信息纳入 PLM 的几种不同方法（表 [3.9](#Tab9)）：
- en: 'Knowledge Base Embeddings:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 知识库嵌入：
- en: There are techniques to represent the entities and relations in a KB by embeddings.
    A number of approaches try to combine these embeddings with the token embeddings
    created by a PLM. In this way, the information in the KB can be injected into
    the PLM and used for downstream tasks.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig12_HTML.png)
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 有技术可以表示 KB 中的实体和关系通过嵌入。许多方法试图将这些嵌入与 PLM 创建的标记嵌入相结合。通过这种方式，KB 中的信息可以注入 PLM 并用于下游任务。![使用知识库嵌入在预训练语言模型中](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig12_HTML.png)
- en: An illustration represents the input and output through the pre-trained language
    model that interacts with the knowledge requirements of the knowledge graph, knowledge
    base, table, and text.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 一幅插图展示了通过预训练语言模型与知识图谱、知识库、表格和文本的知识需求进行交互的输入和输出。
- en: Fig. 3.12
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12
- en: A PLM gets an input text and collects additional knowledge from different sources.
    This knowledge may be added beforehand or can be retrieved on demand. Subsequently,
    an output is generated using the additional knowledge
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: PLM 接收输入文本并从不同来源收集额外的知识。这些知识可以事先添加，也可以按需检索。随后，使用额外的知识生成输出
- en: Table 3.9
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.9
- en: 'Models integrating additional knowledge (cf. [[166](#CR166), p. 10]). Benchmarks:
    GLUE natural language understanding Sect. [4.​1.​1](528393_1_En_4_Chapter.xhtml#Sec2),
    TACRED relation extraction Sect. [5.​4.​2](528393_1_En_5_Chapter.xhtml#Sec22)
    [[199](#CR199)], TriviaQA question answering Sect. [6.​2.​1](528393_1_En_6_Chapter.xhtml#Sec10)
    [[99](#CR99)], English all word WSD [[14](#CR14)], Nat. Quest question answering
    [[109](#CR109)] Sect. [6.​1.​2](528393_1_En_6_Chapter.xhtml#Sec3)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 集成额外知识（参见图 [[166](#CR166), p. 10]）。基准：GLUE 自然语言理解第 [4.1.1](528393_1_En_4_Chapter.xhtml#Sec2)，TACRED
    关系抽取第 [5.4.2](528393_1_En_5_Chapter.xhtml#Sec22)，TriviaQA 问答第 [6.2.1](528393_1_En_6_Chapter.xhtml#Sec10)，[[99](#CR99)]，英语所有词
    WSD [[14](#CR14)]，Nat. Quest 问答 [[109](#CR109)] 第 [6.1.2](528393_1_En_6_Chapter.xhtml#Sec3)
- en: '| Model | Train task | Fine-tuning | Extra | Benchmark |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 训练任务 | 微调 | 额外 | 基准 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| *Using knowledge base embeddings in pre-trained language models* |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| *在预训练语言模型中使用知识库嵌入* |'
- en: '| ERNIE(THU) [[255](#CR255)] | MLM+NSP + masked NEs | GLUE, etc. | KB NE embeddings
    combined with token embeddings | GLUE 79.6 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| ERNIE(THU) [[255](#CR255)] | MLM+NSP + masked NEs | GLUE, etc. | KB NE embeddings
    combined with token embeddings | GLUE 79.6 |'
- en: '| KnowBERT [[157](#CR157)] | MLM+NSP +EL | GLUE, etc | Translate token embeddings
    ↔ KB NE embeddings |   |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| KnowBERT [[157](#CR157)] | MLM+NSP +EL | GLUE, etc | Translate token embeddings
    ↔ KB NE embeddings |   |'
- en: '| KEPLER [[224](#CR224)] | MLM+KE | GLUE, etc | Combine token embeddings with
    NE embeddings; use TransE loss | TACRED 71.5 F1 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| KEPLER [[224](#CR224)] | MLM+KE | GLUE, 等 | 将标记嵌入与 NE 嵌入结合；使用 TransE 损失 |
    TACRED 71.5 F1 |'
- en: '| *Using textual information from knowledge bases* |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| *使用来自知识库的文本信息* |'
- en: '| K-Adapter [[222](#CR222)] | MLM + rel. extr. | – | Add parallel adapter network
    to RoBERTa | TACRED 72.0 F1 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| K-Adapter [[222](#CR222)] | MLM + rel. extr. | – | 向 RoBERTa 添加并行适配器网络 |
    TACRED 72.0 F1 |'
- en: '| WKLM [[234](#CR234)] | MLM+ERD | – | Detect replaced NEs in text | TriviaQA
    63.1 F1 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| WKLM [[234](#CR234)] | MLM+ERD | – | 在文本中检测替换的 NE | TriviaQA 63.1 F1 |'
- en: '| CoLAKE [[202](#CR202)] | MLM | – | Create graph from textual relation triples
    and tokens | GLUE 86.3 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| CoLAKE [[202](#CR202)] | MLM | – | 从文本关系三元组和标记创建图 | GLUE 86.3 |'
- en: '| LUKE [[234](#CR234)] | MLM+ERD | – | Masked language modeling for text and
    contained entities | TACRED 72.7% F1 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| LUKE [[234](#CR234)] | MLM+ERD | – | 对文本和包含的实体进行掩码语言建模 | TACRED 72.7% F1
    |'
- en: '| EWISER [[14](#CR14)] | MLM | Word sense classification | Include wordnet
    supersense graph | English all word WSD 80.1% F1 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| EWISER [[14](#CR14)] | MLM | 词义分类 | 包含 wordnet 超感图 | 英语所有词的 WSD 80.1% F1
    |'
- en: '| *Using text passages retrieved from text collections* |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| *使用从文本集合检索到的文本段落* |'
- en: '| FiD [[91](#CR91)] | MLM, S2S | QA | Encode query and KB by BERT; combine
    query and retrieved docs with Seq2seq | Nat. Quest. 51.4% acc. |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| FiD [[91](#CR91)] | MLM, S2S | QA | 使用 BERT 对查询和 KB 进行编码；结合查询和检索到的文档与 Seq2seq
    | Nat. Quest. 51.4% 准确率 |'
- en: '| Retro [[21](#CR21)] | LM |   | Language generation with periodical retrieval
    | Nat. Quest. 45.5% acc. |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| Retro [[21](#CR21)] | LM |   | 带有周期性检索的语言生成 | Nat. Quest. 45.5% 准确率 |'
- en: 'Textual Encoding of Tables:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 表格的文本编码：
- en: Often additional knowledge is available in tables. The entries in these tables
    can be encoded in a special text format. A PLM can be trained with this text to
    acquire the knowledge in the rows and columns, in a similar way as the relation
    between the words of two languages can be learned.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在表格中还有额外的知识可用。这些表格中的条目可以用一种特殊的文本格式编码。PLM 可以用这种文本进行训练，以获取行和列中的知识，类似于学习两种语言单词之间关系的方式。
- en: 'Textual Encoding of KB Relations:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 知识库关系的文本编码：
- en: An alternative way to use KB information starts with identifying entities or
    concepts in a text. The relations available for these entities and concepts can
    be extracted from the KB and can be included in the training process either as
    text or in another appropriate form.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 使用知识库（KB）信息的另一种方法是从文本中识别实体或概念。这些实体和概念可用的关系可以从 KB 中提取，并可以以文本或其他适当的形式包含在训练过程中。
- en: 'Adding Retrieved Facts:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 添加检索到的事实：
- en: When a PLM needs to answer a question or create a text, it can formulate a query
    on the topic and retrieve corresponding text content from a KB or the Internet.
    This textual information may be picked up by a transformer and enhance the output.
    In this way, the model can use comprehensive and up-to-date information on the
    fly.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个预训练语言模型（PLM）需要回答问题或创建文本时，它可以就主题提出查询，并从 KB 或互联网中检索相应的文本内容。这些文本信息可能被一个转换器拾取并增强输出。这样，模型可以即时使用全面和最新的信息。
- en: 'Enhancing Logical Consistency:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 提高逻辑一致性：
- en: PLMs sometimes do not generate logically consistent content. By additional fine-tuning
    tasks a model can be trained to respect logical consistency.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: PLM 有时不会生成逻辑上一致的内容。通过额外的微调任务，模型可以被训练以尊重逻辑一致性。
- en: Surveys of methods to incorporate domain knowledge into Deep Neural Networks
    are given by Dash et al. [[45](#CR45)] and Yu et al. [[243](#CR243)].
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: Dash 等人 [[45](#CR45)] 和 Yu 等人 [[243](#CR243)] 提供了将领域知识融入深度神经网络的方法调查。
- en: 3.4.1 Exploiting Knowledge Base Embeddings
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 利用知识库嵌入
- en: Typically, *Knowledge Bases* are graph structures where the nodes correspond
    to entities and the edges represent *relations* connecting the entities. Many
    large-scale KBs, such as *WordNet* [[137](#CR137)], *YAGO* [[200](#CR200)], *Freebase*
    [[18](#CR18)], *DBpedia* [[15](#CR15)], and *DiffBot* [[77](#CR77)] have been
    released in recent years with millions of entities. Figure [3.13](#Fig13) shows
    a small subset of the WordNet hierarchy. In most cases a KB can be described by
    triples (*h*, *r*, *t*), where *h* and *t* are entities in a set *E*, and *r*
    is a relation holding between these entities. To assess the semantic contents
    of a KB, it was proposed to encode its entities as well as its relations as embeddings
    in a low-dimensional space, allowing to determine the similarity of entities and
    relations [[43](#CR43)]. Subsequently, these embeddings can be used to disambiguate
    entities (entity linking, Sect. [5.​3.​3](528393_1_En_5_Chapter.xhtml#Sec16)),
    or predict new relations (Sect. [5.​4](528393_1_En_5_Chapter.xhtml#Sec19)).![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig13_HTML.png)
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，*知识库*是图结构，其中节点对应实体，边代表连接实体的*关系*。近年来，许多大规模的知识库，如*WordNet* [[137](#CR137)]，*YAGO*
    [[200](#CR200)]，*Freebase* [[18](#CR18)]，*DBpedia* [[15](#CR15)]和*DiffBot* [[77](#CR77)]，都发布了包含数百万实体的知识库。图[3.13](#Fig13)显示了WordNet层次结构的一个小子集。在大多数情况下，一个知识库可以用三元组(*h*,
    *r*, *t*)来描述，其中*h*和*t*是集合*E*中的实体，而*r*是存在于这些实体之间的关系。为了评估知识库的语义内容，提出了将其实体及其关系编码为低维空间中的嵌入的方法，从而可以确定实体和关系的相似性[[43](#CR43)]。随后，这些嵌入可以用来消歧实体（实体链接，见第[5.3.3](528393_1_En_5_Chapter.xhtml#Sec16)节），或预测新的关系（见第[5.4](528393_1_En_5_Chapter.xhtml#Sec19)节）![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig13_HTML.png)。
- en: A model diagram explains that instrumentality, conveyance, and vehicles have
    nearly the same meaning. It indicates that motorized vehicles have engines and
    other parts, whereas trains and cars are the members of vehicles.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型图解释了工具性、传达性和交通工具几乎具有相同的意义。它表明，机动车辆有发动机和其他部件，而火车和汽车是交通工具的成员。
- en: Fig. 3.13
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13
- en: Small part of the WordNet knowledge base describing the relations between English
    words. It contains synsets of word with approximately the same meaning, which
    are related by the hypernym (is-a) meronym (has-part) and member-of relations
    [[137](#CR137)]
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 描述英语词语之间关系的WordNet知识库的一部分。它包含具有大约相同意义的词语的synsets，这些词语通过上位词（is-a）、组成词（has-part）和成员关系（member-of）相关联[[137](#CR137)]。
- en: For the embeddings ![](../images/528393_1_En_3_Chapter/528393_1_En_3_IEq26_HTML.gif)e
    m b left parenthesis word right parenthesis) of words generated by Word2Vec [[135](#CR135)]
    it turned out that relations between entities often are represented in the space
    of word embeddings as vector differences between entity embeddings (Sect. [1.​5](528393_1_En_1_Chapter.xhtml#Sec5)).
    An example is the relation between a country and its capital, for which we have
    approximately ![](../images/528393_1_En_3_Chapter/528393_1_En_3_IEq27_HTML.gif)e
    m b left parenthesis Germany right parenthesis minus e m b left parenthesis Berlin
    right parenthesis almost equals e m b left parenthesis France right parenthesis
    minus e m b left parenthesis Paris right parenthesis) .
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Word2Vec生成的词语嵌入(![](../images/528393_1_En_3_Chapter/528393_1_En_3_IEq26_HTML.gif)e
    m b left parenthesis word right parenthesis)来说，实体之间的关系通常在词语嵌入的空间中表示为实体嵌入之间的向量差异（见第[1.5](528393_1_En_1_Chapter.xhtml#Sec5)节）。一个例子是国家与其首都之间的关系，其中我们大约有![](../images/528393_1_En_3_Chapter/528393_1_En_3_IEq27_HTML.gif)e
    m b left parenthesis Germany right parenthesis minus e m b left parenthesis Berlin
    right parenthesis几乎等于e m b left parenthesis France right parenthesis minus e m
    b left parenthesis Paris right parenthesis)。
- en: The **TransE** model [[20](#CR20)] is built on this pattern. TransE adapts the
    embeddings in such a way that whenever (*h*, *r*, *t*) holds and *emb*(*h*) and
    *emb*(*t*) are the embeddings of *h* and *t*, then equation *emb*(*h*) + *emb*(*r*) ≈ *emb*(*t*)
    should be approximately valid for some vector *emb*(*r*), which is considered
    as the embedding of the relation *r*. Consequently, for all triples (*h*, *r*,
    *t*) in the set *S* of correct triples the *TransE-loss*![$$f_r(h,t)=\left \lVert
    {emb}(h)+{emb}(r)-{emb}(t)\right \rVert ^2_2$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq28.png)
    should become 0\. The TransE-model uses the hinge loss to approximate this goal,
    which modifies the embeddings in such a way that *f*[*r*](*h*, *t*) for correct
    relation triples gets lower than ![$$f_r(\tilde {h},\tilde {t})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq29.png)
    for randomly selected incorrect triples ![$$(\tilde {h},r,\tilde {t})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq30.png).
    The models and embeddings are trained with relations from WordNet and Freebase.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**TransE**模型[20](#CR20)基于这种模式构建。TransE通过调整嵌入，使得每当(*h*, *r*, *t*)成立，且*emb*(*h*)和*emb*(*t*)是*h*和*t*的嵌入时，方程*emb*(*h*)
    + *emb*(*r*) ≈ *emb*(*t*)对于某些向量*emb*(*r*)应该近似成立，该向量被认为是关系*r*的嵌入。因此，对于正确三元组集合*S*中的所有三元组(*h*,
    *r*, *t*)，*TransE损失*![$$f_r(h,t)=\left \lVert {emb}(h)+{emb}(r)-{emb}(t)\right
    \rVert ^2_2$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq28.png)应该变为0。TransE模型使用
    hinge 损失来近似这个目标，它以这种方式修改嵌入，使得对于正确的关系三元组*f*[*r*](*h*, *t*)的值低于对于随机选择的错误三元组![$$f_r(\tilde
    {h},\tilde {t})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq29.png)的值![$$(\tilde
    {h},r,\tilde {t})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq30.png)。模型和嵌入使用WordNet和Freebase中的关系进行训练。'
- en: There are a number of more elaborate models to encode relations from KBs, as
    described in the surveys [[43](#CR43), [94](#CR94)]. *TransH* overcomes TransE’s
    inability to model complex relations, and *TransD* aims to reduce the parameters
    by proposing two different mapping matrices for head and tail. But these alternatives
    are rarely used for contextual embeddings. Another method for KB representation
    is tensor factorization [[144](#CR144), [145](#CR145)]. This approach, however,
    is not based on word embeddings and therefore mainly used for KB completion and
    not to enhance PLMs.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多更复杂的模型用于从知识库中编码关系，如调查[43](#CR43)、[94](#CR94)中所述。*TransH*克服了TransE无法建模复杂关系的缺点，而*TransD*则通过提出两个不同的映射矩阵来减少头和尾的参数。但是，这些替代方案很少用于上下文嵌入。知识库表示的另一种方法是张量分解[144](#CR144)、[145](#CR145)。然而，这种方法不基于词嵌入，因此主要用于知识库补全，而不是增强PLMs。
- en: In the rest of the section we describe approaches, which merge KB-embeddings
    usually computed by TransE and token embeddings generated by language models.
    A difficulty is to establish a relation between the token embeddings and the entities,
    which usually contain several tokens.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节剩余部分，我们描述了将通常由TransE计算的知识库嵌入和由语言模型生成的标记嵌入合并的方法。一个困难是建立标记嵌入与通常包含多个标记的实体之间的关系。
- en: '**KEPLER** [[224](#CR224)] consists of a BERT-like language model generating
    token embeddings by the MLM objective. In addition, it computes embeddings for
    entities from descriptive text in the KB using a special token “<*S*>” at the
    beginning of the input text. This token is trained to produce an embedding of
    the named entity argument of the relation, e.g. for the input “<*S*> *Johannes
    Kepler”* in Fig. [3.14](#Fig14). In this way, the arguments *h* and *t* of the
    relation are embedded. The embedding of the relation *r* is either a parameter
    to be trained, or it may be determined by the text verbalizing the relation. These
    embeddings are fed into the TransE loss and used as an extra training criterion
    in addition to MLM (Fig. [3.14](#Fig14)). In a number of language understanding
    tasks the approach is able to achieve good results. On the relation extraction
    benchmark *TACRED* [[254](#CR254)] the approach reaches 71.5% F1-value.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig14_HTML.png)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '**KEPLER** [[224](#CR224)] 由一个类似 BERT 的语言模型组成，通过 MLM 目标生成标记嵌入。此外，它使用输入文本开头的特殊标记
    “<*S*>” 计算来自 KB 中描述性文本的实体嵌入。这个标记被训练以产生关系命名实体参数的嵌入，例如，对于图 [3.14](#Fig14) 中的输入 “<*S*>
    *Johannes Kepler*”。这样，关系的参数 *h* 和 *t* 被嵌入。关系 *r* 的嵌入要么是一个需要训练的参数，要么可能由表达关系的文本确定。这些嵌入被输入到
    TransE 损失函数中，并作为额外的训练标准，除了 MLM 之外（图 [3.14](#Fig14)）。在许多语言理解任务中，这种方法能够取得良好的结果。在关系抽取基准
    *TACRED* [[254](#CR254)] 上，该方法达到了 71.5% 的 F1 值。![图 3.14](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig14_HTML.png)'
- en: A model diagram represents the input text that goes through the encoder and
    M L M loss. It also indicates the flow of the knowledge graph through the encoders
    and embeddings along with the K E loss.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 模型图表示通过编码器和 M L M 损失函数的输入文本。它还指示了知识图谱通过编码器和嵌入以及 K E 损失函数的流动。
- en: Fig. 3.14
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.14
- en: KEPLER [[224](#CR224)] trains a conventional BERT-like model by the MLM-loss.
    For a knowledge base with text entries it generates entity embeddings using the
    special <*S*> token and encodes relations by the TransE-loss. Both loss functions
    are added during training
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: KEPLER [[224](#CR224)] 通过 MLM 损失函数训练一个传统的类似 BERT 的模型。对于一个包含文本条目的知识库，它使用特殊的 <*S*>
    标记生成实体嵌入，并通过 TransE 损失函数编码关系。这两个损失函数都在训练期间添加。
- en: '**KnowBERT** [[157](#CR157)] explicitly models entity spans in the input text
    and uses an entity linker to retrieve precomputed entity embeddings from a KB
    to form knowledge enhanced entity-span representations. The KB-embeddings are
    precomputed with a loss function similar to TransE. Projection mappings are used
    to transform LM-embeddings to KB-embeddings and vice versa. Information from the
    best matching KB-embeddings is averaged and retransformed to enhance the LM-embeddings.
    These computations form an additional layer of BERT. Wikipedia and WordNet were
    used as KBs. To test KnowBERT’s ability to retrieve facts from the KB, a relation
    was formulated and one argument of the relation was masked. KnowBERT reaches a
    *mean reciprocal rank* (*MRR*) of 0.31, indicating that on average the correct
    entity appeared on rank 3, whereas for BERT it shows up on rank 9\. Hence, the
    model generates better answers than BERT, but is only approximately able to reproduce
    the relations of the KB. However, it often leads to improvements in downstream
    tasks.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '**KnowBERT** [[157](#CR157)] 显式地建模输入文本中的实体跨度，并使用实体链接器从 KB 中检索预计算的实体嵌入以形成知识增强的实体跨度表示。KB-嵌入使用类似于
    TransE 的损失函数预先计算。投影映射被用来将 LM-嵌入转换为 KB-嵌入，反之亦然。来自最佳匹配 KB-嵌入的信息被平均并重新转换以增强 LM-嵌入。这些计算形成
    BERT 的额外层。Wikipedia 和 WordNet 被用作 KB。为了测试 KnowBERT 从 KB 中检索事实的能力，一个关系被制定，并且关系的一个参数被掩码。KnowBERT
    达到 *平均倒数排名* (*MRR*) 为 0.31，这表明平均而言，正确的实体出现在第 3 位，而 BERT 则在第 9 位出现。因此，该模型生成的答案比
    BERT 更好，但只能大致复制 KB 的关系。然而，它通常会导致下游任务的改进。'
- en: '**ERNIE-THU** [[255](#CR255)] relates named entities in a KB to the named entities
    in a document in a similar way, and transforms embeddings between these two spaces.
    *E-BERT* [[162](#CR162)] is similar in spirit to KnowBert, but it requires no
    expensive further pre-training of the BERT encoder. *Facts as Experts* [[213](#CR213)]
    also links factual information and entities using embeddings, and in this way
    can inject new information into the model.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '**ERNIE-THU** [[255](#CR255)] 以类似的方式将 KB 中的命名实体与文档中的命名实体相关联，并转换这两个空间之间的嵌入。*E-BERT*
    [[162](#CR162)] 在精神上与 KnowBert 相似，但它不需要对 BERT 编码器进行昂贵的进一步预训练。*Facts as Experts*
    [[213](#CR213)] 也使用嵌入将事实信息和实体链接起来，并以此方式将新信息注入到模型中。'
- en: In summary the methods presented in this section directly infuse domain-specific
    knowledge expressed by relation embeddings into token embeddings of PLMs. There
    are, however, a number of disadvantages. The KB entity embeddings are separately
    pre-trained with some knowledge embedding models (e.g., TransE [[20](#CR20)])
    and fixed during training of the PLMs. Thus KB-embedding and token embeddings
    are not learned simultaneously. Moreover, the KB entity embeddings often cannot
    fully capture the rich contextual and relational information of an entity in the
    KB. Furthermore, they are static and do not depend on the context. In addition,
    they rely to a great extent on the performance of the linking algorithm and on
    the reliability of graph embeddings. This means that in general other approaches
    perform better, e.g. for relation extraction (Sect. [5.​4](528393_1_En_5_Chapter.xhtml#Sec19)).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本节中提出的方法直接将关系嵌入表示的特定领域知识注入到PLM的标记嵌入中。然而，存在一些缺点。KB实体嵌入与某些知识嵌入模型（例如，TransE
    [[20](#CR20)]）分别进行预训练，并在PLM的训练过程中保持固定。因此，KB嵌入和标记嵌入不是同时学习的。此外，KB实体嵌入往往不能完全捕捉KB中实体的丰富上下文和关系信息。此外，它们是静态的，不依赖于上下文。此外，它们在很大程度上依赖于链接算法的性能和图嵌入的可靠性。这意味着在一般情况下，其他方法表现更好，例如关系抽取（第[5.4](528393_1_En_5_Chapter.xhtml#Sec19)节）。
- en: 3.4.2 Pre-trained Language Models for Graph Learning
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 图学习中的预训练语言模型
- en: Relations between objects and concepts can be joined in a graph and provide
    a uniform representation for the relatedness of many items. Using the structure
    of a graph many properties of nodes can be predicted. In recent years there was
    a great effort to design models which can capture the composition of a graph and
    predict its parts, e.g. *node2vec* [[67](#CR67)] or *graph convolutional networks*
    [[107](#CR107)]. However, the node representations obtained by such deep models
    tend to be over-smoothed and also become very vague. PLMs potentially are able
    to improve the representation by self-attention over long distances. Xia et al.
    [[233](#CR233)] provide a survey on PLMs for graphs. Nodes and edges are characterized
    by different feature and position embeddings, and are processed with different
    types of PLMs. Prominent applications are *recommender systems* exploiting user-product
    graphs and *drug discovery* evaluating molecule structures.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 物体和概念之间的关系可以在图中连接起来，并为许多项目的相关性提供一个统一的表现形式。利用图的架构，可以预测节点的许多属性。近年来，人们付出了巨大的努力来设计能够捕捉图组成并预测其部分（例如
    *node2vec* [[67](#CR67)] 或 *图卷积网络* [[107](#CR107)]）的模型。然而，由这种深度模型获得的节点表示往往过于平滑，并且也变得非常模糊。PLMs通过长距离的自注意力机制有可能通过自我改进表示。Xia等人
    [[233](#CR233)] 提供了关于图PLMs的综述。节点和边通过不同的特征和位置嵌入来表征，并使用不同类型的PLMs进行处理。突出的应用包括利用用户-产品图的
    *推荐系统* 和评估分子结构的 *药物发现*。
- en: '**Graph-BERT** [[250](#CR250)] is trained on sample nodes taken from a large
    graph together with their context. These samples are drawn using the closeness
    according to the PageRank algorithm [[24](#CR24)] and contain no direct link information.
    Nodes are characterized by feature embeddings, embeddings based on the PageRank
    information, and hop-based distance embeddings. These embeddings are summarized
    and form the input of a BERT model. The model is pre-trained to reconstruct the
    information of masked nodes and to predict the relation between two nodes by evaluating
    their cosine similarity. The model is fine-tuned for node classification and graph
    clustering. Graph-BERT achieves the second-best accuracies for node classification
    on three graph benchmarks [[128](#CR128), p. 16].'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '**Graph-BERT** [[250](#CR250)] 是在从大型图中选取的样本节点及其上下文中训练的。这些样本是根据PageRank算法 [[24](#CR24)]
    的接近度抽取的，并且不包含直接链接信息。节点通过特征嵌入、基于PageRank信息的嵌入和基于跳数的距离嵌入来表征。这些嵌入被汇总并形成BERT模型的输入。该模型经过预训练以重建掩码节点的信息，并通过评估它们的余弦相似度来预测两个节点之间的关系。该模型经过微调以进行节点分类和图聚类。Graph-BERT在三个图基准测试中实现了节点分类的第二高准确率
    [[128](#CR128)，第16页]。'
- en: '**GPT-GNN** [[87](#CR87)] proposes an autoregressive PLM to perform an iterative
    reconstruction on given graphs. The method assumes a random order on the edges
    and nodes. Given the edges and nodes up to a specific position, it predicts the
    properties of the next nodes/edges. GPT-GNN generates one masked node and its
    edges at a time and optimizes the parameterized models via maximizing the likelihood
    of the node and edges generated in the current iteration. Then, it iteratively
    generates nodes and edges until all masked nodes are generated. The model is trained
    on a graph of 178M scientific papers with their features, the venue and the authors,
    and on a graph with 83M Amazon reviews, users and products. On both benchmarks
    the model has the best accuracies.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-GNN** [[87](#CR87)] 提出了一种自回归PLM，用于对给定的图进行迭代重建。该方法假设边和节点有一个随机顺序。给定特定位置的边和节点，它预测下一个节点/边的属性。GPT-GNN一次生成一个带掩码的节点及其边，并通过最大化当前迭代中生成的节点和边的似然性来优化参数化模型。然后，它迭代地生成节点和边，直到所有带掩码的节点都被生成。该模型在包含其特征、会议和作者的178M篇科学论文的图上以及包含8300万条亚马逊评论、用户和产品的图上进行训练。在这两个基准测试中，该模型都取得了最佳的准确率。'
- en: '**MPG** [[120](#CR120)] consists of a BERT model encoding node and edge features.
    As a pre-training task, the model has to learn whether two graphs divided into
    two halves actually belong together or whether the halves are a random pair. The
    model is applied to the modeling of molecules and achieves Sota results on a range
    of 14 benchmarks, especially drug discovery.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '**MPG** [[120](#CR120)] 由一个编码节点和边特征的BERT模型组成。作为一个预训练任务，该模型必须学习两个分成两半的图实际上是否属于一起，或者这两半是否是一对随机配对。该模型应用于分子的建模，并在14个基准测试中取得了Sota结果，特别是在药物发现方面。'
- en: '**GraphFormers** [[238](#CR238)] jointly models a graph structure together
    with sequences of words. Each node of the graph contains a text. A center node
    and its neighbors are tokenized into sequences of tokens. The model has special
    transformer layers for computing the embeddings of text tokens and for the derivation
    of node embeddings by aggregating the corresponding text embeddings. The model
    is pre-trained with the task to predict, if two nodes are linked or not. GraphFormers
    is tested on three benchmark tasks, e.g. a graph with scientific papers characterized
    by their titles and their citation graph. The model consistently outperforms all
    prior approaches in the prediction of links.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**GraphFormers** [[238](#CR238)] 一起对图结构和单词序列进行建模。图中的每个节点包含一段文本。中心节点及其邻居被标记为标记序列。该模型具有特殊的转换层，用于计算文本标记的嵌入以及通过聚合相应的文本嵌入来推导节点嵌入。该模型通过预测两个节点是否链接的任务进行预训练。GraphFormers在三个基准任务上进行了测试，例如，一个由标题和引用图表征的科学论文图。该模型在链接预测方面始终优于所有先前的方法。'
- en: 3.4.3 Textual Encoding of Tables
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 表格的文本编码
- en: Tabular data probably makes up the majority of all business and administrative
    data today. Examples are retail transactions, official statistics, processing
    data from industrial applications, etc. A survey on the interpretation of tables
    on the web is provided by de Alwis et al. [[46](#CR46)]. Previous work often relies
    on manually selected features, cannot handle the flexible schemas in web tables,
    and does not generalize well across tasks.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 表格数据可能占所有商业和行政数据的绝大多数。例如，零售交易、官方统计数据、工业应用的数据处理等。de Alwis等人提供了一份关于网络表格解释的调查[[46](#CR46)]。先前的工作通常依赖于手动选择的特征，无法处理网络表格中的灵活模式，并且在不同任务之间泛化效果不佳。
- en: '**TURL** [[47](#CR47)] characterizes a relational table by the table caption
    *C* (a short text, may be enhanced by section title), column headers *h*[*i*]
    (a sequence of tokens) describing the table scheme *H* = {*h*[1], …, *h*[*m*]}
    and cell values, where each cell may represent an entity, e.g. a person. Cells
    in the same row share some relation, and cells in the same column share another
    relation. This requires a structure-aware attention mechanism implemented by a
    visibility matrix, which restricts the attention to specific columns and rows.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**TURL** [[47](#CR47)] 通过表格标题 *C*（一段简短文本，可能由章节标题增强）、描述表格方案 *H* = { *h*[1],
    …, *h*[m*]} 的列标题 *h*[*i*]（标记序列）和单元格值来表征关系表，其中每个单元格可能代表一个实体，例如一个人。同一行的单元格共享某种关系，而同一列的单元格共享另一种关系。这需要一个通过可见性矩阵实现的具有结构感知的注意力机制，该机制将注意力限制在特定的列和行上。'
- en: TURL is pre-trained according to the masked language model loss on a large unstructured
    dataset consisting of the table captions and headers. Subsequently, the relation
    between entities in the same row or column can be learned. Entities in a table
    are masked, and the model has the task to predict them based on the table context
    and the visibility matrix. By this target TURL can learn factual relations from
    the table and encode them into entity embeddings (Fig. [3.15](#Fig15)).![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig15_HTML.png)
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: TURL根据掩码语言模型损失在一个包含表格标题和标题的大规模非结构化数据集上进行预训练。随后，可以学习同一行或列中实体的关系。表格中的实体被掩码，模型的任务是根据表格上下文和可见性矩阵预测它们。通过这个目标，TURL可以从表格中学习事实关系并将它们编码到实体嵌入中（图[3.15](#Fig15)）。![图3.15](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig15_HTML.png)
- en: A model diagram represents the input tokens and input entities going through
    self-attention. It indicates the layers of word embedding, type embedding, position
    embedding, contextualized representation entity embedding, and mention representation.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 模型图表示输入标记和输入实体通过自注意力进行传递。它指示了词嵌入层、类型嵌入层、位置嵌入层、上下文表示实体嵌入层和提及表示层。
- en: Fig. 3.15
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15
- en: Learning table relations with TURL [[47](#CR47)]. On the left side the table
    caption and the column headers are trained. On the right side the row markers
    together with input entities (cells in a specific row) are processed
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TURL学习表格关系[[47](#CR47)]。在左侧训练表格标题和列标题。在右侧处理行标记以及输入实体（特定行中的单元格）。
- en: The model is trained on 570k tables extracted from Wikipedia. All columns containing
    at least one linked cell are marked as entity columns. After fine-tuning, the
    model is able to predict the masked contents of table cells in the test set with
    precision of 54.8%, beating competing approaches. An ablation study shows that
    the visibility attention matrix is essential for achieving a high performance.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在从维基百科提取的570k张表中进行训练。所有包含至少一个链接单元格的列都被标记为实体列。经过微调后，模型能够以54.8%的精确度预测测试集中表格单元格的掩码内容，击败了竞争方法。消融研究表明，可见性注意力矩阵对于实现高性能至关重要。
- en: '**TaBERT** [[241](#CR241)] aims to include both, natural language text and
    structured table data. TaBERT is trained on 26.6M tables and surrounding text
    from English Wikipedia and the WDC WebTable Corpus [[115](#CR115)]. Each table
    cell is described as (column header, column value type, value). Subsequently,
    the table rows are encoded as text, as shown in Fig. [3.16](#Fig16). For pre-training
    20% of the columns of a table are randomly selected and the model has to predict
    the masked column names and types. In addition, the cell values are reconstructed
    according to a special scheme. The model is fine-tuned on the *WikiTableQuestions
    benchmark* [[155](#CR155)], which contains questions requiring compositional,
    multi-hop reasoning over a series of entries in the given table. To reduce effort
    only table rows containing query tokens are encoded. TaBERT is able to increase
    the Sota accuracy on this benchmark to 51.8%. The authors show that their table
    cell encoding is more effective than alternatives. **RPT** [[205](#CR205)] proposes
    a similar scheme for table encoding. **BRIDGE** [[124](#CR124)] is a system for
    *semantic parsing*, which converts information from text and tables to an SQL
    query extracting information from a database.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig16_HTML.png)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '**TaBERT** [[241](#CR241)]旨在包含自然语言文本和结构化表格数据。TaBERT在26.6M张表及其周围文本（来自英文维基百科和WDC
    WebTable语料库[[115](#CR115)]）上进行训练。每个表格单元格都描述为（列标题，列值类型，值）。随后，表格行被编码为文本，如图[3.16](#Fig16)所示。为了预训练，随机选择表格20%的列，模型必须预测掩码的列名和类型。此外，单元格值根据特殊方案重建。模型在*WikiTableQuestions基准测试*
    [[155](#CR155)]上进行微调，该基准测试包含需要在对给定表格的一系列条目进行组合、多跳推理的问题。为了减少工作量，只有包含查询标记的表格行被编码。TaBERT能够将此基准测试的Sota准确率提高到51.8%。作者表明，他们的表格单元格编码比替代方案更有效。**RPT**
    [[205](#CR205)]为表格编码提出了类似的方案。**BRIDGE** [[124](#CR124)]是一个用于*语义解析*的系统，它将文本和表格中的信息转换为SQL查询，以从数据库中提取信息。![图3.16](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig16_HTML.png)'
- en: A table of 2 rows and 3 columns represents the list of the venues, positions,
    and events in the years 2005 and 2006\. At the bottom, it indicates the types
    of data present in each column.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 一个2行3列的表格表示了2005年和2006年的地点、职位和事件列表。底部指示了每个列中存在的数据类型。
- en: Fig. 3.16
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16
- en: TaBERT [[241](#CR241)] encodes the rows of a table as text in a special format.
    The “context” contains corresponding text. Each table cell is represented as (column
    header, column value type, value). Here the first table row is encoded by the
    line starting with [CLS]
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: TaBERT [[241](#CR241)] 将表格的行编码为特殊格式的文本。这里的“上下文”包含相应的文本。每个表格单元格表示为（列标题，列值类型，值）。在这里，第一行表格是通过以[CLS]开头的行进行编码的。
- en: '**Tapas** [[81](#CR81)] is a variant of BERT optimized for table processing.
    The table is flattened row-by-row, tokenized and enhanced with position embeddings.
    Following embeddings are added: a row id embedding, a column id embedding, and
    a rank embedding indicating the rank in the sorted sequence, e.g. for numbers.
    The model is pre-trained on 6.2M table-text pairs from the English Wikipedia with
    the task to restore words in both table and text that have been replaced with
    a mask. The model can do this with relatively high accuracy (71.4% accuracy on
    a test set).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tapas** [[81](#CR81)] 是一种针对表格处理优化的BERT变体。表格按行展开，分词并添加位置嵌入。随后添加的嵌入包括：行ID嵌入、列ID嵌入以及表示排序序列中排名的排名嵌入，例如对于数字。该模型在来自英语维基百科的6.2M个表格-文本对上进行预训练，任务是在表格和文本中恢复被掩码替换的单词。该模型可以以相对较高的准确性（测试集上的71.4%准确率）完成此任务。'
- en: 'During fine-tuning the model learns to answer questions from a table, e.g.
    *“Which wrestler had the most number of reigns?”* for a table with wrestling results.
    *[CLS]* and a query are prepended to the flattened table and both parts are distinguished
    by an additional segment embedding. The model has two output types: (1) a score
    for each table cell with the probability that this cell will be part of the answer
    and (2) a probability of the result type (none, count, sum, average) for *[CLS]*
    to produce the final answer. Together the result indicates which operation should
    be performed over which table cells to generate the final answer. On several benchmarks
    Tapas reaches Sota results, e.g. improving from 55.1% to 67.2% for *SQA* benchmark
    [[90](#CR90)]. The source code and pre-trained models are available at [Hugging
    Face](https://huggingface.co/transformers/model_doc/tapas.html).'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中，模型学习从表格中回答问题，例如，对于一个包含摔跤结果的表格，回答“哪位摔跤手拥有最多的统治次数？”。[CLS]和一个查询被添加到展开的表格之前，并且这两部分通过额外的段嵌入来区分。模型有两种输出类型：（1）每个表格单元格的分数，表示该单元格成为答案一部分的概率；（2）*[CLS]*产生最终答案的结果类型的概率（无、计数、总和、平均值）。这些结果一起表明应该对哪些表格单元格执行哪些操作以生成最终答案。在多个基准测试中，Tapas达到了Sota结果，例如，将*SQA*基准测试的准确率从55.1%提高到67.2%。源代码和预训练模型可在[Hugging
    Face](https://huggingface.co/transformers/model_doc/tapas.html)获取。
- en: The results show that the models described above are able to extract information
    from tables and answer question about the table content. This makes it possible
    to use a large source of information, since tables are ubiquitous in text documents
    and web pages. In principle, the approach can also be used by large Foundation
    Models to include table information in the text they generate.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，上述描述的模型能够从表格中提取信息并回答有关表格内容的问题。这使得能够使用大量信息源，因为表格在文本文档和网页中无处不在。原则上，这种方法也可以由大型基础模型使用，以在它们生成的文本中包含表格信息。
- en: '**TableGPT** [[63](#CR63)] generate a text from a table using the GPT-2 language
    model. It enhances GPT-2 for table-to-text generation with two auxiliary tasks,
    table structure reconstruction and content matching, for improving text fidelity.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '**TableGPT** [[63](#CR63)] 使用GPT-2语言模型从表格生成文本。它通过两个辅助任务，即表格结构重建和内容匹配，来增强GPT-2的表格到文本生成能力，以提高文本的忠实度。'
- en: 3.4.4 Textual Encoding of Knowledge Base Relations
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.4 知识库关系的文本编码
- en: A number of proposals try to verbalize KB-relations as text. In this way, KB-relations
    may be directly incorporated in the training text of the language models.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多提议试图将知识库关系用文本表达出来。这样，知识库关系可以直接纳入语言模型的训练文本中。
- en: '**WKLM** [[234](#CR234)] randomly replaces a fraction of the entity mentions
    in the original document with names of other entities of the same type. The model
    is trained to distinguish the correct entity mention from the randomly chosen
    ones. In addition, the model has to predict masked token. The types of entities
    are obtained from Wikidata [[214](#CR214)]. In this way, the model can better
    capture entity information from natural language and yields better results for
    entity-related NLP tasks. WKLM is able to predict relation arguments much better
    than BERT. In question answering (SQuAD and open domain, Sect. [6.​2](528393_1_En_6_Chapter.xhtml#Sec9))
    the model is also able to reach Sota results. Similar approaches [[191](#CR191),
    [203](#CR203), [234](#CR234)] propose entity and phrase masking and replacement
    schemes.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**WKLM** [[234](#CR234)] 随机将原始文档中的一部分实体提及替换为同类型其他实体的名称。该模型被训练以区分正确的实体提及和随机选择的提及。此外，模型还需要预测掩码标记。实体类型来自
    Wikidata [[214](#CR214)]。这样，模型可以更好地从自然语言中捕获实体信息，并为实体相关的NLP任务提供更好的结果。WKLM在预测关系论元方面比BERT做得更好。在问答（SQuAD和开放域，第[6.2](528393_1_En_6_Chapter.xhtml#Sec9)节）中，该模型也能达到Sota结果。类似的方法[[191](#CR191)，[203](#CR203)，[234](#CR234)]提出了实体和短语掩码及替换方案。'
- en: '**CoLAKE** [[202](#CR202)] extracts the knowledge context of an entity from
    large-scale knowledge bases. The model links entity mentions to the underlying
    entities in a KB by an entity linker. The mention nodes are then replaced by their
    linked entities. The CoLAKE model is initialized with the RoBERTa[BASE] model.
    It is trained on Wikipedia with 3 million entity embeddings and 822 relation embeddings
    aligned to the *Wikidata5M* KB [[224](#CR224)] on 26M training samples. The example
    input *“[CLS] Harry Potter points his wand at Lord Voldemort [SEP]”* is shown
    in Fig. [3.17](#Fig17). The type of inputs (word, entity, relation) is encoded
    as type embeddings and added to the token and position embeddings. To introduce
    a relation from the KB, e.g. *“(Harry Potter, mother, Lily Potter)”*, the relation
    node *“mother”* and the entity node *“Lily Potter”* are introduced with the position
    embeddings 2 and 3, as the first relation argument *“Harry Potter”* is located
    at position 1\. Self attention is computed between text inputs. There is a masking
    mechanism restricting the self-attention for relation elements, e.g. to the pairs
    *“(Harry Potter, mother)”* as well as *“(mother, Lily Potter)”* in our example.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig17_HTML.png)'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '**CoLAKE** [[202](#CR202)] 从大规模知识库中提取实体的知识上下文。该模型通过实体链接器将实体提及链接到知识库中的底层实体。提及节点随后被其链接的实体替换。CoLAKE模型用RoBERTa[BASE]模型初始化。它在Wikipedia上用300万个实体嵌入和822个与*Wikidata5M*
    KB [[224](#CR224)] 对齐的关系嵌入在260万个训练样本上进行训练。示例输入“[CLS] Harry Potter points his wand
    at Lord Voldemort [SEP]”在图[3.17](#Fig17)中展示。输入类型（单词、实体、关系）被编码为类型嵌入并添加到标记和位置嵌入中。为了引入知识库中的关系，例如“（Harry
    Potter，母亲，Lily Potter）”，关系节点“母亲”和实体节点“Lily Potter”用位置嵌入2和3引入，因为第一个关系论元“Harry Potter”位于位置1。在文本输入之间计算自注意力。有一个掩码机制限制关系元素的自注意力，例如在我们的例子中对“（Harry
    Potter，母亲）”以及“（母亲，Lily Potter）”这样的对进行限制！[](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig17_HTML.png)'
- en: A model diagram represents the input text with named entity mentions replaced
    and knowledge graph elements going through the autoencoder with masked self-attentions.
    It indicates the layers of position, type, input and output embeddings, and token
    probabilities.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 模型图展示了输入文本，其中已用命名实体提及替换，知识图谱元素通过自动编码器进行带掩码的自注意力处理。它指出了位置、类型、输入和输出嵌入层，以及标记概率。
- en: Fig. 3.17
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17
- en: CoLAKE [[202](#CR202)] identifies entities and encodes them with specific embeddings.
    Type embeddings distinguish words, entities and relations. The input embeddings
    are the sum of token/entity, position, and type embeddings. For all entities in
    the input text relations are extracted from the Knowledge Base and appended after
    *“[SEP]”*, e.g. mother(Harry Potter, Lily Potter). A masking mechanism ensures
    that relation elements can attend only to their corresponding elements in the
    input text. During pre-training the model has to predict masked tokens and entities
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: CoLAKE [[202](#CR202)] 识别实体并用特定的嵌入编码它们。类型嵌入区分单词、实体和关系。输入嵌入是标记/实体、位置和类型嵌入的总和。对于输入文本中的所有实体，关系从知识库中提取并附加在“[SEP]”之后，例如：母亲(Harry
    Potter, Lily Potter)。一个掩码机制确保关系元素只能关注输入文本中对应元素。在预训练期间，模型需要预测掩码标记和实体
- en: During pre-training about 15% of the input elements (words, entities, relations)
    are masked and have to be predicted by the model. As entity nodes simultaneously
    appear in the input text and the knowledge base this helps to align the representations
    of language and relations. Masking relation nodes helps CoLAKE to learn contextualized
    representation for relations. On the language understanding tasks of GLUE the
    CoLAKE model achieves a similar average of 86.3 as RoBERTa. An alternative task
    consist of the completion of relation triplets (*h*, *r*, *t*) using a sentence
    describing the relation. It turns out that CoLAKE is much better than its competitors,
    e.g. the correct relation is inferred from two entities in 72.1% of the cases.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练过程中，大约15%的输入元素（单词、实体、关系）被遮蔽，需要由模型进行预测。由于实体节点同时出现在输入文本和知识库中，这有助于对齐语言和关系的表示。遮蔽关系节点有助于CoLAKE学习关系的上下文表示。在GLUE的语言理解任务中，CoLAKE模型达到了与RoBERTa相似的86.3的平均值。另一个任务包括使用描述关系的句子来完成关系三元组（*h*，*r*，*t*）。结果表明，CoLAKE在竞争对手中表现优异，例如，在72.1%的情况下，正确的关系是从两个实体中推断出来的。
- en: '**LUKE** [[237](#CR237)] treats words and entities in a given text as independent
    tokens, and outputs contextualized representations of both. The model is based
    on BERT and trained to predict randomly masked words and entities in a large entity-annotated
    corpus derived from Wikipedia. It contains an entity-aware self-attention mechanism
    that is an extension of BERT’s self-attention. It takes into account embeddings
    indicating if a token represents text or an entity. LUKE yields Sota results in
    relation classification, entity typing and NER. **K-adapter** [[222](#CR222)]
    is a related approach using RoBERTa (Sect. [3.1.1](#Sec2)) as fixed background
    model and building several independent “Adapters” to include knowledge from different
    KBs.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '**LUKE** [[237](#CR237)] 将给定文本中的单词和实体视为独立的标记，并输出两者的上下文表示。该模型基于BERT，并在从维基百科派生的大规模实体标注语料库中训练，以预测随机遮蔽的单词和实体。它包含一个实体感知的自注意力机制，这是BERT自注意力的扩展。它考虑了表示一个标记代表文本或实体的嵌入。LUKE在关系分类、实体类型和命名实体识别（NER）方面取得了Sota结果。**K-adapter**
    [[222](#CR222)] 是一个相关的方法，使用RoBERTa（第[3.1.1](#Sec2)节）作为固定背景模型，并构建几个独立的“适配器”以包含来自不同知识库的知识。'
- en: '**EWISER** [[14](#CR14)] similarly targets word sense disambiguation (WSD).
    Starting with BERT embeddings, it computes scores for WordNet synsets (sets of
    words with similar meaning). Exploiting the interdependence of the synset graph
    the approach computes final scores that a word belongs to a synset. It achieves
    a new Sota on a number of WSD benchmarks (Sect. [5.​2](528393_1_En_5_Chapter.xhtml#Sec7)).'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '**EWISER** [[14](#CR14)] 类似地针对词义消歧（WSD）。从BERT嵌入开始，它计算WordNet同义词集（具有相似意义的单词集合）的分数。利用同义词图之间的相互依赖性，该方法计算一个词属于同义词集的最终分数。它在多个WSD基准测试中实现了新的Sota（第[5.2](528393_1_En_5_Chapter.xhtml#Sec7)节）。'
- en: '**PET** (Pattern-Exploiting Training) [[184](#CR184)] as an alternative constructs
    an additional training set using only a few labeled examples. Consider a 5-star
    scale rating for a restaurant in the Yelp dataset [[185](#CR185)]. The authors
    add text to the reviews to express the ratings, e.g. *“All in all it was great”*.
    Using this approach the authors convert the Yelp dataset to a task for predicting
    masked words, e.g. *“All in all it was [MASK]”*. However, they provide the verbalized
    labels only for a small number of examples. Subsequently, they predict the best
    class for the non-labeled examples and train the model with the predicted classes
    as well as the language modeling loss to avoid *catastrophic forgetting*. This
    can be done in several iterations. Although only a few labels have been used,
    the model performs better on Yelp than standard supervised approaches. The SuperGLUE
    benchmark data covers eight challenging NLP tasks. With just 32 labeled examples
    the PET approach trained according to the above schema yields a better average
    (75.4%) than GPT-3 (71.8%) with the same number of few-shot examples. This shows
    that good results can be achieved with a small model (223M) and only few labeled
    examples. Note that the fine-trained Sota for SuperGLUE is 90.4% using T5 and
    Meena.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '**PET**（模式利用训练）[[184](#CR184)] 作为一种替代方案，仅使用少量标记示例构建一个额外的训练集。以 Yelp 数据集中的餐厅
    5 星级评分为例 [[185](#CR185)]。作者向评论中添加文本来表达评分，例如 *“总的来说，非常好”*。使用这种方法，作者将 Yelp 数据集转换为预测掩码词的任务，例如
    *“总的来说，它非常好”*。然而，他们只为少数示例提供了口头化的标签。随后，他们预测非标记示例的最佳类别，并用预测的类别以及语言建模损失来训练模型，以避免
    *灾难性遗忘*。这可以通过多次迭代来完成。尽管只使用了少量标签，但该模型在 Yelp 上的表现优于标准监督方法。SuperGLUE 基准数据涵盖了八个具有挑战性的自然语言处理（NLP）任务。按照上述方案，仅使用
    32 个标记示例的 PET 方法训练出的平均准确率（75.4%）优于具有相同数量少样本示例的 GPT-3（71.8%）。这表明，使用小型模型（223M）和少量标记示例也能取得良好的结果。请注意，SuperGLUE
    的 Sota 精细训练准确率为 90.4%，使用 T5 和 Meena。'
- en: '**TeKGen** [[1](#CR1)] is a data-to-text sequence-to-sequence model to verbalize
    a complete KB. It is applied to the English *Wikidata knowledge base* [[214](#CR214)]
    with ≈ 6M entities and about 1500 relations. The model starts with a large training
    corpus of heuristically aligned Wikipedia text and Wikidata triples. Relations
    sharing a common entity *subject* are converted to the input *subject relation*[1]*object*[1]*,
    …, relation*[*n*]*object*[*n*] for the T5 transformer (Sect. [3.1.3](#Sec4)).
    As an example *“To kill a Mockingbird, author: Harper Lee, publication date: 11
    July 1960”* is translated to *“To Kill a Mockingbird is a novel by Harper Lee
    published in 1960.”* The T5 model is fine-tuned and subjected to an addition check
    to generate good verbalizations. The resulting dataset of verbalized triples was
    used in a question answering task. It was able to increase the accuracy in the*Natural
    QuestionsNatural Questions (NQ) benchmark* [[109](#CR109)] (Sect. [6.​1.​2](528393_1_En_6_Chapter.xhtml#Sec3))
    from 38.8% to 41.5%. **KGPT** [[30](#CR30)] in a similar way converts structural
    knowledge into the serialized text and lets model learn knowledge-text alignments.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '**TeKGen** [[1](#CR1)] 是一个数据到文本的序列到序列模型，用于将完整的知识库口头化。它应用于包含 ≈ 6M 个实体和约 1500
    个关系的英语 *Wikidata 知识库* [[214](#CR214)]。模型从大量启发式对齐的维基百科文本和 Wikidata 三元组的大规模训练语料库开始。共享相同实体
    *主体* 的关系被转换为 T5 变换器的输入 *主体关系*[1]*对象*[1]*，…，关系*[*n*]*对象*[*n*]（见第 [3.1.3](#Sec4)
    节）。例如，*“杀死一只知更鸟，作者：哈珀·李，出版日期：1960 年 7 月 11 日”* 被翻译为 *“《杀死一只知更鸟》是哈珀·李在 1960 年出版的小说。”*
    T5 模型经过微调，并进行了额外的检查以生成良好的口头化。生成的口头化三元组数据集被用于问答任务。它能够将 *自然问题*（NQ）基准测试中的准确率从 38.8%
    提高到 41.5%。**KGPT** [[30](#CR30)] 以类似的方式将结构化知识转换为序列化文本，并让模型学习知识-文本对齐。'
- en: In summary these methods transform KB relations into text, e.g. as complete
    sentences expressing relations or as concatenated triples (e.g., [head text, relation
    text, tail text]) into LMs for training or fine-tuning. This text is transformed
    into contextual embeddings and the model is trained to detect the underlying relation.
    The drawback is that focusing on knowledge base completion tends to over-adapt
    the models to this specific task, which comes at the cost of generalization.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这些方法将知识库（KB）关系转换为文本，例如作为表达关系的完整句子，或者作为连接的三元组（例如，[头文本，关系文本，尾文本]）输入到语言模型（LM）进行训练或微调。这些文本被转换为上下文嵌入，模型被训练以检测潜在的关联。缺点是专注于知识库补全往往会过度适应模型到这个特定任务，这以泛化能力为代价。
- en: 3.4.5 Enhancing Pre-trained Language Models by Retrieved Texts
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.5 通过检索文本增强预训练语言模型
- en: 'An *open domain question answering* system has the task of answering questions
    not restricted to a specific domain [[27](#CR27)]. Consider the following example
    from the *TriviaQA benchmark* [[99](#CR99)]. *“Question: The Dodecanese Campaign
    of WWII that was an attempt by the Allied forces to capture islands in the Aegean
    Sea was the inspiration for which acclaimed 1961 commando film?”**“Answer: The
    Guns of Navarone”*. It is not plausible that the model can reproduce such a specific
    response from the knowledge stored in its parameters, even if it was present in
    the data before training. Therefore, it would be desirable for the system to be
    able to gather additional evidence by a *retriever* collecting relevant documents
    from a large text repository. Subsequently, it has to align the retrieved information
    with the question and generate an answer by another PLM, a *reader*. New web search
    techniques can be used for this approach. They are based on comparing embeddings
    for words or passages consisting of several sentences. There are numerous applications
    such as question answering, summarization, and dialog systems. In Sect. [6.​1](528393_1_En_6_Chapter.xhtml#Sec1)
    this is discussed in more detail. Recent surveys are provided by Zhu et al. [[259](#CR259)]
    and Yu et al. [[244](#CR244)].'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *开放域问答* 系统的任务是回答不受特定领域限制的问题[[27](#CR27)]。考虑以下来自 *TriviaQA基准* [[99](#CR99)]
    的例子。*“问题：二战中的多德卡尼斯战役是盟军试图占领爱琴海岛屿的一次尝试，这是哪部1961年获奖的突击电影灵感的来源？”**“答案：纳瓦罗恩的枪”*。模型不可能从其参数中存储的知识中重现如此具体的响应，即使它在训练前存在于数据中。因此，系统能够通过
    *检索器* 从大型文本库中收集相关文档来收集额外证据将是理想的。随后，它必须将检索到的信息与问题对齐，并通过另一个PLM，即 *阅读器*，生成答案。可以采用新的网络搜索技术来实现这一方法。这些技术基于比较由几个句子组成的单词或段落的嵌入。有众多应用，如问答、摘要和对话系统。在第[6.1](528393_1_En_6_Chapter.xhtml#Sec1)节中对此进行了更详细的讨论。最近的调查由Zhu等人[[259](#CR259)]和Yu等人[[244](#CR244)]提供。
- en: '**DPR** (Dense Passage Retriever) [[103](#CR103)] employs a PLM to encode KB-passages
    *d*[*i*], e.g. from Wikipedia, as embeddings *emb*(*d*[*i*]). This can be achieved
    by fine-tuning a BERT model to encode passages by the embedding of the token *[CLS]*.
    These embeddings can be stored in an index for fast access. Then the DPR *retriever*
    processes the query sequence *x* by another BERT model and generates the query
    embedding *emb*(*x*). A number of *k* = 100 passages *d*[*j*] with maximal inner
    product ![$${emb}(x)^\intercal {emb}(d_j)$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq31.png)
    is retrieved by a *nearest-neighbor search*. Both BERT encoders can be trained
    together to generate appropriate embeddings using weak supervision in the form
    of question-answer pairs (cf. Sect. [6.​1.​5](528393_1_En_6_Chapter.xhtml#Sec6)).
    If, for instance, the query is *“Who is the bad guy in lord of the rings”*, the
    algorithm can retrieve *“Sala Baker is best known for portraying the villain Sauron
    in the Lord of the Rings trilogy”*, because *“bad guy”* and *“villain”* have similar
    embeddings. Therefore, DPR can find passages with similar meaning, expressed with
    different words. Karpukhin et al. [[103](#CR103)], for instance, show that already
    with 1000 training examples the dense retriever is better than the classical keyword
    search. For 40k training examples the top-20 retrieved passages contain the correct
    answer in about 79% of the time, while this value is only 59% for the classical
    retrieval. An in-depth discussion is given in Sect. [6.​1.​5](528393_1_En_6_Chapter.xhtml#Sec6).'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '**DPR** (密集段落检索器) [[103](#CR103)] 使用PLM对知识库段落 *d*[*i*]，例如来自维基百科的段落，进行编码，生成嵌入
    *emb*(*d*[*i*])。这可以通过微调一个BERT模型，通过标记 *CLS* 的嵌入来编码段落来实现。这些嵌入可以存储在索引中以实现快速访问。然后，DPR
    *检索器* 通过另一个BERT模型处理查询序列 *x*，并生成查询嵌入 *emb*(*x*)。通过最近邻搜索检索到具有最大内积 ![$${emb}(x)^\intercal
    {emb}(d_j)$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq31.png)
    的 *k* = 100 段落 *d*[*j*]。这两个BERT编码器可以一起训练，使用弱监督的形式，即问答对（参见表[6.1.5](528393_1_En_6_Chapter.xhtml#Sec6)）来生成适当的嵌入。例如，如果查询是
    *“谁在指环王中是坏人”*，算法可以检索到 *“Sala Baker因在指环王三部曲中扮演反派索伦而闻名”*，因为 *“坏人”* 和 *“反派”* 有相似的嵌入。因此，DPR可以找到用不同词汇表达但意义相似的段落。例如，Karpukhin等人[[103](#CR103)]表明，即使只有1000个训练示例，密集检索器也比传统的关键词搜索更好。对于40k个训练示例，前20个检索到的段落中有大约79%包含正确答案，而这一比例在传统检索中仅为59%。更深入的讨论见第[6.1.5](528393_1_En_6_Chapter.xhtml#Sec6)节。'
- en: The DPR *reader* is another BERT model. Similar to BERT’s text pair classification,
    it is fine-tuned to predict a probability for each retrieved passage that this
    passage contains the correct answer. In addition, it selects a span of tokens
    by span prediction, which probably provides the answer. In the example it selects
    *“Sala Baker”* as the answer. Together both components form a *retriever-reader
    architecture*, which recently became popular. The approach can be easily applied
    to KBs with billions of passages [[103](#CR103), [201](#CR201)]. On the *Natural
    Questions* [[109](#CR109)] it yields a test set accuracy of 41.5%.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: DPR *读者* 是另一个BERT模型。类似于BERT的文本对分类，它被微调以预测每个检索到的段落包含正确答案的概率。此外，它通过跨度预测选择一个标记跨度，这可能会提供答案。在示例中，它选择了*“Sala
    Baker”*作为答案。这两个组件共同构成了一个*检索器-读者架构*，这种架构最近变得很受欢迎。这种方法可以很容易地应用于包含数十亿段落的KB [[103](#CR103)，
    [201](#CR201)]。在*Natural Questions* [[109](#CR109)]上，它产生了41.5%的测试集准确率。
- en: '**DensePhrases** is a different system creating embeddings for phrases of up
    to 20 words in the KB, which are computed without knowing the query [[114](#CR114)].
    The processing of the retrieved phrases directly yields the answer without much
    computational effort. Using careful workflow optimization the authors achieve
    near-Sota results with a much lower processing time than dense passage retrieval
    systems, e.g. a test set accuracy of 40.9% on Natural Questions.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '**DensePhrases** 是一个不同的系统，为知识库中最多20个单词的短语创建嵌入，这些嵌入是在不知道查询的情况下计算的 [[114](#CR114)]。检索到的短语的直接处理直接产生答案，而不需要太多的计算工作量。通过仔细的工作流程优化，作者们实现了接近Sota的结果，其处理时间比密集段落检索系统低得多，例如在Natural
    Questions测试集上的准确率为40.9%。'
- en: '**FiD** (Fusion in Decoder) [[91](#CR91)] employs DPR as retriever. In the
    reader step it uses the special tokens *“question:”*, *“title:”*, and *“context:”*.
    These tokens mark the question, the retrieved passage title and the passage text
    and are concatenated forming the input. Subsequently, these *k* retrieved triples
    are fed one-by-one into a transformer encoder like T5 [[170](#CR170)] (770M parameters),
    which independently processes each triples by the encoder. Only in the decoder
    the passages are handled jointly and the text of the answer is generated. This
    approach drastically reduces the computational effort. The transformer is fine-tuned
    on a QA-task. The architecture of the model is shown in Fig. [3.18](#Fig18). Raffel
    et al. [[170](#CR170)] provided evidence that generative models like T5 are even
    competitive for QA-tasks such as SQuAD [[173](#CR173)], where answers are spans
    in a given document.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig18_HTML.png)'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '**FiD**（解码器中的融合）[[91](#CR91)] 使用DPR作为检索器。在读者步骤中，它使用特殊标记*“问题：”*、*“标题：”*和*“上下文：”*。这些标记标记了问题、检索到的段落标题和段落文本，并连接形成输入。随后，这些*k*个检索到的三元组逐个输入到T5
    [[170](#CR170)]（770M参数）这样的transformer编码器中，由编码器独立处理每个三元组。只有在解码器中，段落才被联合处理，并生成答案的文本。这种方法大大减少了计算工作量。该transformer在QA任务上进行微调。模型的架构如图[3.18](#Fig18)所示。Raffel等人
    [[170](#CR170)] 提供了证据，表明像T5这样的生成模型在QA任务（如SQuAD [[173](#CR173)]）中甚至具有竞争力，其中答案是在给定文档中的跨度。![图3.18](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig18_HTML.png)'
- en: A diagram illustrates the flow of an input question to the output answer through
    units of retriever and reader. The retriever unit consists of a BERT encoder and
    an inner product. The reader unit consists of t 5 encoder, encoded question, and
    t 5 decoder.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 一张图展示了输入问题通过检索器和读者单元流向输出答案的流程。检索器单元由一个BERT编码器和内积组成。读者单元由t 5编码器、编码后的问题和t 5解码器组成。
- en: Fig. 3.18
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18
- en: A retrieval enhanced language model [[91](#CR91)] encodes the query and the
    KB passages as embeddings and uses a pre-trained retriever to find passages corresponding
    to the query. The reader is a Seq2seq model (T5) combining the query and the passages
    to generate the answer. This model setup is fine-tuned with different benchmark
    datasets
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 一种增强检索的语言模型 [[91](#CR91)] 将查询和知识库段落编码为嵌入，并使用预训练的检索器找到与查询对应的段落。读者是一个Seq2seq模型（T5），它结合查询和段落来生成答案。这种模型设置使用不同的基准数据集进行微调。
- en: The system achieves a test set exact match accuracy of 51.4% on the Natural
    Questions benchmark compared to 41.5% for DPR. The *TriviaQA* benchmark [[99](#CR99)]
    contains a set of trivia questions with answers that were originally scraped from
    the Web. On this benchmark the model yields Sota results with 80.1% exact match
    accuracy [[211](#CR211)]. This is better than the accuracy of other much larger
    models, like GPT3 with 175B parameters (71.2% EM), or T5 without retrieval and
    11B parameters (60.5% EM). It turns out that increasing the number of retrieved
    passages strongly enhances the answer quality.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统在自然问题基准测试中实现了51.4%的测试集精确匹配准确率，相比之下，DPR的准确率为41.5%。*TriviaQA*基准测试[[99](#CR99)]包含一组从网络中抓取的趣味问题及其答案。在此基准测试中，该模型达到了80.1%的精确匹配准确率[[211](#CR211)]。这比其他许多更大规模的模型（如175B参数的GPT3，精确匹配准确率为71.2%）或没有检索功能和11B参数的T5（精确匹配准确率为60.5%）的准确率都要好。结果表明，增加检索到的段落数量可以显著提高答案质量。
- en: There are a number of new approaches to augment PLMs with text from an external
    KB. In Sect. [6.​1](528393_1_En_6_Chapter.xhtml#Sec1) we describe different PLMs
    for retrieval that can be used by web search engines. In Sect. [6.​2](528393_1_En_6_Chapter.xhtml#Sec9)
    we investigate systems for question answering that often employ a PLM-based retrieval
    mechanism and an additional PLM to generate the answer text. It combines the query,
    the knowledge acquired during training, as well as the information in the retrieved
    documents.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多新的方法可以将外部知识库中的文本添加到PLMs中。在第[6.1](528393_1_En_6_Chapter.xhtml#Sec1)节中，我们描述了可以用于网络搜索引擎的不同PLMs检索方法。在第[6.2](528393_1_En_6_Chapter.xhtml#Sec9)节中，我们研究了通常采用基于PLM的检索机制和额外的PLM来生成答案文本的问答系统。它结合了查询、训练期间获得的知识以及检索文档中的信息。
- en: In summary, combining language models with retrieval is currently the most efficient
    way to incorporate additional information into PLMs. The new information is focused
    on the current query and thus very informative. The retrieval model can access
    semantically related passages within fractions of a second using new approximate
    open-source nearest neighbor index structures. By relying on embeddings, synonyms
    and paraphrases can be found and the meaning of words can be disambiguated. In
    addition, the underlying knowledge bases can be updated on the fly to keep the
    information current.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，将语言模型与检索相结合是目前将额外信息纳入PLMs的最有效方式。新的信息专注于当前查询，因此非常有信息量。检索模型可以在几秒钟内访问语义相关的段落，使用新的近似开源最近邻索引结构。通过依赖嵌入，可以找到同义词和释义，并消除词语歧义。此外，底层知识库可以实时更新，以保持信息的时效性。
- en: 3.4.6 Summary
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.6 摘要
- en: The knowledge covered by the textual training data can be leveraged in various
    ways to improve the performance of PLMs. Entities and relations from a knowledge
    base can be represented by embeddings, e.g. by TransE. However, the utilization
    of these embeddings for PLMs is not very efficient and error-prone. A more promising
    alternative is the direct use of table content or knowledge base relations by
    specialized PLMs, which capture relationships between entities and table cells
    by specific self-attention patterns. Similar to Graph-CNNs PLMs have been directly
    used to acquire the relationship between the nodes of a graph by encoding the
    features of links by embeddings in a BERT-like model. Along this line a promising
    way to transfer relational knowledge from a graph to a language model is proposed
    by GraphFormers.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 文本训练数据所涵盖的知识可以通过各种方式被利用来提高PLMs的性能。知识库中的实体和关系可以通过嵌入表示，例如通过TransE。然而，这些嵌入在PLMs中的应用效率不高且容易出错。一个更有前景的替代方案是专门PLMs直接使用表格内容或知识库关系，通过特定的自注意力模式捕获实体和表格单元格之间的关系。类似于Graph-CNNs，PLMs可以直接用于通过在BERT-like模型中用嵌入编码链接特征来获取图节点之间的关系。沿着这条线，GraphFormers提出了一种将关系知识从图转移到语言模型的有前景的方法。
- en: A very simple and efficient approach of incorporating tables and knowledge bases
    in PLMs is the creation of text that expresses the information content. This can
    be used by the PLM either as conditioning text or during training. However, the
    most promising way to include knowledge is *retrieval*, since most information
    is stored in the form of unstructured text on the Web or databases. Here, the
    retriever-reader architecture emerged as an effective way to collect relevant
    passages. Subsequently, the PLM generates new text by combining the internal knowledge,
    the start text, and the retrieved passages.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在PLM（预训练语言模型）中包含表格和知识库的一个非常简单且有效的方法是创建表达信息内容的文本。这可以被PLM用作条件文本或在训练期间使用。然而，包含知识最有前途的方法是*检索*，因为大多数信息以非结构化文本的形式存储在互联网或数据库上。在这里，检索器-阅读器架构作为一种有效的方式来收集相关段落。随后，PLM通过结合内部知识、起始文本和检索到的段落来生成新的文本。
- en: Much effort was devoted to the extension of the length of input sequences (Sect.
    [3.2](#Sec7)). This was mainly achieved by sparse attention patterns reducing
    the increase in computational effort from quadratic to linear with S4 as a leading
    approach. Nevertheless, larger input sequences still have limited range of context
    both within the same sample and outside of it.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 大量努力被投入到扩展输入序列长度（第3.2节）。这主要通过稀疏注意力模式实现，将计算努力的增加从二次减少到线性，S4作为主要方法。尽管如此，较大的输入序列在相同样本内及其外部仍然具有有限的上下文范围。
- en: In contrast, retrieval can cover an indefinite context within the same sample
    by gathering appropriate passages, even if there is no simultaneous attention
    over the whole context. In addition, retrieval can access relevant information
    in huge document collections. Either the highly developed traditional keyword
    search engines may be used. Alternatively dense retrieval may be employed which
    compares embeddings of the query and passages using approximate nearest neighbor
    search over an index. It turns out that relatively small retrieval-based models
    outperform large Foundation Models like GPT-3\. FiD, for example, achieves an
    exact match accuracy of 51.4% on the Natural Questions benchmark compared to 29.9%
    for GPT-3\. Retrieval is extensively used by recent models such as WebGPT and
    Retro.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，检索可以在同一样本内覆盖一个不确定的上下文，通过收集适当的段落，即使在整个上下文中没有同时的关注。此外，检索可以访问大量文档集中的相关信息。可以使用高度发展的传统关键词搜索引擎，或者采用密集检索，它通过在索引上使用近似最近邻搜索来比较查询和段落的嵌入。结果发现，相对较小的基于检索的模型在性能上优于大型基础模型，如GPT-3。例如，FiD在自然问题基准测试中实现了51.4%的精确匹配准确率，而GPT-3为29.9%。检索被最近的一些模型如WebGPT和Retro广泛使用。
- en: 3.5 Changing Model Size
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 改变模型大小
- en: The size of a model, especially its number of parameters, has a marked influence
    on the performance of the model, its memory requirements and the computational
    resources required for training. In the first section we discuss that models with
    more parameters potentially have a better performance. This, however, requires
    a larger computational effort during training and model utilization. An alternative
    are mixture-of-experts models, which define a number of parallel model structures
    which selectively compute a solution. This is described in the second section.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的大小，尤其是其参数数量，对模型性能、内存需求和训练所需的计算资源有显著影响。在第一部分中，我们讨论了具有更多参数的模型可能具有更好的性能。然而，这需要在训练和模型利用过程中投入更大的计算努力。另一种选择是专家混合模型，它定义了多个并行模型结构，这些结构选择性地计算解决方案。这将在第二部分中描述。
- en: As initial versions of successful models often are extremely large, a variety
    of model compression and acceleration techniques have been developed. They reduce
    memory requirements and training time without noticeable degradation of accuracy,
    and allow the models to be deployed on low resource computing devices, such as
    cell phones. There are three main techniques for model size reduction [[65](#CR65)]—parameter
    compression and reduction, low-rank factorization, and knowledge distillation—which
    are outlined in the subsequent sections.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 由于成功的模型初始版本通常非常大，因此已经开发出各种模型压缩和加速技术。它们减少了内存需求和训练时间，而没有明显降低准确度，并允许模型在低资源计算设备上部署，例如手机。模型尺寸减少有三种主要技术[[65](#CR65)]——参数压缩和减少、低秩分解和知识蒸馏，这些将在后续章节中概述。
- en: 3.5.1 Larger Models Usually Have a better Performance
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 较大的模型通常具有更好的性能
- en: 'As a rule for machine learning, the number of parameters of a model should
    be limited to avoid *overfitting*, i.e. adapting to random fluctuations in the
    data. It turned out that this does not hold for PLMs if the amount of training
    data and the number of model parameters are increased simultaneously. Larger PLMs
    have been shown to have better performance on NLP tasks, which is underscored
    by theoretical work on PLMs [[19](#CR19), p. 117]. The benefits of increasing
    the number of parameters come from two factors: additional computations at training
    and inference time, and increased memorization of the training data. Kaplan et
    al. [[102](#CR102)] empirically investigated in detail the dependency between
    the number of model parameters *R* (excluding embeddings), the size *N* of the
    training data, and the amount of computing effort *C* used for training. They
    evaluated a large number of models and draw the following conclusions:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 作为机器学习的一个规则，模型的参数数量应该限制在一定的范围内，以避免**过拟合**，即适应数据中的随机波动。结果发现，如果同时增加训练数据和模型参数的数量，这个规则对PLM不适用。研究表明，较大的PLM在NLP任务上具有更好的性能，这得到了PLM理论工作的强调[[19](#CR19)，p.
    117]。增加参数数量的好处来自两个因素：训练和推理时的额外计算，以及训练数据的增加记忆。Kaplan等人[[102](#CR102)]详细地实证研究了模型参数数量**R**（不包括嵌入层）、训练数据的大小**N**以及用于训练的计算努力**C**之间的依赖关系。他们评估了大量的模型，并得出以下结论：
- en: The performance of the models depends largely on the size quantities *R*, *N*,
    *C*. Other architectural features such as width or depth have only a weak influence.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的性能在很大程度上取决于大小量**R**、**N**、**C**。其他如宽度或深度等架构特征只有轻微的影响。
- en: The performance follows a smooth power-law dependency with each of *R*, *N*,
    *C*, if the other quantities are not too small. As an example the loss is approximately
    *L* ≈ (*N*∕(5.4 ∗ 10^(13)))^(−0.095).
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果其他量不是太小，性能遵循与**R**、**N**、**C**中的每一个的平滑幂律依赖关系。例如，损失大约是*L*≈(N/(5.4×10^13))^(−0.095)。
- en: If *R* and *N* are increased at the same rate, the model accuracy grows reliably.
    If one of these factors is held constant the improvement gets lower. To get the
    best performance, the model size *R* should grow with the factor 8, if the data
    *N* is increased 5 times.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果**R**和**N**以相同的速率增加，模型的准确性会可靠地增长。如果其中一个因素保持不变，改进的效果会降低。为了获得最佳性能，当数据**N**增加5倍时，模型大小**R**应该以8倍的增长。
- en: Training loss has a predictable dependency on computing effort and can be extrapolated.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练损失与计算努力有可预测的依赖关系，并且可以外推。
- en: The performance of fine-tuning of a pre-trained model on a different training
    task depends strongly on the loss for the pre-training validation set. Therefore,
    transfer to a different distribution induces a constant penalty, but roughly improves
    with the performance on the pre-training set.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同的训练任务上对预训练模型进行微调的性能在很大程度上取决于预训练验证集的损失。因此，转移到不同的分布会引入一个恒定的惩罚，但大致会随着预训练集的性能而提高。
- en: Large models are better able to extract information from data than small models.
    They reach the same level of accuracy with fewer optimization steps and using
    fewer data points. If there is only a fixed amount of computation time, but no
    restrictions on size or data, one should use very large models and stop before
    convergence (Fig. [3.19](#Fig19)). The optimal batch size depends on the *gradient
    noise*, which is easy to measure during training [[132](#CR132)] and is larger
    than assumed before.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig19_HTML.png)
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型模型比小型模型更能从数据中提取信息。它们以更少的优化步骤和更少的数据点达到相同的准确性水平。如果只有固定的计算时间，但没有对大小或数据的限制，应该使用非常大的模型，并在收敛之前停止（图[3.19](#Fig19)）。最佳的批量大小取决于**梯度噪声**，这在训练期间很容易测量[[132](#CR132)]，并且比之前假设的要大！[](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig19_HTML.png)
- en: 2 line graphs plot test loss versus tokens processed and compute budget. Both
    graphs denote the decreasing trend of multiple lines indicating the number of
    parameters range from 10 power 3 10 power 6 and 10 power 9\. It indicates that
    compute-efficient training stops far short of convergence.
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两条线图分别绘制了测试损失与处理令牌数和计算预算的关系。两个图都表示多条线的下降趋势，表示参数数量范围从10的3次方到10的6次方和10的9次方。它表明计算高效的训练远远达不到收敛。
- en: Fig. 3.19
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.19
- en: A series of language model training runs with varying model sizes [[102](#CR102)].
    The left graph shows that larger models require fewer samples to reach a fixed
    test loss. The right graph demonstrates that the model size should grow with compute
    budget. Image reprinted with kind permission of the authors [[102](#CR102), p.
    4]
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一系列不同模型大小的语言模型训练运行 [[102](#CR102)]。左图显示，较大的模型需要更少的样本才能达到固定的测试损失。右图表明，模型大小应随着计算预算的增长而增长。图片经作者同意重印
    [[102](#CR102)，第4页]。
- en: These findings show that the success of larger PLMs is a systematic feature.
    A larger number of model parameters is much more sample efficient than thought
    before, when overfitting was a major problem for smaller training tasks. This
    also explains the success of large models like T5, BigBird, or GPT-3\. Hernandez
    et al. [[80](#CR80)] investigate empirical scaling laws for the transfer from
    pre-training to fine-tuning. Figure [3.20](#Fig20) plots the training efforts
    of some Deep Learning models during the last two decades.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig20_HTML.png)
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现表明，较大PLM的成功是一个系统特征。模型参数数量比之前认为的更有效率，当过拟合是较小训练任务的主要问题时。这也解释了T5、BigBird或GPT-3等大型模型的成功。Hernandez等人
    [[80](#CR80)] 研究了从预训练到微调的实证缩放定律。图[3.20](#Fig20)绘制了过去二十年一些深度学习模型的训练努力！[](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig20_HTML.png)
- en: A scatter plot of parameters versus dates represents the distribution of 11
    different domains between 2018 and 2022\. It indicates the evolution of transformer,
    BERT, G P T 2, G P T 3, gopher, PaLM, and DALL E 2, across the durations.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 参数与日期的散点图表示了2018年至2022年之间11个不同领域的分布。它表明了transformer、BERT、GPT 2、GPT 3、gopher、PaLM和DALL
    E 2在持续时间内的演变。
- en: Fig. 3.20
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.20
- en: Number of parameters for Deep Learning Models since 2017 [[188](#CR188)]. Note
    that the parameter scale is logarithmic. The number of parameters roughly increased
    from 100M up to 1000B
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 自2017年以来深度学习模型的参数数量 [[188](#CR188)]。请注意，参数尺度是对数的。参数数量大致从1亿增加到1000亿。
- en: 3.5.2 Mixture-of-Experts Models
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 专家混合模型
- en: As discussed above a model with more parameters usually can achieve a better
    performance. A simple way to increase the number of parameters without a higher
    training effort is a **mixture-of-experts** architecture. It was already proposed
    in the nineties by Nowlan et al. [[147](#CR147)] and has a strong resemblance
    to decision tree models [[152](#CR152)]. It consists of a single gating module
    and a number of expert modules with identical architecture but different parameters.
    Each expert specializes in only a subset of the data, and the gating module assigns
    each input to the appropriate experts. Specifically, the gating network computes
    a probability distribution over the experts indicating how well each expert is
    able to process the incoming input. A reduction in computational effort can be
    achieved, if only a few expert modules are actually used. The model is trained
    by stochastic gradient descent, which can compute the parameter gradient despite
    the discontinuities if some expert is exchanged. Increasing the number of experts
    keeps the computational cost constant because the model always selects the same
    small number of experts for each input, regardless of the total number of experts.
    The architecture enables massive models and is particularly efficient for distributed
    systems where the experts are spread across different computational devices.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，具有更多参数的模型通常可以取得更好的性能。在不增加更高训练努力的情况下增加参数数量的简单方法是**专家混合**架构。它早在九十年代由Nowlan等人提出
    [[147](#CR147)]，并且与决策树模型[[152](#CR152)]有很强的相似性。它由一个门控模块和多个具有相同架构但参数不同的专家模块组成。每个专家只专注于数据的一个子集，门控模块将每个输入分配给适当的专家。具体来说，门控网络计算一个概率分布，表示每个专家处理输入的能力。如果只使用少数专家模块，可以减少计算工作量。该模型通过随机梯度下降进行训练，即使某些专家被替换，也可以计算参数梯度。增加专家数量保持计算成本恒定，因为模型总是为每个输入选择相同的小数量专家，无论专家总数是多少。该架构允许构建大规模模型，并且对于专家分布在不同的计算设备上的分布式系统特别有效。
- en: Clark et al. [[38](#CR38)] analyze the theoretical properties of such *routing
    networks*, where each input is processed only by subnetworks with a fraction of
    the network’s parameters.The authors analyze three different architectures and
    get the following results.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: Clark 等人 [[38](#CR38)] 分析了此类 *路由网络* 的理论特性，其中每个输入仅由具有网络参数一部分的子网络进行处理。作者分析了三种不同的架构，并得到了以下结果。
- en: Routing improves the performance of PLMs in all investigated sizes and variants.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路由提高了所有研究尺寸和变体中 PLM 的性能。
- en: Improvement follows a power-law in the number of experts *E* that diminishes
    with model size *N*, and can be further generalized across routing architectures.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进随着专家数量 *E* 的增加而遵循幂律，该数量随着模型大小 *N* 的增加而减少，并且可以进一步推广到路由架构中。
- en: The analysis is based on the evaluation of several magnitudes of size, including
    models with hundreds of experts and hundreds of billions of parameters.**GLaM**
    [[51](#CR51)] is an autoregressive *mixture-of-experts* (*MoE*) model with up
    to 1200B parameters. It replaces the fully connected layer of every second encoder
    block (Sect. [2.​1.​1](528393_1_En_2_Chapter.xhtml#Sec2)) with 64 copies having
    different parameters. For each embedding, a gating module selects two of these
    64 fully connected layer for processing. The architecture is shown in Fig. [3.21](#Fig21).
    The model was trained on a huge collection of 1.6T tokens documents and quality-checked
    web pages. It has approximately 7 times more parameters than GPT-3 but requires
    only 1/3 of its training effort. In this way, the model has many more parameters
    increasing its representational capacity. As for a given input token, only two
    expert models are used, the computational effort for training and application
    is lower. The zero-shot and one-shot performance is better than for GPT-3 on 29
    NLP tasks. Some results are compared to those of other models in Tables [3.3](#Tab3)
    and [3.4](#Tab4). GLaM is remarkable as it requires only 1/3 of the training effort
    of GPT-3 but it achieves a similar or better performance than GPT-3 on NLP tasks.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig21_HTML.png)
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 该分析基于对多个尺寸级别的评估，包括拥有数百名专家和数百亿个参数的模型。**GLaM** [[51](#CR51)] 是一个具有高达 1200B 参数的自回归
    *专家混合* (*MoE*) 模型。它将每个第二个编码器块（第 [2.1.1](528393_1_En_2_Chapter.xhtml#Sec2) 节）的全连接层替换为具有不同参数的
    64 个副本。对于每个嵌入，一个门控模块选择其中的两个 64 个全连接层进行处理。该架构如图 [3.21](#Fig21) 所示。该模型在包含 1.6T 个标记文档和经过质量检查的网页的巨大集合上进行了训练。它的参数数量大约是
    GPT-3 的 7 倍，但所需的训练努力只有 GPT-3 的 1/3。通过这种方式，模型具有更多的参数，从而增加了其表示能力。对于给定的输入标记，仅使用两个专家模型，从而降低了训练和应用的计算工作量。在
    29 个 NLP 任务上的零样本和单样本性能优于 GPT-3。一些结果与其他模型的结果在表 [3.3](#Tab3) 和 [3.4](#Tab4) 中进行了比较。GLaM
    非凡之处在于它只需 GPT-3 的 1/3 的训练努力，但在 NLP 任务上实现了与 GPT-3 相似或更好的性能。![图 3.21](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig21_HTML.png)
- en: An illustration of the architecture of G L A M. It indicates the layers of input
    and position embeddings, feed-forward, and output embedding, along with the residual
    connections.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: G L A M 架构的示意图。它显示了输入和位置嵌入层、前馈和输出嵌入层，以及残差连接。
- en: Fig. 3.21
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.21
- en: Architecture of GLaM [[51](#CR51)]. For each input token, e.g., *“likes”*, the
    gating module dynamically selects two most relevant experts out of 64 available
    experts. This is indicated by the blue grid. The weighted average of the outputs
    from these two experts’ feedforward models is then passed to the next encoder
    block. For the other inputs different experts are selected. A mixture-of-experts
    layer is used in every second encoder block
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: GLaM 架构 [[51](#CR51)]。对于每个输入标记，例如 *“likes”*，门控模块会动态地从 64 个可用的专家中选择两个最相关的专家。这由蓝色网格表示。然后，这两个专家的前馈模型输出的加权平均值传递到下一个编码器块。对于其他输入，会选择不同的专家。在每个第二个编码器块中使用专家混合层。
- en: '**WuDao-2.0** [[175](#CR175), [178](#CR178), [257](#CR257)] is a recent giant
    autoregressive language model with 1750B parameters, ten times larger than GPT-3\.
    It has *mixture-of-experts* layers, where a gating network selects a submodule
    for processing based on the input. WuDao-2.0 uses the *FastMoE* library [[74](#CR74)]
    and employs the GLM 2.0 architecture (Sect. [3.1.3](#Sec4)) combining the different
    learning paradigms of BERT, GPT and the encoder-decoder transformer [[175](#CR175)].'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '**WuDao-2.0** [[175](#CR175), [178](#CR178), [257](#CR257)] 是一个最近的大型自回归语言模型，拥有1750B参数，是GPT-3的十倍。它具有*混合专家*层，其中门控网络根据输入选择处理子模块。WuDao-2.0使用*FastMoE*库[[74](#CR74)]，并采用GLM
    2.0架构（见第[3.1.3](#Sec4)节），结合BERT、GPT和编码器-解码器变换器的不同学习范式[[175](#CR175)]。'
- en: The training data consist of 1.2TB Chinese text, 2.5TB Chinese graphic data
    and 1.2TB English text data from the *Pile* corpus [[61](#CR61)]. The *Cogview*
    model is used for the joint processing of images Sect. [7.​2](528393_1_En_7_Chapter.xhtml#Sec12).
    In addition, WuDao-2.0 can learn on the fly, draw pictures and compose poetry.
    These capabilities are a significant difference to GPT-3.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据包括来自 *Pile* 语料库的1.2TB中文文本，2.5TB中文图形数据和1.2TB英文文本数据。*Cogview* 模型用于图像的联合处理，见第[7.2](528393_1_En_7_Chapter.xhtml#Sec12)节。此外，WuDao-2.0可以边学边用，绘图和作诗。这些功能与GPT-3相比具有显著差异。
- en: The published performance claims are impressive. On the LAMA benchmark for measuring
    world knowledge [[158](#CR158)] it scores higher than AutoPrompt [[192](#CR192)].
    For the *SuperGLUE* few-shot natural language understanding task [[219](#CR219)]
    it achieves Sota and surpasses GPT-3\. For the Lambada benchmark (Sect. [4.​1.​3](528393_1_En_4_Chapter.xhtml#Sec4)),
    where the last word of a paragraph has to be predicted, it yields better results
    than Microsoft Turing NLG. In addition, it increases Sota for a number of text-graphics
    tasks (Sect. [7.​2.​8](528393_1_En_7_Chapter.xhtml#Sec20)).
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 公布的性能指标令人印象深刻。在衡量世界知识的LAMA基准测试[[158](#CR158)]中，其得分高于AutoPrompt [[192](#CR192)]。对于*SuperGLUE*少样本自然语言理解任务[[219](#CR219)]，它实现了Sota并超越了GPT-3。在Lambada基准测试（见第[4.1.3](528393_1_En_4_Chapter.xhtml#Sec4)节），其中需要预测段落最后一词的情况下，其结果优于微软的Turing
    NLG。此外，它还在多个文本-图形任务（见第[7.2.8](528393_1_En_7_Chapter.xhtml#Sec20)节）上提高了Sota。
- en: '**Switch** [[56](#CR56)] is a variant of the transformer encoder-decoder T5
    (Sect. [3.1.3](#Sec4)). It has a *mixture-of-experts* architecture, which replaces
    the fully connected layer of each encoder block with *k* = 128 copies having different
    parameters. There is a simple linear gating network, which selects one of the
    128 single fully connected layers (the experts) per token. Hence, the number of
    parameters is drastically increased with approximately constant computational
    effort. For this architecture a gradient can be computed and the model may be
    optimized using a number of specific strategies and a special TensorFlow version.
    It turns out that Switch achieves the same loss level compared to the standard
    T5 version with 1/7 of the computing time. On a number of fine-tuning tasks the
    large Switch model with 1600B parameters and 2048 experts yields better results
    than T5-large (Sect. [3.1.3](#Sec4)) with 13B parameters requiring a quarter of
    the computational training effort.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '**Switch** [[56](#CR56)] 是transformer编码器-解码器T5（见第[3.1.3](#Sec4)节）的一个变体。它具有*混合专家*架构，其中用*k*
    = 128个具有不同参数的副本替换了每个编码器块的完全连接层。有一个简单的线性门控网络，为每个令牌选择128个单完全连接层（专家）中的一个。因此，参数数量在计算努力基本恒定的情况下大幅增加。对于这种架构，可以计算梯度，并且可以使用多种特定策略和特殊的TensorFlow版本来优化模型。结果表明，与标准T5版本相比，Switch在1/7的计算时间内达到了相同的损失水平。在许多微调任务中，具有1600B参数和2048个专家的大型Switch模型比具有13B参数的T5-large（见第[3.1.3](#Sec4)节）在计算训练努力上少四分之一，但结果更好。'
- en: As an alternative to the gating network in the mixtures-of-experts architecture,
    it is possible to use hash values to activate different parts of the network.
    **Token Switch** [[177](#CR177)] computes a hash value for each input token and
    routes the generated embeddings of each token to different feedforward networks
    based on the hash values. The authors show that their approach compares favorable
    to Switch and works well on comprehensive language modeling tasks.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 作为混合专家架构中门控网络的替代方案，可以使用哈希值来激活网络的不同部分。**Token Switch** [[177](#CR177)] 为每个输入令牌计算一个哈希值，并根据哈希值将每个令牌生成的嵌入路由到不同的前馈网络。作者表明，他们的方法与Switch相比具有优势，并且在综合语言建模任务上表现良好。
- en: '**ST-MoE-32B** [[261](#CR261)] is a mixture-of-experts model with 269B parameters
    and a comparable training cost of a 32B dense model. The authors modify the routing
    algorithm which dispatches token embeddings to one or two experts, and resolve
    instability issues. The model is similar to a T5-Large encoder-decoder [[170](#CR170)].
    The ST-MoE-32B has 32 experts with an expert layer frequency of 1/4, such that
    every fourth feedforward layer of T5 is replaced by an MoE layer. The authors
    use the *GEGLU* activation function, which contains multiplicative elements [[142](#CR142)]![$$\displaystyle
    \begin{aligned} FFN_{GEGLU}({\boldsymbol{x}},W,V,{\boldsymbol{b}},\boldsymbol{c})
    = GELU({\boldsymbol{x}} W+{\boldsymbol{b}})\odot ({\boldsymbol{x}} V+\boldsymbol{c}).
    \end{aligned} $$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_Equ2.png)(3.2)The
    authors compare a large number of variants and hyperparameters to improve training.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '**ST-MoE-32B** [[261](#CR261)] 是一个具有269B参数的专家混合模型，其训练成本与32B密集模型相当。作者修改了路由算法，该算法将标记嵌入分配给一个或两个专家，并解决了不稳定性问题。该模型类似于T5-Large编码器-解码器
    [[170](#CR170)]。ST-MoE-32B有32个专家，专家层频率为1/4，这意味着T5的每第四个前馈层被MoE层所取代。作者使用了*GEGLU*激活函数，其中包含乘性元素
    [[142](#CR142)]![$$\displaystyle \begin{aligned} FFN_{GEGLU}({\boldsymbol{x}},W,V,{\boldsymbol{b}},\boldsymbol{c})
    = GELU({\boldsymbol{x}} W+{\boldsymbol{b}})\odot ({\boldsymbol{x}} V+\boldsymbol{c}).
    \end{aligned} $$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_Equ2.png)(3.2)作者比较了大量的变体和超参数以改进训练。'
- en: The model achieves Sota in many transfer learning benchmarks, e.g. for SuperGLUE
    with an average accuracy of 93.2% beating the PaLM LM with 540B parameters. Other
    Sota results were reached for summarization (XSum [[143](#CR143)] with 27.1 Rouge-2,
    CNN/Daily Mail [[78](#CR78)] with 21.7 Rouge-2), closed book question answering
    (WebQA [[13](#CR13)] 47.4% exact match, Natural Questions [[109](#CR109)] 41.9%
    exact match), and adversarially constructed tasks for common sense reasoning (Winogrande
    [[182](#CR182)] 96.6%, ANLI R3 [[146](#CR146)] 74.4%).
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在许多迁移学习基准测试中实现了Sota，例如，对于SuperGLUE，平均准确率为93.2%，击败了具有540B参数的PaLM LM。其他Sota结果包括摘要（XSum
    [[143](#CR143)] Rouge-2得分为27.1，CNN/Daily Mail [[78](#CR78)] Rouge-2得分为21.7），闭卷问答（WebQA
    [[13](#CR13)] 准确匹配率为47.4%，Natural Questions [[109](#CR109)] 准确匹配率为41.9%），以及用于常识推理的对抗性构建任务（Winogrande
    [[182](#CR182)] 96.6%，ANLI R3 [[146](#CR146)] 74.4%）。
- en: 3.5.3 Parameter Compression and Reduction
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.3 参数压缩和减少
- en: '*Model quantization* is a parameter reduction technique, where parameters are
    stored in low precision and therefore the computations in PLMs are also less precise.
    Conventional models normally use parameters of 32 bits or 16 bits, while parameters
    after quantization can have 8 bits or even 1 or 2 bits. **Q-BERT** [[190](#CR190)],
    for example, quantizes Transformer models to ultra-low precision. This reduces
    the model size 13-fold while only loosing 2.3% performance. The authors avoid
    the naive approach of simply reducing weight precision, but use additional training
    steps to adjust the quantized weights and allow higher precision for more “sensitive”
    parameters. Other authors propose to delete parameters with small values [[64](#CR64)].
    ALBERT [[113](#CR113)] uses the same weights across all layers and achieves a
    significant parameter reduction. Nevertheless, ALBERT has the same or better performance
    compared to BERT.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型量化*是一种参数减少技术，其中参数以低精度存储，因此PLM中的计算也更不精确。传统模型通常使用32位或16位的参数，而量化后的参数可以具有8位甚至1位或2位。例如，**Q-BERT**
    [[190](#CR190)] 将Transformer模型量化到超低精度。这使模型大小减少了13倍，同时只损失了2.3%的性能。作者避免了简单地降低权重精度的天真方法，而是使用额外的训练步骤来调整量化权重，并允许对“敏感”参数有更高的精度。其他作者提出了删除小值参数
    [[64](#CR64)] 的方法。ALBERT [[113](#CR113)] 在所有层中使用相同的权重，实现了显著的参数减少。尽管如此，ALBERT与BERT相比具有相同或更好的性能。'
- en: Another approach aims to reduce the number of parameters, e.g. by removing attention
    heads. It was shown that most attention heads focus only on nearly identical positional
    relations and can be replaced with fixed attention patterns [[172](#CR172)]. It
    turned out that high performance is possible with only 1–2 attention heads per
    encoder unit instead of the 16 attention heads of the original model. A detailed
    overview on parameter compression techniques is provided by Ganesh et al. [[60](#CR60)]
    .
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法旨在减少参数数量，例如通过移除注意力头。研究表明，大多数注意力头只关注几乎相同的位置关系，可以用固定的注意力模式来替换 [[172](#CR172)]。结果表明，每个编码单元只需要1-2个注意力头而不是原始模型的16个注意力头，也能实现高性能。Ganesh等人
    [[60](#CR60)] 提供了关于参数压缩技术的详细概述。
- en: Another method to reduce model parameters is model pruning, which cuts off irrelevant
    parts in PLMs to achieve a smaller memory footprint and faster execution without
    compromising performance. It could be shown, for example that some attention heads
    of the transformer may be removed with little impact on the accuracy [[256](#CR256)].
    Other researchers prune the weights of attention layers and linear layers to reduce
    the number of parameters without reducing the accuracy [[29](#CR29), [64](#CR64)].
    Note that model pruning does not always lead to speedups, as sparse computations
    may be hard to parallelize on GPUs.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 减少模型参数的另一种方法是模型剪枝，它通过在PLMs中剪掉无关部分来实现更小的内存占用和更快的执行速度，同时不牺牲性能。例如，可以证明，Transformer的一些注意力头可以移除而不会对准确性产生太大影响
    [[256](#CR256)]。其他研究人员通过剪枝注意力层和线性层的权重来减少参数数量，而不会降低准确性 [[29](#CR29), [64](#CR64)]。请注意，模型剪枝并不总是导致加速，因为稀疏计算可能在GPU上难以并行化。
- en: 3.5.4 Low-Rank Factorization
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.4 低秩分解
- en: This technique employs matrix and tensor decomposition to reduce the number
    of parameters of full rank parameter matrices and already has been discussed in
    Sect. [3.2.2](#Sec9) for the extension of the input sequence length. Examples
    are the Performer [[34](#CR34)] and the Linear Transformer [[105](#CR105)] (Sect.
    [3.2.2](#Sec9)). As an alternative, ALBERT (Sect. [3.1.1](#Sec2)) approximates
    the embedding matrix as a product of two smaller matrices.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术采用矩阵和张量分解来减少满秩参数矩阵的参数数量，并且已经在第 [3.2.2](#Sec9) 节中讨论了其扩展输入序列长度的应用。例如，有 Performer
    [[34](#CR34)] 和 Linear Transformer [[105](#CR105)]（第 [3.2.2](#Sec9) 节）。作为替代方案，ALBERT（第
    [3.1.1](#Sec2) 节）将嵌入矩阵近似为两个较小矩阵的乘积。
- en: 3.5.5 Knowledge Distillation
  id: totrans-431
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.5 知识蒸馏
- en: In machine learning the knowledge distillation approach [[82](#CR82)] transfers
    knowledge from a large *teacher model* to a smaller *student model*. The large
    model can often be trained successfully to approximate a functional relation without
    using its full representational capacity. To reduce the high computational and
    memory requirements during application, a smaller model is trained to imitate
    the large model without sacrificing accuracy.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，知识蒸馏方法 [[82](#CR82)] 将知识从大的 *教师模型* 转移到小的 *学生模型*。大模型通常可以成功训练以近似一个功能关系，而不使用其全部表示能力。为了在应用期间降低高计算和内存需求，训练一个较小的模型来模仿大模型，同时不牺牲准确性。
- en: The advantage of this approach is that the student model may be trained to approximate
    *internal activations* of the teacher model. Often the target probabilities generated
    by the teacher model are used to train the student network . Typically the outputs
    of the teacher model for an input ***x*** is *z*(***x***), which can be translated
    to a probability by a scaled softmax![$$\displaystyle \begin{aligned} {\boldsymbol{y}}(x|\tau)
    = \frac{[\exp(z_1({\boldsymbol{x}})/\tau),\ldots,\exp(z_k({\boldsymbol{x}}))/\tau]}
    {\exp(z_1({\boldsymbol{x}})/\tau)+\cdots+\exp(z_k({\boldsymbol{x}})/\tau)} , \end{aligned}
    $$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_Equ3.png)(3.3)where
    ***y***(*x*|*τ*) is a probability vector and *τ* is a parameter called *temperature*,
    which for a standard softmax is normally set to 1.0\. The student model is trained
    to imitate the probabilities ![$$\hat {{\boldsymbol {y}}}(x|\tau )$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq32.png)
    generated by the teacher model by minimizing *cross entropy*![$$\displaystyle
    \begin{aligned} E({\boldsymbol{y}}|\tau) = - \sum_{j=1}^k \hat{y}_j(x|\tau)\log
    y_j(x|\tau), \end{aligned} $$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_Equ4.png)(3.4)where
    ***y***(*x*|*τ*) is the output probability vector of the student model. If observed
    values are available the probabilities of the teacher model *y*[*j*](*x*|*τ*)
    may be replaced by 1.0 for the observed class and 0.0 otherwise. During training
    the temperature may be varied. A high temperature avoids extreme probability values
    and reduces the gradients. This may lead to a faster convergence in the beginning
    of the optimization.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优势在于，学生模型可以被训练来近似教师模型的**内部激活**。通常，教师模型生成的目标概率被用来训练学生网络。通常，教师模型对于输入 ***x***
    的输出是 *z*(***x***)，这可以通过缩放 softmax 转换为概率！[$$\displaystyle \begin{aligned} {\boldsymbol{y}}(x|\tau)
    = \frac{[\exp(z_1({\boldsymbol{x}})/\tau),\ldots,\exp(z_k({\boldsymbol{x}}))/\tau]}
    {\exp(z_1({\boldsymbol{x}})/\tau)+\cdots+\exp(z_k({\boldsymbol{x}})/\tau)} , \end{aligned}
    $$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_Equ3.png)(3.3)其中
    ***y***(*x*|*τ*) 是一个概率向量，*τ* 是一个称为 *温度* 的参数，对于标准的 softmax，通常设置为 1.0。学生模型被训练来通过最小化
    *交叉熵*![$$\displaystyle \begin{aligned} E({\boldsymbol{y}}|\tau) = - \sum_{j=1}^k
    \hat{y}_j(x|\tau)\log y_j(x|\tau), \end{aligned} $$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_Equ4.png)(3.4)来模仿教师模型生成的概率
    ![$$\hat {{\boldsymbol {y}}}(x|\tau )$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq32.png)。如果观察到值，教师模型的概率
    *y*[*j*](*x*|*τ*) 可以用 1.0 替换观察到的类别，否则用 0.0。在训练过程中，温度可能会变化。高温度可以避免极端的概率值并减少梯度。这可能会导致优化开始时的收敛速度更快。
- en: '**DistilBERT** [[183](#CR183)] uses MLM cross-entropy loss to predict token
    probabilities and in addition the cosine similarity between the embedding matrices
    of the teacher and student networks to train a smaller BERT model. It utilizes
    knowledge distillation during pre-training to reduce the size of BERT by 40% while
    retaining 99% of its original capabilities and making the inference 60% faster.
    *MobileBERT* [[204](#CR204)] is based on a specific large BERT model and transfers
    information about multi-head-attention as well as the resulting embeddings. Experiments
    show that MobileBERT is 4.3× smaller and 5.5× faster than BERT while achieving
    competitive results on well-known benchmarks.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '**DistilBERT** [[183](#CR183)] 使用 MLM 交叉熵损失来预测标记概率，并且还使用教师和学生网络嵌入矩阵之间的余弦相似度来训练一个更小的
    BERT 模型。它在预训练期间利用知识蒸馏来将 BERT 的大小减少 40%，同时保留其原始能力的 99%，并使推理速度提高 60%。*MobileBERT*
    [[204](#CR204)] 基于特定的一个大 BERT 模型，并转移了关于多头注意力以及由此产生的嵌入的信息。实验表明，MobileBERT 比BERT
    小 4.3 倍，快 5.5 倍，同时在知名基准测试中取得了具有竞争力的结果。'
- en: '**TinyBERT** [[97](#CR97)] proposes distillation of a BERT model during pre-training
    and fine-tuning. The model is adapted to: (1) the output of the embedding of selected
    layers; (2) the hidden states and attention matrices derived from selected Transformer
    layers; (3) the logit outputs of the prediction layer. As distillation is also
    performed during fine-tuning the model can be better adapted to the fine-tuned
    BERT. On a number of benchmarks TinyBERT is on par with BERT[BASE] and outperforms
    DistilBERT.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '**TinyBERT** [[97](#CR97)] 提出在预训练和微调期间对 BERT 模型进行蒸馏。该模型被调整为：(1) 选择层嵌入的输出；(2)
    从选择的 Transformer 层导出的隐藏状态和注意力矩阵；(3) 预测层的 logit 输出。由于在微调期间也进行了蒸馏，因此模型可以更好地适应微调后的
    BERT。在多个基准测试中，TinyBERT 与 BERT[BASE] 相当，并优于 DistilBERT。'
- en: Note that the knowledge distillation methods discussed above require the data
    used for pre-training the teacher model, which is often not released because of
    data copyright. It has not yet been evaluated whether distillation is also feasible
    with new data. The training time for knowledge distillation is high, because the
    teacher model needs to perform a forward prediction over the entire pre-training
    data to generate activation values or intermediate representations.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，上述讨论的知识蒸馏方法需要用于预训练教师模型的数据，这些数据通常由于数据版权而未发布。尚未评估使用新数据是否也能进行蒸馏。知识蒸馏的训练时间很高，因为教师模型需要对整个预训练数据进行正向预测以生成激活值或中间表示。
- en: Rogers et al. [[176](#CR176)] list a large number of size reduction studies
    for BERT and report parameter size and computing time reduction as well as the
    resulting performance. For a number of approaches there is a marked reduction
    in memory and computing effort with nearly identical performance.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: Rogers等人[[176](#CR176)]列出了一系列BERT的尺寸缩减研究，并报告了参数大小和计算时间的减少以及由此产生的性能。对于许多方法，内存和计算工作量显著减少，性能几乎相同。
- en: 3.5.6 Summary
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.6 总结
- en: The number of model parameters, the size of the training data and the amount
    of computation effort for training are the determining factors for the performance
    of a model. Kaplan et al. [[102](#CR102)] show by experiments that increasing
    parameter count and training set size reliably lead to a better performance and
    provide a detailed formula for the dependency. If a fixed compute budget is available,
    one should use a very large model and much data.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 模型参数的数量、训练数据的大小以及训练所需的计算工作量是决定模型性能的关键因素。Kaplan等人[[102](#CR102)]通过实验表明，增加参数数量和训练集大小可以可靠地提高性能，并提供了详细的依赖关系公式。如果可用固定的计算预算，应使用一个非常大的模型和大量数据。
- en: Mixtures-of-experts follow this approach by increasing the number of parameters
    without requiring more computational effort. By routing inputs to specific subnetworks
    they are able to increase performance compared to monolithic networks. Examples
    are GLaM, WuDao-2.0, and Switch. However, these networks have hundreds of billions
    of parameters and require a specific parallel computational infrastructure.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 专家混合模型通过增加参数数量而不需要更多的计算工作量来遵循这种方法。通过将输入路由到特定的子网络，它们能够提高性能，与单一网络相比。例如，GLaM、WuDao-2.0和Switch就是这样的网络。然而，这些网络有数百亿个参数，需要特定的并行计算基础设施。
- en: Often the trained networks are too large and have to be reduced to fit to smaller
    computing devices. A viable approach is low-precision computation, which reduces
    memory requirements for parameter storing. Low-Rank factorization of matrices
    also has a lower memory footprint as a side effect. Finally, knowledge distillation
    may be employed to create a student model which imitates the inner working of
    a large trained teacher network. DistilBERT, for example, was able to reduce the
    memory size by 40%, kept 99% of the original performance and was 60% faster. There
    are a number of other size reduction approaches with similar results.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 通常训练好的网络太大，需要缩减以适应较小的计算设备。一种可行的方法是低精度计算，这可以减少参数存储的内存需求。矩阵的低秩分解也作为副作用具有更低的内存占用。最后，可以使用知识蒸馏来创建一个学生模型，该模型模仿大型训练教师网络的内部工作。例如，DistilBERT能够将内存大小减少40%，保持了99%的原有性能，并且速度提高了60%。还有许多其他尺寸缩减方法，效果相似。
- en: 3.6 Fine-Tuning for Specific Applications
  id: totrans-442
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 针对特定应用的微调
- en: Self-supervised pre-training of language models on large text collections and
    subsequent fine-tuning them to solve specific tasks has become the standard paradigm
    in natural language processing and understanding. It has been shown that pre-trained
    language models such as BERT are excellent for generalization and can easily be
    fine-tuned to multiple tasks. However, sometimes simple fine-tuning to a domain-specific
    task is not sufficient, and other transfer learning approaches have to be used
    to better adapt models to domain-shift in the data [[166](#CR166)]. There are
    a number of surveys covering transfer learning in depth [[230](#CR230), [252](#CR252),
    [260](#CR260)]
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型文本集合上对语言模型进行自监督预训练，然后对其进行微调以解决特定任务，已经成为自然语言处理和理解的标准化范式。已经证明，预训练语言模型如BERT在泛化方面非常出色，并且可以轻松地微调到多个任务。然而，有时简单的针对特定领域的微调并不足够，必须使用其他迁移学习方法来更好地适应数据中的领域变化[[166](#CR166)]。有多个综述深入探讨了迁移学习[[230](#CR230),
    [252](#CR252), [260](#CR260)]。
- en: 'Fine-tuning updates all the model layers, including the embedding layer, but
    there are larger changes in the higher layers [[133](#CR133)]. First, we discuss
    whether fine-tuning can destroy the knowledge gained during pre-training. *Standard
    fine-tuning* adapts a large pre-trained PLM with many parameters to a relatively
    small fine-tuning training data set with little computational effort. We investigate
    whether *overfitting* occurs during this phase. Subsequent sections introduce
    different approaches for fine-tuning:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 微调更新所有模型层，包括嵌入层，但在高层有更大的变化 [[133](#CR133)]。首先，我们讨论微调是否会破坏预训练期间获得的知识。*标准微调* 以较小的计算成本将具有许多参数的大型预训练
    PLM 适应到相对较小的微调训练数据集。我们研究在这个阶段是否会发生 *过拟合*。随后的章节介绍了不同的微调方法：
- en: '*Intermediate Fine-Tuning* performs an in-between fine-tuning step with a larger
    training set before a final target fine-tuning takes place.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*中间微调* 在最终目标微调之前，使用较大的训练集进行一次中间微调步骤。'
- en: '*Multitask fine-tuning* enhances the model capabilities by simultaneously fine-tuning
    on a number of tasks.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多任务微调* 通过同时在多个任务上进行微调来增强模型的能力。'
- en: '*Fine-tuning a frozen model* adapts a small additional layer to the fine-tuning
    task instead of changing all weights of the large pre-trained model.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*冻结模型微调* 通过添加一个小额外的层来适应微调任务，而不是改变大型预训练模型的所有权重。'
- en: '*Creating Prompts for Few-Shot Instructions* aims to generate inputs for a
    large autoregressive PLM like GPT-3 to solve a task in a zero or few-shot approach.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为少样本指令创建提示* 的目的是为大型自回归 PLM 如 GPT-3 生成输入，以零样本或少样本方法解决任务。'
- en: 3.6.1 Properties of Fine-Tuning
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.1 微调的性质
- en: 'Fine-tuning of PLMs is commonly employed to adapt a pre-trained model to a
    specific task by supervised training. This adaption of the model from a source
    task to a related target task is also called *transfer learning*. Transfer learning
    is especially rewarding if we have abundant training data for self-supervised
    learning—as it is typical for non-annotated text—and only little annotated data
    for the target task. A survey of transfer learning is provided by Zhuang et al.
    [[260](#CR260)]. Fine-tuning has a number of advantages:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: PLM 的微调通常用于通过监督训练将预训练模型适应到特定任务。这种从源任务到相关目标任务的模型适应也称为 *迁移学习*。如果我们在自监督学习中拥有丰富的训练数据（如非标注文本典型），而对于目标任务只有少量标注数据，迁移学习特别有益。Zhuang
    等人提供了一份迁移学习的调查 [[260](#CR260)]。微调具有多个优点：
- en: The model acquires detailed knowledge about the language, its syntax and semantics
    by exploiting the content provided in the pre-training data.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型通过利用预训练数据中提供的内容，获得了关于语言、其语法和语义的详细知识。
- en: Pre-trained models can easily be adapted to new tasks, e.g. by an additional
    layer with a simple classifier. The language representations of the pre-trained
    model support fine-tuning and are only slightly changed during this process.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练模型可以很容易地适应新任务，例如通过添加一个简单的分类器层。预训练模型的语言表示支持微调，并且在微调过程中仅略有变化。
- en: Fine-tuning even with a small data set yields a much better performance than
    direct training of a classifier on the limited data.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使数据集很小，微调也比直接在有限数据上训练分类器有更好的性能。
- en: Autoencoder models like BERT are typically fine-tuned for classification tasks,
    where the logistic classifiers for masked language modeling and next sentence
    prediction have to be removed. Using the *[CLS]* token or other tokens as input,
    new logistic classifier models as well as all model parameters are trained end-to-end
    with the new task for a few epochs (Sect. [2.​1.​3](528393_1_En_2_Chapter.xhtml#Sec5)).
    Compared to pre-training, fine-tuning is relatively inexpensive. Usually, only
    a small fraction of the pre-training effort is required to achieve good results.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 BERT 的自动编码器模型通常用于分类任务，在这些任务中，需要移除用于掩码语言建模和下一句预测的逻辑分类器。使用 *[CLS]* 标记或其他标记作为输入，新的逻辑分类器模型以及所有模型参数都会通过新任务进行端到端训练几个周期（见第
    [2.1.3](528393_1_En_2_Chapter.xhtml#Sec5) 节）。与预训练相比，微调相对成本较低。通常，只需要一小部分预训练的努力就能取得良好的效果。
- en: Tripuraneni et al. [[210](#CR210)] have theoretically proven that transfer learning
    requires far less data than learn tasks in isolation. They prove that transfer
    learning improves if the task diversity is enhanced. Bansal et al. [[7](#CR7)]
    investigate the theoretical properties of fine-tuning a classifier using pre-trained
    embeddings. The authors prove that these classifiers have a smaller generalization
    gap between their train and test accuracy, than standard classifiers.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: Tripuraneni等人[[210](#CR210)]从理论上证明了迁移学习比独立学习任务需要的数据要少得多。他们证明，如果任务多样性得到增强，迁移学习会得到改善。Bansal等人[[7](#CR7)]研究了使用预训练嵌入微调分类器的理论特性。作者证明，这些分类器在训练和测试准确率之间的泛化差距比标准分类器更小。
- en: Catastrophic Forgetting
  id: totrans-456
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 灾难性遗忘
- en: The question is whether fine-tuning can destroy the original capabilities of
    the model. This means, after fine-tuning a pre-trained model for a few epochs,
    it could lose predictive performance available after pre-training. A possible
    reason can be *catastrophic forgetting*, where all parameters are adapted to a
    new learning task while forgetting learned content.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是微调是否会破坏模型的原始能力。这意味着，在微调预训练模型几个epoch之后，它可能会失去预训练后可用的预测性能。可能的原因是*灾难性遗忘*，其中所有参数都适应了新的学习任务，同时忘记了学习内容。
- en: 'Merchant et al. [[133](#CR133)] fine-tune BERT[BASE] with three different tasks:
    (1) MNLI sentence pair classification task [[229](#CR229)] measuring if the first
    sentence entails the second; (2) SQuAD question answering [[173](#CR173)], where
    the answer to a question has to be marked in a text; (3) Dependency Parsing [[50](#CR50)]
    to capture the syntactic structure of sentences. Then they investigate the performance
    of a number of probing classifiers before and after fine-tuning. The results demonstrate
    that the fine-tuned models only show a small decrease in the accuracy to detect
    linguistic concepts. The reduction cause by the MNLI task in most cases is less
    than 1%, while higher differences (less than 3%) are observed for SQuAD and dependency
    parsing. Therefore, catastrophic forgetting cannot be observed. The authors state
    that fine-tuning primarily changes the top layers of BERT, with dependency parsing
    also affecting deeper layers. More detailed results are provided by Wallat et
    al. [[216](#CR216)].'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: Merchant等人[[133](#CR133)]使用三个不同的任务微调BERT[BASE]：(1) MNLI句子对分类任务[[229](#CR229)]，衡量第一个句子是否蕴涵第二个句子；(2)
    SQuAD问答[[173](#CR173)]，其中问题的答案必须在文本中标记；(3) 依存句法分析[[50](#CR50)]，以捕捉句子的句法结构。然后他们调查了微调前后多个探测分类器的性能。结果表明，微调模型在检测语言概念方面的准确率仅略有下降。由MNLI任务引起的减少在大多数情况下小于1%，而SQuAD和依存句法分析观察到的高差异（小于3%）。因此，没有观察到灾难性遗忘。作者表示，微调主要改变了BERT的顶层，依存句法分析也影响了更深层次的层。Wallat等人[[216](#CR216)]提供了更详细的结果。
- en: Fine-tuning only benefits from the pre-training, if there are similarities between
    the two tasks. Hence, pre-training should have a loss function which enforces
    the learning of semantics at word, phrase and document level. In addition, its
    training documents should originate from a domain close to the fine-tuning task.
    Otherwise the vocabulary may not include many domain-specific words. As a result,
    domain-specific words are split into a number of tokens which hinders model learning
    and degrades its performance in downstream tasks. In the next sections we will
    discuss alternative training regimes which improve BERT’s capabilities.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个任务之间存在相似性，微调仅从预训练中受益。因此，预训练应该有一个损失函数，强制学习在单词、短语和文档级别的语义。此外，其训练文档应源自与微调任务相近的领域。否则，词汇可能不包含许多领域特定词汇。结果，领域特定词汇被分割成多个标记，这阻碍了模型学习并降低了其在下游任务中的性能。在下一节中，我们将讨论改进BERT能力的替代训练方案。
- en: Fine-Tuning and Overfitting
  id: totrans-460
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调和过拟合
- en: During pre-training BERT’s parameters are adapted to the pre-training data,
    acquiring universal language representations. As pre-training provides a good
    initialization, it avoids overfitting on the small fine-tuning datasets, if the
    fine-tuning error is not minimized too much.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练期间，BERT的参数被调整以适应预训练数据，获得通用语言表示。由于预训练提供了一个良好的初始化，它避免了在小的微调数据集上过度拟合，如果微调错误没有最小化太多。
- en: Since PLMs have a very large number of parameters, there is the risk of overfitting
    on the fine-tuning data. As a result, generalization from unseen data can be poor
    and counterstrategies may be required. D’Amour [[42](#CR42)] present a comprehensive
    discussion of this *underspecification* phenomenon. Jiang et al. [[95](#CR95)]
    introduces a form of regularization, which makes the model invariant to small
    perturbations of the input, inducing smoothness in the local neighborhood. They
    develop a class of Bregman proximal point optimization methods, which penalize
    large updates of the model at each iteration. Aghajanyan et al. [[2](#CR2)] introduce
    the notion of representational collapse, stating that fine-tuned models lose their
    ability to generalize. They propose fine-tuning optimization based on trust-region
    theory, which alleviates representational collapse at a fraction of the cost of
    other recently proposed fine-tuning methods and, for instance, improves the best
    known results on fine-tuning RoBERTa on GLUE.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 由于PLM具有非常多的参数，因此在微调数据上存在过拟合的风险。结果，从未见过的数据中泛化的能力可能较差，可能需要采取对策。D’Amour [[42](#CR42)]
    对这种**欠指定**现象进行了全面的讨论。Jiang等人[[95](#CR95)]引入了一种正则化形式，使模型对输入的小扰动保持不变，从而在局部邻域中诱导平滑性。他们开发了一类Bregman邻近点优化方法，该方法在每个迭代中惩罚模型的大更新。Aghajanyan等人[[2](#CR2)]引入了表示崩溃的概念，指出微调模型失去了泛化的能力。他们提出了基于信任域理论的微调优化方法，这种方法以其他最近提出的微调方法的成本的一小部分缓解了表示崩溃，例如，在微调RoBERTa在GLUE上取得了最佳结果。
- en: Fine-tuning the same model with multiple random seeds can lead to large variance
    in task performance. Most papers argue that this effect is caused by *catastrophic
    forgetting* and the small size of the fine-tuning datasets. However, Mosbach et
    al. [[140](#CR140)] show that often fine-tuning has an optimization problem due
    to vanishing gradients. In addition, it can often occur that a model does not
    generalize well, although it has the same fine-tuning loss as a successful model.
    This is an indication for the underspecification mention above. The authors recommend
    to use small learning rates with bias correction to avoid vanishing gradients
    early in training. In addition, they propose to use more iterations for fine-tuning.
    More recipes to improve fine-tuning are provided by Rogers et al. [[176](#CR176)].
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个随机种子微调相同的模型可能会导致任务性能出现较大差异。大多数论文认为这种效应是由**灾难性遗忘**和微调数据集的小规模引起的。然而，Mosbach等人[[140](#CR140)]表明，通常微调存在由于梯度消失导致的优化问题。此外，有时会出现模型虽然与成功模型具有相同的微调损失，但泛化能力不佳的情况。这是上述欠指定的一个指示。作者建议使用小的学习率并纠正偏差以避免训练早期梯度消失。此外，他们提出使用更多迭代次数进行微调。Rogers等人[[176](#CR176)]提供了更多改进微调的食谱。
- en: 3.6.2 Fine-Tuning Variants
  id: totrans-464
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.2 微调变体
- en: Fine-Tuning in Two Stages
  id: totrans-465
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分阶段微调
- en: The intermediate training set should be closer to the final task. Although this
    approach can increase performance in some cases, an experimental evaluation demonstrates
    a decrease in performance in 44% of the cases [[163](#CR163)]. An intermediate
    training with a task requiring high-level inference and reasoning abilities tend
    to work best, as was shown in a large experiment [[165](#CR165)]. However, the
    authors also observe catastrophic forgetting of the pre-trained abilities. Gururangan
    et al. [[71](#CR71)] have shown that a second phase of pre-training, using domain-specific
    data, leads to significant performance gains, both in high- and low-resource settings.
    In addition, pre-training on tasks-specific unlabeled data improves performance
    on various tasks and domains.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 中间训练集应更接近最终任务。尽管这种方法在某些情况下可以提高性能，但实验评估表明，在44%的情况下性能有所下降[[163](#CR163)]。在需要高级推理和推理能力的任务上进行中间训练往往效果最佳，正如在一个大型实验[[165](#CR165)]中所展示的。然而，作者也观察到预训练能力的灾难性遗忘。Gururangan等人[[71](#CR71)]表明，使用特定领域的数据进行预训练的第二阶段，无论是在高资源还是低资源设置中，都能带来显著的性能提升。此外，在特定任务的无标签数据上预训练可以提高各种任务和领域的性能。
- en: Fine-Tuning for Multiple Tasks
  id: totrans-467
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多任务微调
- en: For each task, a task-specific layer is added to the underlying pre-trained
    model. Then the model is simultaneously trained with all tasks. However, it sometimes
    happens that performance does not increase compared to standard fine-tuning [[141](#CR141)],
    perhaps because of contradicting requirements of tasks. As an alternative, a subset
    of fine-tuning tasks from the available datasets may be selected based on similarity
    measures [[131](#CR131)].
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个任务，都会在底层预训练模型上添加一个特定于任务的层。然后，模型与所有任务同时训练。然而，有时与标准微调相比，性能没有提高 [[141](#CR141)]，可能是因为任务之间存在矛盾的要求。作为替代方案，可以根据相似度度量从可用的数据集中选择微调任务的子集
    [[131](#CR131)]。
- en: '**HyperGrid** [[208](#CR208)] is a multitask learning approach evaluated on
    the T5 model. It learns grid-wise projections that help to specialize regions
    in weight matrices for different tasks. As an example, a single model is simultaneously
    adapted to all GLUE and SuperGLUE tasks at once. In spite of the multitude of
    tasks, the model has a slightly better performance on SuperGLUE than the single
    models.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '**HyperGrid** [[208](#CR208)] 是一种在T5模型上评估的多任务学习方法。它学习网格投影，有助于为不同任务专门化权重矩阵中的区域。例如，单个模型可以同时适应所有GLUE和SuperGLUE任务。尽管有众多任务，该模型在SuperGLUE上的性能略优于单个模型。'
- en: Meta-Learning to Accelerate Fine-Tuning
  id: totrans-470
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过元学习加速微调
- en: During fine-tuning a pre-trained PLM is adapted to a new NLP task. It is usually
    trained for two or three epochs on a labeled fine-tuning dataset. Although this
    is much faster than pre-training the model on a large training corpus it still
    requires a lot of effort. To reduce this effort researchers tried to prepare the
    pre-trained model to fine-tuning by *meta-learning*. A survey of meta-learning
    is provided by Yin [[242](#CR242)].
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调预训练的PLM时，它会被适应到一个新的NLP任务。通常，它在一个标记的微调数据集上训练两到三个epoch。尽管这比在大规模训练语料库上预训练模型要快得多，但它仍然需要大量的努力。为了减少这种努力，研究人员试图通过**元学习**来准备预训练模型以进行微调。Yin
    [[242](#CR242)]提供了一份元学习的调查。
- en: 'Usually, there is a set ![$$\mathcal {T}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq33.png)
    of related fine-tuning tasks *T*[*i*]. During meta-training a task *T*[*i*] is
    sampled from a distribution ![$$p(\mathcal {T})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq34.png).
    Then the model is trained with *K* training samples from ![$$T_i^{\text{train}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq35.png)
    and then tested on the validation set of ![$$T_i^{\text{val}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq36.png).
    The validation error of *T*[*i*] is utilized as the training error of the meta-learning
    framework for the current iteration. The **MAML** algorithm [[58](#CR58)] follows
    this pattern:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，存在一个相关微调任务集 ![$$\mathcal {T}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq33.png)
    *T*[*i*]。在元训练期间，任务 *T*[*i*] 从分布 ![$$p(\mathcal {T})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq34.png)
    中采样。然后，模型使用 ![$$T_i^{\text{train}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq35.png)
    中的 *K* 个训练样本进行训练，然后在 ![$$T_i^{\text{val}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq36.png)
    的验证集上进行测试。*T*[*i*] 的验证错误被用作当前迭代的元学习框架的训练错误。**MAML**算法 [[58](#CR58)]遵循此模式：
- en: Copy ***w***^([*i*]) of the initial model parameters ***w***.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制初始模型参数的**w**^([*i*])。
- en: 'Train the model on the training set ![$$T_i^{\text{train}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq37.png)
    with a *K* gradient updates: ![$$\hat {{\boldsymbol {w}}}^{[i]} \gets {\boldsymbol
    {w}}^{[i]} - \gamma \partial L_i({\boldsymbol {w}}^{[i]},T_i^{\text{train}})/\partial
    {\boldsymbol {w}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq38.png)'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 *K* 梯度更新在训练集 ![$$T_i^{\text{train}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq37.png)
    上训练模型：![$$\hat {{\boldsymbol {w}}}^{[i]} \gets {\boldsymbol {w}}^{[i]} - \gamma
    \partial L_i({\boldsymbol {w}}^{[i]},T_i^{\text{train}})/\partial {\boldsymbol
    {w}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq38.png)
- en: Apply the model with the updated parameters ![$$\hat {{\boldsymbol {w}}}^{[i]}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq39.png)
    on the validation set ![$$T_i^{\text{val}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq40.png).
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在验证集 ![$$T_i^{\text{val}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq40.png)
    上应用更新参数的模型 ![$$\hat {{\boldsymbol {w}}}^{[i]}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq39.png)。
- en: Update the initial model parameters ***w*** using the loss on the validation
    set ![$${\boldsymbol {w}} \gets {\boldsymbol {w}} - \beta \partial L_i(\hat {{\boldsymbol
    {w}}}^{[i]},T_i^{\text{val}})/\partial {\boldsymbol {w}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq41.png)
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用验证集上的损失更新初始模型参数 ***w*** ![$${\boldsymbol {w}} \gets {\boldsymbol {w}} - \beta
    \partial L_i(\hat {{\boldsymbol {w}}}^{[i]},T_i^{\text{val}})/\partial {\boldsymbol
    {w}}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq41.png)
- en: This scheme was applied to BERT [[6](#CR6)]. The authors generate a large, rich,
    meta-learning task distribution from unlabeled text by gathering tokens-to-be
    masked from a few vocabulary terms. On 17 NLP tasks, they show that this type
    of meta-training leads to better few-shot generalization than language-model pre-training
    followed by fine-tuning. Chen et al. [[28](#CR28)] provide data-dependent generalization
    bounds for these approaches.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 该方案应用于BERT [[6](#CR6)]。作者们通过从少量词汇中收集待掩码的标记，从未标记文本中生成一个大型、丰富、元学习任务分布。在17个NLP任务上，他们表明这种类型的元训练比语言模型预训练后微调的少样本泛化效果更好。Chen等人[[28](#CR28)]为这些方法提供了数据依赖的泛化界限。
- en: Fine-Tuning a Frozen Model by Adapters
  id: totrans-478
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过适配器微调冻结模型
- en: A downside of fine-tuning for task-adoption is that new model parameters are
    needed for every task. *Task adapters* [[84](#CR84)] aim to mitigate this problem.
    The authors introduce adapter layers, which are inserted in a encoder block after
    the multi-head attention and the feedforward layer ([2.​7](528393_1_En_2_Chapter.xhtml#Equ7)).
    Now, to fine-tune transformer models to new tasks, instead of relearning all parameters,
    all weights of the network are frozen except for the adapter layers and the normalization
    layers. On tasks like GLUE this yields a significant reduction of parameters that
    need to be trained while preserving model quality.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 微调任务采用的一个缺点是每个任务都需要新的模型参数。*任务适配器* [[84](#CR84)]旨在减轻这个问题。作者们引入了适配器层，这些层被插入到编码器块中，在多头注意力和前馈层之后([2.​7](528393_1_En_2_Chapter.xhtml#Equ7))。现在，为了微调Transformer模型到新任务，而不是重新学习所有参数，除了适配器层和归一化层之外，网络的所有权重都保持冻结。在GLUE等任务上，这显著减少了需要训练的参数数量，同时保持了模型质量。
- en: Rather than having multiple adapters for different tasks, Stickland et al. [[197](#CR197)]
    propose training a multitasking version of BERT that can be used for several tasks
    simultaneously. They add low-dimensional projected attention layers as bypass
    to BERT encoder blocks, which connect the input to layer-norm layers and the subsequent
    layer-norm layers. They sample data from the different tasks during training proportionally
    to the sizes of the respective training sets and use an annealing mechanism to
    converge towards equally distributed training samples by the end of the training.
    Their results surpass the results of a BERT[BASE] model.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: Stickland等人[[197](#CR197)]提出，而不是为不同的任务准备多个适配器，他们建议训练一个多任务版本的BERT，该版本可以同时用于多个任务。他们添加了低维投影注意力层作为绕过BERT编码器块的旁路，这些层将输入连接到层归一化层以及随后的层归一化层。他们在训练期间按各自训练集的大小成比例地从不同任务中采样数据，并使用退火机制，到训练结束时使训练样本均匀分布。他们的结果超过了BERT[BASE]模型的结果。
- en: '**MAD-X** [[160](#CR160)] is a framework to adapt multilingual models to arbitrary
    languages and tasks. The authors introduce language- and task-specific adapters,
    which consist of a linear down-projection to a small vector, a ReLU activation
    and a linear up-projection. The language specific adapters are trained with an
    MLM objective, while the rest of the model is frozen. The task-specific adapters
    are trained with the task-specific data, fixing the rest of the parameters. Finally,
    invertible adapters are added after the input embedding layer and before the output
    embedding layer to mitigate differences between the multilingual vocabulary and
    the target language vocabulary. MAD-X achieves Sota for NER and common sense reasoning
    for a set of different languages.'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '**MAD-X** [[160](#CR160)]是一个将多语言模型适应到任意语言和任务的框架。作者们引入了语言和任务特定的适配器，这些适配器包括一个线性下投影到小向量、ReLU激活和一个线性上投影。语言特定的适配器使用MLM目标进行训练，而模型的其他部分保持冻结。任务特定的适配器使用特定于任务的训练数据进行训练，固定其他参数。最后，在输入嵌入层之后和输出嵌入层之前添加可逆适配器，以减轻多语言词汇和目标语言词汇之间的差异。MAD-X在多种语言的NER和常识推理任务上实现了Sota。'
- en: '**LoRA** [[85](#CR85)] freezes the weights of the pre-trained model and adds
    trainable bypasses to the model, which consist of trainable matrix transformations
    to a short vector and to the full rank. This drastically reduces the number of
    trainable parameters (1/30 for GPT-3 and 1/100 for GPT-2) while achieving better
    results than with traditional fine-tuning on many NLP tasks. *AdapterHub* [[161](#CR161)]
    is a repository for adapters that as of writing contains around 380 adapters.
    AdapterHub is built on the Hugging Face transformer library for compatibility
    with existing transformer models.'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '**LoRA** [[85](#CR85)] 冻结预训练模型的权重，并向模型添加可训练的旁路，这些旁路包括对短向量和对全秩矩阵的可训练变换。这极大地减少了可训练参数的数量（GPT-3
    为 1/30，GPT-2 为 1/100），同时在许多 NLP 任务上比传统的微调方法取得了更好的结果。*AdapterHub* [[161](#CR161)]
    是一个适配器存储库，截至编写时包含大约 380 个适配器。AdapterHub 建立在 Hugging Face transformer 库之上，以与现有的
    transformer 模型兼容。'
- en: Fine-Tuning GPT-3
  id: totrans-483
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GPT-3 微调
- en: 'GPT-3 is an extremely powerful Foundation Model, but it is not publicly available
    (Sect. [3.1.2](#Sec3)). By using the API for fine-tuning GPT-3 with user-specific
    data [[123](#CR123)], the model can be adapted to specific domain languages and
    particular tasks. This typically yields a higher quality than few-shot examples
    and prompt design described below. To fine-tune the 175B parameter model on a
    1M token file for four epochs OpenAI charges about $120\. The fine-tuning can
    be used in a number of ways [[123](#CR123)]:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 是一个非常强大的基础模型，但它不是公开可用的（第 [3.1.2](#Sec3) 节）。通过使用 API 使用用户特定的数据 [[123](#CR123)]
    对 GPT-3 进行微调，模型可以适应特定领域的语言和特定任务。这通常比下面描述的少样本示例和提示设计产生更高的质量。要在 1M 令牌文件上对 175B 参数模型进行四次epoch的微调，OpenAI
    收取大约 $120。微调可以用多种方式使用 [[123](#CR123)]：
- en: '*Completion*: Generate a completion for a prompt.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*补全（Completion）*: 为提示生成补全内容。'
- en: '*Search*: Given a search query and a set of documents or labels, the model
    ranks each document with a score based on its semantic similarity to the query.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*搜索（Search）*: 给定一个搜索查询和一组文档或标签，模型根据其与查询的语义相似度对每个文档进行评分。'
- en: '*Classification*: Input is a query and a set of labeled examples, e.g., *[“I
    am feeling awesome”, “Positive”]*. Then GPT-3 will predict the most probable label
    for the query. This can be used similar to BERT for any type of classification
    task.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类（Classification）*: 输入是一个查询和一个带有标签的示例集，例如，*[“我感觉很棒”， “积极”]*。然后 GPT-3 将预测查询最可能的标签。这可以用于类似于
    BERT 的任何类型的分类任务。'
- en: '*Answer*: Input is a question, a set of documents with background information,
    and some examples. Based on the information in the documents and the examples,
    an answer is generated. This is similar to the reading comprehension task of question
    answering (Sect. [6.​2](528393_1_En_6_Chapter.xhtml#Sec9)).'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*回答（Answer）*: 输入是一个问题，一些带有背景信息的文档和一些示例。基于文档中的信息和示例，生成一个答案。这与问答任务中的阅读理解任务（第
    [6.2](528393_1_En_6_Chapter.xhtml#Sec9) 节）类似。'
- en: '*Fine-tune*: Adapts GPT-3 to a specific domain text.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*微调（Fine-tune）*: 将 GPT-3 适应到特定领域的文本。'
- en: '*Embeddings*: Get a vector of contextual embeddings for an input text for further
    processing or exploration.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*嵌入（Embeddings）*: 获取输入文本的上下文嵌入向量，用于进一步处理或探索。'
- en: It can be assumed that GPT-3 and other Foundation Models like PaLM fine-tuned
    in this way will increase Sota in many areas due to their comprehensive knowledge
    about language.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 可以假设，通过这种方式微调的 GPT-3 和其他类似的基础模型如 PaLM 将由于它们对语言的全面知识而提高许多领域的 Sota（最先进的技术）。
- en: 3.6.3 Creating Few-Shot Prompts
  id: totrans-492
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.3 创建少样本提示
- en: 'For *zero-shot learning* the model just gets a task description or *prompt*,
    e.g. *“Translate English to French: cheese =*> ”, and directly generates the answer
    *“fromage”*. For *one-shot* or *few-shot learning* the model receives a task description
    as well as one or more examples, e.g. *“Translate English to French: sea otter
    =*> *loutre de mer; cheese =*> ”, which helps the model to find the answer *“fromage”*.
    This happens without training, the parameters of the model are not changed, and
    the model creates the answer based on the knowledge acquired during pre-training.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *零样本学习*，模型只需获得任务描述或 *提示*，例如 *“将英语翻译成法语：奶酪 =*>”，然后直接生成答案 *“fromage”*。对于 *单样本*
    或 *少样本学习*，模型会收到任务描述以及一个或多个示例，例如 *“将英语翻译成法语：海獭 =*> *loutre de mer；奶酪 =*>”，这有助于模型找到答案
    *“fromage”*。这不需要训练，模型的参数不会改变，模型基于预训练期间获得的知识创建答案。
- en: 'In this way, GPT-3 can be instructed by natural language prompts to generate
    short stories, songs, answers to questions, press releases, technical manuals,
    and more [[181](#CR181)]. It can adapt its output texts to specific styles, personalities
    or ideologies. Here are some of the recommended prompts used for few-shot learning
    [[150](#CR150)]:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，GPT-3可以通过自然语言提示来生成短篇小说、歌曲、问题的答案、新闻稿、技术手册等等 [[181](#CR181)]。它可以调整其输出文本以适应特定的风格、个性或意识形态。以下是一些用于少样本学习的推荐提示
    [[150](#CR150)]：
- en: 'Summarization: the model receives a long story and the prompt *“tl;dr:”*.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要：模型接收一个长故事和提示 *“tl;dr：”*。
- en: 'Grammar correction *“Original: She no went to the market. Standard American
    English:”*'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法纠正 *“原文：她没有去市场。标准美式英语：”*
- en: 'Translation: *“English: I do not speak French. French: Je ne parle pas français.
    English: Where is the restroom?”* French:'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翻译：*“英语：我不说法语。法语：Je ne parle pas français. 英语：洗手间在哪里？”* 法语：
- en: 'Generate an outline for an essay: *“Create an outline for an essay about Walt
    Disney and his contributions to animation:*'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为一篇论文生成大纲：*“为关于沃尔特·迪士尼及其对动画贡献的论文创建大纲：*
- en: '*I: Introduction”*'
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*I: 引言*'
- en: Figure [3.22](#Fig22) shows the accuracy of “few-shot learning” for different
    GPT-3 model sizes and different numbers of given examples.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig22_HTML.png)
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3.22](#Fig22)显示了不同GPT-3模型大小和不同给定示例数量的“少样本学习”准确率。![图3.22](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig22_HTML.png)
- en: A line graph of accuracy percentage versus the number of examples in the prompt
    indicates the trends of natural language prompts and no prompts with 175, 13,
    and 1.3 billion parameters. All the lines have increasing trends.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 一条表示准确率百分比与提示中示例数量关系的折线图显示了自然语言提示和无提示的趋势，包括175、13和130亿参数的提示。所有线条都呈上升趋势。
- en: Fig. 3.22
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.22
- en: The accuracy of few-shot learning of GPT-3 is increased by extending the model
    size as well as the number of presented examples [[25](#CR25)]. The task is to
    remove random symbols from a word. A natural language description of the task
    can support the model especially in the one-shot regime. Image reprinted with
    kind permission of the authors [[25](#CR25), p. 4]
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 通过扩展模型大小以及展示的示例数量，GPT-3的少样本学习准确性得到提高 [[25](#CR25)]。任务是移除单词中的随机符号。自然语言描述的任务可以支持模型，特别是在单样本模式下。图片经作者同意重印
    [[25](#CR25)，第4页]
- en: 'In a comprehensive survey Liu et al. [[125](#CR125)] compile approaches to
    prompt design to create prompts for language models that reliably generate the
    desired response. For example, when we want to recognize the sentiment of the
    text *“I missed the bus today.”*, we may insert the prompt *“I felt so* *”*, and
    use the language model to replace the blank. There are two types of prompts: *cloze
    prompts* [[159](#CR159)], which fill in the blanks of a textual string by an autoencoder
    model similar to BERT, and *prefix prompts* [[117](#CR117)], which continue a
    text by an autoregressive language model.'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在一项综合调查中，刘等人 [[125](#CR125)] 汇编了用于创建可靠生成所需响应的语言模型提示设计方法。例如，当我们想要识别文本 *“我今天错过了公交车。”*
    的情感时，我们可能会插入提示 *“我感到如此*”*，并使用语言模型来填充空白。有两种类型的提示：*cloze prompts* [[159](#CR159)]，通过类似于BERT的自动编码器模型填充文本字符串的空白，以及*prefix
    prompts* [[117](#CR117)]，通过自回归语言模型继续文本。
- en: For prompt mining [[96](#CR96)], for instance, a large number of sentences with
    phrases *x* and *y* are collected. Subsequently, prompts are generated using the
    words between *x* and *y*, or on the dependency path generated by parser. Another
    approach is based on paraphrasing existing prompts, for instance by translation
    to another language and back-translation. The probability of desired answers may
    be increased by gradient-based search [[192](#CR192)] as demonstrated with the
    *AutoPrompt* model. Alternative approaches are described in [[62](#CR62), [245](#CR245)].
    It should be noted, however, that the output of a model instructed with few-shot
    prompts can be easily altered if an adversary adds some new prompts [[79](#CR79)].
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 对于提示挖掘 [[96](#CR96)]，例如，收集了大量包含短语 *x* 和 *y* 的句子。随后，使用 *x* 和 *y* 之间的单词或由解析器生成的依赖路径生成的提示生成。另一种方法是基于现有提示的释义，例如通过翻译成另一种语言再回译。通过基于梯度的搜索
    [[192](#CR192)] 可以提高期望答案的概率，如 *AutoPrompt* 模型所示。其他方法在 [[62](#CR62)，[245](#CR245)]
    中描述。然而，需要注意的是，如果对手添加了一些新的提示，那么用少样本提示指令的模型的输出很容易被改变 [[79](#CR79)]。
- en: Instead of improving prompt tokens, which generate a desired output by the language
    model, one can optimize the input embeddings of some “virtual” tokens, such that
    the desired answer is created. The embeddings of this “continuous” prompt can
    be optimized by gradient descent while keeping the parameters of the language
    model fixed [[121](#CR121)]. Lester et al. [[117](#CR117)] apply this approach
    with a continuous prompt sequence of 100 tokens to the T5 transformer. On the
    *SuperGLUE* benchmark they achieve the same performance of 90.5% as for fine-tuning
    T5\. This demonstrates that prompt tuning becomes competitive with fine-tuning
    and is much better than few-shot instructions. Note that the effort for prompt
    tuning is much lower than for fine-tuning, as the number of parameters is much
    smaller. It would be interesting to see this technique applied to recent autoregressive
    models like GPT-3 or PaLM.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 与改进生成期望输出的提示词不同，可以通过优化一些“虚拟”提示词的输入嵌入来优化，从而创建所需的答案。这个“连续”提示词的嵌入可以通过梯度下降进行优化，同时保持语言模型的参数不变
    [[121](#CR121)]。Lester 等人 [[117](#CR117)] 使用100个提示词的连续序列将这种方法应用于 T5 变换器。在 *SuperGLUE*
    基准测试中，他们达到了与微调 T5 相同的90.5%的性能。这表明提示词调整与微调具有竞争力，并且比少样本指令要好得多。请注意，提示词调整的努力远低于微调，因为参数数量要小得多。将这项技术应用于最近的自动回归模型，如
    GPT-3 或 PaLM，将会很有趣。
- en: 3.6.4 Thought Chains for Few-Shot Learning of Reasoning
  id: totrans-507
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.4 少样本学习推理的思维链
- en: To improve the reasoning capabilities of language models, prompts can contain
    a *chain of thought*, a sequence of short sentences that imitate the reasoning
    process a person might have when answering a question [[226](#CR226)]. Two examples
    are shown in Fig. [2.​21](528393_1_En_2_Chapter.xhtml#Fig21). The idea is that
    a chain of thought allows language models to split a multistep problem into intermediate
    steps that are solved one at a time, rather than solving an entire multistep problem
    in a single pass.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高语言模型的推理能力，提示词可以包含一个**思维链**，这是一个模仿人们在回答问题时可能进行的推理过程的短句序列 [[226](#CR226)]。图
    [2.21](528393_1_En_2_Chapter.xhtml#Fig21) 展示了两个示例。其理念是，思维链允许语言模型将一个多步骤问题分解成一系列中间步骤，这些步骤可以逐个解决，而不是一次性解决整个多步骤问题。
- en: The approach has a number of advantages. First, the chain-of-thought approach
    enables a model to decompose complex reasoning tasks into simpler intermediate
    steps, which can be solved by the model. To solve an entire class of problems,
    only a few chains of thought need to be provided. Second, when a model performs
    the intermediate steps, it is easier to check where the model has introduced an
    error. This may give a clue how to improve the chain of thought. Chain of thought
    reasoning can be applied to symbolic manipulation, common sense reasoning and
    math tasks, and is potentially applicable to any task that humans can solve via
    language.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法具有许多优点。首先，思维链方法使模型能够将复杂的推理任务分解成更简单的中间步骤，这些步骤可以被模型解决。为了解决整个问题类别，只需要提供少数几个思维链。其次，当模型执行中间步骤时，更容易检查模型在何处引入了错误。这可能会提供改进思维链的线索。思维链推理可以应用于符号操作、常识推理和数学任务，并且可能适用于任何人类可以通过语言解决的问题。
- en: Prompts also do not need to be restricted to input-output pairs or explanations
    and can cover many arguments, including things to avoid, rules of thumb, reasoning
    chains, positive or negative examples. Mishra et al. [[138](#CR138)] consider
    instructions for crowdworkers, which contain very detailed prescriptions how to
    solve a task. They compile a dataset of tasks, instructions and generated input-output
    pairs. Subsequently, they investigate how well models are able to generalize to
    similar tasks. The results show that PLMs benefit from instructions when evaluated
    in terms of generalization to unseen tasks (19% improvement). However, there is
    much room for improvement.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 提示词也不必局限于输入-输出对或解释，可以涵盖许多论点，包括要避免的事项、经验法则、推理链、正面或负面示例。Mishra 等人 [[138](#CR138)]
    考虑了针对众包工作者的指令，其中包含非常详细的如何解决问题的规定。他们编制了一个包含任务、指令和生成的输入-输出对的数据库。随后，他们研究了模型在泛化到类似任务方面的表现。结果显示，PLM
    在评估泛化到未见任务时的表现（提高19%）中受益于指令。然而，还有很大的改进空间。
- en: Du et al. [[52](#CR52)] investigate few-shot learning theoretically. They investigate
    the case that a model is pre-trained on a number of tasks with a large training
    set and subsequently fine-tuned on a related task. They theoretically derive bounds
    on the required sample size for the fine-tuning task, which can be reduced when
    there is a good common representation.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: Du 等人 [[52](#CR52)] 从理论上研究了少样本学习。他们研究了模型在具有大量训练集的多个任务上预训练，随后在相关任务上微调的情况。他们从理论上推导出微调任务所需的样本大小界限，当存在良好的共同表示时，这个界限可以降低。
- en: 3.6.5 Fine-Tuning Models to Execute Instructions
  id: totrans-512
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.5 微调模型以执行指令
- en: Instead of querying autoregressive PLMs by few-shot instructions it is possible
    to fine-tune these models to execute instructions without additional examples.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是通过少样本指令查询自回归 PLM，可以微调这些模型以执行指令，而无需额外的示例。
- en: '**InstructGPT** [[151](#CR151)] is a new version of GPT-3\. It is optimized
    to follow instructions instead of predicting the probable next words. Instead
    of needing a series of examples, GPT-3 now directly executes an instruction, e.g.
    *“Write a short story about the moon and the stars:”*, and the model generates
    a plausible story. In a first trial a dataset of 13k pairs of instructions and
    completions was collected to adapt GPT-3\. GPT-3 was fine-tuned using this data.
    However, the model did not adequately match the intended human preferences. Therefore,
    the model was modified using a different training approach.'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '**InstructGPT** [[151](#CR151)] 是 GPT-3 的新版本。它被优化为遵循指令，而不是预测可能的下一个单词。GPT-3
    现在可以直接执行指令，例如 *“写一个关于月亮和星星的短故事：”*，然后模型生成一个合理的情节。在第一次试验中，收集了一个包含 13k 对指令和完成的语料库来调整
    GPT-3。使用这些数据对 GPT-3 进行了微调。然而，模型并没有充分匹配预期的人类偏好。因此，使用不同的训练方法对模型进行了修改。'
- en: To adjust GPT-3 a *reinforcement learning* approach with human feedback was
    used. The *proximal policy optimization* (PPO) [[186](#CR186)] follows the policy
    gradient pattern. It approximates the conditional distribution *π*(*a*[*t*]|*s*[*t*];***w***)
    of actions ![$$a_t\in \mathcal {A}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq42.png)
    at step *t* conditional to the current observation ![$$s_t\in \mathcal {S}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq43.png)
    about the state of the environment and a vector ***w*** of parameters. In usual
    reinforcement learning, the environment generates a reward and the algorithm tries
    to maximize the weighted sum of rewards. The gradient for this optimization (policy
    gradient) can be easily computed from the model. PPO computes an update at each
    step that minimizes the cost function while ensuring the deviation from the previous
    policy is relatively small [[186](#CR186)].
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调整 GPT-3，使用了带有人类反馈的**强化学习**方法。**近端策略优化**（PPO）[[186](#CR186)] 遵循策略梯度模式。它近似在步骤
    *t* 时，关于环境状态的当前观察 ![$$s_t\in \mathcal {S}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq43.png)
    和参数向量 ***w*** 的动作条件分布 *π*(*a*[*t*]|*s*[*t*];***w***)。在通常的强化学习中，环境生成奖励，算法试图最大化加权奖励总和。这个优化的梯度（策略梯度）可以很容易地从模型中计算出来。PPO
    在每一步计算一个更新，以最小化成本函数，同时确保与先前策略的偏差相对较小 [[186](#CR186)]。
- en: 'The algorithm needs a numeric score to measure the quality of each generated
    sequence. To reduce the data necessary for optimization, a human can express preferences
    [[198](#CR198)] between trajectories *τ* = (***y***, ***x***) for pairs of instructions
    ***x*** and generated text ***y***. Informally, the goal is to produce trajectories
    which are preferred by the human, while querying the human as little as possible.
    To achieve this goal, a reward function ![$$r({\boldsymbol {y}},{\boldsymbol {x}})\in
    \mathbb {R}$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq44.png)
    is postulated [[36](#CR36)] with the property that (***y***^([1]), ***x***^([1]))
    is preferred to (***y***^([2]), ***x***^([2])) if *r*(***y***^([1]), ***x***^([1])) > *r*(***y***^([2]),
    ***x***^([2])). The original policy *π*(*a*[*t*]|*s*[*t*];***w***) induces a conditional
    distribution *π*(***y***|***x***;***w***). To construct this, the reward function
    *r*(***y***, ***x***) is approximated by a deep neural network ![$$\hat {r}({\boldsymbol
    {y}},{\boldsymbol {x}};\boldsymbol {u})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq45.png)
    with parameter ***u***. The network is trained by three alternating steps (Fig.
    [3.23](#Fig23)):'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 算法需要一个数值分数来衡量每个生成序列的质量。为了减少优化所需的数据，人类可以表达对轨迹*τ*=(***y***, ***x***)的偏好 [[198](#CR198)]，对于指令***x***和生成的文本***y***成对。非正式地说，目标是产生人类偏好的轨迹，同时尽可能少地查询人类。为了实现这一目标，假设了一个奖励函数![r^(y,x)](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq44.png)，其性质是如果*r^(y^([1]),
    x^([1]))>r^(y^([2]), x^([2]))，则(*y^([1]), x^([1]))比(*y^([2]), x^([2]))更受欢迎。原始策略*π*(a[t]|s[t];***w***)诱导一个条件分布*π*(***y***|***x***;***w***)。为了构建这个，奖励函数*r^(y,x*)被一个具有参数***u***的深度神经网络![r^(y,x;u)](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq45.png)近似。网络通过三个交替步骤进行训练（图[3.23](#Fig23))：
- en: 1.The policy *π*(***y***|***x***;***w***) is used to generate set of trajectories
    {*τ*¹, …, *τ*^(*i*)}. The parameter ***w*** is updated by reinforcement learning
    in order to maximize the reward ![$$\hat {r}({\boldsymbol {y}},{\boldsymbol {x}};\boldsymbol
    {u})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq46.png).![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig23_HTML.png)
  id: totrans-517
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1.使用策略*π*(***y***|***x***;***w***)生成轨迹集{τ¹, …, τ^i}。参数***w***通过强化学习进行更新，以最大化奖励![r^(y,x;u)](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq46.png)。![r^(y,x;u)](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig23_HTML.png)
- en: An illustration lists 3 steps as follows. 1, train a supervised model. 2, train
    a reward model for ranking answers. 3, train a stepwise model by reinforcement
    learning to reproduce the ranking. Each step has multiple sub-steps.
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下列出了3个步骤。1，训练一个监督模型。2，训练一个用于排名答案的奖励模型。3，通过强化学习训练一个逐步模型来重现排名。每个步骤都有多个子步骤。
- en: Fig. 3.23
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.23
- en: InstructGPT is trained in three steps [[151](#CR151), p. 3]. First GPT-3 is
    fine-tuned on instructions and the corresponding completions. Then a reward model
    is generated by optimizing the selection of a completion for an instruction. Finally,
    a policy is trained to generate token by token of the answer with maximal reward.
    Credits for image parts in Table [A.​1](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab1)
  id: totrans-520
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: InstructGPT分为三个步骤进行训练 [[151](#CR151), p. 3]。首先，GPT-3在指令及其对应补全上进行微调。然后，通过优化指令的补全选择来生成奖励模型。最后，训练一个策略，通过强化学习逐个生成具有最大奖励的答案。图像部分的信用归功于表[A.1](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab1)
- en: '2.'
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Pairs of trajectories (*σ*^([1]), *σ*^([2])) from the {*τ*¹, …, *τ*^(*i*)} are
    selected and submitted to a human for comparison.
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从{τ¹, …, τ^i}中选择轨迹对(*σ^([1]), *σ^([2]))提交给人类进行比较。
- en: '3.'
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: The parameters ***u*** of the reward function ![$$\hat {r}({\boldsymbol {y}},{\boldsymbol
    {x}};\boldsymbol {u})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq47.png)
    are optimized to correspond to the comparisons collected from the human up to
    now.
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 奖励函数![r^(y,x;u)](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq47.png)的参数***u***被优化，以对应到目前为止收集的人类比较。
- en: For a set of 33k instructions, a *reward model*![$$\hat {r}({\boldsymbol {y}},{\boldsymbol
    {x}};\boldsymbol {u})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq48.png)
    was built with 6B parameters, where ***x*** is the instruction and ***y*** a completion
    [[198](#CR198)]. It selects the best completion from a small set of proposed completions.
    Proximal policy optimization (PPO) was used as reinforcement model [[151](#CR151),
    p. 41]. To avoid catastrophic forgetting (Sect. [3.6.1](#Sec32)), pre-training
    samples were mixed into fine-tuning.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一组 33k 个指令，构建了一个具有 6B 参数的 *奖励模型*![$$\hat {r}({\boldsymbol {y}},{\boldsymbol
    {x}};\boldsymbol {u})$$](../images/528393_1_En_3_Chapter/528393_1_En_3_Chapter_TeX_IEq48.png)，其中
    ***x*** 是指令，***y*** 是完成内容 [[198](#CR198)]。它从一小组提议的完成内容中选择最佳完成内容。使用近端策略优化 (PPO)
    作为强化模型 [[151](#CR151), p. 41]。为了避免灾难性遗忘（第 [3.6.1](#Sec32) 节），预训练样本被混合到微调中。
- en: The reward model was then applied to create a final model by another reinforcement
    learning step. During this process, InstructGPT generates a completion for an
    instruction. The reward model calculates a reward and the policy is updated to
    approximate the preferences encoded in the reward model. By mimicking human utterances,
    the model implicitly learns human intentions and preferences. This process is
    called *alignment to human preferences* and is extensively discussed by Askell
    et al. [[5](#CR5)].
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过另一个强化学习步骤应用奖励模型来创建最终模型。在这个过程中，InstructGPT 为一个指令生成一个完成内容。奖励模型计算一个奖励，并将策略更新以近似奖励模型中编码的偏好。通过模仿人类话语，模型隐式地学习人类意图和偏好。这个过程被称为
    *对人类偏好的对齐*，Askell 等人在 [[5](#CR5)] 中进行了广泛讨论。
- en: InstructGPT Results
  id: totrans-527
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: InstructGPT 结果
- en: The GPT-3 model with 175B parameters fined-tuned in a supervised way to the
    13k instruction-completion examples was taken as the base model called SFT. The
    final completions were again scored by human raters [[151](#CR151)]. The InstructGPT
    completions were preferred to the standard GPT-3 output in 85% of cases and to
    few-shot-GPT-3 in 71% of cases.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 将 175B 参数的 GPT-3 模型以监督方式微调到 13k 个指令-完成示例，作为称为 SFT 的基础模型。最终完成内容再次由人类评分者评分 [[151](#CR151)]。在
    85% 的情况下，InstructGPT 的完成内容比标准 GPT-3 输出更受欢迎，在 71% 的情况下比 few-shot-GPT-3 更受欢迎。
- en: Specifically, raters found that InstructGPT attempts to follow the correct instruction
    in 92% of cases, compared to 85% for SFT and 75% for few-shot GPT-3 [[151](#CR151),
    p. 53]. In addition, InstructGPT follows explicit constraints in 50% of the cases,
    compared to 43% for SFT and 34% for SFT and 28% for few-shot GPT-3\. Hallucinations
    were observed for 20% of the cases for InstructGPT compared to 16% for SFT and
    50% for few-shot GPT-3\. Finally, the raters found that the language use is appropriate
    for a customer assistant in 92% of the cases for InstructGPT, about 90% for SFT
    and about 85% for GPT-3 few-shot. InstructGPT was also evaluated on a few natural
    language benchmarks where it achieved very similar results to GPT-3 [[151](#CR151),
    p. 56].
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，评分者发现，在 92% 的情况下，InstructGPT 尝试遵循正确的指令，而 SFT 为 85%，few-shot GPT-3 为 75%
    [[151](#CR151), p. 53]。此外，InstructGPT 在 50% 的情况下遵循显式约束，而 SFT 为 43%，few-shot GPT-3
    为 34%。对于 InstructGPT，20% 的情况下观察到幻觉，而 SFT 为 16%，few-shot GPT-3 为 50%。最后，评分者发现，在
    92% 的情况下，InstructGPT 的语言使用适合客户助理，SFT 大约为 90%，GPT-3 few-shot 大约为 85%。InstructGPT
    还在几个自然语言基准测试中被评估，其结果与 GPT-3 非常相似 [[151](#CR151), p. 56]。
- en: It turned out that InstructGPT is able to generalize to unseen labeler preferences.
    Thus, InstructGPT does not simply adapt to the preferences of a few training labelers.
    In addition, InstructGPT produces slightly less toxic language than standard GPT-3\.
    However, InstructGPT still makes simple mistakes, e.g., given an instruction with
    a false premise, the model sometimes incorrectly assumes the premise is true.
    Note that the results depend on the subjective preferences of the labelers.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，InstructGPT 能够推广到未见过的标注者偏好。因此，InstructGPT 并不仅仅适应少数训练标注者的偏好。此外，InstructGPT
    产生的有毒语言比标准 GPT-3 少一些。然而，InstructGPT 仍然会犯简单的错误，例如，给定一个包含错误前提的指令，模型有时会错误地假设前提是真实的。请注意，结果取决于标注者的主观偏好。
- en: Comparisons between alternatives are not necessarily the most effective approach
    to generate an improvement signal. For example, one could ask labelers to edit
    model responses to make them better, or generate critiques of model responses
    in natural language. There is also a vast space of options for designing interfaces
    for labelers to provide feedback to language models; this is an interesting human-computer
    interaction problem. The authors note that the cost of aligning GPT-3 to human
    preferences described above is just 1.6% of the cost spent to train GPT-3\. Therefore,
    it seems to make sense to put more effort into alignment than into the mere enlargement
    of the models.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 比较不同的方案并不一定是生成改进信号的最有效方法。例如，可以要求标注者编辑模型响应以使其更好，或者用自然语言生成模型响应的评论。还有大量选项可以设计用于标注者向语言模型提供反馈的界面；这是一个有趣的人机交互问题。作者指出，将
    GPT-3 与上述人类偏好对齐的成本仅为训练 GPT-3 成本的 1.6%。因此，似乎应该将更多精力投入到对齐上，而不是仅仅扩大模型。
- en: The results show that the InstructGPT techniques potentially make language models
    more helpful, truthful, and harmless. In a way InstructGPT works like an intelligent
    assistant for speech generation and information provision. However, the model
    is currently not fit for use in safety-critical applications, because failures
    cannot be ruled out. What is still missing is a comprehensive evaluation similar
    to Gopher or PaLM (Sect. [3.1.2](#Sec3)) that shows the real utility of this approach.
    It can be expected that the combination of this approach with retrieval techniques
    as used for WebGPT (Sect. [6.​2.​3](528393_1_En_6_Chapter.xhtml#Sec16)) and Retro
    (Sect. [6.​2.​3](528393_1_En_6_Chapter.xhtml#Sec16)) will increase the performance,
    reliability, and correctness of InstructGPT.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，InstructGPT 技术可能使语言模型更有帮助、更真实、更无害。从某种意义上说，InstructGPT 的工作方式就像是一个智能语音生成和信息提供助手。然而，该模型目前不适合用于安全关键应用，因为无法排除失败的可能性。仍然缺少的是类似于
    Gopher 或 PaLM（第 [3.1.2](#Sec3) 节）的全面评估，以展示这种方法的真实效用。可以预期，将这种方法与 WebGPT（第 [6.2.3](528393_1_En_6_Chapter.xhtml#Sec16)
    节）和 Retro（第 [6.2.3](528393_1_En_6_Chapter.xhtml#Sec16) 节）中使用的检索技术相结合，将提高 InstructGPT
    的性能、可靠性和正确性。
- en: Instruction Tuning with FLAN
  id: totrans-533
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 FLAN 进行指令调整
- en: '**FLAN** [[227](#CR227)] uses instruction tuning to improve the ability of
    the language model to respond to natural language prompts. The language model
    has to learn through supervision to perform tasks described by prompts, and to
    follow instructions, even for unfamiliar tasks (Fig. [3.24](#Fig24)). The authors
    group 62 publicly available NLP datasets into twelve task clusters, e.g. “sentiment”
    “natural language inference”, “summarization”, etc. For each of the datasets they
    compose ten templates describing the task in natural language. Then an existing
    language model is fine-tuned to provide better answers to the prompts.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig24_HTML.png)'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '**FLAN** [[227](#CR227)] 通过指令调整来提高语言模型对自然语言提示的响应能力。语言模型必须通过监督学习来执行由提示描述的任务，并遵循指令，即使是对于不熟悉的任务（图
    [3.24](#Fig24)）。作者将62个公开可用的NLP数据集分为十二个任务集群，例如“情感”、“自然语言推理”、“摘要”等。对于每个数据集，他们编写了十个用自然语言描述任务的模板。然后，一个现有的语言模型被微调以提供更好的对提示的响应！![图
    3.24](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig24_HTML.png)'
- en: A block diagram exhibits a set of prompt inputs related to commonsense reasoning
    and translation. It indicates the translation input leads to the input of natural
    language inference. It also denotes the tasks of sentiment analysis and conference
    resolution and the F LAN response.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 一个框图展示了与常识推理和翻译相关的提示输入集。它表明翻译输入导致自然语言推理的输入。它还表示情感分析和会议解决的任务以及 FLAN 的响应。
- en: Fig. 3.24
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.24
- en: FLAN instruction tuning fine-tunes a pre-trained language models on a set of
    tasks with instructions of ten different templates (left). The trained model can
    be applied to unseen tasks by formulating prompts according to these templates
    (right). Image adapted from [[227](#CR227), p. 1] with kind permission of the
    authors
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: FLAN 指令调整通过十个不同模板的指令在一系列任务上微调预训练的语言模型（左）。训练后的模型可以通过根据这些模板制定提示来应用于未见过的任务（右）。图片经作者许可，改编自
    [[227](#CR227)，第 1 页]
- en: The approach was applied to a LaMDA-PT language model with 137B parameters using
    retrieval and filters (Sect. [6.​6.​3](528393_1_En_6_Chapter.xhtml#Sec52)). For
    18 NLI tasks the FLAN model was compared to LaMDA-PT 137B, GPT-3 175B, and GLaM
    64B. In 14 of 18 cases FLAN substantially improved the performance of its unmodified
    counterpart and achieved better results than the competitors, while in 4 cases
    it was surpassed by GLaM [[227](#CR227)]. FLAN even outperforms few-shot GPT-3
    by a large margin on a number of tasks.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法被应用于一个具有137B参数的LaMDA-PT语言模型，使用了检索和过滤器（见第[6.6.3](528393_1_En_6_Chapter.xhtml#Sec52)节）。在18个NLI任务中，FLAN模型与LaMDA-PT
    137B、GPT-3 175B和GLaM 64B进行了比较。在18个案例中的14个案例中，FLAN显著提高了其未修改版本的性能，并取得了比竞争对手更好的结果，而在4个案例中则被GLaM超越[[227](#CR227)]。FLAN在许多任务上甚至以大幅优势超越了少样本GPT-3。
- en: 3.6.6 Generating Labeled Data by Foundation Models
  id: totrans-539
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.6 基础模型通过生成标签数据
- en: The performance of GPT-3 and other Foundation Models in few-shot learning enables
    the generation of new high-quality training data for other models. By *Unsupervised
    Data Generation* (*UDG*) the creation of fine-tuning data for models of downstream
    tasks is possible that would otherwise be produced by manual human annotation.
    This approach is similar to Sect. [4.​2.​3](528393_1_En_4_Chapter.xhtml#Sec11).
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3和其他基础模型在少样本学习中的性能使得为其他模型生成新的高质量训练数据成为可能。通过*无监督数据生成*（*UDG*），可以为下游任务的模型创建微调数据，这些数据原本是通过人工标注生成的。这种方法与第[4.2.3](528393_1_En_4_Chapter.xhtml#Sec11)节类似。
- en: The idea for data generation is to utilize the language model to learn the input-label
    relation based on the task description and a few sample input-label pairs [[225](#CR225)].
    Instead of generating and predicting a label for a classification task the language
    model has to create the input text using the output class and a task description
    as input. For a classification task like product reviews on Amazon, the approach
    is able to produce 10k new examples for each class, covering a much larger spectrum
    as the currently available labeled data. It turns out that up to 32 few-shot examples
    still increase the quality of the generated training data. Examples are shown
    in Fig. [3.25](#Fig25). The authors use an additional module to filter out noisy
    examples. In this approach, a given training example is removed if the trained
    classifier does not match its label with high probability.![](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig25_HTML.png)
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 数据生成的想法是利用语言模型根据任务描述和几个样本输入-标签对来学习输入-标签关系[[225](#CR225)]。对于分类任务，语言模型不仅要生成和预测一个标签，还要使用输出类别和任务描述作为输入来创建输入文本。对于像亚马逊产品评论这样的分类任务，该方法能够为每个类别生成10k个新示例，覆盖的谱系比目前可用的标签数据要大得多。结果表明，多达32个少样本示例仍然可以增加生成训练数据的质量。示例在图[3.25](#Fig25)中展示。作者使用了一个额外的模块来过滤掉噪声示例。在这个方法中，如果一个训练示例被训练的分类器以高概率匹配其标签，则该示例将被移除。![图片](../images/528393_1_En_3_Chapter/528393_1_En_3_Fig25_HTML.png)
- en: 3 text boxes represents the prompts in amazon reviews on the left and the Copa
    common sense on the right. Each prompt exhibits its generated answer at the bottom.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 3个文本框代表亚马逊评论左侧的提示和Copa常识右侧的提示。每个提示在底部展示其生成的答案。
- en: Fig. 3.25
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.25
- en: New data can be generated by GPT-3 and other Foundation Models using the few-shot
    UDG strategy. Here the prompts for two examples, Amazon reviews and Copa common
    sense reasoning, and the generated answers are shown [[225](#CR225)]
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用少样本UDG策略，GPT-3和其他基础模型可以生成新数据。这里展示了两个示例的提示，包括亚马逊评论和Copa常识推理，以及生成的答案[[225](#CR225)]。
- en: The T5-XXL encoder-decoder model fine-tuned on SuperGLUE data enhanced with
    UDG data is able to improve the overall accuracy on the SuperGLUE task for natural
    language understanding to 90.4% and is even able to beat DeBERTa with 90.3%. Moreover,
    the approach achieves very high performance scores on a list of text classification
    and sentiment analysis tasks [[225](#CR225)].
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 在SuperGLUE数据上微调的T5-XXL编码器-解码器模型，通过增强UDG数据，能够将自然语言理解任务在SuperGLUE上的整体准确率提升到90.4%，甚至能够以90.3%的准确率击败DeBERTa。此外，该方法在一系列文本分类和情感分析任务上取得了非常高的性能分数[[225](#CR225)]。
- en: 3.6.7 Summary
  id: totrans-546
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.7 摘要
- en: When pre-training Foundation Models on a big text collection and subsequent
    supervised fine-tuning on a small labeled dataset, PLMs achieved unprecedented
    performance on many NLP tasks. Fine-tuning has been shown to change model parameters
    only slightly and, in general, no catastrophic forgetting occurs. Usually, no
    overfitting is observed if fine-tuning is stopped after a few epochs. If necessary,
    there are some approaches to avoid overfitting.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 当在大型文本集合上预训练基础模型并在小标签数据集上进行后续的监督微调时，PLM在许多NLP任务上实现了前所未有的性能。微调已被证明只略微改变模型参数，并且通常在几个epoch后停止微调不会观察到灾难性遗忘。通常，如果微调在几个epoch后停止，就不会观察到过拟合。如果需要，有一些方法可以避免过拟合。
- en: Fine-tuning can be performed in different ways. It has been suggested to use
    an intermediate fine-tuning with a more related dataset before the final fine-tuning
    on the small dataset takes place. The results of such approaches have been mixed.
    Also, simultaneous fine-tuning to several tasks is possible. In some cases, it
    could improve performance. As an alternative, there are strategies to accelerate
    fine-tuning by meta-learning. To avoid that the full model is changed adapter
    layers can be defined, and only their parameters are adapted. This can drastically
    reduce the number of trainable parameters and nevertheless lead to good performance
    on the fine-tuning tasks. Finally, fine-tuning APIs have been recently provided
    for proprietary models like GPT-3.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 微调可以通过不同的方式进行。有人建议在最终在小数据集上进行微调之前，先使用一个与数据集更相关的中间数据集进行微调。这种方法的成果参差不齐。此外，同时针对多个任务进行微调也是可能的。在某些情况下，这可能会提高性能。作为替代方案，有通过元学习加速微调的策略。为了避免改变整个模型，可以定义适配器层，并且只调整它们的参数。这可以大幅减少可训练参数的数量，同时仍然在微调任务上取得良好的性能。最后，最近为像GPT-3这样的专有模型提供了微调API。
- en: Foundation Models like GPT-3 and PaLM can be instructed by prompts to solve
    specific tasks without training. A large number of different prompts has been
    collected to order the model to complete a task. InstructGPT is a new version
    of GPT-3 that directly takes instructions and provides the answers for a large
    spectrum of tasks. The model was customized to carry out the instructions by adapting
    to user judgments through reinforcement learning. Instruction tuning is a variant,
    where a Foundation Model is fine-tuned to provide improved answers to instructions
    for a number of tasks. It turns out that afterwards the model generates better
    answers even for unseen tasks.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型如GPT-3和PaLM可以通过提示来执行特定任务而无需训练。已经收集了大量不同的提示来指示模型完成任务。InstructGPT是GPT-3的一个新版本，它直接接受指令并为广泛的任务提供答案。该模型通过强化学习适应用户判断来定制执行指令。指令微调是一种变体，其中基础模型被微调以提供对多个任务的指令的改进答案。结果证明，之后该模型甚至对未见过的任务也能生成更好的答案。
- en: Finally, big language models may be employed to generate high-quality training
    data for fine-tuning. Again, the few-shot learning technique is used to generate
    input texts for specific learning tasks. In this way, the scarce training data
    can be expanded and better fine-tuning results can be achieved.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以使用大型语言模型来生成高质量的微调训练数据。同样，使用少样本学习技术来生成特定学习任务的输入文本。这样，稀缺的训练数据可以得到扩展，并实现更好的微调结果。
- en: '[![Creative Commons](../css/cc-by.png)](https://creativecommons.org/licenses/by/4.0)'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Creative Commons](../css/cc-by.png)](https://creativecommons.org/licenses/by/4.0)'
- en: '**Open Access** This chapter is licensed under the terms of the Creative Commons
    Attribution 4.0 International License ([http://​creativecommons.​org/​licenses/​by/​4.​0/​](http://creativecommons.org/licenses/by/4.0/)),
    which permits use, sharing, adaptation, distribution and reproduction in any medium
    or format, as long as you give appropriate credit to the original author(s) and
    the source, provide a link to the Creative Commons license and indicate if changes
    were made.'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '**开放获取** 本章节根据Creative Commons Attribution 4.0国际许可协议（[http://creativecommons.org/licenses/by/4.0/](http://creativecommons.org/licenses/by/4.0/)）授权，允许在任何媒介或格式中使用、分享、改编、分发和复制，只要您适当引用原始作者和来源，提供Creative
    Commons许可的链接，并指出是否进行了修改。'
- en: The images or other third party material in this chapter are included in the
    chapter's Creative Commons license, unless indicated otherwise in a credit line
    to the material. If material is not included in the chapter's Creative Commons
    license and your intended use is not permitted by statutory regulation or exceeds
    the permitted use, you will need to obtain permission directly from the copyright
    holder.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中包含的图片或其他第三方材料均包含在章节的Creative Commons许可证中，除非在材料引用行中另有说明。如果材料未包含在章节的Creative
    Commons许可证中，且您的使用意图不受法定法规允许或超出允许的使用范围，您将需要直接从版权持有人处获得许可。
