# 6.图像彩色化

图像彩色化是将颜色添加到原始黑白图像的过程。这意味着艺术家需要计划配色方案，然后花时间费力地手动填充颜色。目前选择的工具是 Photoshop 或类似工具。一张照片可能需要一个月的时间来上色。在这一章中，我们将实现一个简单的卷积神经网络(CNN)模型来理解图像着色是如何工作的。

CNN 模型的灵感来源于人类的视觉。人类的眼睛扫描一个物体，大脑很快地获取该物体的独特特征，以便“识别”它。CNN 模仿这种扫描图像并识别图像的不同特征来识别它的行为。这使它成为影像数据集的理想选择。我们先来看看人类的视觉是如何工作的。然后我们将它与 CNN 模型的工作方式进行比较。

## 人类视觉评论

图 [6-1](#Fig1) 展示了人类视觉的全过程，需要大脑和眼睛同时协同工作。

虽然视觉始于眼睛，但对我们所见的解释发生在大脑中，即初级视觉皮层。当我们看到一个物体时，我们眼中的光感受器通过视神经向初级视觉皮层发送信号，在那里处理输入。

我们能够认出我们生活中看到的所有物体和人。大脑中神经元和连接的复杂层次结构在记忆和标记物体的过程中起着重要作用。

就像一个孩子学习识别物体一样，我们需要对数百万张带标签的图片引入一种算法，然后它才能概括输入，并对它从未见过的图像做出预测。计算机将物体以数字的形式形象化。每幅图像都可以用一组二维数字来表示，称为像素。

在视觉中，单个感觉神经元的*感受域*是视网膜上的特定区域，其中某些东西会激活神经元。每个感觉神经元细胞都有相似的感受野，它们的感受野是重叠的。

*等级观念*在大脑中起着重要作用。信息以连续的顺序存储在模式序列中。位于大脑最外层的*新皮层*，分层存储信息。它储存在皮层柱中，或新皮层中统一组织的神经元群中。

![../images/488562_1_En_6_Chapter/488562_1_En_6_Fig1_HTML.jpg](../images/488562_1_En_6_Chapter/488562_1_En_6_Fig1_HTML.jpg)

图 6-1

人类视觉

你现在应该对人类视觉的工作原理有了基本的了解。让我们看看如何通过使用 CNN 模拟人类视觉，使机器能够“识别”物体。

### 计算机视觉评论

计算机视觉是一个跨学科的科学领域，旨在使机器能够像人类一样看待世界，并以类似的方式感知世界。它寻求自动完成人类视觉系统可以完成的任务，主要使用 CNN。

CNN 的架构类似于人类大脑中神经元的连接模式，并受到视觉皮层组织的启发。单个神经元只在视野的一个有限区域对刺激做出反应，这个区域被称为感受野。这些区域的集合重叠覆盖了整个可视区域。

卷积神经网络(CNN)是一种深度学习算法，可以接受输入图像，为图像中的各个方面/对象分配重要性(可学习的权重和偏差)，并能够区分彼此。

卷积有三个优点:

*   稀疏相互作用

*   参数共享

*   等变表示

CNN 可以通过应用相关的过滤器成功地捕捉图像中的空间和时间依赖性。由于所涉及的参数数量的减少和权重的可重用性，该架构对图像数据集执行更好的拟合。换句话说，网络可以被训练来理解图像的复杂度。

CNN 的作用是将图像简化成一种更容易处理的形式，而不丢失对做出好的预测至关重要的特征。当我们希望设计一个不仅擅长学习特征，而且可扩展到大规模数据集的架构时，这一点非常重要。

CNN 的应用包括:

*   图像和视频识别

*   图像分析和分类

*   媒体娱乐

*   推荐系统

*   自然语言处理

现在您已经知道了 CNN 是什么，以及它在现实生活中的应用，让我们来看看 CNN 内部的过程。

## CNN 是如何工作的

CNN 的隐藏层通常由输入层、卷积层、汇集层和全连接层组成(见图 [6-2](#Fig2) )。每一层都通过可微函数将一个体积转换成另一个体积。

![../images/488562_1_En_6_Chapter/488562_1_En_6_Fig2_HTML.jpg](../images/488562_1_En_6_Chapter/488562_1_En_6_Fig2_HTML.jpg)

图 6-2

CNN 的标准架构

### 输入层

当计算机看到一幅图像(以一幅图像作为输入)时，它将看到一个像素值数组。根据图像的分辨率和大小，它将看到 R x C x 3 的张量，其中 R 表示行，C 表示列，3 表示 RGB 值。这些数字中的每一个都被赋予一个从 0 到 255 的值，该值描述了该点的像素强度。这些数字是计算机仅有的输入。

### 卷积层:内核

卷积层对两个信号进行操作:

*   一维的

*   二维(它接受两幅图像作为输入，产生第三幅图像作为输出)

数学上，卷积层可以表示如下:

![$$ \mathbf{\mathsf{f}}\kern0.5em \left[\mathbf{\mathsf{n}}\right]\kern0.5em =\left(\mathbf{\mathsf{i}}\kern0.5em \mathbf{\mathsf{x}}\kern0.5em \mathbf{\mathsf{k}}\right)\kern0.5em \left[\mathbf{\mathsf{n}}\right]=\sum \limits_{a=-\infty}^{a=\infty}\mathbf{\mathsf{i}}\kern0.5em \left[\mathbf{\mathsf{a}}\right]\kern0.5em \mathbf{\mathsf{k}}\kern0.5em \left[\mathbf{\mathsf{a}}+\mathbf{\mathsf{n}}\right] $$](../images/488562_1_En_6_Chapter/488562_1_En_6_Chapter_TeX_IEq1.png)

特征图:`f`

输入:`i`

内核:`k`

在卷积层的第一部分中执行卷积运算所涉及的元素被称为*内核/滤波器*。卷积运算的目的是提取低层特征，如边缘、颜色、梯度方向等。，来自输入图像。

想象一下，我们在一个黑暗的房间里，只用一个手电筒就可以观察周围的环境。手电筒让我们一次只能看到房间的一小部分。

CNN 以类似的方式工作。手电筒被称为*滤光器*，手电筒覆盖的照明区域被称为*感受野。*

过滤器以一定的步幅值向右移动，直到解析完整的宽度。然后，它以相同的步幅值向下滑动或卷积到图像的开头(左侧),并重复该过程，直到遍历整个图像。

在图像具有多个通道(例如，RGB)的情况下，内核具有与输入图像相同的深度。将所有结果与偏差相加，以产生挤压的单深度通道卷积特征输出。

该操作有两种结果:

*   **有效填充:**与输入相比，卷积特征的维数减少。

*   **相同填充:**维度要么增加，要么保持不变。

从第二卷积层向前，输入是从第一层产生的激活图。因此，输入的每一层基本上都描述了原始图像中某些低级特征出现的位置。现在，当我们在其上应用一组过滤器时(通过第二个卷积层)，输出将是代表更高级特征的激活。当我们通过网络和更多的卷积层时，我们会得到代表越来越复杂功能的激活图。随着我们深入网络，过滤器开始有越来越大的感受域，这意味着它们能够考虑来自原始输入量的更大区域的信息。(换句话说，它们对更大的像素空间区域更敏感。)

Note

滤镜的深度必须与图像的深度相同。

### 上采样层

*上采样层*是一个简单的层，没有权重，会使输入的维度加倍。在传统卷积层之后，它可以用于生成模型。

### DepthwiseConv2D

深度方向可分离卷积仅执行深度方向空间卷积的第一步(分别作用于每个输入通道)。

### 汇集层

汇集是一个基于样本的离散化过程。目标是对输入表示(图像、隐藏层输出矩阵等)进行下采样。)，因此减少了它的维数，并允许对装仓的子区域中包含的特征进行假设。

与卷积层类似，汇集层负责减小卷积要素的空间大小。它也称为缩减像素采样层。这是为了降低处理数据所需的计算能力。通过降维，参数或权重的数量减少了 75%，从而降低了计算成本。此外，它对于提取旋转和位置不变的主要特征是有用的，因此保持了有效训练模型的过程。

池化的工作方式是在特征图上放置一个较小的矩阵，并在该框中选取最大值。矩阵在整个特征图中从左到右移动，每次选取某个值。然后这些值形成一个新的矩阵，称为*汇集特征图。*

共有三种类型的池:

*   **Max pooling:** 返回内核覆盖的图像部分的最大值。

*   最大池也作为噪音抑制剂。它完全丢弃有噪声的激活，并且在降维的同时执行去噪。最大池比平均池表现好得多。Max pooling 通过选择最大值同时减小图像的大小来保留主要特征。这有助于减少过度拟合。

*   **Min pooling:** 选择批次的最小像素值。

*   **Average pooling:** 返回内核覆盖的图像部分的所有值的平均值。平均池简单地执行维数减少作为噪声抑制机制。

### 全连接层

添加全连接层是学习由卷积层的输出表示的高级特征的非线性组合的(通常)廉价方式。这一步由输入层、全连接层和输出层组成。完全连接层类似于人工神经网络中的隐藏层，但在这种情况下，它是完全连接的。输出层是我们获得预测类的地方。信息通过网络传递，计算预测误差。然后，误差通过系统反向传播，以改进预测。它计算类分数并输出大小等于类数量的一维数组。

现在您已经了解了 CNN 是如何工作的，您已经准备好将它应用到项目中了。

## 项目描述

在这个项目中，我们收集了一组彩色图像。这是我们的基准，以便我们知道我们希望我们的模型达到什么结果。然后我们将这些彩色图像转换成灰度。我们将灰度图像分成训练集和测试集。然后，我们将训练集图像及其相应的彩色图像输入到我们的模型(VGG-16 和 CNN 的组合)中，以“学习”图像是如何着色的。然后，为了测试这个模型，我们给它输入一个灰度图像。它自己给这些图像添加颜色。目标是让模型添加颜色，使其看起来令人信服，并尽可能接近原始图像。流程图见图 [6-3](#Fig3) 。

![../images/488562_1_En_6_Chapter/488562_1_En_6_Fig3_HTML.jpg](../images/488562_1_En_6_Chapter/488562_1_En_6_Fig3_HTML.jpg)

图 6-3

图像彩色化流程图

### 关于数据集

名称: Colornet

**来源:** [`www.floydhub.com/emilwallner/datasets/colornet`](http://www.floydhub.com/emilwallner/datasets/colornet)

**创建者:**埃米尔·沃纳

### 重要术语

#### 彩色空间

我们需要知道的第一件重要的事情是使用了 *Lab* 色彩空间。原因很简单:在 RGB 等色彩空间中，我们需要学习三个不同的通道，而在 Lab 中，我们只需要学习两个。通道 L 表示亮度，其值介于 0(暗)和 100(亮)之间。通道 a 和 b 分别是红绿和蓝黄范围之间的轴位置。

Lab 编码的图像有一个灰度层。然后，它将三个颜色层合二为一。这意味着我们可以在最终预测中使用原始灰度图像。那么，我们只有两个渠道可以预测。

使用 Lab 颜色空间的一个很好的原因是它可以保持光强度值的分离。黑白图片可以被视为 L 通道，模型在进行预测时不必学习如何保持正确的光强(如果使用 RGB，则必须这样做)。该模型将只学习如何给图像着色，让它专注于重要的事情。

该模型输出 AB 值，然后可以将其应用于黑白图像以获得彩色版本。

真彩色值从-128 到 128，这是 Lab 色彩空间中的默认间隔。将它们除以 128，它们也落在-1 比 1 的区间内。这使我们能够比较预测的误差。

在计算出最终误差后，网络更新滤波器以减少总误差。网络保持在这个循环中，直到误差尽可能低。1.0/255 表示我们使用的是 24 位 RGB 颜色空间。这意味着我们对每个颜色通道使用 0-255 个数字。这是颜色的标准尺寸，产生 1670 万种颜色组合。

#### 图像彩色化

黑白图像可以用像素网格来表示。每个像素都有一个与其亮度相对应的值。值的范围是 0-255，其中 0 表示黑色，255 表示白色。单通道中的值 0 表示该层中没有颜色。如果所有颜色通道的值都为 0，则图像像素为黑色。

彩色图像由三层组成:

*   红色层

*   绿色层

*   蓝色层

图层不仅决定颜色，还决定亮度。例如，为了获得白色，我们需要每种颜色的平均分布。因此，彩色图像使用这三层对颜色和对比度进行编码。

对于彩色化任务，网络需要找到将灰度图像与彩色图像联系起来的特征。我们正在寻找将灰度值网格与三种颜色网格联系起来的特征。

我们有一个输入的灰度层，我们想预测两个颜色层，Lab 中的 *ab* 。为了创建最终的彩色图像，我们将包括用于输入的 L/灰度图像，从而创建一个 Lab 图像。

为了将一层变成两层，我们使用卷积滤波器。每个滤镜决定了我们在图片中看到的内容。他们可以突出显示或删除某些内容，以从图片中提取信息。网络可以从一个过滤器创建一个新的图像，或者将几个过滤器组合成一个图像。

对于卷积神经网络，每个滤波器都会自动调整，以帮助实现预期的结果。我们将从堆叠数百个过滤器开始，然后将它们缩小到两层，即 *a* 和 *b* 层。

*   输入是代表黑白图像的网格。

*   它输出两个带有颜色值的网格。

*   在输入和输出值之间，我们创建过滤器将它们连接在一起，这是一个卷积神经网络。

当我们训练网络时，我们使用彩色图像。我们将 RGB 颜色转换到 Lab 颜色空间。黑白层是我们的输入，两个彩色层是输出。

我们有黑白输入，或过滤器，以及来自神经网络的预测。

我们将预测值映射到相同区间内的真实值。这样，我们可以比较值。间隔从-1 到 1。为了映射预测值，我们使用了一个`tanh`激活函数。对于我们给`tanh`函数的任何值，它将返回-1 到 1。

我们的神经网络发现了将灰度图像与其彩色版本联系起来的特征。

过程是这样的:

1.  首先，我们寻找简单的图案:一条对角线，全黑像素，等等。

2.  我们在每个方块中寻找相同的图案，并移除不匹配的像素。

3.  如果我们再次扫描图像，我们会看到我们已经检测到的相同的小图案。为了更好地理解图像，我们将图像尺寸缩小了一半。

4.  我们仍然只有一个 3×3 的过滤器来扫描每张图像。但是通过将新的 9 个像素与较低级别的过滤器相结合，我们可以检测到更复杂的图案。

5.  一个像素组合可能形成一个半圆、一个小点或一条线。同样，我们从图像中反复提取相同的模式。这一次，我们生成 128 个新的过滤图像。

如前所述，我们从低级特征开始，比如边。离输出更近的图层组合成图案，再组合成细节，最终转化成人脸。

神经网络以试错的方式运行。它首先对每个像素进行随机预测。基于每个像素的误差，它通过网络反向工作以改进特征提取。

它开始针对产生最大误差的情况进行调整。在这种情况下，是要不要上色以及定位不同的对象。然后它把所有的物体都涂成棕色。它是与所有其他颜色最相似的颜色，因此产生的误差最小。

与其他视觉网络的主要区别在于像素位置的重要性。给网络着色时，图像大小或比例在整个网络中保持不变。在其他网络中，图像越接近最终图层就越失真。

分类网络中的最大池层增加了信息密度，但也扭曲了图像。它只重视信息，而不重视图像的布局。当给网络着色时，我们使用步长 2，将宽度和高度减半。这也增加了信息密度，但不会扭曲图像。

另外两个区别是对图层进行上采样和保持图像比例。分类网络只关心最终的分类。因此，当图像在网络中传输时，它们会不断降低图像的大小和质量。着色网络保持图像比例。这是通过添加白色填充来实现的。否则，每个卷积层都会切割图像。这是用`*padding='same'*`参数完成的。为了将图像的大小加倍，着色网络使用了上采样层。

填充本质上使得由滤波器核产生的特征图与原始图像具有相同的大小。这对于深度 CNN 非常有用，因为我们不希望输出减少，这样我们在网络的末端只剩下一个 2×2 的区域来预测我们的结果。

Note

人类只能感知 200-1000 万种颜色，所以用更大的颜色空间没有太大意义。

#### VGG-16

VGG-16 是一种卷积神经网络(CNN)架构，被认为是一种优秀的视觉模型架构(见图 [6-4](#Fig4) )。该模型具有带 3x3 过滤器和跨度为 1 的卷积层。它始终使用相同的填充和最大池层(2x2 过滤器和步幅为 2 的)。在整个架构中，它始终遵循卷积层和最大池层的这种安排。最后，它有两个 fc(全连接层),后跟一个 Softmax 优化功能。这个网络是一个相当大的网络，它有大约 1.38 亿个参数。

该模型在 ImageNet 中取得了 92.7%的前五名测试准确率，ImageNet 是一个包含属于 1，000 个类别的超过 1，400 万张图像的数据集。它优于 AlexNet，因为它用多个 3×3 内核大小的滤波器一个接一个地替换了大内核大小的滤波器(第一和第二卷积层中分别为 11 和 5 个)。它在 ImageNet 数据库中的超过一百万张图像上进行了训练。该网络有 16 层，可以将图像分为 1000 种对象类别，如键盘、鼠标、铅笔和许多动物。

![../images/488562_1_En_6_Chapter/488562_1_En_6_Fig4_HTML.jpg](../images/488562_1_En_6_Chapter/488562_1_En_6_Fig4_HTML.jpg)

图 6-4

VGG-16 体系结构

#### MAPE 损失函数

平均绝对百分比误差(MAPE)，也称为平均绝对百分比偏差(MAPD)，是统计学中预测方法预测准确性的一种度量。

你现在应该对这个项目有了一个清晰的了解，并且已经学习了一些新的术语，所以让我们继续。

### 必需的库

对于这个项目，我们将使用您在本书第一章中安装的基本库。然而，我们还需要一些额外的库。此项目需要以下库:

*   操作系统(内置 Python2 和更高版本)

*   NumPy(安装说明见第 [1](1.html) 章)

*   熊猫(安装说明见第 [1](1.html) 章)

*   Matplotlib(安装说明见第 [1](1.html) 章)

*   张量流(安装说明见第 [1](1.html) 章)

*   Keras(安装说明见第 [1](1.html) 章)

*   PIL(安装说明在本章中)

*   数学(内置 Python2 和更高版本)

*   随机(内置 Python2 和更高版本)

*   cv2(安装说明在本章中)

*   Scikit-Image(安装说明在本章中)

### 安装说明

在第 [1](1.html) 章，我们安装了每个项目所需的标准库。这些是这个特定项目中使用的附加库的安装说明。为了确保我们可以安装这些库而不用考虑我们的系统，我们将使用名为 PIP 的 Python 包。

#### 安装 PIL

PIL 是 Python 图像库，最初由 Fredrik Lundh 及其贡献者开发。这个库可以用来处理图像。自 2009 年以来，PIL 没有任何发展。所以建议你用枕头代替。

Pillow 是 PIL 的一个分支，由 Alex Clark 和贡献者创建和维护。它以 PIL 电码为基础，然后演变成更好的 PIL 版本。它增加了对打开、操作和保存许多不同图像文件格式的支持。它的很多功能都和最初的 PIL 一样。

Note

截至本书出版时，根据官方网站，PIL 图书馆的状态如下:“当前的免费版本是 PIL 1.1.7。此版本支持 Python 1.5.2 和更新版本，包括 2.5 和 2.6。3 的版本。x 稍后会发布。”Python3 用户可以安装一个名为 Pillow 的分叉版本。

在终端中使用以下命令安装 Pillow。

```
Pip3 install Pillow

```

然后在终端中使用以下命令来检查安装。

```
Pip3 show pillow

```

#### PIL 故障排除

*   或者，您也可以从官方网站 [`http://www.pythonware.com/products/pil/`](http://www.pythonware.com/products/pil/) 下载，手动安装 PIL。

*   卸载任何过时版本的 PIL。

*   Pillow 及以后版本不支持`import image`命令。用`from PIL import image`代替。

*   枕头 2.1.0 及以后版本不支持`import _imaging`。用`from PIL.Image import core as _imaging`代替。

#### 安装 CV2

在终端中使用以下命令安装 CV2:

```
Pip3 install opencv

```

然后在终端中使用以下命令检查 CV2 的安装:

```
Pip3 show cv2

```

#### CV2 故障排除

*   `opencv`有四个不同的包，你应该只选择其中一个。不要在同一环境中安装多个不同的软件包。

#### 安装 Scikit-Image

在终端中使用以下命令安装`scikit-image`。

```
Pip3 install scikit-image

```

然后在终端中使用以下命令检查`scikit-image`的安装。

```
Pip3 show scikit-image

```

#### Scikit 故障排除-图像

*   确保 PIP 已升级。

*   确保您有一个可用的最新 C 编译器。

*   如果在系统上直接安装时出现错误，请尝试使用虚拟环境。

*   有时候`skimage`会给出一个假的错误。要禁用来自`skimage`的错误和警告，导出值为`0`或`False`的环境变量`SKIMAGE_TEST_STRICT_WARNINGS`，并运行如下测试:

```
 export SKIMAGE_TEST_STRICT_WARNINGS=False
pytest --pyargs skimage

```

现在，您应该已经拥有了这个项目所需的所有库。让我们把重点放在我们想要在这个项目中使用的层的类型和每种类型的层的数量上。

### CNN+VGG-16 架构

要使用 CNN+VGG-16 对灰度图像进行着色，我们需要首先加载 VGG-16 模型，这是 TensorFlow 2.0 环境的一部分。这省去了我们从头构建它的麻烦。然后，我们开始构建 CNN，并插入 VGG-16 作为 CNN 的第二层。CNN 的第一层对输入的图像进行整形，这样它们就可以很容易地输入到 VGG-16 中。从那里，输入继续通过 CNN。图 [6-5](#Fig5) 显示了我们将用于本项目的模型。

![../images/488562_1_En_6_Chapter/488562_1_En_6_Fig5_HTML.jpg](../images/488562_1_En_6_Chapter/488562_1_En_6_Fig5_HTML.jpg)

图 6-5

本项目 CNN+VGG-16 的架构

让我们来看看 CNN+VGG-16 的“蓝图”，更好地了解这两种模式是如何结合的。我们的模型将包括以下内容:

*   输入层- 1

*   密集层- 2

*   向上采样 2D - 6

*   DepthwiseConv2D - 3

*   激活层- 3

*   辍学层- 3

*   平均池 2D - 1

我们将使用一个 Keras 输入图层将数据重塑为三个通道:亮度(黑到白)、a(绿到红)和 b(蓝到黄)。密集层是完全连接的，并确保模型中使用了图像中的所有值。

Upsampling2D 层只是将输入的维度加倍。它在输出中重复行和列(由输入提供)。该层没有参数或模型权重，因为它不学习任何东西；它只是加倍了输入。默认情况下，UpSampling2D 将使每个输入维度加倍。此外，默认情况下，UpSampling2D 图层将使用最近邻算法来填充新的行和列。这对于我们的模型是理想的，这样有价值的图像数据在被模型处理时不会丢失。

DepthwiseConv2D 层执行深度方向可分离卷积，这只是深度方向空间卷积的第一步(它分别作用于每个输入通道)。

dropout 层会删除训练过程中获得的一些值，这对于避免过度拟合模型至关重要。

AveragePooling2D 层通过将输入划分为矩形池区域并计算每个区域的平均值来执行下采样。

对于激活函数，由于其梯度的不饱和性，我们将使用 ReLU，这大大加快了随机梯度下降的收敛速度。

对于损失函数，我们将使用 MAPE，如“重要术语”一节所述。由于其尺度无关性和可解释性的优势，它可以帮助我们确定我们的模型工作得有多好。

Adam 优化器是大多数项目使用的标准优化器，包括这个项目。

我们已经规划好了我们的架构。剩下的就是实现模型了。

## 程序

按照以下步骤构建这个项目。

### 第一步。导入库

我们通过导入必要的库来开始这个项目。

```
# Importing Libraries
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout, Input, InputLayer, Conv2D,UpSampling2D,DepthwiseConv2D
from tensorflow.keras.layers import Flatten,MaxPooling2D,Conv2DTranspose, AveragePooling2D
from tensorflow.keras.applications.vgg16  import VGG16
from tensorflow.keras.models import Model,Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import img_to_array,load_img
from PIL import Image
from tensorflow.keras.utils import plot_model
from math import ceil
import random
import cv2
from skimage import io, color

```

设置文件路径，以便 Jupyter 笔记本可以访问数据集。使用`os.path`命令并输入文件路径。

```
os.path=("Macintosh HD/Users/vinitasilaparasetty□/Downloads/")

```

### 第二步。将图像转换为灰度

然后我们将所有的图像转换成灰度，这样我们就有了一组彩色图像和它们相应的灰度版本。

*   `random.sample()`是 Python 中`random`模块的内置函数，返回从序列中选择的特定长度的项目列表。它用于无替换的随机抽样。

*   `cv2.cvtColor(rgb, cv2.COLOR_BGR2LAB)`是 CV2 中的一个函数，用于将颜色空间从 RGB 转换到 LAB。

*   `cv2.split(lab_image)`将图像分割成三个通道。

Note

`cv2.split()`使用更多的计算时间。NumPy 索引是一个很好的选择，但是结果可能不太准确。

```
rootdir = os.getcwd()
filenames = random.sample(os.listdir('D:\\Proj\\vinita\\colornet\\'), 500)
lspace=[]
abspace=[]
for file in filenames:
   rgb = io.imread(file)
   lab_image = cv2.cvtColor(rgb, cv2.COLOR_BGR2LAB) #convert colors space from RGB to LAB
   l_channel,a_channel,b_channel = cv2.split(lab_image)
   lspace.append(l_channel)
   replot_lab=np.zeros((256, 256, 2))
   replot_lab[:,:,0] = a_channel
   replot_lab[:,:,1] = b_channel
   abspace.append(replot_lab)
   transfer = cv2.merge([l_channel, a_channel, b_channel])
   transfer = cv2.cvtColor(transfer.astype("uint8"), cv2.COLOR_LAB2BGR)

lspace=np.asarray(lspace) #convert to array
abspace=np.asarray(abspace) #convert to array

```

### 第三步。加载数据

将`lspace`加载为`X`，将`abspace`加载为`Y`。`lspace`表示图像的亮度，`abspace`是`a`和`b`通道的组合值，它们位于红绿和蓝黄范围之间的轴上。

```
X = np.load("lspace100.npy")
Y = np.load("abspace100.npy")

```

### 第四步。构建模型

我们准备创建 CNN+VGG-16。使用下面的代码。

```
model6 = VGG16(weights='imagenet',include_top=False,input_shape=(256, 256, 3))
model = Sequential()
model.add(InputLayer(input_shape=(X.shape[1], X.shape[2], 1)))
model.add(layers.Dense(units=3))
model.add(Model(inputs=model6.inputs, outputs=model6.layers[-10].output))
model.add(UpSampling2D((2, 2)))
model.add(UpSampling2D((2, 2)))
model.add(DepthwiseConv2D(32, (2, 2), activation="tanh", padding="same"))
model.add(UpSampling2D((2, 2)))
model.add(DepthwiseConv2D(32, (2, 2), activation="tanh", padding="same"))
model.add(layers.ReLU(0.3))
model.add(layers.Dropout(0.4))
model.add(UpSampling2D((2, 2)))
model.add(UpSampling2D((2, 2)))
model.add(DepthwiseConv2D(2, (2, 2), activation="tanh", padding="same"))
model.add(layers.ReLU(0.3))
model.add(layers.Dropout(0.2))
model.add(UpSampling2D((2, 2)))
model.add(layers.ReLU(0.3))
model.add(layers.Dropout(0.2))
model.add(AveragePooling2D(pool_size = (2, 2)))
model.add(layers.Dense(units=2))
print(model.summary())

```

### 第五步。设置模型参数

为了完成 CNN 的架构，我们通过创建一个函数来设置优化器和损失函数，如下所示:

```
def adam_optimizer():
   return Adam(lr=0.001, beta_1=0.99, beta_2=0.999)
model.compile(loss='mape', optimizer=adam_optimizer())

```

### 第六步。数据准备

在将数据输入模型之前，我们需要对其进行整形，以便使用以下代码将值输入适当的通道:

```
X=((X.reshape(X.shape[0],X.shape[1],X.shape[2],1)))
X=(X-255)/255
Y=(Y-255)/255

trainsize= ceil(0.8 * X.shape[0])
testsize= ceil(0.2 * X.shape[0])+1

train_inp=X[:trainsize,]
test_inp=X[testsize:,]

train_out=Y[:trainsize,]
test_out=Y[testsize:,]

```

### 第七步。训练模型

我们现在可以训练模型:

```
model.fit(x=train_inp, y=train_out, batch_size=10, epochs=5)

```

### 第八步。获得预测

现在我们的模型已经训练好了，它已经学会了如何给图像添加颜色，使它看起来尽可能自然。让我们通过使用以下代码将新的灰度图像输入模型来检查结果:

```
train_pred = model.predict(train_inp)
test_pred = model.predict(test_inp)

train_random=random.randint(1,trainsize)
test_random=random.randint(1,testsize)

check=np.interp(train_pred, (train_pred.min(), train_pred.max()), (0,255))
check1=np.interp(test_pred, (test_pred.min(), test_pred.max()), (0,255))

l_channel=test_inp[20]*255
a_channel=check1[20,:,:,0]
b_channel=check1[20,:,:,1]

transfer = cv2.merge([l_channel, a_channel, b_channel])
transfer = cv2.cvtColor(transfer.astype("uint8"), cv2.COLOR_LAB2BGR)

```

### 第九步。查看结果

让我们来看看我们的图像着色模型的结果。

```
plt.imshow(transfer)

```

需要考虑的几点:

*   我们在这个项目中使用的数据集只有彩色图像。我们需要将图像转换为灰度，以便得到一组彩色图像和一组灰度图像。

*   大多数在线数据集只包含彩色图像，因此我们必须手动将图像转换为灰度，然后才能使用它们来训练模型。

*   因为大多数训练数据都非常相似，所以网络很难区分不同的对象。它会调整不同的棕色调，但无法生成更细微的颜色。

## 解决纷争

以下是您可能会遇到的一些常见错误，这些错误很容易修复:

*   如果 PIL 安装给您带来了麻烦，请尝试卸载它，然后使用以下命令进行升级:

*   VGG 模型相当大，需要相当大的内存，所以使用有大量内存的系统或者使用云。

*   如果显示此警告，您可以忽略它，因为它不会影响程序:

```
pip uninstall PIL
pip install PIL —upgrade

```

```
WARNING:tensorflow:From /usr/local/lib/python3.6
/dist-packages/tensorflow/python/framework
/op_def_library.py:263: colocate_with
(from tensorflow.python.framework.ops) is
deprecated and will be removed in a future version.
Instructions for updating: Collocations handled
automatically by placer.

```

## 进一步测试

这里有一些想法可以尝试，并从这个项目中学到更多:

*   尝试单独使用 VGG-16，看看结果有什么不同。

*   试着只用彩色图像训练模型，看看模型表现如何。

*   尝试其他色彩空间，看看结果如何变化。

*   尝试最小/最大池，而不是平均池。

*   移除上采样层，看看它如何影响结果。

*   移除脱落层，查看结果有何不同。

## 摘要

下面是你在本章中学到的所有内容的快速回顾。

*   计算机将物体以数字的形式形象化。每幅图像都可以用一组二维数字来表示，称为像素。

*   CNN 可以通过应用相关的过滤器成功地捕捉图像中的空间和时间依赖性。

*   滤镜的深度必须与图像的深度相同。

*   有三种类型的池:最小池，最大池，平均池。

*   对于 RGB，我们需要学习三个不同的通道，而在实验室中，我们只需要学习两个。

*   Lab 颜色空间保持光强度值分开。真彩色值从-128 到 128，这是 Lab 色彩空间中的默认间隔。将它们除以 128，它们也落在-1 比 1 的区间内。

*   通道 L 表示亮度，其值介于 0(暗)和 100(亮)之间。

*   像素值范围为 0 - 255，其中 0 表示黑色，255 表示白色。

*   平均绝对百分比误差(MAPE)，也称为平均绝对百分比偏差(MAPD)，是统计学中预测方法预测准确性的一种度量。

## 参考

本章中使用的参考资料如下:

*   [T2`www.ncbi.nlm.nih.gov/pmc/articles/PMC1359523/pdf/jphysiol01247-0121.pdf`](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1359523/pdf/jphysiol01247-0121.pdf)

*   [T2`https://docs.w3cub.com/tensorflow~python/tf/keras/layers/depthwiseconv2d/`](https://docs.w3cub.com/tensorflow%257Epython/tf/keras/layers/depthwiseconv2d/)

*   [T2`www.statisticshowto.datasciencecentral.com/mean-absolute-percentage-error-mape/`](http://www.statisticshowto.datasciencecentral.com/mean-absolute-percentage-error-mape/)

## 进一步阅读

有兴趣了解本章中涉及的一些主题吗？这里有一些很棒的链接可以查看:

*   色彩空间:

    [T2`https://programmingdesignsystems.com/color/color-models-and-color-spaces/index.html`](https://programmingdesignsystems.com/color/color-models-and-color-spaces/index.html)

*   人类视觉:

    [T2`www.stat.auckland.ac.nz/~ihaka/120/Notes/ch04.pdf`](http://www.stat.auckland.ac.nz/%257Eihaka/120/Notes/ch04.pdf)

*   计算机视觉:

    [T2`www.sas.com/en_in/insights/analytics/computer-vision.html`](http://www.sas.com/en_in/insights/analytics/computer-vision.html)