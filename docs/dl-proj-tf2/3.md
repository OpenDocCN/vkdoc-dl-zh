# 3.神经网络

这里有一些值得思考的问题:如果单个神经元足够强大，可以执行二进制分类，就像你在上一章看到的那样，那么神经元的集合会强大多少？这就是我们在本章将要发现的。几个神经元一起构成了一个神经网络。在本教程中，我们将创建一个多层神经元，然后用它对 MNIST 数据集进行分类。在 Keras 中，一次迭代被称为一个*时期*。让我们更详细地研究神经网络。

## 什么是神经网络？

神经网络也称为人工神经网络(ANN)。它包含被称为神经元的互连节点层。神经网络利用一系列算法来检测数据集中的潜在关系。神经网络适应内部和外部参数。网络生成最佳可能输出，而不必修改输出标准。神经网络基于自适应学习。他们使用几个原则，包括基于梯度的训练、[模糊逻辑](https://searchenterpriseai.techtarget.com/definition/fuzzy-logic)和[贝叶斯方法](https://whatis.techtarget.com/definition/Bayesian-logic)。人工神经网络用于序列和模式识别系统、数据处理、机器人以及许多其他系统。

## 神经网络组件

神经网络由以下主要组件组成:

*   **输入层:**输入到神经网络的输入。

*   **隐藏层:**包含任意数量的神经元，取决于我们想要达到的目标。

*   **输出层:**得到的最终输出。

## 神经网络的优势

神经网络具有以下优势:

*   当神经网络的一个元素失效时，它将继续发挥作用。这是由于它的并行性。

*   神经网络不需要为看不见的数据重新编程，因为它是一个自适应的智能系统。

*   神经网络擅长解决复杂的问题。

*   神经网络比传统网络具有更强的容错能力。在不丢失存储数据的情况下，网络可以在其任何组件中再生故障。

## 神经网络的缺点

神经网络有以下挫折:

*   大型神经网络需要大量的处理时间。

*   神经网络的架构不同于微处理器的[架构和历史](https://www.elprocus.com/microprocessor-history-and-brief-information-about-its-generations/)，所以它们必须被仿真。

*   增加中子的数量会增加网络的复杂性。

*   成功训练网络需要最佳的数据量。

*   确定层数以及每层神经元的数量需要一些反复试验。这很费时间。

我们在理论上知道什么是神经网络，但是在实践中呢？让我们来看看神经网络是如何工作的。

## 神经网络如何工作

正如你在前一章看到的，神经元内发生的过程是:

1.  正向传播

    输入➤计算净输入➤激活函数应用➤输出

2.  反向传播

    输出➤激活函数应用于净输入➤输入的➤计算

现在让我们将这个过程扩展到神经网络中的大量神经元。

让我们以一个有十个神经元的浅层神经网络为例。在这种类型的网络中，只有一个隐藏层(见图 [3-1](#Fig1) )。

![../images/488562_1_En_3_Chapter/488562_1_En_3_Fig1_HTML.jpg](../images/488562_1_En_3_Chapter/488562_1_En_3_Fig1_HTML.jpg)

图 3-1

使用 MNIST 的浅层神经网络示例

## 正向传播

输入被馈送到隐藏层中的每个神经元。这意味着在隐藏层中，每个神经元都有自己的一套权重和一个偏好。整个过程是正向的，因此称为正向传播。以下是这三层的细分:

*   **输入层:**根据图 [3-1](#Fig1) ，有四个输入。这里，x1、xn-1 和 xn 是变量。我们把一个作为常数，这样它就不会影响偏差。

*   **隐含层:**在图 [3-1](#Fig1) 中，有四个输入，所以每个神经元有四个权值和单个偏置。激活函数应用于每个神经元的净输入。根据图表，我们从隐藏层得到十个输出。每个神经元都有一个。在这种情况下，我们使用 Softmax 激活函数。

*   **输出层:**我们得到十个类别，每个类别对应一个数字，y1，y2..… y10。

Note

根据预期输出的数量，隐藏层可以具有 Sigmoid 函数或 Softmax 函数。如果需要两个输出，则使用 Sigmoid。如果预期有两个以上的输出，则使用 Softmax。

对于具有多个隐藏层的神经网络，在层之间使用的最流行的激活函数是 ReLU 函数。

一旦输入被馈入并被正向处理，就该检查并调整它们，以便通过反向传播获得更好的结果。

## 反向传播

我们在前一章看到，反向传播发生在神经网络的每一层。每层中每个神经元的权重和偏置被调整，直到实现收敛。该过程从输出层移动到隐藏层，最后移动到输入层。

有了这些关于神经网络的知识，让我们看看最常见的神经网络类型。

## 神经网络的类型

在前面的例子中，我们学习了什么是浅层神经网络。现在，让我们快速看一下神经网络是如何大致分类的。

每个神经网络都有几种变体和组合。所以，实际上有几种神经网络可用。对于本书中的项目，我们主要关注人工神经网络的大类。

## 前馈神经网络

在前馈神经网络中，数据仅在一个方向上从第一层向前移动，直到到达输出节点(见图 [3-2](#Fig2) )。这也称为*波前传播波*，通常通过使用分类激活函数来实现。没有反向传播，它可以有一个单一的层或多个隐藏层。

前馈神经网络的用途包括:

![../images/488562_1_En_3_Chapter/488562_1_En_3_Fig2_HTML.png](../images/488562_1_En_3_Chapter/488562_1_En_3_Fig2_HTML.png)

图 3-2

样本前馈神经网络(HC 表示隐藏单元)

*   人脸识别

*   计算机视觉

## 卷积神经网络

卷积神经网络(CNN)是多层神经元的正则化版本，包含一个或多个卷积层(见图 [3-3](#Fig3) )。这些层可以完全互连，也可以汇集在一起。卷积层对输入使用卷积运算。

CNN 的用途包括:

![../images/488562_1_En_3_Chapter/488562_1_En_3_Fig3_HTML.jpg](../images/488562_1_En_3_Chapter/488562_1_En_3_Fig3_HTML.jpg)

图 3-3

样本卷积神经网络(K 表示核，C/P 表示卷积或池，HC 表示隐藏单元)

*   图像和视频识别

*   自然语言处理

*   推荐系统

## 递归神经网络(RNN)

递归神经网络(RNN)保存特定层的输出，并将其作为输入进行反馈。递归神经网络过程在第一层之后的层中开始。每个节点在计算和执行操作时充当一个存储单元，因此在反向传播期间，它可以将以前的值与新值进行比较(见图 [3-4](#Fig4) )。rnn 将在第 [9](9.html) 章中详细讨论。

RNN 的用途包括文本到语音转换技术。

![../images/488562_1_En_3_Chapter/488562_1_En_3_Fig4_HTML.png](../images/488562_1_En_3_Chapter/488562_1_En_3_Fig4_HTML.png)

图 3-4

样本递归神经网络(RC 表示递归细胞)

## 径向基函数神经网络

RBNN 考虑任意点相对于中心的距离。它应用径向基函数作为其激活函数(见图 [3-5](#Fig5) )。有两层。在内层，特征与径向基函数相结合。那么当在下一时间步中计算相同的输出时，考虑这些特征的输出。这样，它实现了线性可分性。

RBNN 的用途包括电力恢复系统。

![../images/488562_1_En_3_Chapter/488562_1_En_3_Fig5_HTML.png](../images/488562_1_En_3_Chapter/488562_1_En_3_Fig5_HTML.png)

图 3-5

样本径向基函数神经网络(HC 表示隐藏单元)

现在你知道了神经网络的基本类别。当我们在本书的项目中工作时，我们将使用神经网络，它们属于这里讨论的子类。

## 项目描述

在这个项目中，我们使用单层神经网络对一组手写数字进行分类。我们首先将二维矩阵中的数据展平为一维数组。然后我们用 Softmax 激活函数把它输入神经网络。输出是一个从 0 到 9 的数字。图 [3-6](#Fig6) 显示了该过程的流程图。

![../images/488562_1_En_3_Chapter/488562_1_En_3_Fig6_HTML.png](../images/488562_1_En_3_Chapter/488562_1_En_3_Fig6_HTML.png)

图 3-6

对 MNIST 数据使用简单神经网络的流程图

## 扁平化数据

MNIST 的数据完全由手写数字的二维图像组成。这意味着数据是二维矩阵的形式。这对于神经网络来说不容易处理。所以我们通过把它转换成一维数据来“扁平化”。二维矩阵被转换成一维数组。这样，神经网络可以更好地处理数据，并给出更好的结果。参见图 [3-7](#Fig7) 。

![../images/488562_1_En_3_Chapter/488562_1_En_3_Fig7_HTML.jpg](../images/488562_1_En_3_Chapter/488562_1_En_3_Fig7_HTML.jpg)

图 3-7

拼合 MNIST 数据

## 关于数据集

**名称:** MNIST 数据库

**内容:**手写数字的 MNIST 数据库具有 60，000 个样本的训练集和 10，000 个样本的测试集。它是可从 MNIST 获得的更大集合的子集。数字已经过大小标准化，并在固定大小的图像中居中。

**来源:** [`http://yann.lecun.com/exdb/mnist/`](http://yann.lecun.com/exdb/mnist/)

## 必需的库

我们的项目需要以下库:

*   硬

*   TensorFlow 2.0

## 神经网络体系结构

我们将使用一个简单的神经网络架构，由以下部分组成:

*   **输入层:**数据作为特征输入到输入层。神经元计算网络输入函数。

*   **输出层:**这一层包含十个神经元，用于数据的每个可能类别。

*   **激活功能:** Softmax

*   **损失函数:**稀疏分类交叉熵

*   **优化器:**亚当

*   **度量:**准确度

## 程序

按照以下步骤创建一个多层神经元:

1.  导入库。使用`import`命令导入 TensorFlow 2.0 和 Keras。

1.  读入数据。Keras 加载了一组数据集。MNIST 是其中之一。所以可以直接从 Keras 加载。然后将数据分成两组，分别用于测试和训练。

```py
#import libraries.
import keras
import tensorflow as tf

```

```py
mnist=tf.keras.datasets.mnist

#split data
(x_train, y_train), (x_test, y_test) = mnist.load_data()

```

我们的输出应该是这样的。

1.  数据处理。MNIST 数据集由二维图像组成。为了方便起见，将它们展平成一维图像。

```py
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 0s 0us/step

```

1.  创建神经网络。使用`Sequential`模块创建一个单独的隐藏层。这个隐藏层有十个神经元。我们将使用的激活函数是 Softmax。接下来，您可以定义优化器。我们将使用 Adam 优化器，因为我们只是对数据进行基本的分类。我们将损失函数定义为稀疏分类交叉熵，因为输出只能是十个可能类别中的一个。为了测试这个模型的能力，使用`accuracy`指标。

```py
x_train, x_test = x_train / 255.0, x_test / 255.0

```

1.  训练模型。使用`fit`模块，训练新创建的模型。设置模型的迭代。首先，将其设置为三个历元，并检查准确度分数。

```py
digit = tf.keras.models.Sequential([
   tf.keras.layers.Flatten(input_shape=(28, 28)),
   tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

#compile model
digit.compile(optimizer='adam',
             loss='sparse_categorical_crossentropy',
             metrics=['accuracy'])

```

```py
digit.fit(x_train, y_train, epochs=3)

```

我们的输出应该如下所示:

1.  评估模型。使用`evaluate`模块，测试模型。测试完成后，检查准确度分数。只需三次迭代，您就可以获得 92%的准确率。

```py
Train on 60000 samples
Epoch 1/3
60000/60000 [==============================] - 3s 47us/sample - loss: 0.4670 - acc: 0.8788
Epoch 2/3
60000/60000 [==============================] - 3s 44us/sample - loss: 0.3036 - acc: 0.9151
Epoch 3/3
60000/60000 [==============================] - 3s 45us/sample - loss: 0.2834 - acc: 0.9204
<tensorflow.python.keras.callbacks.History at 0x7f8709047358>

```

```py
digit.evaluate(x_test, y_test)

```

输出应该类似于以下内容:

1.  进一步的实验。尝试更改历元数，看看它如何影响精确度。

```py
10000/10000 [==============================] - 0s 26us/sample - loss: 0.2765 - acc: 0.9229
[0.2764906617820263, 0.9229]

```

## 摘要

以下是您在本章中学到的所有内容的回顾:

*   神经网络也称为人工神经网络(ANN)。

*   神经网络基于自适应学习。

*   神经网络使用几种原理，包括基于梯度的训练、[模糊逻辑](https://searchenterpriseai.techtarget.com/definition/fuzzy-logic)和[贝叶斯方法](https://whatis.techtarget.com/definition/Bayesian-logic)。

*   神经网络具有并行性。

*   在不丢失存储数据的情况下，网络能够在其任何组件中再生故障。

*   大型神经网络需要大量的处理时间。

*   增加中子的数量会增加网络的复杂性。

*   成功训练网络需要最佳的数据量。

*   确定层数以及每层神经元的数量需要一些反复试验。这很费时间。

*   根据预期输出的数量，隐藏层可以具有 Sigmoid 函数或 Softmax 函数。如果有两个预期输出，则使用 Sigmoid。如果有两个以上的预期输出，则使用 Softmax。

*   前馈神经网络是一种 ANN，其中节点之间的连接不形成循环。

*   递归神经网络应用反向传播。

*   RBNN 是一个单层神经网络，其激活函数是径向基函数。它实现了线性分离。

*   数据扁平化包括将其从一种格式转换为另一种格式。例如，在这个项目中，我们通过将 MNIST 数据从二维矩阵转换为一维数组来将其扁平化。

*   通过第一个项目，您学习了如何处理大型数据集，如何将数据集拆分为测试数据集和定型数据集，以及如何为模型准备数据。

*   您现在应该很清楚如何构造一个简单的神经网络，如何使用训练数据对其进行训练，以及如何使用测试数据对其进行测试。

## 参考

本章中使用的资源如下:

*   [`www.linkedin.com/pulse/artificial-neural-networks-advantages-disadvantages-maad-m-mijwel/`优缺点:](https://www.linkedin.com/pulse/artificial-neural-networks-advantages-disadvantages-maad-m-mijwel/)

*   神经网络的类型: [`www.digitalvidya.com/blog/types-of-neural-networks/`](https://www.digitalvidya.com/blog/types-of-neural-networks/)