# 5.音乐一代

每个人都喜欢音乐。不管我们的心情或偏好如何，总有一首歌适合我们所有人。考虑到这一点，在本章中，我们将使用一种叫做门控递归单元(GRU)的神经网络来生成一首歌曲。别担心，你不需要成为一名专业的音乐家来使用深度学习生成音乐。

## GRU 概述

GRUs 旨在解决标准递归神经网络(RNN)带来的消失梯度问题。当梯度变得越来越小时，消失梯度问题出现在深度神经网络中，这阻止了权重改变其值。

为了解决梯度消失的问题，GRUs 使用了一个更新门和一个重置门。这些门是决定什么信息应该被传递到输出的两个向量。他们可以被训练储存很久以前的信息，而不会随着时间的推移而抹去。他们还可以删除与预测无关的信息。

与 RNN 之前拥有的四个节点的简单神经网络不同，GRU 拥有一个包含多种操作的单元。GRU 将信息向前传递到几个时间段，以便影响未来的时间段。换句话说，该值会在内存中存储一段时间。在一个临界点，它被取出并与当前状态一起使用，以便在将来的某个日期进行更新。GRU 网络涉及的参数更少，这使得它们的训练速度更快。网络学会如何使用门来保护它的记忆，这样它就能够在更长的时间内进行长期预测。类似于传统的 RNNs，GRU 网络在每个时间步产生一个输出，这个输出用于使用梯度下降来训练网络。

GRU 网络的应用包括:

*   复调音乐造型

*   语音信号建模

*   手写识别

*   语音识别

*   人类基因组的研究

*   股票市场分析

## GRU 是如何工作的

GRU 是递归神经网络的门控版本。隐藏状态用于传输信息。GRU 是直观的，因为它能够确定将多少过去的信息传递给未来，而无需显式编程。与 LSTM 不同，GRU 只有两个门:

*   **更新门**:更新门帮助模型确定有多少过去的信息(来自以前的时间步骤)需要传递给未来。这真的很强大，因为该模型可以决定复制过去的所有信息，并消除梯度消失问题的风险。

*   **重置门**:本质上，该门用于从模型中决定要忘记多少过去的信息。

图 [5-1](#Fig1) 显示了一个 GRU 的内部工作方式，这里`Rg(t)`是复位门，`Ug(t)`是更新门。

![../images/488562_1_En_5_Chapter/488562_1_En_5_Fig1_HTML.jpg](../images/488562_1_En_5_Chapter/488562_1_En_5_Fig1_HTML.jpg)

图 5-1

在 GRU 里面

### 起重机实习

让我们来看看 GRU 每个阶段的过程。记住，GRU 的输入是三维数组的形式。

#### 第一阶段

电流存储器的功能非常像一个门，但它集成在复位门中，用于在输入中引入一些非线性，并使输入为零均值。此外，它减少了先前信息对当前信息的影响，而当前信息会传递到未来。请遵循以下步骤:

1.  引入新的存储内容，使用复位门存储过去的相关信息。输入`nt`乘以权重`W`和`Hs(t-1`<sub>`)`</sub>`Rgt`。

2.  计算复位门`Rgt`和`Rgt * Hs(t-1)` *之间的哈达玛乘积。*这将决定从之前的时间步骤中删除什么。

3.  根据输入到矢量中的输入，`Rgt`矢量被赋予一个接近于`0`或`1`的值。

4.  将第 1 步和第 2 步的结果相加。

5.  应用非线性激活功能`tanh`。数学表示如下:

    Hs<sub>t</sub>' = tanh(W【Rg】t* Hs<sub>(t-1)</sub>，n <sub>t</sub> )

#### 第二阶段

本质上，该模型使用重置门来决定忘记多少过去的信息。它类似于 LSTM 循环单元中输入门和遗忘门的组合。总误差由所有时间步的误差总和给出。类似地，该值可以计算为每个时间步长的梯度总和。请遵循以下步骤:

1.  将输入的`nt`乘以权重`Wrg`，将`Hst-1`乘以`Wrg` *。*

2.  计算步骤 1 中的乘积之和。

3.  对结果应用 Sigmoid。

模型可以学习将向量 Rg <sub>*t*</sub> 设置为接近`0`或`1`。如果向量`Rgt`接近`0`，当前内容的大部分将被忽略，因为它与我们的预测无关。同时，由于在这个时间步`Rgt`将接近`0`，因此`1-Rgt`将接近`1`，从而允许保留大部分过去的信息。

数学表示如下:

*   Rg <sub>*t*</sub> =乙状结肠(W<sub>rg</sub>【Hs<sub>t-1</sub>，n <sub>t</sub> )

#### 第 3 阶段

更新门是模型可以决定从过去复制所有信息并消除消失梯度问题的风险的地方。请遵循以下步骤:

1.  将输入的`nt`乘以权重`Wug`，将`Hst-1`乘以`Wug` *。*

2.  计算步骤 1 中的乘积之和。

3.  步骤 3:对结果应用 Sigmoid。

模型可以学习将向量 Ug <sub>*t*</sub> 设置为接近`0`或`1`。如果向量`Ugt`接近`0`，当前内容的大部分将被忽略，因为它与我们的预测无关。同时，由于`Ugt`在这个时间步将接近`0`，(`1-Ugt`)将接近`1`，允许保留大部分过去的信息。

数学表示如下:

*   Ug <sub>t</sub> =乙状结肠(W <sub>ug</sub> [ Hs <sub>t-1</sub> ，n <sub>t</sub> )

#### 第四阶段

当前时间步长的存储器是 GRU 的最后一级。网络计算保存当前单元信息的向量`Hst`，并将其传递给网络。

为此，更新门确定从当前存储器内容`Hst`收集什么，以及从先前步骤`Hs(t-1)`收集什么。这是按如下方式完成的:

1.  将逐元素乘法应用于更新门`Ugt`和`Hs(t-1` <sub>`)`</sub> *。*

2.  对`(1-Rg` <sub>`t`</sub> `)`和`Hs`<sub>`t`</sub>’*应用逐元素乘法。*

3.  将第 1 步和第 2 步的结果相加。

    ' Hs<sub>t</sub>=(1-Rg<sub>t</sub>)*(Hs<sub>t-1</sub>)+Ug<sub>t</sub>* Hs<sub>t</sub>'

### 格鲁层

为了设计有效的 GRU，我们可以利用以下类型的层:

*   **嵌入层:**用于为进来的词创建词向量。它位于输入层和 GRU 层之间。嵌入层的输出是 GRU 层的输入。

*   **GRU 层:**递归神经网络层，将序列作为输入，可以返回序列或矩阵。

*   **丢弃层:**一种正则化技术，包括在训练期间的每次更新时将输入单元的一部分设置为 0，以防止过拟合。

*   **密集层:**全连接层，其中每个输入节点连接到每个输出节点。

*   **激活层:**决定 GRU 使用哪个激活函数来计算节点的输出。

Note

`keras.layers.CuDNNGRU`提供由 cuDNN 支持的快速 GRU 实施。NVIDIA CUDA 深度神经网络库(cuDNN)是一个 GPU 加速的深度神经网络图元库。cuDNN 为标准例程提供了高度优化的实现，例如前向和后向卷积、池化、规范化和激活层。

### 比较 GRU 和 LSTM

乍一看，LSTMs 和 GRUs 似乎非常相似。但是，需要注意两者之间的细微差别:

*   GRU 只有两个大门，而 LSTM 有三个。

*   LSTMs 控制内存内容(单元状态)的公开，而 gru 将整个单元状态公开给网络中的其他单元。LSTM 单元具有独立的输入和遗忘门，而 GRU 通过其复位门同时执行这两种操作。

*   LSTM 给了我们最大的控制，因此，更好的结果。它确实带来了更多的复杂性和更高的运营成本。

与 LSTM 相比，GRU 使用的训练参数更少，因此使用的内存更少，执行速度更快，训练速度更快，而 LSTM 在使用较长序列的数据集上更准确。

你现在知道什么是 GRU 以及它与 LSTM 有什么不同了。是时候看看如何将 GRU 变成音乐 DJ 了。

## 项目描述

在这个项目中，我们将创建一个 GRU，它将根据给定的音符序列生成音乐。一旦模型被训练，当一组随机的音符被提供作为输入时，它应该能够预测接下来的音符序列。数据集由 MIDI 文件组成，我们从中提取音乐成分作为模型的特征。GRU 模型需要以下两个对象来生成音乐:

*   **音符对象:**包含关于音高、八度和偏移的信息。

*   **和弦对象:**一组音符的容器。

然后使用*一键编码*的过程将音符和和弦对象转换成整数向量。这使得模型能够学习各种音乐作品中的模式。然后，该预测将从整数向量重新转换为音符，并保存为 MIDI 文件。目标是获得一个听起来尽可能与原曲相似的 MIDI 文件。

![../images/488562_1_En_5_Chapter/488562_1_En_5_Fig2_HTML.jpg](../images/488562_1_En_5_Chapter/488562_1_En_5_Fig2_HTML.jpg)

图 5-2

GRU 音乐项目工作流程

### 关于数据集

**名称:** 13 万个 MIDI 文件集合

流派包括:流行音乐、古典音乐(钢琴/小提琴/吉他)、电子音乐、电子游戏和电影/电视主题

**来源:** [`www.reddit.com/r/datasets/comments/3akhxy/the_largest_midi_collection_on_the_internet/`](http://www.reddit.com/r/datasets/comments/3akhxy/the_largest_midi_collection_on_the_internet/)

**创作人:**迷笛侠

在你开始创作音乐之前，你需要理解一些术语和概念。

### 重要术语和概念

在开始这个项目之前，这里有一些你应该知道的方便的术语和概念。

![../images/488562_1_En_5_Chapter/488562_1_En_5_Fig3_HTML.jpg](../images/488562_1_En_5_Chapter/488562_1_En_5_Fig3_HTML.jpg)

图 5-3

一键编码工作流程

*   **注:**表示音乐声音的符号。它用乐谱表示声音的音高和音长。

*   **和弦:**同时演奏的两个或两个以上音符的组合。

*   **音高:**声音的频率，或者说高低。用字母 A 到 g 表示。

*   **偏移:**音符在乐曲中的位置。

*   **八度音程:**占据两个音之间(包括两个音在内)间隔的一系列八个音，其中一个音的振动频率是另一个音的两倍或一半。

*   **流:**music 21 对象的基本容器；在 Music21 中，对象可以基于从该容器的开始的偏移来按时间排序和/或放置。

*   **MIDI 文件:**(读作“mid-ee”):乐器数字接口文件的缩写。它有两个扩展--。MID 或. MIDI。它不包含实际的音频，只包含诸如音高、和弦、音符等信息。

*   **分类交叉熵:**用于单标签分类的损失函数。这是指只有一个类别适用于每个数据点。

*   **RMSprop 优化器:**类似于梯度下降算法，但是有动量。它将振动限制在垂直方向。

*   **One-hot 编码:**将分类变量转换为整数或 0 和 1 的向量的过程。例如，在图 [5-3](#Fig3) 中，我们有一个包含元素 A、B、C 和 A 的数据集。目前它们是字符。然而，我们的模型只能接受数值。因此，我们使用一键编码将字符转换为向量，每个向量有一行三列来表示每个类别。对于本例，映射是最后一列表示类别 A，中间一列表示类别 B，第一列表示类别 c。因此，1 用于指示变量属于哪个类别。

最后，你已经准备好开始这个项目了。让我们从准备这个项目的包开始。

### 必需的库

让我们看看这个项目所需库的列表。

*   操作系统(内置 Python)

*   Os.path(内置 Python)

*   随机(内置 Python 2 及更高版本)

*   Shutil(内置 Python 库)

*   NumPy(安装说明见第 [1](1.html) 章)

*   熊猫(安装说明见第 [1](1.html) 章)

*   Matplotlib(安装说明见第 [1](1.html) 章)

*   张量流(安装说明见第 [1](1.html) 章)

*   Keras(安装说明见第 [1](1.html) 章)

*   Music21(本章中的安装说明)

看起来我们需要做的就是安装 Music21 和 Random2。让我们现在做那件事。

## 安装说明

在第 [1](1.html) 章，我们安装了每个项目所需的标准库。这些是这个特定项目中使用的附加库的安装说明。

### 使用画中画

为了确保我们可以安装这些库，而不用考虑我们的系统，我们将使用 Python 包 PIP。

1.  在终端中使用以下命令安装 Music21。

1.  在终端中使用以下命令检查 Music21 的安装。

```py
Pip3 install Music21

```

1.  在终端中使用以下命令安装 Random2。

```py
Pip3 show Music21

```

1.  在终端中使用以下命令检查 Random2 的安装。

```py
Pip3 install random2

```

![../images/488562_1_En_5_Chapter/488562_1_En_5_Fig4_HTML.jpg](../images/488562_1_En_5_Chapter/488562_1_En_5_Fig4_HTML.jpg)

图 5-4

MuseScore 官方网站

1.  安装 MuseScore。因为我们正在处理 MIDI 文件，我们需要软件来打开和查看这些文件。前往官网 [`https://musescore.org/en`](https://musescore.org/en) (见图 [5-4](#Fig4) )。MuseScore 是开源免费的。该网站会自动检测您使用的系统，并推荐合适的 MuseScore 版本。只需点击免费下载按钮。

```py
Pip3 show random2

```

此后，根据您使用的操作系统，说明会有所不同。

### 使用 Windows

在 Windows 中，请按照下列步骤操作:

1.  找到下载文件的位置，双击安装程序开始安装。

2.  单击每个屏幕上的下一步按钮，直到出现安装按钮。点击此按钮，MuseScore 将被安装。

3.  一旦弹出完成按钮，就意味着 MuseScore 已经成功安装。退出安装程序。

4.  进入开始菜单，选择所有程序➤ MuseScore，启动 MuseScore。

### 使用 macOS

在 macOS 中，请按照下列步骤操作:

1.  找到文件(通常在`Downloads`文件夹中)并运行它。将出现 MuseScore 图标。

2.  将图标拖至`Applications`文件夹。它可能会询问管理员密码；输入密码，然后点按“鉴定”。安装将开始。

3.  要运行 MuseScore，导航至`Applications`文件夹并点击 MuseScore 图标。

### 使用 Linux

在 Linux 中，请遵循以下步骤:

1.  在终端类型中:

1.  然后按回车键。

2.  出现提示时，按 Y，然后再次返回。将安装 MuseScore。

3.  MuseScore 可以通过`application`菜单打开，或者通过在终端中键入`musescore`并按回车键打开。

```py
sudo apt-get install musescore

```

### 安装故障排除

以下是一些可能会出现但很容易修复的常见错误:

*   确保所有安装都是最新的。

Note

`random2`提供了 Python 2.7 的随机模块的 Python 3 移植版本。它也被移植到 Python 2.6 中。

*   尽量不要手动安装任何东西。请改用 pip 安装程序。

*   如果你得到一个“不在 sudoers 文件中”的错误，可以通过编辑文件来修复。您将需要在终端中`su`到根目录，然后运行`visudo`并搜索看起来像`root ALL=(ALL) ALL`的一行。复制那一行，用用户名替换`root`。然后保存文件。或者，您可以在“用户”偏好设置面板中将用户设置为管理员。然后，您将拥有管理员权限。

*   在 Windows 中，在使用 pip 之前，必须将 Python 路径添加到环境变量中。

这些是你可能遇到的最常见的问题。为了获得更多的帮助，有一个谷歌小组，用户可以在那里发布关于 Music21 的问题。该组的链接可以在本章末尾的“参考资料”部分找到。

## GRU 建筑

要使用 GRU 生成音乐，您需要一个具有正确层数和正确顺序的模型。图 [5-5](#Fig5) 显示了我们将用于这个项目的模型。

![../images/488562_1_En_5_Chapter/488562_1_En_5_Fig5_HTML.jpg](../images/488562_1_En_5_Chapter/488562_1_En_5_Fig5_HTML.jpg)

图 5-5

该项目的 GRU 建筑

让我们来看看 GRU 的“蓝图”。该模型将由四层组成:

*   GRU 层 - 3

*   脱落层- 3

*   密集层- 2

*   激活层- 1

GRU 层学习音乐作品中的模式。它们利用复位门和更新门来提取模式并“记忆”它。dropout 层完全按照它所暗示的那样工作。通过“丢弃”模型学习的部分模式，我们避免了过度拟合模型。最后，密集层是一个完全连通的层，可以提高模型的准确性。

*   **Activation function:** 我们将使用 Softmax 函数，因为它非常适合于音乐作品中组件的识别和分类。

*   **损失函数:**由于我们是把这个项目当做一个分类问题来处理，分类交叉熵损失函数会给我们最好的结果。它是专门为分类而设计的。

*   优化器:我们使用 RMSprop 优化器，因为它有伪曲率信息。此外，它可以处理随机目标。这两个特性对于小批量学习很重要，这是我们的模型使用的过程。

Note

嵌入层通常用于文本分析。所以我们不会在模型中使用它。

## 程序

您终于准备好实现 GRU 了。让我们打开一个新的 Jupyter 笔记本并开始工作。

### 第一步。导入库

为此项目导入必要的库。

```py
import tensorflow as tf
from music21 import converter, instrument, note, chord,stream
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import random
from keras.layers.core import Dense, Activation, Flatten
from keras.layers import GRU,Convolution1D,Convolution2D, Flatten, Dropout, Dense
from keras import utils as np_utils
from tensorflow.keras import layers

```

设置文件路径，以便 Jupyter 笔记本可以访问该数据集。使用`os.chdir`命令并输入文件路径。

```py
os.chdir("//Users//vinitasilaparasetty//Downloads//Project 1- GRU//Input//music_files")#enter the file path from your system.

```

声明将用于特征提取的变量:

```py
musical_note = []
offset = []
instrumentlist=[]

```

### 第二步。加载数据

创建`filenames`变量，该变量使用随机模块随机选择并加载 MIDI 文件。我们可以设置随机选择的文件数量；在这种情况下，我们选择了`5`。创建`musiclist`变量，加载数据集中所有的 MIDI 文件。

```py
filenames = random.sample(os.listdir('D:\\Proj\\vinita\\Project 1- GRU\Input\\music_files\\'), 5) #Only 5 files are taken at random
musiclist = os.listdir('D:\\Proj\\vinita\\Project 1- GRU\Input\\music_files\\')

```

### 第三步。特征抽出

因为音符的最重要部分可以使用音高的字符串符号重新创建，所以使用音符对象的字符串符号来附加每个音符对象的音高。请遵循以下步骤:

1.  使用`string_midi = converter.parse(r1)`解析注释并初始化`parsednotes = None`变量。

2.  使用`parts = instrument.partitionByInstrument(string_midi)`检测仪器类型，并使用`instrumentlist.append(parts.parts[0].getInstrument().instrumentName)`更新仪器计数器。

3.  使用`if parts: parsednotes = parts.parts[0].recurse()`提取尖锐音符。

4.  使用`else: parsednotes = string_midi.flat.notes`提取平音符。

5.  使用`for element in parsednotes: offset.append(element.offset)`提取偏移量。

6.  使用`if isinstance(element, note.Note):`和`musical_note.append(str(element.pitch))`开始向`musical_note[ ]`数组添加音符。

7.  用一个`.`来分隔每个音符:`elif isinstance(element, chord.Chord):`和`musical_note.append('.'.join(str(n) for n in element.normalOrder))`。

```py
for file in filenames:
   matching = [s for s in musiclist if file.split('_')[0] in s]
   print(matching)
   r1 = matching[random.randint(1, len(matching))]
   string_midi = converter.parse(r1)
   parsednotes = None
   parts = instrument.partitionByInstrument(string_midi)
   instrumentlist.append(parts.parts[0].getInstrument().instrumentName)
   if parts: # file has instrument parts
       parsednotes = parts.parts[0].recurse()
   else: # file has flat notes
       parsednotes = string_midi.flat.notes
   for element in parsednotes: #detect offsets
       offset.append(element.offset)
       if isinstance(element, note.Note):
           musical_note.append(str(element.pitch))
       elif isinstance(element, chord.Chord):
           musical_note.append('.'.join(str(n) for n in element.normalOrder))

```

### 第四步。探索性数据分析

使用以下方法计算 MIDI 文件中独特乐器的数量:

```py
pd.Series(instrumentlist).value_counts()
pd.Series(instrumentlist).value_counts()

```

使用以下方法计算音符和和弦的数量:

```py
pd.Series(musical_note).value_counts()
pd.Series(musical_note).value_counts()

```

使用以下命令可视化 MIDI 文件中的偏移:

```py
 offset = [float(item) for item in offset]
offset = [float(item) for item in offset]
plt.plot(offset)
plt.show() # this shows that offset is normally started from 0 for each musical file

```

输出应该类似于图 [5-6](#Fig6) 。

![../images/488562_1_En_5_Chapter/488562_1_En_5_Fig6_HTML.jpg](../images/488562_1_En_5_Chapter/488562_1_En_5_Fig6_HTML.jpg)

图 5-6

生成偏移的散点图

### 第五步。数据准备(输入)

使用变量`sequence_length = 100`变量将前 100 个音符序列作为输入。

现在我们来整理一下笔记:

1.  在`musical_note[ ]`数组中，按升序使用`pitchcategory = sorted(set(item for item in musical_note))`。

2.  然后通过将 MIDI 文件中的音符映射到整数来使用一个热编码。

3.  使用以下内容创建一个字典来保存映射的注释:

1.  通过声明`model_input_original = []`变量来创建模型的输入。

2.  使用`model_output = []`收集模型的输出。

3.  将作为输入序列输入的音符分开:

    ```py
    for i in range(0, len(musical_note) - sequence_length, 1):
    sequence_in = musical_note[i:i + sequence_length]

    ```

4.  将预期输出中的注释分开:

    ```py
    sequence_out = musical_note[i + sequence_length]

    ```

5.  将输入序列添加到字典中:

    ```py
    model_input_original.append([note_encoding[char] for char in sequence_in])

    ```

6.  添加音符的输出序列:

    ```py
    model_output.append(note_encoding[sequence_out])

    ```

```py
note_encoding = dict((note, number) for number, note in enumerate(pitchcategory))

```

MIDI 文件的组成模式存储在`n_patterns = len(model_input_original)`变量中。现在我们已经有了输入，是时候对它进行整形，以便它可以在模型上使用。使用下面的行来改变数据的形状。

```py
model_input = np.reshape(model_input_original, (n_patterns, sequence_length, 1))

```

在重塑数据之后，是时候对其进行标准化，以获得准确的结果，避免在训练过程中出现问题。

```py
sequence_length = 100

# Arranging notes and chords in ascending order
pitchcategory = sorted(set(item for item in musical_note))

# One hot encoding
note_encoding = dict((note, number) for number, note in enumerate(pitchcategory))
model_input_original = []
model_output = []

# Prepare input and output data for model
for i in range(0, len(musical_note) - sequence_length, 1):
   sequence_in = musical_note[i:i + sequence_length]
   sequence_out = musical_note[i + sequence_length]
   model_input_original.append([note_encoding[char] for char in sequence_in])
   model_output.append(note_encoding[sequence_out])

n_patterns = len(model_input_original)

# converting data for compatibility with GRU
model_input = np.reshape(model_input_original, (n_patterns, sequence_length, 1))

# standardizing model input data
model_output = np_utils.to_categorical(model_output)
Len_Notes = model_output.shape[1]
model_input = model_input / float(Len_Notes)

```

### 第六步。构建模型

您现在已经准备好创建 GRU 模型了。请遵循以下步骤:

1.  使用`model_GRU = tf.keras.models.Sequential()`初始化模型。

2.  使用`model_GRU.add(layers.GRU(16,input_shape=(model_input.shape[1], model_input.shape[2]),return_sequences=True))`添加第一个 GRU 层。该层在数据经过处理后返回值。

3.  使用`model_GRU.add(layers.Dropout(0.3))`添加脱落层。

4.  这是其次是第二个 GRU 层使用`model_GRU.add(layers.GRU(64, return_sequences=True))`。

5.  使用`model_GRU.add(layers.Dropout(0.3))`添加第二个脱落层。

6.  使用`model_GRU.add(layers.GRU(64))`添加最后的 GRU 层。

7.  使用`model_GRU.add(layers.Dense(16))`添加第一个密集层。

8.  使用`model_GRU.add(layers.Dropout(0.3))`添加最终的脱落层。

9.  使用`model_GRU.add(layers.Dense(Len_Notes))`，你已经为最终的密集层做好了准备。

10.  现在你只需要添加最后一层，也就是激活层。使用`model_GRU.add(layers.Activation('softmax'))`将 Softmax 设置为激活功能。

11.  要完成模型的设置，使用`model_GRU.compile(loss='categorical_crossentropy', optimizer="rmsprop")`添加编译特性。在这里，您将损失函数设置为分类交叉熵，将优化器设置为`rmsprop`。

12.  现在使用`model_GRU.fit(model_input, model_output, epochs=30, batch_size=64)`为模型设置时期和批量大小。对于第一次运行，您将从 30 个时期和 6 个批量开始。这可以在以后更改以改进模型。

13.  为了确保模型的架构是正确的，您可以使用`model_GRU.summary()`来获得模型结构的概述。

```py
model_GRU = tf.keras.models.Sequential()
model_GRU.add(layers.GRU(16,input_shape=(model_input.shape[1], model_input.shape[2]),return_sequences=True))
model_GRU.add(layers.Dropout(0.3))
model_GRU.add(layers.GRU(64, return_sequences=True))
model_GRU.add(layers.Dropout(0.3))
model_GRU.add(layers.GRU(64))
model_GRU.add(layers.Dense(16))
model_GRU.add(layers.Dropout(0.3))
model_GRU.add(layers.Dense(Len_Notes))
model_GRU.add(layers.Activation('softmax'))
model_GRU.compile(loss='categorical_crossentropy', optimizer="rmsprop")
model_GRU.summary() #Displays model architecture

```

### 第七步。训练模型

按照以下步骤训练模型:

1.  使用`int_to_note = dict((number, note) for number, note in enumerate(pitchcategory))`为笔记初始化一个新的字典。

2.  使用`pattern = model_input_original[0]`初始化一个新数组来检测 MIDI 文件中的模式。

3.  使用`prediction_output = [ ]`初始化一个新数组来存储预测的音符。

```py
# initializing data for model prediction
int_to_note = dict((number, note) for number, note in enumerate(pitchcategory))
pattern = model_input_original[0]
prediction_output = []
model_GRU.fit(model_input, model_output, epochs=30, batch_size=64)

```

### 第八步。预报

通过使用 for `note_index in range(500)`并遵循以下步骤，您可以预测接下来的 500 个音符:

1.  必须使用`prediction_input = np.reshape(pattern, (1, len(pattern), 1))`对预测输入进行整形，以与 GRU 兼容。

2.  需要使用`prediction_input = prediction_input / float(Len_Notes)`对输入进行标准化。

3.  接下来，使用`prediction_GRU = model_GRU.predict(prediction_input, verbose=0)`获得预测音符。

4.  使用`index_GRU = np.argmax(prediction_GRU)`找到频率最高的音符。

5.  使用以下方法将整数重新转换为相应的音符:

1.  使用以下命令将注释添加到数组中:

```py
index = index_GRU
  result = int_to_note[index]

```

```py
    prediction_output.append(result)
    pattern = np.append(pattern,index)
    pattern = pattern[1:len(pattern)]

```

```py
# generate 500 notes

for note_index in range(500):
   prediction_input = np.reshape(pattern, (1, len(pattern), 1))
   prediction_input = prediction_input / float(Len_Notes)
   prediction_GRU = model_GRU.predict(prediction_input, verbose=0)
   index_GRU = np.argmax(prediction_GRU)
   index = index_GRU
   result = int_to_note[index]
   prediction_output.append(result)
   pattern = np.append(pattern,index)
   pattern = pattern[1:len(pattern)]

```

### 第九步。数据准备(偏移)

按照以下步骤准备偏移:

1.  使用`offlen = len(offset)`找到偏移的长度。

2.  现在使用`DifferentialOffset = (max(offset)-min(offset))/len(offset)`找到偏移的微分。

3.  创建使用`offset2 = offset.copy()`收集的偏移的副本。

4.  使用`output_notes = [ ]`初始化输出音符数组，并使用`i = 0`将计数器设置为零。

5.  现在使用`offset = [ ]`初始化偏移量数组，并使用`initial = 0`将计数器设置为零。

6.  使用以下公式开始计算偏移量:

    ```py
    for i in range(len(offset2)):
        offset.append(initial)
        initial  = initial+DifferentialOffset

    ```

7.  现在是时候检测音符和和弦了。使用`i=0`重置计数器，确保计数器为零。

8.  使用以下内容搜索句号，以检测每个音符开始的时间:

    ```py
    for pattern in prediction_output:
        if ('.' in pattern) or pattern.isdigit():

    ```

9.  使用以下方法将和弦从弦中分离出来，并将其添加到音符数组中:

    ```py
    notes_in_chord = pattern.split('.')
        notes = [ ]

    ```

10.  使用以下方法检测音符并将其分开:

    ```py
    for check_note in notes_in_chord:
         gen_note = note.Note(int(check_note))
         gen_note.storedInstrument = instrument.Guitar()
         notes.append(gen_note)
         gen_chord = chord.Chord(notes)

    ```

11.  使用以下方法检测偏移:

    ```py
    gen_chord.offset = offset[i]
      output_notes.append(gen_chord)

    ```

12.  为了防止异常，请使用以下代码:

    ```py
    else:
            gen_note = note.Note(pattern)
            gen_note.offset = offset[i]
            gen_note.storedInstrument = instrument.Guitar()
            output_notes.append(gen_note)
        i=i+1

    ```

    ```py
    # prepare notes , chords and offset separately
    offlen = len(offset)
    DifferentialOffset = (max(offset)-min(offset))/len(offset)
    offset2 = offset.copy()
    output_notes = []
    i = 0
    offset = []
    initial = 0
    for i in range(len(offset2)):
       offset.append(initial)
       initial  = initial+DifferentialOffset
    # Differentiate notes and chords
    i=0
    for pattern in prediction_output:
       if ('.' in pattern) or pattern.isdigit():
           notes_in_chord = pattern.split('.')
           notes = []
           for check_note in notes_in_chord:
               gen_note = note.Note(int(check_note))
               gen_note.storedInstrument = instrument.Guitar()
               notes.append(gen_note)
           gen_chord = chord.Chord(notes)
           gen_chord.offset = offset[i]
           output_notes.append(gen_chord)
       else:
           gen_note = note.Note(pattern)
           gen_note.offset = offset[i]
           gen_note.storedInstrument = instrument.Guitar()

           output_notes.append(gen_note)
       i=i+1

    ```

### 第十步。将输出存储为 MIDI 文件

按照以下步骤将输出存储为 MIDI 文件:

1.  使用`os.chdir('D:\\Proj\\vinita\\Project 1- GRU\Output\\')`指定存储 MIDI 文件的文件路径。

2.  使用`midi_stream = stream.Stream(output_notes) #create stream`创建带有输出的流。

3.  使用下面的代码创建一个 MIDI 文件:

```py
midi_stream.write('midi', fp='GRU_output.mid') #create MIDI file using stream

os.chdir('D:\\Proj\\vinita\\Project 1- GRU\Output\\') #Specify file path to store the MIDI file.
midi_stream = stream.Stream(output_notes) #create stream
midi_stream.write('midi', fp='GRU_output.mid') #create MIDI file using stream

```

我们现在可以使用 MuseScore 查看 MIDI 文件(参见图 [5-7](#Fig7) )。将原始音乐作品与生成的输出进行比较，看看 GRU 的表现如何。

![../images/488562_1_En_5_Chapter/488562_1_En_5_Fig7_HTML.jpg](../images/488562_1_En_5_Chapter/488562_1_En_5_Fig7_HTML.jpg)

图 5-7

使用 MuseScore 查看生成的输出

## 进一步测试

这里有一些想法可以尝试，并从这个项目中学到更多:

*   尝试增加和减少 GRU 中的层数。

*   尝试增加下降图层。

*   尝试增加和减少历元的数量，以查看其对模型结果的影响。

## 解决纷争

以下是您可能遇到的一些常见错误，这些错误很容易修复:

*   当您开始定型模型时，可能会出现以下错误:

    `WARNING: Logging before flag parsing goes to stderr.`

    不要担心这个。不会影响你的节目。

*   如果您使层有状态，请记住指定大小。

*   确保所有路径文件都是正确的。

*   数据量相当大，可能会耗尽 RAM。若要避免此问题，请减少定型集中的文件数量。

*   当测试其他数据集时，如果识别乐器有问题，请确保数据集包含在单个乐器上演奏的音乐片段。

*   使用`.show('midi')`在 Jupyter 笔记本中直接播放 MIDI 文件。

## 摘要

以下是你在本章中学到的所有内容的回顾。

*   GRU 能够学习长期依赖，而不是香草 RNN。

*   在反向传播过程中，梯度随着通过神经网络向后移动而减小。这就是所谓的消失梯度问题。

*   gru 的架构比 LSTMs 简单，训练速度也更快。

*   GRUs 的应用包括复调音乐建模、语音信号建模、手写识别、语音识别、人类基因组研究和股票市场分析。

*   GRU 有两个门:复位门和更新门。

*   GRU 中的不同图层包括嵌入图层、GRU 图层、下降图层和密集图层。

*   我们可以将 RNN 层设置为有状态的，这意味着为一批中的样本计算的状态将在下一批中作为样本的初始状态重用。

*   GRU 的输入是三维数组的形式。第一个维度表示批量大小，第二个维度表示我们输入到序列中的时间步长数，第三个维度表示一个输入序列中的单元数。

*   电流存储器的功能非常像一个门，但它集成在复位门中，用于在输入中引入一些非线性，并使输入为零均值。此外，它减少了以前的信息对传递到未来的当前信息的影响。

*   模型使用重置门来决定要忘记多少过去的信息。它类似于 LSTM 循环单元中输入门和遗忘门的组合。

*   更新门是模型可以决定从过去复制所有信息并消除消失梯度问题的风险的地方。

*   当前时间步长的存储器是 GRU 的最后一级。网络计算 Hs <sub>t</sub> ，这是一个向量，它保存当前单元的信息，并将其传递给网络。

*   分类交叉熵是用于单标签分类的损失函数。这是指每个数据点仅适用一个类别。

*   RMSprop 优化器类似于梯度下降算法，但具有动量。它限制垂直方向的振动。

*   一键编码是一个过程，通过该过程，分类变量被转换成可以提供给 ML 算法的形式，以在预测中做得更好。

## 参考

本章中使用的参考资料如下:

*   [T2`https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21`](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)

*   [T2`www.lifewire.com/midi-file-2621979`](http://www.lifewire.com/midi-file-2621979)

*   [T2`https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b`](https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b)

*   [T2`https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy`](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy)

*   [T2`www.lexico.com/en/definition/octave`](http://www.lexico.com/en/definition/octave)

## 资源

以下是对你的项目有帮助的附加材料的链接。

*   数据集:

    [T2`https://medium.com/@vinitasilaparasetty/list-of-midi-file-datasets-for-music-analysis-a4963360096e?sk=467d814e336abc52c655e3858eb52ae9`](https://medium.com/%2540vinitasilaparasetty/list-of-midi-file-datasets-for-music-analysis-a4963360096e%253Fsk%253D467d814e336abc52c655e3858eb52ae9)

*   音乐疑难解答帮助 21:

    [T2`https://groups.google.com/forum/#!forum/music21list`](https://groups.google.com/forum/%2523%2521forum/music21list)

*   MuseScore 的疑难解答帮助:

    [T2`https://musescore.org/en/forum`](https://musescore.org/en/forum)

## 进一步阅读

有兴趣了解本章中涉及的一些主题吗？这里有一些很棒的链接可以查看:

*   GRU 如何解决渐变消失问题:

    [T2`www.geeksforgeeks.org/gated-recurrent-unit-networks/`](http://www.geeksforgeeks.org/gated-recurrent-unit-networks/)

*   关于 MIDI 文件的更多信息:

    [T2`www.somascape.org/midi/tech/mfile.html`](http://www.somascape.org/midi/tech/mfile.html)

*   关于音乐理论的更多信息:

    [T2`https://iconcollective.com/basic-music-theory/`](https://iconcollective.com/basic-music-theory/)

*   关于音乐的官方文件 21:

    [T2`https://web.mit.edu/music21/doc/about/index.html`](https://web.mit.edu/music21/doc/about/index.html)

*   关于 cuDNN GRU 的官方文件:

    [T2`https://developer.nvidia.com/cudnn`](https://developer.nvidia.com/cudnn)

*   在各种类型的 rnn 中有状态:

    [T2`http://keras.io/layers/recurrent/`](http://keras.io/layers/recurrent/)

*   关于一键编码的更多信息:

    [T2`https://medium.com/@vinitasilaparasetty/what-is-one-hot-encoding-ffd381f9a8a2`](https://medium.com/%2540vinitasilaparasetty/what-is-one-hot-encoding-ffd381f9a8a2)

    [T2`https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f`](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)

*   关于 MuseScore 的更多信息:

    [T2`https://musescore.org/en/handbook`](https://musescore.org/en/handbook)