# 9.生成模型

机器学习模型可以分为两类:判别型和生成型。辨别模型被训练来执行分类或回归。也就是说，我们输入一组特征，并期望接收类标签的概率或预测值作为输出。相反，生成模型被训练来学习数据的底层分布。一旦我们训练了一个生成模型，我们就可以用它来产生一个类的新例子。图 [9-1](#Fig1) 说明了两类模型之间的区别。

![../images/496662_1_En_9_Chapter/496662_1_En_9_Fig1_HTML.jpg](../images/496662_1_En_9_Chapter/496662_1_En_9_Fig1_HTML.jpg)

图 9-1

鉴别器和发生器模型的比较

到目前为止，我们在这本书里已经关注了判别模型；然而，有一个例外:潜在的狄利克雷分配(Blei et al. 2003)，我们在第 [6](06.html) 章中介绍了它。LDA 模型将文本语料库作为输入，并返回一组主题，其中每个主题被定义为词汇的分布。

最近，生成机器学习文献取得了相当大的进展，其中大部分集中在两种类型的模型的开发上:变分自动编码器(VAEs)和生成对抗网络(GANs)。关于图像、文本和音乐生成，这两类模型已经实现了相当大的突破。

在很大程度上，这种进步还没有达到经济学和金融学学科；然而，经济学中的一些工作已经开始使用 GANs。在本章的最后一节，我们将简要讨论甘斯在经济学中的两个最新应用。艾尔。2019 年和 Kaji 等人 2018 年)，并推测未来的潜在用途。

## 可变自动编码器

在第 8 章中，我们介绍了自动编码器的概念，它由两个共享权重的网络组成:一个编码器和一个解码器。编码器将模型输入转换为潜在状态。解码器将潜在状态作为输入，并产生输入到编码器的特征的重构。我们通过计算重建损失来训练模型，重建损失是输入和它们的预测值之间的差异的转换。

我们使用自动编码器来执行降维，但是讨论了自动编码器的其他用途，主要涉及生成任务，例如创建新的图像、音乐和文本。我们没有提到的是，自动编码器受到两个问题的困扰，这两个问题阻碍了它们在这些任务上的性能。我们下面讨论的这两个问题都与它们产生潜在状态的方式有关:

1.  **潜在状态的位置和分布**:具有 *N* 个节点的自动编码器的潜在状态是 *ℝ* <sup>*N*</sup> 中的点。对于很多问题，这些点会倾向于聚集在同一个区域；然而，自动编码器不允许我们明确地确定这样的点在 *ℝ* <sup>*N*</sup> 中如何以及在哪里聚集。这可能看起来不重要，但它将最终决定哪些潜在状态可以输入到模型中。例如，如果我们试图生成一幅图像，那么知道什么构成了有效的潜在状态，从而知道什么可以被输入到模型中，这将是非常有用的。否则，我们将使用远离模型所观察到的任何东西的状态，这将产生一个新颖的、但也许不可信的图像。

2.  **训练中不存在的潜在状态的性能**:自动编码器被训练来为一组例子重建输入。对于与一组特征相关联的潜在状态，解码器应该产生类似于输入特征的输出。然而，如果我们稍微扰动潜在向量，就不能保证解码器有能力从一个从未访问过的点生成一个令人信服的例子。

变分自动编码器(VAEs)被开发来克服这些限制。VAEs 不具有潜在状态层，而是具有均值层、对数方差层和采样层。采样图层采用由前面图层中的平均值和对数方差参数定义的正态分布。然后，采样层的输出作为训练过程中的潜在状态传递给解码器。将相同的特征传递给编码器两次，每次都会产生不同的潜在状态。

除了架构上的差异，VAEs 还修改了损失函数，以包括采样层中每个正态分布的 Kullback-Leibler (KL)散度。KL 散度惩罚了每个正态分布与均值和对数方差均为零的正态分布之间的距离。

这些特性的组合完成了三件事。首先，它消除了潜在状态的决定论。每组特征现在将与潜在状态的分布相关联，而不是与单个潜在状态相关联。这将通过强制模型将每个单独的潜在状态特征视为连续变量来提高生成性能。其次，它消除了采样问题。我们现在可以通过使用采样层来随机绘制有效的状态。第三，它修正了潜在空间分布的问题。损失的 KL 散度分量将使分布均值接近于零，并迫使它们具有相似的方差。

本节的剩余部分将重点介绍在 TensorFlow 中实现 VAEs。有关 VAE 模型发展的扩展概述及其理论属性的详细探索，请参见金玛和韦林(2019)。

我们将在本章中使用的例子利用了我们在第 [8](08.html) 章中介绍的 GDP 增长数据。作为更新，它包括 25 个不同经合组织国家的季度时间序列，时间跨度从 1961 年的 Q2 到 2020 年的 Q1。在第 [8](08.html) 章中，我们使用降维技术在每个时间点从 25 个系列中提取少量的公共成分。

在本章中，我们将使用 GDP 增长数据来训练一个能够生成类似序列的 VAE。我们将从清单 [9-1](#PC1) 开始，导入我们将在本练习中使用的库，然后加载并准备数据。注意，我们转置了 GDP 数据，因此列对应于特定的季度，行对应于国家。然后，我们将数据转换成一个`np.array()`，并为批量大小和潜在空间中输出节点的数量设置参数。

```py
import tensorflow as tf
import pandas as pd
import numpy as np

# Define data path.
data_path = '../data/chapter9/'

# Load and transpose data.
GDP = pd.read_csv(data_path+'gdp_growth.csv',
        index_col = 'Date').T

# Print data preview.
print(GDP.head())

Time    4/1/61    7/1/61   10/1/61    1/1/62
AUS  -1.097616 -0.715607  1.139175  2.806800 ...
AUT  -0.349959  1.256452  0.227988  1.463310 ...
BEL   1.167163  1.275744  1.381074  1.346942 ...
CAN   2.529317  2.409293  1.396820  2.650176 ...
CHE   1.355571  1.242126  1.958044  0.575396 ...

# Convert data to numpy array.
GDP = np.array(GDP)

# Set number of countries and quarters.
nCountries, nQuarters = GDP.shape

# Set number of latent nodes and batch size.
latentNodes = 2
batchSize = 1

Listing 9-1Prepare GDP growth data for use in a VAE

```

下一步是定义 VAE 模型架构，它将由一个编码器和一个解码器组成，类似于第 [8](08.html) 章的自动编码器模型。然而，与自动编码器相反，在训练过程中，潜在状态将从一组独立的正态分布中采样。我们将从定义一个执行清单 [9-2](#PC2) 中的采样任务的函数开始。

```py
# Define function for sampling layer.
def sampling(params, batchSize = batchSize, latentNodes = latentNodes):
        mean, lvar = params
epsilon = tf.random.normal(shape=(
        batchSize, latentNodes))
        return mean + tf.exp(lvar / 2.0) * epsilon

Listing 9-2Define function to perform sampling task in VAE

```

注意`sampling`层不包含任何自己的参数。相反，它将一对参数作为输入，从潜在状态中的每个输出节点的标准正态分布中提取`epsilon`，然后使用与该状态中的节点相对应的`mean`和`lvar`参数来转换每个提取。

一旦我们定义了一个采样层，我们还可以定义一个编码器模型，它将非常类似于我们为 autoencoder 模型构建的模型。我们将在清单 [9-3](#PC3) 中这样做。唯一的初始区别是，我们将一个国家的完整时间序列作为输入，而不是某个时间点上各国的横截面值。

另一个差异出现在`mean`和`lvar`层，这在自动编码器中不存在。这些层具有与潜在状态相同数量的节点。这是因为它们由与潜在状态中的每个节点相关联的正态分布的均值和对数方差参数值组成。

我们接下来定义一个`Lambda`层，它接受我们之前定义的`sampling`函数，并传递给它`mean`和`lvar`参数。我们可以看到，采样层为潜在状态中的每个特征(节点)生成一个输出。最后，我们定义了一个函数模型`encoder`，它采用输入特征(季度 GDP 增长观察值)并返回一个均值层、一个对数方差层以及使用均值和对数方差参数化正态分布的抽样输出。

```py
# Define input layer for encoder.
encoderInput = tf.keras.layers.Input(shape = (nQuarters))

# Define latent state.
latent = tf.keras.layers.Input(shape = (latentNodes))

# Define mean layer.
mean = tf.keras.layers.Dense(latentNodes)(encoderInput)

# Define log variance layer.
lvar = tf.keras.layers.Dense(latentNodes)(encoderInput)

# Define sampling layer.
encoded = tf.keras.layers.Lambda(sampling, output_shape=(latentNodes,))([mean, lvar])

# Define model for encoder.
encoder = tf.keras.Model(encoderInput, [mean, lvar, encoded])

Listing 9-3Define encoder model for VAE

```

在清单 [9-4](#PC4) 中，我们将为解码器模型和整个可变自动编码器定义功能模型。类似于自动编码器的解码器组件，它接受潜在状态作为来自编码器的输入，然后生成输入的重构作为输出。全 VAE 模型也与自动编码器相似，将时间序列作为输入，并将其转换为同一时间序列的重构。

最后一步是定义损失函数，它由两个部分组成——重建损失和 KL 散度——并将其附加到模型中，我们在清单 [9-5](#PC5) 中就是这么做的。重建损失与我们用于自动编码器的损失没有什么不同。KL 散度测量每个采样层分布离标准正态分布有多远。它们离得越远，惩罚就越高。

```py
# Compute the reconstruction component of the loss.
reconstruction = tf.keras.losses.binary_crossentropy(
        vae.inputs[0], vae.outputs[0])

# Compute the KL loss component.
kl = -0.5 * tf.reduce_mean(1 + lvar - tf.square(mean) - tf.exp(lvar), axis = -1)

# Combine the losses and add them to the model.
combinedLoss = reconstruction + kl
vae.add_loss(combinedLoss)

Listing 9-5Define VAE loss

```

```py
# Define output for decoder.
decoded = tf.keras.layers.Dense(nQuarters, activation = 'linear')(latent)

# Define the decoder model.
decoder = tf.keras.Model(latent, decoded)

# Define functional model for autoencoder.
vae = tf.keras.Model(encoderInput, decoder(encoded))

Listing 9-4Define decoder model for VAE

```

最后，在清单 [9-6](#PC6) 中，我们编译并训练模型。在清单 [9-7](#PC7) 中，我们现在有了一个经过训练的变量自动编码器，我们可以用它来执行各种不同的生成任务。例如，我们可以使用`vae`的`predict()`方法为给定的时间序列输入生成重建。我们还可以生成给定输入的潜在状态的实现，例如美国的 GDP 增长。我们还可以通过添加随机噪声来扰乱这些潜在状态，然后使用解码器的`predict()`方法，根据修改后的潜在状态生成一个全新的时间序列。

```py
# Generate series reconstruction.
prediction = vae.predict(GDP[0,:].reshape(1,236))

# Generate (random) latent state from inputs.
latentState = encoder.predict(GDP[0,:].reshape(1,236))

# Perturb latent state.
latentState[0] = latentState[0] + np.random.normal(1)

# Pass perturbed latent state to decoder.
decoder.predict(latentState)

Listing 9-7Generate latent states and time series with trained VAE.

```

```py
# Compile the model.
vae.compile(optimizer='adam')

# Fit model.
vae.fit(GDP, batch_size = batchSize, epochs = 100)

Listing 9-6Compile and fit VAE

```

最后，在图 [9-2](#Fig2) 中，我们展示了 25 个生成的时间序列，它们基于美国 GDP 增长序列的潜在状态实现。然后，我们在 5×5 网格上扰动该原始状态，其中行将[–1，1]间隔上的等间距值添加到第一潜在状态，列将[–1，1]间隔上的等间距值添加到第二潜在状态。网格中心的系列，显示为红色，加上[0，0]，因此，是原始的潜在状态。

![../images/496662_1_En_9_Chapter/496662_1_En_9_Fig2_HTML.png](../images/496662_1_En_9_Chapter/496662_1_En_9_Fig2_HTML.png)

图 9-2

VAE 生成的美国 GDP 增长时间序列

虽然这个例子很简单，并且为了演示的目的，潜在状态只包含两个节点，但是 VAE 体系结构可以应用于各种各样的问题。例如，我们可以在编码器和解码器中添加卷积层，并改变输入和输出形状。这将给我们一个产生图像的 VAE。或者，我们可以将 LSTM 细胞添加到编码器和编码器中，这将为我们提供一个可以生成文本或音乐的 VAE。 <sup>[1](#Fn1)</sup> 此外，基于 LSTM 的架构可以在时间序列生成方面比我们在本例中采用的密集网络方法有所改进。

## 生成对抗网络

两个模型家族主导了生成机器学习文献:变分自动编码器和生成对抗网络。正如我们所见，VAEs 通过操纵潜在状态和它们编码的特征，提供了对实例生成的粒度控制。相比之下，GANs 在制作极具说服力的课程范例方面更为成功。例如，一些最有说服力的生成图像是使用 GANs 生成的。

正如我们在上一节中讨论的，vae 是两个模型的组合:编码器和解码器，由采样层连接。类似地，GANs 也由两个模型组成:生成器和鉴别器。生成器取一个随机的输入向量，我们可能会认为它是一个潜在状态，生成一个类的例子，比如一个真实的 GDP 增长时间序列(或者一个图像，一句话，或者一个乐谱)。

一旦 GAN 的生成器组件生成了一个类的几个示例，它们就被传递给鉴别器，同时还有相同数量的真实示例。在我们的例子中，这将是真实的和生成的真实 GDP 增长序列的组合。然后训练鉴别器来区分真实和虚假的例子。

在鉴别器完成分类任务后，我们可以使用一个对抗性网络来训练发生器，该网络结合了发生器和鉴别器模型。正如 VAE 的编码器和解码器组件的情况一样，敌对网络将与两个网络共享权重。敌对网络将训练发生器使鉴别器网络的损耗最大化。

正如 Goodfellow 等人(2017 年)所讨论的，我们可以将这两个网络视为试图在零和游戏中最大化它们各自的收益，其中鉴别器接收 *v* ( *g* ， *d* )，生成器接收 *v* ( *g* ， *d* )。发生器选择样本 *g* 来欺骗鉴别器；鉴别器为每个样本选择概率 *d* 。等式 9-1 给出了由一组生成图像*g*T18】∫表征的平衡。

*方程式 9-1。GAN 中图像生成* *的平衡条件。*

![$$ {g}^{\ast }=\arg \underset{g}{\min}\underset{d}{\max }v\left(g,d\right) $$](../images/496662_1_En_9_Chapter/496662_1_En_9_Chapter_TeX_Equa.png)

因此，当我们训练网络的敌对部分时，我们必须冻结鉴别器权重。这将约束网络改进生成过程，而不是削弱鉴别器。在训练过程中重复这些步骤将最终产生方程 9-1 中描述的进化平衡。

图 [9-3](#Fig3) 显示了 GAN 的发生器和鉴别器网络。总的来说，生成器产生了新的例子，这些例子不是从数据中提取的。鉴别器将这些示例与真实示例相结合，然后执行分类。敌对网络通过将发电机连接到一个鉴别器上来训练发电机，但权重是固定的。网络上的训练迭代发生。

按照 VAEs 一节中的例子，我们将再次使用 GDP 增长数据，我们在清单 [9-8](#PC8) 中加载并准备了这些数据。我们的目的是训练一个 GAN 从随机抽取的向量输入中生成可信的 GDP 增长时间序列。我们将遵循克罗恩等人(2020)描述的 GAN 构建方法。

![../images/496662_1_En_9_Chapter/496662_1_En_9_Fig3_HTML.png](../images/496662_1_En_9_Chapter/496662_1_En_9_Fig3_HTML.png)

图 9-3

来自 GAN 的发生器和鉴别器的描述

```py
import tensorflow as tf
import pandas as pd
import numpy as np

# Load and transpose data.
GDP = pd.read_csv(data_path+'gdp_growth.csv',
        index_col = 'Date').T

# Convert pandas DataFrame to numpy array.
GDP = np.array(GDP)

Listing 9-8Prepare GDP growth data for use in a GAN

```

在清单 [9-9](#PC9) 中，我们定义了生成模型。我们再次遵循简单的 VAE 模型，画一个有两个元素的向量作为生成器的输入。由于发生器的输入可以看作是 VAE 中潜在向量的类比，我们应该把发生器看作是一个解码器。这意味着我们将从一个狭窄的瓶颈型层开始，并将向上采样到产出，这将是一个生成的 GDP 增长时间序列。

生成器的最简单版本由接受潜在向量的输入层和对输入层进行上采样的输出层组成。由于我们的输出层由 GDP 增长值组成，我们将使用一个`linear`激活函数。我们还将包含一个带有`relu`激活的隐藏层，因为否则模型将无法捕捉非线性。

```py
# Set dimension of latent state vector.
nLatent = 2

# Set number of countries and quarters.
nCountries, nQuarters = GDP.shape

# Define input layer.
generatorInput = tf.keras.layers.Input(shape = (nLatent,))

# Define hidden layer.
generatorHidden = tf.keras.layers.Dense(16, activation="relu")(generatorInput)

# Define generator output layer.
generatorOutput = tf.keras.layers.Dense(236, activation="linear")(generatorHidden)

# Define generator model.
generator = tf.keras.Model(inputs = generatorInput, outputs = generatorOutput)

Listing 9-9Define the generative model of a GAN

```

接下来我们将在清单 [9-10](#PC10) 中定义鉴别器。它将实际和生成的 GDP 增长序列作为输入，每个序列的长度为`nQuarters`。然后，它将为每个投入序列产生一个成为实际国内生产总值增长序列的概率。注意我们没有编译`generator`，但是编译了`discriminator`。这是因为我们将使用对抗网络来训练`generator`。

```py
# Define input layer.
discriminatorInput = tf.keras.layers.Input(shape = (nQuarters,))

# Define hidden layer.
discriminatorHidden = tf.keras.layers.Dense(16, activation="relu")(discriminatorInput)

# Define discriminator output layer.
discriminatorOutput = tf.keras.layers.Dense(1, activation="sigmoid")(discriminatorHidden)

# Define discriminator model.
discriminator = tf.keras.Model(inputs = discriminatorInput, outputs = discriminatorOutput)

# Compile discriminator.
discriminator.compile(loss='binary_crossentropy', optimizer=tf.optimizers.Adam(0.0001))

Listing 9-10Define and compile the discriminator model of a GAN

```

我们现在已经定义了一个生成器模型和一个鉴别器模型。我们还编写了鉴别器。下一步是定义和编译一个对抗模型，用于训练生成器。对抗模型将与生成器共享权重，并将为鉴别器使用冻结版本的权重——也就是说，当我们训练对抗网络时，权重不会更新，但当我们训练鉴别器时，权重会更新。

清单 [9-11](#PC11) 定义了敌对网络。敌对网络的输入是一个潜在向量，因此它将与`generator`的输入具有相同的大小。我们接下来将发电机模型的输出定义为`timeSeries`，这将是一个伪 GDP 增长时间序列。然后我们可以将`discriminator`的可训练性设置为`False`，这样它就不会在我们训练敌对网络时更新。最后，我们将网络的输出设置为鉴别器的输出，并定义和编译一个功能模型`adversarial`。在清单 [9-12](#PC12) 中，我们将训练`discriminator`和`adversarial`。

```py
# Set batch size.
batch, halfBatch = 12, 6

for j in range(1000):
        # Draw real training data.
        idx = np.random.randint(nCountries,
        size = halfBatch)
        real_gdp_series = GDP[idx, :]

        # Generate fake training data.
        latentState = np.random.normal(size=[halfBatch, nLatent])
        fake_gdp_series = generator.predict(latentState)

        # Combine input data.
        features = np.concatenate((real_gdp_series,
        fake_gdp_series))

        # Create labels.
        labels = np.ones([batch,1])
        labels[halfBatch:, :] = 0

        # Train discriminator.
        discriminator.train_on_batch(features, labels)

        # Generate latent state for adversarial net.
        latentState = np.random.normal(size=[batch, nLatent])

        # Generate labels for adversarial network.
        labels = np.ones([batch, 1])

        # Train adversarial network.
        adversarial.train_on_batch(latentState, labels)

Listing 9-12Train the discriminator and the adversarial network

```

```py
# Define input layer for adversarial network.
adversarialInput = tf.keras.layers.Input(shape=(nLatent))

# Define generator output as generated time series.
timeSeries = generator(adversarialInput)

# Set discriminator to be untrainable.
discriminator.trainable = False

# Compute predictions from discriminator.
adversarialOutput = discriminator(timeSeries)

# Define adversarial model.
adversarial = tf.keras.Model(adversarialInput, adversarialOutput)

# Compile adversarial network.
adversarial.compile(loss='binary_crossentropy', optimizer=tf.optimizers.Adam(0.0001))

Listing 9-11Define and compile the adversarial model of a GAN

```

我们从定义批量开始。然后，我们进入由几个步骤组成的训练循环。首先，我们绘制随机整数并使用它们来选择`GDP`矩阵中的行，每个行都由一个 GDP 增长时间序列组成。这将是鉴别器训练集中的真实样本。接下来，我们通过绘制潜在向量来生成假数据，然后将它们传递给`generator`。然后，我们将这两种类型的系列组合起来，并给它们分配相应的标签(例如，1 =真实，0 =虚假)。我们现在可以将这些数据传递给鉴别器来执行一批训练。

接下来，我们对敌对网络进行迭代训练。在这里，我们将生成一批潜在状态，将它们输入到`generator`中，然后进行训练，目的是欺骗`discriminator`将它们归类为真实状态。请注意，我们正在迭代两个模型的训练，并且不会在训练过程中使用正常的停止标准。相反，我们将寻找一种稳定的进化均衡，在这种均衡中，任何一种模式似乎都无法获得优势。

在图 [9-4](#Fig4) 中，我们绘制了随时间变化的模型损耗。我们可以看到，在大约 500 次训练迭代之后，两个模型都没有出现实质性的改进，这表明我们已经达到了稳定的进化平衡。

![../images/496662_1_En_9_Chapter/496662_1_En_9_Fig4_HTML.png](../images/496662_1_En_9_Chapter/496662_1_En_9_Fig4_HTML.png)

图 9-4

通过训练迭代的鉴别器和对抗模型损失

最后，我们在图 [9-5](#Fig5) 中绘制了 GAN 产生的 GDP 增长序列之一。除了白噪声向量输入和关于鉴别器性能的信息，敌对网络在 1000 次训练迭代后，设法训练生成器产生一个相当可信的假 GDP 增长序列。当然，如果允许更多的潜在特性和更高级的模型架构，比如 LSTM，我们本可以大大提高性能。

![../images/496662_1_En_9_Chapter/496662_1_En_9_Fig5_HTML.png](../images/496662_1_En_9_Chapter/496662_1_En_9_Fig5_HTML.png)

图 9-5

虚假 GDP 增长序列示例

## 经济学和金融学的应用

在这一章中，我们集中讨论了一个看似晦涩的例子:通过使用生成式机器学习模型生成模拟 GDP 增长序列；然而，这种练习在蒙特卡罗模拟研究中很常见，蒙特卡罗模拟研究用于检验计量经济学中估计量的小样本性质。如果不生成真实的序列并充分捕捉序列之间的相互依赖性，就很难准确评估估计量的属性。

事实上，GANs 在经济学文献中的最早应用之一就是为了实现这一目标。Athey 等人(2019 年)考虑了使用 Wasserstein GANs 来模拟数据的可能性，这些数据看起来与现有数据集中的观察值相似，但现有数据集不够大，无法用于蒙特卡罗模拟。这样做的价值在于，它允许计量经济学家避免这种方法的两种常见替代方案:(1)从小数据集本身随机抽取，这将导致相同观察值的多次重复，以及(2)生成模拟序列，这些模拟序列通常无法准确捕捉数据集中序列之间的相关性。Athey 等人(2019 年)通过使用 WGAN 生成的人工数据评估估值器，证明了他们的方法(以及更一般的 GAN)的价值。

除了 Athey 等人(2019 年)，经济学文献中最近的工作(Kaji 等人，2018 年)检查了 WGANs 是否可用于执行间接推断，间接推断通常用于估计经济学和金融学中的结构模型。在 Kaji 等人(2018 年)的研究中，他们试图估计一个模型，其中不同类型的工人从工资和地点菜单中进行选择。他们要恢复的参数是结构性的，无法从数据中直接估计出来，这就需要他们使用间接推断的方法。他们使用的方法是将模型模拟与鉴别器结合起来，训练模型，直到模拟数据与真实数据无法区分。

除了目前集中于模型估计的现有应用，GANs 和 VAEs 也可以用于图像和文本生成的现成应用。尽管图像数据在经济学中的应用仍然有限——即使是在判别模型中——但 GANs 和 VAEs 提供了用经济数据进行可视化反事实模拟的可能性。例如，在城市经济学中，我们可以根据公共政策和其他因素推断公共基础设施的布局会如何变化。

类似地，经济和金融领域日益增长的自然语言处理文献可以利用文本生成来研究，例如，当经济或行业的基本状态发生变化时，公司新闻稿会如何变化。

## 摘要

在这一章之前，这本书主要讨论了判别机器学习模型。这种模型执行分类或回归。也就是说，它们从训练集中提取特征，并尝试区分不同的类别或对目标进行连续预测。生成式机器学习不同于鉴别式机器学习，因为它生成新的示例，而不是在示例之间进行鉴别。

在经济和金融学科之外，生成式机器学习已经被用于创建引人注目的图像、音乐和文本。它还被用于改进蒙特卡罗模拟(Athey 等人，2019 年)和对经济学中的结构模型进行间接推断(Kaji 等人，2018 年)。

在这一章中，我们主要讨论了两种生成模型:变分自动编码器(VAE)和生成对抗网络(GAN)。VAE 模型通过包括均值、方差和采样层来扩展自动编码器。这通过对其潜在空间施加限制来改进自动编码器，迫使状态围绕原点聚集，并且具有 0 的对数方差。

类似于自动编码器和 VAEs，GANs 也由多个组件模型组成:一个生成器模型、一个鉴别器模型和一个对抗模型。发电机模型创造了新的例子。鉴别器模型试图对它们进行分类。对抗模型训练生成器创造引人注目的例子来欺骗鉴别者。GANs 的训练过程包括寻找一个稳定的进化平衡。

最后，我们展示了 VAEs 和 GANs 如何用于生成人工 GDP 增长数据。我们还讨论了它们目前是如何在经济学中应用的，以及如果它们得到更广泛的采用，它们在未来可能如何应用。

## 文献学

Athey，s .，G.W. Imbens，J. Metzger 和 E. Munro。2019."使用 Wasserstein 生成对抗网络设计蒙特卡罗模拟."*第 3824 号工作文件。*

布莱，D.M .，A.Y. Ng 和 M.I .乔丹。2003."潜在的狄利克雷分配."*机器学习研究杂志*3(993–1022)。

古德费勒，我，y .本吉奥，和 a .库维尔。2017.*深度学习。马萨诸塞州剑桥:麻省理工学院出版社。*

古德费勒、国际法院、普热-阿巴迪、米尔扎、徐、沃德-法利、奥泽尔、库维尔和本吉奥。“生成性对抗网络”*NIPS’2014。* 2014 年。

Kaji、E. Manresa 和 G. Pouliot。2018."深度推理:用于结构估计的人工智能."*工作文件。*

金玛，D.P .和 m .韦林。2019."变分自动编码器的介绍."*机器学习的基础和趋势*12(4):307–392。

j .克罗恩、g .贝维尔德和 a .巴森斯。2020.深度学习图解:人工智能的可视化互动指南。艾迪森-卫斯理。

<aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

参见 [`www.datacamp.com/community/tutorials/using-tensorflow-to-compose-music`](http://www.datacamp.com/community/tutorials/using-tensorflow-to-compose-music) 获取音乐生成的生成模型的扩展教程。

 </aside>