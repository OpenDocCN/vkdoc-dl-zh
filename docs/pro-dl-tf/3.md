# 3.卷积神经网络

近年来，人工神经网络在处理非结构化数据，尤其是图像、文本、音频和语音方面蓬勃发展。卷积神经网络(CNN)最适合这种非结构化数据。每当有拓扑结构与数据相关联时，卷积神经网络就能很好地从数据中提取出重要的特征。从架构的角度来看，CNN 的灵感来自多层感知器。通过在相邻层的神经元之间施加局部连通性约束，CNN 利用了局部空间相关性。

卷积神经网络的核心元素是通过卷积运算处理数据。任何信号与另一信号的卷积产生第三信号，该第三信号可能比原始信号本身揭示更多关于该信号的信息。在深入研究卷积神经网络之前，我们先来详细了解一下卷积。

## 卷积运算

时间或空间信号与另一信号的卷积产生初始信号的修改版本。修改后的信号可能比适合于特定任务的原始信号具有更好的特征表示。例如，通过将灰度图像作为 2D 信号与另一信号(通常称为滤波器或内核)进行卷积，可以获得包含原始图像边缘的输出信号。图像中的边缘可以对应于对象边界、照明的变化、材料属性的变化、深度的不连续性等等，这对于若干应用可能是有用的。关于系统的线性时间不变性或移位不变性的知识有助于人们更好地理解信号的卷积。在讨论卷积本身之前，我们将首先讨论这一点。

### 线性时不变(LTI) /线性移位不变(LSI)系统

系统以某种方式作用于输入信号，产生输出信号。如果一个输入信号 x(t)产生一个输出 y(t)，那么 y(t)可以表示为

![ $$ y(t)=f\left(x(t)\right) $$ ](A448418_1_En_3_Chapter_Equa.gif)

对于线性系统，以下用于缩放和叠加的属性应该成立:

![ $$ \boldsymbol{Scaling}:f\left(\alpha x(t)\right)=\alpha f\left(x(t)\right) $$ ](A448418_1_En_3_Chapter_Equb.gif)

![ $$ \boldsymbol{Superposition}:f\left(\alpha {x}_1(t)+\beta {x}_2(t)\right)=\alpha f\left(x(t)\right)+\beta f\left({x}_2(t)\right) $$ ](A448418_1_En_3_Chapter_Equc.gif)

同样，对于时间不变或一般平移不变的系统，

![ $$ f\left(x\left(t-\tau \right)\right)=y\left(t-\tau \right) $$ ](A448418_1_En_3_Chapter_Equd.gif)

这种具有线性和移位不变性的系统通常被称为线性移位不变(LSI)系统。当这样的系统处理时间信号时，它们被称为线性时不变(LTI)系统。在本章的其余部分，我们将把这样的系统称为 LSI 系统，不失一般性。见图 [3-1](#Fig1) 。

![A448418_1_En_3_Fig1_HTML.gif](A448418_1_En_3_Fig1_HTML.gif)

图 3-1。

Input–Output system

LSI 系统的关键特性是，如果知道系统对脉冲响应的输出，就可以计算出对任何信号的输出响应。

![A448418_1_En_3_Fig3_HTML.gif](A448418_1_En_3_Fig3_HTML.gif)

图 3-2b。

Response of an LTI system to a unit step impulse

![A448418_1_En_3_Fig2_HTML.gif](A448418_1_En_3_Fig2_HTML.gif)

图 3-2a。

Response of an LSI system to an impulse (Dirac Delta) function

在图 [3-2a](#Fig2) 和 [3-2b](#Fig3) 中，我们展示了系统对不同类型脉冲函数的脉冲响应。图 [3-2a](#Fig2) 显示了系统对狄拉克δ脉冲的连续脉冲响应，而图 [3-2b](#Fig3) 显示了系统对阶跃脉冲函数的离散脉冲响应。图 [3-2a](#Fig2) 中的系统是一个连续的 LTI 系统，因此需要一个狄拉克δ来确定其脉冲响应。另一方面，图 [3-2b](#Fig3) 中的系统是一个离散 LTI 系统，因此需要一个单位阶跃脉冲来确定其脉冲响应。

一旦我们知道了 LSI 系统对脉冲函数δ(t)的响应 h(t ),我们就可以通过将其与 h(t)卷积来计算 LTI 系统对任意输入信号 x(t)的响应 y(t)。数学上可以表示为![ $$ y(t)=x(t)\left({}^{\ast}\right)h(t) $$ ](A448418_1_En_3_Chapter_IEq1.gif)，其中(*)运算表示卷积。

系统的脉冲响应既可以是已知的，也可以通过记录系统对脉冲函数的响应来确定。例如，哈勃太空望远镜的脉冲响应可以通过将其聚焦在黑暗夜空中的一颗遥远的恒星上，然后记下记录的图像来找出。记录的图像是望远镜的脉冲响应。

### 一维信号的卷积

直观地说，卷积测量一个函数与另一个函数的反向和平移版本之间的重叠程度。在离散的情况下，

![ $$ y(t)=x(t)\left({}^{\ast}\right)h(t)=\sum \limits_{\tau =-\infty}^{+\infty }x\left(\tau \right)h\left(t-\tau \right) $$ ](A448418_1_En_3_Chapter_Eque.gif)

同样，在连续域中两个函数的卷积可以表示为

![ $$ y(t)=x(t)\left({}^{\ast}\right)h(t)=\underset{\tau =-\infty }{\overset{+\infty }{\int }}x\left(\tau \right)h\left(t-\tau \right) d\tau $$ ](A448418_1_En_3_Chapter_Equf.gif)

让我们对两个离散信号进行卷积，以便更好地解释这一操作。参见图 [3-3a](#Fig4) 至 [3-3c](#Fig6) 。

![A448418_1_En_3_Fig6_HTML.gif](A448418_1_En_3_Fig6_HTML.gif)

图 3-3c。

Output function from convolution

![A448418_1_En_3_Fig5_HTML.gif](A448418_1_En_3_Fig5_HTML.gif)

图 3-3b。

Functions for computing convolution operation

![A448418_1_En_3_Fig4_HTML.gif](A448418_1_En_3_Fig4_HTML.gif)

图 3-3a。

Input signals

在图 [3-3b](#Fig5) 中，函数![ $$ h\left(t-\tau \right) $$ ](A448418_1_En_3_Chapter_IEq2.gif)需要通过在水平轴上滑动来计算不同的 t 值。在 t 的每个值处，需要计算卷积和![ $$ {\sum}_{\tau =-\infty}^{+\infty }x\left(\tau \right)h\left(t-\tau \right) $$ ](A448418_1_En_3_Chapter_IEq3.gif)。总和可以被认为是 x(τ)的加权平均值，权重由![ $$ h\left(t-\tau \right). $$ ](A448418_1_En_3_Chapter_IEq4.gif)提供

![A448418_1_En_3_Fig7_HTML.gif](A448418_1_En_3_Fig7_HTML.gif)

图 3-3d。

Overlap of the functions in convolution at t = 2

*   当![ $$ t=-1 $$ ](A448418_1_En_3_Chapter_IEq5.gif)时，权重由![ $$ h\left(1-\tau \right) $$ ](A448418_1_En_3_Chapter_IEq6.gif)给出，但是权重不与 x(τ)重叠，因此总和为 0。
*   当![ $$ t=0 $$ ](A448418_1_En_3_Chapter_IEq7.gif)的权重由![ $$ h\left(-\tau \right) $$ ](A448418_1_En_3_Chapter_IEq8.gif)给出，x(τ)中唯一与权重重叠的元素是![ $$ x\left(\tau =0\right) $$ ](A448418_1_En_3_Chapter_IEq9.gif)，重叠的权重是 h( 0)。因此，卷积和为![ $$ x\left(\tau =0\right) $$ ](A448418_1_En_3_Chapter_IEq10.gif) *h( 0) = 1*3 = 3。由此，![ $$ y(0)=3\. $$ ](A448418_1_En_3_Chapter_IEq11.gif)
*   当![ $$ t=1 $$ ](A448418_1_En_3_Chapter_IEq12.gif)时，权重由![ $$ h\left(1-\tau \right) $$ ](A448418_1_En_3_Chapter_IEq13.gif)给出。元素 x(0)和 x(1)分别与权重 h(1)和 h(0)重叠。因此，卷积和是![ $$ x{(0)}^{\ast }h(1)+x{(1)}^{\ast }h(0)={1}^{\ast }2+{2}^{\ast }3=8\. $$ ](A448418_1_En_3_Chapter_IEq14.gif)
*   当![ $$ t=2 $$ ](A448418_1_En_3_Chapter_IEq15.gif)权重由![ $$ h\left(2-\tau \right). $$ ](A448418_1_En_3_Chapter_IEq16.gif)给出时，元素 x(0)、x(1)和 x(2)分别与权重 h(2)、h(1)和 h(0)重叠。因此，卷积和是元素![ $$ x{(0)}^{\ast}\mathrm{h}(2)+x{(1)}^{\ast}\mathrm{h}(1)+x{(2)}^{\ast}\mathrm{h}(0)={1}^{\ast }1+{2}^{\ast }2+{2}^{\ast }3=11 $$ ](A448418_1_En_3_Chapter_IEq17.gif)。图 [3-3d](#Fig7) 显示了![ $$ t=2 $$ ](A448418_1_En_3_Chapter_IEq18.gif)的两个功能的重叠。

## 模拟和数字信号

通常，显示时间和/或空间变化的任何感兴趣的量代表一个信号。因此，信号是时间和/或空间的函数。例如，特定股票在一周内的市场价格代表一个信号。

信号本质上可以是模拟的或数字的。然而，计算机不能处理模拟的连续信号，所以信号被制成数字信号进行处理。例如，语音是时间上的声学信号，其中语音能量的时间和幅度都是连续信号。当语音通过麦克风传输时，该声学连续信号被转换成电连续信号。如果我们想通过数字计算机处理模拟电信号，我们需要将模拟连续信号转换成离散信号。这是通过模拟信号的采样和量化来实现的。

采样指的是仅在固定的空间或时间间隔获取信号幅度。图 [3-4a](#Fig8) 对此进行了说明。

一般不会标注信号幅度的所有可能的连续值，但信号幅度一般会量化为一些固定的离散值，如图 [3-4b](#Fig9) 所示。通过采样和量化，一些信息从模拟连续信号中丢失。

![A448418_1_En_3_Fig9_HTML.gif](A448418_1_En_3_Fig9_HTML.gif)

图 3-4b。

Quantization of signal at discrete amplitude values

![A448418_1_En_3_Fig8_HTML.gif](A448418_1_En_3_Fig8_HTML.gif)

图 3-4a。

Sampling of a signal

采样和量化活动将模拟信号转换成数字信号。

数字图像可以表示为二维空间域中的数字信号。彩色 RGB 图像有三个通道:红色、绿色和蓝色。每个通道可以被认为是空间域中的信号，使得在每个空间位置，信号由像素强度表示。每个像素可以由 8 位表示，这在二进制中允许从 0 到 255 的 256 个像素强度。任何位置处的颜色都由对应于三个通道的该位置处的像素强度矢量来确定。因此，为了表示一种特定的颜色，需要使用 24 位信息。对于灰度图像，只有一个通道，像素亮度范围从 0 到 255。255 代表白色，而 0 代表黑色。

视频是具有时间维度的图像序列。黑白视频可以表示为其空间和时间坐标(x，y，t)的信号。彩色视频可以表示为三个信号的组合，空间和时间坐标对应于三个颜色通道，即红色、绿色和蓝色。

因此，灰度图像可以表示为函数 I(x，y ),其中 I 表示 x，y 坐标处的像素强度。对于数字图像，x、y 是采样坐标，取离散值。类似地，像素强度在 0 和 255 之间量化。

### 2D 和 3D 信号

维度为![ $$ N\times M $$ ](A448418_1_En_3_Chapter_IEq20.gif)的灰度图像可以表示为其空间坐标的标量 2D 信号。信号可以表示为

![ $$ x\left({n}_1,{n}_2\right),\kern0.75em 0<{n}_1<M-1,0<{n}_2<N-1 $$ ](A448418_1_En_3_Chapter_Equg.gif)

，其中 n <sub>1</sub> 和 n <sub>2</sub> 分别是沿水平轴和垂直轴的离散空间坐标，x(n <sub>1</sub> ，n <sub>2</sub> 表示空间坐标处的像素强度。像素亮度取值从 0 到 255。

彩色 RGB 图像是矢量 2D 信号，因为在每个空间坐标上有一个像素强度矢量。对于尺寸为![ $$ N\times M\times 3 $$ ](A448418_1_En_3_Chapter_IEq21.gif)的 RGB 图像，信号可以表示为

![ $$ x\left({n}_1,{n}_2\right)=\left[{x}_R\left({n}_1,{n}_2\right),{x}_G\left({n}_1,{n}_2\right),{x}_B\left({n}_1,{n}_2\right)\right],\kern0.5em 0<{n}_1<M-1,0<{n}_2<N-1 $$ ](A448418_1_En_3_Chapter_Equh.gif)

，其中 x <sub>R</sub> ，x <sub>G</sub> ，x <sub>B</sub> 表示沿着红色、绿色和蓝色通道的像素强度。参见图 [3-5a](#Fig10) 和 [3-5b](#Fig11) 。

![A448418_1_En_3_Fig11_HTML.gif](A448418_1_En_3_Fig11_HTML.gif)

图 3-5b。

Video as a 3D object

![A448418_1_En_3_Fig10_HTML.gif](A448418_1_En_3_Fig10_HTML.gif)

图 3-5a。

Grayscale image as a 2D discrete signal

## 2D 卷积

既然我们已经将灰度图像表示为 2D 信号，我们希望通过 2D 卷积来处理这些信号。图像可以与图像处理系统的脉冲响应进行卷积，以实现不同的目标，例如:

*   通过降噪滤波器去除图像中的可见噪声。对于白噪声，我们可以使用高斯滤波器。对于椒盐噪声，可以使用中值滤波器。
*   为了检测边缘，我们需要从图像中提取高频成分的滤波器。

图像处理滤波器可以被认为是线性和平移不变的图像处理系统。在我们开始图像处理之前，有必要了解不同的脉冲函数。

### 二维单位阶跃函数

一个二维单位阶跃函数δ(n <sub>1</sub> ，n <sub>2</sub> ，其中 n <sub>1</sub> 和 n <sub>2</sub> 为横坐标和纵坐标，可以表示为

![ $$ {\displaystyle \begin{array}{l}\delta \left({n}_1,{n}_2\right)=1\ when\ {n}_1=0\ and\ {n}_2=0\ \\ {}\kern4.75em =0\ elsewhere\ \end{array}} $$ ](A448418_1_En_3_Chapter_Equi.gif)

类似地，移位的单位阶跃函数可以表示为

![ $$ {\displaystyle \begin{array}{l}\delta \left({n}_1-{k}_1,{n}_2-{k}_2\right)=1\ when\ {n}_1={k}_1\ and\ {n}_2={k}_2\\ {}\kern9.5em =0\ elsewhere\ \end{array}} $$ ](A448418_1_En_3_Chapter_Equj.gif)

图 [3-6](#Fig12) 对此进行了说明。

![A448418_1_En_3_Fig12_HTML.gif](A448418_1_En_3_Fig12_HTML.gif)

图 3-6。

Unit step functions

任何离散的二维信号都可以表示为不同坐标下单位阶跃函数的加权和。让我们考虑如图 [3-7](#Fig13) 所示的信号 x(n <sub>1</sub> ，n <sub>2</sub> )。

![ $$ {\displaystyle \begin{array}{l}x\left({n}_1,{n}_2\right)\kern0.5em =\kern0.5em 1\kern0.5em when\ {n}_1=0\kern0.5em and\kern0.5em {n}_2=0\\ {}\kern4em =2\kern0.75em when\ {n}_1=0\kern0.5em and\kern0.5em {n}_2=1\\ {}\kern4em =3\kern0.75em when\ {n}_1=1\kern0.5em and\kern0.5em {n}_2=1\\ {}=\kern0.5em 0\kern0.5em elsewhere\\ {}x\left({n}_1,{n}_2\right)=x{\left(0,0\right)}^{\ast}\delta \left({n}_1,{n}_2\right)+x{\left(0,1\right)}^{\ast}\delta \left({n}_1,{n}_2-1\right)+x{\left(1,1\right)}^{\ast}\delta \left({n}_1-1,{n}_2-1\right)\\ {}\kern6.5em =\kern0.5em {1}^{\ast}\delta \left({n}_1,{n}_2\right)+{2}^{\ast}\delta \left({n}_1,{n}_2-1\right)+{3}^{\ast}\delta \left({n}_1-1,{n}_2-1\right)\end{array}} $$ ](A448418_1_En_3_Chapter_Equk.gif)

![A448418_1_En_3_Fig13_HTML.gif](A448418_1_En_3_Fig13_HTML.gif)

图 3-7。

Representing a 2D discrete signal as the weighted sum of unit step functions

所以，一般来说，任何离散的 2D 信号都可以写成:

![ $$ x\left({n}_1,{n}_2\right)=\sum \limits_{k_2=-\infty}^{+\infty}\sum \limits_{k_1=-\infty}^{+\infty }x\left({k}_1,{k}_2\right)\delta \left({n}_1-{k}_1,{n}_2-{k}_2\right) $$ ](A448418_1_En_3_Chapter_Equl.gif)

### 信号与大规模集成电路系统单位阶跃响应的 2D 卷积

当如上所述的任意离散 2D 信号通过具有变换 f 的 LSI 系统时，由于 LSI 系统的线性特性，

![ $$ f\left(x\left({n}_1,{n}_2\right)\right)=\sum \limits_{k_2=-\infty}^{+\infty}\sum \limits_{k_1=-\infty}^{+\infty }x\left({k}_1,{k}_2\right)f\left(\delta \left({n}_1-{k}_1,{n}_2-{k}_2\right)\right) $$ ](A448418_1_En_3_Chapter_Equm.gif)

现在，LSI 系统的单位阶跃响应![ $$ f\Big(\delta \left({n}_1,{n}_2\right)=h\left({n}_1,{n}_2\right) $$ ](A448418_1_En_3_Chapter_IEq22.gif)，并且由于 LSI 系统是移位不变的，![ $$ f\Big(\delta \left({n}_1-{k}_1,{n}_2-{k}_2\right)=h\left({n}_1-{k}_1,{n}_2-{k}_2\right). $$ ](A448418_1_En_3_Chapter_IEq23.gif)

因此，f (x(n <sub>1</sub> ，n <sub>2</sub> )可以表示为:

![ $$ f\left(x\left({n}_1,{n}_2\right)\right)=\sum \limits_{k_2=-\infty}^{+\infty}\sum \limits_{k_1=-\infty}^{+\infty }x\left({k}_1,{k}_2\right)h\left({n}_1-{k}_1,{n}_2-{k}_2\right) $$ ](A448418_1_En_3_Chapter_Equ1.gif)

(1)

前面的表达式表示信号与 LSI 系统的单位阶跃响应的 2D 卷积的表达式。为了说明 2D 卷积，让我们看一个例子，其中我们将 x(n <sub>1</sub> ，n <sub>2</sub> )与 h(n <sub>1</sub> ，n <sub>2</sub> )进行卷积。信号和单位阶跃响应信号定义如下，并在图 [3-8](#Fig14) :

![ $$ {\displaystyle \begin{array}{l}x\left({n}_1,{n}_2\right)\kern0.5em =4\kern1.5em when\ {n}_1=0,{n}_2=0\ \\ {}\kern6.5em =5\kern1.5em when\ {n}_1=1,{n}_2=0\\ {}\kern3.75em =2\kern1.5em when\ {n}_1=0,{n}_2=1\\ {}\kern3.75em =3\kern1.5em when\ {n}_1=1,{n}_2=1\\ {}\kern4em =0\kern1.5em elsewhere\\ {}h\left({n}_1,{n}_2\right)=1\kern1.5em when\ {n}_1=0,{n}_2=0\ \\ {}\kern6.5em =1\kern1.25em when\ {n}_1=1,{n}_2=0\\ {}\kern3.75em =1\kern1.5em when\ {n}_1=0,{n}_2=1\\ {}\kern3.75em =1\kern1.5em when\ {n}_1=1,{n}_2=1\\ {}\kern3.75em =0\kern1.5em elsewhere\end{array}} $$ ](A448418_1_En_3_Chapter_Equn.gif)

中进行了说明

![A448418_1_En_3_Fig14_HTML.gif](A448418_1_En_3_Fig14_HTML.gif)

图 3-8。

2D signal and unit step response of LSI system

为了计算卷积，我们需要在一组不同的坐标点上绘制信号。我们分别在横轴和纵轴上选择了 k <sub>1</sub> 和 k <sub>2</sub> 。同样，我们将脉冲响应 h(k <sub>1</sub> ，k <sub>2</sub> )反转为![ $$ h\left(-{k}_1,-{k}_2\right) $$ ](A448418_1_En_3_Chapter_IEq24.gif)，如图 [3-9(b)](#Fig15) 所示。然后，我们将反转函数![ $$ h\left(-{k}_1,-{k}_2\right) $$ ](A448418_1_En_3_Chapter_IEq25.gif)置于 n <sub>1</sub> 和 n <sub>2</sub> 的不同偏移值。广义反函数可以表示为![ $$ h\left({n}_1-{k}_1,{n}_2-{k}_2\right) $$ ](A448418_1_En_3_Chapter_IEq26.gif)。为了计算在 n <sub>1</sub> 和 n <sub>2</sub> 的特定值处卷积的输出 y(n <sub>1</sub> ，n <sub>2</sub> ，我们看到![ $$ h\left({n}_1-{k}_1,{n}_2-{k}_2\right) $$ ](A448418_1_En_3_Chapter_IEq27.gif)与 x(k <sub>1</sub> ，k <sub>2</sub> )重叠的点，并取信号和脉冲响应值的坐标方向乘积的总和作为输出。

正如我们在图 [3-9(c)](#Fig15) 中看到的，对于![ $$ \left({n}_1=0,{n}_2=0\right) $$ ](A448418_1_En_3_Chapter_IEq28.gif)偏移，唯一的重叠点是![ $$ \left({k}_1=0,{k}_2=0\right) $$ ](A448418_1_En_3_Chapter_IEq29.gif)，因此![ $$ y\left(0,0\right)=x{\left(0,0\right)}^{\ast }h\left(0,0\right)={4}^{\ast }1=4\. $$ ](A448418_1_En_3_Chapter_IEq30.gif)

同样，对于偏移![ $$ \left({n}_1=1,{n}_2=0\right) $$ ](A448418_1_En_3_Chapter_IEq31.gif)，重叠的点是点![ $$ \left({k}_1=0,{k}_2=0\right) $$ ](A448418_1_En_3_Chapter_IEq32.gif)和![ $$ \left({k}_1=1,{k}_2=0\right) $$ ](A448418_1_En_3_Chapter_IEq33.gif)，如图 [3-9(d)](#Fig15) 所示。

![ $$ {\displaystyle \begin{array}{l}y\left(1,0\right)=x{\left(0,0\right)}^{\ast }h\left(1-0,0-0\right)+x{\left(1,0\right)}^{\ast }h\left(1-1,0-0\right)\\ {}\kern3em =x{\left(0,0\right)}^{\ast }h\left(1,0\right)+x{\left(1,0\right)}^{\ast }h\left(0,0\right)\\ {}\kern3em ={4}^{\ast }2+{5}^{\ast }1=13\end{array}} $$ ](A448418_1_En_3_Chapter_Equo.gif)

对于偏移![ $$ \left({n}_1=1,{n}_2=1\right) $$ ](A448418_1_En_3_Chapter_IEq34.gif)，重叠的点就是点![ $$ \left({k}_1=1,{k}_2=0\right) $$ ](A448418_1_En_3_Chapter_IEq35.gif)，如图 [3-9(e)](#Fig15) 所示。

![ $$ {\displaystyle \begin{array}{l}y\left(2,0\right)=x{\left(1,0\right)}^{\ast }h\left(2-1,0-0\right)\\ {}\kern3.5em =x{\left(1,0\right)}^{\ast }h\left(1,0\right)\\ {}\kern3.5em ={5}^{\ast }2=10\end{array}} $$ ](A448418_1_En_3_Chapter_Equp.gif)

按照这种通过改变 n <sub>1</sub> 和 n <sub>2</sub> 来移动单位阶跃响应信号的方法，可以计算整个函数 y(n <sub>1</sub> ，n <sub>2</sub> )。

![A448418_1_En_3_Fig15_HTML.gif](A448418_1_En_3_Fig15_HTML.gif)

图 3-9。

Convolution at different coordinate points

### 图像对不同 LSI 系统响应的 2D 卷积

任何图像都可以与 LSI 系统的单位阶跃响应进行卷积。那些 LSI 系统单元阶跃响应被称为滤波器或内核。例如，当我们试图通过相机拍摄图像，而图像由于手的抖动而变得模糊时，引入的模糊可以被视为具有特定单位阶跃响应的 LSI 系统。这个单位阶跃响应对实际图像进行卷积，并产生模糊图像作为输出。我们通过相机拍摄的任何图像都会与相机的单位阶跃响应进行卷积。因此，摄像机可以被视为具有特定单位阶跃响应的 LSI 系统。

任何数字图像都是 2D 离散信号。![ $$ N\times M $$ ](A448418_1_En_3_Chapter_IEq36.gif) 2D 图像 x(n <sub>1</sub> ，n <sub>2</sub> )与 2D 图像处理滤波器 h(n <sub>1</sub> ，n <sub>2</sub> )的卷积由

![ $$ y\left({n}_1,{n}_2\right)=\sum \limits_{k_2=0}^{N-1}\sum \limits_{k_1=0}^{M-1}x\left({k}_1,{k}_2\right)h\left({n}_1-{k}_1,{n}_2-{k}_2\right) $$ ](A448418_1_En_3_Chapter_Equq.gif)

![ $$ 0\le {n}_1\le N-1,0\le {n}_2\le M-1\. $$ ](A448418_1_En_3_Chapter_Equ2.gif)

给出

图像处理滤波器处理灰度图像的(2D)信号，以产生另一个图像(2D 信号)。在多通道图像的情况下，通常使用 2D 图像处理滤波器进行图像处理，这意味着必须将每个图像通道作为 2D 信号进行处理，或者将图像转换为灰度图像。

既然我们已经学习了卷积的概念，我们知道可以将 LSI 系统的任何单位阶跃响应称为滤波器或内核。

2D 卷积的一个例子如图 [3-10a](#Fig16) 所示。

![A448418_1_En_3_Fig17_HTML.gif](A448418_1_En_3_Fig17_HTML.gif)

Figure 3-10b.

![A448418_1_En_3_Fig16_HTML.gif](A448418_1_En_3_Fig16_HTML.gif)

图 3-10a。

Example of 2D convolution of images

为了保持输出图像的长度与输入图像的长度相同，对原始图像进行了零填充。正如我们所看到的，翻转的滤波器或内核在原始图像的各个区域上滑动，并在每个坐标点计算卷积和。请注意，图 [3-10b](#Fig17) 中提到的强度 I[i，j]中的索引表示矩阵坐标。同样的示例问题通过 scipy 2D 卷积以及通过清单 [3-1](#Par54) 中的基本逻辑来解决。在这两种情况下，结果是一样的。

![A448418_1_En_3_Fig18_HTML.jpg](A448418_1_En_3_Fig18_HTML.jpg)

Figure 3-11.

```
## Illustate 2D convolution of images through an example

import scipy.signal
import numpy as np
# Take a 7x7 image as example
image = np.array([[1, 2, 3, 4, 5, 6, 7],
         [8, 9, 10, 11, 12, 13, 14],
         [15, 16, 17, 18, 19, 20, 21],
         [22, 23, 24, 25, 26, 27, 28],
         [29, 30, 31, 32, 33, 34, 35],
         [36, 37, 38, 39, 40, 41, 42],
         [43, 44, 45, 46, 47, 48, 49]])

# Defined an image-processing kernel
filter_kernel = np.array([[-1, 1, -1],
                 [-2, 3, 1],
                 [2, -4, 0]])

# Convolve the image with the filter kernel through scipy 2D convolution to produce an output image of same dimension as that of the input

I = scipy.signal.convolve2d(image, filter_kernel,mode='same', boundary='fill', fillvalue=0)
print(I)

# We replicate the logic of a scipy 2D convolution by going through the following steps
# a) The boundaries need to be extended in both directions for the image and padded with zeroes.
#  For convolving the 7x7 image by 3x3 kernel, the dimensions need to be extended by (3-1)/2—i.e., 1—
#on either side for each dimension. So a skeleton image of 9x9 image would be created
# in which the boundaries of 1 pixel are pre-filled with zero.
# b) The kernel needs to be flipped—i.e., rotated—by 180 degrees
# c) The flipped kernel needs to placed at each coordinate location for the image and then the sum of
#coordinate-wise product with the image intensities needs to be computed. These sums for each coordinate would give
#the intensities for the output image.

row,col=7,7
## Rotate the filter kernel twice by 90 degrees to get 180 rotation
filter_kernel_flipped = np.rot90(filter_kernel,2)
## Pad the boundaries of the image with zeroes and fill the rest from the original image
image1 = np.zeros((9,9))
for i in xrange(row):
    for j in xrange(col):
        image1[i+1,j+1] = image[i,j]
print(image1)

## Define the output image
image_out = np.zeros((row,col))
## Dynamic shifting of the flipped filter at each image coordinate and then computing the convolved sum.
for i in xrange(1,1+row):
    for j in xrange(1,1+col):
        arr_chunk = np.zeros((3,3))

        for k,k1 in zip(xrange(i-1,i+2),xrange(3)):
            for l,l1 in zip(xrange(j-1,j+2),xrange(3)):
                arr_chunk[k1,l1] = image1[k,l]

        image_out[i-1,j-1] = np.sum(np.multiply(arr_chunk,filter_kernel_flipped))

print(image_out)

Listing 3-1.

```

基于图像处理滤波器的选择，输出图像的性质会有所不同。例如，高斯滤波器将创建输入图像的模糊版本的输出图像，而 Sobel 滤波器将检测图像中的边缘并产生包含输入图像边缘的输出图像。

## 常见的图像处理过滤器

让我们讨论一下 2D 图像上常用的图像处理滤波器。一定要弄清楚符号，因为索引图像的自然方式与定义 x 轴和 y 轴的方式不太一致。每当我们在坐标空间中表示图像处理滤波器或图像时，n <sub>1</sub> 和 n <sub>2</sub> 是 x 和 y 方向的离散坐标。numpy 矩阵形式的图像的列索引正好与 x 轴重合，而行索引则向 y 轴的相反方向移动。此外，在进行卷积时，选择哪个像素位置作为图像信号的原点并不重要。基于是否使用零填充，可以相应地处理边缘。由于滤波器核的尺寸较小，我们通常翻转滤波器核，然后在图像上滑动，而不是反过来。

### 均值滤波器

均值滤波器或平均滤波器是一种低通滤波器，用于计算任意特定点的像素强度的局部平均值。均值滤波器的脉冲响应可以是这里看到的任何形式(见图 [3-12](#Fig19) ):

![ $$ \left[\begin{array}{ccc}1/9& 1/9& 1/9\\ {}1/9& 1/9& 1/9\\ {}1/9& 1/9& 1/9\end{array}\right] $$ ](A448418_1_En_3_Chapter_Equr.gif)

![A448418_1_En_3_Fig19_HTML.gif](A448418_1_En_3_Fig19_HTML.gif)

图 3-12。

Impulse response of a Mean filter

这里，矩阵条目 h <sub>22</sub> 对应于原点处的条目。因此，在任何给定点，卷积将代表该点像素强度的平均值。清单 [3-2](#Par60) 中的代码说明了如何使用图像处理滤波器(如均值滤波器)对图像进行卷积。

请注意，在许多 Python 实现中，我们将使用 OpenCV 对图像执行基本操作，例如读取图像、将图像从 RGB 格式转换为灰度格式等等。OpenCV 是一个开源的图像处理包，它有一套丰富的图像处理方法。建议读者探索 OpenCV 或任何其他图像处理工具箱，以便熟悉基本的图像处理功能。

```
import cv2
img = cv2.imread('monalisa.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
plt.imshow(gray,cmap='gray')
mean = 0
var = 100
sigma = var**0.5
row,col = 650,442
gauss = np.random.normal(mean,sigma,(row,col))
gauss = gauss.reshape(row,col)
gray_noisy = gray + gauss
plt.imshow(gray_noisy,cmap='gray')
## Mean filter
Hm = np.array([[1,1,1],[1,1,1],[1,1,1]])/float(9)
Gm = convolve2d(gray_noisy,Hm,mode='same')
plt.imshow(Gm,cmap='gray')
Listing 3-2.Convolution of an Image with Mean Filter

```

在清单 [3-2](#Par60) 中，我们读取了蒙娜丽莎的图像，然后在图像中引入了一些高斯白噪声。高斯噪声的平均值为 0，方差为 100。然后，我们用均值滤波器卷积噪声图像，以减少白噪声。图 [3-13](#Fig20) 显示了噪声图像和卷积后的图像。

![A448418_1_En_3_Fig20_HTML.gif](A448418_1_En_3_Fig20_HTML.gif)

图 3-13。

Mean filter processing on Mona Lisa image

均值滤波器主要用于降低图像中的噪声。如果图像中存在一些白高斯噪声，则均值滤波器将减少噪声，因为它在其邻域上求平均值，因此零均值的白噪声将被抑制。从图 [3-13](#Fig20) 中我们可以看到，一旦图像与均值滤波器卷积，高斯白噪声就会减少。新图像具有较少的高频分量，因此与卷积之前的图像相比相对不太清晰，但是滤波器在减少白噪声方面做得很好。

### 中值滤波器

2D 中值滤波器根据滤波器大小，用邻域中的中值像素强度替换邻域中的每个像素。中值滤波器有利于去除椒盐噪声。这种类型的噪声以黑白像素的形式出现在图像中，通常是由捕捉图像时的突然干扰引起的。清单 [3-3](#Par64) 展示了如何将椒盐噪声添加到图像中，然后如何使用中值滤波器抑制噪声。

![A448418_1_En_3_Fig21_HTML.gif](A448418_1_En_3_Fig21_HTML.gif)

图 3-14。

Median filter processing

```
## Generate random integers from 0 to 20
## If the value is zero we will replace the image pixel with a low value of 0 that corresponds to a black pixel
## If the value is 20 we will replace the image pixel with a high value of 255 that corresponds to a white pixel
## We have taken 20 integers, out of which we will only tag integers 1 and 20 as salt and pepper noise
## Hence, approximately 10% of the overall pixels are salt and pepper noise. If we want to reduce it
## to 5% we can take integers

from 0 to 40 and then treat 0 as an indicator for a black pixel and 40 as an indicator for a white pixel.

np.random.seed(0)
gray_sp = gray*1
sp_indices = np.random.randint(0,21,[row,col])
for i in xrange(row):
    for j in xrange(col):
        if sp_indices[i,j] == 0:
            gray_sp[i,j] = 0
        if sp_indices[i,j] == 20:
            gray_sp[i,j] = 255
plt.imshow(gray_sp,cmap='gray')

## Now we want to remove the salt and pepper noise through a Median filter.
## Using the opencv Median filter for the same

gray_sp_removed = cv2.medianBlur(gray_sp,3)
plt.imshow(gray_sp_removed,cmap='gray')

##Implementation of the 3x3 Median filter without using opencv

gray_sp_removed_exp = gray*1
for i in xrange(row):
    for j in xrange(col):
        local_arr = []
        for k in xrange(np.max([0,i-1]),np.min([i+2,row])):
            for l in xrange(np.max([0,j-1]),np.min([j+2,col])):
                local_arr.append(gray_sp[k,l])
        gray_sp_removed_exp[i,j] = np.median(local_arr)
plt.imshow(gray_sp_removed_exp,cmap='gray')

Listing 3-3.

```

正如我们所看到的，椒盐噪声已经被中值滤波器去除了。

### 高斯滤波器

高斯滤波器是均值滤波器的修改版本，其中脉冲函数的权重正态分布在原点周围。重量在过滤器的中心最高，通常远离中心下降。可以用清单 [3-4](#Par68) 中的代码创建一个高斯滤波器。正如我们所见，强度以高斯方式远离原点下降。当显示为图像时，高斯滤波器在原点处具有最高的强度，然后对于远离中心的像素逐渐减弱。高斯滤波器通过抑制高频分量来降低噪声。然而，在追求抑制高频成分的过程中，它最终产生了一个模糊的图像，称为高斯模糊。

在图 [3-15](#Fig22) 中，原始图像与高斯滤波器进行卷积，以产生具有高斯模糊的图像。然后，我们从原始图像中减去模糊图像，以获得图像的高频分量。高频图像的一小部分被添加到原始图像中，以提高图像的清晰度。

![A448418_1_En_3_Fig22_HTML.gif](A448418_1_En_3_Fig22_HTML.gif)

图 3-15。

Various activities with Gaussian filter kernel

```
Hg = np.zeros((20,20))
for i in xrange(20):
    for j in xrange(20):
        Hg[i,j] = np.exp(-((i-10)**2 + (j-10)**2)/10)
plt.imshow(Hg,cmap='gray')
gray_blur = convolve2d(gray,Hg,mode='same')
plt.imshow(gray_blur,cmap='gray')
gray_enhanced = gray + 0.025*gray_high
plt.imshow(gray_enhanced,cmap='gray')
Listing 3-4.

```

### 基于梯度的过滤器

回顾一下，二维函数 I(x，y)的梯度由下式给出:

![ $$ \nabla I\left(x,y\right)={\left[\frac{\partial I\left(x,y\right)}{\partial x}\frac{\partial I\left(x,y\right)}{\partial y}\right]}^T $$ ](A448418_1_En_3_Chapter_Equs.gif)

其中沿水平方向的梯度由下式给出

![ $$ \frac{\partial I\left(x,y\right)}{\partial x}=\underset{h\to 0}{\lim}\frac{I\left(x+h,y\right)-I\left(x,y\right)}{h} $$ ](A448418_1_En_3_Chapter_IEq37.gif)或![ $$ \underset{h\to 0}{\lim}\frac{I\left(x+h,y\right)-I\left(x-h,y\right)}{2h} $$ ](A448418_1_En_3_Chapter_IEq38.gif)基于方便和手头的问题。

对于离散坐标，我们可以取![ $$ h=1 $$ ](A448418_1_En_3_Chapter_IEq39.gif)，沿水平方向近似梯度如下:

![ $$ \frac{\partial I\left(x,y\right)}{\partial x}=I\left(x+1,y\right)-I\left(x,y\right) $$ ](A448418_1_En_3_Chapter_Equt.gif)

信号的这种导数可以通过将信号与滤波器核![ $$ \left[\begin{array}{ccc}0& 0& 0\\ {}0& 1& -1\\ {}0& 0& 0\end{array}\right] $$ ](A448418_1_En_3_Chapter_IEq40.gif)进行卷积来实现。

同样，

![ $$ \frac{\partial I\left(x,y\right)}{\partial x}\propto I\left(x+1,y\right)-I\left(x-1,y\right) $$ ](A448418_1_En_3_Chapter_Equu.gif)

来自第二个表象。

这种形式的导数可以通过将信号与滤波器内核![ $$ \left[\begin{array}{ccc}0& 0& 0\\ {}1& 0& -1\\ {}0& 0& 0\end{array}\right] $$ ](A448418_1_En_3_Chapter_IEq41.gif)进行卷积来实现。

对于垂直方向，离散情况下的梯度分量可表示为

![ $$ \frac{\partial I\left(x,y\right)}{\partial y}=I\left(x,y+1\right)-I\left(x,y\right) $$ ](A448418_1_En_3_Chapter_IEq42.gif)或![ $$ \frac{\partial I\left(x,y\right)}{\partial y}\propto I\left(x,y+1\right)-I\left(x,y-1\right) $$ ](A448418_1_En_3_Chapter_IEq43.gif)

通过卷积计算梯度对应的滤波器核分别是![ $$ \left[\begin{array}{ccc}0& -1& 0\\ {}0& 1& 0\\ {}0& 0& 0\end{array}\right] $$ ](A448418_1_En_3_Chapter_IEq44.gif)和![ $$ \left[\begin{array}{ccc}0& -1& 0\\ {}0& 0& 0\\ {}0& 1& 0\end{array}\right] $$ ](A448418_1_En_3_Chapter_IEq45.gif)。

请注意，这些过滤器采用 x 轴和 y 轴方向，如图 [3-16](#Fig23) 所示。x 方向与矩阵索引 n <sub>2</sub> 增量一致，而 y 方向与矩阵索引 n <sub>1</sub> 增量相反。

![A448418_1_En_3_Fig23_HTML.gif](A448418_1_En_3_Fig23_HTML.gif)

图 3-16。

Vertical and horizontal gradient filters

图 [3-16](#Fig23) 展示了蒙娜丽莎图像与水平和垂直渐变滤镜的卷积。

### 索贝尔边缘检测滤波器

索贝尔边缘检测器沿着水平轴和垂直轴的脉冲响应可以分别由下面的 H <sub>x</sub> 和 H <sub>y</sub> 矩阵表示。索贝尔检测器是刚才说明的水平和垂直梯度滤波器的扩展。它不是只取该点的梯度，而是取其两侧各点的梯度之和。此外，它赋予了兴趣点双倍的权重。见图 [3-17](#Fig24) 。

![ $$ Hx=\left[\begin{array}{ccc}1& 0& -1\\ {}2& 0& -2\\ {}1& 0& -1\end{array}\right]=\left[1\kern0.5em 2\kern0.5em 1\right]\left[\begin{array}{ccc}1& 0& -1\end{array}\right] $$ ](A448418_1_En_3_Chapter_Equv.gif)

![ $$ Hy=\left[\begin{array}{ccc}-1& -2& -1\\ {}0& 0& 0\\ {}1& 2& 1\end{array}\right] $$ ](A448418_1_En_3_Chapter_Equw.gif)T11】

![A448418_1_En_3_Fig24_HTML.gif](A448418_1_En_3_Fig24_HTML.gif)

图 3-17。

Sobel filter impulse response

列表 [3-5](#Par82) 中显示了图像与 Sobel 滤波器的卷积。

![A448418_1_En_3_Fig25_HTML.gif](A448418_1_En_3_Fig25_HTML.gif)

图 3-18。

Output of various Sobel filters

```
Hx = np.array([[ 1,0, -1],[2,0,-2],[1,0,-1]],dtype=np.float32)
Gx = convolve2d(gray,Hx,mode='same')
plt.imshow(Gx,cmap='gray')

Hy = np.array([[ -1,-2, -1],[0,0,0],[1,2,1]],dtype=np.float32)
Gy = convolve2d(gray,Hy,mode='same')
plt.imshow(Gy,cmap='gray')

G = (Gx*Gx + Gy*Gy)**0.5
plt.imshow(G,cmap='gray')

Listing 3-5.Convolution Using a Sobel Filter

```

清单 [3-5](#Par82) 具有用 Sobel 滤波器卷积图像所需的逻辑。水平 Sobel 滤波器检测水平方向上的边缘，而垂直 Sobel 滤波器检测垂直方向上的边缘。两者都是高通滤波器，因为它们会衰减信号中的低频成分，只捕捉图像中的高频成分。边缘是图像的重要特征，有助于检测图像中的局部变化。边缘通常出现在图像中两个区域之间的边界上，并且通常是从图像中检索信息的第一步。我们在图 [3-18](#Fig25) 中看到列表 [3-5](#Par82) 的输出。针对每个位置使用水平和垂直索贝尔滤波器获得的图像的像素值可以被认为是向量![ $$ {I}^{\hbox{'}}\left(x,y\right)={\left[{I}_x\left(x,y\right){I}_y\left(\mathrm{x},\mathrm{y}\right)\right]}^T $$ ](A448418_1_En_3_Chapter_IEq46.gif)，其中 I <sub> x </sub> (x，y)表示通过水平索贝尔滤波器获得的图像的像素强度，I <sub> y </sub> (x，y)表示通过垂直索贝尔滤波器获得的图像的像素强度。向量 I’(x，y)的大小可以用作组合 Sobel 滤波器的像素强度。

![ $$ C\left(x,y\right)=\sqrt{{\left({I}_x\left(x,y\right)\right)}^2+{\left({I}_y\left(x,y\right)\right)}^2} $$ ](A448418_1_En_3_Chapter_IEq47.gif)，其中 C(x，y)表示组合索贝尔滤波器的像素强度函数。

### 身份转换

通过卷积进行身份变换的滤波器如下:

![ $$ \left[\begin{array}{ccc}0& 0& 0\\ {}0& 1& 0\\ {}0& 0& 0\end{array}\right] $$ ](A448418_1_En_3_Chapter_Equx.gif)

图 [3-19](#Fig26) 显示了通过卷积进行的单位变换。

![A448418_1_En_3_Fig26_HTML.gif](A448418_1_En_3_Fig26_HTML.gif)

图 3-19。

Identity transform through convolution

表 [3-1](#Tab1) 列出了几种有用的图像处理过滤器及其用途。

表 3-1。

Image-Processing Filters and Their Uses

<colgroup><col> <col></colgroup> 
| 过滤器 | 使用 |
| --- | --- |
| 均值滤波器 | 减少高斯噪声，平滑上采样后的图像 |
| 中值滤波器 | 减少盐和胡椒的噪音 |
| 索贝尔过滤器 | 检测图像中的边缘 |
| 高斯滤波器 | 减少图像中的噪声 |
| Canny 滤波器 | 检测图像中的边缘 |
| 韦纳过滤器 | 减少附加噪声和模糊 |

## 卷积神经网络

卷积神经网络(CNN)基于图像的卷积，并基于 CNN 通过训练学习的过滤器来检测特征。例如，我们不应用任何已知的滤波器，如用于检测边缘或去除高斯噪声的滤波器，但通过卷积神经网络的训练，该算法自己学习图像处理滤波器，这可能与普通的图像处理滤波器非常不同。对于监督训练，以尽可能降低总成本函数的方式学习滤波器。通常，第一卷积层学习检测边缘，而第二卷积层可以学习检测更复杂的形状，这些形状可以通过组合不同的边缘来形成，例如圆形和矩形等等。第三层和更高层基于前一层生成的特征学习更复杂的特征。

卷积神经网络的优点是权重共享导致的稀疏连接，这大大减少了要学习的参数数量。同一个滤波器可以通过它的等方差特性来学习检测图像的任何给定部分中的相同边缘，这是卷积对于特征检测非常有用的特性。

## 卷积神经网络的组件

以下是卷积神经网络的典型组件:

![A448418_1_En_3_Fig27_HTML.gif](A448418_1_En_3_Fig27_HTML.gif)

图 3-20。

Basic flow diagram of a convolutional neural network

*   输入层将保存图像的像素强度。例如，红色、绿色和蓝色通道(RGB)的宽度为 64、高度为 64、深度为 3 的输入图像的输入尺寸为![ $$ 64\times 64\times 3 $$ ](A448418_1_En_3_Chapter_IEq48.gif)。
*   卷积图层将从前面的图层中提取图像，并与指定数量的过滤器进行卷积，以创建称为输出要素地图的图像。输出特征地图的数量等于指定的过滤器数量。到目前为止，张量流中的 CNN 大多使用 2D 滤波器；然而，最近已经引入了 3D 卷积滤波器。
*   细胞神经网络的激活函数通常是 ReLUs，我们在第 [2](2.html) 章中讨论过。通过 ReLU 激活层后，输出维度与输入维度相同。ReLU 层增加了网络的非线性，同时为正的网络输入提供了不饱和的梯度。
*   池图层将沿高度和宽度维度对 2D 激活图进行缩减采样。激活图的深度或数量没有受到损害，并且保持不变。
*   完全连接的层包含传统的神经元，这些神经元从前面的层接收不同组的权重；它们之间没有典型的卷积运算中的权重分配。该层中的每个神经元将通过单独的权重连接到前一层中的所有神经元，或者连接到输出图中的所有坐标输出。对于分类，类输出神经元从最终完全连接的层接收输入。

图 [3-20](#Fig27) 展示了一个基本的卷积神经网络(CNN ),它使用一个卷积层、一个 ReLU 层和一个池层，然后是一个全连接层，最后是输出分类层。该网络试图从非蒙娜丽莎图像中辨别出蒙娜丽莎图像。输出单元可以采用 sigmoid 激活函数，因为它是图像的二进制分类问题。通常，对于大多数 CNN 架构，在完全连接的层之前，几个到几个卷积层-ReLU 层-池层组合被一个接一个地堆叠。我们将在稍后讨论不同的架构。现在，让我们更详细地看看不同的层。

### 输入层

这一层的输入是图像。通常，图像作为四维张量被成批地馈送，其中第一维特定于图像索引，第二维和第三维特定于图像的高度和宽度，第四维对应于不同的通道。对于彩色图像，通常我们有红色(R)、绿色(G)和蓝色(B)通道，而对于灰度图像，我们只有一个通道。一批中图像的数量将由为小批量随机梯度下降选择的小批量大小决定。随机梯度下降的批量大小为 1。

输入可以通过 TensorFlow placeholder `tf.placeholder at`运行时以小批量的形式输入到输入层。

### 卷积层

卷积是任何 CNN 网络的核心。TensorFlow 支持 2D 和 3D 卷积。然而，2D 卷积更常见，因为三维卷积是计算内存密集型。输出特征图形式的输入图像或中间图像与指定大小的 2D 滤波器进行 2D 卷积。2D 卷积沿着空间维度发生，而沿着图像体的深度通道没有卷积。对于每个深度通道，生成相同数量的特征图，然后在它们通过 ReLU 激活之前，沿着深度维度将它们加在一起。这些过滤器有助于检测图像中的特征。网络中的卷积层越深，它学习的复杂特征就越多。例如，初始卷积层可以学习检测图像中的边缘，而第二卷积层可以学习连接边缘以形成几何形状，例如圆形和矩形。更深的卷积层可能学会检测更复杂的特征；例如，在猫和狗的分类中，它可以学习检测动物的眼睛、鼻子或其他身体部位。

在 CNN 中，只规定了过滤器的尺寸；在训练开始之前，权重被初始化为任意值。滤波器的权重是通过 CNN 训练过程学习的，因此它们可能不代表传统的图像处理滤波器，例如 Sobel、高斯、均值、中值或其他类型的滤波器。相反，所学习的滤波器将使得所定义的总损失函数最小化，或者基于验证实现良好的概括。虽然它可能不会学习传统的边缘检测滤波器，但它会学习几种以某种形式检测边缘的滤波器，因为边缘是图像的良好特征检测器。

定义卷积层时应该熟悉的一些术语如下:

过滤器尺寸–过滤器尺寸定义过滤器内核的高度和宽度。大小为![ $$ 3\times 3 $$ ](A448418_1_En_3_Chapter_IEq49.gif)的滤波器核将具有九个权重。通常，这些滤波器被初始化并在输入图像上滑动以进行卷积，而不翻转这些滤波器。从技术上讲，当卷积在不翻转滤波器核的情况下执行时，它被称为互相关，而不是卷积。然而，这没关系，因为我们可以将学习到的过滤器视为图像处理过滤器的翻转版本。

跨距–跨距决定执行卷积时在每个空间方向上移动的像素数。在信号的正常卷积中，我们通常不跳过任何像素，而是在每个像素位置计算卷积和，因此对于 2D 信号，我们沿两个空间方向的步幅为 1。然而，在卷积时可以选择跳过每一个交替的像素位置，因此选择步幅 2。如果沿图像的高度和宽度选择步长 2，那么在卷积之后，输出图像将大约是输入图像大小的![ $$ \frac{1}{4} $$ ](A448418_1_En_3_Chapter_IEq50.gif)。为什么它大约是原始图像或特征地图大小的![ $$ \frac{1}{4} $$ ](A448418_1_En_3_Chapter_IEq51.gif)而不是精确的![ $$ \frac{1}{4} $$ ](A448418_1_En_3_Chapter_IEq52.gif)将在我们下一个讨论的主题中涉及。

填充–当我们通过一个过滤器卷积一个特定大小的图像时，结果图像通常比原始图像小。例如，如果我们通过大小为![ $$ 3\times 3 $$ ](A448418_1_En_3_Chapter_IEq54.gif)的滤波器对 5 ![ $$ \times $$ ](A448418_1_En_3_Chapter_IEq53.gif) 5 的 2D 图像进行卷积，得到的图像就是![ $$ 3\times 3 $$ ](A448418_1_En_3_Chapter_IEq55.gif)。

填充是一种将零附加到图像边界以控制卷积输出大小的方法。沿着特定空间维度的卷积输出图像长度 L<sup>′</sup>由

![ $$ {L}^{\prime }=\frac{L-K+2P}{S}+1 $$ ](A448418_1_En_3_Chapter_Equy.gif)

给出，其中

![ $$ L\to $$ ](A448418_1_En_3_Chapter_IEq56.gif)输入图像在特定维度的长度

![ $$ K\to $$ ](A448418_1_En_3_Chapter_IEq57.gif)特定维度中内核/过滤器的长度

![ $$ P\to $$ ](A448418_1_En_3_Chapter_IEq58.gif)沿维度两端填充的零

![ $$ S\to $$ ](A448418_1_En_3_Chapter_IEq59.gif)卷积的步幅

一般来说，对于 1 的步幅，沿着每个维度的图像尺寸在任一端都减少了![ $$ \left(K-1\right)/2 $$ ](A448418_1_En_3_Chapter_IEq60.gif)，其中 K 是沿着该维度的滤波器核的长度。因此，为了保持输出图像与输入图像相同，需要长度为![ $$ \frac{K-1}{2} $$ ](A448418_1_En_3_Chapter_IEq61.gif)的焊盘。

可以从沿着特定方向的输出图像长度中找出特定步幅大小是否可行。例如，如果![ $$ L=12,\kern0.5em K=3 $$ ](A448418_1_En_3_Chapter_IEq62.gif)和![ $$ P=0 $$ ](A448418_1_En_3_Chapter_IEq63.gif)，跨距![ $$ S=2 $$ ](A448418_1_En_3_Chapter_IEq64.gif)是不可能的，因为它将产生沿空间维度的输出长度![ $$ \frac{\left(12-3\right)}{2}=4.5 $$ ](A448418_1_En_3_Chapter_IEq65.gif)，它不是一个整数值。

在 TensorFlow 中，填充可以选择为`"VALID"`或`"SAME"`。`"SAME"`确保在选择步长为 1 的情况下，图像的输出空间尺寸与输入空间尺寸相同。它使用零填充来实现这一点。它试图在尺寸的两侧保持零填充长度均匀，但是如果该尺寸的总填充长度为奇数，则额外的长度将被添加到水平尺寸的右侧和垂直尺寸的底部。

`"VALID"`不使用零填充，因此输出图像尺寸将小于输入图像尺寸，即使步幅为 1。

#### tensorflow 用法

```
def conv2d(x,W,b,strides=1):
    x = tf.nn.conv2d(x,W,strides=[1,strides,strides,1],padding='SAME')
    x = tf.nn.bias_add(x,b)
    return tf.nn.relu(x)

```

为了定义张量流卷积层，我们使用`tf.nn.conv2d`来定义卷积的输入、与卷积相关的权重、步长和填充类型。此外，我们为每个输出特征图添加了一个偏差。最后，我们使用经过整流的线性单元 ReLUs 作为激活，以将非线性添加到系统中。

### 汇集层

对图像的汇集操作通常概括了图像的局部性，该局部性由滤波器核的大小给出，也称为感受域。汇总通常以最大池或平均池的形式进行。在最大池中，一个位置的最大像素强度被作为该位置的代表。在平均池中，一个地点周围的像素强度的平均值被作为该地点的代表。池减少了图像的空间维度。决定局部性的内核大小通常选择为![ $$ 2\times 2 $$ ](A448418_1_En_3_Chapter_IEq66.gif)，而步幅选择为 2。这将图像大小缩小到原始图像的大小。

#### tensorflow 用法

```
''' P O O L I N G  L A Y E R'''
def maxpool2d(x,stride=2):
    return tf.nn.max_pool(x,ksize=[1,stride,stride,1],strides=[1,stride,stride,1],padding='SAME')

```

`tf.nn.max_pool`定义用于定义最大池层，而`tf.nn.avg_pool`用于定义平均池层。除了输入，我们还需要通过`ksize`参数输入最大池的感受野或内核大小。此外，我们需要提供用于最大池的步长。为了确保池化输出要素地图的每个空间位置中的值来自输入中的独立邻域，每个空间维度中的步幅应选择为等于相应空间维度中的核大小。

## 通过卷积层的反向传播

![A448418_1_En_3_Fig28_HTML.gif](A448418_1_En_3_Fig28_HTML.gif)

图 3-21。

Backpropagation through the convolutional layer

通过卷积层的反向传播非常类似于多层感知器网络的反向传播。唯一的区别是权重连接是稀疏的，因为不同的输入邻域共享相同的权重来创建输出要素地图。每个输出特征图是来自前一层的图像或特征图与滤波器核的卷积的结果，滤波器核的值是我们需要通过反向传播来学习的权重。对于特定的输入-输出特征映射组合，滤波器内核中的权重是共享的。

在图 [3-21](#Fig28) 中，层 L 中的特征图 A 与一个滤波器核卷积，以产生层![ $$ \left(L+1\right) $$ ](A448418_1_En_3_Chapter_IEq67.gif)中的输出特征图 B。

输出特征图的值是卷积的结果，可以表示为![ $$ {s}_{ij}\kern0.5em \forall i,j\in \left\{1,2\right\}: $$ ](A448418_1_En_3_Chapter_IEq68.gif)

![ $$ {\displaystyle \begin{array}{l}{s}_{11}={w_{22}}^{\ast }{a}_{11}+{w_{21}}^{\ast }{a}_{12}+{w_{12}}^{\ast }{a}_{21}+{w_{11}}^{\ast }{a}_{22}\\ {}{s}_{12}={w_{22}}^{\ast }{a}_{12}+{w_{21}}^{\ast }{a}_{13}+{w_{12}}^{\ast }{a}_{22}+{w_{11}}^{\ast }{a}_{23}\\ {}{s}_{21}={w_{22}}^{\ast }{a}_{21}+{w_{21}}^{\ast }{a}_{22}+{w_{12}}^{\ast }{a}_{31}+{w_{11}}^{\ast }{a}_{32}\\ {}{s}_{22}={w_{22}}^{\ast }{a}_{22}+{w_{23}}^{\ast }{a}_{22}+{w_{12}}^{\ast }{a}_{32}+{w_{11}}^{\ast }{a}_{33}\end{array}} $$ ](A448418_1_En_3_Chapter_Equz.gif)

概括地说:

![ $$ {s}_{ij}=\sum \limits_{n=1}^2\sum \limits_{m=1}^2{w_{\left(3-m\right)\left(3-n\right)}}^{\ast }{a}_{\left(i-1+m\right)\left(j-1+n\right)} $$ ](A448418_1_En_3_Chapter_Equaa.gif)

现在，让成本函数 L 相对于净输入 s <sub>ij</sub> 的梯度由

![ $$ \frac{\partial L}{\partial {s}_{ij}}={\delta}_{ij} $$ ](A448418_1_En_3_Chapter_Equab.gif)

表示

让我们计算成本函数相对于权重 w <sub>22</sub> 的梯度。权重与所有的 s <sub>ij</sub> 相关联，因此将具有来自所有δ<sub>ij</sub>:

![ $$ \frac{\partial \mathrm{L}}{\partial {w}_{22}}=\sum \limits_{j=1}^2\sum \limits_{i=1}^2\frac{\partial \mathrm{L}}{\partial {s}_{ij}}\frac{\partial {s}_{ij}}{\partial {w}_{22}} $$ ](A448418_1_En_3_Chapter_Equac.gif)

![ $$ =\sum \limits_{j=1}^2\sum \limits_{i=1}^2{\delta}_{ij}\frac{\partial {s}_{ij}}{\partial {w}_{22}} $$ ](A448418_1_En_3_Chapter_Equad.gif)

的梯度分量

此外，从不同 s <sub>ij</sub> 的前述等式中，可以导出以下等式:

![ $$ \frac{\partial {s}_{11}}{\partial {w}_{22}}={a}_{11},\kern0.5em \frac{\partial {s}_{12}}{\partial {w}_{22}}={a}_{12},\kern0.5em \frac{\partial {s}_{13}}{\partial {w}_{22}}={a}_{21},\kern0.5em \frac{\partial {s}_{14}}{\partial {w}_{22}}={a}_{22} $$ ](A448418_1_En_3_Chapter_Equae.gif)

于是，

![ $$ \frac{\partial \mathrm{L}}{\partial {w}_{22}}={\delta_{11}}^{\ast }{a}_{11}+{\delta_{12}}^{\ast }{a}_{12}+{\delta_{21}}^{\ast }{a}_{21}+{\delta_{22}}^{\ast }{a}_{22} $$ ](A448418_1_En_3_Chapter_Equaf.gif)

同样，

![ $$ \frac{\partial \mathrm{L}}{\partial {w}_{21}}=\sum \limits_{j=1}^2\sum \limits_{i=1}^2\frac{\partial \mathrm{L}}{\partial {s}_{ij}}\frac{\partial {s}_{ij}}{\partial {w}_{21}} $$ ](A448418_1_En_3_Chapter_Equag.gif)

![ $$ =\sum \limits_{j=1}^2\sum \limits_{i=1}^2{\delta}_{ij}\frac{\partial {s}_{ij}}{\partial {w}_{21}} $$ ](A448418_1_En_3_Chapter_Equah.gif)

再次，![ $$ \frac{\partial {s}_{11}}{\partial {w}_{21}}={a}_{12} $$ ](A448418_1_En_3_Chapter_IEq69.gif)，![ $$ \frac{\partial {s}_{12}}{\partial {w}_{21}}={a}_{13} $$ ](A448418_1_En_3_Chapter_IEq70.gif)，![ $$ \frac{\partial {s}_{21}}{\partial {w}_{21}}={a}_{22} $$ ](A448418_1_En_3_Chapter_IEq71.gif)，![ $$ \frac{\partial {s}_{22}}{\partial {w}_{21}}={a}_{23} $$ ](A448418_1_En_3_Chapter_IEq72.gif)

于是，

![ $$ \frac{\partial \mathrm{L}}{\partial {w}_{21}}=\kern0.5em {\delta_{11}}^{\ast }{a}_{12}+{\delta_{12}}^{\ast }{a}_{13}+{\delta_{21}}^{\ast }{a}_{22}+{\delta_{22}}^{\ast }{a}_{23} $$ ](A448418_1_En_3_Chapter_Equai.gif)

用同样的方法对另外两个权重进行处理，我们得到

![ $$ \frac{\partial \mathrm{L}}{\partial {w}_{11}}=\sum \limits_{j=1}^2\sum \limits_{i=1}^2\frac{\partial \mathrm{L}}{\partial {s}_{ij}}\frac{\partial {s}_{ij}}{\partial {w}_{11}} $$ ](A448418_1_En_3_Chapter_Equaj.gif)

![ $$ =\sum \limits_{j=1}^2\sum \limits_{i=1}^2{\delta}_{ij}\frac{\partial {s}_{ij}}{\partial {w}_{11}} $$ ](A448418_1_En_3_Chapter_Equak.gif)

![ $$ \frac{\partial {s}_{11}}{\partial {w}_{11}}={a}_{22},\kern0.5em \frac{\partial {s}_{12}}{\partial {w}_{11}}={a}_{23},\kern0.5em \frac{\partial {s}_{21}}{\partial {w}_{11}}={a}_{32},\kern0.5em \frac{\partial {s}_{22}}{\partial {w}_{21}}={a}_{33} $$ ](A448418_1_En_3_Chapter_Equal.gif)

![ $$ \frac{\partial \mathrm{L}}{\partial {w}_{11}}=\kern0.5em {\delta_{11}}^{\ast }{a}_{22}+{\delta_{12}}^{\ast }{a}_{23}+{\delta_{21}}^{\ast }{a}_{32}+{\delta_{22}}^{\ast }{a}_{33} $$ ](A448418_1_En_3_Chapter_Equam.gif)

![ $$ \frac{\partial \mathrm{L}}{\partial {w}_{12}}=\sum \limits_{j=1}^2\sum \limits_{i=1}^2\frac{\partial \mathrm{L}}{\partial {s}_{ij}}\frac{\partial {s}_{ij}}{\partial {w}_{12}} $$ ](A448418_1_En_3_Chapter_Equan.gif)

![ $$ \frac{\partial {s}_{11}}{\partial {w}_{12}}={a}_{21},\kern0.5em \frac{\partial {s}_{12}}{\partial {w}_{12}}={a}_{22},\kern0.5em \frac{\partial {s}_{21}}{\partial {w}_{12}}={a}_{31},\kern0.5em \frac{\partial {s}_{22}}{\partial {w}_{22}}={a}_{32} $$ ](A448418_1_En_3_Chapter_Equao.gif)

![ $$ \frac{\partial \mathrm{L}}{\partial {w}_{12}}=\kern0.5em {\delta_{11}}^{\ast }{a}_{21}+{\delta_{12}}^{\ast }{a}_{22}+{\delta_{21}}^{\ast }{a}_{31}+{\delta_{22}}^{\ast }{a}_{32} $$ ](A448418_1_En_3_Chapter_Equap.gif)

基于成本函数 L 相对于滤波器核的四个权重的先前梯度，我们得到以下关系:

![ $$ \frac{\partial \mathrm{L}}{\partial {w}_{ij}}=\sum \limits_{n=1}^2\sum \limits_{m=1}^2{\delta_{mn}}^{\ast }{a}_{\left(i-1+m\right)\left(j-1+n\right)} $$ ](A448418_1_En_3_Chapter_Equaq.gif)

当以矩阵形式排列时，我们得到下面的关系；(x)表示互相关:

![ $$ \left[\begin{array}{cc}\frac{\partial \mathrm{L}}{\partial {w}_{22}}& \frac{\partial \mathrm{L}}{\partial {w}_{21}}\\ {}\frac{\partial \mathrm{L}}{\partial {w}_{12}}& \frac{\partial \mathrm{L}}{\partial {w}_{11}}\end{array}\right]=\left[\begin{array}{ccc}{a}_{11}& {a}_{12}& {a}_{13}\\ {}{a}_{21}& {a}_{22}& {a}_{23}\\ {}{a}_{31}& {a}_{32}& {a}_{33}\end{array}\right]\left(\mathrm{x}\right)\left[\begin{array}{cc}{\delta}_{11}& {\delta}_{12}\\ {}{\delta}_{21}& {\delta}_{22}\end{array}\right] $$ ](A448418_1_En_3_Chapter_Equar.gif)

![ $$ \left[\begin{array}{ccc}{a}_{11}& {a}_{12}& {a}_{13}\\ {}{a}_{21}& {a}_{22}& {a}_{23}\\ {}{a}_{31}& {a}_{32}& {a}_{33}\end{array}\right] $$ ](A448418_1_En_3_Chapter_IEq73.gif)与![ $$ \left[\begin{array}{cc}{\delta}_{11}& {\delta}_{12}\\ {}{\delta}_{21}& {\delta}_{22}\end{array}\right] $$ ](A448418_1_En_3_Chapter_IEq74.gif)的互相关也可以认为是![ $$ \left[\begin{array}{ccc}{a}_{11}& {a}_{12}& {a}_{13}\\ {}{a}_{21}& {a}_{22}& {a}_{23}\\ {}{a}_{31}& {a}_{32}& {a}_{33}\end{array}\right] $$ ](A448418_1_En_3_Chapter_IEq75.gif)与翻转![ $$ \left[\begin{array}{cc}{\delta}_{11}& {\delta}_{12}\\ {}{\delta}_{21}& {\delta}_{22}\end{array}\right] $$ ](A448418_1_En_3_Chapter_IEq76.gif)的卷积；亦即![ $$ \left[\begin{array}{cc}{\delta}_{22}& {\delta}_{21}\\ {}{\delta}_{12}& {\delta}_{11}\end{array}\right] $$ ](A448418_1_En_3_Chapter_IEq77.gif)。

因此，梯度矩阵的翻转是![ $$ \left[\begin{array}{ccc}{a}_{11}& {a}_{12}& {a}_{13}\\ {}{a}_{21}& {a}_{22}& {a}_{23}\\ {}{a}_{31}& {a}_{32}& {a}_{33}\end{array}\right] $$ ](A448418_1_En_3_Chapter_IEq78.gif)与![ $$ \left[\begin{array}{cc}{\delta}_{22}& {\delta}_{21}\\ {}{\delta}_{12}& {\delta}_{11}\end{array}\right] $$ ](A448418_1_En_3_Chapter_IEq79.gif)的卷积；即

![ $$ \left[\begin{array}{cc}\frac{\partial \mathrm{L}}{\partial {w}_{22}}& \frac{\partial \mathrm{L}}{\partial {w}_{21}}\\ {}\frac{\partial \mathrm{L}}{\partial {w}_{12}}& \frac{\partial \mathrm{L}}{\partial {w}_{11}}\end{array}\right]=\left[\begin{array}{ccc}{a}_{11}& {a}_{12}& {a}_{13}\\ {}{a}_{21}& {a}_{22}& {a}_{23}\\ {}{a}_{31}& {a}_{32}& {a}_{33}\end{array}\right]\left({}^{\ast}\right)\left[\begin{array}{cc}{\delta}_{22}& {\delta}_{12}\\ {}{\delta}_{21}& {\delta}_{11}\end{array}\right] $$ ](A448418_1_En_3_Chapter_Equas.gif)

就层而言，可以说梯度矩阵的翻转是![ $$ \left(L+1\right) $$ ](A448418_1_En_3_Chapter_IEq80.gif)层的梯度与层 l 的特征图输出的互相关。同样，等效地，梯度矩阵的翻转是![ $$ \left(L+1\right) $$ ](A448418_1_En_3_Chapter_IEq81.gif)层的梯度矩阵翻转与层 l 的特征图输出的卷积

## 通过池层反向传播

![A448418_1_En_3_Fig29_HTML.gif](A448418_1_En_3_Fig29_HTML.gif)

图 3-22。

Backpropagation through max pooling layer

图 [3-22](#Fig29) 说明了最大汇集操作。让一个特征映射，在经过层 L 的卷积和 ReLU 激活之后，经过层![ $$ \Big(L+1 $$ ](A448418_1_En_3_Chapter_IEq82.gif)的最大池操作，以产生输出特征映射。最大池的核心或感受域的大小为![ $$ 2\times 2 $$ ](A448418_1_En_3_Chapter_IEq83.gif)，步长为 2。最大池层的输出是![ $$ \frac{1}{4} $$ ](A448418_1_En_3_Chapter_IEq84.gif)输入特征图的大小，其输出值由![ $$ {z}_{ij},\forall i,j\in \left\{1,2\right\}. $$ ](A448418_1_En_3_Chapter_IEq85.gif)表示

我们可以看到 z <sub>11</sub> 的值是 5，因为![ $$ 2\times 2 $$ ](A448418_1_En_3_Chapter_IEq86.gif)块中的最大值是 5。如果 z <sub>11</sub> 处的误差导数是![ $$ \frac{\partial C}{\partial {z}_{ij}} $$ ](A448418_1_En_3_Chapter_IEq87.gif)，那么整个梯度被传递到值为 5 的 x <sub>21</sub> ，并且其块中的其余元素——x<sub>11</sub>，x <sub>12</sub> 和 x<sub>22</sub>——从 z <sub>11</sub> 接收零梯度。

![A448418_1_En_3_Fig30_HTML.gif](A448418_1_En_3_Fig30_HTML.gif)

图 3-23。

Backpropagation through average pooling layer

对于同一个示例使用平均池，输出是输入的![ $$ 2\times 2 $$ ](A448418_1_En_3_Chapter_IEq88.gif)块中值的平均值。因此，z <sub>11</sub> 得到 x <sub>11</sub> ，x <sub>12</sub> ，x <sub>21</sub> ，x <sub>22</sub> 的平均值。这里，z <sub>11</sub> 处的误差梯度![ $$ \frac{\partial C}{\partial {z}_{11}} $$ ](A448418_1_En_3_Chapter_IEq89.gif)将由 x <sub>11</sub> ，x <sub>12</sub> ，x <sub>21</sub> 和 x <sub>22</sub> 平均分担。于是，

![ $$ \frac{\partial C}{\partial {x}_{11}}=\frac{\partial C}{\partial {x}_{12}}=\frac{\partial C}{\partial {x}_{21}}=\frac{\partial C}{\partial {x}_{22}}=\frac{1}{4}\frac{\partial C}{\partial {z}_{11}} $$ ](A448418_1_En_3_Chapter_Equat.gif)

## 卷积权重分配及其优势

通过卷积的权重共享极大地减少了卷积神经网络中的参数数量。想象一下，我们从一个大小为![ $$ n\times n $$ ](A448418_1_En_3_Chapter_IEq91.gif)的图像中创建了一个大小为![ $$ k\times k $$ ](A448418_1_En_3_Chapter_IEq90.gif)的特征图，它具有完整的连接而不是卷积。仅对于那一个特征地图，就有 k 个 <sup>2</sup> n 个 <sup>2</sup> 权重，这是要学习的大量权重。相反，因为在卷积中，由滤波器核大小定义的位置共享相同的权重，所以要学习的参数数量大大减少。在卷积的情况下，就像在这个场景中，我们只需要学习特定滤波器核的权重。由于滤波器尺寸相对于图像相对较小，因此权重的数量显著减少。对于任何图像，我们生成对应于不同滤波器核的若干特征图。每个过滤器内核学习检测不同种类的特征。创建的特征映射再次与其他滤波器核卷积，以学习后续层中更复杂的特征。

## 翻译等值

卷积运算提供平移等变。也就是说，如果输入中的特征 A 在输出中产生特定的特征 B，那么即使特征 A 在图像中被平移，特征 B 也将继续在输出的不同位置产生。

![A448418_1_En_3_Fig31_HTML.gif](A448418_1_En_3_Fig31_HTML.gif)

图 3-24。

Translational equivariance illustration

在图 [3-24](#Fig31) 中，我们可以看到数字 9 在图像(B)中已经从它在图像(A)中的位置被平移。输入图像(A)和(B)都已经用相同的滤波器核进行了卷积，并且在输出图像(C)和(D)中基于其在输入中的位置在不同的位置检测到了数字 9 的相同特征。不管翻译如何，卷积仍然为数字产生相同的特征。卷积的这个性质叫做平移等方差。事实上，如果数字用一组像素强度 x 来表示，f 是对 x 的平移运算，而 g 是带滤波核的卷积运算，那么以下对卷积成立:

![ $$ g\left(f(x)\right)=f\left(g(x)\right) $$ ](A448418_1_En_3_Chapter_Equau.gif)

在我们的例子中，f (x)产生图像(B)中的平移 9，平移 9 通过 g 卷积以产生 9 的激活特征，如图像(D)所示。图像(D)中的 9(即 g( f (x)))的激活特征也可以通过相同的平移 f 平移图像(C)中的激活 9(即 g(x))来实现

![A448418_1_En_3_Fig32_HTML.gif](A448418_1_En_3_Fig32_HTML.gif)

图 3-25。

Illustration of equivariance with an example

用一个小例子更容易看出等方差，如图 [3-25](#Fig32) 所示。我们感兴趣的输入图像或 2D 信号的部分是左边最上面的块；亦即![ $$ \left[\begin{array}{ccc}44& 47& 64\\ {}9& 83& 21\\ {}70& 88& 88\end{array}\right] $$ ](A448418_1_En_3_Chapter_IEq92.gif)。为了便于参考，我们将该块命名为 a。

在将输入与求和滤波器(即![ $$ \left[\begin{array}{ccc}1& 1& 1\\ {}1& 1& 1\\ {}1& 1& 1\end{array}\right] $$ ](A448418_1_En_3_Chapter_IEq93.gif))进行卷积时，模块 A 将对应于输出值 183，该值可被视为 A 的特征检测器。

在将相同的求和滤波器与平移图像进行卷积时，移位后的块 A 仍将产生 183 的输出值。此外，我们可以看到，如果我们将相同的转换应用于原始的卷积图像输出，183 的值将出现在与转换后的卷积图像的输出相同的位置。

## 池化带来的平移不变性

汇集基于汇集的受体场内核大小提供了某种形式的平移不变性。让我们以最大池为例，如图 [3-26](#Fig33) 所示。图像 A 中特定位置处的数字通过卷积滤波器 H 以输出特征图 P 中的值 100 和 80 的形式被检测到。同样，相同的数字出现在另一个图像 B 中相对于图像 A 稍微平移的位置处。当这些特征映射由于最大汇集而以步幅 2 通过大小为![ $$ 2\times 2 $$ ](A448418_1_En_3_Chapter_IEq94.gif)的受体域核时，100 和 80 值出现在输出 M 和 M’的相同位置。以这种方式，如果平移距离相对于最大汇集的受体区域或核心的大小不是很高，则最大汇集为特征检测提供了一些平移不变性。

![A448418_1_En_3_Fig33_HTML.gif](A448418_1_En_3_Fig33_HTML.gif)

图 3-26。

Translational invariance through max pooling

类似地，平均池基于受体域内核的大小，取特征映射的局部中的值的平均值。因此，如果某个特定特征在其局部特征图中被高值检测到，比如说在边缘区域，那么即使图像有一点平移，平均值也会继续很高。

## 辍学和正规化

丢弃是调整卷积神经网络的全连接层中的权重以避免过度拟合的活动。然而，它不限于卷积神经网络，而是适用于所有前馈神经网络。对于小批量的每个训练样本，在训练时随机丢弃指定比例的神经网络单元，包括隐藏的和可见的，以便剩余的神经元可以自己学习重要的特征，而不依赖于其他神经元的合作。当神经元被随机丢弃时，到这些神经元的所有传入和传出连接也被丢弃。神经元之间过多的合作使得神经元相互依赖，它们无法学习不同的特征。这种高度合作导致过度拟合，因为它在训练数据集上表现良好，而如果测试数据集与训练数据集有些不同，则测试数据集上的预测会变得混乱。

当神经元单元被随机丢弃时，剩余可用神经元的每个这样的设置产生不同的网络。假设我们有一个有 N 个神经单元的网络；可能的神经网络配置的可能数量是 N <sup>2</sup> 。对于小批量中的每个训练样本，基于退出概率随机选择一组不同的神经元。因此，训练一个辍学的神经网络相当于训练一组不同的神经网络，其中每个网络很少得到训练，如果有的话。

正如我们所猜测的，对许多不同模型的预测进行平均会减少总体模型的方差并减少过度拟合，因此我们通常会获得更好、更稳定的预测。

对于两类问题，在两个不同的模型 M <sub>1</sub> 和 M <sub>2</sub> 上训练，如果一个数据点的类概率对于模型 M <sub>1</sub> 是 p <sub>11</sub> 和 p <sub>12</sub> ，对于模型 M <sub>2</sub> 是 p <sub>21</sub> 和 p <sub>22</sub> ，那么我们取集合模型 M <sub>1</sub> 的平均概率集合模型对于类别 1 的概率为![ $$ \frac{\left({p}_{11}+{p}_{21}\right)}{2} $$ ](A448418_1_En_3_Chapter_IEq95.gif)，对于类别 2 的概率为![ $$ \frac{\left({p}_{12}+{p}_{22}\right)}{2} $$ ](A448418_1_En_3_Chapter_IEq96.gif)。

另一种平均方法是取不同模型预测值的几何平均值。在这种情况下，我们需要对几何平均值进行归一化，以使新概率的总和为 1。

先前示例的集合模型的新概率将分别是![ $$ \frac{\sqrt{p_{11}\times {p}_{21}}}{\sqrt{p_{11}\times {p}_{21}}+\sqrt{p_{12}\times {p}_{22}}} $$ ](A448418_1_En_3_Chapter_IEq97.gif)和![ $$ \frac{\sqrt{p_{12}\times {p}_{22}}}{\sqrt{p_{11}\times {p}_{21}}+\sqrt{p_{12}\times {p}_{22}}} $$ ](A448418_1_En_3_Chapter_IEq98.gif)。

在测试时，不可能从所有这些可能的网络中计算出预测值，然后进行平均。相反，使用了具有所有权重和连接的单一神经网络——但是进行了权重调整。如果在训练期间以概率 p 保留神经网络单元，则通过将权重乘以概率 p 来缩小该单元的输出权重。通常，这种对测试数据集进行预测的近似效果很好。可以证明，对于具有 SoftMax 输出层的模型，前面的排列相当于从那些由漏失产生的单个模型中取出预测，然后计算它们的几何平均值。

在图 [3-27](#Fig34) 中，展示了一个随机丢弃了三个单元的神经网络。正如我们所看到的，被丢弃单元的所有输入和输出连接也被丢弃。

![A448418_1_En_3_Fig34_HTML.gif](A448418_1_En_3_Fig34_HTML.gif)

图 3-27。

Neural network with three units dropped randomly

对于卷积神经网络，全连接层中的单元及其相应的传入和传出连接通常被丢弃。因此，在预测测试数据集时，不同的过滤器核权重不需要任何调整。

## 卷积神经网络在 MNIST 数据集上的数字识别

现在，我们已经完成了卷积神经网络的基本构建模块，让我们看看 CNN 在学习分类 MNIST 数据集方面有多好。TensorFlow 中基本实现的详细逻辑记录在清单 [3-6](#Par160) 中。CNN 接收对应于 RGB 通道的高度 28、宽度 28 和深度 3 的图像。图像经过一系列卷积、ReLU 激活和最大池操作两次，然后被馈送到完全连接的层，最后到达输出层。第一卷积层产生 64 个特征图，第二卷积层提供 128 个特征图，全连通层有 1024 个单元。选择最大池图层是为了通过![ $$ \frac{1}{4} $$ ](A448418_1_En_3_Chapter_IEq99.gif)减少要素地图的大小。特征地图可以被认为是 2D 图像。

![A448418_1_En_3_Fig35_HTML.gif](A448418_1_En_3_Fig35_HTML.gif)

图 3-28。

Predicted digits versus actual digits from CNN model

```
##########################################################
##Import the required libraries and read the MNIST dataset
##########################################################
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from tensorflow.examples.tutorials.mnist import input_data
import time
mnist = input_data.read_data_sets("MNIST_data/",one_hot=True)

###########################################
## Set the value of the Parameters
###########################################

learning_rate = 0.01
epochs = 20
batch_size = 256
num_batches = mnist.train.num_examples/batch_size
input_height = 28
input_width = 28
n_classes = 10
dropout = 0.75
display_step = 1
filter_height = 5
filter_width = 5
depth_in = 1
depth_out1 = 64
depth_out2 = 128

###########################################
# input output definition
###########################################
x = tf.placeholder(tf.float32,[None,28*28])
y = tf.placeholder(tf.float32,[None,n_classes])
keep_prob = tf.placeholder(tf.float32)
###########################################
## Store the weights
## Number of weights of filters to be learnt in 'wc1' => filter_height*filter_width*depth_in*depth_out1
## Number of weights of filters to be learnt in 'wc1' => filter_height*filter_width*depth_out1*depth_out2
## No of Connections to the fully Connected layer => Each maxpooling operation reduces the image size to 1/4.
## So two maxpooling reduces the imase size to /16\. There are depth_out2 number of images each of size 1/16 ## of the original image size of input_height*input_width. So there is total of
## (1/16)*input_height* input_width* depth_out2 pixel outputs which when connected to the fully connected layer ## with 1024 units would provide (1/16)*input_height* input_width* depth_out2*1024 connections.
###########################################
weights = {
'wc1' : tf.Variable(tf.random_normal([filter_height,filter_width,depth_in,depth_out1])),
'wc2' : tf.Variable(tf.random_normal([filter_height,filter_width,depth_out1,depth_out2])),
'wd1' : tf.Variable(tf.random_normal([(input_height/4)*(input_height/4)* depth_out2,1024])),
'out' : tf.Variable(tf.random_normal([1024,n_classes]))
}
#################################################
## In the 1st Convolutional Layer there are 64 feature maps and that corresponds to 64 biases in 'bc1'
## In the 2nd Convolutional Layer there are 128 feature maps and that corresponds to 128 biases in 'bc2'
## In the Fully Connected Layer there are 1024units and that corresponds to 1024 biases in 'bd1'
## In the output layet there are 10 classes for the Softmax and that corresponds to 10 biases in 'out'
#################################################
biases = {
'bc1' : tf.Variable(tf.random_normal([64])),
'bc2' : tf.Variable(tf.random_normal([128])),
'bd1' : tf.Variable(tf.random_normal([1024])),
'out' : tf.Variable(tf.random_normal([n_classes]))
}

##################################################
## Create the different layers
##################################################

'''C O N V O L U T I O N L A Y E R'''
def conv2d(x,W,b,strides=1):
    x = tf.nn.conv2d(x,W,strides=[1,strides,strides,1],padding='SAME')
    x = tf.nn.bias_add(x,b)
    return tf.nn.relu(x)

''' P O O L I N G L A Y E R'''
def maxpool2d(x,stride=2):
    return tf.nn.max_pool(x,ksize=[1,stride,stride,1],strides=[1,stride,stride,1],padding='SAME')
##################################################
## Create the feed forward model
##################################################
def conv_net(x,weights,biases,dropout):
##################################################
## Reshape the input in the 4 dimensional image
## 1st dimension - image index
## 2nd dimension - height
## 3rd dimension - width
## 4th dimension - depth
    x = tf.reshape(x,shape=[-1,28,28,1])
##################################################
## Convolutional layer 1
    conv1 = conv2d(x,weights['wc1'],biases['bc1'])
    conv1 = maxpool2d(conv1,2)
## Convolutional layer 2
    conv2 = conv2d(conv1,weights['wc2'],biases['bc2'])
    conv2 = maxpool2d(conv2,2)
## Now comes the fully connected layer
    fc1 = tf.reshape(conv2,[-1,weights['wd1'].get_shape().as_list()[0]])
    fc1 = tf.add(tf.matmul(fc1,weights['wd1']),biases['bd1'])
    fc1 = tf.nn.relu(fc1)
## Apply Dropout
    fc1 = tf.nn.dropout(fc1,dropout)
## Output class prediction
    out = tf.add(tf.matmul(fc1,weights['out']),biases['out'])
    return out

#######################################################
# Defining the tensorflow Ops for different activities
#######################################################
pred = conv_net(x,weights,biases,keep_prob)
# Define loss function and optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)
# Evaluate model
correct_pred = tf.equal(tf.argmax(pred,1),tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))
## initializing all variables
init = tf.global_variables_initializer()
####################################################
## Launch the execution Graph
####################################################
start_time = time.time()
with tf.Session() as sess:
    sess.run(init)
    for i in range(epochs):
        for j in range(num_batches):

            batch_x,batch_y = mnist.train.next_batch(batch_size)
            sess.run(optimizer, feed_dict={x:batch_x,y:batch_y,keep_prob:dropout})
            loss,acc = sess.run([cost,accuracy],feed_dict={x:batch_x,y:batch_y,keep_prob: 1.})
            if epochs % display_step == 0:
                print("Epoch:", '%04d' % (i+1),
                "cost=", "{:.9f}".format(loss),
                "Training accuracy","{:.5f}".format(acc))
    print('Optimization Completed')

    y1 = sess.run(pred,feed_dict={x:mnist.test.images[:256],keep_prob: 1})
    test_classes = np.argmax(y1,1)
    print('Testing Accuracy:',sess.run(accuracy,feed_dict={x:mnist.test.images[:256],y:mnist.test.labels[:256],keep_prob: 1}))
    f, a = plt.subplots(1, 10, figsize=(10, 2))

    for i in range(10):
        a[i].imshow(np.reshape(mnist.test.images[i],(28, 28)))
        print test_classes[i]

end_time = time.time()
print('Total processing time:',end_time - start_time)

Listing 3-6.

```

前面的基本卷积神经网络包括两个卷积–最大池–ReLU 对以及最终输出 SoftMax 单元之前的全连接层，我们可以在短短 20 个时期内实现 0.9765625 的测试集精度。正如我们之前在第 [2](2.html) 章中通过多层感知器方法看到的，使用这种方法，我们只能在 1000 个时期内获得大约 91%的准确度。这证明了对于图像识别问题，卷积神经网络工作得最好。

我想强调的另一件事是，使用正确的超参数集和先验信息调整模型的重要性。诸如学习率选择的参数可能非常棘手，因为神经网络的成本函数通常是非凸的。大的学习率可以导致更快地收敛到局部最小值，但是可能引入振荡，而低的学习率将导致非常慢的收敛。理想情况下，学习率应该足够低，以使网络参数能够收敛到有意义的局部最小值，同时它应该足够高，以使模型能够更快地达到最小值。通常，对于前面的神经网络，0.01 的学习率有点偏高，但是因为我们只训练 20 个时期的数据，所以它工作得很好。较低的学习率不会在仅仅 20 个历元的情况下达到如此高的准确度。类似地，为随机梯度下降的小批量版本选择的批量大小影响训练过程的收敛。批量越大越好，因为梯度估计的噪声越小；然而，这可能以增加计算量为代价。人们还需要尝试不同的过滤器大小，以及在每个卷积层中试验不同数量的特征图。我们选择的模型体系结构是网络的先验知识。

## 用于解决现实世界问题的卷积神经网络

现在，我们将简要讨论如何解决现实世界中的图像分析问题，方法是研究最近由英特尔在 Kaggle 主办的一个问题，该问题涉及对不同类型的宫颈癌进行分类。在这场比赛中，需要建立一个模型，根据图像识别女性的子宫颈类型。这样做将允许对患者进行有效治疗。为竞赛提供了三种癌症的特定图像。因此，商业问题归结为一个三级图像分类问题。清单 [3-7](#Par164) 中提供了解决该问题的基本方法。

```
########################################################
## Load the relevant libraries
########################################################
from PIL import ImageFilter, ImageStat, Image, ImageDraw
from multiprocessing import Pool, cpu_count
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np
import glob
import cv2
import time
from keras.utils import np_utils
import os
import tensorflow as tf
import shuffle

##########################################################
## Read the input images and then resize the image to 64 x 64 x 3 size
###########################################################
def get_im_cv2(path):
    img = cv2.imread(path)
    resized = cv2.resize(img, (64,64), cv2.INTER_LINEAR)
    return resized

###########################################################
## Each of the folders corresponds to a different class
## Load the images into array and then define their output classes based on
## the folder number

###########################################################

def load_train():
    X_train = []
    X_train_id = []
    y_train = []
    start_time = time.time()

    print('Read train images')
    folders = ['Type_1', 'Type_2', 'Type_3']
    for fld in folders:
        index = folders.index(fld)
        print('Load folder {} (Index: {})'.format(fld, index))
        path = os.path.join('.', 'Downloads', 'Intel','train', fld, '*.jpg')
        files = glob.glob(path)

        for fl in files:
            flbase = os.path.basename(fl)
            img = get_im_cv2(fl)
            X_train.append(img)
            X_train_id.append(flbase)
            y_train.append(index)

    for fld in folders:
        index = folders.index(fld)
        print('Load folder {} (Index: {})'.format(fld, index))
        path = os.path.join('.', 'Downloads', 'Intel','Additional', fld, '*.jpg')
        files = glob.glob(path)

        for fl in files:
            flbase = os.path.basename(fl)
            img = get_im_cv2(fl)
            X_train.append(img)
            X_train_id.append(flbase)
            y_train.append(index)

    print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))
    return X_train, y_train, X_train_id

###################################################################
## Load the test images
###################################################################

def load_test():
    path = os.path.join('.', 'Downloads', 'Intel','test', '*.jpg')
    files = sorted(glob.glob(path))

    X_test = []
    X_test_id = []
    for fl in files:
        flbase = os.path.basename(fl)
        img = get_im_cv2(fl)
        X_test.append(img)
        X_test_id.append(flbase)
    path = os.path.join('.', 'Downloads', 'Intel','test_stg2', '*.jpg')
    files = sorted(glob.glob(path))
    for fl in files:
        flbase = os.path.basename(fl)
        img = get_im_cv2(fl)
        X_test.append(img)
        X_test_id.append(flbase)

    return X_test, X_test_id

##################################################
## Normalize the image data to have values between 0 and 1
## by diving the pixel intensity values by 255.
## Also convert the class label into vectors of length 3 corresponding to
## the 3 classes
## Class 1 - [1 0 0]
## Class 2 - [0 1 0]
## Class 3 - [0 0 1]
##################################################
def read_and_normalize_train_data():
    train_data, train_target, train_id = load_train()

    print('Convert to numpy...')
    train_data = np.array(train_data, dtype=np.uint8)
    train_target = np.array(train_target, dtype=np.uint8)

    print('Reshape...')

    train_data = train_data.transpose((0, 2,3, 1))
    train_data = train_data.transpose((0, 1,3, 2))

    print('Convert to float...')
    train_data = train_data.astype('float32')
    train_data = train_data / 255
    train_target = np_utils.to_categorical(train_target, 3)

    print('Train shape:', train_data.shape)
    print(train_data.shape[0], 'train samples')
    return train_data, train_target, train_id

###############################################################
## Normalize test-image data
###############################################################

def read_and_normalize_test_data():
    start_time = time.time()
    test_data, test_id = load_test()

    test_data = np.array(test_data, dtype=np.uint8)
    test_data = test_data.transpose((0,2,3,1))
    train_data = test_data.transpose((0, 1,3, 2))

    test_data = test_data.astype('float32')
    test_data = test_data / 255

    print('Test shape:', test_data.shape)
    print(test_data.shape[0], 'test samples')
    print('Read and process test data time: {} seconds'.format(round(time.time() - start_time, 2)))
    return test_data, test_id

##########################################################
## Read and normalize the train data

##########################################################

train_data, train_target, train_id = read_and_normalize_train_data()

##########################################################
## Shuffle the input training data to aid stochastic gradient descent
##########################################################

list1_shuf = []
list2_shuf = []
index_shuf = range(len(train_data))
shuffle(index_shuf)
for i in index_shuf:
    list1_shuf.append(train_data[i,:,:,:])
    list2_shuf.append(train_target[i,])
list1_shuf = np.array(list1_shuf,dtype=np.uint8)
list2_shuf = np.array(list2_shuf,dtype=np.uint8)

##########################################################
## TensorFlow activities for Network Definition and Training
##########################################################
## Create  the different layers

channel_in = 3
channel_out = 64
channel_out1 = 128

'''C O N V O L U T I O N    L A Y E R'''
def conv2d(x,W,b,strides=1):
    x = tf.nn.conv2d(x,W,strides=[1,strides,strides,1],padding='SAME')
    x = tf.nn.bias_add(x,b)

    return tf.nn.relu(x)

''' P O O L I N G  L A Y E R'''
def maxpool2d(x,stride=2):
    return tf.nn.max_pool(x,ksize=[1,stride,stride,1],strides=[1,stride,stride,1],padding='SAME')

## Create the feed-forward model 

def conv_net(x,weights,biases,dropout):

    ## Convolutional layer 1
    conv1 = conv2d(x,weights['wc1'],biases['bc1'])
    conv1 = maxpool2d(conv1,stride=2)
    ## Convolutional layer 2
    conv2a = conv2d(conv1,weights['wc2'],biases['bc2'])
    conv2a = maxpool2d(conv2a,stride=2)
    conv2 = conv2d(conv2a,weights['wc3'],biases['bc3'])
    conv2 = maxpool2d(conv2,stride=2)

    ## Now comes the fully connected layer

    fc1 = tf.reshape(conv2,[-1,weights['wd1'].get_shape().as_list()[0]])
    fc1 = tf.add(tf.matmul(fc1,weights['wd1']),biases['bd1'])
    fc1 = tf.nn.relu(fc1)

    ## Apply Dropout
    fc1 = tf.nn.dropout(fc1,dropout)
    ## Another fully Connected Layer
    fc2 = tf.add(tf.matmul(fc1,weights['wd2']),biases['bd2'])
    fc2 = tf.nn.relu(fc2)
    ## Apply Dropout
    fc2 = tf.nn.dropout(fc2,dropout)

    ## Output class prediction

    out = tf.add(tf.matmul(fc2,weights['out']),biases['out'])
    return out

######################################################
## Define several parameters for the network and learning
#######################################################
start_time = time.time()
learning_rate = 0.01
epochs = 200
batch_size = 128
num_batches = list1_shuf.shape[0]/128
input_height = 64
input_width = 64
n_classes = 3
dropout = 0.5
display_step = 1
filter_height = 3
filter_width = 3
depth_in = 3
depth_out1 = 64
depth_out2 = 128
depth_out3 = 256

#######################################################
# input–output definition
#######################################################

x = tf.placeholder(tf.float32,[None,input_height,input_width,depth_in])
y = tf.placeholder(tf.float32,[None,n_classes])
keep_prob = tf.placeholder(tf.float32)

########################################################
## Define the weights and biases
########################################################

weights = {
    'wc1' : tf.Variable(tf.random_normal([filter_height,filter_width,depth_in,depth_out1])),
    'wc2' : tf.Variable(tf.random_normal([filter_height,filter_width,depth_out1,depth_out2])),
    'wc3' : tf.Variable(tf.random_normal([filter_height,filter_width,depth_out2,depth_out3])),
    'wd1' : tf.Variable(tf.random_normal([(input_height/8)*(input_height/8)*256,512])),
    'wd2' : tf.Variable(tf.random_normal([512,512])),
    'out' : tf.Variable(tf.random_normal([512,n_classes]))

}

biases = {
    'bc1' : tf.Variable(tf.random_normal([64])),
    'bc2' : tf.Variable(tf.random_normal([128])),
    'bc3' : tf.Variable(tf.random_normal([256])),
    'bd1' : tf.Variable(tf.random_normal([512])),
    'bd2' : tf.Variable(tf.random_normal([512])),

    'out' : tf.Variable(tf.random_normal([n_classes]))

}

######################################################
## Define the TensorFlow ops for training
######################################################

pred = conv_net(x,weights,biases,keep_prob)

# Define loss function and optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

# Evaluate model

correct_pred = tf.equal(tf.argmax(pred,1),tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))

## Define the initialization op

init = tf.global_variables_initializer()
######################################################
## Launch the execution graph and invoke the training
######################################################
start_time = time.time()
with tf.Session() as sess:
    sess.run(init)

    for i in range(epochs):

        for j in range(num_batches):

            batch_x,batch_y = list1_shuf[i*(batch_size):(i+1)*(batch_size)],list2_shuf[i*(batch_size):(i+1)*(batch_size)]
            sess.run(optimizer, feed_dict={x:batch_x,y:batch_y,keep_prob:dropout})
            loss,acc = sess.run([cost,accuracy],feed_dict={x:batch_x,y:batch_y,keep_prob: 1.})

        if epochs % display_step == 0:
            print("Epoch:", '%04d' % (i+1),
                  "cost=", "{:.9f}".format(loss),
                  "Training accuracy","{:.5f}".format(acc))

    print('Optimization Completed')

end_time = time.time()
print('Total processing time:',end_time - start_time)

-- output --

('Epoch:', '0045', 'cost=', '0.994687378', 'Training accuracy', '0.53125')
('Epoch:', '0046', 'cost=', '1.003623009', 'Training accuracy', '0.52344')
('Epoch:', '0047', 'cost=', '0.960040927', 'Training accuracy', '0.56250')
('Epoch:', '0048', 'cost=', '0.998520255', 'Training accuracy', '0.54688')
('Epoch:', '0049', 'cost=', '1.016047001', 'Training accuracy', '0.50781')
('Epoch:', '0050', 'cost=', '1.043521643', 'Training accuracy', '0.49219')
('Epoch:', '0051', 'cost=', '0.959320068', 'Training accuracy', '0.58594')
('Epoch:', '0052', 'cost=', '0.935006618', 'Training accuracy', '0.57031')
('Epoch:', '0053', 'cost=', '1.031400681', 'Training accuracy', '0.49219')
('Epoch:', '0054', 'cost=', '1.023633003', 'Training accuracy', '0.50781')
('Epoch:', '0055', 'cost=', '1.007938623', 'Training accuracy', '0.53906')

('Epoch:', '0056', 'cost=', '1.033236384', 'Training accuracy', '0.46094')
('Epoch:', '0057', 'cost=', '0.939492166', 'Training accuracy', '0.60938')
('Epoch:', '0058', 'cost=', '0.986051500', 'Training accuracy', '0.56250')
('Epoch:', '0059', 'cost=', '1.019751549', 'Training accuracy', '0.51562')
('Epoch:', '0060', 'cost=', '0.955037951', 'Training accuracy', '0.57031')
('Epoch:', '0061', 'cost=', '0.963475347', 'Training accuracy', '0.58594')
('Epoch:', '0062', 'cost=', '1.019685864', 'Training accuracy', '0.50000')
('Epoch:', '0063', 'cost=', '0.970604420', 'Training accuracy', '0.53125')
('Epoch:', '0064', 'cost=', '0.962844968', 'Training accuracy', '0.54688')

Listing 3-7.

```

该模型在竞赛排行榜中的对数损失约为 0.97，而该竞赛的最佳模型的对数损失约为 0.78。这是因为该模型是一个基本的实现，没有考虑图像处理中的其他高级概念。我们将在本书的后面研究一种叫做迁移学习的高级技术，这种技术在提供的图像数量较少时效果很好。读者可能感兴趣的关于实现的几点如下:

*   这些图像被读取为三维 Numpy 数组，并通过 OpenCV 调整大小，然后附加到一个列表中。然后，该列表被转换为 Numpy 数组，因此，我们为训练和测试数据集都获得了一个四维 Numpy 数组或张量。训练和测试图像张量已经被转置，以具有按照图像编号、沿着图像高度的位置、沿着图像宽度的位置和图像通道的顺序排列的维度。
*   通过除以像素强度的最大值，图像已经被归一化为具有 0 和 1 之间的值；即 255。这有助于基于梯度的优化。
*   图像被随机打乱，使得小批量具有随机排列的三个类别的图像。
*   网络实现的其余部分类似于 MNIST 分类问题，但在最终的 SoftMax 输出层之前有三层卷积-ReLU-max 池组合和两层完全连接的层。
*   这里省略了涉及预测和提交的代码。

## 批量标准化

批量规范化是由 Sergey Ioffe 和 Christian Szegedy 发明的，是深度学习领域的先驱元素之一。批量归一化的原论文题目为《批量归一化:通过减少内部协变量移位加速深度网络训练》，可位于 [`https://arxiv.org/abs/1502.03167`](https://arxiv.org/abs/1502.03167) 。

当通过随机梯度下降训练神经网络时，由于前面层上权重的更新，每层的输入分布发生变化。这减慢了训练过程，并且使得很难训练非常深的神经网络。神经网络的训练过程很复杂，因为任何层的输入都依赖于所有前面层的参数，因此随着网络的增长，即使很小的参数变化也会产生放大效应。这导致层中输入分布的变化。

现在，让我们试着理解当一层中的激活函数的输入分布由于前一层中的权重改变而改变时，什么可能出错。

sigmoid 或 tanh 激活函数仅在其输入的特定范围内具有良好的线性梯度，一旦输入变大，梯度下降到零。

![A448418_1_En_3_Fig36_HTML.gif](A448418_1_En_3_Fig36_HTML.gif)

图 3-29。

Sigmoid function with its small unsaturated region

前面层中的参数变化可能会改变 sigmoid 单元层的输入概率分布，使得 sigmoid 的大部分输入属于饱和区，从而产生近零梯度，如图 [3-29](#Fig36) 所示。由于这些零或接近零的梯度，学习变得非常缓慢或完全停止。避免这个问题的一个方法是使用校正线性单元(ReLUs)。避免这一问题的另一种方法是在非饱和区内保持 sigmoid 单元的输入分布稳定，以便随机梯度下降不会卡在饱和区内。

这种内部网络单元输入分布的变化现象被批量标准化过程的发明者称为内部协变量移位。

批量归一化通过将图层的输入归一化为零均值和单位标准差来减少内部协变量偏移。训练时，从每层的小批量样本中估计平均值和标准偏差，而在测试预测时，通常使用总体方差和平均值。

如果一个层从前面的层接收输入激活的矢量![ $$ x={\left[{x}_1{x}_2\dots {x}_n\right]}^T\in {\mathrm{\mathbb{R}}}^{n\times 1} $$ ](A448418_1_En_3_Chapter_IEq100.gif)，那么在由 m 个数据点组成的每个小批量中，输入激活被归一化如下:

![ $$ \hat{x_i}=\frac{x_i-E\left[{x}_i\right]}{\sqrt{Var\left[{x}_i\right]+\in }} $$ ](A448418_1_En_3_Chapter_Equav.gif)

其中

![ $$ {u}_B=\frac{1}{m}\sum \limits_{k=1}^m{x_i}^{(k)} $$ ](A448418_1_En_3_Chapter_Equaw.gif)

![ $$ {\upsigma}_B^2=\frac{1}{m}\sum \limits_{k=1}^m{\left({x_i}^{(k)}-E\left[{x}_i\right]\right)}^2 $$ ](A448418_1_En_3_Chapter_Equax.gif)

在统计学上，u <sub>B</sub> 和σ<sub>B</sub>T4】2 不过是样本均值和有偏样本标准差。

一旦完成归一化，![ $$ \hat{x_i} $$ ](A448418_1_En_3_Chapter_IEq101.gif)不直接馈送给激活函数，而是在馈送给激活函数之前，通过引入参数γ和β进行缩放和移位。如果我们将输入激活限制为归一化值，它们可能会改变图层所能表示的内容。因此，我们的想法是通过以下变换对标准化值应用线性变换，这样，如果网络通过训练认为任何变换前的原始值对网络都有好处，它就可以恢复原始值。由

![ $$ {y}_i=\gamma \hat{x_i}+\beta $$ ](A448418_1_En_3_Chapter_Equay.gif)

给出馈入激活函数的实际变换输入激活 y <sub>i</sub>

参数 u <sub>B</sub> 、σ <sub>B</sub> <sup>2</sup> 、γ和β将像其他参数一样通过反向传播来学习。如前所述，如果模型认为来自网络的原始值更理想，它可以学习γ =Var[x <sub>i</sub> 和![ $$ \beta =E\left[{x}_i\right] $$ ](A448418_1_En_3_Chapter_IEq102.gif)。

可能出现的一个非常自然的问题是，为什么我们将小批量平均值 u <sub>B</sub> 和方差σ<sub>B</sub>B<sup>2</sup>作为要通过批量传播学习的参数，而不是出于标准化目的将它们估计为小批量的运行平均值。这不起作用，因为 u <sub>B</sub> 和σ<sub>B</sub>T10】2 通过 x <sub>i</sub> 依赖于模型的其他参数，并且当我们直接将这些参数估计为运行平均值时，在优化过程中不考虑这种依赖性。为了保持这些依赖性不变，u <sub>B</sub> 和σ<sub>B</sub>B<sup>2</sup>应该作为参数参与优化过程，因为 u <sub>B</sub> 和σ<sub>B</sub>B<sup>2</sup>相对于 x <sub>i</sub> 所依赖的其他参数的梯度对于学习过程至关重要。这种优化的总体效果是以输入![ $$ \hat{x_i} $$ ](A448418_1_En_3_Chapter_IEq103.gif)保持零均值和单位标准偏差的方式修改模型。

在推断或测试期间，总体统计量 E[x<sub>I</sub>和 Var[x<sub>I</sub>通过保持小批量统计量的运行平均值而用于标准化。![ $$ E\left[{x}_i\right]=E\left[{u}_B\right] $$ ](A448418_1_En_3_Chapter_Equaz.gif)

![ $$ Var\left[{x}_i\right]=\left(\frac{m}{m-1}\right)E\left[{\upsigma}_B^2\right] $$ ](A448418_1_En_3_Chapter_Equba.gif)

需要这个校正因子来获得总体方差的无偏估计。

这里提到了批处理规范化的几个优点:

*   由于内部协变量移位的消除或减少，模型可以更快地被训练。获得好的模型参数需要较少的训练迭代次数。
*   批处理规范化有一定的规范化能力，有时可以消除退出的需要。
*   批量归一化适用于卷积神经网络，其中每个输出特征图都有一组γ和β。

## 卷积神经网络的不同结构

在本节中，我们将介绍几种目前广泛使用的卷积神经网络架构。这些网络结构不仅用于分类，而且稍加修改后，还用于分割、定位和检测。此外，每个网络都有预先训练的版本，使社区能够进行迁移学习或微调模型。除了 LeNet，几乎所有的 CNN 模型都赢得了 ImageNet 的千类分类比赛。

### 蓝尼

第一个成功的卷积神经网络是由 Yann LeCunn 在 1990 年开发的，用于为基于 OCR 的活动成功地分类手写数字，例如读取邮政编码、支票等。LeNet5 是 Yann LeCunn 及其同事的最新成果。它接收![ $$ 32\times 32 $$ ](A448418_1_En_3_Chapter_IEq104.gif)大小的图像作为输入，并通过一个卷积层产生 6 个 28x28 大小的特征图。然后，对六个特征图进行子采样，以产生大小为 14×14 的六个输出图像。子采样可以被认为是一种汇集操作。第二卷积层具有大小为![ $$ 28\times 28 $$ ](A448418_1_En_3_Chapter_IEq105.gif)的 16 个特征图，而第二子采样层将特征图大小减小到![ $$ 14\times 14 $$ ](A448418_1_En_3_Chapter_IEq106.gif)。接下来是两个全连接层，分别为 120 和 84 个单元，然后是输出层，十个类对应十个数字。图 [3-30](#Fig37) 表示 LeNet5 的架构图。

![A448418_1_En_3_Fig37_HTML.gif](A448418_1_En_3_Fig37_HTML.gif)

图 3-30。

LeNet5 architectural diagram

LeNet5 网络的主要特点如下:

*   通过子采样的汇集采用![ $$ 2\times 2 $$ ](A448418_1_En_3_Chapter_IEq107.gif)个邻域片，并对四个像素的亮度值求和。通过可训练的权重和偏差对总和进行缩放，然后通过 sigmoid 激活函数进行馈送。这与最大池化和平均池化有一点不同。
*   The filter kernel used for convolution is of size ![ $$ 5\times 5\. $$ ](A448418_1_En_3_Chapter_IEq108.gif) The output units are radial basis function (RBF) units instead of the SoftMax functions that we generally use. The 84 units of the fully connected layers had 84 connections to each of the classes and hence 84 corresponding weights. The 84 weights/class represent each class’s characteristics. If the inputs to those 84 units are very close to the weights corresponding to a class, then the inputs are more likely to belong to that class. In a SoftMax we look at the dot product of the inputs to each of the class’s weight vectors, while in RBF units we look at the Euclidean distance between the input and the output class representative’s weight vectors. The greater the Euclidean distance , the smaller the chance is of the input belonging to that class. The same can be converted to probability by exponentiating the negative of the distance and then normalizing over the different classes. The Euclidean distances over all the classes for an input record would act as the loss function for that input. Let![ $$ x={\left[{x}_1{x}_2..{x}_{84}\right]}^T\in {\mathrm{\mathbb{R}}}^{84\times 1} $$ ](A448418_1_En_3_Chapter_IEq109.gif) be the output vector of the fully connected layer. For each class, there would be 84 weight connections. If the representative class’s weight vector for the ith class is ![ $$ {w}_i\in {\mathrm{\mathbb{R}}}^{84\times 1} $$ ](A448418_1_En_3_Chapter_IEq110.gif) then the output of the ith class unit can be given by the following:

    ![ $$ x-{w_i}_2^2=\sum \limits_{j=1}^{84}{\left({x}_j-{w}_{ij}\right)}^2 $$ ](A448418_1_En_3_Chapter_Equbb.gif)

*   每个类别的代表性权重是预先固定的，并且不是学习的权重。

### 阿勒克斯网

AlexNet CNN 架构由 Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 于 2012 年开发，赢得了 2012 ImageNet ILSVRC (ImageNet 大规模视觉识别挑战赛)。关于 AlexNet 的原始论文题为“使用深度卷积神经网络的 ImageNet 分类”，可以位于 [`https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf`](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) 。

这是 CNN 架构第一次以巨大优势击败其他方法。与第二好的条目的 26.2%的错误率相比，他们的网络在前五个预测中实现了 15.4%的错误率。AlexNet 的架构图如图 [3-31](#Fig38) 所示。

AlexNet 包括五个卷积层、最大池层和丢弃层，以及除了一千个类单元的输入和输出层之外的三个全连接层。网络的输入是大小为![ $$ 224\times 224\times 3\. $$ ](A448418_1_En_3_Chapter_IEq111.gif)的图像。第一卷积层产生 96 个特征图，对应于大小为![ $$ 11\times 11\times 3 $$ ](A448418_1_En_3_Chapter_IEq112.gif)的 96 个滤波器核，步长为 4 个像素单位。第二卷积层产生 256 个对应于大小为![ $$ 5\times 5\times 48 $$ ](A448418_1_En_3_Chapter_IEq113.gif)的滤波器核的特征图。前两个卷积层之后是最大池层，而接下来的三个卷积层一个接一个放置，没有任何中间最大池层。第五卷积层之后是 max 池化层、4096 个单元的两个全连接层，最后是一千个类的 SoftMax 输出层。第三卷积层具有 384 个大小为![ $$ 3\times 3\times 256 $$ ](A448418_1_En_3_Chapter_IEq114.gif)的滤波器核，而第四和第五卷积层分别具有 384 和 256 个大小为![ $$ 3\times 3\times 192 $$ ](A448418_1_En_3_Chapter_IEq115.gif)的滤波器核。在最后两个完全连接的层中使用 0.5 的压差。你会注意到，对于除了第三卷积层之外的所有卷积层，卷积的滤波器核的深度是前一层中特征映射数量的一半。这是因为 AlexNet 当时的计算成本很高，因此训练必须在两个独立的 GPU 之间进行。然而，如果您仔细观察，对于第三个卷积活动，卷积存在交叉连接，因此滤波器核的维数是![ $$ 3\times 3\times 256 $$ ](A448418_1_En_3_Chapter_IEq116.gif)而不是![ $$ 3\times 3\times 128 $$ ](A448418_1_En_3_Chapter_IEq117.gif)。相同类型的交叉连接适用于全连接层，因此它们表现为具有 4096 个单元的普通全连接层。

![A448418_1_En_3_Fig38_HTML.gif](A448418_1_En_3_Fig38_HTML.gif)

图 3-31。

AlexNet architecture

AlexNet 的主要特点如下:

*   ReLU 激活功能用于非线性。它们产生了巨大的影响，因为 RELUs 更容易计算，并且具有恒定的不饱和梯度，而 sigmoid 和 tanh 激活函数的梯度在输入值非常高和非常低时趋于零。
*   在模型中，放弃用于减少过度拟合。
*   相对于非重叠汇集而言，使用了重叠汇集。
*   该模型在两个 GPU GTX 580 上训练了大约五天，以进行快速计算。
*   通过数据扩充技术，如图像平移、水平反射和碎片提取，数据集的大小增加了。

### VGG16

2014 年，VGG 集团凭借名为 VGG16 的 16 层架构在 ILSVRC-2014 竞赛中获得亚军。它使用了一种深入而简单的体系结构，从那以后，这种体系结构受到了广泛的欢迎。与 VGG 网络相关的论文题为“用于大规模图像识别的非常深度的卷积网络”，由[卡伦·西蒙扬](https://arxiv.org/find/cs/1/au:+Simonyan_K/0/1/0/all/0/1)和[安德鲁·齐泽曼](https://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1)撰写。纸张可以位于 [`https://arxiv.org/abs/1409.1556`](https://arxiv.org/abs/1409.1556) 。

VGG16 架构使用了![ $$ 3\times 3 $$ ](A448418_1_En_3_Chapter_IEq118.gif)过滤器，并随后使用了 ReLU 激活和带有![ $$ 2\times 2 $$ ](A448418_1_En_3_Chapter_IEq119.gif)感受域的最大池，而不是使用大型内核过滤器进行卷积。发明人的推理是，使用两个![ $$ 3\times 3 $$ ](A448418_1_En_3_Chapter_IEq120.gif)卷积层相当于具有一个![ $$ 5\times 5 $$ ](A448418_1_En_3_Chapter_IEq121.gif)卷积，同时保留了较小内核滤波器尺寸的优点；即，由于两个卷积 ReLU 对而不是一个卷积 ReLU 对，实现了参数数量的减少和更多的非线性。该网络的一个特殊属性是，随着输入体积的空间维度因卷积和最大池化而减小，随着我们深入网络，特征图的数量会因过滤器数量的增加而增加。

![A448418_1_En_3_Fig39_HTML.gif](A448418_1_En_3_Fig39_HTML.gif)

图 3-32。

VGG16 architecture

图 [3-32](#Fig39) 表示 VGG16 的架构。网络的输入是大小为![ $$ 224\times 224\times 3 $$ ](A448418_1_En_3_Chapter_IEq122.gif)的图像。前两个卷积层产生 64 个特征图，每个特征图之后是最大池。卷积滤波器的空间大小为![ $$ 3\times 3 $$ ](A448418_1_En_3_Chapter_IEq123.gif)，跨距为 1，填充为 1。最大池的大小为![ $$ 2\times 2 $$ ](A448418_1_En_3_Chapter_IEq124.gif)，整个网络的步幅为 2。第三和第四卷积层产生 128 个特征图，每个特征图之后是最大汇集层。网络的其余部分以类似的方式运行，如图 [3-32](#Fig39) 所示。在网络的末端，有三个 4096 个单元的全连接层，每个层之后是一千个类的输出 SoftMax 层。对于完全连接的层，压差设置为 0.5。网络中的所有单元都有 ReLU 激活。

### 瑞斯网

ResNet 是微软的一个 152 层深度卷积神经网络，它以仅 3.6%的错误率赢得了 ILSVRC 2015 比赛，这被认为优于人类 5-10%的错误率。ResNet 上的论文由[何](https://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1)、[、](https://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1)、[任](https://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1)、[孙健](https://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1)撰写，论文题目为《图像识别的深度残差学习》，可定位于 [`https://arxiv.org/abs/1512.03385`](https://arxiv.org/abs/1512.03385) 。除了深度之外，ResNet 还实现了一种独特的剩余块思想。在每一系列卷积 ReLUs 卷积运算之后，运算的输入被反馈到运算的输出。在传统方法中，当进行卷积和其他变换时，我们试图将基础映射拟合到原始数据以解决分类任务。然而，使用 ResNet 的残差块概念，我们试图学习残差映射，而不是从输入到输出的直接映射。在形式上，在每个小的活动块中，我们将输入添加到输出块中。如图 [3-33](#Fig40) 所示。这个概念基于这样的假设，即拟合残差映射比拟合从输入到输出的原始映射更容易。

![A448418_1_En_3_Fig40_HTML.gif](A448418_1_En_3_Fig40_HTML.gif)

图 3-33。

Residual block

## 迁移学习

广义的迁移学习指的是储存在解决问题时获得的知识，并将这些知识用于类似领域的不同问题。由于各种原因，迁移学习在深度学习领域取得了巨大成功。

由于隐藏层的性质和不同单元内的连接方案，深度学习模型通常具有大量参数。为了训练这样一个庞大的模型，需要大量的数据，否则模型将会遇到过度拟合的问题。在许多问题中，训练模型所需的大量数据不可用，但问题的性质需要深度学习解决方案，以便产生合理的影响。例如，在用于对象识别的图像处理中，深度学习模型已知能够提供最先进的解决方案。在这种情况下，迁移学习可以用于从预训练的深度学习模型中生成通用特征，然后使用这些特征建立一个简单的模型来解决问题。因此，这个问题的唯一参数是用于构建简单模型的参数。预训练模型通常是在巨大的数据语料库上训练的，因此具有可靠的参数。

当我们通过几层卷积处理图像时，初始层学会检测非常普通的特征，如卷曲和边缘。随着网络越来越深，更深层中的卷积层学会检测与特定种类的数据集相关的更复杂的特征。例如，在分类中，较深的层将学习检测诸如眼睛、鼻子、脸等特征。

让我们假设我们有一个在 ImageNet 数据集的一千个类别上训练的 VGG16 架构模型。现在，如果我们得到一个更小的数据集，它具有更少的与 VGG16 预训练模型数据集相似的图像类别，那么我们可以使用相同的 VGG16 模型直到完全连接的层，然后用新的类别替换输出层。此外，我们保持网络的权重固定，直到全连接层，并且仅训练模型来学习从全连接层到输出层的权重。这是因为数据集的性质与较小数据集的性质相同，因此通过不同参数在预训练模型中学习的特征对于新的分类问题来说足够好，并且我们只需要学习从完全连接的层到输出层的权重。这是要学习的参数数量的巨大减少，并且它将减少过拟合。如果我们使用 VGG16 架构训练小数据集，它可能会遭受严重的过拟合，因为要在小数据集上学习大量参数。

当数据集的性质与用于预训练模型的数据集的性质非常不同时，您会怎么做？

在这种情况下，我们可以使用相同的预训练模型，但只固定前几组卷积–ReLUs–max 池图层的参数，然后添加几个卷积–ReLU–max 池图层，这些图层将学习检测新数据集的内在特征。最后，我们必须有一个完全连接的层，然后是输出层。由于我们使用来自预训练 VGG16 网络的初始卷积–ReLUs–max 池层集合的权重，因此无需学习这些层的相关参数。如前所述，卷积的早期层学习适用于所有类型图像的非常通用的特征，例如边缘和曲线。网络的其余部分将需要被训练以学习特定问题数据集固有的特定特征。

### 使用迁移学习的指南

以下是关于何时以及如何使用预训练模型进行迁移学习的一些指导原则:

*   问题数据集的大小很大，并且数据集类似于用于预训练模型的数据集，这是理想的情况。我们可以保留整个模型体系结构，除了输出层，当它的类的数量与预训练的不同时。然后，我们可以使用预训练模型的权重作为模型的初始权重来训练模型。
*   问题数据集的大小很大，但数据集不同于用于预训练模型的数据集，在这种情况下，由于数据集很大，我们可以从头开始训练模型。预训练模型在这里不会给出任何收益，因为数据集的性质非常不同，并且因为我们有一个大的数据集，所以我们可以负担得起从头训练整个网络，而不会过度适应与在小数据集上训练的大网络相关的内容。
*   问题数据集的大小很小，并且数据集类似于用于预训练模型的数据集，这就是我们之前讨论的情况。由于数据集内容相似，我们可以重用大部分模型的现有权重，并且只根据问题数据集中的类更改输出图层。然后，我们只为最后一层的权重训练模型。例如，如果我们只为狗和猫获得像 ImageNet 这样的图像，我们可以选择在 ImageNet 上预先训练的 VGG16 模型，只需修改输出层，使其具有两个类，而不是一千个。对于新的网络模型，我们只需要训练特定于最终输出层的权重，保持所有其他权重与预训练的 VGG16 模型的权重相同。
*   问题数据集的大小很小，并且数据集与预训练模型中使用的数据集不同，这不是一个好的情况。如前所述，我们可以冻结预训练网络的几个初始层的权重，然后在问题数据集上训练模型的其余部分。像往常一样，输出图层需要根据问题数据集中类的数量进行更改。由于我们没有大型数据集，我们正试图通过重用预训练模型的初始层的权重来尽可能减少参数的数量。由于 CNN 的前几层了解任何类型图像固有的一般特征，这是可能的。

### 用 Google 的 InceptionV3 迁移学习

InceptionV3 是谷歌最先进的卷积神经网络之一。它是 GoogLeNet 的高级版本，凭借其开箱即用的卷积神经网络架构赢得了 ImageNetILSVRC-2014 竞赛。该网络的细节记录在由[克里斯蒂安·塞格迪](https://arxiv.org/find/cs/1/au:+Szegedy_C/0/1/0/all/0/1)和他的合作者撰写的题为“重新思考计算机视觉的初始架构”的论文中。论文可以位于 [`https://arxiv.org/abs/1512.00567`](https://arxiv.org/abs/1512.00567) 。GoogLeNet 及其修改版本的核心元素是引入了一个 inception 模块来进行卷积和池化。在传统的卷积神经网络中，在卷积层之后，我们要么执行另一个卷积，要么执行最大池，而在初始模块中，一系列卷积和最大池在每一层并行完成，然后合并特征图。此外，在每一层中，卷积不是用一个内核滤波器大小来完成的，而是用多个内核滤波器大小来完成的。初始模块如下图 [3-34](#Fig41) 所示。正如我们所看到的，有一系列并行的卷积和最大池，最后所有的输出特征映射在 gilter 连接块中合并。![ $$ 1\times 1 $$ ](A448418_1_En_3_Chapter_IEq125.gif)卷积进行维度缩减，并执行类似平均池的操作。比如说我们有一个![ $$ 224\times 224\times 160 $$ ](A448418_1_En_3_Chapter_IEq126.gif)的输入量，160 是特征图的数量。与![ $$ 1\times 1\times 20 $$ ](A448418_1_En_3_Chapter_IEq127.gif)滤波器内核的卷积将创建![ $$ 224\times 224\times 20\. $$ ](A448418_1_En_3_Chapter_IEq128.gif)的输出音量

这种网络工作良好，因为不同的核大小基于过滤器的感受域的大小在不同的粒度级别提取特征信息。![ $$ 3\times 3 $$ ](A448418_1_En_3_Chapter_IEq129.gif)感受野将比![ $$ 5\times 5 $$ ](A448418_1_En_3_Chapter_IEq130.gif)感受野提取更多的颗粒信息。

![A448418_1_En_3_Fig41_HTML.gif](A448418_1_En_3_Fig41_HTML.gif)

图 3-34。

Inception module

谷歌的 TensorFlow 提供了一个预训练模型，该模型是在 ImageNet 数据上训练的。它可以用于迁移学习。我们使用来自谷歌的预训练模型，并在从 [`https://www.kaggle.com/c/dogs-vs-cats/data`](https://www.kaggle.com/c/dogs-vs-cats/data) 提取的一组猫和狗的图像上重新训练它。`train.zip`数据集包含 25000 张图片，其中猫和狗各有 12500 张图片。

预训练模型可以在 TensorFlow GitHub 示例文件夹中找到。清单 [3-8](#Par223) 显示了执行和使用迁移学习模型需要遵循的步骤。克隆 TensorFlow GitHub 存储库，因为模型位于 Examples 文件夹中。一旦完成，我们就可以进入克隆的 TensorFlow 文件夹并执行清单 [3-8](#Par223) 中的命令。

![ $$ cd\sim \mathit{\operatorname{curl}}-O\ http:// download. tensorflow. org/ example\_ images/ flower\_ photos. tgz $$ ](A448418_1_En_3_Chapter_Equbc.gif)

![ $$ tar\ xzf\ flower\_ photos. tgz $$ ](A448418_1_En_3_Chapter_Equbd.gif)

![ $$ bazel build tensorflow/ examples/ image\_ retraining: retrain $$ ](A448418_1_En_3_Chapter_Eqube.gif)

![ $$ bazel- bin/ tensorflow/ examples/ image\_ retraining/ retrain-- image\_ dir\sim / Downloads/ animals $$ ](A448418_1_En_3_Chapter_Equbf.gif)T11】

```
-- Output Log from Model retraining in the Final Few Steps of Learning --

2017-07-05 09:28:26.133994: Step 3750: Cross entropy = 0.006824
2017-07-05 09:28:26.173795: Step 3750: Validation accuracy = 100.0% (N=100)
2017-07-05 09:28:26.616457: Step 3760: Train accuracy = 99.0%
2017-07-05 09:28:26.616500: Step 3760: Cross entropy = 0.017717
2017-07-05 09:28:26.656621: Step 3760: Validation accuracy = 100.0% (N=100)
2017-07-05 09:28:27.055419: Step 3770: Train accuracy = 100.0%
2017-07-05 09:28:27.055461: Step 3770: Cross entropy = 0.004180
2017-07-05 09:28:27.094449: Step 3770: Validation accuracy = 99.0% (N=100)
2017-07-05 09:28:27.495100: Step 3780: Train accuracy = 100.0%
2017-07-05 09:28:27.495154: Step 3780: Cross entropy = 0.014055
2017-07-05 09:28:27.540385: Step 3780: Validation accuracy = 99.0% (N=100)
2017-07-05 09:28:27.953271: Step 3790: Train accuracy = 99.0%
2017-07-05 09:28:27.953315: Step 3790: Cross entropy = 0.029298
2017-07-05 09:28:27.992974: Step 3790: Validation accuracy = 100.0% (N=100)
2017-07-05 09:28:28.393039: Step 3800: Train accuracy = 98.0%
2017-07-05 09:28:28.393083: Step 3800: Cross entropy = 0.039568
2017-07-05 09:28:28.432261: Step 3800: Validation accuracy = 99.0% (N=100)
2017-07-05 09:28:28.830621: Step 3810: Train accuracy = 98.0%
2017-07-05 09:28:28.830664: Step 3810: Cross entropy = 0.032378
2017-07-05 09:28:28.870126: Step 3810: Validation accuracy = 100.0% (N=100)
2017-07-05 09:28:29.265780: Step 3820: Train accuracy = 100.0%
2017-07-05 09:28:29.265823: Step 3820: Cross entropy = 0.004463
2017-07-05 09:28:29.304641: Step 3820: Validation accuracy = 98.0% (N=100)
2017-07-05 09:28:29.700730: Step 3830: Train accuracy = 100.0%
2017-07-05 09:28:29.700774: Step 3830: Cross entropy = 0.010076
2017-07-05 09:28:29.741322: Step 3830: Validation accuracy = 100.0% (N=100)
2017-07-05 09:28:30.139802: Step 3840: Train accuracy = 99.0%
2017-07-05 09:28:30.139847: Step 3840: Cross entropy = 0.034331
2017-07-05 09:28:30.179052: Step 3840: Validation accuracy = 100.0% (N=100)
2017-07-05 09:28:30.575682: Step 3850: Train accuracy = 97.0%
2017-07-05 09:28:30.575727: Step 3850: Cross entropy = 0.032292
2017-07-05 09:28:30.615107: Step 3850: Validation accuracy = 100.0% (N=100)
2017-07-05 09:28:31.036590: Step 3860: Train accuracy = 100.0%
2017-07-05 09:28:31.036635: Step 3860: Cross entropy = 0.005654
2017-07-05 09:28:31.076715: Step 3860: Validation accuracy = 99.0% (N=100)
2017-07-05 09:28:31.489839: Step 3870: Train accuracy = 99.0%
2017-07-05 09:28:31.489885: Step 3870: Cross entropy = 0.047375
2017-07-05 09:28:31.531109: Step 3870: Validation accuracy = 99.0% (N=100)
2017-07-05 09:28:31.931939: Step 3880: Train accuracy = 99.0%
2017-07-05 09:28:31.931983: Step 3880: Cross entropy = 0.021294
2017-07-05 09:28:31.972032: Step 3880: Validation accuracy = 98.0% (N=100)
2017-07-05 09:28:32.375811: Step 3890: Train accuracy = 100.0%
2017-07-05 09:28:32.375855: Step 3890: Cross entropy = 0.007524
2017-07-05 09:28:32.415831: Step 3890: Validation accuracy = 99.0% (N=100)
2017-07-05 09:28:32.815560: Step 3900: Train accuracy = 100.0%
2017-07-05 09:28:32.815604: Step 3900: Cross entropy = 0.005150
2017-07-05 09:28:32.855788: Step 3900: Validation accuracy = 99.0% (N=100)
2017-07-05 09:28:33.276503: Step 3910: Train accuracy = 99.0%
2017-07-05 09:28:33.276547: Step 3910: Cross entropy = 0.033086
2017-07-05 09:28:33.316980: Step 3910: Validation accuracy = 98.0% (N=100)
2017-07-05 09:28:33.711042: Step 3920: Train accuracy = 100.0%
2017-07-05 09:28:33.711085: Step 3920: Cross entropy = 0.004519
2017-07-05 09:28:33.750476: Step 3920: Validation accuracy = 99.0% (N=100)
2017-07-05 09:28:34.147856: Step 3930: Train accuracy = 100.0%
2017-07-05 09:28:34.147901: Step 3930: Cross entropy = 0.005670
2017-07-05 09:28:34.191036: Step 3930: Validation accuracy = 99.0% (N=100)
2017-07-05 09:28:34.592015: Step 3940: Train accuracy = 99.0%
2017-07-05 09:28:34.592059: Step 3940: Cross entropy = 0.019866
2017-07-05 09:28:34.632025: Step 3940: Validation accuracy = 98.0% (N=100)
2017-07-05 09:28:35.054357: Step 3950: Train accuracy = 100.0%
2017-07-05 09:28:35.054409: Step 3950: Cross entropy = 0.004421
2017-07-05 09:28:35.100622: Step 3950: Validation accuracy = 96.0% (N=100)
2017-07-05 09:28:35.504866: Step 3960: Train accuracy = 100.0%
2017-07-05 09:28:35.504910: Step 3960: Cross entropy = 0.009696
2017-07-05 09:28:35.544595: Step 3960: Validation accuracy = 99.0% (N=100)
2017-07-05 09:28:35.940758: Step 3970: Train accuracy = 99.0%
2017-07-05 09:28:35.940802: Step 3970: Cross entropy = 0.013898
2017-07-05 09:28:35.982500: Step 3970: Validation accuracy = 100.0% (N=100)
2017-07-05 09:28:36.381933: Step 3980: Train accuracy = 99.0%
2017-07-05 09:28:36.381975: Step 3980: Cross entropy = 0.022074
2017-07-05 09:28:36.422327: Step 3980: Validation accuracy = 100.0% (N=100)
2017-07-05 09:28:36.826422: Step 3990: Train accuracy = 100.0%
2017-07-05 09:28:36.826464: Step 3990: Cross entropy = 0.009017
2017-07-05 09:28:36.866917: Step 3990: Validation accuracy = 99.0% (N=100)
2017-07-05 09:28:37.222010: Step 3999: Train accuracy = 99.0%
2017-07-05 09:28:37.222055: Step 3999: Cross entropy = 0.031987
2017-07-05 09:28:37.261577: Step 3999: Validation accuracy = 99.0% (N=100)
Final test accuracy = 99.2% (N=2593)
Converted 2 variables to const ops.

```

```
# Step3 - Once the model is built with the preceding command we are all set to retrain the model based on our input.
# In this case we will test with the Cat vs Dog dataset extracted from Kaggle. The dataset has two classes. For using the dataset on this model the images pertaining to the classes have to be kept in different folders. The Cat and Dog sub-folders were created within an animals folder. Next we retrain the model using the pre-trained InceptionV3 model. All the layers and weights of the pre-trained model would be transferred to the re-trained model. Only the output layer would be modified to have two classes instead of the 1000 on which the pre-trained model is built. In the re-training only the weights from the last fully connected layer to the new output layer

of two classes would be learned in the retraining. The following command does the retraining:

```

```
# Step2 - Enter the cloned tensorflow folder and build the image retrainer by executing the following command

```

```
#Step1 - Download the following dataset and un-tar it since it would be used for building the retrainer for transfer learning
Listing 3-8.

```

我们可以从清单 [3-8](#Par223) 的输出中看到，我们在猫与狗的分类问题上实现了 99.2%的测试准确率，通过只训练新输出层的权重来重用预训练的 InceptionV3 模型。这就是迁移学习在恰当的环境下的力量。

### 使用预训练的 VGG16 进行迁移学习

在本节中，我们将通过使用在一千类 ImageNet 上预先训练的 VGG16 网络来执行迁移学习，以对来自 Kaggle 的猫与狗数据集进行分类。到数据集的链接是 [`https://www.kaggle.com/c/dogs-vs-cats/data`](https://www.kaggle.com/c/dogs-vs-cats/data) 。首先，我们将从 TensorFlow Slim 导入 VGG16 模型，然后在 VGG16 网络中加载预训练的权重。权重来自在 ImageNet 数据集的一千个类上训练的 VGG16。由于对于我们的问题，我们只有两个类，我们将从最后一个完全连接的层获取输出，并将其与一组新的权重组合，导致输出层将一个神经元对来自 Kaggle 的猫狗数据集进行二进制分类。这个想法是使用预先训练的权重来生成特征，最后我们只学习一组权重，从而得到输出。通过这种方式，我们可以学习相对较少的一组权重，并且可以用较少的数据来训练模型。请在清单 [3-9](#Par226) 中找到详细的实现。

![A448418_1_En_3_Fig42_HTML.gif](A448418_1_En_3_Fig42_HTML.gif)

图 3-35。

Validation set images and their actual versus predicted classes

```
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from scipy.misc import imresize

from sklearn.model_selection import train_test_split
import cv2
from nets import vgg
from preprocessing import vgg_preprocessing
from mlxtend.preprocessing import shuffle_arrays_unison
sys.path.append("/home/santanu/models/slim")

%matplotlib inline

batch_size = 32
width = 224
height = 224
cat_train = '/home/santanu/CatvsDog/train/cat/'
dog_train = '/home/santanu/CatvsDog/train/dog/'
checkpoints_dir = '/home/santanu/checkpoints'
slim = tf.contrib.slim

all_images = os.listdir(cat_train) + os.listdir(dog_train)
train_images, validation_images = train_test_split(all_images, train_size=0.8, test_size=0.2)

MEAN_VALUE = np.array([103.939, 116.779, 123.68])
################################################
# Logic to read the images and also do mean correction
################################################

def image_preprocess(img_path,width,height):
    img = cv2.imread(img_path)
    img = imresize(img,(width,height))
    img = img - MEAN_VALUE
    return(img)

################################################
# Create generator for image batches so that only the batch is in memory

################################################

def data_gen_small(images, batch_size, width,height):
        while True:
            ix = np.random.choice(np.arange(len(images)), batch_size)
            imgs = []
            labels = []
            for i in ix:
                data_dir = ' '
                # images
                if images[i].split('.')[0] == 'cat':
                    labels.append(1)
                    data_dir = cat_train
                else:
                    if images[i].split('.')[0] == 'dog':
                        labels.append(0)
                        data_dir = dog_train
                #print 'data_dir',data_dir
                img_path = data_dir + images[i]
                array_img = image_preprocess(img_path,width,height)
                imgs.append(array_img)

            imgs = np.array(imgs)
            labels = np.array(labels)
            labels = np.reshape(labels,(batch_size,1))
            yield imgs,labels
#######################################################
## Defining the generators for training and validation batches
#######################################################
train_gen = data_gen_small(train_images,batch_size,width,height)
val_gen = data_gen_small(validation_images,batch_size, width,height)

with tf.Graph().as_default():

    x = tf.placeholder(tf.float32,[None,width,height,3])
    y = tf.placeholder(tf.float32,[None,1])

##############################################
## Load the VGG16 model from slim extract the fully connected layer
## before the final output layer
      ###############################################
    with slim.arg_scope(vgg.vgg_arg_scope()):
        logits, end_points = vgg.vgg_16(x,
                               num_classes=1000,
                               is_training=False)
               fc_7 = end_points['vgg_16/fc7']
      ###############################################
## Define the only set of weights that we will learn W1 and b1
###############################################
    Wn =tf.Variable(tf.random_normal([4096,1],mean=0.0,stddev=0.02),name='Wn')
    b = tf.Variable(tf.random_normal([1],mean=0.0,stddev=0.02),name='b')

   ##############################################
   ##  Reshape the fully connected layer fc_7 and define
   ## the logits and probability
   ##############################################
    fc_7 = tf.reshape(fc_7, [-1,W1.get_shape().as_list()[0]])
    logitx = tf.nn.bias_add(tf.matmul(fc_7,W1),b1)
    probx = tf.nn.sigmoid(logitx)

    ####################################################
    # Define Cost and Optimizer
    # Only we wish to learn the weights Wn and b and hence included them in var_list

   ####################################################

    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logitx,labels=y))
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,var_list=[W1,b1])

    ####################################################
    # Loading the pre-trained weights for VGG16

    ####################################################
    init_fn = slim.assign_from_checkpoint_fn(
        os.path.join(checkpoints_dir, 'vgg_16.ckpt'),
        slim.get_model_variables('vgg_16'))

    ####################################################
    # Running the optimization for only 50 batches of size 32
    ####################################################
    with tf.Session() as sess:
        init_op = tf.global_variables_initializer()
        sess.run(init_op)
        # Load weights
        init_fn(sess)
        for i in xrange(1):
            for j in xrange(50):
                batch_x,batch_y = next(train_gen)
                #val_x,val_y = next(val_gen)
                sess.run(optimizer,feed_dict={x:batch_x,y:batch_y})
                cost_train = sess.run(cost,feed_dict={x:batch_x,y:batch_y})
                cost_val = sess.run(cost,feed_dict={x:val_x,y:val_y})
                prob_out = sess.run(probx,feed_dict={x:val_x,y:val_y})
                print "Training Cost",cost_train,"Validation Cost",cost_val
        out_val = (prob_out > 0.5)*1
        print 'accuracy', np.sum(out_val == val_y)*100/float(len(val_y))
        plt.imshow(val_x[1] + MEAN_VALUE)
        print "Actual Class:",class_dict[val_y[1][0]]
        print "Predicted Class:",class_dict[out_val[1][0]]
        plt.imshow(val_x[3] + MEAN_VALUE)

        print "Actual Class:",class_dict[val_y[2][0]]
        print "Predicted Class:",class_dict[out_val[2][0]]

--output--

Training Cost 0.12381 Validation Cost 0.398074
Training Cost 0.160159 Validation Cost 0.118745
Training Cost 0.196818 Validation Cost 0.237163
Training Cost 0.0502732 Validation Cost 0.183091
Training Cost 0.00245218 Validation Cost 0.129029
Training Cost 0.0913893 Validation Cost 0.104865
Training Cost 0.155342 Validation Cost 0.050149
Training Cost 0.00783684 Validation Cost 0.0179586
Training Cost 0.0533897 Validation Cost 0.00746072
Training Cost 0.0112999 Validation Cost 0.00399635
Training Cost 0.0126569 Validation Cost 0.00537223
Training Cost 0.315704 Validation Cost 0.00140141
Training Cost 0.222557 Validation Cost 0.00225646
Training Cost 0.00431023 Validation Cost 0.00342855
Training Cost 0.0266347 Validation Cost 0.00358525
Training Cost 0.0939392 Validation Cost 0.00183608
Training Cost 0.00192089 Validation Cost 0.00105589
Training Cost 0.101151 Validation Cost 0.00049641
Training Cost 0.139303 Validation Cost 0.000168802
Training Cost 0.777244 Validation Cost 0.000357215
Training Cost 2.20503e-06 Validation Cost 0.00628659
Training Cost 0.00145492 Validation Cost 0.0483692
Training Cost 0.0259771 Validation Cost 0.102233
Training Cost 0.278693 Validation Cost 0.11214
Training Cost 0.0387182 Validation Cost 0.0736753
Training Cost 9.19127e-05 Validation Cost 0.0431452
Training Cost 1.19147 Validation Cost 0.0102272
Training Cost 0.302676 Validation Cost 0.0036657
Training Cost 2.22961e-07 Validation Cost 0.00135369
Training Cost 8.65403e-05 Validation Cost 0.000532816
Training Cost 0.00838018 Validation Cost 0.00029422
Training Cost 0.0604016 Validation Cost 0.000262787
Training Cost 0.648359 Validation Cost 0.000327267
Training Cost 0.00821085 Validation Cost 0.000334495
Training Cost 0.178719 Validation Cost 0.000776928
Training Cost 0.362365 Validation Cost 0.000317593
Training Cost 0.000330557 Validation Cost 0.000139824
Training Cost 0.0879459 Validation Cost 5.76907e-05
Training Cost 0.0881795 Validation Cost 1.21865e-05
Training Cost 1.11339 Validation Cost 1.9081e-05
Training Cost 0.000440863 Validation Cost 3.60468e-05
Training Cost 0.00730334 Validation Cost 6.98846e-05
Training Cost 3.65983e-05 Validation Cost 0.000141883
Training Cost 0.296884 Validation Cost 0.000196292
Training Cost 2.10772e-06 Validation Cost 0.000269568
Training Cost 0.179874 Validation Cost 0.000185331
Training Cost 0.380936 Validation Cost 9.48413e-05
Training Cost 0.0146583 Validation Cost 3.80007e-05
Training Cost 0.387566 Validation Cost 5.26306e-05
Training Cost 7.43922e-06 Validation Cost 7.17469e-05
accuracy 100.0

Listing 3-9.Transfer Learning with Pre-trained VGG16

```

我们看到，仅在每批 32 个的中等规模的 50 个批次上训练模型后，验证准确率为 100%。由于批量较小，准确性和成本会有一些问题，但总的来说，验证成本会下降，而验证准确性会上升。在图 [3-35](#Fig42) 中，绘制了几个验证集图像以及它们的实际和预测类，以说明预测的正确性。因此，恰当地利用迁移学习有助于我们在解决一个新问题时重用为一个问题学习的特征检测器。迁移学习大大减少了需要学习的参数数量，从而减少了网络的计算负担。此外，由于较少的参数需要较少的数据来进行训练，因此减少了训练数据大小的限制。

## 摘要

在本章中，我们学习了卷积运算以及如何用它来构造卷积神经网络。此外，我们还学习了 CNN 的各种关键组件，以及训练卷积层和池层的反向传播方法。我们讨论了 CNN 在图像处理中取得成功的两个关键概念——卷积提供的等方差特性和合并操作提供的平移不变性。此外，我们讨论了几种已建立的 CNN 架构，以及如何使用这些 CNN 的预训练版本来执行迁移学习。在下一章，我们将讨论自然语言处理领域中的递归神经网络及其变体。