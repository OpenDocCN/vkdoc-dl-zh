# 2.深度学习概念和张量流介绍

## 深度学习及其进展

深度学习是从人工神经网络进化而来的，人工神经网络自 20 世纪 40 年代就存在了。神经网络是被称为人工神经元的处理单元的互连网络，它松散地模仿生物大脑中的轴突。在生物神经元中，树突从各种相邻的神经元(通常超过一千个)接收输入信号。这些修改后的信号然后被传递到神经元的细胞体或胞体，在那里这些信号被累加在一起，然后被传递到神经元的轴突。如果接收到的输入信号超过指定的阈值，轴突就会释放一个信号，这个信号会传递给其他神经元的邻近树突。图 [2-1](#Fig1) 描绘了一个生物神经元的结构以供参考。

![A448418_1_En_2_Fig1_HTML.jpg](A448418_1_En_2_Fig1_HTML.jpg)

图 2-1。

Structure of a biological neuron

人工神经元单元受生物神经元的启发，为方便起见做了一些修改。很像树突，神经元的输入连接携带来自其他邻近神经元的衰减或放大的输入信号。信号被传递到神经元，在那里输入信号被累加，然后根据接收到的总输入决定输出什么。例如，对于二进制阈值神经元，当总输入超过预定义阈值时，提供输出值 1；否则，输出保持为 0。在人工神经网络中使用了几种其他类型的神经元，它们的实现仅在总输入产生神经元输出的激活函数方面有所不同。在图 [2-2](#Fig2) 中，不同的生物等效物被标记在人工神经元中，以便于类比和解释。

![A448418_1_En_2_Fig2_HTML.gif](A448418_1_En_2_Fig2_HTML.gif)

图 2-2。

Structure of an artificial neuron

人工神经网络始于 20 世纪 40 年代初，前景看好。我们将通过人工神经网络社区中的主要事件的年表来了解这个学科这些年来是如何发展的，以及在此过程中面临了哪些挑战。

![A448418_1_En_2_Fig3_HTML.gif](A448418_1_En_2_Fig3_HTML.gif)

图 2-3。

Evolution of artificial neural networks

*   1943 年，两位电气工程师 Warren McCullogh 和 Walter Pitts 发表了一篇题为“神经活动中固有思想的逻辑演算”的论文，与神经网络有关。论文可以位于 [`http://www.cs.cmu.edu/∼epxing/Class/10715/reading/McCulloch.and.Pitts.pdf`](http://www.cs.cmu.edu/%E2%88%BCepxing/Class/10715/reading/McCulloch.and.Pitts.pdf) 。他们的神经元有一个二进制输出状态，神经元有两种类型的输入:兴奋性输入和抑制性输入。神经元的所有兴奋性输入都具有相等的正权重。如果神经元的所有输入都是兴奋性的，并且如果总输入为![ $$ \sum \limits_i{w}_i{x}_i>0 $$ ](A448418_1_En_2_Chapter_IEq1.gif)，则神经元将输出 1。在任何抑制输入激活或![ $$ \sum \limits_i{w}_i{x}_i\le 0 $$ ](A448418_1_En_2_Chapter_IEq2.gif)的情况下，输出将为 0。使用这种逻辑，所有布尔逻辑功能可以由一个或多个这样的神经元来实现。这些网络的缺点是它们无法通过训练来学习权重。人们必须手动计算出权重，并组合神经元来实现所需的计算。
*   下一件大事是感知机，由弗兰克·罗森布拉特于 1957 年发明。他和他的合作者亚历山大·斯蒂伯和罗伯特·h·沙茨在一份名为《感知机——一种感知和识别自动机，可以位于 [`https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf`](https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf) 的报告中记录了他们的发明。感知器是以二进制分类任务为动机构建的。神经元的权值和偏差都可以通过感知器学习规则来训练。权重可以是正的也可以是负的。弗兰克·罗森布拉特对感知机模型的能力提出了强有力的主张。不幸的是，并不是所有的都是真的。
*   马文·明斯基和西摩·a·帕尔特在 1969 年写了一本名为《感知器:计算几何导论》的书(麻省理工学院出版社)，该书显示了感知器学习算法的局限性，即使是在简单的任务上，如用单个感知器开发 XOR 布尔函数。大部分人工神经网络团体认为，Minsky 和 Papert 所展示的这些局限性适用于所有的神经网络，因此人工神经网络的研究几乎停止了十年，直到 20 世纪 80 年代。
*   在 20 世纪 80 年代，Geoffrey Hilton、David Rumelhart、Ronald Williams 等人重新引起了对人工神经网络的兴趣，这主要是因为学习多层问题的反向传播方法以及神经网络解决非线性分类问题的能力。
*   在 20 世纪 90 年代，由 V. Vapnik 和 C. Cortes 发明的支持向量机(SVM)变得流行，因为神经网络没有扩大到大问题。
*   人工神经网络在 2006 年被重新命名为深度学习，当时 Geoffrey Hinton 等人引入了无监督预训练和深度信念网络的概念。他们在深度信念网络方面的工作发表在题为“深度信念网络的快速学习算法”的论文中论文可以位于 [`https://www.cs.toronto.edu/∼hinton/absps/fastnc.pdf`](https://www.cs.toronto.edu/%E2%88%BChinton/absps/fastnc.pdf) 。
*   ImageNet 是一个标记图像的大型集合，由斯坦福的一个小组在 2010 年创建并发布。
*   2012 年，Alex Krizhevsky、 [Ilya Sutskever](http://www.cs.toronto.edu/%E2%88%BCilya/) 和 Geoffrey Hinton 赢得了 ImageNet 竞赛，实现了 16%的错误率，而在前两年，最好的模型有大约 28%和 26%的错误率。这是一次巨大胜利。该解决方案的实现具有深度学习的几个方面，这些方面是当今任何深度学习实现中的标准。
    *   图形处理单元(GPU)用于训练模型。GPU 非常擅长做矩阵运算，计算速度非常快，因为它们有数千个内核来进行并行计算。
    *   辍学被用作一种正则化技术，以减少过度拟合。
    *   校正线性单元(ReLU)被用作隐藏层的激活函数。

图 [2-3](#Fig3) 展示了人工神经网络向深度学习的演进。ANN 代表人工神经网络，MLP 代表多层感知器，AI 代表人工智能。

## 感知器和感知器学习算法

虽然感知器学习算法能够做的事情有局限性，但它们是我们今天看到的深度学习高级技术的先驱。因此，对感知器和感知器学习算法的详细研究是值得的。感知器是线性二元分类器，它使用一个超平面来区分两个类别。感知器学习算法保证获取一组正确分类所有输入的权重和偏差，只要存在这样一组可行的权重和偏差。

感知器是一个线性分类器，正如我们在第 [1](1.html) 章看到的，线性分类器一般通过构建一个将正类和负类分开的超平面来进行二元分类。

超平面由垂直于超平面的单位权重向量![ $$ {w}^{\hbox{'}}\in {\mathrm{\mathbb{R}}}^{n\times 1} $$ ](A448418_1_En_2_Chapter_IEq3.gif)和确定超平面离原点的距离的偏置项 b 来表示。选择矢量![ $$ {w}^{\hbox{'}}\in {\mathrm{\mathbb{R}}}^{n\times 1} $$ ](A448418_1_En_2_Chapter_IEq4.gif)指向正类。

如图 [2-4](#Fig4) 所示，对于任何输入向量![ $$ {x}^{\prime}\in {\mathrm{\mathbb{R}}}^{n\times 1} $$ ](A448418_1_En_2_Chapter_IEq5.gif)，与单位向量![ $$ {w}^{\hbox{'}}\in {\mathrm{\mathbb{R}}}^{n\times 1} $$ ](A448418_1_En_2_Chapter_IEq6.gif)的负值的点积将给出超平面到原点的距离 b，因为 x’和 w’位于原点的相反侧。形式上，对于躺在超平面上的点，

![ $$ -{w^{\prime}}^T{x}^{\prime }=b=>{w^{\prime}}^T{x}^{\prime }+b=0 $$ ](A448418_1_En_2_Chapter_Equa.gif)

![A448418_1_En_2_Fig4_HTML.gif](A448418_1_En_2_Fig4_HTML.gif)

图 2-4。

Hyperplane separating two classes

类似地，对于位于超平面下面的点，即属于正类的输入向量![ $$ {x}_{+}^{\hbox{'}}\in {\mathrm{\mathbb{R}}}^{n\times 1} $$ ](A448418_1_En_2_Chapter_IEq7.gif)，w’上的![ $$ {x}_{+}^{\hbox{'}} $$ ](A448418_1_En_2_Chapter_IEq8.gif)的投影的负值应该小于 b。因此，对于属于正类的点，

![ $$ -{w^{\prime}}^T{x}^{\prime}\left\langle b=\right\rangle {w^{\prime}}^T{x}^{\prime }+b>0 $$ ](A448418_1_En_2_Chapter_Equb.gif)

类似地，对于位于超平面上方的点，即属于负类的输入向量![ $$ {x}_{-}^{\hbox{'}}\in {\mathrm{\mathbb{R}}}^{n\times 1} $$ ](A448418_1_En_2_Chapter_IEq9.gif)，![ $$ {x}_{-}^{\hbox{'}} $$ ](A448418_1_En_2_Chapter_IEq10.gif)在 w’上的投影的负值应该大于 b。因此，对于属于负类的点，

![ $$ -{w^{\prime}}^T{x}^{\prime }>b=>{w^{\prime}}^T{x}^{\prime }+b<0 $$ ](A448418_1_En_2_Chapter_Equc.gif)

总结前面的推论，我们可以得出以下结论:

*   ![ $$ w{\hbox{'}}^T{x}^{\hbox{'}}+b=0 $$ ](A448418_1_En_2_Chapter_IEq11.gif)对应于超平面，所有位于该超平面上的![ $$ {x}^{\hbox{'}}\in {\mathrm{\mathbb{R}}}^{n\times 1} $$ ](A448418_1_En_2_Chapter_IEq12.gif)将满足该条件。通常，超平面上的点被认为属于负类。
*   ![ $$ {w}^{\hbox{'}T}{x}^{\prime }+b>0 $$ ](A448418_1_En_2_Chapter_IEq13.gif)对应正类中的所有点。
*   ![ $$ w{\hbox{'}}^T{x}^{\hbox{'}}+b\le 0 $$ ](A448418_1_En_2_Chapter_IEq14.gif)对应负类中的所有点。

然而，对于感知器，我们不将权重向量 w’保持为单位向量，而是将其保持为任何一般向量。在这种情况下，偏差 b 将不对应于超平面距原点的距离，而是距原点的距离的缩放版本，缩放因子是向量 w’的大小或 l <sup>2</sup> 范数，即| | w’| |<sub>2</sub>。总结一下，如果 w’是垂直于超平面的任意一般向量，并且指向正类，那么![ $$ w{\hbox{'}}^Tx+b=0 $$ ](A448418_1_En_2_Chapter_IEq15.gif)仍然代表一个超平面，其中 b 代表超平面到原点的距离乘以 w’的大小。

在机器学习领域中，任务是学习超平面的参数(即，w’和 b)。我们通常倾向于简化问题，去掉偏置项，将其作为 w 内的一个参数，对应于恒定输入特性 1，1，正如我们在第 [1](1.html) 章中所讨论的。

设添加偏差后的新参数向量为![ $$ w\in {\mathrm{\mathbb{R}}}^{\left(n+1\right)\times 1} $$ ](A448418_1_En_2_Chapter_IEq16.gif)，添加常数项 1 后的新输入特征向量为![ $$ x\in {\mathrm{\mathbb{R}}}^{\left(n+1\right)\times 1} $$ ](A448418_1_En_2_Chapter_IEq17.gif)，其中

![ $$ {x}^{\prime }={\left[{x}_1\ {x}_2{x}_3..\kern0.5em {x}_n\right]}^T $$ ](A448418_1_En_2_Chapter_Equd.gif)

![ $$ x={\left[1\ {x}_1\ {x}_2{x}_3..\kern0.5em {x}_n\right]}^T $$ ](A448418_1_En_2_Chapter_Eque.gif)

![ $$ {w}^{\hbox{'}}={\left[{w}_1\ {w}_2{w}_3..\kern0.5em {w}_n\right]}^T $$ ](A448418_1_En_2_Chapter_Equf.gif)

![ $$ w={\left[b\ {w}_1\ {w}_2{w}_3..\kern0.5em {w}_n\right]}^T $$ ](A448418_1_En_2_Chapter_Equg.gif)

通过前面的操作，我们已经使ℝ <sup>n</sup> 中距离原点一定距离的超平面通过了![ $$ {\mathrm{\mathbb{R}}}^{\left(n+1\right)} $$ ](A448418_1_En_2_Chapter_IEq18.gif)向量空间中的原点。超平面现在仅由其权重参数向量![ $$ w\in {\mathrm{\mathbb{R}}}^{\left(n+1\right)\times 1} $$ ](A448418_1_En_2_Chapter_IEq19.gif)确定，分类规则简化如下:

*   ![ $$ {w}^Tx=0 $$ ](A448418_1_En_2_Chapter_IEq20.gif)对应超平面，所有位于超平面上的![ $$ x\in {\mathrm{\mathbb{R}}}^{\left(n+1\right)\times 1} $$ ](A448418_1_En_2_Chapter_IEq21.gif)都会满足这个条件。
*   ![ $$ {w}^Tx>0 $$ ](A448418_1_En_2_Chapter_IEq22.gif)对应正类中的所有点。这意味着分类现在仅由向量 w 和 x 之间的角度决定。如果输入向量 x 与权重参数向量 w 之间的角度在![ $$ -90 $$ ](A448418_1_En_2_Chapter_IEq23.gif)度到![ $$ +90 $$ ](A448418_1_En_2_Chapter_IEq24.gif)度之间，则输出分类为正。
*   ![ $$ {w}^Tx\le 0 $$ ](A448418_1_En_2_Chapter_IEq25.gif)对应负类中的点。在不同的分类算法中，相等条件被不同地对待。对于感知器，超平面上的点被视为属于负类。

现在，我们已经拥有了进行感知器学习算法所需的一切。

设![ $$ {x}^{(i)}\in {\mathrm{\mathbb{R}}}^{\left(n+1\right)\times 1}\kern0.5em \forall i=\left\{1,2,\dots .,m\right\} $$ ](A448418_1_En_2_Chapter_IEq26.gif)代表 m 个输入特征向量，![ $$ {y}^{(i)}\in \left\{0,1\right\}\forall i=\left\{1,2,\dots .,m\right\} $$ ](A448418_1_En_2_Chapter_IEq27.gif)代表相应的类别标签。

感知器学习问题如下:

*   步骤 1–从一组随机重量开始![ $$ w\in {\mathrm{\mathbb{R}}}^{\left(n+1\right)\times 1}. $$ ](A448418_1_En_2_Chapter_IEq28.gif)
*   步骤 2-评估数据点的预测类别。对于一个输入数据点 x <sup>(i)</sup> 如果![ $$ {w}^T{x}^{(i)}>0 $$ ](A448418_1_En_2_Chapter_IEq29.gif)则预测类![ $$ {y}_p^{(i)}=1 $$ ](A448418_1_En_2_Chapter_IEq30.gif)，否则![ $$ {y}_p^{(i)}=0 $$ ](A448418_1_En_2_Chapter_IEq31.gif)。对于感知器分类器，超平面上的点通常被认为属于负类。
*   步骤 3–更新权重向量 w，如下所示:
    *   如果 y <sub>p</sub> <sup>(i)</sup> = 0 且实际类别![ $$ {y}^{(i)}=1 $$ ](A448418_1_En_2_Chapter_IEq32.gif)，则更新权重向量为![ $$ w=w+{x}^{(i)}. $$ ](A448418_1_En_2_Chapter_IEq33.gif)
    *   如果 y <sub>p</sub> <sup>(i)</sup> = 1 且实际类别![ $$ {y}^{(i)}=0 $$ ](A448418_1_En_2_Chapter_IEq34.gif)，则更新权重向量为![ $$ w=w-{x}^{(i)}. $$ ](A448418_1_En_2_Chapter_IEq35.gif)
    *   如果![ $$ {y}_p^{(i)}={y}^{(i)} $$ ](A448418_1_En_2_Chapter_IEq36.gif)，w 不需要更新。
*   第 4 步–转到第 2 步，处理下一个数据点。
*   第 5 步-当所有数据点都被正确分类后，停止。

感知器将仅能够正确地分类这两个类别，如果存在可以线性地分离这两个类别的可行的权重向量 w。在这种情况下，感知器收敛定理保证收敛。

### 感知机学习的几何解释

感知器学习的几何解释揭示了可行的权重向量 w，该权重向量 w 表示分离正类和负类的超平面。

![A448418_1_En_2_Fig5_HTML.gif](A448418_1_En_2_Fig5_HTML.gif)

图 2-5。

Hyperplanes in weight space and feasible set of weight vectors

我们取两个数据点，(x <sup>(1)</sup> ，y <sup>(1)</sup> )和(x <sup>(2)</sup> ，y <sup>(2)</sup> ，如图 [2-5](#Fig5) 所示。此外，让![ $$ {x}^{(i)}\in {\mathrm{\mathbb{R}}}^{3\times 1} $$ ](A448418_1_En_2_Chapter_IEq37.gif)包括截距项的常数特征 1。同样，我们取![ $$ {y}^{(1)}=1 $$ ](A448418_1_En_2_Chapter_IEq38.gif)和![ $$ {y}^{(2)}=0 $$ ](A448418_1_En_2_Chapter_IEq39.gif)(即数据点 1 属于正类，而数据点 2 属于负类)。

在输入特征向量空间中，权重向量确定超平面。同样，我们需要将各个输入向量视为权重空间中超平面的代表，以确定用于正确分类数据点的可行权重向量集。

在图 [2-5](#Fig5) 中，超平面 1 由输入向量 x <sup>(1)</sup> 确定，该向量垂直于超平面 1。此外，超平面穿过原点，因为偏置项已经作为权重向量 w 内的参数被消耗。对于第一个数据点，![ $$ {y}^{(1)}=1 $$ ](A448418_1_En_2_Chapter_IEq40.gif)。如果![ $$ {w}^T{x}^{(1)}>0 $$ ](A448418_1_En_2_Chapter_IEq41.gif)，第一个数据点的预测将是正确的。与输入向量 x <sup>(1)</sup> 成-90 到+90 度角的所有权重向量 w 将满足条件![ $$ {w}^T{x}^{(1)}>0 $$ ](A448418_1_En_2_Chapter_IEq42.gif)。它们形成了第一个数据点的可行权重向量集，如图 [2-5](#Fig5) 中超平面 1 上方的阴影区域所示。

类似地，超平面 2 由垂直于超平面 2 的输入向量 x <sup>(2)</sup> 确定。对于第二个数据点，![ $$ {y}^{(2)}=0 $$ ](A448418_1_En_2_Chapter_IEq43.gif)。如果![ $$ {w}^T{x}^{(2)}\le 0 $$ ](A448418_1_En_2_Chapter_IEq44.gif)，第二个数据点的预测将是正确的。与输入向量 x <sup>(2)</sup> 成-90 度到+90 度角以外的所有权重向量 w 将满足条件![ $$ {w}^T{x}^{(2)}\le 0 $$ ](A448418_1_En_2_Chapter_IEq45.gif)。它们形成了第二个数据点的可行权重向量集，如图 [2-5](#Fig5) 中超平面 2 下方的阴影区域所示。

因此，满足两个数据点的权重向量 w 的集合是两个阴影区域之间的重叠区域。重叠区域中的任何权重向量 w 将能够通过它们在输入向量空间中定义的超平面来线性分离这两个数据点。

### 感知器学习的局限性

感知器学习规则只能分离输入空间中可线性分离的类。连最基本的异或门逻辑都无法用感知器学习规则实现。

对于 XOR 逻辑下面是输入和相应的输出标签或类

![ $$ {x}_1=1,{x}_2=0\kern1.5em y=1 $$ ](A448418_1_En_2_Chapter_Equh.gif)

![ $$ {x}_1=0,{x}_2=1\kern1.5em y=1 $$ ](A448418_1_En_2_Chapter_Equi.gif)

![ $$ {x}_1=1,{x}_2=1\kern1.5em y=0 $$ ](A448418_1_En_2_Chapter_Equj.gif)

![ $$ {x}_1=0,{x}_2=0\kern1.5em y=0 $$ ](A448418_1_En_2_Chapter_Equk.gif)

让我们初始化权重向量![ $$ w\to {\left[0\ 0\ 0\right]}^T $$ ](A448418_1_En_2_Chapter_IEq46.gif)，其中权重向量的第一个分量对应于偏置项。类似地，所有输入向量的第一个分量都是 1。

*   对于![ $$ {x}_1=1,{x}_2=0,y=1 $$ ](A448418_1_En_2_Chapter_IEq47.gif)，预测值为![ $$ {\mathrm{w}}^Tx=\left[0\ 0\ 0\right]\left[\begin{array}{c}1\\ {}1\\ {}0\end{array}\right] $$ ](A448418_1_En_2_Chapter_IEq48.gif) = 0。由于![ $$ {\mathrm{w}}^Tx=0 $$ ](A448418_1_En_2_Chapter_IEq49.gif)，数据点将被归类为 0，这与 1 的实际类别不匹配。因此，根据感知器规则更新的权重向量应该是![ $$ w\to w+x=\left[\begin{array}{c}0\\ {}0\\ {}0\end{array}\right]+\left[\begin{array}{c}1\\ {}1\\ {}0\end{array}\right]=\left[\begin{array}{c}1\\ {}1\\ {}0\end{array}\right] $$ ](A448418_1_En_2_Chapter_IEq50.gif)。
*   对于![ $$ {x}_1=0,{x}_2=1,y=1 $$ ](A448418_1_En_2_Chapter_IEq51.gif)，预测值为![ $$ {\mathrm{w}}^Tx=\left[1\ 1\ 0\right]\left[\begin{array}{c}1\\ {}0\\ {}1\end{array}\right] $$ ](A448418_1_En_2_Chapter_IEq52.gif) = 1。由于![ $$ {\mathrm{w}}^Tx=1>0 $$ ](A448418_1_En_2_Chapter_IEq53.gif)，数据点将被正确分类为 1。因此，权重向量没有更新，它停留在![ $$ \left[\begin{array}{c}1\\ {}1\\ {}0\end{array}\right] $$ ](A448418_1_En_2_Chapter_IEq54.gif)。
*   对于![ $$ {x}_1=1,{x}_2=1,y=0 $$ ](A448418_1_En_2_Chapter_IEq55.gif)，预测值为![ $$ {\mathrm{w}}^Tx=\left[1\ 1\ 0\right]\left[\begin{array}{c}1\\ {}1\\ {}1\end{array}\right] $$ ](A448418_1_En_2_Chapter_IEq56.gif) = 2。由于![ $$ {\mathrm{w}}^Tx=2 $$ ](A448418_1_En_2_Chapter_IEq57.gif)，数据点将被归类为 1，这与 0 的实际类别不匹配。因此，更新后的权重向量应该是![ $$ w\to w-x=\left[\begin{array}{c}1\\ {}1\\ {}0\end{array}\right]-\left[\begin{array}{c}1\\ {}1\\ {}1\end{array}\right]=\left[\begin{array}{c}0\\ {}0\\ {}-1\end{array}\right] $$ ](A448418_1_En_2_Chapter_IEq58.gif)。
*   对于![ $$ {x}_1=0,{x}_2=0,y=0 $$ ](A448418_1_En_2_Chapter_IEq59.gif)，预测值为![ $$ {\mathrm{w}}^Tx=\left[0\ 0-1\right]\left[\begin{array}{c}1\\ {}0\\ {}0\end{array}\right] $$ ](A448418_1_En_2_Chapter_IEq60.gif) = 0。由于![ $$ {\mathrm{w}}^Tx=0 $$ ](A448418_1_En_2_Chapter_IEq61.gif)，数据点将被正确分类为 0。因此，没有对权重向量 w 的更新

因此，第一次通过数据点后的权重向量为![ $$ w={\left[0\ 0-1\right]}^T $$ ](A448418_1_En_2_Chapter_IEq62.gif)。基于更新后的权重向量 w，让我们评估这些点的分类情况。

*   对于数据点 1，![ $$ {\mathrm{w}}^Tx=\left[0\ 0-1\right]\left[\begin{array}{c}1\\ {}1\\ {}0\end{array}\right]=0 $$ ](A448418_1_En_2_Chapter_IEq63.gif)，因此被错误归类为 0 类。
*   对于数据点 2，![ $$ {\mathrm{w}}^Tx=\left[0\ 0-1\right]\left[\begin{array}{c}1\\ {}0\\ {}1\end{array}\right]=-1 $$ ](A448418_1_En_2_Chapter_IEq64.gif)，因此被错误归类为 0 类。
*   对于数据点 3，![ $$ {\mathrm{w}}^Tx=\left[0\ 0-1\right]\left[\begin{array}{c}1\\ {}1\\ {}1\end{array}\right]=-1 $$ ](A448418_1_En_2_Chapter_IEq65.gif)，因此被正确分类为 0 类。
*   对于数据点 4，![ $$ {\mathrm{w}}^Tx=\left[0\ 0-1\right]\left[\begin{array}{c}1\\ {}0\\ {}0\end{array}\right]=0 $$ ](A448418_1_En_2_Chapter_IEq66.gif)，因此被正确分类为 0 类。

基于前面的分类，我们看到在第一次迭代之后，感知器算法设法只正确地分类了负类。如果我们再次对数据点应用感知器学习规则，则在第二遍中对权重向量 w 的更新将如下:

*   对于数据点 1，![ $$ {\mathrm{w}}^Tx=\left[0\ 0-1\right]\left[\begin{array}{c}1\\ {}1\\ {}0\end{array}\right]=0 $$ ](A448418_1_En_2_Chapter_IEq67.gif)，因此被错误归类为 0 类。因此，根据感知器规则更新的权重是![ $$ w\to w+x=\kern0.5em \left[\begin{array}{c}0\\ {}0\\ {}-1\end{array}\right]+\left[\begin{array}{c}1\\ {}1\\ {}0\end{array}\right]=\left[\begin{array}{c}1\\ {}1\\ {}-1\end{array}\right] $$ ](A448418_1_En_2_Chapter_IEq68.gif)。
*   对于数据点 2，![ $$ {\mathrm{w}}^Tx=\left[1\ 1-1\right]\left[\begin{array}{c}1\\ {}0\\ {}1\end{array}\right]=0 $$ ](A448418_1_En_2_Chapter_IEq69.gif)，因此被错误归类为 0 类。因此，根据感知器规则更新的权重是![ $$ w\to w+x=\kern0.5em \left[\begin{array}{c}1\\ {}1\\ {}-1\end{array}\right]+\left[\begin{array}{c}1\\ {}0\\ {}1\end{array}\right]=\left[\begin{array}{c}2\\ {}1\\ {}0\end{array}\right] $$ ](A448418_1_En_2_Chapter_IEq70.gif)。
*   对于数据点 3，![ $$ {\mathrm{w}}^Tx=\left[2\ 10\right]\left[\begin{array}{c}1\\ {}1\\ {}1\end{array}\right]=3 $$ ](A448418_1_En_2_Chapter_IEq71.gif)，因此被错误归类为 1 类。因此，根据感知器规则更新的权重是![ $$ w\to w-x=\kern0.5em \left[\begin{array}{c}2\\ {}1\\ {}0\end{array}\right]-\left[\begin{array}{c}1\\ {}1\\ {}1\end{array}\right]=\left[\begin{array}{c}1\\ {}0\\ {}-1\end{array}\right] $$ ](A448418_1_En_2_Chapter_IEq72.gif)。
*   对于数据点 4，![ $$ {\mathrm{w}}^Tx=\left[1\ 0-1\right]\left[\begin{array}{c}1\\ {}0\\ {}0\end{array}\right]=1 $$ ](A448418_1_En_2_Chapter_IEq73.gif)，因此被错误归类为 1 类。因此，根据感知器规则更新的权重是![ $$ w\to w-x=\kern0.5em \left[\begin{array}{c}1\\ {}0\\ {}-1\end{array}\right]-\left[\begin{array}{c}1\\ {}0\\ {}0\end{array}\right]=\left[\begin{array}{c}0\\ {}0\\ {}-1\end{array}\right] $$ ](A448418_1_En_2_Chapter_IEq74.gif)。

第二遍后的权重向量为![ $$ {\left[0\ 0-1\right]}^T $$ ](A448418_1_En_2_Chapter_IEq75.gif)，与第一遍后的权重向量相同。从感知机学习的第一次和第二次过程中进行的观察来看，很明显，无论我们对数据点进行多少次，我们最终都会得到权重向量![ $$ {\left[0\ 0-1\right]}^T $$ ](A448418_1_En_2_Chapter_IEq76.gif)。正如我们前面看到的，这个权重向量只能正确地对否定类进行分类，因此我们可以在不失一般性的情况下有把握地推断，感知器算法将总是无法对 XOR 逻辑进行建模。

### 非线性需求

正如我们所看到的，感知器算法只能学习用于分类的线性决策边界，因此不能解决决策边界中需要非线性的问题。通过对 XOR 问题的说明，我们看到感知器不能正确地线性分离这两类。

我们需要两个超平面来分隔这两个类别，如图 [2-6](#Fig6) 所示，通过感知器算法学习的一个超平面不足以提供所需的分类。在图 [2-6](#Fig6) 中，两条超平面线之间的数据点属于正类，另外两个数据点属于负类。需要两个超平面来分离两个类相当于拥有一个非线性分类器。

![A448418_1_En_2_Fig6_HTML.gif](A448418_1_En_2_Fig6_HTML.gif)

图 2-6。

XOR problem with two hyperplanes to separate the two classes

多层感知器(MLP)可以通过在隐藏层中引入非线性来提供类别之间的非线性分离。请注意，当感知器基于接收到的总输入输出 0 或 1 时，输出是其输入的非线性函数。学习多层感知器的权重时所说的和所做的一切都不可能通过感知器学习规则来实现。

![A448418_1_En_2_Fig7_HTML.gif](A448418_1_En_2_Fig7_HTML.gif)

图 2-7。

XOR logic implementation with multi-layer Perceptrons network

在图 [2-7](#Fig7) 中，XOR 逻辑通过多层感知器网络实现。如果我们有一个包含两个感知器的隐藏层，其中一个能够执行 OR 逻辑，而另一个能够执行 AND 逻辑，那么整个网络将能够实现 XOR 逻辑。用于 or 和 and 逻辑的感知器可以使用感知器学习规则来训练。然而，网络作为一个整体不能通过感知器学习规则来训练。如果我们观察 XOR 门的最终输入，它将是其输入的非线性函数，以产生非线性判定边界。

### 非线性隐层感知器的激活函数

如果我们使隐藏层的激活函数是线性的，那么最终神经元的输出将是线性的，因此我们将不能学习任何非线性的决策边界。为了说明这一点，让我们尝试通过具有线性激活函数的隐藏层单元来实现 XOR 函数。

![A448418_1_En_2_Fig8_HTML.gif](A448418_1_En_2_Fig8_HTML.gif)

图 2-8。

Linear output hidden layers in a two-layer Perceptrons network

图 [2-8](#Fig8) 显示了一个带有一个隐藏层的两层感知器网络。隐藏层由两个神经元单元组成。当隐藏单元中的激活是线性时，我们观察网络的总输出:

隐藏单元的输出![ $$ {h}_1={w}_{11}{x}_1+{w}_{21}{x}_2+{b}_1 $$ ](A448418_1_En_2_Chapter_IEq77.gif)

隐藏单元的输出![ $$ {h}_2={w}_{12}{x}_1+{w}_{22}{x}_2+{b}_2 $$ ](A448418_1_En_2_Chapter_IEq78.gif)

输出单元的输出![ $$ {p}_1=\kern0.5em {w}_1\left({w}_{11}{x}_1+{w}_{21}{x}_2+{b}_1\right)+\kern0.5em {w}_2\left({w}_{12}{x}_1+{w}_{22}{x}_2+{b}_2\right)+{b}_3 $$ ](A448418_1_En_2_Chapter_IEq79.gif)

![ $$ =\left({w}_1{w}_{11}+\kern0.5em {w}_2{w}_{12}\right){x}_1+\left({w}_1{w}_{21}+\kern0.5em {w}_2{w}_{22}\right){x}_2+{w}_1{b}_1+{w}_2{b}_2+{b}_3 $$ ](A448418_1_En_2_Chapter_Equl.gif)

如前所述，网络的最终输出——即单元 p <sub>1</sub> 的输出——是其输入的线性函数，因此网络不会在类之间产生非线性分离。

如果我们引入表示为![ $$ f(x)=1/\Big(1+{e}^{-x} $$ ](A448418_1_En_2_Chapter_IEq80.gif)的激活函数，而不是由隐藏层产生的线性输出，则隐藏单元的输出![ $$ {h}_1=1/\Big(1+{e}^{-\left({w}_{11}{x}_1+{w}_{21}{x}_2+{b}_1\right)} $$ ](A448418_1_En_2_Chapter_IEq81.gif)。

同样，隐藏单元的输出![ $$ {h}_2=1/\left(1+{e}^{-\left({w}_{12}{x}_1+{w}_{22}{x}_2+{b}_2\right)}\right) $$ ](A448418_1_En_2_Chapter_IEq82.gif)。

输出单元的输出![ $$ {p}_1=\kern0.5em {w}_1/\left(1+{e}^{-\left({w}_{11}{x}_1+{w}_{21}{x}_2+{b}_1\right)}\right)+\kern0.5em {w}_2/\left(1+{e}^{-\left({w}_{12}{x}_1+{w}_{22}{x}_2+{b}_2\right)}\right)+{b}_3 $$ ](A448418_1_En_2_Chapter_IEq83.gif)。

显然，前面的输出在其输入中是非线性的，因此可以学习更复杂的非线性决策边界，而不是使用线性超平面来解决分类问题。隐藏层的激活函数称为 sigmoid 函数，我们将在后面的章节中更详细地讨论它。

### 神经元/感知器的不同激活函数

神经单元有几个激活函数，它们的使用根据手头的问题和神经网络的拓扑而变化。在这一节中，我们将讨论当今人工神经网络中使用的所有相关激活函数。

#### 线性激活函数

在线性神经元中，输出线性依赖于其输入。如果神经元接收三个输入 x <sub>1</sub> ，x <sub>2</sub> 和 x <sub>3</sub> ，那么线性神经元的输出 y 由![ $$ y={w}_1{x}_1+{w}_2{x}_2+{w}_3{x}_3+b $$ ](A448418_1_En_2_Chapter_IEq84.gif)给出，其中 w <sub>1</sub> ，w <sub>2</sub> ，w <sub>3</sub> 分别是输入 x <sub>1</sub> ，x <sub>2</sub> 和 x <sub>3</sub> 的突触权重，b

用向量表示法，我们可以表示输出![ $$ y={w}^Tx+b $$ ](A448418_1_En_2_Chapter_IEq85.gif)。

如果我们取![ $$ {w}^Tx+b=z $$ ](A448418_1_En_2_Chapter_IEq86.gif)，那么相对于净输入 z 的输出将如图 [2-9](#Fig9) 所示。

![A448418_1_En_2_Fig9_HTML.gif](A448418_1_En_2_Fig9_HTML.gif)

图 2-9。

Linear output hidden layers in a two-layer Perceptrons network

#### 二元阈值激活函数

在二进制阈值神经元中(见图 [2-10](#Fig10) )，如果神经元的净输入超过指定阈值，则该神经元被激活；即输出 1，否则输出 0。如果神经元的净线性输入是![ $$ z={w}^Tx+b $$ ](A448418_1_En_2_Chapter_IEq87.gif)，k 是神经元激活的阈值，那么

![ $$ y=1\kern2.25em if\ z>k $$ ](A448418_1_En_2_Chapter_Equm.gif)

![ $$ y=0\kern2.25em if\ z\le k $$ ](A448418_1_En_2_Chapter_Equn.gif)

![A448418_1_En_2_Fig10_HTML.gif](A448418_1_En_2_Fig10_HTML.gif)

图 2-10。

Binary threshold neuron

通常，通过调整偏置，二进制阈值神经元被调整为在阈值 0 处激活。神经元在![ $$ {w}^Tx+b>k=>{w}^Tx+\left(b-k\right)>0 $$ ](A448418_1_En_2_Chapter_IEq88.gif)时被激活。

#### Sigmoid 激活函数

乙状结肠神经元的输入-输出关系表示如下:

![ $$ y=1/\left(1+{e}^{-z}\right) $$ ](A448418_1_En_2_Chapter_Equo.gif)

其中![ $$ z={w}^Tx+b $$ ](A448418_1_En_2_Chapter_IEq89.gif)是乙状结肠激活函数的净输入。

*   当 sigmoid 函数的净输入 z 是一个正大数时![ $$ {e}^{-z} $$ ](A448418_1_En_2_Chapter_IEq90.gif) ➤为 0，因此 y ➤为 1。
*   当 sigmoid 的净输入 z 是一个负的大数时![ $$ {e}^{-z} $$ ](A448418_1_En_2_Chapter_IEq91.gif) ➤ ∞，因此 y ➤为 0。
*   When the net input z to a sigmoid function is 0 then ![ $$ {e}^{-z} $$ ](A448418_1_En_2_Chapter_IEq92.gif) = 1 and so ![ $$ y=\frac{1}{2} $$ ](A448418_1_En_2_Chapter_IEq93.gif) .

    ![A448418_1_En_2_Fig11_HTML.gif](A448418_1_En_2_Fig11_HTML.gif)

    图 2-11。

    Sigmoid activation function

图 [2-11](#Fig11) 显示了一个 sigmoid 激活函数的输入-输出关系。具有 sigmoid 激活函数的神经元的输出非常平滑，并且给出良好的连续导数，这在训练神经网络时工作良好。sigmoid 激活函数的输出范围在 0 和 1 之间。由于 sigmoid 函数能够提供 0 到 1 范围内的连续值，因此通常用于输出二元分类中给定类别的概率。隐藏层中的 sigmoid 激活函数引入了非线性，使得模型可以学习更复杂的特征。

#### SoftMax 激活功能

SoftMax 激活函数是 sigmoid 函数的推广，最适合多类分类问题。如果有 k 个输出类别，并且第 I 个类别的权重向量是 w <sup>(i)</sup> ，则给定输入向量![ $$ x\in {\mathrm{\mathbb{R}}}^{n\times 1} $$ ](A448418_1_En_2_Chapter_IEq94.gif)的第 I 个类别的预测概率由下式给出:

![ $$ P\left({y}_i=1/x\right)=\frac{e^{{w^{(i)}}^Tx+{b}^{(i)}}}{\sum_{j=1}^k{e}^{{w^{(j)}}^Tx+{b}^{(j)}}} $$ ](A448418_1_En_2_Chapter_Equp.gif)

其中 b <sup>(i)</sup> 是 SoftMax 的每个输出单元的偏差项。

让我们试着看看 sigmoid 函数和两类 SoftMax 函数之间的联系。

假设这两个类是 y <sub>1</sub> 和 y <sub>2</sub> ，它们对应的权重向量是 w <sup>(1)</sup> 和 w <sup>(2)</sup> 。同样，让它们的偏差分别为 b <sup>(1)</sup> 和 b <sup>(2)</sup> 。假设![ $$ {y}_1=1 $$ ](A448418_1_En_2_Chapter_IEq95.gif)对应的类是正类。

T14

T16![ $$ =\frac{1}{1+{e}^{-{\left({w}^{(1)}-{w}^{(2)}\right)}^Tx-\left({\mathrm{b}}^{(1)}-{\mathrm{b}}^{(2)}\right)}} $$ ](A448418_1_En_2_Chapter_Equr.gif)T18】

我们可以从前面的表达式中看到，两类 SoftMax 的正类概率与 sigmoid 激活函数的表达式相同，唯一的区别是在 sigmoid 中我们只使用一组权重，而在两类 SoftMax 中有两组权重。在 sigmoid 激活函数中，我们不会对两个不同的类使用不同的权重集，所取的权重集通常是正类相对于负类的权重。在 SoftMax 激活函数中，我们为不同的类显式地采用不同的权重集。

由图 [2-12](#Fig12) 表示的 SoftMax 层的损失函数被称为分类交叉熵，由下式给出:

![ $$ C=\sum \limits_{i=1}^k-{y}_i\log P\left({y}_i=1/x\right) $$ ](A448418_1_En_2_Chapter_Equs.gif)

![A448418_1_En_2_Fig12_HTML.gif](A448418_1_En_2_Fig12_HTML.gif)

图 2-12。

SoftMax activation function

#### 整流线性单元(ReLU)激活功能

在整流线性单元中，如图 [2-13](#Fig13) 所示，如果总输入大于 0，则输出等于神经元的净输入；然而，如果总输入小于或等于 0，神经元输出 0。

ReLU 单元的输出可以表示为:

![ $$ y=\mathit{\max}\left(0,{w}^Tx+b\right) $$ ](A448418_1_En_2_Chapter_Equt.gif)

![A448418_1_En_2_Fig13_HTML.gif](A448418_1_En_2_Fig13_HTML.gif)

图 2-13。

Rectified linear unit

ReLU 是彻底改变深度学习的关键元素之一。它们更容易计算。ReLUs 结合了两个世界的优点——它们有一个恒定的梯度，而净输入是正的，其他地方是零梯度。例如，如果我们采用 sigmoid 激活函数，则对于非常大的正值和负值，sigmoid 激活函数的梯度几乎为零，因此神经网络可能会遇到梯度消失的问题。正网络输入的恒定梯度确保梯度下降算法不会因为梯度消失而停止学习。同时，非正净输入的零输出呈现非线性。

整流线性单元激活功能有多种版本，如参数整流线性单元(PReLU)和泄漏整流线性单元。

对于正常的 ReLU 激活函数，对于非正的输入值，输出和梯度都是零，因此训练可以因为零梯度而停止。即使在输入为负的情况下，对于具有非零梯度的模型，PReLU 可能会很有用。预卢激活函数的输入-输出关系由下面给出:

![ $$ y=\max \left(0,z\right)+\beta \min \left(0,z\right) $$ ](A448418_1_En_2_Chapter_Equu.gif)

其中![ $$ z={w}^Tx+b $$ ](A448418_1_En_2_Chapter_IEq96.gif)是预卢激活函数的净输入，β是通过训练学习的参数。

当β设置为![ $$ -1 $$ ](A448418_1_En_2_Chapter_IEq97.gif)时，则![ $$ y=\left|z\right| $$ ](A448418_1_En_2_Chapter_IEq98.gif)和激活函数称为绝对值 ReLU。当β设置为某个较小的正值(通常约为 0.01)时，激活函数称为泄漏 ReLU。

#### Tanh 激活函数

双曲正切激活函数的输入-输出关系(见图 [2-14](#Fig14) )表示为

![ $$ y=\frac{e^z-{e}^{-z}}{e^z+{e}^{-z}} $$ ](A448418_1_En_2_Chapter_Equv.gif)

，其中![ $$ z={w}^Tx+b $$ ](A448418_1_En_2_Chapter_IEq99.gif)为双曲正切激活函数的净输入。

*   当净输入 z 是一个正大数时![ $$ {e}^{-z} $$ ](A448418_1_En_2_Chapter_IEq100.gif) ➤为 0，所以 y ➤为 1。
*   当净输入 z 是一个负数时，e <sup>z</sup> ➤为 0，因此 y ➤为 1。
*   当净输入 z 为 0 时，则![ $$ {e}^{-z} $$ ](A448418_1_En_2_Chapter_IEq101.gif) = 1，因此![ $$ y=0 $$ ](A448418_1_En_2_Chapter_IEq102.gif)。

![A448418_1_En_2_Fig14_HTML.gif](A448418_1_En_2_Fig14_HTML.gif)

图 2-14。

Tanh activation function

正如我们所看到的，tanh 激活函数可以输出-1 到+1 之间的值。

sigmoid 激活函数在输出 0 附近饱和。训练网络时，如果层中的输出接近于零，则梯度消失，训练停止。双曲正切激活函数在输出的-1 和+ 1 值处饱和，并在输出的 0 值附近具有明确定义的梯度。因此，利用双曲正切激活函数，可以在输出 0 附近避免这种消失梯度问题。

### 多层感知器网络的学习规则

在前面的章节中，我们看到感知器学习规则只能学习线性决策边界。非线性复杂决策边界可以通过多层感知器建模；然而，这样的模型不能通过感知器学习规则来学习。因此，人们需要不同的学习算法。

在感知器学习规则中，目标是不断更新模型的权重，直到所有训练数据点都被正确分类。如果没有这样一个可行的权向量来正确地分类所有的点，算法就不会收敛。在这种情况下，可以通过预先定义要训练的遍数(迭代次数)或者通过定义正确分类的训练数据点的数量的阈值来停止算法，在该阈值之后停止训练。

对于多层感知器和大多数深度学习训练网络，训练模型的最佳方式是基于错误分类的误差计算成本函数，然后最小化关于模型参数的成本函数。由于基于成本的学习算法会最小化成本函数，因此对于二元分类(通常是对数损失成本函数)，会使用对数似然函数的负值。作为参考，在第 [1](1.html) 章“逻辑回归”中说明了如何从最大似然法中推导出对数损失成本函数

多层感知器网络将具有隐藏层，并且为了学习非线性决策边界，激活函数本身应该是非线性的，例如 sigmoid、ReLu、tanh 等等。用于二进制分类的输出神经元应该具有 sigmoid 激活函数，以便迎合对数损失成本函数并输出类别的概率值。

现在，根据前面的考虑，让我们尝试通过构建对数损失成本函数来求解 XOR 函数，然后根据模型的权重和偏差参数将其最小化。网络中的所有神经元都具有 sigmoid 激活函数。

参照图 [2-7](#Fig7) ，设隐藏单元 h <sub>1</sub> 处的输入输出分别为 i <sub>1</sub> 和 z <sub>1</sub> 。同样，设隐藏单元 h <sub>2</sub> 处的输入输出分别为 i <sub>2</sub> 和 z <sub>2</sub> 。最后，设输出层 p <sub>1</sub> 的输入和输出分别为 i <sub>3</sub> 和 z <sub>3</sub> 。

![ $$ {i}_1={w}_{11}{x}_1+{w}_{21}{x}_2+{b}_1 $$ ](A448418_1_En_2_Chapter_Equw.gif)

![ $$ {i}_2={w}_{12}{x}_1+{w}_{22}{x}_2+{b}_2 $$ ](A448418_1_En_2_Chapter_Equx.gif)

![ $$ {z}_1=1/\left(1+{e}^{-{i}_1}\right) $$ ](A448418_1_En_2_Chapter_Equy.gif)

![ $$ {z}_2=1/\left(1+{e}^{-{i}_2}\right) $$ ](A448418_1_En_2_Chapter_Equz.gif)

![ $$ {i}_3={w}_1{z}_1+{w}_2{z}_2+{b}_3 $$ ](A448418_1_En_2_Chapter_Equaa.gif)

![ $$ {z}_3=1/\left(1+{e}^{-{i}_3}\right) $$ ](A448418_1_En_2_Chapter_Equab.gif)

考虑对数损失成本函数，XOR 问题的总成本函数可以定义如下:

![ $$ C=\kern0.5em \sum \limits_{i=1}^4-{y}^{(i)}\mathit{\log}{z_3}^{(i)}-\left(1-{y}^{(i)}\right)\log \left(1-{z_3}^{(i)}\right) $$ ](A448418_1_En_2_Chapter_Equac.gif)

如果所有的权重和偏差放在一起可以认为是一个参数向量θ，我们可以通过最小化代价函数 C(θ)来学习模型:

![ $$ {\theta}^{\ast }=\underset{\theta }{\mathrm{Arg} \operatorname {Min}}C\left(\theta \right) $$ ](A448418_1_En_2_Chapter_Equad.gif)

对于最小值，成本函数 C(θ)相对于θ(即![ $$ \nabla C\left(\theta \right) $$ ](A448418_1_En_2_Chapter_IEq103.gif))的梯度应该为零。通过梯度下降法可以达到最小值。梯度下降的更新规则为![ $$ {\theta}^{\left(t+1\right)} $$ ](A448418_1_En_2_Chapter_IEq104.gif) = ![ $$ {\theta}^{(t)}-\upeta \nabla C\left({\theta}^{(t)}\right) $$ ](A448418_1_En_2_Chapter_IEq105.gif)，其中η为学习率，![ $$ {\theta}^{\left(t+1\right)} $$ ](A448418_1_En_2_Chapter_IEq106.gif)和θ <sup>(t)</sup> 分别为迭代![ $$ t+1 $$ ](A448418_1_En_2_Chapter_IEq107.gif)和 t 时的参数向量。

如果我们考虑参数向量中的个体权重，梯度下降更新规则变成如下:

![ $$ {w_k}^{\left(t+1\right)}={w_k}^{(t)}-\upeta \frac{\partial C\left({w_k}^{(t)}\right)}{\partial {w}_k}\kern0.75em \forall {w}_k\in \theta $$ ](A448418_1_En_2_Chapter_Equae.gif)

梯度向量不会像线性或逻辑回归中那样容易计算，因为在神经网络中，权重遵循等级顺序。然而，导数的链式法则提供了一些简化，以系统地计算相对于权重(包括偏差)的偏导数。

这种方法称为反向传播，它简化了梯度计算。

### 梯度计算的反向传播

反向传播是一种将输出层的误差反向传播的有用方法，因此可以使用导数的链式法则轻松计算前面层的梯度。

让我们考虑一个训练示例，并通过反向传播，将 XOR 网络结构考虑在内(见图 [2-8](#Fig8) )。设输入为![ $$ x={\left[{x}_1\ {x}_2\right]}^T $$ ](A448418_1_En_2_Chapter_IEq108.gif)，对应的类为 y，则单条记录的代价函数为:

![ $$ C=- ylog{z}_3-\left(1-y\right)\ \log \left(1-{z}_3\right) $$ ](A448418_1_En_2_Chapter_Equaf.gif)

![ $$ \frac{\partial C}{\partial {w}_1}=\frac{dC}{d{z}_3}\frac{d{z}_3}{d{i}_3}\frac{\partial {i}_3}{\partial {w}_1} $$ ](A448418_1_En_2_Chapter_Equag.gif)

![ $$ \frac{dC}{d{z}_3}=\frac{\left({z}_3-y\right)}{z_3\left(1-{z}_3\right)} $$ ](A448418_1_En_2_Chapter_Equah.gif)

![ $$ {z}_3=1/\left(1+{e}^{-{z}_3}\right) $$ ](A448418_1_En_2_Chapter_Equ1.gif)

![ $$ \frac{d{z}_3}{d{i}_3}={z}_3\left(1-{z}_3\right) $$ ](A448418_1_En_2_Chapter_Equai.gif)

![ $$ \frac{dC}{d{i}_3}=\frac{dC}{d{z}_3}\frac{d{z}_3}{d{i}_3}=\frac{\left({z}_3-y\right)}{z_3\left(1-{z}_3\right)}{z}_3\left(1-{z}_3\right)=\left({z}_3-y\right) $$ ](A448418_1_En_2_Chapter_Equaj.gif)

正如我们所看到的，成本函数相对于最终层中的净输入的导数只不过是估计输出的误差![ $$ \left({z}_3-y\right) $$ ](A448418_1_En_2_Chapter_IEq109.gif) :

![ $$ \frac{\partial {i}_3}{\partial {w}_1}={z}_1 $$ ](A448418_1_En_2_Chapter_Equak.gif)

![ $$ \frac{\partial C}{\partial {w}_1}=\frac{dC}{d{z}_3}\frac{d{z}_3}{d{i}_3}\frac{\partial {i}_3}{\partial {w}_1}=\left({z}_3-y\right){z}_1 $$ ](A448418_1_En_2_Chapter_Equal.gif)

同样，

![ $$ \frac{\partial C}{\partial {w}_2}=\frac{dC}{d{z}_3}\frac{d{z}_3}{d{i}_3}\frac{\partial {i}_3}{\partial {w}_2}=\left({z}_3-y\right){z}_2 $$ ](A448418_1_En_2_Chapter_Equam.gif)

![ $$ \frac{\partial C}{\partial {b}_3}=\frac{dC}{d{z}_3}\frac{d{z}_3}{d{i}_3}\frac{\partial {i}_3}{\partial {b}_3}=\left({z}_3-y\right) $$ ](A448418_1_En_2_Chapter_Equan.gif)

现在，让我们计算成本函数相对于前一层中的权重的偏导数:

![ $$ \frac{\partial C}{\partial {z}_1}=\frac{dC}{d{z}_3}\frac{d{z}_3}{d{i}_3}\frac{\partial {i}_3}{\partial {z}_1}=\left({z}_3-y\right){w}_1 $$ ](A448418_1_En_2_Chapter_Equao.gif)

![ $$ \frac{\partial C}{\partial {z}_1} $$ ](A448418_1_En_2_Chapter_IEq110.gif)可视为相对于隐含层单元 h <sub>1</sub> 的输出的误差。误差的传播与将输出单元连接到隐藏层单元的权重成比例。如果有多个输出单元，那么![ $$ \frac{\partial C}{\partial {z}_1} $$ ](A448418_1_En_2_Chapter_IEq111.gif)将有来自每个输出单元的贡献。我们将在下一节中详细了解这一点。

同样，

![ $$ \frac{\partial C}{\partial {i}_1}=\frac{dC}{d{z}_3}\frac{d{z}_3}{d{i}_3}\frac{\partial {i}_3}{\partial {z}_1}\frac{d{z}_1}{d{i}_1}=\left({z}_3-y\right){w}_1{z}_1\left(1-{z}_1\right) $$ ](A448418_1_En_2_Chapter_Equap.gif)

![ $$ \frac{\partial C}{\partial {i}_1} $$ ](A448418_1_En_2_Chapter_IEq112.gif)可以认为是相对于隐含层单元 h <sub>1</sub> 的网络输入的误差。它可以通过将![ $$ {z}_1\left(1-{z}_1\right) $$ ](A448418_1_En_2_Chapter_IEq113.gif)因子乘以![ $$ \frac{\partial C}{\partial {z}_1} $$ ](A448418_1_En_2_Chapter_IEq114.gif):

![ $$ \frac{\partial C}{\partial {w}_{11}}=\frac{dC}{d{z}_3}\frac{d{z}_3}{d{i}_3}\frac{\partial {i}_3}{\partial {z}_1}\frac{d{z}_1}{d{i}_1}\frac{\partial {i}_i}{\partial {w}_{11}}=\left({z}_3-y\right){w}_1{z}_1\left(1-{z}_1\right){x}_1 $$ ](A448418_1_En_2_Chapter_Equaq.gif)

![ $$ \frac{\partial C}{\partial {w}_{21}}=\frac{dC}{d{z}_3}\frac{d{z}_3}{d{i}_3}\frac{\partial {i}_3}{\partial {z}_1}\frac{d{z}_1}{d{i}_1}\frac{\partial {i}_i}{\partial {w}_{21}}=\left({z}_3-y\right){w}_1{z}_1\left(1-{z}_1\right){x}_2 $$ ](A448418_1_En_2_Chapter_Equar.gif)

![ $$ \frac{\partial C}{\partial {b}_1}=\frac{dC}{d{z}_3}\frac{d{z}_3}{d{i}_3}\frac{\partial {i}_3}{\partial {z}_1}\frac{d{z}_1}{d{i}_1}\frac{\partial {i}_i}{\partial {w}_{21}}=\left({z}_3-y\right){w}_1{z}_1\left(1-{z}_1\right) $$ ](A448418_1_En_2_Chapter_Equas.gif)

来计算

一旦我们有了成本函数相对于每个神经元单元中的输入的偏导数，我们就可以计算成本函数相对于对输入有贡献的权重的偏导数——我们只需要乘以来自该权重的输入。

### 梯度计算中反向传播方法的推广

在这一节中，我们试图通过一个更复杂的网络来推广反向传播方法。我们假设最终输出层由三个独立的 sigmoid 输出单元组成，如图 [2-15](#Fig15) 所示。此外，为了便于标记和简化学习，我们假设网络只有一条记录。

![A448418_1_En_2_Fig15_HTML.gif](A448418_1_En_2_Fig15_HTML.gif)

图 2-15。

Network to illustrate backpropagation for independent sigmoid output layers

单个输入记录的成本函数由以下给出:

![ $$ C=\sum \limits_{i=1}^3-{y}_i\log P\left({y}_i=1\right)-\left(1-{y}_i\right)\log \left(1-P\left({y}_i=1\right)\right) $$ ](A448418_1_En_2_Chapter_Equat.gif)

![ $$ =\sum \limits_{i=1}^3-{y}_i\log {\mathrm{z}}_i^{(3)}-\left(1-{y}_i\right)\log \left(1-{\mathrm{z}}_i^{(3)}\right) $$ ](A448418_1_En_2_Chapter_Equau.gif)

在前面的表达式中，![ $$ {y}_i\in \left\{0,1\right\} $$ ](A448418_1_En_2_Chapter_IEq115.gif)，取决于特定于 y <sub>i</sub> 的事件是否活动。

![ $$ P\left({y}_i=1\right)={\mathrm{z}}_i^{(3)} $$ ](A448418_1_En_2_Chapter_IEq116.gif)表示第 I 类的预测概率。

让我们计算成本函数相对于权重 w <sub>ji</sub> <sup>(2)</sup> 的偏导数。权重只会影响网络的第 I 个输出单元的输出。

![ $$ \frac{\partial C}{\partial {w}_{ji}^{(2)}}=\frac{\partial C}{\partial {\mathrm{z}}_i^{(3)}}\frac{\partial {\mathrm{z}}_i^{(3)}}{\partial {\mathrm{s}}_i^{(3)}}\frac{\partial {\mathrm{s}}_i^{(3)}}{\partial {w}_{ji}^{(2)}} $$ ](A448418_1_En_2_Chapter_Equav.gif)

![ $$ \frac{\partial C}{\partial {\mathrm{z}}_i^{(3)}}=\frac{\left({\mathrm{z}}_i^{(3)}-{y}_i\right)}{{\mathrm{z}}_i^{(3)}\left(1-{\mathrm{z}}_i^{(3)}\right)} $$ ](A448418_1_En_2_Chapter_Equaw.gif)

![ $$ P\left({y}_i=1\right)={\mathrm{z}}_i^{(3)}=1/\left(1+{e}^{-{\mathrm{s}}_i^{(3)}}\right) $$ ](A448418_1_En_2_Chapter_Equax.gif)

![ $$ \frac{\partial {\mathrm{z}}_i^{(3)}}{\partial {\mathrm{s}}_i^{(3)}}={\mathrm{z}}_i^{(3)}\left(1-{\mathrm{z}}_i^{(3)}\right) $$ ](A448418_1_En_2_Chapter_Equay.gif)

![ $$ \frac{\partial C}{\partial {\mathrm{s}}_i^{(3)}}=\frac{\partial C}{\partial {\mathrm{z}}_i^{(3)}}\frac{\partial {\mathrm{z}}_i^{(3)}}{\partial {\mathrm{s}}_i^{(3)}}=\frac{\left({\mathrm{z}}_i^{(3)}-{y}_i\right)}{{\mathrm{z}}_i^{(3)}\left(1-{\mathrm{z}}_i^{(3)}\right)}{\mathrm{z}}_i^{(3)}\left(1-{\mathrm{z}}_i^{(3)}\right)=\left({\mathrm{z}}_i^{(3)}-{y}_i\right) $$ ](A448418_1_En_2_Chapter_Equaz.gif)

因此，如前所述，成本函数相对于第 I 个输出单元的净输入的偏导数是![ $$ \left({\mathrm{z}}_i^{(3)}-{y}_i\right) $$ ](A448418_1_En_2_Chapter_IEq117.gif)，这仅仅是第 I 个输出单元的预测误差。

![ $$ \frac{\partial {\mathrm{s}}_i^{(3)}}{\partial {w}_{ji}^{(2)}}={\mathrm{z}}_j^{(2)} $$ ](A448418_1_En_2_Chapter_Equba.gif)

结合![ $$ \frac{\partial C}{\partial {\mathrm{s}}_i^{(3)}} $$ ](A448418_1_En_2_Chapter_IEq118.gif)和![ $$ \frac{\partial {\mathrm{s}}_i^{(3)}}{\partial {w}_{ji}^{(2)}} $$ ](A448418_1_En_2_Chapter_IEq119.gif)，我们得到，

![ $$ \frac{\partial C}{\partial {w}_{ji}^{(2)}}=\left({\mathrm{z}}_i^{(3)}-{y}_i\right){\mathrm{z}}_j^{(2)} $$ ](A448418_1_En_2_Chapter_Equbb.gif)

![ $$ \frac{\partial C}{\partial {b}_i^{(2)}}=\left({\mathrm{z}}_i^{(3)}-{y}_i\right) $$ ](A448418_1_En_2_Chapter_Equbc.gif)

前面给出了成本函数相对于网络最后一层中的权重和偏差的偏导数的通用表达式。接下来，让我们计算较低层中权重和偏差的偏导数。事情变得有点复杂，但仍然遵循一个普遍的趋势。让我们计算成本函数相对于权重 w <sub>kj</sub> <sup>(1)</sup> 的偏导数。重量会受到所有三个输出单元的误差的影响。基本上，隐藏层中第 j 个单元的输出端的误差将具有来自所有输出单元的误差贡献，该误差贡献由将输出层连接到第 j 个隐藏单元的权重来缩放。让我们用链式法则计算偏导数，看看它是否符合我们所说的:

![ $$ \frac{\partial C}{\partial {w}_{kj}^{(1)}}=\frac{\partial C}{\partial {\mathrm{z}}_j^{(2)}}\frac{\partial {\mathrm{z}}_j^{(2)}}{\partial {\mathrm{s}}_j^{(2)}}\frac{\partial {\mathrm{s}}_j^{(2)}}{\partial {w}_{kj}^{(1)}} $$ ](A448418_1_En_2_Chapter_Equbd.gif)

![ $$ \frac{\partial {\mathrm{s}}_j^{(2)}}{\partial {w}_{kj}^{(1)}}={\mathrm{z}}_k^{(1)} $$ ](A448418_1_En_2_Chapter_Eqube.gif)

![ $$ \frac{\partial {\mathrm{z}}_j^{(2)}}{\partial {\mathrm{s}}_j^{(2)}}={\mathrm{z}}_j^{(2)}\left(1-{\mathrm{z}}_j^{(2)}\right) $$ ](A448418_1_En_2_Chapter_Equbf.gif)

现在，![ $$ \frac{\partial C}{\partial {\mathrm{z}}_j^{(2)}} $$ ](A448418_1_En_2_Chapter_IEq120.gif)是棘手的计算自 z<sub>j</sub>T3】(2)影响所有三个输出单元:

![ $$ \frac{\partial C}{\partial {\mathrm{z}}_j^{(2)}}=\sum \limits_{i=1}^3\frac{\partial C}{\partial {\mathrm{z}}_i^{(3)}}\frac{\partial {\mathrm{z}}_i^{(3)}}{\partial {\mathrm{s}}_i^{(3)}}\frac{\partial {\mathrm{s}}_i^{(3)}}{\partial {\mathrm{z}}_j^{(2)}} $$ ](A448418_1_En_2_Chapter_Equbg.gif)

![ $$ =\kern0.5em \sum \limits_{i=1}^3\left({\mathrm{z}}_i^{(3)}-{y}_i\right){w}_{ji}^{(2)} $$ ](A448418_1_En_2_Chapter_Equbh.gif)

结合![ $$ \frac{\partial {\mathrm{s}}_j^{(2)}}{\partial {w}_{kj}^{(1)}},\frac{\partial {\mathrm{z}}_j^{(2)}}{\partial {\mathrm{s}}_j^{(2)}} $$ ](A448418_1_En_2_Chapter_IEq121.gif)和![ $$ \frac{\partial C}{\partial {\mathrm{z}}_j^{(2)}} $$ ](A448418_1_En_2_Chapter_IEq122.gif)的表达，我们有

![ $$ \frac{\partial C}{\partial {w}_{kj}^{(1)}}=\sum \limits_{i=1}^3\left({\mathrm{z}}_i^{(3)}-{y}_i\right){w}_{ji}^{(2)}{\mathrm{z}}_j^{(2)}\left(1-{\mathrm{z}}_j^{(2)}\right){\mathrm{x}}_k^{(1)} $$ ](A448418_1_En_2_Chapter_Equbi.gif)

通常，对于多层神经网络，为了计算成本函数 C 相对于对神经元单元中的净输入 s 有贡献的特定权重 w 的偏导数，我们需要计算成本函数相对于净输入(即![ $$ \frac{\partial C}{\partial s} $$ ](A448418_1_En_2_Chapter_IEq123.gif))的偏导数，然后乘以与权重 w 相关联的输入 x，如下:

![ $$ \frac{\partial C}{\partial w}=\frac{\partial C}{\partial s}\frac{\partial s}{\partial w}=\frac{\partial C}{\partial s}x $$ ](A448418_1_En_2_Chapter_Equbj.gif)

可以认为是神经单元的误差，并且可以通过将输出层的误差传递给较低层的神经单元来迭代计算。另一点要注意的是，较高层神经单元中的误差被分配到前一层神经单元的输出，与它们之间的权重连接成比例。同样，成本函数相对于 sigmoid 激活神经元的净输入的偏导数![ $$ \frac{\partial C}{\partial s} $$ ](A448418_1_En_2_Chapter_IEq125.gif)可以通过将![ $$ \frac{\partial C}{\partial z\ } $$ ](A448418_1_En_2_Chapter_IEq127.gif)乘以![ $$ z\left(1-z\right) $$ ](A448418_1_En_2_Chapter_IEq128.gif)从成本函数相对于神经元的输出 z(即![ $$ \frac{\partial C}{\partial z} $$ ](A448418_1_En_2_Chapter_IEq126.gif))的偏导数来计算。对于线性神经元，这个倍增因子变成 1。

神经网络的所有这些特性使得计算梯度变得容易。这就是神经网络通过反向传播在每次迭代中学习的方式。

每次迭代由一次正向传递和一次反向传递或反向传播组成。在前向传递中，计算每层中每个神经元单元的净输入和输出。基于预测输出和实际目标值，在输出层中计算误差。通过将误差与前向传递中计算的神经元输出和现有权重相结合，误差被反向传播。通过反向传播，梯度得到迭代计算。一旦计算出梯度，就通过梯度下降法更新权重。

请注意，所示的扣除对 sigmoid 激活功能有效。对于其他激活功能，虽然方法保持不变，但在实现中需要针对激活功能进行特定的更改。

SoftMax 函数的成本函数不同于独立多类分类的成本函数。

![A448418_1_En_2_Fig16_HTML.gif](A448418_1_En_2_Fig16_HTML.gif)

图 2-16。

Network to illustrate backpropagation for Softmax output layer

图 [2-16](#Fig16) 所示网络中 SoftMax 激活层的交叉熵代价由

![ $$ C=\sum \limits_{i=1}^3-{y}_i\log P\left({y}_i=1\right)=\kern0.5em \sum \limits_{i=1}^3-{y}_i\log {z}_i^{(3)} $$ ](A448418_1_En_2_Chapter_Equbk.gif)

给出

让我们计算成本函数相对于权重 w <sub>ji</sub> <sup>(2)</sup> 的偏导数。现在，权重将影响第 I 个 SoftMax 单元的净输入 s<sub>I</sub>T6(3)。然而，与早期网络中的独立二进制激活不同，这里所有三个 SoftMax 输出单元![ $$ {z}_k^{(3)}\forall k\in \left\{1,2,3\right\} $$ ](A448418_1_En_2_Chapter_IEq129.gif)将受到 s<sub>I</sub>T11】(3)的影响，因为

![ $$ {z}_k^{(3)}=\frac{e^{s_k^{(3)}}}{\sum_{l=1}^3{e}^{s_l^{(3)}}}=\frac{e^{s_k^{(3)}}}{\sum_{l\ne i}{e}^{s_l^{(3)}}+{e}^{s_i^{(3)}}} $$ ](A448418_1_En_2_Chapter_Equbl.gif)

因此，导数![ $$ \frac{\partial C}{\partial {w}_{ji}^{(2)}} $$ ](A448418_1_En_2_Chapter_IEq130.gif)可以写成:

![ $$ \frac{\partial C}{\partial {w}_{ji}^{(2)}}=\frac{\partial C}{\partial {s}_i^{(3)}}\frac{\partial {s}_i^{(3)}}{\partial {w}_{ji}^{(2)}} $$ ](A448418_1_En_2_Chapter_Equbm.gif)

现在，正如刚才所说的，由于 s <sub>i</sub> <sup>(3)</sup> 影响所有的输出 z <sub>k</sub> <sup>(3)</sup> 在 SoftMax 层，

![ $$ \frac{\partial C}{\partial {s}_i^{(3)}}=\sum \limits_{k=1}^3\frac{\partial C}{\partial {z}_k^{(3)}}\frac{\partial {z}_k^{(3)}}{\partial {s}_i^{(3)}} $$ ](A448418_1_En_2_Chapter_Equbn.gif)

偏导数的各个分量如下:

![ $$ \frac{\partial C}{\partial {z}_k^{(3)}}=\frac{-{y}_k}{z_k^{(3)}} $$ ](A448418_1_En_2_Chapter_Equbo.gif)

对于![ $$ k=i $$ ](A448418_1_En_2_Chapter_IEq131.gif)、![ $$ \frac{\partial {z}_k^{(3)}}{\partial {s}_i^{(3)}}={z}_i^{(3)}\left(1-{z}_i^{(3)}\right) $$ ](A448418_1_En_2_Chapter_IEq132.gif)

![ $$ k\ne i,\kern0.5em \frac{\partial {z}_k^{(3)}}{\partial {s}_i^{(3)}}=-{z}_i^{(3)}{z}_k^{(3)} $$ ](A448418_1_En_2_Chapter_IEq133.gif)

![ $$ \frac{\partial {s}_i^{(3)}}{\partial {w}_{ji}^{(2)}}={z}_j^{(2)} $$ ](A448418_1_En_2_Chapter_Equbp.gif)

![ $$ \frac{\partial C}{\partial {s}_i^{(3)}}=\sum \limits_{k=1}^3\frac{\partial C}{\partial {z}_k^{(3)}}\frac{\partial {z}_k^{(3)}}{\partial {s}_i^{(3)}}=\sum \limits_{k=i}\frac{\partial C}{\partial {z}_k^{(3)}}\frac{\partial {z}_k^{(3)}}{\partial {s}_i^{(3)}}+\sum \limits_{k\ne i}\frac{\partial C}{\partial {z}_k^{(3)}}\frac{\partial {z}_k^{(3)}}{\partial {s}_i^{(3)}} $$ ](A448418_1_En_2_Chapter_Equbq.gif)

![ $$ =\frac{-{y}_i}{z_i^{(3)}}{z}_i^{(3)}\left(1-{z}_i^{(3)}\right)+\sum \limits_{k\ne i}\frac{-{y}_k}{z_k^{(3)}}\left(-{z}_i^{(3)}{z}_k^{(3)}\right) $$ ](A448418_1_En_2_Chapter_Equbr.gif)

![ $$ =-{y}_i\left(1-{z}_i^{(3)}\right)+{z}_i^{(3)}\sum \limits_{k\ne i}{y}_k $$ ](A448418_1_En_2_Chapter_Equbs.gif)

![ $$ =-{y}_i+{y}_i{z}_i^{(3)}+{z}_i^{(3)}\sum \limits_{k\ne i}{y}_k $$ ](A448418_1_En_2_Chapter_Equbt.gif)

![ $$ =-{y}_i+{z}_i^{(3)}\sum \limits_k{y}_k $$ ](A448418_1_En_2_Chapter_Equbu.gif)

![ $$ =-{y}_i+{z}_i^{(3)}\ Since only\ one\ of the\ {y}_k\ can\ be\ 1\therefore \sum \limits_k{y}_k=1 $$ ](A448418_1_En_2_Chapter_Equbv.gif)

![ $$ =\left({z}_i^{(3)}-{y}_i\right) $$ ](A448418_1_En_2_Chapter_Equbw.gif)

事实证明，相对于第 I 个 SoftMax 单元的净输入的成本导数是预测第 I 个 SoftMax 输出单元的输出的误差。结合![ $$ \frac{\partial C}{\partial {s}_i^{(3)}} $$ ](A448418_1_En_2_Chapter_IEq134.gif)和![ $$ \frac{\partial {s}_i^{(3)}}{\partial {w}_{ji}^{(2)}} $$ ](A448418_1_En_2_Chapter_IEq135.gif)，我们得到如下:

![ $$ \frac{\partial C}{\partial {w}_{ji}^{(2)}}=\frac{\partial C}{\partial {s}_i^{(3)}}\frac{\partial {s}_i^{(3)}}{\partial {w}_{ji}^{(2)}}=\left({z}_i^{(3)}-{y}_i\right){z}_j^{(2)} $$ ](A448418_1_En_2_Chapter_Equbx.gif)

类似地，对于第 I 个 SoftMax 输出单元的偏置项，我们有如下:

![ $$ \frac{\partial C}{\partial {b}_i^{(2)}}=\left({\mathrm{z}}_i^{(3)}-{y}_i\right) $$ ](A448418_1_En_2_Chapter_Equby.gif)

计算成本函数相对于前一层，即![ $$ \frac{\partial C}{\partial {w}_{kj}^{(1)}} $$ ](A448418_1_En_2_Chapter_IEq136.gif)中的权重 w<sub>kj</sub><sup>【1】</sup>的偏导数，将具有与具有独立二进制类的网络的情况相同的形式。这是显而易见的，因为网络仅在输出单元的激活函数方面不同，即使如此，我们得到的![ $$ \frac{\partial C}{\partial {s}_i^{(3)}} $$ ](A448418_1_En_2_Chapter_IEq137.gif)和![ $$ \frac{\partial {s}_i^{(3)}}{\partial {w}_{ji}^{(2)}} $$ ](A448418_1_En_2_Chapter_IEq138.gif)的表达式仍然相同。作为练习，感兴趣的读者可以验证![ $$ \frac{\partial C}{\partial {w}_{kj}^{(1)}}=\sum \limits_{i=1}^3\left({\mathrm{z}}_i^{(3)}-{y}_i\right){w}_{ji}^{(2)}{\mathrm{z}}_j^{(2)}\left(1-{\mathrm{z}}_j^{(2)}\right){\mathrm{x}}_k^{(1)} $$ ](A448418_1_En_2_Chapter_IEq139.gif)是否仍然成立。

#### 深度学习与传统方法

在本书中，我们将使用谷歌的 TensorFlow 作为深度学习库，因为它有几个优点。在继续讨论 TensorFlow 之前，让我们看看深度学习的一些关键优势，以及如果它没有用在正确的地方，它的一些缺点。

*   深度学习在几个领域远远超过了传统的机器学习方法，特别是在计算机视觉、语音识别、自然语言处理和时间序列领域。
*   通过深度学习，随着深度学习神经网络中层数的增加，可以学习越来越复杂的特征。由于这种自动特征学习属性，深度学习减少了特征工程时间，这在传统的机器学习方法中是一种耗时的活动。
*   深度学习最适合非结构化数据，并且存在大量图像、文本、语音、传感器数据等形式的非结构化数据，这些数据在分析时将彻底改变不同的领域，如医疗保健、制造、银行、航空、电子商务等。

深度学习的一些限制如下:

![A448418_1_En_2_Fig17_HTML.gif](A448418_1_En_2_Fig17_HTML.gif)

图 2-17。

Performance comparison of traditional methods versus deep-learning methods

*   深度学习网络通常倾向于具有大量参数，对于这样的实现，应该有足够大的数据量来训练。如果没有足够的数据，深度学习方法将不会很好地工作，因为模型将遭受过度拟合。
*   深度学习网络学习到的复杂特征通常很难解释。
*   由于模型中的大量权重以及数据量，深度学习网络需要大量的计算能力来训练。

当数据量较少时，传统方法往往比深度学习方法表现更好。然而，当数据量巨大时，深度学习方法以巨大的优势战胜传统方法，这已经在图 [2-17](#Fig17) 中粗略描绘。

## TensorFlow

Google 的 TensorFlow 是一个开源库，主要专注于深度学习。它使用计算数据流图来表示复杂的神经网络架构。图中的节点表示数学计算，也称为 ops(运算)，而边表示它们之间传输的数据张量。此外，相关梯度存储在计算图的每个节点处，并且在反向传播期间，这些梯度被组合以获得关于每个权重的梯度。张量是 TensorFlow 使用的多维数据数组。

### 常见的深度学习包

常见的深度学习包如下:

*   torch——一个科学计算框架，使用底层 C 实现和 LuaJIT 作为脚本语言。Torch 最初发布于 2002 年。Torch 运行的操作系统有 Linux、Android、Mac OS X 和 iOS。著名的组织如脸书人工智能研究所和 IBM 使用火炬。Torch 可以利用 GPU 进行快速计算。
*   the ano–是 Python 中的深度学习包，主要用于计算密集型的研究型活动。它与 Numpy 数组紧密集成，并具有高效的符号区分器。它还提供了对 GPU 的透明使用，以加快计算速度。
*   caffe——由 Berkeley AI Research ( [BAIR](http://bair.berkeley.edu/) )开发的深度学习框架。速度使 Caffe 成为研究实验和工业部署的完美选择。Caffe 实现可以非常高效的使用 GPU。
*   cud nn–cud nn 代表 CUDA 深度神经网络库。它为深度神经网络的 GPU 实现提供了一个原语库。
*   tensor flow——谷歌的开源深度学习框架，灵感来自 Theano。TensorFlow 正在慢慢成为研究型工作和生产实施中深度学习的首选库。此外，对于云上的分布式生产实施，TensorFlow 正在成为首选库。
*   MxNet——开源深度学习框架，可以扩展到多个 GPU 和机器。由 AWS、Azure 等主要云提供商支持。流行的机器学习库 GraphLab 使用 MxNet 实现了很好的深度学习。
*   deep learning 4j——面向 Java 虚拟机的开源分布式深度学习框架。

这些深度学习框架的一些显著特征如下:

*   Python 是 TensorFlow 和 Theano 选择的高级语言，而 Lua 是 Torch 选择的高级语言。MxNet 也有 Python APIs。
*   TensorFlow 和 Theano 在性质上非常相似。TensorFlow 对分布式系统的支持更好。Theano 是一个学术项目，而 TensorFlow 是由谷歌资助的。
*   TensorFlow、Theano、MxNet 和 Caffe 都使用自动微分器，而 Torch 使用 AutoGrad。自动微分器不同于符号微分和数值微分。自动微分器在神经网络中使用时非常有效，因为反向传播学习方法利用了微分的链式法则。
*   对于云上的生产实现，TensorFlow 正在成为面向大型分布式系统的应用程序的首选平台。

### tensorflow 安装

TensorFlow 可以轻松安装在基于 Linux、Mac OS 和 Windows 的机器上。为 TensorFlow 创建单独的环境总是更可取。需要注意的一点是，在 Windows 中安装 TensorFlow 需要您的 Python 版本大于或等于 3.5。就此而言，基于 Linux 的机器或 Mac OS 不存在这样的限制。基于 Windows 的机器的安装细节在 TensorFlow 的官方网站上有很好的记录: [`https://www.tensorflow.org/install/install_windows`](https://www.tensorflow.org/install/install_windows) 。基于 Linux 的机器和 Mac OS 的安装链接是:

[T2`https://www.tensorflow.org/install/install_linux`](https://www.tensorflow.org/install/install_linux)

[T2`https://www.tensorflow.org/install/install_mac`](https://www.tensorflow.org/install/install_mac)

### TensorFlow 开发基础

TensorFlow 有自己的命令格式来定义和操作张量。此外，TensorFlow 在激活的会话中执行计算图形。清单 [2-1](#Par184) 到 [2-15](#Par203) 是一些基本的张量流命令，用于定义张量和张量流变量，并在会话中执行张量流计算图。

```
import tensorflow as tf
import numpy as np
Listing 2-1.Import TensorFlow and Numpy Library

```

```
tf.InteractiveSession()
Listing 2-2.Activate a TensorFlow Interactive Session

```

```
a = tf.zeros((2,2));
b = tf.ones((2,2))
Listing 2-3.Define Tensors

```

```
tf.reduce_sum(b,reduction_indices = 1).eval()

-- output --
array([ 2., 2.], dtype=float32)

Listing 2-4.Sum the Elements of the Matrix (2D Tensor) Across the Horizontal Axis

```

要在交互模式下运行张量流命令，可以调用清单 [2-2](#Par185) 中的`Interactive Session()`命令，通过使用`eval()`方法，可以在激活的交互会话下运行张量流命令，如清单 [2-4](#Par187) 所示。

```
a.get_shape()

-- output --

TensorShape([Dimension(2), Dimension(2)])

Listing 2-5.
Check

the Shape of the Tensor

```

```
tf.reshape(a,(1,4)).eval()

-- output --

array([[ 0., 0., 0., 0.]], dtype=float32)

Listing 2-6.Reshape a Tensor

```

```
ta = tf.zeros((2,2))
print(ta)

-- output --

Tensor("zeros_1:0", shape=(2, 2), dtype=float32)

print(ta.eval())

-- output --

[[ 0\.  0.]
 [ 0\.  0.]]

a = np.zeros((2,2))
print(a)

-- output --

[[ 0\.  0.]
 [ 0\.  0.]]

Listing 2-7.
Explicit Evaluation

in TensorFlow and Difference with Numpy

```

```
a = tf.constant(1)
b = tf.constant(5)
c= a*b
Listing 2-8.Define TensorFlow Constants

```

```
with tf.Session() as sess:
    print(c.eval())
    print(sess.run(c))

-- output --

5
5

Listing 2-9.
TensorFlow Session

for Execution of the Commands Through Run and Eval

```

```
w = tf.Variable(tf.ones(2,2),name='weights')
Listing 2-10a.Define TensorFlow Variables

```

```
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(w))

-- output --

[[ 1\.  1.]
 [ 1\.  1.]]

Listing 2-10b.Initialize the Variables

After Invoking the Session

```

TensorFlow 会话一般通过清单 [2-10b](#Par195) 所示的`tf.Session()` `as`激活，计算图操作(ops)在激活的会话下执行。

```
rw = tf.Variable(tf.random_normal((2,2)),name='random_weights')
Listing 2-11a.Define the TensorFlow Variable with Random Initial Values

from Standard Normal Distribution

```

```
with tf.Session()as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(rw))

-- output --

[[ 0.37590656 -0.11246648]
 [-0.61900514 -0.93398571]]

Listing 2-11b.Invoke Session and Display

the Initial State of the Variable

```

如清单 [2-11b](#Par198) 所示，`run`方法用于在激活的会话中执行计算操作(ops)，当`run`初始化定义的张量流变量时使用`tf.global_variables_initializer()`。2-11a 中定义的随机变量`rw`在清单 [2-11b](#Par198) 中被初始化。

```
var_1 = tf.Variable(0,name='var_1')
add_op =  tf.add(var_1,tf.constant(1))
upd_op = tf.assign(var_1,add_op)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in xrange(5):
        print(sess.run(upd_op))

-- output --

1
2
3
4
5

Listing 2-12.TensorFlow Variable State Update

```

```
x = tf.constant(1)
y = tf.constant(5)
z = tf.constant(7)

mul_x_y = x*y
final_op = mul_x_y + z

with tf.Session() as sess:
    print(sess.run([mul_x_y,final_op]))

-- output --

      5 12

Listing 2-13.Display the TensorFlow Variable State

```

```
a = np.ones((3,3))
b = tf.convert_to_tensor(a)
with tf.Session() as sess:
    print(sess.run(b))

-- output --

[[ 1\.  1\.  1.]
 [ 1\.  1\.  1.]
 [ 1\.  1\.  1.]]

Listing 2-14.Convert a Numpy Array to Tensor

```

```
inp1 = tf.placeholder(tf.float32,shape=(1,2))
inp2 = tf.placeholder(tf.float32,shape=(2,1))
output = tf.matmul(inp1,inp2)
with tf.Session() as sess:
    print(sess.run([output],feed_dict={inp1:[[1.,3.]],inp2:[[1],[3]]}))

-- output --

[array([[ 10.]], dtype=float32)]

Listing 2-15.
Placeholders and Feed Dictionary

```

TensorFlow 占位符定义了一个变量，该变量的数据将在稍后的时间点进行赋值。运行涉及 TensorFlow 占位符的 ops 时，数据通常通过`feed_dict`传递给占位符。清单 [2-15](#Par203) 对此进行了说明。

### 深度学习视角下的梯度下降优化方法

在我们深入研究 TensorFlow 优化器之前，重要的是要了解关于整批梯度下降和随机梯度下降的几个关键点，包括它们的缺点，以便人们能够理解提出这些基于梯度的优化器的变体的需要。

#### 椭圆形轮廓

具有最小平方误差的线性神经元的成本函数是二次的。当成本函数为二次型时，全批次梯度下降法得到的梯度方向给出了线性意义下成本降低的最佳方向，但它并不指向最小值，除非成本函数的不同椭圆轮廓是圆。在长椭圆轮廓的情况下，梯度分量在需要较少变化的方向上可能较大，而在需要更多变化以移动到最小点的方向上可能较小。

如图 [2-18](#Fig18) 所示，S 处的梯度并不指向最小值的方向；即点 m。这种情况的问题是，如果我们通过使学习率变小来采取小的步骤，那么梯度下降将需要一段时间来收敛，而如果我们使用大的学习率，梯度将在成本函数具有曲率的方向上快速改变方向，导致振荡。多层神经网络的成本函数不是二次函数，而是平滑函数。局部地，这种非二次成本函数可以用二次函数来近似，因此椭圆轮廓固有的梯度下降问题对于非二次成本函数仍然普遍存在。

![A448418_1_En_2_Fig18_HTML.gif](A448418_1_En_2_Fig18_HTML.gif)

图 2-18。

Contour plot for a quadratic cost function with elliptical contours

解决这个问题的最好方法是在梯度小但一致的方向上迈出较大的步伐，在梯度大但不一致的方向上迈出较小的步伐。如果我们对每个维度都有一个单独的学习率，而不是对所有维度都有一个固定的学习率，这是可以实现的。

![A448418_1_En_2_Fig19_HTML.gif](A448418_1_En_2_Fig19_HTML.gif)

图 2-19。

Gradient descent for a cost function with one variable

在图 [2-19](#Fig19) 中，A 和 C 之间的成本函数几乎是线性的，因此梯度下降效果很好。然而，从点 C 开始，成本函数的曲率接管，因此 C 处的梯度不能跟上成本函数的变化方向。基于梯度，如果我们在 C 取一个小的学习率，我们将在 D 结束，这是足够合理的，因为它没有超过最小值点。然而，C 处的较大步长会使我们到达 D’，这是不可取的，因为它在最小值的另一侧。同样，D '处的大步长将使我们到达 E，如果学习速率没有降低，算法倾向于在最小值两侧的点之间切换，导致振荡。当这种情况发生时，停止它并实现收敛的一种方法是在连续迭代中查看梯度![ $$ \frac{\partial C}{\partial w} $$ ](A448418_1_En_2_Chapter_IEq140.gif)或![ $$ \frac{dC}{dw} $$ ](A448418_1_En_2_Chapter_IEq141.gif)的符号，如果它们具有相反的符号，则降低学习速率，从而减少振荡。类似地，如果连续的梯度具有相同的符号，那么学习率可以相应地增加。当成本函数是多个权重的函数时，成本函数可能在权重的某些维度上具有曲率，而在其他维度上可能是线性的。因此，对于多元成本函数，可以类似地分析成本函数相对于每个权重![ $$ \left(\frac{\partial C}{\partial {w}_i}\right) $$ ](A448418_1_En_2_Chapter_IEq142.gif)的偏导数，以更新成本函数的每个权重或维度的学习率。

#### 成本函数的非凸性

神经网络的另一个大问题是成本函数大多是非凸的，因此梯度下降法可能会陷入局部最小点，导致次优解。神经网络的非凸性质是具有非线性激活函数(如 sigmoid)的隐藏层单元的结果。整批梯度下降使用整个数据集进行梯度计算。虽然这对于凸成本表面是好的，但是在非凸成本函数的情况下，它有它自己的问题。对于具有整批梯度的非凸成本表面，模型将以其吸引盆中的最小值结束。如果初始化的参数处于局部最小值的吸引盆中，而该局部最小值不能提供良好的泛化能力，则全批次梯度将给出次优解。

使用随机梯度下降，计算出的噪声梯度可能会迫使模型脱离不良局部极小值(不能提供良好泛化能力的极小值)的吸引范围，并将其置于更优的区域。具有单个数据点的随机梯度下降产生非常随机和嘈杂的梯度。与单个数据点的梯度相比，小批次的梯度往往会产生更稳定的梯度估计，但它们仍然比全批次产生的梯度更嘈杂。理想地，应该仔细选择小批量大小，使得梯度足够嘈杂以避免或逃脱坏的局部极小点，但是足够稳定以收敛于全局极小点或提供良好泛化的局部极小点。

![A448418_1_En_2_Fig20_HTML.gif](A448418_1_En_2_Fig20_HTML.gif)

图 2-20。

Contour plot showing basins of attraction for global and local minima and traversal of paths for gradient descent and stochastic gradient descent

在图 [2-20](#Fig20) 中，虚线箭头对应随机梯度下降(SGD)的路径，实线箭头对应全批次梯度下降的路径。全批次梯度下降计算某一点的实际梯度，如果该点在一个差的局部最小值的吸引盆中，梯度下降几乎肯定确保达到局部最小值 L。然而，在随机梯度下降的情况下，因为梯度仅基于部分数据，而不是基于整批数据，所以梯度方向只是粗略的估计。由于有噪声的粗略估计并不总是指向点 C 处的实际梯度，随机梯度下降可以避开局部最小值的吸引盆，并且幸运地落在全局最小值的吸引盆中。随机梯度下降也可以避开全局最小吸引盆，但是一般来说，如果吸引盆很大，并且仔细选择小批量大小，使得它产生的梯度适度嘈杂，则随机梯度下降最有可能达到全局最小值 G(如在这种情况下)或具有大吸引盆的一些其他最优最小值。对于非凸优化，也有其他启发式算法，如动量，当与随机梯度下降一起采用时，增加了 SGD 避免浅局部最小值的机会。动量通常通过速度分量跟踪先前的梯度。所以，如果梯度稳定地指向一个好的局部极小值，这个极小值有一个大的吸引盆，那么速度分量在好的局部极小值的方向上会很高。如果新的梯度是有噪声的，并且指向坏的局部最小值，速度分量将提供动量以在相同的方向上继续，并且不会受到新梯度太多的影响。

#### 高维代价函数中的鞍点

优化非凸成本函数的另一个障碍是鞍点的存在。鞍点的数量随着代价函数的参数空间的维数增加而指数增加。鞍点是静止点(即梯度为零的点)，但既不是局部最小值也不是局部最大值点。由于鞍点与具有与鞍点相同成本的点的长平台相关联，所以平台区域中的梯度为零或非常接近零。由于这种在所有方向上接近零的梯度，基于梯度的优化器很难走出这些鞍点。数学上，为了确定一个点是否是鞍点，必须在给定点计算成本函数的 Hessian 矩阵的特征值。如果既有正负特征值，那么它就是一个鞍点。只是为了刷新我们对局部和全局最小值测试的记忆，如果 Hessian 矩阵的所有特征值在静止点处都是正的，则该点是全局最小值，而如果 Hessian 矩阵的所有特征值在静止点处都是负的，则该点是全局最大值。成本函数的 Hessian 矩阵的特征向量给出了成本函数曲率的变化方向，而特征值表示曲率沿这些方向变化的幅度。此外，对于具有连续二阶导数的成本函数，Hessian 矩阵是对称的，因此将总是产生一组正交的特征向量，从而给出成本曲率变化的相互正交的方向。如果在所有这些由特征向量给出的方向上，曲率变化的值(特征值)是正的，那么该点一定是局部最小值，而如果所有曲率变化的值都是负的，那么该点就是局部最大值。这种推广适用于具有任何输入维度的成本函数，而用于确定极值点的判定规则随着成本函数的输入维度而变化。回到鞍点，由于特征值对于某些方向是正的，而对于其他方向是负的，所以成本函数的曲率在正特征值的方向上增加，而在具有负系数的特征向量的方向上减少。围绕鞍点的成本表面的这种性质通常导致具有接近于零的梯度的长平台区域，并且使得梯度下降方法难以脱离这种低梯度的平台。点(0，0)是函数![ $$ f\left(x,y\right)={x}^2-{y}^2 $$ ](A448418_1_En_2_Chapter_IEq143.gif)的鞍点，我们可以从下面的评估中看出:

![ $$ \nabla f\left(x,y\right)=0 $$ ](A448418_1_En_2_Chapter_IEq144.gif) = > ![ $$ \frac{\partial f}{\partial x}=0 $$ ](A448418_1_En_2_Chapter_IEq145.gif)和![ $$ \frac{\partial f}{\partial y}=0 $$ ](A448418_1_En_2_Chapter_IEq146.gif)

![ $$ \frac{\partial f}{\partial x}=2x=0=>x=0 $$ ](A448418_1_En_2_Chapter_Equbz.gif)

![ $$ \frac{\partial f}{\partial y}=-2y=0=>y=0 $$ ](A448418_1_En_2_Chapter_Equca.gif)

所以，![ $$ \left(x,y\right)=\left(0,0\right) $$ ](A448418_1_En_2_Chapter_IEq147.gif)是一个不动点。接下来要做的是计算 Hessian 矩阵，并在![ $$ \left(x,y\right)=\left(0,0\right) $$ ](A448418_1_En_2_Chapter_IEq148.gif)评估其特征值。海森矩阵 Hf(x，y)如下:

![ $$ Hf\left(x,y\right)=\left[\begin{array}{cc}\frac{\partial^2f}{\partial {x}^2}& \frac{\partial^2f}{\partial x\partial y}\\ {}\frac{\partial^2f}{\partial x\partial y}& \frac{\partial^2f}{\partial {y}^2}\end{array}\right]=\left[\begin{array}{cc}2& 0\\ {}0& -2\end{array}\right] $$ ](A448418_1_En_2_Chapter_Equcb.gif)

所以，包括![ $$ \left(x,y\right)=\left(0,0\right) $$ ](A448418_1_En_2_Chapter_IEq149.gif)在内的所有点的 Hessian Hf(x，y)是![ $$ \left[\begin{array}{cc}2& 0\\ {}0& -2\end{array}\right]. $$ ](A448418_1_En_2_Chapter_IEq150.gif)

Hf(x，Y)的两个本征值分别是 2 和-2，对应的本征向量![ $$ \left[\begin{array}{c}1\\ {}0\end{array}\right] $$ ](A448418_1_En_2_Chapter_IEq151.gif)和![ $$ \left[\begin{array}{c}0\\ {}1\end{array}\right] $$ ](A448418_1_En_2_Chapter_IEq152.gif)，无非是沿着 X 轴和 Y 轴的方向。由于一个特征值为正，另一个为负，![ $$ \left(x,y\right)=\left(0,0\right) $$ ](A448418_1_En_2_Chapter_IEq153.gif)就是鞍点。

非凸函数![ $$ f\left(x,y\right)={x}^2-{y}^2 $$ ](A448418_1_En_2_Chapter_IEq154.gif)如图 [2-21](#Fig21) 所示，其中 S 为![ $$ x,y=\left(0,0\right) $$ ](A448418_1_En_2_Chapter_IEq155.gif)处的鞍点

![A448418_1_En_2_Fig21_HTML.gif](A448418_1_En_2_Fig21_HTML.gif)

图 2-21。

Plot of ![ $$ f\left(x,y\right)={x}^2-{y}^2 $$ ](A448418_1_En_2_Chapter_IEq156.gif)

### 随机梯度下降小批量方法中的学习速率

当数据集中存在高冗余度时，在小批量数据点上计算的梯度几乎与在整个数据集上计算的梯度相同，前提是小批量数据点是整个数据集的良好代表。在这种情况下，可以避免计算整个数据集的梯度，而是可以将小批量数据点的梯度用作整个数据集的近似梯度。这是梯度下降的小批量方法，也称为小批量随机梯度下降。当梯度由一个数据点逼近而不是使用小批量时，它被称为在线学习或随机梯度下降。然而，与在线学习相比，使用随机梯度下降的小批量版本总是更好，因为与在线学习模式相比，小批量方法的梯度噪音更小。学习速率在小批量随机梯度下降算法的收敛性中起着至关重要的作用。以下方法倾向于提供良好的收敛性:

*   从最初的学习速度开始。
*   如果误差减小，则增加学习率。
*   如果误差增加，降低学习率。
*   如果误差不再减小，停止学习过程。

正如我们将在下一节中看到的，不同的优化器在它们的实现中采用了自适应学习率的方法。

### TensorFlow 中的优化器

TensorFlow 拥有大量用于优化成本函数的优化器。优化器都是基于梯度的，还有一些特殊的优化器来处理局部极小问题。由于我们在第一章中讨论了机器学习和深度学习中使用的最常见的基于梯度的优化器，这里我们将强调 TensorFlow 中添加到基本算法的定制。

#### 梯度下降优化器

`GradientDescentOptimizer`执行基本的整批梯度下降算法，并将学习率作为输入。梯度下降算法不会自动循环迭代，所以必须在实现中指定这样的逻辑，我们将在后面看到。

最重要的方法是`minimize`方法，其中需要指定最小化的成本函数(用`loss`表示)和成本函数必须最小化的变量列表(用`var_list`表示)。`minimize`方法在内部调用`compute_gradients()`和`apply_gradients()`方法。声明变量列表是可选的，如果未指定，则根据定义为张量流变量的变量(即声明为`tensorflow.Variable()`的变量)计算梯度

##### 使用

```
train_op =  tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

```

其中`learning_` `rate`是常数学习率，`cost`是需要通过梯度下降最小化的成本函数。成本函数相对于与成本函数相关联的张量流变量被最小化。

#### adagradpoptimizer

`AdagradOptimizer`是一个一阶优化器，类似梯度下降，但有一些修改。不是具有全局学习率，而是对成本函数所依赖的每个维度标准化学习率。每次迭代中的学习率是全局学习率除以到每个维度的当前迭代为止的先前梯度的 l <sup>2</sup> 范数。

如果我们有一个成本函数 C(θ)其中![ $$ \theta ={\left[{\theta}_1{\theta}_2{\theta}_3\dots {\theta}_n\right]}^T\in {\mathrm{\mathbb{R}}}^{n\times 1} $$ ](A448418_1_En_2_Chapter_IEq157.gif)，那么θ <sub>i</sub> 的更新规则如下:

![ $$ {\theta_i}^{\left(t+1\right)}={\theta_i}^{(t)}-\frac{\eta }{\sqrt{\sum_{\tau =1}^t{{\theta_i}^{\left(\tau \right)}}^2+\epsilon }}\frac{\partial {C}^{(t)}}{\partial {\theta}_i} $$ ](A448418_1_En_2_Chapter_Equcc.gif)

其中η是学习率，θ <sub>i</sub> <sup>(t)</sup> 和![ $$ {\theta_i}^{\left(t+1\right)} $$ ](A448418_1_En_2_Chapter_IEq158.gif)分别是迭代 t 和![ $$ t+1 $$ ](A448418_1_En_2_Chapter_IEq159.gif)时第 I 个参数的值。

在矩阵格式中，向量θ的参数更新可以表示为如下:

![ $$ {\theta}^{\left(t+1\right)}={\theta}^{(t)}-\eta {G}_{(t)}^{-1}\nabla C\left({\theta}^{(t)}\right) $$ ](A448418_1_En_2_Chapter_Equcd.gif)

其中 G <sub>(t)</sub> 是对角矩阵，其包含每个维度直到迭代 t 之前的过去梯度的 l <sup>2</sup> 范数。矩阵 G <sub>(t)</sub> 将具有以下形式:

![ $$ {G}_{(t)}=\left[\begin{array}{ccc}\sqrt{\sum \limits_{\tau =1}^t{{\theta_1}^{\left(\tau \right)}}^2+\epsilon }& \cdots & 0\\ {}\vdots & \sqrt{\sum \limits_{\tau =1}^t{{\theta_i}^{\left(\tau \right)}}^2+\epsilon }& \vdots \\ {}0& \cdots & \sqrt{\sum \limits_{\tau =1}^t{{\theta_n}^{\left(\tau \right)}}^2+\epsilon}\end{array}\right] $$ ](A448418_1_En_2_Chapter_Equce.gif)

有时，在数据中不常出现的稀疏特征对于优化问题非常有用。然而，对于基本梯度下降或随机梯度下降，学习率在每次迭代中给予所有特征同等的重要性。由于学习率相同，非稀疏特征的总体贡献将比稀疏特征大得多。因此，我们最终会丢失稀疏特征中的关键信息。使用`Adagrad`，每个参数以不同的学习率更新。特征越稀疏，其参数更新在迭代中就越高。这是因为对于稀疏特征，数量![ $$ \sqrt{\sum \limits_{\tau =1}^t{{\theta_i}^{\left(\tau \right)}}^2+\epsilon } $$ ](A448418_1_En_2_Chapter_IEq160.gif)会更少，因此整体学习速率会更高。

在数据稀疏的自然语言处理和图像处理应用程序中，这是一个很好的优化器。

##### 使用

```
train_op = tf.train.AdagradOptimizer.(learning_rate=0.001, initial_accumulator_value=0.1)

```

其中`learning_rate`代表η，`initial_accumulator_value`代表每个权重的初始非零归一化因子。

#### RMSprop

`RMSprop`是弹性反向传播(`Rprop`)优化技术的小批量版本，最适合整批学习。`Rprop`解决了在成本函数轮廓为椭圆形的情况下梯度不指向最小值的问题。如前所述，在这种情况下，不同于全局学习规则，针对每个权重的单独的自适应更新规则会导致更好的收敛。`Rprop`的特别之处在于，它不使用权重梯度的大小，而只使用符号来确定如何更新每个权重。以下是`Rprop`的工作逻辑:

*   对于所有权重，以相同幅度的权重更新开始；即![ $$ {\varDelta_{ij}}^{\left(t=0\right)}={\varDelta_{ij}}^{(0)}=\varDelta $$ ](A448418_1_En_2_Chapter_IEq161.gif)。此外，将最大和最小允许重量更新分别设置为δ<sub>max</sub>和δ<sub>min</sub>。
*   在每次迭代中，检查先前和当前梯度分量的符号；即成本函数相对于不同权重的偏导数。
*   If the signs of the current and previous gradient components for a weight connection are the same—i.e., ![ $$ \mathit{\operatorname{sign}}\left(\frac{\partial {C}^{(t)}}{\partial {w}_{ij}}\frac{\partial {C}^{\left(t-1\right)}}{\partial {w}_{ij}}\right)=+ ve $$ ](A448418_1_En_2_Chapter_IEq162.gif) —then increase the learning by a factor ![ $$ {\eta}_{+}=1.2 $$ ](A448418_1_En_2_Chapter_IEq163.gif) . The update rule becomes

    ![ $$ {\varDelta_{ij}}^{\left(t+1\right)}=\min \left({\eta}_{+}{\varDelta_{ij}}^{(t)},{\varDelta}_{max}\right) $$ ](A448418_1_En_2_Chapter_Equcf.gif)

    ![ $$ {w_{ij}}^{\left(t+1\right)}={w_{ij}}^{(t)}-\mathit{\operatorname{sign}}\left(\frac{\partial {C}^{(t)}}{\partial {w}_{ij}}\right).{\varDelta_{ij}}^{\left(t+1\right)} $$ ](A448418_1_En_2_Chapter_Equcg.gif)

*   If the signs of the current and previous gradient components for a dimension are different—i.e., ![ $$ \mathit{\operatorname{sign}}\left(\frac{\partial {C}^{(t)}}{\partial {w}_{ij}}\frac{\partial {C}^{\left(t-1\right)}}{\partial {w}_{ij}}\right)=- ve $$ ](A448418_1_En_2_Chapter_IEq164.gif) —then reduce the learning rate by a factor![ $$ {\eta}_{-}=0.5 $$ ](A448418_1_En_2_Chapter_IEq165.gif) . The update rule becomes

    ![ $$ {\varDelta_{ij}}^{\left(t+1\right)}=\max \left({\eta}_{-}{\varDelta_{ij}}^{(t)},{\varDelta}_{min}\right) $$ ](A448418_1_En_2_Chapter_Equch.gif)

    ![ $$ {w_{ij}}^{\left(t+1\right)}={w_{ij}}^{(t)}-\mathit{\operatorname{sign}}\left(\frac{\partial {C}^{(t)}}{\partial {w}_{ij}}\right).{\varDelta_{ij}}^{\left(t+1\right)} $$ ](A448418_1_En_2_Chapter_Equci.gif)

*   If ![ $$ \frac{\partial {C}^{(t)}}{\partial {w}_{ij}}\frac{\partial {C}^{\left(t-1\right)}}{\partial {w}_{ij}}=0 $$ ](A448418_1_En_2_Chapter_IEq166.gif), the update rule is as follows:

    ![ $$ {\varDelta_{ij}}^{\left(t+1\right)}={\varDelta_{ij}}^{(t)} $$ ](A448418_1_En_2_Chapter_Equcj.gif)

    ![ $$ {w_{ij}}^{\left(t+1\right)}={w_{ij}}^{(t)}-\mathit{\operatorname{sign}}\left(\frac{\partial {C}^{(t)}}{\partial {w}_{ij}}\right).{\varDelta_{ij}}^{\left(t+1\right)} $$ ](A448418_1_En_2_Chapter_Equck.gif)

在梯度下降期间，梯度在特定间隔内不改变符号的维度是重量变化一致的维度。因此，增加学习速率将导致这些权重更快地收敛到它们的最终值。

梯度沿其改变符号的维度表明，沿这些维度的权重变化是不一致的，因此通过降低学习速率，可以避免振荡并更好地赶上曲率。对于凸函数，当代价函数曲面存在曲率且学习率设置较高时，通常会出现梯度符号变化。由于梯度不具有曲率信息，大的学习率将更新的参数值带到极小点之外，并且该现象在极小点的任一侧持续重复。

`Rprop`适用于完整批次，但在涉及随机梯度下降时效果不佳。当学习速率非常小时，在随机梯度下降的情况下，来自不同小批量的梯度达到平均。如果通过成本函数的随机梯度下降，当学习速率较小时，重量的梯度对于九个小批量是+0.2，对于第十个小批量是-0.18，则随机梯度下降的有效梯度效应几乎为零，重量几乎保持在相同的位置，这是期望的结果。

然而，对于`Rprop`,学习率将增加大约九倍，并且仅减少一次，因此有效权重将远大于零。这是不可取的。

为了将`Rprop`针对每个权重的自适应学习规则的质量与随机梯度下降的效率相结合，`RMSprop`出现了。在`Rprop`中，我们不使用大小，而是仅仅使用每个重量梯度的符号。每个权重的梯度的符号可以被认为是将权重的梯度除以其大小。随机梯度下降的问题在于，对于每个小批量，成本函数不断变化，因此梯度也不断变化。因此，我们的想法是获得一个重量梯度的大小，它不会在附近的小批量中波动太大。比较好的方法是，在最近的小批量中，每个重量的平方梯度的均方根，以标准化梯度。

![ $$ {g_{ij}}^{(t)}=\alpha {g_{ij}}^{\left(t-1\right)}+\left(1-\alpha \right){\left(\frac{\partial {C}^{(t)}}{\partial {w}_{ij}}\right)}^2 $$ ](A448418_1_En_2_Chapter_Equcl.gif)

![ $$ {w_{ij}}^{\left(t+1\right)}={w_{ij}}^{(t)}-\frac{\eta }{\sqrt{{g_{ij}}^{(t)}+\epsilon }}\frac{\partial {C}^{(t)}}{\partial {w}_{ij}} $$ ](A448418_1_En_2_Chapter_Equcm.gif)

其中 g <sup>(t)</sup> 是迭代 t 时权重 w <sub>ij</sub> 的梯度均方根，α是每个权重 w <sub>ij</sub> 的梯度均方根的衰减率。

##### 使用

```
train_op = tf.train.RMSPropOptimizer(learning_rate=0.001, decay =0.9, momentum=0.0, epsilon=1e-10)

```

其中`decay`代表α，ε代表ϵ，η代表学习率。

#### AdadeltaOptimizer

`AdadeltaOptimizer`是`AdagradOptimizer`的变体，在降低学习率方面不那么激进。对于每个权重连接，`AdagradOptimizer`通过将学习速率常数除以该权重直到该迭代的所有过去梯度的均方根来缩放迭代中的学习速率常数。因此，每个权重的有效学习率是迭代次数的单调递减函数，并且在相当多的迭代次数之后，学习率变得无穷小。`AdagradOptimizer`通过对每个重量或维度取指数衰减平方梯度的平均值，克服了这个问题。因此，`AdadeltaOptimizer`中的有效学习率更多的是对其当前梯度的局部估计，不会像`AdagradOptimizer`方法那样快速下降。这确保了即使在相当数量的迭代或时期之后学习仍在继续。`Adadelta`的学习规则可以总结如下:

![ $$ {g}_{ij}^{(t)}=\kern0.5em \upgamma\ {g}_{ij}^{\left(t-1\right)}+\left(1-\kern0.5em \upgamma \right){\left(\frac{\partial {C}^{(t)}}{\partial {w}_{ij}}\right)}^2 $$ ](A448418_1_En_2_Chapter_Equcn.gif)

![ $$ {w}_{ij}^{\left(t+1\right)}={w}_{ij}^{(t)}-\frac{\eta }{\sqrt{g_{ij}^{(t)}+\in }}\frac{\partial {C}^{(t)}}{\partial {w}_{ij}} $$ ](A448418_1_En_2_Chapter_Equco.gif)

其中γ是指数衰减常数，η是学习速率常数，g <sub>ij</sub> <sup>(t)</sup> 表示迭代 t 时的有效均方梯度。我们可以将![ $$ \sqrt{g_{ij}^{(t)}+\in } $$ ](A448418_1_En_2_Chapter_IEq167.gif)项表示为 RMS(g<sub>ij</sub><sup>【t】</sup>，其给出的更新规则如下:

如果我们仔细观察，重量变化的单位没有重量的单位。![ $$ \frac{\partial {C}^{(t)}}{\partial {w}_{ij}} $$ ](A448418_1_En_2_Chapter_IEq168.gif)和 RMS 的单位(g <sub> ij </sub> <sup>(t)</sup> )是相同的，即梯度的单位(成本函数变化/单位重量变化)，因此它们相互抵消。因此，权重变化的单位是学习速率常数的单位。`Adadelta`通过将学习速率常数η替换为到当前迭代为止的指数衰减平方权重更新的平均值的平方根来解决这个问题。设 h <sub> ij </sub> <sup>(t)</sup> 为迭代 t 前权值更新的平方的平均值，β为衰减常数，δw<sub>ij</sub><sup>(t)</sup>为迭代 t 中的权值更新，则 h <sub> ij </sub> <sup>(t)</sup> 的更新规则和`Adadelta`的最终权值更新规则可表示如下:

![ $$ {h}_{ij}^{(t)}=\kern0.5em \upbeta {h}_{ij}^{\left(t-1\right)}+\left(1-\kern0.5em \upbeta \right){\left(\varDelta {w}_{ij}^{(t)}\right)}^2 $$ ](A448418_1_En_2_Chapter_Equcq.gif)

如果我们将![ $$ \sqrt{h_{ij}^{(t)}+\epsilon } $$ ](A448418_1_En_2_Chapter_IEq169.gif)表示为 RMS(h <sub> ij </sub> <sup>(t)</sup> ，那么更新规则变成-

![ $$ {w}_{ij}^{\left(t+1\right)}={w}_{ij}^{(t)}-\frac{RMS\left({h}_{ij}^{(t)}\right)}{RMS\left({g}_{ij}^{(t)}\right)}\frac{\partial {C}^{(t)}}{\partial {w}_{ij}} $$ ](A448418_1_En_2_Chapter_Equcs.gif)

##### 使用

```
train_op = tf.train.AdadeltaOptimizer(learning_rate=0.001, rho=0.95, epsilon=1e-08)

```

其中`rho`代表γ，`epsilon`代表ϵ，η代表学习率。

`Adadelta`的一个显著优点是它完全消除了学习速率常数。如果我们比较`Adadelta`和`RMSprop,`，如果我们把学习率常数消去法放在一边，两者是一样的。`Adadelta`和`RMSprop`几乎同时独立开发，以解决`Adagrad`的快速学习率衰减问题。

#### 阿达莫优化器

`Adam`，或自适应矩估计器，是另一种优化技术，很像`RMSprop`或`Adagrad`，对每个参数或权重有一个自适应的学习率。`Adam`不仅保持平方梯度的移动平均值，还保持过去梯度的移动平均值。

设每个权重 w <sub>ij</sub> 的梯度均值 m<sub>ij</sub>T3【t】和梯度平方均值 v<sub>ij</sub>T7【t】的衰减率分别为β <sub>1</sub> 和β <sub>2</sub> 。此外，假设η是常数学习率因子。那么，`Adam`的更新规则如下:

![ $$ {m}_{ij}^{(t)}={\beta}_1{m}_{ij}^{\left(t-1\right)}+\left(1-{\beta}_1\right)\frac{\partial {C}^{(t)}}{\partial {w}_{ij}} $$ ](A448418_1_En_2_Chapter_Equct.gif)

![ $$ {v}_{ij}^{(t)}={\beta}_2{v}_{ij}^{\left(t-1\right)}+\left(1-{\beta}_2\right){\left(\frac{\partial {C}^{(t)}}{\partial {w}_{ij}}\right)}^2 $$ ](A448418_1_En_2_Chapter_Equcu.gif)

梯度的归一化平均值![ $$ \hat{m_{ij}^{(t)}} $$ ](A448418_1_En_2_Chapter_IEq170.gif)和平方梯度的平均值![ $$ \hat{v_{ij}^{(t)}} $$ ](A448418_1_En_2_Chapter_IEq171.gif)计算如下:

![ $$ \hat{m_{ij}^{(t)}}=\frac{m_{ij}^{(t)}}{\left(1-{\beta_1}^t\right)\ } $$ ](A448418_1_En_2_Chapter_Equcv.gif)

![ $$ \hat{v_{ij}^{(t)}}=\frac{v_{ij}^{(t)}}{\left(1-{\beta_2}^t\right)} $$ ](A448418_1_En_2_Chapter_Equcw.gif)

每个权重 w <sub>ij</sub> 的最终更新规则如下:

![ $$ {w_{ij}}^{\left(t+1\right)}={w_{ij}}^{(t)}-\frac{\eta }{\sqrt{\hat{v_{ij}^{(t)}}+\in }}\hat{m_{ij}^{(t)}} $$ ](A448418_1_En_2_Chapter_Equcx.gif)

##### 使用

```
train_op = tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.9,beta2=0.999,epsilon=1e-08).minimize(cost)

```

其中`learning_rate`是常数学习率η，`cost` C 是需要通过`AdamOptimizer`最小化的代价函数。参数`beta1`和`beta2`分别对应β <sub>1</sub> 和β <sub>2</sub> ，而`epsilon`代表![ $$ \in $$ ](A448418_1_En_2_Chapter_IEq172.gif)。

成本函数相对于与成本函数相关联的张量流变量被最小化。

#### 动量优化器和内斯特罗夫算法

基于动量的优化器已经发展到处理非凸优化。每当我们使用神经网络时，我们通常得到的成本函数本质上是非凸的，因此基于梯度的优化方法可能会陷入糟糕的局部最小值。如前所述，这是非常不可取的，因为在这种情况下，我们得到的是优化问题的次优解，很可能是次优模型。此外，梯度下降遵循每个点的斜率，并向局部最小值小幅前进，但它可能非常慢。基于动量的方法引入了称为速度 v 的分量，当所计算的梯度改变符号时，该分量抑制参数更新，而当梯度与速度方向相同时，该分量加速参数更新。这引入了更快的收敛以及围绕全局最小值或围绕提供良好泛化能力的局部最小值的更少振荡。基于动量的优化器的更新规则如下:

![ $$ {v}_i^{\left(t+1\right)}=\alpha {v}_i^{(t)}-\eta \frac{\partial C}{\partial {w}_i}\ \left({w_i}^{(t)}\right) $$ ](A448418_1_En_2_Chapter_Equcy.gif)

![ $$ {w_i}^{\left(t+1\right)}={w_i}^{(t)}+{v}_i^{\left(t+1\right)} $$ ](A448418_1_En_2_Chapter_Equcz.gif)

其中，α是动量参数，η是学习率。项 v <sub>i</sub> <sup>(t)</sup> 和![ $$ {v}_i^{\left(t+1\right)} $$ ](A448418_1_En_2_Chapter_IEq173.gif)分别表示第 I 个参数在迭代 t 和![ $$ \left(t+1\right) $$ ](A448418_1_En_2_Chapter_IEq174.gif)时的速度。类似地，w <sub> i </sub> <sup>(t)</sup> 和![ $$ {w_i}^{\left(t+1\right)} $$ ](A448418_1_En_2_Chapter_IEq175.gif)分别代表第 I 个参数在迭代 t 和![ $$ t+1 $$ ](A448418_1_En_2_Chapter_IEq176.gif)时的权重。

想象在优化成本函数时，优化算法在![ $$ \frac{\partial C}{\partial {w}_i}\left({w_i}^{(t)}\right)\to 0\forall i\in {\mathrm{\mathbb{R}}}^{n\times 1} $$ ](A448418_1_En_2_Chapter_IEq177.gif)处达到局部最小值。在不考虑动量的正常梯度下降方法中，参数更新将在局部最小值或鞍点处停止。然而，在基于动量的优化中，考虑到局部极小值具有小的吸引盆，先验速度会驱使算法脱离局部极小值，因为![ $$ {v}_i^{\left(t+1\right)} $$ ](A448418_1_En_2_Chapter_IEq178.gif)会因为来自先验梯度的非零速度而非零。此外，如果先前的梯度一致地指向全局最小值或局部最小值，具有良好的推广性和相当大的吸引盆地，则梯度下降的速度或动量将在该方向上。因此，即使存在具有小吸引盆的坏的局部最小值，动量分量也将不仅驱使算法脱离坏的局部最小值，而且将继续朝向全局最小值或好的局部最小值的梯度下降。

如果权重是参数向量θ的一部分，则基于动量的优化器的矢量化更新规则将如下(参见图 [2-22](#Fig22) 的基于向量的图示):

![ $$ {v}^{\left(t+1\right)}=\alpha {v}^{(t)}-\eta \nabla C\ \left(\theta ={\theta}^{(t)}\right) $$ ](A448418_1_En_2_Chapter_Equda.gif)

![ $$ {\theta}^{\left(t+1\right)}={\theta}^{(t)}+{v}^{\left(t+1\right)} $$ ](A448418_1_En_2_Chapter_Equdb.gif)

![A448418_1_En_2_Fig22_HTML.gif](A448418_1_En_2_Fig22_HTML.gif)

图 2-22。

Parameter vector update in momentum-based gradient-descent optimizer

基于动量的优化器的一个特殊变体是内斯特罗夫加速梯度技术。该方法利用现有的速度 v <sup>(t)</sup> 对参数向量进行更新。因为它是对参数向量的中间更新，所以用![ $$ {\theta}^{\left(t+\frac{1}{2}\right)} $$ ](A448418_1_En_2_Chapter_IEq179.gif)来表示很方便。在![ $$ {\theta}^{\left(t+\frac{1}{2}\right)} $$ ](A448418_1_En_2_Chapter_IEq180.gif)评估成本函数的梯度，并将其用于更新新的速度。最后，新的参数向量是前一次迭代的参数向量和新的速度之和。

![ $$ {\theta}^{\left(t+\frac{1}{2}\right)}={\theta}^{(t)}+\alpha {v}^{(t)} $$ ](A448418_1_En_2_Chapter_Equdc.gif)

![ $$ {v}^{\left(t+1\right)}=\alpha {v}^{(t)}-\eta \nabla C\ \left(\uptheta ={\theta}^{\left(t+\frac{1}{2}\right)}\right) $$ ](A448418_1_En_2_Chapter_Equdd.gif)

![ $$ {\theta}^{\left(t+1\right)}={\theta}^{(t)}+{v}^{\left(t+1\right)} $$ ](A448418_1_En_2_Chapter_Equde.gif)

##### 使用

```
train_op = tf.train.MomentumOptimizer.(learning_rate=0.001, momentum=0.9,use_nesterov=False)

```

其中`learning_rate`代表η，`momentum`代表α，`use_nesterov`决定是否使用内斯特罗夫版动量。

#### 纪元、批次数量和批次大小

深度学习网络，如前所述，一般通过小批量随机梯度下降来训练。我们需要熟悉的一些术语如下:

*   批量大小–批量大小决定了每个小批量中训练数据点的数量。应该选择批量大小，使得它对整个训练数据集的梯度给出足够好的估计，同时噪声足够大，以避开不提供良好泛化的坏的局部最小值。
*   批次数量–批次数量给出了整个训练数据集中小型批次的总数。它可以通过将总训练数据点的计数除以批量大小来计算。请注意，最后一个小批量的数据点数量可能比批量少。
*   历元–一个历元由对整个数据集的一次完整训练组成。更具体地说，一个历元相当于整个训练数据集上的一次正向传递加上一次反向传播。因此，一个时期将由 n 个(正向传递+反向传播)组成，其中 n 表示批次的数量。

### 使用张量流实现 XOR

既然我们对人工神经网络所涉及的组件和训练方法有了大致的了解，我们将在隐藏层和输出中使用 sigmoid 激活函数来实现 XOR 网络。清单 [2-16](#Par275) 中概述了详细的实现。

```
#-------------------------------------------------------------------------------------------
#XOR  implementation in Tensorflow with hidden layers being sigmoid to
# introduce Non-Linearity
#-------------------------------------------------------------------------------------------
import tensorflow as tf
#-------------------------------------------------------------------------------------------
# Create placeholders for training input and output labels
#-------------------------------------------------------------------------------------------
x_ = tf.placeholder(tf.float32, shape=[4,2], name="x-input")
y_ = tf.placeholder(tf.float32, shape=[4,1], name="y-input")
#-------------------------------------------------------------------------------------------
#Define the weights to the hidden and output layer respectively.
#-------------------------------------------------------------------------------------------
w1 = tf.Variable(tf.random_uniform([2,2], -1, 1), name="Weights1")
w2 = tf.Variable(tf.random_uniform([2,1], -1, 1), name="Weights2")
#-------------------------------------------------------------------------------------------
# Define the bias to the hidden and output layers respectively
#-------------------------------------------------------------------------------------------
b1 = tf.Variable(tf.zeros([2]), name="Bias1")
b2 = tf.Variable(tf.zeros([1]), name="Bias2")
#-------------------------------------------------------------------------------------------
# Define the final output through forward pass
#-------------------------------------------------------------------------------------------
z2 = tf.sigmoid(tf.matmul(x_, w1) + b1)
pred = tf.sigmoid(tf.matmul(z2,w2) + b2)
#------------------------------------------------------------------------------------------
#Define the Cross-entropy/Log-loss Cost function based on the output label y and
# the predicted probability by the forward pass
#-------------------------------------------------------------------------------------------
cost = tf.reduce_mean(( (y_ * tf.log(pred)) +
        ((1 - y_) * tf.log(1.0 - pred)) ) * -1)
learning_rate = 0.01
train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)
#-------------------------------------------------------------------------------------------
#Now that we have all that we need set up we will start the training
#-------------------------------------------------------------------------------------------
XOR_X = [[0,0],[0,1],[1,0],[1,1]]
XOR_Y = [[0],[1],[1],[0]]
#-------------------------------------------------------------------------------------------
# Initialize the variables
#-------------------------------------------------------------------------------------------
init = tf.initialize_all_variables()
sess = tf.Session()
writer = tf.summary.FileWriter("./Downloads/XOR_logs", sess.graph_def)

sess.run(init)
for i in range(100000):
        sess.run(train_step, feed_dict={x_: XOR_X, y_: XOR_Y})

#-------------------------------------------------------------------------------------------
print('Final Prediction', sess.run(pred, feed_dict={x_: XOR_X, y_: XOR_Y}))
#-------------------------------------------------------------------------------------------

--output --

('Final Prediction', array([[ 0.06764214],
       [ 0.93982035],
       [ 0.95572311],
       [ 0.05693595]], dtype=float32))

Listing 2-16.XOR Implementation with Hidden Layers That Have Sigmoid Activation Functions

```

在清单 [2-16](#Par275) 中，XOR 逻辑是使用 TensorFlow 实现的。隐藏层单元具有 sigmoid 激活函数以引入非线性。输出激活函数具有 sigmoid 激活函数，以给出概率输出。我们使用梯度下降优化器，学习率为 0.01，总迭代次数约为 100，000 次。如果我们看到最终预测，第一个和第四个训练样本的概率值接近于零，而第二个和第四个训练样本的概率接近于 1。因此，该网络可以准确且高精度地预测类别。任何合理的阈值都会正确分类数据点。

#### 异或网络的张量流计算图

在图 [2-23](#Fig23) 中，说明了前面实现的 XOR 网络的计算图。通过包含以下代码行，计算图摘要被写入日志文件。短语`". /Downloads/XOR_logs"`表示摘要日志文件的存储位置。然而，它可以是你选择的任何位置。

```
writer = tf.summary.FileWriter("./Downloads/XOR_logs", sess.graph_def)

```

将摘要写入终端上的日志文件后，我们需要执行以下命令来激活 Tensorboard:

```
tensorboard --logdir=./Downloads/XOR_logs

```

这将启动 Tensorboard 会话，并提示我们在`http://localhost:6006`访问 Tensorboard，在这里可以可视化计算图形。

![A448418_1_En_2_Fig23_HTML.jpg](A448418_1_En_2_Fig23_HTML.jpg)

图 2-23。

Computation graph for the XOR network

现在，我们再次实现 XOR 逻辑，在隐藏层使用线性激活函数，并保持网络的其余部分不变。清单 [2-17](#Par283) 展示了 TensorFlow 的实现。

```
#-------------------------------------------------------------------------------------------
#XOR  implementation in TensorFlow with linear activation for hidden layers
#-------------------------------------------------------------------------------------------
import tensorflow as tf
#-------------------------------------------------------------------------------------------
# Create placeholders for training input and output labels
#-------------------------------------------------------------------------------------------
x_ = tf.placeholder(tf.float32, shape=[4,2], name="x-input")
y_ = tf.placeholder(tf.float32, shape=[4,1], name="y-input")
#-------------------------------------------------------------------------------------------
#Define the weights to the hidden and output layer respectively.
#-------------------------------------------------------------------------------------------
w1 = tf.Variable(tf.random_uniform([2,2], -1, 1), name="Weights1")
w2 = tf.Variable(tf.random_uniform([2,1], -1, 1), name="Weights2")
#------------------------------------------------------------------------------------------
# Define the bias to the hidden and output layers respectively
#-------------------------------------------------------------------------------------------
b1 = tf.Variable(tf.zeros([2]), name="Bias1")
b2 = tf.Variable(tf.zeros([1]), name="Bias2")
#-------------------------------------------------------------------------------------------
# Define the final output through forward pass
#-------------------------------------------------------------------------------------------
z2 = tf.matmul(x_, w1) + b1
pred = tf.sigmoid(tf.matmul(z2,w2) + b2)
#-------------------------------------------------------------------------------------------
#Define the Cross-entropy/Log-loss Cost function based on the output label y and the predicted
#probability by the forward pass
#-------------------------------------------------------------------------------------------
cost = tf.reduce_mean(( (y_ * tf.log(pred)) +
        ((1 - y_) * tf.log(1.0 - pred)) ) * -1)
learning_rate = 0.01
train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)
#-------------------------------------------------------------------------------------------
#Now that we have all that we need, start the training
#-------------------------------------------------------------------------------------------
XOR_X = [[0,0],[0,1],[1,0],[1,1]]
XOR_Y = [[0],[1],[1],[0]]

init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)
for i in range(100000):
        sess.run(train_step, feed_dict={x_: XOR_X, y_: XOR_Y})

#------------------------------------------------------------------------------------------
print('Final Prediction', sess.run(pred, feed_dict={x_: XOR_X, y_: XOR_Y}))
#-------------------------------------------------------------------------------------------

-- output --

('Final Prediction', array([[ 0.5000003 ],
       [ 0.50001115],
       [ 0.49998885],
       [ 0.4999997 ]], dtype=float32))

Listing 2-17.XOR Implementation with Linear Activation Functions in Hidden Layer

```

清单 [2-2](#Par185) 中显示的最终预测都接近 0.5，这意味着实现的 XOR 逻辑不能很好地区分正类和负类。当我们在隐藏层中具有线性激活函数时，网络主要保持线性，正如我们之前所看到的，因此在需要非线性决策边界来分离类的情况下，该模型不能够做得很好。

### 张量流中的线性回归

线性回归可以表示为单神经元回归问题。预测误差的均方值作为成本函数，根据模型的系数进行优化。清单 [2-18](#Par286) 显示了使用波士顿房价数据集进行线性回归的 TensorFlow 实现。

```
#-------------------------------------------------------------------------------------------
# Importing TensorFlow, Numpy, and the Boston Housing price dataset
#-------------------------------------------------------------------------------------------

import tensorflow as tf
import numpy as np
from sklearn.datasets import load_boston

#-------------------------------------------------------------------------------------------
# Function to load the Boston data set
#------------------------------------------------------------------------------------------

def read_infile():
    data = load_boston()
    features = np.array(data.data)
    target = np.array(data.target)
    return features,target

#-------------------------------------------------------------------------------------------
# Normalize the features by Z scaling; i.e., subtract from each feature value its mean and then divide by its #standard deviation. Accelerates gradient descent.
#-------------------------------------------------------------------------------------------

def feature_normalize(data):
    mu = np.mean(data,axis=0)
    std = np.std(data,axis=0)
    return (data - mu)/std

#-------------------------------------------------------------------------------------------
# Append the feature for the bias term.
#-------------------------------------------------------------------------------------------

def append_bias(features,target):
    n_samples = features.shape[0]
    n_features = features.shape[1]
    intercept_feature  = np.ones((n_samples,1))
    X = np.concatenate((features,intercept_feature),axis=1)
    X = np.reshape(X,[n_samples,n_features +1])
    Y = np.reshape(target,[n_samples,1])
    return X,Y
#-------------------------------------------------------------------------------------------
#  Execute the functions to read, normalize, and add append bias term to the data
#-------------------------------------------------------------------------------------------

features,target = read_infile()
z_features = feature_normalize(features)
X_input,Y_input = append_bias(z_features,target)
num_features = X_input.shape[1]

#------------------------------------------------------------------------------------------
# Create TensorFlow ops for placeholders, weights, and weight initialization
#-------------------------------------------------------------------------------------------

X = tf.placeholder(tf.float32,[None,num_features])
Y = tf.placeholder(tf.float32,[None,1])
w = tf.Variable(tf.random_normal((num_features,1)),name='weights')
init = tf.global_variables_initializer()

#-------------------------------------------------------------------------------------------
# Define the different TensorFlow ops and input parameters for Cost and Optimization.
#-------------------------------------------------------------------------------------------

learning_rate = 0.01
num_epochs = 1000
cost_trace = []
pred = tf.matmul(X,w)
error = pred - Y
cost = tf.reduce_mean(tf.square(error))
train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

#-------------------------------------------------------------------------------------------
# Execute the gradient-descent learning
#-------------------------------------------------------------------------------------------

with tf.Session() as sess:
    sess.run(init)
    for i in xrange(num_epochs):
        sess.run(train_op,feed_dict={X:X_input,Y:Y_input})
        cost_trace.append(sess.run(cost,feed_dict={X:X_input,Y:Y_input}))
    error_ = sess.run(error,{X:X_input,Y:Y_input})
    pred_ = sess.run(pred,{X:X_input})

print 'MSE in training:',cost_trace[-1]

-- output --

MSE in training: 21.9711

Listing 2-18.Linear Regression Implementation in TensorFlow

```

![A448418_1_En_2_Fig24_HTML.gif](A448418_1_En_2_Fig24_HTML.gif)

图 2-24。

Cost (MSE) versus epochs while training

```
#-------------------------------------------------------------------------------------------
# Plot the reduction in cost over iterations or epochs
#-------------------------------------------------------------------------------------------

import matplotlib.pyplot as plt
%matplotlib inline
plt.plot(cost_trace)

Listing 2-18a.Linear Regression Cost Plot over Epochs

or Iterations

```

![A448418_1_En_2_Fig25_HTML.jpg](A448418_1_En_2_Fig25_HTML.jpg)

图 2-25。

Actual house price versus predicted house price

```
#-------------------------------------------------------------------------------------------
# Plot the Predicted House Prices vs the Actual House Prices
#-------------------------------------------------------------------------------------------

fig, ax = plt.subplots()
plt.scatter(Y_input,pred_)
ax.set_xlabel('Actual House price')
ax.set_ylabel('Predicted House price')

Listing 2-18b.Linear Regression Actual House Price Versus Predicted House Price

```

图 [2-24](#Fig24) 显示了各时期的成本进展，图 [2-25](#Fig25) 显示了培训后的预测房价与实际房价。

### 使用全批次梯度下降的 SoftMax 函数进行多类别分类

在本节中，我们将使用整批梯度下降来说明一个多类分类问题。之所以使用 MNIST 数据集，是因为有 10 个输出类对应于 10 个整数。清单 [2-19](#Par291) 中提供了详细的实现。SoftMax 被用作输出层。

```
#-------------------------------------------------------------------------------------------
# Import the required libraries
#-------------------------------------------------------------------------------------------

import tensorflow as tf
import numpy as np
from sklearn import datasets
from tensorflow.examples.tutorials.mnist import input_data

#------------------------------------------------------------------------------------------
# Function to read the MNIST dataset along with the labels
#------------------------------------------------------------------------------------------

def read_infile():
    mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
    train_X, train_Y,test_X, test_Y = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels
    return train_X, train_Y,test_X, test_Y

#-------------------------------------------------------------------------------------------
#  Define the weights and biases for the neural network
#-------------------------------------------------------------------------------------------

def weights_biases_placeholder(n_dim,n_classes):
    X = tf.placeholder(tf.float32,[None,n_dim])
    Y = tf.placeholder(tf.float32,[None,n_classes])
    w = tf.Variable(tf.random_normal([n_dim,n_classes],stddev=0.01),name='weights')
    b = tf.Variable(tf.random_normal([n_classes]),name='weights')
    return X,Y,w,b

#-------------------------------------------------------------------------------------------
# Define the forward pass
#-------------------------------------------------------------------------------------------

def forward_pass(w,b,X):
    out = tf.matmul(X,w) + b
    return out

#-------------------------------------------------------------------------------------------
# Define the cost function for the SoftMax unit
#-------------------------------------------------------------------------------------------

def multiclass_cost(out,Y):
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out,labels=Y))
    return cost

#-------------------------------------------------------------------------------------------
# Define the initialization op
#------------------------------------------------------------------------------------------

def init():
    return tf.global_variables_initializer()

#-------------------------------------------------------------------------------------------
# Define the training op
#-------------------------------------------------------------------------------------------

def train_op(learning_rate,cost):
    op_train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)
    return op_train

train_X, train_Y,test_X, test_Y = read_infile()
X,Y,w,b = weights_biases_placeholder(train_X.shape[1],train_Y.shape[1])
out = forward_pass(w,b,X)
cost = multiclass_cost(out,Y)
learning_rate,epochs = 0.01,1000
op_train = train_op(learning_rate,cost)
init = init()
loss_trace = []
accuracy_trace = []

#-------------------------------------------------------------------------------------------
# Activate the TensorFlow session and execute the stochastic gradient descent
#-------------------------------------------------------------------------------------------

with tf.Session() as sess:
    sess.run(init)

    for i in xrange(epochs):
        sess.run(op_train,feed_dict={X:train_X,Y:train_Y})
        loss_ = sess.run(cost,feed_dict={X:train_X,Y:train_Y})
        accuracy_ = np.mean(np.argmax(sess.run(out,feed_dict={X:train_X,Y:train_Y}),axis=1) == np.argmax(train_Y,axis=1))
        loss_trace.append(loss_)
        accuracy_trace.append(accuracy_)
        if (((i+1) >= 100) and ((i+1) % 100 == 0 )) :
            print 'Epoch:',(i+1),'loss:',loss_,'accuracy:',accuracy_

    print 'Final training result:','loss:',loss_,'accuracy:',accuracy_
    loss_test = sess.run(cost,feed_dict={X:test_X,Y:test_Y})
    test_pred = np.argmax(sess.run(out,feed_dict={X:test_X,Y:test_Y}),axis=1)
    accuracy_test = np.mean(test_pred == np.argmax(test_Y,axis=1))
    print 'Results on test dataset:','loss:',loss_test,'accuracy:',accuracy_test

-- output --
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
Epoch: 100 loss: 1.56331 accuracy: 0.702781818182
Epoch: 200 loss: 1.20598 accuracy: 0.772127272727
Epoch: 300 loss: 1.0129 accuracy: 0.800363636364
Epoch: 400 loss: 0.893824 accuracy: 0.815618181818
Epoch: 500 loss: 0.81304 accuracy: 0.826618181818
Epoch: 600 loss: 0.754416 accuracy: 0.834309090909
Epoch: 700 loss: 0.709744 accuracy: 0.840236363636
Epoch: 800 loss: 0.674433 accuracy: 0.845
Epoch: 900 loss: 0.645718 accuracy: 0.848945454545
Epoch: 1000 loss: 0.621835 accuracy: 0.852527272727
Final training result: loss: 0.621835 accuracy: 0.852527272727
Results on test dataset: loss: 0.596687 accuracy: 0.8614

Listing 2-19.Multi-class Classification with Softmax Function Using Full-Batch Gradient Descent

```

![A448418_1_En_2_Fig26_HTML.jpg](A448418_1_En_2_Fig26_HTML.jpg)

图 2-26。

Actual digits versus predicted digits for SoftMax classification through gradient descent

```
import matplotlib.pyplot as plt
%matplotlib inline
f, a = plt.subplots(1, 10, figsize=(10, 2))
print 'Actual digits:   ', np.argmax(test_Y[0:10],axis=1)
print 'Predicted digits:',test_pred[0:10]
print 'Actual images of the digits follow:'
for i in range(10):
        a[i].imshow(np.reshape(test_X[i],(28, 28)))

-- output --

Listing 2-19a.Display the Actual Digits Versus the Predicted Digits Along with the Images of the Actual Digits

```

图 [2-26](#Fig26) 显示了通过梯度下降全批次学习训练后，验证数据集样本的 SoftMax 分类的实际位数与预测位数。

### 使用随机梯度下降的 SoftMax 函数的多类分类

我们现在执行相同的分类任务，但是我们不使用整批学习，而是求助于批量大小为 1000 的随机梯度下降。清单 [2-20](#Par295) 中概述了详细的实现。

```
def read_infile():
    mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
    train_X, train_Y,test_X, test_Y = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels
    return train_X, train_Y,test_X, test_Y

def weights_biases_placeholder(n_dim,n_classes):
    X = tf.placeholder(tf.float32,[None,n_dim])
    Y = tf.placeholder(tf.float32,[None,n_classes])
    w = tf.Variable(tf.random_normal([n_dim,n_classes],stddev=0.01),name='weights')
    b = tf.Variable(tf.random_normal([n_classes]),name='weights')
    return X,Y,w,b

def forward_pass(w,b,X):
    out = tf.matmul(X,w) + b
    return out

def multiclass_cost(out,Y):
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out,labels=Y))
    return cost

def init():
    return tf.global_variables_initializer()

def train_op(learning_rate,cost):
    op_train = tf.train.AdamOptimizer(learning_rate).minimize(cost)
    return op_train

train_X, train_Y,test_X, test_Y = read_infile()
X,Y,w,b = weights_biases_placeholder(train_X.shape[1],train_Y.shape[1])
out = forward_pass(w,b,X)
cost = multiclass_cost(out,Y)
learning_rate,epochs,batch_size = 0.01,1000,1000
num_batches = train_X.shape[0]/batch_size
op_train = train_op(learning_rate,cost)
init = init()
epoch_cost_trace = []
epoch_accuracy_trace = []

with tf.Session() as sess:
    sess.run(init)

    for i in xrange(epochs):
        epoch_cost,epoch_accuracy = 0,0

        for j in xrange(num_batches):
            sess.run(op_train,feed_dict={X:train_X[j*batch_size:(j+1)*batch_size],Y:train_Y[j*batch_size:(j+1)*batch_size]})
            actual_batch_size = train_X[j*batch_size:(j+1)*batch_size].shape[0]
            epoch_cost += actual_batch_size*sess.run(cost,feed_dict={X:train_X[j*batch_size:(j+1)*batch_size],Y:train_Y[j*batch_size:(j+1)*batch_size]})

        epoch_cost = epoch_cost/float(train_X.shape[0])
        epoch_accuracy = np.mean(np.argmax(sess.run(out,feed_dict={X:train_X,Y:train_Y}),axis=1) == np.argmax(train_Y,axis=1))
        epoch_cost_trace.append(epoch_cost)
        epoch_accuracy_trace.append(epoch_accuracy)

        if (((i +1) >= 100) and ((i+1) % 100 == 0 )) :
            print 'Epoch:',(i+1),'Average loss:',epoch_cost,'accuracy:',epoch_accuracy

    print 'Final epoch training results:','Average loss:',epoch_cost,'accuracy:',epoch_accuracy
    loss_test = sess.run(cost,feed_dict={X:test_X,Y:test_Y})
    test_pred = np.argmax(sess.run(out,feed_dict={X:test_X,Y:test_Y}),axis=1)
    accuracy_test = np.mean(test_pred == np.argmax(test_Y,axis=1))
    print 'Results on test dataset:','Average loss:',loss_test,'accuracy:',accuracy_test

-- output --
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
Epoch: 100 Average loss: 0.217337096686 accuracy: 0.9388
Epoch: 200 Average loss: 0.212256691131 accuracy: 0.939672727273
Epoch: 300 Average loss: 0.210445133664 accuracy: 0.940054545455
Epoch: 400 Average loss: 0.209570150484 accuracy: 0.940181818182
Epoch: 500 Average loss: 0.209083143689 accuracy: 0.940527272727
Epoch: 600 Average loss: 0.208780818907 accuracy: 0.9406
Epoch: 700 Average loss: 0.208577176387 accuracy: 0.940636363636
Epoch: 800 Average loss: 0.208430663293 accuracy: 0.940636363636
Epoch: 900 Average loss: 0.208319870586 accuracy: 0.940781818182
Epoch: 1000 Average loss: 0.208232710849 accuracy: 0.940872727273
Final epoch training results: Average loss: 0.208232710849 accuracy: 0.940872727273
Results on test dataset: Average loss: 0.459194 accuracy: 0.9155

Listing 2-20.Multi-class Classification with Softmax Function Using Stochastic Gradient Descent

```

![A448418_1_En_2_Fig27_HTML.jpg](A448418_1_En_2_Fig27_HTML.jpg)

图 2-27。

Actual digits versus predicted digits for SoftMax classification through stochastic gradient descent

```
import matplotlib.pyplot as plt
%matplotlib inline
f, a = plt.subplots(1, 10, figsize=(10, 2))
print 'Actual digits:   ', np.argmax(test_Y[0:10],axis=1)
print 'Predicted digits:',test_pred[0:10]
print 'Actual images of the digits follow:'
for i in range(10):
        a[i].imshow(np.reshape(test_X[i],(28, 28)))

--output --

Listing 2-20a.Actual Digits Versus Predicted Digits for SoftMax Classification Through Stochastic Gradient Descent

```

图 [2-27](#Fig27) 显示了通过随机梯度下降训练后验证数据集样本的 SoftMax 分类的实际位数与预测位数。

## 国家政治保卫局。参见 OGPU

在我们结束本章之前，我们想谈谈 GPU，它彻底改变了深度学习世界。GPU 代表图形处理单元，最初用于游戏目的，以每秒显示更多屏幕，获得更好的游戏分辨率。深度学习网络使用大量矩阵乘法，尤其是卷积，用于正向传递和反向传播。GPU 擅长矩阵到矩阵的乘法；因此，几千个 GPU 核心被用来并行处理数据。这加快了深度学习训练的速度。市场上常见的 GPU 有

*   NVIDIA GTX 泰坦 XGE
*   NVIDIA GTX 泰坦 x
*   NVIDIA GeForce GTX 1080
*   NVIDIA GeForce GTX 1070

## 摘要

在这一章中，我们已经讲述了深度学习是如何从人工神经网络发展而来的。此外，我们讨论了感知器的学习方法，它的局限性，以及目前的训练神经网络的方法。详细讨论了与非凸成本函数、椭圆局部成本等值线和鞍点相关的问题，以及解决这些问题所需的不同优化器。此外，在本章的后半部分，我们学习了 TensorFlow 基础知识，以及如何通过 TensorFlow 执行与线性回归、多类 SoftMax 和 XOR 分类相关的简单模型。在下一章，重点将放在图像的卷积神经网络上。