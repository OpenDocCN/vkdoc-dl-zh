# 8.结合策略梯度和 Q-学习

到目前为止，在本书中，在深度学习与强化学习相结合的背景下，我们已经在第 [6](06.html) 章中查看了深度 Q 学习及其变体，并在第 [7](07.html) 章中查看了策略梯度。神经网络训练需要多次迭代，Q-learning 是一种非策略方法，它使我们能够多次使用转换，从而提高样本效率。然而，Q-learning 有时会不稳定。此外，这是一种间接的学习方式。我们不是直接学习一个最优策略，而是先学习 q 值，然后用这些动作值来学习最优行为。在第 7 章[中，我们看到了直接学习策略的方法，这给了我们更好的改进保证。然而，我们在第](07.html) [7](07.html) 章看到的所有政策都是政策上的。我们使用策略与环境进行交互，并对策略权重进行更新，以增加好的轨迹/动作的概率，同时减少坏的轨迹/动作的概率。然而，我们在策略上进行学习，因为在更新策略权重之后，先前的转换变得无效。

在这一章中，我们将着眼于结合这两种方法的优点，即非策略学习和直接学习策略。我们将首先讨论 Q 学习与政策梯度方法的权衡。在此之后，我们将研究三种将 Q 学习与策略梯度相结合的流行方法:深度确定性策略梯度(DDPG)、双延迟 DDPG (TD3)和软行动者批评(SAC)。我们将主要遵循 OpenAI Spinning Up 库中记录的符号、方法和示例代码。 <sup>[1](#Fn1)</sup>

## 政策梯度和 Q-Learning 的权衡

在第 7 章中，我们看了 DQN，深度学习版的 Q-learning。在 Q-learning(一种非策略方法)中，我们从探索性行为策略中收集转换，然后在批量随机梯度更新中使用这些转换来学习 Q 值。当我们学习 q 值时，我们通过取一个状态中所有可能行动的 q 值的最大值来选择最佳行动，从而改进策略。这是我们遵循的等式:

![$$ {w}_{t+1}={w}_t+\alpha .\kern0.5em \frac{1}{N}\sum \limits_{i=1}^N\left[{r}_i+\gamma\ {\mathit{\max}}_{a_i^{\prime }}\overset{\sim }{q}\left({s}_i^{\prime },{a}_i^{\prime };{w_t}^{-}\right)-\hat{q}\left({s}_i,{a}_i;w\right)\right].{\nabla}_w\ \hat{q}\left({s}_i,{a}_i;w\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ1.png)

(8.1)

请注意最大值。通过取最大值，即![$$ {\mathit{\max}}_{a_i^{\prime }}\overset{\sim }{q}\left({s}_i^{\prime },{a}_i^{\prime };{w_t}^{-}\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq1.png)，我们正在提高目标值![$$ {r}_i+\gamma\ {\mathit{\max}}_{a_i^{\prime }}\overset{\sim }{q}\left({s}_i^{\prime },{a}_i^{\prime };{w_t}^{-}\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq2.png)，这迫使当前状态动作 q 值![$$ \hat{q}\left({s}_i,{a}_i;w\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq3.png)更新权重以达到更高的目标。这就是我们强迫网络权重满足的贝尔曼最优方程。整个学习是偏离策略的，因为无论遵循什么策略，贝尔曼最优方程都适用于最优策略。所有的( *s* 、 *a* 、 *r* 、 *s* 、 <sup>'</sup> )转换都需要满足这一点，不管这些转换是使用哪个策略生成的。我们能够使用重放缓冲区重用转换，这使得学习非常简单高效。然而，Q-learning 也有一些问题。

第一个是关于使用 Q-learning 的当前形式进行持续的行动。请看![$$ {\mathit{\max}}_{a_i^{\prime }}\overset{\sim }{q}\left({s}_i^{\prime },{a}_i^{\prime };{w_t}^{-}\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq4.png)。我们在第 6 章中看到的所有例子都是离散动作的例子。你知道为什么吗？当动作空间是连续和多维的，例如一起移动机器人的多个关节时，你认为你将如何执行 *max* ？

当动作离散时，很容易取 *max* 。我们将状态 *s* 输入到模型中，对于所有可能的动作，我们得到 *Q* ( *s* ， *a* )。由于离散动作的数量有限，选择最大值*很容易。图 [8-1](#Fig1) 显示了一个样品模型。*

![../images/502835_1_En_8_Chapter/502835_1_En_8_Fig1_HTML.jpg](../images/502835_1_En_8_Chapter/502835_1_En_8_Fig1_HTML.jpg)

图 8-1

具有离散动作的 DQN 学习的一般模型

现在想象这些动作是连续的！你会如何取最大值？为了找到每个![$$ {s}_i^{\prime } $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq6.png)的![$$ {\mathit{\max}}_{a_i^{\prime }}\overset{\sim }{q}\left({s}_i^{\prime },{a}_i^{\prime };{w_t}^{-}\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq5.png)，我们必须运行另一个优化算法来找到最大值。这将是一个昂贵的过程，因为作为政策改进的一部分，需要对批次中的每个过渡执行该过程。

第二个问题是学习错误的目标。我们实际上想要一个最优的政策，但我们不会在 DQN 的直接领导下这样做。我们学习行动值函数，然后使用 *max* 找到最佳 q 值/最佳行动。

第三个问题是，DQN 有时也不稳定。没有理论上的保证，我们正在尝试使用我们在第 [5](05.html) 章中谈到的半梯度更新来更新权重。我们基本上是在尝试遵循目标不断变化的监督学习过程。我们所学的会影响新轨迹的生成，这反过来会影响我们的学习质量。我们看到的所有 DQN 平均回报的进度图都没有持续改善。它们非常不稳定，需要仔细调整超参数，以确保算法朝着好的策略发展。

最后，第四个问题是 DQN 学会了确定性政策。我们使用探索性行为策略来生成和探索代理在确定性策略中学习的内容。特别是在机器人领域的实验已经表明，一定量的随机策略更好，因为我们对世界的建模和关节的操作并不总是完美的。我们需要一些随机性来调整不完美的建模或动作值到实际机器人关节运动的转换。此外，确定性策略是随机策略的极限情况。

让我们把注意力转向政策梯度方法。在策略梯度方法中，我们输入状态，得到的输出是离散动作的动作概率或连续动作的概率分布的参数。我们可以看到，策略梯度允许我们学习离散和连续动作的策略。然而，学习连续动作在 DQN 是不可行的。图 [8-2](#Fig2) 显示了政策梯度中使用的模型。

![../images/502835_1_En_8_Chapter/502835_1_En_8_Fig2_HTML.jpg](../images/502835_1_En_8_Chapter/502835_1_En_8_Fig2_HTML.jpg)

图 8-2

政策梯度方法的政策网络。在第 [7](07.html) 章中，我们看到了不连续的动作，但是这个过程对于连续的动作也很有效，就像在那一章中解释的那样

此外，使用策略梯度方法，我们直接学习改进策略，而不是先学习值函数，然后使用它们来寻找最优策略的迂回方式。普通政策梯度确实遭遇了向坏区域的崩溃，我们看到了 TRPO 和 PPO 等方法控制步长以改善政策梯度的保证，从而产生更好的政策。

与 DQN 不同，政策梯度学习随机政策，因此探索是我们试图学习的政策的一部分。然而，政策梯度法的最大缺陷是，它是一种政策性方法。一旦我们使用转换来计算梯度更新，模型就移动到新的策略。在这个更新的政策世界中，早期的转变不再适用。我们需要在更新后丢弃以前的转换，并生成新的轨迹/转换来训练模型。这使得策略学习非常样本低效。

我们确实使用行动者-批评家方法将价值学习作为政策梯度的一部分，其中政策网络是试图学习最佳行动的行动者，而价值网络是告知政策网络行动好坏的批评家。然而，即使使用演员-评论家的方法，学习是在政策上。我们使用批评家来指导参与者，但是在更新策略(和/或价值)网络之后，我们仍然需要丢弃所有的转换。

有没有一种方法可以让我们直接学习策略，但同时利用 Q-learning 来学习非策略？对于连续行动空间，我们能这样做吗？这就是我们将在本章中讨论的内容。我们将把 Q-learning 和策略梯度结合起来，提出不依赖于策略的算法，并很好地用于连续动作。

## 结合策略梯度和 Q-Learning 的通用框架

我们将着眼于持续行动政策。我们将有两个网络。一个是学习给定状态下的最优行动，即行动者网络。假设策略网络用θ参数化，网络学习到一个策略，这个策略产生动作 a = μ <sub>θ</sub> ( *s* )，这个动作最大化 *Q* ( *s* ， *a* )。在数学符号中:

![$$ \underset{a^{\prime }}{\mathit{\max}}{Q}^{\ast}\left({s}^{\prime },{a}^{\prime}\right)\approx {Q}^{\ast}\left({s}^{\prime },{\upmu}_{\uptheta}(s)\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equa.png)

第二个网络，评论家网络，将再次把状态( *s* )作为一个输入，并把来自第一个网络的最优动作，*μ*<sub>*θ*</sub>(*s*)，作为另一个输入，以产生 q 值*q*<sub>*【ϕ*</sub>(*s*， *μ* <sub>*θ* 图 [8-3](#Fig3) 从概念上展示了网络的相互作用。</sub>

![../images/502835_1_En_8_Chapter/502835_1_En_8_Fig3_HTML.jpg](../images/502835_1_En_8_Chapter/502835_1_En_8_Fig3_HTML.jpg)

图 8-3

结合策略和 Q-learning。我们使用两个网络直接学习策略和 Q，在这两个网络中，来自第一个网络(行动者)的行动输出被馈送到第二个网络(批评家)，第二个网络学习 *Q* ( *s* ， *a* )

为了确保探索，我们要采取行动 *a* ，这是探索性的。这类似于我们在 Q-learning 中采用的方法，在 Q-learning 中，我们学习了确定性策略，但从探索性ε-贪婪策略生成了转换。同样，在这里，我们在学习 ***a*** 的同时，加入一点随机性 *ϵ* ~ *N* (0， *σ* <sup>2</sup> )并使用 ***a + ε*** 动作来探索环境，生成轨迹。

像 Q-learning 一样，我们将使用重放缓冲区来存储转换，并重用以前的转换来学习。这是我们将在本章中看到的所有方法的最大好处之一。它们将使策略学习偏离策略，从而提高样本效率。

在 Q-learning 中，我们必须使用一个目标网络，它是 Q-网络的副本。原因是在学习 q 值时提供某种文具目标。你可以在第 [5](05.html) 章和第 [6](06.html) 章重温目标网络的讨论。在这些方法中，目标网络权重定期用在线/代理网络权重更新。这里我们也将使用一个目标网络。然而，用于更新本章中算法的目标网络权重的方法将是 *polyak 平均* ( *指数平均*)的方法，如下式所示:

![$$ {\upphi}_{target}\leftarrow \uprho {\upphi}_{target}+\left(1-\uprho \right)\upphi $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ2.png)

(8.2)

我们还将使用策略网络的目标网络。这与提供稳定的目标 Q 值的原因相同，这允许我们执行梯度下降的监督学习风格，并将权重调整到近似的 *Q* ( *s* ， *a* )。

有了这个背景，我们就可以开始研究我们的第一个算法:深度确定性策略梯度。

## 深度确定性政策梯度

2016 年，在一篇题为“深度强化学习的连续控制”的论文中，来自 DeepMind 的作者 <sup>[2](#Fn2)</sup> 介绍了 DDPG 算法。作者对他们的方法提出了以下几点:

*   而 DQN 求解高维状态空间，只能处理离散的低维动作空间。DQN 不能应用于连续和高维的动作领域，例如，像机器人这样的物理控制任务。

*   由于*维数灾难*，离散化行动空间不是一个选项。假设你有一个有七个关节的机器人，每个关节可以在范围内移动( *k* 、 *k* )。让我们对每个关节进行粗略的离散化，每个关节有三个可能的值{*k*，0， *k* }。即使采用这种粗略的离散化，所有七个维度中的离散动作的总组合也达到 3 <sup>7</sup> = 2187。相反，如果我们决定将每个关节的离散范围划分为范围内的 10 个可能值( *k* ， *k* )，我们会得到 10 <sup>7</sup> = 10 *百万个*选项。这是维度的*诅咒*，其中可能的动作组合集合随着每个新维度/关节呈指数增长。

*   DDPG 是一种如下的算法:
    *   *型号自由*:我们不知道型号。我们从主体与环境的互动中学习。

    *   *偏离政策* : DDPG 和 DQN 一样，使用探索性政策来产生转变，并学习确定性政策。

    *   *连续高维动作空间* : DDPG 只对连续动作域有效，对高维动作空间也很有效。

    *   行动者-批评家:这意味着我们有一个行动者(政策网络)和批评家，行动-价值(q 值)网络。

    *   *重放缓冲器*:像 DQN 一样，DDPG 使用重放缓冲器来存储过渡，并利用它们来学习。这打破了训练示例的时间依赖性/相关性，否则会搞乱学习。

    *   *目标网络*:和 DQN 一样，它使用目标网络为 q 值学习提供相当稳定的目标。然而，与 DQN 不同，它不通过定期复制在线/代理/主网络的权重来更新目标网络。相反，它使用 polyak/指数平均值在每次更新主网络后将目标网络移动一点点。

现在让我们将注意力转向网络架构和我们计算的损耗。首先让我们看看 Q-learning 部分，然后我们将看看政策学习网络。

### DDPG 的 Q-Learning(评论家)

在 DQN，我们计算了通过梯度下降最小化的损失。损失由方程式( [6)给出。3](06.html#Equ3) )，转载于此:

![$$ L=\frac{1}{N}{\sum}_{i=1}^N{\left[{r}_i+\left(\left(1- don{e}_i\right).\upgamma .\underset{a_i^{\prime }}{\max}\hat{q}\left({s}_i^{\prime },{a}_i^{\prime };{w}_t^{-}\right)\right)\hbox{--} \hat{q}\left({s}_i,{a}_i;{w}_t\right)\right]}^2 $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ3.png)

(8.3)

让我们重写等式。我们将删除 subindex ***i*** 和 ***t*** 以减少符号的混乱。我们将求和改为期望，以强调我们通常想要的是一个期望，但它是在蒙特卡罗下通过样本的平均值来估计的。最终在代码中，我们得到了和，但是它们是一些期望值的蒙特卡罗估计。我们还将用ϕ <sub>*目标*</sub> 替换目标网络权重![$$ {w}_t^{-} $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq7.png)。同样，我们将主要权重 *w* <sub>* t *</sub> 替换为ϕ.进一步，我们将把权重从函数参数内部移到函数上的子索引，即*q*<sub>ϕ</sub>(…)←*q*(…)。；ϕ).由于所有这些符号变化，等式( [8.3](#Equ3) )看起来像这样:

![$$ L\left(\upphi, D\right)=\underset{\left(s,a,r,{s}^{'},d\right)\sim D}{E}\left[{\left({Q}_{\upphi}\left(s,a\right)-\left(r+\upgamma \left(1-d\right)\underset{a^{\prime }}{\mathit{\max}}{Q}_{\upphi_{targ}}\left({s}^{\prime },{a}^{\prime}\right)\right)\right)}^2\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ4.png)

(8.4)

这仍然是 DQN 公式，我们在状态(*s*<sup>’</sup>)中取最大离散动作来得到![$$ \underset{a^{\prime }}{\mathit{\max}}{Q}_{\phi_{targ}}\left({s}^{\prime },{a}^{\prime}\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq8.png)。在连续空间中，我们不能取 *max* ，因此我们有另一个网络(actor)来取输入状态 *s* 并产生动作，这使![$$ {Q}_{\phi_{targ}}\left({s}^{\prime },{a}^{\prime}\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq9.png)最大化；即，我们用![$$ {Q}_{\upphi_{targ et}}\left({s}^{'},{\upmu}_{\uptheta_{targ}}\left({s}^{'}\right)\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq11.png)替换![$$ \underset{a^{\prime }}{\mathit{\max}}{Q}_{\phi_{targ}}\left({s}^{\prime },{a}^{\prime}\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq10.png)，其中![$$ {a}^{\prime }={\mu}_{\theta_{targ}}\left({s}^{'}\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq12.png)是目标策略。更新后的损失表达式如下:

![$$ L\left(\phi, D\right)=\underset{\left(s,a,r,{s}^{'},d\right)\sim D}{E}\left[{\left({Q}_{\phi}\left(s,a\right)-\left(r+\gamma \left(1-d\right){Q}_{\phi_{targ et}}\left({s}^{'},{\mu}_{\theta_{targ}}\left({s}^{'}\right)\right)\right)\right)}^2\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ5.png)

(8.5)

这是更新的均方贝尔曼误差(MSBE)，我们将在代码中实现，然后进行反向传播，以最小化损失函数。请注意，这只是ϕ的函数，因此 *L* ( *ϕ* ， *D* )的梯度是相对于ϕ.的如前所述，在代码中，我们将用样本平均值代替期望值，样本平均值是期望值的 MC 估计值。

接下来我们来看看政策学习部分。

### DDPG 的政策学习(演员)

在策略学习部分，我们试图学习 *a* = μ <sub>θ</sub> ( *s* )，一个确定性策略，给出最大化 *Q* <sub>ϕ</sub> ( *s* ， *a* )的动作。由于动作空间是连续的，并且我们假设 Q 函数相对于动作是可微的，我们可以相对于要求解的策略参数执行梯度上升。

![$$ \underset{\theta }{\mathit{\max}}\ J\left(\theta, D\right)=\underset{\theta }{\mathit{\max}}\underset{s\sim D}{E}\left[{Q}_{\upphi}\left(s,{\upmu}_{\uptheta}(s)\right)\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ6.png)

(8.6)

由于策略是确定性的，( [8.6](#Equ6) )中的期望不依赖于策略，这与我们在前一章的随机梯度中看到的不同。那里的期望算子依赖于策略参数，因为策略是随机的，这反过来影响期望的 q 值。

我们可以取 *J* 相对于 *θ* 的梯度，得到如下结果:

![$$ {\nabla}_{\uptheta}J\left(\uptheta, D\right)=\underset{s\sim D}{E}\left[{\nabla}_a{Q}_{\upphi}\left(s,a\right){\left.\kern0em \right|}_{a={\upmu}_{\upphi}(s)}{\nabla}_{\upphi}{\upmu}_{\upphi}(s)\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ7.png)

(8.7)

这是链式法则的直接应用。还请注意，我们在期望中没有得到任何∇ *对数* (…)项，因为对其进行期望的状态 *s* 来自重放缓冲器，并且它与关于哪个梯度进行期望的参数θ无关。

此外，在 2014 年题为“确定性政策梯度算法”的论文中， <sup>[3](#Fn3)</sup> 作者表明，方程( [8.7](#Equ7) )是政策梯度，即政策绩效的梯度。建议你通读这两篇论文，以便对 DDPG 背后的数学有更深入的理论理解。

如前所述，为了帮助探索，当我们学习确定性策略时，我们将使用所学策略的嘈杂探索版本来探索和生成转换。我们通过向学习策略中添加平均零高斯噪声来做到这一点。

### 伪代码和实现

至此，我们已经准备好给出完整的伪代码了。请参见图 [8-4](#Fig4) 进行说明。

Deep Deterministic Policy Gradient

![../images/502835_1_En_8_Chapter/502835_1_En_8_Fig4_HTML.png](../images/502835_1_En_8_Chapter/502835_1_En_8_Fig4_HTML.png)

图 8-4

深度确定性策略梯度算法

#### 代码中使用的健身房环境

转向实现，我们将在本章中使用两个环境来运行代码。第一个是称为`Pendulum-v0` `.`的钟摆摆动环境，这里的状态是给出钟摆角度的三维向量(即，其 *cos* 和 *sin* 分量)，第三维是角速度(θ点):![$$ \left[\mathit{\cos}\left(\uptheta \right),\mathit{\sin}\left(\uptheta \right),\dot{\theta}\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq13.png)。这个动作是一个单一的值，力矩施加在钟摆上。这个想法是尽可能长时间地保持钟摆直立。参见图 [8-5](#Fig5) 。

![../images/502835_1_En_8_Chapter/502835_1_En_8_Fig5_HTML.jpg](../images/502835_1_En_8_Chapter/502835_1_En_8_Fig5_HTML.jpg)

图 8-5

来自开放体育馆图书馆的钟摆环境

在我们在这个具有一维动作空间的简单连续动作环境上训练网络之后，我们将研究另一个称为月球着陆器连续环境的环境:`LunarLanderContinuous-v2`。在这种环境下，我们试图将登月舱降落在月球的两面旗帜之间。状态向量是八维的:`[x_pos, y_pos, x_vel, y_vel, lander_angle, lander_angular_vel, left_leg_ground_contact_flag, right_leg_ground_contact_flag]`。

动作是二维浮动:`[main engine, left-right engines]`。

*   *主机* : -1..0 表示发动机关闭，范围(0，1)是从 50%到 100%功率的发动机油门。发动机不能在低于 50%的功率下工作。

*   *左右* : `range(-1.0, -0.5)`点火左发动机，`range(+0.5, +1.0)`点火右发动机，`range(-0.5, 0.5)`两个发动机都关。

图 [8-6](#Fig6) 显示了环境的快照。

![../images/502835_1_En_8_Chapter/502835_1_En_8_Fig6_HTML.jpg](../images/502835_1_En_8_Chapter/502835_1_En_8_Fig6_HTML.jpg)

图 8-6

月球着陆器连续从开放体育馆图书馆

### 代码列表

现在让我们把注意力转向实现图 [8-4](#Fig4) 中给出的 DDPG 伪代码的实际代码。代码来自文件`listing8_1_ddpg_pytorch.ipynb.`我们在 TensorFlow 2.0 的文件`listing8_1_ddpg_tensorflow.ipynb.`中也有完整的实现，所有的代码遍历都将借用这两个文件中的代码片段。我们将首先讨论 Q 和策略网络，然后是损失计算，然后是训练循环。最后，我们将讨论运行和测试经过培训的代理的性能的代码。

#### 政策网络行动者

首先让我们看看*演员/政策*网络。清单 [8-1](#PC1) 显示了 PyTorch 中的策略网络代码。我们定义了一个简单的神经网络，它有两个大小为 256 的隐藏层，每个层都有 ReLU 激活。如果你查看函数`forward,`，你会注意到最后一层(`self.actor`)通过了`tanh`激活。`Tanh`是挤压功能；它将(∞，∞)中的值重新映射到一个压缩范围`(-1,1)`。然后，我们将该压缩值乘以动作限值(`self.act_limit`)，以便`MLPActor`的连续输出在环境可接受的动作值的有效范围内。我们通过扩展 PyTorch `nn.Module`类来创建我们的网络类，这需要我们定义一个`forward`函数，将输入状态 *S* 作为产生动作值作为网络输出的参数。

```
class MLPActor(nn.Module):
    def __init__(self, state_dim, act_dim, act_limit):
        super().__init__()
        self.act_limit = act_limit
        self.fc1 = nn.Linear(state_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.actor = nn.Linear(256, act_dim)

    def forward(self, s):
        x = self.fc1(s)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.actor(x)
        x = torch.tanh(x)  # to output in range(-1,1)
        x = self.act_limit * x
        return x

Listing 8-1Policy Network in PyTorch

```

#### 政策网络参与者(TensorFlow)

清单 [8-2](#PC2) 包含了 TensorFlow 2.0 中相同的函数。它非常类似于 PyTorch 实现，除了我们子类化了`tf.keras.Model`而不是`nn.Module`。我们在一个名为`call`的函数中实现网络转发逻辑，而不是在函数`forward`中。此外，层的命名方式也有细微的差别，例如`dense`与`linear`以及层的维度传递方式。

```
class MLPActor(tf.keras.Model):
    def __init__(self, state_dim, act_dim, act_limit):
        super().__init__()
        self.act_limit = act_limit
        self.fc1 = layers.Dense(256, activation="relu")
        self.fc2 = layers.Dense(256, activation="relu")
        self.actor = layers.Dense(act_dim)

    def call(self, s):
        x = self.fc1(s)
        x = self.fc2(x)
        x = self.actor(x)
        x = tf.keras.activations.tanh(x)  # to output in range(-1,1)
        x = self.act_limit * x
        return x

Listing 8-2Policy Network in TensorFlow

```

#### q-网络评论家实现

接下来我们来看看 Q-network ( *评论家*)。这也是一个简单的两层隐藏网络，具有 ReLU 激活，然后是最后一层，输出数量等于 1。最后一层没有任何激活，使网络能够产生任何值作为网络的输出。该网络输出 q 值，因此我们需要一个可能的范围(∞，∞)。

##### PyTorch

清单 [8-3](#PC3) 显示了 PyTorch 中批评家网络的代码，清单 [8-4](#PC4) 显示了 TensorFlow 中的代码。它们非常类似于参与者/策略网络的实现，除了前面讨论的一些小的区别。

```
class MLPQFunction(nn.Module):
    def __init__(self, state_dim, act_dim):
        super().__init__()
        self.fc1 = nn.Linear(state_dim+act_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.Q = nn.Linear(256, 1)

    def forward(self, s, a):
        x = torch.cat([s,a], dim=-1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        q = self.Q(x)
        return torch.squeeze(q, -1)

Listing 8-3Q/Critic Network in PyTorch

```

##### TensorFlow

清单 [8-4](#PC4) 显示了 TensorFlow 中 critic 网络的代码。

```
class MLPQFunction(tf.keras.Model):
    def __init__(self, state_dim, act_dim):
        super().__init__()
        self.fc1 = layers.Dense(256, activation="relu")
        self.fc2 = layers.Dense(256, activation="relu")
        self.Q = layers.Dense(1)

    def call(self, s, a):
        x = tf.concat([s,a], axis=-1)
        x = self.fc1(x)
        x = self.fc2(x)
        q = self.Q(x)
        return tf.squeeze(q, -1)

Listing 8-4Q/Critic Network in TensorFlow

```

##### 组合的模型-参与者评论实现

一旦这两个网络都定义好了，我们就把它们组合成一个类，这样我们就可以以一种更加模块化的方式来管理在线网络和目标网络。这只是为了更好地组织代码，仅此而已。结合两个网络的类实现为`MLPActorCritic`。在这个类中，我们还定义了一个函数`get_action`，它接受状态和噪声比例。它通过策略网络传递状态得到*μ*<sub>T5】θ</sub>(*s*)，然后它加上一个噪声(零均值高斯噪声)给出一个有噪声的动作进行探索。这是实现算法步骤 4 的函数:

![$$ a= clip\left({\mu}_{\theta }(s)+\epsilon, {a}_{Low},{a}_{High}\right),\mathrm{where}\kern0.5em \epsilon \sim N $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equg.png)

清单 [8-5](#PC5) 展示了 PyTorch 中`MLPActorCritic`的实现，清单 [8-6](#PC6) 展示了 TensorFlow 版本。实现非常相似，除了个别的细微差别，比如子类化哪个类。另一个区别是，在 PyTorch 中，您需要将 NumPy 数组转换为 Torch 张量，然后才能通过网络传递它们，而在 TensorFlow 中，我们可以直接将 NumPy 数组传递给模型。

```
class MLPActorCritic(tf.keras.Model):
    def __init__(self, observation_space, action_space):
        super().__init__()
        self.state_dim = observation_space.shape[0]
        self.act_dim = action_space.shape[0]
        self.act_limit = action_space.high[0]

        #build Q and policy functions
        self.q = MLPQFunction(self.state_dim, self.act_dim)
        self.policy = MLPActor(self.state_dim, self.act_dim, self.act_limit)

    def act(self, state):
        return self.policy(state).numpy()

    def get_action(self, s, noise_scale):
        a = self.act(s.reshape(1,-1).astype("float32")).reshape(-1)
        a += noise_scale * np.random.randn(self.act_dim)
        return np.clip(a, -self.act_limit, self.act_limit)

Listing 8-6MLPActorCritic in TensorFlow

```

```
class MLPActorCritic(nn.Module):
    def __init__(self, observation_space, action_space):
        super().__init__()
        self.state_dim = observation_space.shape[0]
        self.act_dim = action_space.shape[0]
        self.act_limit = action_space.high[0]

        #build Q and policy functions
        self.q = MLPQFunction(self.state_dim, self.act_dim)
        self.policy = MLPActor(self.state_dim, self.act_dim, self.act_limit)

    def act(self, state):
        with torch.no_grad():
            return self.policy(state).numpy()

    def get_action(self, s, noise_scale):
        a = self.act(torch.as_tensor(s, dtype=torch.float32))
        a += noise_scale * np.random.randn(self.act_dim)
        return np.clip(a, -self.act_limit, self.act_limit)

Listing 8-5MLPActorCritic in PyTorch

```

#### 体验回放

像 DQN 一样，我们使用经验回放。这和我们在 DDPG 使用的 PyTorch 和 TensorFlow 来自 DQN 的版本是一样的。它是使用 NumPy 数组实现的，同样的代码适用于 PyTorch 和 TensorFlow。因为它是从 DQN 借来的相同的实现，我们不给出它的代码。要查看`ReplayBuffer`的代码，请查看 Jupyter 笔记本中的实现。

#### q 损耗实现

接下来，我们来看看 Q 损耗的计算。我们实际上是在实现伪代码的步骤 11 和 12 中的等式。

![$$ y\left(r,{s}^{\prime },d\right)=r+\upgamma \left(1-d\right){Q}_{targ}\left({s}^{\prime },{\upmu}_{\uptheta_{targ}}\left({s}^{\prime}\right)\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equh.png)

![$$ {\nabla}_{\upphi}\frac{1}{\mid B\mid }{\sum}_{\left(s,a,r,{s}^{\prime },d\right)\in B}{\left({Q}_{\upphi}\left(s,a\right)-y\left(r,{s}^{\prime },d\right)\right)}^2 $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equi.png)

#### PyTorch

清单 8-7 给出了 PyTorch 的实现。我们先把这批( *s* ， *a* ， *r* ， *s* ， <sup>'</sup> ， *d* )转换成 PyTorch 张量。接下来我们用批( *s* ， *a* )计算 *Q* ， <sub>ϕ</sub> ( *s* ， *a* )，并通过*策略*网络传递。接下来，我们根据上式计算目标值 *y* ( *r* ， *s* ，<sup>′</sup>， *d* )。在计算目标时，我们使用`with torch.no_grad()`来停止梯度计算，因为我们不想使用 PyTorch 中的 auto-diff 来调整目标网络权重。我们将使用 polyak 平均手动调整目标网络权重。停止计算不需要的梯度可以加快训练速度，还可以确保梯度步长不会对您想要保持冻结或手动调整的权重产生任何意外的副作用。最后，我们计算损失。

![$$ {Q}_{Loss}=\frac{1}{\mid B\mid }{\sum}_{\left(s,a,r,{s}^{\prime },d\right)\in B}{\left({Q}_{\upphi}\left(s,a\right)-y\left(r,{s}^{\prime },d\right)\right)}^2 $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equj.png)

PyTorch 执行反向传播来计算梯度。我们不需要在代码中明确计算梯度。

```
def compute_q_loss(agent, target_network, states, actions, rewards, next_states, done_flags,
                    gamma=0.99):

    # convert numpy array to torch tensors
    states = torch.tensor(states, dtype=torch.float)
    actions = torch.tensor(actions, dtype=torch.float)
    rewards = torch.tensor(rewards, dtype=torch.float)
    next_states = torch.tensor(next_states, dtype=torch.float)
    done_flags = torch.tensor(done_flags.astype('float32'),dtype=torch.float)

    # get q-values for all actions in current states
    # use agent network
    predicted_qvalues = agent.q(states, actions)

    # Bellman backup for Q function
    with torch.no_grad():
        q__next_state_values = target_network.q(next_states, target_network.policy(next_states))
        target = rewards + gamma * (1 - done_flags) * q__next_state_values

    # MSE loss against Bellman backup
    loss_q = ((predicted_qvalues - target)**2).mean()

    return loss_q

Listing 8-7Q-Loss Computation in PyTorch

```

#### TensorFlow

TensorFlow 版本类似，清单 [8-8](#PC8) 列出了完整的实现。与 PyTorch 的主要区别在于，我们没有将 NumPy 数组转换为张量。但是，我们将数据类型转换为`float32`,使其与网络权重的默认数据类型兼容。请记住，对于所有 TensorFlow 实现，我们都使用热切执行模式，与 PyTorch 类似，我们使用`with tape.stop_recording()`来停止目标网络中的梯度计算。

```
def compute_q_loss(agent, target_network, states, actions, rewards, next_states, done_flags,
                    gamma, tape):

    # convert numpy array to proper data types
    states = states.astype('float32')
    actions = actions.astype('float32')
    rewards = rewards.astype('float32')
    next_states = next_states.astype('float32')
    done_flags = done_flags.astype('float32')

    # get q-values for all actions in current states
    # use agent network
    predicted_qvalues = agent.q(states, actions)

    # Bellman backup for Q function
    with tape.stop_recording():
        q__next_state_values = target_network.q(next_states, target_network.policy(next_states))
        target = rewards + gamma * (1 - done_flags) * q__next_state_values

    # MSE loss against Bellman backup
    loss_q = tf.reduce_mean((predicted_qvalues - target)**2)

    return loss_q

Listing 8-8Q-Loss Computation in TensorFlow

```

#### 保单损失执行

接下来，我们按照伪代码的第 13 步计算保单损失。

![$$ {Policy}_{Loss}=-\frac{1}{\left|B\right|}{\sum}_{s\in B}{Q}_{\phi}\left(s,{\mu}_{\theta }(s)\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equk.png)

这是一个简单的计算。它只是 PyTorch 和 TensorFlow 中的一个三行代码实现。清单 [8-9](#PC9) 包含 PyTorch 版本，清单 [8-10](#PC10) 包含 TensorFlow 版本。请注意损失中的`-ve`符号。我们的算法需要在策略目标上做梯度上升，但是像 PyTorch 和 TensorFlow 这样的自动微分库实现梯度下降。将政策目标乘以-1.0 会导致亏损，亏损的梯度下降与政策目标的梯度上升相同。

```
def compute_policy_loss(agent, states, tape):

    # convert numpy array to proper data type
    states = states.astype('float32')

    predicted_qvalues = agent.q(states, agent.policy(states))

    loss_policy = - tf.reduce_mean(predicted_qvalues)

    return loss_policy

Listing 8-10Policy-Loss Computation in TensorFlow

```

```
def compute_policy_loss(agent, states):

    # convert numpy array to torch tensors
    states = torch.tensor(states, dtype=torch.float)

    predicted_qvalues = agent.q(states, agent.policy(states))

    loss_policy = - predicted_qvalues.mean()

    return loss_policy

Listing 8-9Policy-Loss Computation in PyTorch

```

#### 一步更新实施

接下来，我们定义一个名为`one_step_update`的函数，它获取一批( *s* ， *a* ， *r* ， *s* <sup>'</sup> ， *d* )并计算 Q 损失，随后是反向传播，然后是类似的策略损失计算步骤，随后是梯度步骤。最后，它使用 polyak 平均对目标网络权重进行更新。本质上，这个步骤和前面的两个函数`compute_q_loss`和`compute_policy_loss`一起实现了伪代码的步骤 11 到 14。

清单 [8-11](#PC11) 显示了`one_step_update`的 PyTorch 版本。第一步是计算 Q 损失，并对 critic/Q 网络权重执行梯度下降。然后，我们冻结 Q-网络权重，使得策略网络上的梯度下降不会影响 Q-网络的权重。随后计算行动者/策略网络权重的策略损失和梯度下降。我们再次解冻 Q-网络权重。最后，我们使用 polyak 平均来更新目标网络权重。

```
def one_step_update(agent, target_network, q_optimizer, policy_optimizer,
                    states, actions, rewards, next_states, done_flags,
                    gamma=0.99, polyak=0.995):

    #one step gradient for q-values
    q_optimizer.zero_grad()
    loss_q = compute_q_loss(agent, target_network, states, actions, rewards, next_states, done_flags,
                    gamma)
    loss_q.backward()
    q_optimizer.step()

    #Freeze Q-network
    for params in agent.q.parameters():
        params.requires_grad = False

    #one setep gradient for policy network
    policy_optimizer.zero_grad()
    loss_policy = compute_policy_loss(agent, states)
    loss_policy.backward()
    policy_optimizer.step()

    #UnFreeze Q-network
    for params in agent.q.parameters():
        params.requires_grad = True

    # update target networks with polyak averaging
    with torch.no_grad():
        for params, params_target in zip(agent.parameters(), target_network.parameters()):
            params_target.data.mul_(polyak)
            params_target.data.add_((1-polyak)*params.data)

Listing 8-11One-Step Update in PyTorch

```

清单 [8-12](#PC12) 给出了`one_step_update`的 TensorFlow 版本，类似于 PyTorch 实现流程。不同之处在于计算梯度的方式、在每个库中采用梯度步长的方式、冻结和解冻权重的方式以及更新目标网络权重的方式。逻辑是一样的；这只是调用哪个库函数和传递什么参数的区别——基本上是两个库的语法区别。

```
def one_step_update(agent, target_network, q_optimizer, policy_optimizer,
                    states, actions, rewards, next_states, done_flags,
                    gamma=0.99, polyak=0.995):

    #one step gradient for q-values
    with tf.GradientTape() as tape:
        loss_q = compute_q_loss(agent, target_network, states, actions, rewards, next_states, done_flags,
                    gamma, tape)

        gradients = tape.gradient(loss_q, agent.q.trainable_variables)
        q_optimizer.apply_gradients(zip(gradients, agent.q.trainable_variables))

    #Freeze Q-network
    agent.q.trainable=False

    #one setep gradient for policy network
    with tf.GradientTape() as tape:
        loss_policy = compute_policy_loss(agent, states, tape)
        gradients = tape.gradient(loss_policy, agent.policy.trainable_variables)
        policy_optimizer.apply_gradients(zip(gradients, agent.policy.trainable_variables))

    #UnFreeze Q-network
    agent.q.trainable=True

    # update target networks with polyak averaging
    updated_model_weights = []
    for weights, weights_target in zip(agent.get_weights(), target_network.get_weights()):
        new_weights = polyak*weights_target+(1-polyak)*weights
        updated_model_weights.append(new_weights)
    target_network.set_weights(updated_model_weights)

Listing 8-12One-Step Update in TensorFlow

```

#### DDPG:主循环

最后一步是 DDPG 算法的实现，它使用了前面的`one_step_update`函数。它创建优化器并初始化环境。它使用当前的在线策略在环境中不断地步进。最初，对于第一个`start_steps=10000`，它采取一个随机动作来探索环境，一旦收集到足够多的转换，它就使用当前的带有噪声的策略来选择动作。转换被添加到`ReplayBuffer`，如果缓冲区已满，则从缓冲区中删除最早的一个。`update_after`告诉算法仅在`update_after=1000`转换被收集到缓冲器后才开始进行梯度更新。代码按照参数`epoch=5`的定义多次运行循环。我们使用了`epoch=5`进行演示。你可能想要运行更长的时间，比如说 100 个纪元左右。这绝对是推荐给月球着陆器环境的。我们在清单 [8-13](#PC13) 中仅给出 PyTorch 版本的清单。

```
def ddpg(env_fn, seed=0,
         steps_per_epoch=4000, epochs=5, replay_size=int(1e6), gamma=0.99,
         polyak=0.995, policy_lr=1e-3, q_lr=1e-3, batch_size=100, start_steps=10000,
         update_after=1000, update_every=50, act_noise=0.1, num_test_episodes=10,
         max_ep_len=1000):

    torch.manual_seed(seed)
    np.random.seed(seed)

    env, test_env = env_fn(), env_fn()

    ep_rets, ep_lens = [], []

    state_dim = env.observation_space.shape
    act_dim = env.action_space.shape[0]

    act_limit = env.action_space.high[0]

    agent = MLPActorCritic(env.observation_space, env.action_space)
    target_network = deepcopy(agent)

    # Freeze target networks with respect to optimizers (only update via polyak averaging)
    for params in target_network.parameters():
        params.requires_grad = False

    # Experience buffer

    replay_buffer = ReplayBuffer(replay_size)

    #optimizers
    q_optimizer = Adam(agent.q.parameters(), lr=q_lr)
    policy_optimizer = Adam(agent.policy.parameters(), lr=policy_lr)

    total_steps = steps_per_epoch*epochs
    state, ep_ret, ep_len = env.reset(), 0, 0

    for t in range(total_steps):
        if t > start_steps:
            action = agent.get_action(state, act_noise)
        else:
            action = env.action_space.sample()

        next_state, reward, done, _ = env.step(action)
        ep_ret += reward
        ep_len += 1

        # Ignore the "done" signal if it comes from hitting the time
        # horizon (that is, when it's an artificial terminal signal
        # that isn't based on the agent's state)
        done = False if ep_len==max_ep_len else done

        # Store experience to replay buffer
        replay_buffer.add(state, action, reward, next_state, done)

        state = next_state

        # End of trajectory handling
        if done or (ep_len == max_ep_len):
            ep_rets.append(ep_ret)
            ep_lens.append(ep_len)
            state, ep_ret, ep_len = env.reset(), 0, 0

        # Update handling
        if t >= update_after and t % update_every == 0:
            for _ in range(update_every):
                states, actions, rewards, next_states, done_flags = replay_buffer.sample(batch_size)

                one_step_update(
                        agent, target_network, q_optimizer, policy_optimizer,
                        states, actions, rewards, next_states, done_flags,
                        gamma, polyak
                )

        # End of epoch handling
        if (t+1) % steps_per_epoch == 0:
            epoch = (t+1) // steps_per_epoch

            avg_ret, avg_len = test_agent(test_env, agent, num_test_episodes, max_ep_len)
            print("End of epoch: {:.0f}, Training Average Reward: {:.0f}, Training Average Length: {:.0f}".format(epoch, np.mean(ep_rets), np.mean(ep_lens)))
            print("End of epoch: {:.0f}, Test Average Reward: {:.0f}, Test Average Length: {:.0f}".format(epoch, avg_ret, avg_len))
            ep_rets, ep_lens = [], []

    return agent

Listing 8-13DDPG Outer Training Loop in PyTorch

```

TensorFlow 版本非常相似，除了在调用哪个库函数和如何传递参数上有微小的区别。在 TensorFlow 版本中，我们有一段额外的代码来初始化网络权重，这样我们就可以在开始训练之前冻结目标网络权重。按照我们构建模型的方式，直到训练的第一步才构建模型，因此我们需要在 TensorFlow 中添加额外的代码来强制构建模型。我们不会给出 TensorFlow 版本的代码清单。

剩下的代码是训练代理，然后记录被训练的代理的表现。我们首先为钟摆环境运行该算法，然后为月球着陆器健身房环境运行该算法。这些代码版本读起来很有趣，但是因为它们是我们学习 DDPG 的目标的附带内容，所以我们不会深入这些代码实现的细节。然而，感兴趣的读者可能希望查阅相关的库文档并逐步阅读代码。

这就完成了代码实现演练。我们可以看到，即使经过五个时期的训练，代理人能够在简单的摆环境中表现得非常好，并且能够在更复杂的月球着陆环境中表现得相当好。

接下来，我们将看看孪生延迟 DDPG，也称为 TD3。它有一些其他的增强和技巧来解决一些在 DDPG 看到的稳定性和收敛速度问题。

## 孪生延迟 DDPG

双延迟 DDPG 于 2018 年在一篇题为“解决演员-评论家方法中的函数逼近误差”的论文中提出 <sup>[4](#Fn4)</sup> DDPG 患有我们在第 [4](04.html) 章的 Q-learning 中看到的高估偏差(在“最大化偏差和双 Q 学习”一节)。我们在第 [6](06.html) 章中看到了双 DQN 方法，通过解耦最大化动作和最大 q 值来解决偏差。在前面提到的论文中，作者表明 DDPG 也遭受同样的高估偏差。他们提出了一种双 Q 学习的变体，解决了 DDPG 的这种高估偏差。该方法使用以下修改:

*   *削波双 Q 学习* : TD3 使用两个独立的 Q 函数，在贝尔曼方程下形成目标时取两者中的最小值，即图 [8-4](#Fig4) 中 DDPG 伪代码第 11 步中的目标。这种修改就是该算法被称为*孪生*的原因。

*   *延迟策略更新*:与 Q 函数更新相比，TD3 更新策略和目标网络的频率较低。该论文建议对于 Q 函数的每两次更新，对策略和目标网络进行一次更新。这意味着在图 [8-4](#Fig4) 中的 DDPG 伪代码的步骤 13 和 14 中，对于步骤 11 和 12 中的 Q 函数的每两次更新，执行一次更新。这个修改就是把这个算法叫做*延迟*的原因。

*   *目标策略平滑* : TD3 给目标动作增加了噪声，使得策略更难利用 Q 函数估计误差和控制高估偏差。

### 目标政策平滑

用于计算目标 *y* ( *r* ， *s* <sup>'</sup> ， *d* )的动作基于目标网络。在 DDPG，我们在图 [8-4](#Fig4) 的步骤 11 中计算![$$ {a}^{\prime}\left({s}^{\prime}\right)={\upmu}_{\uptheta_{targ}}\left({s}^{\prime}\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq14.png)。然而，在 TD3 中，我们通过向动作添加噪声来执行目标策略平滑。对于确定性动作![$$ {\upmu}_{\uptheta_{targ}}\left({s}^{\prime}\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq15.png)，我们添加了一个带有一些剪辑范围的平均零高斯噪声。然后使用 *tanh* 进一步剪切动作，并乘以`max_action_range`以确保动作值在可接受的动作值范围内。

![$$ {a}^{\prime}\left({s}^{\prime}\right)=\mathrm{clip}\left({\upmu}_{\uptheta_{\mathrm{targ}}}\left({s}^{\prime}\right)+\mathrm{clip}\left(\upepsilon, -c,c\right),{a}_{Low},{a}_{High}\right),\kern1.25em \upepsilon \sim \mathcal{N}\left(0,\upsigma \right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ8.png)

(8.8)

### Q-Loss(评论家)

我们使用两个独立的 Q 函数，并从使用两个独立的 Q 函数中的最小值的公共目标学习它们。以数学方式表达目标看起来像这样:

![$$ y\left(r,{s}^{\prime },d\right)=r+\upgamma \left(1-d\right)\underset{i=1,2}{\min }{Q}_{\upphi_{\mathrm{targ},i}}\left({s}^{\prime },{a}^{\prime}\left({s}^{\prime}\right)\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ9.png)

(8.9)

我们首先利用方程( [8.8](#Equ8) )来寻找有噪声的目标动作*a*<sup>’</sup>(*s*<sup>’</sup>)。这又用于计算目标 Q 值:第一和第二 Q 目标网络的 Q 值:![$$ {Q}_{\phi_{\mathrm{targ},1}}\left({s}^{\prime },{a}^{\prime}\left({s}^{\prime}\right)\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq16.png)和![$$ {Q}_{\phi_{\mathrm{targ},2}}\left({s}^{\prime },{a}^{\prime}\left({s}^{\prime}\right)\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq17.png)。

( [8.9](#Equ9) )中的共同目标用于查找两个 Q 网络的损耗，如下所示:

![$$ {Q}_{Loss,1}=\frac{1}{B}{\sum}_{\left(s,a,r,{s}^{\prime },d\right)\in B}{\left({Q}_{\upphi_1}\left(s,a\right)-y\left(r,{s}^{\prime },d\right)\right)}^2 $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equl.png)

还有这里:

![$$ {Q}_{Loss,2}=\frac{1}{B}{\sum}_{\left(s,a,r,{s}^{\prime },d\right)\in B}{\left({Q}_{\upphi_2}\left(s,a\right)-y\left(r,{s}^{\prime },d\right)\right)}^2 $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ10.png)

(8.10)

损失加在一起，然后独立地最小化，以训练![$$ {Q}_{\phi_1} $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq18.png)和![$$ {Q}_{\phi_2} $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq19.png)网络(即两个在线评论家网络)。

![$$ {Q}_{Loss}={\sum}_{i=1,2}{Q}_{Loss,i} $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ11.png)

(8.11)

### 保单损失(参与者)

保单损失计算方法保持不变，与 DDPG 使用的方法相同。

![$$ \mathrm{Polic}{\mathrm{y}}_{\mathrm{Loss}}=-\frac{1}{\mathrm{B}}{\sum}_{\mathrm{s}\in B}{\mathrm{Q}}_{\upphi_1}\left(\mathrm{s},{\upmu}_{\upphi}\left(\mathrm{s},{\upmu}_{\uptheta}\left(\mathrm{s}\right)\right)\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ12.png)

(8.12)

请注意，我们在等式中只使用了![$$ {Q}_{\phi_1} $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq20.png)。和 DDPG 一样，也请注意`-ve`标志。我们需要做梯度上升，但是 PyTorch 和 TensorFlow 做梯度下降。我们使用一个`-ve`符号将上升转换为下降。

### 延迟更新

我们以延迟的方式更新在线策略和代理网络权重，即在线 Q 网络![$$ {Q}_{\phi_1} $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq21.png)和![$$ {Q}_{\phi_2} $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq22.png)的每两次更新更新一次。

### 伪代码和实现

至此，我们已经准备好给出完整的伪代码。请参见图 [8-7](#Fig7) 。

Twin Delayed DDPG

![../images/502835_1_En_8_Chapter/502835_1_En_8_Fig7_HTML.png](../images/502835_1_En_8_Chapter/502835_1_En_8_Fig7_HTML.png)

图 8-7

双延迟 DDPG 算法

### 代码实现

现在让我们浏览一下代码实现。像 DDPG 一样，我们将在摆锤和月球着陆器上运行该算法。除了我们前面谈到的三处修改，大部分代码与 DDPG 的代码相似。因此，我们将只介绍 PyTorch 和 TensorFlow 版本中这些变化的亮点。您可以在文件`listing8_2_td3_pytorch.ipynb`中找到 PyTorch 版本的完整代码，在`listing8_2_td3_tensorflow.ipynb`中找到 TensorFlow 版本的完整代码。

#### 组合的模型-参与者评论实现

我们首先来看看代理网络。个人 Q-网络(评论家)`MLPQFunction`和政策-网络(演员)`MLPActor`与之前相同。然而，我们将演员和评论家结合在一起的代理人，即`MLPActorCritic`，看到了一个微小的变化。我们现在有两个 Q 网络与 TD3 的“孪生”部分一致。清单 [8-14](#PC14) 包含了`MLPActorCritic`的代码，在 PyTorch 和 TensorFlow 中都有。

```
#################PyTorch#################
class MLPActorCritic(nn.Module):
    def __init__(self, observation_space, action_space):
        super().__init__()
        self.state_dim = observation_space.shape[0]
        self.act_dim = action_space.shape[0]
        self.act_limit = action_space.high[0]

        #build Q and policy functions
        self.q = MLPQFunction(self.state_dim, self.act_dim)
        self.policy = MLPActor(self.state_dim, self.act_dim, self.act_limit)

    def act(self, state):
        with torch.no_grad():
            return self.policy(state).numpy()

    def get_action(self, s, noise_scale):
        a = self.act(torch.as_tensor(s, dtype=torch.float32))
        a += noise_scale * np.random.randn(self.act_dim)
        return np.clip(a, -self.act_limit, self.act_limit)

################# TensorFlow #################
class MLPActorCritic(tf.keras.Model):
    def __init__(self, observation_space, action_space):
        super().__init__()
        self.state_dim = observation_space.shape[0]
        self.act_dim = action_space.shape[0]
        self.act_limit = action_space.high[0]

        #build Q and policy functions

        self.q1 = MLPQFunction(self.state_dim, self.act_dim)
        self.q2 = MLPQFunction(self.state_dim, self.act_dim)
        self.policy = MLPActor(self.state_dim, self.act_dim, self.act_limit)

    def act(self, state):
        return self.policy(state).numpy()

    def get_action(self, s, noise_scale):
        a = self.act(s.reshape(1,-1).astype("float32")).reshape(-1)
        a += noise_scale * np.random.randn(self.act_dim)
        return np.clip(a, -self.act_limit, self.act_limit)

Listing 8-14MPLActorCritic in PyTorch and TensorFlow

```

#### q 损耗实现

重放缓冲区保持不变。下一个变化是 Q 损耗的计算方式。我们按照等式( [8.8](#Equ8) )到( [8.11](#Equ11) )实现目标策略平滑和限幅双 Q 学习。清单 [8-15](#PC15) 包含 PyTorch 中`compute_q_loss`的代码。这次我们没有明确列出 TensorFlow 的代码，可以在文件`listing8_2_td3_tensorflow.ipynb`中进一步探究。

```
def compute_q_loss(agent, target_network, states, actions, rewards, next_states, done_flags,
                    gamma, target_noise, noise_clip, act_limit, tape):

    # convert numpy array to proper data types
    states = states.astype('float32')
    actions = actions.astype('float32')
    rewards = rewards.astype('float32')
    next_states = next_states.astype('float32')
    done_flags = done_flags.astype('float32')

    # get q-values for all actions in current states
    # use agent network

    q1 = agent.q1(states, actions)
    q2 = agent.q2(states, actions)

    # Bellman backup for Q function
    with tape.stop_recording():

        action_target = target_network.policy(next_states)

        # Target policy smoothing
        epsilon = tf.random.normal(action_target.shape) * target_noise
        epsilon = tf.clip_by_value(epsilon, -noise_clip, noise_clip)
        action_target = action_target + epsilon
        action_target = tf.clip_by_value(action_target, -act_limit, act_limit)

        q1_target = target_network.q1(next_states, action_target)
        q2_target = target_network.q2(next_states, action_target)
        q_target = tf.minimum(q1_target, q2_target)
        target = rewards + gamma * (1 - done_flags) * q_target

    # MSE loss against Bellman backup
    loss_q1 = tf.reduce_mean((q1 - target)**2)
    loss_q2 = tf.reduce_mean((q2 - target)**2)
    loss_q = loss_q1 + loss_q2

    return loss_q

Listing 8-15Q-Loss in PyTorch

```

#### 保单损失执行

策略损失计算保持不变，除了我们仅使用 q 网络之一，这实际上是第一个具有权重ϕ <sub>1</sub> 的网络。

#### 一步更新实施

`one_step_update`函数的实现也非常相似，除了我们需要针对作为`q_params`传递到函数中的 Q1 和 Q2 网络的组合网络权重来实现 q 损耗的梯度。此外，我们需要冻结和解冻`q1`和`q2`的网络权重。

清单 [8-16](#PC16) 包含 PyTorch 中`one_step_update`的实现，清单 [8-17](#PC17) 包含 TensorFlow 中的代码。请特别注意权重是如何冻结和解冻的，梯度更新是如何计算的，以及权重是如何更新到目标网络的。这些操作在 PyTorch 代码和 TensorFlow 代码之间有一些细微差别。

```
def one_step_update(agent, target_network, q_params, q_optimizer, policy_optimizer,
                    states, actions, rewards, next_states, done_flags,
                    gamma, polyak, target_noise, noise_clip, act_limit,
                    policy_delay, timer):

    #one step gradient for q-values
    q_optimizer.zero_grad()
    loss_q = compute_q_loss(agent, target_network, states, actions, rewards, next_states, done_flags,
                    gamma, target_noise, noise_clip, act_limit)
    loss_q.backward()
    q_optimizer.step()

    # Update policy and target networks after policy_delay updates of Q-networks
    if timer % policy_delay == 0:
        #Freeze Q-network
        for params in q_params:
            params.requires_grad = False

        #one setep gradient for policy network
        policy_optimizer.zero_grad()
        loss_policy = compute_policy_loss(agent, states)
        loss_policy.backward()
        policy_optimizer.step()

        #UnFreeze Q-network

        for params in q_params:
            params.requires_grad = True

        # update target networks with polyak averaging
        with torch.no_grad():
            for params, params_target in zip(agent.parameters(), target_network.parameters()):
                params_target.data.mul_(polyak)
                params_target.data.add_((1-polyak)*params.data)

Listing 8-16One-Step Update in PyTorch

```

清单 [8-17](#PC17) 显示了一步更新中的 TensorFlow 版本。

```
def one_step_update(agent, target_network, q_params, q_optimizer, policy_optimizer,
                    states, actions, rewards, next_states, done_flags,
                    gamma, polyak, target_noise, noise_clip, act_limit,
                    policy_delay, timer):

    #one step gradient for q-values
    with tf.GradientTape() as tape:
        loss_q = compute_q_loss(agent, target_network, states, actions, rewards, next_states, done_flags,
                        gamma, target_noise, noise_clip, act_limit, tape)

        gradients = tape.gradient(loss_q, q_params)
        q_optimizer.apply_gradients(zip(gradients, q_params))

    # Update policy and target networks after policy_delay updates of Q-networks
    if timer % policy_delay == 0:
        #Freeze Q-network
        agent.q1.trainable=False
        agent.q2.trainable=False

        #one setep gradient for policy network
        with tf.GradientTape() as tape:
            loss_policy = compute_policy_loss(agent, states, tape)
            gradients = tape.gradient(loss_policy, agent.policy.trainable_variables)
            policy_optimizer.apply_gradients(zip(gradients, agent.policy.trainable_variables))

        #UnFreeze Q-network

        agent.q1.trainable=True
        agent.q2.trainable=True

        # update target networks with polyak averaging
        updated_model_weights = []
        for weights, weights_target in zip(agent.get_weights(), target_network.get_weights()):
            new_weights = polyak*weights_target+(1-polyak)*weights
            updated_model_weights.append(new_weights)
        target_network.set_weights(updated_model_weights)

Listing 8-17One-Step Update in TensorFlow

```

#### TD3 主回路

下一个变化是更新的频率。与 DDPG 不同，在 TD3 中，我们每更新两次 Q 网络就更新一次在线策略和目标权重。这是对 DDPG 规范的一个小改动，因此我们不在这里列出。

我们现在首先为钟摆环境运行 TD3，然后为月球着陆器健身房环境运行 TD3。我们可以看到，在五集之后，钟摆在直立状态下变得很平衡。月球着陆器的训练质量，就像 DDPG 一样，有点平庸。月球着陆器的环境是复杂的，因此我们需要运行更多次数的训练，比如 50 或 100 次。

我们可能也看不出 DDPG 和 TD3 在学习质量上有什么明显的区别。然而，如果我们可以在更复杂的环境中运行它，我们将会看到 TD3 相对于 DDPG 更高的性能。感兴趣的读者可以参考 TD3 的原始论文，查看 TD3 的作者对其他算法所做的基准研究。

很快我们就会看到本章的最后一个算法，一个叫做*软演员评论家*的算法。在此之前，我们将绕一小段路来理解 SAC 使用的一种叫做*的重新参数化技巧*。

## 重新参数化技巧

重新参数化技巧是变分自动编码器(VAEs)中使用的变量方法的一个变化。在那里，需要通过随机的节点传播梯度。重新参数化也被用来降低梯度估计的方差。第二个原因是我们将在这里探讨的。这篇深度文章是在戈克尔·埃尔多安 <sup>[5](#Fn5)</sup> 的一篇博客文章之后进行的，其中有额外的分析推导和解释。

假设我们有一个随机变量 *x* ，它遵循正态分布。让分布由 *θ* 参数化如下:

![$$ x\sim {p}_{\uptheta}(x)=N\left(\uptheta, 1\right)=\frac{1}{\sqrt{2\uppi}}{e}^{-\frac{1}{2}{\left(x-\uptheta \right)}^2} $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ13.png)

(8.13)

我们从中抽取样本，然后使用这些样本找出下列各项中的最小值:

![$$ J\left(\uptheta \right)={E}_{x\sim {p}_{\uptheta}(x)}\left[{x}^2\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equt.png)

我们使用梯度下降法。我们的重点是找到两种不同的方法来确定导数/梯度∇<sub>θ</sub>T2】j(θ)的估计值。

### 得分/强化方式

首先，我们将遵循 log 技巧，这是我们在讨论使用策略梯度进行强化时所做的。我们看到它有很高的方差，这就是我们希望为前面显示的简单示例分布所展示的。

我们对 *J* ( *θ* )相对于 *θ* 求导。

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equu.png)

![$$ ={\nabla}_{\uptheta}{E}_{x\sim {p}_{\uptheta}(x)}\left[{x}^2\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equv.png)

![$$ ={\nabla}_{\uptheta}\int {p}_{\uptheta}(x){x}^2 dx $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equw.png)

![$$ =\int {\nabla}_{\uptheta}{p}_{\uptheta}(x){x}^2 dx $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equx.png)

![$$ =\int \frac{p_{\uptheta}(x)}{p_{\uptheta}(x)}{\nabla}_{\uptheta}{p}_{\uptheta}(x){x}^2 dx $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equy.png)

![$$ =\int {p}_{\uptheta}(x)\frac{\nabla_{\uptheta}{p}_{\uptheta}(x)}{p_{\uptheta}(x)}{x}^2 dx $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equz.png)

![$$ =\int {p}_{\uptheta}(x){\nabla}_{\uptheta}\mathit{\log}\ {p}_{\uptheta}(x){x}^2 dx $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equaa.png)

![$$ ={E}_{x\sim {p}_{\uptheta}(x)}\left[{\nabla}_{\uptheta}\mathit{\log}\ {p}_{\uptheta}(x){x}^2\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equab.png)

接下来，我们使用蒙特卡罗来使用样本形成∇<sub>*θ*</sub>*j*(*θ*)的估计。

![$$ \hat{\nabla_{\uptheta}J\left(\uptheta \right)}=\frac{1}{N}{\sum}_{i=1}^N{\nabla}_{\uptheta}\mathit{\log}\ {p}_{\uptheta}\left({x}_i\right){x}_i^2 $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equac.png)

将前面的表达式代入*p*T2】θ(*x*)并取一个后跟梯度 wrt θ的对数，我们得到如下结果:

![$$ \hat{\nabla_{\uptheta}J\left(\uptheta \right)}=\frac{1}{N}{\sum}_{i=1}^N\left({x}_i-\uptheta \right){x}_i^2 $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ14.png)

(8.14)

### 重新参数化技巧和路径导数

第二种方法是重新参数化技巧。我们将把 *x* 重新定义为一个常数和一个没有参数 *θ* 的正态分布的组合。让 *x* 定义如下:

![$$ x=\uptheta +\upepsilon \kern1em \mathrm{where}\kern0.75em \upepsilon \sim N\left(0,1\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equad.png)

我们可以看到，之前的重新参数化使 *x* 的分布保持不变。

![$$ {p}_{\uptheta}(x)=N\left(\uptheta, 1\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equae.png)

我们来计算一下![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)={\nabla}_{\uptheta}{E}_{x\sim {p}_{\uptheta}(x)}\left[{x}^2\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq23.png)。

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equaf.png)

![$$ ={\nabla}_{\uptheta}{E}_{x\sim {p}_{\uptheta}(x)}\left[{x}^2\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equag.png)

![$$ ={\nabla}_{\uptheta}{E}_{\upepsilon \sim N\left(0,1\right)}\left[{\left(\uptheta +\upepsilon \right)}^2\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equah.png)

由于期望值不依赖于θ，我们可以将梯度移入，而不会遇到前面方法中所示的“ *log* ”问题(即在导数内找到 log)。

![$$ ={\nabla}_{\uptheta}\int p\left(\upepsilon \right){\left(\uptheta +\upepsilon \right)}^2d\upepsilon $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equai.png)

![$$ =\int p\left(\upepsilon \right){\nabla}_{\uptheta}{\left(\uptheta +\upepsilon \right)}^2d\upepsilon $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equaj.png)

![$$ =\int p\left(\upepsilon \right)2\left(\uptheta +\upepsilon \right)d\upepsilon $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equak.png)

![$$ ={E}_{\upepsilon \sim N\left(0,1\right)}\left[2\left(\uptheta +\upepsilon \right)\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equal.png)

接下来，我们将期望值转换为 MC 估计值，得到以下结果:

![$$ \hat{\nabla_{\uptheta}J\left(\uptheta \right)}=\frac{1}{N}{\sum}_{i=1}^N2\left(\uptheta +{\upepsilon}_i\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ15.png)

(8.15)

### 实验

我们使用方程( [8.14](#Equ14) )和( [8.15](#Equ15) )通过两种方法计算估计值的均值和方差。我们使用不同的 *N* 值来计算( [8.14](#Equ14) )和( [8.15](#Equ15) )，并且我们对 *N* 的每个值重复实验 10，000 次，以计算在( [8.14](#Equ14) )和( [8.15](#Equ15) )中给出的梯度估计的平均值和方差。我们的实验将表明这两个方程的平均值是相同的。换句话说，他们估计的是同一个值，但是( [8.14](#Equ14) )中估计的方差比( [8.15](#Equ15) )中估计的方差高了几乎一个数量级。在我们的例子中，它高出了 21.75 倍。

让我们冻结这个:

![$$ \uptheta =2 $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equam.png)

*   我们为*x*∽*n*(θ，1)生成样本，并在等式( [8.14](#Equ14) )中使用这些样本来计算∇<sub>θ</sub>t8】j(θ)的加固估计。

*   我们为ϵ∽*n*(0，1)生成样本，并在等式( [8.15](#Equ15) )中使用这些样本来计算∇<sub>θ</sub>T6】j(θ)的重新参数化估计。

实验的细节和代码请看`listing8_3_reparameterization.ipynb`。在笔记本中，我们还计算了解析解以得出结果，如下所示。

对于使用( [8.14](#Equ14) )的加固梯度，梯度如下:

![$$ mean=4 $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equan.png)

![$$ \mathrm{variance}=\frac{87}{N} $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equao.png)

对于使用( [8.15](#Equ15) )重新参数化的梯度，梯度如下:

![$$ mean=4 $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equap.png)

![$$ \mathrm{variance}=\frac{4}{N} $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equaq.png)

我们可以看到，在两种方法下，梯度估计具有相同的均值。然而，重新参数化方法的方差要小得多，小了一个数量级。这正是我们的代码运行所确认的。

综上所述，假设我们有一个以状态 *s* 为输入的策略网络，网络由 *θ* 参数化。策略网络产生策略的均值和方差，即一个正态分布的随机策略，其均值和方差是网络的输出，如图 [8-8](#Fig8) 所示。

![../images/502835_1_En_8_Chapter/502835_1_En_8_Fig8_HTML.jpg](../images/502835_1_En_8_Chapter/502835_1_En_8_Fig8_HTML.jpg)

图 8-8

随机政策网络

我们将动作 *a* 定义如下:

![$$ a\sim N\left({\upmu}_{\uptheta}(s),{\upsigma}_{\uptheta}^2(s)\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ16.png)

(8.16)

我们重新参数化动作 *a* 以分解出确定性和随机部分，使得随机部分不依赖于网络参数 *θ* 。

![$$ a={\upmu}_{\uptheta}(s)+{\upsigma}_{\uptheta}^2(s).\varepsilon $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equar.png)

![$$ \mathrm{where}:\kern0.75em \upepsilon \sim N\left(0,1\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ17.png)

(8.17)

与使用非参数化方法相比，重新参数化允许我们计算具有较低方差的策略梯度。此外，重新参数化允许我们通过将随机部分分离为非参数化部分，以另一种方式将梯度流回网络。我们将在软演员-评论家算法中使用这种方法。

## 熵解释

在我们开始深入研究 SAC 的细节之前，还有一件事:让我们重温一下熵。在前一章中，我们讨论了熵作为正则项作为增强代码遍历的一部分。我们会做一些类似的事情。我们来理解一下熵是什么。

假设我们有一个随机变量 *x* 遵循某种分布 *P* ( *x* )。 *x* 的熵定义如下:

![$$ H(P)=\underset{x\sim P}{E}\left[- logP(x)\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ18.png)

(8.18)

假设我们有一个硬币，硬币的 *P* ( *H* ) = *ρ* 和*P*(*T*)= 1*ρ*。我们针对 *ρ ε* (0，1)的不同值计算熵 *H* 。

![$$ H(x)=-\left[\rho\ log\rho +\left(1-\rho \right)\ \mathit{\log}\left(1-\rho \right)\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equas.png)

我们可以绘制出 *H* ( *x* )对 *ρ* 的曲线，如图 [8-9](#Fig9) 所示。

![../images/502835_1_En_8_Chapter/502835_1_En_8_Fig9_HTML.jpg](../images/502835_1_En_8_Chapter/502835_1_En_8_Fig9_HTML.jpg)

图 8-9

作为 *p* 的函数的伯努利分布的熵，它是在试验中得到 1 的概率

我们可以看到，熵 *H* 是 *ρ* = 0.5 的最大值，也就是说，当我们在得到 1 或 0 之间具有最大不确定性时。换句话说，通过最大化熵，我们可以确保随机行动策略具有广泛的分布，并且不会过早地崩溃到一个尖峰。尖锐的峰会减少探索。

## 软演员评论家

软演员评论家大约与 TD3 同时出现。像 DDPG 和 TD3 一样，SAC 也使用了一个行动者-批评家结构，通过政策外学习进行连续控制。然而，与 DDPG 和 TD3 不同，SAC 学习随机策略。因此，SAC 在确定性策略算法(如 DDPG 和 TD3)与随机策略优化之间架起了一座桥梁。该算法是在 2018 年发表的一篇题为“软行动者-批评家:随机行动者的非策略最大熵深度强化学习”的论文中引入的。 <sup>[6](#Fn6)</sup>

它使用像 TD3 一样的限幅双 Q 技巧，并且由于它的学习随机策略，它间接地受益于目标策略平滑，而不明显需要向目标策略添加噪声。

SAC 的核心特性是使用熵作为最大化的一部分。引用论文作者的话:

> *“在这个框架中，行动者的目标是同时最大化预期收益和熵；也就是说，在尽可能随机行动的同时成功完成任务。”*

### SAC 对 TD3

这是两者的相似之处:

*   两者都使用均方贝尔曼误差(MSBE)最小化来达到共同目标。

*   使用目标 Q 网络来计算公共目标，该目标 Q 网络是使用聚丙烯平均获得的。

*   两者都使用限幅双 Q，它至少由两个 Q 值组成，以避免高估。

这就是不同之处:

*   SAC 使用熵正则化，这是 TD3 中没有的。

*   TD3 目标策略用于计算下一个状态的动作，而在 SAC 中，我们使用当前策略来获得下一个状态的动作。

*   在 TD3 中，目标策略通过向动作添加随机噪声来使用平滑。然而，在 SAC 中，学习到的策略是随机的，它提供了平滑效果，而没有任何明显的噪声添加。

### 熵正则化 q 损失

熵测量分布的随机性。熵越高，分布越平坦。一个尖峰政策的所有概率都集中在那个尖峰附近，因此它将具有低熵。通过熵正则化，策略被训练为最大化期望回报和熵之间的权衡，其中 *α* 控制该权衡。该策略被训练为最大化预期回报和熵之间的权衡，熵是策略中随机性的度量。

![$$ {\uppi}^{\ast }=\arg \underset{\uppi}{\max}\underset{\uptau \sim \uppi}{E}\left[{\sum}_{t=0}^{\infty }{\upgamma}^t\left(R\left({s}_t,{a}_t,{s}_{t+1}\right)+\upalpha H\left(\uppi \left(\cdotp |{s}_t\right)\right)\right)\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ19.png)

(8.19)

在此设置中， *V* <sup>π</sup> 被更改为包含每个时间步长的熵。

![$$ {V}^{\uppi}(s)=\underset{\uptau \sim \uppi}{E}\left[\left.{\sum}_{t=0}^{\infty }{\upgamma}^t\left(R\left({s}_t,{a}_t,{s}_{t+1}\right)+\upalpha H\left(\uppi \left(\cdotp |{s}_t\right)\right)\right)\right|{s}_0=s\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ20.png)

(8.20)

此外， *Q* <sup>π</sup> 被更改为包括除第一个时间步之外的每个时间步的熵奖励。

![$$ {Q}^{\uppi}\left(s,a\right)=\underset{\uptau \sim \uppi}{E}\left[\left.{\sum}_{t=0}^{\infty }{\upgamma}^tR\left({s}_t,{a}_t,{s}_{t+1}\right)+\upalpha {\sum}_{t=1}^{\infty }{\upgamma}^tH\left(\uppi \left(\cdotp |{s}_t\right)\right)\right|{s}_0=s,{a}_0=a\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ21.png)

(8.21)

有了这些定义， *V* <sup>π</sup> 和 *Q* <sup>π</sup> 通过以下方式连接:

![$$ {V}^{\uppi}(s)=\underset{a\sim \uppi}{E}\left[{Q}^{\uppi}\left(s,a\right)+\upalpha H\left(\uppi \left(\cdotp |s\right)\right)\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ22.png)

(8.22)

关于*Q*T2】π的贝尔曼方程如下:

![$$ {Q}^{\uppi}\left(s,a\right)=\underset{s^{\prime}\sim P,{a}^{\prime}\sim \uppi}{E}\left[R\left(s,a,{s}^{\prime}\right)+\upgamma \left({Q}^{\uppi}\left({s}^{\prime },{a}^{\prime}\right)+\upalpha H\left(\uppi \left(\cdotp |{s}^{\prime}\right)\right)\right)\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equat.png)

![$$ =\underset{s^{\prime}\sim P}{E}\left[R\left(s,a,{s}^{\prime}\right)+\upgamma {V}^{\uppi}\left({s}^{\prime}\right)\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ23.png)

(8.23)

右边是我们转换成样本估计的期望值。

![$$ {Q}^{\uppi}\left(s,a\right)\approx r+\upgamma \left({Q}^{\uppi}\left({s}^{\prime },\overset{\sim }{a^{\prime }}\right)-\upalpha \mathrm{log}\uppi \left(\overset{\sim }{a^{\prime }}|{s}^{\prime}\right)\right),\kern1.25em \overset{\sim }{a^{\prime }}\sim \uppi \left(\cdotp |{s}^{\prime}\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ24.png)

(8.24)

以上，( *s* ， *a* ， *r* ， *s* ，<sup>′</sup>)来自重放缓冲区，![$$ \overset{\sim }{a^{\prime }} $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq24.png)来自在线/代理策略采样。*在 SAC 中，我们根本不使用目标网络策略。*

像 TD3 一样，SAC 使用限幅双 Q 并最小化均方贝尔曼误差(MSBE)。综上所述，SAC 中 Q 网络的损耗函数如下:

![$$ L\left({\upphi}_i,D\right)=\underset{\left(s,a,r,{s}^{\prime },d\right)\sim D}{E}\left[{\left({Q}_{\upphi_i}\left(s,a\right)-y\left(r,{s}^{\prime },d\right)\right)}^2\right],\kern1.25em i=1,2 $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ25.png)

(8.25)

其中目标由下式给出:

![$$ y\left(r,{s}^{\prime },d\right)=r+\upgamma \left(1-d\right)\left(\underset{i=1,2}{\min }{Q}_{\upphi_{\mathrm{targ},i}}\left({s}^{\prime },\overset{\sim }{a^{\prime }}\right)-\upalpha \mathrm{log}{\uppi}_{\uptheta}\left(\overset{\sim }{a^{\prime }}|{s}^{\prime}\right)\right),\kern1.25em \overset{\sim }{a^{\prime }}\sim {\uppi}_{\uptheta}\left(\cdotp |{s}^{\prime}\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ26.png)

(8.26)

我们将期望值转化为样本平均值。

![$$ L\left({\upphi}_i,\mathcal{D}\right)=\frac{1}{\left|B\right|}{\sum}_{\left(s,a,r,{s}^{\prime },d\right)\in B}{\left({Q}_{\upphi_i}\left(s,a\right)-y\left(r,{s}^{\prime },d\right)\right)}^2,\kern1.25em \mathrm{for}\ i=1,2 $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ27.png)

(8.27)

我们将最小化的最终 Q 损耗如下:

![$$ {Q}_{Loss}=L\left({\upphi}_1,\mathcal{D}\right)+L\left({\upphi}_1,\mathcal{D}\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ28.png)

(8.28)

### 重新参数化策略的策略损失

政策要选择最大化预期未来收益和未来熵的行动，即*V*<sup>【π】</sup>(*s*)。

![$$ {V}^{\uppi}(s)=\underset{a\sim \uppi}{E}\left[{Q}^{\uppi}\left(s,a\right)+\upalpha H\left(\uppi \left(\cdotp |s\right)\right)\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equau.png)

我们将其改写如下:

![$$ {V}^{\uppi}(s)=\underset{a\sim \uppi}{E}\left[{Q}^{\uppi}\left(s,a\right)-\upalpha \mathrm{log}\uppi \left(a|s\right)\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ29.png)

(8.29)

该论文的作者使用了重新参数化和压缩高斯策略。

![$$ \overset{\sim }{a_{\uptheta}}\left(s,\upxi \right)=\tanh \left({\upmu}_{\uptheta}(s)+{\upsigma}_{\uptheta}(s)\odot \upxi \right),\kern1.25em \upxi \sim \mathcal{N}\left(0,I\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ30.png)

(8.30)

结合前面的两个等式( [8.29](#Equ29) )和( [8.30](#Equ30) )，并且还注意到我们的策略网络由策略网络权重 *θ* 参数化，我们得到以下等式:

![$$ \underset{a\sim {\uppi}_{\uptheta}}{E}\left[{Q}^{\uppi_{\uptheta}}\left(s,a\right)-\upalpha \mathrm{log}{\uppi}_{\uptheta}\left(a|s\right)\right]=\underset{\upxi \sim \mathcal{N}}{E}\left[{Q}^{\uppi_{\uptheta}}\left(s,\overset{\sim }{a_{\uptheta}}\left(s,\upxi \right)\right)-\upalpha \mathrm{log}{\uppi}_{\uptheta}\left(\overset{\sim }{a_{\uptheta}}\left(s,\upxi \right)|s\right)\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ31.png)

(8.31)

接下来，我们用函数逼近器代替 Q，取两个 Q 函数中的最小值。

![$$ {Q}^{\uppi_{\uptheta}}\left(s,\overset{\sim }{a_{\uptheta}}\left(s,\upxi \right)\right)=\underset{i=1,2}{\min }{Q}_{\upphi_i}\left(s,\overset{\sim }{a_{\uptheta}}\left(s,\upxi \right)\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ32.png)

(8.32)

政策目标相应地转变为:

![$$ \underset{\uptheta}{\max}\underset{\underset{\upxi \sim \mathcal{N}}{s\sim \mathcal{D}}}{E}\left[\underset{i=1,2}{\min }{Q}_{\upphi_i}\left(s,\overset{\sim }{a_{\uptheta}}\left(s,\upxi \right)\right)-\upalpha \mathrm{log}{\uppi}_{\uptheta}\left(\overset{\sim }{a_{\uptheta}}\left(s,\upxi \right)|s\right)\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ33.png)

(8.33)

像以前一样，我们在 PyTorch/TensorFlow 中使用最小化器。因此，我们引入了一个`-ve`符号来将最大化转换为损失最小化。

![$$ Polic{y}_{Loss}=-\underset{\underset{\upxi \sim \mathcal{N}}{s\sim \mathcal{D}}}{E}\left[\underset{i=1,2}{\min }{Q}_{\upphi_i}\left(s,\overset{\sim }{a_{\uptheta}}\left(s,\upxi \right)\right)-\upalpha \mathrm{log}{\uppi}_{\uptheta}\left(\overset{\sim }{a_{\uptheta}}\left(s,\upxi \right)|s\right)\right] $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ34.png)

(8.34)

我们还使用样本将期望值转换为估计值，得到以下结果:

![$$ Polic{y}_{Loss}=-\frac{1}{\left|B\right|}{\sum}_{s\in B}\left(\underset{i=1,2}{\min }{Q}_{\upphi_i}\left(s,\overset{\sim }{a_{\uptheta}}(s)\right)-\upalpha \mathrm{log}{\uppi}_{\uptheta}\left(\left.\overset{\sim }{a_{\uptheta}}(s)\right|s\right)\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equ35.png)

(8.35)

### 伪代码和实现

至此，我们已经准备好给出完整的伪代码。请参见图 [8-10](#Fig10) 。

Soft Actor Critic

![../images/502835_1_En_8_Chapter/502835_1_En_8_Fig10_HTML.png](../images/502835_1_En_8_Chapter/502835_1_En_8_Fig10_HTML.png)

图 8-10

软演员评论家算法

### 代码实现

有了所有的数学推导和伪代码，是时候深入 PyTorch 和 TensorFlow 的实现了。使用 PyTorch 的实现在文件`listing_8_4_sac_pytorch.ipynb`中。像以前一样，我们使用 SAC 在钟摆和月球着陆器环境中训练代理。TensorFlow 2.0 实现在文件`listing_8_4_sac_tensorflow.ipynb`中。

#### 策略网络-参与者实施

我们先看演员网。这一次，actor 实现将状态作为输入，和以前一样。然而，输出有两个部分。

*   要么是压扁(即通过 tanh 传递动作值)的确定性动作 *a* ，即μ <sub>θ</sub> ( *s* )，要么是来自分布![$$ N\left({\upmu}_{\uptheta}(s),{\upsigma}_{\uptheta}^2(s)\right) $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_IEq25.png)的样本动作 *a* 。采样使用了重新参数化技巧，PyTorch 为您实现了这个技巧，如`distribution.rsample()`。你可以在 [`https://pytorch.org/docs/stable/distributions.html`](https://pytorch.org/docs/stable/distributions.html) 的“路径导数”主题下阅读

*   The second output is the log probability that we will need for calculating entropy inside the Q-loss as per equation ([8.26](#Equ26)). Since we are using squashed/tanh transformation, the log probability needs to apply a change of variables for random distribution using the following:

    ![$$ {f}_Y(y)={f}_X\left({g}^{-1}(y)\right)\left|\frac{d}{dy}\left({g}^{-1}(y)\right)\right| $$](../images/502835_1_En_8_Chapter/502835_1_En_8_Chapter_TeX_Equba.png)

代码使用一些技巧来计算数值稳定的版本。你可以在原文中找到更多的细节。

清单 [8-18](#PC18) 列出了前面讨论过的`SquashedGaussianMLPActor`的代码。神经网络仍然和以前一样:两个大小为 256 单位的隐藏层，具有 ReLU 激活。

```
LOG_STD_MAX = 2
LOG_STD_MIN = -20

class SquashedGaussianMLPActor(nn.Module):
    def __init__(self, state_dim, act_dim, act_limit):
        super().__init__()
        self.act_limit = act_limit
        self.fc1 = nn.Linear(state_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.mu_layer = nn.Linear(256, act_dim)
        self.log_std_layer = nn.Linear(256, act_dim)
        self.act_limit = act_limit

    def forward(self, s, deterministic=False, with_logprob=True):
        x = self.fc1(s)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        mu = self.mu_layer(x)
        log_std = self.log_std_layer(x)
        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)
        std = torch.exp(log_std)

        # Pre-squash distribution and sample
        pi_distribution = Normal(mu, std)
        if deterministic:
            # Only used for evaluating policy at test time.
            pi_action = mu
        else:
            pi_action = pi_distribution.rsample()

        if with_logprob

:
            # Compute logprob from Gaussian, and then apply correction for Tanh squashing.
            # NOTE: The correction formula is a little bit magic. To get an understanding
            # of where it comes from, check out the original SAC paper (arXiv 1801.01290)
            # and look in appendix C. This is a more numerically-stable equivalent to Eq 21.
            # Try deriving it yourself as a (very difficult) exercise. :)
            logp_pi = pi_distribution.log_prob(pi_action).sum(axis=-1)
            logp_pi -= (2*(np.log(2) - pi_action - F.softplus(-2*pi_action))).sum(axis=1)
        else:
            logp_pi = None

        pi_action = torch.tanh(pi_action)
        pi_action = self.act_limit * pi_action

        return pi_action, logp_pi

Listing 8-18SquashedGaussianMLPActor in PyTorch

```

清单 [8-19](#PC20) 显示了 TensorFlow 版本。对于重新参数化，我们使用下面的代码行对其进行示例说明:

```
pi = mu + tf.random.normal(tf.shape(mu)) * std

```

核心 TensorFlow 包没有计算熵的对数似然的函数。我们实现了自己的小函数`gaussian_likelihood`来做这件事。所有这些的一个替代方案是使用 TensorFlow 分布包:`tfp.distributions.Distribution`。actor 的其余实现类似于我们在 PyTorch 中看到的。

```
LOG_STD_MAX = 2
LOG_STD_MIN = -20

EPS = 1e-8

def gaussian_likelihood(x, mu, log_std):
    pre_sum = -0.5 * (((x-mu)/(tf.exp(log_std)+EPS))**2 + 2*log_std + np.log(2*np.pi))
    return tf.reduce_sum(pre_sum, axis=1)

def apply_squashing_func(mu, pi, logp_pi):
    # Adjustment to log prob
    # NOTE: This formula is a little bit magic. To get an understanding of where it
    # comes from, check out the original SAC paper (arXiv 1801.01290) and look in
    # appendix C. This is a more numerically-stable equivalent to Eq 21.
    # Try deriving it yourself as a (very difficult) exercise. :)
    logp_pi -= tf.reduce_sum(2*(np.log(2) - pi - tf.nn.softplus(-2*pi)), axis=1)

    # Squash those unbounded actions!
    mu = tf.tanh(mu)
    pi = tf.tanh(pi)
    return mu, pi, logp_pi

class SquashedGaussianMLPActor(tf.keras.Model):
    def __init__(self, state_dim, act_dim, act_limit):
        super().__init__()
        self.act_limit = act_limit
        self.fc1 = layers.Dense(256, activation="relu")
        self.fc2 = layers.Dense(256, activation="relu")
        self.mu_layer = layers.Dense(act_dim)
        self.log_std_layer = layers.Dense(act_dim)
        self.act_limit = act_limit

    def call(self, s):
        x = self.fc1(s)
        x = self.fc2(x)
        mu = self.mu_layer(x)
        log_std = self.log_std_layer(x)
        log_std = tf.clip_by_value(log_std, LOG_STD_MIN, LOG_STD_MAX)
        std = tf.exp(log_std)

        pi = mu + tf.random.normal(tf.shape(mu)) * std
        logp_pi = gaussian_likelihood(pi, mu, log_std)
        mu, pi, logp_pi = apply_squashing_func(mu, pi, logp_pi)

        mu *= self.act_limit
        pi *= self.act_limit

        return mu, pi, logp_pi

Listing 8-19SquashedGaussianMLPActor in TensorFlow

```

#### q-网络、组合模型和经验重放

Q-function 网络`MLPQFunction`，它将演员和评论家组合成类`MLPActorCritic`中的一个代理，和`ReplayBuffer`的实现是相同的，或者至少非常相似。因此，我们不在这里列出代码的这些部分。

#### q 损失和保单损失执行

接下来我们看看`compute_q_loss`和`compute_policy_loss`。这是图 [8-10](#Fig10) 中伪代码的步骤 11 到 13 的直接实现。如果你将这些步骤与图 [8-7](#Fig7) 中 TD3 的步骤 11 到 14 进行比较，你会发现很多相似之处，除了动作是从在线网络中采样的，SAC 在两个损失中都有一个额外的熵项。这些变化很小，因此我们没有在这里的文本中明确列出代码。

#### 单步更新和 SAC 主循环

同样，`one_step_update`的代码和整个训练算法遵循与之前类似的模式。

一旦我们运行并训练了代理，我们会看到类似于 DDPG 和 TD3 的结果。如前所述，我们使用的是简单的环境，因此本章中的三种连续控制算法(DDPG、TD3 和 SAC)都表现良好。请参考本章中引用的各种论文，深入研究这些不同方法的官方性能比较。

这就把我们带到了在演员-评论家环境中持续控制这一章的结尾。到目前为止，我们已经看到了基于模型的策略迭代方法、基于深度学习的 Q 学习(DPN)方法、针对离散动作的策略梯度以及针对连续控制的策略梯度。这涵盖了大多数强化学习的流行方法。在结束我们的旅程之前，我们还有一个主要的主题要考虑:在无模型的世界中使用模型学习，以及在我们知道模型但它太复杂或太庞大而无法彻底探索的环境中进行有效的模型探索。

## 摘要

在这一章中，我们研究了持续控制的行动者-批评者方法，其中我们将非策略 Q 学习类型与策略梯度相结合，以导出非策略持续控制行动者-批评者方法。

我们首先看了 2016 年推出的深度确定性政策梯度。这是我们第一个连续控制算法。DDPG 是一种具有确定性连续控制策略的行动者-批评家方法。

接下来，我们看了双延迟 DDPG，它于 2018 年问世，解决了 DDPG 存在的一些稳定性和低效率挑战。像 DDPG 一样，它也通过演员-评论家架构学习了非政策环境中的确定性政策。

最后，我们看了软行动者批评家，它把 DDPG 式的学习和使用熵的随机政策优化联系起来。SAC 是一种使用行动者-批评家设置的非策略随机策略优化。我们还看到了重新参数化技巧，以获得梯度的较低方差估计。

<aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

[T2`https://spinningup.openai.com/`](https://spinningup.openai.com/)

  [2](#Fn2_source)

[T2`https://arxiv.org/pdf/1509.02971.pdf`](https://arxiv.org/pdf/1509.02971.pdf)

  [3](#Fn3_source)

[T2`http://proceedings.mlr.press/v32/silver14.pdf`](http://proceedings.mlr.press/v32/silver14.pdf)

  [4](#Fn4_source)

[T2`https://arxiv.org/pdf/1802.09477.pdf`](https://arxiv.org/pdf/1802.09477.pdf)

  [5](#Fn5_source)

[T2`http://gokererdogan.github.io/2016/07/01/reparameterization-trick/`](http://gokererdogan.github.io/2016/07/01/reparameterization-trick/)

  [6](#Fn6_source)

[`https://arxiv.org/abs/1801.01290`](https://arxiv.org/abs/1801.01290) 和 [`https://arxiv.org/pdf/1812.05905.pdf`](https://arxiv.org/pdf/1812.05905.pdf)

 </aside>