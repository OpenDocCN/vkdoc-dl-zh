# 6.深度 Q 学习

在这一章中，我们将深入探讨 Q 学习与使用神经网络的函数逼近的结合。使用神经网络的深度学习环境中的 Q-learning 也被称为*深度 Q 网络* (DQN)。我们将首先总结到目前为止我们所谈论的关于 Q-learning 的内容。然后我们将看看 DQN 在简单问题上的代码实现，然后训练一个代理玩 Atari 游戏。接下来，我们将通过查看可以对 DQN 进行的各种修改来扩展我们的知识，以改进学习，包括一些非常新的和最先进的方法。其中一些方法可能涉及一些数学知识，以理解这些方法的基本原理。然而，我们将努力保持数学最少，只包括所需的细节，以了解背景和推理。本章中的所有例子都将使用 PyTorch 或 TensorFlow 库进行编码。一些代码演练将同时包含 PyTorch 和 TensorFlow 的代码，而其他代码将仅使用 PyTorch 进行讨论。

## 深度 Q 网络

在第 [4](04.html) 章中，我们讨论了 Q-learning 作为一种无模型的非策略 TD 控制方法。我们首先看一下在线版本，其中我们使用了一个探索性行为策略(ε-贪婪)来在状态 *S* 时采取一个步骤(动作 *A* )。奖励 *R* 和下一个状态*S*<sup>’</sup>然后被用来更新 Q 值 *Q* ( *S* ， *A* )。图 [4-14](04.html#Fig14) 和清单 [4-4](04.html#PC4) 详细说明了伪代码和实际实现。这里给出了在这种情况下使用的更新公式。在你继续前进之前，你可能想重温这一点。

![$$ Q\left({S}_t,{A}_t\right)\leftarrow Q\left({S}_t,{A}_t\right)+\alpha \ast \left[{R}_{t+1}+\gamma \ast {}_{a\kern0.5em }{}^{\mathit{\max}}Q\left({S}_{t+1},{A}_{t+1}\right)-Q\left({S}_t,{A}_t\right)\right] $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equ1.png)

(6.1)

我们简要地讨论了最大化偏差和双 Q 学习的方法，其中我们使用了两个 Q 值表。在这一章中，当我们研究双 DQN 时，我们会有更多的内容要说。

接下来，我们研究了多次使用一个样本来将在线 TD 更新转换为批量 TD 更新的方法，从而提高样本效率。它向我们介绍了重放缓冲区的概念。虽然它只是关于离散状态和状态-动作空间上下文中的样本效率，但通过神经网络的函数逼近，它几乎成为了使深度学习神经网络收敛的必备条件。我们将再次讨论这一点，当我们谈到优先重放时，我们将看看从缓冲区中采样过渡/体验的其他选项。

接下来，在第 5 章[中，我们看了函数逼近的各种方法。我们将瓦片编码视为实现线性函数逼近的一种方式。然后，我们讨论了 DQN，即使用神经网络作为函数逼近器的批量 Q 学习。我们经历了长时间推导，以得出如等式(](05.html) [5 中给出的权重(具有神经网络参数)更新等式。25](05.html#Equ24) 。此处转载如下:

![$$ {w}_{t+1}={w}_t+\alpha .\kern0.5em \frac{1}{N}\sum \limits_{i=1}^N\left[{r}_i+\gamma\ {\mathit{\max}}_{a_i^{\prime }}\overset{\sim }{q}\left({s}_i^{\prime },{a}_i^{\prime };{w_t}^{-}\right)-\hat{q}\left({s}_i,{a}_i;w\right)\right].{\nabla}_w\ \hat{q}\left({s}_i,{a}_i;w\right) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equ2.png)

(6.2)

还请注意，我们使用下标 *i* 表示小批量中的样品，使用 *i* 表示权重更新的指数。方程式( [6.2](#Equ2) )是我们将在本章中广泛使用的一个。当我们谈论不同的修改和研究它们的影响时，我们将对这个等式进行各种调整。

我们还讨论了在梯度更新的非线性函数近似下没有收敛的理论保证。在这一章里，我们将对此有更多的论述。Q-learning 方法用于离散状态和动作，其中使用( [6.1](#Equ1) )更新 Q 值，而不是调整 DQN 基于深度学习的方法的权重参数。Q-learning 的案例有收敛的保证，而 DQN 的案例则没有这样的保证。DQN 也是计算密集型的。然而，尽管 DQN 有这些缺点，DQN 使得使用原始图像训练代理成为可能，这在普通的 Q-learning 中是完全不可想象的。现在让我们将等式( [6.2](#Equ2) )付诸实践，以在各种环境中训练 DQN 代理。

让我们再来看一下`CartPole`问题，它有一个四维连续状态，包含当前购物车位置、速度、杆的角度和杆的角速度的值。动作有两种:向左推车或向右推车，目的是尽可能长时间保持杆子平衡。以下是环境的详细信息:

```py
Observation:
    Type: Box(4)
    Num  Observation            Min                    Max
    0    Cart Position           -4.8                   4.8
    1    Cart Velocity           -Inf                   Inf
    2    Pole Angle             0.418 rad (-24 deg)    0.418 rad (24 deg)
    3    Pole Angular Velocity  -Inf                   Inf
Actions:
    Type: Discrete(2)
    Num   Action
    0     Push cart to the left
    1     Push cart to the right

```

我们将建立一个小的神经网络，以 4 为输入维度，三个隐藏层，然后是一个输出层，维度 2 是可能的动作数。图 [6-1](#Fig1) 为网络图。

![../images/502835_1_En_6_Chapter/502835_1_En_6_Fig1_HTML.jpg](../images/502835_1_En_6_Chapter/502835_1_En_6_Fig1_HTML.jpg)

图 6-1

简单神经网络

我们将使用 PyTorch 的`nn.Module`类来构建网络。我们还将实现一些额外的功能。函数`get_qvalues`取一批状态作为输入，即一个(N×4)维的张量，其中 N 是样本数。它通过网络传递状态值以产生 q 值。输出向量的大小为(n×2)；即每个输入一行。每行有两个 q 值，一个用于左推动作，另一个用于右推动作。同一个类中的函数`sample_actions`接收一批 q 值(N×2)。它使用ε-贪婪策略(等式 [4。3](04.html#Equ3) 选择一个动作。输出是(N×1)向量。清单 [6-2](#PC3) 显示了 PyTorch 中的代码。清单 [6-3](#PC4) 显示了 TensorFlow 2.0 的急切执行模式中的相同代码。您可以在文件`listing6_1_dqn_pytorch.ipynb`中找到 PyTorch 的完整实现，在文件`listing6_1_dqn_tensorflow.ipynb`中找到 TensorFlow 的完整实现。

**注意**虽然不是必需的，但是您将从代码讨论中获得更多关于 PyTorch 或 TensorFlow 的知识。你应该能够创建基本网络，定义损失函数，并执行优化的基本训练步骤。TensorFlow 新的急切执行模型类似于 PyTorch。出于这个原因，我们将在两个库中提供有限的示例代码，以帮助您入门。否则，书中的大部分代码都是 PyTorch。

```py
class DQNAgent(nn.Module):
    def __init__(self, state_shape, n_actions, epsilon=0):
        super().__init__()
        self.epsilon = epsilon
        self.n_actions = n_actions
        self.state_shape = state_shape

        state_dim = state_shape[0]
        # a simple NN with state_dim as input vector (inout is state s)
        # and self.n_actions as output vector of logits of q(s, a)
        self.network = nn.Sequential()
        self.network.add_module('layer1', nn.Linear(state_dim, 192))
        self.network.add_module('relu1', nn.ReLU())
        self.network.add_module('layer2', nn.Linear(192, 256))
        self.network.add_module('relu2', nn.ReLU())
        self.network.add_module('layer3', nn.Linear(256, 64))
        self.network.add_module('relu3', nn.ReLU())
        self.network.add_module('layer4', nn.Linear(64, n_actions))
        #
        self.parameters = self.network.parameters

    def forward(self, state_t):
        # pass the state at time t through the newrok to get Q(s,a)
        qvalues = self.network(state_t)
        return qvalues

    def get_qvalues(self, states):
        # input is an array of states in numpy and outout is Qvals as numpy array
        states = torch.tensor(states, device=device, dtype=torch.float32)
        qvalues = self.forward(states)
        return qvalues.data.cpu().numpy()

    def sample_actions(self, qvalues):
        # sample actions from a batch of q_values using epsilon greedy policy
        epsilon = self.epsilon
        batch_size, n_actions = qvalues.shape
        random_actions = np.random.choice(n_actions, size=batch_size)
        best_actions = qvalues.argmax(axis=-1)
        should_explore = np.random.choice(
            [0, 1], batch_size, p=[1-epsilon, epsilon])
        return np.where(should_explore, random_actions, best_actions)

Listing 6-1A Simple DQN Agent in PyTorch

```

清单 [6-2](#PC3) 显示了 TensorFlow 2.x 中使用 Keras 接口的相同代码。我们使用新的急切执行模型，它类似于 PyTorch 采用的方法。TensorFlow 在早期版本中使用了不同的模型，这有点难以概念化，分为两个独立的阶段:一个是构建所有网络操作的符号图，然后是通过将数据作为张量传递到第一阶段的模型构建中来训练模型的第二阶段。

```py
class DQNAgent:
    def __init__(self, state_shape, n_actions, epsilon=0):
        self.epsilon = epsilon
        self.n_actions = n_actions
        self.state_shape = state_shape

        state_dim = state_shape[0]
        self.model = tf.keras.models.Sequential()
        self.model.add(tf.keras.Input(shape=(state_dim,)))
        self.model.add(tf.keras.layers.Dense(192, activation="relu"))
        self.model.add(tf.keras.layers.Dense(256, activation="relu"))
        self.model.add(tf.keras.layers.Dense(64, activation="relu"))
        self.model.add(tf.keras.layers.Dense(n_actions))

    def __call__(self, state_t):
        # pass the state at time t through the newrok to get Q(s,a)
        qvalues = self.model(state_t)
        return qvalues

    def get_qvalues(self, states):
        # input is an array of states in numpy and outout is Qvals as numpy array
        qvalues = self.model(states)
        return qvalues.numpy()

    def sample_actions(self, qvalues):
        # sample actions from a batch of q_values using epsilon greedy policy
        epsilon = self.epsilon
        batch_size, n_actions = qvalues.shape
        random_actions = np.random.choice(n_actions, size=batch_size)
        best_actions = qvalues.argmax(axis=-1)
        should_explore = np.random.choice(
            [0, 1], batch_size, p=[1-epsilon, epsilon])
        return np.where(should_explore, random_actions, best_actions)

Listing 6-2A Simple DQN Agent in TensorFlow

```

重放缓冲区的代码很简单。我们有一个名为`self.buffer`的缓冲区来保存前面的例子。函数`add`接收`(state, action, reward, next_state, done)`，即来自代理单步/转换的值，并将其添加到缓冲区。如果缓冲区已经达到最大长度，它会丢弃最早的过渡，为新添加的内容腾出空间。函数`sample`获取整数`batch_size`并从缓冲器返回`batch_size`样本/转换。在这种普通实现中，存储在缓冲器中的每个转换被采样的概率相等。清单 [6-3](#PC4) 显示了重放缓冲区的代码。

```py
class ReplayBuffer:
    def __init__(self, size):
        self.size = size #max number of items in buffer
        self.buffer =[] #array to hold samples
        self.next_id = 0

    def __len__(self):
        return len(self.buffer)

    def add(self, state, action, reward, next_state, done):
        item = (state, action, reward, next_state, done)
        if len(self.buffer) < self.size:
           self.buffer.append(item)
        else:
            self.buffer[self.next_id] = item
        self.next_id = (self.next_id + 1) % self.size

    def sample(self, batch_size):
        idxs = np.random.choice(len(self.buffer), batch_size)
        samples = [self.buffer[i] for i in idxs]
        states, actions, rewards, next_states, done_flags = list(zip(*samples))
        return np.array(states),
                    np.array(actions),
      np.array(rewards),
      np.array(next_states),
      np.array(done_flags)

Listing 6-3Replay Buffer (Same in PyTorch or TensorFlow)

```

接下来，我们有一个效用函数`play_and_store`，它接受一个`env`(例如`CartPole`)、一个`agent`(例如`DQNAgent`)、一个`exp_replay` ( `ReplayBuffer`)、代理的`start_state`和`n_steps`(即在环境中要采取的步骤/动作的数量)。该功能使`agent`从初始状态`start_state`开始走`n_steps`步数。基于代理使用`agent.sample_actions`遵循的当前ε-贪婪策略采取这些步骤，并将这些`n_steps`转换记录在缓冲器中。清单 [6-4](#PC5) 显示了代码。

```py
def play_and_record(start_state, agent, env, exp_replay, n_steps=1):

    s = start_state
    sum_rewards = 0

    # Play the game for n_steps and record transitions in buffer
    for _ in range(n_steps):
        qvalues = agent.get_qvalues([s])
        a = agent.sample_actions(qvalues)[0]
        next_s, r, done, _ = env.step(a)
        sum_rewards += r
        exp_replay.add(s, a, r, next_s, done)
        if done:
            s = env.reset()
        else:
            s = next_s

    return sum_rewards, s

Listing 6-4Implementation of Function play_and_record

```

接下来，我们看看学习过程。我们首先建立我们想要最小化的损失 *L* 。它是使用一步 TD 值的当前状态动作的目标值和当前状态值之间的平均平方误差。正如在第 [5](05.html) 章中所讨论的，我们使用原始神经网络的副本，它具有权重*w*<sup>—</sup>(*w*带上标)。我们使用损失来计算代理(在线/原始)网络的权重 *w* 的梯度，并在梯度的负方向上采取一个步骤来减少损失。请注意，如第 [5](05.html) 章所述，我们保持目标网络的权重*w*<sup>—</sup>不变，并以较低的频率更新这些权重。引用第 [5](05.html) 章 DQN 批量方法部分:

> *这里我们使用了不同的权重向量 w*<sub>*t*</sub><sup>*来计算目标的估计值。本质上，我们有两个网络，一个称为在线网络，权重为“w*<sup>”</sup>*，根据等式(*[*5.24*](05.html#Equ23)*)进行更新，第二个类似的网络称为目标网络，但具有权重“w”的副本，称为“**【w*<sup>*。权重向量**w*<sup>*更新频率较低，比如说在线网络每更新 100 次。这种方法保持目标网络不变，并允许我们使用监督学习的机制。*</sup></sup></sup>

损失函数如下:

![$$ L=\frac{1}{N}{\sum}_{i=1}^N{\left[{r}_i+\left(\left(1- don{e}_i\right).\upgamma .\underset{a_i^{\prime }}{\max}\hat{q}\left({s}_i^{\prime },{a}_i^{\prime };{w}_t^{-}\right)\right)\hbox{--} \hat{q}\left({s}_i,{a}_i;{w}_t\right)\right]}^2 $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equ3.png)

(6.3)

我们采用 ***L*** 相对于 *w* 的梯度(导数),然后使用该梯度来更新在线网络的权重 *w* 。这些等式如下:

![$$ {\nabla}_{\mathrm{w}}\mathrm{L}=-\frac{1}{\mathrm{N}}{\sum}_{\mathrm{i}=1}^{\mathrm{N}}\left[{\mathrm{r}}_{\mathrm{i}}+\left(\left(1-\mathrm{don}{\mathrm{e}}_{\mathrm{i}}\right).\upgamma .\underset{{\mathrm{a}}_{\mathrm{i}}^{\prime }}{\max}\hat{\mathrm{q}}\left({\mathrm{s}}_{\mathrm{i}}^{\prime },{\mathrm{a}}_{\mathrm{i}}^{\prime };{\mathrm{w}}_{\mathrm{t}}^{-}\right)\right)\hbox{--} \hat{\mathrm{q}}\left({\mathrm{s}}_{\mathrm{i}},{\mathrm{a}}_{\mathrm{i}};{\mathrm{w}}_{\mathrm{t}}\right)\right]\nabla \hat{\mathrm{q}}\left({\mathrm{s}}_{\mathrm{i}},{\mathrm{a}}_{\mathrm{i}};{\mathrm{w}}_{\mathrm{t}}\right) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equ4.png)

(6.4)

![$$ {w}_{t+1}\leftarrow {w}_t-\upalpha {\nabla}_wL $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equ5.png)

(6.5)

结合这两者，我们得到我们熟悉的方程更新( [6.2](#Equ2) )。然而，在 PyTorch 和 TensorFlow 中，我们不通过直接编码来进行更新，因为计算梯度![$$ \nabla \hat{q}\left({s}_i,{a}_i;{w}_t\right) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_IEq1.png)并不容易。这是使用 PyTorch 和 TensorFlow 等软件包的主要原因之一，这些软件包可以根据计算损耗度量 *L* 的操作自动计算梯度。我们只需要一个函数来计算这个度量。这是通过函数`compute_td_loss.`完成的，它接收一批`(states, actions, rewards, next_states, done_flags)`。它还接受折扣参数 *γ* 以及代理/在线和目标网络。该函数然后根据等式( [6.3](#Equ3) )计算损耗 *L* 。清单 [6-5](#PC6) 给出 PyTorch 中的实现，清单 [6-6](#PC7) 给出 TensorFlow 中的实现。

```py
def compute_td_loss(agent, target_network, states, actions, rewards, next_states,
                     done_flags, gamma=0.99):

    # get q-values for all actions in current states
    # use agent network
    predicted_qvalues = agent(states)

    # compute q-values for all actions in next states
    # use target network
    predicted_next_qvalues = target_network(next_states)

    # select q-values for chosen actions
    row_indices= tf.range(len(actions))
    indices = tf.transpose([row_indices, actions])
    predicted_qvalues_for_actions = tf.gather_nd(predicted_qvalues, indices)

    # compute Qmax(next_states, actions) using predicted next q-values
    next_state_values = tf.reduce_max(predicted_next_qvalues, axis=1)

    # compute "target q-values"
    target_qvalues_for_actions = rewards + gamma * next_state_values * (1-done_flags)

    # mean squared error loss to minimize
    loss = tf.keras.losses.MSE(target_qvalues_for_actions, predicted_qvalues_for_actions)

    return loss

Listing 6-6Compute TD Loss in TensorFlow

```

```py
def compute_td_loss(agent, target_network, states, actions, rewards, next_states, done_flags,
                    gamma=0.99, device=device):

    # convert numpy array to torch tensors
    states = torch.tensor(states, device=device, dtype=torch.float)
    actions = torch.tensor(actions, device=device, dtype=torch.long)
    rewards = torch.tensor(rewards, device=device, dtype=torch.float)
    next_states = torch.tensor(next_states, device=device, dtype=torch.float)
    done_flags = torch.tensor(done_flags.astype('float32'),device=device,dtype=torch.float)

    # get q-values for all actions in current states
    # use agent network
    predicted_qvalues = agent(states)

    # compute q-values for all actions in next states
    # use target network
    predicted_next_qvalues = target_network(next_states)

    # select q-values for chosen actions
    predicted_qvalues_for_actions = predicted_qvalues[range(
        len(actions)), actions]

    # compute Qmax(next_states, actions) using predicted next q-values
    next_state_values,_ = torch.max(predicted_next_qvalues, dim=1)

    # compute "target q-values"
    target_qvalues_for_actions = rewards + gamma * next_state_values * (1-done_flags)

    # mean squared error loss to minimize
    loss = torch.mean((predicted_qvalues_for_actions -
                       target_qvalues_for_actions.detach()) ** 2)

    return loss

Listing 6-5Compute TD Loss in PyTorch

```

在这一点上，我们有所有的机器来训练代理平衡杆。首先，我们定义一些超参数，如`batch_size`，总训练步数`total_steps`，以及探索ε衰减的速率。它从 1.0 开始，随着代理学习最优策略，慢慢地将探索减少到 0.05。我们还定义了一个`optimizer`，它可以接受前面列表中创建的损失 *L* ，并帮助我们采取梯度步骤来调整权重，本质上实现了等式( [6.4](#Equ4) )和( [6.5](#Equ5) )。清单 [6-7](#PC8) 给出了 PyTorch 中的训练代码。清单 [6-8](#PC9) 给出了 TensorFlow 中相同的代码。

```py
for step in trange(total_steps + 1):

    # reduce exploration as we progress
    agent.epsilon = epsilon_schedule(start_epsilon, end_epsilon, step, eps_decay_final_step)

    # take timesteps_per_epoch and update experience replay buffer
    _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)

    # train by sampling batch_size of data from experience replay
    states, actions, rewards, next_states, done_flags = exp_replay.sample(batch_size)

    with tf.GradientTape() as tape:
        # loss = <compute TD loss>
        loss = compute_td_loss(agent, target_network,
                               states, actions, rewards, next_states, done_flags,
                               gamma=0.99)

    gradients = tape.gradient(loss, agent.model.trainable_variables)
    clipped_grads = [tf.clip_by_norm(g, max_grad_norm) for g in gradients]
    optimizer.apply_gradients(zip(clipped_grads, agent.model.trainable_variables))

Listing 6-8Train the Agent in TensorFlow

```

```py
for step in trange(total_steps + 1):

    # reduce exploration as we progress
    agent.epsilon = epsilon_schedule(start_epsilon, end_epsilon, step, eps_decay_final_step)

    # take timesteps_per_epoch and update experience replay buffer
    _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)

    # train by sampling batch_size of data from experience replay
    states, actions, rewards, next_states, done_flags = exp_replay.sample(batch_size)

    # loss = <compute TD loss>
    loss = compute_td_loss(agent, target_network,
                           states, actions, rewards, next_states, done_flags,
                           gamma=0.99,
                           device=device)

    loss.backward()
    grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)
    opt.step()
    opt.zero_grad()

###Omitted code here###
### code to periodically evaluate the performance and plot some graphs

Listing 6-7Train the Agent in PyTorch

```

我们现在有一个训练有素的特工。我们训练代理人，并在我们训练代理人 50，000 步时定期绘制每集的平均奖励。在图 [6-2](#Fig2) 的左图中，x 轴值 10 对应第 10000<sup>步</sup>。我们还绘制了每 20 步的 TD 损耗，这就是为什么右侧图中的 x 轴从 0 到 2500，即 0 到 2500x20=50，000 步。与监督学习不同，目标不是固定的。我们在短时间内保持目标网络固定，并通过用在线网络刷新目标网络权重来定期更新它。此外，如所讨论的，具有偏离策略学习(Q 学习)和引导目标(目标网络只是实际值的估计和使用其他 Q 值的当前估计形成的估计)的非线性函数近似(神经网络)没有收敛保证。培训可能会看到损失上升，爆炸或波动。与通常的监督学习中的损失图相比，这个损失图是违反直觉的。图 [6-2](#Fig2) 显示了来自训练 DQN 的图表。

![../images/502835_1_En_6_Chapter/502835_1_En_6_Fig2_HTML.jpg](../images/502835_1_En_6_Chapter/502835_1_En_6_Fig2_HTML.jpg)

图 6-2

DQN 的训练曲线

代码笔记本`listing6_1_dqn_pytorch.ipynb`和`listing6_1_dqn_tensorflow.ipynb`有更多的代码，以视频文件的形式记录受训代理的行为，然后播放视频来展示行为。

这完成了使用深度学习来训练代理的完整 DQN 的实现。对于这样一个简单的网络来说，使用一个复杂的神经网络可能看起来有些矫枉过正。核心思想是专注于算法，教你如何写一个 DQN 学习代理。我们现在将使用相同的实现，但稍作调整，以便代理可以使用游戏图像像素值作为状态来玩 Atari 游戏。

### 使用 DQN 的雅达利游戏代理

在 2013 年题为“用深度强化学习玩雅达利”的开创性论文中， <sup>[1](#Fn1)</sup> 作者使用深度学习模型创建了一种基于神经网络的 Q 学习算法。他们将其命名为深度 Q 网络。这正是我们在上一节中实现的。我们现在将简要讨论作者采取的训练代理玩 Atari 游戏的附加步骤。主要要点与上一节相同，但有两个关键区别:使用游戏图像像素值作为需要一些预处理的状态输入，以及在代理内部使用卷积网络，而不是我们在上一节中看到的线性层。计算损耗 *L* 和进行训练的其余方法与上一节相同。请注意，使用卷积网络进行训练需要大量时间，尤其是在普通 PC/笔记本电脑上。准备好观看训练代码运行几个小时，即使是在中等强大的基于 GPU 的机器上。

您可以在文件`listing6_2_dqn_atari_pytorch.ipynb`中找到在 PyTorch 中训练代理的完整代码。你可以在`listing6_2_dqn_atari_tensorflow.ipynb`的 TensorFlow 中找到相同的代码。体育馆图书馆已经在 Atari 图像上实现了许多所需的转换，只要有可能，我们将使用相同的东西。

现在让我们来讨论一下为了将图像像素值输入到深度学习网络而进行的图像预处理。我们将在一个名为 Breakout 的游戏中讨论这个问题，在这个游戏中，底部有一个桨，这个想法是移动桨以确保球不会掉到它下面。我们需要用桨击打并取出尽可能多的砖块。每当球错过球拍，玩家就失去一条生命。玩家有五次生命开始。图 [6-3](#Fig3) 显示了游戏的三帧画面。

![../images/502835_1_En_6_Chapter/502835_1_En_6_Fig3_HTML.jpg](../images/502835_1_En_6_Chapter/502835_1_En_6_Fig3_HTML.jpg)

图 6-3

雅达利突破游戏图片

Atari 游戏图像是具有 128 色调色板的 210×160 像素图像。我们将做预处理来修剪图像，使卷积网络运行得更快。我们缩小图像。我们还删除了侧面的一些信息，只保留图像的相关部分用于训练。我们可以再次将图像转换为灰度，以减少输入向量的大小，用一个灰度通道代替 RGB 的三个颜色通道(红、绿、蓝通道)。尺寸为(PyTorch 中的 1×84×84 或 TensorFlow 中的 84×84×1)的预处理单帧图像仅给出静态。球或桨的位置不能告诉我们两者运动的方向。相应地，我们将把几帧连续的游戏图像堆叠在一起，来训练代理。我们将叠加四幅缩小尺寸的灰度图像，这些图像将状态 *s* 输入神经网络。输入(即状态 *s* )在 PyTorch 中将是 4×84×84 的大小，在 TensorFlow 中将是 84×84×4 的大小，其中 4 是指游戏图像的四帧，84×84 是每帧的灰度图像大小。将四帧堆叠在一起将允许代理网络推断球和桨的运动方向。我们使用 Gym 的`AtariPreprocessing`来执行从 210×160×3 大小的彩色图像阵列到 84×84 的灰度图像阵列的图像缩减。该函数还通过设置`scale_obs=True.`将单个像素值从范围(0，255)缩小到(0.0，1.0)。接下来，我们使用`FrameStack`将前面讨论的四幅图像叠加在一起。最后，根据最初的方法，我们还将奖励值裁剪为-1 或 1。清单 [6-9](#PC10) 给出了执行所有这些转换的代码。

```py
from gym.wrappers import AtariPreprocessing
from gym.wrappers import FrameStack
from gym.wrappers import TransformReward

def make_env(env_name, clip_rewards=True, seed=None):
    env = gym.make(env_name)
    if seed is not None:
        env.seed(seed)
    env = AtariPreprocessing(env, screen_size=84, scale_obs=True)
    env = FrameStack(env, num_stack=4)
    if clip_rewards:
        env = TransformReward(env, lambda r: np.sign(r))
    return env

Listing 6-9Train the Agent in PyTorch

```

前面的预处理步骤产生了我们将输入网络的最终状态。这在 PyTorch 中将是 4×84×84 的大小，在 TensorFlow 中将是 84×84×4 的大小，其中 4 是指游戏图像的四帧，84×84 是每帧的灰度图像大小。图 [6-4](#Fig4) 显示了网络的输入。

![../images/502835_1_En_6_Chapter/502835_1_En_6_Fig4_HTML.jpg](../images/502835_1_En_6_Chapter/502835_1_En_6_Fig4_HTML.jpg)

图 6-4

经处理的图像将用作神经网络的状态输入

接下来，我们构建神经网络，该网络将接收之前的图像，即状态/观察值 *s* ，并为这种情况下的所有四个动作产生 q 值。这个游戏的动作是`['NOOP', 'FIRE', 'RIGHT', 'LEFT']`，用空格键开始，也就是开火，按键盘上的 A 向左移动拨片，按 D 向右移动拨片，最后按 Esc 退出游戏。以下是我们将要构建的网络的规格:

```py
    input: tensorflow: [batch_size, 84, 84, 4]
        pytorch:  [batch_size, 4, 84, 84]

    1st hidden layer: 16 nos of 8x8 filters with stride 4 and ReLU activation
    2nd hidden layer: 32 nos of 4x4 filters with stride of 2 and ReLU activation
    3nd hidden layer: Linear layer with 256 outputs and ReLU activation
    output layer: Linear with “n_actions” units with no activation

```

其余的代码与我们之前的代码相似。清单 [6-10](#PC12) 和清单 [6-11](#PC13) 分别显示了 PyTorch 和 TensorFlow 中修改后的 DQN 代理的代码。

```py
class DQNAgent:
    def __init__(self, state_shape, n_actions, epsilon=0):

        super().__init__()
        self.epsilon = epsilon
        self.n_actions = n_actions
        self.state_shape = state_shape

        # a simple NN with state_dim as input vector (inout is state s)
        # and self.n_actions as output vector of logits of q(s, a)
        self.model = tf.keras.models.Sequential()
        self.model.add(tf.keras.Input(shape=state_shape))
        self.model.add(tf.keras.layers.Conv2D(16, kernel_size=8, strides=4, activation="relu"))
        self.model.add(tf.keras.layers.Conv2D(32, kernel_size=4, strides=2, activation="relu"))
        self.model.add(tf.keras.layers.Flatten())
        self.model.add(tf.keras.layers.Dense(256, activation="relu"))
        self.model.add(tf.keras.layers.Dense(n_actions))

    def __call__(self, state_t):
        # pass the state at time t through the newrok to get Q(s,a)
        qvalues = self.model(state_t)
        return qvalues

    def get_qvalues(self, states):
        # input is an array of states in numpy and outout is Qvals as numpy array
        qvalues = self.model(states)
        return qvalues.numpy()

    def sample_actions(self, qvalues):
        # sample actions from a batch of q_values using epsilon greedy policy
        epsilon = self.epsilon
        batch_size, n_actions = qvalues.shape
        random_actions = np.random.choice(n_actions, size=batch_size)
        best_actions = qvalues.argmax(axis=-1)
        should_explore = np.random.choice(
            [0, 1], batch_size, p=[1-epsilon, epsilon])
        return np.where(should_explore, random_actions, best_actions)

Listing 6-11DQN Agent in TensorFlow

```

```py
class DQNAgent(nn.Module):
    def __init__(self, state_shape, n_actions, epsilon=0):

        super().__init__()
        self.epsilon = epsilon
        self.n_actions = n_actions
        self.state_shape = state_shape

        state_dim = state_shape[0]
        # a simple NN with state_dim as input vector (inout is state s)
        # and self.n_actions as output vector of logits of q(s, a)
        self.network = nn.Sequential()
        self.network.add_module('conv1', nn.Conv2d(4,16,kernel_size=8, stride=4))
        self.network.add_module('relu1', nn.ReLU())
        self.network.add_module('conv2', nn.Conv2d(16,32,kernel_size=4, stride=2))
        self.network.add_module('relu2', nn.ReLU())
        self.network.add_module('flatten', nn.Flatten())
        self.network.add_module('linear3', nn.Linear(2592, 256)) #2592 calculated above
        self.network.add_module('relu3', nn.ReLU())
        self.network.add_module('linear4', nn.Linear(256, n_actions))

        self.parameters = self.network.parameters

    def forward(self, state_t):
        # pass the state at time t through the newrok to get Q(s,a)
        qvalues = self.network(state_t)
        return qvalues

    def get_qvalues(self, states):
        # input is an array of states in numpy and outout is Qvals as numpy array
        states = torch.tensor(states, device=device, dtype=torch.float32)
        qvalues = self.forward(states)
        return qvalues.data.cpu().numpy()

    def sample_actions(self, qvalues):
        # sample actions from a batch of q_values using epsilon greedy policy
        epsilon = self.epsilon
        batch_size, n_actions = qvalues.shape
        random_actions = np.random.choice(n_actions, size=batch_size)
        best_actions = qvalues.argmax(axis=-1)
        should_explore = np.random.choice(
            [0, 1], batch_size, p=[1-epsilon, epsilon])
        return np.where(should_explore, random_actions, best_actions)

Listing 6-10DQN Agent in PyTorch

```

您会注意到 PyTorch 和 TensorFlow 在急切执行模式下的代码相似。建议你专注于一个框架并掌握概念。一旦你掌握了一个，将代码移植到另一个框架就很容易了。我们将在本书的大部分例子中使用 PyTorch，并在一些地方使用 TensorFlow 版本。

除了这两个变化，即一些特定问题的预处理和一个适合问题的神经网络，其余代码在`CartPole`和 Atari 之间保持不变。您还可以使用雅达利版本在雅达利游戏的任何版本上培训代理。此外，除了这两个变化之外，相同的代码可以用于为任何环境训练 DQN 代理。您可以从健身房库文档中查找可用的健身房环境，并尝试修改来自`listing6_1_dqn_pytorch.ipynb`或`listing6_1_dqn_atari_pytorch.ipynb`的代码，以便为不同的环境培训代理。

这就完成了 DQN 的实施和培训。现在，我们知道了如何训练 DQN 代理，我们将研究一些问题和各种方法，我们可以采取修改 DQN。正如我们在本章开始时谈到的，我们将看看一些最新的和最先进的变化。

## 优先重放

在前一章中，我们了解了如何在 DQN 中使用批量版本的更新来解决在线版本中存在的一些关键问题，更新是在每次转换时完成的，而转换会在学习一步后立即被丢弃。以下是在线版本中的关键问题:

*   训练样本(转换)是相关的，打破了独立同分布假设。在网上学习中，我们有一系列相互关联的转变。每个过渡都与前一个过渡相链接。这打破了应用梯度下降所需的内径假设。

*   随着代理学习和丢弃，它可能永远不会访问初始的探索性转换。如果代理走上了一条错误的道路，它将继续从状态空间的那一部分看到例子。它可能会选择一个非常次优的解决方案。

*   对于神经网络，基于单个转换的学习是困难且低效的。对于神经网络来说，将会有太多的变化来学习任何有效的东西。神经网络在成批学习训练样本时效果最佳。

这些问题在 DQN 通过使用存储所有过渡的体验重放得到了解决。每个跃迁都是一个`(state, action, reward, next_state, done)`元组。随着缓冲区变满，我们丢弃旧的样本来添加新的样本。然后，我们从当前缓冲区中抽样一批，缓冲区中的每个转换在一批中被选中的概率相等。它允许从缓冲区中多次选取罕见的和更具探索性的转换。然而，一个普通的*体验回放*没有任何方法来选择一些优先的重要过渡。以某种方式为存储在重放缓冲器中的每个过渡分配一个重要性分数，并使用这些重要性分数作为选择的概率从缓冲器中对批次进行采样，为重要的过渡分配较高的选择概率，如它们各自的重要性分数所表示的，这是否会有所帮助？

这就是来自 DeepMind 的论文《优先化体验回放》 <sup>[2](#Fn2)</sup> 的作者在 2016 年探索的。我们将遵循本文的主要概念来创建我们自己的体验重放实现，并将其应用于`CartPole`环境的 DQN 代理。让我们先谈一谈这些重要性分数是如何分配的，以及损失 *L* 是如何修改的。

本文的主要方法是利用训练样本的时延误差为缓冲区中的训练样本分配重要性分数。当从缓冲区中选取一批样本时，我们计算 TD 误差，作为损耗 *L* 计算的一部分。TD 误差由下式给出:

![$$ {\updelta}_i={r}_i+\left(\left(1- don{e}_i\right).\upgamma .\underset{a_i^{\prime }}{\max}\hat{q}\left({s}_i^{\prime },{a}_i^{\prime };{w}_t^{-}\right)\right)\hbox{--} \hat{q}\left({s}_i,{a}_i;{w}_t\right) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equ6.png)

(6.6)

它出现在我们计算损失的方程式( [6.3](#Equ3) )中。对所有样本的误差进行平方和平均，以计算权重向量的更新幅度，如等式( [6.4](#Equ4) 和( [6.5](#Equ5) )所示。TD 误差的幅度 *δ* <sub>*i*</sub> 表示采样跳变 *(i)* 对更新的贡献。作者使用这种推理来为每个样本分配重要性分数 *p* <sub>*i*</sub> ，其中 *p* <sub>*i*</sub> 由以下等式给出:

![$$ {\mathrm{p}}_{\mathrm{i}}=\left|{\updelta}_{\mathrm{i}}\right|+\varepsilon $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equ7.png)

(6.7)

增加一个小常数ε，避免 TD 误差 *δ* <sub>*i*</sub> 为零时 *p* <sub>*i*</sub> 为零的边缘情况。当一个新的转换被添加到缓冲器中时，我们将缓冲器中所有当前转换的最大值 *p* <sub>*i*</sub> 分配给它。当选择一个批次进行训练时，我们计算每个样本的 TD 误差 *δ* <sub>*i*</sub> 作为损失/梯度计算的一部分。然后，这个 TD 误差被用于更新缓冲器中这些样本的重要性分数。

本文还谈到了另一种基于等级的优先级排序方法。使用该方法，![$$ {p}_i=\frac{1}{\mathit{\operatorname{rank}}(i)} $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_IEq2.png)，其中*等级* ( *i* )是当基于| * δ * <sub> * i * </sub> |对重放缓冲器转换进行排序时转换(I)的等级。在我们的代码示例中，我们将使用第一种方法，称为*比例优先*。

接下来，在采样时，我们通过使用以下等式将 *p* <sub>*i*</sub> 转换成概率:

![$$ P(i)=\frac{p_i^{\upalpha}}{\sum_i{p}_i^{\upalpha}} $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equ8.png)

(6.8)

这里， *P* ( *i* )表示缓冲器中的转换(I)被采样并作为训练批次的一部分的概率。这为具有较高 TD 误差的转换分配了较高的采样概率。这里，α是一个超参数，使用网格搜索进行了调整，作者发现 *α* = 0.6 是我们将要实现的比例变量的最佳值。

先前用某种基于重要性的采样来打破均匀采样的方法引入了偏差。我们需要在计算损耗 *L* 时修正偏差。在本文中，通过用权重 *w* <sub>*i*</sub> 对每个样本进行加权，然后求和得到修正的损失函数 *L* ，使用*重要性抽样*对其进行了修正。计算重量的公式如下:

![$$ {w}_i={\left(\frac{1}{N}.\frac{1}{P(i)}\right)}^{\upbeta} $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equ9.png)

(6.9)

这里， *N* 是训练批次中的样本数， *P* ( *i* )是前面的表达式中计算出的选择样本的概率。β是另一个超参数，我们将使用论文中的值 0.4。权重由![$$ \frac{1}{ma{x}_i{w}_i} $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_IEq3.png)进一步标准化，以确保权重保持在界限内。

![$$ {w}_i=\frac{1}{ma{x}_i{w}_i}{w}_i $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equ10.png)

(6.10)

有了这些适当的改变，损耗 *L* 等式也被更新，以用*w*<sub>T5】I</sub>对批次中的每个转变进行加权，如下所示:

![$$ L=\frac{1}{N}{\sum}_{i=1}^N{\left[\left({r}_i+\left(\left(1- don{e}_i\right).\upgamma .\underset{a_i^{\prime }}{\max}\hat{q}\left({s}_i^{\prime },{a}_i^{\prime };{w}_t^{-}\right)\right)\hbox{--} \hat{q}\left({s}_i,{a}_i;{w}_t\right)\right).{w}_i\right]}^2 $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equ11.png)

(6.11)

注意等式中的 *w* <sub>*i*</sub> 。在计算出 *L* 之后，我们使用损失梯度相对于在线神经网络权重 *w* 的反向传播来遵循通常的梯度步骤。

请记住，上一个等式中的 TD 误差用于为当前训练批次中的这些转换更新重放缓冲区中的重要性分数。这就完成了对优先重放的理论讨论。我们现在来看看实现。在`listing6_3_dqn_prioritized_replay.ipynb`中给出了用优先重放训练 DQN 代理的完整代码，它有两种风格，一种在 PyTorch 中，另一种在 TensorFlow 中。然而，从现在开始，我们将只列出 PyTorch 版本。建议您在阅读完下面给出的解释后，详细研究代码和参考文件。跟踪学术论文并将论文中的细节与工作代码相匹配的能力是成为一名优秀实践者的重要组成部分。解释只是让你开始。为了牢固地掌握材料，你应该详细地遵循伴随的代码。如果你在吸收了代码的工作原理后，尝试自己编码，那就更好了。

回到解释上来，我们首先看一下优先重放实现，这是与以前的 DQN 培训笔记相比，代码中的主要变化。清单 [6-12](#PC14) 给出了优先重放的代码。大部分代码与我们之前看到的普通代码`ReplayBuffer`相似。我们现在有一个名为`self.priorities`的附加数组来保存每个样本的重要性/优先级分数 *p* <sub>*i*</sub> 。修改`add`函数，将 *p* <sub>*i*</sub> 赋值给正在添加的新样本。它只是数组`self.priorities`中的最大值。功能`sample`是变化最大的一个。使用等式( [6.8](#Equ8) )计算第一个概率，然后使用( [6.9](#Equ9) )和( [6.10](#Equ10) )计算权重。该函数现在返回另外两个数组:权重数组`np.array(weights)`和索引数组`np.array(idxs)`。索引数组包含批次中采样的缓冲区中样本的索引。这是必需的，以便在丢失步骤中计算 TD 误差之后，我们可以更新缓冲器中的优先级/重要性。功能`update_priorities(idxs, new_priorities)`正是为了这个目的。

```py
class PrioritizedReplayBuffer:
    def __init__(self, size, alpha=0.6, beta=0.4):
        self.size = size #max number of items in buffer
        self.buffer =[] #array to holde buffer
        self.next_id = 0
        self.alpha = alpha
        self.beta = beta
        self.priorities = np.ones(size)
        self.epsilon = 1e-5

    def __len__(self):
        return len(self.buffer)

    def add(self, state, action, reward, next_state, done):
        item = (state, action, reward, next_state, done)
        max_priority = self.priorities.max()
        if len(self.buffer) < self.size:
           self.buffer.append(item)
        else:
            self.buffer[self.next_id] = item
        self.priorities[self.next_id] = max_priority
        self.next_id = (self.next_id + 1) % self.size

    def sample(self, batch_size):
        priorities = self.priorities[:len(self.buffer)]
        probabilities = priorities ** self.alpha
        probabilities /= probabilities.sum()
        N = len(self.buffer)
        weights = (N * probabilities) ** (-self.beta)
        weights /= weights.max()

        idxs = np.random.choice(len(self.buffer), batch_size, p=probabilities)

        samples = [self.buffer[i] for i in idxs]
        states, actions, rewards, next_states, done_flags = list(zip(*samples))
        weights = weights[idxs]

        return  (np.array(states), np.array(actions), np.array(rewards),
                np.array(next_states), np.array(done_flags), np.array(weights), np.array(idxs))

    def update_priorities(self, idxs, new_priorities):
        self.priorities[idxs] = new_priorities+self.epsilon

Listing 6-12Prioritized Replay

```

接下来，我们来看看损失计算。代码几乎类似于我们在清单 [6-5](#PC6) 中看到的 TD 损耗计算。有两个变化。第一个是将 TD 误差乘以权重，符合等式( [6.11](#Equ11) )。第二个变化是从函数内部调用`update_priorities`来更新缓冲区中的优先级。清单 [6-13](#PC15) 显示了修改后的`TD_loss compute_td_loss_priority_replay`计算的代码。

```py
def compute_td_loss_priority_replay(agent, target_network, replay_buffer,
                                    states, actions, rewards, next_states, done_flags, weights, buffer_idxs,
                                    gamma=0.99, device=device):

    # convert numpy array to torch tensors
    states = torch.tensor(states, device=device, dtype=torch.float)
    actions = torch.tensor(actions, device=device, dtype=torch.long)
    rewards = torch.tensor(rewards, device=device, dtype=torch.float)
    next_states = torch.tensor(next_states, device=device, dtype=torch.float)
    done_flags = torch.tensor(done_flags.astype('float32'),device=device,dtype=torch.float)
    weights = torch.tensor(weights, device=device, dtype=torch.float)

    # get q-values for all actions in current states
    # use agent network
    predicted_qvalues = agent(states)

    # compute q-values for all actions in next states
    # use target network
    predicted_next_qvalues = target_network(next_states)

    # select q-values for chosen actions
    predicted_qvalues_for_actions = predicted_qvalues[range(
        len(actions)), actions]

    # compute Qmax(next_states, actions) using predicted next q-values
    next_state_values,_ = torch.max(predicted_next_qvalues, dim=1)

    # compute "target q-values"
    target_qvalues_for_actions = rewards + gamma * next_state_values * (1-done_flags)

    #compute each sample TD error
    loss = ((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2) * weights

    # mean squared error loss to minimize
    loss = loss.mean()

    # calculate new priorities and update buffer
    with torch.no_grad():
        new_priorities = predicted_qvalues_for_actions.detach() - target_qvalues_for_actions.detach()
        new_priorities = np.absolute(new_priorities.detach().numpy())
        replay_buffer.update_priorities(buffer_idxs, new_priorities)

    return loss

Listing 6-13TD Loss with Prioritized Replay

```

训练代码和以前一样。可以看看`listing6_3_dqn_prioritized_replay_pytorch.ipynb`笔记本看详情。像以前一样，我们训练代理人，我们可以看到代理人学会了用这种方法很好地平衡杆子。图 [6-5](#Fig5) 显示了训练曲线。

![../images/502835_1_En_6_Chapter/502835_1_En_6_Fig5_HTML.jpg](../images/502835_1_En_6_Chapter/502835_1_En_6_Fig5_HTML.jpg)

图 6-5

DQN 代理人的训练曲线，具有在钢管上优先重放的经验

这就完成了关于优先重放的部分。建议您参考原始论文和代码笔记本以了解更多详细信息。

## 双 Q 学习

您在第 [5](05.html) 章中看到，使用相同的网络来选择最大化行动以及该最大化行动的 q 值会导致高估偏差，进而可能导致次优政策。论文《深度强化学习与双 Q 学习》的作者首先从数学上探讨了这种偏差，然后在 DQN 关于雅达利游戏的背景下进行了探讨。

让我们看看常规 DQN 中的 max 运算。我们计算 TD 目标如下:

![$$ {Y}^{DQN}=r+\upgamma .\underset{a^{\prime }}{\max}\hat{q}\left({s}^{\prime },{a}^{\prime };{w}_t^{-}\right) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equa.png)

我们通过去掉下标( *i* )以及去掉`(1-done)`乘数(去掉了终态的第二项)来稍微简化这个等式。我们这样做是为了保持解释的整洁。现在，让我们通过将“ *max* ”移入来解开这个等式。先前的更新可以等价地写成如下:

![$$ r+\upgamma .\hat{q}\left({s}^{\prime }, argma{x}_{a^{\prime }}\hat{q}\left({s}^{\prime },{a}^{\prime };{w}_t^{-}\right);{w}_t^{-}\right) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equb.png)

我们通过首先采取最大动作，然后采取该最大动作的 q 值，将最大值移到了内部。这类似于直接取最大 q 值。在之前的展开方程中，我们可以清楚地看到，我们使用了相同的网络权重![$$ {w}_t^{-} $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_IEq4.png)，首先用于选择最佳动作，然后用于获得该动作的 q 值。这就是导致最大化偏差的原因。该论文的作者提出了一种他们称为双 DQN (DDQN)的方法，其中用于选择最佳行动的权重![$$ argma{x}_{a^{\prime }}\hat{q}\left({s}^{\prime },{a}^{\prime}\right) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_IEq5.png)来自具有权重*<sub>*t*</sub>的在线网络，然后具有权重![$$ {w}_t^{-} $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_IEq6.png)的目标网络用于选择最佳行动的 q 值。这一变化导致更新的 TD 目标如下:*

*![$$ r+\gamma .\hat{q}\left({s}^{\prime }, argma{x}_{a^{\prime }}\hat{q}\left({s}^{\prime },{a}^{\prime };{w}_t\right);{w}_t^{-}\right) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equc.png)*

 *注意，现在用于选择最佳行动的内部网络使用在线权重 *w* <sub>*t*</sub> 。其他一切保持不变。我们像以前一样计算损失，然后使用梯度步长来更新在线网络的权重。我们还定期用来自在线网络的权重更新目标网络权重。我们使用的更新损失函数如下:

![$$ L=\frac{1}{N}{\sum}_{i=1}^N{\left[{r}_i+\left(\left(1- don{e}_i\right).\upgamma .\hat{q}\left({s}_i^{\prime }, argma{x}_{a^{\prime }}\hat{q}\left({s}_i^{\prime },{a}^{\prime };{w}_t\right);{w}_t^{-}\right)\right)\hbox{--} \hat{q}\left({s}_i,{a}_i;{w}_t\right)\right]}^2 $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equ12.png)

(6.12)

作者表明，前面的方法导致高估偏差的显著减少，这反过来导致更好的政策。现在让我们来看看实现细节。与 DQN 实施相比，唯一会改变的是损失的计算方式。我们现在将使用方程 [6.12](#Equ12) 来计算损失。其他的一切，包括 DQN 代理代码，重放缓冲区，以及通过梯度反向传播进行训练的方式，都将保持不变。清单 [6-14](#PC17) 给出了修正的损失函数计算。我们用`q_s = agent(states)`计算当前的 q 值，然后，对于每一行，选择对应于动作 a <sub>i</sub> 的 q 值。然后我们使用代理网络来计算下一个状态的 q 值:`q_s1 = agent(next_states)`。这用于查找每行的最佳动作，然后我们使用具有最佳动作的目标网络来查找目标 q 值。

```py
def td_loss_ddqn(agent, target_network, states, actions, rewards, next_states, done_flags,
                    gamma=0.99, device=device):

    # convert numpy array to torch tensors
    states = torch.tensor(states, device=device, dtype=torch.float)
    actions = torch.tensor(actions, device=device, dtype=torch.long)
    rewards = torch.tensor(rewards, device=device, dtype=torch.float)
    next_states = torch.tensor(next_states, device=device, dtype=torch.float)
    done_flags = torch.tensor(done_flags.astype('float32'),device=device,dtype=torch.float)

    # get q-values for all actions in current states
    # use agent network
    q_s = agent(states)

    # select q-values for chosen actions
    q_s_a = q_s[range(
        len(actions)), actions]

    # compute q-values for all actions in next states
    # use agent network (online network)
    q_s1 = agent(next_states).detach()

    # compute Q argmax(next_states, actions) using predicted next q-values
    _,a1max = torch.max(q_s1, dim=1)

    #use target network to calclaute the q value for best action chosen above
    q_s1_target = target_network(next_states)

    q_s1_a1max = q_s1_target[range(len(a1max)), a1max]

    # compute "target q-values"
    target_q = rewards + gamma * q_s1_a1max * (1-done_flags)

    # mean squared error loss to minimize
    loss = torch.mean((q_s_a - target_q).pow(2))

    return loss

Listing 6-14TD Loss with Double Q-Learning

```

```py
    q_s1 = agent(next_states).detach()
    _,a1max = torch.max(q_s1, dim=1)
    q_s1_target = target_network(next_states)
    q_s1_a1max = q_s1_target[range(len(a1max)), a1max]

```

在`CartPole`上运行 DDQN 产生图 [6-6](#Fig6) 中给出的训练图。您可能没有注意到很大的区别，因为`CartPole`是一个太简单的问题，无法显示其优势。此外，我们已经运行了少量剧集的训练算法来演示这些算法。要了解该方法的量化优势，您应该查看参考文献。

![../images/502835_1_En_6_Chapter/502835_1_En_6_Fig6_HTML.jpg](../images/502835_1_En_6_Chapter/502835_1_En_6_Fig6_HTML.jpg)

图 6-6

扁担 DDQN 训练曲线

关于 DDQN 的讨论到此结束。接下来，我们看看决斗 DQN。

## 决斗 DQN

到目前为止，我们所有的网络都接收状态 S，并为状态 *S* 中的所有动作 *A* 产生 Q 值 *Q* ( *S* ， *A* )。图 [6-1](#Fig1) 显示了这种网络的一个例子。然而，很多时候在特定的状态下，采取任何特定的动作都没有影响。考虑这样一种情况，一辆汽车行驶在路中间，而你的汽车周围没有汽车。在这样的场景中，稍微向左或向右，或稍微加速或稍微刹车的动作都没有影响；这些动作都产生相似的 q 值。有没有办法把一个状态的平均值和采取特定行动的优势比那个平均值分开？这就是题为“深度强化学习的决斗网络架构” <sup>[4](#Fn4)</sup> 的论文作者在 2016 年采取的方法。他们表明，这导致了显著的改善，并且随着一个状态中可能的动作数量的增加，改善越大。

让我们来推导决斗 DQN 网络执行的计算。我们在第 [2 章](02.html)中的方程( [2)中看到了状态值和动作值函数的定义。9](02.html#Equ8) )和( [2。10](02.html#Equ10) )，现转载如下:

![$$ {v}_{\pi }(s)={E}_{\pi}\left[\ {G}_t\ \right|\ {S}_t=s\ \Big] $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equd.png)

![$$ {q}_{\pi}\left(s,a\right)={E}_{\pi }\ \left[\ {G}_t\ \right|\ {S}_t=s,{A}_t=a\ \Big] $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Eque.png)

然后，在关于函数逼近的第 [5](05.html) 章中，我们看到当我们切换到将状态/动作值表示为参数化函数时，这些方程随着参数 *w* 的引入而发生了一些变化。

![$$ \hat{v}\left(s,w\right)\approx {v}_{\pi }(s) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equf.png)

![$$ \hat{q}\left(s,a,w\right)\approx {q}_{\pi}\left(s,a\right) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equg.png)

两组方程都向我们展示了 *v* <sub>*π*</sub> 度量的是处于一般状态的值， *q* <sub>*π*</sub> 向我们展示了从状态 *S* 采取特定动作的值。如果我们从 *V* 中减去 *Q* ，我们会得到一个叫做*优势 A* 的东西。请注意，有一点符号超载。 *A* 内 *Q* ( *S* ， *A* )代表动作，等式左边的*A*<sub>*【π*</sub>代表优势，而不是动作。

![$$ {A}_{\uppi}\left(s,a\right)={Q}_{\uppi}\left(s,a\right)-{V}_{\uppi}(s) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equ13.png)

(6.13)

作者创建了一个网络，像以前一样接受状态 S 作为输入，在几层网络之后产生两个流，一个给出状态值 *V* ，另一个给出优势 *A* ，网络的一部分是单独的层集合，一个用于 *V* ，一个用于 *A* 。最后，最后一层结合优势 *A* 和状态值 *V* 来恢复 *Q* 。然而，为了具有更好的稳定性，他们做了一个额外的改变，从 *Q* ( *S* ， *A* )的每个输出节点中减去优势值的平均值。神经网络实现的等式如下:

![$$ \hat{Q}\left(s,a;{w}_1,{w}_2,{w}_3\right)=\hat{V}\left(s;{w}_1,{w}_2\right)+\left(\hat{A}\left(s,a;{w}_1,{w}_3\right)-\frac{1}{\left|A\right|}{\sum}_{a^{\prime }}\hat{A}\left(s,{a}^{\prime };{w}_1,{w}_3\right)\right) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equ14.png)

(6.14)

在上式中，权重 *w* <sub>1</sub> 对应网络的初始公共部分， *w* <sub>2</sub> 对应网络预测状态值![$$ \hat{V} $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_IEq7.png)的部分，最后 *w* <sub>3</sub> 对应网络预测优势![$$ \hat{A} $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_IEq8.png)的部分。图 [6-7](#Fig7) 显示了一个典型的网络架构。

![../images/502835_1_En_6_Chapter/502835_1_En_6_Fig7_HTML.jpg](../images/502835_1_En_6_Chapter/502835_1_En_6_Fig7_HTML.jpg)

图 6-7

决斗网。网络在初始层有一组公共的权重，然后它分支，一组权重产生值 *V* ，另一组产生优势 *A*

作者将这种架构命名为*决斗网络*，因为它有两个融合在一起的网络和一个初始的公共部分。由于决斗网络处于代理网络级别，因此它独立于其他组件，如重放缓冲器的类型或学习权重的方式(即，简单 DQN 或双 DQN)。因此，我们可以独立于重放缓冲器的类型或学习的类型来使用决斗网络。在我们的演练中，我们将使用一个简单的重放缓冲区，该缓冲区中的每个过渡都有统一的选择概率。此外，我们将使用 DQN 代理。与 DQN 相比，唯一的变化将是网络的构建方式。清单 6-15 显示了决斗代理网络的代码。

```py
class DuelingDQNAgent(nn.Module):
    def __init__(self, state_shape, n_actions, epsilon=0):

        super().__init__()
        self.epsilon = epsilon
        self.n_actions = n_actions
        self.state_shape = state_shape

        state_dim = state_shape[0]
        # a simple NN with state_dim as input vector (inout is state s)
        # and self.n_actions as output vector of logits of q(s, a)
        self.fc1 = nn.Linear(state_dim, 64)
        self.fc2 = nn.Linear(64, 128)
        self.fc_value = nn.Linear(128, 32)
        self.fc_adv = nn.Linear(128, 32)
        self.value = nn.Linear(32, 1)
        self.adv = nn.Linear(32, n_actions)

    def forward(self, state_t):
        # pass the state at time t through the newrok to get Q(s,a)
        x = F.relu(self.fc1(state_t))
        x = F.relu(self.fc2(x))
        v = F.relu(self.fc_value(x))
        v = self.value(v)
        adv = F.relu(self.fc_adv(x))
        adv = self.adv(adv)
        adv_avg = torch.mean(adv, dim=1, keepdim=True)
        qvalues = v + adv - adv_avg
        return qvalues

    def get_qvalues(self, states):
        # input is an array of states in numpy and outout is Qvals as numpy array
        states = torch.tensor(states, device=device, dtype=torch.float32)
        qvalues = self.forward(states)
        return qvalues.data.cpu().numpy()

    def sample_actions(self, qvalues):
        # sample actions from a batch of q_values using epsilon greedy policy
        epsilon = self.epsilon
        batch_size, n_actions = qvalues.shape
        random_actions = np.random.choice(n_actions, size=batch_size)
        best_actions = qvalues.argmax(axis=-1)
        should_explore = np.random.choice(
            [0, 1], batch_size, p=[1-epsilon, epsilon])
        return np.where(should_explore, random_actions, best_actions)

Listing 6-15Dueling Network

```

我们有两层公共网络(`self.fc1`和`self.fc2`)。对于 V 预测，我们在`fc1`和`fc2`之上还有另外两层(`self.fc_value`和`self.value`)。类似地，为了进行优势评估，我们再次在`fc1`和`fc2`之上设置了单独的两层(`self.fc_adv`和`self.adv`)。然后将这些输出组合起来，按照公式( [6.14](#Equ14) )给出修正的 q 值。代码的其余部分，如 TD 损失的计算和权重更新的梯度下降，与 DQN 保持相同。图 [6-8](#Fig8) 显示了在`CartPole`上训练前一个网络的结果。

![../images/502835_1_En_6_Chapter/502835_1_En_6_Fig8_HTML.jpg](../images/502835_1_En_6_Chapter/502835_1_En_6_Fig8_HTML.jpg)

图 6-8

决斗网络的训练曲线

就像我们说的，你可以试着用`PrioritizedReplayBuffer.`代替`ReplayBuffer`，他们也可以用双 DQN 代替 DQN 作为学习代理。关于决斗 DQN 的讨论到此结束。我们现在在下一节看一个非常不同的变体。

## 大声公 DQN

我们需要探索部分状态空间。我们一直在使用ε贪婪策略来这样做。在这个探索下，我们采取概率为(1- ε)的最大 q 值动作，我们采取概率为ε的随机动作。最近一篇题为“探索的嘈杂网络”的 2018 年论文的作者， <sup>[5](#Fn5)</sup> 使用了一种不同的方法，将随机扰动添加到线性图层作为参数，像网络权重一样，这些也是学习的。

通常的线性层是仿射变换，如下式所示:

![$$ y= wx+b $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equh.png)

在噪声线性版本中，我们在权重中引入随机扰动，如下所示:

![$$ y=\left({\upmu}^w+{\upsigma}^w\odot {\upepsilon}^w\right)x+\left({\upmu}^b+{\upsigma}^b\odot {\upepsilon}^b\right) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equi.png)

在前面的等式中，*μ*<sup>T3】wT5】， *σ* <sup>*w*</sup> ， *μ* <sup>*b*</sup> ， *σ* <sup>*b*</sup> 是学习到的网络的权值。 *ϵ* <sup>*w*</sup> 和 *ϵ* <sup>*b*</sup> 是引入随机性导致探索的随机噪声。图 [6-9](#Fig9) 给出了线性层的噪点版本示意图，解释了我们上一段刚刚讲的方程。</sup>

![../images/502835_1_En_6_Chapter/502835_1_En_6_Fig9_HTML.jpg](../images/502835_1_En_6_Chapter/502835_1_En_6_Fig9_HTML.jpg)

图 6-9

噪声线性层。权重和偏差是均值和标准差的线性组合，就像常规线性图层中的权重和偏差一样

我们将实现论文中讨论的分解版本，其中矩阵的每个元素![$$ {\upepsilon}_{i,j}^w $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_IEq9.png)都被分解。假设我们有*个 p* 单位的投入和*个 q* 单位的产出。相应地，我们生成一个*p*-大小的高斯噪声向量ϵ <sub>* i *</sub> 和一个*q*-大小的高斯噪声向量ϵ <sub>* j *</sub> 。每个![$$ {\epsilon}_{i,j}^w $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_IEq10.png)和![$$ {\upepsilon}_j^b $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_IEq11.png)现在可以写成如下形式:

![$$ {\upepsilon}_{i,j}^w=f\left({\upepsilon}_i\right)f\left({\upepsilon}_j\right) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equj.png)

![$$ {\upepsilon}_j^b=f\left({\upepsilon}_j\right) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equk.png)

![$$ f(x)=\mathit{\operatorname{sgn}}(x)\sqrt{\left|x\right|} $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equ15.png)

对于我们正在使用的因式分解网络，我们建议您按如下方式初始化权重:

*   *μ* <sup>*w*</sup> 和 *μ* <sup>*b*</sup> 的每一个元素μ<sub>I*j*</sub>，其中 p 为输入单元的个数。

*   同样，每个元素σ <sub>*i* ， *σ* <sup>*w*</sup> 和 *σ* <sup>*b*</sup> 被初始化为常数![$ \frac{\upsigma_0}{\sqrt{p}} $](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_IEq13.png)，超参数 *σ* <sub>0</sub> 设置为 0.5。</sub>

我们沿着 PyTorch 提供的线性图层创建一个噪波图层。我们通过从 PyTorch 扩展`nn.Module`来实现。这是一个简单的标准实现，您可以在`init`函数中创建您的权重向量。然后你写一个`forward`函数来获取一个输入，并通过一组噪声线性和规则线性图层进行转换。你还需要一些额外的功能。在我们的例子中，我们编写了一个名为`reset_noise`的函数来生成噪声 *ϵ* <sup>*w*</sup> 和 *ϵ* <sup>*b*</sup> 。这个函数在内部使用了一个叫做`_noise`的辅助函数。我们还有一个功能`reset_parameters`来按照前面概述的策略重置参数。我们可以使用一个嘈杂的网络与 DQN，DDQN，决斗 DQN，并优先重播在各种组合。但是，出于演练的目的，我们将重点关注对 DQN 使用常规重放缓冲区。我们也使用常规的 DQN 方法训练，而不是 DDQN。清单 [6-16](#PC19) 给出了噪声线性的代码。

```py
class NoisyLinear(nn.Module):
    def __init__(self, in_features, out_features, sigma_0 = 0.4):
        super(NoisyLinear, self).__init__()
        self.in_features  = in_features
        self.out_features = out_features
        self.sigma_0= sigma_0

        self.mu_w = nn.Parameter(torch.FloatTensor(out_features, in_features))
        self.sigma_w = nn.Parameter(torch.FloatTensor(out_features, in_features))
        self.mu_b = nn.Parameter(torch.FloatTensor(out_features))
        self.sigma_b = nn.Parameter(torch.FloatTensor(out_features))

        self.register_buffer('epsilon_w', torch.FloatTensor(out_features, in_features))
        self.register_buffer('epsilon_b', torch.FloatTensor(out_features))

        self.reset_noise()
        self.reset_params()

    def forward(self, x):
        if self.training:
            w = self.mu_w + self.sigma_w * self.epsilon_w
            b = self.mu_b + self.sigma_b * self.epsilon_b
        else:
            w = self.mu_w
            b = self.mu_b
        return F.linear(x, w, b)

    def reset_params(self):
        k = 1/self.in_features
        k_sqrt = math.sqrt(k)
        self.mu_w.data.uniform_(-k_sqrt, k_sqrt)
        self.sigma_w.data.fill_(k_sqrt*self.sigma_0)
        self.mu_b.data.uniform_(-k_sqrt, k_sqrt)
        self.sigma_b.data.fill_(k_sqrt*self.sigma_0)

    def reset_noise(self):
        eps_in = self._noise(self.in_features)
        eps_out = self._noise(self.out_features)
        self.epsilon_w.copy_(eps_out.ger(eps_in))
        self.epsilon_b.copy_(self._noise(self.out_features))

    def _noise(self, size):
        x = torch.randn(size)
        x = torch.sign(x)*torch.sqrt(torch.abs(x))
        return x

Listing 6-16Noisy Linear Layer in PyTorch

```

实现的其余部分保持不变。现在唯一的不同是，我们在 DQN 代理的函数`sample_actions`中没有ε-贪婪选择。我们还有一个`reset_noise`功能，用于在每批之后重置噪声。这符合论文中去相关的建议。清单 [6-17](#PC20) 包含了之前修改过的`NoisyDQN`版本。其余的实现类似于普通的 DQN 代理。

```py
class NoisyDQN(nn.Module):
    def __init__(self, state_shape, n_actions):
        super(NoisyDQN, self).__init__()
        self.n_actions = n_actions
        self.state_shape = state_shape
        state_dim = state_shape[0]
        # a simple NN with state_dim as input vector (inout is state s)
        # and self.n_actions as output vector of logits of q(s, a)
        self.fc1 = NoisyLinear(state_dim, 64)
        self.fc2 = NoisyLinear(64, 128)
        self.fc3 = NoisyLinear(128, 32)
        self.q = NoisyLinear(32, n_actions)

    def forward(self, state_t):
        # pass the state at time t through the newrok to get Q(s,a)
        x = F.relu(self.fc1(state_t))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        qvalues = self.q(x)
        return qvalues

    def get_qvalues(self, states):
        # input is an array of states in numpy and outout is Qvals as numpy array
        states = torch.tensor(states, device=device, dtype=torch.float32)
        qvalues = self.forward(states)
        return qvalues.data.cpu().numpy()

    def sample_actions(self, qvalues):
        # sample actions from a batch of q_values using greedy policy
        batch_size, n_actions = qvalues.shape
        best_actions = qvalues.argmax(axis=-1)
        return best_actions

    def reset_noise(self):
        self.fc1.reset_noise()
        self.fc2.reset_noise()
        self.fc3.reset_noise()
        self.q.reset_noise()

Listing 6-17NoisyDQN Agent in PyTorch

```

在`CartPole`环境中训练一个 NoisyDQN 产生训练曲线，如图 [6-10](#Fig10) 所示。我们可能看不出这个变体和 DQN(或者所有的变体)之间有什么显著的区别。原因是我们在用一个简单的问题，对它进行短集的训练。书中的想法是教你一个特定变体的内部细节。对于改进和其他观察的彻底研究，建议你参考原始论文。此外，我们再次强调，您应该详细阅读附带的 Python 笔记本，在掌握了细节之后，您应该尝试重新编写示例代码。

![../images/502835_1_En_6_Chapter/502835_1_En_6_Fig10_HTML.jpg](../images/502835_1_En_6_Chapter/502835_1_En_6_Fig10_HTML.jpg)

图 6-10

DQN 训练图

你也可以尝试编写一个吵闹版的决斗 DQN。此外，你也可以尝试学习的 DDQN 变体。换句话说，根据我们目前所学，我们可以尝试以下组合:

*   网络

*   DDQN(影响我们的学习方式)

*   决斗 DQN(影响训练架构)

*   决斗 DDQN

*   用优先重放缓冲区替换普通重放缓冲区

*   在任何以前的方法中，用 NoisyNets 代替ε-exploration

*   对 TensorFlow 上的所有组合进行编码

*   尝试许多其他健身房环境，对网络进行适当的更改(如果有的话)

*   在 Atari 上运行其中的一些，尤其是如果你有 GPU 机器的话

## 分类 51 原子 DQN (C51)

在 2017 年题为“强化学习的分布式视角”的论文中， <sup>[6](#Fn6)</sup> 作者支持 RL 的分布式本质。他们没有看 Q 值这样的期望值，而是看了 *Z* ，一个期望值为 *Q* 的随机分布。

到目前为止，我们一直在输出输入状态 *s* 的 *Q* ( *s* ， *a* )值。输出中的单元数量大小为`n_action`。在某种程度上，输出值是预期的 *Q* ( *s* ， *a* )，使用蒙特卡洛技术对多个样本进行平均，以形成对实际预期值*E*[*Q*(*s*， *a* )的估计![$$ \hat{Q}\left(s,a\right) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_IEq14.png)。

在分类的 51 原子 DQN 中，对于每个 *Q* ( *s* ， *a* ) ( `n_action`其中)，我们现在产生一个 *Q* ( *s* ， *a* )值的分布估计:每个 *Q* ( *s* ， *a* 的`n_atom`(准确地说是 51)值网络现在预测建模为分类概率分布的整个分布，而不是仅仅估计 *Q* ( *s* ， *a* )的平均值。

![$$ Q\left(s,a\right)={\sum}_i{z}_i{p}_i\left(s,a\right) $$](../images/502835_1_En_6_Chapter/502835_1_En_6_Chapter_TeX_Equl.png)

*p*<sub>*I*</sub>(*s*， *a* )是动作值在( *s* ， *a* )将为 *z* <sub>*i*</sub> 的概率。

我们现在有`n_action * n_atom`个输出，即`n_action`的每个值有`n_atom`个输出。此外，这些输出是概率。对于一个动作，我们有`n_atom`个概率，这些是 q 值在`V_min`到`V_max`范围内的任意一个`n_atom`离散值中的概率。你应该参考前面提到的论文了解更多细节。

在分布式 RL 的 C51 版本中，作者在值-10 到 10 上取𝑖为 51 个原子(支持点)。我们将在代码中使用相同的设置。因为这些值在代码中是参数化的，所以欢迎您更改它们并探究其影响。

应用贝尔曼更新后，值会发生变化，可能不会落在 51 个支持点上。有一个投影的步骤，将概率分布带回 51 个原子的支撑点。

损失也从均方误差替换为*交叉熵*损失。代理使用ε-贪婪策略进行训练，类似于 DQN。整个数学是相当复杂的，这将是一个很好的练习，让你把文章和代码一起过一遍，把每一行代码和文章中的具体细节联系起来。这是你作为 RL 实践者需要具备的一项重要技能。

类似于 DQN 方法，我们有一个类`CategoricalDQN`，它是神经网络，通过它将状态 *s* 作为输入来产生 *Q* 的分布*Z*(*s*， *a* )。有一个计算 TD 损耗的函数:`td_loss_categorical_dqn`。如前所述，我们需要一个投影步骤将值带回`n_atom`支持点，这在函数`compute_projection`中执行。计算损耗计算时，函数`compute_projection`在`td_loss_categorical_dqn`内使用。其余的训练和以前一样。

图 [6-11](#Fig11) 给出了在`CartPole`环境下运行的训练曲线。

![../images/502835_1_En_6_Chapter/502835_1_En_6_Fig11_HTML.jpg](../images/502835_1_En_6_Chapter/502835_1_En_6_Fig11_HTML.jpg)

图 6-11

分类 51 原子 DQN (C51)训练图

## 分位数回归 DQN

在关于 C51 算法的论文于 2017 年年中发表后不久，一些原始作者和其他几位作者(都来自 DeepMind)提出了一种变体，他们称之为*分位数回归 DQN* (QR-DQN)。在一篇题为“使用分位数回归的分布式强化学习” <sup>[7](#Fn7)</sup> 的论文中，作者使用了一种与最初的 C51 略有不同的方法，但仍然在相同的分布焦点区域 RL 内。

类似于分布 RL 的 C51 方法，QR-DQN 方法也依赖于使用分位数来预测 *Q* ( *s* ， *a* )的分布，而不是预测 *Q* ( *s* ， *a* )的平均值的估计。C51 和 QR DQN 都是分布式 RL 的变体，由 DeepMind 的科学家制作。

C51 方法将名为 Z <sup>π</sup> (s，a)的 Q <sup>π</sup> (s，a)的分布建模为 V <sub>min</sub> 到 V <sub>max</sub> 范围内的概率过定点的分类分布。这些点上的概率就是网络所学习到的。这种方法导致在贝尔曼更新之后使用*投影*步骤，以将新的概率带回到固定支持点`n_atoms`，该固定支持点在*V*<sub>*min*</sub>到*V*<sub>*max*</sub>上均匀分布。虽然结果是可行的，但这与推导该算法的理论基础有点脱节。

QR-DQN 的方法略有不同。支撑点仍然是 N，但是现在概率固定为 1/N，这些点的位置由网络学习。引用作者的话:

> 我们“转置”来自 C51 的参数化:前者使用 N 个固定位置用于其近似分布并调整它们的概率，而我们将固定的、均匀的概率分配给 N 个可调整的位置。

DQ DQN 使用的损失是分位数回归损失与胡伯损失的混合。这叫做*分位数胡伯损失*；参考文献中的等式 9 和 10 给出了细节。我们没有在这里显示代码清单，因为我们希望您阅读论文，并将论文中的等式与笔记本`listing6_8_qr_dqn_pytorch.ipynb`中的代码进行匹配。这篇论文充满了数学，除非你对高等数学很熟悉，否则你应该试着把重点放在方法的更高层次的细节上。

图 [6-12](#Fig12) 为训练曲线。

![../images/502835_1_En_6_Chapter/502835_1_En_6_Fig12_HTML.jpg](../images/502835_1_En_6_Chapter/502835_1_En_6_Fig12_HTML.jpg)

图 6-12

分位数回归 DQN

## 马后炮经验回放

在 open ai 2018 年发表的题为“后知之明体验回放”的论文中， <sup>[8](#Fn8)</sup> 作者提出了一种在回报稀少的环境中学习的有效方法。常见的方法是以某种方式塑造奖励函数，以引导代理朝向最优化。这是不能一概而论的。

与从成功的结果中学习的 RL 代理相比，人类似乎不仅从成功的结果中学习，也从不成功的结果中学习。这是被称为*后见之明经验回放*(她)的后见之明回放方法中提出的想法的基础。虽然她可以结合各种 RL 方法，在我们的代码走查，我们将使用她与决斗 DQN，给我们她-DQN。

在 HER 方法中，在一个情节结束后，比如说一个不成功的情节，我们形成一个次要目标，其中原始目标被终止前的最后一个状态代替，作为该轨迹的目标。

说一集已经打完: *s* <sub>0</sub> ， *s* <sub>1</sub> ，…。*s*T10*T*T13】。通常我们在重放缓冲区中存储一个元组( *s* <sub>*t*</sub> ， *a* <sub>*t*</sub> ， *r* ，*s*<sub>*t*+1</sub>， *done* )。假设这一集的目标是 *g* ，这一次无法实现。在 HER 方法中，我们将在重放缓冲区中存储以下内容:

*   (*s**| |*，*【a】***

****   (*s*<sub>*t*</sub>|*g*<sup>′</sup>， *a* <sub>*t*</sub> ，*r*(*s*<sub>*t*</sub>， *a* <sub>*t*</sub> ， *g* 对奖励进行了修改，以显示状态转换*s*<sub>*t*</sub>→*s*<sub>*t*+1</sub>对于*g*<sup>′</sup>的子目标是好是坏。*** 

 **原始文件讨论了形成这些子目标的各种策略。我们将使用一个名为 *future* 的，它是一个带有 k 个随机状态的重播，这些状态来自与正在重播的过渡相同的一集，并在它之后被观察到。

我们还使用了不同于以往笔记本电脑的环境。我们将使用一个比特翻转实验环境。假设您有一个 n 位向量，每一位都是{0，1}范围内的二进制数。因此，有 2 个 <sup>*n 个*</sup> 的组合可能。在重置时，环境以随机选择的 n 位配置开始，目标也随机选择为一些不同的 n 位配置。每个动作都是翻转一点。要翻转的位是代理试图学习的策略π( *a* | *s* )。如果代理能够找到与目标匹配的正确配置，或者当代理在一集中用尽了 **n** 个动作时，一集结束。清单 [6-18](#PC21) 显示了环境的代码。完整的代码在笔记本`listing6_9_her_dqn_pytorch.ipynb`里。

```py
class BitFlipEnvironment:

    def __init__(self, bits):
        self.bits = bits
        self.state = np.zeros((self.bits, ))
        self.goal = np.zeros((self.bits, ))
        self.reset()

    def reset(self):
        self.state = np.random.randint(2, size=self.bits).astype(np.float32)
        self.goal = np.random.randint(2, size=self.bits).astype(np.float32)
        if np.allclose(self.state, self.goal):
            self.reset()
        return self.state.copy(), self.goal.copy()

    def step(self, action):
        self.state[action] = 1 - self.state[action]  # Flip the bit on position of the action
        reward, done = self.compute_reward(self.state, self.goal)
        return self.state.copy(), reward, done

    def render(self):
        print("State: {}".format(self.state.tolist()))
        print("Goal : {}\n".format(self.goal.tolist()))

    @staticmethod
    def compute_reward(state, goal):
        done = np.allclose(state, goal)
        return 0.0 if done else -1.0, done

Listing 6-18Bit-Flipping Environment

```

我们已经实现了我们自己的`render`和`step`功能，因此我们的环境界面仍然与 Gym 中的界面相似，这样我们就可以使用我们以前开发的机器。我们还有一个自定义函数`compute_reward`，当输入一个状态和一个目标时，返回`reward`和`done`标志。

作者表明，对于常规的 DQN，其中状态(n 位的配置)被表示为深度网络，常规的 DQN 代理几乎不可能学习超过 15 位的组合。然而，结合赫尔-DQN 方法，代理人能够很容易地学习甚至像 50 左右的大数字组合。在图 [6-13](#Fig13) 中，我们给出了论文中的完整伪代码，并做了一些修改，使其与我们的符号相匹配。

Hindsight Experience Replay (HER)

![../images/502835_1_En_6_Chapter/502835_1_En_6_Fig13_HTML.png](../images/502835_1_En_6_Chapter/502835_1_En_6_Fig13_HTML.png)

图 6-13

她使用了未来策略

我们使用决斗 DQN。代码中最有趣的部分是按照图 [6-13](#Fig13) 中给出的伪代码实现 HER 算法。清单 [6-19](#PC22) 是该伪代码的逐行实现。

```py
def train_her(env, agent, target_network, optimizer, td_loss_fn):

    success_rate = 0.0
    success_rates = []

    exp_replay = ReplayBuffer(10**6)

    for epoch in range(num_epochs):

        # Decay epsilon linearly from eps_max to eps_min
        eps = max(eps_max - epoch * (eps_max - eps_min) / int(num_epochs * exploration_fraction), eps_min)
        print("Epoch: {}, exploration: {:.0f}%, success rate: {:.2f}".format(epoch + 1, 100 * eps, success_rate))
        agent.epsilon = eps
        target_network.epsilon = eps

        successes = 0
        for cycle in range(num_cycles):

            for episode in range(num_episodes):

                # Run episode and cache trajectory
                episode_trajectory = []
                state, goal = env.reset()

                for step in range(num_bits):

                    state_ = np.concatenate((state, goal))
                    qvalues = agent.get_qvalues([state_])
                    action = agent.sample_actions(qvalues)[0]
                    next_state, reward, done = env.step(action)

                    episode_trajectory.append((state, action, reward, next_state, done))
                    state = next_state
                    if done:
                        successes += 1
                        break

                # Fill up replay memory
                steps_taken = step
                for t in range(steps_taken):

                    # Usual experience replay
                    state, action, reward, next_state, done = episode_trajectory[t]
                    state_, next_state_ = np.concatenate((state, goal)), np.concatenate((next_state, goal))
                    exp_replay.add(state_, action, reward, next_state_, done)

                    # Hindsight experience replay
                    for _ in range(future_k):
                        future = random.randint(t, steps_taken)  # index of future time step
                        new_goal = episode_trajectory[future][3]  # take future next_state from (s,a,r,s',d) and set as goal
                        new_reward, new_done = env.compute_reward(next_state, new_goal)
                        state_, next_state_ = np.concatenate((state, new_goal)), np.concatenate((next_state, new_goal))
                        exp_replay.add(state_, action, new_reward, next_state_, new_done)

            # Optimize DQN
            for opt_step in range(num_opt_steps):
                # train by sampling batch_size of data from experience replay
                states, actions, rewards, next_states, done_flags = exp_replay.sample(batch_size)
                # loss = <compute TD loss>
                optimizer.zero_grad()
                loss = td_loss_fn(agent, target_network,
                                  states, actions, rewards, next_states, done_flags,
                                  gamma=0.99,
                                  device=device)
                loss.backward()
                optimizer.step()

            target_network.load_state_dict(agent.state_dict())

        success_rate = successes / (num_episodes * num_cycles)
        success_rates.append(success_rate)

    # print graph
    plt.plot(success_rates, label="HER-DQN")

    plt.legend()
    plt.xlabel("Epoch")
    plt.ylabel("Success rate")
    plt.title("Number of bits: {}".format(num_bits))
    plt.show()

Listing 6-19Hindsight Experience Replay Implementation

```

在代码中，我们使用之前编码的`td_loss_dqn`函数来计算 TD 损耗，并采取梯度步骤。我们还从ε=0.2 的非常探索性的行为策略开始，并在训练进行到一半时慢慢将其降低到零。代码的其余部分与图 [6-13](#Fig13) 中的伪代码逐行匹配。

图 [6-14](#Fig14) 为训练曲线。对于 50 位的`BitFlipping`环境，代理和她一起能够 100%成功地解决环境问题。请记住，环境以 50 位的随机组合作为起点，以另一个随机组合作为目标。代理最多有 50 个翻转动作来达到目标组合。彻底的搜索需要代理尝试 2 个 <sup>50 个</sup>组合中的每一个，除了最初开始的那个。

![../images/502835_1_En_6_Chapter/502835_1_En_6_Fig14_HTML.jpg](../images/502835_1_En_6_Chapter/502835_1_En_6_Fig14_HTML.jpg)

图 6-14

成功率图:她使用未来策略的比特翻转环境

这让我们结束了对她的讨论，也结束了这一章。

## 摘要

这是一个相当长的章节，我们看了 DQN 和它的大多数流行和最近的变种。

我们从快速回顾 Q-learning 和 DQN 更新方程的推导开始。然后，我们在 PyTorch 和 TensorFlow 中查看了一个简单的`CartPole`环境中 DQN 的实现。在这之后，我们研究了雅达利游戏，这是 2013 年的原始灵感，能够在强化学习的背景下使用深度学习。我们研究了额外的预处理步骤和网络从线性到基于卷积层的变化。

接下来，我们讨论了优先重放，其中根据分配给样本的某个重要性分数从缓冲区中选取样本，该分数与 TD 误差的大小成比例。

接下来，我们在 DQN 的背景下重新审视了双 Q 学习，即所谓的双 DQN。这是一种影响学习方式并试图减少最大化偏差的方法。

然后我们看了决斗 DQN，其中使用了两个网络和一个初始共享网络。随后是噪声层，ε贪婪的探索被噪声层所取代。

接下来，我们研究了两种类型的分布 RL，在这两种分布 RL 下，网络产生了 q 值的分布 *Z* 。它不是产生预期的动作值 *Q* ( *S* ， *A* )，而是输出整个分布，特别是分类分布。我们还看到了投影步骤和损失的使用，如*交叉熵*和*分位数 huber 损失*。

最后一部分是关于后知之明的经验回放，它解决了在奖励稀少的环境中学习的问题。以前的学习方法集中在只从成功的结果中学习，但是事后诸葛亮也允许我们从不成功的结果中学习。

我们在本章看到的许多算法和方法都是最先进的研究。通过查看原始论文以及逐行浏览代码，您将会收获良多。我们还建议了各种组合，你可以尝试编码，以进一步巩固你脑海中的概念。

本章总结了我们对基于价值的方法的探索，在这种方法中，我们首先通过使用 *V* 或 *Q* 函数来学习策略，然后使用这些函数来找到最优策略。在下一章中，我们将切换到基于策略的方法，在这种方法中，我们找到最佳策略，而不需要学习 *V* /Q 函数这一中间步骤。

<aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

[T2`https://arxiv.org/pdf/1312.5602.pdf`](https://arxiv.org/pdf/1312.5602.pdf)

  [2](#Fn2_source)

[T2`https://arxiv.org/pdf/1511.05952.pdf`](https://arxiv.org/pdf/1511.05952.pdf)

  [3](#Fn3_source)

[T2`https://arxiv.org/pdf/1509.06461v3.pdf`](https://arxiv.org/pdf/1509.06461v3.pdf)

  [4](#Fn4_source)

[T2`https://arxiv.org/pdf/1511.06581.pdf`](https://arxiv.org/pdf/1511.06581.pdf)

  [5](#Fn5_source)

[T2`https://arxiv.org/pdf/1706.10295.pdf`](https://arxiv.org/pdf/1706.10295.pdf)

  [6](#Fn6_source)

[T2`https://arxiv.org/pdf/1707.06887.pdf`](https://arxiv.org/pdf/1707.06887.pdf)

  [7](#Fn7_source)

[T2`https://arxiv.org/pdf/1710.10044.pdf`](https://arxiv.org/pdf/1710.10044.pdf)

  [8](#Fn8_source)

[T2`https://arxiv.org/pdf/1707.01495.pdf`](https://arxiv.org/pdf/1707.01495.pdf)

 </aside>***