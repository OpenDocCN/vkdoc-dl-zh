# 7.策略梯度算法

到目前为止，我们一直专注于基于模型和无模型的方法。使用这些方法的所有算法都估计了给定当前策略的动作值。在第二步中，这些估计值被用于通过选择给定状态中的最佳动作来找到更好的策略。这两个步骤反复循环进行，直到观察不到数值的进一步提高。在这一章中，我们将通过直接在策略空间中操作来研究学习最优策略的不同方法。我们将在不明确学习或使用状态或状态行为值的情况下改进策略。

我们还将看到，基于策略的方法和基于价值的方法并不是两种不相交的方法。有些方法将基于价值的方法和基于政策的方法结合起来，如行动者-批评家方法。

本章的核心将是建立定义，并从数学上推导出基于策略的优化的关键部分。基于策略的方法是目前强化学习中解决大规模连续空间问题最流行的方法之一。

## 介绍

我们首先从简单的基于模型的方法开始我们的旅程，其中我们通过迭代贝尔曼方程来解决小的、离散的状态空间问题。接下来，我们讨论了使用蒙特卡罗和时间差分方法的无模型设置。然后，我们使用函数近似将分析扩展到大的或连续的状态空间。特别是，我们将 DQN 及其许多变体视为政策学习的途径。

所有这些方法的核心思想是首先了解当前政策的价值，然后对政策进行迭代改进以获得更好的回报。这是使用*广义政策迭代* (GPI) *的一般框架完成的。*如果你想一想，你会意识到我们的真正目标是学习一个好的政策，我们使用价值函数作为中间步骤来指导我们找到一个好的政策。

这种学习价值函数以改进策略的方法是间接的。与直接学习好的政策相比，学习价值观并不总是那么容易。考虑一下你在慢跑道上遇到一只熊的情况。你首先想到的是什么？你的大脑是否试图评估可能行动的状态(你面前的熊)和行动值(“冻结”、“抚摸熊”、“逃命”或“攻击熊”)？还是几乎确定地“运行”，即遵循概率为 1.0 的`action="run"`策略？我确信答案是后者。让我们举另一个玩 Atari Breakout 游戏的例子，我们在前一章的 DQN 例子中使用的那个。考虑球几乎接近你的球拍的右边缘并远离球拍的情况(“状态”)。作为一个人类玩家，你会怎么做？你是否试图评估两个动作的状态动作值 *Q* ( *s* ， *a* )，然后决定桨需要向右还是向左移动？还是只看状态，学会右移球拍避免球掉下来？同样，我肯定答案是第二个。在这两个例子中，后一种更容易的选择是学习直接行动，而不是先学习值，然后使用状态值在可能的选择中找到最佳行动。

### 基于政策的方法的利弊

前面的例子表明，在许多情况下，与学习值函数然后使用它们来学习策略相比，学习策略(在给定状态下采取什么行动)更容易。那么，为什么我们要经历像萨莎、Q-learning、DQN 等价值方法的途径呢？，一点都没有？好吧，政策学习虽然更容易，但也不是一帆风顺的。它有自己的一系列挑战，特别是基于我们目前的知识和可用的算法，这些挑战如下:

*   优势
    *   更好的融合

    *   在高维连续动作空间有效

    *   学习随机政策

*   不足之处
    *   通常收敛于局部最大值而不是全局最大值

    *   政策评估效率低且差异大

详细阐述这几点，还记得 DQN 学习曲线吗？我们看到政策的价值在培训中变化很大。在“车杆子”问题中，我们看到分数(如上一章所有训练进度图左图所示)波动很大。对于更好的政策没有稳定的一致意见。基于策略的方法，特别是我们将在本章末尾讨论的一些附加控制，确保我们在学习过程中朝着更好的策略平稳地前进。

我们的行动空间一直是一个可能行动的小集合。即使在函数逼近与 DQN 这样的深度学习相结合的情况下，我们的动作空间也仅限于个位数。这些动作是一维的。想象一下，试图一起控制行走机器人的各个关节。我们需要对机器人的每个关节做出决定，在机器人的给定状态下，这些单独的选择合在一起会做出一个完整的动作。此外，每个关节的单独动作不会是离散的。最有可能的是，这些动作，如马达的速度或手臂或腿需要移动的角度，将会在一个连续的范围内。基于策略的方法更适合处理这些操作。

在我们迄今为止看到的所有基于价值的方法中，我们总是学到一种最优策略——一种确定性策略，在这种策略中，我们确切地知道在给定状态下应该采取的最佳行动。实际上，我们不得不引入探索的概念，使用ε-贪婪策略来尝试不同的行动，随着代理人学会采取更好的行动，探索概率会降低。最终的结果总是一个确定的政策。然而，确定性策略并不总是最优的。在有些情况下，最优策略是以某种概率分布采取多个行动，尤其是在多代理环境中。如果你有一些博弈论的经验，你会立即从囚徒困境和相应的纳什均衡中意识到这一点。不管怎样，我们来看一个简单的情况。

你玩过石头剪子布的游戏吗？这是一个双人游戏。在一个回合中，每个玩家必须从剪刀、石头或布三个选项中选择一个。两个玩家同时这样做，同时展示他们的选择。规则规定剪刀打败布是因为剪刀能剪出布，石头打败剪刀是因为石头能砸破剪刀，而布打败石头是因为纸能盖住石头。

什么是最好的政策？没有明显的赢家。如果你总是选择，比如说，摇滚，那么我作为你的对手会利用这些知识，总是选择纸。你能想到任何其他确定性的政策吗(例如，总是从三者中选择一个)？为了避免对手利用你的策略，你必须完全随机地做出选择。你必须以相等的概率随机选择剪刀或石头或布，即随机策略。确定性策略是随机策略的一种特殊形式，其中一个选择的概率为 1.0，所有其他行为的概率为零。随机策略更通用，这就是基于策略的方法所学习的。

以下是确定性策略:

![$$ a={\uppi}_{\uptheta}(s) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equa.png)

换句话说，这是在状态 *s* 时要采取的具体动作 *a* 。

这是随机策略:

![$$ a\sim {\uppi}_{\uptheta}\left(a|s\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equb.png)

换句话说，这就是给定状态下 *s* 的动作概率分布。

也有不利之处。基于策略的方法虽然具有良好的收敛性，但可能收敛到局部最大值。第二大缺点是基于策略的方法不学习任何价值函数的直接表示，这使得评估给定策略的价值效率低下。评估策略通常需要使用策略播放代理的多个片段，然后使用这些结果来计算策略值，本质上是 MC 方法，这带来了估计策略值的高方差问题。我们将通过结合基于价值的方法和基于政策的方法这两个领域的优点，找到解决这一问题的方法。这就是所谓的*演员兼评论家*算法家族。

### 策略表示

在前一章中，我们讨论了函数逼近的无模型设置，我们在方程( [5)中表示了值函数。1](05.html#Equ1) )如下:

![$$ \hat{v}\left(s;w\right)\approx {v}_{\pi }(s) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equc.png)

![$$ \hat{q}\left(s,a;w\right)\approx {q}_{\pi}\left(s,a\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equd.png)

我们有一个权重为 *w* 的模型(线性模型或神经网络)。我们用由权重 *w* 参数化的函数来表示状态值 *v* 和状态动作值 *q* 。相反，我们现在将直接参数化策略，如下所示:

![$$ \pi \left(a|s;\uptheta \right)\approx {\uppi}_{\uptheta}\left(a|s\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Eque.png)

#### 离散案例

对于不太大的离散动作空间，我们实际上会参数化另一个函数 *h* ( *s* ，*a*；θ)用于状态-动作对。概率分布将使用 *h* 的软最大值形成。

![$$ \uppi \left(a|s;\uptheta \right)=\frac{e^{h\left(s,a;\uptheta \right)}}{\sum_b{e}^{h\left(s,b;\uptheta \right)}} $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equf.png)

值 *h* ( *s* ，*a*；θ)被称为*逻辑*或*动作偏好*。这类似于我们在监督分类情况下采用的方法。在监督学习中，我们输入观察值 *X* ，在 RL 中，我们将状态 *S* 输入到模型中。在监督情况下，模型的输出是属于不同类别的输入 *X* 的逻辑值。而在 RL 中，模型的输出是*采取该特定动作的动作偏好 h*a。

#### 连续案例

在连续动作空间中，策略的高斯表示是自然的选择。假设我们的行动空间是连续和多维的，比如说，维度为 *d* 。我们的模型将以状态 *S* 作为输入，并产生多维均值向量∈ *R* <sup>*d*</sup> 。方差*σ*<sup>2</sup>***I***<sub>*d*</sub>也可以参数化或者可以保持不变。代理将遵循的策略是具有均值 **μ** 和方差*σ*<sup>2</sup>***I***<sub>*d*</sub>的高斯策略。

![$$ \pi \left(a|s;\theta \right)\sim N\left(\mu, {\sigma}^2{I}_d\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equg.png)

## 政策梯度推导

推导基于策略的算法的方法类似于我们在监督学习中所做的。下面是我们提出算法的步骤概要:

1.  我们形成一个我们想要最大化的目标，就像监督学习一样。这将是遵循一项政策所获得的全部回报。这将是我们希望最大化的目标。

2.  我们将导出梯度更新规则来执行梯度上升。我们正在做梯度上升，而不是梯度下降，因为我们的目标是最大化总平均奖励。

3.  我们需要将梯度更新公式重新转换为期望值，以便梯度更新可以使用样本来近似。

4.  我们将正式地将更新规则转换成一个算法，该算法可以与 PyTorch 和 TensorFlow 等自动微分库一起使用。

### 目标函数

让我们从我们想要最大化的目标开始。正如前面列表中第一个项目符号所强调的，它将是策略的价值，即代理通过遵循策略可以获得的奖励。预期回报的表现形式有很多变化。我们将看看其中的一些，并简要讨论何时使用哪种表示法的背景。然而，算法的详细推导将使用其中一个变体来完成，因为其他奖励公式的推导非常相似。奖励函数及其变体如下:

*   *未打折的插曲* : ![$$ J\left(\theta \right)={\sum}_{t=0}^{T-1}{r}_r $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq1.png)

*   *偶发性打折* : ![$$ J\left(\uptheta \right)={\sum}_{t=0}^{T-1}{\upgamma}^t{r}_t $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq2.png)

*   *无限地平线打折* : ![$$ J\left(\uptheta \right)={\sum}_{t=0}^{\infty }{\upgamma}^t{r}_t $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq3.png)

*   *平均奖励* : ![$$ J\left(\uptheta \right)=\underset{T\to \infty }{\mathit{\lim}}\frac{1}{T}\sum {r}_t $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq4.png)

我们的大部分推导将遵循一个不确定的奖励结构，只是为了保持数学简单，并专注于推导的关键方面。

我也想让你感受一下折现因子 *γ* 。折扣用于无限公式中，以保持总和有界。通常，我们使用 0.99 或类似的折扣值来获得理论上有界的总和。在某些公式中，贴现因子还扮演着利息的角色——例如，今天的奖励比明天同样的奖励更有价值。使用一个折扣因子带来了今天有利于奖励的概念。贴现因子也用于通过提供时间范围的软截止来减少估计中的方差。

假设你在每一个时间步都得到 1 的回报，你使用的贴现因子是γ。这个无穷级数的和是![$$ \frac{1}{1-\upgamma} $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq5.png)。假设我们有 *γ* = 0.99。无穷级数和等于 100。因此，你可以认为 0.99 的折扣将你的视野限制在 100 步，在这 100 步中，你每一步都获得 1 英镑的奖励，这样你总共获得 100 英镑。

总的来说，折现率γ意味着时间跨度为![$$ \frac{1}{1-\gamma } $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq6.png)步。

使用γ还可以确保在轨迹的初始阶段改变政策行动的影响比在轨迹的后期阶段决策的影响对政策的整体质量有更大的影响。

回到推导，现在让我们计算用于改进策略的梯度更新。代理遵循由 *θ* 参数化的策略。

策略由 *θ* 参数化。

![$$ {\uppi}_{\uptheta}\left(a|s\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ1.png)

(7.1)

代理遵循策略并生成轨迹τ，如下所示:

![$$ {s}_1\to {a}_1\to {s}_2\to {a}_2\to \dots .\to {s}_{T-1}\to {a}_{T-1}\to {s}_T\to {a}_T $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equh.png)

这里， *s* <sub>*T*</sub> 不一定是终点状态，而是某个时间范围 *T* 直到我们考虑的轨迹。

轨迹的概率τ取决于转移概率*p*(*s*<sub>T5】t+1</sub>|*s*<sub>*t*</sub>， *a* <sub>*t*</sub> )和政策π<sub>θ</sub>(*a*<sub>*t*</sub>|)它由以下表达式给出:

![$$ {p}_{\uptheta}\left(\uptau \right)={p}_{\uptheta}\left({s}_1,{a}_1,{s}_2,{a}_2,\dots, {s}_T,{a}_T\right)=p\left({s}_1\right){\prod}_{t=1}^T{\uppi}_{\uptheta}\left({a}_t|{s}_t\right)p\left({s}_{t+1}|{s}_t,{a}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ2.png)

(7.2)

遵循策略π的预期回报由下式给出:

![$$ J\left(\uptheta \right)={E}_{\uptau \sim {p}_{\uptheta}\left(\uptau \right)}\left[{\sum}_tr\left({s}_t,{a}_t\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ3.png)

(7.3)

我们要找到使期望报酬/回报 *J* (θ)最大化的θ。换句话说，最优θ=θ<sup>∫</sup>由以下表达式给出:

![$$ {\uptheta}^{\ast }=\underset{\uptheta}{\arg\ \max}\kern0.75em {E}_{\uptau \sim {p}_{\uptheta}\left(\uptau \right)}\left[{\sum}_tr\left({s}_t,{a}_t\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ4.png)

(7.4)

在我们继续之前，让我们看看我们将如何评估目标 *J* (θ)。我们将( [7.3](#Equ3) 中的期望值转换为样本的平均值；也就是说，我们通过策略多次运行代理，收集 *N* 条轨迹。我们计算每条轨迹中的总奖励，并对跨越 *N* 条轨迹的总奖励取平均值。这是期望值的蒙特卡罗(MC)估计。这就是我们谈论评估政策时的意思。我们得到的表达式如下:

![$$ J\left(\uptheta \right)\approx \frac{1}{N}{\sum}_{i=1}^N{\sum}_{t=1}^Tr\left({s}_t^i,{a}_t^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ5.png)

(7.5)

### 导数更新规则

继续，让我们试着找到最优θ。为了让记法更容易理解，我们将∑<sub>*t*</sub>*r*(*s*<sub>*t*</sub>， *a* <sub>*t*</sub> )替换为 *r* (τ)。重写( [7.3](#Equ3) )，我们得到如下:

![$$ J\left(\uptheta \right)={E}_{\uptau \sim {p}_{\uptheta}\left(\uptau \right)}\left[r\left(\uptau \right)\right]=\int {p}_{\uptheta}\left(\uptau \right)r\left(\uptau \right)\ d\tau $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ6.png)

(7.6)

我们取上一个表达式相对于θ的梯度/导数。

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)={\nabla}_{\uptheta}\int {p}_{\uptheta}\left(\uptau \right)r\left(\uptau \right)\ d\uptau $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ7.png)

(7.7)

利用线性，我们可以移动积分内的梯度。

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)=\int {\nabla}_{\uptheta}{p}_{\uptheta}\left(\uptau \right)\kern0.5em r\left(\uptau \right)\ d\uptau $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ8.png)

(7.8)

用对数求导的小技巧，我们知道∇<sub>*x*</sub>*f*(*x*)=*f*(*x*)∇<sub>*x*</sub>对数 *f* ( *x* )。利用这一点，我们可以把前面的表达式( [7.8](#Equ8) )写成如下:

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)=\int {p}_{\uptheta}\left(\uptau \right)\ \left[{\nabla}_{\uptheta}\log {p}_{\uptheta}\left(\uptau \right)r\left(\uptau \right)\right]\kern0.5em d\uptau $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ9.png)

(7.9)

我们现在可以将积分写回期望值，这给出了下面的表达式:

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)={E}_{\uptau \sim {p}_{\uptheta}\left(\uptau \right)}\kern0.5em \left[{\nabla}_{\theta}\log {p}_{\theta}\left(\tau \right)\ r\left(\tau \right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ10.png)

(7.10)

让我们从方程( [7.2](#Equ2) )写出 *p* <sub>θ</sub> (τ)的完整表达式，从而展开术语∇<sub>θ</sub>log*p*<sub>θ</sub>(τ)。

![$$ {\nabla}_{\uptheta}\log {p}_{\uptheta}\left(\uptau \right)={\nabla}_{\uptheta}\log \left[p\left({s}_1\right){\prod}_{t=1}^T{\uppi}_{\uptheta}\left({a}_t|{s}_t\right)p\left({s}_{t+1}|{s}_t,{a}_t\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ11.png)

(7.11)

我们知道，项数乘积的对数可以写成项数对数的和。换句话说:

![$$ \log {\prod}_i{f}_i(x)={\sum}_i\mathit{\log}{f}_i(x) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ12.png)

(7.12)

将( [7.12](#Equ12) )代入方程( [7.11](#Equ11) ，得到如下结果:

![$$ {\nabla}_{\uptheta}\log {p}_{\uptheta}\left(\uptau \right)={\nabla}_{\uptheta}\left[\mathit{\log}\ p\left({s}_1\right)+{\sum}_{t=1}^T\left\{\log {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)+\log p\left({s}_{t+1}|{s}_t,{a}_t\right)\right\}\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ13.png)

(7.13)

( [7.13](#Equ13) )中唯一依赖于θ的项是π<sub>θ</sub>(*a*<sub>*t*</sub>|*s*<sub>*t*</sub>)。另外两个术语*log p*(*s*<sub>1</sub>)和 log*p*(*s*<sub>*t*+1</sub>|*s*<sub>*t*</sub>， *a* <sub>*t*</sub> )不依赖于θ。相应地，我们可以将前面的表达式( [7.13](#Equ13) )简化如下:

![$$ {\nabla}_{\uptheta}\log {p}_{\uptheta}\left(\uptau \right)={\sum}_{t=1}^T{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t|{s}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ14.png)

(7.14)

将方程( [7.14](#Equ14) )代入方程( [7.10](#Equ10) )中∇ <sub>θ</sub> *J* (θ)的表达式，并将 *r* (τ)展开为∑<sub>*t*</sub>*r*(*s*<sub>*t*</sub>*a*

*![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)={E}_{\uptau \sim {p}_{\uptheta}\left(\uptau \right)}\left[\left({\sum}_{t=1}^T{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)\right)\left({\sum}_{t=1}^Tr\left({s}_t,{a}_t\right)\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ15.png)*

*(7.15)

现在，我们可以用多个轨迹的估计值/平均值替换外部预期，以获得政策目标的*梯度的以下表达式:*

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)\approx \frac{1}{N}{\sum}_{i=1}^N\left[\left({\sum}_{t=1}^T{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right)\right)\left({\sum}_{t=1}^Tr\left({s}_t^i,{a}_t^i\right)\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ16.png)

(7.16)

其中上标索引 *i* 表示*I*<sup>T5】th</sup>轨迹。

为了改进政策，我们朝着∇<sub>θ</sub>T2】j(θ)的方向迈出了+ve 的一步。

![$$ \uptheta =\uptheta +\upalpha {\nabla}_{\uptheta}J\left(\uptheta \right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ17.png)

(7.17)

综上所述，我们设计一个以状态 *s* 为输入的模型，并产生策略分布π <sub>θ</sub> ( *a* | *s* )作为模型的输出。我们使用由当前模型参数θ确定的策略来生成轨迹，计算每个轨迹的总回报。我们用( [7.16](#Equ16) )计算∇<sub>*θ*</sub>*j*(*θ*)，然后用( [7.17](#Equ17) )中的表达式θ = θ + α∇ <sub>θ</sub> *J* (θ)改变模型参数θ。

### 更新规则背后的直觉

让我们开发一些方程式背后的直觉( [7.16](#Equ16) )。让我们用文字来解释这个等式。我们对 N 条轨迹进行平均，这是最外面的和。轨迹的平均值是多少？对于每一条轨迹，我们查看我们在该轨迹中获得的总回报，并将其乘以该轨迹上所有行为的对数概率之和。

现在假设一条轨迹的总回报*r*(τ<sup>T3】I</sup>)为+ve。第一个内部和中的每个梯度——即![$$ {\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq7.png)，该行为对数概率的梯度——乘以总回报 *r* (τ <sup>* i *</sup> )。它导致单个*梯度对数*项被轨迹的总回报放大，在方程( [7.17](#Equ17) )中，它的贡献是将模型参数θ向![$$ {\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq8.png)的+ve 方向移动，即增加系统处于状态![$$ {s}_t^i $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq10.png)时采取行动![$$ {a}_t^i $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq9.png)的概率。但是，如果*r*(τ<sup>*I*</sup>)是一个-ve 量，则方程( [7.16](#Equ16) )和( [7.17](#Equ17) )导致θ向-ve 方向移动，导致系统处于状态![$$ {s}_t^i $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq12.png)时采取动作![$$ {a}_t^i $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq11.png)的概率降低。

我们可以总结整个解释说，政策优化是所有关于试错。我们推出多种轨迹。对于那些好的轨迹，沿着轨迹的所有动作的概率增加。对于不良轨迹，沿着这些不良轨迹的所有动作的概率都降低了，如图 [7-1](#Fig1) 所示。

![../images/502835_1_En_7_Chapter/502835_1_En_7_Fig1_HTML.jpg](../images/502835_1_En_7_Chapter/502835_1_En_7_Fig1_HTML.jpg)

图 7-1

轨迹展开。轨迹 1 是好的，我们希望模型产生更多的轨迹。轨迹 2 不好也不坏，模型不要太担心。轨迹 3 是不好的，我们希望模型能降低它的概率

让我们通过比较( [7.17](#Equ17) )中的表达式和最大似然的表达式来看看同样的解释。如果我们只想对看到我们看到的轨迹的概率进行建模，我们会得到最大似然估计——我们观察到一些数据(轨迹)，我们希望建立一个产生观察到的数据/轨迹的概率最高的模型。这是最大似然模型的建立。在这种情况下，我们将得到如下等式:

![$$ {\nabla}_{\uptheta}{J}_{ML}\left(\uptheta \right)\approx \frac{1}{N}{\sum}_{i=1}^N\left[\left({\sum}_{t=1}^T{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right)\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ18.png)

(7.18)

在方程( [7.18](#Equ18) )中，我们只是增加动作的概率来增加轨迹的整体概率。我们在( [7.16](#Equ16) )的政策梯度中做了同样的事情，只是我们用回报来衡量对数概率梯度，以便增加好的轨迹和减少坏的轨迹概率——而不是增加所有轨迹的概率。

在我们结束这一节之前，我们想做的一个观察是关于马尔可夫性质和部分可观测性。在推导过程中，我们并没有真正使用马尔可夫假设。最后方程( [7.16](#Equ16) )只是说增加好东西的概率，减少坏东西的概率。到目前为止，我们还没有使用过贝尔曼方程。政策梯度也适用于非马尔可夫结构。

## 强化算法

我们现在将方程( [7.16](#Equ16) )转换成策略优化的算法。我们给出了图 [7-2](#Fig2) 中的基本算法。它被称为强化。

REINFORCE

![../images/502835_1_En_7_Chapter/502835_1_En_7_Fig2_HTML.png](../images/502835_1_En_7_Chapter/502835_1_En_7_Fig2_HTML.png)

图 7-2

强化算法

让我们看看一些实现级别的细节。假设您使用神经网络作为模型，该模型将状态值作为输入，并生成在该状态下采取所有可能行动的 logit(对数概率)。图 [7-3](#Fig3) 显示了这种模型的示意图。

![../images/502835_1_En_7_Chapter/502835_1_En_7_Fig3_HTML.jpg](../images/502835_1_En_7_Chapter/502835_1_En_7_Fig3_HTML.jpg)

图 7-3

预测政策的神经网络模型

我们使用 PyTorch 或 TensorFlow 等自动分化库。我们不明确地计算微分。等式( [7.16](#Equ16) )给出了∇<sub>θ</sub>T4】j(θ)的表达式。用 PyTorch 或者 TensorFlow，我们需要一个表达式 *J* (θ)。神经网络模型会以状态 *S* 为输入，产生π<sub>θ</sub>(*a*<sub>*t*</sub>|*S*<sub>*t*</sub>)。我们需要使用这个输出，并执行进一步的计算，以得出 *J* (θ)的表达式。PyTorch 或 TensorFlow 等自动微分软件包将根据 *J* (θ)的表达式自动计算梯度∇ <sub>θ</sub> *J* (θ)。 *J* (θ)的正确表达式如下:

![$$ \overset{\sim }{J}\left(\uptheta \right)=\frac{1}{N}{\sum}_{i=1}^N\left[\left({\sum}_{t=1}^T\log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right)\right)\left({\sum}_{t=1}^Tr\left({s}_t^i,{a}_t^i\right)\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ19.png)

(7.19)

您可以检查并确认该表达式的梯度将为我们提供∇<sub>θ</sub>T2】j(θ)的正确值，如( [7.16](#Equ16) )所示。

( [7.19](#Equ19) 中的表达式被称为*伪目标*。这是我们需要在 PyTorch 和 TensorFlow 等自动差异库中实现的表达式。我们计算对数概率![$$ \log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq13.png)，用轨迹的总回报![$$ \left({\sum}_{t=1}^Tr\left({s}_t^i,{a}_t^i\right)\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq14.png)对概率进行加权，然后计算加权量的负对数似然(NLL，或交叉熵损失)，给出我们在( [7.20](#Equ20) 中的表达式。这类似于我们在监督学习设置中用于训练多类分类模型的方法。唯一的区别是用轨迹回报来衡量对数概率。这是我们在行动离散时将采取的方法。我们在 PyTorch/TensorFlow 中实现的损失如下:

![$$ {L}_{cross- entropy}\left(\uptheta \right)=-1\ast \kern0.5em \frac{1}{N}{\sum}_{i=1}^N\left[\left({\sum}_{t=1}^T\log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right)\right)\left({\sum}_{t=1}^Tr\left({s}_t^i,{a}_t^i\right)\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ20.png)

(7.20)

请注意 PyTorch 和 TensorFlow 通过向损失的负方向迈出一步来最小化损失。还要注意( [7.20](#Equ20) )的-ve 梯度是( [7.19](#Equ19) )的+ve 梯度，因为在( [7.20](#Equ20) )中存在-1 的因子。

接下来，我们看看动作连续的情况。如所讨论的，由θ参数化的模型将状态 *S* 作为输入，并产生多元正态分布的平均值μ。我们考虑的是正态分布的方差已知并固定为某个小值的情况，比如说σ<sup>2</sup>*I*<sub>*d*</sub>。

![$$ \pi \left(a|s;\theta \right)\sim N\left(\mu, {\sigma}^2{I}_d\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equk.png)

假设状态为![$$ {s}_t^i $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq15.png)，模型产生的平均值为![$$ {\upmu}_{\theta}\left({s}_t^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq16.png)。![$$ \log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq17.png)的值由下式给出:

![$$ \log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right)=\log \frac{1}{\sqrt{2\pi}\sigma }{e}^{-\frac{1}{2{\sigma}^2}{\left({a}_t^i-{\mu}_{\theta}\right)}^2} $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equl.png)

![$$ =-\frac{1}{2}\ \log 2\pi -\log \sigma -\frac{1}{2{\sigma}^2}{\left({a}_t^i-{\mu}_{\theta}\right)}^2 $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ21.png)

(7.20)

上式中唯一依赖于模型参数θ的值是![$$ {\mu}_{\theta}\left({s}_t^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq18.png)。让我们取一个关于θ的梯度( [7.20](#Equ20) )。我们得到以下结果:

![$$ {\nabla}_{\uptheta}\log {\pi}_{\theta}\left({a}_t^i|{s}_t^i\right)= const\ x\ \left({a}_t^i-{\mu}_{\theta}\right)\ {\nabla}_{\theta }\ {\mu}_{\theta}\left({s}_t^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equm.png)

为了在 PyTorch 或 TensorFlow 中实现这一点，我们将形成一个修正的均方误差，就像我们对之前的离散操作采取的方法一样。我们用弹道返回来衡量均方误差。我们在 PyTorch 或 TensorFlow 中实现的损耗方程在( [7.21](#Equ22) )中给出。

![$$ {L}_{MSE}\left(\uptheta \right)=\kern0.75em \frac{1}{N}{\sum}_{i=1}^N\left[\left({\sum}_{t=1}^T{\left({a}_t^i-{\mu}_{\theta}\right)}^2\right)\left({\sum}_{t=1}^Tr\left({s}_t^i,{a}_t^i\right)\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ22.png)

(7.21)

再次注意，使用梯度 *L* <sub>*MSE*</sub> (θ)然后在梯度的-ve 方向上迈出一步将产生以下结果:

![$$ -{\nabla}_{\theta }{L}_{MSE}\left(\uptheta \right)=\kern0.75em \frac{1}{N}{\sum}_{i=1}^N\left[\left({\sum}_{t=1}^T\left({a}_t^i-{\mu}_{\theta}\right)\ {\nabla}_{\theta }\ {\mu}_{\theta}\left({s}_t^i\right)\right)\left({\sum}_{t=1}^Tr\left({s}_t^i,{a}_t^i\right)\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ23.png)

(7.22)

向−∇<sub>*θ*</sub>*l*<sub>*MSE*</sub>(θ)方向的一步是向![$$ \nabla \overset{\sim }{J}\left(\uptheta \right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq19.png)方向的一步，如( [7.19](#Equ19) )中给出。这是一个尝试增加![$$ \overset{\sim }{J}\left(\uptheta \right), $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq20.png)价值的步骤，即最大化保单回报。

总之，PyTorch 或 TensorFlow 中的实现要求我们在离散动作空间中形成交叉熵损失，或者在连续动作空间的情况下形成均方损失，其中每个损失项由对![$$ \left({s}_t^i,{a}_t^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq21.png)来自的轨迹的总回报加权。这类似于我们在监督学习中采取的方法，除了额外的通过轨迹返回*r*(τ<sup>*I*</sup>)=![$$ \left({\sum}_{t=1}^Tr\left({s}_t^i,{a}_t^i\right)\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq22.png)进行加权的步骤。

还请注意，*加权交叉熵损失*或*加权均方损失*没有任何意义或重要性。它只是一个方便的表达式，允许我们使用 PyTorch 和 TensorFlow 的 auto-diff 功能，通过反向传播计算梯度，然后采取措施改进策略。相比之下，在监督学习中，损失确实表明了预测的质量。在政策梯度的情况下，没有这样的推论或含义。这就是为什么我们称它们为伪亏损/目标。

### 带奖励的方差缩减

我们在方程式( [7.16](#Equ16) )中推导出的表达式，如果以目前的形式使用，就有问题。它有很高的方差。我们现在将利用问题的时间性质来做一些方差减少。

当我们推出政策(即根据政策采取行动)产生一条轨迹时，我们计算这条轨迹的总回报*r*(τ<sup>T3】I</sup>)。接下来，轨迹中动作的每个动作概率项由该轨迹回报加权。

然而，在一个时间步中采取的行动，比如说**<sup>’</sup>，只能影响我们在那个行动之后看到的回报。我们在时间步 ***t*** <sup>'</sup> 之前看到的奖励不受我们在时间步 ***t*** <sup>'</sup> 采取的动作或任何后续动作的影响。原因是世界是因果的。未来的行动不会影响过去的回报。我们将使用该属性删除( [7.16](#Equ16) )中的某些术语，并减少差异。给出了推导修正公式的步骤。请注意，它不是严格的数学证明。**

 **我们从方程开始( [7.15](#Equ15) )。

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)={E}_{\uptau \sim {p}_{\uptheta}\left(\uptau \right)}\left[\left({\sum}_{t=1}^T{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equn.png)

我们将奖励项的求和索引从 *t* 更改为*t*<sup>’</sup>，并将第一次求和中的和移到*π*T8】θ上。这给出了下面的表达式:

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)={E}_{\uptau \sim {p}_{\uptheta}\left(\uptau \right)}\left[\left({\sum}_{t=1}^T\left({\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t|{s}_t\right){\sum}_{t\prime =1}^Tr\left({s}_{t\prime },{a}_{t\prime}\right)\right)\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equo.png)

在指标 ***t*** 总和的求和项中，我们去掉了时间 ***t*** 之前的奖励项。在时间 ***t*** 时，我们采取的行动只能影响在时间 ***t*** 及以后到来的奖励。这导致第二个内和从*T*<sup>'</sup>=*T*变为 *T* ，而不是从 *t* <sup>'</sup> = 1 变为 *T* 。换句话说，开始索引现在是***t***<sup>'</sup>***= t***而不是***t =*****1**。修改后的表达式如下:

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)={E}_{\uptau \sim {p}_{\uptheta}\left(\uptau \right)}\left[\left({\sum}_{t=1}^T\left({\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t|{s}_t\right){\sum}_{t\prime =t}^Tr\left({s}_{t\prime },{a}_{t\prime}\right)\right)\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equp.png)

内部总和![$$ {\sum}_{t\prime =t}^Tr\left({s}_{t\prime },{a}_{t\prime}\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq23.png)不再是轨迹的总回报。而是我们从*时间* = *t* 到 *T* 看到的剩余轨迹的回报。如您所知，这就是 q 值。q 值是我们在状态*s*t<sub>t*t*t</sub>时，在时间 t 采取一个步骤/动作 *a* <sub>* t *</sub> 之后，从时间 *t* 开始直到结束，我们得到的预期奖励。我们也可以称之为*赏去*。因为表达式![$$ {\sum}_{t\prime =t}^Tr\left({s}_{t\prime },{a}_{t\prime}\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq24.png)只针对一条轨迹，我们表示它是对预期收益的估计。更新的梯度方程如下:

![$$ {\hat{Q}}_t^i={\sum}_{t\prime =t}^Tr\left({s}_{t\prime}^i,{a}_{t\prime}^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equq.png)

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)=\frac{1}{N}{\sum}_{i=1}^N{\sum}_{t=1}^T{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right)\ {\hat{Q}}_t^i $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ24.png)

(7.23)

要在 PyTorch 或 TensorFlow 中使用这个方程，我们只需要做一个小小的修改。不是用总的轨迹回报来衡量每个对数概率项，我们现在用该时间步的剩余回报来衡量它；换句话说，我们用奖励去价值来衡量它。图 [7-4](#Fig4) 显示了一个使用 reward to go 的修正增强算法。

Reinforce With Reward to go

![../images/502835_1_En_7_Chapter/502835_1_En_7_Fig4_HTML.png](../images/502835_1_En_7_Chapter/502835_1_En_7_Fig4_HTML.png)

图 7-4

用奖励去强化算法

本章到目前为止我们已经做了很多理论，数学公式有点超载。我们试图保持它的最小化，如果到目前为止从这一章有什么收获的话，那就是图 [7-4](#Fig4) 中的加强算法。现在让我们把这个等式付诸实践。我们将从图 [7-4](#Fig4) 对我们通常的`CartPole`问题用连续的状态空间和离散的动作来实现加强。

在此之前，我们先介绍最后一个数学术语。策略梯度算法中对状态-动作空间的探索来自于这样一个事实，即我们学习一个随机策略，该策略为给定状态的所有动作分配一个概率，而不是使用 DQN 选择最佳可能动作。为了确保探索得以维持，并确保π <sub>θ</sub> ( *a* | *s* )不会以高概率崩溃为单个动作，我们引入了一个正则化项，称为*熵*。分布的熵定义如下:

![$$ H(X)={\sum}_x-p(x).\mathit{\log}\ p(x) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equt.png)

为了保持足够的探索，我们将希望概率具有分散的分布，并且不要让概率分布过早地在单个值或小区域附近达到峰值。分布的扩散越大，分布的熵 H(x)越高。因此，输入 PyTorch/TensorFlow 最小化器的项如下:

![$$ Loss\left(\uptheta \right)=-J\left(\uptheta \right)-H\left({\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right)\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equu.png)

![$$ =-\frac{1}{N}{\sum}_{i=1}^N\left[{\sum}_{t=1}^T\left(\log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right){\sum}_{t^{\prime }=t}^T{\upgamma}^{t^{\prime }-t}r\left({s}_{t^{\prime}}^i,{a}_{t^{\prime}}^i\right)\right)-\upbeta {\sum}_{a_i}{\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right).\log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equv.png)

在我们的代码示例中，我们只采用一条轨迹，即 *N* = 1。但是，我们将对其进行平均，得出平均损失。我们将实际实现的功能如下:

![$$ Loss\left(\uptheta \right)=-J\left(\uptheta \right)-H\left({\uppi}_{\uptheta}\left({a}_t|{s}_t\right)\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equw.png)

![$$ =-\frac{1}{T}{\sum}_{t=1}^T\left(\log {\pi}_{\theta}\left({a}_t|{s}_t\right)G\left({s}_t\right)-\beta {\sum}_{a_i}{\pi}_{\theta}\left({a}_t|{s}_t\right).\log {\pi}_{\theta}\left({a}_t|{s}_t\right)\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equx.png)

在哪里，

![$$ G\left({s}_t\right)={\sum}_{t^{\prime }=t}^T{\upgamma}^{t-{t}^{\prime }}r\left({s}_{t^{\prime}}^i,{a}_{t^{\prime}}^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equy.png)

请注意，我们在前面的表达式中重新引入了贴现因子γ。

现在让我们浏览一下实现。您可以在`listing7_1_reinforce_pytorch.ipynb`中找到完整的代码清单。我们在`listing7_1_reinforce_tensorflow.ipynb`中也有一个 TensorFlow 版本的代码。然而，我们将只浏览 PyTorch 版本。TensorFlow 版本遵循几乎相同的步骤，除了我们定义网络或计算损耗以及逐步通过梯度的方式略有不同。在我们的代码中，我们在急切执行模式下使用了 TensorFlow 2.0。

我们之前解释过环境。它有一个四维连续的状态空间和两个动作的离散动作空间:“左移”和“右移”。让我们首先定义一个简单的策略网络，它有一个 192 个单元的隐藏层和 ReLU 激活。最终输出没有激活。清单 [7-1](#PC1) 显示了代码。

```
model = nn.Sequential(
            nn.Linear(state_dim,192),
            nn.ReLU(),
            nn.Linear(192,n_actions),
)

Listing 7-1Policy Network in PyTorch

```

接下来，我们定义一个`generate_trajectory`函数，该函数采用当前策略来生成一集的`(states, actions, rewards)`轨迹。它使用一个助手函数`predict_probs`来完成这项工作。清单 [7-2](#PC2) 给出了代码。它从初始化环境开始，然后按照当前策略连续采取步骤，返回它展开的轨迹的`(states, actions, rewards)`。

```
def generate_trajectory(env, n_steps=1000):
    """
    Play a session and genrate a trajectory
    returns: arrays of states, actions, rewards
    """
    states, actions, rewards = [], [], []

    # initialize the environment
    s = env.reset()

    #generate n_steps of trajectory:
    for t in range(n_steps):
        action_probs = predict_probs(np.array([s]))[0]
        #sample action based on action_probs
        a = np.random.choice(n_actions, p=action_probs)
        next_state, r, done, _ = env.step(a)

        #update arrays

        states.append(s)
        actions.append(a)
        rewards.append(r)

        s = next_state
        if done:
            break

    return states, actions, rewards

Listing 7-2generate_trajectory in PyTorch

```

我们还有另一个助手函数，根据表达式![$$ G\left({s}_t\right)={\sum}_{t^{\prime }=t}^T{\gamma}^{t-{t}^{\prime }}r\left({s}_{t^{\prime}}^i,{a}_{t^{\prime}}^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq26.png)将单个步骤的回报![$$ r\left({s}_{t^{\prime }},{a}_{t^{\prime }}\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq25.png)转换为奖励。清单 [7-3](#PC3) 包含了这个函数的实现。

```
def get_rewards_to_go(rewards, gamma=0.99):

    T = len(rewards) # total number of individual rewards
    # empty array to return the rewards to go
    rewards_to_go = [0]*T
    rewards_to_go[T-1] = rewards[T-1]

    for i in range(T-2, -1, -1): #go from T-2 to 0
        rewards_to_go[i] = gamma * rewards_to_go[i+1] + rewards[i]

    return rewards_to_go

Listing 7-3get_rewards_to_go in PyTorch

```

我们现在准备实施培训。我们构建了损失函数，我们将把它输入 PyTorch 优化器。如前所述，我们将实现以下表达式:

![$$ Loss\left(\theta \right)=-\frac{1}{T}{\sum}_{t=1}^T\left(\log {\pi}_{\theta}\left({a}_t|{s}_t\right)G\left({s}_t\right)-\beta {\sum}_{a_i}{\pi}_{\theta}\left({a}_t|{s}_t\right).\log {\pi}_{\theta}\left({a}_t|{s}_t\right)\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equz.png)

清单 [7-4](#PC4) 包含损失计算的代码。

```
#init Optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

def train_one_episode(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):

    # get rewards to go
    rewards_to_go = get_rewards_to_go(rewards, gamma)

    # convert numpy array to torch tensors

    states = torch.tensor(states, device=device, dtype=torch.float)
    actions = torch.tensor(actions, device=device, dtype=torch.long)
    rewards_to_go = torch.tensor(rewards_to_go, device=device, dtype=torch.float)

    # get action probabilities from states
    logits = model(states)
    probs = nn.functional.softmax(logits, -1)
    log_probs = nn.functional.log_softmax(logits, -1)

    log_probs_for_actions = log_probs[range(len(actions)), actions]

    #Compute loss to be minized
    J = torch.mean(log_probs_for_actions*rewards_to_go)
    H = -(probs*log_probs).sum(-1).mean()

    loss = -(J+entropy_coef*H)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return np.sum(rewards) #to show progress on training

Listing 7-4Training for One Trajectory in PyTorch

```

我们现在准备进行训练。清单 [7-5](#PC5) 展示了我们如何训练代理 10000 步，打印 100 步轨迹训练后的平均剧集奖励。一旦我们达到 300 的平均奖励，我们也停止训练。

```
total_rewards = []
for i in range(10000):
    states, actions, rewards = generate_trajectory(env)
    reward = train_one_episode(states, actions, rewards)
    total_rewards.append(reward)
    if i != 0 and i % 100 == 0:
        mean_reward = np.mean(total_rewards[-100:-1])
        print("mean reward:%.3f" % (mean_reward))
        if mean_reward > 300:
            break
env.close()

Listing 7-5Training the Agent in PyTorch

```

训练结束时，代理已经学会了很好地平衡杆子。您还会注意到，与基于 DQN 的方法相比，该程序实现这一结果所需的迭代次数和时间要少得多。

请注意，加强是一个*基于策略的*算法。

### 使用基线进一步减少差异

我们从( [7.15](#Equ15) 中的原始政策梯度更新表达式开始，并使用( [7.16](#Equ16) 中的平均值将期望值转换为估计值。接下来，我们展示了如何通过考虑奖励而不是全轨迹奖励来减少方差。等式( [7.23](#Equ24) )给出了这个奖励的表达式。

在这一节中，我们来看另一个使政策梯度更加稳定的变化。我们来考虑一下动机。假设您已经按照一个策略完成了三次轨迹的展开。假设奖励是 300，200，100。为了使解释简单，请考虑总报酬和总轨迹概率版本的梯度更新方程的情况，如方程( [7.10](#Equ10) )所示，复制如下:

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)={E}_{\uptau \sim {p}_{\uptheta}\left(\uptau \right)}\kern0.5em \left[{\nabla}_{\theta}\log {p}_{\theta}\left(\tau \right)\ r\left(\tau \right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equaa.png)

那么，渐变更新会有什么作用呢？它会用 300 衡量第一个轨迹的对数概率的梯度，用 200 衡量第二个轨迹，用 100 衡量第三个轨迹。这意味着三个轨迹中的每一个的概率都增加了不同的量。让我们来看看它的图示，如图 [7-5](#Fig5) 所示。

![../images/502835_1_En_7_Chapter/502835_1_En_7_Fig5_HTML.jpg](../images/502835_1_En_7_Chapter/502835_1_En_7_Fig5_HTML.jpg)

图 7-5

具有实际轨迹回报的策略的梯度更新

从图中可以看出，我们用不同的权重因子增加了所有三个轨迹的概率，都是+ve 权重，使得所有轨迹的概率都上升了。理想情况下，我们会喜欢增加奖励为 300 的轨迹的概率，减少奖励为 100 的轨迹的概率，因为它不是一个很好的轨迹。我们希望政策能够改变，这样就不会经常产生奖励 100 的轨迹。然而，使用当前的方法，修正的概率曲线变得更平坦，因为它试图增加所有三个轨迹的概率，并且概率曲线下的总面积必须为 1。

让我们考虑一个场景，从三个回报中减去三个轨迹的平均回报。我们得到 100，0，和-100(300-200；200-200;100-200).让我们使用修正后的轨迹奖励作为权重来进行梯度更新。图 7-6 显示了这种更新的结果。我们可以看到，概率曲线变得越来越窄，越来越陡，因为它沿 x 轴的分布越来越小。

![../images/502835_1_En_7_Chapter/502835_1_En_7_Fig6_HTML.jpg](../images/502835_1_En_7_Chapter/502835_1_En_7_Fig6_HTML.jpg)

图 7-6

具有减少基线的轨迹奖励的策略的梯度更新

用基线减少奖励会减少更新的方差。在极限中，无论我们使用还是不使用基线，结果都是一样的。基线的引入不会改变最优解。它只是减少了差异，从而加快了学习。我们将从数学上证明基线的引入不会改变梯度更新的期望值。基线可以是跨越所有轨迹和轨迹中所有步骤的固定基线，或者可以是取决于状态的变化量。然而，它不能依赖于行动。让我们先来看看基线是状态函数的推导![$$ {s}_t^i $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq28.png)。

让我们更新( [7.15](#Equ15) )中的等式，引入一个基线。

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)={E}_{\uptau \sim {p}_{\uptheta}\left(\uptau \right)}\left[\left({\sum}_{t=1}^T{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)\right)\left(r\left(\tau \right)-\mathrm{b}\Big({\mathrm{s}}_t\right)\Big)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equab.png)

让我们把*b*(*s*<sub>*t*</sub>)的项分离出来，评估期望会是什么。

![$$ {E}_{\uptau \sim {p}_{\uptheta}\left(\uptau \right)}\left[\left({\sum}_{t=1}^T{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)\right)b\left({s}_t\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equac.png)

由于期望的线性性质，我们将第一个内和移出，得到表达式。

![$$ {\sum}_{t=1}^T{E}_{{\mathrm{a}}_t\sim {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)}\left[{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)b\left({s}_t\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equad.png)

我们把期望从τ∽*p*<sub>θ</sub>(τ)切换到了 a<sub>T5】t</sub>∼π<sub>θ</sub>(*a*<sub>*t*</sub>|*s*<sub>*t*</sub>)。这是因为我们将第一个内和移到了期望值之外，之后唯一依赖于概率分布的项就是概率为π的动作*a*<sub>*t*</sub><sub>θ</sub>(*a*<sub>*t*</sub>|*s*<sub>*t*</sub>)。

我们只关注内心的期待:![$$ {E}_{\uptau \sim {p}_{\uptheta}\left(\uptau \right)}\left[{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)b\left({s}_t\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq29.png)。我们可以把它写成积分，如下所示:

![$$ {E}_{{\mathrm{a}}_t\sim {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)}\left[{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)b\left({s}_t\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equae.png)

![$$ =\int {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)\kern0.5em \left({\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)\right)\ b\left({s}_t\right)\ d{a}_t $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equaf.png)

![$$ =\int {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)\kern0.5em \frac{\nabla_{\uptheta}{\uppi}_{\uptheta}\left({a}_t|{s}_t\right)}{\uppi_{\uptheta}\left({a}_t|{s}_t\right)}\kern0.5em b\left({s}_t\right)\ d{a}_t $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equag.png)

![$$ =\int {\nabla}_{\uptheta}{\uppi}_{\uptheta}\left({a}_t|{s}_t\right)\kern0.5em b\left({s}_t\right)\ d{a}_t $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equah.png)

![$$ =b\left({s}_t\right)\ {\nabla}_{\uptheta}\int {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)\kern0.75em d{a}_t $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equai.png)

作为*b*(*s*<sub>T5】t</sub>)不依赖于 *a* <sub>*t*</sub> ，我们可以拿出来。同样，由于积分的线性，我们可以交换梯度和积分。现在积分将计算为 1，因为这是使用π <sub>θ</sub> 曲线的总概率。相应地，我们得到以下结果:

![$$ {E}_{{\mathrm{a}}_t\sim {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)}\left[{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)b\left({s}_{t\prime}\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equaj.png)

![$$ =b\left({s}_t\right){\nabla}_{\uptheta}(1)=b\left({s}_t\right).0 $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equak.png)

![$$ =0 $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equal.png)

前面的推导告诉我们，减去一个依赖于状态或者可能是常数的基线不会改变期望值。*条件是不应该依赖于动作 a* <sub>*t*</sub> 。

因此，强化基线将经历如下更新:

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)={E}_{\uptau \sim {p}_{\uptheta}\left(\uptau \right)}\left[\left({\sum}_{t=1}^T{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)\right)\left(r\left(\tau \right)-\mathrm{b}\Big({\mathrm{s}}_t\right)\Big)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ25.png)

(7.24)

我们可以用基线修改( [7.23](#Equ24) )中给出的奖励，得到以下结果:

![$$ \hat{Q}\left({s}_t^i,{a}_t^i\right)={\sum}_{t^{\prime }=t}^T{\upgamma}^{t^{\prime }-t}r\left({s}_{t^{\prime}}^i,{a}_{t^{\prime}}^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equam.png)

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)=\frac{1}{N}{\sum}_{i=1}^N{\sum}_{t=1}^T{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right)\ \left[{\hat{Q}}^i\left({s}_t,{a}_t\right)-{\mathrm{b}}^i\left({s}_t\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ26.png)

(7.25)

方程 7.25 用基线和奖励来加强。我们使用了两个技巧来减少普通钢筋的差异。我们使用一个时间结构来移除过去的奖励，而不受现在的行为的影响。然后，我们使用基线让坏政策获得-ve 奖励，让好政策获得+ve 奖励，以使政策梯度在我们学习过程中显示较低的变化。

请注意，REINFORCE 及其所有变体都是*基于策略的算法*。政策权重更新后，我们需要推出新的轨迹。旧的轨迹不再代表旧的政策。这也是为什么像基于价值的政策方法一样，加强也是样本低效的原因之一。我们不能使用早期政策的过渡。我们必须在每次权重更新后丢弃它们并生成新的转换。

## 演员-评论家方法

在本节中，我们将通过将策略梯度与价值函数相结合来进一步完善算法，以获得所谓的*行动者-批评家*算法家族(A2C/A3C)。我们先来定义一个术语叫做*优势 A* ( *s* ， *a* )。

### 定义优势

先说方程中的表达式![$$ \hat{Q}\left({s}_t^i,{a}_t^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq30.png)([7.25](#Equ26))。就是在给定的轨迹( *i* )和给定的状态 *s* <sub>* t *</sub> 中走下去的奖励。

![$$ \hat{Q}\left({s}_t^i,{a}_t^i\right)={\sum}_{t^{\prime }=t}^Tr\left({s}_{t^{\prime}}^i,{a}_{t^{\prime}}^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equan.png)

为了使用前面的表达式评估![$$ \hat{Q} $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq31.png)值，我们使用蒙特卡罗模拟。换句话说，我们正在累加从那个时间步 *t* 直到结束的所有奖励，即，直到 *T* 。它将再次具有高方差，因为它只是期望值的一个轨迹估计。在前一章无模型策略学习中，我们看到 MC 方法零偏差，但方差很大。相比之下，TD 方法有一些偏差，但方差较低，并且由于方差较低，可以导致更快的收敛。我们能在这里做类似的事情吗？这是什么奖励？表情![$$ \hat{Q}\left({s}_t^i,{a}_t^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq32.png)有什么期待？无非是状态-动作对的 q 值( *s* <sub>* t *</sub> ， *a* <sub>* t *</sub> )。如果我们可以得到 q 值，我们可以用 q 估计值代替个人奖励的总和。

![$$ {\hat{Q}}^i\left({s}_t,{a}_t\right)=q\left({s}_t,{a}_t;\phi \right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ27.png)

(7.26)

让我们将*q*(*s*<sub>*t*</sub>， *a* <sub>*t*</sub> )的值滚动一个时间步长。这类似于我们在第 [5](05.html) 章中看到的 TD(0)方法。我们可以这样写![$$ {\hat{Q}}^i\left({s}_t,{a}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq33.png):

![$$ \hat{Q}\left({s}_t^i,{a}_t^i\right)=r\left({s}_{t^{\prime}}^i,{a}_{t^{\prime}}^i\right)+V\left({s}_{t+1}\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ28.png)

(7.27)

这是未打折的展示。正如本章开始时所讨论的，我们将在有限视界未贴现设置的背景下进行所有推导。该分析可以容易地扩展到其他设置。我们将在算法的最终伪代码中切换到更一般的情况，同时将我们的分析限制在未折扣的情况。

再看方程( [7.25](#Equ26) )，你能想到一个可以用的好基线 b<sup>T3】IT5(*s*<sub>*t*</sub>)吗？用状态值*V*(*s*<sub>*t*</sub>)怎么样？如上所述，我们可以使用任何值作为基线，只要它不依赖于动作 *a* <sub>*t*</sub> 。*V*(*s*<sub>*t*</sub>)就是这样一个依赖于状态 *s* <sub>*t*</sub> 而不依赖于动作*<sub>*t*</sub>的量。*</sup>

*![$$ {b}^i\left({s}_t\right)=V\left({s}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ29.png)*

*(7.28)

使用前面的表达式:

![$$ \hat{Q}\left({s}_t^i,{a}_t^i\right)-{\mathrm{b}}^i\left({s}_t\right)={\hat{Q}}^i\left({s}_t,{a}_t\right)-V\left({s}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ30.png)

(7.29)

右侧称为*优势 A*(*s*<sub>T5】t</sub>， *a* <sub>*t*</sub> )。它是我们在状态 *s* <sub>*t*</sub> 采取步骤 *a* <sub>*t*</sub> 所获得的额外收益/奖励，它给出的奖励是![$$ \hat{Q}\left({s}_t^i,{a}_t^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq34.png)相对于我们在状态 *s* <sub>* t *</sub> 所获得的平均奖励，用 *V* ( *s* 表示我们现在可以将方程式( [7.27](#Equ28) )代入( [7.29](#Equ30) )得到如下:

![$$ \hat{A}\left({s}_t^i,{a}_t^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equao.png)

![$$ =\hat{Q}\left({s}_t^i,{a}_t^i\right)-{\mathrm{b}}^i\left({s}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equap.png)

![$$ =\hat{Q}\left({s}_t^i,{a}_t^i\right)-V\left({s}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equaq.png)

![$$ =\kern0.5em r\left({s}_{t^{\prime}}^i,{a}_{t^{\prime}}^i\right)+V\left({s}_{t+1}\right)-V\left({s}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ31.png)

(7.30)

### 优势演员评论家

继续上一节，让我们根据前面的表达式重写等式( [7.25](#Equ26) )中给出的梯度更新。

这是来自方程( [7.25](#Equ26) )的原始梯度更新:

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)=\frac{1}{N}{\sum}_{i=1}^N{\sum}_{t=1}^T{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right)\ \left[{\hat{Q}}^i\left({s}_t,{a}_t\right)-{\mathrm{b}}^i\left({s}_t\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equar.png)

代入，*b*<sup>*I*</sup>(*s*<sub>*t*</sub>)= V(*s*<sub>*t*</sub>)由( [7.28](#Equ29) )，我们得到如下:

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)=\frac{1}{N}{\sum}_{i=1}^N{\sum}_{t=1}^T{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right)\ \left[\hat{Q}\left({s}_t^i,{a}_t^i\right)-V\left({s}_t\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equas.png)

使用 MC 方法，我们得到:

![$$ \hat{Q}\left({s}_t^i,{a}_t^i\right)={\sum}_{t^{\prime }=t}^Tr\left({s}_{t^{\prime}}^i,{a}_{t^{\prime}}^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equat.png)

或者，使用 TD(0)方法，我们得到这个:

![$$ \hat{Q}\left({s}_t^i,{a}_t^i\right)=\kern0.5em r\left({s}_{t^{\prime}}^i,{a}_{t^{\prime}}^i\right)+V\left({s}_{t+1}\right)-V\left({s}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ32.png)

(7.31)

看前面表达式中的内心表达式![$$ \hat{Q}\left({s}_t^i,{a}_t^i\right)-V\left({s}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq35.png)。 *Q* 是使用当前策略遵循特定步骤 *a* <sub>* t *</sub> 的值。换句话说，“actor”和 *V* 是下面当前政策的平均值，即“critical”演员试图最大化回报，评论家告诉算法，与平均水平相比，特定步骤是好是坏。*行动者-批评者*方法是一系列算法，其中*行动者*改变政策梯度以改进行动，而*批评者*告知算法使用当前政策的行动的良好性。

我们可以用优势的形式重写( [7.30](#Equ31) )得到如下:

![$$ {\nabla}_{\uptheta}J\left(\uptheta \right)=\frac{1}{N}{\sum}_{i=1}^N{\sum}_{t=1}^T{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right)\ \left[{\hat{A}}^i\left({s}_t,{a}_t\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ33.png)

(7.32)

这种表达也是我们称之为*优势演员评论家* (A2C)的原因。请注意，演员批评家是一个方法家族，A2C 和 A3C 是其中的两个具体例子。有时在文学中演员评论家也可互换地称为 A2C。与此同时，一些论文将 A2C 称为 A3C 的同步版本，我们将在下一节简要讨论。

我们可以进一步将( [7.32](#Equ33) )与( [7.30](#Equ31) )组合起来表达如下:

![$$ MC\  approach:\hat{A}\left({s}_t^i,{a}_t^i\right)={\sum}_{t^{\prime }=t}^Tr\left({s}_{t^{\prime}}^i,{a}_{t^{\prime}}^i\right)-V\left({s}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equau.png)

![$$ TD(0)\  approach:\hat{A}\left({s}_t^i,{a}_t^i\right)=r\left({s}_{t^{\prime}}^i,{a}_{t^{\prime}}^i\right)+V\left({s}_{t+1}\right)-V\left({s}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equav.png)

使用 actor critic 的修订更新规则如下:

![$$ MC:{\nabla}_{\uptheta}J\left(\uptheta \right)=\frac{1}{N}{\sum}_{i=1}^N{\sum}_{t=1}^T{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right)\ \left[{\sum}_{t^{\prime }=t}^Tr\left({s}_{t^{\prime}}^i,{a}_{t^{\prime}}^i\right)-V\left({s}_t\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equaw.png)

![$$ TD:{\nabla}_{\uptheta}J\left(\uptheta \right)=\frac{1}{N}{\sum}_{i=1}^N{\sum}_{t=1}^T{\nabla}_{\uptheta}\log {\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right)\ \left[r\left({s}_{t^{\prime}}^i,{a}_{t^{\prime}}^i\right)+V\left({s}_{t+1}\right)-V\left({s}_t\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ34.png)

(7.33)

我们需要两个网络，一个网络估计由参数 *ϕ* 参数化的状态值函数 *V* ( *s* <sub>*t* )另一个网络输出由 *θ参数化的策略π<sub>θ</sub>(*a*<sub>*t*</sub>|*s*<sub>*t*</sub>图 [7-7](#Fig7) 显示了演员评论家的完整伪代码(也称为优势演员评论家 A2C)。*</sub>

Advantage Actor-Critic Algorithm

![../images/502835_1_En_7_Chapter/502835_1_En_7_Fig7_HTML.png](../images/502835_1_En_7_Chapter/502835_1_En_7_Fig7_HTML.png)

图 7-7

优势行动者-批评家算法

请注意，在前面的伪代码中，我们使用了一步不打折返回来获得优势![$$ \hat{A}\left({s}_t^i,{a}_t^i\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq36.png)。

![$$ r\left({s}_t^i,{a}_t^i\right)+{V}_{\varnothing}\left({s}_{t+1}\right)-{V}_{\varnothing}\left({s}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbb.png)

折扣后的一步到位版本如下:

![$$ r\left({s}_t^i,{a}_t^i\right)+\gamma {V}_{\varnothing}\left({s}_{t+1}\right)-{V}_{\varnothing}\left({s}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbc.png)

同样，n 步返回的折扣版本如下:

![$$ \left(\sum \limits_{t^{\prime }=t}^{t+n-1}{\gamma}^{t\prime -t}r\left({s}_{t^{\prime}}^i,{a}_{t^{\prime}}^i\right)\right)+{\gamma}^n{V}_{\varnothing}\left({s}_{t+n}\right)-{V}_{\varnothing}\left({s}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbd.png)

使用直接使用 rewards to go 的 MC 方法，优势如下:

![$$ \hat{A}\left({s}_t^i,{a}_t^i\right)={\sum}_{t^{\prime }=t}^T{\gamma}^{t\prime -t}r\left({s}_{t^{\prime}}^i,{a}_{t^{\prime}}^i\right)-V\left({s}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Eqube.png)

这是我们将在代码中实现的版本。

### A2C 算法的实现

我们来看看图 [7-7](#Fig7) 中伪代码的实现细节。我们需要两个网络/模型——一个是带有参数向量θ的政策网络(参与者),另一个是带有参数向量ϕ.的价值评估网络(批评家)在实际设计中，策略网络和价值评估网络可以共享一些初始权重。这类似于我们在前一章看到的决斗网络架构。这实际上是加快收敛的理想设计选择。图 [7-8](#Fig8) 给出了组合模型的示意图。

![../images/502835_1_En_7_Chapter/502835_1_En_7_Fig8_HTML.jpg](../images/502835_1_En_7_Chapter/502835_1_En_7_Fig8_HTML.jpg)

图 7-8

初始层具有公共权重的演员评论网络

在我们的代码遍历中，我们将对图 [7-7](#Fig7) 中给出的 actor-critic 算法进行以下更改:

*   We will use the MC discounted version of the advantage.

    ![$$ \hat{\mathrm{A}}\left({s}_t^i,{\mathrm{a}}_t^i\right)={\sum}_{t^{\prime }=t}^T{\gamma}^{t\prime -t}r\left({s}_{t^{\prime}}^i,{a}_{t^{\prime}}^i\right)-V\left({s}_t\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbf.png)

*   像加强，我们将介绍熵正则化。

*   代替训练第一拟合 V(s)的两个单独的损失训练步骤，然后进行策略梯度，我们将形成单个损失目标，其将与熵正则化器一起执行 V(s)拟合以及策略梯度步骤。

使用具有先前修改的演员评论家的损失如下:

![$$ Loss\left(\uptheta, \upphi \right)=-J\left(\uptheta, \upphi \right)-H\left({\uppi}_{\uptheta}\left({a}_t^i|{s}_t^i\right)\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbg.png)

![$$ =-\frac{1}{N}{\sum}_{i=1}^N\left[{\sum}_{t=1}^T\left(\log {\pi}_{\theta}\left({a}_t^i|{s}_t^i\right)\left[\hat{Q}\left({s}_t^i,{a}_t^i\right)-{V}_{\phi}\left({s}_t^i\right)\right]\right)-\beta {\sum}_a{\pi}_{\theta}\left(a|{s}_t^i\right).\log {\pi}_{\theta}\left(a|{s}_t^i\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbh.png)

像加强，我们将进行重量更新后，每个轨迹。因此， *N* = 1。但是，我们将对其进行平均，得出平均损失。因此，我们将实际实现的函数如下:

![$$ Loss\left(\uptheta, \upphi \right)=-\frac{1}{T}\left[{\sum}_{t=1}^T\left(\log {\uppi}_{\uptheta}\left({a}_t|{s}_t\right)\left[\hat{Q}\left({s}_t,{a}_t\right)-{V}_{\upphi}\left({s}_t\right)\right]\right)-\upbeta {\sum}_a{\uppi}_{\uptheta}\left(a|{s}_t\right).\log {\uppi}_{\uptheta}\left(a|{s}_t\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbi.png)

这是我们将实施的损失。你可以在`listing7_2_actor_critic_pytorch.ipynb`中找到 PyTorch 中实现 actor critic 的完整代码。代码库也有一个 TensorFlow 版本，在`listing7_2_actor_critic_tensorflow.ipynb`中给出。实施将遵循我们在加强中的相同步骤。只有一些小的变化:网络结构和损耗计算与前面的表达式一样。

先说网络。我们将有一个共享权重的联合网络，一个生成政策行动概率，另一个生成状态值。对于`CartPole`，这是一个相当简单的网络，如图 [7-9](#Fig9) 所示。

![../images/502835_1_En_7_Chapter/502835_1_En_7_Fig9_HTML.jpg](../images/502835_1_En_7_Chapter/502835_1_En_7_Fig9_HTML.jpg)

图 7-9

南极环境演员-评论家网络

清单 7-6 显示了 PyTorch 中的实现。它是网络的直接实现，如图 [7-9](#Fig9) 所示。

```
class ActorCritic(nn.Module):
    def __init__(self):
        super(ActorCritic, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.actor = nn.Linear(128,n_actions)
        self.critic = nn.Linear(128,1)

    def forward(self, s):
        x = F.relu(self.fc1(s))
        logits = self.actor(x)
        state_value = self.critic(x)
        return logits, state_value

model = ActorCritic()

Listing 7-6Actor-Critic Network in PyTorch

```

另一个变化是我们为一集实现训练代码的方式。它类似于清单 [7-4](#PC4) 中的代码，只是引入了*V*(*s*<sub>*t*</sub>)作为基线值。清单 [7-7](#PC7) 给出了`train_one_episode`的完整代码。

```
#init Optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

def train_one_episode(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):

    # get rewards to go
    rewards_to_go = get_rewards_to_go(rewards, gamma)

    # convert numpy array to torch tensors

    states = torch.tensor(states, device=device, dtype=torch.float)
    actions = torch.tensor(actions, device=device, dtype=torch.long)
    rewards_to_go = torch.tensor(rewards_to_go, device=device, dtype=torch.float)

    # get action probabilities from states
    logits, state_values = model(states)
    probs = nn.functional.softmax(logits, -1)
    log_probs = nn.functional.log_softmax(logits, -1)

    log_probs_for_actions = log_probs[range(len(actions)), actions]

    advantage = rewards_to_go - state_values.squeeze(-1)

    #Compute loss to be minized
    J = torch.mean(log_probs_for_actions*(advantage))
    H = -(probs*log_probs).sum(-1).mean()

    loss = -(J+entropy_coef*H)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return np.sum(rewards) #to show progress on training

Listing 7-7train_one_episode for Actor Critic Using MC Rewards to Go in PyTorch

```

请注意，在多个轨迹上训练的代码与之前相同。当我们运行代码时，我们看到，与强化相比，使用 A2C 的训练进行得更快，朝着更好的策略稳步前进。

请注意，演员评论也是一种政策上的方法，就像加强一样。

### 异步优势演员评论家

2016 年，论文《深度强化学习的异步方法》作者 <sup>[1](#Fn1)</sup> 介绍了 A2C 的异步版本。基本想法很简单。我们有一个全局服务器，它是提供网络参数的“参数”服务器:θ，ϕ.有多个演员-评论家代理并行运行。每个演员-评论家代理从服务器获得参数，进行轨迹滚动，并在 *θ* 、 *ϕ* 上进行梯度下降。代理将参数更新回服务器。它允许更快的学习，特别是在我们使用模拟器的环境中，例如机器人环境。我们可以首先在模拟器的多个实例上使用 A3C 训练一个算法。随后的学习将是在真实环境中的物理机器人上进一步微调/训练算法。

图 [7-10](#Fig10) 显示了 A3C 的高级示意图。请注意，这是对该方法的简单解释。对于实际的实现细节，建议您详细参考参考文献。

![../images/502835_1_En_7_Chapter/502835_1_En_7_Fig10_HTML.jpg](../images/502835_1_En_7_Chapter/502835_1_En_7_Fig10_HTML.jpg)

图 7-10

异步优势演员评论家

正如所解释的，一些论文将多个代理一起训练的同步版本称为 A3C 的 A2C 版本，即没有异步部分的 A3C。然而，有时有一个经纪人的演员评论家也被称为*优势演员评论家* (A2C)。最后，演员评论家是一个算法家族，其中我们一起使用两个网络:一个价值网络来估计 *V* ( *s* )和一个政策网络来估计政策*π*T9】θ(*a*|*s*<sub>*t*</sub>)。我们正在利用两个世界的优势:基于价值的方法和政策梯度方法。

## 信赖域策略优化算法

到目前为止，我们在本章中详细介绍的方法也被称为*标准政策梯度* (VPG)。我们使用 VPG 训练的策略是一个随机策略，它提供了自己的探索，而没有显式地使用ε-贪婪探索。随着培训的进行，策略分布变得更加清晰，以最佳行动为中心。这减少了探索，使算法越来越多地利用它所学到的东西。这会导致策略卡在局部最大值。我们试图通过引入正则化来解决这个问题，但这不是唯一的方法。

正如我们在前面章节的策略梯度方法中看到的，我们通过以下等式给出的小量来更新策略参数:

![$$ {\uptheta}_{new}={\uptheta}_{old}+\upalpha {\left.{\nabla}_{\uptheta}J\left(\uptheta \right)\right|}_{\theta ={\theta}_{old}} $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbj.png)

换句话说，在旧的策略参数*θ*=*θ*<sub>T5】旧的</sub> 处评估梯度，然后通过采取由步长 *α* 确定的小步长来更新梯度。VPG 通过使用学习率α限制策略参数从θ <sub>*旧*</sub> 到θ <sub>*新*</sub> 的变化，试图在参数空间中保持新旧策略彼此接近。但是，仅仅因为策略参数在附近，并不能保证新旧策略(即动作概率分布)实际上是相互接近的。参数 *θ* 的微小变化可能导致政策概率的巨大差异。理想情况下，我们希望在概率空间而不是参数空间中保持新旧策略彼此接近。这是 2015 年题为“信任区域策略优化”的论文的作者详述的关键见解 <sup>[2](#Fn2)</sup> 在深入细节之前，我们先花几分钟时间来讲一个叫做 Kullback-Liebler 散度(KL-divergence)的度量。这是衡量两个概率有多大不同的尺度。它来自信息论领域，深入研究它需要一本自己的书。我们将只试图给出公式及其背后的一些直觉，而不进入数学证明。

假设我们有两个离散的概率分布 P 和 Q 定义在某个值的范围内(称为*支持*)。假设支持度为“x ”,从 1 到 6。*P*<sub>T5】X</sub>(*X*=*X*)使用概率分布 P 定义了 X=x 的概率，类似地，我们在相同的支持度上定义了另一个概率分布 Q。作为例子，考虑具有概率分布的六个面的模具。

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"> <col class="tcol7 align-left"></colgroup> 
| 

x

 | 

one

 | 

Two

 | 

three

 | 

four

 | 

five

 | 

six

 |
| --- | --- | --- | --- | --- | --- | --- |
| P(x) | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 |
| Q(x) | 2/9 | 1/6 | 1/6 | 1/6 | 1/6 | 1/9 |

骰子 Q 被加载以显示少于 6 而多于 1，而 P 是公平骰子，显示骰子的任何面的概率相等。

P 和 Q 之间的 KL-散度表示如下:

![$$ {D}_{KL}\left(P\Big\Vert Q\right)=\sum \limits_xP(x)\log \frac{P(x)}{Q(x)} $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ35.png)

(7.34)

我们来计算一下上表的*D*<sub>*KL*</sub>(*P*‖*Q*)。

![$$ {D}_{KL}\left(P\Big\Vert Q\right)=\frac{1}{6}\log \frac{1/6}{2/9}+\frac{1}{6}\log \frac{1/6}{1/6}+\frac{1}{6}\log \frac{1/6}{1/6}\kern0.5em +\frac{1}{6}\log \frac{1/6}{1/6}+\frac{1}{6}\log \frac{1/6}{1/6}+\frac{1}{6}\log \frac{1/6}{1/9} $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbk.png)

![$$ =\frac{1}{6}\log \frac{3}{4}+\frac{1}{6}\log 1+\frac{1}{6}\log 1\kern0.5em +\frac{1}{6}\log 1+\frac{1}{6}\log 1+\frac{1}{6}\log \frac{3}{2} $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbl.png)

![$$ =\frac{1}{6}\log \frac{3}{4}+\frac{1}{6}x\ 0+\frac{1}{6}x\ 0+\frac{1}{6}x\ 0+\frac{1}{6}x0+\frac{1}{6}\log \frac{3}{2} $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbm.png)

![$$ =\frac{1}{6}\left(\log \frac{3}{4}+\log \frac{3}{2}\ \right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbn.png)

![$$ =\frac{1}{6}\log \left(\frac{3}{4}x\ \frac{3}{2}\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbo.png)

![$$ =\frac{1}{6}\log \left(\frac{9}{8}\right)=0.0196 $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbp.png)

你可以通过放 P = Q 得到*D*<sub>*KL*</sub>(*P*‖*Q*)= 0 来满足自己。当两个概率相等时，KL 散度为 0。对于其他任意两个不相等的概率分布，你会得到一个+ve KL 散度。分布越远，KL 散度值越高。有严格的数学证明表明，只有当两个分布相等时，KL 散度才总是+ve 和 0。

还要注意 KL 散度是不对称的。

![$$ {D}_{KL}\left(P\Big\Vert Q\right)\ne {D}_{KL}\left(Q\Big\Vert P\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbq.png)

KL 散度是概率空间中两个概率分布之间距离的一种伪测度。连续概率分布的 KL 散度公式如下所示:

![$$ {D}_{KL}\left(P\Big\Vert Q\right)=\int P(x)\log \frac{P(x)}{Q(x)} dx $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ36.png)

(7.35)

回到 TRPO，我们希望保持新旧政策在概率空间而不是参数空间上相互接近。这就等于说我们希望 KL-divergence 在每个更新步骤中都是有界的，以确保新旧策略不会偏离太远。

![$$ {D}_{KL}\left(\theta \Big\Vert {\theta}_k\right)\le \delta $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbr.png)

这里， *θ* <sub>*k*</sub> 为当前策略参数， *θ* 为更新后的策略参数。

现在让我们把注意力转向我们试图最大化的目标。我们之前的度量 *J* (θ)对新旧政策参数没有任何显式的依赖，分别说*θ*<sub>T5】k+1</sub>和 *θ* <sub>*k*</sub> 。有一个使用重要性抽样的政策目标的替代公式。我们将在没有数学推导的情况下陈述这一点，如下所示:

![$$ J\left(\uptheta, {\uptheta}_k\right)={E}_{a\sim {\uppi}_{\uptheta_k}\left(a|s\right)}\left[\frac{\uppi_{\uptheta}\left(a|s\right)}{\uppi_{\uptheta_k}\left(a|s\right)}{A}^{\uppi_{\uptheta_k}}\left(s,a\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ37.png)

(7.36)

这里，θ是修改/更新策略的参数， *θ* <sub>*k*</sub> 是旧策略的参数。我们正试图采取最大可能的步骤，从旧政策参数 *θ* <sub>*k*</sub> 到具有参数 *θ* 的修订政策，使得新老政策之间的 KL 差异不会相差太多。换句话说，找到一个最大限度地增加目标而不走出旧政策周围的信任区域的新政策，定义为*D*<sub>*KL*</sub>(*θ**θ*<sub>*k*</sub>)≤*δ*。用数学术语来说，我们可以将最大化问题总结如下:

![$$ {\uptheta}_{k+1}=\underset{\uptheta}{\arg\ \max }\ J\left(\uptheta, {\uptheta}_k\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbs.png)

![$$ s.t.{D}_{KL}\left(\uptheta \Big\Vert {\theta}_k\right)\le \updelta $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbt.png)

![$$ where,J\left(\uptheta, {\uptheta}_k\right)={E}_{a\sim {\uppi}_{\uptheta_k}\left(a|s\right)}\left[\frac{\uppi_{\uptheta}\left(a|s\right)}{\uppi_{\uptheta_k}\left(a|s\right)}{A}^{\uppi_{\uptheta_k}}\left(s,a\right)\right] $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ38.png)

(7.37)

优势![$$ {A}^{\uppi_{\uptheta_k}}\left(s,a\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq37.png)定义如前:

![$$ {A}^{\uppi_{\uptheta_k}}\left(s,a\right)={Q}^{\uppi_{\uptheta_k}}\left(s,a\right)-{V}^{\uppi_{\uptheta_k}}(s) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbu.png)

或者，当我们推出一个步骤，并且 v 由另一个具有参数 *ϕ* 的网络参数化时，下面是等式:

![$$ {A}^{\uppi_{\uptheta_k}}\left({s}_t,{a}_t\right)=r\left({s}_t,{a}_t\right)+{V}^{\uppi_{\uptheta_k}}\left({s}_{t+1};\upphi \right)-{V}^{\uppi_{\uptheta_k}}\left({s}_t;\upphi \right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbv.png)

这是 TRPO 目标最大化的理论表述。但是利用目标![$$ {\theta}_{k+1}=\underset{\uptheta}{\arg\ \max }\ J\left(\theta, {\theta}_k\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq38.png)的泰勒级数展开和 KL 约束*D*<sub>*KL*</sub>(*θ**θ*<sub>*k*</sub>)≤*δ*，再加上拉格朗日对偶利用凸优化，就可以得到一个近似的更新表达式。这种近似可以打破 KL-散度有界的保证，为此在更新规则中增加了回溯线搜索。最后，它涉及一个 *** nxn *** 矩阵的求逆，这不容易计算。在这种情况下，使用共轭梯度算法。此时，我们有了一个使用 TRPO 计算更新的实用算法。

我们不会深入这些推导的细节，也不会给出完整的算法。我们希望您了解基本设置。大多数时候，我们不会自己动手实现这些算法。

## 近似策略优化算法

近似策略优化(PPO)也是由与 TRPO 相同的问题驱动的。“我们如何在策略参数中采用最大可能的步长，而不会走得太远，导致比更新前的原始策略更差的策略？”

我们将要详述的 PPO-clip 变体没有 KL-divergence。它依赖于裁剪目标函数中的梯度，使得更新没有动机将策略移动得离原始步骤太远。PPO 实现起来更简单，并且经验表明其性能与 TRPO 一样好。详细信息见 2017 年题为“近似策略优化算法”的论文。 <sup>[3](#Fn3)</sup>

使用 PPO-clip 变体的目的如下:

![$$ J\left(\uptheta, {\uptheta}_k\right)=\mathit{\min}\left(\frac{\uppi_{\uptheta}\left(a|s\right)}{\uppi_{\uptheta_k}\left(a|s\right)}{A}^{\uppi_{\uptheta_k}}\left(s,a\right),g\left(\upepsilon, {A}^{\uppi_{\uptheta_k}}\left(s,a\right)\right)\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbw.png)

其中:

![$$ g\left(\upepsilon, A\right)=\kern0.5em \left\{\begin{array}{c}\left(1+\epsilon \right)A,A\ge 0\\ {}\left(1-\epsilon \right)A,A&lt;0\end{array}\right. $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equ39.png)

(7.38)

我们改写一下 *J* ( *θ* ， *θ* <sub>*k*</sub> )，优势 A 为+ve 时，如下图:

![$$ J\left(\uptheta, {\uptheta}_k\right)=\mathit{\min}\left(\frac{\uppi_{\uptheta}\left(a|s\right)}{\uppi_{\uptheta_k}\left(a|s\right)},\left(1+\upepsilon \right)\right){A}^{\uppi_{\uptheta_k}}\left(s,a\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equbx.png)

当优势为+ve 时，我们要更新参数，使新策略*π*<sub>T3】θT5】(*a*|*s*)高于旧策略![$ {\pi}_{\theta_k}\left(a|s\right) $](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_IEq39.png)。但是，我们没有将其增加太多，而是剪切梯度以确保新策略的增加在旧策略的 1+ε倍以内。</sub>

类似地，当优势为-ve 时，我们得到以下结果:

![$$ J\left(\theta, {\theta}_k\right)=\mathit{\min}\left(\frac{\pi_{\theta}\left(a|s\right)}{\pi_{\theta_k}\left(a|s\right)},\left(1-\epsilon \right)\right){A}^{\pi_{\theta_k}}\left(s,a\right) $$](../images/502835_1_En_7_Chapter/502835_1_En_7_Chapter_TeX_Equby.png)

换句话说，当优势为-ve 时，我们希望更新参数，从而降低( *s* ， *a* )对的策略概率。然而，我们不是一直减小，而是剪切梯度，使得新策略概率不会下降到旧策略概率的(1-ε)倍以下。

换句话说，我们剪切梯度以确保策略更新使策略概率分布在旧概率分布的(1-ε)到(1+ε)倍之内。ϵ作为一个正则化。与 TRPO 相比，PPO 很容易实现。对于 A2C，我们可以遵循图 [7-7](#Fig7) 中给出的相同伪代码，仅做一处修改，将图 [7-7](#Fig7) 中的目标 J(θ)与( [7.38](#Equ39) 中给出的目标互换。

这一次，我们将使用一个库，而不是自己编码。OpenAI 有一个库叫做 Baselines ( [`https://github.com/openai/baselines`](https://github.com/openai/baselines) )。它实现了许多流行和最新的算法。还有另一个基于基线的库，叫做稳定基线 3。你可以在 [`https://stable-baselines3.readthedocs.io/en/master/`](https://stable-baselines3.readthedocs.io/en/master/) 了解更多。

我们的代码将遵循与以前相同的模式，只是我们不会显式地定义策略网络。我们也不会自己写计算损失和通过梯度的训练步骤。您可以在`listing7_3_ppo_baselines3.ipynb`中找到使用 PPO 训练并记录`CartPole`训练表现的完整代码。我们现在将浏览创建代理、在`CartPole`上训练它并评估性能的代码片段，如清单 [7-8](#PC8) 所示。

```
from stable_baselines3 import PPO
from stable_baselines3.ppo.policies import MlpPolicy
from stable_baselines3.common.evaluation import evaluate_policy

#create enviroment
env_name = 'CartPole-v1'
env = gym.make(env_name)

# build model
model = PPO(MlpPolicy, env, verbose=0)

# Train the agent for 30000 steps
model.learn(total_timesteps=30000)

# Evaluate the trained agent
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)
print(f"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}")

Listing 7-8PPO Agent for CartPole Using a Stable Baselines 3 Implementation

```

就是这样。用 PPO 训练代理只需要几行代码。Python 笔记本包含额外的代码，用于记录受训代理的表现并播放视频。我们不打算讨论它的代码细节。建议感兴趣的读者使用之前的链接深入研究 OpenAI 基线和稳定基线。

我们也想利用这个机会再次强调了解流行的 RL 库的重要性。在浏览本书中各种算法的实现时，您应该注意到，熟悉流行的 RL 实现并学会使用它们来满足您的特定需求同样重要。本书附带的代码有助于您更好地理解这些概念。它绝不是生产代码。像 Baselines 这样的库拥有高度优化的代码，能够利用 GPU 和多核并行运行许多代理。

## 摘要

本章向您介绍了另一种方法，即直接学习策略，而不是先学习状态/动作值，然后使用它们来找到最佳策略。

我们研究了 REINFROCE 的推导，这是最基本的政策梯度方法。在最初的推导之后，我们研究了一些减少方差的技术，比如奖励和基线的使用。

这让我们看到了演员-评论家家族，在这个家族中，我们结合了基于价值的方法来学习使用强化作为基线的状态值。政策网络(actor)与国家价值网络(critical)使我们能够结合基于价值的方法和政策梯度方法的优点。我们简要介绍了异步版本 A3C。

最后，我们看了一些高级策略优化技术，比如信任区域策略优化(TRPO)和邻近策略优化(PPO)。我们讨论了使用这两种技术的主要动机和方法。我们还看到了使用库来训练使用 PPO 的代理。

<aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

[T2`https://arxiv.org/pdf/1602.01783.pdf`](https://arxiv.org/pdf/1602.01783.pdf)

  [2](#Fn2_source)

[T2`https://arxiv.org/pdf/1502.05477.pdf`](https://arxiv.org/pdf/1502.05477.pdf)

  [3](#Fn3_source)

[T2`https://arxiv.org/pdf/1707.06347.pdf`](https://arxiv.org/pdf/1707.06347.pdf)

 </aside>****