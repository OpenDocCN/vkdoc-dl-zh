# 六、前方的路

本快速入门指南旨在让您以最快且最有效的方式熟悉使用 Keras 的 DL 领域。我希望你在这次旅行中过得愉快。在这最后一章中，我们将简要地看一下前方的道路。我们将尝试回答以下问题:对于一名数据科学家来说，在 DL 旅程中取得成功还有哪些重要的额外主题？

让我们开始吧。

## DL 专家的下一步是什么？

我们已经用分类和回归的 DNNs 介绍了 DL 中的基础知识。最有趣的部分，事实上也是 DL 在 2012 年获得人气和发展势头的主要原因，是计算机视觉领域的 DL。几年前，设计一种算法来帮助计算机理解图像几乎是不可能的。使用算法从图像中提取含义或将图像归类到特定类别的想法是不可想象的。随着时间的推移，ML 变得流行起来，在图像中使用手工制作的特征，然后使用分类器来训练算法的方法展示了改进的结果，但这不是我们想要的结果。2012 年，Alexnet(由 Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 开发的架构)被用于参加“ImageNet 大规模视觉识别挑战赛”。这是一场开发算法的比赛，该算法可以学习和预测如何将图像分类到一组定义的类别中。Alexnet 取得了 15.3%的前五名误差；这比之前的最好成绩低了近 11%，创下了挑战赛的历史纪录。该建筑是一种 DNN 建筑，专门用于图像分类。就在那时，DL 受到了关注，并立即成为研究的热门话题。DL 的旅程从此一飞冲天。随着对 DL 的更多研究和实验，这个领域扩展到了视频、音频、文本和几乎任何形式的数据。如今，DL 无处不在。几乎每一个主要的技术公司都在它的整个产品堆栈中采用了 DL。

作为一个 DL 爱好者，探索高级 DL 主题的一小步是首先从计算机视觉的 DL 开始。这是您将探索卷积神经网络(CNN)的地方。

### 美国有线新闻网；卷积神经网络

CNN 是一类用于计算机视觉用例的 DL 算法，例如对图像或视频进行分类，并检测图像中的对象，甚至图像中的区域。CNN 算法是计算机视觉领域的一个巨大突破，因为与当时其他流行的技术相比，它只需要最少的图像处理，并且表现得非常好。CNN 对图像分类的性能改进是惊人的。构建 CNN 的过程在 Keras 中也得到了简化，所有的逻辑组件都被巧妙地抽象出来。Keras 提供了 CNN 层，开发 CNN 模型的整体过程与我们在开发回归和分类模型时所学的非常相似。

为了简单地理解这个过程，我们将使用一个小例子来说明它的实现。以下代码片段展示了 CNN 的“hello world”等效实现。我们将使用 MNIST 数据(即带有手写数字的图像集合)。目标是将图像分类为[0，1，2，3，4，5，6，7，8，9]中的一个数字。Keras 数据集模块中已经提供了这些数据。尽管这个主题是全新的，但是代码片段中的注释将为您提供模型设计的基本概念。

```py
#Importing the necessary packages
import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
#Importing the CNN related layers as described in Chapter 2
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.utils import np_utils

#Loading data from Keras datasets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

#Defining the height and weight and number of samples
#Each Image is a 28 x 28 with 1 channel matrix
training_samples, height, width = x_train.shape
testing_samples,_,_ = x_test.shape

print("Training Samples:",training_samples)
print("Testing Samples:",testing_samples)
print("Height: "+str(height)+" x Width:"+ str(width))

```

**输出**

```py
Training Samples: 60000
Testing Samples: 10000
Height: 28 x Width:28

```

该法典继续规定:

```py
#Lets have a look at a sample image in the training data
plt.imshow(x_train[0],cmap='gray', interpolation="none")

#We now have to engineer the image data into the right form
#For CNN, we would need the data in Height x Width X Channels form Since the image is in grayscale, we will use channel = 1
channel =1
x_train = x_train.reshape(training_samples, height, width,channel).astype('float32')
x_test = x_test.reshape(testing_samples, height, width, channel).astype('float32')

#To improve the training process, we would need to standardize or normalize the values We can achieve this using a simple
divide by 256 for all values
x_train = x_train/255
x_test =x_test/255

#Total number of digits  =10
target_classes = 10

# numbers 0-9, so ten classes
n_classes = 10

# convert integer labels into one-hot vectors
y_train = np_utils.to_categorical(y_train, n_classes)
y_test = np_utils.to_categorical(y_test, n_classes)

#Designing the CNN Model
model = Sequential()
model.add(Conv2D(64, (5, 5), input_shape=(height,width ,1), activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, (3, 3), activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation="relu"))
model.add(Dense(n_classes, activation="softmax"))

# Compile model
model.compile(loss='categorical_crossentropy', optimizer="adam", metrics=['accuracy'])

# Fit the model
model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200)

```

**输出**

```py
Train on 60000 samples, validate on 10000 samples
Epoch 1/10
60000/60000 [==============================] - 61s 1ms/step - loss: 0.2452 - acc: 0.9266 - val_loss: 0.0627 - val_acc: 0.9806
Epoch 2/10
60000/60000 [==============================] - 64s 1ms/step - loss: 0.0651 - acc: 0.9804 - val_loss: 0.0414 - val_acc: 0.9860
Epoch 3/10
60000/60000 [==============================] - 62s 1ms/step - loss: 0.0457 - acc: 0.9858 - val_loss: 0.0274 - val_acc: 0.9912

 --- Skipping intermediate output----

Epoch 9/10
60000/60000 [==============================] - 58s 963us/step - loss: 0.0172 - acc: 0.9943 - val_loss: 0.0284 - val_acc: 0.9904
Epoch 10/10
60000/60000 [==============================] - 56s 930us/step - loss: 0.0149 - acc: 0.9949 - val_loss: 0.0204 - val_acc: 0.9936

```

最后，让我们评估模型性能:

```py
metrics = model.evaluate(x_test, y_test, verbose=0)
for i in range(0,len(model.metrics_names)):
    print(str(model.metrics_names[i])+" = "+str(metrics[i]))

```

**输出**

```py
loss = 0.02039033946258933
acc = 0.9936

```

我们可以看到，我们在测试数据集上的总体准确率约为 99%。这是一个相当简单的例子。当图像的大小和要预测的类的数量增加时，复杂性就出现了。

为了对 CNN 的工作有一个高层次的理解，你可以参考一些有趣的博客:

*   [`https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/`](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)

*   [`https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050`](https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050)

为了进行更多的实验并研究一些非常酷且简单易懂的例子，您可以查看一些流行的 git 库，以了解与 CNN 相关的用例。

以下是一些例子:

*   [`https://github.com/pranoyr/image-classification`](https://github.com/pranoyr/image-classification)

*   [`https://github.com/lrogar/distracted-driver-detection`](https://github.com/lrogar/distracted-driver-detection)

### RNN

在探索了 CNN 之后，DL 的下一步是开始探索 RNN，俗称“序列模型”这个名字变得流行是因为 RNN 利用了顺序信息。到目前为止，我们研究的所有 dnn 都在假设任何两个训练样本之间没有关系的情况下处理训练数据。然而，这是我们可以使用数据解决的许多问题中的一个问题。考虑你的 iOS 或 Android 手机中的预测文本功能；下一个单词的预测高度依赖于您已经键入的最后几个单词。这就是顺序模型发挥作用的地方。RNNs 也可以理解为有记忆的神经网络。它将一个层连接到自身，从而可以同时访问两个或多个连续的输入样本，以处理最终输出。这一特性是 RNN 独有的，随着其研究的兴起，它在自然语言理解领域取得了惊人的成功。所有遗留的自然语言处理技术现在都可以用 RNNs 得到显著的改进。聊天机器人的兴起，短信中改进的自动更正，电子邮件客户端和其他应用程序中的建议回复，以及机器翻译(即，将文本从源语言翻译成目标语言，谷歌翻译是典型的例子)都是随着 RNN 的采用而推动的。还有不同类型的 LSTM(长短期记忆)网络，它们克服了现有 RNN 架构中的限制，并将自然语言处理相关任务的性能提升了一个档次。RNN 最流行的版本是 LSTM 和 GRU(门控循环单元)网络。

与我们为 CNN 所做的类似，我们将看一下 RNN/LSTM 网络的一个简单(hello world 等效)示例实现。以下代码片段对 Keras 中的 IMDB reviews 数据集执行二进制分类。这是一个用例，其中我们提供了用户评论(文本日期)和相关的积极或消极的结果。

```py
#Import the necessary packages
from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Dense, LSTM
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence

#Setting a max cap for the number of distinct words
top_words = 5000
#Loading the training and test data from keras datasets
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)

#Since the length of each text will be varying
#We will pad the sequences (i.e. text) to get a uniform length throughout
max_text_length = 500
x_train = sequence.pad_sequences(x_train, maxlen=max_text_length)
x_test = sequence.pad_sequences(x_test, maxlen=max_text_length)

#Design the network
embedding_length = 32
model = Sequential()
model.add(Embedding(top_words, embedding_length, input_length=max_text_length))
model.add(LSTM(100))
model.add(Dense(1, activation="sigmoid"))

#Compile the  model
model.compile(loss='binary_crossentropy', optimizer="adam", metrics=['accuracy'])

#Fit the model
model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=64)

```

**输出**

```py
Train on 25000 samples, validate on 25000 samples

Epoch 1/3
25000/25000 [==============================] - 222s 9ms/step - loss: 0.5108 - acc: 0.7601 - val_loss: 0.3946 - val_acc: 0.8272
Epoch 2/3
25000/25000 [==============================] - 217s 9ms/step - loss: 0.3241 - acc: 0.8707 - val_loss: 0.3489 - val_acc: 0.8517
Epoch 3/3
25000/25000 [==============================] - 214s 9ms/step - loss: 0.3044 - acc: 0.8730 - val_loss: 0.5213 - val_acc: 0.7358

```

评估测试数据集的准确性:

```py
scores = model.evaluate(x_test, y_test, verbose=0)
print("Accuracy:",scores[1])

```

**输出**

```py
Accuracy: 0.73584

```

准确度随着用于训练和改进的架构的时期数量的增加而提高。要全面了解 RNN 是如何运作的，你可以浏览几个博客:

*   [`https://colah.github.io/posts/2015-08-Understanding-LSTMs/`](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

*   [`https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714`](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714)

*   [` towardsdatascien ce。com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explain-44e 9 EB 85 BF 21`](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)

为了进行更多的实验和研究一些非常酷的例子，您可以查看一些流行的 git 库，以了解与 LSTM 相关的用例。以下是一些例子:

*   [`https://github.com/philiparvidsson/LSTM-Text-Generation`](https://github.com/philiparvidsson/LSTM-Text-Generation)

*   [`https://github.com/danielefranceschi/lstm-climatological-time-series`](https://github.com/danielefranceschi/lstm-climatological-time-series)

*   [`https://github.com/shashankbhatt/Keras-LSTM-Sentiment-Classification`](https://github.com/shashankbhatt/Keras-LSTM-Sentiment-Classification)

### CNN + RNN

另一个有趣的探索领域是有线电视新闻网和 RNN 的交集。听起来很困惑？想象一下，你可以结合 CNN(即理解图像)和 RNN(即理解自然文本)的力量；交集或组合会是什么样子？你可以用文字描述一幅画。没错，通过将 RNN 和 CNN 结合在一起，我们可以帮助计算机用自然风格的文本描述图像。这个过程被称为图像字幕。今天，如果你在 google.com 搜索类似“黄色汽车”的查询，你的结果实际上会返回一吨黄色汽车。如果你认为这些图片的说明是由人类完成的，然后可以被搜索引擎索引，那你就大错特错了。对于人类来说，我们无法将为图像添加字幕的过程扩展到每天数十亿张图像。这个过程是不可行的。你需要一个更聪明的方法来做到这一点。CNN+RNN 的图像字幕不仅在搜索引擎的图像搜索方面带来了突破，而且在我们日常生活中使用的其他一些产品上也带来了突破。RNN 和 CNN 的交集带给人类的最重要和最具革命性的成果是智能眼镜(百度称之为 duLight):一种配备在老花镜上的摄像头，可以描述周围的环境。对于视力受损的人来说，这是一个很棒的产品。今天，我们有一个较小的版本，在几个应用程序中实现，可以安装在手机上，并与手机摄像头配合工作。如果你有兴趣阅读更多，你可以浏览以下博客:

*   [`https://towardsdatascience.com/image-captioning-in-deep-learning-9cd23fb4d8d2`](https://towardsdatascience.com/image-captioning-in-deep-learning-9cd23fb4d8d2)

*   [`https://machinelearningmastery.com/introduction-neural-machine-translation/`](https://machinelearningmastery.com/introduction-neural-machine-translation/)

*   [`https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd`](https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd)

展示图像标题的例子超出了本书的范围。但是，这里有几个 github 存储库，您可以开始探索:

*   [`https://github.com/yashk2810/Image-Captioning`](https://github.com/yashk2810/Image-Captioning)

*   [`https://github.com/danieljl/keras-image-captioning`](https://github.com/danieljl/keras-image-captioning)

## DL 为什么需要 GPU？

在探索第二章设置的环境时，我们偶然发现为 GPU 安装了 TensorFlow。我相信你已经听说了很多关于 GPU 用于 DL 和 NVIDIA 等公司推出专门为 DL 设计的 GPU。一般来说，任何人首先会问的问题是 GPU 与 DL 有什么关系。我们将尝试立即获得这个问题和其他问题的答案。

鉴于您在本指南中对 DL 的了解，我假设您已经意识到 DL 是计算密集型的。为特定任务训练模型确实需要大量的 CPU 能力和时间。如果你更深入一步，并试图理解在一个 DL 模型的训练过程中实际发生了什么，它将归结为一个简单的任务(即矩阵乘法)。你有张量形式的输入数据(比如三维矩阵)，测试数据也有类似的形式，神经元连接的权重也以矩阵形式存储，事实上，任何形式的 DNN 的一切，比如 CNN、RNN、DNN 或所有这些的组合，在内部很大程度上都表示为不同维度的矩阵。具有反向传播的学习过程也通过矩阵乘法来执行。

有趣的是，矩阵乘法很大一部分可以并行处理。因此，为了加快训练过程，您的 CPU 中的核心数量可以进一步提高模型所需的训练时间。不幸的是，虽然 CPU 实现的并行处理水平很高，但还不是最好的。特别是对于大型矩阵乘法，这个过程并不像我们希望的那样有效。

但是，我们有市场上已经有的 GPU。使用 GPU 的主要目的是为了增强视频性能(即更高的屏幕刷新率)。一般来说，你的笔记本电脑或计算机的屏幕是一个确定大小的图像，比如 1920 × 1080 像素。这个图像也是一个大小为 1920 × 1080 × 3 的三维矩阵。第三维表示颜色通道“RGB”。因此，简而言之，你在屏幕上看到的任何时间点都是使用 1920 × 1080 × 3 矩阵显示的图像。当这个矩阵每秒刷新(计算)30 次时，它就变成了一个平滑的视频，你可以看到物体没有延迟地移动。因此，为了在屏幕上显示一秒钟，计算机内部计算 1920 × 1080 × 3 矩阵的值至少 30 次。那是相当多的计算。此外，当您玩游戏或执行任何需要高端图形的任务(如视频编辑或在 Photoshop 中设计图像等任务)时，刷新率需要大幅提高。一个好的估计是每秒 60 的屏幕刷新率，而不是每秒 30。现在，为了显示如此高的图形内容，CPU 上有不寻常的额外负载，它可能无法提供所需的性能。为了解决这个问题，我们有专门设计用于渲染高端图形的 GPU，帮助计算机处理刷新屏幕 60 次所需的计算。GPU 承担处理屏幕刷新计算的全部责任。这种处理是通过大规模并行处理完成的。我们在普通笔记本电脑中使用的现代 GPU 至少有 400 个内核，而台式机上的 GPU 要强大得多。这些内核有助于大规模并行处理，以高刷新率显示高清图形内容。

碰巧的是，同样的技术可以用来解决我们在数字图书馆面临的问题。对矩阵进行大规模并行处理以在屏幕上呈现平滑的图形内容可以替代地用于处理 DL 模型训练过程中的计算。在那一刻之后，NVIDIA 开发了 CUDA，这是一个为 GPU 创建的并行处理接口模型。它允许开发人员使用支持 CUDA 的图形处理单元进行通用处理。这项技术为训练 DL 模型带来了巨大的突破。用数字来描述，一个模型在我的笔记本电脑上用 CPU 训练了 40 分钟，用 GPU 训练了 2 分钟。几乎快了 20 倍。您可以想象使用更强大的 GPU 我们可以实现什么。今天，大多数 DL 库都支持 GPU。一旦您为您的 GPU 安装并设置了 CUDA 驱动程序，并安装了一个 GPU 兼容的 DL 库，您就一切就绪了。剩下的就完全抽象给你了。你所需要做的就是以通常的方式训练模型，框架会无缝地使用来自 GPU 和 CPU 的资源。

同样的过程也可以用其他厂商的 GPU 实现，比如 AMD 用 OpenGL。但 NVIDAs GPUs 要优越得多，至少领先其他任何竞争对手五年。如果你打算投资硬件来研究 DL，我强烈建议你购买一台配备兼容 NVIDIA CUDA 的 GPU 的笔记本电脑或台式机(首选)。你将在实验中节省大量的时间。

## DL (GAN)中的其他热点区域

我们为您探索了掌握高级 DL 主题的前进道路。但是如果不讨论 DL 中最热门的研究领域，这个讨论将是不完整的。我们将简要讨论生成性对抗网络(GANs ),尽管还有更多。

gan 处于 DL 中断的最前沿，最近一直是一个活跃的研究课题。简而言之，GAN 允许网络从代表现实世界实体(比如，猫或狗；当我们简单地开发 DL 模型以在猫和狗之间进行分类，然后使用它在该过程中学习到的相同特征来生成新图像时；也就是说，它可以生成一个看起来(几乎)真实的猫的新图像，并且与您为训练提供的图像集完全不同。我们可以将 GAN 的整个解释简化为一个简单的任务(即图像生成)。如果训练时间和训练时提供的样本图像足够大，它可以学习一个网络，该网络可以生成与训练时提供的图像不相同的新图像；它会生成新的图像。

如果你想知道图像生成的应用，有一个直到最近才被想到的全新的可能性。之前，大多数 DL 模型只有推理(相对容易)和勉强生成(非常难)。如果你看一看*蒙娜丽莎*，很容易把它归类为一幅女人的画，但是要做出一幅真的很难。然而，如果有可能这样做，那么就可以开发出全新一代的应用程序。给你一个很好的例子，印度在线时尚零售商 Myntra 使用 GAN 来创建新的 t 恤设计。它用一系列 t 恤设计训练 GAN 网络，然后该模型生成新的设计。在系统生成的 100 个新设计中，即使有 50 个被认为是他们可以制造的好设计，这个领域的奇迹也将是无穷无尽的。同样的想法可以扩展到任何其他领域。在上一节中，我们讨论了图像字幕(即从图像中生成类似描述的自然文本)。这已经是一个很酷的应用程序了，现在想想相反的情况；想象一下，向一个系统提供一个自然的文本描述，然后它生成一张图片作为回报。这个想法听起来太超前了，但是我们已经非常接近这种可能性了。想象一下，你在路上看到一个罪犯，警察需要你帮忙画出他的脸，以便进一步调查；对于未来的 GAN 系统，我们可以想象一个系统，你描述罪犯面部的细节，系统为你绘制面部草图。GAN 的应用过于超前，但研究仍在进行中。到目前为止，研究人员设计的 GAN 网络能够以高清晰度呈现/生成图像，并且在该领域中有持续的实验和研究来开发能够生成高清晰度视频的 GAN 网络。

您可以在此阅读更多关于 GAN 及其应用的信息:

*   [`https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39`](https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39)

*   [`https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f`](https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f)

*   [`https://towardsdatascience.com/understanding-generative-adversarial-networks-4dafc963f2ef`](https://towardsdatascience.com/understanding-generative-adversarial-networks-4dafc963f2ef)

## 总结想法

这一章的目的是强调数字图书馆领域是多么有前途，现在是开始学习它的基础的好时机。我希望您现在对该领域的高级主题以及您可以立即采取的进一步探索 DL 前沿的下一步有一个公平的想法。这本书旨在以最快但最有效的方式帮助您入门，作为使用 DNNs 的现代 DL 的入门指南。

我们从 DL 主题的简单介绍开始了本指南，并理解了它的基本原理和与市场上流行词汇的区别。我们研究了使用框架开发 DL 模型的必要性，探索了当今市场上几种流行的选择，并理解了为什么 Keras 最有可能成为初学者的首选框架。在后面的章节中，我们通过研究 Keras 框架提供的逻辑抽象和在 DL 生态系统中以小而渐进的步骤映射它的对等物，探索了 Keras 框架，然后将所有的知识与分类和回归中的两个以业务为中心的基本用例结合在一起。然后，我们研究了设计网络的技巧和诀窍，在难以入门的情况下的一些变通方法，以及通过正则化和超参数优化进行模型调整的过程。我们还研究了在生产中部署 DL 模型时应该遵守的一些准则，并最终通过 CNN、、CNN+RNN 以及 DL 中最热门的研究领域(即 GAN)对 DL 中的高级产品进行了初步了解。

我非常享受以加速模式交付本指南内容的过程，我希望你也喜欢这个旅程。现在是结束的时候了，祝你们在 DL 的旅程中好运。我希望你们在发展 DL 技能的过程中有一个非常快乐和愉快的学习之路。