# 7\. TensorFlow 中的 CNN

本章将演示如何使用 TensorFlow 建立一个 CNN 模型。CNN 模型可以帮助你建立一个可以预测/分类图像的图像分类器。一般来说，您在模型架构中创建一些层，这些层具有权重和偏差的初始值。然后，您可以在训练数据集的帮助下调整权重和偏差。还有另一种方法，包括使用预训练模型(如 InceptionV3)对图像进行分类。您可以使用这种迁移学习方法，在预训练模型(参数值保持不变)的层之上添加一些层(其参数已被训练),以制作非常强大的分类器。

在这一章中，我将使用 TensorFlow 展示如何为各种计算机视觉应用开发卷积网络。将 CNN 架构表示为数据流图更容易。

## CNN 模型为什么要 TensorFlow？

在 TensorFlow 中，图像可以表示为三维数组或形状的张量(高度、宽度和通道)。TensorFlow 提供了快速迭代的灵活性，允许您更快地训练模型，并使您能够运行更多的实验。将 TensorFlow 模型投入生产时，可以在大规模 GPU 和 TPU 上运行。

## 用于为 MNIST 数据构建图像分类器的张量流代码

在本节中，我将带您通过一个示例来了解如何在 TensorFlow 中实现 CNN。

以下代码从 TensorFlow contrib 包中导入包含数字的 28×28 灰度图像的 MNIST 数据集，并加载所有必需的库。这里，目标是建立分类器来预测图像中给定的数字。

```
from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets
from tensorflow.python.framework import ops
import tensorflow as tf
import numpy as np

```

然后启动图形会话。

```
# Start a graph session
sess = tf.Session()

```

您加载 MNIST 数据并创建训练集和测试集。

```
# Load data
from keras.datasets import mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

```

然后规范化训练和测试集特征。

```
# Z- score  or Gaussian Normalization
X_train = X_train - np.mean(X_train) / X_train.std()
X_test = X_test - np.mean(X_test) / X_test.std()

```

由于这是一个多类分类问题，因此最好使用输出类值的一次性编码。

```
# Convert labels into one-hot encoded vectors
num_class = 10
train_labels = tf.one_hot(y_train, num_class)
test_labels = tf.one_hot(y_test, num_class)

```

现在让我们设置模型参数，因为这些图像是灰度的。因此，图像(通道)的深度是 1。

```
# Set model parameters
batch_size = 784
samples =500
learning_rate = 0.03
img_width = X_train[0].shape[0]
img_height = X_train[0].shape[1]
target_size = max(train_labels) + 1
num_channels = 1 # greyscale = 1 channel
epoch = 200
no_channels = 1
conv1_features = 30
filt1_features = 5
conv2_features = 15
filt2_features = 3
max_pool_size1 = 2 # NxN window for 1st max pool layer
max_pool_size2 = 2 # NxN window for 2nd max pool layer
fully_connected_size1 = 150

```

让我们声明模型的占位符。可以为训练集和评估集更改输入数据特征、目标变量和批量大小。

```
# Declare model placeholders
x_input_shape = (batch_size, img_width, img_height, num_channels)
x_input = tf.placeholder(tf.float32, shape=x_input_shape)
y_target = tf.placeholder(tf.int32, shape=(batch_size))
eval_input_shape = (samples, img_width, img_height, num_channels)
eval_input = tf.placeholder(tf.float32, shape=eval_input_shape)
eval_target = tf.placeholder(tf.int32, shape=(samples))

```

让我们为输入层和隐藏层的神经元声明模型变量的权重和偏差值。

```
# Declare model variables
W1 = tf.Variable(tf.random_normal([filt1_features, filt1_features, no_channels, conv1_features]))
b1 = tf.Variable(tf.ones([conv1_features]))
W2 = tf.Variable(tf.random_normal([filt2_features, filt2_features, conv1_features, conv2_features]))
b2 = tf.Variable(tf.ones([conv2_features]))

```

让我们为完全连接的层声明模型变量，并为最后两层定义权重和偏差。

```
# Declare model variables for fully connected layers
resulting_width = img_width // (max_pool_size1 * max_pool_size2)
resulting_height = img_height // (max_pool_size1 * max_pool_size2)
full1_input_size = resulting_width * resulting_height * conv2_features
W3 = tf.Variable(tf.truncated_normal([full1_input_size, fully_connected_size1], stddev=0.1, dtype=tf.float32))
b3 = tf.Variable(tf.truncated_normal([fully_connected_size1], stddev=0.1, dtype=tf.float32))
W_out = tf.Variable(tf.truncated_normal([fully_connected_size1, target_size], stddev=0.1, dtype=tf.float32))
b_out = tf.Variable(tf.truncated_normal([target_size], stddev=0.1, dtype=tf.float32))

```

让我们创建一个助手函数来定义卷积层和最大池层。

```
# Define helper functions for the convolution and maxpool layers:
def conv_layer(x, W, b):
    conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding="SAME")
    conv_with_b = tf.nn.bias_add(conv, b)
    conv_out = tf.nn.relu(conv_with_b)
    return conv_out
def maxpool_layer(conv, k=2):
    return tf.nn.max_pool(conv, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding="SAME")

```

神经网络模型由两个隐藏层和两个完全连接的层定义。整流线性单元被用作隐藏层和最终输出层的激活函数。

```
# Initialize Model Operations

def my_conv_net(input_data):
    # First Conv-ReLU-MaxPool Layer
    conv_out1 = conv_layer(input_data, W1, b1)
    maxpool_out1 = maxpool_layer(conv_out1)

    # Second Conv-ReLU-MaxPool Layer
    conv_out2 = conv_layer(maxpool_out1, W2, b2)
    maxpool_out2 = maxpool_layer(conv_out2)

    # Transform Output into a 1xN layer for next fully connected layer
    final_conv_shape = maxpool_out2.get_shape().as_list()
    final_shape = final_conv_shape[1] * final_conv_shape[2] * final_conv_shape[3]
    flat_output = tf.reshape(maxpool_out2, [final_conv_shape[0], final_shape])

    # First Fully Connected Layer
    fully_connected1 = tf.nn.relu(tf.add(tf.matmul(flat_output, W3), b3))
    # Second Fully Connected Layer
    final_model_output = tf.add(tf.matmul(fully_connected1, W_out), b_out)

    return(final_model_output)

model_output = my_conv_net(x_input)
test_model_output = my_conv_net(eval_input)

```

您将使用 softmax 交叉熵函数(更适用于多类分类)来定义作用于逻辑的损失。

```
# Declare Loss Function

(softmax cross entropy)
loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=y_target))

```

让我们定义训练集和测试集的预测函数。

```
# Create a prediction function
prediction = tf.nn.softmax(model_output)
test_prediction = tf.nn.softmax(test_model_output)

```

为了确定每一批的模型精度，让我们定义精度函数。

```
# Create accuracy function
def get_accuracy(logits, targets):
    batch_predictions = np.argmax(logits, axis=1)
    num_correct = np.sum(np.equal(batch_predictions, targets))
    return(100\. * num_correct/batch_predictions.shape[0])

```

让我们声明训练步骤并定义优化器函数。

```
# Create an optimizer
my_optimizer = tf.train.AdamOptimizer(learning_rate, 0.9)
train_step = my_optimizer.minimize(loss)

```

让我们初始化前面声明的所有模型变量。

```
# Initialize Variables
varInit = tf.global_variables_initializer()
sess.run(varInit)

```

让我们开始训练模型，并随机循环处理这些数据。您希望在训练和测试集批次上评估模型，并记录损失和准确性。

```
# Start training loop

train_loss = []
train_acc = []
test_acc = []
for i in range(epoch):
    random_index = np.random.choice(len(X_train), size=batch_size)
    random_x = X_train[random_index]
    random_x = np.expand_dims(random_x, 3)
    random_y = train_labels[random_index]   

    train_dict = {x_input: random_x, y_target: random_y}

    sess.run(train_step, feed_dict=train_dict)
    temp_train_loss, temp_train_preds = sess.run([loss, prediction], feed_dict=train_dict)
    temp_train_acc = get_accuracy(temp_train_preds, random_y)

    eval_index = np.random.choice(len(X_test), size=evaluation_size)
    eval_x = X_test[eval_index]
    eval_x = np.expand_dims(eval_x, 3)
    eval_y = test_labels[eval_index]
    test_dict = {eval_input: eval_x, eval_target: eval_y}
    test_preds = sess.run(test_prediction, feed_dict=test_dict)
    temp_test_acc = get_accuracy(test_preds, eval_y)

```

模型的结果以下列格式记录并打印在输出中:

```
# Record and print results

train_loss.append(temp_train_loss)
train_acc.append(temp_train_acc)
test_acc.append(temp_test_acc)
print('Epoch # {}. Train Loss: {:.2f}. Train Acc : {:.2f} . temp_test_acc : {:.2f}'.format(i+1,temp_train_loss,temp_train_acc,temp_test_acc))

```

## 使用高级 API 构建 CNN 模型

TFLearn、TensorLayer、tflayers、TF-Slim、tf.contrib.learn、Pretty Tensor、keras 和 Sonnet 都是高级 TensorFlow APIs。如果您使用这些高级 API 中的任何一个，您都可以用几行代码构建 CNN 模型。因此，您可以探索这些 API 中的任何一个来更好地工作。