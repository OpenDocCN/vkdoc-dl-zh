# 1.张量流基础

本章涵盖深度学习框架 TensorFlow 的基础知识。深度学习在模式识别方面做得非常好，特别是在图像、声音、语音、语言和时序数据的背景下。在深度学习的帮助下，你可以分类、预测、聚类和提取特征。好在 2015 年 11 月，谷歌发布了 TensorFlow，已经在谷歌搜索、垃圾邮件检测、语音识别、谷歌助手、Google Now、Google Photos 等谷歌大部分产品中使用。本章的目的是解释张量流的基本组成部分。

TensorFlow 具有执行部分子图计算的独特能力，从而允许在划分神经网络的帮助下进行分布式训练。换句话说，TensorFlow 允许模型并行和数据并行。TensorFlow 提供了多个 API。最低级别的 API—tensor flow Core—为您提供完整的编程控制。

关于张量流，请注意以下要点:

*   它的图表是对计算的描述。
*   它的图有节点是操作。
*   它在给定的会话上下文中执行计算。
*   对于任何计算，必须在会话中启动图形。
*   会话将图形操作放到 CPU 和 GPU 等设备上。
*   会话提供了执行图形操作的方法。

请前往 [`https://www.tensorflow.org/install/`](https://www.tensorflow.org/install/) 进行安装。

我将讨论以下主题:

![A456157_1_En_1_Figa_HTML.jpg](A456157_1_En_1_Figa_HTML.jpg)

## 张量

在进入 TensorFlow 库之前，让我们先熟悉一下 TensorFlow 中的基本数据单位。张量是一个数学对象，是标量、向量和矩阵的概括。张量可以表示为多维数组。零阶张量只不过是一个标量。向量/数组是秩为 1 的张量，而矩阵是秩为 2 的张量。简而言之，张量可以认为是一个 n 维数组。

下面是一些张量的例子:

*   `5`:这是一个秩为 0 的张量；这是一个形状为[ ]的标量。
*   `[2.,5., 3.]`:这是一个秩为 1 的张量；这是一个形状为`[3]`的矢量。
*   `[[1., 2., 7.], [3., 5., 4.]]`:这是一个秩 2 张量；它是一个形状为`[2, 3]`的矩阵。
*   `[[[1., 2., 3.]], [[7., 8., 9.]]]`:这是一个形状为`[2, 1, 3]`的秩为 3 的张量。

## 计算图形和会话

TensorFlow 因其 TensorFlow 核心程序而广受欢迎，该程序有两个主要动作。

*   在构建阶段构建计算图
*   在执行阶段运行计算图

我们来了解一下 TensorFlow 是如何工作的。

*   它的程序通常分为构建阶段和执行阶段。
*   构建阶段组装一个有节点(ops/operations)和边(tensors)的图。
*   执行阶段使用一个会话来执行图中的操作。
*   最简单的操作是一个常数，它不接受输入，但将输出传递给其他进行计算的操作。
*   运算的一个例子是乘法(或加法或减法，将两个矩阵作为输入，并将一个矩阵作为输出)。
*   TensorFlow 库有一个默认图形，ops 构造函数会向其中添加节点。

因此，张量流程序的结构有两个阶段，如下所示:

![A456157_1_En_1_Figb_HTML.jpg](A456157_1_En_1_Figb_HTML.jpg)

计算图是排列成节点图的一系列张量流运算。

让我们看看 TensorFlow 对 Numpy。在 Numpy 中，如果你计划乘两个矩阵，你创建矩阵并乘它们。但是在 TensorFlow 中，你设置了一个图(默认的图，除非你创建另一个图)。接下来，您需要创建变量、占位符和常量值，然后创建会话并初始化变量。最后，将数据输入占位符，以便调用任何操作。

要实际评估节点，您必须在会话中运行计算图。

会话封装了 TensorFlow 运行时的控制和状态。

下面的代码创建了一个`Session`对象:

```
sess = tf.Session()

```

然后，它调用 run 方法运行足够的计算图来计算 node1 和 node2。

计算图定义了计算。它既不计算任何东西，也不保存任何值。它旨在定义代码中提到的操作。将创建一个默认图表。所以，你不需要创建它，除非你想创建多用途的图表。

会话允许您执行图形或部分图形。它为执行分配资源(在一个或多个 CPU 或 GPU 上)。它保存中间结果和变量的实际值。

在 TensorFlow 中创建的变量值仅在一个会话中有效。如果您稍后在第二个会话中尝试查询该值，TensorFlow 将会引发一个错误，因为该变量没有在那里初始化。

要运行任何操作，您需要为该图创建一个会话。会话还将分配内存来存储变量的当前值

下面是演示的代码:

![A456157_1_En_1_Figc_HTML.jpg](A456157_1_En_1_Figc_HTML.jpg)

## 常量、占位符和变量

TensorFlow 程序使用张量数据结构来表示所有数据，在计算图中的操作之间只传递张量。可以把 TensorFlow 张量想象成一个 n 维数组或列表。张量有静态类型、秩和形状。这里，图形产生一个恒定的结果。变量在图的执行过程中保持状态。

深度学习一般要处理很多图像，所以要为每张图像放置像素值，对所有图像不断迭代。

要训练模型，您需要能够修改图形来调整一些对象，如权重和偏差。简而言之，变量使您能够将可训练参数添加到图形中。它们是用类型和初始值构造的。

让我们在 TensorFlow 中创建一个常量并打印出来。

![A456157_1_En_1_Figd_HTML.jpg](A456157_1_En_1_Figd_HTML.jpg)

下面是对前面代码的简单解释:

1.  导入`tensorflow`模块，将其命名为`tf`。
2.  创建一个常数值(`x`)，并将其赋值为数值 12。
3.  创建一个用于计算值的会话。
4.  只运行变量`x`并打印出其当前值。

前两步属于构建阶段，后两步属于执行阶段。我现在将讨论 TensorFlow 的构建和执行阶段。

您可以用另一种方式重写前面的代码，如下所示:

![A456157_1_En_1_Fige_HTML.jpg](A456157_1_En_1_Fige_HTML.jpg)

现在您将探索如何创建一个变量并初始化它。下面是实现这一点的代码:

![A456157_1_En_1_Figf_HTML.jpg](A456157_1_En_1_Figf_HTML.jpg)

下面是对前面代码的解释:

1.  导入`tensorflow`模块，将其命名为`tf`。
2.  创建一个名为`x`的常量值，并赋予其数值 12。
3.  创建一个名为`y`的变量，并将其定义为等式 12+11。
4.  用`tf.global_variables_initializer()`初始化变量。
5.  创建一个用于计算值的会话。
6.  运行步骤 4 中创建的模型。
7.  只运行变量`y`并打印出其当前值。

这里有更多的代码供您阅读:

![A456157_1_En_1_Figg_HTML.jpg](A456157_1_En_1_Figg_HTML.jpg)

## 占位符

占位符是一个变量，您可以在以后向其输入内容。它意味着接受外部输入。占位符可以有一维或多维，用于存储 n 维数组。

![A456157_1_En_1_Figh_HTML.jpg](A456157_1_En_1_Figh_HTML.jpg)

下面是对前面代码的解释:

1.  导入`tensorflow`模块，将其命名为`tf`。
2.  创建一个名为`x`的占位符，提及`float`类型。
3.  创建一个名为`y`的张量，它是将`x`乘以 10 再加上 500 的运算。注意，`x`的任何初始值都没有定义。
4.  创建一个用于计算值的会话。
5.  定义`feed_dict`中`x`的值，以便运行`y`。
6.  打印出它的值。

在下面的示例中，您创建了一个 2×4 矩阵(2D 数组)来存储一些数字。然后，使用与前面相同的操作，按元素乘以 10 并加 1。占位符的第一个维度是`None`，这意味着允许任意数量的行。

你也可以考虑用 2D 数组代替 1D 数组。代码如下:

![A456157_1_En_1_Figi_HTML.jpg](A456157_1_En_1_Figi_HTML.jpg)

这是一个 2×4 矩阵。所以，如果用 2 代替`None`，可以看到同样的输出。

![A456157_1_En_1_Figj_HTML.jpg](A456157_1_En_1_Figj_HTML.jpg)

但是，如果您创建了一个`[3, 4]`形状的占位符(注意，您将在稍后填充一个 2×4 的矩阵)，就会出现错误，如下所示:

![A456157_1_En_1_Figk_HTML.jpg](A456157_1_En_1_Figk_HTML.jpg)

```
################# What happens in a linear model ##########
# Weight and Bias as Variables as they are to be tuned
W = tf.Variable([2], dtype=tf.float32)
b = tf.Variable([3], dtype=tf.float32)
# Training dataset that will be fed while training as Placeholders

x = tf.placeholder(tf.float32)
# Linear Model
y = W * x + b

```

常数在你调用`tf.constant`时被初始化，它们的值永远不能改变。相比之下，当您调用`tf.Variable`时，变量不会被初始化。要初始化 TensorFlow 程序中的所有变量，必须显式调用一个特殊操作，如下所示。

![A456157_1_En_1_Figl_HTML.jpg](A456157_1_En_1_Figl_HTML.jpg)

重要的是要认识到`init`是初始化所有全局变量的张量流子图的句柄。在调用`sess.run`之前，变量是未初始化的。

## 创建张量

图像是三阶张量，其中维度属于高度、宽度和通道数(红色、蓝色和绿色)。

这里你可以看到图像是如何转换成张量的:

![A456157_1_En_1_Figo_HTML.jpg](A456157_1_En_1_Figo_HTML.jpg)

![A456157_1_En_1_Fign_HTML.jpg](A456157_1_En_1_Fign_HTML.jpg)

![A456157_1_En_1_Figm_HTML.jpg](A456157_1_En_1_Figm_HTML.jpg)

您可以生成各种类型的张量，如固定张量、随机张量和顺序张量。

### 固定张量

这里有一个固定的张量:

![A456157_1_En_1_Figq_HTML.jpg](A456157_1_En_1_Figq_HTML.jpg)

![A456157_1_En_1_Figp_HTML.jpg](A456157_1_En_1_Figp_HTML.jpg)

`tf:.fill`创建一个具有唯一数字的形状张量(2×3)。

![A456157_1_En_1_Figr_HTML.jpg](A456157_1_En_1_Figr_HTML.jpg)

`tf.diag`创建一个具有指定对角元素的对角矩阵。

![A456157_1_En_1_Figs_HTML.jpg](A456157_1_En_1_Figs_HTML.jpg)

`tf.constant`创建一个常量张量。

![A456157_1_En_1_Figt_HTML.jpg](A456157_1_En_1_Figt_HTML.jpg)

### 序列张量

`tf.range`创建从指定值开始并具有指定增量的数字序列。

![A456157_1_En_1_Figu_HTML.jpg](A456157_1_En_1_Figu_HTML.jpg)

`tf.linspace`创建一个等距值序列。

![A456157_1_En_1_Figv_HTML.jpg](A456157_1_En_1_Figv_HTML.jpg)

### 随机张量

`tf.random_uniform`从一个范围内的均匀分布中生成随机值。

![A456157_1_En_1_Figw_HTML.jpg](A456157_1_En_1_Figw_HTML.jpg)

`tf.random_normal`根据具有指定平均值和标准偏差的正态分布生成随机值。

![A456157_1_En_1_Figz_HTML.jpg](A456157_1_En_1_Figz_HTML.jpg)

![A456157_1_En_1_Figy_HTML.jpg](A456157_1_En_1_Figy_HTML.jpg)

![A456157_1_En_1_Figx_HTML.jpg](A456157_1_En_1_Figx_HTML.jpg)

你能猜出结果吗？

![A456157_1_En_1_Figab_HTML.jpg](A456157_1_En_1_Figab_HTML.jpg)

![A456157_1_En_1_Figaa_HTML.jpg](A456157_1_En_1_Figaa_HTML.jpg)

如果你找不到结果，请修改之前我讨论张量创建的部分。

您可以在这里看到结果:

![A456157_1_En_1_Figac_HTML.jpg](A456157_1_En_1_Figac_HTML.jpg)

## 处理矩阵

一旦你习惯了创建张量，你就可以享受处理矩阵的乐趣了(2D 张量)。

![A456157_1_En_1_Figae_HTML.jpg](A456157_1_En_1_Figae_HTML.jpg)

![A456157_1_En_1_Figad_HTML.jpg](A456157_1_En_1_Figad_HTML.jpg)

## 激活功能

激活函数的想法来自于对人类大脑中神经元如何工作的分析(见图 [1-1](#Fig1) )。神经元在超过某个阈值后变得活跃，这个阈值更好地被称为激活电位。在大多数情况下，它还试图将输出放在一个小范围内。

Sigmoid、双曲正切(tanh)、ReLU 和 eLU 是最流行的激活函数。

我们来看看现在比较流行的激活功能。

### 正切双曲线和 Sigmoid

图 [1-2](#Fig2) 显示了正切双曲线和 sigmoid 激活函数。

![A456157_1_En_1_Fig2_HTML.jpg](A456157_1_En_1_Fig2_HTML.jpg)

图 1-2

Two popular activation functions

![A456157_1_En_1_Fig1_HTML.jpg](A456157_1_En_1_Fig1_HTML.jpg)

图 1-1

An activation function

下面是演示代码:

![A456157_1_En_1_Figaf_HTML.jpg](A456157_1_En_1_Figaf_HTML.jpg)

### ReLU 和 eLU

图 [1-3](#Fig3) 显示了 ReLU 和 eLU 功能。

![A456157_1_En_1_Fig3_HTML.jpg](A456157_1_En_1_Fig3_HTML.jpg)

图 1-3

The ReLU and ELU functions

下面是生成这些函数的代码:

![A456157_1_En_1_Figag_HTML.jpg](A456157_1_En_1_Figag_HTML.jpg)

### ReLU6

ReLU6 类似于 ReLU，只是输出不能超过 6。

![A456157_1_En_1_Figah_HTML.jpg](A456157_1_En_1_Figah_HTML.jpg)

请注意，tanh 是一个重新调整的逻辑 sigmoid 函数。

![A456157_1_En_1_Figai_HTML.jpg](A456157_1_En_1_Figai_HTML.jpg)

![A456157_1_En_1_Figaj_HTML.jpg](A456157_1_En_1_Figaj_HTML.jpg)

![A456157_1_En_1_Figak_HTML.jpg](A456157_1_En_1_Figak_HTML.jpg)

![A456157_1_En_1_Figal_HTML.jpg](A456157_1_En_1_Figal_HTML.jpg)

![A456157_1_En_1_Figam_HTML.jpg](A456157_1_En_1_Figam_HTML.jpg)

![A456157_1_En_1_Figan_HTML.jpg](A456157_1_En_1_Figan_HTML.jpg)

## 损失函数

损失函数(成本函数)将被最小化，以便获得模型的每个参数的最佳值。例如，您需要获得权重(斜率)和偏差(y 截距)的最佳值，以便根据预测值(X)来解释目标(y)。方法是实现斜率的最佳值，y 截距是最小化代价函数/损失函数/平方和。对于任何模型，都有许多参数，并且预测或分类中的模型结构是用参数值来表示的。

你需要评估你的模型，为此你需要定义成本函数(损失函数)。损失函数的最小化可以是找到每个参数的最佳值的驱动力。对于回归/数值预测，L1 或 L2 可能是有用的损失函数。对于分类，交叉熵可以是有用的损失函数。Softmax 或 sigmoid 交叉熵可以是相当流行的损失函数。

### 损失函数示例

下面是演示的代码:

![A456157_1_En_1_Figao_HTML.jpg](A456157_1_En_1_Figao_HTML.jpg)

### 常见损失函数

以下是最常见的损失函数列表:

<colgroup><col align="left"></colgroup> 
| `tf.contrib.losses.absolute_difference` |
| `tf.contrib.losses.add_loss` |
| `tf.contrib.losses.hinge_loss` |
| `tf.contrib.losses.compute_weighted_loss` |
| `tf.contrib.losses.cosine_distance` |
| `tf.contrib.losses.get_losses` |
| `tf.contrib.losses.get_regularization_losses` |
| `tf.contrib.losses.get_total_loss` |
| `tf.contrib.losses.log_loss` |
| `tf.contrib.losses.mean_pairwise_squared_error` |
| `tf.contrib.losses.mean_squared_error` |
| `tf.contrib.losses.sigmoid_cross_entropy` |
| `tf.contrib.losses.softmax_cross_entropy` |
| `tf.contrib.losses.sparse_softmax_cross_entropy` |
| `tf.contrib.losses.log(predictions,labels,weight=2.0)` |

## 优化者

现在你应该确信你需要使用一个损失函数来获得模型的每个参数的最佳值。如何才能获得最佳价值？

最初，您假设模型的权重和偏差的初始值(线性回归等)。).现在你需要找到达到参数最佳值的方法。优化器是达到参数最佳值的方法。在每次迭代中，该值都按照优化器建议的方向变化。假设你有 16 个权重值(`w1`、`w2`、`w3`、…、`w16`)和 4 个偏差(`b1`、`b2`、`b3`、`b4`)。最初，你可以假设每一个权重和偏差都是零(或者一或任何数字)。优化器建议`w1`(和其他参数)是否应该在下一次迭代中增加或减少，同时牢记最小化的目标。经过多次迭代后，`w1`(以及其他参数)将稳定到参数的最佳值。

换句话说，TensorFlow 和其他每一个深度学习框架都提供了优化器，这些优化器缓慢地改变每个参数，以便最小化损失函数。优化器的目的是为下一次迭代中的变化给出权重和偏差的方向。假设你有 64 个权重和 16 个偏差；您尝试在每次迭代(反向传播期间)中改变权重和偏差值，以便在多次迭代后获得正确的权重和偏差值，同时尝试最小化损失函数。

为模型选择最佳优化器以快速收敛并正确学习权重和偏差是一项棘手的任务。

适应性技术(adadelta、adagrad 等。)对于复杂的神经网络收敛更快是很好的优化器。Adam 被认为是大多数情况下的最佳优化器。它也优于其他自适应技术(adadelta、adagrad 等)。)，但是计算量很大。对于稀疏数据集，SGD、NAG、momentum 等方法不是最佳选择；自适应学习率方法有。一个额外的好处是，你不需要调整学习率，但可以用默认值达到最佳效果。

### 损失函数示例

下面是演示的代码:

![A456157_1_En_1_Figaq_HTML.jpg](A456157_1_En_1_Figaq_HTML.jpg)

![A456157_1_En_1_Figap_HTML.jpg](A456157_1_En_1_Figap_HTML.jpg)

### 常见优化器

以下是常见优化器的列表:

![A456157_1_En_1_Figar_HTML.jpg](A456157_1_En_1_Figar_HTML.jpg)

## 韵律学

学了一些建立模型的方法，是时候评估模型了。所以，你需要评估回归器或分类器。

评价指标有很多，其中分类准确率、对数损失、ROC 曲线下面积是最受欢迎的。

分类准确度是正确预测数与所有预测数的比率。当每个类别的观察值没有太大偏差时，精确度可以被认为是一个很好的度量。

```
tf.contrib.metrics.accuracy(actual_labels, predictions)

```

还有其他评估指标。

### 指标示例

本节展示了要演示的代码。

在这里，您创建实际值(称为`x`)和预测值(称为`y`)。然后你检查准确性。准确度表示实际值等于预测值的次数与实例总数的比率。

### 通用指标

以下是常见指标的列表:

![A456157_1_En_1_Figat_HTML.jpg](A456157_1_En_1_Figat_HTML.jpg)

![A456157_1_En_1_Figas_HTML.jpg](A456157_1_En_1_Figas_HTML.jpg)