# 10.语音到文本，反之亦然

在本章中，您将了解语音到文本和文本到语音转换的重要性。您还将了解进行这种类型的转换所需的函数和组件。

具体来说，我将介绍以下内容:

*   为什么你想把语音转换成文本
*   语音作为数据
*   将语音映射到矩阵的语音特征
*   光谱图，将语音映射成图像
*   利用梅尔倒谱系数(MFCC)特征构建语音识别分类器
*   通过频谱图建立语音识别的分类器
*   语音识别的开源方法
*   流行的认知服务提供商
*   文本话语的未来

## 语音到文本转换

语音转文本，通俗地说就是一个 app 识别一个人说的话，把语音转换成文字。有很多原因让你想使用语音到文本的转换。

*   盲人或残障人士可以仅使用语音控制不同的设备。
*   您可以通过将口头对话转换为文本记录来保存会议和其他事件的记录。
*   您可以转换视频和音频文件中的音频，以获得正在朗读的单词的字幕。
*   通过对着设备用一种语言说话，然后将文本转换成另一种语言的语音，可以将单词翻译成另一种语言。

## 语音作为数据

制造任何自动语音识别系统的第一步是获取特征。换句话说，你识别出了对识别语言内容有用的音频成分，并删除了所有其他无用的背景噪音。

每个人的语音都被他们声道的形状以及舌头和牙齿过滤。发出什么声音取决于这个形状。为了准确地识别正在产生的音素，你需要准确地确定这个形状。你可以说声道的形状表明它自己形成了短时功率谱的包络。MFCCs 的工作是准确地表示这个包络。

语音也可以通过转换成声谱图来表示为数据(图 [10-1](#Fig1) )。

![A456157_1_En_10_Fig1_HTML.jpg](A456157_1_En_10_Fig1_HTML.jpg)

图 10-1

Speech as data

## 语音特征:将语音映射到矩阵

MFCCs 广泛用于自动语音和说话人识别。mel 标度将纯音的感知频率或音高与其实际测量频率相关联。

您可以使用以下公式将音频的频率标度转换为 mel 标度:

![ $$ M(f)=1125\kern0.50em \ln \left(1+f/700\right) $$ ](A456157_1_En_10_Chapter_Equ1.gif)

要将其转换回频率，请使用以下公式:

![ $$ {M}^{-1}(m)=700\left(\exp \left(m/1125\right)-1\right) $$ ](A456157_1_En_10_Chapter_Equ2.gif)

以下是 Python 中提取 MFCC 要素的函数:

```
def mfcc(signal,samplerate=16000,winlen=0.025,winstep=0.01,numcep=13, nfilt=26,nfft=512,lowfreq=0,highfreq=None,preemph=0.97, ceplifter=22,appendEnergy=True)

```

这些是使用的参数:

*   `signal`:这是需要计算 MFCC 特征的信号。应该是 N*1 的数组(读取 WAV 文件)。
*   这是您正在工作的信号的采样率。
*   `winlen`:这是以秒为单位的分析窗口长度。默认情况下，它是 0.025 秒。
*   `winstep`:这是连续的窗口步骤。默认情况下是 0.01 秒。
*   `numcep`:这是函数应该返回的 ceptrum 的编号。默认情况下，它是 13。
*   `nfilt`:这是滤波器组中滤波器的数量。默认情况下是 26。
*   `nfft`:这是快速傅立叶变换(FFT)的大小。默认情况下，它是 512。
*   这是最低的频带边缘，单位为赫兹。默认情况下，该值为 0。
*   这是最高频带边缘，单位为赫兹。默认情况下，它是采样速率除以 2。
*   `preemph`:应用以`preemph`为系数的预加重滤波器。0 表示没有过滤器。默认情况下，它是 0.97。
*   `ceplifter`:将提升器应用于最终倒谱系数。0 表示没有升降机。默认情况下是 22。
*   `appendEnergy`:如果设置为真，则第零倒谱系数被替换为总帧能量的对数。

这个函数返回一个包含特性的 Numpy 数组。每行包含一个特征向量。

## 频谱图:将语音映射到图像

光谱图是光谱的照片或电子图像。这个想法是将音频文件转换成图像，并将图像传递到深度学习模型，如 CNN 和 LSTM，以进行分析和分类。

频谱图被计算为窗口数据段的 FFT 序列。一种常见的格式是具有两个几何维度的图形；一个轴代表时间，另一个轴代表频率。第三维度使用点的颜色或大小来表示特定时间特定频率的振幅。光谱图通常用两种方法之一制作。它们可以近似为由一系列带通滤波器产生的滤波器组。或者，在 Python 中，有一个将音频映射到声谱图的直接函数。

## 利用 MFCC 特征构建语音识别分类器

要构建语音识别的分类器，您需要安装 python_speech_features Python 包。

您可以使用命令`pip install python_speech_features`来安装这个包。

`mfcc`函数为音频文件创建一个特征矩阵。为了建立一个识别不同人声音的分类器，你需要以 WAV 格式收集他们的语音数据。然后使用`mfcc`函数将所有音频文件转换成一个矩阵。从 WAV 文件中提取特征的代码如下所示:

![A456157_1_En_10_Figa_HTML.jpg](A456157_1_En_10_Figa_HTML.jpg)

如果您运行前面的代码，您将获得以下形式的输出:

```
[[ 7.66608682  7.04137131  7.30715423 ...,  9.43362359  9.11932984
   9.93454603]
 [ 4.9474559   4.97057377  6.90352236 ...,  8.6771281   8.86454547
   9.7975147 ]
 [ 7.4795622   6.63821063  5.98854983 ...,  8.78622734  8.805521
   9.83712966]
 ...,
 [ 7.8886269   6.57456605  6.47895433 ...,  8.62870034  8.79965464
   9.67997298]
 [ 5.73028657  4.87985847  6.64977329 ...,  8.64089442  8.62887745
   9.90470194]
 [ 8.8449656   6.67098127  7.09752316 ...,  8.84914694  8.97807983
   9.45123015]]

```

这里，每一行代表一个特征向量。

尽可能多地收集一个人的录音，并在这个矩阵中附加每个音频文件的特征矩阵。

这将作为您的训练数据集。

对所有其他类重复相同的步骤。

一旦数据集准备好，你就可以将这些数据放入任何深度学习模型(用于分类)中，对不同人的声音进行分类。

Note

要查看使用 MFCC 功能的分类器的完整代码，您可以访问 [`www.navinmanaswi.com/SpeechRecognizer`](http://www.navinmanaswi.com/SpeechRecognizer) 。

## 通过声谱图建立用于语音识别的分类器

使用声谱图方法将所有音频文件转换为图像(图 [10-2](#Fig2) )，因此您所要做的就是将训练数据中的所有声音文件转换为图像，并将这些图像馈送到深度学习模型，就像您在 CNN 中所做的那样。

![A456157_1_En_10_Fig2_HTML.jpg](A456157_1_En_10_Fig2_HTML.jpg)

图 10-2

Spectogram of speech sample

下面是将音频文件转换为声谱图的 Python 代码:

![A456157_1_En_10_Figb_HTML.jpg](A456157_1_En_10_Figb_HTML.jpg)

## 开源方法

Python 有一些开源包可以执行语音到文本和文本到语音的转换。

以下是一些开源的语音到文本转换 API:

*   PocketSphinx
*   谷歌语音
*   谷歌云演讲
*   Wit.ai
*   Houndify
*   IBM 语音转文本 API
*   微软必应语音

在使用了所有这些之后，我可以说它们工作得相当好；美国口音特别清晰。

如果您对评估转换的准确性感兴趣，您需要一个度量:单词错误率(WER)。

在下一节中，我将讨论前面提到的每个 API。

## 使用每个 API 的示例

让我们看一下每个 API。

### 使用 PocketSphinx

PocketSphinx 是一个用于语音到文本转换的开源 API。这是一个轻量级的语音识别引擎，专门针对手持和移动设备进行了调整，尽管它在桌面上也同样适用。只需使用命令`pip install PocketSphinx`安装软件包。

```
import speech_recognition as sr
from os import path
AUDIO_FILE = "MyAudioFile.wav"

r = sr.Recognizer()
with sr.AudioFile(AUDIO_FILE) as source:
 audio = r.record(source)

try:
   print("Sphinx thinks you said " + r.recognize_sphinx(audio))
except sr.UnknownValueError:
   print("Sphinx could not understand audio")
except sr.RequestError as e:
  print("Sphinx error; {0}".format(e))

===============================================================

```

### 使用谷歌语音 API

Google 提供了自己的语音 API，可以用 Python 代码实现，可以用来创建不同的应用。

```
# recognize speech using Google Speech Recognition
try:
    print("Google Speech Recognition thinks you said " + r.recognize_google(audio))
except sr.UnknownValueError:
    print("Google Speech Recognition could not understand audio")
except sr.RequestError as e:
    print("Could not request results from Google Speech Recognition service;{0}".format(e))

```

### 使用谷歌云语音 API

你也可以使用谷歌云语音 API 进行转换。在 Google Cloud 上创建一个帐户并复制凭证。

```
GOOGLE_CLOUD_SPEECH_CREDENTIALS = r"INSERT THE CONTENTS OF THE GOOGLE CLOUD SPEECH JSON CREDENTIALS FILE HERE" try:     print("Google Cloud Speech thinks you said " + r.recognize_google_cloud(audio, credentials_json=GOOGLE_CLOUD_SPEECH_CREDENTIALS))
except sr.UnknownValueError:
    print("Google Cloud Speech could not understand audio")
except sr.RequestError as e:
    print("Could not request results from Google Cloud Speech service; {0}".format(e))

```

### 使用 Wit.ai API

Wit.ai API 使您能够制作语音到文本转换器。您需要创建一个帐户，然后创建一个项目。复制你的 Wit.ai 密钥，开始编码。

```
#recognize speech using Wit.ai
WIT_AI_KEY = "INSERT WIT.AI API KEY HERE" # Wit.ai keys are 32-character uppercase alphanumeric strings
try:
    print("Wit.ai thinks you said " + r.recognize_wit(audio, key=WIT_AI_KEY))
except sr.UnknownValueError:
    print("Wit.ai could not understand audio")
except sr.RequestError as e:
    print("Could not request results from Wit.ai service; {0}".format(e))

```

### 使用 Houndify API

与前面的 API 类似，您需要在 Houndify 创建一个帐户，并获得您的客户机 ID 和密钥。这允许你建立一个对声音有反应的应用程序。

```
# recognize speech using Houndify
HOUNDIFY_CLIENT_ID = "INSERT HOUNDIFY CLIENT ID HERE" # Houndify client IDs are Base64-encoded strings
HOUNDIFY_CLIENT_KEY = "INSERT HOUNDIFY CLIENT KEY HERE" # Houndify client keys are Base64-encoded strings
try:
    print("Houndify thinks you said " + r.recognize_houndify(audio, client_id=HOUNDIFY_CLIENT_ID, client_key=HOUNDIFY_CLIENT_KEY))
except sr.UnknownValueError:
    print("Houndify could not understand audio")
except sr.RequestError as e:
    print("Could not request results from Houndify service; {0}".format(e))

```

### 使用 IBM 语音到文本 API

IBM 语音到文本 API 使您能够将 IBM 的语音识别功能添加到您的应用程序中。登录 IBM cloud 并启动您的项目，获取一个 IBM 用户名和密码。

```
# IBM Speech to Text
# recognize speech using IBM Speech to Text
IBM_USERNAME = "INSERT IBM SPEECH TO TEXT USERNAME HERE" # IBM Speech to Text usernames are strings of the form XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX
IBM_PASSWORD = "INSERT IBM SPEECH TO TEXT PASSWORD HERE" # IBM Speech to Text passwords are mixed-case alphanumeric strings
try:
    print("IBM Speech to Text thinks you said " + r.recognize_ibm(audio, username=IBM_USERNAME, password=IBM_PASSWORD))
except sr.UnknownValueError:
    print("IBM Speech to Text could not understand audio")
except sr.RequestError as e:
    print("Could not request results from IBM Speech to Text service; {0}".format(e))

```

### 使用 Bing 语音识别 API

这个 API 实时识别来自麦克风的音频。在 Bing.com 上创建一个帐户，并获得阿炳语音识别 API 密钥。

```
# recognize speech using Microsoft Bing Voice Recognition
BING_KEY = "INSERT BING API KEY HERE" # Microsoft Bing Voice Recognition API key is 32-character lowercase hexadecimal strings
try:
    print("Microsoft Bing Voice Recognition thinks you said " + r.recognize_bing(audio, key=BING_KEY))
except sr.UnknownValueError:
    print("Microsoft Bing Voice Recognition could not understand audio")
except sr.RequestError as e:
    print("Could not request results from Microsoft Bing Voice Recognition service; {0}".format(e))

```

一旦你把演讲转换成文本，你就不能指望百分之百的准确。要测量精确度，可以使用 WER。

## 文本到语音转换

本章的这一节重点介绍如何将书面文本转换为音频文件。

### 使用 pyttsx

使用名为 pyttsx 的 Python 包，可以将文本转换成音频。

```
Do a pip install pyttsx. If you are using python 3.6 then do pip3 install pyttsx3.

import pyttsx
engine = pyttsx.init()
engine.say("Your Message")
engine.runAndWait()

```

### 使用 SAPI

在 Python 中，还可以使用 SAPI 进行文本到语音的转换。

```
from win32com.client import constants, Dispatch
Msg = "Hi this is a test"
speaker = Dispatch("SAPI.SpVoice")  #Create SAPI SpVoice Object
speaker.Speak(Msg)                  #Process TTS
del speaker

```

### 使用 SpeechLib

您可以从文本文件中获取输入，并使用 SpeechLib 将其转换为音频，如下所示:

```
from comtypes.client import CreateObject    
engine = CreateObject("SAPI.SpVoice")
stream = CreateObject("SAPI.SpFileStream")
from comtypes.gen import SpeechLib    
infile = "SHIVA.txt"
outfile = "SHIVA-audio.wav"
stream.Open(outfile, SpeechLib.SSFMCreateForWrite)
engine.AudioOutputStream = stream
f = open(infile, 'r')
theText = f.read()
f.close()
engine.speak(theText)
stream.Close()

```

很多时候，您必须编辑音频，以便从音频文件中删除声音。下一节将向您展示如何操作。

### 音频切割代码

制作一个包含音频详细信息的逗号分隔值的音频 CSV 文件，并使用 Python 执行以下操作:

```
import wave
import sys
import os
import csv
origAudio = wave.open('Howard.wav', 'r') #change path
frameRate = origAudio.getframerate()
nChannels = origAudio.getnchannels()
sampWidth = origAudio.getsampwidth()
nFrames   = origAudio.getnframes()

filename =  'result1.csv' #change path

exampleFile = open(filename)
exampleReader = csv.reader(exampleFile)
exampleData = list(exampleReader)

count = 0

for data in exampleData:
 #for selections in data:
    print('Selections ', data[4], data[5])
    count += 1
    if data[4] == 'startTime' and data[5] == 'endTime':
        print('Start time')
    else:
        start = float(data[4])
        end = float(data[5])
        origAudio.setpos(start*frameRate)
        chunkData = origAudio.readframes(int((end-start)*frameRate))

        outputFilePath = 'C:/Users/Navin/outputFile{0}.wav'.format(count) # change path
        chunkAudio = wave.open(outputFilePath, 'w')
        chunkAudio.setnchannels(nChannels)
        chunkAudio.setsampwidth(sampWidth)
        chunkAudio.setframerate(frameRate)
        chunkAudio.writeframes(chunkData)
        chunkAudio.close()

```

## 认知服务提供商

让我们看看一些帮助语音处理的认知服务提供商。

### 微软 Azure

Microsoft Azure 提供了以下功能:

*   自定义语音服务:这克服了语音识别的障碍，如说话方式、词汇和背景噪音。
*   Translator Speech API:这支持实时语音翻译。
*   说话人识别 API:它可以根据给定音频数据中每个说话人的语音样本来识别说话人。
*   Bing 语音 API:它将音频转换为文本，理解意图，并将文本转换回语音，以获得自然的响应。

### 亚马逊认知服务

亚马逊认知服务(Amazon Cognitive Services)提供亚马逊 Polly，一种将文本转化为语音的服务。Amazon Polly 允许您创建会说话的应用程序，使您能够构建全新类别的支持语音的产品。

*   可以使用 47 种语音和 24 种语言，并提供印度英语选项。
*   使用 Amazon effects，可以将耳语、愤怒等音调添加到讲话的特定部分。
*   您可以指示系统如何以不同的方式发出特定短语或单词的发音。例如，“W3C”读作万维网联盟，但是您可以将其改为只读作缩写。您还可以提供 SSML 格式的输入文本。

### IBM 沃森服务

IBM Watson 提供了两种服务。

*   语音转文本:美国英语、西班牙语和日语
*   文本到语音转换:美国英语、英国英语、西班牙语、法语、意大利语和德语

## 语音分析的未来

语音识别技术已经取得了很大的进步。每年，它都比前一年精确 10%到 15%。未来，它将为计算机提供迄今为止最具交互性的界面。

你很快就会在市场上看到许多应用，包括交互式书籍、机器人控制和自动驾驶汽车界面。语音数据提供了一些令人兴奋的新的可能性，因为它是行业的未来。语音智能使人们能够发送信息、接受或下达命令、提出投诉，以及做任何他们过去需要手动输入的工作。它提供了很好的客户体验，也许这就是为什么所有面向客户的部门和企业都倾向于大量使用语音应用程序。我可以预见语音应用开发者的美好未来。