# 5\. 高级张量流

人工智能将是谷歌的终极版本。理解网络上一切的终极搜索引擎。它会准确理解你想要什么，并给你正确的东西。

—拉里·佩奇

在介绍了基本的张量运算后，让我们进一步探讨高级运算，如张量合并和分割、范数统计、张量填充和裁剪。我们也将再次使用 MNIST 数据集来增强我们对张量流中张量运算的理解。

## 5.1 合并和拆分

### 合并

归并就是将多个张量合并成某一维的一个张量。以某学校的成绩册数据为例，张量 *A* 用于保存 1-4 班的成绩册。每个班有 35 名学生，共有 8 门课程。张量 *A* 的形状为【4，35，8】。类似地，张量 *B* 保存其他六个类的成绩册，形状为【6，35，8】。将这两个成绩册合并，就可以得到学校所有班级的成绩册，记为张量 *C* ，对应的形状应该是【10，35，8】，其中 10 代表十个班级，35 代表 35 个学生，8 代表八个科目。

张量可以使用连接和堆栈操作来合并。串联操作不会生成新的维度。它仅沿现有维度合并。但是堆栈操作会创建新的维度。是否使用连接或堆叠操作来合并张量取决于是否需要为特定场景创建新的维度。我们将在下一节课中讨论这两个问题。

**串联**。在 TensorFlow 中，可以使用 tf.concat(tensors，axis)函数连接张量，其中第一个参数包含需要合并的张量列表，第二个参数指定要合并的维度索引。回到前面的例子，我们合并班级维度中的年级册。这里，类维的索引号是 0，即 axis = 0。合并 *A* 和 *B* 的代码如下:

```py
In [1]:
a = tf.random.normal([4,35,8]) # Create gradebook A
b = tf.random.normal([6,35,8]) # Create gradebook B
tf.concat([a,b],axis=0) # Merge gradebooks
Out[1]:
<tf.Tensor: id=13, shape=(10, 35, 8), dtype=float32, numpy=
array([[[ 1.95299834e-01,  6.87859178e-01, -5.80048323e-01, ...,
          1.29430830e+00,  2.56610274e-01, -1.27798581e+00],
        [ 4.29753691e-01,  9.11329567e-01, -4.47975427e-01, ...,

```

除了类维度，我们还可以合并其他维度的张量。考虑张量 *A* 用 shape [10，35，4]保存所有班级所有学生的前四科成绩，张量 *B* 用 shape [10，35，4]保存其余 4 科成绩。我们可以通过合并 *A* 和 *B* 得到总的年级簿张量，如下所示:

```py
In [2]:
a = tf.random.normal([10,35,4])
b = tf.random.normal([10,35,4])
tf.concat([a,b],axis=2) # Merge along the last dimension
Out[2]:
<tf.Tensor: id=28, shape=(10, 35, 8), dtype=float32, numpy=
array([[[-5.13509691e-01, -1.79707789e+00,  6.50747120e-01, ...,
          2.58447856e-01,  8.47878829e-02,  4.13468748e-01],
        [-1.17108583e+00,  1.93961406e+00,  1.27830813e-02, ...,

```

从语法上讲，concatenate 操作可以在任何维度上执行。唯一的限制是非合并维度的长度必须相同。例如，shape [4，32，8]和 shape [6，35，8]的张量不能在班级维度中直接合并，因为学生人数维度的长度不一样——一个是 32，一个是 35，例如:

```py
In [3]:
a = tf.random.normal([4,32,8])
b = tf.random.normal([6,35,8])
tf.concat([a,b],axis=0) # Illegal merge. Second dimension is different.
Out[3]:
InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [4,32,8] vs. shape[1] = [6,35,8] [Op:ConcatV2] name: concat

```

**堆栈**。concatenate 操作直接合并现有维度上的数据，并且不创建新维度。如果我们想在合并数据时创建一个新的维度，我们需要使用 tf.stack 操作。考虑张量 *A* 以【35，8】的形状保存一个班级的成绩册，张量 *B* 以【35，8】的形状保存另一个班级的成绩册。当合并这两个类的数据时，我们需要创建一个新的维度，定义为类维度。新尺寸可以放置在任何位置。一般把班级维度放在学生维度之前，也就是合并张量的新形状应该是[2，35，8]。

tf.stack(tensors，axis)函数可用于合并多个张量。第一个参数表示要合并的张量列表，第二个参数指定插入新维度的位置。axis 的用法与 tf.expand_dims 函数的用法相同。当*轴* ≥ 0 时，在轴前插入一个新尺寸。当*轴* <为 0 时，我们在轴后插入一个新的尺寸。图 [5-1](#Fig1) 显示了形状为 *b* 、 *c* 、 *h* 、 *w* 的张量对应不同轴参数设置的新维度位置。

![../images/515226_1_En_5_Chapter/515226_1_En_5_Fig1_HTML.png](../images/515226_1_En_5_Chapter/515226_1_En_5_Fig1_HTML.png)

图 5-1

具有不同轴值的堆栈操作的新尺寸插入位置

使用堆栈操作合并两个班级的成绩册，并在 axis = 0 位置插入班级维度。代码如下:

```py
In [4]:
a = tf.random.normal([35,8])
b = tf.random.normal([35,8])
tf.stack([a,b],axis=0) # Stack a and b and insert new dimension at axis=0
Out[4]:
<tf.Tensor: id=55, shape=(2, 35, 8), dtype=float32, numpy=
array([[[ 3.68728966e-01, -8.54765773e-01, -4.77824420e-01,
         -3.83714020e-01, -1.73216307e+00,  2.03872994e-02,
          2.63810277e+00, -1.12998331e+00],...

```

我们也可以选择在其他地方插入新的维度。例如，在末尾插入类维度:

```py
In [5]:
a = tf.random.normal([35,8])
b = tf.random.normal([35,8])
tf.stack([a,b],axis=-1) # Insert new dimension at the end
Out[5]:
<tf.Tensor: id=69, shape=(35, 8, 2), dtype=float32, numpy=
array([[[ 0.3456724 , -1.7037214 ],
        [ 0.41140947, -1.1554345 ],
        [ 1.8998919 ,  0.56994915],...

```

现在类维度在 axis = 2 上，我们需要根据最新维度顺序代表的视图来理解数据。如果我们选择使用 tf.concat 来合并前面的脚本，那么它将是

```py
In [6]:
a = tf.random.normal([35,8])
b = tf.random.normal([35,8])
tf.concat([a,b],axis=0) # No class dimension
Out[6]:
<tf.Tensor: id=108, shape=(70, 8), dtype=float32, numpy=
array([[-0.5516891 , -1.5031327 , -0.35369992,  0.31304857,  0.13965549,
         0.6696881 , -0.50115544,  0.15550546],
       [ 0.8622069 ,  1.0188094 ,  0.18977325,  0.6353301 ,  0.05809061,...

```

可以看出 tf.concat 也可以平滑地合并数据，但是我们需要按照前 35 个学生来自第一节课，后 35 个学生来自第二节课的方式来理解张量数据，这不是很直观。对于这个例子，通过 tf.stack 方法创建一个新的维度显然更合理。

tf.stack 函数也需要满足一定的条件才能使用。它需要所有的张量合并成相同的形状。让我们来看看当堆叠两个不同形状的张量时会发生什么:

```py
In [7]:
a = tf.random.normal([35,4])
b = tf.random.normal([35,8])
tf.stack([a,b],axis=-1) # Illegal use of stack function. Different shapes.
Out[7]:
InvalidArgumentError: Shapes of all inputs must match: values[0].shape = [35,4] != values[1].shape = [35,8] [Op:Pack] name: stack

```

前面的操作试图合并两个形状分别为[35，4]和[35，8]的张量。因为两个张量的形状不一样，合并操作无法完成。

### 拆分

合并操作的逆过程是拆分，即将一个张量拆分成多个张量。让我们继续学习成绩册的例子。我们得到全校形状为[10，35，8]的年级册张量。现在我们需要将数据在班级维度上切割成十个张量，每个张量保存对应班级的年级簿数据。tf.split(x，num_or_size_splits，axis)可以用来完成张量分裂运算。函数中参数的含义如下:

*   x:要分割的张量。

*   数量 _ 或 _ 大小 _ 分割:切割方案。当 num_or_size_splits 为单值时，如 10，则表示张量 x 被切成等长的十份。当 num_or_size_splits 是一个列表时，列表中的每个元素代表每个部分的长度。比如 num_or_size_splits=[2，4，2，2]表示张量被切割成四个部分，每个部分的长度为 2，4，2，2。

*   轴:指定分割的尺寸索引。

现在，我们将总成绩册张量分成十份，如下所示:

```py
In [8]:
x = tf.random.normal([10,35,8])
# Cut into 10 pieces with equal length
result = tf.split(x, num_or_size_splits=10, axis=0)
len(result)  # Return a list with 10 tensors of equal length
Out[8]: 10

```

我们可以查看一个张量切割后的形状，应该是形状为[1，35，8]的一个类的所有年级书数据:

```py
In [9]: result[0] # Check the first class gradebook
Out[9]: <tf.Tensor: id=136, shape=(1, 35, 8), dtype=float32, numpy=
array([[[-1.7786729 ,  0.2970506 ,  0.02983334,  1.3970423 ,
          1.315918  , -0.79110134, -0.8501629 , -1.5549672 ],
        [ 0.5398711 ,  0.21478991, -0.08685189,  0.7730989 ,...

```

可以看出，第一类张量的形状为[1，35，8]，*其中*仍然具有类维数。让我们进行不等长切割。例如，将数据分成四部分，每个部分的长度为[4，2，2，2]:

```py
In [10]: x = tf.random.normal([10,35,8])
# Split tensor into 4 parts
result = tf.split(x, num_or_size_splits=[4,2,2,2] ,axis=0)
len(result)
Out[10]: 4

```

检查第一个分裂张量的形状。根据我们的拆分方案，它应该包含四个班的成绩册。形状应该是[4，35，8]:

```py
In [10]: result[0]
Out[10]: <tf.Tensor: id=155, shape=(4, 35, 8), dtype=float32, numpy=
array([[[-6.95693314e-01,  3.01393479e-01,  1.33964568e-01, ...,

```

特别是，如果我们想将某个维度除以长度 1，我们可以使用 tf.unstack(x，axis)函数。这个方法是 tf.split 的一个特例，拆分长度固定为 1。我们只需要指定拆分维度的索引号。例如，在班级维度中拆分总成绩簿张量:

```py
In [11]: x = tf.random.normal([10,35,8])
result = tf.unstack(x,axis=0)
len(result) # Return a list with 10 tensors
Out[11]: 10

```

查看分割张量的形状:

```py
In [12]: result[0] # The first class tensor
Out[12]: <tf.Tensor: id=166, shape=(35, 8), dtype=float32, numpy=
array([[-0.2034383 ,  1.1851563 ,  0.25327438, -0.10160723,  2.094969  ,
        -0.8571669 , -0.48985648,  0.55798006],...

```

可以看到，通过 tf.unstack 分裂后，分裂张量形状变成了[35，8]，即类维消失，这与 tf.split 不同。

## 5.2 常见统计数据

在神经网络计算期间，需要计算各种统计属性，例如最大值、最小值、平均值和范数。由于张量通常包含大量的数据，通过获取这些张量的统计信息，更容易推断出张量值的分布。

### 5.2.1 规范

范数是向量“长度”的度量。可以推广到张量。在神经网络中，它通常用于表示张量权重和梯度大小。常用的规范有:

*   L1 norm, defined as the sum of the absolute values of all the elements of the vector:

    ![$$ {\left\Vert x\right\Vert}_1={\sum}_i\left|{x}_i\right| $$](../images/515226_1_En_5_Chapter/515226_1_En_5_Chapter_TeX_Equa.png)

*   L2 norm, defined as the root sum of the squares of all the elements of the vector:

    ![$$ {\left\Vert x\right\Vert}_2=\sqrt{\sum_i{\left|{x}_i\right|}^2} $$](../images/515226_1_En_5_Chapter/515226_1_En_5_Chapter_TeX_Equb.png)

*   ∞ norm, defined as the maximum of the absolute values of all elements of a vector:

    ![$$ {\left\Vert x\right\Vert}_{\infty }={\mathit{\max}}_i\left(\left|{x}_i\right|\right) $$](../images/515226_1_En_5_Chapter/515226_1_En_5_Chapter_TeX_Equc.png)

对于矩阵和张量，在将矩阵和张量展平成向量后，也可以使用前面的公式。在 TensorFlow 中，tf.norm(x，ord)函数可用于求解 L1、L2 和∞范数，其中 L1、L2 和∞范数的参数 ord 分别指定为 1、1 和 np.inf:

```py
In [13]: x = tf.ones([2,2])
tf.norm(x,ord=1) # L1 norm
Out[13]: <tf.Tensor: id=183, shape=(), dtype=float32, numpy=4.0>
In [14]: tf.norm(x,ord=2) # L2 norm
Out[14]: <tf.Tensor: id=189, shape=(), dtype=float32, numpy=2.0>
In [15]: import numpy as np
tf.norm(x,ord=np.inf) # ∞ norm
Out[15]: <tf.Tensor: id=194, shape=(), dtype=float32, numpy=1.0>

```

### 5.2.2 最大值、最小值、平均值和总和

tf.reduce_max、tf.reduce_min、tf.reduce_mean 和 tf.reduce_sum 函数可用于获取某维或所有维中张量的最大值、最小值、平均值和总和。

考虑形状为[4，10]的张量，其中第一维表示样本的数量，第二维表示当前样本属于十个类别中的每一个的概率。每个样本概率的最大值可以通过 tf.reduce_max 函数获得:

```py
In [16]: x = tf.random.normal([4,10])
tf.reduce_max(x,axis=1) # get maximum value at 2nd dimension
Out[16]:<tf.Tensor: id=203, shape=(4,), dtype=float32, numpy=array([1.2410722 , 0.88495886, 1.4170984 , 0.9550192 ], dtype=float32)>

```

前面的代码返回一个长度为 4 的向量，它表示每个样本的最大概率值。类似地，我们可以找到每个样本的概率最小值，如下所示:

```py
In [17]: tf.reduce_min(x,axis=1) # get the minimum value at 2nd dimension
Out[17]:<tf.Tensor: id=206, shape=(4,), dtype=float32, numpy=array([-0.27862206, -2.4480672 , -1.9983795 , -1.5287997 ], dtype=float32)>

```

求每个样本的平均概率:

```py
In [18]: tf.reduce_mean(x,axis=1)
Out[18]:<tf.Tensor: id=209, shape=(4,), dtype=float32, numpy=array([ 0.39526337, -0.17684573, -0.148988  , -0.43544054], dtype=float32)>

```

当未指定轴参数时，tf.reduce_*函数将查找所有数据的最大值、最小值、平均值和总和:

```py
In [19]:x = tf.random.normal([4,10])
tf.reduce_max(x),tf.reduce_min(x),tf.reduce_mean(x)
Out [19]: (<tf.Tensor: id=218, shape=(), dtype=float32, numpy=1.8653786>,
 <tf.Tensor: id=220, shape=(), dtype=float32, numpy=-1.9751656>,
 <tf.Tensor: id=222, shape=(), dtype=float32, numpy=0.014772797>)

```

在求解误差函数时，可以通过 MSE 函数得到每个样本的误差，需要计算样本的平均误差。这里我们可以使用 tf.reduce_mean 函数如下:

```py
In [20]:
out = tf.random.normal([4,10]) # Simulate output
y = tf.constant([1,2,2,0]) # Real labels
y = tf.one_hot(y,depth=10) # One-hot encoding
loss = keras.losses.mse(y,out) # Calculate loss of each sample
loss = tf.reduce_mean(loss) # Calculate mean loss
loss
Out[20]:
<tf.Tensor: id=241, shape=(), dtype=float32, numpy=1.1921183>

```

与 tf.reduce_mean 函数类似，sum 函数 tf.reduce_sum(x，axis)可以计算张量在相应轴上的所有特征的和:

```py
In [21]:out = tf.random.normal([4,10])
tf.reduce_sum(out,axis=-1) # Calculate sum along the last dimension
Out[21]:<tf.Tensor: id=303, shape=(4,), dtype=float32, numpy=array([-0.588144 ,  2.2382064,  2.1582587,  4.962141 ], dtype=float32)>

```

另外，为了获得张量的最大值或最小值，我们有时也想获得相应的位置指数。例如，对于分类任务，我们需要知道最大概率的位置索引，这通常被用作预测类别。考虑具有十个类别的分类问题，我们得到形状为[2，10]的输出张量，其中 2 表示两个样本，10 表示属于十个类别的概率。由于元素的位置索引代表了当前样本属于该类别的概率，所以我们经常使用最大概率对应的索引作为预测类别。

```py
In [22]:out = tf.random.normal([2,10])
out = tf.nn.softmax(out, axis=1) # Use softmax to convert to probability
out
Out[22]:<tf.Tensor: id=257, shape=(2, 10), dtype=float32, numpy=
array([[0.18773547, 0.1510464 , 0.09431915, 0.13652141, 0.06579739,
        0.02033597, 0.06067333, 0.0666793 , 0.14594753, 0.07094406],
       [0.5092072 , 0.03887136, 0.0390687 , 0.01911005, 0.03850609,
        0.03442522, 0.08060656, 0.10171875, 0.08244187, 0.05604421]],
       dtype=float32)>

```

以第一个样本为例，可以看出概率最高(0.1877)的指数为 0。因为每个指标上的概率代表样本属于该类别的概率，所以第一个样本属于 0 类的概率最大。因此，第一个样本最有可能属于类别 0。这是一个典型的应用，其中需要求解最大值的指数。

我们可以用 tf.argmax(x，axis)和 tf.argmin(x，axis)来求 x 在轴参数上的最大值和最小值的索引。例如:

```py
In [23]:pred = tf.argmax(out, axis=1)
pred
Out[23]:<tf.Tensor: id=262, shape=(2,), dtype=int64, numpy=array([0, 0], dtype=int64)>

```

可以看出，两个样本的最大概率出现在索引 0 上，因此最有可能的是，它们都属于类别 0。我们可以使用类别 0 作为两个样本的预测类别。

## 5.3 张量比较

为了得到准确率等分类度量，一般需要将预测结果与真实标签进行比较。考虑 100 个样本的预测结果，可以通过 tf.argmax 得到预测的类别。

```py
In [24]:out = tf.random.normal([100,10])
out = tf.nn.softmax(out, axis=1) # Convert to probability
pred = tf.argmax(out, axis=1) # Find corresponding category
Out[24]:<tf.Tensor: id=272, shape=(100,), dtype=int64, numpy=
array([0, 6, 4, 3, 6, 8, 6, 3, 7, 9, 5, 7, 3, 7, 1, 5, 6, 1, 2, 9, 0, 6,
       5, 4, 9, 5, 6, 4, 6, 0, 8, 4, 7, 3, 4, 7, 4, 1, 2, 4, 9, 4,...

```

pred 变量保存 100 个样本的预测类别。我们将它们与真实标签进行比较，以获得一个布尔张量，表示每个样本是否预测了正确的样本。tf.equal(a，b)(或 tf.math.equal(a，b)，二者等价)函数可以比较两个张量是否相等，例如:

```py
In [25]: # Simiulate the true labels
y = tf.random.uniform([100],dtype=tf.int64,maxval=10)
Out[25]:<tf.Tensor: id=281, shape=(100,), dtype=int64, numpy=
array([0, 9, 8, 4, 9, 7, 2, 7, 6, 7, 3, 4, 2, 6, 5, 0, 9, 4, 5, 8, 4, 2,
       5, 5, 5, 3, 8, 5, 2, 0, 3, 6, 0, 7, 1, 1, 7, 0, 6, 1, 2, 1, 3, ...
In [26]:out = tf.equal(pred,y) # Compare true and prediction
Out[26]:<tf.Tensor: id=288, shape=(100,), dtype=bool, numpy=
array([False, False, False, False, True, False, False, False, False,
       False, False, False, False, False, True, False, False, True,...

```

tf.equal 函数将比较结果作为布尔张量返回。我们只需要计算真实元素的数量，就可以得到正确的预测数量。为了实现这一点，我们先将布尔类型转换为整数张量，即 True 对应 1，False 对应 0，然后将 1 的个数求和，得到比较结果中 True 元素的个数:

```py
In [27]:out = tf.cast(out, dtype=tf.float32) # convert to int type
correct = tf.reduce_sum(out) # get the number of True elements
Out[27]:<tf.Tensor: id=293, shape=(), dtype=float32, numpy=12.0>

```

可以看出，我们随机生成的预测数据中，正确预测的数量是 12 个，所以其准确率为:

![$$ accuracy=\frac{12}{100}=12\% $$](../images/515226_1_En_5_Chapter/515226_1_En_5_Chapter_TeX_Equd.png)

这是随机预测模型的正常水平。

除 tf.equal 函数外，其他常用的比较函数如表 [5-1](#Tab1) 所示。

表 5-1

常见比较函数

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

功能

 | 

比较逻辑

 |
| --- | --- |
| tf.math.greater | *甲* > *乙* |
| tf.math.less | *甲* < *乙* |
| tf.math.greater_equal | *a**b* |
| tf.math.less_equal | *a*≤T2】【b】 |
| tf.math.not_equal | *甲*≦*乙* |
| tf.math.is_nan | *一* = *楠* |

## 5.4 填写和复制

### 填充

图像的高度和宽度以及序列信号的长度可以不同。为了便于网络的并行计算，需要将不同长度的数据扩展到相同的长度。我们之前介绍过，可以通过复制来增加数据的长度。但是，重复复制数据会破坏原有的数据结构，不适合某些情况。常见的做法是在数据的开头或结尾填入足够数量的特定值。这些特定值(例如，0)通常表示无效的含义。这种操作称为填充。

考虑一个两句话的张量，每个单词用一个数字代码表示，比如 1 代表 I，2 代表 like，等等。第一句是“我喜欢今天的天气。”我们假设句号编码为[1，2，3，4，5，6]。第二句是“我也是”，编码为[7，8，1，6]。为了将这两个句子存储在一个张量中，我们需要保持这两个句子的长度一致，即需要将第二个句子的长度扩展为 6。常见的填充方案是在第二句话的末尾填充若干个零，即[7，8，1，6，0，0]。现在这两个句子可以堆叠起来，组合成一个形状为[2，6]的张量。

填充操作可以通过 tf.pad(x，paddings)函数实现。参数 paddings 是多个嵌套方案的列表，格式为[ *左填充*，*右填充*。例如， *paddings* = [[0，0]，[2，1]，[1，2]]表示第一维度不填充，第二维度左边(开头)填充两个单位，第二维度右边(结尾)填充一个单位，第三维度左边填充一个单位，第三维度右边填充两个单位。考虑到前面两个句子的例子，第二个句子的第一维右边需要填充两个单位，paddings 方案为[[0，2]]:

```py
In [28]:a = tf.constant([1,2,3,4,5,6]) # 1st sentence
b = tf.constant([7,8,1,6]) # 2nd sentence
b = tf.pad(b, [[0,2]]) # Pad two 0's in the end of 2nd sentence
b
Out[28]:<tf.Tensor: id=3, shape=(6,), dtype=int32, numpy=array([7, 8, 1, 6, 0, 0])>

```

填充后，两个张量的形状是一致的，我们可以把它们叠加在一起。代码如下:

```py
In [29]:tf.stack([a,b],axis=0) # Stack a and b
Out[29]:<tf.Tensor: id=5, shape=(2, 6), dtype=int32, numpy=
array([[1, 2, 3, 4, 5, 6],
       [7, 8, 1, 6, 0, 0]])>

```

在自然语言处理中，需要加载不同长度的句子。有的句子比较短，比如只有十个字，有的句子比较长，比如 100 多个字。为了能够保存在同一个张量中，一般选择一个能够覆盖大部分句子长度的阈值，比如 80 个单词。对于少于 80 个单词的句子，我们在句尾用 0 填充。对于超过 80 个单词的句子，我们通过删除结尾的一些单词将句子截短为 80 个单词。我们将以 IMDB 数据集为例，演示如何将长度不等的句子转换成长度相等的结构。代码如下:

```py
In [30]:total_words = 10000 # Set word number
max_review_len = 80 # Maximum length for each sentence
embedding_len = 100 # Word vector length
# Load IMDB dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)
# Pad or truncate sentences to the same length with end padding and truncation
x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len,truncating='post',padding='post')
x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len,truncating='post',padding='post')
print(x_train.shape, x_test.shape)
Out[30]: (25000, 80) (25000, 80)

```

在前面的代码中，我们将语句 max_review_len 的最大长度设置为 80 个单词。通过 keras . preprocessing . sequence . pad _ sequences 函数，我们可以快速完成填充和截断实现。以其中一个句子为例，变换后的向量是这样的:

```py
[   1  778  128   74   12  630  163   15    4 1766 7982 1051    2   32
   85  156   45   40  148  139  121  664  665   10   10 1361  173    4
  749    2   16 3804    8    4  226   65   12   43  127   24    2   10
   10    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0]

```

我们可以看到，句子的最后一部分用 0 填充，因此句子的长度正好是 80。其实在句子长度不够的情况下，我们也可以选择填充句子的开头部分。经过处理后，所有句子长度都变成 80，这样训练集可以统一存储在 shape [25000，80]的张量中，测试集可以存储在 shape [25000，80]的张量中。

下面介绍一个同时填写多个维度的例子。考虑填充图像的高度和宽度尺寸。如果我们有尺寸为 28 × 28 的图片，神经网络的输入层形状为 32 × 32，我们需要填充图像以获得 32 × 32 的形状。我们可以选择在图像矩阵的上、下、左、右各填充 2 个单元，如图 [5-2](#Fig2) 所示。

![../images/515226_1_En_5_Chapter/515226_1_En_5_Fig2_HTML.png](../images/515226_1_En_5_Chapter/515226_1_En_5_Fig2_HTML.png)

图 5-2

图像填充示例

前述填充方案可以如下实现:

```py
In [31]:
x = tf.random.normal([4,28,28,1])
# Pad two units at each edge of the image
tf.pad(x,[[0,0],[2,2],[2,2],[0,0]])
Out[31]:
<tf.Tensor: id=16, shape=(4, 32, 32, 1), dtype=float32, numpy=
array([[[[ 0\.        ],
         [ 0\.        ],
         [ 0\.        ],...

```

经过填充操作后，图片大小变为 32 × 32，满足了神经网络的输入要求。

### 副本

在维度转换一节中，我们介绍了复制长度为 1 的维度的 tf.tile 函数。实际上，tf.tile 函数可用于在任何维度上重复复制多个数据副本。例如，对于形状为[4，32，32，3]的图像数据，如果复制方案为 multiples=[2，3，3，1]，则表示通道维度不复制，高度和宽度维度复制三份，图像编号维度复制两份。实现如下:

```py
In [32]:x = tf.random.normal([4,32,32,3])
tf.tile(x,[2,3,3,1])
Out[32]:<tf.Tensor: id=25, shape=(8, 96, 96, 3), dtype=float32, numpy=
array([[[[ 1.20957184e+00,  2.82766962e+00,  1.65782201e+00],
         [ 3.85402292e-01,  2.00732923e+00, -2.79068202e-01],
         [-2.52583921e-01,  7.82584965e-01,  7.56870627e-01],...

```

## 5.5 数据限制

考虑如何实现非线性激活函数 ReLU。事实上，它可以通过简单的数据限制操作来实现，其中元素的范围被限制为 *x* ∈ [0，+∞)。

在 TensorFlow 中，可以通过 tf.maximum (x，a)设置数据的下限，也就是可以通过 tf.minimum (x，a)设置数据的上限。

```py
In [33]:x = tf.range(9)
tf.maximum(x,2) # Set lower limit of x to 2
Out[33]:<tf.Tensor: id=48, shape=(9,), dtype=int32, numpy=array([2, 2, 2, 3, 4, 5, 6, 7, 8])>
In [34]:tf.minimum(x,7) # Set x upper limit to 7
Out[34]:<tf.Tensor: id=41, shape=(9,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 7])>

```

基于 tf.maximum 函数，我们可以如下实现 ReLU:

```py
def relu(x): # ReLU function
    return tf.maximum(x,0.) # Set lower limit of x to be 0

```

通过组合 tf.maximum(x，a)和 tf.minimum(x，b)，可以同时限定数据的上下边界，即 *x* ∈ [ *a* ， *b* 。

```py
In [35]:x = tf.range(9)
tf.minimum(tf.maximum(x,2),7) # Set x range to be [2, 7]
Out[35]:<tf.Tensor: id=57, shape=(9,), dtype=int32, numpy=array([2, 2, 2, 3, 4, 5, 6, 7, 7])>

```

更方便的是，我们可以使用 tf.clip_by_value 函数来实现上下限幅:

```py
In [36]:x = tf.range(9)
tf.clip_by_value(x,2,7) # Set x range to be [2, 7]
Out[36]:<tf.Tensor: id=66, shape=(9,), dtype=int32, numpy=array([2, 2, 2, 3, 4, 5, 6, 7, 7])>

```

## 5.6 高级操作

前面的大多数函数都很常见，很容易理解。接下来，我们将介绍一些常用但稍微复杂一些的函数。

### 收集 tf

tf.gather 函数可以根据索引号收集数据。考虑年级书的例子。假设有四个班，每个班 35 个学生，共八个科目，年级书的张量形状为[4，35，8]。

```py
x = tf.random.uniform([4,35,8],maxval=100,dtype=tf.int32)

```

现在需要收集一、二班的年级书。我们可以给出想要收集的类的索引号(例如[0，1])，并指定类的维数(例如 axis = 0)。然后通过 tf.gather 函数收集数据。

```py
In [38]:tf.gather(x,[0,1],axis=0) # Collect data for 1st and 2nd classes
Out[38]:<tf.Tensor: id=83, shape=(2, 35, 8), dtype=int32, numpy=
array([[[43, 10, 93, 85, 75, 87, 28, 19],
        [52, 17, 44, 88, 82, 54, 16, 65],
        [98, 26,  1, 47, 59,  3, 59, 70],...

```

事实上，通过切片可以更方便地实现前面的要求。但是对于不规则的索引方式，比如需要抽查 1、4、9、12、13、27 名学生的年级数据，切片方式就不适合了。tf.gather 函数就是针对这种情况设计的，使用起来更方便。实现如下:

```py
In [39]: # Collect the grade of students 1,4,9,12,13 and 27
tf.gather(x,[0,3,8,11,12,26],axis=1)
Out[39]:<tf.Tensor: id=87, shape=(4, 6, 8), dtype=int32, numpy=
array([[[43, 10, 93, 85, 75, 87, 28, 19],
        [74, 11, 25, 64, 84, 89, 79, 85],...

```

如果需要汇总所有学生的第三、第五科成绩，可以指定科目维度 axis = 2，实现如下:

```py
# Collect the grades of the 3rd and 5th subjects of all students
In [40]:tf.gather(x,[2,4],axis=2)
Out[40]:<tf.Tensor: id=91, shape=(4, 35, 2), dtype=int32, numpy=
array([[[93, 75],
        [44, 82],
        [ 1, 59],...

```

可以看出 tf.gather 非常适合索引号没有规律的情况。索引号可以不按顺序排列，收集的数据也将按相应的顺序排列。例如:

```py
In [41]:a=tf.range(8)
a=tf.reshape(a,[4,2])
Out[41]:<tf.Tensor: id=115, shape=(4, 2), dtype=int32, numpy=
array([[0, 1],
       [2, 3],
       [4, 5],
       [6, 7]])>
In [42]:tf.gather(a,[3,1,0,2],axis=0) # Collect element 4,2,1,3
Out[42]:<tf.Tensor: id=119, shape=(4, 2), dtype=int32, numpy=
array([[6, 7],
       [2, 3],
       [0, 1],
       [4, 5]])>

```

我们会把问题变得复杂一点。如果我们想检查[2，3]班[3，4，6，27]学生的学科成绩，可以通过组合多个 tf.gather 操作来实现。首先提取类[2，3]的数据:

```py
In [43]:
students=tf.gather(x,[1,2],axis=0) # Collect data for class 2 and 3
Out[43]:<tf.Tensor: id=227, shape=(2, 35, 8), dtype=int32, numpy=
array([[[ 0, 62, 99,  7, 66, 56, 95, 98],...

```

然后，我们提取所选学生的相应数据:

```py
In [44]:
tf.gather(students,[2,3,5,26],axis=1) # Collect data for students 3,4,6,27
Out[44]:<tf.Tensor: id=231, shape=(2, 4, 8), dtype=int32, numpy=
array([[[69, 67, 93,  2, 31,  5, 66, 65], ...

```

现在我们得到了形状为[2，4，8]的选定张量。

这次要抽查二班第二同学的所有科目，三班第三同学的所有科目，四班第四同学的所有科目。那么它是如何工作的呢？可以用笨拙的方式逐个手动提取数据。先提取第一个采样点的数据:*x*【1，1】。

```py
In [45]: x[1,1]
Out[45]:<tf.Tensor: id=236, shape=(8,), dtype=int32, numpy=array([45, 34, 99, 17,  3,  1, 43, 86])>

```

然后提取第二个采样点*x*【2，2】的数据和第三个采样点*x*【3，3】的数据，最后将采样结果合并在一起。

```py
In [46]: tf.stack([x[1,1],x[2,2],x[3,3]],axis=0)
Out[46]:<tf.Tensor: id=250, shape=(3, 8), dtype=int32, numpy=
array([[45, 34, 99, 17,  3,  1, 43, 86],
       [11, 25, 84, 95, 97, 95, 69, 69],
       [ 0, 89, 52, 29, 76,  7,  2, 98]])>

```

使用前面的方法，我们可以正确地获得 shape [3，8]的结果，其中 3 代表采样点数，4 代表每个采样点的数据。最大的问题是采样是手工串行进行的，计算效率极低。有没有更好的方法来实现这一点？

### 5.6.2 tf.gather_nd

使用 tf.gather_nd 函数，我们可以通过指定每个采样点的多维坐标来采样多个点。回到前面的挑战，我们要抽查二班第二个同学的所有科目，三班第三个同学的所有科目，四班第四个同学的所有科目。那么三个采样点的索引坐标就可以记录为[1，1]，[2，2]，*，* [3，3]，我们就可以把这个采样方案组合成一个列表[[1，1]，[2，2]，[3，3]]。

```py
In [47]:
tf.gather_nd(x,[[1,1],[2,2],[3,3]])
Out[47]:<tf.Tensor: id=256, shape=(3, 8), dtype=int32, numpy=
array([[45, 34, 99, 17,  3,  1, 43, 86],
       [11, 25, 84, 95, 97, 95, 69, 69],
       [ 0, 89, 52, 29, 76,  7,  2, 98]])>

```

结果与串行采样方法一致，且实现更加简洁高效。

一般来说，当使用 tf.gather_nd 对多个样本进行采样时，例如，如果我们想要对类 *i* ，学生 *j* ，主题 *k* 进行采样，我们可以使用表达式[...，[ *i* ， *j* ， *k* ，...].内部列表包含每个采样点的相应索引坐标，例如:

```py
In [48]:
tf.gather_nd(x,[[1,1,2],[2,2,3],[3,3,4]])
Out[48]:<tf.Tensor: id=259, shape=(3,), dtype=int32, numpy=array([99, 95, 76])>

```

在前面的代码中，我们提取了 1 班学生 2 的科目 1、2 班学生 3 的科目 2 和 3 班学生 3 的科目 4 的成绩。总共有三个年级的数据，结果总结成一个张量，形状为[3]。

### 5.6.3 tf.boolean_mask

除了通过给定的索引号进行采样之外，还可以通过给定的掩码进行采样。继续以形状为[4，35，8]的年级书张量为例；这次我们使用掩码方法进行数据提取。

考虑类维度中的采样，设置对应的掩码为:

![$$ mask=\left[ True, False, False, True\right] $$](../images/515226_1_En_5_Chapter/515226_1_En_5_Chapter_TeX_Eque.png)

即采样第一类和第四类。使用函数 tf.boolean_mask(x，mask，axis)，可以根据掩码方案在相应的轴上执行采样，具体实现如下:

```py
In [49]:
tf.boolean_mask(x,mask=[True, False,False,True],axis=0)
Out[49]:<tf.Tensor: id=288, shape=(2, 35, 8), dtype=int32, numpy=
array([[[43, 10, 93, 85, 75, 87, 28, 19],...

```

请注意，遮罩的长度必须与相应尺寸的长度相同。如果我们在类维中采样，我们必须指定长度为 4 的掩码，以指定四个类是否在采样。

如果对八个对象进行掩码采样，我们需要将掩码采样方案设置为

![$$ mask=\left[ True, False, False, True, True, False, False, True\right] $$](../images/515226_1_En_5_Chapter/515226_1_En_5_Chapter_TeX_Equf.png)

也就是说，对第一、第四、第五和第八个受试者进行采样:

```py
In [50]:
tf.boolean_mask(x,mask=[True,False,False,True,True,False,False,True],axis=2)
Out[50]:<tf.Tensor: id=318, shape=(4, 35, 4), dtype=int32, numpy=
array([[[43, 85, 75, 19],...

```

不难发现，这里 tf.boolean_mask 的用法其实和 tf.gather 很像，只不过一个是用 mask 方法采样，另一个是直接给索引号。

现在让我们考虑一个类似于 tf.gather_nd 的多维掩码采样方法。为了便于演示，我们将班级数量减少到两个，学生数量减少到三个。即一个班只有三个学生，张量形状为[2，3，8]。如果我们想对第一个班的学生 1 到 2 和第二个班的学生 2 到 3 进行采样，我们可以使用 tf.gather_nd:

```py
In [51]:x = tf.random.uniform([2,3,8],maxval=100,dtype=tf.int32)
tf.gather_nd(x,[[0,0],[0,1],[1,1],[1,2]])
Out[51]:<tf.Tensor: id=325, shape=(4, 8), dtype=int32, numpy=
array([[52, 81, 78, 21, 50,  6, 68, 19],
       [53, 70, 62, 12,  7, 68, 36, 84],
       [62, 30, 52, 60, 10, 93, 33,  6],
       [97, 92, 59, 87, 86, 49, 47, 11]])>

```

总共有四个学生的结果被取样，形状为[4，8]。

如果我们使用面具，我们如何表达它？表 [5-2](#Tab2) 表示相应位置的采样:

表 5-2

使用掩模法采样

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
|   | 

学生 0

 | 

学生 1

 | 

学生 2

 |
| --- | --- | --- | --- |
| 0 类 | 真实的 | 真实的 | 错误的 |
| 1 类 | 错误的 | 真实的 | 真实的 |

因此，通过该表，可以很好地表达采用掩膜法的采样方案。代码实现如下:

```py
In [52]:
tf.boolean_mask(x,[[True,True,False],[False,True,True]])
Out[52]:<tf.Tensor: id=354, shape=(4, 8), dtype=int32, numpy=
array([[52, 81, 78, 21, 50,  6, 68, 19],
       [53, 70, 62, 12,  7, 68, 36, 84],
       [62, 30, 52, 60, 10, 93, 33,  6],
       [97, 92, 59, 87, 86, 49, 47, 11]])>

```

结果和 tf.gather_nd 方法完全一样。可以看出，tf.boolean_mask 方法可用于一维和多维采样。

前面三个操作比较常用，尤其是 tf.gather 和 tf.gather_nd。下面添加了三个额外的高级操作。

### 在哪里

通过 tf.where(cond，a，b)函数，我们可以根据 cond 条件的真假情况从参数 a 或 b 中读取数据。条件确定规则如下:

![../images/515226_1_En_5_Chapter/515226_1_En_5_Figa_HTML.png](../images/515226_1_En_5_Chapter/515226_1_En_5_Figa_HTML.png)

其中 *i* 是张量的元素索引。返回的张量大小与 a 和 b 一致，当*cond*<sub>T5】I</sub>对应位置为真时，数据从 *a* <sub>*i*</sub> 复制到 *o* <sub>*i*</sub> 。否则，将数据从 *b* <sub>*i*</sub> 复制到 *o* <sub>*i*</sub> 。考虑从所有 1 和 0 的两个张量 *A* 和 *B* 中提取数据，其中中*cond*<sub>*I*</sub>从 *A* 的对应位置提取元素 1，否则从 *B* 的对应位置提取元素 0。代码如下:

```py
In [53]:
a = tf.ones([3,3])  # Tensor A
b = tf.zeros([3,3]) # Tensor B
# Create condition matrix
cond = tf.constant([[True,False,False],[False,True,False],[True,True,False]])
tf.where(cond,a,b)
Out[53]:<tf.Tensor: id=384, shape=(3, 3), dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [1., 1., 0.]], dtype=float32)>

```

可以看出，返回张量中 1 的位置都来自张量 *A* ，返回张量中 0 的位置来自张量 *B* 。

当参数 a=b=None 时，即不指定 a 和 b 参数；tf.where 返回 cond 张量中所有真元素的索引坐标。考虑下面的 cond 张量:

```py
In [54]: cond
Out[54]:<tf.Tensor: id=383, shape=(3, 3), dtype=bool, numpy=
array([[ True, False, False],
       [False,  True, False],
       [ True,  True, False]])>

```

True 总共出现四次，每个 True 元素所在位置的索引分别为[0，0]、[1，1]、[2，0]和[2，1]。这些元素的索引坐标可以通过 tf.where(cond)的形式直接获得，如下所示:

```py
In [55]:tf.where(cond)
Out[55]:<tf.Tensor: id=387, shape=(4, 2), dtype=int64, numpy=
array([[0, 0],
       [1, 1],
       [2, 0],
       [2, 1]], dtype=int64)>

```

那么这个有什么用呢？考虑一个场景，我们需要提取一个张量中所有的正数据和索引。首先构造张量 a，通过比较运算得到所有正数的位置掩码:

```py
In [56]:x = tf.random.normal([3,3]) # Create tensor a
Out[56]:<tf.Tensor: id=403, shape=(3, 3), dtype=float32, numpy=
array([[-2.2946844 ,  0.6708417 , -0.5222212 ],
       [-0.6919401 , -1.9418817 ,  0.3559235 ],
       [-0.8005251 ,  1.0603906 , -0.68819374]], dtype=float32)>

```

通过比较运算，我们得到所有正数的掩码:

```py
In [57]:mask=x>0 # equivalent to tf.math.greater()
mask
Out[57]:<tf.Tensor: id=405, shape=(3, 3), dtype=bool, numpy=
array([[False,  True, False],
       [False, False,  True],
       [False,  True, False]])>

```

通过 tf 提取掩膜张量中真元素的索引坐标，其中:

```py
In [58]:indices=tf.where(mask) # Extract all element greater than 0
Out[58]:<tf.Tensor: id=407, shape=(3, 2), dtype=int64, numpy=
array([[0, 1],
       [1, 2],
       [2, 1]], dtype=int64)>

```

拿到索引后，我们可以通过 tf.gather_nd 恢复所有的正元素:

```py
In [59]:tf.gather_nd(x,indices) # Extract all positive elements
Out[59]:<tf.Tensor: id=410, shape=(3,), dtype=float32, numpy=array([0.6708417, 0.3559235, 1.0603906], dtype=float32)>

```

其实在我们得到了 mask 之后，也可以直接通过 tf.boolean_mask 得到所有的正元素:

```py
In [60]:tf.boolean_mask(x,mask) # Extract all positive elements
Out[60]:<tf.Tensor: id=439, shape=(3,), dtype=float32, numpy=array([0.6708417, 0.3559235, 1.0603906], dtype=float32)>

```

通过前面的一系列比较，我们可以直观地感受到这个函数有很大的实际应用价值，也可以深入了解它们的本质，以便能够以更灵活、简单、高效的方式实现我们的目的。

### 分散 _nd

TF . scatter _ nd(indexes，updates，shape)函数可以高效地刷新部分张量数据，但该函数只能对所有 0 张量进行刷新操作，因此可能需要结合其他操作来实现非零张量的数据刷新功能。

图 [5-3](#Fig3) 给出了一维全零张量的刷新计算原理。白板的形状由 shape 参数表示，要刷新的数据的索引号由 indexes 表示，updates 参数包含新数据。TF . scatter _ nd(indexes，updates，shape)函数根据 indexes 给出的索引位置将新数据写入全零张量，并返回更新后的结果张量。

![../images/515226_1_En_5_Chapter/515226_1_En_5_Fig3_HTML.png](../images/515226_1_En_5_Chapter/515226_1_En_5_Fig3_HTML.png)

图 5-3

用于刷新数据的 scatter_nd 函数

我们实现了图 [5-3](#Fig3) 中张量的刷新示例，如下所示:

```py
In [61]: # Create indices for refreshing data
indices = tf.constant([[4], [3], [1], [7]])
# Create data for filling the indices
updates = tf.constant([4.4, 3.3, 1.1, 7.7])
# Refresh data for all 0 vector of length 8
tf.scatter_nd(indices, updates, [8])
Out[61]:<tf.Tensor: id=467, shape=(8,), dtype=float32, numpy=array([0\. , 1.1, 0\. , 3.3, 4.4, 0\. , 0\. , 7.7], dtype=float32)>

```

可以看出，在长度为 8 的全零张量上，相应位置的数据用来自更新的值填充。

考虑一个三维张量的例子。如图 [5-4](#Fig4) 所示，全零张量的形状是一个共有四个通道的特征图，每个通道的大小为 4 × 4。新的数据更新有一个形状[2，4，4]，需要写入索引[1，3]。

![../images/515226_1_En_5_Chapter/515226_1_En_5_Fig4_HTML.png](../images/515226_1_En_5_Chapter/515226_1_En_5_Fig4_HTML.png)

图 5-4

3D 张量数据刷新

我们将新的特征映射写入现有张量，如下所示:

```py
In [62]:
indices = tf.constant([[1],[3]])
updates = tf.constant([
    [[5,5,5,5],[6,6,6,6],[7,7,7,7],[8,8,8,8]],
    [[1,1,1,1],[2,2,2,2],[3,3,3,3],[4,4,4,4]]
])
tf.scatter_nd(indices,updates,[4,4,4])
Out[62]:<tf.Tensor: id=477, shape=(4, 4, 4), dtype=int32, numpy=
array([[[0, 0, 0, 0],
        [0, 0, 0, 0],
        [0, 0, 0, 0],
        [0, 0, 0, 0]],
       [[5, 5, 5, 5], # New data 1
        [6, 6, 6, 6],
        [7, 7, 7, 7],
        [8, 8, 8, 8]],
       [[0, 0, 0, 0],
        [0, 0, 0, 0],
        [0, 0, 0, 0],
        [0, 0, 0, 0]],
       [[1, 1, 1, 1], # New data 2
        [2, 2, 2, 2],
        [3, 3, 3, 3],
        [4, 4, 4, 4]]])>

```

可以看出，数据被刷新到第二和第四通道特征图上。

### 5.6.6 tf.网栅

tf.meshgrid 函数可以方便地生成二维网格的采样点坐标，方便可视化等应用。考虑两个自变量 x 和 y 的 Sinc 函数为:

![$$ z=\frac{sinsin\ \left({x}^2+{y}^2\right)\ }{x^2+{y}^2} $$](../images/515226_1_En_5_Chapter/515226_1_En_5_Chapter_TeX_Equh.png)

如果我们需要绘制区间*x*∈[8，8]，*y*∈[8，8]的 Sinc 函数的 3D 曲面，如图 [5-5](#Fig5) 所示，我们首先需要生成 x 轴和 y 轴的网格点坐标集，这样就可以通过 Sinc 函数 z 的表达式计算出函数在每个位置的输出值，我们可以通过下式生成 10000 个坐标采样点:

```py
points = []
for x in range(-8,8,100): # Loop to generate 100 sampling point for x-axis
for y in range(-8,8,100): # Loop to generate 100 sampling point for y-axis
        z = sinc(x,y)
        points.append([x,y,z])

```

显然，这种串行采样方法效率极低。有没有简单高效的生成网格坐标的方法？答案是 tf.meshgrid 函数。

![../images/515226_1_En_5_Chapter/515226_1_En_5_Fig5_HTML.jpg](../images/515226_1_En_5_Chapter/515226_1_En_5_Fig5_HTML.jpg)

图 5-5

正弦函数

通过分别在 x 轴和 y 轴上采样 100 个数据点，可以使用 tf.meshgrid(x，y)来生成这 10，000 个数据点的张量数据，并将它们保存在形状为[100，100，2]的张量中。为了计算方便，tf.meshgrid 在轴=二维切割后会返回两个张量，其中张量 *A* 包含所有点的 x 坐标，张量 *B* 包含所有点的 y 坐标。

```py
In [63]:
x = tf.linspace(-8.,8,100) # x-axis
y = tf.linspace(-8.,8,100) # y-axis
x,y = tf.meshgrid(x,y)
x.shape,y.shape
Out[63]: (TensorShape([100, 100]), TensorShape([100, 100]))

```

使用生成的网格点坐标张量，Sinc 函数在 TensorFlow 中实现如下:

```py
z = tf.sqrt(x**2+y**2)
z = tf.sin(z)/z  # sinc function

```

matplotlib 库可以用来绘制函数的 3D 曲面，如图 [5-5](#Fig5) 所示。

```py
import matplotlib
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure()
ax = Axes3D(fig)
# Plot Sinc function
ax.contour3D(x.numpy(), y.numpy(), z.numpy(), 50)
plt.show()

```

## 5.7 加载经典数据集

到目前为止，我们已经学习了常见的张量运算，并准备实现大部分深度网络。最后，我们将用一个以张量形式实现的分类网络模型来完成这一章。在此之前，我们先正式介绍一下，对于常用的经典数据集，如何使用 TensorFlow 提供的工具方便地加载数据集。对于加载自定义数据集，我们将在后续章节中介绍。

在 TensorFlow 中，keras.datasets 模块提供了常用经典数据集的自动下载、管理、加载和转换功能，以及相应的数据集对象，这有助于多线程、预处理、混排和批量训练。

一些常用的经典数据集:

*   波士顿住房:波士顿住房价格趋势数据集，用于训练和测试回归模型。

*   CIFAR10/100:用于图片分类任务的真实图片数据集。

*   MNIST/时尚 _MNIST:一个手写的数字图片数据集，用于图片分类任务。

*   IMDB:情感分类任务数据集，用于文本分类任务。

这些数据集在机器学习或深度学习中使用非常频繁。对于新提出的算法，一般倾向于在经典数据集上测试，然后尝试迁移到更大更复杂的数据集。

我们可以使用 datasets.xxx.load_data()函数自动加载经典数据集，其中 xxx 代表具体的数据集名称，如“CIFAR10”和“MNIST”。TensorFlow 将数据缓存在。keras/datasets 文件夹默认在用户目录下，如图 [5-6](#Fig6) 所示。用户不需要关心数据集是如何保存的。如果当前数据集不在缓存中，将自动从网络下载、解压缩和加载该数据集。如果它已经在缓存中，加载将自动完成。例如，要自动加载 MNIST 数据集:

```py
In [66]:
import  tensorflow as tf
from    tensorflow import keras
from    tensorflow.keras import datasets # Load dataset loading module
# Load MNIST dataset
(x, y), (x_test, y_test) = datasets.mnist.load_data()
print('x:', x.shape, 'y:', y.shape, 'x test:', x_test.shape, 'y test:', y_test)
Out [66]:
x: (60000, 28, 28) y: (60000,) x test: (10000, 28, 28) y test: [7 2 1 ... 4 5 6]

```

函数将以相应的格式返回数据。对于图像数据集 MNIST 和 CIFAR10，将返回两个元组。第一个元组保存训练数据 x 和 y 对象；第二个元组是测试数据 x_test 和 y_test 对象。所有数据都存储在 Numpy 数组容器中。

![../images/515226_1_En_5_Chapter/515226_1_En_5_Fig6_HTML.jpg](../images/515226_1_En_5_Chapter/515226_1_En_5_Fig6_HTML.jpg)

图 5-6

TensorFlow 经典数据集保存目录

将数据加载到内存后，需要将其转换为 Dataset 对象，以便利用 TensorFlow 提供的各种便利功能。Dataset.from_tensor_slices 可用于将训练数据图像 x 和标签 y 转换成数据集对象:

```py
# Convert to Dataset objects
train_db = tf.data.Dataset.from_tensor_slices((x, y))

```

将数据转换成 Dataset 对象后，我们一般需要为数据集添加一系列标准的处理步骤，比如随机洗牌、预处理、批量加载等。

### 洗牌

使用 Dataset.shuffle(buffer_size)函数，我们可以随机地对 Dataset 对象进行混排，以防止在每次训练过程中按照固定的顺序生成数据，这样模型就不会“记住”标签信息。代码实现如下:

```py
train_db = train_db.shuffle(10000)

```

这里，buffer_size 参数指定缓冲池的大小，它通常被设置为一个较大的常数。调用数据集提供的这些实用函数将返回一个新的数据集对象。

![$$ db= db. step1\left(\right). step2\left(\right). step3.\left(\right) $$](../images/515226_1_En_5_Chapter/515226_1_En_5_Chapter_TeX_Equi.png)T2】

这种方法按顺序完成所有的数据处理步骤，实现起来非常方便。

### 批量培训

为了利用 GPU 的并行计算能力，网络计算过程中一般会同时计算多个样本。我们把这种训练方法叫做批量训练，一个批量的样本数叫做批量。为了从数据集一次性生成批量样本，需要将数据集设置为批量训练模式。实现如下:

```py
train_db = train_db.batch(128) # batch size is 128

```

这里 128 是批量参数，即一次并行计算 128 个样本。批量 sis 一般根据用户的 GPU 内存资源来设置。当 GPU 内存不足时，可以适当减小批量。

### 预处理

从 keras.datasets 加载的数据集格式在大多数情况下无法满足模型输入要求，因此需要根据用户的逻辑实现预处理步骤。Dataset 对象通过提供 map(func)实用函数可以非常方便地调用用户自定义的预处理逻辑，而预处理逻辑是在 func 函数中实现的。例如，下面的代码调用名为 preprocess 的函数来完成每个样本的预处理:

```py
# Preprocessing is implemented in the preprocess function
train_db = train_db.map(preprocess)

```

考虑到 MNIST 手写数字图片数据集，图像 x 从 keras.datasets 加载后。batch()操作有 shape [ *b* ，28，28]，其中像素用 0 到 255 的整数表示，标签形状为[ *b* ，数字编码。实际的神经网络输入通常需要将图像数据归一化到 0 附近的区间[0，1]或[1，1]。同时，根据网络设置，需要将 shape [28，28]的输入视图调整为合适的格式。对于标签信息，我们可以在预处理或误差计算期间选择一键编码。

这里，我们将 MNIST 图像数据映射到区间[0，1]，并将视图调整为[*b*，28∫28]。对于标签数据，我们选择在预处理函数中执行一键编码。预处理功能实现如下:

```py
def preprocess(x, y): # Customized preprocessing function
    x = tf.cast(x, dtype=tf.float32) / 255.
    x = tf.reshape(x, [-1, 28*28])     # flatten
    y = tf.cast(y, dtype=tf.int32)    # convert to int
    y = tf.one_hot(y, depth=10)    # one-hot encoding
    return x,y

```

### 5.7.4 新纪元培训

对于数据集对象，我们可以通过以下方式进行迭代:

```py
   for step, (x,y) in enumerate(train_db): # Iterate with step
or
    for x,y in train_db: # Iterate without step

```

每次返回的 x 和 y 对象是批量样本和标签。当对 train_db 的所有样本完成一次迭代时，for 循环终止。完成一批数据训练称为一个步骤，通过多个步骤完成整个训练集的一次迭代称为一个历元。在训练中，通常需要在数据集上迭代多个历元，以获得更好的训练结果。例如，20 个历元的固定训练实现如下:

```py
    for epoch in range(20): # Epoch number
        for step, (x,y) in enumerate(train_db): # Iteration step number
            # training...

```

此外，我们还可以设置一个数据集对象，以便数据集在退出之前将遍历多次，例如:

```py
train_db = train_db.repeat(20) # Dataset iteration 20 times

```

上述代码使 train_db 中的 for x，y 在退出前迭代 20 个历元。无论采用这几种方法中的哪一种，都能达到同样的效果。由于上一章已经完成了正向计算的实际计算，这里就跳过了。

## 5.8 动手操作 MNIST 数据集

我们已经介绍并实现了前向传播和数据集。现在让我们完成剩下的分类任务逻辑。在训练过程中，通过几个步骤后打印出来，可以有效地监控错误数据。代码如下:

```py
        # Print training error every 100 steps
        if step % 100 == 0:
            print(step, 'loss:', float(loss))

```

由于 loss 是张量类型的 TensorFlow，因此可以通过 float()函数将其转换为标准的 Python 浮点数。在几个步骤或几个历元训练之后，可以执行测试(验证)以获得模型的当前性能，例如:

```py
        if step % 500 == 0: # Do a test every 500 steps
            # evaluate/test

```

现在让我们用张量运算函数来完成精度的实际计算。首先考虑一个批量样本 x。网络的预测值可以通过如下正向计算获得:

```py
            for x, y in test_db: # Iterate through test dataset
                h1 = x @ w1 + b1 # 1st layer
                h1 = tf.nn.relu(h1) # Activation function
                h2 = h1 @ w2 + b2 # 2nd layer
                h2 = tf.nn.relu(h2) # Activation function
                out = h2 @ w3 + b3 # Output layer

```

预测值的形状是[ *b* ，10]。它表示样本属于每个类别的概率。我们根据 tf.argmax 函数选择出现最大概率的索引号，这是样本最可能的类别号:

```py
                # Select the max probability category
                pred = tf.argmax(out, axis=1)

```

由于 y 已经在预处理中被一键编码，我们可以类似地得到 y 的类别号:

```py
                y = tf.argmax(y, axis=1)

```

使用 tf.equal，我们可以比较两个结果是否相等:

```py
                correct = tf.equal(pred, y)

```

对结果中所有 True(转换为 1)元素的数量求和，这是正确的预测数量:

```py
                total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()

```

将正确的预测数除以测试总数，得到准确度，并将其打印出来，如下所示:

```py
             # Calcualte accuracy
            print(step, 'Evaluate Acc:', total_correct/total)

```

在用 20 个历元训练一个简单的三层神经网络后，我们在测试集上取得了 87.25%的准确率。如果我们使用复杂的神经网络模型并微调网络超参数，我们可以获得更好的精度。训练误差曲线如图 [5-7](#Fig7) 所示，测试精度曲线如图 [5-8](#Fig8) 所示。

![../images/515226_1_En_5_Chapter/515226_1_En_5_Fig8_HTML.jpg](../images/515226_1_En_5_Chapter/515226_1_En_5_Fig8_HTML.jpg)

图 5-8

MNIST 测试精度

![../images/515226_1_En_5_Chapter/515226_1_En_5_Fig7_HTML.jpg](../images/515226_1_En_5_Chapter/515226_1_En_5_Fig7_HTML.jpg)

图 5-7

MNIST 培训损失