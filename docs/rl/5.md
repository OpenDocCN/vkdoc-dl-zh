© Abhishek Nandy and Manisha Biswas  2018 Abhishek Nandy and Manisha BiswasReinforcement Learning [https://doi.org/10.1007/978-1-4842-3285-9_5](5.html)

# 5.使用 Keras、TensorFlow 和 ChainerRL 进行强化学习

Abhishek Nandy<sup class="calibre7">1 </sup> and Manisha Biswas<sup class="calibre7">2</sup> (1)Rm HIG L-2/4, Bldg Swaranika Co-Opt HSG, Kolkata, West Bengal, India (2)North 24 Parganas, West Bengal, India   This chapter covers using Keras with Reinforcement Learning and defines how Keras can be used for Deep Q Learning as well.

## Keras 是什么？

Keras is an open source frontend library for neural networks. We can say that it works as a backbone for the neural network, as it has very good capabilities for forming activation functions. Keras can run different deep learning frameworks as the backend. Keras runs with lots of deep learning frameworks. The way to change from one framework to another is to modify the keras.json file, which is located in the same directory where Keras is installed. The backend parameter needs to change as follows: { "backend" : "tensorflow" } You can change the parameter from TensorFlow to another framework if you want. In the JSON file, if you want to use it with Theano or CNTK, you can do so by changing the backend parameter. The structure of a keras.json file looks like this: {     "image_data_format": "channels_last",     "epsilon": 1e-07,     "floatx": "float32",     "backend": "tensorflow" } The flow of all the Keras frameworks is shown in Figure [5-1](#Fig1).![A454310_1_En_5_Fig1_HTML.jpg](img/Images/A454310_1_En_5_Fig1_HTML.jpg) Figure 5-1.Keras and its modification with different frameworks

## 使用 Keras 进行强化学习

This section covers installing Keras and shows an example of Reinforcement Learning. You first need to install the dependencies. The dependencies are as follows:

*   计算机编程语言
*   硬 1.0
*   皮尤游戏
*   Scikit-image

Let’s start installing Keras 1.0\. This example shows how to install Keras from the Anaconda environment: conda install -c jaikumarm keras It asks for permission to install the new packages. Choose yes to proceed, as shown in Figure [5-2](#Fig2).![A454310_1_En_5_Fig2_HTML.jpg](img/Images/A454310_1_En_5_Fig2_HTML.jpg) Figure 5-2.The updates to be installed When the package installation is successful and completed, you’ll see the information shown in Figure [5-3](#Fig3).![A454310_1_En_5_Fig3_HTML.jpg](img/Images/A454310_1_En_5_Fig3_HTML.jpg) Figure 5-3.The package installation is complete You can also install Keras in a different way too. This example shows you how to install it using pip3. First, use sudo apt update as follows: (universe) abhi@ubuntu:∼$ sudo apt-get update Then install pip3 as follows: sudo apt-get -y install python3-pip Figure [5-4](#Fig4) shows the installation process.![A454310_1_En_5_Fig4_HTML.jpg](img/Images/A454310_1_En_5_Fig4_HTML.jpg) Figure 5-4.Installing pip3 After the dependencies, you need to install Keras (see Figure [5-5](#Fig5)): (universe) abhi@ubuntu:∼$ sudo pip3 install keras ![A454310_1_En_5_Fig5_HTML.jpg](img/Images/A454310_1_En_5_Fig5_HTML.jpg) Figure 5-5. Installing Keras We will check now if Keras uses the TensorFlow backend or not. From the terminal Anaconda environment you enabled first, you need to switch to Python mode. If you get the following result importing Keras, that means everything is working (see Figure [5-6](#Fig6)). (universe) abhi@ubuntu:∼$ python Python 3.5.3 |Anaconda custom (64-bit)| (default, Mar  6 2017, 11:58:13) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux Type "help", "copyright", "credits" or "license" for more information. >>> import keras Using TensorFlow backend. ![A454310_1_En_5_Fig6_HTML.jpg](img/Images/A454310_1_En_5_Fig6_HTML.jpg) Figure 5-6.Keras with the TensorFlow backend

## 使用 ChainerRL

This section covers ChainerRL and explains how to apply Reinforcement Learning using it. ChainerRL is a deep Reinforcement Learning library especially built with the help of the Chainer Framework. See Figure [5-7](#Fig7).![A454310_1_En_5_Fig7_HTML.jpg](img/Images/A454310_1_En_5_Fig7_HTML.jpg) Figure 5-7.ChainerRL

### 安装链条 RL

We will install ChainerRL first from the terminal window. Figure [5-8](#Fig8) shows the Anaconda environment.![A454310_1_En_5_Fig8_HTML.jpg](img/Images/A454310_1_En_5_Fig8_HTML.jpg) Figure 5-8.Activating the Anaconda environment You can now install ChainerRL. To do so, type this command in the terminal: pip install chainerrl Figure [5-9](#Fig9) shows the result of the installation.![A454310_1_En_5_Fig9_HTML.jpg](img/Images/A454310_1_En_5_Fig9_HTML.jpg) Figure 5-9.Installing ChainerRL Now you can git clone the repo. Use this command to do so: git clone https://github.com/chainer/chainerrl.git Figure [5-10](#Fig10) shows the result.![A454310_1_En_5_Fig10_HTML.jpg](img/Images/A454310_1_En_5_Fig10_HTML.jpg) Figure 5-10.Cloning ChainerRL Then get inside the chainerrl folder, as shown in Figure [5-11](#Fig11).![A454310_1_En_5_Fig11_HTML.jpg](img/Images/A454310_1_En_5_Fig11_HTML.jpg) Figure 5-11.Inside the chainerrl folder

### 使用 ChainerRL 的管道

Since the library is based on Python, the obvious language of choice is Python. Follow these steps to set up ChainerRL:

1.  1.导入 gym、numpy 和支持的 chainerrl 库。import chainer import chainer . functions as F import chainer . links as L import chainer rl import Gym import numpy as NP 你必须建模一个环境，这样你才能使用 OpenAI Gym(见图 [5-12](#Fig12) )。环境有两个空间:它们必须有两个方法，reset 和 step。![A454310_1_En_5_Fig12_HTML.jpg](img/Images/A454310_1_En_5_Fig12_HTML.jpg)图 5-12。ChainerRL 如何使用状态转换
    *   了望处
    *   行为空间They must have two methods, reset and step .
2.  2.以 OpenAI 仿真环境中的一个仿真环境如 Cartpole-v0 为例。env = gym . make(' cart pole-v 0 ')print(' observation space:'，env . observation _ space)print(' action space:'，env . action _ space)OBS = env . reset()env . render()print(' initial observation:'，OBS)action . sample()OBS，r，done，info = env . step(action)print(' next observation:'，obs) print('reward:'，r) print('done:'，done) print('info:'，info)
3.  3.现在定义一个代理，它将在与环境的交互中运行。这里，是 QFunction (chainer。Chain) class: def __init__(self，obs_size，n_actions，n_hidden_channels=50): super()。__init__( l0=L.Linear(obs_size，n_hidden_channels)，l1=L.Linear(n_hidden_channels，n_hidden_channels)，l2=L.Linear(n_hidden_channels，n_actions)) def __call__(self，x，test=False): """ Args: x (ndarray 或 chainer。变量):一个观察测试(bool):一个标志，表示是否处于测试模式" " " h = f . tanh(self . l0(x))h = f . tanh(self . L1(h))返回 chainerrl.action_value。discretactionvalue(self . L2(h))OBS _ size = env . observation _ space . shape[0]n_actions = env . action _ space . n Q _ func = Q function(OBS _ size，n _ actions)我们应用 Q 学习等。我们从代理人开始。gamma = 0.95 #使用 epsilon-greedy for exploration explorer = chain errl . explorers . constantepsilongreedy(epsilon = 0.3，random _ action _ func = env . action _ space . sample)# DQN 使用经验回放。#指定重放缓冲区及其容量。replay _ buffer = chain errl . replay _ buffer。ReplayBuffer(capacity=10 ** 6) #由于来自 CartPole-v0 的观测值是 numpy.float64，而# Chainer 默认只接受 numpy.float32，因此指定# a 转换器作为特征提取器函数 phi。phi = lambda x:x . astype(NP . float 32，copy=False) #现在创建一个将与环境交互的代理。agent = chain errl . agents . doubled qn(q _ func，optimizer，replay_buffer，gamma，explorer，replay_start_size=500，update_interval=1，target_update_interval=100，phi=phi)
4.  4.开始强化学习过程。你得先在宇宙环境下打开 jupyter 笔记本，如图 [5-13](#Fig13) 。![A454310_1_En_5_Fig13_HTML.jpg](img/Images/A454310_1_En_5_Fig13_HTML.jpg)图 5-13。进入 jupyter 笔记本 abhi @ Ubuntu:∽$ source activate universe(universe)abhi @ Ubuntu:∽$ jupyter 笔记本图 [5-14](#Fig14) 显示最后运行代码。![A454310_1_En_5_Fig14_HTML.jpg](img/Images/A454310_1_En_5_Fig14_HTML.jpg)图 5-14。运行代码
5.  5.现在你测试代理，如图 [5-15](#Fig15) 所示。![A454310_1_En_5_Fig15_HTML.jpg](img/Images/A454310_1_En_5_Fig15_HTML.jpg)图 5-15。测试代理

We completed the entire program in the jupyter notebook. Now we will work on one of the repos for understanding Deep Q Learning with TensorFlow. See Figure [5-16](#Fig16).![A454310_1_En_5_Fig16_HTML.jpg](img/Images/A454310_1_En_5_Fig16_HTML.jpg) Figure 5-16.Cloning the GitHub repo First you need to install the prerequisites as follows (see Figure [5-17](#Fig17)): pip install -U 'gym[all]' tqdm scipy ![A454310_1_En_5_Fig17_HTML.jpg](img/Images/A454310_1_En_5_Fig17_HTML.jpg) Figure 5-17.Getting inside the folder Then run the program and train it without using GPU support, as shown in Figure [5-18](#Fig18).![A454310_1_En_5_Fig18_HTML.jpg](img/Images/A454310_1_En_5_Fig18_HTML.jpg) Figure 5-18.Training the program without GPU support The command is as follows: $ python main.py --network_header_type=nips --env_name=Breakout-v0 --use_gpu=False The command uses the main.py Python file and runs the Breakout game simulation in CPU mode only. You can now open the terminal to get inside the Anaconda environment, as shown in Figure [5-19](#Fig19).![A454310_1_En_5_Fig19_HTML.jpg](img/Images/A454310_1_En_5_Fig19_HTML.jpg) Figure 5-19.Activating the environment Now switch to Python mode, as shown in Figure [5-20](#Fig20): (universe) abhi@ubuntu:∼$ python Python 3.5.3 |Anaconda custom (64-bit)| (default, Mar  6 2017, 11:58:13) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux Type "help", "copyright", "credits" or "license" for more information. >>> ![A454310_1_En_5_Fig20_HTML.jpg](img/Images/A454310_1_En_5_Fig20_HTML.jpg) Figure 5-20.Switching to Python mode As you switch to Python mode, you first import the utilities: import gym import numpy as np To get the observation along the frozen lake simulation, you have to formulate the Q table as follows: Q = np.zeros([env.observation_space.n,env.action_space.n]) After that, you declare the learning rates and create the lists to contain the rewards for each state. import gym import numpy as np env = gym.make('FrozenLake-v0') #Initialize table with all zeros Q = np.zeros([env.observation_space.n,env.action_space.n]) # Set learning parameters lr = .8 y = .95 num_episodes = 2000 #create lists to contain total rewards and steps per episode #jList = [] rList = [] for i in range(num_episodes):     #Reset environment and get first new observation     s = env.reset()     rAll = 0     d = False     j = 0     #The Q-Table learning algorithm     while j < 99:         j+=1         #Choose an action by greedily (with noise) picking from Q table         a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))         #Get new state and reward from environment         s1,r,d,_ = env.step(a)         #Update Q-Table with new knowledge         Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])         rAll += r         s = s1         if d == True:             break     #jList.append(j)     rList.append(rAll) print "Score over time: " +  str(sum(rList)/num_episodes) print "Final Q-Table Values" print Q After going through all the steps, you can finally print the Q table. Each line should be placed into Python mode .

## 深度 Q 学习:使用 Keras 和 TensorFlow

We will touch on Deep Q Learning with Keras. We will clone an important reinforcement library, which is known as Keras-rl. It has several states of the Deep Q Learning algorithms. See Figure [5-21](#Fig21).![A454310_1_En_5_Fig21_HTML.jpg](img/Images/A454310_1_En_5_Fig21_HTML.jpg) Figure 5-21.Keras-rl representation

### 安装 Keras-rl

The command for installing Keras-rl is as follows (see Figure [5-22](#Fig22)): pip install keras-rl ![A454310_1_En_5_Fig22_HTML.jpg](img/Images/A454310_1_En_5_Fig22_HTML.jpg) Figure 5-22.Installing Keras-rl You also need to install h5py if it is not already installed and then you need to clone the repo, as shown in Figure [5-23](#Fig23).![A454310_1_En_5_Fig23_HTML.jpg](img/Images/A454310_1_En_5_Fig23_HTML.jpg) Figure 5-23.Cloning the git repo

### 使用 Keras-rl 进行培训

You will see how to run a program in this section. First, get inside the rl folder, as shown in Figure [5-24](#Fig24). abhi@ubuntu:∼$ cd keras-rl abhi@ubuntu:∼/keras-rl$ dir assets  examples           LICENSE     pytest.ini  rl         setup.py docs    ISSUE_TEMPLATE.md  mkdocs.yml  README.md   setup.cfg  tests abhi@ubuntu:∼/keras-rl$ cd examples abhi@ubuntu:∼/keras-rl/examples$ dir cem_cartpole.py   dqn_atari.py     duel_dqn_cartpole.py  sarsa_cartpole.py ddpg_pendulum.py  dqn_cartpole.py  naf_pendulum.py      visualize_log.py abhi@ubuntu:∼/keras-rl/examples$ ![A454310_1_En_5_Fig24_HTML.jpg](img/Images/A454310_1_En_5_Fig24_HTML.jpg) Figure 5-24.Getting inside the Keras-rl directory Now you can run one of the examples: abhi@ubuntu:∼/keras-rl/examples$ python dqn_cartpole.py Activating the anaconda environment (universe) abhi@ubuntu:∼/keras-rl/examples$ python dqn_cartpole.py See Figure [5-25](#Fig25).![A454310_1_En_5_Fig25_HTML.jpg](img/Images/A454310_1_En_5_Fig25_HTML.jpg) Figure 5-25.Using the TensorFlow backend The simulation will now begin, as shown in Figure [5-26](#Fig26).![A454310_1_En_5_Fig26_HTML.jpg](img/Images/A454310_1_En_5_Fig26_HTML.jpg) Figure 5-26.Simulation happens The simulation occurs and trains the model using Deep Q Learning. With practice, the cart-pole will balance along the rope; its stability increases with learning . The entire process creates the following log: (universe) abhi@ubuntu:∼/keras-rl/examples$ python dqn_cartpole.py Using TensorFlow backend. [2017-09-24 09:36:27,476] Making new env: CartPole-v0 _________________________________________________________________ Layer (type)                 Output Shape              Param # ================================================================= flatten_1 (Flatten)          (None, 4)                 0 _________________________________________________________________ dense_1 (Dense)              (None, 16)                80 _________________________________________________________________ activation_1 (Activation)    (None, 16)                0 _________________________________________________________________ dense_2 (Dense)              (None, 16)                272 _________________________________________________________________ activation_2 (Activation)    (None, 16)                0 _________________________________________________________________ dense_3 (Dense)              (None, 16)                272 _________________________________________________________________ activation_3 (Activation)    (None, 16)                0 _________________________________________________________________ dense_4 (Dense)              (None, 2)                 34 _________________________________________________________________ activation_4 (Activation)    (None, 2)                 0 ================================================================= Total params: 658 Trainable params: 658 Non-trainable params: 0 _________________________________________________________________ None 2017-09-24 09:36:27.932219: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations. ...    712/50000: episode: 38, duration: 0.243s, episode steps: 14, steps per second: 58, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.568, 0.957], loss: 0.291389, mean_absolute_error: 3.054634, mean_q: 5.816398 The episodes are iterations for the simulations. The cartpole.py code is discussed next. You need to import the utilities first. The utilities included are very useful, as they have built-in agents for applying Deep Q Learning . First, declare the environment as follows: ENV_NAME = 'CartPole-v0' env = gym.make(ENV_NAME) Since we want to implement Deep Q Learning, we use parameters for initializing the Convolution Neural Network (CNN). We also use an activation function to propagate the neural network. We keep it sequential. model = Sequential() model.add(Flatten(input_shape=(1,) + env.observation_space.shape)) model.add(Dense(16)) model.add(Activation('relu')) model.add(Dense(16)) model.add(Activation('relu')) model.add(Dense(16)) model.add(Activation('relu')) model.add(Dense(nb_actions)) model.add(Activation('linear')) You can print the model details too, as follows: print(model.summary()) Next, configure the model and use all the Reinforcement Learning options with the help of a function. import numpy as np import gym from keras.models import Sequential from keras.layers import Dense, Activation, Flatten from keras.optimizers import Adam from rl.agents.dqn import DQNAgent from rl.policy import BoltzmannQPolicy from rl.memory import SequentialMemory ENV_NAME = 'CartPole-v0' # Get the environment and extract the number of actions. env = gym.make(ENV_NAME) np.random.seed(123) env.seed(123) nb_actions = env.action_space.n # Next, we build a very simple model. model = Sequential() model.add(Flatten(input_shape=(1,) + env.observation_space.shape)) model.add(Dense(16)) model.add(Activation('relu')) model.add(Dense(16)) model.add(Activation('relu')) model.add(Dense(16)) model.add(Activation('relu')) model.add(Dense(nb_actions)) model.add(Activation('linear')) print(model.summary()) # Finally, we configure and compile our agent. You can use every built-in Keras optimizer and # even the metrics! memory = SequentialMemory(limit=50000, window_length=1) policy = BoltzmannQPolicy() dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,                target_model_update=1e-2, policy=policy) dqn.compile(Adam(lr=1e-3), metrics=['mae']) # Okay, now it's time to learn something! We visualize the training here for show, but this # slows down training quite a lot. You can always safely abort the training prematurely using # Ctrl + C. dqn.fit(env, nb_steps=50000, visualize=True, verbose=2) # After training is done, we save the final weights. dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True) # Finally, evaluate our algorithm for 5 episodes. dqn.test(env, nb_episodes=5, visualize=True) To get all the capabilities of Keras-rl, you need to run the setup.py file within the Keras-rl folder, as follows: (universe) abhi@ubuntu:∼/keras-rl$ python setup.py install You will see that all the dependencies are being installed, one by one: running install running bdist_egg running egg_info creating keras_rl.egg-info writing requirements to keras_rl.egg-info/requires.txt writing dependency_links to keras_rl.egg-info/dependency_links.txt writing top-level names to keras_rl.egg-info/top_level.txt writing keras_rl.egg-info/PKG-INFO writing manifest file 'keras_rl.egg-info/SOURCES.txt' reading manifest file 'keras_rl.egg-info/SOURCES.txt' writing manifest file 'keras_rl.egg-info/SOURCES.txt' installing library code to build/bdist.linux-x86_64/egg running install_lib running build_py creating build creating build/lib creating build/lib/tests copying tests/__init__.py -> build/lib/tests creating build/lib/rl copying rl/util.py -> build/lib/rl copying rl/callbacks.py -> build/lib/rl copying rl/keras_future.py -> build/lib/rl copying rl/memory.py -> build/lib/rl copying rl/random.py -> build/lib/rl copying rl/core.py -> build/lib/rl copying rl/__init__.py -> build/lib/rl copying rl/policy.py -> build/lib/rl creating build/lib/tests/rl copying tests/rl/test_util.py -> build/lib/tests/rl copying tests/rl/util.py -> build/lib/tests/rl copying tests/rl/test_memory.py -> build/lib/tests/rl copying tests/rl/test_core.py -> build/lib/tests/rl copying tests/rl/__init__.py -> build/lib/tests/rl creating build/lib/tests/rl/agents copying tests/rl/agents/test_cem.py -> build/lib/tests/rl/agents copying tests/rl/agents/__init__.py -> build/lib/tests/rl/agents copying tests/rl/agents/test_ddpg.py -> build/lib/tests/rl/agents copying tests/rl/agents/test_dqn.py -> build/lib/tests/rl/agents creating build/lib/rl/agents copying rl/agents/sarsa.py -> build/lib/rl/agents copying rl/agents/ddpg.py -> build/lib/rl/agents copying rl/agents/dqn.py -> build/lib/rl/agents copying rl/agents/cem.py -> build/lib/rl/agents copying rl/agents/__init__.py -> build/lib/rl/agents Keras-rl is now set up and you can use the built-in functions to their fullest effect.

## 结论

This chapter introduced and defined Keras and explained how to use it with Reinforcement Learning. The chapter also explained how to use TensorFlow with Reinforcement Learning and discussed using ChainerRL. Chapter [6](6.html) covers Google DeepMind and the future of Reinforcement Learning.