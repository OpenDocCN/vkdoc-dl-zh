© Abhishek Nandy and Manisha Biswas  2018 Abhishek Nandy and Manisha BiswasReinforcement Learning [https://doi.org/10.1007/978-1-4842-3285-9_1](1.html)

# 1.强化学习基础

Abhishek Nandy<sup class="calibre7">1 </sup> and Manisha Biswas<sup class="calibre7">2</sup> (1)Rm HIG L-2/4, Bldg Swaranika Co-Opt HSG, Kolkata, West Bengal, India (2)North 24 Parganas, West Bengal, India   This chapter is a brief introduction to Reinforcement Learning (RL) and includes some key concepts associated with it. In this chapter, we talk about Reinforcement Learning as a core concept and then define it further. We show a complete flow of how Reinforcement Learning works. We discuss exactly where Reinforcement Learning fits into artificial intelligence (AI). After that we define key terms related to Reinforcement Learning. We start with agents and then touch on environments and then finally talk about the connection between agents and environments.

## 什么是强化学习？

We use Machine Learning to constantly improve the performance of machines or programs over time. The simplified way of implementing a process that improves machine performance with time is using Reinforcement Learning (RL). Reinforcement Learning is an approach through which intelligent programs, known as agents, work in a known or unknown environment to constantly adapt and learn based on giving points. The feedback might be positive, also known as rewards, or negative, also called punishments. Considering the agents and the environment interaction, we then determine which action to take. In a nutshell, Reinforcement Learning is based on rewards and punishments . Some important points about Reinforcement Learning:

*   它不同于普通的机器学习，因为我们不看训练数据集。
*   交互不是与数据发生的，而是与环境发生的，通过环境我们描绘了真实世界的场景。
*   因为强化学习是基于环境的，所以许多参数都会发挥作用。学习和采取相应的行动需要大量的信息。
*   强化学习中的环境是真实世界的场景，可能是 2D 或 3D 模拟世界或基于游戏的场景。
*   强化学习在某种意义上是更广泛的，因为环境可以是大规模的，并且可能有许多与它们相关的因素。
*   强化学习的目的是达到一个目标。
*   强化学习中的奖励是从环境中获得的。

The Reinforcement Learning cycle is depicted in Figure [1-1](#Fig1) with the help of a robot.![A454310_1_En_1_Fig1_HTML.jpg](Images/A454310_1_En_1_Fig1_HTML.jpg) Figure 1-1.Reinforcement Learning cycle A maze is a good example that can be studied using Reinforcement Learning , in order to determine the exact right moves to complete the maze (see Figure [1-2](#Fig2)).![A454310_1_En_1_Fig2_HTML.jpg](Images/A454310_1_En_1_Fig2_HTML.jpg) Figure 1-2.Reinforcement Learning can be applied to mazes In Figure [1-3](#Fig3), we are applying Reinforcement Learning and we call it the Reinforcement Learning box because within its vicinity the process of RL works. RL starts with an intelligent program, known as agents, and when they interact with environments, there are rewards and punishments associated. An environment can be either known or unknown to the agents. The agents take actions to move to the next state in order to maximize rewards. ![A454310_1_En_1_Fig3_HTML.jpg](Images/A454310_1_En_1_Fig3_HTML.jpg) Figure 1-3.Reinforcement Learning flow In the maze, the centralized concept is to keep moving. The goal is to clear the maze and reach the end as quickly as possible. The following concepts of Reinforcement Learning and the working scenario are discussed later this chapter.

*   代理是智能程序
*   环境就是迷宫
*   状态是代理所在的迷宫中的位置
*   动作是我们移动到下一个状态所采取的步骤
*   奖励是与达到特定状态相关的点数。它可以是正数、负数或零

We use the maze example to apply concepts of Reinforcement Learning. We will be describing the following steps :

1.  1.迷宫的概念是给代理的。
2.  2.有一个与代理相关联的任务，强化学习应用于该任务。
3.  3.代理人从一个状态到另一个状态的每一次移动都会得到(a-1)次强化。
4.  4.当代理从一个州移动到另一个州时，有一个奖励系统。

The rewards predictions are made iteratively, where we update the value of each state in a maze based on the value of the best subsequent state and the immediate reward obtained. This is called the update rule. The constant movement of the Reinforcement Learning process is based on decision-making. Reinforcement Learning works on a trial-and-error basis because it is very difficult to predict which action to take when it is in one state. From the maze problem itself, you can see that in order get the optimal path for the next move, you have to weigh a lot of factors. It is always on the basis of state action and rewards. For the maze, we have to compute and account for probability to take the step. The maze also does not consider the reward of the previous step; it is specifically considering the move to the next state. The concept is the same for all Reinforcement Learning processes. Here are the steps of this process:

1.  1.我们有麻烦了。
2.  2.我们必须应用强化学习。
3.  3.我们考虑应用强化学习作为强化学习盒。
4.  4.强化学习框包含应用强化学习过程所需的所有基本组件。
5.  5.强化学习盒包含代理、环境、奖励、惩罚和行动。

Reinforcement Learning works well with intelligent program agents that give rewards and punishments when interacting with an environment. The interaction happens between the agents and the environments, as shown in Figure [1-4](#Fig4).![A454310_1_En_1_Fig4_HTML.jpg](Images/A454310_1_En_1_Fig4_HTML.jpg) Figure 1-4.Interaction between agents and environments From Figure [1-4](#Fig4), you can see that there is a direct interaction between the agents and its environments. This interaction is very important because through these exchanges, the agent adapts to the environments. When a Machine Learning program, robot, or Reinforcement Learning program starts working, the agents are exposed to known or unknown environments and the Reinforcement Learning technique allows the agents to interact and adapt according to the environment’s features. Accordingly, the agents work and the Reinforcement Learning robot learns. In order to get to a desired position, we assign rewards and punishments. Now, the program has to work around the optimal path to get maximum rewards if it fails (that is, it takes punishments or receives negative points). In order to reach a new position, which also is known as a state, it must perform what we call an action. To perform an action, we implement a function, also known as a policy. A policy is therefore a function that does some work.

## 强化学习的面貌

As you see from the Venn diagram in Figure [1-5](#Fig5), Reinforcement Learning sits at the intersection of many different fields of science.![A454310_1_En_1_Fig5_HTML.jpg](Images/A454310_1_En_1_Fig5_HTML.jpg) Figure 1-5.All the faces of Reinforcement Learning The intersection points reveal a very strong feature of Reinforcement Learning—it shows the science of decision-making . If we have two paths and have to decide which path to take so that some point is met, a scientific decision-making process can be designed. Reinforcement Learning is the fundamental science of optimal decision-making. If we focus on the computer science part of the Venn diagram in Figure [1-5](#Fig5), we see that if we want to learn, it falls under the category of Machine Learning, which is specifically mapped to Reinforcement Learning. Reinforcement Learning can be applied to many different fields of science. In engineering, we have devices that focus mostly on optimal control. In neuroscience, we are concerned with how the brain works as a stimulant for making decisions and study the reward system that works on the brain (the dopamine system). Psychologists can apply Reinforcement Learning to determine how animals make decisions. In mathematics, we have a lot of data applying Reinforcement Learning in operations research.

## 强化学习的流程

Figure [1-6](#Fig6) connects agents and environments.![A454310_1_En_1_Fig6_HTML.jpg](Images/A454310_1_En_1_Fig6_HTML.jpg) Figure 1-6.RL structure The interaction happens from one state to another. The exact connection starts between an agent and the environment. Rewards are happening on a regular basis. We take appropriate actions to move from one state to another. The key points of consideration after going through the details are the following:

*   强化学习循环以一种相互关联的方式进行。
*   代理和环境之间有明显的通信。
*   这种独特的交流是带着回报而来的。
*   物体或机器人从一种状态移动到另一种状态。
*   采取行动从一个状态转移到另一个状态

Figure [1-7](#Fig7) simplifies the interaction process.![A454310_1_En_1_Fig7_HTML.jpg](Images/A454310_1_En_1_Fig7_HTML.jpg) Figure 1-7.The entire interaction process An agent is always learning and finally makes a decision. An agent is a learner, which means there might be different paths. When the agent starts training, it starts to adapt and intelligently learns from its surroundings. The agent is also a decision maker because it tries to take an action that will get it the maximum reward. When the agent starts interacting with the environment, it can choose an action and respond accordingly . From then on, new scenes are created. When the agent changes from one place to another in an environment, every change results in some kind of modification. These changes are depicted as scenes. The transition that happens in each step helps the agent solve the Reinforcement Learning problem more effectively. Let’s look at another scenario of state transitioning, as shown in Figures [1-8](#Fig8) and [1-9](#Fig9).![A454310_1_En_1_Fig8_HTML.jpg](Images/A454310_1_En_1_Fig8_HTML.jpg) Figure 1-8.Scenario of state changes ![A454310_1_En_1_Fig9_HTML.jpg](Images/A454310_1_En_1_Fig9_HTML.jpg) Figure 1-9.The state transition process Learn to choose actions that maximize the following : r0 +γr1 +γ2r2 +............... where 0< γ<1 At each state transition, the reward is a different value, hence we describe reward with varying values in each step, such as r0, r1, r2, etc. Gamma (γ) is called a discount factor and it determines what future reward types we get:

*   灰度值为 0 表示奖励仅与当前状态相关
*   伽玛值为 1 意味着奖励是长期的

## 强化学习中的不同术语

Now we cover some common terms associated with Reinforcement Learning. There are two constants that are important in this case—gamma (γ) and lambda (λ), as shown in Figure [1-10](#Fig10).![A454310_1_En_1_Fig10_HTML.jpg](Images/A454310_1_En_1_Fig10_HTML.jpg) Figure 1-10.Showing values of constants Gamma is common in Reinforcement Learning problems but lambda is used generally in terms of temporal difference problems.

### 微克

Gamma is used in each state transition and is a constant value at each state change. Gamma allows you to give information about the type of reward you will be getting in every state. Generally, the values determine whether we are looking for reward values in each state only (in which case, it’s 0) or if we are looking for long-term reward values (in which case it’s 1).

### 希腊字母的第 11 个

Lambda is generally used when we are dealing with temporal difference problems. It is more involved with predictions in successive states. Increasing values of lambda in each state shows that our algorithm is learning fast. The faster algorithm yields better results when using Reinforcement Learning techniques. As you’ll learn later, temporal differences can be generalized to what we call TD(Lambda). We discuss it in greater depth later.

## 与强化学习的互动

Let’s now talk about Reinforcement Learning and its interactions. As shown in Figure [1-11](#Fig11), the interactions between the agent and the environment occur with a reward. We need to take an action to move from one state to another.![A454310_1_En_1_Fig11_HTML.jpg](Images/A454310_1_En_1_Fig11_HTML.jpg) Figure 1-11.Reinforcement Learning interactions Reinforcement Learning is a way of implementing how to map situations to actions so as to maximize and find a way to get the highest rewards. The machine or robot is not told which actions to take, as with other forms of Machine Learning, but instead the machine must discover which actions yield the maximum reward by trying them. In the most interesting and challenging cases, actions affect not only the immediate reward but also the next situation and all subsequent rewards.

### RL 特性

We talk about characteristics next. The characteristics are generally what the agent does to move to the next state. The agent considers which approach works best to make the next move. The two characteristics are

*   试错搜索。
*   延迟奖励。

As you probably have gathered, Reinforcement Learning works on three things combined : (S,A,R) Where S represents state, A represents action, and R represents reward. If you are in a state S, you perform an action A so that you get a reward R at time frame t+1\. Now, the most important part is when you move to the next state. In this case, we do not use the reward we just earned to decide where to move next. Each transition has a unique reward and no reward from any previous state is used to determine the next move. See Figure [1-12](#Fig12).![A454310_1_En_1_Fig12_HTML.jpg](Images/A454310_1_En_1_Fig12_HTML.jpg) Figure 1-12.State change with time The T change (the time frame) is important in terms of Reinforcement Learning. Every occurrence of what we do is always a combination of what we perform in terms of states, actions, and rewards. See Figure [1-13](#Fig13).![A454310_1_En_1_Fig13_HTML.jpg](Images/A454310_1_En_1_Fig13_HTML.jpg) Figure 1-13.Another way of representing the state transition

### 奖励是如何运作的

A reward is some motivator we receive when we transition from one state to another. It can be points, as in a video game. The more we train, the more accurate we become, and the greater our reward.

### 代理人

In terms of Reinforcement Learning, agents are the software programs that make intelligent decisions. Agents should be able to perceive what is happening in the environment. Here are the basic steps of the agents:

1.  1.当智能体可以感知环境时，它可以做出更好的决策。
2.  2.代理人做出的决定导致了行动。
3.  3.代理执行的动作必须是最好的，最优的。

Software agents might be autonomous or they might work together with other agents or with people. Figure [1-14](#Fig14) shows how the agent works.![A454310_1_En_1_Fig14_HTML.jpg](Images/A454310_1_En_1_Fig14_HTML.jpg) Figure 1-14.The flow of the environment

### RL 环境

The environments in the Reinforcement Learning space are comprised of certain factors that determine the impact on the Reinforcement Learning agent. The agent must adapt accordingly to the environment. These environments can be 2D worlds or grids or even a 3D world. Here are some important features of environments:

*   确定性的
*   可观察量
*   离散或连续
*   单代理或多代理。

#### 确定性的

If we can infer and predict what will happen with a certain scenario in the future, we say the scenario is deterministic. It is easier for RL problems to be deterministic because we don’t rely on the decision-making process to change state. It’s an immediate effect that happens with state transitions when we are moving from one state to another. The life of a Reinforcement Learning problem becomes easier. When we are dealing with RL, the state model we get will be either deterministic or non-deterministic. That means we need to understand the mechanisms behind how DFA and NDFA work.

##### 确定性有限自动机

DFA goes through a finite number of steps. It can only perform one action for a state. See Figure [1-15](#Fig15).![A454310_1_En_1_Fig15_HTML.jpg](Images/A454310_1_En_1_Fig15_HTML.jpg) Figure 1-15.Showing DFA We are showing a state transition from a start state to a final state with the help of a diagram. It is a simple depiction where we can say that, with some input value that is assumed as 1 and 0, the state transition occurs. The self-loop is created when it gets a value and stays in the same state.

##### NDFA(非确定有限自动机)

If we are working in a scenario where we don’t know exactly which state a machine will move into, this is a case of NDFA. See Figure [1-16](#Fig16).![A454310_1_En_1_Fig16_HTML.jpg](Images/A454310_1_En_1_Fig16_HTML.jpg) Figure 1-16. NDFA The working principle of the state diagram in Figure [1-16](#Fig16) can be explained as follows. In NDFA the issue is when we are transitioning from one state to another, there is more than one option available, as we can see in Figure [1-16](#Fig16). From State S0 after getting an input such as 0, it can stay in state S0 or move to state S1\. There is decision-making involved here, so it becomes difficult to know which action to take.

#### 可观察量

If we can say that the environment around us is fully observable, we have a perfect scenario for implementing Reinforcement Learning. An example of perfect observability is a chess game. An example of partial observability is a poker game, where some of the cards are unknown to any one player.

#### 离散或连续

If there is more than one choice for transitioning to the next state, that is a continuous scenario. When there are a limited number of choices, that’s called a discrete scenario.

#### 单代理和多代理环境

Solutions in Reinforcement Learning can be of single agent types or multiagent types. Let’s take a look at multiagent Reinforcement Learning first. When we are dealing with complex problems, we use multiagent Reinforcement Learning. Complex problems might have different environments where the agent is doing different jobs to get involved in RL and the agent also wants to interact. This introduces different complications in determining transitions in states. Multiagent solutions are based on the non-deterministic approach. They are non-deterministic because when the multiagents interact, there might be more than one option to change or move to the next state and we have to make decisions based on that ambiguity. In multiagent solutions, the agent interactions between different environments are enormous. They are enormous because the amount of activity involved in references to environments is very large. This is because the environments might be different types and the multiagents might have different tasks to do in each state transition. The difference between single-agent and multiagent solutions are as follows:

*   单代理场景涉及智能软件，其中交互只发生在一个环境中。如果同时存在另一个环境，它不能与第一个环境交互。
*   当强化学习有一点点收敛的时候。收敛是指代理需要在不同的环境中更频繁地交互才能做出决策。这种情况由多代理人解决，因为单个代理人无法解决收敛问题。单个智能体无法解决趋同问题，因为当可能存在涉及同时决策的不同场景时，它会连接到其他环境。
*   与单个代理相比，多代理具有动态环境。动态环境可能涉及到要与之交互的地方不断变化的环境。

Figure [1-17](#Fig17) shows the single-agent scenario.![A454310_1_En_1_Fig17_HTML.jpg](Images/A454310_1_En_1_Fig17_HTML.jpg) Figure 1-17.Single agent Figure [1-18](#Fig18) shows how multiagents work. There is an interaction between two agents in order to make the decision.![A454310_1_En_1_Fig18_HTML.jpg](Images/A454310_1_En_1_Fig18_HTML.jpg) Figure 1-18.Multiagent scenario

## 结论

This chapter touched on the basics of Reinforcement Learning and covered some key concepts. We covered states and environments and how the structure of Reinforcement Learning looks. We also touched on the different kinds of interactions and learned about single-agent and multiagent solutions. The next chapter covers algorithms and discusses the building blocks of Reinforcement Learning.