# 四、开发聊天机器人

在本章中，我们将创建一个聊天机器人。我们将以一种渐进的方式做这件事，并且将聊天机器人分成两层。本章的第一节介绍了聊天机器人的概念，接下来的一节介绍了如何实现一个基于规则的聊天机器人系统。最后一节讨论了在公开可用的数据集上训练序列对序列(seq2seq)循环神经网络(RNN)模型。最终的聊天机器人将能够回答针对数据集领域提出的特定问题，该模型已在该数据集领域中进行了训练。我们希望你喜欢前面的章节，这一章也会让你参与到深度学习和自然语言处理(NLP)的实现中。

## 聊天机器人简介

事实上，我们都在使用聊天机器人，甚至不知道如何定义它，这使得聊天机器人的定义变得无关紧要。

在我们的日常生活中，我们都在使用各种各样的应用程序，如果有人在阅读这一章时没有听说过“聊天机器人”，那将是令人惊讶的聊天机器人就像任何其他应用程序一样。聊天机器人与普通应用程序的唯一区别在于它们的用户界面。聊天机器人有一个聊天界面，用户可以与应用程序进行文字聊天，而不是发送消息，并以对话的方式操作它，而不是由按钮和图标组成的视觉界面。我们希望这个定义现在是清晰的，你可以深入到聊天机器人的奇妙世界。

### 聊天机器人的起源

就像我们讨厌起源的想法一样，我们喜欢起源的想法。不要仅仅成为事实的记录者，而是要试着去洞察它们起源的奥秘。——伊凡·巴甫洛夫

不探究聊天机器人的起源就去讨论它们是没有用的。你可能会觉得有趣的是，1950 年，当世界从第二次世界大战的冲击中恢复过来时，英国学者艾伦·图灵有先见之明，发明了一种测试，看一个人能否区分人和机器。[这就是所谓的图灵测试](https://en.wikipedia.org/wiki/Turing_test?utm_source=ubisend.com&utm_medium=blog-link&utm_campaign=ubisend#_blank) ( [`https://en.wikipedia.org/wiki/Turing_test`](https://en.wikipedia.org/wiki/Turing_test) )。

16 年后的 1966 年，约瑟夫·韦岑鲍姆发明了一种叫做伊莱扎的计算机程序。它只用了 200 行代码就模仿了一位心理治疗师的语言。你还可以在这里跟它对话: [`http://psych.fullerton.edu/mbirnbaum/psych101/Eliza.htm`](http://psych.fullerton.edu/mbirnbaum/psych101/Eliza.htm) 。

![A461351_1_En_4_Figa_HTML.jpg](img/A461351_1_En_4_Figa_HTML.jpg)

机器学习的最新发展为聊天机器人提供了前所未有的动力，解释自然语言，以便随着时间的推移更好地理解和学习。脸书、苹果、谷歌(Alphabet)和微软等大公司正在投入大量资源，研究如何模仿消费者和机器之间的真实对话，以及商业上可行的商业模式。

### 但是聊天机器人是如何工作的呢？

好了，介绍够了。让我们言归正传。

*   “嘿，怎么了？”
*   “你过得怎么样？”
*   “你好！”

这些句子似乎很熟悉。不是吗？它们都是某种问候某人的信息。我们如何回应这些问候？通常，我们会回答“我很好。你呢？”

这正是聊天机器人的工作方式。典型的聊天机器人会找到所提问题的所谓上下文，在这种情况下，就是“问候”然后，机器人获得适当的响应，并将其发送回用户。它如何找到适当的响应，它能处理图像、音频和视频等附件吗？我们将在接下来的部分中处理这个问题。

### 为什么聊天机器人是如此大的机会？

Forrester ( [`https://go.forrester.com/data/consumer-technographics/`](https://go.forrester.com/data/consumer-technographics/) )进行的研究指出，我们在移动设备上大约 85%的时间花在主要应用程序上，如电子邮件和消息平台。借助深度学习和 NLP 提供的巨大优势，几乎每家公司都在尝试构建应用程序，以保持潜在消费者对其产品和服务的兴趣，聊天机器人是实现这一目的的独特工具。通过安装聊天机器人，可以轻松避免由传统客户服务处理的多种人为错误和客户请求。此外，聊天机器人可以允许客户和相关公司访问所有以前的聊天/问题记录。

虽然聊天机器人可以被认为是与最终客户进行对话的应用程序，但是聊天机器人执行的任务和少数相关应用程序可以在更高的级别上进行分类，分为以下类别:

*   问答:每个用户一轮；当出现带标签的答案时很有用
    1.  产品查询用例
    2.  提取用户信息
*   句子补全:在对话的下一句话中补上缺失的单词
    1.  将正确的产品映射到客户
*   面向目标的对话:以实现目标为任务的对话
    1.  给客户的建议
    2.  与顾客协商价格
*   聊天对话:没有明确目标的对话，更多的是讨论。现在没有这样的用例需要关注
*   可视对话:包含文本、图像和音频的任务
    1.  与顾客交换图像，并在此基础上建立推论

好吧，你现在可能在想，“我很兴奋。我怎么能造一个呢？”

### 构建聊天机器人听起来很吓人。是真的吗？

构建聊天机器人的困难不在于技术，而在于用户体验。市场上最流行的成功的机器人之一是那些用户希望定期回来，并为他们的日常任务和需求提供一致价值的机器人。—Matt Hartman，Betaworks 的种子投资总监

在构建聊天机器人之前，如果我们提前解决以下四个问题，然后决定如何推进项目，那会更有意义:

*   我们要用机器人解决什么问题？
*   我们的机器人将生活在哪个平台上(脸书，Slack 等)。)?
*   我们将使用什么服务器来托管机器人？ [Heroku](https://www.heroku.com/) ( [`www.heroku.com`](http://www.heroku.com) )还是我们自己？
*   我们是想从头开始还是使用现有的聊天机器人平台工具(如下)？
    *   Botsify ( [`https://botsify.com/`](https://botsify.com/) )
    *   Pandorabots ( [`https://playground.pandorabots.com/en/`](https://playground.pandorabots.com/en/)
    *   Chattypeople ( [`www.chattypeople.com/`](http://www.chattypeople.com/)
    *   Wit.ai ( [`https://wit.ai/`](https://wit.ai/)
    *   Api.ai ( [`https://api.ai/`](https://api.ai/)

要更深入地了解不同平台的工作方法以及业务用例的最佳匹配，可以参考以下文档，这些文档来自一些流行的聊天机器人平台的链接:

*   脸书信使( [`https://developers.facebook.com/products/messenger/`](https://developers.facebook.com/products/messenger/) )
*   懈怠( [`https://api.slack.com/bot-users`](https://api.slack.com/bot-users) )
*   不和( [`https://blog.discordapp.com/the-robot-revolution-has-unofficially-begun/`](https://blog.discordapp.com/the-robot-revolution-has-unofficially-begun/) )
*   电报`(` [`https://core.telegram.org/bots/api`](https://core.telegram.org/bots/api) )
*   Kik ( [`https://dev.kik.com/#/home`](https://dev.kik.com/#/home)

## 对话机器人

对于我们的对话聊天机器人的第一个版本，我们将制作一个基于规则的机器人，它将帮助开发人员定义他/她对最终用户提出的特定类别问题的期望答案。创建这样一个机器人将有助于我们对使用机器人有一个基本的了解，然后我们再进入下一个层次，使用文本生成机器人。

我们将使用 Facebook Messenger 作为我们想要的平台，使用 Heroku 作为我们想要的服务器，来发布 chatbot 的基本版本。重要的事情先来。你一定有一个脸书的页面。如果没有，请创建一个。要与机器人通信，必须访问该页面并选择消息选项，以启动对话。

按照图 [4-1](#Fig1) 中的步骤在脸书上创建页面:

1.  选择“创建页面”选项。
2.  Select the desired category of the organization and choose a name to create the page. We have selected Insurance as the field of the organization, as later on, we will build test cases around it and use an Insurance-related conversation dataset to train our model.

    ![A461351_1_En_4_Fig1_HTML.jpg](img/A461351_1_En_4_Fig1_HTML.jpg)

    图 4-1

    Creating a Facebook page  
3.  根据需要，为页面添加个人资料和封面照片。

在执行了前面的步骤后，最终的页面 Dl4nlp_cb， [`www.facebook.com/dlnlpcb/`](http://www.facebook.com/dlnlpcb/) ，将如图 [4-2](#Fig2) 所示。

![A461351_1_En_4_Fig2_HTML.jpg](img/A461351_1_En_4_Fig2_HTML.jpg)

图 4-2

Dl4nlp_cb Facebook page

下一步是创建一个脸书应用程序。使用您的官方脸书帐户登录，访问以下网址创建一个: [`https://developers.facebook.com/apps/`](https://developers.facebook.com/apps/) 。该应用程序将订阅创建的页面，并代表该页面处理所有响应(图 [4-3](#Fig3) )。

![A461351_1_En_4_Fig3_HTML.jpg](img/A461351_1_En_4_Fig3_HTML.jpg)

图 4-3

Creating a Facebook app

我们为该应用程序分配了与之前创建的脸书页面相同的显示名称，并使用所需的电子邮件 ID 注册了该应用程序。发布应用程序创建。应用仪表板将如图 [4-4](#Fig4) 所示。

![A461351_1_En_4_Fig4_HTML.jpg](img/A461351_1_En_4_Fig4_HTML.jpg)

图 4-4

Facebook App Dashboard

脸书提供了一系列可以添加到新创建的应用程序中的产品。对于聊天机器人，我们需要选择 Messenger 作为选项(上图中第二行中间的选项)。单击设置按钮。这将把用户重定向到设置页面(图 [4-5](#Fig5) )，在这里，除了选择教程之外，我们还可以创建令牌并设置 webhooks(见下文)。

![A461351_1_En_4_Fig5_HTML.jpg](img/A461351_1_En_4_Fig5_HTML.jpg)

图 4-5

Facebook app Settings page

从“设置”页面，转到“令牌生成”部分，并选择在第一步中创建的页面。将弹出一个警告框，要求授予权限。点击继续并继续(图 [4-6](#Fig6) )。

![A461351_1_En_4_Fig6_HTML.jpg](img/A461351_1_En_4_Fig6_HTML.jpg)

图 4-6

Facebook Token Generation Note

人们可以查看脸书正在访问的有关该应用程序的信息。单击查看您提供的信息链接进行检查。

选择 Continue 选项后，您将获得另一个窗口，显示授予该页面的权限。用户可以选择要授予的权限。出于当前目的，建议不要更改特权部分中之前选择的任何选项(图 [4-7](#Fig7) )。

![A461351_1_En_4_Fig7_HTML.jpg](img/A461351_1_En_4_Fig7_HTML.jpg)

图 4-7

Privilege grant section

单击“选择您允许的内容”将显示授予该页面的权限。检查完毕后，点击确定并进入其他步骤(图 [4-8](#Fig8) )。

![A461351_1_En_4_Fig8_HTML.jpg](img/A461351_1_En_4_Fig8_HTML.jpg)

图 4-8

Permissions granted

这将在应用程序设置页面上开始生成令牌(生成令牌可能需要几秒钟)。参见图 [4-9](#Fig9) 。

![A461351_1_En_4_Fig9_HTML.jpg](img/A461351_1_En_4_Fig9_HTML.jpg)

图 4-9

Final page access token generation

页面访问令牌是一个长字符串，是数字和字母的组合，我们稍后将使用它来用 Heroku 创建应用程序。它将被设置为 Heroku 应用程序中的配置参数。

令牌在每次生成时都是唯一的，并且对于每个应用程序、页面和用户组合都是独立的。生成后会像图 [4-10](#Fig10) 中的样子。

![A461351_1_En_4_Fig10_HTML.jpg](img/A461351_1_En_4_Fig10_HTML.jpg)

图 4-10

Page access token

创建脸书页面和应用程序后，在 [Heroku](https://www.heroku.com/) ( [`www.heroku.com`](http://www.heroku.com) )上注册并打开一个帐户，并在这里创建一个应用程序，选择 Python 语言。

在 Heroku 上创建一个应用程序将为我们提供一个 webhook，脸书应用程序将向其发送请求，以防事件被触发，例如聊天机器人，无论何时接收或发送一些消息。

Note

确保 Heroku 使用的密码是字母、数字和符号的组合——全部三个，而不仅仅是两个。

创建帐户后，Heroku 仪表板将如图 [4-11](#Fig11) 所示。

![A461351_1_En_4_Fig11_HTML.jpg](img/A461351_1_En_4_Fig11_HTML.jpg)

图 4-11

Heroku dashboard

点击创建新应用程序，在 Heroku 上创建应用程序。关于 Python 语言的教程，可以点击 Python 按钮 [`https://devcenter.heroku.com/articles/getting-started-with-python#introduction`](https://devcenter.heroku.com/articles/getting-started-with-python#introduction) 访问共享教程。目前，保持默认选择“美国”，对于 pipeline，在创建应用程序时不要做任何选择(图 [4-12](#Fig12) )。

Note

应用程序的名称不能包含数字、下划线或符号。应用程序名称中只允许使用小写字母。

![A461351_1_En_4_Fig12_HTML.jpg](img/A461351_1_En_4_Fig12_HTML.jpg)

图 4-12

Heroku app creation

Heroku 应用仪表板将如图 [4-13](#Fig13) 所示，默认情况下，在应用创建后选择部署选项卡。

![A461351_1_En_4_Fig13_HTML.jpg](img/A461351_1_En_4_Fig13_HTML.jpg)

图 4-13

Heroku app dashboard

现在，我们都准备好使用脸书应用程序、page 和 Heroku 应用程序了。下一步是创建代码并将其导入 Heroku 应用程序。

从以下 URL，访问 GitHub 存储库，并将其克隆到您的个人 GitHub 帐户，以访问为我们的 chatbot 第一版上的测试用例提供的示例代码: [`https://github.com/palashgoyal1/DL4NLP`](https://github.com/palashgoyal1/DL4NLP) 。该存储库包含您需要开始使用的四个重要文件。

的。gitignore file 告诉 Git 应该忽略哪些文件(或模式)。它有以下内容:

```py
> *.pyc
> .*

```

Procfile 用于声明各种流程类型，在我们的例子中，是一个 web 应用程序。

```py
> web: gunicorn app:app --log-file=-

```

Requirements.txt 安装 Python 依赖项。

```py
> Flask==0.11.1
> Jinja2==2.8
> MarkupSafe==0.23
> Werkzeug==0.11.10
> click==6.6
> gunicorn==19.6.0
> itsdangerous==0.24
> requests==2.10.0
> wsgiref==0.1.2
> chatterbot>=0.4.6
> urllib
> clarifai==2.0.30
> enum34

```

App.py 是包含 chatbot 应用程序主要代码的 Python 文件。由于文件很大，我们已经把它放在前面提到的 GitHub 存储库中。请读者访问，以供参考。这样，克隆存储库也将变得更加容易。

让我们设置 webhook。(webhook 是一个 HTTP 回调——一个在某件事情发生时发生的 HTTP POST，比如一个通过 HTTP POST 的简单事件通知。)我们使用了 Heroku，因为它提供了一个 webhook，脸书使用它来发送请求，并在发生任何事件时检索适当的结果。

访问您在 Heroku 中创建的应用程序，然后转到 Deploy 选项卡。有四种方法可以通过 Heroku Git、GitHub、Dropbox 和 Container Registry 部署应用程序(图 [4-14](#Fig14) )。为了简单起见，我们将使用 GitHub 部署我们的代码。

![A461351_1_En_4_Fig14_HTML.jpg](img/A461351_1_En_4_Fig14_HTML.jpg)

图 4-14

Heroku deploy app section

一旦我们选择了 Connect to GitHub，它将询问放置代码的 GitHub 存储库。确保这里提到的名称是正确的，并且主目录是存储库。选择正确的存储库后，点击连接按钮(图 [4-15](#Fig15) )。

![A461351_1_En_4_Fig15_HTML.jpg](img/A461351_1_En_4_Fig15_HTML.jpg)

图 4-15

Heroku deploy app via GitHub

代码将使用您的个人 GitHub 存储库的链接来部署，这个特定的应用程序的代码已经被放置在那里。在 Heroku 的 Settings 标签中，你可以在 Domains 和 Certificates 部分找到应用程序的域名，它看起来与`https://*******.herokuapp.com/`的格式相似。对于之前创建的测试应用程序，它是 [`https://dlnlpcbapp.herokuapp.com/`](https://dlnlpcbapp.herokuapp.com/) 。把它单独记下来，因为我们以后会需要它。

现在是整合脸书页面 Dl4nlp_cb 和 Heroku app dlnlpcbapp 的时候了。访问脸书应用仪表板，在显示页面访问令牌的 Messenger 设置选项卡下，转到 webhooks 以设置 webhook(图 [4-16](#Fig16) )。

![A461351_1_En_4_Fig16_HTML.jpg](img/A461351_1_En_4_Fig16_HTML.jpg)

图 4-16

Setting the webhook

弹出窗口将要求输入以下三个字段:

![A461351_1_En_4_Fig17_HTML.jpg](img/A461351_1_En_4_Fig17_HTML.jpg)

图 4-17

Setting the webhook—adding relevant information

*   回调 URL:我们之前设置的 Heroku URL(我们在步骤 1 中生成的设置 URL)
*   验证令牌:一个将被发送到您的 bot 的秘密值，以验证请求来自脸书。无论您在这里设置了什么值，请确保将其添加到您的 Heroku 环境中。
*   订阅字段:这告诉脸书你关心什么消息事件，并希望它通知你的 webhook。如果你不确定，检查所有的方框(图 [4-17](#Fig17) )。

Note

“回拨验证失败”是最常见的报告错误之一，当脸书在尝试将 Heroku 端点添加到脸书聊天应用程序时返回错误消息(图 [4-18](#Fig18) )时会遇到这种情况。

如果脸书发送的令牌与使用 Heroku 配置变量设置的令牌不匹配，Flask 应用程序会故意返回 403 禁止错误。

如果遇到图 [4-18](#Fig18) 所示的错误，这意味着 Heroku 配置值没有正确设置。在应用程序中从命令行运行`heroku config`并验证名为`VERIFY_TOKEN`的键被设置为等于在脸书窗口中键入的值，这将纠正错误。

回调 URL 框中显示的 URL 将是 Heroku 应用程序的 URL。

![A461351_1_En_4_Fig18_HTML.jpg](img/A461351_1_En_4_Fig18_HTML.jpg)

图 4-18

Error: “Callback verification failed”

webhook 的成功配置将带您进入另一个显示完成信息的屏幕(图 [4-19](#Fig19) )。

![A461351_1_En_4_Fig19_HTML.jpg](img/A461351_1_En_4_Fig19_HTML.jpg)

图 4-19

Successful webhook configuration

配置好 webhook 后，选择所需的脸书页面并点击订阅(图 [4-20](#Fig20) )。

![A461351_1_En_4_Fig20_HTML.jpg](img/A461351_1_En_4_Fig20_HTML.jpg)

图 4-20

Subscribe webhook to desired Facebook page Dl4nlp_cb

现在再次回到 Heroku 应用程序。在“设置”选项卡下，您会发现“配置变量选项”您必须设置两个变量:`PAGE_ACCESS_TOKEN`(从前面的步骤中选择)和`VERIFY_TOKEN`(从在应用仪表板中设置 webhook 时使用的变量中选择)。除了前面的两个参数外，还要从应用页面的基本设置中获取应用 ID 和 Api Secret token(图 [4-21](#Fig21) )。这两个也必须在 Heroku 配置参数中设置(单击 Show 按钮获得 Api Secret 令牌)。

![A461351_1_En_4_Fig21_HTML.jpg](img/A461351_1_En_4_Fig21_HTML.jpg)

图 4-21

Configuring Heroku settings

现在打开 Heroku 应用中的设置选项卡，将应用 ID 设置为`api_key`，应用秘密设置为`api_secret`，同时设置`PAGE_ACCESS_TOKEN`和`VERIFY_TOKEN`(图 [4-22](#Fig22) )。

![A461351_1_En_4_Fig22_HTML.jpg](img/A461351_1_En_4_Fig22_HTML.jpg)

图 4-22

Adding configuration variables in Heroku settings

保存配置参数后，转到 Heroku 上的 Deploy 选项卡，向下滚动到 Manual Deploy 部分，然后单击 Deploy Branch 按钮。这将部署从存储库中选择的当前分支，并进行必要的编译。通过检查日志部分，确保没有错误。

现在转到已创建的脸书页面，单击页面顶部“喜欢”按钮旁边的“消息”按钮。这将打开一个消息窗格，显示页面的消息框。开始和你定制的聊天机器人聊天吧(图 [4-23](#Fig23) )！

![A461351_1_En_4_Fig23_HTML.jpg](img/A461351_1_En_4_Fig23_HTML.jpg)

图 4-23

Enjoy your conversations with the chatbot!

## 聊天机器人:自动文本生成

在上一节中，我们使用不同的平台和库构建了一个简单的对话聊天机器人。它的问题是它只能处理一组固定的问题。如果我们能建造一个从现有的对话中学习的机器人，会怎么样？这就是自然语言生成派上用场的地方。我们将制作一个 seq2seq 模型，它可以处理任何类型的问题，也就是说，即使问题是由一些随机的单词组成的。这个答案在语法和语境上是否正确是一个完全不同的问题，取决于各种因素，如数据集的大小和质量。

在本节中，我们将尝试构建一个模型，该模型将一组问题和答案作为输入，并在被问及与输入数据相关的问题时预测答案。如果该问题与用于训练模型的问题集相匹配，则会以最佳方式回答该问题。

我们将使用序列到序列模型来解决所描述的问题。我们使用的数据集由从保险领域的客户服务站记录的问答组成。该数据集是从网站 [`www.insurancelibrary.com/`](http://www.insurancelibrary.com/) 收集的，是保险行业首次发布的此类问答语料库。这些问题属于客户就保险公司提供的多种服务和产品提出的一系列问题，答案由对保险行业有深入了解的专业人士给出。

用于训练的数据集取自 URL [`https://github.com/shuzi/insuranceQA`](https://github.com/shuzi/insuranceQA) ，目前位于 [`https://github.com/palashgoyal1/InsuranceQnA`](https://github.com/palashgoyal1/InsuranceQnA) ，此外还有用于问题、答案和词汇的所需文件。该数据集被 IBM 的几位员工用于论文“将深度学习应用于答案选择:一项研究和一项开放任务”( [`https://arxiv.org/pdf/1508.01585v2.pdf`](https://arxiv.org/pdf/1508.01585v2.pdf) )，他们使用了 CNN 框架的多个变体。在所有的变体中，他们都让模型学习了给定问题及其对应答案的单词嵌入，然后使用余弦距离作为相似性度量来衡量匹配程度。

图 [4-24](#Fig24) 是本文中演示的多种架构的快照。对于架构 II、III 和 IV，问答部分对于隐藏层和 CNN 层具有相同的权重。CNN <sub>Q</sub> 和 CNN <sub>A</sub> 层分别用于提取问答双方的特征。

![A461351_1_En_4_Fig24a_HTML.jpg](img/A461351_1_En_4_Fig24a_HTML.jpg)![A461351_1_En_4_Fig24b_HTML.jpg](img/A461351_1_En_4_Fig24b_HTML.jpg)T2】

图 4-24

Architectures used in the research paper

GitHub 存储库中的原始数据集结合了问题的训练、验证和测试分区。我们结合了给定的问题和答案，并在最终选择用于建模目的的 qna 之前执行了一些处理步骤。此外，一组序列到序列模型已被用于生成用户所提问题的答案。如果使用适当的模型进行训练，并且经过足够的迭代，该模型也将能够回答以前看不到的问题。

为了准备模型要使用的数据，我们做了一些更改，并完成了对初始给定数据集的选择。随后，我们利用整个数据集的词汇，以及问答中使用的单词标记，以英语可理解的格式创建问题及其相应答案的完美组合。

Note

在开始执行代码之前，请确保您已经安装了 TensorFlow 1 . 0 . 0 版，并且没有安装其他版本，因为 tensor flow 的后续更新版本中已经发生了变化。

以编码格式导入所需的包和数据集。

```py
import pandas as pd
import numpy as np
import tensorflow as tf
import re
import time
tf.__version__

> '1.0.0'

# Make sure the vocabulary.txt file and the encoded datasets for Question and Answer are present in the same folder
# reading vocabulary
lines = open('vocabulary.txt', encoding='utf-8', errors="ignore").read().split('\n')
# reading questions
conv_lines = open('InsuranceQAquestionanslabelraw.encoded', encoding='utf-8', errors="ignore").read().split('\n')
# reading answers
conv_lines1 = open('InsuranceQAlabel2answerraw.encoded', encoding='utf-8', errors="ignore").read().split('\n')

# The print command shows the token value associated with each of the words in the 3 datasets

print(" -- Vocabulary -- ")
print(lines[:2])

> -- Vocabulary –
> ['idx_17904\trating/result', 'idx_14300\tconsidered,']

print(" -- Questions -- ")
print(conv_lines[:2])

> -- Questions –
> ['medicare-insurance\tidx_1285 idx_1010 idx_467 idx_47610 idx_18488 idx_65760\t16696', 'long-term-care-insurance\tidx_3815 idx_604 idx_605 idx_891 idx_136 idx_5293 idx_65761\t10277']

print(" -- Answers -- ")
print(conv_lines1[:2])

> -- Answers –
> ['1\tidx_1 idx_2 idx_3 idx_4 idx_5 idx_6 idx_7 idx_8 idx_9 idx_10 idx_11 idx_12 idx_13 idx_14 idx_3 idx_12 idx_15 idx_16 idx_17 idx_8 idx_18 idx_19 idx_20 idx_21 idx_3 idx_12 idx_14 idx_22 idx_20 idx_23 idx_24 idx_25 idx_26 idx_27 idx_28 idx_29 idx_8 idx_30 idx_19 idx_11 idx_4 idx_31 idx_32 idx_22 idx_33 idx_34 idx_35 idx_36 idx_37 idx_30 idx_38 idx_39 idx_11 idx_40 idx_41 idx_42 idx_43 idx_44 idx_22 idx_45 idx_46 ...

```

在接下来的几行中，我们根据分配给问题和答案的 ID，将问题与其对应的答案组合在一起。

```py
id2line = {}
for line in vocab_lines:
    _line = line.split('\t')
    if len(_line) == 2:
        id2line[_line[0]] = _line[1]

# Creating the word tokens for both questions and answers, along with the mapping of the answers enlisted for questions
convs, ansid = [], []
for line in question_lines[:-1]:
    _line = line.split('\t')
    ansid.append(_line[2].split(' '))
    convs.append(_line[1])

convs1 = [ ]
for line in answer_lines[:-1]:
    _line = line.split('\t')
    convs1.append(_line[1])

print(convs[:2])  # word tokens present in the question
> ['idx_1285 idx_1010 idx_467 idx_47610 idx_18488 idx_65760', 'idx_3815 idx_604 idx_605 idx_891 idx_136 idx_5293 idx_65761']

print(ansid[:2])  # answers IDs mapped to the questions
> [['16696'], ['10277']]

print(convs1[:2])  # word tokens present in the answer
> ['idx_1 idx_2 idx_3 idx_4 idx_5 idx_6 idx_7 idx_8 idx_9 idx_10 idx_11 idx_12 idx_13 idx_14 idx_3 idx_12 idx_15 idx_16 idx_17 idx_8 idx_18 idx_19 idx_20 idx_21 ...

# Creating matching pair between questions and answers on the basis of the ID allocated to each.
questions, answers = [], []
for a in range(len(ansid)):
      for b in range(len(ansid[a])):
            questions.append(convs[a])

for a in range(len(ansid)):
      for b in range(len(ansid[a])):
            answers.append(convs1[int(ansid[a][b])-1])

ques, ans  =[], []
m=0
while m<len(questions):
       i=0
       a=[]
       while i < (len(questions[m].split(' '))):
            a.append(id2line[questions[m].split(' ')[i]])
            i=i+1
       ques.append(' '.join(a))
       m=m+1

n=0
while n<len(answers):  
        j=0
        b=[]
        while j < (len(answers[n].split(' '))):
            b.append(id2line[answers[n].split(' ')[j]])
            j=j+1
        ans.append(' '.join(b))
        n=n+1     

```

保险 QnA 数据集中前五个问题的以下输出将给出客户所提问题的类型以及专业人员给出的相应答案。在本练习的最后，我们的模型将尝试以类似于提问的方式提供答案。

```py
# Printing top 5 questions along with their answers
limit = 0
for i in range(limit, limit+5):
    print(ques[i])
    print(ans[i])
    print("---")

> What Does Medicare IME Stand For?
According to the Centers for Medicare and Medicaid Services website, cms.gov, IME stands for Indirect Medical Education and is in regards to payment calculation adjustments for a Medicare discharge of higher cost patients receiving care from teaching hospitals relative to non-teaching hospitals. I would recommend contacting CMS to get more information about IME
---
> Is Long Term Care Insurance Tax Free?
As a rule, if you buy a tax qualified long term care insurance policy (as nearly all are, these days), and if you are paying the premium yourself, there are tax advantages you will receive. If you are self employed, the entire premium is tax deductible. If working somewhere but paying your own premium for an individual or group policy, you can deduct the premium as a medical expense under the same IRS rules as apply to all medical expenses. In both situations, you also receive the benefits from the policy tax free, if they are ever needed.
---
> Can Husband Drop Wife From Health Insurance?
Can a spouse drop another spouse from health insurance? Usually not without the spouse's who is being dropped consent in writting. Most employers who have a quality HR department will require a paper trial for any changes in an employee's benefit plan. When changes are attempted that could come back to haunt the employer, steps are usually taken to comfirm something like this.
---
> Is Medicare Run By The Government?
Medicare Part A and Part B is provided by the Federal government for Americans who are 65 and older who have worked and paid Social Security taxes into the system. Medicare is also available to people under the age of 65 that have certain disabilities and people with End-Stage Renal Disease (ESRD).
---
> Is Medicare Run By The Government?
Definitely. It is ran by the Center for Medicare and Medicaid Services, a Government Agency given the responsibility of overseeing and administering Medicare and Medicaid. Even Medicare Advantage Plans, which are administered by private insurance companies are strongly regulated by CMMS. They work along with Social Security and Jobs and Family Services to insure that your benefits are available and properly administered.
---

```

虽然前面示例中的第四个和第五个问题是相同的，但它们有不同的答案，这取决于有多少专业人士回答了该问题。

```py
# Checking the count of the total number of questions and answers
print(len(questions))
>  27987

print(len(answers))
> 27987

```

通过用实际的扩展单词替换单词的短形式来创建文本清理功能，以便单词可以在以后被它们的实际标记替换。

```py
def clean_text(text):
        """Cleaning the text by replacing the abbreviated words with their proper full replacement, and converting all the characters to lower case"""

        text = text.lower()

        text = re.sub(r"i'm", "i am", text)
        text = re.sub(r"he's", "he is", text)
        text = re.sub(r"she's", "she is", text)
        text = re.sub(r"it's", "it is", text)
        text = re.sub(r"that's", "that is", text)
        text = re.sub(r"what's", "that is", text)
        text = re.sub(r"where's", "where is", text)
        text = re.sub(r"how's", "how is", text)
        text = re.sub(r"\'ll", " will", text)
        text = re.sub(r"\'ve", " have", text)
        text = re.sub(r"\'re", " are", text)
        text = re.sub(r"\'d", " would", text)
        text = re.sub(r"\'re", " are", text)
        text = re.sub(r"won't", "will not", text)
        text = re.sub(r"can't", "cannot", text)
        text = re.sub(r"n't", " not", text)
        text = re.sub(r"n'", "ng", text)
        text = re.sub(r"'bout", "about", text)
        text = re.sub(r"'til", "until", text)
        text = re.sub(r"[-()\"#/@;:<>{}`+=~|.!?,']", "", text)

        return text

# Applying the 'clean_text()' function on the set of Questions and Answers
clean_questions = []
for question in ques:
    clean_questions.append(clean_text(question))

clean_answers = []    
for answer in ans:
    clean_answers.append(clean_text(answer))

```

看看在对问题和答案执行清理操作后数据集是如何出现的。这个清理后的数据集将作为输入提供给我们的模型，以确保提供给模型的输入在结构和格式上彼此同步:

```py
limit = 0
for i in range(limit, limit+5):
    print(clean_questions[i])
    print(clean_answers[i])
    print()

> what does medicare ime stand for
according to the centers for medicare and medicaid services website cmsgov ime stands for indirect medical education and is in regards to payment calculation adjustments for a medicare discharge of higher cost patients receiving care from teaching hospitals relative to nonteaching hospitals i would recommend contacting cms to get more information about ime
----
> is long term care insurance tax free
as a rule if you buy a tax qualified long term care insurance policy as nearly all are these days and if you are paying the premium yourself there are tax advantages you will receive if you are self employed the entire premium is tax deductible if working somewhere but paying your own premium for an individual or group policy you can deduct the premium as a medical expense under the same irs rules as apply to all medical expenses in both situations you also receive the benefits from the policy tax free if they are ever needed
----
> can husband drop wife from health insurance
can a spouse drop another spouse from health insurance usually not without the spouses who is being dropped consent in writting most employers who have a quality hr department will require a paper trial for any changes in an employees benefit plan when changes are attempted that could come back to haunt the employer steps are usually taken to comfirm something like this
----
> is medicare run by the government
medicare part a and part b is provided by the federal government for americans who are 65 and older who have worked and paid social security taxes into the system medicare is also available to people under the age of 65 that have certain disabilities and people with endstage renal disease esrd
----
> is medicare run by the government
definitely it is ran by the center for medicare and medicaid services a government agency given the responsibility of overseeing and administering medicare and medicaid even medicare advantage plans which are administered by private insurance companies are strongly regulated by cmms they work along with social security and jobs and family services to insure that your benefits are available and properly administered
----

```

根据两个问题和答案中出现的单词数分析问题和答案，并检查不同区间的百分位数。

```py
lengths.describe(percentiles=[0,0.25,0.5,0.75,0.85,0.9,0.95,0.99])

>              counts
        count  55974.000000
        mean   54.176725
        std    67.638972
        min    2.000000
        0%     2.000000
        25%    7.000000
        50%    30.000000
        75%    78.000000
        85%    103.000000
        90%    126.000000
        95%    173.000000
        99%    314.000000
        max    1176.000000

```

由于提供给模型的数据需要所提问题的完整答案，而不是半生不熟的答案，因此我们必须确保我们为模型训练选择的问答组合在问题和答案中都有足够数量的单词，从而对单词数设置了最低限制。同时，我们希望该模型能够对问题产生简明扼要的答案，因此我们也对问题和答案中的字数设置了最大限制。

在这里，我们只列出最少两个单词、最多 100 个单词的文本。

```py
# Remove questions and answers that are shorter than 1 words and longer than 100 words.
min_line_length, max_line_length = 2, 100

# Filter out the questions that are too short/long
short_questions_temp, short_answers_temp = [], []

i = 0
for question in clean_questions:
    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:
        short_questions_temp.append(question)
        short_answers_temp.append(clean_answers[i])
    i += 1

# Filter out the answers that are too short/long
short_questions, short_answers = [], []

i = 0
for answer in short_answers_temp:
    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:
        short_answers.append(answer)
        short_questions.append(short_questions_temp[i])
    i += 1

```

执行上述选择后的数据集统计如下:

```py
print("# of questions:", len(short_questions))
> # of questions: 19108

print("# of answers:", len(short_answers))
> # of answers: 19108

print("% of data used: {}%".format(round(len(short_questions)/len(questions),4)*100))
> % of data used: 68.27%

```

直接输入文本的问题是模型不能处理可变长度的序列，下一个大问题是词汇量。解码器必须对大词汇量运行 softmax，比如说，对输出中的每个单词运行 20，000 个单词。这会减慢训练过程。那么，我们如何处理这个问题呢？填充。

填充是将可变长度序列转换为固定长度序列的一种方式。假设我们想要这个句子“你好吗？”为固定长度，比如说 10，在应用填充后，这一对被转换为[PAD，PAD，PAD，PAD，PAD，PAD，"？"、“你”、“是”、“怎么样”】。

```py
def pad_sentence_batch(sentence_batch, vocab_to_int):
"""Including <PAD> token in sentence to make all batches of same length"""
    max_sentence = max([len(sentence) for sentence in sentence_batch])
    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]

```

下面的代码映射新形成的训练数据集的词汇表中的单词，并为每个单词分配一个频率标记。

```py
# Create a dictionary for the frequency of the vocabulary
vocab = {}
for question in short_questions:
    for word in question.split():
        if word not in vocab:
            vocab[word] = 1
        else:
            vocab[word] += 1

for answer in short_answers:
    for word in answer.split():
        if word not in vocab:
            vocab[word] = 1
        else:
            vocab[word] += 1

```

与第 [2](2.html) 章中执行的操作一样，我们将删除训练数据集中出现频率较低的单词，因为这些单词不会向模型引入任何重要信息。

```py
# Remove rare words from the vocabulary.
threshold = 1
count = 0
for k,v in vocab.items():
    if v >= threshold:
        count += 1

print("Size of total vocab:", len(vocab))
> Size of total vocab: 18983

print("Size of vocab we will use:", count)
> Size of vocab we will use: 18983

# Create dictionaries to provide a unique integer for each word.
questions_vocab_to_int = {}

word_num = 0
for word, count in vocab.items():
    if count >= threshold:
        questions_vocab_to_int[word] = word_num
        word_num += 1

answers_vocab_to_int = {}

word_num = 0
for word, count in vocab.items():
    if count >= threshold:
        answers_vocab_to_int[word] = word_num
        word_num += 1

```

由于解码器生成了多个单词或定制符号，我们必须将新的标记添加到训练数据集的当前词汇中，并且也将这些标记包括在当前词典中。关于所包含的四个令牌的基本信息如下:

*   `GO`:与`<start>`令牌相同。它是馈送给解码器的第一个标记，与思想向量一起，开始为答案生成标记。
*   `EOS`:“句子结束”，与表示句子结束或回答完成的`<end>`标记相同。我们不能用标点符号来代替它，因为它们在上下文中有完全不同的含义。解码器一生成`EOS`令牌，它就表示答案的完成。
*   `UNK`:“未知”令牌。如果没有对单词的最小出现次数进行额外的检查/筛选，这用于替换词汇表中频率低得多的单词。例如，输入的句子`Insurance is highly criticalll1090`将被转换为`Insurance is highly <UNK>`。
*   `PAD`:由于训练数据是等长批量处理的，一批中的所有序列也是等长的，所以输入的句子将在句子所需的两边用`PAD`标记填充。例如，对于允许最大长度的情况，输入句子`Insurance is highly criticalll1090`将被转换为`Insurance is highly criticalll1090 <PAD> <PAD> <PAD> <PAD>`。

图 [4-25](#Fig25) 显示用户自定义令牌在模型响应中的用法(来源: [`http://colah.github.io/`](http://colah.github.io/) )。添加这些令牌的代码如下。

![A461351_1_En_4_Fig25_HTML.jpg](img/A461351_1_En_4_Fig25_HTML.jpg)

图 4-25

Sample encoder-decoder with usage of tokens

```py
# Adding unique tokens to the present vocabulary
codes = ['<PAD>','<EOS>','<UNK>','<GO>']

for code in codes:
    questions_vocab_to_int[code] = len(questions_vocab_to_int)+1

for code in codes:
    answers_vocab_to_int[code] = len(answers_vocab_to_int)+1

# Creating dictionary so as to map the integers to their respective words, inverse of vocab_to_int
questions_int_to_vocab = {v_i: v for v, v_i in questions_vocab_to_int.items()}
answers_int_to_vocab = {v_i: v for v, v_i in answers_vocab_to_int.items()}

print(len(questions_vocab_to_int))
> 18987

print(len(questions_int_to_vocab))
> 18987

print(len(answers_vocab_to_int))
> 18987

print(len(answers_int_to_vocab))
> 18987

```

我们试图减少有效词汇表的大小，这将加快训练和测试步骤，方法是简单地将它限制在一个很小的数目，并用一个`UNK`标签替换词汇表之外的单词。现在，训练和测试时间都可以显著减少，但这显然并不理想，因为我们可能会生成带有大量`UNK`的输出，但现在，我们确保这些令牌的百分比足够低，我们不会面临任何严重的问题。

此外，在我们将数据输入模型之前，我们必须将句子中的每个单词转换为唯一的整数。这可以通过建立一个包含所有单词的词汇表并给它们分配唯一的编号来实现(一键编码向量)。

```py
# Convert the text to integers, and replacing any of the words not present in the respective vocabulary with <UNK> token
questions_int = []
for question in short_questions:
    ints = []
    for word in question.split():
        if word not in questions_vocab_to_int:
            ints.append(questions_vocab_to_int['<UNK>'])
        else:
            ints.append(questions_vocab_to_int[word])
    questions_int.append(ints)

answers_int = []
for answer in short_answers:
    ints = []
    for word in answer.split():
        if word not in answers_vocab_to_int:
            ints.append(answers_vocab_to_int['<UNK>'])
        else:
            ints.append(answers_vocab_to_int[word])
    answers_int.append(ints)

```

进一步检查被替换为`<UNK>`标记的单词数。由于我们已经完成了预处理步骤，删除了词汇表中出现频率较低的单词，因此没有一个单词会被替换为`<UNK>`标记。但是，建议将它们包含在通用脚本中。

```py
# Calculate what percentage of all words have been replaced with <UNK>
word_count = 0
unk_count = 0

for question in questions_int:
    for word in question:
        if word == questions_vocab_to_int["<UNK>"]:
            unk_count += 1
        word_count += 1

for answer in answers_int:
    for word in answer:
        if word == answers_vocab_to_int["<UNK>"]:
            unk_count += 1
        word_count += 1

unk_ratio = round(unk_count/word_count,4)*100

print("Total number of words:", word_count)
> Total number of words: 1450824

print("Number of times <UNK> is used:", unk_count)
> Number of times <UNK> is used: 0

print("Percent of words that are <UNK>: {}%".format(round(unk_ratio,3)))
> Percent of words that are <UNK>: 0.0%

```

根据问题中的单词数创建问题和答案的有序集合。以这种方式对文本进行排序将有助于我们稍后使用的填充方法。

```py
# Next, sorting the questions and answers on basis of the length of the questions.
# This exercise will reduce the amount of padding being done during the training process.
# This will speed up the training process and reduce the training loss.

sorted_questions = []
short_questions1 = []
sorted_answers = []
short_answers1= []

for length in range(1, max_line_length+1):
    for i in enumerate(questions_int):
        if len(i[1]) == length:
            sorted_questions.append(questions_int[i[0]])
            short_questions1.append(short_questions[i[0]])
            sorted_answers.append(answers_int[i[0]])
            short_answers1.append(short_answers[i[0]])

print(len(sorted_questions))
> 19108
print(len(sorted_answers))
> 19108
print(len(short_questions1))
> 19108
print(len(short_answers1))
> 19108
print()

for i in range(3):
    print(sorted_questions[i])
    print(sorted_answers[i])
    print(short_questions1[i])
    print(short_answers1[i])
    print()

> [219, 13]
[219, 13, 58, 2310, 3636, 1384, 3365... ]

why can
why can a simple question but yet so complex why can someone do this or why can someone do that i have often pondered for hours to come up with the answer and i believe after years of thoughtprovoking consultation with friends and relativesi have the answer to the question why can the answer why not

[133, 479, 56]
[242, 4123, 3646, 282, 306, 56, ... ]

who governs annuities
if youre asking about all annuities then here are two governing bodies for variable annuities finra and the department of insurance variable products like variable annuities are registered products and come under the oversight of finras jurisdiction but because it is an annuity insurance product as well it falls under the department of insurance non finra annuities are governed by the department of insurance in each state

[0, 201, 56]
[29, 202, 6, 29, 10, 3602, 58, 36, ... ]

what are annuities
an annuity is an insurance product a life insurance policy protects you from dying too soon an annuity protects you from living too long annuities are complex basically in exchange for a sum of money either immediate or in installments the company will pay the annuitant a specific amount normally monthly for the life of the annuitant there are many modifications of this basic form annuities are taxed differently from other programs

```

从排序对中检查一个随机问题答案。

```py
print(sorted_questions[1547])
> [37, 6, 36, 10, 466]

print(short_questions1[1547])
> how is life insurance used

print(sorted_answers[1547])
> [8, 36, 10, 6, 466, 26, 626, 58, 199, 200, 1130, 58, 3512, 31, 105, 208, 601, 10, 6, 466, 26, 626, ...

print(short_answers1[1547])
> term life insurance is used to provide a death benefit during a specified period of time permanent insurance is used to provide a death benefit at any time the policy is in force in order to accomplish this and have level premiums policies accumulate extra funds these funds are designed to allow the policy to meet its lifelong obligations however these funds accumulate tax free and give the policy the potential of solving many problems from funding education to providing long term care

```

现在是时候定义 seq2seq 模型将使用的助手函数了。其中一些函数来自 GitHub 代码库( [`https://github.com/Currie32/Chatbot-from-Movie-Dialogue`](https://github.com/Currie32/Chatbot-from-Movie-Dialogue) )，它有类似的应用。

定义函数来为我们的模型输入创建占位符。

```py
def model_inputs():
    input_data = tf.placeholder(tf.int32, [None, None], name="input")
    targets = tf.placeholder(tf.int32, [None, None], name="targets")
    lr = tf.placeholder(tf.float32, name="learning_rate")
    keep_prob = tf.placeholder(tf.float32, name="keep_prob")
    return input_data, targets, lr, keep_prob

```

删除每个批次中的最后一个单词 ID，并在每个批次的开头添加`<GO>`标记。

```py
def process_encoding_input(target_data, vocab_to_int, batch_size):

    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])
    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)

    return dec_input

```

正常的 RNN 处理过去的状态(将它们保存在记忆中)，但是如果你想以某种方式将未来也包含在上下文中呢？通过使用双向 RNNs，我们可以将两个方向相反的隐藏层连接到同一个输出。通过这种结构，输出层可以从过去和未来的状态中获取信息。

因此，我们用 LSTM 单元和双向编码器定义了 seq2seq 模型的编码层。编码器层的状态，即权重，被作为解码层的输入。

```py
def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):
    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)
    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)
    enc_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)
    _, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = enc_cell, cell_bw = enc_cell, sequence_length = sequence_length, inputs = rnn_inputs, dtype=tf.float32)
    return enc_state

```

已经使用了第 [3 章](3.html)中解释的注意机制。这将大大减少产生的损失。注意状态被设置为 0，以最大化模型性能，并且对于注意机制，使用较便宜的 Bahdanau 注意。关于 Luong 和 Bahdanau 注意力技术的比较，请参考论文“基于注意力的神经机器翻译的有效方法”( [`https://arxiv.org/pdf/1508.04025.pdf`](https://arxiv.org/pdf/1508.04025.pdf) )。

```py
def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob, batch_size):

    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])

    att_keys, att_vals, att_score_fn, att_construct_fn = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option="bahdanau", num_units=dec_cell.output_size)

    train_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0], att_keys, att_vals,  att_score_fn, att_construct_fn,  name = "attn_dec_train")

    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, train_decoder_fn,  dec_embed_input, sequence_length, scope=decoding_scope)
    train_pred_drop = tf.nn.dropout(train_pred, keep_prob)

    return output_fn(train_pred_drop)

```

`decoding_layer_infer()`函数为查询的问题创建正确的响应。该函数利用额外的注意力参数来预测答案中的单词，并且它不像在最终评分阶段那样与任何漏失相关联。这里，在生成答案时，不考虑退出，以便利用网络中存在的所有神经元。

```py
def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,
                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob, batch_size):

    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])

    att_keys, att_vals, att_score_fn, att_construct_fn = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option="bahdanau", num_units=dec_cell.output_size)

    infer_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_inference(output_fn, encoder_state[0],  att_keys, att_vals,  att_score_fn, att_construct_fn,
                        dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size, name = "attn_dec_inf")

    infer_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, infer_decoder_fn, scope=decoding_scope)

    return infer_logits

```

`decoding_layer()`函数使用截尾正态分布创建推理和训练逻辑，并用给定的标准偏差初始化权重和偏差。

```py
def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,
                   num_layers, vocab_to_int, keep_prob, batch_size):

    with tf.variable_scope("decoding") as decoding_scope:
        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)
        drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)
        dec_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)

        weights = tf.truncated_normal_initializer(stddev=0.1)
        biases = tf.zeros_initializer()
        output_fn = lambda x: tf.contrib.layers.fully_connected(x, vocab_size, None,  scope=decoding_scope, weights_initializer = weights, biases_initializer = biases)

        train_logits = decoding_layer_train(encoder_state, dec_cell,  dec_embed_input, sequence_length,  decoding_scope, output_fn, keep_prob, batch_size)

        decoding_scope.reuse_variables()
        infer_logits = decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, vocab_to_int['<GO>'], vocab_to_int['<EOS>'],
                    sequence_length - 1, vocab_size,  decoding_scope, output_fn, keep_prob, batch_size)

    return train_logits, infer_logits

```

`seq2seq_model()`函数用于将所有之前定义的函数放在一起，并使用随机均匀分布初始化嵌入。该函数将在最终图形中用于计算训练和推理逻辑。

```py
def seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, answers_vocab_size,
                  questions_vocab_size, enc_embedding_size, dec_embedding_size, rnn_size, num_layers,
                  questions_vocab_to_int):

    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, answers_vocab_size+1,  enc_embedding_size, initializer = tf.random_uniform_initializer(0,1))

    enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, sequence_length)

    dec_input = process_encoding_input(target_data, questions_vocab_to_int, batch_size)
    dec_embeddings = tf.Variable(tf.random_uniform([questions_vocab_size+1, dec_embedding_size], 0, 1))
    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)

    train_logits, infer_logits = decoding_layer(dec_embed_input, dec_embeddings, enc_state, questions_vocab_size,
                            sequence_length, rnn_size, num_layers, questions_vocab_to_int,  keep_prob, batch_size)

    return train_logits, infer_logits

```

当训练实例总数(N)较大时，少量训练实例(B <<n which="" constitute="" a="" batch="" can="" be="" used="" in="" one="" iteration="" to="" estimate="" the="" gradient="" of="" loss="" function="" and="" update="" parameters="" id="ITerm93">网络。</n>

Note

使用整个训练数据一次需要 n (=N/B)次迭代。这构成了一个时代。因此，参数更新的总次数是(N/B)*E，其中 E 是历元数。

最后，我们定义了我们的 seq2seq 模型，它将接受编码和解码部分，并同时训练它们。现在，设置以下模型参数并启动会话进行优化。

*   历元:单次通过整个训练集
*   批量大小:输入中同时出现的句子数量
*   Rnn_size:隐藏层中的节点数
*   Num_layers:隐藏层数
*   嵌入大小:嵌入尺寸
*   学习率:一个网络抛弃旧的信念去追求新的信念的速度有多快
*   保持概率:用于控制掉线。辍学是一个简单的技术，以防止过度拟合。它本质上通过使它们为零来丢弃层中的一些单位激活。

```py
# Setting the model parameters
epochs = 50
batch_size = 64
rnn_size = 512
num_layers = 2
encoding_embedding_size = 512
decoding_embedding_size = 512
learning_rate = 0.005
learning_rate_decay = 0.9
min_learning_rate = 0.0001
keep_probability = 0.75

tf.reset_default_graph()
# Starting the session
sess = tf.InteractiveSession()

# Loading the model inputs    
input_data, targets, lr, keep_prob = model_inputs()

# Sequence length is max_line_length for each batch
sequence_length = tf.placeholder_with_default(max_line_length, None, name="sequence_length")

# Finding shape of the input data for sequence_loss
input_shape = tf.shape(input_data)

# Create the training and inference logits
train_logits, inference_logits = seq2seq_model( tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(answers_vocab_to_int),
    len(questions_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers,  questions_vocab_to_int)

# Create inference logits tensor
tf.identity(inference_logits, 'logits')

with tf.name_scope("optimization"):
    # Calculating Loss function
    cost = tf.contrib.seq2seq.sequence_loss( train_logits, targets, tf.ones([input_shape[0], sequence_length]))

    # Using Adam Optimizer
    optimizer = tf.train.AdamOptimizer(learning_rate)

    # Performing Gradient Clipping to handle the vanishing gradient problem
    gradients = optimizer.compute_gradients(cost)
    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]
    train_op = optimizer.apply_gradients(capped_gradients)

```

`batch_data()`函数有助于为问题和答案创建批处理。

```py
def batch_data(questions, answers, batch_size):

    for batch_i in range(0, len(questions)//batch_size):
        start_i = batch_i * batch_size
        questions_batch = questions[start_i:start_i + batch_size]
        answers_batch = answers[start_i:start_i + batch_size]
        pad_questions_batch = np.array(pad_sentence_batch(questions_batch, questions_vocab_to_int))
        pad_answers_batch = np.array(pad_sentence_batch(answers_batch, answers_vocab_to_int))
        yield pad_questions_batch, pad_answers_batch

```

保留总数据集的 15%用于验证，其余 85%用于训练模型。

```py
# Creating train and validation datasets for both questions and answers, with 15% to validation
train_valid_split = int(len(sorted_questions)*0.15)

train_questions = sorted_questions[train_valid_split:]
train_answers = sorted_answers[train_valid_split:]

valid_questions = sorted_questions[:train_valid_split]
valid_answers = sorted_answers[:train_valid_split]

print(len(train_questions))
print(len(valid_questions))

```

设置训练参数并初始化声明的变量。

```py
display_step = 20        # Check training loss after every 20 batches

stop_early = 0

stop = 5                 # If the validation loss decreases after 5 consecutive checks, stop training

validation_check = ((len(train_questions))//batch_size//2)-1        # Counter for checking validation loss

total_train_loss = 0     # Record the training loss for each display step

summary_valid_loss = []     # Record the validation loss for saving improvements in the model

checkpoint= "./best_model.ckpt"   # creating the checkpoint file in the current directory

sess.run(tf.global_variables_initializer())

```

训练模型。

```py
for epoch_i in range(1, epochs+1):
    for batch_i, (questions_batch, answers_batch) in enumerate(
            batch_data(train_questions, train_answers, batch_size)):
        start_time = time.time()
        _, loss = sess.run(
            [train_op, cost],
            {input_data: questions_batch, targets: answers_batch,  lr: learning_rate,
             sequence_length: answers_batch.shape[1], keep_prob: keep_probability})

        total_train_loss += loss
        end_time = time.time()
        batch_time = end_time - start_time

        if batch_i % display_step == 0:
            print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'
                  .format(epoch_i, epochs, batch_i,
                          len(train_questions) // batch_size, total_train_loss / display_step,
                          batch_time*display_step))
            total_train_loss = 0

        if batch_i % validation_check == 0 and batch_i > 0:
            total_valid_loss = 0
            start_time = time.time()
            for batch_ii, (questions_batch, answers_batch) in enumerate(batch_data(valid_questions, valid_answers, batch_size)):
                valid_loss = sess.run(
                cost, {input_data: questions_batch, targets: answers_batch, lr: learning_rate,
                       sequence_length: answers_batch.shape[1], keep_prob: 1})
                total_valid_loss += valid_loss
            end_time = time.time()
            batch_time = end_time - start_time
            avg_valid_loss = total_valid_loss / (len(valid_questions) / batch_size)
            print('Valid Loss: {:>6.3f}, Seconds: {:>5.2f}'.format(avg_valid_loss, batch_time))

            # Reduce learning rate, but not below its minimum value
            learning_rate *= learning_rate_decay
            if learning_rate < min_learning_rate:
                learning_rate = min_learning_rate

            summary_valid_loss.append(avg_valid_loss)
            if avg_valid_loss <= min(summary_valid_loss):
                print('New Record!')
                stop_early = 0
                saver = tf.train.Saver()
                saver.save(sess, checkpoint)

            else:
                print("No Improvement.")
                stop_early += 1
                if stop_early == stop:
                    break

    if stop_early == stop:
        print("Stopping Training.")
        break

> Epoch   1/50 Batch    0/253 - Loss:  0.494, Seconds: 1060.06
> Epoch   1/50 Batch   20/253 - Loss:  8.450, Seconds: 905.71
> Epoch   1/50 Batch   40/253 - Loss:  4.540, Seconds: 933.88
> Epoch   1/50 Batch   60/253 - Loss:  4.401, Seconds: 740.15
> Epoch   1/50 Batch   80/253 - Loss:  4.453, Seconds: 831.04
> Epoch   1/50 Batch  100/253 - Loss:  4.338, Seconds: 774.67
> Epoch   1/50 Batch  120/253 - Loss:  4.295, Seconds: 832.49
Valid Loss:  4.091, Seconds: 675.05
New Record!
> Epoch   1/50 Batch  140/253 - Loss:  4.255, Seconds: 822.40
> Epoch   1/50 Batch  160/253 - Loss:  4.232, Seconds: 888.85
> Epoch   1/50 Batch  180/253 - Loss:  4.168, Seconds: 858.95
> Epoch   1/50 Batch  200/253 - Loss:  4.093, Seconds: 849.23
> Epoch   1/50 Batch  220/253 - Loss:  4.034, Seconds: 846.77
> Epoch   1/50 Batch  240/253 - Loss:  4.005, Seconds: 809.77
Valid Loss:  3.903, Seconds: 509.83
New Record!
...
...
...
...
...

```

定义`question_to_seq()`函数，从用户那里获取输入问题，或者从数据集中选取一个随机问题，并将其转换为模型使用的整数格式。

```py
def question_to_seq(question, vocab_to_int):
    """Creating the question to be taken as input by the model"""
    question = clean_text(question)
    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in question.split()]

```

现在是从本节开始时种下的树上获得果实的时候了。因此，这里我们将通过给出一个随机问题作为输入来检查 seq2seq 模型的输出。答案将由经过训练的模型生成。

```py
# Selecting a random question from the full lot
random = np.random.choice(len(short_questions))
input_question = short_questions[random]
print(input_question)

> what exactly does adjustable life insurance mean

# Transforming the selected question in the desired format of IDs and Words
input_question = question_to_seq(input_question, questions_vocab_to_int)

# Applying Padding to the question to reach the max_line_length
input_question = input_question + [questions_vocab_to_int["<PAD>"]] * (max_line_length - len(input_question))

# Correcting the shape of input_data, by adding the empty questions
batch_shell = np.zeros((batch_size, max_line_length))

# Setting the input question as the first question
batch_shell[0] = input_question    

# Passing  input question to the model
answer_logits = sess.run(inference_logits, {input_data: batch_shell, keep_prob: 1.0})[0]

# Removing padding from Question and Answer both
pad_q = questions_vocab_to_int["<PAD>"]
pad_a = answers_vocab_to_int["<PAD>"]

# Printing the final Answer output by the model
print('Question')
print('Word Ids: {}'.format([i for i in input_question if i != pad_q]))
print('Input Words: {}'.format([questions_int_to_vocab[i] for i in input_question if i != pad_q]))
print('\n')

> Question

> Word Ids: [17288, 16123, 9831, 13347, 1694, 11205, 7655]

> Input Words: ['what', 'exactly', 'does', 'adjustable', 'life', 'insurance', 'mean']

print('\nAnswer')
print('Word Ids: {}'.format([i for i in np.argmax(answer_logits, 1) if i != pad_a]))

print('Response Words: {}'.format([answers_int_to_vocab[i] for i in np.argmax(answer_logits, 1) if i != pad_a]))

print('\n')

print(' '.join(([questions_int_to_vocab[i] for i in input_question if i != pad_q])))

print(' '.join(([answers_int_to_vocab[i] for i in np.argmax(answer_logits, 1) if i != pad_a])))

> Answer

>   Word Ids:      [10130, 10344, 13123, 2313, 1133, 1694, 11205, 6968, 966, 10130, 3030, 2313, 5964, 10561, 10130, 9158, 17702, 13344, 13278, 10130, 7457, 14167, 17931, 14479, 10130, 6968, 9158, 8521, 10130, 9158, 17702, 12230, 10130, 6968, 8679, 1688, 10130, 7457, 14167, 17931, 9472, 10130, 9158, 12230, 10130, 6968, 8679, 1688, 10130, 7457, 14167, 17931, 18293, 10130, 16405, 16640, 6396, 3613, 2313, 10130, 6968, 10130, 6968, 8679, 1688, 10130, 7457, 14167, 17931, 18293, 10130, 16405, 16640, 6396, 3613, 10628, 13040, 10130, 6968]

>   Response Words: ['the', 'face', 'value', 'of', 'a', 'life', 'insurance', 'policy', 'is', 'the', 'amount', 'of', 'time', 'that', 'the', 'insured', 'person', 'passes', 'with', 'the', 'death', 'benefit', 'proceeds', 'from', 'the', 'policy', 'insured', 'if', 'the', 'insured', 'person', 'dies', 'the', 'policy', 'will', 'pay', 'the', 'death', 'benefit', 'proceeds', 'whenever', 'the', 'insured', 'dies', 'the', 'policy', 'will', 'pay', 'the', 'death', 'benefit', 'proceeds', 'within', 'the', 'two', 'year', 'contestability', 'period', 'of', 'the', 'policy', 'the', 'policy', 'will', 'pay', 'the', 'death', 'benefit', 'proceeds', 'within', 'the', 'two', 'year', 'contestability', 'period', 'specified', 'in', 'the', 'policy']

> what exactly does adjustable life insurance mean

> the face value of a life insurance policy is the amount of time that the insured person passes with the death benefit proceeds from the policy insured if the insured person dies the policy will pay the death benefit proceeds whenever the insured dies the policy will pay the death benefit proceeds within the two year contestability period of the policy the policy will pay the death benefit proceeds within the two year contestability period specified in the policy

```

最后一段是问题“可调寿险到底是什么意思？”我们放入模型中。嗯，这听起来在语法上不正确，但这是一个完全不同的问题，可以通过用更多的数据集和精炼的嵌入来训练模型，以更好的方式来处理。

假设随着时间的推移，对话文本中没有发生重大更新，人们可以利用经过训练的模型对象，并在聊天机器人应用程序中吸收它，以对聊天机器人的最终用户提出的问题做出漂亮的回答。这是留给读者的一个练习。享受与您自己的聊天机器人交谈！为了增加乐趣，你可以试着在与朋友的个人聊天中训练这个模型，看看你的聊天机器人是否能够成功地模仿你所爱的人。现在你知道了，要创建一个功能齐全的聊天机器人，只需要两个人的对话文本文件。

## 后续步骤

本章利用了第 [3](3.html) 章中解释的概念，帮助制作了一个聊天机器人，并训练了一个可以进一步嵌入 Facebook Messenger 聊天机器人的文本生成模型。在第 [5](5.html) 章中，我们将展示从第五届学习表征国际会议(2017 年 ICLR)上发布的一篇论文中提取的情感分类的实现。我们建议读者复制本章中的示例，并在不同的可用公共数据集上探索文本生成技术的不同用例。