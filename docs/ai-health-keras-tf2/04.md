# 4.根据临床记录预测医疗账单代码

临床记录包含关于医生开出的处方程序和诊断的信息，并且在当前的医疗系统中用于准确计费，但是它们并不容易获得。我们必须手动提取它们，或者使用一些辅助技术来无缝地执行这个过程。

这增加了支付者和提供者的管理成本。仅医疗服务提供商在保险和医疗账单成本上就花费了大约 2820 亿美元。良好的记录和质量跟踪是额外的成本。与每种类型的就诊相关的专业收入相比，急诊就诊产生的账单成本最高，相当于收入的 25.2%。

在本章中，您将深入了解 BERT 和 transformer 架构，探索最新的 transformer 模型。您还将了解如何将不同的微调技术应用于 transformer 模型。最后，您将学习在 NLP 中使用迁移学习的概念，并将多标签分类作为下游任务。

从非结构化临床记录中预测诊断和程序可以节省时间、消除错误并最大限度地降低成本，所以让我们开始吧。

## 介绍

首先，我说的这些 ICD 电码是什么？那些熟悉 ICD 电码的人可能会混淆 ICD-9 和 ICD-10 电码之间的区别。

ICD 代表国际疾病分类，它是由卫生与公众服务部管理和维护的一套标准代码(还记得第 [1](01.html) 章的 HHS 吗？).这些代码用于准确测量结果和为患者提供的护理，同时还为研究和临床决策提供了一种结构化的疾病和症状报告方式。

HHS 要求 HIPAA 法案下的所有实体必须将其 ICD 代码转换为 ICD-10 格式。这样做有各种原因，但最主要的是

*   **跟踪新的疾病和健康状况**:旧系统包含大约 17.8K 个不同的 ICD 代码，但 ICD-10 将超过 15 万种状况和疾病映射到不同的代码。

*   更大的空间允许更好和更准确地定义 ICD 编码，并支持流行病学研究，如疾病的共病或严重程度等。

*   防止报销欺诈

由于 MIMIC 3 包含新的代码系统被授权之前的 EHR 数据，您可以轻松地继续使用现有的 ICD 数据，但请记住这一点，以防您看到新的 EHR 数据。别担心。您可以亲自动手，将从这里学到的知识应用到新的 ICD 公约中。

由于有许多 ICD-9 代码，实际上，您只需尝试识别前 15 个 ICD-9 代码，这取决于有多少住院患者贴上了特定的 ICD-9 代码标签。

我已经深入讨论了 MIMIC 3 数据，所以让我们只关注选择正确的表和概述准备数据的步骤。让我们深入研究一下。

图 [4-1](#Fig1) 显示了 ICD-9 和 ICD-10 CM 代码的差异。注意，ICD 码有两种类型。

![../images/502837_1_En_4_Chapter/502837_1_En_4_Fig1_HTML.jpg](../images/502837_1_En_4_Chapter/502837_1_En_4_Fig1_HTML.jpg)

图 4-1

ICD 9 厘米和 ICD 10 厘米诊断编码系统

*   CM(临床修改) :住院和门诊数据的诊断编码

*   PCS(程序编码系统) :住院数据的程序编码

## 数据

我在上一章深入讨论了 MIMIC 3 数据集，所以让我们直接开始创建数据。

### 注释事件

此表包含与患者入院后记录的所有临床记录相关的文本。在`NOTEEVENTS`表中要查看的两个重要列是`CATEGORY`和`DESCRIPTION`。`CATEGORY`包含匿名的临床记录，`DESCRIPTION`告诉我们这些是完整的报告还是附录。

因为用例的中心是降低提供商和支付者的管理成本，所以该信息的最佳来源是“出院总结-报告”。

```py
    n_rows = 100000

# create the iterator
noteevents_iterator = pd.read_csv(
        "./Data/NOTEEVENTS.csv",
    iterator=True,
    chunksize=n_rows)

# concatenate according to a filter to get our noteevents data
    noteevents = pd.concat( [noteevents_chunk[np.logical_and(noteevents_chunk.CATEGORY.isin(["Discharge summary"]), noteevents_chunk.DESCRIPTION.isin(["Report"]))]
    for noteevents_chunk in noteevents_iterator])

noteevents.HADM_ID = noteevents.HADM_ID.astype(int)

```

现在您已经有了自己的数据集，让我们稍微探索一下。

**主键上的重复**:尽管`SUBJECT_ID`和`HADM_ID`对应该有一个唯一的记录，但是`NOTEEVENTS`数据集中还是有重复的。

经过进一步调查，看起来记录在不同的日期对相同的入院 ID 有不同的出院摘要文本。这看起来像一个不可能的事件，因此是一个更数据的问题。现在，您将对`CHARTDATE`列中的数据进行排序，并保留第一个条目。

```py
try:
        assert len(noteevents.drop_duplicates(["SUBJECT_ID","HADM_ID"])) == len(noteevents)
except AssertionError as e:
        print("There are duplicates on Primary Key Set")

    noteevents.CHARTDATE  = pd.to_datetime(noteevents.CHARTDATE , format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')
    pd.set_option('display.max_colwidth',50)
    noteevents.sort_values(["SUBJECT_ID","HADM_ID","CHARTDATE"], inplace =True)
    noteevents.drop_duplicates(["SUBJECT_ID","HADM_ID"], inplace = True)

noteevents.reset_index(drop = True, inplace = True)

```

在移动到下一个数据源查看文本数据之前，还有一件事要做。您可以在下面看到文本的样本摘要:

```py
    Admission Date: [**2118-6-2**] Discharge Date: [**2118-6-14**]

Date of Birth: Sex: F

Service: MICU and then to [**Doctor Last Name **] Medicine

    HISTORY OF PRESENT ILLNESS: This is an 81-year-old female
with a history of emphysema (not on home O2), who presents
with three days of shortness of breath thought by her primary
care doctor to be a COPD flare. Two days prior to admission,
she was started on a prednisone taper and one day prior to
admission she required oxygen at home in order to maintain
    oxygen saturation greater than 90%. She has also been on
levofloxacin and nebulizers, and was not getting better, and
    presented to the [**Hospital1 18**] Emergency Room.

```

您可以看到某些可用于清理文本的图案:

1.  **匿名**日期、患者姓名、医院和医生姓名

2.  **使用一种模式**如“主题:文本”如“入院日期:`[**2118-6-2**]:, "HISTORY OF PRESENT ILLNESS: This is an 81-year-old female....`

3.  **使用换行符** ("\n ")

您将利用所有这些模式来清理数据，并确保每个独特的句子得到正确记录。

你要做两件事。首先，你要确保所有不相关的话题都从出院小结中删除。对于这一点，你会发现最常见的话题。

```py
import re
import itertools

    def clean_text(text):
        return [x for x in list(itertools.chain.from_iterable([t.split("<>") for t in text.replace("\n"," ").split("|")])) if len(x) > 0]

    most_frequent_tags = [re.match("^(.*?):",x).group() for text in noteevents.TEXT for x in text.split("\n\n") if pd.notnull(re.match("^(.*?):",x))]
    pd.Series(most_frequent_tags).value_counts().head(10)

```

图 [4-2](#Fig2) 中显示了最常见主题标签的摘录。

![../images/502837_1_En_4_Chapter/502837_1_En_4_Fig2_HTML.jpg](../images/502837_1_En_4_Chapter/502837_1_En_4_Fig2_HTML.jpg)

图。4-2

出院小结中最常见的话题

```py
    irrelevant_tags = ["Admission Date:", "Date of Birth:", "Service:", "Attending:", "Facility:", "Medications on Admission:", "Discharge Medications:", "Completed by:", "Dictated By:" , "Department:" , "Provider:"]

    updated_text = ["<>".join(["|".join(re.split("\n\d|\n\s+",re.sub("^(.*?):","",x).strip())) for x in text.split("\n\n") if pd.notnull(re.match("^(.*?):",x)) and re.match("^(.*?):",x).group() not in irrelevant_tags ]) for text in noteevents.TEXT]
    updated_text = [re.sub("(\[.*?\])", "", text) for text in updated_text]

    updated_text = ["|".join(clean_text(x)) for x in updated_text]
    noteevents["CLEAN_TEXT"] = updated_text

```

对于上面的示例，下面是清理后的文本。很漂亮，对吧？

```py
    'This is an 81-year-old female with a history of emphysema (not on home O2), who presents with three days of shortness of breath thought by her primary care doctor to be a COPD flare. Two days prior to admission, she was started on a prednisone taper and one day prior to admission she required oxygen at home in order to maintain oxygen saturation greater than 90%. She has also been on levofloxacin and nebulizers, and was not getting better, and presented to the Emergency Room.',

     'Fevers, chills, nausea, vomiting, night sweats, change in weight, gastrointestinal complaints, neurologic changes

, rashes, palpitations, orthopnea. Is positive for the following: Chest pressure occasionally with shortness of breath with exertion, some shortness of breath that is positionally related, but is improved with nebulizer treatment.'

```

### 诊断 _ICD

这是 ICD-9 代码表。它包含与受试者入院事件相关的所有 ICD-9 代码。正如引言中所讨论的，您正在为手头的问题寻找前 15 个最常见的 ICD-9 代码。

```py
    top_values = (icd9_code.groupby('ICD9_CODE').
                  agg({"SUBJECT_ID": "nunique"}).
                  reset_index().sort_values(['SUBJECT_ID'], ascending = False).ICD9_CODE.tolist()[:15])

icd9_code = icd9_code[icd9_code.ICD9_CODE.isin(top_values)]

```

## 理解语言建模如何工作

在您直接使用 BERT 之前，让我们先了解一下它是如何工作的，构建模块是什么，为什么需要它，等等。

谷歌 AI 语言团队在 2018 年发布的题为“BERT:用于语言理解的深度双向变压器的预训练”的论文，是非研究社区对语言建模的新形式和变压器模型的应用真正感到兴奋的时候。2017 年，谷歌大脑团队在一篇题为“注意力是你所需要的一切”的论文中介绍了变形金刚模型。

很有趣，对吧？注意力被引入是为了以更人性化的方式学习语言，例如通过关联句子中的单词。注意力有助于更好地为 NLP 中的转导问题建立句子模型，从而改进编码器-解码器架构。

编码器-解码器架构依次建立在 **RNNs、LSTMs 和 Bi-LSTMs** 之上，它们在某个阶段是序列建模的最新技术。它们都属于循环网络类。因为一个句子是一个单词序列，所以你需要一个序列建模网络，在这个网络中，当前输入在序列的第二个元素中重复出现，以便更好地理解单词。这个信息链有助于对一个句子的有意义的表达进行编码。

我在这里想说的是，要真正理解 BERT 或任何其他基于 transformer 的架构模型，您需要对许多相互关联的概念有深刻的理解。为了让讨论集中在 BERT 上，我将主要讨论注意力和 BERT 架构。

### 集中注意力

先说个例子。如果我让你告诉我下列句子的意思，你会怎么说？

1.  狗很可爱。我喜欢和他们在一起。

2.  狗很可爱。我喜欢和他们在一起。

对于这两个句子，人们很容易理解说话者对狗有积极的情感。但是下面这句话呢？

1.  狗很可爱。我喜欢和他们在一起。

对于这句话，虽然不是决定性的，但我们可以说这句话应该是关于狗的积极的东西。这叫做*注意力*。为了理解一个句子，我们只依靠某些单词，而其他的都是垃圾(从理解的角度来看)。

递归网络族虽然有助于建模序列，但对于非常大的句子来说是失败的，因为编码上下文信息的固定长度表示只能捕获这么多的相关性。但是如果我们只从一个大句子中挑选重要的句子呢？那我们就不用担心残留了。

我喜欢从信息论的角度来理解这一点。我们只需要使用 2 的幂的不同组合就可以对所有的整数建模，如图 [4-3](#Fig3) 所示。

![../images/502837_1_En_4_Chapter/502837_1_En_4_Fig3_HTML.jpg](../images/502837_1_En_4_Chapter/502837_1_En_4_Fig3_HTML.jpg)

图 4-3

作为 2 的幂的整数

所以要得到任何数，我们要做的就是取两个向量的点积:

![$$ Any\  whole\ number\ x, $$](../images/502837_1_En_4_Chapter/502837_1_En_4_Chapter_TeX_Equa.png)

![$$ x=\left[\dots, 32,16,8,4,2,1\right]\odot \left[\dots, 0,0,0,0,1\right] $$](../images/502837_1_En_4_Chapter/502837_1_En_4_Chapter_TeX_Equb.png)

注意力以非常相似的方式工作。它采用序列的上下文向量或编码向量，只对重要的方面进行加权。虽然在我们的例子中，我们从整数转移到实数。参见图 [4-4](#Fig4) 。

![../images/502837_1_En_4_Chapter/502837_1_En_4_Fig4_HTML.jpg](../images/502837_1_En_4_Chapter/502837_1_En_4_Fig4_HTML.jpg)

图 4-4

展示了添加前馈层如何帮助我们学习注意力权重

Dzmitry Bahdanau 等人在 2014 年发表的题为“通过联合学习对齐和翻译进行神经机器翻译”的论文中首次讨论了注意力的概念。

在图 [4-4](#Fig4) 中，注意来自最后一个解码器单元的绿色箭头。这是由 S <sub>t-1</sub> 表示的解码器状态。我们结合隐藏状态和最后一个隐藏层的输出的方式可以提供各种关注，如表 [4-1](#Tab1) 所示。这也称为分数或编码器输出的能量。该组合函数或评分函数被设计成最大化解码器的隐藏状态和编码器输出之间的相似性。这样就产生了更多连贯的单词，给 MTL(多语言翻译)系统带来了更大的能力。

表 4-1

用于计算解码器和编码器状态之间相似性的不同评分函数

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

注意名称

 | 

纸

 |
| --- | --- |
| **加法或串联**:最后一个解码器单元的隐藏状态被添加到编码器单元的隐藏状态。假设维度是 d，那么连接的维度就变成了 2d。 | Bahdanau 等人，2014，“通过联合学习对齐和翻译的神经机器翻译” |
| **点积**:最后一个解码器单元的隐藏状态乘以编码器单元的隐藏状态。假设维度是 d，那么连接的维度就变成了 d。 | Luong 等人，2015，“基于注意力的神经机器翻译的有效方法” |
| **比例点积**:同上，只是增加了一个比例因子，使数值标准化，并在 Softmax 函数的可微分范围内。 | 瓦斯瓦尼等人，2017，“注意力是你所需要的一切” |
| **一般(点积)**:编码器隐藏状态在计算分数之前通过一个前馈网。 | Luong 等人，2015，“基于注意力的神经机器翻译的有效方法” |

一些你应该记住的细节:

*   为了使这个过程更快，您利用 Keras 的 TimeDistributedLayer，它确保每个时间单位的前馈发生得更快。(只是密密麻麻一层。)

*   最后合并的编码器隐藏状态作为输入被馈送到第一解码器单元。这个解码器单元的输出被称为第一解码器隐藏状态。

*   所有分数都通过 Softmax 层传递，以给出注意力权重，然后乘以编码器的隐藏状态，以获得来自每个编码器单元的上下文向量 C <sub>t</sub> 。

最后，注意力有很多种:

*   当地和全球的关注

*   自我关注

*   多头注意力

为了完整起见，我将简要地讨论它们，因为每一个都值得写一篇自己的文章，因此，为了有一个总体的理解，我只包括定义。我将在 transformer 架构讨论中详细讨论多头关注。请参考表 [4-2](#Tab2) 了解不同类型注意力的概述。

表 4-2

不同类型的关注

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

注意力

 | 

描述

 | 

纸

 |
| --- | --- | --- |
| 当地和全球的关注 | **全局**:所有编码器单元都被赋予了重要性。**Local** :上下文向量生成只考虑输入的一部分。该输入以位置 p <sub>t</sub> 为中心，宽度为 p <sub>t</sub> -2L 到 p <sub>t</sub> +2L，其中 L 是窗口长度。 | 灵感来源于徐等，2015，“展示、参与、讲述:视觉注意下的神经图像字幕生成” |
| 自我关注 | 工作原理类似于上面编码器-解码器架构中所解释的注意事项；我们只是用输入序列本身替换目标序列。 | 程等，2016，“长短期记忆-机器阅读的网络” |
| 多头注意力 | 多头注意力是实现自我注意力的一种方式，但是有多个键。稍后将详细介绍。 | 瓦斯瓦尼等人，2017，“注意力是你所需要的一切” |

### 转变 NLP 空间:变压器架构

Transformer 模型可以说是为 NLP 迁移学习任务带来了 ImageNet 运动。到目前为止，这需要大量数据集来捕获上下文、大量计算资源，甚至更多时间。但是，一旦 transformer 架构出现，它就可以更好地捕捉上下文，因为可以并行化，所以训练时间更短，并且还可以为许多任务设置 SOTA。图 [4-5](#Fig5) 显示了 Vaswani 等人题为“你只需要关注”的论文中的变压器架构

![../images/502837_1_En_4_Chapter/502837_1_En_4_Fig5_HTML.jpg](../images/502837_1_En_4_Chapter/502837_1_En_4_Fig5_HTML.jpg)

图 4-5

变压器模型

对于门外汉来说，该模型一开始可能会令人望而生畏，但如果在筒仓中理解不同的概念，则非常容易理解。

要理解变形金刚，你需要理解

*   编码器-解码器框架

*   多头注意力

*   位置编码

*   剩余连接

编码器和解码器模块与上面的关注主题一起讨论，残差连接只是为了确保来自目标损失的残差可以容易地帮助准确地改变权重，因为有时由于非线性，梯度不会产生期望的效果。

#### 位置编码

变压器能够并行处理更大的数据和更多的参数，实现更快的训练。但是怎么可能呢？通过移除所有有状态的细胞，如 RNN、GRU 或 LSTM，这是可能的。

但是，我们如何确保句子的句法语法没有被打乱，并且句子的单词有一定的顺序感呢？我们通过使用一个密集的向量来编码一个单词在序列中的位置。

思考这个问题的一个非常简单的方法是将每个单词标记为正整数(无界)。但是如果我们得到一个很长的句子或者不同长度的句子呢？在这两种情况下，拥有一个无界的数字表示是行不通的。

好的，那么有界表示可以工作吗？让我们对[a 到 b]之间的所有内容进行排序，其中 a 代表第一个单词，b 代表最后一个单词，其他内容位于两者之间。因为它是一个 10 个单词的句子的模型，你必须增加索引![$$ \left[\frac{\Big(b-a}{10}\right] $$](../images/502837_1_En_4_Chapter/502837_1_En_4_Chapter_TeX_IEq1.png)，对于一个 20 个单词的句子，增量是![$$ \left[\frac{\Big(b-a}{20}\right] $$](../images/502837_1_En_4_Chapter/502837_1_En_4_Chapter_TeX_IEq2.png)。因此，增量没有相同的意义。作者提出了图 [4-6](#Fig6) 中位置编码矢量的公式。

![../images/502837_1_En_4_Chapter/502837_1_En_4_Fig6_HTML.jpg](../images/502837_1_En_4_Chapter/502837_1_En_4_Fig6_HTML.jpg)

图 4-6

位置编码

理解这一点的一个好方法，不需要太多的数学知识，就是

*   位置编码器是一个 d 维向量。这个向量被添加到单词的单词向量表示中，因此 dpe = dmodel。

*   它是一个大小为(n，d)的矩阵，其中 n 是序列中的单词数，d 是单词嵌入的维数。

*   它是每个位置的唯一向量。

*   sin 和 cos 函数的组合允许模型很好地学习相对位置。因为任何偏移 k，PEpos+k 可以表示为 PEpos 的线性函数。

*   由于残留块的存在，该位置信息也保留在较深层中。

#### 多头注意力

多头关注是 transformer 架构的主要创新。我们来详细了解一下。本文使用一个通用的框架来定义注意。

它引入了三个术语:

1.  钥匙(K)

2.  查询(Q)

3.  价值(伏特)

单词的每次嵌入都应该有这三个向量。它们是通过矩阵乘法得到的。这从嵌入向量中捕获了特定的信息子空间。

一个抽象的理解是这样的:你试图通过使用一个查询来识别某些键、值对。您试图确定其关注分数的单词是查询。

由于天气不好，交通堵塞。

假设你的查询是**流量**。查询向量捕获单词 traffic 的一些语义，可能是它的 pos 标签或与旅行/通勤相关的标签，等等。类似地，对于键和值向量，也捕捉到了一些细微差别。

现在，您到达单词**天气**，类似地，您捕获 K、Q 和 v。如果*交通的查询与天气*的关键字具有高相似性，则天气的值对单词**交通的自我关注向量贡献很大。**见图 [4-7](#Fig7) 。

![../images/502837_1_En_4_Chapter/502837_1_En_4_Fig7_HTML.jpg](../images/502837_1_En_4_Chapter/502837_1_En_4_Fig7_HTML.jpg)

图 4-7

自我关注。图片改编自《注意力是你需要的全部》

在多头注意力中，有多个这样的矩阵乘法，可以让你每次捕捉到不同的子空间。它们都是并行完成的。参见图 [4-8](#Fig8) 。

![../images/502837_1_En_4_Chapter/502837_1_En_4_Fig8_HTML.jpg](../images/502837_1_En_4_Chapter/502837_1_En_4_Fig8_HTML.jpg)

图 4-8

多头自我关注。图片改编自《注意力是你需要的全部》

以上两项是变压器模型中的主要创新。毫无疑问，它能够很好地捕捉句子语义。以下是一些值得一提的其他细节:

*   解码器模型包含掩蔽的多头注意力模型。它屏蔽掉查询词之后的所有词。价值向量被屏蔽，然后转移到自我关注向量。

*   来自被掩蔽的注意块的自我注意向量充当其上方的多头注意块的值向量。

*   使用跳跃连接(灵感来自 ResNet，由何等人在“图像识别的深度残差学习”中介绍)来防止信号丢失。

*   有多个编码器-解码器模块堆叠在一起，图 [4-5](#Fig5) 显示了最后一对。Softmax 仅被添加到最后一个解码器块。

Note

来自最后一个编码器的输出被传递到所有的解码器单元，而不仅仅是最后一个。

### BERT:来自变压器的双向编码器表示

BERT 为将 NLP 的 ImageNet 运动带入现实奠定了基础。现在我们有了一个 BERT 模型动物园，这基本上意味着几乎每种应用程序都有一个 BERT 模型。

从架构的角度来看，BERT 只不过是堆叠的变压器(只有编码器模块)。但它在处理输入数据和训练方面带来了一些新的创新。在深入研究代码之前，让我们简单地讨论一下它们。

#### 投入

BERT 作者分享了一些输入文本的创新方法。我已经讨论了长度中的位置嵌入，所以让我们快速跳到令牌和段嵌入。参见图 [4-9](#Fig9) 。

![../images/502837_1_En_4_Chapter/502837_1_En_4_Fig9_HTML.jpg](../images/502837_1_En_4_Chapter/502837_1_En_4_Fig9_HTML.jpg)

图 4-9

BERT 输入表示。图片改编自《BERT:深度双向转换者语言理解预训练》

##### 令牌嵌入

令牌嵌入只是以数字形式表示每个令牌的一种方式。在 BERT 中，这是一个 768 维的向量。这里有趣的是工件记号化技术。它帮助 BERT 维持了一个相当大的图书馆，有 30，522 个，但没有在未收录的单词上妥协。

我们举个例子来理解一下。假设最初你有一个只有五个单词的字典，它们在语料库中的数量是已知的:

1.  教堂，5

2.  孩子，3

3.  返回，8

4.  赚，10

5.  提升，5

结尾的代表单词边界。单词块算法检查文本中的每个字符，并试图找到频率最高的字符对。

假设系统遇到一个像 **Churn** 这样的不在词汇表中的单词。对于这个单词，BERT 会做如下处理:

1.  c : 5 + 3 = 8

2.  c+h:5+3 = 8

3.  c + h + u: 5，由于总数下降而被拒绝

4.  n : 10

5.  r + n : 8，拒绝，因为它也减少了 n 的计数。

6.  u + r :8，u + r 的计数

因此，创建的令牌是

[ch，ur，n .]

我上面讨论的是 BPE 或二进制编码。正如您所观察到的，它以贪婪的方式根据频率合并单个字符。单词块算法略有不同，在某种程度上，字符合并仍然基于频率，但最终决定是基于出现的可能性(查看哪些单词块更有可能出现)。

##### 片段嵌入

伯特接受两种不同训练任务的训练:

1.  **分类**:确定输入句子的类别

2.  **下一个句子预测**:预测下一个句子或理想地/连贯地跟随前一个句子的句子(如在训练语料库中存在的)

为了预测下一个句子，BERT 需要一种方法来区分这两个句子，因此在每个句子的末尾引入了一个特殊的标记[SEP]。

因为我已经谈到了位置嵌入，所以我不会在这里再次讨论它。

#### 培养

BERT 模型针对两项任务进行了预训练:

1.  掩蔽语言建模

2.  下一句预测

##### 掩蔽语言建模

引入屏蔽语言建模主要是为了允许模型以双向方式学习，并使模型能够捕捉序列中任何随机单词的上下文。

分类层被添加到编码器输出的顶部。这些输出通过时间分布的密集层，将它们转换成词汇的维数，然后计算每个单词的概率。参见图 [4-10](#Fig10) 。

![../images/502837_1_En_4_Chapter/502837_1_En_4_Fig10_HTML.jpg](../images/502837_1_En_4_Chapter/502837_1_En_4_Fig10_HTML.jpg)

图 4-10

掩蔽语言建模

*   为了使模型位置不可知，同时给出足够的上下文，在**的每个序列**中只有 **15%** 的单词被**随机**屏蔽。

*   如图 [4-10](#Fig10) 所示，并非所有被屏蔽的单词都被替换为[MASK]标记。而是选择了以下方法:
    *   80%的时间使用了[MASK]标记。

    *   10%的情况下，这些单词被替换成随机单词。

    *   剩下的 10%的时间单词保持不变。

如果你深入思考，你会想到很多关于选择这些百分比的问题。没有进行消融研究来支持这些经验数据；但是，有一些直觉。

*   使用随机单词会让 BERT 学习错误的嵌入吗？理想情况下不会，因为它在反向传播过程中被正确的标签所纠正。这样做也是为了引入方差。

*   **为什么不保留 100%【面具】令牌**？这样做是为了避免微调过程中的任何混乱，如果没有找到[MASK]标记，它将根据任务给出一些随机输出。

##### 下一句预测

根据作者的说法，学习如何将两个句子联系起来可以显著提高问答和自然语言推理等任务的性能。

在这里，他们也提出了某些比率，并使用这些比率为 NSP 创建了一个训练数据:

*   对于语料库中 50%的句子，下一个句子是与语料库中存在的句子相同的句子。

*   剩下的 50%，下一句随机抽取。

这给了我们一个二进制分类器来训练。[CLS]令牌用于二进制分类，其最终状态被传递到 FFN 加软件最大层。

我希望您现在对基于 transformer 的模型，尤其是 BERT 有了更深的理解。我认为这应该足以让您将 BERT 应用于该案例，并学习如何对其进行微调。

## 建模

现在让我们深入研究建模。您已经在上面的“数据”部分准备好了数据。您正在尝试进行多标签分类。你必须以这样的方式准备你的数据。

对于您的任务，您将使用来自高丽大学 DMIS(数据挖掘和信息系统)实验室的 BERT 大模型。您这样做是因为它是为数不多的为 BERT 提供定制词汇表的预训练模型之一。大多数免费的预训练模型保持相同的词汇，在我看来这是一种不好的做法。

第二，你还将利用拥抱脸小组的变形金刚库，它为语言理解(NLU)和自然语言生成(NLG)任务提供通用架构(伯特、GPT-2、罗伯塔、XLM、蒸馏伯特、XLNet)。

但在此之前，让我们先了解一下 BERT 模型的词汇。发布后，您将形成您的数据并进行多标签分类。

### 伯特深潜

除了更好的性能之外，拥有自定义词汇表的一个好处是能够看到哪些概念被捕获。您将使用一个 UML 数据库来识别词汇表中哪些概念正在被捕获；为此，您将看到子词标记(没有“##”)，并选取长度大于 3 的所有标记。

为此，您必须设置 **scispacy** 库。它建立在 spacy 之上，对于应用 NLP 工作来说是一个非常快速和有用的库。请参见第 [2](02.html) 章中的安装步骤。

Scispacy 提供了一种链接知识库的方法。概念提取对字符串重叠起作用。它涵盖了大多数公开可用的主要生物医学数据库，如 UMLs、Mesh、RxNorm 等。

此外，您将使用基于生物医学数据的大型空间模型。确保您已经通过下载并链接到 spacy 来设置模型。保留匹配的默认参数，因为这只是一个探索性的练习，您的建模不会直接受到此选择的影响。官方文档在 [`https://github.com/allenai/scispacy`](https://github.com/allenai/scispacy) `.`

#### 词汇实际上包含什么？

在深入训练分类模型或使用微调进一步改进它之前，您应该仔细检查一下您拥有的词汇表。它甚至包括生物医学概念吗？平均令牌长度是多少？(生物医学词汇一般有体面的令牌长度一般> 5 个字符。)

让我们一个一个来看看这些问题。

1.  找到任何生物医学概念

为了找到生物医学的概念，你将利用一个广泛的 UMLs 知识库。它通过一个简单的界面与科学联系起来。

首次运行时，链接可能需要一些时间，具体取决于您的电脑配置。首先，从导入库和加载相关模型开始。

```py
# Load Hugging-face transformers
from transformers import TFBertModel, BertConfig, BertTokenizerFast
import tensorflow as tf

# For data processing
import pandas as pd
from sklearn.model_selection import train_test_split

# Load pre-trained model tokenizer (vocabulary)
    tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-large-cased-v1.1')

```

接下来，让我们找出唯一令牌的总数。

```py
vocab = tokenizer.vocab.keys()
# Total Length
    print("Total Length of Vocabulary words are : ", len(vocab))

```

词汇单词的总长度为 58996，几乎是谷歌团队分享的第一个 BERT 模型的两倍。猜猜为什么？

嗯，词汇量的大小是基于你能够用词汇表的子词对语料库中的每个词进行编码的清晰程度来决定的。谷歌没有分享代码，所以确切的原因不得而知，但我打赌上述大小足以以优化的方式表示语料库中的不同单词。你可以在 [`https://github.com/google-research/bert#learning-a-new-wordpiece-vocabulary`](https://github.com/google-research/bert%2523learning-a-new-wordpiece-vocabulary) 从谷歌官方回购了解更多信息。

让我们连接 UMLs 数据库。

```py
import spacy
import scispacy

from scispacy.linking import EntityLinker
    nlp = spacy.load('en_core_sci_lg')
    linker = EntityLinker(resolve_abbreviations=False, name="umls") # keeping default thresholds for match percentage.
nlp.add_pipe(linker)

# subword vs whole word selection based on length
    target_vocab = [word[2:] for word in vocab if "##" in word and (len(word[2:]) > 3)] + [word[2:] for word in vocab if "##" not in word and (len(word) > 3)]

umls_concept_extracted = [[umls_ent for entity in doc.ents for umls_ent in entity._.umls_ents] for doc in nlp.pipe(target_vocab)]

    umls_concept_cui = [linker.kb.cui_to_entity[concepts[0][0]] for concepts in umls_concept_extracted if len(concepts) > 0]
# Capturing all the information shared from the UMLS DB in a dataframe
umls_concept_df = pd.DataFrame(umls_concept_cui)

```

UMLs 为它的每个 TXXX 标识符提供一个类名。TXXX 是每个 CUI 编号的父项代码，是 UMLs KB 使用的唯一概念标识符。接下来，让我们将 TXXX ids 映射到人类可读的标签。

```py
# To obtain this file please login to https://www.nlm.nih.gov/research/umls/index.html
# Shared in Github Repo of the book :)
    type2namemap = pd.read_csv("SRDEF", sep ="|", header = None)
    type2namemap = type2namemap.iloc[:,:3]
    type2namemap.columns = ["ClassType","TypeID","TypeName"]
    typenamemap = {row["TypeID"]:row["TypeName"] for i,row in type2namemap.iterrows()}

```

为每个类型 ID 创建计数。

```py
concept_df = pd.Series([typenamemap[typeid] for types in umls_concept_df.types for typeid in types]).value_counts().reset_index()
    concept_df.columns = ["concept","count"]

```

让我们想象一下这 20 个最重要的概念。见图 [4-11](#Fig11) 。

![../images/502837_1_En_4_Chapter/502837_1_En_4_Fig11_HTML.jpg](../images/502837_1_En_4_Chapter/502837_1_En_4_Fig11_HTML.jpg)

图 4-11

生物医学概念在伯特词汇中的分布

哇，这些词汇实际上包含了各种生物医学概念，如疾病、身体部位、有机化学物质(化合物)和药理物质(用于治疗病理障碍)。看起来你有适合你任务的模型。所有这些概念在 EHR 笔记中也很常见。

接下来，让我们看看您在数据集中观察到的子词和实际标记的标记长度。

```py
    subword_len = [len(x.replace("##","")) for x in vocab]
token_len = [len(x) for x in vocab]

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

    with sns.plotting_context(font_scale=2):
        fig, axes = plt.subplots(1,2, figsize=(10, 6))
        sns.countplot(subword_len, palette="Set2", ax=axes[0])
    sns.despine()
        axes[0].set_title("Subword length distribution")
        axes[0].set_xlabel("Length in characters")
        axes[0].set_ylabel("Frequency")

        sns.countplot(token_len, palette="Set2", ax=axes[1])
    sns.despine()
        axes[1].set_title("Token length distribution")
        axes[1].set_xlabel("Length in characters")
        axes[1].set_ylabel("Frequency")

```

在图 [4-12](#Fig12) 中，您确实看到了分布在【5-8】之间的平均值，这是一个很好的指标，表明您正在使用一个正确的预训练模型。

![../images/502837_1_En_4_Chapter/502837_1_En_4_Fig12_HTML.jpg](../images/502837_1_En_4_Chapter/502837_1_En_4_Fig12_HTML.jpg)

图 4-12

词汇标记的长度分布

如果你想仔细阅读词汇表中的不同单词，你可以访问下面的链接:

[`https://huggingface.co/dmis-lab/biobert-large-cased-v1.1/blob/main/vocab.txt`](https://huggingface.co/dmis-lab/biobert-large-cased-v1.1/blob/main/vocab.txt) 。

### 培养

BERT 可用于多种方式的微调:

*   **微调:**您在 BERT 模型的最后一个预训练层的顶部添加另一组层，然后用特定于任务的数据集训练整个模型，尽管在此过程中您必须确保预训练模型的权重不被破坏，因此您将它们冻结一些时期，然后在另一组时期恢复到 BERT 层的完全反向传播。这也叫热身。

*   **从最后一组层中提取权重:**提取的上下文嵌入被用作下游任务的输入。它们是固定向量，因此不可训练。原始论文中讨论了四种不同类型的方法(表 7)。
    *   12 层的加权和。权衡可以是经验性的。

    *   使用最后一个隐藏层。

    *   提取倒数第二个隐藏层(倒数第二)。

    *   连接最后四个隐藏层。

*   **单词嵌入:**从 BERT 的编码器层获取单词嵌入。包装器存在于拥抱脸的变形库中。

微调被认为是更好地控制模型性能的最佳方法，所以您将采用这种方法。

由于您将训练多标签分类，因此让我们为其准备最终数据集。你正在做一个实际的决定，不要保留只有三个或更少标记的短句。

```py
# Making icd9_code unique at SUBJECT ID and HADM_ID level by clubbing different ICD9_CODE
    icd9_code = icd9_code.groupby(["SUBJECT_ID","HADM_ID"])["ICD9_CODE"].apply(list).reset_index()

    full_data = pd.merge(noteevents, icd9_code, how="left", on = ["SUBJECT_ID","HADM_ID"])

# Removing any SUBJECT_ID and HADM_ID pair not having the top 15 ICD9 Codes
    full_data = full_data.dropna(subset = ["ICD9_CODE"]).reset_index(drop = True)

# Make sure we have text of considerable length
    full_data.CLEAN_TEXT = [" ".join([y for y in x.split("|") if len(y.split()) > 3]) for x in full_data.CLEAN_TEXT]

```

您还将使用`full_data`变量创建训练和验证集。此外，您的目标将是一个独热矩阵，每个样本有一个其所属的 ICD-9 代码的标签，其余的标签为零。

```py
# Binarizing the multi- labels
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split

mlb = MultiLabelBinarizer()
mlb_fit = mlb.fit(full_data.ICD9_CODE.tolist())

    train_X,val_X,train_y,val_y = train_test_split(full_data[["SUBJECT_ID"," ","CLEAN_TEXT"]],full_data.ICD9_CODE.values, test_size=0.2, random_state=42)

```

你终于准备好加载拥抱脸变压器库，并从 DMIS 实验室获得伯特模型。

```py
# Load Huggingface transformers
from transformers import TFBertModel, BertConfig, BertTokenizerFast
import tensorflow as tf
import numpy as np

# For data processing
import pandas as pd
from sklearn.model_selection import train_test_split

# Load pre-trained model tokenizer (vocabulary)
    tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-large-cased-v1.1')

# Import BERT Model
from transformers import BertModel, BertConfig, TFBertModel
    bert = TFBertModel.from_pretrained("./dmis-lab/biobert-large-cased-v1.1",
                                   from_pt = True)

```

DMIS 团队共享的模型是 pytorch 模型，因此不能直接用于您的任务。您将使用 transformers 库中提供的包装函数将 pytorch 模型转换为 TensorFlow BERT 模型。

您必须确保传递了参数`from_pt = True`,这表示您正试图从 Python 预训练文件创建 TFBertModel。

接下来，决定您将要使用的模型参数。

```py
    EPOCHS = 5
    BATCH_SIZE = 32
    MAX_LEN = 510
    LR = 2e-5
    NUM_LABELS = 15 # Since we have 15 classes to predict for

```

理想情况下，你决定`MAX_LEN`。您可以绘制语料库中句子长度的直方图，但是由于文本通常很长，您已经根据标记的数量为句子取了最大长度。

目前，学习速度保持不变，没有热身。所使用的设计参数，如激活函数的选择、批量大小等。，只是经验性的设计选择，因此您可以探索和试验不同的设计选择。

就像在第 [3](03.html) 章中一样，您将创建一个生成器函数，该函数将生成批量维度的输入数据。

```py
    X = (BATCH_SIZE, {'input_ids':[0 to VOCAB LENGTH],'token_type_ids':[1/0],'attention_mask':[1/0]}

```

BERT 将字典作为输入:

*   **输入 id**表示根据 BERT 模型词汇的标记化单词的索引

*   **令牌类型 ID**也称为段 ID。因为您正在训练一个序列分类问题，所以所有的令牌类型 id 都是零。

*   **注意力屏蔽**是一个 1/0 向量，它告诉我们应该关注哪个单词。一般来说，所有的单词都被认为是重要的，但这可以根据设计决策很容易地改变。

请注意，您还将句子填充到可能的最大标记长度。

```py
    def df_to_dataset(dataframe,
                  dataframe_labels,
                  batch_size = BATCH_SIZE,
                  max_length = MAX_LEN,
                  tokenizer  = tokenizer):
        """
        Loads data into a tf.data.Dataset for finetuning a given model.
        """
    while True:
        for i in range(len(dataframe)):
                if (i+1) % batch_size == 0:
                    multiplier = int((i+1)/batch_size)
                print(multiplier)
                    _df = dataframe.iloc[(multiplier-1)*batch_size:multiplier*batch_size,:]
                input_df_dict = tokenizer(
                    _df.CLEAN_TEXT.tolist(),
                    add_special_tokens=True,
                    max_length=max_length, # TO truncate larger sentences, similar to truncation = True
                    truncation=True,
                    return_token_type_ids=True,
                    return_attention_mask=True,
                        padding='max_length', # right padded
                )
                input_df_dict = {k:np.array(v) for k,v in input_df_dict.items()}
                    yield input_df_dict, mlb_fit.transform(dataframe_labels[(multiplier-1)*batch_size:multiplier*batch_size])

train_gen = df_to_dataset(train_X.reset_index(drop = True),
train_y)
val_gen = df_to_dataset(val_X.reset_index(drop = True),
val_y)

from tensorflow.keras import layers
    def create_final_model(bert_model = bert):

        input_ids = layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_ids')
        token_type_ids = layers.Input((MAX_LEN,), dtype=tf.int32, name='token_type_ids')
        attention_mask = layers.Input((MAX_LEN,), dtype=tf.int32, name='attention_mask')

    # Use pooled_output(hidden states of [CLS]) as sentence level embedding
        cls_output = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids})[1]
        x = layers.Dense(512, activation='selu')(cls_output)
        x = layers.Dense(256, activation='selu')(x)
        x = layers.Dropout(rate=0.1)(x)
        x = layers.Dense(NUM_LABELS, activation='sigmoid')(x)
        model = tf.keras.models.Model(inputs={'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids}, outputs=x)
    return model

model = create_final_model(bert_model = bert)

```

此外，请确保您只学习自定义层，至少对于几个第一时代；然后就可以学习全网了。为此，您将冻结 BERT 层，只训练自定义层。

```py
for layers in bert.layers:
    print(layers.name)
    layers.trainable= False

```

让我们检查一下模型的外观；参见图 [4-13](#Fig13) 。特别注意可训练和不可训练参数的数量。

![../images/502837_1_En_4_Chapter/502837_1_En_4_Fig13_HTML.jpg](../images/502837_1_En_4_Chapter/502837_1_En_4_Fig13_HTML.jpg)

图 4-13

模型摘要

```py
model.summary()

```

这里需要注意的一点是，您使用的是 sigmoid 函数，而不是 Softmax 函数，因为您试图识别特定的 ICD 码是否存在，因此 Softmax 足以满足相同的要求。

```py
model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=LR),
                  loss='binary_crossentropy',
                  metrics=['AUC'])

```

由于这是一个大模型，可能需要很多时间来训练，因此建立一个 TensorBoard 来跟踪损失和 AUC 会很好。

```py
# You can change the directory name
    LOG_DIR = 'tb_logs'

import os
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)

    with tf.device('/device:GPU:0'):
    history = model.fit(train_gen,
                      steps_per_epoch=len(train_X)//BATCH_SIZE,
                      epochs=EPOCHS,
                      validation_data=val_gen,
                        callbacks=[tensorboard_callback])

```

您可以使用或不使用 GPU 来训练模型，但请确保您使用的硬件启用了 GPU。如果您没有设置，请重新查看第 [2](02.html) 章的注释。

要了解您是否有可用的 GPU，请运行以下命令:

```py
tf.test.gpu_device_name()

```

对于 NVIDIA GeForce GTX 1660Ti，这种模型的训练在 CPU 上可能需要很多时间，在 GPU 上可能需要一点时间。一个历元大约需要四个小时，而在 CPU 机器上几乎需要五倍的时间。因此，我不会在这里讨论模型的结果。

这里有一些加强训练的想法:

1.  在几个时期内，您可以保持 BERT 层冻结，但最终为了在下游任务中获得稍好的性能，您也可以解冻并训练 BERT 层的参数。

2.  尝试使用一个更精炼的模型。经过提炼的模型是对参数需求较少的模型，在许多下游任务上实现了几乎相同的性能。这使得整体训练非常快。

3.  另一个修改可以在数据集生成中进行。`input_token_dict`可以对全部数据进行处理，也可以对每批数据进行子集处理。

## 结论

好了，带着这些想法，我想结束这一章。在这一章中，你学习了变形金刚，多重注意力概念，以及伯特长度。您应用所有这些学到的概念，通过使用拥抱人脸库来训练多标签分类模型。

你在这一章学到的变形金刚的基础在未来几年将会非常重要，因为有很多论文试图利用变形金刚完成各种任务。它们被用于图像问题、药物预测、图形网络等等。

尽管人们越来越有兴趣在不损失太多性能的情况下更快地从这种模型中做出推断，但罗杰斯等人的论文“当伯特玩彩票时，所有的彩票都中奖”表明，您可以删除伯特的许多组件，它仍然有效。本文根据彩票假设分析了 BERT 修剪，发现即使是“坏”彩票也可以被微调到良好的准确度。它仍然是推进 NLU 边界的一个非常重要的里程碑。我劝你去读读 XLNext，Longformer and Reformer，Roberta 等。它们是其他基于 transformer 或受其启发的架构，在某些任务上比 BERT 表现得更好。您将使用 BERT 模型来开发问答系统。在此之前，继续阅读和学习。