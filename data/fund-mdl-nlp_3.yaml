- en: '© The Author(s) 2023G. Paaß, S. GiesselbachFoundation Models for Natural Language
    ProcessingArtificial Intelligence: Foundations, Theory, and Algorithms[https://doi.org/10.1007/978-3-031-23190-2_4](https://doi.org/10.1007/978-3-031-23190-2_4)'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: © 作者(们) 2023 G. Paaß, S. Giesselbach基础模型用于自然语言处理人工智能：基础、理论与算法[https://doi.org/10.1007/978-3-031-23190-2_4](https://doi.org/10.1007/978-3-031-23190-2_4)
- en: 4. Knowledge Acquired by Foundation Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 基础模型获得的知识
- en: Gerhard Paaß^([1](#Aff5)  ) and Sven Giesselbach^([1](#Aff5))(1)Knowledge Discovery
    Department, Team NLU, Fraunhofer Institute for Intelligent Analysis and Information
    Systems (IAIS), Sankt Augustin, Nordrhein-Westfalen, Germany
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Gerhard Paaß^([1](#Aff5)  ) 和 Sven Giesselbach^([1](#Aff5))(1)知识发现部门，NLU团队，弗劳恩霍夫智能分析和信息系统研究所（IAIS），圣奥古斯丁，北莱茵-威斯特法伦，德国
- en: Abstract
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: During pre-training, a Foundation Model is trained on an extensive collection
    of documents and learns the distribution of words in correct and fluent language.
    In this chapter, we investigate the knowledge acquired by PLMs and the larger
    Foundation Models. We first discuss the application of Foundation Models to specific
    benchmarks to test knowledge in a large number of areas and examine if the models
    are able to derive correct conclusions from the content. Another group of tests
    assesses Foundation Models by completing text and by applying specific probing
    classifiers that consider syntactic knowledge, semantic knowledge, and logical
    reasoning separately. Finally, we investigate if the benchmarks are reliable and
    reproducible, i.e. whether they actually test the targeted properties and yield
    the same performance values when repeated by other researchers.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练过程中，基础模型在大量文档集合上训练，并学习正确流畅语言中单词的分布。在本章中，我们研究PLMs和更大规模的基础模型所获得的知识。我们首先讨论基础模型在特定基准上的应用，以测试大量领域的知识，并检查模型是否能够从内容中推导出正确的结论。另一组测试通过完成文本和运用特定的探针分类器来评估基础模型，这些分类器分别考虑语法知识、语义知识和逻辑推理。最后，我们研究基准测试是否可靠和可重复，即它们是否真正测试了目标属性，并在其他研究人员重复时产生相同的性能值。
- en: 'KeywordsKnowledge in foundation modelsCommon Sense knowledgeLogical coherenceBenchmark
    collectionsReproducibilityDuring pre-training, Pre-trained Language Models (PLMs)
    and the larger Foundation Models are trained on an extensive collection of documents
    and learn the distribution of words in correct and fluent language. During fine-tuning,
    the models are adapted to a specific task using the knowledge from the pre-training
    and requiring only a small set of manually labeled fine-tuning data. In this chapter,
    we investigate the knowledge acquired by these models by different types of tests:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词基础模型中的知识常识知识逻辑一致性基准集合可重复性在预训练期间，预训练语言模型（PLMs）和更大规模的基础模型在大量文档集合上训练，并学习正确流畅语言中单词的分布。在微调期间，模型使用预训练的知识适应特定任务，并且只需要一小部分手动标记的微调数据。在本章中，我们通过不同类型的测试研究这些模型获得的知识：
- en: We first assess PLMs and Foundation Models by specific benchmarks to test knowledge
    in a large number of areas and examine if the models are able to derive correct
    conclusions from the content (Sect. [4.1](#Sec1)). Usually these benchmark collections
    have an aggregated performance measure averaging over different tests. Benchmark
    tests can be accomplished by fine-tuning models to perform specific classification
    tasks or by few-shot querying Foundation Models.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先通过特定的基准测试评估PLMs和基础模型，以测试大量领域的知识，并检查模型是否能够从内容中推导出正确的结论（第[4.1](#Sec1)节）。通常这些基准集合有一个综合的性能度量，平均覆盖不同的测试。基准测试可以通过微调模型以执行特定的分类任务或通过少样本查询基础模型来完成。
- en: Then we assess Foundation Models by completing text and by applying specific
    probing classifiers without adapting model parameters (Sect. [4.2](#Sec7)). We
    separately consider syntactic knowledge, semantic knowledge and logical reasoning
    and demonstrate the achievements and deficits in different areas and for different
    model architectures.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们通过完成文本和运用特定的探针分类器来评估基础模型，而不调整模型参数（第[4.2](#Sec7)节）。我们分别考虑语法知识、语义知识和逻辑推理，并展示不同领域和不同模型架构的成就和不足。
- en: Finally, we investigate if the benchmarks are reliable, i.e. actually test the
    targeted properties (Sect. [4.3](#Sec13)). Moreover, we analyze if published benchmark
    results are reproducible and yield the same performance values if they are repeated
    by other researchers.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们调查了基准测试是否可靠，即实际上是否测试了目标属性（第[4.3节](#Sec13)）。此外，我们还分析了已发布的基准测试结果是否可重复，如果其他研究人员重复进行，是否会得到相同的表现值。
- en: 4.1 Benchmark Collections
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 基准测试集合
- en: In order to arrive at quantitative measures of common sense knowledge and commonsense
    reasoning, the community has compiled a number of benchmarks. These allow a standardized
    comparison of different aspects of natural language understanding and provide
    comparable scores for the strength and weaknesses of different PLMs. Benchmarks
    have been a key driver for the development of language models. A comprehensive
    collection of benchmarks and the corresponding leaderboards are provided by PapersWithCode
    [[45](#CR45)]. A survey of actual benchmarks is given by Storks et al. [[62](#CR62)].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到常识知识和常识推理的定量度量，社区已经编制了一系列基准测试。这些基准测试允许对自然语言理解的不同方面进行标准化比较，并为不同PLMs的强弱提供可比的分数。基准测试是语言模型发展的关键驱动力。PapersWithCode
    [[45](#CR45)]提供了一系列基准测试和相应的排行榜。Storks等人[[62](#CR62)]提供了一份实际基准测试的调查。
- en: 'A fair comparison of model architectures requires that the number of parameters,
    the size of the training data, and the computing effort for training are similar.
    This has been extensively discussed in Sect. [3.​5.​1](528393_1_En_3_Chapter.xhtml#Sec25).
    Therefore, many authors conduct extensive ablation studies to adjust their training
    resources to a standard, e.g. to BERT as a “benchmark model”. This is really important,
    as it helps the reader to get an intuition for the impact of pre-training resources.
    Nevertheless, comparability is often hampered by two problems:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了公平地比较模型架构，需要确保参数数量、训练数据的大小以及训练所需的计算工作量相似。这一点在第[3.5.1节](528393_1_En_3_Chapter.xhtml#Sec25)中已经进行了广泛讨论。因此，许多作者进行了广泛的消融研究，以调整他们的训练资源到一个标准，例如以BERT作为一个“基准模型”。这非常重要，因为它帮助读者对预训练资源的影响有一个直观的认识。尽管如此，可比性通常受到两个问题的阻碍：
- en: '1.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Some training datasets, e.g. the BooksCorpus of BERT, are not publicly available.
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些训练数据集，例如BERT的BooksCorpus，并不是公开可用的。
- en: '2.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: These comparisons do not show the performance of a model when the size of data,
    the number of parameters, or the computing effort are increased.
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些比较没有显示出当数据量、参数数量或计算工作量增加时，模型的性能。
- en: Therefore, statements like *“Model architecture A is superior to model architecture
    B on performing task X.”* in general are not valid, but have to be qualified [[2](#CR2)],
    e.g. “Model architecture *A* is superior to model architecture *B* on performing
    task *X*, when pre-trained on a small/large corpus of low/high quality data from
    domain *Y*  with computing effort *Z*.”
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，像“模型架构A在执行任务X方面优于模型架构B。”这样的陈述在一般情况下是不成立的，但必须进行限定[[2](#CR2)]，例如：“当在来自领域Y的小/大数据集上预训练，并且计算工作量是Z时，模型架构*A*在执行任务*X*方面优于模型架构*B*。”
- en: 4.1.1 The GLUE Benchmark Collection
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 GLUE基准测试集合
- en: To test the ability of PLMs to capture the content of a document, the GLUE (Sect.
    [2.​1.​5](528393_1_En_2_Chapter.xhtml#Sec7)) set of benchmarks has been developed.
    This is a collection of 9 benchmarks testing different aspects of *Natural Language
    Understanding* (*NLU*). The joint performance is measured by a single score, which
    has the value 87.1 for human annotators. The tasks are described in detail by
    examples in Table [2.​1](528393_1_En_2_Chapter.xhtml#Tab1). It turns out that
    variants of BERT fine-tuned to the different GLUE-tasks can yield better results
    than people. The results are determined for the large variants of the models and
    shown in Table [4.1](#Tab1).Table 4.1
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试PLMs（预训练语言模型）捕捉文档内容的能力，已经开发了GLUE（第[2.1.5节](528393_1_En_2_Chapter.xhtml#Sec7)）的基准测试集。这是一个包含9个基准测试的集合，用于测试*自然语言理解*（NLU）的不同方面。联合性能通过一个单一分数来衡量，对于人工标注员来说，这个分数是87.1。任务通过表[2.1](528393_1_En_2_Chapter.xhtml#Tab1)中的例子进行了详细描述。结果表明，针对不同GLUE任务微调的BERT变体可以比人类产生更好的结果。对于模型的大规模变体，结果已在表[4.1](#Tab1)中确定并展示。表4.1
- en: Results for the GLUE benchmark for four different models and human annotators.
    The best value of a PLM for each task is printed in bold [[18](#CR18), p. 7].
    Human scores better than all model scores are underlined
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 四种不同模型和人类标注者的 GLUE 基准测试结果。每个任务的 PLM 最佳值以粗体打印 [[18](#CR18), p. 7]。人类得分优于所有模型得分的是下划线
- en: '|   | CoLA | QQP | MNLI m | SST-2 | STS-B | QNLI | RTE | WNLI | MRPC |   |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|   | CoLA | QQP | MNLI m | SST-2 | STS-B | QNLI | RTE | WNLI | MRPC |   |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|   | Mcc | Acc | Acc | Acc | Corr | Acc | Acc | Acc | F1 |   |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|   | Mcc | Acc | Acc | Acc | Corr | Acc | Acc | Acc | F1 |   |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Model | Grammar | Paraphr. | Entail | Sentim. | Similar | Question | Entail
    | Coref | Paraphr. | Avg |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| Model | Grammar | Paraphr. | Entail | Sentim. | Similar | Question | Entail
    | Coref | Paraphr. | Avg |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Human [[42](#CR42)] | 66.4 | 80.4 | 92.0 | 97.8 | 92.7 | 91.2 | 93.6 | 95.9
    | 86.3 | 87.1 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| Human [[42](#CR42)] | 66.4 | 80.4 | 92.0 | 97.8 | 92.7 | 91.2 | 93.6 | 95.9
    | 86.3 | 87.1 |'
- en: '| BERT[LARGE] | 60.6 | 91.3 | 86.6 | 93.2 | 90.0 | 92.3 | 70.4 | 65.1 | 88.0
    | 84.1 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| BERT[LARGE] | 60.6 | 91.3 | 86.6 | 93.2 | 90.0 | 92.3 | 70.4 | 65.1 | 88.0
    | 84.1 |'
- en: '| RoBERTa[LARGE] | 68.0 | 92.2 | 90.2 | 96.4 | 92.4 | 93.9 | 86.6 | 89.9 |
    90.9 | 88.8 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa[LARGE] | 68.0 | 92.2 | 90.2 | 96.4 | 92.4 | 93.9 | 86.6 | 89.9 |
    90.9 | 88.8 |'
- en: '| XLNET[LARGE] | 69.0 | 92.3 | 90.8 | **97.0** | 92.5 | 94.9 | 85.9 | 92.5
    | 90.8 | 89.2 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| XLNET[LARGE] | 69.0 | 92.3 | 90.8 | **97.0** | 92.5 | 94.9 | 85.9 | 92.5
    | 90.8 | 89.2 |'
- en: '| DeBERTaV3[LARGE] | **75.3** | **93.0** | **91.8** | 96.9 | **93.0** | **96.0**
    | 92.7 | – | **92.2** | **91.4** |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| DeBERTaV3[LARGE] | **75.3** | **93.0** | **91.8** | 96.9 | **93.0** | **96.0**
    | 92.7 | – | **92.2** | **91.4** |'
- en: In the past years GLUE was routinely employed to demonstrate the NLU capabilities
    of PLMs. Currently, the best average value of 91.4 after fine-tuning was reached
    by DeBERTaV3 [[18](#CR18)] (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2)).
    It uses separate embeddings for content and position and employs a corresponding
    disentangled attention mechanism. There are only three tasks where PLMs are worse
    than humans, but only by a small margin. Note that ensembles of several models
    often yield slightly better results. Nangia et al. [[42](#CR42)] also measures
    the performance of human teams of 5 people. The numbers are not comparable as
    cases were excluded when the teams arrived at split judgment. Newer models such
    as PaLM use SuperGLUE instead of GLUE because GLUE is considered too simple.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，GLUE 经常被用来展示 PLM 的 NLU 能力。目前，经过微调后，DeBERTaV3 [[18](#CR18)] 达到了 91.4 的最佳平均值（Sect.
    [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2)）。它使用内容位置分别嵌入，并采用相应的解耦注意力机制。只有三个任务中
    PLM 的表现不如人类，但差距很小。请注意，多个模型的集成通常会产生略微更好的结果。Nangia 等人 [[42](#CR42)] 也测量了由 5 人组成的人类团队的性能。由于团队在达成分歧判断时排除了案例，因此这些数字不可比较。较新的模型，如
    PaLM，使用 SuperGLUE 而不是 GLUE，因为认为 GLUE 太简单。
- en: '4.1.2 SuperGLUE: An Advanced Version of GLUE'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 SuperGLUE：GLUE的高级版本
- en: Due to the progress in the last years, PLMs have reached human performance in
    most tasks and the GLUE is no longer able to discriminate between models. Therefore,
    the authors of GLUE proposed a more demanding test suite called **SuperGLUE**
    [[68](#CR68)] as an advanced version of GLUE with eight challenging tasks. The
    tasks are similar to GLUE with longer contexts to consider.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于过去几年的进步，PLM 在大多数任务中已经达到了人类水平，GLUE 已经无法区分模型。因此，GLUE 的作者提出了一个更具挑战性的测试套件，称为 **SuperGLUE**
    [[68](#CR68)]，作为 GLUE 的高级版本，包含八个具有挑战性的任务。这些任务与 GLUE 类似，但考虑了更长的上下文。
- en: '*BoolQ* is a QA-task with questions collected from Google search and yes/no
    answers.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*BoolQ* 是一个从 Google 搜索收集问题和是/否答案的 QA 任务。'
- en: '*CB* is a textual entailment task.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CB* 是一个文本蕴涵任务。'
- en: '*COPA* is a causal reasoning task in which a system must determine either the
    cause or effect of a given premise from two possible choices.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*COPA* 是一个因果推理任务，其中系统必须从两个可能的选择中确定给定前提的原因或结果。'
- en: '*MultiRC* is a QA task where each instance consists of a context passage, a
    question about that passage, and a list of possible answers.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MultiRC* 是一个 QA 任务，其中每个实例由一个上下文段落、关于该段落的提问和一个可能的答案列表组成。'
- en: In *ReCoRD* each example consists of a news article and an article in which
    one entity is masked out. The system must predict the masked entity from a list
    of possible entities.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 *ReCoRD* 中，每个示例由一篇文章和一个实体被遮挡的文章组成。系统必须从可能实体列表中预测被遮挡的实体。
- en: '*RTE* requires detecting whether a hypothesis is implied by a premise.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*RTE* 需要检测一个假设是否被前提所暗示。'
- en: '*WiC* is a word sense disambiguation task, where for two given sentences the
    system has to determine if a polysemous word is used with the same sense in both
    sentences.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*WiC* 是一个词语语义消歧任务，其中对于两个给定的句子，系统需要确定一个多义词是否在两个句子中使用了相同的语义。'
- en: '*WSC* is the Winograd Schema Challenge, where the system has to determine the
    correct noun phrase represented by a pronoun.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*WSC* 是 Winograd 框架挑战，其中系统需要确定由代词表示的正确名词短语。'
- en: The performance again is measured by a single average score with a value of
    89.8 for human annotators [[66](#CR66)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 性能再次通过一个平均分数来衡量，该分数为人类标注者的 89.8 [[66](#CR66)]。
- en: '*GPT-3* [[7](#CR7)] is a huge language model (Sect. [3.​1.​2](528393_1_En_3_Chapter.xhtml#Sec3)),
    which can be instructed to perform a task without fine-tuning (Sect. [3.​2](528393_1_En_3_Chapter.xhtml#Sec7)).
    With this few-shot learning GPT-3 achieved an average SuperGLUE score of only
    71.8 as shown in Table [4.2](#Tab2). Obviously fine-tuning the specific tasks
    seems to be important. Recently a fine-tuned DeBERTa ensemble (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2))
    surpassed human performance on SuperGLUE with an average score of 90.3\. The most
    difficult task is a comparison of word senses in two sentences (WiC), where an
    accuracy of about 77% can be reached. The autoregressive LM *PaLM* 540B was fine-tuned
    on SuperGLUE and achieved an average of 90.4% on the test set [[9](#CR9), p. 13].
    The best average of 91.2% was obtained by the *ST-MoE*[32B] mixture-of-experts
    model (Sect. [3.​5.​2](528393_1_En_3_Chapter.xhtml#Sec26)) with 269B parameters
    [[73](#CR73)]. This shows that Foundation Models are able to analyze complex text
    semantics.Table 4.2'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*GPT-3* [[7](#CR7)] 是一个巨大的语言模型（第 [3.1.2](528393_1_En_3_Chapter.xhtml#Sec3)
    节），可以指令执行任务而无需微调（第 [3.2](528393_1_En_3_Chapter.xhtml#Sec7) 节）。通过这种少样本学习，GPT-3
    在表 [4.2](#Tab2) 中显示的平均 SuperGLUE 分数为 71.8。显然，针对特定任务的微调似乎很重要。最近，经过微调的 DeBERTa 集成（第
    [3.1.1](528393_1_En_3_Chapter.xhtml#Sec2) 节）在 SuperGLUE 上的平均分数达到了 90.3，超过了人类的表现。最困难的任务是两个句子中词语语义的比较（WiC），可以达到大约
    77% 的准确率。自回归语言模型 *PaLM* 540B 在 SuperGLUE 上进行了微调，并在测试集上达到了 90.4% 的平均分数 [[9](#CR9),
    p. 13]。最佳平均分数为 91.2%，由具有 269B 参数的 *ST-MoE*[32B] 专家混合模型（第 [3.5.2](528393_1_En_3_Chapter.xhtml#Sec26)
    节）获得。这表明基础模型能够分析复杂的文本语义。表 4.2'
- en: Results for the SuperGLUE benchmark on the test set for human annotators and
    five different models. The best value for each task is printed in bold and human
    values better than the model values are underlined. For GPT-3 few-shot values
    (FS) are reported, fine-tuned otherwise
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 人类标注者和五个不同模型在测试集上对 SuperGLUE 基准测试的结果。每个任务的最好值以粗体打印，人类值优于模型值则以下划线表示。对于 GPT-3
    的少样本值（FS）进行了报告，否则进行了微调
- en: '|   | BoolQ | CB | COPA | MultiRC | ReCoRD | RTE | WiC | WNLI |   |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|   | BoolQ | CB | COPA | MultiRC | ReCoRD | RTE | WiC | WNLI |   |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|   | Acc | Acc/F1 | Acc | F1a/EM | F1/EM | F1/EM | Acc | Acc |   |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|   | Acc | Acc/F1 | Acc | F1a/EM | F1/EM | F1/EM | Acc | Acc |   |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Model | QA y/n | Entail | Cause | QA mult. | Entities | Entail | WSD | Coref
    | Avg |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Model | QA y/n | Entail | Cause | QA mult. | Entities | Entail | WSD | Coref
    | Avg |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Human [[68](#CR68)] | 89.0 | 95.8/98.9 | 100.0 | 81.8/51.9 | 91.7/91.3 |
    93.6 | 80.0 | 100.0 | 89.8 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Human [[68](#CR68)] | 89.0 | 95.8/98.9 | 100.0 | 81.8/51.9 | 91.7/91.3 |
    93.6 | 80.0 | 100.0 | 89.8 |'
- en: '| BERT[336M] [[68](#CR68)] | 77.4 | 83.6/75.7 | 70.6 | 70.0/24.0 | 72.0/71.3
    | 71.6 | 69.5 | 64.3 | 69.0 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| BERT[336M] [[68](#CR68)] | 77.4 | 83.6/75.7 | 70.6 | 70.0/24.0 | 72.0/71.3
    | 71.6 | 69.5 | 64.3 | 69.0 |'
- en: '| GPT-3[270B] FS [[7](#CR7)] | 76.4 | 75.6/52.0 | 92.0 | 75.4/30.5 | 91.1/90.2
    | 69.0 | 49.4 | 80.1 | 71.8 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3[270B] FS [[7](#CR7)] | 76.4 | 75.6/52.0 | 92.0 | 75.4/30.5 | 91.1/90.2
    | 69.0 | 49.4 | 80.1 | 71.8 |'
- en: '| DeBERTA Ens. [[19](#CR19)] | 90.4 | 94.9/97.2 | 98.4 | 88.2/63.7 | 94.5/94.1
    | 93.2 | 77.5 | 95.9 | 90.3 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| DeBERTA Ens. [[19](#CR19)] | 90.4 | 94.9/97.2 | 98.4 | 88.2/63.7 | 94.5/94.1
    | 93.2 | 77.5 | 95.9 | 90.3 |'
- en: '| PaLM[540B] [[9](#CR9)] | 91.9 | 94.4/96.0 | 99.0 | 88.7/63.6 | 94.2/93.3
    | **95.9** | 77.4 | 95.9 | 90.4 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| PaLM[540B] [[9](#CR9)] | 91.9 | 94.4/96.0 | 99.0 | 88.7/63.6 | 94.2/93.3
    | **95.9** | 77.4 | 95.9 | 90.4 |'
- en: '| ST-MoE[32B] [[73](#CR73)] | **92.4** | **96.9/98.0** | **99.2** | **89.6/65.8**
    | **95.1/94.4** | 93.5 | **77.7** | **96.6** | **91.2** |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| ST-MoE[32B] [[73](#CR73)] | **92.4** | **96.9/98.0** | **99.2** | **89.6/65.8**
    | **95.1/94.4** | 93.5 | **77.7** | **96.6** | **91.2** |'
- en: GLUE and SuperGLUE have been criticized, as the answers of the posed problems
    always can be reduced to a classification task and the systems do not have to
    formulate an answer in natural language. In addition, it turns out that the performance
    of PLMs is not very stable. It has been shown that the prediction of current models
    often change in an inconsistent way, if some words are replaced [[51](#CR51)].
    If, for instance, in a sentiment analysis the input *“I love the flight”* is classified
    as *positive*, then *“I didn’t love the flight”* should not be classified as *neutral*.
    Ribeiro et al. [[51](#CR51)] show that inconsistencies like this often occur.
    They developed the **CheckList** system (Sect. [4.3.1](#Sec15)), which automatically
    generates test examples for probing a model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: GLUE和SuperGLUE受到了批评，因为提出的问题的答案总是可以简化为分类任务，系统无需用自然语言来构建答案。此外，PLMs的表现并不非常稳定。研究表明，当前模型的预测往往在替换某些单词时以不一致的方式变化
    [[51](#CR51)]。例如，在情感分析中，如果输入*“我喜欢这次飞行”*被分类为*正面*，那么*“我不喜欢这次飞行”*则不应被分类为*中性*。Ribeiro等人
    [[51](#CR51)] 表明，这种不一致性经常发生。他们开发了**CheckList**系统（第[4.3.1](#Sec15)节），该系统能够自动生成测试示例以探测模型。
- en: 4.1.3 Text Completion Benchmarks
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 文本补全基准
- en: The task of an autoregressive language models is the reliable generation of
    the next word in a text. This has to obey grammatical correctness as well as semantic
    consistency. The *LAMBADA benchmark* [[44](#CR44)] is a good test to demonstrate
    this ability. It consists of about 10,000 passages from the BooksCorpus containing
    unpublished novels. The task is to predict the missing last word of the last sentence
    of each passage. Examples were filtered by humans to ensure that models need to
    take into account the full passage of at least 50 tokens to induce the final word.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归语言模型的任务是可靠地生成文本中的下一个单词。这必须遵守语法正确性和语义一致性。*LAMBADA基准* [[44](#CR44)] 是一个很好的测试，可以展示这种能力。它由BooksCorpus中的大约10,000段未发表的小说组成。任务是预测每个段落的最后一句中缺失的最后一个单词。示例经过人工筛选，以确保模型需要考虑至少50个标记的整个段落来推断最后一个单词。
- en: An example is the passage *“Both its sun-speckled shade and the cool grass beneath
    were a welcome respite after the stifling kitchen, and I was glad to relax against
    the tree’s rough, brittle bark and begin my breakfast of buttery, toasted bread
    and fresh fruit. Even the water was tasty, it was so clean and cold. It almost
    made up for the lack of*  *.”*, where *“coffee”* is the missing target word to
    be predicted. Examples which could be easily predicted by simpler language models
    were omitted. Examples were only selected, if the target word could be predicted
    by humans from the full passage but not from the last sentence.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，段落*“在炎热的厨房之后，它那阳光斑驳的阴影和下面凉爽的草地都是一种受欢迎的休息，我很高兴靠在树那粗糙、脆弱的树皮上，开始我的黄油烤面包和新鲜水果的早餐。连水都很好喝，它那么干净、冰凉。这几乎弥补了*……*的不足。”*，其中*“咖啡”*是待预测的目标单词。那些可以被简单语言模型轻易预测的示例被省略了。只有当目标单词可以从整个段落中而不是从最后一句中由人类预测时，才会选择示例。
- en: The GPT-3[175B] autoregressive language model [[48](#CR48)] predicted the last
    word with 76.2% [[7](#CR7), p. 12]. PaLM[540B] with few-shot instructions could
    increase the accuracy to 89.7 [[9](#CR9), p. 79]. This means that in nearly nine
    of ten cases, the predicted word was exactly the missing word in the test data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3[175B]自回归语言模型 [[48](#CR48)] 预测最后一个单词的准确率为76.2% [[7](#CR7)，第12页]。PaLM[540B]在少量指令下可以将准确率提高到89.7
    [[9](#CR9)，第79页]。这意味着在近九成的情况下，预测的单词正是测试数据中的缺失单词。
- en: 'Another relevant benchmark for language modeling is *WikiText-103* [[38](#CR38)]
    of 28k articles from Wikipedia with 103M tokens. If large Foundation Models are
    applied to this corpus the following perplexities result: GPT-2[1.7B] 17.5 [[48](#CR48)],
    Megatron-LM 10.8 [[58](#CR58)], Gopher[280B] 8.1 [[49](#CR49), p. 61]. Recently
    a small Retro[1.8B] model with retrieval could reduce this perplexity to 3.9 [[49](#CR49),
    p. 12]. Note that there might be a partial overlap of Wikitext 103 with Retro’s
    training data not caught by deduplication.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个与语言模型相关的基准是来自维基百科的28k篇文章，包含1.03亿个标记的*WikiText-103* [[38](#CR38)]。如果将大型基础模型应用于这个语料库，以下困惑度结果：GPT-2[1.7B]
    17.5 [[48](#CR48)]，Megatron-LM 10.8 [[58](#CR58)]，Gopher[280B] 8.1 [[49](#CR49)，第61页]。最近，一个带有检索功能的小型Retro[1.8B]模型将这种困惑度降低到3.9
    [[49](#CR49)，第12页]。请注意，Wikitext 103可能与Retro的训练数据部分重叠，而这种重叠没有被去重检测到。
- en: 4.1.4 Large Benchmark Collections
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.4 大型基准集合
- en: Recently large autoregressive language models like GPT-3, Gopher, and PaLM have
    been developed, which are trained on extremely large document collections with
    hundreds of billions of tokens. The models should perform well across a wide range
    of tasks. Therefore, instead of the limited GLUE benchmarks a large number of
    benchmarks covering many aspects of possible applications are used to evaluate
    their performance.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最近开发了像GPT-3、Gopher和PaLM这样的大型自回归语言模型，这些模型在包含数百亿个标记的极大规模文档集合上进行了训练。这些模型应该在广泛的任务上表现良好。因此，除了有限的GLUE基准测试之外，还使用了覆盖可能应用许多方面的众多基准测试来评估它们的性能。
- en: 'A frequent opinion is that current benchmarks are insufficient and “saturate”,
    “have artifacts”, and are “overfitted by researchers”. Bowman et al. [[5](#CR5)]
    argue that “evaluation for many natural language understanding (NLU) tasks is
    broken”. They complain that there are systems at the top of the leaderboards which
    fail in simple test cases (cf. [[51](#CR51)]). As a consequence they formulate
    four requirements on new benchmarks:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的观点是，当前的基准测试不足且“饱和”，“存在缺陷”，“被研究人员过度拟合”。Bowman等人[[5](#CR5)]认为“许多自然语言理解（NLU）任务的评估已经破裂”。他们抱怨排行榜顶部的系统在简单的测试案例中失败（参见图[[51](#CR51)]）。因此，他们提出了对新基准的四项要求：
- en: A model should only reach good performance on the benchmark if it also has a
    good performance on actual applications.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个模型在实际应用中也表现出良好的性能，那么它才应该在基准测试中达到良好的性能。
- en: The annotation of benchmarks should be accurate and not ambiguous (e.g. 36%
    of the answers in Natural Questions are ambiguous).
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准测试的标注应该准确且不模糊（例如，自然问题中的36%的答案都是模糊的）。
- en: The benchmarks should be large and challenging enough to detect relevant performance
    differences between models.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准测试应该足够大且具有挑战性，以便能够检测模型之间相关的性能差异。
- en: Benchmarks should reveal plausibly harmful social biases in systems, and should
    not encourage the creation of biases.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准测试应该揭示系统中可能的有害社会偏见，并且不应鼓励偏见的产生。
- en: They summarize some promising developments that could support these challenges,
    including data collection involving both crowdworkers and domain experts, and
    larger-scale data validation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 他们总结了一些可能支持这些挑战的积极发展，包括涉及众包工作者和领域专家的数据收集，以及更大规模的数据验证。
- en: To address this criticism, two comprehensive collections of benchmarks have
    been defined. The *Massive Multitask Language Understanding* (MMLU) benchmark
    [[20](#CR20)] emulates human exams with multiple choice questions, each with four
    responses. In addition to logical and mathematical reasoning it tests a model’s
    ability across a wide range of academic subjects from computer science to history
    and law. The other collection is the *BIG-bench* collaborative benchmark [[1](#CR1),
    [60](#CR60)], designed to evaluate language interpretation aspects like reading
    comprehension, question answering, world understanding, etc. Both benchmark collections
    include more than a hundred tasks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这种批评，已经定义了两个全面的基准测试集合。*大规模多任务语言理解*（MMLU）基准[[20](#CR20)]通过多项选择题模拟人类考试，每个问题有四个选项。除了逻辑和数学推理之外，它还测试了模型在从计算机科学到历史和法律等广泛学术领域的各种能力。另一个集合是*BIG-bench*协作基准[[1](#CR1)，[60](#CR60)]，旨在评估语言解释方面，如阅读理解、问答、世界理解等。这两个基准测试集合都包括超过一百个任务。
- en: The *Gopher* model with 280B parameters together with alternatives like GPT-3,
    Jurassic-1, and Megatron-Turing NLG (all discussed in Sect. [3.​1.​2](528393_1_En_3_Chapter.xhtml#Sec3))
    were tested on these and other benchmarks. Note that this was done with a total
    of 152 benchmarks described in Table [4.3](#Tab3). Gopher shows an improvement
    on 100 of 124 tasks (81%) compared to the previous Sota scores. In language modeling
    (next word prediction) Gopher improves Sota for 10 of 19 benchmarks. Note that
    all benchmark results were not obtained after fine-tuning but by zero-shot or
    few-shot learning.Table 4.3
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 具有280B参数的*Gopher*模型以及GPT-3、Jurassic-1和Megatron-Turing NLG（所有这些都在第[3.1.2](528393_1_En_3_Chapter.xhtml#Sec3)节中讨论）等替代方案在这些和其他基准测试上进行了测试。请注意，这是通过总共152个基准测试完成的，这些基准测试在表[4.3](#Tab3)中描述。与之前的Sota分数相比，Gopher在124个任务中的100个任务上有所改进（81%）。在语言建模（下一个单词预测）中，Gopher在19个基准测试中的10个基准测试上提高了Sota。请注意，所有基准测试结果都不是通过微调获得的，而是通过零样本或少量样本学习获得的。表4.3
- en: Groups of evaluation benchmarks for Gopher and related models [[49](#CR49),
    p. 8]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Gopher和相关模型[[49](#CR49)，第8页]的评估基准组
- en: '| Task group | # Tasks | Examples |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 任务组 | # 任务 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Language modeling | 20 | WikiText-103, The Pile: PG-19, arXiv, FreeLaw, …
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | 20 | WikiText-103, The Pile: PG-19, arXiv, FreeLaw, … |'
- en: '| Reading comprehension | 3 | RACE-m, RACE-h, LAMBADA |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 阅读理解 | 3 | RACE-m, RACE-h, LAMBADA |'
- en: '| Fact checking | 3 | FEVER (2-way & 3-way), MultiFC |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 事实核查 | 3 | FEVER (2-way & 3-way), MultiFC |'
- en: '| Question answering | 3 | Natural questions, TriviaQA, TruthfulQA |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 问答 | 3 | Natural questions, TriviaQA, TruthfulQA |'
- en: '| Common sense | 4 | HellaSwag, Winogrande, PIQA, SIQA |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 常识 | 4 | HellaSwag, Winogrande, PIQA, SIQA |'
- en: '| Massive multitask language understanding (MMLU) [[20](#CR20)] | 57 | High
    school chemistry, astronomy, clinical knowledge, social science, math, … |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 大规模多任务语言理解（MMLU） [[20](#CR20)] | 57 | 高中化学，天文学，临床知识，社会科学，数学，… |'
- en: '| BIG-bench [[60](#CR60)] | 62 | Causal judgement, epistemic reasoning, temporal
    sequences, logic, math, code, social reasoning, … | The distribution Gopher accuracies
    for thematic groups are shown in Fig. [4.1](#Fig1). Gopher is able to increase
    Sota for 4 out of 7 math tasks, 5 out of 9 common sense tasks, 9 out of 12 logical
    reasoning tasks, 22 out of 24 fact checking and general knowledge tasks, all 24
    STEM (Science Technology Engineering Mathematics) and medicine tasks, all 15 humanities
    and ethics task, and 10 out of 11 reading comprehension tasks. The average accuracies
    for common sense and general knowledge are about 50%, indicating that some knowledge
    exists but can be improved. Among these tests were benchmarks on logical reasoning,
    which, for instance, include “Formal Fallacies Syllogisms Negation” or “Logical
    Fallacy Detection”. Only two of the 19 benchmarks achieved an accuracy of more
    than 60% [[49](#CR49), p. 58], indicating that even for this large model correct
    reasoning is a major obstacle. Obviously this spectrum of evaluation gives a deep
    insight into the capabilities of the compared models. It can be expected that
    the new Retro model (Sect. [6.​2.​3](528393_1_En_6_Chapter.xhtml#Sec15)), which
    performs retrieval during language generation, will improve these results.![](../images/528393_1_En_4_Chapter/528393_1_En_4_Fig1_HTML.png)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '| BIG-bench [[60](#CR60)] | 62 | 因果判断，认知推理，时间序列，逻辑，数学，代码，社会推理，… | 主题组别的Gopher准确率分布如图[4.1](#Fig1)所示。Gopher能够提高7个数学任务中的4个，9个常识任务中的5个，12个逻辑推理任务中的9个，24个事实核查和常识任务中的22个，所有24个STEM（科学、技术、工程、数学）和医学任务，所有15个人文和伦理任务，以及11个阅读理解任务中的10个。常识和常识的平均准确率约为50%，表明存在一些知识但可以改进。在这些测试中包括逻辑推理的基准测试，例如“形式谬误三段论否定”或“逻辑谬误检测”。只有19个基准测试中的两个达到了超过60%的准确率
    [[49](#CR49)，第58页]，这表明即使是对于这个大型模型，正确的推理也是一个主要障碍。显然，这个评估范围对比较模型的性能提供了深刻的洞察。可以预期，新的Retro模型（第[6.2.3](528393_1_En_6_Chapter.xhtml#Sec15)节），在语言生成过程中进行检索，将提高这些结果。![图片](../images/528393_1_En_4_Chapter/528393_1_En_4_Fig1_HTML.png)'
- en: A box plot of accuracy versus the different groups. General knowledge ranges
    between 40 and 75, and has more than 50 % of the median value. Humanities and
    Medicine have less range and mark one outliner for humanities below lower whisker.
    Medicine marks one outliners for upper and lower whiskers.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率与不同组别的箱线图。常识范围在40到75之间，中位数以上超过50%。人文和医学的范围较小，人文在低于下须处有一个异常值。医学在上须和下须处各有一个异常值。
- en: Fig. 4.1
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1
- en: Accuracies in percent of different groups covering 152 different benchmarks
    evaluated for the Gopher model [[49](#CR49), p. 57]. The 25% and 75% percentiles
    are given by the box, and the inner line is the median. The outside lines indicate
    variability outside the upper and lower quartiles
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 不同组别覆盖的152个不同基准测试的Gopher模型的准确率百分比 [[49](#CR49)，第57页]。箱形图给出了25%和75%的分位数，内线是中位数。外线表示超出上下四分位数的变化范围
- en: The *PaLM* autoregressive language model with 580B parameters [[9](#CR9), p.
    15] recently was evaluated with the BIG-bench benchmark. On the 150 tasks, PaLM
    with 5-shot prompts achieved an normalized average score of 46%, which was better
    than the average human score of 39%. However, the best human experts have a score
    of about 77%. The detailed results for the different BIG benchmark areas are not
    yet available. On a subset of 58 BIG-tasks, which were also used by prior models,
    PaLM obtained a 5-shot normalized score of about 55%, again above the human average
    of 49%, outperforming Chinchilla (47%) and Gopher (30%). GPT-3 achieved a 1-shot
    performance of 16% on the 58 tasks. In general Foundation Models like Gopher and
    PaLM with several hundred billion parameters have ‘dramatically better’ results
    on BIG than smaller models, even if the model architecture is not fundamentally
    different [[1](#CR1)]. In this respect Foundation Models show a qualitatively
    new behavior.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 参数量为580亿的*PaLM*自回归语言模型[[9](#CR9), p. 15]最近使用BIG-bench基准进行了评估。在150个任务中，PaLM通过5次提示实现了46%的正常化平均得分，这超过了39%的人类平均得分。然而，最佳人类专家的得分约为77%。不同BIG基准领域的详细结果尚未公布。在58个BIG任务子集上，这些任务也被先前模型使用，PaLM获得了约55%的5次提示正常化得分，再次超过了49%的人类平均得分，超过了Chinchilla（47%）和Gopher（30%）。GPT-3在这58个任务上实现了16%的单次性能。总的来说，具有数百亿参数的通用模型如Gopher和PaLM在BIG基准上的表现比小模型“显著更好”，即使模型架构没有本质上的不同[[1](#CR1)]。在这方面，通用模型表现出质的新行为。
- en: Researchers at Google have proposed to use the BIG-bench benchmark with currently
    200 tasks as a replacement for the Turing test for “intelligence” [[61](#CR61)].
    In this way the knowledge of an AI-System can be checked at a large scale. Recently,
    a Google engineer published a dialog [[31](#CR31)] with the LaMDA language model
    (Sect. [6.​6.​3](528393_1_En_6_Chapter.xhtml#Sec52)). In his view this indicates
    that LaMDA is “sentient”. However, this aspect of human intelligence is not checked
    by knowledge and reasoning tests such as BIG and requires the development of new
    types of tests.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌的研究人员提出了使用目前包含200个任务的BIG-bench基准作为“智能”的图灵测试的替代品[[61](#CR61)]。这样就可以在大规模上检查AI系统的知识。最近，一位谷歌工程师发布了一个与LaMDA语言模型（见第[6.6.3](528393_1_En_6_Chapter.xhtml#Sec52)节）的对话[[31](#CR31)]。在他看来，这表明LaMDA是“有感知的”。然而，这种人类智能的方面并没有通过BIG和类似的知识与推理测试来检查，需要开发新的测试类型。
- en: 4.1.5 Summary
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.5 摘要
- en: Benchmark collections are a popular way to demonstrate the superiority of a
    Pre-trained Language Model for specific tasks. To show the merits of an architecture,
    however, also the number of parameters, the size of training data, and the computing
    effort has to be reported and compared, because these numbers also affect the
    model performance.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 基准集合是展示特定任务中预训练语言模型优越性的流行方式。然而，为了展示架构的优点，还必须报告和比较参数数量、训练数据大小和计算工作量，因为这些数字也会影响模型性能。
- en: The GLUE benchmark collection of nine language understanding tasks has demonstrated
    the considerable progress of PLMs during the last years. It tests the ability
    of PLMs to detect paraphrases, coreference relations, logical entailments and
    grammatical correctness. Meanwhile, the average accuracy exceeds the average human
    performance. The similar, more challenging SuperGLUE benchmark suite has been
    introduced, where human performance is also surpassed. For autoregressive language
    models the LAMBADA benchmark requires an impressive ability to determine the most
    probable last word of a paragraph. Current models like PaLM are able to predict
    the last word with an accuracy of nearly 90% demonstrating its ability to capture
    the flow of arguments.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: GLUE基准集合包含九个语言理解任务，展示了过去几年中预训练语言模型（PLM）的显著进步。它测试了PLM检测释义、共指关系、逻辑蕴涵和语法正确性的能力。同时，平均准确率超过了人类平均表现。类似的、更具挑战性的SuperGLUE基准套件也已引入，其中人类表现也被超越。对于自回归语言模型，LAMBADA基准要求具有确定段落最后最可能单词的令人印象深刻的能力。当前模型如PaLM能够以近90%的准确率预测最后单词，展示了其捕捉论点流的能力。
- en: Foundation Models are usually tested by extensive standardized test collections
    covering many aspects like common sense knowledge, emotional intelligence, logical
    reasoning, or social sciences. Recent Foundation Models like Gopher and PaLM,
    with several hundred billion parameters, have been able to achieve performance
    better than that the human average and ‘dramatically better’ than smaller models.
    However, these models still have a lower accuracy than human experts. Although
    the benchmarks are very expressive, they do not take into account the societal
    impact of the models and are unable to detect features like self-awareness and
    sentience.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型通常通过涵盖许多方面（如常识知识、情商、逻辑推理或社会科学）的广泛标准化测试集合进行测试。最近的基座模型，如 Gopher 和 PaLM，具有数百亿个参数，已经能够实现优于人类平均水平的性能，并且比小模型“显著更好”。然而，这些模型的准确率仍然低于人类专家。尽管基准测试非常具有表现力，但它们并没有考虑模型的社会影响，并且无法检测到自我意识和感知等特征。
- en: 4.2 Evaluating Knowledge by Probing Classifiers
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 通过探针分类器评估知识
- en: In this section, we examine the extent to which PLMs acquire different types
    of knowledge. We discuss the covered knowledge for the small BERT model and later
    review the improvements for foundation models such as GPT-3 and PaLM. First, we
    consider their syntactic knowledge of correct language. Then, we investigate how
    much common sense knowledge is represented by PLMs. Finally, we explore whether
    the output produced by PLMs is logically consistent.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们检查 PLMs 获取不同类型知识的程度。我们讨论了小 BERT 模型的覆盖知识，并随后回顾了 GPT-3 和 PaLM 等基础模型的改进。首先，我们考虑它们的正确语言句法知识。然后，我们调查
    PLMs 表示的常识知识有多少。最后，我们探索 PLMs 生成的输出是否逻辑一致。
- en: 4.2.1 BERT’s Syntactic Knowledge
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 BERT 的句法知识
- en: We discuss the syntactic knowledge incorporated in PLMs using BERT as an example.
    In the course of pre-training BERT is able to capture *syntactic knowledge* [[54](#CR54)].
    Embeddings can encode information about parts of speech, syntactic phrases and
    syntactic roles. Probing classifiers can predict part-of-speech tags and supersense
    information with an accuracy of 85% [[33](#CR33)]. Obviously, this information
    has to be encoded in BERT’s final embeddings. BERT also has knowledge of subject-verb
    agreement [[17](#CR17)] and semantic roles [[14](#CR14)]. It is also possible
    to extract dependency trees and syntactic constituency trees from BERT [[21](#CR21),
    [23](#CR23), [27](#CR27)]. While probing indicates that the information can be
    extracted from the representation, it can be shown [[13](#CR13)] that in some
    cases the features are not used for prediction. According to an empirical evaluation
    PLMs encode linguistic information with phrase features in the bottom layers,
    syntactic features in the middle layers and semantic features in the top layers
    [[23](#CR23)].
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以 BERT 为例，讨论了在 PLMs 中融入的句法知识。在预训练过程中，BERT 能够捕捉到 *句法知识* [[54](#CR54)]。嵌入可以编码关于词性、句法短语和句法角色的信息。探针分类器可以以
    85% 的准确率预测词性标签和超感信息 [[33](#CR33)]。显然，这些信息必须编码在 BERT 的最终嵌入中。BERT 还具有主谓一致 [[17](#CR17)]
    和语义角色 [[14](#CR14)] 的知识。从 BERT 中还可以提取依存树和句法成分树 [[21](#CR21), [23](#CR23), [27](#CR27)]。虽然探针表明可以从表示中提取信息，但可以证明
    [[13](#CR13)] 在某些情况下，特征并未用于预测。根据经验评估，PLMs 在底层使用短语特征编码语言信息，在中层使用句法特征，在顶层使用语义特征
    [[23](#CR23)]。
- en: However, BERT’s syntactic knowledge is incomplete and there is, for example,
    evidence that BERT often does not capture *negations*. For instance, BERT[LARGE]
    is able to determine the correct supersense, e.g. *“bird”* in the masked sentence
    *“A robin is a [MASK]”* with high probability [[14](#CR14)]. On the other hand,
    the model predicts *“robin”*, *“bird”*, *“penguin”*, *“man”*, *“fly”* with maximum
    probabilities for the mask in *“A robin is not a [MASK]”*, effectively ignoring
    the negation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，BERT 的句法知识是不完整的，例如，有证据表明 BERT 经常无法捕捉到 *否定*。例如，BERT[LARGE] 能够以高概率确定掩码句子 *“A
    robin is a
- en: Some benchmarks described in Sect. [4.1](#Sec1) check the syntactic knowledge
    of PLMs. An example is the GLUE’s CoLA task testing the grammatical correctness
    of sentences, which is the most difficult task of GLUE where the best models only
    yield about 75% correct answers (Table [4.1](#Tab1)). SuperGLUE (Sect. [4.1.2](#Sec3))
    is a benchmark, which requires syntactic knowledge, e.g. for the textual entailment
    task COPA and the coreference resolution task WSC. While the fine-tuned BERT gets
    an average score of 69.0 the fine-tuned PaLM[540B] achieves an average of 91.4
    (Table [4.2](#Tab2)). Large foundation models such as PaLM, which has more than
    1000 times as many parameters as BERT, are obviously able to capture syntactical
    knowledge much better than the ‘small’ BERT.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第[4.1](#Sec1)节中描述的一些基准测试检查了PLM的句法知识。一个例子是GLUE的CoLA任务，测试句子的语法正确性，这是GLUE中最困难的任务，最佳模型仅产生大约75%的正确答案（表[4.1](#Tab1)）。SuperGLUE（第[4.1.2](#Sec3)节）是一个基准，它需要句法知识，例如文本蕴涵任务COPA和指代消解任务WSC。虽然微调后的BERT的平均得分为69.0，但微调后的PaLM[540B]的平均得分为91.4（表[4.2](#Tab2)）。具有超过1000倍于BERT参数的大规模基础模型，如PaLM，显然能够比“小型”BERT更好地捕捉句法知识。
- en: 4.2.2 Common Sense Knowledge
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 常识知识
- en: '*World knowledge*, also called *common sense knowledge*, consists of facts
    about our every day world, such as *“fire is hot”*. A simple method of checking
    world knowledge is to query BERT with cloze statements, for example, *“Einstein
    was born in [MASK]”*. BERT acquires some *semantic knowledge* about semantic roles
    and encodes information about entity types and relations [[54](#CR54)]. For instance,
    in the sentence *“to tip a [MASK]”* the token *“waiter”* gets a high probability
    for the position of *[MASK]*. Petroni et al. [[46](#CR46)] and Zhou et al. [[72](#CR72)]
    experimented with such queries and concluded that BERT contains world knowledge
    competitive with traditional supervised information extraction methods. It has
    been shown that BERT’s contextual embeddings make up clusters corresponding to
    *word senses* [[56](#CR56)]. This explains why BERT is quite capable of word sense
    disambiguation (Fig. [2.​10](528393_1_En_2_Chapter.xhtml#Fig10)).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*世界知识*，也称为*常识知识*，包括我们日常生活中的事实，例如*“火是热的”*。检查世界知识的一个简单方法是用BERT进行填空语句查询，例如，*“爱因斯坦出生于  ”*。BERT获取了一些关于语义角色的*语义知识*，并编码了关于实体类型和关系的信息
    [[54](#CR54)]。例如，在句子*“给 付小费”*中，标记*“waiter”*获得高概率的位置为*  。Petroni等人 [[46](#CR46)]
    和Zhou等人 [[72](#CR72)] 对此类查询进行了实验，并得出结论，BERT包含与传统监督信息提取方法相媲美的世界知识。已经证明，BERT的上下文嵌入形成了对应于*词义*的簇
    [[56](#CR56)]。这解释了为什么BERT在词义消歧方面相当有能力（图[2.10](528393_1_En_2_Chapter.xhtml#Fig10)）。'
- en: Petroni et al. [[46](#CR46)] remark that certain types of factual knowledge
    are learned much more easily than others by the standard language model pre-training
    approaches. They state that without fine-tuning, BERT contains relational knowledge
    competitive with traditional NLP methods that have some access to oracle knowledge.
    In addition, BERT also does remarkably well on open-domain question answering
    against a supervised baseline. These capabilities of BERT are a great achievement.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Petroni等人 [[46](#CR46)] 指出，某些类型的事实知识比其他类型的事实知识更容易通过标准的语言模型预训练方法学习。他们表示，在没有微调的情况下，BERT包含与传统NLP方法相媲美的关系知识，这些方法可以访问一些神谕知识。此外，BERT在开放域问答方面也表现出色，优于监督基线。BERT的这些能力是一项巨大的成就。
- en: 'The language model GPT-3 has one hundred times more parameters than BERT and
    a dramatically better common sense knowledge. This, for example, can be seen from
    its answers (A) to the questions (Q): *“Q: Are there any animals with three legs?”**“A:
    No, there are no animals with three legs.”* or *“Q: Which is heavier, a football
    player or a car?”**“A: A car is heavier than a football player.”* [[29](#CR29)].
    In an initial experiment eighty persons were asked to assess, if short 200 word
    articles were written by humans or GPT-3\. The persons judged incorrectly 48%
    of the time, doing only slightly better than random guessing [[7](#CR7)].'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型GPT-3的参数比BERT多一百倍，常识知识也大大提高。例如，可以从其答案（A）对问题（Q）的回答中看出：“Q：有没有三条腿的动物？”**“A：没有，没有三条腿的动物。”**或**“Q：足球运动员和汽车哪个更重？”**“A：汽车比足球运动员重。”**
    [[29](#CR29)]。在一个初步实验中，有80人被要求评估，如果简短的200字文章是由人类还是GPT-3写的。人们48%的时间判断错误，仅略好于随机猜测
    [[7](#CR7)]。
- en: However, the semantic knowledge of PLMs is not perfect. BERT, for instance,
    has difficulties with the representation of numbers and often has problems with
    the replacement of *named entities* (*NE*s), e.g. person names or location names.
    For example, replacing names in the coreference task changes 85% of coreference
    assignments of expressions that refer to the same entity [[3](#CR3)]. Obviously
    the pre-trained version of BERT struggles to generalize the relations involving
    one named entity to other named entities of the same type. Moreover, BERT has
    problems to transfer knowledge based on the roles or types of objects. In addition,
    it is possible to mislead BERT by adding some content to a cloze query. An example
    is the word *“Talk”* in *“Talk? Birds can [MASK]”*. A human would ignore *“Talk?”*
    and use his world knowledge to generate a result like *“fly”*. In contrast, PLMs
    can be misled and produce the wrong answer *“talk”* for the mask [[26](#CR26)].
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，PLM的语义知识并不完美。例如，BERT在表示数字和替换*命名实体*（NEs）方面有困难，经常有关于替换人名或地点名的问题。例如，在指代任务中替换名字改变了85%的指代分配，这些分配指的是同一实体[[3](#CR3)]。显然，BERT的预训练版本在将涉及一个命名实体的关系泛化到同一类型的其他命名实体方面有困难。此外，BERT在基于对象的角色或类型转移知识方面也有问题。此外，通过向完形填空查询添加一些内容，可能会误导BERT。例如，“*Talk*”这个词在“*Talk?
    Birds can*”中。一个人会忽略“*Talk?*”并利用他的世界知识生成一个像“*fly*”这样的结果。相比之下，PLM可能会被误导，并产生对掩码错误的答案“*talk*”[[26](#CR26)]。
- en: A related phenomenon is the invariance to *paraphrases*. Elazar et al. [[12](#CR12)]
    generate a high-quality set of 328 paraphrases to express 38 relations. Examples
    are *“X originally aired on [MASK]”* and *“X premiered on [MASK]”*, which should
    give the same prediction for *[MASK]*, if *“X”* is replaced by some TV series
    like *“Seinfeld”*. Although the models in about 60% of the cases have access to
    the required knowledge to fill the mask correctly, BERT[LARGE] yields a consistency
    in paraphrases in only 48.7% of the cases. This indicates that not every fact
    present in the training data is encoded in the parameters and that the model does
    not always detect the equivalence of paraphrases. The model variants RoBERTa and
    ALBERT achieve a lower consistency, although they are superior to BERT in other
    tasks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 相关现象之一是对*释义*的不变性。Elazar等人[[12](#CR12)]生成了一个包含328个高质量释义的集合，用以表达38个关系。例如，“*X最初在*”和“*X首映于*”，如果将*“X”*替换为像*“Seinfeld”*这样的电视剧，它们应该对*“X”*给出相同的预测。尽管大约60%的模型能够访问到正确填充掩码所需的知识，但BERT[LARGE]在仅48.7%的情况下在释义上保持一致性。这表明训练数据中并非每个事实都被编码在参数中，并且模型并不总是能够检测到释义之间的等价性。模型变体RoBERTa和ALBERT在一致性方面表现较低，尽管它们在其他任务上优于BERT。
- en: It is instructive to consider the influence of word order on the performance
    of BERT. Word order is taken into account by specific position embeddings, which
    are added to the token embeddings. It turns out, however that masked language
    models like BERT still achieve a high accuracy, if word positions are permuted.
    For pre-training Sinha et al. [[59](#CR59)] perform sentence permutations, where
    each word in a sentence is randomly placed at a different position. The model
    was fine-tuned on GLUE, a set of classification tasks for natural language understanding
    (Sect. [2.​1.​5](528393_1_En_2_Chapter.xhtml#Sec7)). If we ignore the CoLA-task,
    which checks linguistic acceptability, the model on average only looses 3.4% accuracy
    if the word order is permuted compared to the original RoBERTa results (88.7%
    on average). The authors conclude that BERT-like models achieve high performance
    on downstream tasks almost entirely by exploiting higher-order word co-occurrence
    statistics.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑词序对BERT性能的影响是有教育意义的。词序通过特定的位置嵌入被考虑在内，这些嵌入被添加到标记嵌入中。然而，结果却表明，即使词的位置被重新排列，像BERT这样的掩码语言模型仍然能够达到很高的准确率。对于预训练，Sinha等人[[59](#CR59)]执行了句子排列，其中句子中的每个词都被随机放置在不同的位置。该模型在GLUE上进行了微调，GLUE是一组用于自然语言理解的分类任务（第[2.1.5](528393_1_En_2_Chapter.xhtml#Sec7)节）。如果我们忽略检查语言可接受性的CoLA任务，与原始RoBERTa结果（平均88.7%）相比，如果词序被重新排列，模型平均只降低了3.4%的准确率。作者得出结论，BERT类模型在下游任务上几乎完全通过利用高阶词共现统计来实现高性能。
- en: Another aspect of common sense knowledge is time. When a PLM is applied to new
    documents it often does not know the meaning of new named entities and concepts
    [[30](#CR30)]. Often, the model cannot infer the time and region of a document
    and may not be able to correctly combine facts from documents that originate from
    different time periods or geographical regions. A benchmark for assessing the
    temporal reasoning capabilities of PLMs in dialogs shows that BERT and T5 have
    major deficits on this task [[47](#CR47)]. In summary it can be expected that
    the new Retro (Sect. [6.​2.​3](528393_1_En_6_Chapter.xhtml#Sec15)) or WebGPT (Sect.
    [6.​2.​3](528393_1_En_6_Chapter.xhtml#Sec16)) models, which perform retrieval
    during language generation, will considerably mitigate the problems discussed
    in this section.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 常识知识的另一个方面是时间。当一个PLM应用于新的文档时，它通常不知道新命名实体和概念的含义 [[30](#CR30)]。通常，模型无法推断文档的时间和地区，并且可能无法正确地将来自不同时间或地理区域的文档中的事实结合起来。一个用于评估PLM在对话中时间推理能力的基准显示，BERT和T5在这个任务上存在重大缺陷
    [[47](#CR47)]。总的来说，可以预期，在语言生成期间进行检索的新Retro（第[6.2.3](528393_1_En_6_Chapter.xhtml#Sec15)节）或WebGPT（第[6.2.3](528393_1_En_6_Chapter.xhtml#Sec16)节）模型将显著减轻本节中讨论的问题。
- en: To be able to check a multitude of different knowledge types in a standardized
    way large benchmarks like BIG-bench have been developed (Sect. [4.1.4](#Sec5)).
    It comprises benchmarks on common sense, emotional intelligence, ethics, fact
    checking, general knowledge, humanities, mathematics, medicine, reading comprehension,
    science and social sciences. Figure [4.1](#Fig1) shows the performance of the
    Gopher model with 280B parameters on these benchmark groups. On most groups more
    than 50% accuracy was achieved. The PaLM model with 540B parameters was able to
    improve these performance figures. On about 2∕3 of these tasks PaLM using 5-shot
    prompts achieves a better performance than average humans [[9](#CR9), p. 17].
    This indicates that PaLM has a much better common sense knowledge than earlier
    models. Nevertheless, PaLM surpasses the performance of human experts only in
    a small fraction of cases suggesting further headroom for improvement.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够以标准化的方式检查多种不同的知识类型，已经开发了像BIG-bench这样的大型基准（第[4.1.4](#Sec5)节）。它包括常识、情商、伦理、事实核查、一般知识、人文学科、数学、医学、阅读理解、科学和社会科学等基准。图[4.1](#Fig1)显示了具有280B参数的Gopher模型在这些基准组上的性能。在大多数组中，准确率超过了50%。具有540B参数的PaLM模型能够提高这些性能指标。在这些任务的大约2/3中，使用5次提示的PaLM比平均人类表现更好
    [[9](#CR9), p. 17]。这表明PaLM比早期模型具有更好的常识知识。尽管如此，PaLM仅在少数情况下超越了人类专家的表现，这表明仍有进一步改进的空间。
- en: An interesting idea is to use large pre-trained multilingual language models
    as a multilingual knowledge base [[25](#CR25)]. The authors evaluate this for
    mBERT (Sect. [3.​3.​1](528393_1_En_3_Chapter.xhtml#Sec13)), a standard BERT model,
    which has been pre-trained with the MLM loss on non-parallel Wikipedia texts from
    104 languages. The authors find that correct entities can be retrieved for many
    languages. However, there is a clear performance gap between English and, e.g.,
    Japanese and Thai. This suggests that mBERT does not store knowledge about entities
    in a language-independent way. It would be revealing if these experiments could
    be repeated with up-to-date language models like PaLM.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的想法是使用大型预训练的多语言语言模型作为多语言知识库 [[25](#CR25)]。作者评估了mBERT（第[3.3.1](528393_1_En_3_Chapter.xhtml#Sec13)节），这是一个标准的BERT模型，它已经在104种语言的非平行维基百科文本上使用MLM损失进行了预训练。作者发现，对于许多语言，可以检索到正确的实体。然而，英语与日语和泰语等语言之间存在明显的性能差距。这表明mBERT并没有以语言无关的方式存储实体知识。如果这些实验可以用像PaLM这样的最新语言模型重复进行，那将是有启发性的。
- en: 4.2.3 Logical Consistency
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3 逻辑一致性
- en: A set of statements is logically inconsistent if they cannot all be true at
    the same time. As an example consider the statements “John is Tom’s father. Tom
    is the daughter of John.” Sometimes, BERT is unable to reason, i.e. logically
    connect different pieces of knowledge. It reproduces, for instance, the relations
    that persons can walk into houses, and that houses are big, but it cannot infer
    that houses are bigger than persons [[15](#CR15), [52](#CR52)]. However, semantic
    knowledge problems tend to be smaller for models with more parameters.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一组陈述不能同时都为真，则它们在逻辑上是不一致的。例如，考虑以下陈述：“约翰是汤姆的父亲。汤姆是约翰的女儿。”有时，BERT无法推理，即逻辑地连接不同的知识片段。例如，它重现了人们可以走进房子，房子很大这样的关系，但它无法推断房子比人更大[[15](#CR15),
    [52](#CR52)]。然而，对于具有更多参数的模型，语义知识问题往往较小。
- en: Richardson et al. [[52](#CR52)] formulated nine different types of simple sentence
    pairs containing e.g. negations, quantifiers, comparatives, etc. These sentences
    express logical entailment, contradiction or neutrality. In addition, they also
    employ chains of hypernomy, e.g. *poodle* ≤ *dog* ≤ *mammal* ≤ *animal*, and use
    these relations to generate new sentences expressing the corresponding logical
    properties. It turns out that BERT fine-tuned with the ‘logical tasks’ SNLI and
    MNLI predicts correct statements only with 47.3% accuracy of the cases.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Richardson等人[[52](#CR52)]提出了包含例如否定、量词、比较级等不同类型的简单句子对。这些句子表达逻辑蕴涵、矛盾或中立。此外，他们还使用了超名词链，例如*poodle*
    ≤ *dog* ≤ *mammal* ≤ *animal*，并利用这些关系生成表达相应逻辑属性的新句子。结果发现，使用“逻辑任务”SNLI和MNLI微调的BERT在47.3%的情况下预测正确陈述。
- en: Ribeiro et al. [[51](#CR51)] propose to generate a large number of simple examples
    to test relations by a *CheckList procedure* described in Sect. [4.3.1](#Sec15).
    It tests, for instance, whether negating a positive sentiment expression leads
    to a negative sentiment rating. For more than half of the tests with commercial
    and open-source models they observed failure rates of more than 50%.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Ribeiro等人[[51](#CR51)]提出通过Sect. [4.3.1](#Sec15)中描述的*CheckList程序*生成大量简单示例来测试关系。例如，它测试了否定一个积极情感表达是否会导致负面情感评分。在超过一半的测试中，他们观察到商业和开源模型的失败率超过50%。
- en: 'Even the larger model GPT-3 is not perfect, e.g. it incorrectly answers some
    common sense physics questions like *“If I put cheese into the fridge, will it
    melt?”* [[7](#CR7)]. In addition, it has difficulties with logical reasoning,
    e.g. to determine if one sentence implies another. If a question is not covered
    in its training material, GPT-3 compiles the most probable answer and sometimes
    this is wrong, e.g. *“Q: How many eyes does the sun have?”**“A: The sun has one
    eye.”* or *“Q: Who was president of the United States in 1600?”**“A: Queen Elizabeth
    I was president of the United States in 1600.”* [[29](#CR29)]. As another example
    consider the following input *“You poured yourself a glass of cranberry, but then
    absentmindedly, you poured about a teaspoon of grape juice into it. It looks OK.
    You try sniffing it, but you have a bad cold, so you can’t smell anything. You
    are very thirsty. So you …”*. The continuation generated by GPT-3 is *“drink it.
    You are now dead.”*. GPT-3 assumes wrongly that *“grape juice”* is a poison and
    drinking it will kill you [[36](#CR36)].'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '即使是更大的模型GPT-3也不是完美的，例如，它错误地回答了一些常识物理问题，例如*“如果我把奶酪放进冰箱，它会融化吗？”* [[7](#CR7)]。此外，它在逻辑推理上也有困难，例如确定一个句子是否意味着另一个句子。如果一个问题没有包含在其训练材料中，GPT-3会编译最可能的答案，有时这是错误的，例如*“Q:
    太阳有多少只眼睛？”**“A: 太阳有一只眼睛。”* 或 *“Q: 1600年美国总统是谁？”**“A: 伊丽莎白一世是1600年美国总统。”* [[29](#CR29)]。作为另一个例子，考虑以下输入*“你给自己倒了一杯蔓越莓汁，但后来无意中倒了一茶匙葡萄汁进去。看起来还可以。你试着闻一闻，但你感冒了，所以你闻不到任何东西。你非常渴。所以你……”*
    GPT-3生成的后续句子是*“喝它。你现在死了。”*。GPT-3错误地假设*“葡萄汁”*是有毒的，喝它会杀死你[[36](#CR36)]。'
- en: Improving Logical Consistency
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提高逻辑一致性
- en: PLMs can improve logical reasoning capabilities if they are trained with appropriately
    generated textual expressions. By fine-tuning a BERT model with created sentences
    containing negations, hypernomy, etc., and testing with other generated sentences,
    Richardson et al. [[52](#CR52)] achieve an accuracy of 98%. This approach is similar
    to the data generation strategy proposed in Sect. [3.​6.​6](528393_1_En_3_Chapter.xhtml#Sec46).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: PLMs可以通过使用适当生成的文本表达式进行训练来提高逻辑推理能力。通过使用包含否定、超名等内容的句子微调BERT模型，并用其他生成的句子进行测试，Richardson等人[[52](#CR52)]实现了98%的准确率。这种方法与第[3.6.6](528393_1_En_3_Chapter.xhtml#Sec46)节中提出的数据生成策略类似。
- en: Similarly, Clark et al. [[10](#CR10)] generate datasets of the form (context,
    statement, answer), where context contains different logical facts and rules,
    statement is a logical question to prove and answer is either T or F. Facts, rules,
    and the question statements are then expressed in (synthetic) English. The problems
    require simultaneous consideration of a number of different statements to reach
    a conclusion, from depth 0 (simple lookup) to depth 5\. During fine-tuning on
    this data, RoBERTa was trained to answer these questions as true or false. On
    the test data RoBERTa is able to answer the questions with 99% accuracy. If the
    facts and rules are paraphrased the accuracy drops to 66%. However, by training
    on paraphrased rules the model again reaches an accuracy beyond 90%. Clark et
    al. [[10](#CR10)] suggest that by this approach the transformer can be considered
    as a “soft theorem prover” able to work with statements in language.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，Clark等人[[10](#CR10)]生成了形式为（上下文，陈述，答案）的数据集，其中上下文包含不同的逻辑事实和规则，陈述是一个需要证明的逻辑问题，答案是T或F。事实、规则和问题陈述随后用（合成）英语表达。这些问题需要同时考虑多个不同的陈述来得出结论，从深度0（简单查找）到深度5。在对此数据进行微调时，RoBERTa被训练来回答这些问题是真是假。在测试数据上，RoBERTa能够以99%的准确率回答这些问题。如果事实和规则被释义，准确率下降到66%。然而，通过在释义的规则上训练，模型再次达到了超过90%的准确率。Clark等人[[10](#CR10)]建议，通过这种方法，可以将transformer视为一个“软定理证明器”，能够处理语言中的陈述。
- en: It is possible to combine the implicit, pre-trained knowledge of an LM and explicit
    statements in natural language. Talmor et al. [[64](#CR64)] show that models trained
    with such datasets can perform inferences involving implicit world knowledge and
    taxonomic knowledge (e.g. the WordNet hierarchy) . In addition, inference patterns
    provided by examples are used by the model to solve logical problems.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将LM的隐式、预训练知识和自然语言中的显式陈述结合起来。Talmor等人[[64](#CR64)]表明，使用此类数据集训练的模型可以执行涉及隐式世界知识和分类知识（例如WordNet层次结构）的推理。此外，模型使用示例提供的推理模式来解决逻辑问题。
- en: There were a number of prior approaches to combine logical reasoning with neural
    networks. If a neural network provides probabilities for logical facts, then we
    can use a probabilistic reasoning system to enforce additional constraints. Examples
    are *DeepProblog* [[35](#CR35)] that incorporates Deep Learning by means of neural
    predicates, i.e. statements whose probability is determined by a neural network.
    An alternative is *probabilistic soft logic* (*PSL*) [[28](#CR28)], which allows
    first order probabilistic reasoning in relational domains. However, PLMs do not
    directly provide probabilities for facts. There have been approaches to translate
    natural language sentences to logical statements and apply logical reasoning.
    However, this “semantic parsing” [[24](#CR24)] was not very successful.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 之前有许多方法将逻辑推理与神经网络相结合。如果一个神经网络为逻辑事实提供概率，那么我们可以使用概率推理系统来强制执行额外的约束。例如，*DeepProblog*
    [[35](#CR35)]通过使用神经网络谓词（即其概率由神经网络确定的陈述）结合深度学习。另一种选择是*概率软逻辑* (*PSL*) [[28](#CR28)]，它允许在关系域中进行一阶概率推理。然而，PLMs并不直接提供事实的概率。已经有一些方法将自然语言句子翻译成逻辑陈述并应用逻辑推理。然而，这种“语义解析”
    [[24](#CR24)]并不非常成功。
- en: A number of researchers have developed methods for neural theorem proving. This
    work combines symbolic and neural methods to reason about results derived from
    language. Examples are e.g. Minervini et al. [[39](#CR39)], which jointly embed
    logical predicates and text in a shared space by using an end-to-end differentiable
    model, or Weber et al. [[70](#CR70)] which combine a Prolog prover with a language
    model to apply rule-based reasoning to natural language. The **DeepCTRL** approach
    [[57](#CR57)] integrates rules with Deep Learning. It has a rule encoder which
    allows to control the strengths of the rules at inference. It can be applied to
    domains like healthcare, physical models or accounting, where obeying rules is
    essential.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究人员已经开发了用于神经定理证明的方法。这项工作结合了符号和神经方法，用于从语言中推理结果。例如，Minervini等人[[39](#CR39)]通过使用端到端可微分的模型，将逻辑谓词和文本共同嵌入到共享空间中，或者Weber等人[[70](#CR70)]将Prolog证明器与语言模型结合，将基于规则的推理应用于自然语言。**DeepCTRL**方法[[57](#CR57)]将规则与深度学习相结合。它有一个规则编码器，允许在推理时控制规则的强度。它可以应用于需要遵守规则的重要领域，如医疗保健、物理模型或会计。
- en: A simple but effective way to improve logical consistency is to increase the
    number of model parameters creating a Foundation Model. A large fraction of the
    tasks in the BIG-bench benchmark [[1](#CR1), [60](#CR60)] is devoted to checking
    logical consistency, e.g. the benchmark groups with analogical reasoning and logical
    reasoning. *Gopher* (Sect. [3.​1.​2](528393_1_En_3_Chapter.xhtml#Sec3)) is a language
    model with 280B parameters. It was applied to about 150 benchmarks, among them
    19 logical reasoning tasks. In all but 4 benchmarks it could increase Sota indicating
    that larger PLMs have better reasoning capabilities. Nevertheless, the average
    accuracy was only about 50%. It was not yet evaluated whether the recent *Retro*
    (Sect. [6.​2.​3](528393_1_En_6_Chapter.xhtml#Sec15)) language model with retrieval
    of additional text documents is able to improve these results.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 提高逻辑一致性的简单而有效的方法是增加模型参数的数量，创建一个基础模型。在BIG-bench基准测试[[1](#CR1)，[60](#CR60)]中，大部分任务都致力于检查逻辑一致性，例如，基准测试组包括类比推理和逻辑推理。*Gopher*（第[3.1.2](528393_1_En_3_Chapter.xhtml#Sec3)节）是一个拥有280B参数的语言模型。它被应用于大约150个基准测试中，其中包含19个逻辑推理任务。在除了4个基准测试之外的所有基准测试中，它都能提高Sota，这表明更大的PLM具有更好的推理能力。然而，平均准确率仅为大约50%。尚未评估最近的*Retro*（第[6.2.3](528393_1_En_6_Chapter.xhtml#Sec15)节）语言模型，通过检索额外的文本文档是否能够改善这些结果。
- en: '*PaLM* (Sect. [3.​1.​2](528393_1_En_3_Chapter.xhtml#Sec3)) is an even larger
    language model with 540B parameters. On the SuperGLUE logical tasks CB, COPA,
    RTE, it can drastically increase the scores compared to BERT, e.g. for COPA from
    70.6 to 99.2 (Table [4.2](#Tab2)). It has been evaluated on hundreds of benchmarks
    including those used for Gopher. It uses a new prompt technique to pose logical
    questions, where examples are presented to the system together with *thought chains*
    partitioning a reasoning task into smaller problems (Sect. [3.​6.​4](528393_1_En_3_Chapter.xhtml#Sec42)).
    Two examples are shown in Fig. [2.​21](528393_1_En_2_Chapter.xhtml#Fig21). Note
    that *k*-shot reasoning only requires a single sequence of *k* thought chain prompts
    to be provided for the training examples. The model then generates a thought chain
    for each test example. This can be used for error analysis and explaining the
    model behavior.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*PaLM*（第[3.1.2](528393_1_En_3_Chapter.xhtml#Sec3)节）是一个拥有540B参数的更大型的语言模型。在SuperGLUE逻辑任务CB、COPA、RTE上，它可以将分数与BERT相比大幅提升，例如COPA从70.6提升到99.2（见表[4.2](#Tab2)）。它已在包括用于Gopher的数百个基准测试中进行了评估。它使用一种新的提示技术来提出逻辑问题，其中将推理任务分割成更小问题的*思维链*与示例一起呈现给系统（第[3.6.4](528393_1_En_3_Chapter.xhtml#Sec42)节）。图[2.21](528393_1_En_2_Chapter.xhtml#Fig21)展示了两个示例。请注意，*k*次推理只需要为训练示例提供单个包含*k*个思维链提示的序列。然后模型为每个测试示例生成一个思维链。这可以用于错误分析和解释模型行为。'
- en: Using this technique, PaLM is able to match or surpass the performance level
    of an average human asked to solve the task. As an example consider the *StrategyQA
    benchmark* [[16](#CR16)], which contains questions like *“Did Aristotle use a
    laptop?”*. For this question the model has to collect facts on the lifespan of
    Aristotle and the year, when the first laptop was invented to arrive at the answer
    *“No”*. Without thought chain prompts PaLM reached 69%, while the use of thought
    chain prompts could improve the prior Sota from 70% to 73.9%. As a comparison,
    average humans achieve 62.9%, while expert humans have an accuracy of 90%.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种技术，PaLM能够匹配或超过被要求解决任务的普通人类的平均表现水平。以*StrategyQA基准测试[[16](#CR16)]*为例，其中包含像*“亚里士多德使用过笔记本电脑吗？”*这样的问题。对于这个问题，模型必须收集关于亚里士多德寿命和第一台笔记本电脑发明年份的事实，以得出答案*“没有”*。没有思维链提示时，PaLM达到了69%，而使用思维链提示可以将先前Sota从70%提高到73.9%。相比之下，普通人类的准确率为62.9%，而专家人类的准确率为90%。
- en: There are other ways to improve learning with such intermediate outputs. Wang
    et al. [[69](#CR69)] sample multiple chains of thought exploiting the diversity
    of reasoning paths and then return the most consistent final answer in the set.
    Since it is expensive to obtain chains-of-thought for a large number of examples,
    Zelikman et al. [[71](#CR71)] generate explanations for a large dataset by bootstrapping
    a model in the few-shot setting and only retaining chains-of-thought that lead
    to correct answers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有其他方法可以通过这样的中间输出提高学习效果。Wang等人[[69](#CR69)]通过利用推理路径的多样性，采样多个思维链，然后在集合中返回最一致的最终答案。由于为大量示例获取思维链的成本很高，Zelikman等人[[71](#CR71)]通过在少样本设置中启动模型并仅保留导致正确答案的思维链，为大量数据集生成解释。
- en: 4.2.4 Summary
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.4 摘要
- en: Pre-trained PLMs have a huge number of parameters and are able to represent
    an enormous amount of syntactic and factual knowledge. This knowledge can be elicited
    by probing classifiers, the prediction of masked words, by generating answers
    to inputs, or by solving benchmark tasks.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的PLM拥有庞大的参数数量，能够表示大量的语法和事实知识。这些知识可以通过探查分类器、预测掩码词、生成输入答案或解决基准任务来提取。
- en: As far as syntactic knowledge is concerned, Foundation Models like GPT-3 produce
    almost error-free text and ‘know’ a lot about syntactic rules. One problem is
    to adequately reflect the effect of negations.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 就语法知识而言，像GPT-3这样的基础模型能够产生几乎无错误的文本，并且“知道”很多关于语法规则的知识。一个问题是如何充分反映否定的影响。
- en: Even smaller models like BERT are capable of producing a lot of commonsense
    knowledge. Here, the effect of substituting names or using paraphrases is problematic.
    Larger language models like GPT-3 are more robust, and the recently proposed language
    models with retrieval (WebGPT, Retro) are able to include relevant external documents
    for the current task. This information can reduce errors considerably. However,
    there is no comprehensive evaluation yet. One problem is the correct temporal
    and spatial location of information. Here, smaller models like BERT and T5 have
    large deficits. Foundation Models meanwhile surpass the average human score in
    2/3 of the BIG-bench tests on common sense knowledge. They can even be used as
    a multilingual knowledge base, since models like PaLM cover many languages.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是像BERT这样的小型模型也能产生大量的常识知识。在这里，替换名称或使用释义的效果是有问题的。更大的语言模型如GPT-3更稳健，最近提出的具有检索功能的语言模型（WebGPT、Retro）能够为当前任务包含相关的外部文档。这些信息可以显著减少错误。然而，目前还没有全面的评估。一个问题是要正确地确定信息的时空位置。在这里，像BERT和T5这样的小型模型在这方面存在很大的不足。与此同时，基础模型在BIG-bench测试的常识知识方面有2/3的测试超过了平均人类得分。它们甚至可以用作多语言知识库，因为像PaLM这样的模型覆盖了许多语言。
- en: Logical consistency of inferences is a problem, and the PLMs often associate
    answers that are plausible but wrong. The models are only able to make logical
    inferences for relationships mentioned in the training text, and they are often
    incapable of making abstractions and generalizing an observed relationship to
    other objects or entities of the same type. Logical consistency can be improved
    by generating additional training texts containing assumptions and valid logical
    consequences resulting from them. The direct inclusion of logical reasoning systems
    in Foundation Models was not very successful. The PaLM language model with 540B
    parameters achieved a fundamental improvement of the accuracy of logical reasoning
    through the use of thought chain prompts. Here in a few-shot prompt a logical
    derivation is broken down into smaller logical substeps . At present, it is not
    clear, to what extent language models with retrieval can reduce the still existing
    deficits in logical reasoning.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 推理的逻辑一致性是一个问题，PLM通常关联那些看似合理但实际上错误的答案。模型只能对训练文本中提到的关系进行逻辑推理，并且它们通常无法进行抽象，无法将观察到的关系推广到其他相同类型的对象或实体。通过生成包含假设及其有效逻辑后果的额外训练文本，可以提高逻辑一致性。直接将逻辑推理系统纳入基础模型并没有取得很大的成功。具有540B参数的PaLM语言模型通过使用思维链提示实现了对逻辑推理准确性的根本改进。在这里，在少量提示中，逻辑推导被分解成更小的逻辑子步骤。目前，尚不清楚具有检索功能的语言模型在多大程度上可以减少逻辑推理中仍然存在的缺陷。
- en: 4.3 Transferability and Reproducibility of Benchmarks
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 基准的可迁移性和可重复性
- en: In this section, we consider whether benchmarks actually evaluate the properties
    they are supposed to test. We also discuss the extent to which they are reproducible.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们考虑基准是否真正评估了它们应该测试的性质。我们还讨论了它们的可重复性程度。
- en: 4.3.1 Transferability of Benchmark Results
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 基准结果的可迁移性
- en: On a number of benchmarks, the performance of human annotators is exceeded by
    Foundation Models. This is an indication that the model has learned valuable contents
    about language. However, Ribeiro et al. [[51](#CR51)] argue that this can be misleading,
    because the test sets often do not cover the right content. While performance
    on held-out test data is a useful measure, these datasets are often not comprehensive.
    Hence, there is the danger of overestimating the usability of the model in real
    applications.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多基准测试中，人类标注者的性能被基础模型超越。这是模型已经学会了关于语言的有价值内容的指示。然而，Ribeiro等人 [[51](#CR51)] 认为，这可能具有误导性，因为测试集通常不涵盖正确的内容。虽然保留测试数据上的性能是一个有用的衡量标准，但这些数据集通常并不全面。因此，存在高估模型在实际应用中可用性的风险。
- en: Benchmarks May Not Test All Aspects
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准可能无法测试所有方面
- en: On the MRPC task of the GLUE benchmark for detecting paraphrases RoBERTa, BERT[LARGE],
    and humans have F1 scores of 90.9% [[34](#CR34)], 89.3% [[42](#CR42)] and 86.3%
    respectively. Therefore, both models perform better than humans. To test whether
    the models respect basic logical relationships, Ribeiro et al. [[51](#CR51)] propose
    to generate a large number of simple examples using a **CheckList procedure**.
    This approach is similar to testing software by systematically generating a large
    variety of inputs in unit tests.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在GLUE基准测试中，用于检测释义的MRPC任务中，RoBERTa、BERT[LARGE]和人类分别取得了90.9% [[34](#CR34)]、89.3%
    [[42](#CR42)]和86.3%的F1分数。因此，这两个模型的表现都优于人类。为了测试模型是否尊重基本的逻辑关系，Ribeiro等人 [[51](#CR51)]
    提出使用**CheckList程序**生成大量简单示例。这种方法类似于通过在单元测试中系统地生成大量不同输入来测试软件。
- en: The following scheme, for instance, can be used to check the effect of a negation
    in a sentiment classification task *“I* <* negation*>  <* positive_verb*>*  the*
    <* thing*>* ”*. It generates sentences like *“I didn’t love the food”* or *“I
    don’t enjoy sailing”*. The authors formulate *minimum functionality tests*, which
    are useful to check if the model actually detected the reason of an outcome or
    used some unjustified association. In addition, they utilize *invariance tests*
    to find out, if neutral perturbations or paraphrases change the result. Finally,
    they create *directional expectation tests*, where a modification is known to
    change the result in an expected way.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下方案可以用来检查情感分类任务中否定的影响*“我* <*否定*> *<积极动词*> *<事物*>”。它生成像*“我不喜欢这食物”*或*“我不喜欢航海”*这样的句子。作者提出了*最小功能测试*，这有助于检查模型是否实际上检测到了结果的原因或使用了某些未经证实的关联。此外，他们利用*不变性测试*来找出，中性的扰动或释义是否会改变结果。最后，他们创建了*方向预期测试*，其中修改已知会以预期的方式改变结果。
- en: For MPRC it turned out that the failure rates of RoBERTa and BERT on these 23
    test templates are larger than 50% for 11 and 14 of the templates respectively.
    Therefore, the “superhuman” performance of the two models should be taken with
    a grain of salt.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MPRC来说，RoBERTa和BERT在这23个测试模板上的失败率分别为11个和14个，超过了50%。因此，这两个模型的“超人类”性能应该持保留态度。
- en: 'The authors also tested five current PLMs: BERT[BASE], RoBERTa[BASE], Microsoft’s
    Text Analytics, Google Cloud’s Natural Language, and Amazon’s Comprehend. They
    report the results of 17 tests for sentiment classification, where most problems
    occurred with negations. For instance, the following example *“I thought the plane
    would be awful, but it wasn’t.”* was misclassified by most models. Also very difficult
    is the detection of paraphrases with 23 tests templates. Here RoBERTa had for
    11 and BERT for 14 of the test templates a failure rate of more than 50%. A similar
    failure rate was observed for reading comprehension when test cases were generated
    with logical templates. These results indicate that the examples in the original
    test sets of the benchmarks are too easy.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还测试了五种当前的PLM：BERT[BASE]、RoBERTa[BASE]、微软的文本分析、谷歌云的自然语言和亚马逊的Comprehend。他们报告了17次情感分类测试的结果，其中大多数问题出现在否定上。例如，以下例子*“我以为飞机会很糟糕，但事实并非如此。”*被大多数模型错误分类。检测23个测试模板中的释义也非常困难。在这里，RoBERTa在11个测试模板上，BERT在14个测试模板上的失败率超过50%。当使用逻辑模板生成测试案例时，阅读理解的失败率也相似。这些结果表明，基准测试原始测试集中的例子太简单了。
- en: To increase robustness of PLMs it is possible to generate adversarial examples
    [[8](#CR8), [65](#CR65)]. The authors discuss methods that augment training data
    with adversarial examples as well as methods that produce certificates of robustness.
    They also investigate methods to avoid spurious correlations, i.e. predictive
    patterns that work well on a specific dataset but do not hold in general.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高PLM的鲁棒性，可以生成对抗性示例[[8](#CR8), [65](#CR65)]。作者讨论了通过添加对抗性示例来增强训练数据的方法，以及产生鲁棒性证书的方法。他们还研究了避免虚假相关性的方法，即那些在特定数据集上表现良好但通常不成立的可预测模式。
- en: Talman et al. [[63](#CR63)] checked, if the results for benchmarks may be transferred
    to similar datasets. They trained six PLMs on different benchmarks for *natural
    language inference* (*NLI*) containing sentence pairs manually labeled with the
    labels entailment, contradiction, and neutral. While six models perform well when
    the test set matches the training set, accuracy is significantly lower when a
    test set from another benchmark is used. BERT[BASE], for instance, yields a test
    accuracy of 90.4% for SNLI, which drops on average 21.2% for the test sets of
    the other benchmarks. The reason behind this drop is a slightly different definition
    of the task as well as small differences in the documents domains. Obviously,
    it cannot be expected that the performance of PLMs can simply be transferred to
    new data.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Talman等人[[63](#CR63)]检查了，对于基准测试的结果是否可以转移到类似的数据库中。他们在包含手动标注为蕴涵、矛盾和中立的句子对的六个不同基准测试上训练了六个PLM，用于*自然语言推理*（NLI）。当测试集与训练集匹配时，六个模型表现良好，但当使用来自另一个基准测试的测试集时，准确率显著降低。例如，BERT[BASE]在SNLI上的测试准确率为90.4%，在其他基准测试的测试集上平均下降了21.2%。这种下降的原因是任务定义的略微不同以及文档领域的小差异。显然，不能期望PLM的性能可以简单地转移到新的数据上。
- en: Logical Reasoning by Correlation
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过相关性进行逻辑推理
- en: The *Winograd schema challenge* (WNLI) was developed by Levesque et al. [[32](#CR32)]
    and is part of the GLUE benchmark collection. The test consists of a pair of sentences
    differing by exactly one word, each followed by a question [[41](#CR41)], e.g.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*Winograd方案挑战*（WNLI）是由Levesque等人 [[32](#CR32)] 开发的，它是GLUE基准集合的一部分。测试由一对恰好只有一个词差异的句子组成，每个句子后面跟着一个问题
    [[41](#CR41)]，例如：'
- en: 'The sports car passed the mail truck because it was going faster. Question:
    Which was going faster, the sports car or the mail truck?'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跑车超过了邮件卡车，因为它开得更快。问题：哪辆车开得更快，跑车还是邮件卡车？
- en: 'The sports car passed the mail truck because it was going slower. Question:
    Which was going slower, the sports car or the mail truck?'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跑车超过了邮件卡车，因为它开得更慢。问题：哪辆车开得更慢，跑车还是邮件卡车？
- en: 'In this pair of sentences, the difference of one word changes which thing or
    person a pronoun refers to. Answering these questions correctly seems to require
    common sense reasoning and world knowledge. In addition, the authors have designed
    the questions to be “Google-proof”: The system should not be able to use a web
    search (or anything similar) to answer the questions. GPT-3 reaches a value of
    88.6% using few-shot prompts without fine-tuning [[7](#CR7)] and DeBERTa managed
    an accuracy of 95.6% after fine-tuning [[19](#CR19)]. This accuracy roughly equals
    human performance.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两句话中，一个词的差异改变了代词所指的事物或人物。正确回答这些问题似乎需要常识推理和世界知识。此外，作者们设计了这些问题以使其“无法通过谷歌搜索”：系统不应该能够使用网络搜索（或类似的东西）来回答这些问题。GPT-3在未经过微调的情况下，使用少量提示达到了88.6%的准确率
    [[7](#CR7)]，而DeBERTa在微调后达到了95.6%的准确率 [[19](#CR19)]。这个准确率大致等于人类的表现。
- en: As Mitchell [[41](#CR41)] argues, this does not necessarily mean that neural
    network language models have attained human-like understanding. For a number of
    question pairs it seems possible to answer the question by some sort of correlation
    instead of actual world knowledge. If pre-trained on a large corpus the model
    will learn the high correlation between *“sports car”* and *“fast”* and between
    *“mail truck”* and *“slow”* for the above example. Therefore, it can give the
    correct answer on the coreference of *“it”* based on those correlations alone
    and not by recourse to any understanding. It turns out that many of the Winograd
    schema challenge question follow this pattern. A similar argument states [[6](#CR6),
    [37](#CR37)] that a model might heuristically accept a hypothesis by assuming
    that the premise entails any hypothesis whose words all appear in the premise.
    This means that the model can give the right answer without ‘understanding’ the
    situation in question.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如Mitchell [[41](#CR41)] 所言，这并不一定意味着神经网络语言模型已经达到了类似人类的理解。对于许多问题对，似乎可以通过某种相关性而不是实际的世界知识来回答问题。如果在一个大型语料库上预训练，模型将学会“跑车”和“快”以及“邮件卡车”和“慢”之间的高度相关性。因此，它可以根据这些相关性单独给出正确的答案，而不需要任何理解。结果发现，许多Winograd方案挑战问题都遵循这种模式。一个类似的论点
    [[6](#CR6), [37](#CR37)] 表示，一个模型可能会通过假设前提蕴涵任何其所有单词都出现在前提中的假设来启发式地接受一个假设。这意味着模型可以在不“理解”所讨论的情况的情况下给出正确的答案。
- en: 'To reduce the deficits of the Winograd schema challenge a much larger *Winogrande*
    benchmark [[55](#CR55)] was created using crowdsourcing. The researchers discarded
    sentences which could be answered by exploiting intuition and correlation. They
    used the embeddings created by RoBERTa (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2))
    to determine if these embeddings strongly indicated the correct response option.
    In this case they discarded the question pair and finally ended up with 44k sentences.
    An example for a question pair without correlation problems is:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少Winograd方案挑战的不足，一个更大的*Winogrande*基准 [[55](#CR55)] 通过众包创建。研究人员丢弃了可以通过利用直觉和相关性来回答的句子。他们使用RoBERTa（第[3.1.1](528393_1_En_3_Chapter.xhtml#Sec2)节）创建的嵌入来确定这些嵌入是否强烈表明正确的响应选项。在这种情况下，他们丢弃了问题对，最终得到了44k个句子。一个没有相关性问题的问答对示例是：
- en: 'The trophy doesn’t fit into the brown suitcase because it’s too large. (it:
    trophy)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖杯放不进棕色手提箱，因为它太大。（it：奖杯）
- en: 'The trophy doesn’t fit into the brown suitcase because it’s too small. (it:
    suitcase)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖杯放不进棕色手提箱，因为它太小了。（it：手提箱）
- en: While humans reach an accuracy of 94%, the best PLMs, standard models like RoBERTa
    only reached 79.1% accuracy. Recently, *T5-XXL* achieved an accuracy of about
    91% [[43](#CR43)] and the *ST-MoE-32B* mixture-of-experts model [[73](#CR73)]
    with 269B parameters (Sect. [3.​5.​2](528393_1_En_3_Chapter.xhtml#Sec26)) obtained
    96.1%, drastically reducing the errors. It appears that in most cases the latter
    models are able to perform ‘reasoning’ without simply correlating statements.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当人类达到94%的准确率时，最好的PLM，如RoBERTa标准模型，仅达到79.1%的准确率。最近，*T5-XXL*达到了约91%的准确率 [[43](#CR43)]，而具有269B参数的*ST-MoE-32B*混合专家模型
    [[73](#CR73)]（第[3.5.2](528393_1_En_3_Chapter.xhtml#Sec26)节）达到了96.1%，大幅减少了错误。看起来在大多数情况下，后一种模型能够进行“推理”，而不仅仅是关联陈述。
- en: 4.3.2 Reproducibility of Published Results in Natural Language Processing
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 自然语言处理中已发布结果的复现性。
- en: Many publications in NLP claim that their model achieves Sota for some benchmark.
    Examples are the GLUE benchmark [[67](#CR67)] for language understanding and the
    SQuAD data [[50](#CR50)] for reading comprehension. There are two main problems
    with this approach. First it is difficult to assess, if the results are reproducible
    and significant. As Crane [[11](#CR11)] demonstrates, there are usually a number
    of unreported conditions that affect the reproducibility of the result. An example
    is the random initialization of the network parameters. The resulting variance
    is often larger than the reported improvement in Sota scores. However, the variance
    resulting from these phenomena is usually not reported. Other effects are the
    underlying programming frameworks and libraries, which change over time. Often
    the hyperparameters and the details of preprocessing and model configuration are
    not communicated.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 许多自然语言处理领域的出版物声称他们的模型在某些基准测试中达到了Sota。例如，用于语言理解的GLUE基准 [[67](#CR67)] 和用于阅读理解的SQuAD数据
    [[50](#CR50)]。这种方法的两个主要问题是：首先，很难评估结果是否可复现和有显著性。正如克雷恩 [[11](#CR11)] 所证明的，通常有许多未报告的条件会影响结果的可复现性。一个例子是网络参数的随机初始化。由此产生的方差通常大于Sota分数报告的改进。然而，这些现象产生的方差通常未报告。其他影响包括随时间变化的底层编程框架和库，以及通常未传达的超参数和预处理以及模型配置的细节。
- en: 'To document the model architecture, the training and evaluation process of
    a model, Mitchell et al. [[40](#CR40)] proposed the description of relevant facts
    and hyperparameters in a **model card**. After a short high-level description
    of the model and its purpose the model card should contain nine different sections
    [[40](#CR40)]:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了记录模型架构、模型的训练和评估过程，米切尔等人 [[40](#CR40)] 提出了在**模型卡片**中描述相关事实和超参数。在简要介绍模型及其目的之后，模型卡片应包含九个不同的部分
    [[40](#CR40)]：
- en: '1.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Basic information about the model,
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型的基本信息，
- en: '2.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Intended uses and scope limitations,
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期用途和范围限制，
- en: '3.'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Model performance across a variety of relevant factors,
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在各种相关因素下的性能，
- en: '4.'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Performance metrics,
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能指标，
- en: '5.'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Evaluation data,
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估数据，
- en: '6.'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Training data,
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练数据，
- en: '7.'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: Evaluation results according to the chosen metrics.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据所选指标得出的评估结果。
- en: '8.'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: Ethical consideration, risks and harms.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 伦理考量，风险和危害。
- en: '9.'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: Caveats and recommendations.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意事项和建议。
- en: More details are given by huggingface [[22](#CR22)]. Even if models still can
    be published without a model card, the explicit documentation of the model can
    only benefit future users. Therefore, model cards should be provided if possible.
    For most recent models, a model card is provided even if the model is not open-source.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 更多细节由huggingface [[22](#CR22)] 提供。即使模型可以在没有模型卡片的情况下发布，但模型的确切文档对未来的用户只有好处。因此，如果可能的话，应提供模型卡片。对于大多数最新模型，即使模型不是开源的，也会提供模型卡片。
- en: A survey on *reproducibility in NLP* is given by Belz et al. [[4](#CR4)]. They
    note that the performance results often depend on seemingly small differences
    in model parameters and settings, for example minimum counts for rare word or
    the normalization of writing. The authors state in their study on repeated experiments
    that only 14% of the 513 reported scores were the same. An annoying fraction of
    59% of the scores were worse than the published numbers. Therefore, the experimental
    results published in papers should be treated with caution.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔兹等人对自然语言处理中的**可复现性**进行了调查 [[4](#CR4)]。他们指出，性能结果往往取决于模型参数和设置中看似微小的差异，例如罕见词的最小计数或写作的归一化。作者在其关于重复实验的研究中指出，513个报告中只有14%的分数是相同的。令人烦恼的是，59%的分数比发表的数字更差。因此，应谨慎对待论文中发表的实验结果。
- en: Another issue is the question of what causes an increase in performance. As
    we have discussed above, a growth in the number of parameters and in the computing
    effort regularly leads to better results for PLMs (Sect. [3.​5.​1](528393_1_En_3_Chapter.xhtml#Sec25)).
    As a consequence, it is often not clear, whether the architectural changes to
    a model yield the improved performance or just the number of additional parameters
    or the larger training set [[53](#CR53)].
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是什么导致了性能的提升。正如我们上面所讨论的，参数数量和计算努力的增加通常会导致 PLM（Sect. [3.5.1](528393_1_En_3_Chapter.xhtml#Sec25)）的更好结果。因此，通常不清楚模型架构的改变是否导致了性能的提升，或者只是额外的参数数量或更大的训练集导致的
    [[53](#CR53)]。
- en: 'Obviously a first place in a leaderboard can be achieved with a larger model
    and more computing effort. This, however, “is not research news” according to
    Rogers [[53](#CR53)]. In addition, these results are often not reproducible: Who
    can afford to retrain GPT-3 for 4.6 million dollars. As a consequence, the development
    of smaller but more innovative models is less rewarding, as it is difficult to
    beat the bigger model. Only if the authors of a new model can show that their
    architecture is better than the previous Sota model with the same number of parameters
    and compute budget, they can claim to have made a valuable contribution. Rogers
    [[53](#CR53)] proposes to provide a standard training corpus for a leaderboard
    and limit the amount of computation effort to that of a strong baseline model.
    As an alternative the size of the training data and the computational effort should
    be reported and taken into account in the final score.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，通过更大的模型和更多的计算努力可以在排行榜上获得第一名。然而，根据 Rogers [[53](#CR53)] 的说法，这“并不是研究新闻”。此外，这些结果通常不可重复：谁能够承担起为
    GPT-3 重新训练花费 460 万美元的费用。因此，开发更小但更具创新性的模型回报较少，因为很难打败更大的模型。只有当新模型的作者能够证明他们的架构在相同数量的参数和计算预算下比之前的
    Sota 模型更好，他们才能声称做出了有价值的贡献。Rogers [[53](#CR53)] 建议为排行榜提供一个标准训练语料库，并限制计算努力量与强大基线模型相当。作为替代方案，应报告和考虑训练数据量和计算努力量在最终得分中。
- en: Available Implementations
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: There are model codes and trained models for RoBERTa and ELECTRA at Hugging
    Face [https://​huggingface.​co/​transformers/​](https://huggingface.co/transformers/).
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RoBERTa 和 ELECTRA 的模型代码和训练模型可在 Hugging Face [https://huggingface.co/transformers/](https://huggingface.co/transformers/)
    上找到。
- en: The code for DeBERTa is available at [https://​github.​com/​microsoft/​DeBERTa](https://github.com/microsoft/DeBERTa)
    and Hugging Face.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeBERTa 的代码可在 [https://github.com/microsoft/DeBERTa](https://github.com/microsoft/DeBERTa)
    和 Hugging Face 上找到。
- en: The Checklist code is at [https://​github.​com/​marcotcr/​checklist](https://github.com/marcotcr/checklist).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Checklist 代码位于 [https://github.com/marcotcr/checklist](https://github.com/marcotcr/checklist)。
- en: 4.3.3 Summary
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 摘要
- en: The transferability of benchmark results to real applications is not always
    granted. Even if a PLM is better than humans at logical reasoning on the test
    set, it may not be able to classify generated logical reasoning chains correctly.
    This indicates that the test set does not cover the full spectrum of possible
    examples. It is common for performance to be lower on related benchmarks because
    the domain or the definition of the task may deviate.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试结果的可迁移性到实际应用并不总是有保证。即使一个 PLM 在测试集上的逻辑推理优于人类，它可能也无法正确分类生成的逻辑推理链。这表明测试集并没有涵盖所有可能的例子。在相关基准测试中，性能通常较低，因为领域或任务的定义可能有所偏差。
- en: There are cases where a logical conclusion is obtained not by logical deduction,
    but by a simple correlation of antecedent and consequent. This could be demonstrated
    for the Winograd task of the GLUE benchmark. To avoid this type of ‘reasoning’
    a new variant task called Winogrande was developed where correlations are unrelated
    to the reasoning task. Meanwhile, a Foundation Model with 269B parameters was
    also able to solve this task better than humans.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，逻辑结论并非通过逻辑推理得出，而是通过简单的前因后果关联。这可以通过 GLUE 基准测试中的 Winograd 任务来证明。为了避免这种类型的“推理”，开发了一个新的变体任务，称为
    Winogrande，其中关联与推理任务无关。同时，一个具有 269B 参数的基础模型也能比人类更好地解决这个任务。
- en: A survey on the reproducibility of results in NLP demonstrated that the published
    performance often depends on a number of unreported effects, such as random number
    initialization. Often the variability of such effects is larger than the reported
    improvement. Therefore, it is necessary to report the variance caused by these
    effects. In addition, the details of the model architecture, its training and
    evaluation should be documented in a model card. In about 500 repeated experiments,
    an irritating rate of about 60% of final scores were lower than reported. Note
    that improvements due to more parameters, more training data, or higher computational
    effort are not indicative of a better model architecture.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一项关于自然语言处理（NLP）结果可重复性的调查表明，已发表的性能通常依赖于许多未报告的影响因素，例如随机数初始化。通常，这些影响因素的变异性大于报告的改进。因此，有必要报告这些因素引起的方差。此外，模型架构的细节、其训练和评估应记录在模型卡片中。在大约500次重复实验中，大约60%的最终得分低于报告值。请注意，由于更多参数、更多训练数据或更高的计算工作量所带来的改进并不能表明模型架构更好。
- en: '[![Creative Commons](../css/cc-by.png)](https://creativecommons.org/licenses/by/4.0)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Creative Commons](../css/cc-by.png)](https://creativecommons.org/licenses/by/4.0)'
- en: '**Open Access** This chapter is licensed under the terms of the Creative Commons
    Attribution 4.0 International License ([http://​creativecommons.​org/​licenses/​by/​4.​0/​](http://creativecommons.org/licenses/by/4.0/)),
    which permits use, sharing, adaptation, distribution and reproduction in any medium
    or format, as long as you give appropriate credit to the original author(s) and
    the source, provide a link to the Creative Commons license and indicate if changes
    were made.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**开放获取**本章根据Creative Commons Attribution 4.0国际许可协议（[http://creativecommons.org/licenses/by/4.0/](http://creativecommons.org/licenses/by/4.0/)）许可，允许在任何媒介或格式中使用、分享、改编、分发和复制，只要您适当引用原始作者和来源，提供Creative
    Commons许可的链接，并指出是否进行了更改。'
- en: The images or other third party material in this chapter are included in the
    chapter's Creative Commons license, unless indicated otherwise in a credit line
    to the material. If material is not included in the chapter's Creative Commons
    license and your intended use is not permitted by statutory regulation or exceeds
    the permitted use, you will need to obtain permission directly from the copyright
    holder.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的图像或其他第三方材料包含在本章的Creative Commons许可范围内，除非在材料引用行中另有说明。如果材料未包含在本章的Creative
    Commons许可范围内，且您的使用未得到法定规定的许可或超出了许可的使用范围，您将需要直接从版权持有人处获得许可。
