- en: '© The Author(s) 2023G. Paaß, S. GiesselbachFoundation Models for Natural Language
    ProcessingArtificial Intelligence: Foundations, Theory, and Algorithms[https://doi.org/10.1007/978-3-031-23190-2_6](https://doi.org/10.1007/978-3-031-23190-2_6)'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: © 作者（2023）G. Paaß, S. Giesselbach自然语言处理基础模型人工智能：基础、理论和算法[https://doi.org/10.1007/978-3-031-23190-2_6](https://doi.org/10.1007/978-3-031-23190-2_6)
- en: 6. Foundation Models for Text Generation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6. 文本生成的基础模型
- en: Gerhard Paaß^([1](#Aff5)  ) and Sven Giesselbach^([1](#Aff5))(1)Knowledge Discovery
    Department, Team NLU, Fraunhofer Institute for Intelligent Analysis and Information
    Systems (IAIS), Sankt Augustin, Nordrhein-Westfalen, Germany
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Gerhard Paaß^([1](#Aff5)  ) 和 Sven Giesselbach^([1](#Aff5))(1)知识发现部门，NLU 团队，弗劳恩霍夫智能分析与信息系统研究所（IAIS），圣奥古斯丁，北莱茵-威斯特法伦，德国
- en: Abstract
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter discusses Foundation Models for Text Generation. This includes
    systems for Document Retrieval, which accept a query and return an ordered list
    of text documents from a document collection, often evaluating the similarity
    of embeddings to retrieve relevant text passages. Question Answering systems are
    given a natural language question and must provide an answer, usually in natural
    language. Machine Translation models take a text in one language and translate
    it into another language. Text Summarization systems receive a long document and
    generate a short summary covering the most important contents of the document.
    Text Generation models use an autoregressive Language Model to generate a longer
    story, usually starting from an initial text input. Dialog systems have the task
    of conducting a dialog with a human partner, typically not limited to a specific
    topic.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了文本生成的基础模型。这包括文档检索系统，它接受一个查询并返回文档集合中按顺序排列的文本文档列表，通常通过评估嵌入的相似性来检索相关文本段落。问答系统被给出一个自然语言问题，必须提供答案，通常以自然语言形式。机器翻译模型将一种语言的文本翻译成另一种语言。文本摘要系统接收一个长文档并生成一个简短的摘要，涵盖文档的最重要内容。文本生成模型使用自回归语言模型生成较长的故事，通常从初始文本输入开始。对话系统有与人类伙伴进行对话的任务，通常不限于特定主题。
- en: KeywordsQuestion answeringMachine translationText summarizationText generationDialog
    systemsDocument retrievalIn this chapter we describe Foundation Models, i.e. large
    Pre-trained Language Models for generating new text in different application areas.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词问答机器翻译文本摘要文本生成对话系统文档检索在本章中，我们描述了基础模型，即用于在不同应用领域中生成新文本的大型预训练语言模型。
- en: '*Document Retrieval* systems accept a query and return an ordered list of text
    documents from a document collection, often evaluating the similarity of embeddings
    to retrieve relevant text passages (Sect. [6.1](#Sec1)).'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文档检索* 系统接受一个查询并返回文档集合中按顺序排列的文本文档列表，通常通过评估嵌入的相似性来检索相关文本段落（第 [6.1](#Sec1) 节）。'
- en: '*Question Answering* systems are given a natural language question and must
    provide an answer, usually in natural language (Sect. [6.2](#Sec9)).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*问答* 系统被给出一个自然语言问题，必须提供答案，通常以自然语言形式（第 [6.2](#Sec9) 节）。'
- en: '*Machine Translation* takes a text in one language and generates a translation
    into another language (Sect. [6.3](#Sec19)).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器翻译* 将一种语言的文本翻译成另一种语言（第 [6.3](#Sec19) 节）。'
- en: '*Text Summarization* receives a long document and has to write a short summary
    covering the most important contents of the document (Sect. [6.4](#Sec25)).'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文本摘要* 接收一个长文档并必须编写一个简短的摘要，涵盖文档的最重要内容（第 [6.4](#Sec25) 节）。'
- en: '*Text Generation* uses an autoregressive Language Model to generate a longer
    story, usually starting from an initial text input (Sect. [6.5](#Sec31)).'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文本生成* 使用自回归语言模型生成较长的故事，通常从初始文本输入开始（第 [6.5](#Sec31) 节）。'
- en: '*Dialog systems* have the task of conducting a dialog with a human partner,
    typically not limited to a specific topic (Sect. [6.6](#Sec49)).'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对话系统* 有与人类伙伴进行对话的任务，通常不限于特定主题（第 [6.6](#Sec49) 节）。'
- en: Due to the large number of different approaches, we focus on representative
    models which exhibit a high performance at the time of writing. We review the
    current best techniques for each area, measured against appropriate benchmarks
    and taking into account the computational resources required. For standard models
    a link to the description in earlier chapters is provided. Examples for each application
    area are shown in Table [6.1](#Tab1).Table 6.1
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在大量不同的方法，我们专注于在写作时表现出高性能的代表性模型。我们根据适当的基准和所需的计算资源，回顾了每个领域的当前最佳技术。对于标准模型，提供了早期章节中描述的链接。每个应用领域的示例在表[6.1](#Tab1)中展示。表6.1
- en: Language generation tasks illustrated by an example
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过示例展示的语言生成任务
- en: '| Task | Description | Example |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 描述 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Document retrieval | For a query return an ordered list of text documents
    | *Covid 19?*![$$\rightarrow $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq1.png)[http://​doi.​org/​](http://doi.org/)wikipedia/covid-19,
    [www.​cdc.​gov/​](http://www.cdc.gov/), … |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 文档检索 | 对于一个查询返回一个文本文档的有序列表 | *Covid 19?*![→](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq1.png)[http://​doi.​org/​](http://doi.org/)wikipedia/covid-19,
    [www.​cdc.​gov/​](http://www.cdc.gov/), … |'
- en: '| Generative question answering | Generate the answer to a question, often
    using some background knowledge | *What did Albert Einstein invent?* |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 生成式问答 | 生成问题的答案，通常使用一些背景知识 | *阿尔伯特·爱因斯坦发明了什么？* |'
- en: '|   |   | ![$$\rightarrow $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq2.png)*Einstein
    developed the theory of relativity* |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|   |   | ![→](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq2.png)*爱因斯坦发展了相对论*
    |'
- en: '| Translation | For a text in the source language generate a text in the target
    language with the same meaning | *Fritz isst gerne Schinken* |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 翻译 | 对于源语言中的文本生成目标语言中的文本，具有相同的意思 | *弗里茨喜欢吃香肠* |'
- en: '|   |   | ![$$\rightarrow $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq3.png)*Fritz
    likes to eat ham* |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|   |   | ![→](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq3.png)*弗里茨喜欢吃火腿*
    |'
- en: '| Summarization | For a long text generate a concise summary | *It was the
    middle of winter, …* |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 摘要 | 对于长文本生成一个简洁的摘要 | *那是冬天的中间，…* |'
- en: '|   |   | ![$$\rightarrow $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq4.png)*Snow
    White is awoken by the prince, whom she marries …* |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|   |   | ![→](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq4.png)*白雪公主被王子唤醒，并与他结婚…*
    |'
- en: '| Text generation | Starting from an initial text, a consistent continuation
    text is created | *Beethoven was born in Bonn* |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 文本生成 | 从初始文本开始，创建一个一致的后续文本 | *贝多芬出生于波恩* |'
- en: '|   |   | ![$$\rightarrow $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq5.png)*His
    father was a singer at the Duke’s court* … |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|   |   | ![→](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq5.png)*他的父亲是公爵法庭上的歌手*
    … |'
- en: '| Dialog answer generation | Generate a consistent response in a dialogue based
    on the sequence of previous utterances | *Could you recommend a video for tonight?*
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 对话回答生成 | 基于先前话语的序列生成一致的回答 | *你能推荐今晚看哪个视频吗？* |'
- en: '|   |   | ![$$\rightarrow $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq6.png)*There
    is “Memento” on Netflix* |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|   |   | ![→](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq6.png)*Netflix上有“记忆碎片”*
    |'
- en: 6.1 Document Retrieval
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 文档检索
- en: '*Information retrieval* (*IR*) uses computer systems to search databases for
    content. The resulting IR system is often called a *search engine*. Often, the
    user formulates a sentence or a *query* about to some topic, and the system is
    expected to return a sorted list of documents relevant to the query (*ad hoc retrieval*).
    Here we focus on retrieving textual information from a stored collection of documents.
    In contrast to question answering approaches in Sect. [6.2](#Sec9), the system
    does not generate a direct answer to the query in natural language.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*信息检索* (*IR*) 使用计算机系统在数据库中搜索内容。产生的IR系统通常被称为 *搜索引擎*。通常，用户会就某个主题提出一个句子或 *查询*，系统应返回与查询相关的文档的排序列表（*即席检索*）。在这里，我们专注于从存储的文档集合中检索文本信息。与第[6.2](#Sec9)节中问答方法不同，系统不会直接以自然语言生成对查询的直接答案。'
- en: 'Former IR systems were *keyword-based*: all words contained in a document were
    stored in an *inverted index*. The retrieval algorithm searched the index to identify
    documents that contained the query words. Then, these documents were ranked according
    to the information content of each query word found in a document, e.g. measured
    by tf-idf or BM25 [[186](#CR186)]. These two steps are shown in Fig. [6.1](#Fig1).
    A survey of earlier retrieval techniques is given by Abbasiyantaeb and Momtazi
    [[2](#CR2)]. However, this approach had three major problems:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的IR系统是*基于关键词的*：文档中包含的所有单词都存储在*倒排索引*中。检索算法搜索索引以识别包含查询单词的文档。然后，根据文档中找到的每个查询单词的信息内容对这些文档进行排名，例如通过tf-idf或BM25
    [[186](#CR186)]来衡量。这两个步骤在图[6.1](#Fig1)中显示。Abbasiyantaeb和Momtazi [[2](#CR2)]提供了一项早期检索技术的调查。然而，这种方法有三个主要问题：
- en: Many objects, activities, or events may be expressed by different words called
    *synonyms*, e.g. *“drink”* and *“beverage”* or *“buy”* and *“purchase”*. The documents
    containing alternative words are not returned by keyword retrieval. *Paraphrases*
    like *“he has tons of stuff to throw away”* and *“he needs to get rid of a lot
    of junk”* are even harder to spot and were ignored. This is called the *vocabulary
    mismatch problem*.![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig1_HTML.png)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多物体、活动或事件可能由不同的单词表达，这些单词被称为*同义词*，例如“drink”和“beverage”或“buy”和“purchase”。包含替代单词的文档不会被关键词检索返回。*释义*如“他有很多东西要扔掉”和“他需要摆脱很多垃圾”甚至更难发现，并且被忽略。这被称为*词汇不匹配问题*。![图片](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig1_HTML.png)
- en: A flow diagram represents the sequence of processes starting from the text database
    and going through the inverted keyword index, initial keyword retrieval, candidate
    texts, and P L M tracker, before generating the ranked list.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 流程图表示从文本数据库开始，经过倒排关键词索引、初始关键词检索、候选文本和PLM跟踪器，最后生成排名列表的过程。
- en: Fig. 6.1
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.1
- en: Retrieve-and-rerank architecture using PLMs. First, texts are retrieved from
    the document collection, usually with exact-match bag-of-words queries. These
    candidates are then reranked using PLM embeddings, e.g. from BERT. Image adapted
    from [[123](#CR123)], reprinted with kind permission of authors
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用PLM的检索和重新排名架构。首先，从文档集合中检索文本，通常使用精确匹配的词袋查询。然后，使用PLM嵌入（例如BERT）对这些候选进行重新排名。图像改编自[[123](#CR123)]，经作者同意重印
- en: 'Many words have different meanings depending on the context (e.g. “rock”: music
    or stone). These words are called *homonyms*. Part of the retrieved documents
    containing such a word will be mismatches.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多单词的含义取决于上下文（例如，“rock”：音乐或石头）。这些单词被称为*同音异义词*。包含此类单词的部分检索文档将是不匹配的。
- en: The order of words is often crucial for the meaning of the sentences (e.g. *“dog
    kills person”* vs. “person kills dog”). This is usually ignored with keyword search.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词的顺序对于句子的意义往往至关重要（例如，“狗杀死了人”与“人杀死了狗”）。这在关键词搜索中通常被忽略。
- en: As an alternative, contextual embeddings were used to represent queries and
    documents. By identifying matching documents through comparison of contextual
    semantic representations, word meaning differences between documents and queries
    can be reduced and texts with synonyms, homonyms, and paraphrases can be retrieved.
    These models have achieved Sota results on various retrieval benchmarks [[137](#CR137)]
    and have recently been introduced in commercial search engines. They are therefore
    one of the most commercially important applications of PLMs to date.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种替代方案，上下文嵌入被用来表示查询和文档。通过比较上下文语义表示来识别匹配的文档，可以减少文档和查询之间的词义差异，并检索出同义词、同音异义词和释义文本。这些模型在各种检索基准测试中实现了Sota结果
    [[137](#CR137)]，并且最近被引入到商业搜索引擎中。因此，它们是目前PLM在商业上最重要的应用之一。
- en: 6.1.1 Dense Retrieval
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 稠密检索
- en: '*Dense retrieval* methods encode text as an embedding vector with a fixed length
    much smaller than the text length. Whether a document is relevant to a given query
    is determined by the similarity of embedding vectors, which is computed by cosine
    similarity or inner products. Unlike question answering (Sect. [6.2](#Sec9)),
    these models do not generate a direct natural language response to a search query,
    but return complete documents or text passages. Recently, dense retrieval methods
    based on PLMs outperformed their keyword counterparts when fine-tuned on a small
    set of in-domain relevance-labeled documents. Lin et al. [[124](#CR124)] provide
    a comprehensive overview of retrieval systems with PLMs. Different approaches
    for dense retrieval can be distinguished and are covered in the next sections:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*密集检索* 方法将文本编码为长度远小于文本长度的固定长度嵌入向量。一个文档是否与给定的查询相关由嵌入向量的相似度决定，该相似度通过余弦相似度或内积计算。与问答（第
    [6.2](#Sec9) 节）不同，这些模型不会对搜索查询生成直接的自然语言响应，而是返回完整的文档或文本段落。最近，基于 PLM 的密集检索方法在针对一小部分领域相关、带有相关性标签的文档微调时，优于它们的基于关键词的对应方法。Lin
    等人 [[124](#CR124)] 提供了关于基于 PLM 的检索系统的全面概述。密集检索的不同方法可以区分，并在下一节中进行介绍。'
- en: '**Cross-Encoder**: Use the concatenated query and a document as input to BERT
    and determine the relevance of the document for the query (Sect. [6.1.3](#Sec4)).'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉编码器**：使用连接的查询和文档作为输入到 BERT 中，并确定文档对查询的相关性（第 [6.1.3](#Sec4) 节）。'
- en: '**Retrieval with token embeddings**: The tokens of the query and the document
    are encoded by contextual embeddings. Then different metrics are used to compare
    these embeddings and to collect relevant documents (Sect. [6.1.4](#Sec5)).'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于标记嵌入的检索**：查询和文档的标记通过上下文嵌入进行编码。然后使用不同的指标来比较这些嵌入，并收集相关文档（第 [6.1.4](#Sec5)
    节）。'
- en: '**Retrieval with passage embeddings**: These techniques encode the query and
    passages of the document by an embedding. Subsequently, these embeddings are compared.
    This type of embedding respects word order and thus has the potential to return
    better matches (Sect. [6.1.5](#Sec6)). Only a very small selection of methods
    can be described, which should give an impression of the approaches currently
    used as shown in Table [6.2](#Tab2). In Sects. [6.2.2](#Sec13) and [6.2.3](#Sec14)
    retrieval techniques for question answering are discussed, which are even more
    powerful. A very comprehensive survey on PLMs for retrieval is provided by Lin
    et al. [[124](#CR124)].Table 6.2'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于段落嵌入的检索**：这些技术通过嵌入将查询和文档的段落进行编码。随后，比较这些嵌入。这种嵌入类型尊重词序，因此有可能返回更好的匹配（第 [6.1.5](#Sec6)
    节）。只能描述非常小的一部分方法，这应该可以给人一个目前使用方法的印象，如表 [6.2](#Tab2) 所示。在第 [6.2.2](#Sec13) 和 [6.2.3](#Sec14)
    节中讨论了用于问答的检索技术，这些技术甚至更强大。Lin 等人 [[124](#CR124)] 提供了关于检索用 PLM 的非常全面的调查。在下一节中，将区分不同的密集检索方法，并对其进行介绍。'
- en: 'Document retrieval models with their performance. Benchmarks (Sect. [6.1.2](#Sec3)):
    MARCO: MS-MARCO [[16](#CR16)], NQuest: Natural Questions benchmark [[109](#CR109)],
    Wiki65K: long Wikipedia documents [[247](#CR247)]'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 文档检索模型及其性能。基准（第 [6.1.2](#Sec3) 节）：MARCO：MS-MARCO [[16](#CR16)]，NQuest：自然问题基准
    [[109](#CR109)]，Wiki65K：长维基百科文档 [[247](#CR247)]
- en: '| Model | Description | Benchmark |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 描述 | 基准 |'
- en: '| --- | --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| monoBERT (Sect. [6.1.3](#Sec4)) | Process each query-passage pair with BERT
    | MARCO 35.9% MRR@10 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| monoBERT (第 [6.1.3](#Sec4) 节)) | 使用 BERT 处理每个查询-段落对 | MARCO 35.9% MRR@10
    |'
- en: '| monoT5 (Sect. [6.1.3](#Sec4)) | Process each query-passage pair with T5 |
    MARCO 38% MRR@10 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| monoT5 (第 [6.1.3](#Sec4) 节)) | 使用 T5 处理每个查询-段落对 | MARCO 38% MRR@10 |'
- en: '| ColBERT (Sect. [6.1.4](#Sec5)) | Reranks search results documents based on
    token embeddings | MARCO 36.7% MRR@10 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| ColBERT (第 [6.1.4](#Sec5) 节)) | 根据标记嵌入重新排序搜索结果文档 | MARCO 36.7% MRR@10 |'
- en: '| Model 1 (Sect. [6.1.4](#Sec5)) | Compute the probability that the query is
    a ‘translation’ of the document | MARCO 39.1% MRR@100 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 模型 1 (第 [6.1.4](#Sec5) 节)) | 计算查询是文档“翻译”的概率 | MARCO 39.1% MRR@100 |'
- en: '| SMITH (Sect. [6.1.4](#Sec5)) | Use a BERT-based hierarchical encoder | Wiki65K
    95.9% acc. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| SMITH (第 [6.1.4](#Sec5) 节)) | 使用基于 BERT 的分层编码器 | Wiki65K 95.9% 准确率 |'
- en: '| SentenceBERT (Sect. [6.1.5](#Sec6)) | BERT encoder for query and documents
    | Reduce recall time from 65 h to 5 s |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| SentenceBERT (第 [6.1.5](#Sec6) 节)) | 查询和文档的 BERT 编码器 | 将召回时间从 65 小时减少到 5
    秒 |'
- en: '| DPR (Sect. [6.1.5](#Sec6)) | Different BERT encoders for query and documents,
    fine-tuned to reduce retrieval loss. FAISS index for approximate nearest neighbor
    search | NQuest 79.4% top-20 acc. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| DPR (第 [6.1.5](#Sec6) 节) | 查询和文档使用不同的 BERT 编码器，经过微调以减少检索损失。使用 FAISS 索引进行近似最近邻搜索
    | NQuest 79.4% top-20 准确率 |'
- en: '| RocketQA (Sect. [6.1.5](#Sec6)) | RoBERTa encoders for query and documents.
    Later reranking | MARCO 41.9% MRR@10 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| RocketQA (第 [6.1.5](#Sec6) 节) | 查询和文档使用 RoBERTa 编码器。后续进行重排序 | MARCO 41.9%
    MRR@10 |'
- en: '| coCondenser (Sect. [6.1.5](#Sec6)) | RoBERTa encoders for query and documents
    using CLS token. Later reranking | MARCO 40.8% MRR@100 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| coCondenser (第 [6.1.5](#Sec6) 节) | 使用 CLS 标记的 RoBERTa 编码器进行查询和文档处理。后续进行重排序
    | MARCO 40.8% MRR@100 |'
- en: 6.1.2 Measuring Text Retrieval Performance
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 测量文本检索性能
- en: There are a number of benchmark datasets used for training and comparing retrieval
    approaches. The *MS-MARCO* benchmark [[16](#CR16)] is a large-scale collection
    created from about half a million anonymized questions sampled from Bing’s search
    query logs. For the passage ranking task it contains a corpus of 8.8M passages
    with an average length of 55 words extracted from 3.6M web documents. The goal
    is to retrieve passages that answer the question. The training set contains approximately
    500k pairs of queries and relevant documents, and another 400M pairs of queries
    and non-relevant documents. There is a development set and a secret test set with
    about each 7k queries. However, there is a discussion that the gold annotation
    of the MS-MARCO benchmark is biased to some extent [[10](#CR10)].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多基准数据集用于训练和比较检索方法。*MS-MARCO* 基准 [[16](#CR16)] 是从 Bing 的搜索查询日志中抽取的大约五十万个匿名问题创建的大规模集合。对于段落排序任务，它包含从
    360 万个网页文档中提取的 880 万个段落，平均长度为 55 个单词。目标是检索出回答问题的段落。训练集包含大约 50 万对查询和相关文档，以及另外 4
    亿对查询和非相关文档。存在一个开发集和一个大约有 7k 个查询的保密测试集。然而，有人讨论说，MS-MARCO 基准的金标注在一定程度上是有偏见的 [[10](#CR10)]。
- en: The *Natural Questions* (*NQ*) [[109](#CR109)] contains questions with at least
    8 words from real users to the Google search engine. It requires QA systems to
    read and comprehend an entire Wikipedia article, which may or may not contain
    the answer to the question. An example is the question *“Where is blood pumped
    after it leaves the right ventricle?”* The task is to retrieve a long answer,
    i.e. a paragraph from the page that answers the question, e.g. *“From the right
    ventricle, blood is pumped through the semilunar pulmonary valve …”*, or an indication
    that there is no answer. The task was designed to be close to an end-to-end question
    answering application. One to five answers are provided by human annotators. While
    the original Natural Questions benchmark was a reading comprehension task providing
    a number of evidence documents for each question, the *EfficientQA benchmark*
    [[147](#CR147)] adapted this to open-domain QA by taking examples with up to five
    token answers and discarding the evidence documents.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*自然问题* (*NQ*) [[109](#CR109)] 包含来自真实用户的至少 8 个单词的问题。它要求问答系统阅读并理解整个维基百科文章，该文章可能或可能不包含问题的答案。例如，问题是
    *“血液在离开右心室后流向哪里？”* 任务是检索一个长答案，即页面上的一个段落，该段落回答了问题，例如 *“从右心室，血液通过肺动脉瓣泵出……”*，或者指示没有答案。该任务被设计成接近端到端问答应用。人类标注员提供一到五个答案。虽然原始的自然问题基准是一个阅读理解任务，为每个问题提供多个证据文档，但
    *EfficientQA* 基准 [[147](#CR147)] 通过使用最多五个标记的答案示例并将其证据文档丢弃，将其调整为开放域问答。'
- en: Min et al. [[146](#CR146)] note that over half of the queries in Natural Questions
    are ambiguous, with many sources of ambiguity such as event and entity references.
    They develop an *AmbigQA* with reformulated questions that yield a unique answer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Min 等人 [[146](#CR146)] 指出，在自然问题中，超过一半的查询是模糊的，存在许多模糊来源，如事件和实体引用。他们开发了一个 *AmbigQA*，其中包含重新表述的问题，这些问题可以产生一个独特的答案。
- en: A simple evaluation measure is the *top-k accuracy*, the proportion of queries
    for which one of the *k* most likely answers returned is correct. More complex
    is the *mean reciprocal rank* (*MRR*), the inverse of the rank of the first correct
    answer and 0, if no correct answer was returned. If, for instance, the third answer
    is correct, the reciprocal rank is 1∕3\. The MRR for |*Q*| queries is![$$\displaystyle
    \begin{aligned} MRR = \frac 1{|Q|}\sum_{i=1}^{|Q|}\frac 1{rank_i}. \end{aligned}
    $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ1.png)(6.1)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的评估指标是*top-k准确率*，即返回的*k*个最可能答案中至少有一个是正确的查询比例。更复杂的是*平均倒数排名*(*MRR*)，是第一个正确答案的排名的倒数和0，如果没有返回正确答案。例如，如果第三个答案是正确的，则倒数排名是1/3。|*Q*|个查询的MRR如下！[$$\displaystyle
    \begin{aligned} MRR = \frac 1{|Q|}\sum_{i=1}^{|Q|}\frac 1{rank_i}. \end{aligned}
    $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ1.png)(6.1)
- en: '*MRR@m* indicates that always an ordered list of *m* documents is returned.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*MRR@m*表示总是返回一个包含*m*个文档的有序列表。'
- en: We may define *Pr*(*i*) as the precision reached by the first *i* elements of
    the list of size *m*, i.e. the fraction of relevant documents of the first *i*.
    Then we may define the *average precision* as![$$\displaystyle \begin{aligned}
    AP = \frac 1m \sum_{i=1}^m Pr(i) * rel(i) \qquad  MAP = \frac 1{|Q|}\sum_{j=1}^{|Q|}
    AP_j \end{aligned} $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ2.png)(6.2)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将*Pr*(*i*)定义为列表的前*i*个元素达到的精度，即前*i*个相关文档的比例。然后我们可以定义*平均精度*如下！[$$\displaystyle
    \begin{aligned} AP = \frac 1m \sum_{i=1}^m Pr(i) * rel(i) \qquad  MAP = \frac
    1{|Q|}\sum_{j=1}^{|Q|} AP_j \end{aligned} $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ2.png)(6.2)
- en: where *rel*(*i*) = 1 if the *i*-th document is relevant and 0 otherwise. The
    *mean average precision* (*MAP*) is the average of AP over |*Q*| different queries.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*rel*(*i*)=1如果第*i*个文档是相关的，否则为0。*平均平均精度*(*MAP*)是AP在|*Q*|个不同查询上的平均值。
- en: 6.1.3 Cross-Encoders with BERT
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 基于BERT的交叉编码器
- en: '**monoBERT** [[155](#CR155)] performs reranking based on a fine-tuned BERT
    classifier based on the embedding of the *[CLS]* token. Query and document are
    combined to the input *“[CLS]* <* query*>*  [SEP]* <* document*>*  [SEP]”*. This
    is processed by a BERT fine-tuned on MS-MARCO, where the embedding of *[CLS]*
    in the last layer is used by a logistic classifier to predict the probability
    that the current document is relevant for the query. This output score is used
    for ranking (Fig. [6.2](#Fig2)). Note that by this technique paraphrases like
    *“symptoms of influenza include fever and nasal congestion”* and *“a stuffy nose
    and elevated temperature are signs you may have the flu”* may be identified.![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig2_HTML.png)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**monoBERT** [[155](#CR155)]基于*[CLS]*标记的嵌入进行重排序，查询和文档被组合到输入“*[CLS]* <*query*>
    [SEP] <*document*> [SEP]”。这通过在MS-MARCO上微调的BERT处理，其中使用最后一个层中的*[CLS]*嵌入由逻辑分类器预测当前文档与查询的相关概率。此输出分数用于排序（图[6.2](#Fig2)）。注意，通过这种技术可以识别出诸如“流感的症状包括发热和鼻塞”和“鼻塞和体温升高可能是你得了流感的迹象”之类的释义！![图6.2](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig2_HTML.png)'
- en: An illustration indicates the position, segment, tokens, sum of embeddings,
    output embeddings, and probability of relevance of the query and passage going
    through the BERT layers with masked self-attentions.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一个插图说明了查询和段落通过BERT层进行掩码自注意力时的位置、段、标记、嵌入和输出嵌入以及相关性的概率。
- en: Fig. 6.2
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2
- en: The monoBERT model uses a fine-tuned BERT model for ranking passages with respect
    to queries. The input contains the query concatenated with the passage. The *[CLS]*
    token embedding is trained to return the probability that the passage answers
    the query
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: monoBERT模型使用经过微调的BERT模型对查询进行排序。输入包含查询与段落拼接。*[CLS]*标记嵌入被训练以返回段落回答查询的概率
- en: On the MS-MARCO benchmark [[153](#CR153)] monoBERT yields an MRR@10 value of
    35.9% (i.e. the first relevant document at position 2.8 on average). As the keyword-based
    BM25-search before had an MRR@10-value of 16.5% (first relevant document at position
    6.1 on average), this result was a dramatic increase in performance of search
    engines. Such a big jump in effectiveness caused by an individual model is rarely
    observed in either academia or industry, which led to immediate excitement in
    the community.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MS-MARCO 基准测试 [[153](#CR153)] 中，monoBERT 得到了 35.9% 的 MRR@10 值（即平均位置 2.8 的第一个相关文档）。由于之前基于关键字的
    BM25 搜索的 MRR@10 值为 16.5%（平均位置 6.1 的第一个相关文档），这一结果在搜索引擎性能方面实现了显著提升。由单个模型引起的这种效果的大幅提升在学术界或工业界都很少见，这导致了社区的立即兴奋。
- en: It is quite striking how monoBERT provides a simple yet effective solution to
    the problem of text ranking (at least for texts that are shorter than its maximal
    input length) [[124](#CR124)]. In several studies monoBERT has been found to be
    better than BM25 in estimating relevance when term frequency is held constant.
    Using textual manipulation tests that alter existing documents, rearranging the
    order of words within a sentence or across sentences was found to have a large
    negative effect, while shuffling the order of sentences within a document has
    a modest negative effect. In contrast, rearranging only prepositions had little
    effect. Experimental results from input template variations show that monoBERT
    uses exact match, “soft” semantic matches, and information about the position
    of words. Exactly how these different components are combined—for different types
    of queries, across different corpora, and under different settings, etc.—remains
    an open question. Note that this search approach requires enormous computational
    resources, as for each passage a new evaluation has to be performed, while the
    effort for index search grows only logarithmically.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: monoBERT 提供了一种简单而有效的解决方案来解决文本排序问题（至少对于短于其最大输入长度的文本）[[124](#CR124)]，这一点相当引人注目。在多项研究中发现，当保持词频不变时，monoBERT
    在估计相关性方面优于 BM25。使用改变现有文档的文本操作测试，发现改变句子内或句子间的单词顺序有较大的负面影响，而改变文档内句子顺序的负面影响则较小。相比之下，仅重新排列介词的影响很小。从输入模板变体实验的结果显示，monoBERT
    使用精确匹配、“软”语义匹配和关于单词位置的信息。这些不同组件如何结合——针对不同类型的查询、不同语料库以及不同设置等——仍然是一个未解之谜。请注意，这种搜索方法需要巨大的计算资源，因为对于每个段落都需要进行新的评估，而索引搜索的努力仅呈对数增长。
- en: '**monoT5** [[154](#CR154)] used the T5 encoder-decoder model instead of BERT
    to rerank retrieved documents. The model receives the input *“Query:* <* query*>* 
    Document:* <* document*>*  Relevant:”*. monoT5 is fine-tuned to produce the tokens
    *true* or *false* if the document is relevant to the query or not. The predicted
    probability of *true* can be used as a relevance score. For T5 with 3B parameters
    the authors get an MRR@10-value of 38% for MS-MARCO passage retrieval. This shows
    that larger models increase performance of retrieval systems.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**monoT5** [[154](#CR154)] 使用了 T5 编码器-解码器模型而不是 BERT 来重新排序检索到的文档。该模型接收输入 *“Query:*
    <*query*> Document:* <*document*> Relevant:”。monoT5 被微调以生成表示文档是否与查询相关的标记 *true*
    或 *false*。预测的 *true* 的概率可以用作相关性得分。对于具有 30 亿参数的 T5，作者在 MS-MARCO 文档检索中获得了 38% 的
    MRR@10 值。这表明更大的模型可以提高检索系统的性能。'
- en: 6.1.4 Using Token Embeddings for Retrieval
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.4 使用标记嵌入进行检索
- en: The all-to-all nature of the BERT attention patterns at each transformer encoder
    layer means that there is a quadratic complexity in terms of time and space with
    respect to the input length. In Sect. [3.​2](528393_1_En_3_Chapter.xhtml#Sec7)
    we have introduced a number of approaches to cope with longer inputs. These all
    can be used to process longer documents. Among the many approaches we discuss
    ColBERT and Model 1 in more detail.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个 Transformer 编码器层中，BERT 注意模式的完全全连接性质意味着，与输入长度相比，时间和空间复杂度呈二次方。在第 [3.2](528393_1_En_3_Chapter.xhtml#Sec7)
    节中，我们介绍了一些处理更长时间输入的方法。这些方法都可以用于处理更长的文档。在众多方法中，我们更详细地讨论了 ColBERT 和 Model 1。
- en: '**ColBERT** [[99](#CR99)] reranks the output of another (cheaper) retrieval
    model, typically a term-based model, or directly for end-to-end retrieval from
    a document collection. Queries and documents were prepended by different special
    tokens. ColBERT uses a single pre-trained BERT model to encode each query or document
    into a bag of token embeddings. In a final layer the size of embeddings is reduced
    and they are normalized to Euclidean length 1.0\. Hence, the inner product is
    equivalent to the cosine similarity. If (*q*[1], …, *q*[*m*]) are the query tokens
    and *d*[*i*,1], …, *d*[*i*,*k*] are the tokens of the *i*-th document, the similarity
    of *q* and *d*[*i*] is computed as![$$\displaystyle \begin{aligned} s_{q,d_i}
    = \sum_{r=1}^m \max_j {\boldsymbol{\eta}}(q_r)^\intercal{\boldsymbol{\eta}}(d_{i,j}).
    \end{aligned} $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ3.png)(6.3)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**ColBERT** [[99](#CR99)] 对另一个（更便宜）的检索模型的输出进行重新排序，通常是基于词的模型，或者直接从文档集合中进行端到端检索。查询和文档前面添加了不同的特殊标记。ColBERT使用单个预训练的BERT模型将每个查询或文档编码成一个标记嵌入的集合。在最后一层，嵌入的大小被减小，并且它们被归一化到欧几里得长度1.0。因此，内积等价于余弦相似度。如果(*q*[1],
    …, *q*[*m*])是查询标记，*d*[*i*,1], …, *d*[*i*,*k*]是第*i*-个文档的标记，则*q*和*d*[*i*]之间的相似度计算如下![$$\displaystyle
    \begin{aligned} s_{q,d_i} = \sum_{r=1}^m \max_j {\boldsymbol{\eta}}(q_r)^\intercal{\boldsymbol{\eta}}(d_{i,j}).
    \end{aligned} $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ3.png)(6.3)'
- en: This is the sum of maximum cosine similarities (MaxSim) between each query term
    and the “best” matching term contained in the document *d*[*i*]. For each query
    embedding the L2-nearest 10 embeddings are taken into account and *k* = 1000 closest
    document vectors are retrieved.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是每个查询词与文档*d*[*i*]中包含的“最佳”匹配词之间的最大余弦相似度（MaxSim）之和。对于每个查询嵌入，考虑L2最近的10个嵌入，并检索*
    k* = 1000个最近的文档向量。
- en: For ranking a preliminary search result of, say 1000 documents, the maximum
    similarities (e.g. cosine similarity) between all query embeddings and all embeddings
    in the retrieved documents are computed. This approach is very efficient as it
    requires orders of magnitude fewer FLOPS than previous approaches. On the MS-MARCO
    benchmark [[153](#CR153)] a reranking ColBERT achieves a MRR@10-value of 34.9%
    (first relevant document at position 2.9 on average), which is slightly below
    the cross-encoder monoBERT.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对初步搜索结果进行排序，例如1000个文档，计算所有查询嵌入和检索文档中所有嵌入之间的最大相似度（例如余弦相似度）。这种方法非常高效，因为它需要的FLOPS数量比以前的方法少几个数量级。在MS-MARCO基准测试
    [[153](#CR153)] 中，重新排序的ColBERT实现了MRR@10值为34.9%（平均第2.9个相关文档），这略低于交叉编码的单个BERT。
- en: 'ColBERT can also be used for end-to-end retrieval. It employs the *FAISS* index
    [[91](#CR91)] to store the document token embeddings for a *k*-nearest neighbor
    search in a preparatory step. Note that for each token in each document an embedding
    has to be stored, as the embedding depends on the context. The retrieval requires
    two stages: in the first stage, a number of approximate searches for each query
    token is performed. In the second refinement stage, these approximate matches
    are reranked according to the MaxSim criterion. On the MS-MARCO benchmark the
    end-to-end retrieval by ColBERT has a MRR@10-value of 36.7%, which is much better
    than the reranking performance and on par with the much more expensive BERT cross-encoder
    approach.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ColBERT也可以用于端到端检索。它使用*FAISS*索引 [[91](#CR91)] 在准备步骤中存储文档标记嵌入，以进行*k*-最近邻搜索。请注意，对于每个文档中的每个标记，都需要存储一个嵌入，因为嵌入依赖于上下文。检索需要两个阶段：在第一阶段，对每个查询标记执行多个近似搜索。在第二阶段的细化阶段，根据MaxSim标准对这些近似匹配进行重新排序。在MS-MARCO基准测试中，ColBERT的端到端检索的MRR@10值为36.7%，这比重新排序性能好得多，并且与更昂贵的BERT交叉编码方法相当。
- en: '**Model 1** [[28](#CR28)] mixes a number of techniques for their retrieval
    model based on token embeddings. First the authors estimate the probability *p*(***q***|***d***)
    that the query ***q*** has been generated as a “translation” of the document ***d***.
    Using Bayes rule the authors get![$$\displaystyle \begin{aligned} p({\boldsymbol{d}}|{\boldsymbol{q}})\propto
    p({\boldsymbol{q}}|{\boldsymbol{d}})p({\boldsymbol{d}})\propto p({\boldsymbol{q}}|{\boldsymbol{d}})
    \end{aligned} $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ4.png)(6.4)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型1** [[28](#CR28)] 在其基于标记嵌入的检索模型中混合了多种技术。首先，作者估计查询***q***被生成作为文档***d***的“翻译”的概率*p*(***q***|***d***)。使用贝叶斯定理，作者得到![$$\displaystyle
    \begin{aligned} p({\boldsymbol{d}}|{\boldsymbol{q}})\propto p({\boldsymbol{q}}|{\boldsymbol{d}})p({\boldsymbol{d}})\propto
    p({\boldsymbol{q}}|{\boldsymbol{d}}) \end{aligned} $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ4.png)(6.4)'
- en: assuming a uniform prior *p*(***d***) [[21](#CR21)]. They consider the probability
    *r*(*q*[*i*]|*d*[*j*]) that a query token *q*[*i*] is a translation of a document
    token *d*[*j*]. Approximating *r*(*q*[*i*]|*d*[*j*]) by a neural network, they
    use embeddings of tokens *q*[*i*] and *d*[*j*] as inputs and are able to estimate
    *p*(***d***|***q***). The approach requires little computational effort. The authors
    combined the BERT dense retriever with a Lucene search index. Finally, they expand
    documents for Model 1 with Doc2query. *Doc2query* [[156](#CR156)] aims at generating
    queries, for which the document is relevant. The approach trains a transformer
    to generate up to 100 query tokens from a document of up to 400 tokens. The model
    is trained using datasets consisting of pairs of query and relevant documents,
    e.g. MS-MARCO. On MS-MARCO they achieve 39.1% MRR@100\. The context-free neural
    Model 1 is less effective than a BERT-based ranking model, but it can run efficiently
    on a CPU (without expensive index-time precomputation or query-time operations
    on large tensors).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个均匀先验*p*(***d***) [[21](#CR21)]。他们考虑查询标记*q*[*i*]是文档标记*d*[*j*]的翻译的概率*r*(*q*[*i*]|*d*[*j*])。通过神经网络近似*r*(*q*[*i*]|*d*[*j*])，他们使用标记*q*[*i*]和*d*[*j*]的嵌入作为输入，并能够估计*p*(***d***|***q***)。这种方法需要很少的计算努力。作者将BERT密集检索器与Lucene搜索索引相结合。最后，他们使用Doc2query为模型1扩展文档。*Doc2query*
    [[156](#CR156)]旨在生成与文档相关的查询。该方法训练一个transformer，从最多400个标记的文档中生成多达100个查询标记。该模型使用包含查询和相关文档对的集合数据集进行训练，例如MS-MARCO。在MS-MARCO上，他们实现了39.1%的MRR@100。与基于BERT的排名模型相比，无上下文的神经模型1效果较差，但它可以在CPU上高效运行（无需昂贵的索引时间预计算或在大张量上的查询时间操作）。
- en: Currently, no retriever tries to process long documents. This has many important
    applications like news recommendation, related article recommendation and paper
    citation suggestion. Usually, long documents are partitioned into passages with
    the idea that the relevant contents is contained in a passage. Note that PLMs
    with longer inputs, e.g. BigBird, can improve performance (Sect. [3.​2](528393_1_En_3_Chapter.xhtml#Sec7)).
    However, it is clear that this has to be evaluated. The **SMITH** model [[247](#CR247)]
    uses a BERT-based hierarchical encoder to capture the document structure information.
    The document is first partitioned into sentences and for each sentence token embeddings
    are computed. Each sentence starts with an *[CLS]* token, whose embedding represents
    the sentence. There is a higher sentence level BERT which just receives the sentence
    embeddings as input. The first artificial token of second level BERT is used as
    the embedding of the whole document.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，没有任何检索器尝试处理长文档。这有许多重要的应用，如新闻推荐、相关文章推荐和论文引用建议。通常，长文档会被分割成段落，其理念是相关内容包含在段落中。请注意，具有更长输入的PLMs，例如BigBird，可以提高性能（见第[3.2](528393_1_En_3_Chapter.xhtml#Sec7)节）。然而，这一点必须经过评估。**SMITH**模型[[247](#CR247)]使用基于BERT的分层编码器来捕获文档结构信息。文档首先被分割成句子，并为每个句子计算词嵌入。每个句子都以一个*[CLS]*标记开始，其嵌入代表该句子。存在一个更高层次的BERT，它只接收句子嵌入作为输入。第二层BERT的第一个人工标记被用作整个文档的嵌入。
- en: The model is pre-trained by the masked language modeling task to get token embeddings.
    In addition, in the second level there is a masked sentence block prediction task
    where the model has to select the correct embedding from all sentence embeddings
    in a batch. The fine-tuning task maximizes the relevance score predicted from
    the document embedding by a logistic classifier for the relevance-annotated fine-tuning
    dataset. On the *Wiki65K* with long Wikipedia articles [[87](#CR87)] the approach
    achieves an accuracy of 95.9% which is a significant improvement over prior approaches.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型通过掩码语言建模任务进行预训练以获取标记嵌入。此外，在第二级中，有一个需要模型从一批句子嵌入中选择正确嵌入的掩码句子块预测任务。微调任务通过逻辑分类器从相关性注释的微调数据集中预测的相关性分数最大化。在包含长维基百科文章的*Wiki65K*
    [[87](#CR87)]上，该方法实现了95.9%的准确率，这比先前的方法有显著改进。
- en: 6.1.5 Dense Passage Embeddings and Nearest Neighbor Search
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.5 稠密段落嵌入和最近邻搜索
- en: Representing text passages by embedding vectors has the potential to solve the
    problem of vocabulary mismatch by directly matching “meaning” in a representation
    space. These so-called *dense retrieval* techniques can perform ranking directly
    on vector representations generated by PLMs. In contrast to calculating pairwise
    differences of token embeddings, this approach offers a much more efficient retrieval
    procedure. This is performed by matching the embedding vector of a query with
    the embedding vectors of passages employing an index and approximate nearest neighbor
    search. Efficient, scalable solutions are available today in open-source libraries.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用嵌入向量表示文本段落有可能通过直接在表示空间中匹配“意义”来解决词汇不匹配的问题。这些所谓的*密集检索*技术可以直接在PLM生成的向量表示上进行排名。与计算标记嵌入的成对差异相比，这种方法提供了一个更高效的检索过程。这是通过将查询的嵌入向量与使用索引和近似最近邻搜索的段落嵌入向量进行匹配来完成的。今天，开源库中提供了高效、可扩展的解决方案。
- en: Given a query *q* and a set of documents *D* = {*d*[1], …, *d*[*n*]} we want
    to define functions ***η***[*q*](⋅) and ***η***[*d*](⋅), which convert the token
    sequences *q* and *d* into fixed-width vectors. The functions should have the
    property that the similarity between ***η***[*q*](*q*) and ***η***[*d*](*d*[*i*])
    is maximal if *d*[*i*] is relevant for query *q*. We want to estimate![$$\displaystyle
    \begin{aligned} p(\text{relevant}=1|d_i,q) := \phi({\boldsymbol{\eta}}_q(q),{\boldsymbol{\eta}}_d(d_i)),
    \end{aligned} $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ5.png)(6.5)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个查询*q*和一组文档*D*={*d*[1]，…，*d*[n]}，我们想要定义函数***η***[*q*](⋅)和***η***[*d*](⋅)，这些函数将标记序列*q*和*d*转换为固定宽度的向量。这些函数应具有以下属性：如果*d*[i*]对查询*q*相关，则***η***[*q*](*q*)和***η***[*d*](*d*[i*])之间的相似度最大。我们想要估计！[$$\displaystyle
    \begin{aligned} p(\text{relevant}=1|d_i,q) := \phi({\boldsymbol{\eta}}_q(q),{\boldsymbol{\eta}}_d(d_i)),
    \end{aligned} $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ5.png)(6.5)
- en: where *ϕ*(⋅) is a similarity comparison function, e.g. the scalar product [[124](#CR124),
    p. 133]. Note that ***η***[*d*](*d*[*i*]) may be precomputed and organized in
    an index. By using different encoders ***η***[*q*](⋅) and ***η***[*d*](⋅) for
    queries and documents, we can take into account the different roles and wordings
    of queries and documents.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*ϕ*(⋅)是一个相似度比较函数，例如标量积[[124](#CR124)，第133页]。请注意，***η***[*d*](*d*[i*])可能预先计算并组织在一个索引中。通过为查询和文档使用不同的编码器***η***[*q*](⋅)和***η***[*d*](⋅)，我们可以考虑查询和文档的不同角色和措辞。
- en: '**SentenceBERT** [[183](#CR183)] is the prototype of a bi-encoder design for
    generating semantically meaningful sentence embeddings to be used in large-scale
    textual similarity comparisons (Fig. [6.3](#Fig3)). The query *q* and the documents
    *d*[*i*] are processed by the same PLM (BERT or RoBERTa). Similarity was compared
    by the *cosine similarity*![$$\displaystyle \begin{aligned} \phi({\boldsymbol{\eta}}_q(q),{\boldsymbol{\eta}}_d(d_i))=\frac{{\boldsymbol{\eta}}_q(q)^\intercal
    {\boldsymbol{\eta}}_d(d_i)}{\left\lVert {\boldsymbol{\eta}}_q(q)\right\rVert *\left\lVert
    {\boldsymbol{\eta}}_d(d_i)\right\rVert }. \end{aligned} $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ6.png)(6.6)![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig3_HTML.png)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**SentenceBERT** [[183](#CR183)] 是一种双编码器设计的原型，用于生成具有语义意义的句子嵌入，用于大规模文本相似度比较（图[6.3](#Fig3)）。查询*q*和文档*d*[i*]由相同的PLM（BERT或RoBERTa）处理。相似度是通过*余弦相似度*比较的！[$$\displaystyle
    \begin{aligned} \phi({\boldsymbol{\eta}}_q(q),{\boldsymbol{\eta}}_d(d_i))=\frac{{\boldsymbol{\eta}}_q(q)^\intercal
    {\boldsymbol{\eta}}_d(d_i)}{\left\lVert {\boldsymbol{\eta}}_q(q)\right\rVert *\left\lVert
    {\boldsymbol{\eta}}_d(d_i)\right\rVert }. \end{aligned} $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ6.png)(6.6)![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig3_HTML.png)'
- en: An illustration represents the flow of query and passage through the BERT layers
    with masked self-attentions. It indicates the layers of position, tokens, sum
    of embeddings, output embeddings, cosine similarity, and similarity value.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一幅插图展示了查询和段落通过BERT层以及掩码自注意力机制的流程。它表明了位置、标记、嵌入和输出嵌入、余弦相似度和相似度值的层级。
- en: Fig. 6.3
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3
- en: The SentenceBERT model uses two fine-tuned BERT models to transform queries
    and passages to embeddings of the *[CLS]* token. Subsequently, a cosine similarity
    module is used to compute a similarity value
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: SentenceBERT模型使用两个微调的BERT模型将查询和段落转换为*[CLS]*标记的嵌入。随后，使用余弦相似度模块来计算相似度值
- en: To generate sentence embeddings the authors investigated three alternatives.
    (1) Use the embedding of the *[CLS]* token. (2) Averaging (mean-pooling) of all
    output embeddings. (3) Component-wise maximum (max-pooling) of all output embeddings.
    Without fine-tuning the results were worse than for non-contextual embeddings.
    Fine-tuning boosted performance and yields a new Sota. It turned out that average
    pooling was the most effective design, slightly better than max pooling or using
    the *[CLS]* token. Most important the computation time for finding the best match
    in 10,000 documents was reduced from 65 h to 5 s.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成句子嵌入，作者调查了三种替代方案。（1）使用 *[CLS]* 标记的嵌入。（2）所有输出嵌入的平均值（平均池化）。（3）所有输出嵌入的分量最大值（最大池化）。未经微调的结果比非上下文嵌入的结果更差。微调提高了性能，并产生了一个新的
    Sota。结果证明，平均池化是最有效的设计，略好于最大池化或使用 *[CLS]* 标记。最重要的是，在 10,000 个文档中找到最佳匹配的计算时间从 65
    小时减少到 5 秒。
- en: '**DPR** [[94](#CR94)] used separate encoders ***η***[*q*](*q*) and ***η***[*d*](*d*[*i*])
    for the query *q* and the text passages *d*[*i*] of about 100 words. Both encoders
    took the *[CLS]* embedding from BERT[BASE] as its output representation. As comparison
    function the inner product ![$${\boldsymbol {\eta }}_q(q)^\intercal {\boldsymbol
    {\eta }}_d(d_i)$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq7.png)
    was used. For each query *q*[*i*] the training set contained one correct passage
    ![$$d^+_i$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq8.png)
    and a number of negative passages ![$$d^-_{i,1},\ldots ,d^-_{i,m}$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq9.png).
    The loss function encoded the goal to get a large *ϕ*-value (i.e. similarity)
    for *q*[*i*] and ![$$d^+_i$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq10.png)
    and small similarities for *q*[*i*] and ![$$d^-_{i,j}$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq11.png)![$$\displaystyle
    \begin{aligned} L(w) = -\log \frac{\exp[ {\boldsymbol{\eta}}_q(q)^\intercal{\boldsymbol{\eta}}_d(d^+_i)]}
    {\exp[ {\boldsymbol{\eta}}_q(q)^\intercal{\boldsymbol{\eta}}_d(d_i)] + \sum_{j=1}^m
    \exp [{\boldsymbol{\eta}}_q(q)^\intercal{\boldsymbol{\eta}}_d(d^-_{i,j})]} {}
    \end{aligned} $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ7.png)(6.7)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**DPR** [[94](#CR94)] 使用了分别针对查询 *q* 和约100个单词的文本段落 *d*[*i*] 的独立编码器 ***η***[*q*](*q*)
    和 ***η***[*d*](*d*[*i*])。这两个编码器都采用 BERT[BASE] 的 *[CLS]* 嵌入作为其输出表示。作为比较函数，使用了内积
    ![$${\boldsymbol {\eta }}_q(q)^\intercal {\boldsymbol {\eta }}_d(d_i)$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq7.png)。对于每个查询
    *q*[*i*]，训练集中包含一个正确段落 ![$$d^+_i$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq8.png)
    和多个负段落 ![$$d^-_{i,1},\ldots ,d^-_{i,m}$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq9.png)。损失函数编码了目标，即对于
    *q*[*i*] 和 ![$$d^+_i$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq10.png)
    获取大的 *ϕ*-值（即相似度），而对于 *q*[*i*] 和 ![$$d^-_{i,j}$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq11.png)
    获取小的相似度！'
- en: The negative examples were a mixture of passages retrieved with keyword search
    that did not contain the answer and thus were difficult negatives. In addition,
    passages from other examples in the same training batch were used. Instead of
    performing an exhaustive computation of similarities for all documents between
    *η*[*q*](*q*) and the *η*[*d*](*d*[*i*]), we can employ an approximate nearest
    neighbor search. *FAISS* [[91](#CR91)] is an open-source method based on hierarchical
    navigable small world graphs. For the Natural Questions benchmark they achieved
    a top-20 accuracy of 79.4%, which is much better than the previous top-20 accuracy
    of 59.1% for the keyword-based BM25 search. The replication study [[136](#CR136)]
    could confirm these results, but found that a hybrid approach of DPR and BM25
    could increase the performance to 82.6%.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 负样本是使用关键词搜索检索到的段落混合体，这些段落不包含答案，因此是难以处理的负样本。此外，还使用了同一训练批次中其他示例的段落。我们不是在 *η*[*q*](*q*)
    和 *η*[*d*](*d*[*i*]) 之间的所有文档之间进行相似度的全面计算，而是可以采用近似最近邻搜索。*FAISS* [[91](#CR91)] 是一种基于层次可导航小世界图的开放源代码方法。对于自然问题基准测试，他们实现了
    79.4% 的前20名准确率，这比基于关键词的 BM25 搜索的前20名准确率 59.1% 高得多。复制研究 [[136](#CR136)] 可以确认这些结果，但发现
    DPR 和 BM25 的混合方法可以将性能提高到 82.6%。
- en: '**ANCE** [[238](#CR238)] uses a single RoBERTa model to encode query and document.
    During training, hard negative examples are selected by approximate nearest neighbor
    search on an index over the representations generated by the trained encoder.
    In this way, they can select “difficult” negative examples. The index is periodically
    updated. On Natural Questions ANCE achieved 82.1% top-20 accuracy. The performance
    was also compared with the monoBERT cross-encoder, which reranks first-stage BM25
    results with monoBERT by comparing all documents to the query. It turned out that
    on MS-MARCO the application of monoBERT to BM25 had a MRR@10 of 34.7% while ANCE
    has 33%. The cross-encoder obviously is more effective than ANCE. The authors
    also applied ANCE to 8 billion documents using embeddings of size 64 and approximate
    nearest neighbor search. They reported a gain of 16% compared to the prior commercial
    implementation.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**ANCE** [[238](#CR238)] 使用单个RoBERTa模型对查询和文档进行编码。在训练过程中，通过在训练编码器生成的表示上的索引上进行近似最近邻搜索来选择硬负例。这样，他们可以选择“困难”的负例。索引定期更新。在Natural
    Questions上，ANCE实现了82.1%的top-20准确率。性能还与使用monoBERT进行重排序的单BERT交叉编码器进行了比较，该编码器通过将所有文档与查询进行比较来重排序第一阶段BM25结果。结果发现，在MS-MARCO上，将monoBERT应用于BM25的MRR@10为34.7%，而ANCE为33%。显然，交叉编码器比ANCE更有效。作者还将ANCE应用于80亿文档，使用大小为64的嵌入和近似最近邻搜索。他们报告了与先前商业实现相比提高了16%。'
- en: '**RocketQA** [[184](#CR184)] performs a first retrieval step and subsequently
    a re-ranking procedure. Both approaches are jointly optimized using a listwise
    training approach, where a list of positive and negative examples is used for
    training both modules. In addition, they perform a data augmentation to construct
    diverse training instances by incorporating both random sampling and denoised
    sampling. They report a MRR@10 on MS-MARCO of 38.8% for passage retrieval. When
    the 50 top results are reranked later, they can increase MRR@10 to 41.9%.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**RocketQA** [[184](#CR184)] 执行首次检索步骤，随后进行重排序过程。两种方法都使用列表训练方法联合优化，其中使用正例和负例的列表来训练两个模块。此外，他们通过结合随机采样和去噪采样来构建多样化的训练实例。他们报告了MS-MARCO段落检索的MRR@10为38.8%。当稍后对前50个结果进行重排序时，他们可以将MRR@10提高到41.9%。'
- en: '**coCondenser** [[63](#CR63)] is one of the highest entries of the MS-MARCO
    leaderboard [[140](#CR140)]. The model is forced to learn to aggregate information
    into the *“CLS”* embedding, which will then participate in the LM prediction.
    Then an additional “contrastive loss” is used: *“CLS”* embeddings of passages
    from the same document close together should be similar, while those for passages
    in different documents should have a larger distance. This yields highly expressive
    embeddings for passages. When the model is fine-tuned on MS-MARCO, it returns
    an *MRR@*100 of 40.8% on the MS-MARCO leaderboard [[140](#CR140)].'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**coCondenser** [[63](#CR63)] 是MS-MARCO排行榜上最高的条目之一 [[140](#CR140)]。该模型被迫学习将信息聚合到“CLS”嵌入中，然后该嵌入将参与LM预测。然后使用额外的“对比损失”：来自同一文档的段落“CLS”嵌入应靠近，而来自不同文档的段落应具有更大的距离。这为段落提供了高度表达的嵌入。当模型在MS-MARCO上进行微调时，它在MS-MARCO排行榜上返回了40.8%的*MRR@*100。'
- en: Available Implementations
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: DPR code is available at [https://​github.​com/​facebookresearch​/​DPR](https://github.com/facebookresearch/DPR).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DPR代码位于[https://github.com/facebookresearch/DPR](https://github.com/facebookresearch/DPR)。
- en: The code for the FAISS nearest neighbor search is available at [https://​github.​com/​facebookresearch​/​faiss](https://github.com/facebookresearch/faiss).
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FAISS最近邻搜索的代码位于[https://github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss)。
- en: ANCE code and data trained nearest neighbor search is available at [https://​github.​com/​microsoft/​ANCE](https://github.com/microsoft/ANCE).
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ANCE代码和训练最近邻搜索的数据位于[https://github.com/microsoft/ANCE](https://github.com/microsoft/ANCE)。
- en: RocketQA code and data is available at [https://​github.​com/​PaddlePaddle/​RocketQA](https://github.com/PaddlePaddle/RocketQA).
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RocketQA代码和数据位于[https://github.com/PaddlePaddle/RocketQA](https://github.com/PaddlePaddle/RocketQA)。
- en: FlexNeuART [[27](#CR27)] implements the Model 1 retrieval system [[28](#CR28)].
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FlexNeuART [[27](#CR27)] 实现了模型1检索系统 [[28](#CR28)]。
- en: coCondenser code at [https://​github.​com/​luyug/​Condenser](https://github.com/luyug/Condenser).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: coCondenser代码位于[https://github.com/luyug/Condenser](https://github.com/luyug/Condenser)。
- en: 6.1.6 Summary
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.6 摘要
- en: Retrieval is a crucial step in web search, in which a small set of query-relevant
    candidate passages are identified from a corpus of billions of texts. Discovering
    more semantically related candidates in the retrieval phase holds great promise
    for presenting more high-quality results to the end user. Dense retrieval approaches
    represent a paradigm shift in search engine technology. They make it possible
    to recognize the meaning of words and paraphrases and thus find much better passages
    matching a query. Search results can also be used for question-answer models (Sect.
    [6.2](#Sec9)) and dialog systems (Sect. [6.6](#Sec49)). They are already being
    used in production search engine by Bing [[35](#CR35), [238](#CR238), [266](#CR266)],
    Google [[152](#CR152), [197](#CR197)], and Facebook [[82](#CR82)].
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 检索是网络搜索中的一个关键步骤，其中从数十亿文本语料库中识别出一小部分与查询相关的候选段落。在检索阶段发现更多语义相关的候选者，对于向最终用户提供更多高质量结果具有很大潜力。密集检索方法代表了搜索引擎技术的范式转变。它们使得能够识别单词和释义的含义，从而找到与查询更匹配的段落。搜索结果还可以用于问答模型（第[6.2](#Sec9)节）和对话系统（第[6.6](#Sec49)节）。它们已经被Bing
    [[35](#CR35), [238](#CR238), [266](#CR266)]、Google [[152](#CR152), [197](#CR197)]和Facebook
    [[82](#CR82)]等生产搜索引擎所使用。
- en: Dense retrieval methods discussed above are fine-tuned in a supervised setting
    using human relevance labels as input, e.g. from MS-MARCO. Best results are obtained
    by two different PLMs to encode the query and the documents. Both PLMs are trained
    to improve the probability of a correct reference document in contrast to some
    negative documents. As two different PLMs require more effort, most systems use
    a single model to encode the question and the documents. Experiments show that
    the combination of dense retrieval and keyword retrieval seems to have advantages.
    In Sects. [6.2.2](#Sec13) and [6.2.3](#Sec14) retrieval techniques for question
    answering are discussed, which are even more powerful.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 上述密集检索方法在监督设置中使用人类相关性标签作为输入进行微调，例如来自MS-MARCO。通过使用两个不同的PLM来编码查询和文档，可以获得最佳结果。这两个PLM都被训练以提高正确参考文档的概率，与一些负面文档相比。由于需要更多的努力，大多数系统使用单个模型来编码问题和文档。实验表明，密集检索和关键字检索的组合似乎具有优势。在第[6.2.2](#Sec13)节和[6.2.3](#Sec14)节中讨论了用于问题回答的检索技术，这些技术甚至更强大。
- en: A problem is the transferability of a search system to a new domain. BERT was
    found to have strong cross-domain relevance classification capabilities when used
    in a similar way as monoBERT [[124](#CR124), p. 72]. If a BERT model is fine-tuned
    using relevance judgments from one domain (e.g., tweets) it can be successfully
    applied to a different domain (e.g., newswire articles). On the other hand, Thakur
    et al. [[221](#CR221)] created a benchmark called *BEIR* with 18 retrieval tasks
    from very different domains like bio-medicine and tweets. The authors trained
    a large number of dense retrieval techniques on MS-MARCO and evaluated then on
    the other tasks. They found that they were on average less effective than BM25,
    which due to its simplicity just works in most cases.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一个问题是搜索系统对新领域的可迁移性。当BERT以类似monoBERT的方式使用时，被发现具有强大的跨领域相关性分类能力 [[124](#CR124),
    p. 72]。如果使用一个领域的相关性判断（例如，推文）微调BERT模型，它可以成功应用于不同的领域（例如，新闻文章）。另一方面，Thakur等人 [[221](#CR221)]
    创建了一个名为*BEIR*的基准，其中包含来自非常不同领域（如生物医学和推文）的18个检索任务。作者在MS-MARCO上训练了大量密集检索技术，并在其他任务上进行了评估。他们发现，平均而言，这些技术比BM25更不有效，因为BM25由于其简单性，在大多数情况下都能正常工作。
- en: The memory requirements for an index for embeddings cannot be ignored. While
    a keyword Lucene index for the MS-MARCO passage corpus with 8.8M passages needs
    661 MB, a FAISS index for vectors of size 768 requires 42 GB and an index for
    ColBERT takes 156 GB [[124](#CR124), p. 159]. To apply these techniques to web-scale,
    approaches with a smaller memory footprint are needed.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于嵌入索引的内存需求不容忽视。虽然MS-MARCO段落语料库中包含8.8M个段落的Lucene索引需要661 MB的内存，但768大小向量的FAISS索引需要42
    GB，而ColBERT索引则需要156 GB [[124](#CR124), p. 159]。为了将这些技术应用于网络规模，需要采用内存占用更小的方法。
- en: 6.2 Question Answering
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 问题回答
- en: '*Question Answering* (QA) is an application of NLP that receives a natural
    language query and automatically generates a precise answer in natural language.
    It is a long-standing AI task dating back to the 1960s [[69](#CR69)]. Compared
    with search engines discussed in Sect. [6.1](#Sec1), the QA system presents the
    final answer to a question directly instead of returning a list of relevant snippets
    or hyperlinks. Thus, it is more user-friendly and efficient. Often, the system
    has access to a database or a *knowledge base* (*KB*) of documents, such as Wikipedia,
    where it can search for relevant information.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*问答*（QA）是自然语言处理的应用，它接收自然语言查询并自动以自然语言生成精确答案。这是一个从20世纪60年代开始的长期AI任务 [[69](#CR69)]。与第[6.1](#Sec1)节中讨论的搜索引擎相比，问答系统直接呈现问题的最终答案，而不是返回相关片段或超链接列表。因此，它更友好且效率更高。通常，系统可以访问数据库或*知识库*（KB），例如维基百科，其中可以搜索相关信息。'
- en: A *Closed Domain QA system* handles questions for a specific domain, e.g. medicine,
    and has background knowledge about that domain or is trained with a large training
    set covering that domain. *Open Domain QA systems* (ODQA) deal with questions
    on almost any topic and usually rely on general KBs or Internet search [[37](#CR37)].
    *Multimodal QA* systems address questions in different media, e.g., text and images.
    A survey of ODQA is given by Zhu et al. [[265](#CR265)]. Table [6.3](#Tab3) compiles
    leading QA Models with their performance.Table 6.3
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*封闭域问答系统* 处理特定领域的问答，例如医学，并具有该领域的背景知识或使用覆盖该领域的大规模训练集进行训练。*开放域问答系统*（ODQA）处理几乎所有主题的问答，通常依赖于通用知识库或互联网搜索
    [[37](#CR37)]。*多模态问答*系统处理不同媒体中的问题，例如文本和图像。Zhu等人提供了一份ODQA调查 [[265](#CR265)]。表[6.3](#Tab3)汇总了领先的问答模型及其性能。表6.3'
- en: 'Question answering models with their performance. The lower part contains retrieval
    models. Benchmarks: NQ: natural Questions benchmark of Google queries [[109](#CR109)],
    TriviaQA: TriviaQA benchmark [[92](#CR92), [226](#CR226)], HotpotQA: multihop
    benchmark [[249](#CR249)], EM: exact match'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 带有性能的问答模型。下半部分包含检索模型。基准：NQ：谷歌查询的自然问题基准 [[109](#CR109)]，TriviaQA：TriviaQA基准 [[92](#CR92)，[226](#CR226)]，HotpotQA：多跳基准
    [[249](#CR249)]，EM：精确匹配
- en: '| Model | Details | Benchmark |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 详细信息 | 基准 |'
- en: '| --- | --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| BigBird (Sect. [6.2.1](#Sec10)) | Autoencoder with long input, supervised
    training with QA pairs | NQ with ref-docs 57.9% EM WikiHop 82.3% acc. |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| BigBird (第[6.2.1](#Sec10)节) | 长输入自动编码器，使用问答对进行监督训练 | 带参考文档的NQ 57.9% EM WikiHop
    82.3% 准确率 |'
- en: '| PoolingFormer (Sect. [6.2.1](#Sec10)) | Autoencoder with two-level attention
    schema, supervised training with QA pairs | NQ with ref-docs 61.6% EM |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| PoolingFormer (第[6.2.1](#Sec10)节) | 具有两个级联注意力模式的自动编码器，使用问答对进行监督训练 | 带参考文档的NQ
    61.6% EM |'
- en: '| RealFormer (Sect. [6.2.1](#Sec10)) | Autoencoder with bypass attention, supervised
    training with QA pairs, multihop QA | WikiHop 84.4% acc. |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| RealFormer (第[6.2.1](#Sec10)节) | 具有旁路注意力的自动编码器，使用问答对进行监督训练，多跳问答 | WikiHop
    84.4% 准确率 |'
- en: '| GPT-3 (Sect. [6.2.1](#Sec10)) | Large autoencoder 175B, only pre-training
    | NQ few-shot 29.9% TriviaQA few-shot 71.2% |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3 (第[6.2.1](#Sec10)节) | 175B大型自动编码器，仅预训练 | NQ少样本29.9%，TriviaQA少样本71.2%
    |'
- en: '| Gopher (Sect. [6.2.1](#Sec10)) | Large autoencoder 280B, only pre-training
    | NQ few-shot 28.2% |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Gopher (第[6.2.1](#Sec10)节) | 280B大型自动编码器，仅预训练 | NQ少样本28.2% |'
- en: '| PaLM (Sect. [6.2.1](#Sec10)) | Large autoencoder 540B, only pre-training
    | NQ few-shot 36.0% TriviaQA few-shot 81.4% |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| PaLM (第[6.2.1](#Sec10)节) | 540B大型自动编码器，仅预训练 | NQ少样本36.0%，TriviaQA少样本81.4%
    |'
- en: '| DPR (Sect. [3.​4.​5](528393_1_En_3_Chapter.xhtml#Sec22)) | Retriever-reader
    with two BERT models and FAISS index | NQ exact match acc 41.5% TriviaQA 57.9%
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| DPR (第[3.4.5](528393_1_En_3_Chapter.xhtml#Sec22)节) | 带有两个BERT模型和FAISS索引的检索器-阅读器
    | NQ精确匹配准确率41.5%，TriviaQA 57.9% |'
- en: '| FiD (Sect. [3.​4.​5](528393_1_En_3_Chapter.xhtml#Sec22)) | Retriever-reader
    with T5 models and FAISS index | NQ exact match acc 51.4% TriviaQA 67.6% |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| FiD (第[3.4.5](528393_1_En_3_Chapter.xhtml#Sec22)节) | 带有T5模型和FAISS索引的检索器-阅读器
    | NQ精确匹配准确率51.4%，TriviaQA 67.6% |'
- en: '| REALM (Sect. [3.​4.​5](528393_1_En_3_Chapter.xhtml#Sec22)) | Retriever-reader
    with dot product of BERT embeddings, slow | NQ exact match acc 40.4% |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| REALM (第[3.4.5](528393_1_En_3_Chapter.xhtml#Sec22)节) | 带有BERT嵌入点积的检索器-阅读器，速度慢
    | NQ精确匹配准确率40.4% |'
- en: '| FB HYBRID (Sect. [3.​4.​5](528393_1_En_3_Chapter.xhtml#Sec22)) | DPR retriever
    combined with other retriever, FiD reader | NQ exact match acc 53.9%, corresponds
    to 67.4% correct |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| FB HYBRID (第[3.4.5](528393_1_En_3_Chapter.xhtml#Sec22)节) | 将DPR检索器与其他检索器结合，FiD阅读器
    | NQ精确匹配准确率53.9%，对应于67.4%正确率 |'
- en: '| MS UNITED (Sect. [3.​4.​5](528393_1_En_3_Chapter.xhtml#Sec22)) | BERT-based
    retriever, T5+ELECTRA as readers, final re-ranking | NQ exact match acc 54.0%,
    corresponds to 65.8% correct |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| MS UNITED (第[3.4.5](528393_1_En_3_Chapter.xhtml#Sec22)节) | 基于BERT的检索器，T5+ELECTRA作为读者，最终重排序
    | NQ精确匹配准确率54.0%，相当于65.8%的正确率 |'
- en: '| AISO (Sect. [3.​4.​5](528393_1_En_3_Chapter.xhtml#Sec22)) | Retriever-reader
    with repeated retrieval rounds, multihop QA | HotpotQA 72.0% F1 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| AISO (第[3.4.5](528393_1_En_3_Chapter.xhtml#Sec22)节) | 具有重复检索轮次的检索-阅读器，多跳问答
    | HotpotQA 72.0% F1 |'
- en: '| RETRO (Sect. [6.2.3](#Sec15)) | Language model with frozen BERT retriever,
    language model periodically includes retrieved token chunks | NQ exact match acc
    45.5% |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| RETRO (第[6.2.3](#Sec15)节) | 具有冻结BERT检索器的语言模型，语言模型定期包括检索到的标记块 | NQ精确匹配准确率45.5%
    |'
- en: '| WEBGPT (Sect. [6.2.3](#Sec15)) | GPT-3 combined with Bing search engine,
    which can be periodically invoked | TriviaQA 69.5% |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| WEBGPT (第[6.2.3](#Sec15)节) | 结合Bing搜索引擎的GPT-3，可以定期调用 | TriviaQA 69.5% |'
- en: A simple form of question answering is *Reading Comprehension*, where the system
    has to identify an answer to a question in a given text. Often a BERT-like system
    marks the answer span in the text by span prediction (Sect. [2.​1.​3](528393_1_En_2_Chapter.xhtml#Sec5)).
    This task can mainly be considered as solved. For the *SQuAD 2.0 benchmark* [[179](#CR179)]
    ALBERT yields more than 93% F1-value and the fine-tuned *ST-MoE-32B* mixture-of-experts
    model (Sect. [3.​5.​2](528393_1_En_3_Chapter.xhtml#Sec26)) with 269B parameters
    [[270](#CR270)] achieves 96.3% F1-value, while the human F1-value is 89.5% [[178](#CR178)].
    However, Sen et al. [[199](#CR199)] indicate that systems trained on one dataset
    may not generalize well to other benchmarks.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 问答的一种简单形式是*阅读理解*，其中系统需要在给定的文本中识别问题的答案。通常，BERT类似的系统通过跨度预测（第[2.1.3](528393_1_En_2_Chapter.xhtml#Sec5)节）在文本中标记答案跨度。这项任务基本上可以被认为是解决了。对于*SQuAD
    2.0基准* [[179](#CR179)]，ALBERT实现了超过93%的F1值，而经过微调的*ST-MoE-32B*混合专家模型（第[3.5.2](528393_1_En_3_Chapter.xhtml#Sec26)节）拥有269B参数[[270](#CR270)]，实现了96.3%的F1值，而人类的F1值为89.5%
    [[178](#CR178)]。然而，Sen等人[[199](#CR199)]指出，在一个数据集上训练的系统可能无法很好地泛化到其他基准。
- en: 6.2.1 Question Answering Based on Training Data Knowledge
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 基于训练数据知识的问答
- en: Language models often are trained on comprehensive text collections and are
    able to memorize a large amount of information. A frequently used benchmark is
    *Natural Questions* (*NQ*) [[109](#CR109)], which has been sampled from the Google
    search logs (Sect. [6.1.2](#Sec3)). For the given question, the system has to
    find a short answer span in the given support documents. An example is the question
    *“When are hops added to the brewing process?”*, which should yield the answer
    *“The boiling process”*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型通常在综合文本集合上训练，能够记住大量信息。常用的基准是*Natural Questions* (*NQ*) [[109](#CR109)]，它已经从Google搜索日志中抽取（第[6.1.2](#Sec3)节）。对于给定的问题，系统需要在提供的支持文档中找到一个短的答案跨度。一个例子是问题*“何时在酿造过程中添加跳跃？”*，应该得到答案*“煮沸过程”*。
- en: The *TriviaQA* benchmark [[92](#CR92), [226](#CR226)] contains a set of trivia
    questions with answers that were originally scraped from the Web. Different from
    Natural Questions, the questions here are written with known answers in mind.
    *TruthfulQA* [[125](#CR125)] is a special QA benchmark with short questions that
    are constructed adversarially, so that some people’s answers might be wrong due
    to false beliefs and misconceptions. The answers are evaluated according to informativeness
    and truthfulness.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*TriviaQA*基准[[92](#CR92), [226](#CR226)]包含一组从网络中抓取的趣味问题及其答案。与Natural Questions不同，这里的问题是以已知答案为前提编写的。*TruthfulQA*
    [[125](#CR125)]是一个特殊的问答基准，包含短问题，这些问题是通过对抗性构建的，因此有些人可能会因为错误信念和误解而给出错误的答案。答案根据信息量和真实性进行评估。'
- en: Fine-Tuned Question Answering Models
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调的问答模型
- en: The **BigBird** (Sect. [3.​2](528393_1_En_3_Chapter.xhtml#Sec7)) self-attention
    was used as an autoencoder and trained with the MLM objective using an input sequence
    of 4096 tokens [[253](#CR253)]. During fine-tuning on Natural Questions the model
    had to find a short answer span in one of the given evidence documents. The model
    achieved 57.9% F1-value on this task. The **PoolingFormer** [[256](#CR256)] is
    an alternative model for long input sequences with a two-level attention schema.
    Its first level uses a smaller sliding window pattern to aggregate information
    from neighbors. Its second level employs a larger window to increase receptive
    fields with pooling attention. An ensemble of fine-tuned PoolingFormers achieves
    61.6% F1-value on the Natural Questions benchmark. The model is similar to the
    **SMITH** model [[247](#CR247)], which uses a BERT-based hierarchical encoder
    to capture the document structure information (Sect. [6.1.4](#Sec5)).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**BigBird**（第[3.2](528393_1_En_3_Chapter.xhtml#Sec7)节）的自注意力被用作自动编码器，并使用4096个标记的输入序列通过MLM目标进行训练
    [[253](#CR253)]。在Natural Questions的微调期间，模型必须在给定的证据文档中找到一个简短的答案跨度。该模型在这个任务上实现了57.9%的F1值。**PoolingFormer**
    [[256](#CR256)]是一个用于长输入序列的替代模型，具有两级注意力模式。其第一级使用较小的滑动窗口模式从邻居中聚合信息。其第二级使用较大的窗口通过池化注意力来增加感受野。一组微调的PoolingFormers在Natural
    Questions基准测试上实现了61.6%的F1值。该模型类似于**SMITH**模型 [[247](#CR247)]，它使用基于BERT的分层编码器来捕获文档结构信息（第[6.1.4](#Sec5)节）。'
- en: An alternative is **Macaw** [[218](#CR218)], a freely available QA-system with
    11B parameters. It is built on T5 and has strong zero-shot QA-capabilities. On
    a set of 300 challenge questions the authors claim that Macaw outperforms GPT-3
    by 10%, although it has only a small fraction of its parameters. In addition to
    providing an answers for a question, Macaw can also take an answer and produce
    a question; or generate multiple-choice options for an answer and a question.
    The authors also provide a detailed analysis of errors.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是**Macaw** [[218](#CR218)]，一个带有11B参数的免费问答系统。它基于T5，并具有强大的零样本问答能力。在300个挑战性问题集上，作者声称Macaw的表现比GPT-3高出10%，尽管它的参数只有后者的一小部分。除了为问题提供答案外，Macaw还可以接受一个答案并生成一个问题；或者为答案和问题生成多项选择题选项。作者还提供了一份详细的错误分析。
- en: It is much more difficult to combine different pieces of evidence to find an
    answer. A benchmark to test this ability is *WikiHop* [[232](#CR232)], where information
    from different documents has to be merged. An example is the question *“Hanging
    gardens of Mumbai, country?”* and the documents *“The Hanging Gardens, in Mumbai,
    also known as Pherozeshah Mehta Gardens, are terraced gardens …”* and *“Mumbai
    is the capital city of the Indian state of Maharashtra. It is the most populous
    city in India …”*. For each query up to 140 background paragraphs are provided
    to the model. On this benchmark BigBird-ETC (Sect. [3.​2.​1](528393_1_En_3_Chapter.xhtml#Sec8))
    achieved an accuracy of 82.3%. Currently, the best model for this task is the
    **RealFormer** with an accuracy of 84.4% [[171](#CR171)], which is slightly below
    the human performance of 85%. The RealFormer is an autoencoder with a modified
    architecture, which provides a bypass with the raw attention scores of all attention
    heads from the previous layer in the subsequent layers [[76](#CR76)].
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 将不同证据片段组合起来以找到答案要困难得多。测试这种能力的基准是*WikiHop* [[232](#CR232)]，其中需要合并来自不同文档的信息。一个例子是问题*“孟买的悬空花园，国家？”*以及文档*“孟买的悬空花园，也称为费罗泽沙·梅赫塔花园，是梯田式花园……”*和*“孟买是印度马哈拉施特拉邦的首府。它是印度人口最多的城市……”*。对于每个查询，模型都提供了多达140个背景段落。在这个基准测试中，BigBird-ETC（第[3.2.1](528393_1_En_3_Chapter.xhtml#Sec8)节）实现了82.3%的准确率。目前，这个任务的最好模型是**RealFormer**，准确率为84.4%
    [[171](#CR171)]，略低于85%的人类表现。RealFormer是一个具有修改后架构的自动编码器，它通过后续层中所有注意力头原始注意力分数的旁路提供了一种绕过方法
    [[76](#CR76)]。
- en: Question Answering with Few-Shot Language Models
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于少量样本语言模型的问答
- en: Recent Foundation Models are trained with an enormous collection of documents
    and can generate answers to questions without additional knowledge input. An example
    is the autoregressive language model **GPT-3** with 175B parameters, which was
    pre-trained on a text collection of books, Wikipedia and web pages of about 500
    billion tokens (Sect. [3.​1.​2](528393_1_En_3_Chapter.xhtml#Sec3)). Because of
    its high model capacity it can absorb a lot of ‘knowledge’ in its parameters.
    When a Foundation Model is not allowed to use external information, this is called
    *Closed-book QA*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 近期的基础模型使用大量文档进行训练，并且可以在没有额外知识输入的情况下生成问题的答案。一个例子是参数量为175B的自动回归语言模型**GPT-3**，它在约5000亿个标记的书籍、维基百科和网页文本集合上进行了预训练（见[3.1.2节](528393_1_En_3_Chapter.xhtml#Sec3)）。由于其高模型容量，它可以在其参数中吸收大量的“知识”。当基础模型不允许使用外部信息时，这被称为*闭卷问答*。
- en: As discussed in Sect. [3.​6.​3](528393_1_En_3_Chapter.xhtml#Sec41), GPT-3 can
    be instructed by a few examples (few-shot) to solve a task. Figure [6.4](#Fig4)
    provides a few-shot prompt example. For Natural Questions, GPT-3 achieves an exact
    match accuracy of 14.6% in the zero-shot setting, 23.0% in the one-shot setting,
    and 29.9% in the few-shot setting [[29](#CR29), p. 14]. This was achieved without
    fine-tuning on Natural Questions. The larger **Gopher** model with 280B parameters
    (Sect. [3.​1.​2](528393_1_En_3_Chapter.xhtml#Sec3)) performs slightly worse with
    28.2% in the few-shot setting [[175](#CR175), p. 80].![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig4_HTML.png)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如[3.6.3节](528393_1_En_3_Chapter.xhtml#Sec41)所述，GPT-3可以通过几个示例（少样本）来完成任务。图[6.4](#Fig4)提供了一个少样本提示示例。对于自然问题，GPT-3在零样本设置下达到了14.6%的精确匹配准确率，在单样本设置下达到了23.0%，在少样本设置下达到了29.9%（见[29](#CR29)，第14页）。这是在没有对自然问题进行微调的情况下实现的。参数量为280B的更大**Gopher**模型在少样本设置下的表现略差，准确率为28.2%（见[175](#CR175)，第80页）![图片](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig4_HTML.png)
- en: A text box represents a set of prompts and their answers given by the bot. It
    provides the response unknown if there is no clear answer.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 文本框表示由机器人提供的提示及其答案的集合。如果没有明确的答案，则提供未知响应。
- en: Fig. 6.4
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4
- en: A possible few-shot prompt for GPT-3 to get an answer based on existing knowledge
    acquired during pre-training [[160](#CR160)]
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3的一个可能的少样本提示，以基于预训练期间获取的现有知识来获取答案[[160](#CR160)]
- en: The even larger **PaLM** model with 540B parameters (Sect. [3.​1.​2](528393_1_En_3_Chapter.xhtml#Sec3))
    was trained on a high-quality dataset with 780B tokens. It uses a new prompt technique
    to pose logical questions, where examples are presented to the system together
    with *thought chains* partitioning a reasoning task into smaller problems (Sect.
    [3.​6.​4](528393_1_En_3_Chapter.xhtml#Sec42)). In this way it gets the recipe
    to combine facts from different sources to arrive at the final answer.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 参数量更大的**PaLM**模型，拥有540B个参数（见[3.1.2节](528393_1_En_3_Chapter.xhtml#Sec3)），是在包含780B个标记的高质量数据集上训练的。它采用了一种新的提示技术来提出逻辑问题，其中将推理任务分割成更小的子问题，并将示例与*思维链*一起呈现给系统（见[3.6.4节](528393_1_En_3_Chapter.xhtml#Sec42)）。通过这种方式，它获得了将来自不同来源的事实结合在一起以得出最终答案的配方。
- en: 'PaLM was evaluated on a large number of other benchmarks, which in part are
    QA-tasks. On Natural Questions it arrived at an accuracy of 21.2% with 0-shots
    and at 36.0% with few-shot prompts [[43](#CR43), p. 47]. On Trivia QA (questions
    concerning the Wikipedia), BoolQ (question answering with yes/no answers), and
    PIQA (question answering with reasoning) PaLM also achieved a new Sota. The results
    are shown in Table [3.​4](528393_1_En_3_Chapter.xhtml#Tab4). PaLM was benchmarked
    with a large number of tests, among them the more than 150 BIG-bench tasks (Sect.
    [4.​1.​4](528393_1_En_4_Chapter.xhtml#Sec5)). Many of them are QA-related tasks:
    21 contextual QA tasks, 24 context-free QA tasks, 36 reading comprehension tasks,
    and a large number of tasks on specific knowledge and common sense [[1](#CR1),
    [22](#CR22)]. Additional outcomes for QA-benchmarks of PaLM are given in [[43](#CR43),
    p. 12], where PaLM always achieves Sota.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: PaLM在大量其他基准测试中进行了评估，其中部分是问答任务。在自然问题中，它在零样本设置下达到了21.2%的准确率，在少样本提示下达到了36.0%（见[43](#CR43)，第47页）。在事实问答（涉及维基百科的问题）、BoolQ（是/否回答的问答）和PIQA（推理问答）中，PaLM也实现了新的Sota。结果在表[3.4](528393_1_En_3_Chapter.xhtml#Tab4)中显示。PaLM与大量测试进行了基准测试，其中包括超过150个BIG-bench任务（见[4.1.4节](528393_1_En_4_Chapter.xhtml#Sec5)）。其中许多是与问答相关的任务：21个上下文问答任务，24个无上下文问答任务，36个阅读理解任务，以及大量关于特定知识和常识的任务[[1](#CR1)，[22](#CR22)]。PaLM在问答基准测试中的额外结果见[[43](#CR43)，第12页]，其中PaLM总是实现Sota。
- en: 6.2.2 Question Answering Based on Retrieval
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 基于检索的问答
- en: 'Retrieval ODQA systems usually work in two stages: for a question a *retriever*
    module finds a number of documents from a text collection, which might contain
    the answer. Subsequently, a *reader* considers the question and the retrieved
    documents and generates a natural language answer (Fig. [6.5](#Fig5)). Since the
    model relies on external information, it is referred to as *Open-book QA*.![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig5_HTML.png)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 检索ODQA系统通常分为两个阶段：对于一个问题，*检索器*模块从一个文本集合中找到可能包含答案的多个文档。随后，*读者*考虑问题和检索到的文档，并生成一个自然语言答案（见图[6.5](#Fig5)）。由于该模型依赖于外部信息，因此被称为*开放式问答*。![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig5_HTML.png)
- en: A flow diagram represents the flow of unstructured documents through the retriever,
    followed by relevant documents, and the reader, before generating the answer.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 流程图表示非结构化文档通过检索器、相关文档和读者流动，最后生成答案。
- en: Fig. 6.5
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5
- en: Question answering often combines dense retrieval with an answer selection module.
    The retriever performs a dense retrieval by comparing the embedding of the query
    with the embeddings of passages. The reader ranks the retrieved documents and
    generates an answer by an autoregressive Pre-trained Language Model [[36](#CR36)].
    Credits for image parts in Table [A.​2](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab3)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 问答通常结合密集检索和答案选择模块。检索器通过比较查询的嵌入与段落的嵌入进行密集检索。读者对检索到的文档进行排序，并通过自回归预训练语言模型生成答案[[36](#CR36)]。表格[A.2](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab3)中的图像部分归功于。
- en: Retrievers have been introduced in Sect. [3.​4.​5](528393_1_En_3_Chapter.xhtml#Sec22)
    and were discussed in the context of document retrieval in Sect. [6.1](#Sec1).
    The retriever may employ a traditional search engine using tf-idf weighting or
    BM25\. Alternatively the retriever may be a *dense retriever* based on document
    and question embeddings. It is trained to retrieve passages by computing embedding
    similarities e.g. by *DPR* [[94](#CR94)] (Sect. [3.​4.​5](528393_1_En_3_Chapter.xhtml#Sec22)).
    A tutorial on ODQA is provided by Chen [[36](#CR36)].
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[3.4.5](528393_1_En_3_Chapter.xhtml#Sec22)节中介绍了检索器，并在第[6.1](#Sec1)节中讨论了其在文档检索中的应用。检索器可能使用传统的搜索引擎，例如使用tf-idf权重或BM25。或者检索器可能是一个基于文档和问题嵌入的*密集检索器*。它被训练通过计算嵌入相似度来检索段落，例如通过*DPR*[[94](#CR94)]（见第[3.4.5](528393_1_En_3_Chapter.xhtml#Sec22)节）。Chen[[36](#CR36)]提供了一个关于ODQA的教程。
- en: The *reader* is usually an autoregressive language model that receives both
    the query and the retrieved documents as inputs. It is fine-tuned to generate
    a response to the query based on the retrieved information and its internal knowledge.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*读者*通常是一个自回归语言模型，它接收查询和检索到的文档作为输入。它被微调以根据检索到的信息和其内部知识生成对查询的响应。'
- en: Question answering with external knowledge bases has the advantage that curated
    KBs usually are checked for correctness. They may have, however, limited coverage
    of entities and relations may not be up-to-date. There are a number of approaches
    to combine PLMs with KBs using techniques like entity mapping (Sect. [3.​4.​1](528393_1_En_3_Chapter.xhtml#Sec18)).
    Recent papers propose a hybrid approach using KBs and retrieval [[239](#CR239)].
    Knowledge-Guided Text Retrieval [[145](#CR145)] starts with retrieving text passages
    for a query. It creates a passage graph, where vertices are passages of text and
    edges represent relationships that are derived either from an external knowledge
    base or co-occurrence in the same article. On Natural Questions [[109](#CR109)]
    they achieve an accuracy of 34.5%.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用外部知识库进行问答具有优势，因为经过精心挑选的KB通常经过正确性检查。然而，它们可能对实体的覆盖范围有限，关系可能不是最新的。有几种方法可以将PLMs与KB结合使用，例如使用实体映射（见第[3.4.1](528393_1_En_3_Chapter.xhtml#Sec18)节）。最近的研究论文提出了一种混合方法，使用KB和检索[[239](#CR239)]。知识引导文本检索[[145](#CR145)]从检索查询的文本段落开始。它创建一个段落图，其中顶点是文本段落，边表示从外部知识库或同一文章中的共现中推导出的关系。在自然问题[[109](#CR109)]中，他们实现了34.5%的准确率。
- en: '**HYBRIDER** [[41](#CR41)] uses information from a retriever as well as from
    a structured KB and tables. The authors collected Wikipedia pages and constructed
    a benchmark dataset HybridQA containing question-answer pairs requiring multi-hop
    reasoning using text, tables and hyperlinks (Fig. [6.6](#Fig6)). The model first
    links questions to tables cells as well as Wikipedia passages and hyperlinks.
    In a reasoning phase the linked information is ranked and consolidated to derive
    the probabilities of different answers. The experiments with the dataset show
    that the utilization of tables or retrieval alone achieves an exact match accuracy
    of about 20% while the joint model yields more than 40%. However, the hybrid model’s
    score is still far below human performance.![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig6_HTML.png)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**HYBRIDER** [[41](#CR41)] 使用检索器以及结构化KB和表格的信息。作者收集了维基百科页面并构建了一个基准数据集HybridQA，其中包含需要使用文本、表格和超链接进行多跳推理的问题-答案对（图
    [6.6](#Fig6)）。模型首先将问题链接到表格单元格以及维基百科段落和超链接。在推理阶段，链接的信息被排序和整合以推导出不同答案的概率。使用数据集的实验表明，仅使用表格或检索可以达到大约20%的精确匹配准确率，而联合模型则超过40%。然而，混合模型的得分仍然远低于人类表现。![图片](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig6_HTML.png)'
- en: A table exhibits the list of names, years, seasons, and flag bearers in the
    Olympic events. It indicates various details of the flag bearer on the right side.
    Below are 2 questions related to the year and sports category along with their
    answers.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表格展示了奥运事件中的名单、年份、赛季和旗手。它指出了右侧旗手的各种细节。下方是关于年份和运动类别的2个相关问题及其答案。
- en: Fig. 6.6
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6
- en: For hybrid question answering Wikipedia pages are retrieved by HYBRIDER [[41](#CR41)]
    (top left). Some pages contain tables (left). Here the column titles may be interpreted
    as well as hyperlinks to entities (underlined). The lower part shows two human-annotated
    question-answer pairs. Image reprinted with kind permission of the authors [[41](#CR41),
    p. 2]
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于混合问答，通过HYBRIDER [[41](#CR41)]（左上角）检索维基百科页面。一些页面包含表格（左侧）。在这里，列标题可以解释为以及指向实体的超链接（下划线）。下方显示了两个由人类标注的问题-答案对。图片经作者许可重新印刷
    [[41](#CR41)，p. 2]
- en: One of the first retrieval-reader systems was **DPR** (Dense Passage Retriever)
    [[94](#CR94)]. It employs a BERT model to encode passages by embeddings and retrieves
    them by approximate *k*-nearest neighbor search with the FAISS index (Sect. [6.1.5](#Sec6)).
    In this way it can gather passages with similar meaning but different wording.
    The DPR reader is another BERT model which is fine-tuned to predict a probability
    for each retrieved passage that this passage contains the correct answer. In addition,
    it selects a span of tokens by span prediction, which probably provides the answer.
    The approach can be easily applied to KBs with billions of passages [[94](#CR94),
    [213](#CR213)]. On the *Natural Questions* [[109](#CR109)] it yields a test set
    accuracy of 41.5%.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最早的检索-阅读系统之一是 **DPR**（密集段落检索器）[[94](#CR94)]。它使用BERT模型通过嵌入编码段落，并通过FAISS索引（第 [6.1.5](#Sec6)
    节）进行近似 *k* 近邻搜索来检索它们。这样，它可以收集具有相似意义但措辞不同的段落。DPR阅读器是另一个BERT模型，经过微调以预测每个检索到的段落包含正确答案的概率。此外，它通过跨度预测选择一个标记跨度，这可能会提供答案。这种方法可以轻松应用于包含数十亿段落的KB
    [[94](#CR94)，[213](#CR213)]。在 *自然问题* [[109](#CR109)] 上，它实现了41.5%的测试集准确率。
- en: '**FiD** [[84](#CR84)] is described in Sect. [3.​4.​5](528393_1_En_3_Chapter.xhtml#Sec22).
    The retriever is based on DPR and compares query and passages embeddings. Raffel
    et al. [[177](#CR177)] have shown that generative models like T5 can produce the
    answer for QA-tasks. FiD processes the query and the retrieved passages by a *reader*
    based on a T5 model to generate an answer. Since the first step is to process
    the passages one by one, the system is very efficient. FiD achieves an exact match
    accuracy of 51.4% on the Natural Questions test set compared to 41.5% for DPR.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**FiD** [[84](#CR84)] 在第 [3.4.5](528393_1_En_3_Chapter.xhtml#Sec22) 节中进行了描述。检索器基于DPR，比较查询和段落嵌入。Raffel等人
    [[177](#CR177)] 已经表明，像T5这样的生成模型可以为问答任务生成答案。FiD通过基于T5模型的*reader*处理查询和检索到的段落以生成答案。由于第一步是逐个处理段落，因此系统非常高效。与DPR相比，FiD在自然问题测试集上实现了51.4%的精确匹配准确率，而DPR为41.5%。'
- en: '**REALM** [[75](#CR75)] and **RAG** [[114](#CR114)] are retrieval augmented
    generative models for open domain question answering. However, they process all
    retrieved passages simultaneously in an autoregressive language model and were
    unable to take into account a large number of passages leading to lower accuracies
    on Natural Questions of 40.4 for REALM and 44.5 for RAG. Sachan et al. [[194](#CR194)]
    propose an end-to-end differentiable training method for retrieval-augmented ODQA.
    Latent variables indicate which of the relevant documents should be included.
    The values of the latent variables are iteratively estimated by an EM-algorithm.
    On Natural Questions they achieve an exact match accuracy of 52.5%.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**REALM** [[75](#CR75)] 和 **RAG** [[114](#CR114)] 是用于开放域问答的检索增强生成模型。然而，它们在一个自回归语言模型中同时处理所有检索到的段落，因此无法考虑到大量段落，导致
    REALM 在 Natural Questions 上的准确率降低至 40.4%，而 RAG 降低至 44.5%。Sachan 等人 [[194](#CR194)]
    提出了一种用于检索增强开放域问答的端到端可微分训练方法。潜在变量指示哪些相关文档应该被包含。通过 EM 算法迭代估计潜在变量的值。在 Natural Questions
    上，他们实现了 52.5% 的精确匹配准确率。'
- en: '**MTR** [[138](#CR138)] starts from the observation that neural retrievers
    perform well on their fine-tuning domain, but will typically achieve low out-of-domain
    performance. The authors propose a multitask retriever similar to DPR which is
    jointly fine-tuned on eight diverse retrieval tasks. They use a shared passage
    encoder—so that a single index of encoded passages can be used—as well as a query
    encoder that is shared across all tasks. In five of the eight models they achieve
    a higher performance than special models tuned to the corresponding domain.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**MTR** [[138](#CR138)] 从观察出发，神经检索器在其微调领域表现良好，但通常在域外性能较低。作者提出了一种类似于 DPR 的多任务检索器，它在八个不同的检索任务上联合微调。他们使用了一个共享的段落编码器——这样就可以使用单个编码段落的索引——以及一个在所有任务中共享的查询编码器。在八个模型中的五个中，他们实现了比针对相应领域微调的特殊模型更高的性能。'
- en: '**AISO** [[268](#CR268)] is a retriever-reader architecture for solving multi-hop
    QA tasks, where multiple documents are required to answer a question. Repeated
    retrieval rounds are performed in which associated terms are taken as new search
    queries to find additional evidence. The approach is adaptive and at each step
    selects one of three types of retrieval operations (e.g., BM25, DPR, and hyperlink)
    or one answer operation. On the *HotpotQA benchmark* [[249](#CR249)], the question-answering
    system must find the answer to a query in the scope of the entire Wikipedia. The
    AISO model achieved a new Sota with a joint F1-value of 72.0%.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**AISO** [[268](#CR268)] 是一种用于解决多跳问答任务的检索-阅读架构，其中需要多个文档来回答一个问题。执行重复的检索轮次，其中关联的术语被用作新的搜索查询以找到额外的证据。该方法具有适应性，并在每个步骤中选择三种类型之一的检索操作（例如，BM25、DPR
    和超链接）或一个答案操作。在 *HotpotQA 基准测试* [[249](#CR249)] 中，问答系统必须在整个维基百科的范围内找到查询的答案。AISO
    模型实现了 72.0% 的联合 F1 值的新 Sota。'
- en: The **FB Hybrid** system was presented at the EfficientQA competition [[147](#CR147)],
    where real user questions for the Google search engine from the Natural Questions
    dataset [[109](#CR109)] were tackled. While the original NQ was a reading comprehension
    task providing a number of evidence documents for each question, the EfficientQA
    benchmark [[147](#CR147)] adapted this to open-domain QA by taking examples with
    up to five token answers and discarding the evidence documents. The system uses
    a retriever-reader architecture [[158](#CR158)]. The retriever is a mixture of
    DPR and another retrieval system, which covers lists and tables as well as KB-relations
    and retrieves 100 passages. The reader is a T5-large Seq2seq model, which is given
    100 passages from the retriever and generates an answer. The background corpus
    contained 18.8M passages from Wikipedia. On Natural Questions the model achieves
    an exact match accuracy of 53.9%. According to an evaluation by human raters the
    model was able to answer 67.4% of the questions correctly, which is about as good
    as the performance of human experts using a search engine. The **MS UnitedQA**
    model had a similar architecture [[139](#CR139)]. It uses a BERT-based retriever
    and a reader combined from a T5-model and ELECTRA processes the returned documents
    to generate different answers. A final re-ranking model selects the answer. MS
    UnitedQA yields an exact match accuracy of 54.0% and 65.8% correctness on Natural
    Questions. If the systems were restricted to a memory footprint of 6 GB the performance
    was only marginally reduced.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**FB混合**系统在EfficientQA竞赛[[147](#CR147)]中展出，该竞赛处理了来自自然问题数据集[[109](#CR109)]中针对谷歌搜索引擎的真实用户问题。虽然原始NQ是一个提供每个问题多个证据文档的阅读理解任务，但EfficientQA基准[[147](#CR147)]通过采用最多五个标记答案的示例并丢弃证据文档来适应开放域问答。该系统使用检索器-阅读器架构[[158](#CR158)]。检索器是DPR和另一个检索系统的混合体，涵盖了列表和表格以及知识库关系，并检索100个段落。阅读器是一个T5-large
    Seq2seq模型，它从检索器接收100个段落并生成一个答案。背景语料库包含来自维基百科的1880万个段落。在自然问题数据集上，该模型实现了53.9%的精确匹配准确率。根据人工评分员的评估，该模型能够正确回答67.4%的问题，这几乎与使用搜索引擎的人类专家的表现相当。**MS
    UnitedQA**模型具有类似的架构[[139](#CR139)]。它使用基于BERT的检索器和由T5模型和ELECTRA处理返回文档以生成不同答案的阅读器。最终的重新排序模型选择答案。MS
    UnitedQA在自然问题数据集上实现了54.0%的精确匹配准确率和65.8%的正确率。如果系统被限制在6GB的内存占用内，性能仅略有下降。'
- en: 6.2.3 Long-Form Question Answering Using Retrieval
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 使用检索的长文本问答
- en: A Language Model with Integrated Retrieval
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 集成检索的语言模型
- en: '**Retro** [[25](#CR25)] is an autoregressive language model with 7B parameters
    using retrieved information to predict the next token. As retriever a frozen BERT
    model is employed (Fig. [6.7](#Fig7)). Each training sequence is split into chunks,
    which are augmented with their *k*-nearest neighbors retrieved from the database
    of 2 trillion tokens. The returned information is processed in a language model
    to improve the prediction of the next token leading to large performance gains.
    The reader consists of a differentiable autoregressive encoder and a chunked cross-attention
    module to predict tokens.![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig7_HTML.png)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**Retro** [[25](#CR25)]是一个具有70亿参数的自回归语言模型，使用检索信息来预测下一个标记。作为检索器，使用了一个冻结的BERT模型（图[6.7](#Fig7)）。每个训练序列被分割成块，这些块通过从包含2000亿个标记的数据库中检索其*k*个最近邻来增强。返回的信息在语言模型中处理，以提高下一个标记的预测，从而带来巨大的性能提升。阅读器由一个可微分的自回归编码器和用于预测标记的块状交叉注意力模块组成。![图片](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig7_HTML.png)'
- en: A model diagram illustrates the input sequence finds the nearest neighbor embeddings
    followed by the database of 2 trillion words to encode neighbors, which further
    progresses to the autoregressive language model to generate the output.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型图说明了输入序列找到最近的邻居嵌入，然后是包含2000亿个单词的数据库来编码邻居，这进一步进展到自回归语言模型以生成输出。
- en: Fig. 6.7
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7
- en: The Retro language model retrieves information for the input sequence. The model
    uses the input sequence and the retrieved neighbor chunks from the database as
    input and generates an appropriate output [[176](#CR176)] An input sequence ***v*** = (*v*[1],
    …, *v*[*n*]) of *n*=2048 tokens is split into chunks ***c***[*t*] = (*v*[(*t*−1)∗*m*+1],
    …, *v*[*t*∗*m*]) of length *m*=64\. Each chunk ***c***[*t*] is expanded with a
    set Ret(***c***[*t*]) of retrieved *k* nearest neighbor chunks from the database.
    The probability of a token *v*[*t*∗*m*+*i*] in the next chunk ***c***[*t*+1] then
    can be recursively computed as![$$\displaystyle \begin{aligned} p(v_{t*m+i}|v_{t*m+(i-1)},\ldots,v_{t*m+1},\boldsymbol{c}_t,\text{RET}(\boldsymbol{c}_t),\ldots,
    \boldsymbol{c}_1,\text{RET}(\boldsymbol{c}_1) ) {}. \end{aligned} $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ8.png)(6.8)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Retro 语言模型检索输入序列的信息。该模型使用输入序列和从数据库中检索到的邻居块作为输入，并生成适当的输出 [[176](#CR176)] 一个长度为
    *n*=2048 令牌的输入序列 ***v*** = (*v*[1], …, *v*[*n*]) 被分割成长度为 *m*=64 的块 ***c***[*t*]
    = (*v*[(*t*−1)∗*m*+1], …, *v*[*t*∗*m*])。每个块 ***c***[*t*] 都通过从数据库中检索到的 *k* 个最近邻块集
    Ret(***c***[*t*]) 来扩展。然后，下一个块 ***c***[*t*+1] 中令牌 *v*[*t*∗*m*+*i*] 的概率可以递归地计算如下![公式](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ8.png)(6.8)
- en: The probability of the *i*-th token of the (*t* + 1)-th chunk ***c***[*t*+1]
    depends only on the previous tokens and on the data Ret(***c***[*j*]) retrieved
    from the database for the previous chunks. This integrates the retrieval process
    in the language model.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 第 (*t* + 1)-th 块的 *i*-th 令牌的概率仅取决于之前的令牌以及从数据库中检索到的之前块的数据 Ret(***c***[*j*])。这将在语言模型中整合检索过程。
- en: The retriever for a chunk ***c***[*t*] uses the average Bert(***c***[*t*]) of
    all BERT embeddings of the tokens in ***c***[*t*] as key. It retrieves the *k*
    nearest neighbors from the database with respect to the *L*[2] distance ![$$||\text{BERT}(\boldsymbol
    {c}_t)-\text{BERT}(\tilde {\boldsymbol {c}_s})||{ }_2^2$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq12.png).
    The model receives the corresponding chunks ![$$\tilde {\boldsymbol {c}}_{s,j}$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq13.png)
    and additionally their continuation chunk ![$$\tilde {\boldsymbol {c}}_{s+1,j}$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq14.png)
    for *j* = 1, …, *k*, which collectively form the elements of Ret(***c***[*t*]).
    By filtering it is avoided that the chunk to be predicted is included in Ret(***c***[*t*]),
    as this would invalidate the conditional probability definition. The retrieval
    is performed in ![$$O(\log T)$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq15.png)
    time using the *SCaNN* library [[73](#CR73)], which collects a set of chunks from
    a database of 2 trillion tokens in 10ms. Note that the document corpus of Retro
    is about 1000 times larger than the databases of FiD and other retrieval models.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 块 ***c***[*t*] 的检索器使用 ***c***[*t*] 中所有令牌的 BERT 嵌入的平均值 BERT(***c***[*t*]) 作为键。它根据
    *L*[2] 距离检索数据库中的 *k* 个最近邻 ![$$||\text{BERT}(\boldsymbol {c}_t)-\text{BERT}(\tilde
    {\boldsymbol {c}_s})||{ }_2^2$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq12.png)。模型接收相应的块
    ![$$\tilde {\boldsymbol {c}}_{s,j}$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq13.png)
    以及它们的后续块 ![$$\tilde {\boldsymbol {c}}_{s+1,j}$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq14.png)，对于
    *j* = 1, …, *k*，这些共同构成了 Ret(***c***[*t*]) 的元素。通过过滤，可以避免预测的块包含在 Ret(***c***[*t*])
    中，因为这会无效化条件概率定义。检索是在 ![$$O(\log T)$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq15.png)
    时间内使用 *SCaNN* 库 [[73](#CR73)] 完成的，该库可以在 10ms 内从包含 2 万亿个令牌的数据库中收集一组块。请注意，Retro
    的文档语料库比 FiD 和其他检索模型的数据库大 1000 倍。
- en: Inside the reader the retrieved tokens in Ret(***c***[*t*]) are fed into an
    autoencoder, which computes a set *E* of encoded neighbors. Then, so-called Retro
    blocks![$$\displaystyle \begin{aligned} \text{RETRO}(H,E) := \text{FCL}(\text{CATL}(\text{ATTL}(H),E))
    {}, \end{aligned} $$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ9.png)(6.9)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在读者内部，Ret(***c***[*t*]) 中检索到的令牌被输入到一个自动编码器中，该编码器计算一组编码邻居 *E*。然后，所谓的 Retro 块![公式](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_Equ9.png)(6.9)
- en: and standard self-attention blocks Lm(*H*) := Fcl(Attl(*H*)) are interleaved
    and operate on the intermediate embeddings ![$$H\in \mathbb {R}^{n\times d}$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq16.png).
    Here Fcl(⋅) is a fully connected layer, Attl(⋅) a self-attention layer, and Catl(⋅,
    *E*) a cross-attention layer which includes the information in *E*. The input
    and output dimension of these modules is ![$$\mathbb {R}^{n\times d}$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq17.png).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 并且标准自注意力块 Lm(*H*) := Fcl(Attl(*H*)) 交错并作用于中间嵌入 ![$$H\in \mathbb {R}^{n\times
    d}$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq16.png)。这里
    Fcl(⋅) 是一个全连接层，Attl(⋅) 是一个自注意力层，Catl(⋅, *E*) 是一个交叉注意力层，它包括 *E* 中的信息。这些模块的输入和输出维度是
    ![$$\mathbb {R}^{n\times d}$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq17.png)。
- en: The resulting language model is able to predict the next token with a high reliability.
    The *Pile data* [[62](#CR62)] is a 825GB open-source text collection set that
    consists of 22 diverse, high-quality datasets. It was screened for toxic language
    and bias, e.g. with respect to gender, religion, and race. Its authors recommend
    measuring the quality of token prediction in *bits per byte* (*bpb*), which in
    contrast to perplexity is independent of the tokenizer [[62](#CR62), p. 6]. The
    authors compare Retro with GPT-3[175B] [[29](#CR29)], Jurassic-1[178B] [[121](#CR121)],
    and Gopher[280B] [[176](#CR176)]. It turns out that Jurassic-1 has the lowest
    (and best) bpb-value on 5 Pile datasets, Gopher on 2 datasets and Retro on 9 datasets,
    although it is far smaller than the other models [[25](#CR25)]. GPT-3 was inferior
    to all three models. A possible problem for these results is the overlap of the
    retrieval corpus with the test data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 结果语言模型能够以高可靠性预测下一个标记。*Pile数据集* [[62](#CR62)] 是一个包含 22 个多样化、高质量数据集的 825GB 开源文本集合。它经过筛选，排除了有害语言和偏见，例如关于性别、宗教和种族的偏见。其作者建议用
    *每字节比特数* (*bpb*) 来衡量标记预测的质量，这与困惑度不同，它与分词器无关 [[62](#CR62)，第 6 页]。作者将 Retro 与 GPT-3[175B]
    [[29](#CR29)]、Jurassic-1[178B] [[121](#CR121)] 和 Gopher[280B] [[176](#CR176)]
    进行了比较。结果显示，Jurassic-1 在 5 个 Pile 数据集上具有最低（也是最好的）bpb 值，Gopher 在 2 个数据集上，Retro 在
    9 个数据集上，尽管它比其他模型小得多 [[25](#CR25)]。GPT-3 在这三个模型中表现较差。这些结果可能存在的一个问题是检索语料库与测试数据的重叠。
- en: 'For the *LAMBADA benchmark* [[165](#CR165)] a model has to predict the last
    word of a paragraph. The authors measure the following accuracies: Retro without
    retrieval 70%, Retro with retrieval 73%, Gopher 74.5%, and GPT-3 76.2%. Note that
    Retro has only 4% of the parameters of GPT-3\. For question answering the Natural
    Question benchmark is relevant. Here, Retro achieved an exact match accuracy of
    45.5%.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *LAMBADA 基准测试* [[165](#CR165)]，一个模型需要预测段落中的最后一个单词。作者测量了以下准确率：Retro 无检索 70%，Retro
    有检索 73%，Gopher 74.5%，和 GPT-3 76.2%。请注意，Retro 只有 GPT-3 参数的 4%。对于问答，自然问题基准是相关的。在这里，Retro
    实现了 45.5% 的精确匹配准确率。
- en: The *LaMDA* [[222](#CR222)] dialog system (Sect. [6.6.3](#Sec52)) is an expanded
    version of Retro with 137B parameters. It demonstrates that facticity can be improved
    by retrieval models. In addition, it is able to reduce toxic language by a system
    of filters that block unwanted speech. Although this model could also easily be
    used for question answering, no corresponding benchmark results are known.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*LaMDA* [[222](#CR222)] 对话系统（第 [6.6.3](#Sec52) 节）是 Retro 的扩展版本，具有 137B 个参数。它表明通过检索模型可以提高事实性。此外，它能够通过阻止不受欢迎的言论的过滤器系统来减少有害语言。尽管这个模型也可以很容易地用于问答，但尚未知道相应的基准结果。'
- en: Controlling a Search Engine by a Pre-trained Language Model
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过预训练语言模型控制搜索引擎
- en: '**WebGPT** [[149](#CR149)] extends GPT-3 to control the *Bing search engine*
    and performs a web search for a specific query. The language model must issue
    commands such as *“Search …”*, *“Find in page: …”* or *“Quote: …”*, as shown in
    Fig. [6.8](#Fig8). In this way, the model collects passages from web pages which
    contain information relevant for the question. The utilization of Bing has the
    advantage that it has powerful search capabilities, and covers a large number
    of up-to-date documents.![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig8_HTML.png)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**WebGPT** [[149](#CR149)] 将 GPT-3 扩展到控制 *Bing 搜索引擎* 并执行特定查询的网页搜索。语言模型必须发出诸如
    *“搜索 …”*、*“在页面中查找： …”* 或 *“引用： …”* 等命令，如图 [6.8](#Fig8) 所示。这样，模型从包含与问题相关的信息的网页中收集段落。使用
    Bing 的优点是它具有强大的搜索能力，覆盖了大量最新的文档。![图片](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig8_HTML.png)'
- en: A table represents a set of 10 commands and their effects. The commands are,
    search, clicked on link, find in page, quote, scroll down, scrolled up, top, back,
    and end
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 表格表示一组10个命令及其效果。这些命令包括搜索、点击链接、在页面中查找、引用、向下滚动、向上滚动、顶部、后退和结束
- en: Fig. 6.8
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8
- en: Possible actions of the WebGPT language model. If another text is generated,
    this is an invalid action and ignored [[149](#CR149)]
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: WebGPT 语言模型的可能操作。如果生成了其他文本，这将被视为无效操作并被忽略 [[149](#CR149)]
- en: Browsing continues until the model issues a command to end browsing, the maximum
    total length of references has been reached, or the maximum number of actions
    has been reached. If a relevant reference has been retrieved, the model will generate
    a long-form answer to the question.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览将继续，直到模型发出结束浏览的命令、引用的最大总长度达到、或操作的最大数量达到。如果检索到了相关引用，模型将生成一个长篇回答来回答问题。
- en: The GPT-3 model is first fine-tuned to mimic human demonstrations, enabling
    it to use the text-based browser to answer questions. Then, the usefulness and
    accuracy of the model’s answers is improved by fine-tuning a reward model to predict
    human preferences, and optimizing it by rejection sampling. Specifically the model
    is fine-tuned to answer questions from *ELI5* [[56](#CR56)], a dataset of open-ended
    questions obtained from the subreddit ‘Explain Like I’m Five’. An example is given
    in Fig. [6.9](#Fig9). The proposed WebGPT answers should be coherent, relevant,
    and supported by trustworthy documents. No details are reported on the input prompts
    of GPT-3 containing the current state of search, and how the GPT-3 model combines
    the returned documents into an answer. Note, however, that there is significant
    overlap between training and validation in ELI5, as at least 81% of ELI5 validation
    questions occur in the training set [[106](#CR106)] in circumscribed form.![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig9_HTML.png)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 模型首先经过微调以模仿人类演示，使其能够使用基于文本的浏览器来回答问题。然后，通过微调一个用于预测人类偏好的奖励模型来提高模型回答的有用性和准确性，并通过拒绝采样进行优化。具体来说，模型被微调以回答来自
    *ELI5* [[56](#CR56)] 的问答，这是一个从“Explain Like I’m Five”子版块获取的开放式问题数据集。一个示例在图 [6.9](#Fig9)
    中给出。提出的 WebGPT 回答应该是连贯的、相关的，并由可信的文件支持。没有报告关于 GPT-3 输入提示的细节，这些提示包含当前搜索状态，以及 GPT-3
    模型如何将返回的文档组合成答案。然而，请注意，在 ELI5 中训练和验证之间存在显著的重叠，因为至少 81% 的 ELI5 验证问题以限制性形式出现在训练集中
    [[106](#CR106)]。![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig9_HTML.png)
- en: A text box represents a question related to the application of contact lenses.
    There is a passage of answers at the bottom along with the citations.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 文本框表示与隐形眼镜应用相关的问题。底部有一段答案以及引用。
- en: Fig. 6.9
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9
- en: Long-form answer to a question generated by WebGPT. The best of 64 answers was
    automatically selected. The citations were automatically retrieved from the Bing
    search engine and added to the answer [[80](#CR80)]
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: WebGPT 生成的长篇回答。从 64 个答案中自动选择了最佳答案。引用自动从 Bing 搜索引擎检索并添加到答案中 [[80](#CR80)]
- en: The final answers were selected from 64 trials of the 175B WebGPT model by ranking.
    These answers were preferred by human raters to the reference responses from the
    ELI5 dataset 69% of the time. Moreover, they were preferred to the human demonstrator
    responses in 56% of the cases.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 最终答案是通过从 175B WebGPT 模型的 64 次试验中按排名选择得出的。这些答案在 69% 的情况下被人类评分者优先于 ELI5 数据集中的参考回答。此外，在
    56% 的情况下，它们被优先于人类演示者的回答。
- en: For WebGPT, responses to TruthfulQA [[125](#CR125)] were correct about 75% of
    time, whereas GPT-3 scored 64% with helpful prompts. While GPT-3’s answers were
    truthful and informative in about 20% of the time, the best version of WebGPT
    increased this to about 56%. Since people answered 94% of the questions correctly,
    the models still have a significant performance difference. On TriviaQA WEBGPT
    achieved a score of 69.5%, which is far less than the value of PaLM with 81.4%.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 WebGPT，对 TruthfulQA [[125](#CR125)] 的回答大约有 75% 的时间是正确的，而 GPT-3 在有提示的情况下得分为
    64%。虽然 GPT-3 的回答在大约 20% 的时间内是真实且信息丰富的，但 WebGPT 的最佳版本将这一比例提高到了大约 56%。由于人们正确回答了
    94% 的问题，因此模型之间仍然存在显著的性能差异。在 TriviaQA 中，WEBGPT 实现了 69.5% 的得分，这远低于 PaLM 的 81.4%。
- en: An innovative feature is the support of text passages by references. This corresponds
    to the approach of scientific papers to underpin claims by references and was
    already suggested by Metzler et al. [[143](#CR143)]. The references explain the
    answer and support the factual accuracy of the statements. The citations are selected
    by Bing in response to the query. They should therefore be close to the final
    reader-generated response and provide an easy way to assess the correctness of
    the response.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一个创新功能是支持通过参考文献来支持文本段落。这对应于科学论文通过参考文献来支持论点的方法，并且已经被Metzler等人提出[[143](#CR143)]。参考文献解释了答案并支持陈述的事实准确性。引用是由Bing针对查询选择的。因此，它们应该接近最终读者生成的响应，并提供一种简单的方法来评估响应的正确性。
- en: However, the authors point out that the references are not always representative
    for the available evidence, although the model cites references that correspond
    to the generated text. In addition, it is difficult for the model to verify the
    trustworthiness of references. Here, Web-of-Trust systems and search engine technology
    could be employed, which favor trust-checked frequently linked web pages.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，作者指出，虽然模型引用了与生成文本相对应的参考文献，但这些参考文献并不总是代表现有证据。此外，模型很难验证参考文献的可靠性。在这里，可以采用Web-of-Trust系统和技术搜索引擎，这些系统有利于经过信任检查且频繁链接的网页。
- en: Available Implementations
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: BigBird code and models are available at [https://​huggingface.​co/​google/​bigbird-roberta-base](https://huggingface.co/google/bigbird-roberta-base)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BigBird代码和模型可在[https://huggingface.co/google/bigbird-roberta-base](https://huggingface.co/google/bigbird-roberta-base)找到。
- en: DPR code and models [https://​github.​com/​facebookresearch​/​DPR](https://github.com/facebookresearch/DPR)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DPR代码和模型[https://github.com/facebookresearch/DPR](https://github.com/facebookresearch/DPR)。
- en: FiD code and models [https://​github.​com/​facebookresearch​/​FiD](https://github.com/facebookresearch/FiD)
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FiD代码和模型[https://github.com/facebookresearch/FiD](https://github.com/facebookresearch/FiD)。
- en: RealFormer code [https://​github.​com/​jaketae/​realformer](https://github.com/jaketae/realformer)
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RealFormer代码[https://github.com/jaketae/realformer](https://github.com/jaketae/realformer)。
- en: REALM code [https://​github.​com/​google-research/​language/​blob/​master/​language/​realm/​README.​md](https://github.com/google-research/language/blob/master/language/realm/README.md)
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: REALM代码[https://github.com/google-research/language/blob/master/language/realm/README.md](https://github.com/google-research/language/blob/master/language/realm/README.md)。
- en: RETRO implementation, Deepmind’s Retrieval based Attention net, in PyTorch.
    This will deviate from the paper slightly, using rotary embeddings for relative
    positional encoding, as well as FAISS library instead of SCaNN [https://​github.​com/​lucidrains/​RETRO-pytorch](https://github.com/lucidrains/RETRO-pytorch).
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RETRO实现，Deepmind的基于检索的注意力网络，在PyTorch中。这将与论文略有不同，使用旋转嵌入进行相对位置编码，以及使用FAISS库而不是SCaNN[https://github.com/lucidrains/RETRO-pytorch](https://github.com/lucidrains/RETRO-pytorch)。
- en: 6.2.4 Summary
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.4 摘要
- en: A number of Foundation Models have been presented, which were able to improve
    Question Answering performance. Examples are the autoregressive language models
    GPT-3 (175B), Gopher (175B), and PaLM (540B) with huge parameter sets, which are
    trained on a large document collections and can acquire extensive knowledge. Using
    few-shot prompts they are able to answer questions with high accuracy without
    employing external knowledge.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了许多基础模型，它们能够提高问答性能。例如，具有巨大参数集的自回归语言模型GPT-3（175B）、Gopher（175B）和PaLM（540B），它们在大型文档集合上训练，可以获取广泛的知识。使用少量提示，它们能够以高精度回答问题，而不需要使用外部知识。
- en: Recently, the retriever-reader architecture has been increasingly used for QA
    systems. It has the potential to tap into a larger knowledge base or the Internet
    that can easily be kept up-to-date. The retriever can employ keyword search or
    dense retrieval. Dense retrieval mitigates the term-mismatch problem, where relevant
    paraphrases are ignored. Usually, embeddings for each document or phrase are pre-computed
    and the embedding index is constructed beforehand. Current systems can access
    document collections of up to trillions of tokens using advanced nearest-neighbor
    search engines like FAISS and SCaNN to compare embeddings.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，检索器-阅读器架构越来越多地用于问答系统。它有可能挖掘更大的知识库或互联网，这些知识库可以轻松更新。检索器可以使用关键字搜索或密集检索。密集检索减轻了术语不匹配问题，其中相关的释义被忽略。通常，每个文档或短语的嵌入都是预先计算的，嵌入索引是事先构建的。当前系统可以使用FAISS和SCaNN等高级最近邻搜索引擎访问高达万亿个标记的文档集合，以比较嵌入。
- en: The reader usually receives the query and the returned passages in text form
    and generates the answer. It is fine-tuned to select the correct answer and to
    provide answers which are expressive and truthful. The Retro model is an autoregressive
    language model with only 7B parameters, which uses passages retrieved by a frozen
    BERT model as additional current state information to generate the next tokens.
    It is capable of improving accuracy to high levels for many QA tasks, but can
    also be used for other applications such as story generation.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 读者通常以文本形式接收查询和返回的段落，并生成答案。它经过微调以选择正确的答案，并提供表达清晰、真实的答案。Retro模型是一个只有7B参数的自回归语言模型，它使用由冻结的BERT模型检索的段落作为额外的当前状态信息来生成下一个标记。它能够将许多问答任务的准确性提高到很高水平，但也可以用于其他应用，如故事生成。
- en: WebGPT combines GPT-3 and the Bing search engine to retrieve documents and create
    appropriate answers. It is able to enhance the generated text by references to
    documents, which justify and explain the answer. The LaMDA dialog model is an
    expanded version of Retro with 137B parameters with specific tuning to increase
    usability and factual accuracy. In addition, it is able to reduce toxic language
    by a system of filters that block unwanted speech. These techniques can also be
    applied to question answering.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: WebGPT结合了GPT-3和Bing搜索引擎来检索文档并创建适当的答案。它能够通过引用文档来增强生成的文本，这些文档可以证明和解释答案。LaMDA对话模型是Retro的扩展版本，具有137B参数，并针对提高可用性和事实准确性进行了特定调整。此外，它还能够通过一个阻止不受欢迎言论的过滤系统来减少有害语言。这些技术也可以应用于问答。
- en: Still difficult is the generation of answers where the correct response needs
    information from multiple documents. In this case several rounds of querying are
    necessary. Special models like RealFormer, HYBRIDER, or AISO can improve the performance
    for benchmarks like WikiHop.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 生成需要从多个文档中获取信息的正确答案仍然是一个难题。在这种情况下，需要进行几轮查询。像RealFormer、HYBRIDER或AISO这样的特殊模型可以提高WikiHop等基准测试的性能。
- en: 6.3 Neural Machine Translation
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 神经机器翻译
- en: Language is the cornerstone of most human communication and interaction. Moreover,
    many persons think in terms of language, and use it to express and communicate
    feelings, goals, and ideas. We communicate knowledge by language and use it to
    establish social and emotional relationships. There are more than 7100 languages
    in the world [[19](#CR19)], some of which are shown in Fig. [6.10](#Fig10). The
    ability to understand each other across language barriers is essential for communicating
    ideas between people.![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig10_HTML.png)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 语言是人类沟通和互动的基石。此外，许多人用语言来思考，并用它来表达和沟通情感、目标和思想。我们通过语言来传达知识，并利用它建立社会和情感关系。世界上有超过7100种语言
    [[19](#CR19)]，其中一些在图[6.10](#Fig10)中展示。跨越语言障碍相互理解的能力对于人们在思想上的沟通至关重要！![图片](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig10_HTML.png)
- en: A world map indicates different languages used in different regions all across
    the world.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一张世界地图显示了世界上不同地区使用的不同语言。
- en: Fig. 6.10
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10
- en: This map shows some of the world’s 7100 languages, with each dot representing
    a language and the color indicating the top language family for each language.
    Only a small fraction of the world’s languages are currently represented in Foundation
    Models. Image reprinted with kind permission of the authors [[24](#CR24), p. 23]
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这张地图展示了世界上7100多种语言中的一些，每个点代表一种语言，颜色表示每种语言的顶级语系。目前，世界上只有一小部分语言在基础模型中有所代表。图片经作者同意重印
    [[24](#CR24)，第23页]
- en: After an initial success with Recurrent Neural Networks [[15](#CR15), [215](#CR215)]
    the development of the Transformer encoder-decoder (Sect. [2.​3](528393_1_En_2_Chapter.xhtml#Sec19))
    has driven progress in Neural Machine Translation (NMT). By cross-attention a
    “correlation” between each token of the source text and the translated text can
    be established, producing better translations than before. The availability of
    large training sets and better model architectures has steadily increased the
    performance of Pre-trained Language Models for NMT (Fig. [6.11](#Fig11)). Standard
    models for multilingual processing are described in Sect. [3.​3](528393_1_En_3_Chapter.xhtml#Sec12).
    A survey is provided by Yang et al. [[248](#CR248)].![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig11_HTML.png)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在Recurrent Neural Networks [[15](#CR15), [215](#CR215)]初获成功之后，Transformer编码器-解码器（第[2.3](528393_1_En_2_Chapter.xhtml#Sec19)节）的发展推动了神经机器翻译（NMT）的进步。通过交叉注意力，可以在源文本的每个标记和翻译文本之间建立“相关性”，从而产生比以前更好的翻译。大型训练集和更好的模型架构的可用性稳步提高了NMT预训练语言模型的表现（图[6.11](#Fig11)）。关于多语言处理的标准模型在第[3.3](528393_1_En_3_Chapter.xhtml#Sec12)节中描述。杨等人[[248](#CR248)]提供了一份调查。![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig11_HTML.png)
- en: An area graph represents the B L E U score of translation of different languages
    into English. The graph highlights the durations of March 2008, May 2019, and
    May 2020.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 面积图表示了将不同语言翻译成英语的BLEU分数。图表突出了2008年3月、2019年5月和2020年5月的持续时间。
- en: Fig. 6.11
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11
- en: Bleu scores for Google translation of 100+ different languages to English for
    different years. Image credits in Table [A.​2](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab3)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 不同年份Google翻译将100多种语言翻译成英语的BLEU分数。图像归功于表[A.2](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab3)。
- en: 6.3.1 Translation for a Single Language Pair
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 单语对翻译
- en: The training data of NMT consist of text pairs of the source language and its
    translations to the target language. Traditionally evaluation is done by comparing
    one or more reference translations to the proposed translation, as described in
    the survey [[195](#CR195)]. There are a number of automatic metrics like Bleu,
    Meteor or BERT-score (Sect. [2.​3.​3](528393_1_En_2_Chapter.xhtml#Sec23)). It
    turned out that there is a noticeable difference between human judgment and automatic
    evaluation. Therefore, most high-end comparisons today use human translators to
    assess the quality of translation methods.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: NMT（神经机器翻译）的训练数据由源语言及其目标语言翻译的文本对组成。传统上，评估是通过将一个或多个参考翻译与所提出的翻译进行比较来完成的，如调查[[195](#CR195)]中所述。存在许多自动指标，如Bleu、Meteor或BERT-score（第[2.3.3](528393_1_En_2_Chapter.xhtml#Sec23)节）。结果表明，人工判断和自动评估之间存在明显的差异。因此，今天大多数高端比较都使用人工翻译员来评估翻译方法的质量。
- en: At the WMT2021 Machine Translation conference, numerous teams solved benchmarks
    tests for translating English news texts from/to German, Japanese, Russian, Chinese,
    and a number of low-resource languages [[5](#CR5)]. Instead of using comparison
    statistics like Bleu, the translations of each system was evaluated by a number
    of human evaluators without showing them a reference translation. They were asked
    to rate a given translation according to how adequately it expressed the meaning
    of the corresponding source language input on an analog scale, which corresponds
    to an underlying absolute rating scale of 0–100\. As some raters could be stricter,
    the systems are ranked by a z-score, where the score is mean-centered and normalized
    per rater. Systems are grouped together according to which system significantly
    outperforms all others measured by the Wilcoxon rank-sum test. A large effort
    was spent to assess the validity of human evaluation.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在WMT2021机器翻译会议上，许多团队解决了将英语新闻文本翻译成/从德语、日语、俄语、中文以及多种低资源语言的标准测试[[5](#CR5)]。他们没有向评估者展示参考翻译，而是由多位人工评估者对每个系统的翻译进行了评估。他们被要求根据所给翻译在类比尺度上如何充分表达相应源语言输入的意义来对翻译进行评分，这对应于一个潜在的绝对评分尺度0-100。由于一些评分者可能更为严格，系统根据z分数进行排名，其中分数是按评分者平均中心化和归一化的。系统根据威尔科森秩和检验中哪个系统显著优于所有其他系统进行分组。为了评估人工评估的有效性，付出了大量努力。
- en: In total 173 submissions were received. In addition, five anonymized online
    systems were included. Further human-produced reference translations were denoted
    by “HUMAN” in all tables. Results show that almost all good systems are based
    on transformer encoder-decoders. Words are mostly encoded by the SentencePiece
    [[107](#CR107)] tokenizer (Sect. [1.​2](528393_1_En_1_Chapter.xhtml#Sec2)). A
    widely used technique is *back-translation* [[200](#CR200)]. Here a monolingual
    text is translated to the other language and then back-translated. By minimizing
    the difference to the original text, both models may be improved. Up to 500M sentences
    per language were available and could be used for back-translation, which led
    to a significant improvement in quality. In addition, ensembles are able to increase
    the performance in most cases.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 总共收到了173份提交。此外，还包含了五个匿名在线系统。所有表格中用“HUMAN”表示进一步的人工参考翻译。结果显示，几乎所有优秀的系统都是基于transformer编码器-解码器。单词大多由SentencePiece
    [[107](#CR107)] 分词器（第 [1.2](528393_1_En_1_Chapter.xhtml#Sec2) 节）编码。一种广泛使用的技术是*回译*
    [[200](#CR200)]。在这里，一种单语文本被翻译成另一种语言，然后再次翻译回原语言。通过最小化与原始文本的差异，两种模型都可能得到改进。每种语言最多有500M个句子可供使用，可用于回译，这导致了质量的显著提高。此外，集成在大多数情况下都能提高性能。
- en: The result of the best system for each language pair is shown in Table [6.4](#Tab4).
    Usually, there is a cluster of 2–5 models at the top, whose performance differences
    are not significant. The Facebook-AI model (FB) had the best results for half
    of the language pairs. In addition, the Bleu scores for the best systems automatically
    computed from n-grams are shown. As can be seen, the values are in general better
    for the translation “to English” than “from English” especially for morphology
    rich languages like Czech and German. Compared to the human reference translation,
    the best system was significantly better for three language pairs. This has already
    been discussed critically by Toral [[223](#CR223)], who decry the limited amount
    of context between sentences and the limited translation proficiency of the evaluators.Table
    6.4
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 每种语言对最佳系统的结果显示在表 [6.4](#Tab4) 中。通常，前五名有2-5个模型，其性能差异并不显著。Facebook-AI模型（FB）在半数语言对中取得了最佳结果。此外，还显示了从n-gram自动计算出的最佳系统的Bleu分数。可以看出，对于像捷克语和德语这样的形态丰富的语言，翻译成英语的值通常比翻译成英语的值要好。与人工参考翻译相比，最佳系统在三个语言对中显著优于人工翻译。这一点已经被Toral
    [[223](#CR223)] 从批判的角度进行了讨论，他批评了句子之间有限的上下文和评估者有限的翻译能力。表 6.4
- en: 'Leading systems of the WMT2021 News Translation Task. The systems are ordered
    by normalized z-score [[5](#CR5), pp. 15–19]. If either the best system or a human
    reference translation is significantly better, the value is printed in bold. Systems:
    FB: Facebook-AI, BL: Borderline, HW: HW-TSC, NV: Nvidia-NeMo, NI: NiuTrans, OB:
    Online-B, OW: Online-W, HN: HappyNewYear'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: WMT2021新闻翻译任务的前沿系统。系统按标准化z分数排序 [[5](#CR5)，第15-19页]。如果最佳系统或人工参考翻译显著优于其他系统，则该值以粗体显示。系统：FB：Facebook-AI，BL：Borderline，HW：HW-TSC，NV：Nvidia-NeMo，NI：NiuTrans，OB：Online-B，OW：Online-W，HN：HappyNewYear
- en: '| Score | Czech | German | Hausa | Icelandic | Japanese | Russian | Chinese
    |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 分数 | 捷克语 | 德语 | 豪萨语 | 冰岛语 | 日语 | 俄语 | 中文 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **To English** |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| **到英语** |'
- en: '| Best model z-score | FB **0.111** | BL **0.126** | FB 0.248 | FB 0.293 |
    HW 0.141 | NV 0.137 | NI 0.042 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 最佳模型 z 分数 | FB **0.111** | BL **0.126** | FB 0.248 | FB 0.293 | HW 0.141
    | NV 0.137 | NI 0.042 |'
- en: '| Human z-score | −  0.085 | −  0.081 |   |   |   | 0.089 | 0.019 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 人工 z 分数 | − 0.085 | − 0.081 |   |   |   | 0.089 | 0.019 |'
- en: '| Best model Bleu | 43.1 | 53.0 | 18.8 | 40.6 | 27.8 | 56.3 | 33.4 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 最佳模型 Bleu | 43.1 | 53.0 | 18.8 | 40.6 | 27.8 | 56.3 | 33.4 |'
- en: '| **From English** |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| **从英语** |'
- en: '| Best model z-score | FB 0.263 | OB **0.266** | FB 0.264 | FB 0.594 | FB 0.430
    | OW 0.277 | HN 0.284 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 最佳模型 z 分数 | FB 0.263 | OB **0.266** | FB 0.264 | FB 0.594 | FB 0.430 | OW
    0.277 | HN 0.284 |'
- en: '| Human z-score | **0.397** | 0.030 | 0.362 | **0.872** | 0.314 | 0.317 | 0.325
    |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 人工 z 分数 | **0.397** | 0.030 | 0.362 | **0.872** | 0.314 | 0.317 | 0.325 |'
- en: '| Best model Bleu | 33.6 | 31.3 | 20.4 | 30.6 | 46.9 | 45.0 | 49.2 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 最佳模型 Bleu | 33.6 | 31.3 | 20.4 | 30.6 | 46.9 | 45.0 | 49.2 |'
- en: Improved performance was reached by increasing the number of parameters. The
    Facebook model [[224](#CR224)], for instance, used a standard model of 4.7B parameters
    and a sparsely gated mixture-of-experts system with up to 128 experts. In each
    Sparsely Gated MoE layer, each token is routed to the top-2 expert feedforward
    blocks based on the score of a learned gating function. In addition, the models
    were fine-tuned with domain-specific data from the news domain. The *n*-best hypotheses
    were generated with a beam search. These were ranked with a weighted average of
    the probabilities *p*(tgt|src), *p*(src|tgt), and *p*(tgt), where src is the source
    and tgt is the target sentence.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增加参数数量，达到了改进的性能。例如，Facebook模型 [[224](#CR224)] 使用了4.7B参数的标准模型和最多128个专家的稀疏门控混合专家系统。在每个稀疏门控MoE层中，每个标记根据学习到的门控函数的分数路由到前两个专家前馈块。此外，模型还使用新闻领域的特定领域数据进行了微调。使用带搜索生成了
    *n*-best 假设。这些假设通过加权平均概率 *p*(tgt|src)，*p*(src|tgt)，和 *p*(tgt) 进行排名，其中src是源句子，tgt是目标句子。
- en: It is well-known that the translation of single sentences suffers from ambiguities
    (e.g. pronouns or homonyms), which can be resolved by considering the document
    context. In WMT2021 this is taken into account by assessing the quality of translation
    within the document context [[5](#CR5)]. As current encoder-decoder Foundation
    Models are able to consider larger contexts, this could improve translation performance
    [[141](#CR141)]. Instead of finding the most probable translation of a sentence,
    we need to generate the best translation for a given complete source document.
    While comparing sentence-level translation often does not indicate a difference
    between human and machine translation, the comparison of document-level translation
    often yields a statistically significant preference for human translations [[110](#CR110)].
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，单句翻译往往存在歧义（例如代词或同音异义词），这些歧义可以通过考虑文档上下文来解决。在WMT2021中，这是通过在文档上下文中评估翻译质量来考虑的
    [[5](#CR5)]。由于当前的编码器-解码器基础模型能够考虑更大的上下文，这可能会提高翻译性能 [[141](#CR141)]。我们不再需要找到句子最可能的翻译，而是需要为给定的完整源文档生成最佳翻译。虽然句子级别的翻译比较通常不会表明人类翻译和机器翻译之间的差异，但文档级别的翻译比较往往会对人类翻译产生统计上显著的偏好
    [[110](#CR110)]。
- en: Instead of using a Seq2seq model with extra long input sequence, **HAT** [[187](#CR187)]
    proposes a hierarchical attention transformer. The authors split the input text
    into sentences and start each sentence *i* with a specific [*BOS*[*i*]] token.
    These tokens summarize the sentence content and are connected to the other sentences
    by the usual self-attention and cross-attention. While the usual encoder-decoder
    transformer has a Bleu of 32.5 for the document translation from English to German
    on WMT2019, HAT is able to yield a SotaBleu of 34.5.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**HAT** [[187](#CR187)] 不是使用具有额外长输入序列的Seq2seq模型，而是提出了一种分层注意力转换器。作者将输入文本分割成句子，并以特定的
    [*BOS*[*i*]] 标记开始每个句子 *i*。这些标记总结了句子内容，并通过通常的自注意力机制和交叉注意力与其他句子连接。而通常的编码器-解码器转换器在WMT2019的英语到德语的文档翻译中Bleu值为32.5，HAT能够产生34.5的SotaBleu。'
- en: 6.3.2 Multilingual Translation
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 多语言翻译
- en: Usually, languages with scarce training data have a much lower translation accuracy,
    as holds for Hausa in Table [6.4](#Tab4). A recent success was the extension of
    NMT by multilinguality, which was already discussed in Sect. [3.​3](528393_1_En_3_Chapter.xhtml#Sec12).
    This led to a marked improvement of translations for languages with few resources.
    For a survey see [[48](#CR48)].
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，训练数据稀缺的语言翻译准确性要低得多，正如表 [6.4](#Tab4) 中的豪萨语所示。最近的一项成功是将NMT通过多语言扩展，这在第 [3.3](528393_1_En_3_Chapter.xhtml#Sec12)
    节中已经讨论过。这导致了资源较少的语言翻译质量的显著提高。关于调查，请参阅 [[48](#CR48)]。
- en: '**M2M** of Facebook AI [[57](#CR57)] improves translation between many languages
    by utilizing a massive corpus of 7.5B sentences covering 100 languages and thousands
    of translation directions with supervised data, created through large-scale mining.
    The model is a transformer encoder-decoder with 15B parameters. The authors add
    a special token in the encoder indicating the source language and a special token
    in the decoder indicating the target language. The transformer has 12 encoder
    and 12 decoder layers and an embedding size of 1024\. As there is a joint token
    vocabulary for all languages, the input and output embeddings are shared. To improve
    performance the authors added language-specific layers to the decoder for five
    languages. Using specific parallelization techniques they were able to train the
    model with only hundreds of GPUs.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**M2M** of Facebook AI [[57](#CR57)] 通过利用大规模的7.5B句子语料库，涵盖100种语言和数千个翻译方向，并通过大规模挖掘生成的监督数据，改进了多种语言之间的翻译。该模型是一个具有15B参数的transformer编码器-解码器。作者在编码器中添加了一个特殊标记来指示源语言，在解码器中添加了一个特殊标记来指示目标语言。transformer具有12个编码器和12个解码器层，嵌入大小为1024。由于所有语言共享一个联合标记词汇表，输入和输出嵌入是共享的。为了提高性能，作者为五种语言在解码器中添加了特定语言的层。使用特定的并行化技术，他们能够只用几百个GPU来训练模型。'
- en: Except for four language directions (En→Chinese, Chinese→En, En→Fi, En→Estonian)
    the model improved translation results on the WMT benchmarks for 1.9 Bleu points
    on average. Especially marked is the improvement for regional languages with an
    average increase of 7.6 Bleu. For resource-rich language pairs Liu et al. [[130](#CR130)]
    propose to use very deep transformers with up to 60 encoder layers and 12 decoder
    layers. They develop a simple yet effective initialization technique that stabilizes
    training and achieve Sota on WMT2014 En-Fr of 46.4 Bleu.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 除了四种语言方向（En→Chinese, Chinese→En, En→Fi, En→Estonian）之外，该模型在WMT基准测试中平均提高了1.9
    Bleu分。特别是，对于区域语言，平均提高了7.6 Bleu分。对于资源丰富的语言对，Liu等人[[130](#CR130)]建议使用深度高达60个编码器层和12个解码器层的非常深的transformer。他们开发了一种简单而有效的初始化技术，该技术稳定了训练，并在WMT2014
    En-Fr上实现了46.4 Bleu的Sota。
- en: 'Although multilingual translation has many advantages, it usually performs
    worse than specially trained bilingual models for high-resource language pairs.
    Recently Facebook [[225](#CR225)] presented a single multilingual model, which
    outperformed the best specially trained bilingual models across 10 out of 14 language
    pairs of the WMT2021 news benchmark. Facebook built two multilingual systems:
    any-to-English and English-to-any. They employed data mining techniques to identify
    translations in large web crawl data and leverage available monolingual data with
    hundreds of millions of sentences from all eight languages to maximize performance
    of MT systems. They filtered the available monolingual data to reduce the amount
    of noise, and then back-translated them with an ensemble of the strongest multilingual
    models available. The number of parameters was increased from 15B to 53B to enhance
    the model capacity.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管多语言翻译有许多优点，但它通常在资源丰富的语言对中表现不如专门训练的双语模型。最近，Facebook [[225](#CR225)] 提出了一个单一的多语言模型，在WMT2021新闻基准的14个语言对中的10个语言对上优于最佳专门训练的双语模型。Facebook构建了两个多语言系统：任意到英语和英语到任意。他们采用了数据挖掘技术来识别大型网络爬虫数据中的翻译，并利用来自所有八种语言的数亿个句子的单语数据来最大化机器翻译系统的性能。他们过滤了可用的单语数据，以减少噪声量，然后使用最强多语言模型的集成进行回译。参数数量从15B增加到53B，以增强模型容量。
- en: The Bleu scores are shown in Table [6.5](#Tab5). In comparison to the best bilingual
    models of WMT2021, the multilingual model achieves a better Bleu in 9 of 14 cases
    indicating that the additional training data from other languages supports translation.
    Only for Chinese→English there was a larger drop of 1.3 Bleu points. The authors
    also performed a human evaluation for the language pairs English→Russian and English→German.
    It turned out that there was no perceived difference between the quality of bilingual
    and multilingual translations.Table 6.5
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Bleu分数显示在表[6.5](#Tab5)中。与WMT2021的最佳双语模型相比，多语言模型在14个案例中的9个案例中实现了更好的Bleu分，这表明来自其他语言的额外训练数据支持翻译。只有对于Chinese→English，Bleu分下降了1.3分。作者还对英语→俄罗斯和英语→德国的语言对进行了人工评估。结果表明，双语和多语言翻译的质量在感知上没有差异。表6.5
- en: Bleu scores of the Facebook multilingual model and the best language pair model
    submitted to the WMT2021 news task. The numbers reported are Bleu scores on the
    final WMT2021 test set [[225](#CR225)]. The difference between the models is printed
    in bold, if the multilingual model is better
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook多语言模型和提交给WMT2021新闻任务的最佳语言对模型的Bleu分数。所报告的数字是WMT2021最终测试集上的Bleu分数[[225](#CR225)]。如果多语言模型表现更好，则模型之间的差异将以粗体显示
- en: '| Model | Czech | German | Hausa | Icelandic | Japanese | Russian | Chinese
    |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 捷克语 | 德语 | 豪萨语 | 冰岛语 | 日语 | 俄语 | 中文 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **From English** |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| **从英语翻译** |'
- en: '| FB-Mult | 36.1 | 31.3 | 20.1 | 33.3 | 46.8 | 46.0 | 49.9 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| FB-Mult | 36.1 | 31.3 | 20.1 | 33.3 | 46.8 | 46.0 | 49.9 |'
- en: '| WMT2021 best | 33.6 | 31.3 | 20.4 | 30.6 | 46.9 | 45.0 | 49.2 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| WMT2021最佳 | 33.6 | 31.3 | 20.4 | 30.6 | 46.9 | 45.0 | 49.2 |'
- en: '| Difference | **2**.**5** | 0.0 | -0.3 | **2**.**7** | − 0.1 | **1**.**0**
    | **0**.**7** |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 差异 | **2**.**5** | 0.0 | -0.3 | **2**.**7** | − 0.1 | **1**.**0** | **0**.**7**
    |'
- en: '| **To English** |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| **翻译成英语** |'
- en: '| FB-Mult | 43.5 | 53.3 | 21.0 | 41.7 | 27.7 | 57.1 | 32.1 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| FB-Mult | 43.5 | 53.3 | 21.0 | 41.7 | 27.7 | 57.1 | 32.1 |'
- en: '| WMT2021 best | 43.1 | 53.0 | 18.8 | 40.6 | 27.8 | 56.3 | 33.4 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| WMT2021最佳 | 43.1 | 53.0 | 18.8 | 40.6 | 27.8 | 56.3 | 33.4 |'
- en: '| Difference | **0**.**4** | **0**.**3** | **2**.**1** | **1**.**1** | − 0.1
    | **0**.**8** | − 1.3 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 差异 | **0**.**4** | **0**.**3** | **2**.**1** | **1**.**1** | − 0.1 | **0**.**8**
    | − 1.3 |'
- en: Table [6.6](#Tab6) shows the effect of employed improvement strategies for the
    different languages of the multilingual model. Back-translation has a large effect
    for languages with little training data like Hausa and Icelandic. The authors
    note, however that back-translation produces *translationese* by generating artificial
    uncommon phrases in a language. These effects may be mitigated by fine-tuning
    on the specific domain, e.g. news texts. This yields about 3 Bleu points for translation
    into English and 0.7 Bleu points for translation out of English. Switching to
    the multilingual model generates an improvement for all models. While the effect
    of model ensembles is minor, re-ranking the BEAM translations with conditional
    target-source probabilities yields about 0.4 Bleu points. Postprocessing (for
    example applying standard punctuation rules) can have a large effect, e.g. 5 Bleu
    points for Chinese.Table 6.6
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.6显示了用于多语言模型不同语言的改进策略的效果。反向翻译对于训练数据较少的语言（如豪萨语和冰岛语）有显著影响。然而，作者指出，反向翻译通过生成语言中的不常见短语而产生*翻译腔*。这些影响可以通过在特定领域（例如新闻文本）上进行微调来减轻。这为英语翻译提供了大约3个Bleu分数，为英语以外的翻译提供了0.7个Bleu分数。切换到多语言模型对所有模型都产生了改进。虽然模型集成的效果较小，但使用条件目标源概率重新排序BEAM翻译可以获得大约0.4个Bleu分数。后处理（例如应用标准标点规则）可以产生很大的影响，例如中文可以获得5个Bleu分数。表6.6
- en: Influence of different modeling improvements on the Bleu scores on the development
    set of WMT2021 for Facebook AI’s WMT2021 submission [[225](#CR225)]. The version
    of the last row was submitted
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook AI的WMT2021提交的开发集上不同建模改进对Bleu分数的影响[[225](#CR225)]。最后一行的版本已提交
- en: '| Improvement strategy | Czech | German | Hausa | Icelandic | Japanese | Russian
    | Chinese |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 改进策略 | 捷克语 | 德语 | 豪萨语 | 冰岛语 | 日语 | 俄语 | 中文 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Bilingual | 33.1 | 38.7 | 14.7 | 25.8 | 25.4 | 25.8 | 40.0 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 双语 | 33.1 | 38.7 | 14.7 | 25.8 | 25.4 | 25.8 | 40.0 |'
- en: '| + Back-translation | 33.1 | 39.6 | 23.1 | 29.4 | 26.1 | 25.7 | 42.4 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| + 反向翻译 | 33.1 | 39.6 | 23.1 | 29.4 | 26.1 | 25.7 | 42.4 |'
- en: '| + Fine-tuning | 35.7 | 39.5 | 23.3 | 29.4 | 27.7 | 26.0 | 43.0 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| + 微调 | 35.7 | 39.5 | 23.3 | 29.4 | 27.7 | 26.0 | 43.0 |'
- en: '| + Multilingual | 36.4 | 40.8 | 24.6 | 31.2 | 29.7 | 26.8 | 43.6 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| + 多语言 | 36.4 | 40.8 | 24.6 | 31.2 | 29.7 | 26.8 | 43.6 |'
- en: '| + Ensemble | 36.8 | 41.1 | 25.0 | 32.5 | 29.7 | 26.9 | 43.6 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| + 集成 | 36.8 | 41.1 | 25.0 | 32.5 | 29.7 | 26.9 | 43.6 |'
- en: '| + Reranking | 37.2 | 41.1 | 25.5 | 32.8 | 29.7 | 27.4 | 43.6 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| + 重新排序 | 37.2 | 41.1 | 25.5 | 32.8 | 29.7 | 27.4 | 43.6 |'
- en: '| + Postprocessing | 39.8 | 42.6 | 25.5 | 34.5 | 29.8 | 28.8 | 48.2 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| + 后处理 | 39.8 | 42.6 | 25.5 | 34.5 | 29.8 | 28.8 | 48.2 |'
- en: The **PaLM** autoregressive language model with 540B parameters [[43](#CR43)]
    has about 22% non-English training texts among its 780B training tokens (Sect.
    [3.​1.​2](528393_1_En_3_Chapter.xhtml#Sec3)). Similar to other large LMs, PaLM
    is not trained explicitly on parallel text, although some such data is likely
    to exist naturally in the training corpus. In Table [6.7](#Tab7) the results of
    PaLM 540B few-shot translation is compared with prior few-shot and fine-tuned
    Sota [[43](#CR43), p. 27]. The best Bleu value per language pair is underlined
    and the best few-shot Bleu is printed in bold. The table shows that PaLM on the
    traditional WMT translation pairs always achieves the best few-shot Bleu, often
    improving by a wide margin. For the low-resource language Kazakh (kk) the fine-tuned
    translation models have a better Bleu than PaLM. However, for de→en and ro→en
    PaLM is able to outperform the supervised models. In addition, the 0-shot PaLM
    translation of fr→en achieves a Bleu value of 25.2, which is better than the fine-tuned
    Sota of 24.9\. Overall, PaLM performs well close to the fine-tuned models without
    having been trained for this task.Table 6.7
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**PaLM** 自回归语言模型，拥有 540B 个参数 [[43](#CR43)]，在其 780B 个训练标记中大约有 22% 的非英语训练文本（第
    [3.1.2](528393_1_En_3_Chapter.xhtml#Sec3) 节）。与其他大型语言模型类似，PaLM 并未明确在并行文本上进行训练，尽管训练语料库中可能自然存在一些此类数据。在表
    [6.7](#Tab7) 中，PaLM 540B 的少样本翻译结果与之前的少样本和微调 Sota 进行了比较 [[43](#CR43)，第 27 页]。每个语言对的最佳
    Bleu 值用下划线标出，最佳少样本 Bleu 值用粗体打印。表格显示，PaLM 在传统的 WMT 翻译对上总是实现最佳少样本 Bleu，通常大幅提升。对于低资源语言哈萨克语（kk），微调的翻译模型比
    PaLM 的 Bleu 值更好。然而，对于 de→en 和 ro→en，PaLM 能够超越监督模型。此外，0-shot PaLM 的 fr→en 翻译 Bleu
    值达到 25.2，优于 24.9 的微调 Sota。总的来说，PaLM 在没有为此任务进行训练的情况下，表现接近微调模型。表 6.7'
- en: Comparison of PaLM few-shot translation performance against prior fine-tuned
    translation performance by specialized models and prior few-shot performance.
    On the left you find the translation from English and into English for the traditional
    WMT language pairs. On the right there is the translation to and from English
    to Kazakh (kk) and a translation between German and French [[43](#CR43), p. 27]
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 将 PaLM 的少样本翻译性能与之前由专业模型微调的翻译性能以及之前的少样本性能进行比较。在左侧，您可以看到对于传统 WMT 语言对的英译英翻译。在右侧，有从英语到哈萨克语（kk）以及德语和法语之间的翻译
    [[43](#CR43)，第 27 页]
- en: '| From | en | en | en | fr | de | ro | en | de | kk | fr |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 从 | en | en | en | fr | de | ro | en | de | kk | fr |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| To | fr | de | ro | en | en | en | kk | fr | en | de |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 到 | fr | de | ro | en | en | en | kk | fr | en | de |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Prior fine-tuned Sota | 45.6 | 41.2 | 33.4 | 45.4 | 41.2 | 39.1 | 15.5 |
    31.5 | 30.5 | 24.9 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 之前的微调 Sota | 45.6 | 41.2 | 33.4 | 45.4 | 41.2 | 39.1 | 15.5 | 31.5 | 30.5
    | 24.9 |'
- en: '| Prior few-shot Sota | 33.9 | 26.8 | 20.5 | 38.8 | 40.6 | 37.3 | – | – | –
    | – |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 之前的少样本 Sota | 33.9 | 26.8 | 20.5 | 38.8 | 40.6 | 37.3 | – | – | – | – |'
- en: '| PaLM 540B few-shot | **44.0** | **37.4** | **28.7** | **42.8** | **47.5**
    | **43.8** | 5.1 | 25.7 | 20.8 | 17.4 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| PaLM 540B 少样本 | **44.0** | **37.4** | **28.7** | **42.8** | **47.5** | **43.8**
    | 5.1 | 25.7 | 20.8 | 17.4 |'
- en: 6.3.3 Multilingual Question Answering
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 多语言问答
- en: In recent years open domain question answering (ODQA) has taken a rapid development
    (Sect. [6.2](#Sec9)). Therefore, it is extremely rewarding to extend these techniques
    to multilingual question answering. In this way, information encoded with the
    world’s different languages can be tapped and the digital divide can be narrowed
    by bringing answers to people who speak rarer languages. There is a tutorial on
    multilingual ODQA by Ruder [[192](#CR192), [193](#CR193)].
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，开放域问答（ODQA）取得了快速发展（第 [6.2](#Sec9) 节）。因此，将这些技术扩展到多语言问答是非常有价值的。通过这种方式，可以挖掘用世界不同语言编码的信息，并通过将答案带给说稀有语言的人来缩小数字鸿沟。Ruder
    提供了一篇关于多语言 ODQA 的教程 [[192](#CR192)，[193](#CR193)]。
- en: A simple way to perform multilingual ODQA is to translate the question to English,
    use an English ODQA system to generate an answer, and translate the answer back
    to the target language. Because of ambiguities in translation, this procedure
    may generate errors in some cases [[132](#CR132)]. In addition, information specific
    to the target language and conceptualizations of the target culture may not be
    available in English [[258](#CR258)].
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 执行多语言ODQA的一个简单方法是将问题翻译成英语，使用英语ODQA系统生成答案，然后将答案翻译回目标语言。由于翻译中的歧义，这个过程在某些情况下可能会产生错误[[132](#CR132)]。此外，特定于目标语言和目标文化概念的信息可能不在英语中[[258](#CR258)]。
- en: 'The *TyDiQA-GoldP benchmark* [[44](#CR44)] is a question answering dataset
    covering 11 typologically different languages with 204K question-answer pairs.
    The following languages are included: English, Arabic, Bengali, Finnish, Indonesian,
    Japanese, Kiswahili, Korean, Russian, Telugu, Thai. As the languages represented
    in this benchmarks have a very diverse structure, a model which performs well
    on this data can be expected to have a good QA-accuracy on other languages. *MKQA*
    [[133](#CR133)] is an evaluation dataset created by translating 10k Natural Questions
    [[109](#CR109)] to 25 target languages.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '**TyDiQA-GoldP基准** [[44](#CR44)] 是一个包含11种类型不同语言的问答数据集，有204K个问题-答案对。以下语言包括：英语、阿拉伯语、孟加拉语、芬兰语、印度尼西亚语、日语、斯瓦希里语、韩语、俄语、泰卢固语、泰语。由于这个基准中代表的语言结构非常多样化，因此可以预期，在这个数据上表现良好的模型在其他语言上也会有很好的问答准确率。*MKQA*
    [[133](#CR133)] 是一个评估数据集，通过将10k Natural Questions [[109](#CR109)] 翻译成25种目标语言创建的。'
- en: As an alternative, one can train cross-lingual retriever and reader models combining
    the information from multiple languages to generate an answer in the target language
    (Fig. [6.12](#Fig12)). **CORA** [[13](#CR13)] answers questions across many languages,
    even for ones without language-specific annotated data or knowledge sources. It
    includes a dense passage retriever collecting documents with different languages
    for a question. A pre-trained multilingual language model *mDPR* using mBERT (Sect.
    [3.​3.​1](528393_1_En_3_Chapter.xhtml#Sec13)) is fine-tuned to encode passages
    and questions separately. By performing a maximum inner product search the top
    *k* documents are retrieved similar to DPR (Sect. [3.​4.​5](528393_1_En_3_Chapter.xhtml#Sec22)).
    It could be shown that mBERT improves the search quality in non-English mono-lingual
    retrieval [[203](#CR203)]. The reader *mGEN* is a multilingual autoregressive
    sequence model (e.g. mT5, Sect. [3.​3.​2](528393_1_En_3_Chapter.xhtml#Sec14))
    generating the answer in the target language by compiling the information in the
    retrieved passages. No specific translation models are used. The initial training
    data is a combination of multilingual QA datasets. Each training instance from
    these datasets comprises a question, a positive passage, and an answer. However,
    these datasets suffer from limitations on language diversity. Therefore, the authors
    iteratively generate more representative training data for low-resource languages
    by exploiting links between Wikipedia articles in different languages.![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig12_HTML.png)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种替代方案，可以训练跨语言检索器和阅读器模型，结合多种语言的信息，在目标语言中生成答案（图[6.12](#Fig12)）。**CORA** [[13](#CR13)]
    在许多语言中回答问题，甚至对于没有特定语言标注数据或知识来源的语言也是如此。它包括一个密集的段落检索器，为问题收集不同语言的文档。使用mBERT（第[3.3.1](528393_1_En_3_Chapter.xhtml#Sec13)节）预训练的多语言语言模型*mDPR*被微调以分别编码段落和问题。通过执行最大内积搜索，检索到最相似的顶部*k*个文档，类似于DPR（第[3.4.5](528393_1_En_3_Chapter.xhtml#Sec22)节）。可以证明mBERT在非英语单语检索中提高了搜索质量[[203](#CR203)]。阅读器*mGEN*是一个多语言自回归序列模型（例如mT5，第[3.3.2](528393_1_En_3_Chapter.xhtml#Sec14)节），通过编译检索到的段落中的信息在目标语言中生成答案。没有使用特定的翻译模型。初始训练数据是多种语言问答数据集的组合。这些数据集中的每个训练实例都包含一个问题、一个正例段落和一个答案。然而，这些数据集在语言多样性方面存在局限性。因此，作者通过利用不同语言维基百科文章之间的链接，迭代地生成更多代表性训练数据，以解决低资源语言的问题。![图6.12](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig12_HTML.png)
- en: An illustration represents a set of 2 queries, one in French and the other in
    Norwegian, that go through the retrieved documents to generate the answers. The
    generated answer for the French question is wrong, while for the Norwegian question
    is right.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 一幅插图展示了两组查询，一组用法语，另一组用挪威语，它们通过检索到的文档来生成答案。对于法语问题生成的答案是错误的，而对于挪威语问题则是正确的。
- en: Fig. 6.12
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12
- en: Cross-lingual retrieval by mDPR and answer generation with mGEN for the CORA
    system [[13](#CR13), p. 9]. The answers to the questions are correct, however,
    on the left side the answer should have been given in French
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CORA 系统通过 mDPR 进行跨语言检索和通过 mGEN 进行答案生成 [[13](#CR13), p. 9]。问题的答案都是正确的，然而，在左侧的答案本应使用法语给出
- en: It turns out that CORA substantially outperforms the previous Sota on multilingual
    open QA benchmarks across 26 languages, 9 of which are unseen during training.
    Here CORA can improve the average F1-value from 17.1 to 21.8\. Retrieval with
    mDPR performs well in Indo-European languages with Latin script, even when the
    language is unseen. There is a major drop for languages with non-Latin script
    (e.g., Japanese, Russian, Chinese). Here, perhaps, the model is unable to use
    relevant passages from other languages to answer questions.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，CORA 在 26 种语言的跨语言开放问答基准测试中显著优于之前的 Sota，其中 9 种语言在训练期间未见。在这里，CORA 可以将平均 F1
    值从 17.1 提高到 21.8。在具有拉丁字母的印欧语言中使用 mDPR 进行检索表现良好，即使语言在训练期间未见。对于非拉丁字母语言（例如，日语、俄语、中文），存在重大下降。在这里，也许模型无法使用其他语言的相关段落来回答问题。
- en: '**mT5** (Sect. [3.​3.​2](528393_1_En_3_Chapter.xhtml#Sec14)) is a multilingual
    version of the T5 Seq2seq transformer with up to 13B parameters [[246](#CR246)].
    It was pre-trained using a training dataset of web pages covering 101 languages
    with about 48B tokens and a common vocabulary of 250k tokens. After fine-tuning
    on the TyDiQA benchmark, it arrives at an exact match score of 79.1%. **ByT5**
    [[245](#CR245)] is a variation of the mT5 multilingual encoder-decoder with 12.9B
    parameters. It operates on utf-8 bytes with a vocabulary of 256 possible byte
    values instead of tokens. The model is pre-trained to replace corrupted spans
    of 20 bytes on average. The largest model uses 36 encoder and 12 decoder layers.
    When the model is fine-tuned on gold data in all target languages, it achieves
    an exact match score of 81.4% on the TyDiQA benchmark.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '**mT5** (Sect. [3.3.2](528393_1_En_3_Chapter.xhtml#Sec14)) 是 T5 Seq2seq Transformer
    的多语言版本，参数量高达 13B [[246](#CR246)]。它使用包含 101 种语言的网页训练数据集进行预训练，数据集包含约 48B 个标记和一个
    25 万个标记的通用词汇表。在 TyDiQA 基准测试上进行微调后，其精确匹配分数达到 79.1%。**ByT5** [[245](#CR245)] 是 mT5
    多语言编码器-解码器的变体，参数量为 12.9B。它使用 utf-8 字节进行操作，具有 256 个可能的字节值词汇表，而不是标记。该模型预训练用于替换平均
    20 个字节的损坏跨度。最大的模型使用 36 个编码器和 12 个解码器层。当模型在所有目标语言的黄金数据上进行微调时，在 TyDiQA 基准测试上达到的精确匹配分数为
    81.4%。'
- en: The **PaLM** Foundation Model [[43](#CR43)] has about 22% non-English training
    texts in its 780B training tokens (Sect. [3.​1.​2](528393_1_En_3_Chapter.xhtml#Sec3)).
    Therefore, it can be applied to multilingual tasks such as translation and question
    answering. With few-shot prompts it gets an exact match score on TyDiQA of 60.5%.
    When the model is fine-tuned on TyDiQA, the score grows to 80.0%, which is slightly
    below of the performance of ByT5 XXL. The detailed results in Table [6.8](#Tab8)
    show the performance for different languages. Here PaLM has a better score for
    two languages than ByT5\. The authors remark, that ByT5 was trained with 50% more
    non-English text compared to PaLM, which may explain the difference.Table 6.8
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '**PaLM** 基础模型 [[43](#CR43)] 在其 780B 训练标记中有大约 22% 的非英语训练文本（Sect. [3.1.2](528393_1_En_3_Chapter.xhtml#Sec3)）。因此，它可以应用于多语言任务，如翻译和问答。使用少量提示，它在
    TyDiQA 上获得 60.5% 的精确匹配分数。当模型在 TyDiQA 上进行微调时，分数增长到 80.0%，略低于 ByT5 XXL 的性能。表 [6.8](#Tab8)
    中的详细结果显示了不同语言的性能。在这里，PaLM 在两种语言上的得分优于 ByT5。作者指出，与 PaLM 相比，ByT5 使用了 50% 的更多非英语文本，这可能是差异的原因。表
    6.8'
- en: Comparison against Sota on TyDiQA question answering benchmark with 11 typologically
    different languages. The values are for the validation set with respect to the
    exact match accuracy [[43](#CR43), p. 32]. Best values for each language printed
    in bold
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TyDiQA 问答基准测试中与 Sota 的比较，涉及 11 种不同语法的语言。这些值是针对验证集的精确匹配准确率 [[43](#CR43), p.
    32]。每个语言的最高值以粗体显示
- en: '| Model | Ar | Bn | En | Fi | Id | Ko | Ru | Sw | Te | Avg |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Ar | Bn | En | Fi | Id | Ko | Ru | Sw | Te | Avg |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| mT5 XXL | 76.9 | 80.5 | 75.5 | 76.3 | 81.8 | 75.7 | 76.8 | 84.4 | 83.9 |
    79.1 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| mT5 XXL | 76.9 | 80.5 | 75.5 | 76.3 | 81.8 | 75.7 | 76.8 | 84.4 | 83.9 |
    79.1 |'
- en: '| ByT5 XXL | **80.0** | **85.0** | **77.7** | 78.8 | **85.7** | **78.3** |
    **78.2** | 84.0 | **85.5** | **81.4** |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| ByT5 XXL | **80.0** | **85.0** | **77.7** | 78.8 | **85.7** | **78.3** |
    **78.2** | 84.0 | **85.5** | **81.4** |'
- en: '| PaLM 540B fine-tuned | 75.0 | 83.2 | 75.5 | **78.9** | 84.1 | 75.7 | 77.1
    | **85.2** | 84.9 | 80.0 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| PaLM 540B 微调 | 75.0 | 83.2 | 75.5 | **78.9** | 84.1 | 75.7 | 77.1 | **85.2**
    | 84.9 | 80.0 |'
- en: '| PaLM 540B few-shot | 56.4 | 54.0 | 65.5 | 66.4 | 69.2 | 63.8 | 46.8 | 75.6
    | 46.9 | 60.5 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| PaLM 540B 少样本 | 56.4 | 54.0 | 65.5 | 66.4 | 69.2 | 63.8 | 46.8 | 75.6 | 46.9
    | 60.5 |'
- en: Available Implementations
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: Hugging Face provides Marian, BART and T5 (up to 11B parameters) as well as
    multilingual mBART and mT5 implementations and trained models [https://​huggingface.​co/​transformers/​](https://huggingface.co/transformers/).
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face 提供了Marian、BART和T5（高达11B参数）以及多语言mBART和mT5实现和训练模型 [https://huggingface.co/transformers/](https://huggingface.co/transformers/)。
- en: The M2M-100 [[55](#CR55)] is available with open-source data collection scripts,
    model code and parameters of trained models. In addition, the Fairseq system [https://​github.​com/​pytorch/​fairseq](https://github.com/pytorch/fairseq)
    can freely be used.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M2M-100 [[55](#CR55)] 可用，附带开源数据收集脚本、模型代码和训练模型参数。此外，Fairseq系统 [https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq)
    可免费使用。
- en: The CORA [[13](#CR13)] implementation of multilingual QA, generated training
    data and trained models are available at [https://​github.​com/​AkariAsai/​CORA](https://github.com/AkariAsai/CORA).
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CORA [[13](#CR13)] 的多语言问答实现，生成的训练数据和训练模型可在 [https://github.com/AkariAsai/CORA](https://github.com/AkariAsai/CORA)
    获取。
- en: 6.3.4 Summary
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.4 摘要
- en: In recent years, machine translation has taken a dramatic development. The use
    of encoder-decoder PLMs could overcome the limitations of RNN architectures and
    increase the performance to near-human levels. Besides the utilization of encoder-decoder
    Transformers, the availability of high-quality training examples by web crawlers
    using Foundation Models and specific assessment procedures is a reason for progress
    [[33](#CR33)]. A further improvement resulted from sentence back-translation,
    which particularly increases results for low-resource languages, and from training
    a single multilingual model for translation between all languages. Training multilingual
    translation models with up to 600B parameters—using appropriate parallelization
    strategies—leads to significant performance increase for 100 languages, as measured
    by Bleu [[113](#CR113)]. Recently multilingual models even were able to outperform
    high-resource bilingual translation models. This is also demonstrated by the PaLM
    Foundation Model, which achieved higher performance in few-shot translation than
    the prior fine-tuned models for some language pairs. Therefore, multilingual models
    are likely to become standard in the future. However, current multilingual models
    using unsupervised multilingual training may not deeply model the subtleties of
    languages and language varieties to their full extent. This has to be checked
    in future applications.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，机器翻译取得了显著的发展。使用编码器-解码器预训练模型（PLM）可以克服RNN架构的限制，并将性能提升到接近人类水平。除了使用编码器-解码器Transformer之外，通过使用基础模型和特定评估程序进行网络爬虫获取的高质量训练示例也是进步的原因之一
    [[33](#CR33)]。进一步改进来自于句子回译，这特别提高了低资源语言的翻译结果，以及训练一个用于所有语言之间翻译的单语言模型。使用适当的并行化策略训练多达600B参数的多语言翻译模型——根据Bleu
    [[113](#CR113)] 测量——对于100种语言来说，性能显著提升。最近，多语言模型甚至能够超越高资源双语翻译模型。这一点也由PaLM基础模型所证明，它在某些语言对上的少样本翻译性能高于先前微调的模型。因此，多语言模型很可能会在未来成为标准。然而，当前使用无监督多语言训练的多语言模型可能无法充分建模语言及其变体的细微差别。这需要在未来的应用中进行检查。
- en: The developments opened up the opportunity for multilingual question answering
    systems, e.g. CORA, where queries can be posed in a large number of languages.
    The answers are compiled from information available in multiple languages. In
    this way, cultural characteristics and concepts that are not available in all
    languages can be taken into account. There are also close links to cross-lingual
    semantic parsing, where a natural language utterance is translated to a logical
    form for execution in some knowledge base to return an answer [[202](#CR202)].
    Again the PaLM Foundation Model provided few-shot answers to multilingual questions,
    which are competitive in accuracy to fine-tuned models for the same benchmarks.
    A fine-tuned version of PaLM is even able to outperform prior fined-tuned Sota
    for two languages.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发展为多语言问答系统打开了机会，例如CORA，其中查询可以用多种语言提出。答案是从多种语言中可用的信息中汇编而成的。这样，就可以考虑那些在所有语言中不可用的文化特征和概念。这还与跨语言语义解析有密切联系，其中自然语言表述被翻译成逻辑形式，以便在某个知识库中执行并返回答案
    [[202](#CR202)]。再次，PaLM基础模型为多语言问题提供了少量样本答案，其准确性可与针对相同基准的微调模型相媲美。PaLM的微调版本甚至能够超越先前针对两种语言的微调Sota。
- en: However, machine translation is not yet solved. There is still the problem of
    domain mismatch between train and test data. In some cases, it fails to accurately
    capture the meaning of a sentence. Systems can generate biased text, e.g. if gender
    is handled differently in different languages. But attention allows the decoder
    to look directly at faraway text and provides a soft alignment between words for
    free. Recently, performance could be increased by translating entire documents,
    as sentences often are not sufficient to disambiguate all words. To extend current
    multilingual models to thousands of languages, new techniques are required [[19](#CR19)].
    One approach is to use monolingual datasets to improve translation, since the
    amount of available monolingual text is orders of magnitude greater than the amount
    of translated text. This in addition requires highly reliable language detectors
    which also work for low-resource languages.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，机器翻译尚未解决。仍然存在训练数据和测试数据之间的领域不匹配问题。在某些情况下，它无法准确捕捉句子的含义。系统可以生成有偏见的文本，例如，如果不同语言中处理性别的方式不同。但是，注意力机制允许解码器直接查看远处的文本，并且免费提供单词之间的软对齐。最近，通过翻译整个文档，性能得到了提高，因为句子通常不足以区分所有单词。为了将当前的跨语言模型扩展到数千种语言，需要新的技术
    [[19](#CR19)]。一种方法是通过使用单语语料库来改进翻译，因为可用的单语文本量比翻译文本量大得多。这还要求有高度可靠的语言检测器，这些检测器也适用于低资源语言。
- en: 6.4 Text Summarization
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 文本摘要
- en: With the rapid increase of textual information in companies and on the Internet,
    it is increasingly difficult for people to keep track of a topic. Automatic *summarization*
    of documents, which compiles the essential statements from a text, can help to
    grasp the most relevant information in the documents. A *summary* is a short version
    produced from a single document or multiple documents conveying the main points
    of the original texts. The purpose of automatic text summarization is to create
    a *summarizer* method to produce this summary efficiently and precisely. Recent
    in-depth surveys are provided by Ma et al. [[135](#CR135)], Guan et al. [[71](#CR71)],
    Syed et al. [[216](#CR216)], and El-Kassas et al. [[95](#CR95)].
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 随着公司和互联网上文本信息的快速增加，人们越来越难以跟踪一个主题。自动*摘要*文档，从文本中编译出基本陈述，可以帮助抓住文档中最相关的信息。*摘要*是从单个文档或多个文档中产生的简短版本，传达了原始文本的主要观点。自动文本摘要的目的是创建一个*摘要器*方法，以高效且精确地生成这个摘要。最近，Ma等人
    [[135](#CR135)]、Guan等人 [[71](#CR71)]、Syed等人 [[216](#CR216)] 和 El-Kassas等人 [[95](#CR95)]
    提供了深入的综述。
- en: Earlier machine learning approaches produced *extractive summaries* selecting
    a few sentences from the document. This approach typically selected grammatically
    correct sentence parts, but the language style of the combined parts and the coverage
    were usually not sufficient. Modern summarizers pose summarization as a translation
    problem, which translates the original document to a short version covering the
    main points. Since 2017 the encoder-decoder transformer (Sect. [2.​3](528393_1_En_2_Chapter.xhtml#Sec19))
    provided an effective technique to generate *abstractive summaries* containing
    the main points of the document. Abstractive summarization is a bit more complex
    because the text is paraphrased, and the summary usually has words different from
    the original document. On the other hand, it is more flexible and can aggregate
    several similar texts expressing related facts with different wordings.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的机器学习方法产生了*提取式摘要*，从文档中选择几个句子。这种方法通常选择语法正确的句子部分，但组合部分的文体和覆盖范围通常不足。现代摘要生成器将摘要视为一个翻译问题，它将原始文档翻译成一个简短的版本，涵盖主要观点。自2017年以来，编码器-解码器转换器（Sect.
    [2.3](528393_1_En_2_Chapter.xhtml#Sec19)）提供了一种有效的方法来生成包含文档主要观点的*抽象式摘要*。抽象式摘要稍微复杂一些，因为文本被改写，摘要通常包含与原文不同的词汇。另一方面，它更加灵活，可以汇总几个用不同措辞表达相关事实的相似文本。
- en: Basically, summarization is treated as a translation task, where the long document
    is translated into the short summary. Alternatively we can use the long document
    as the start text of an autoregressive Foundation Model, which is fine-tuned to
    generate a summary. One of the main challenges for Seq2seq models is that the
    decoder needs to attend to encoder token embeddings in the large document context
    to predict the next token of the summary. Therefore, Seq2seq models covering a
    long input context (Sect. [3.​2](528393_1_En_3_Chapter.xhtml#Sec7)) are natural
    candidates. Summarization systems can be either *single document summarizers*
    or *multi-document summarizers*. Table [6.9](#Tab9) lists popular summarization
    models and their performance.Table 6.9
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，摘要被视为一个翻译任务，将长文档翻译成简短的摘要。或者，我们可以将长文档作为自回归基础模型（Foundation Model）的起始文本，该模型经过微调以生成摘要。Seq2seq模型的主要挑战之一是解码器需要关注大文档上下文中的编码器标记嵌入以预测摘要的下一个标记。因此，覆盖长输入上下文的Seq2seq模型（Sect.
    [3.2](528393_1_En_3_Chapter.xhtml#Sec7)）是自然的选择。摘要系统可以是*单文档摘要生成器*或*多文档摘要生成器*。表[6.9](#Tab9)列出了流行的摘要模型及其性能。表6.9
- en: 'Summarization models with their performance measured in Rouge-2. Benchmarks
    are CNN/DM: CNN/Daily Mail benchmark [[78](#CR78)], XSum [[151](#CR151)] summarize
    an news article in a single sentence, arXiv [[46](#CR46)] long scientific documents,
    PubMed [[46](#CR46)] long medical documents, Multi-News [[54](#CR54)] with an
    average document length of 1793 and 2.8 documents per cluster'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 以Rouge-2衡量性能的摘要模型。基准测试包括CNN/DM：CNN/Daily Mail基准[[78](#CR78)]，XSum [[151](#CR151)]将新闻文章总结为单句，arXiv
    [[46](#CR46)]长篇科学文档，PubMed [[46](#CR46)]长篇医学文档，Multi-News [[54](#CR54)]平均文档长度为1793，每个簇有2.8个文档
- en: '| Model | Details | Rouge-2 on benchmark |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 详细信息 | 基准测试上的Rouge-2 |'
- en: '| --- | --- | --- |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| PEGASUS (Sect. [6.4.1](#Sec26)) | Seq2seq model pre-trained with masked sentences
    | CNN/DM 21.7, XSum 24.6 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| PEGASUS (Sect. [6.4.1](#Sec26)) | 使用掩码句子预训练的Seq2seq模型 | CNN/DM 21.7, XSum
    24.6 |'
- en: '| BRIO (Sect. [6.4.1](#Sec26)) | GPT architecture trained to generate text
    spans | CNN/DM 23.6, XSum 25.6 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| BRIO (Sect. [6.4.1](#Sec26)) | 训练生成文本片段的GPT架构 | CNN/DM 23.6, XSum 25.6 |'
- en: '| PaLM (Sect. [6.4.1](#Sec26)) | 540B large LM to generate text | XSum 1-shot
    12.2, fine-tuned 21.7 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| PaLM (Sect. [6.4.1](#Sec26)) | 用于生成文本的540B大型语言模型 | XSum 1-shot 12.2, 微调 21.7
    |'
- en: '| ST-MoE (Sect. [6.4.1](#Sec26)) | 269B large mixture-of-experts to generate
    text | CNN/DM 20.7, XSum 21.7 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| ST-MoE (Sect. [6.4.1](#Sec26)) | 用于生成文本的269B大型专家混合模型 | CNN/DM 20.7, XSum
    21.7 |'
- en: '| STIE (Sect. [6.4.1](#Sec26)) | 6.7B GPT model adapted to human preference
    judgments by reinforcement learning | STIE summaries are preferred to reference
    summaries in 70% of the cases |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| STIE (Sect. [6.4.1](#Sec26)) | 通过强化学习适应人类偏好判断的6.7B GPT模型 | STIE摘要有70%的情况比参考摘要更受欢迎
    |'
- en: '| BigBird (Sect. [6.4.2](#Sec27)) | Model for large inputs | arXiv 19.0, PubMed
    20.7 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| BigBird (Sect. [6.4.2](#Sec27)) | 用于大输入的模型 | arXiv 19.0, PubMed 20.7 |'
- en: '| HAT (Sect. [6.4.2](#Sec27)) | Model for large inputs using PEGASUS | arXiv
    19.7, PubMed 21.4, CNN/DM 21.3 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| HAT (Sect. [6.4.2](#Sec27)) | 使用PEGASUS进行大输入的模型 | arXiv 19.7, PubMed 21.4,
    CNN/DM 21.3 |'
- en: '| RL-175B (Sect. [6.4.2](#Sec27)) | Model based on GPT-3 for stepwise summarizing
    a book using reinforcement learning | Human comparison: Likert value 3.5 of 7
    |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| RL-175B（第[6.4.2](#Sec27)节） | 基于GPT-3的模型，用于使用强化学习逐步摘要一本书 | 人类比较：7分中的利克特值3.5
    |'
- en: '| PRIMER (Sect. [6.4.3](#Sec28)) | Summarize several documents based on Longformer
    Seq2seq model | Fine-tuned arXiv 20.8, fine-tuned Multi-News 21.1 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| PRIMER（第[6.4.3](#Sec28)节） | 基于Longformer Seq2seq模型对多个文档进行摘要 | 微调arXiv 20.8，微调Multi-News
    21.1 |'
- en: 6.4.1 Shorter Documents
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 较短的文档
- en: The training data usually consist of documents and the corresponding summaries
    or abstracts. There are a number of actual benchmark datasets for summarization
    like CNN/Daily Mail [[78](#CR78)], Gigaword [[150](#CR150)], and Reddit TIFU [[101](#CR101)],
    which have an input document with a length below 1000 tokens and a corresponding
    summary, which can be used for fine-tuning. The difference between a reference
    summary and a predicted summary is assessed by measures like Rouge, Bleu, or Meteor
    (Sect. [2.​3.​3](528393_1_En_2_Chapter.xhtml#Sec23)) with the recall-oriented
    Rouge most frequently used.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据通常包括文档及其相应的摘要或摘要。存在许多用于摘要的实际基准数据集，如CNN/Daily Mail [[78](#CR78)]、Gigaword
    [[150](#CR150)] 和 Reddit TIFU [[101](#CR101)]，这些数据集的输入文档长度低于1000个标记，并附有相应的摘要，可用于微调。参考摘要与预测摘要之间的差异通过Rouge、Bleu或Meteor（第[2.3.3](528393_1_En_2_Chapter.xhtml#Sec23)节）等指标来评估，其中以召回率为导向的Rouge最常使用。
- en: '**PEGASUS** [[128](#CR128)] is large transformer-based Seq2seq model pre-trained
    on massive text corpora (Sect. [3.​1.​3](528393_1_En_3_Chapter.xhtml#Sec4)). It
    follows a new pre-training objective in which not tokens are masked, but sentences.
    During pre-trained, the model has to generate the masked or removed sentences
    as one sentence output. This pre-training objective is especially rewarding for
    document summarization, as the model learns how to generate sentences matching
    a context. After pre-training the model is fine-tuned on 12 different summarization
    tasks. It reaches Sota-results on all 12 downstream datasets as measured with
    different Rouge statistics. In most cases the improvements are considerable [[128](#CR128)],
    e.g. for the CNN/Daily Mail benchmark it had a Rouge-2-score of 21.7\. The Rouge-2-scores
    of other Seq2seq models are similar, e.g. 21.6 for T5, 21.3 for BART, and 21.5
    for R3F [[4](#CR4)]. Note that for text generation often a BEAM search (Sect.
    [2.​2.​3](528393_1_En_2_Chapter.xhtml#Sec15)) is employed keeping several high
    probability versions of the text to increase the consistency of the resulting
    text.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '**PEGASUS** [[128](#CR128)] 是一个基于大规模文本语料库预训练的大型Transformer-based Seq2seq模型（第[3.1.3](528393_1_En_3_Chapter.xhtml#Sec4)节）。它遵循一个新的预训练目标，其中不是标记被遮蔽，而是句子。在预训练过程中，模型必须生成遮蔽或删除的句子作为单个句子输出。这个预训练目标对于文档摘要特别有益，因为模型学会了如何生成与上下文匹配的句子。预训练后，模型在12个不同的摘要任务上进行微调。它达到了所有12个下游数据集的Sota结果，这是通过不同的Rouge统计量来衡量的。在大多数情况下，改进是显著的[[128](#CR128)]，例如，对于CNN/Daily
    Mail基准，它的Rouge-2分数为21.7。其他Seq2seq模型的Rouge-2分数类似，例如T5为21.6，BART为21.3，R3F为21.5[[4](#CR4)]。请注意，对于文本生成，通常使用BEAM搜索（第[2.2.3](528393_1_En_2_Chapter.xhtml#Sec15)节）来保持几个高概率的文本版本，以提高生成文本的一致性。'
- en: '**BRIO** [[131](#CR131)] starts from the observation that the usual ML-training
    only takes into account a single reference summary for each example and ignore
    possible other summaries. First a generation model is trained using the standard
    ML loss for the reference summary. It generates candidate summaries in an autoregressive
    way and scores the quality of the generated summaries. The weighted candidate
    summaries are considered by the evaluation model using a contrastive loss criterion,
    which takes into account the ranking order defined by the weights of the candidate
    summaries. The approach uses BART or PEGASUS as backbone Seq2seq models. On the
    *CNN/Daily Mail benchmark* benchmark [[78](#CR78)] the BRIO model with 10B parameters
    has Sota performance with the Rouge-2 score of 23.6 on CNN/DM and 25.6 on XSum.
    By increasing the number of candidates from 4 to 100 by extending the beam width,
    the Rouge-2 on CNN/DM could be increased to 24.1\. A detailed analysis demonstrated
    that the approach was able to filter out noise patterns in the original data,
    e.g. the phrase “click here”.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '**BRIO** [[131](#CR131)] 从观察出发，通常的机器学习训练只考虑每个示例的单个参考摘要，而忽略了可能的其他摘要。首先，使用标准机器学习损失函数对参考摘要进行训练以生成一个生成模型。该模型以自回归的方式生成候选摘要并评分生成摘要的质量。通过对比损失准则，评估模型考虑了候选摘要的权重，该准则考虑了由候选摘要权重定义的排名顺序。该方法使用
    BART 或 PEGASUS 作为骨干 Seq2seq 模型。在 *CNN/Daily Mail 基准测试* [[78](#CR78)] 上，具有 10B
    参数的 BRIO 模型实现了 Sota 性能，CNN/DM 上的 Rouge-2 得分为 23.6，XSum 上的得分为 25.6。通过扩展搜索宽度，将候选数量从
    4 增加到 100，CNN/DM 上的 Rouge-2 得分可以提高到 24.1。详细分析表明，该方法能够过滤掉原始数据中的噪声模式，例如短语“点击此处”。'
- en: The autoregressive language models GPT-3, Gopher, InstructGPT and PaLM can be
    instructed to summarize, e.g. by entering a text and appending *“TL;DR:”* [[159](#CR159)].
    For **PaLM** with 540B parameters an evaluation is available. The *MLSum benchmark*
    [[198](#CR198)] requires the model to summarize a news article in multiple sentences.
    For German texts PaLM 1-shot arrives at 12.8 Rouge-2 and a fine-tuned version
    of PaLM achieves a Rouge-2 score of 33.1, which is below the fine-tuned Sota at
    36.4 [[43](#CR43), p. 30]. The *XSum benchmark* [[151](#CR151)] requires to summarize
    a news article in a single sentence. Here PaLM gets a few-shot Rouge-2 score of
    12.2 and a fine-tuned Rouge-2 of 21.2, whereas the fine-tuned SotaRouge-2 by BRIO
    is 25.6.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归语言模型 GPT-3、Gopher、InstructGPT 和 PaLM 可以被指示进行摘要，例如通过输入文本并附加 *“TL;DR:”* [[159](#CR159)]。对于具有
    540B 参数的 **PaLM**，有一个可用的评估。*MLSum 基准测试* [[198](#CR198)] 要求模型用多句总结新闻文章。对于德语文本，PaLM
    1-shot 得到 12.8 的 Rouge-2，而经过微调的 PaLM 版本达到 33.1 的 Rouge-2，这低于 36.4 的微调 Sota [[43](#CR43)，第
    30 页]。*XSum 基准测试* [[151](#CR151)] 要求用一句话总结新闻文章。在这里，PaLM 得到少样本 Rouge-2 得分为 12.2
    和微调 Rouge-2 得分为 21.2，而 BRIO 的微调 SotaRouge-2 为 25.6。
- en: '**ST-MoE-32B** [[270](#CR270)] is a mixture-of-experts model (Sect. [3.​5.​2](528393_1_En_3_Chapter.xhtml#Sec26))
    with 269B parameters. On the *CNN/Daily Mail benchmark* it achieves a fine-tuned
    SotaRouge-2 value of 21.7 and on the *XSum benchmark* it yields 27.1 Rouge-2 with
    fine-tuning. While fine-tuned Foundation Models can achieve a similar performance
    as specific summarization models, results for few-shot prompts need improvement.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '**ST-MoE-32B** [[270](#CR270)] 是一个具有 269B 参数的专家混合模型（见第 [3.5.2](528393_1_En_3_Chapter.xhtml#Sec26)
    节）。在 *CNN/Daily Mail 基准测试* 上，它实现了 21.7 的微调 SotaRouge-2 值，在 *XSum 基准测试* 上，经过微调后得到
    27.1 的 Rouge-2。尽管微调的基础模型可以达到特定摘要模型相似的性能，但少样本提示的结果需要改进。'
- en: 'Rouge metrics are only a crude guide to what people really care about: the
    quality of a summary. Stiennon et al. [[211](#CR211)] directly optimize their
    model with respect to human judgment. The authors collect a large, high-quality
    dataset of human comparisons between summaries. Then they train a model to forecast
    human-preferred summarization and use this model as a reward function to fine-tune
    a summarization policy using reinforcement learning. They apply their model to
    the *TL;DR benchmark* [[230](#CR230)], because this summarization task is significantly
    more challenging than CNN/DM. They find that the summaries of their 6.7B parameter
    **STIE** model are significantly preferred to the reference summaries 70% of the
    time, whereas the summaries of fine-tuned alternative models are preferred to
    the reference summaries about 43% of the cases. The model can also be applied
    to new domains better than other methods. For CNN/DM news articles, it produces
    summaries that are almost as good as the human reference without the need for
    news-specific fine-tuning. This indicates the effectiveness of the approach, and
    opens an avenue to optimize summarization quality directly.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: Rouge指标只是对人们真正关心的事情的一个粗略指南：摘要的质量。Stiennon等人[[211](#CR211)]直接根据人类判断优化他们的模型。作者收集了大量高质量的人类比较摘要的数据集。然后他们训练一个模型来预测人类偏好的摘要，并使用这个模型作为奖励函数，通过强化学习微调摘要策略。他们将他们的模型应用于*TL;DR基准*[[230](#CR230)]，因为这项摘要任务比CNN/DM更具挑战性。他们发现，他们的6.7B参数**STIE**模型的摘要有70%的时间比参考摘要更受欢迎，而微调的替代模型的摘要有大约43%的时间比参考摘要更受欢迎。该模型还可以比其他方法更好地应用于新领域。对于CNN/DM新闻文章，它产生的摘要几乎与人类参考一样好，而无需针对新闻进行特定的微调。这表明了该方法的有效性，并为直接优化摘要质量开辟了途径。
- en: 6.4.2 Longer Documents
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 较长的文档
- en: While the input document length of documents is generally less than 1000 tokens,
    it is greater for the *PubMed corpus* (4k tokens) and *ArXiv benchmark* (8.6k
    tokens) [[46](#CR46)]. For these benchmarks transformers with longer input sequences
    (Sect. [3.​2](528393_1_En_3_Chapter.xhtml#Sec7)) are capable of taking into account
    the whole document.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然文档的输入长度通常小于1000个标记，但对于*PubMed语料库*（4k个标记）和*ArXiv基准*（8.6k个标记）[[46](#CR46)]来说则更大。对于这些基准，具有更长输入序列的transformers（见第[3.2](528393_1_En_3_Chapter.xhtml#Sec7)节）能够考虑整个文档。
- en: '**BigBird** [[253](#CR253)] is able to cope with long documents (Sect. [3.​2.​1](528393_1_En_3_Chapter.xhtml#Sec8)).
    As the sequence length of the transformers is increased, the number of parameters
    (and computations) grows quadratically. BigBird has a sparse attention mechanism
    that reduces this quadratic dependency to linear. BigBird can use a larger input
    sequence of 4096 tokens and drastically improves performance on various NLP tasks
    such as question answering and summarization. Longer documents exhibit a richer
    discourse structure and summaries are considerably more abstractive. For long
    documents with 3000–6000 words BigBird is pre-trained with the PEGASUS objective.
    After fine-tuning it yields a marked improvement on Sota, e.g. on the ArXiv benchmark
    with the Rouge-2 score 19.0\. **TLDR** [[31](#CR31)] is a similar summarizer based
    on BART, which generates a one-sentence summary for scientific papers. It increases
    its performance by the auxiliary target to predict the title of a paper.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '**BigBird** [[253](#CR253)]能够处理长文档（见第[3.2.1](528393_1_En_3_Chapter.xhtml#Sec8)节）。随着transformers序列长度的增加，参数数量（和计算量）呈二次增长。BigBird具有稀疏注意力机制，将这种二次依赖性降低到线性。BigBird可以使用4096个标记的更大输入序列，并在各种NLP任务（如问答和摘要）上显著提高性能。较长的文档展现出更丰富的话语结构，摘要也更为抽象。对于3000-6000字的较长文档，BigBird使用PEGASUS目标进行预训练。经过微调后，它在Sota上取得了显著改进，例如在ArXiv基准上，Rouge-2得分为19.0。**TLDR**
    [[31](#CR31)]是一个基于BART的类似摘要器，为科学论文生成一句话摘要。它通过辅助目标预测论文标题来提高其性能。'
- en: '**HAT** [[187](#CR187)] aims to capture the content of longer documents in
    a better way. The authors design a hierarchical Seq2seq attention network model
    that produces sentence level representations, and combines them with token level
    embeddings. They determine sentence boundaries by punctuation and insert [*BOS*]
    tokens at the start of every sentence. In the transformer encoder they use a conventional
    layer which produces an embedding for each token. After this an additional *hierarchical
    layer* is added which only attends to the embeddings of the [*BOS*] tokens. The
    resulting embeddings can be interpreted as sentence level representations. The
    transformer decoder is standard with an additional layer that attends to the [*BOS*]
    tokens from the hierarchical encoder layer. On the *PubMed benchmark* of long
    documents [[46](#CR46)] it yields a SotaRouge-1 score of 21.4\. while on arXiv
    it has a Rouge-1 score of 19.7\. But also on the *CNN/Daily Mail benchmark* of
    shorter documents [[78](#CR78)] it achieves a SotaRouge-2 scores of 21.3,'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '**HAT** [[187](#CR187)] 旨在以更好的方式捕捉较长文档的内容。作者设计了一个层次化的 Seq2seq 注意力网络模型，它生成句子级表示，并将它们与标记级嵌入相结合。他们通过标点符号确定句子边界，并在每个句子的开头插入
    [*BOS*] 标记。在 Transformer 编码器中，他们使用一个传统的层，为每个标记生成嵌入。之后，添加了一个额外的 *层次层*，它只关注 [*BOS*]
    标记的嵌入。生成的嵌入可以解释为句子级表示。Transformer 解码器是标准的，并添加了一个额外的层，该层关注来自层次编码器的 [*BOS*] 标记。在长文档的
    *PubMed 基准测试* [[46](#CR46)] 中，它获得了 21.4 的 SotaRouge-1 分数，而在 arXiv 上，它有 19.7 的
    Rouge-1 分数。但在较短的文档的 *CNN/Daily Mail 基准测试* [[78](#CR78)] 中，它也实现了 21.3 的 SotaRouge-2
    分数。'
- en: '**RL-175B** is a summarizer for whole books by OpenAI using a reinforcement
    learning algorithm to follow human preferences [[236](#CR236)]. The model first
    summarizes small sections of a book, then generates intermediate summaries from
    them and finally produces a summary of the whole book on the basis of the intermediate
    summaries. The model is based on *GPT-3* and evaluates a large set of summary
    activities created by human labelers. The small sections are generated by a fixed
    chunking algorithm. Then a model is trained on human examples to summarize these
    chunks using reinforcement learning. It uses the approach explained in Sect. [3.​6.​5](528393_1_En_3_Chapter.xhtml#Sec43).
    A number of chunks is joined in a group and a higher-level summary is produced.
    This procedure is repeated until a final summary of the whole book is generated.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '**RL-175B** 是 OpenAI 使用强化学习算法来遵循人类偏好的全书摘要器 [[236](#CR236)]。该模型首先总结书籍的小节，然后从中生成中间摘要，并最终基于中间摘要生成整本书的摘要。该模型基于
    *GPT-3* 并评估由人类标注者创建的大量摘要活动。小节是通过一个固定的分块算法生成的。然后，通过强化学习在人类示例上训练一个模型来总结这些块。它使用第
    [3.6.5](528393_1_En_3_Chapter.xhtml#Sec43) 节中解释的方法。将多个块组合成一组，并生成一个更高层次的摘要。这个过程会重复进行，直到生成整本书的最终摘要。'
- en: The fine-tuning was performed for the GPT-3 with 7B and 175B parameters. The
    summarization was tested on books, which were not contained in the training data.
    The scoring is done by a *Likert scale* from 1 to 7\. It assigns numbers to human
    judgments (e.g. 1 = very bad, 2 = bad, …, 7 = very good), and computes averages
    from these numbers. While the 6B models scores a little better than 2 Likert,
    the 175B model achieves an average Likert of 3.5\. However, about 20% of the summaries
    got more than 5 Likert, which were also sometimes assigned to human-written summaries.
    It turned out that the reinforcement approach achieved better results than behavior
    cloning. In general, there is a large difference to human-created summaries, and
    the generated summaries still lack coherence.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 对具有 7B 和 175B 参数的 GPT-3 进行了微调。在训练数据中未包含的书籍上进行了摘要测试。评分是通过从 1 到 7 的 *李克特量表* 进行的。它将数字分配给人类判断（例如，1
    = 非常差，2 = 差，…，7 = 非常好），并从这些数字中计算平均值。虽然 6B 模型的得分略好于 2 个李克特，但 175B 模型实现了 3.5 的平均李克特。然而，大约
    20% 的摘要得分超过 5 个李克特，有时也被分配给人类编写的摘要。结果证明，强化方法比行为克隆方法取得了更好的结果。总的来说，与人类创建的摘要存在很大差异，生成的摘要仍然缺乏连贯性。
- en: 6.4.3 Multi-Document Summarization
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 多文档摘要
- en: Often, information is spread across multiple documents, and it makes sense to
    summarize this content. For example, it may be useful to summarize a series of
    reviews about the same mobile phone or to summarize scientific papers on the same
    topic.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，信息会分散在多个文档中，总结这些内容是有意义的。例如，总结同一款手机的系列评论或总结同一主题的科学论文可能是有用的。
- en: '**Primer** [[237](#CR237)] is based on the *Longformer* encoder-decoder (Sect.
    [3.​2.​1](528393_1_En_3_Chapter.xhtml#Sec8)), an efficient transformer model with
    an input length of 4096 tokens, where the effort for processing long documents
    grows linearly with their length. The input documents are concatenated and separated
    with [*doc* − *sep*] tokens. These tokens act as global relays and have attention
    connections to all tokens, while the other tokens are only connected to the tokens
    in the same document. In this way, large sequences of input documents can be processed.
    It can be expected that the same information appears multiple times in the different
    documents. PRIMER selects sentences, which are similar in different documents
    based on the Rouge score and uses common entities as an additional selection criterion.
    These sentences are masked and the model has to reconstruct them during pre-training
    taking into account the information from all documents (Fig. [6.13](#Fig13)).![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig13_HTML.png)'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '**Primer** [[237](#CR237)] 基于 *Longformer* 编码器-解码器（第 [3.2.1](528393_1_En_3_Chapter.xhtml#Sec8)
    节），这是一个输入长度为 4096 个标记的高效 Transformer 模型，其中处理长文档的努力与其长度成线性增长。输入文档通过 [*doc*−*sep*]
    标记连接并分隔。这些标记作为全局中继，并与所有标记有注意力连接，而其他标记仅与同一文档中的标记连接。这样，可以处理大型输入文档序列。可以预期，相同的信息会在不同的文档中多次出现。PRIMER
    根据Rouge分数选择在不同文档中相似的句子，并使用共同实体作为额外的选择标准。这些句子被掩码，模型在预训练期间必须重建它们，同时考虑来自所有文档的信息（图
    [6.13](#Fig13)）。![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig13_HTML.png)'
- en: An illustration represents the interaction of 3 documents with the long former
    encoder, that goes through the decoder to recover the masked sentence. It indicates
    the layers of input, local attention, and global attention.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 一幅插图展示了 3 个文档与长前向编码器的交互，通过解码器恢复被掩码的句子。它表示了输入层、局部注意力和全局注意力的层级。
- en: Fig. 6.13
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13
- en: Multiple documents form the input for PRIMER, separated with [doc-sep] tokens.
    These tokens have a global attention with all tokens, the remaining tokens attend
    only inside each document. Some sentences are selected and have to be recovered
    by the decoder [[237](#CR237)]
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 多个文档作为 PRIMER 的输入，使用 [doc-sep] 标记分隔。这些标记对所有标记具有全局注意力，其余标记只关注每个文档内部。一些句子被选中并需要通过解码器恢复
    [[237](#CR237)]。
- en: The pre-training already enables the model to combine the information from different
    documents. Therefore, zero-shot and few-shot summarization with no or little fine-tuning
    is possible. For the *Multi-News benchmark* [[54](#CR54)] with an average document
    length of 1793 and 2.8 documents per cluster, PRIMER achieves a zero-shot Rouge-2
    score of 13.6 and can increase this to 21.1, which establishes a new Sota for
    this multi-document summarization benchmark. On the *ArXiv benchmark* with an
    average document length of 6021 tokens [[46](#CR46)], the fine-tuned PRIMER yields
    a Rouge-2 score of 20.8, indicating the performance on long documents.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练已经使模型能够结合来自不同文档的信息。因此，无需或仅需少量微调即可实现零样本和少样本摘要。对于平均文档长度为 1793 字符和每个簇包含 2.8
    个文档的 *Multi-News 基准测试* [[54](#CR54)]，PRIMER 实现了 13.6 的零样本 Rouge-2 分数，并且可以将此分数提高到
    21.1，从而为这个多文档摘要基准测试建立了新的 Sota。在平均文档长度为 6021 个标记的 *ArXiv 基准测试* [[46](#CR46)] 中，经过微调的
    PRIMER 得到了 20.8 的 Rouge-2 分数，这表明了在长文档上的性能。
- en: Available Implementations
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: T5, BigBird, and Pegasus code and trained models are available on Hugging Face
    [https://​huggingface.​co/​transformers/​](https://huggingface.co/transformers/).
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T5、BigBird 和 Pegasus 的代码和训练模型可在 Hugging Face [https://huggingface.co/transformers/](https://huggingface.co/transformers/)
    上找到。
- en: Further summarization scripts at [https://​huggingface.​co/​tasks/​summarization](https://huggingface.co/tasks/summarization).
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他摘要脚本在 [https://huggingface.co/tasks/summarization](https://huggingface.co/tasks/summarization)。
- en: STIE data and code [https://​github.​com/​openai/​summarize-from-feedback](https://github.com/openai/summarize-from-feedback)
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: STIE 数据和代码 [https://github.com/openai/summarize-from-feedback](https://github.com/openai/summarize-from-feedback)
- en: PRIMER code for Multi-document Summarization [https://​github.​com/​allenai/​PRIMER](https://github.com/allenai/PRIMER)
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多文档摘要的 PRIMER 代码 [https://github.com/allenai/PRIMER](https://github.com/allenai/PRIMER)
- en: 6.4.4 Summary
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.4 摘要
- en: Foundation Models initiated a breakthrough for summarization models. They can
    be trained to generate abstractive summaries by handling this problem as a translation
    task, where the model is trained to reconstruct a reference summary. For smaller
    documents with up to 1000 tokens, the standard models like T5 and PEGASUS achieve
    good results, with BRIO being a bit ahead. Models with more parameters have a
    slightly better performance. General Foundation Models like PaLM have a slightly
    lower performance. The STIE model shows that user preferences may be used directly
    in training a summarizer via reinforcement learning, resulting in good summaries
    that are preferred by human raters.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型为摘要模型带来了突破。它们可以通过将这个问题作为一个翻译任务来训练，以生成抽象摘要，其中模型被训练来重建一个参考摘要。对于长度不超过1000个标记的小型文档，标准模型如T5和PEGASUS可以实现良好的结果，BRIO略胜一筹。具有更多参数的模型性能略好。像PaLM这样的通用基础模型性能略低。STIE模型表明，可以通过强化学习直接在训练摘要器时使用用户偏好，从而产生人类评分者更喜欢的良好摘要。
- en: For larger documents a transformer encoder-decoder with a larger input sequence
    is required, e.g. BigBird. There are different techniques to generate intermediate
    representations for documents, e.g. for sentences by HAT or chunks by RL-175B.
    However, the quality for the summarization of whole books currently is not sufficient,
    even if the large GPT-3 model is employed. A recent alternative is InstructGPT
    (Sect. [3.​6.​5](528393_1_En_3_Chapter.xhtml#Sec43)), which can be easily directed
    to perform a summarization, e.g. by the prompt *“Summarize this for a second-grade
    student:* <* text*>* ”* [[162](#CR162), p. 30]. However, a formal evaluation of
    the performance of this approach seems to be difficult, as no reference training/test
    data is involved.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更大的文档，需要一个具有更大输入序列的transformer编码器-解码器，例如BigBird。有不同技术用于生成文档的中间表示，例如通过HAT生成句子或通过RL-175B生成块。然而，即使使用大型GPT-3模型，目前对整本书的摘要质量仍然不足。最近的一个替代方案是InstructGPT（第[3.6.5](528393_1_En_3_Chapter.xhtml#Sec43)节），它可以很容易地被引导执行摘要，例如通过提示“*为二年级学生总结一下这个：*
    <*文本*>”* [[162](#CR162)，第30页]。然而，由于没有涉及参考训练/测试数据，似乎很难对这个方法的表现进行正式评估。
- en: Multi-document summarization has to cope with the repetition of contents in
    different documents. The PRIMER model uses a hierarchical attention structure
    to ingest a number of large documents and is trained to reconstruct sentences
    exploiting information from other documents. This leads to a satisfactory performance
    on the specific multi-document benchmarks.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 多文档摘要必须处理不同文档中内容的重复。PRIMER模型使用分层注意力结构来处理大量文档，并经过训练以利用其他文档中的信息来重建句子。这导致在特定的多文档基准测试上取得了令人满意的表现。
- en: 6.5 Text Generation
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 文本生成
- en: A system for *Natural language generation* (NLG) has the task of producing fluent,
    coherent, and understandable text. Usually, the system generates a continuation
    of a start text. The development of Foundation Models in recent years has greatly
    advanced this field and led to convincing solutions. This section concentrates
    on writing larger texts and complete stories. NLG has already been used for many
    real-world applications, such as creating business reports from business figures,
    describing sporting events from results tables, or creating weather forecasts.
    Microsoft has announced to fire about 50 employees of MSN news [[17](#CR17)],
    using Deep Learning instead to identify trending news stories or optimize the
    content. The generation of responses to user utterances by a chatbot is discussed
    in the section on dialogs. A number of surveys for text generation is available
    [[65](#CR65), [83](#CR83), [116](#CR116)]. Yu et al. [[251](#CR251)] give an overview
    of knowledge-enhanced text generation.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言生成（NLG）系统有生成流畅、连贯、可理解文本的任务。通常，系统生成一个起始文本的延续。近年来，基础模型的发展极大地推动了这一领域，并导致了令人信服的解决方案。本节专注于撰写更长的文本和完整的故事。NLG已经被用于许多实际应用，例如从商业数据创建商业报告，从结果表中描述体育赛事，或创建天气预报。微软宣布将解雇大约50名MSN新闻的员工
    [[17](#CR17)]，转而使用深度学习来识别趋势新闻故事或优化内容。聊天机器人对用户话语的响应生成在对话部分讨论。有许多关于文本生成的调查可用 [[65](#CR65),
    [83](#CR83), [116](#CR116)]。Yu等人 [[251](#CR251)] 对知识增强文本生成进行了概述。
- en: Here we will describe story generation systems based on Foundation Models that
    currently provide the best results. A high-level overview of approaches is given
    in Table [6.10](#Tab10). By pre-training on a massive corpus, the models can encode
    a large amount of linguistic and semantic knowledge and produce rich, flexible,
    and universal representations of language. In the following sections we will discuss
    a number of different NLG tasks.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将描述基于基础模型的故事生成系统，这些系统目前提供了最佳结果。方法的高级概述见表 [6.10](#Tab10)。通过在大量语料库上进行预训练，模型可以编码大量的语言和语义知识，并产生丰富、灵活和通用的语言表示。在接下来的章节中，我们将讨论许多不同的
    NLG 任务。
- en: First, we describe NLG basics, where the next token *y* has to be generated
    according to a language model *p*(*y*|***x***) (Sect. [6.5.1](#Sec32)).Table 6.10
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们描述了自然语言生成（NLG）的基础，其中下一个标记 *y* 必须根据语言模型 *p*(*y*|***x***)（第 [6.5.1](#Sec32)
    节）生成。表 6.10
- en: Main text generation techniques
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 主要文本生成技术
- en: '| Architecture | Mechanism | Advantages | Disadvantages |'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 架构 | 机制 | 优点 | 缺点 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Variational Autoencoder (VAE) [[26](#CR26)] | Compress a text ***x*** to
    a hidden vector ***h*** distributed as a Gaussian, reconstruct the text ***x***
    from ***h*** | Constraint on the latent vector ***h*** creates a continuous representation
    space and increases the diversity of the generated text | Often less fluent and
    coherent in text generation compared to Foundation Models |'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 变分自编码器（VAE）[[26](#CR26)] | 将文本 ***x*** 压缩为一个高斯分布的隐藏向量 ***h***，从 ***h*** 重建文本
    ***x*** | 对潜在向量 ***h*** 的约束创建了一个连续的表示空间，并增加了生成文本的多样性 | 与基础模型相比，在文本生成中通常不太流畅和连贯
    |'
- en: '| Generative Adversarial Network (GAN) [[68](#CR68)] | A generator transforms
    a random vector ***s*** to a text ***x***. A discriminator checks, if ***x***
    is synthetic. Both are trained in adversarial style | Unsupervised learning; Generating
    clearer and more realistic samples than other generative models | Instable training
    process; sampling of ***x*** is non-differentiable: needs reinforcement learning
    or Gumbel-softmax |'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 生成对抗网络（GAN）[[68](#CR68)] | 一个生成器将随机向量 ***s*** 转换为文本 ***x***。一个判别器检查 ***x***
    是否为合成文本。两者都以对抗风格进行训练 | 无监督学习；生成比其他生成模型更清晰、更逼真的样本 | 训练过程不稳定；***x*** 的采样不可微：需要强化学习或
    Gumbel-softmax |'
- en: '| Autoregressive Language Model (GPT) (Sect. [2.​2](528393_1_En_2_Chapter.xhtml#Sec11))
    | Self-attention with previous tokens *x*[1], …, *x*[*t*−1] to generate next token
    *x*[*t*] | Efficient contextual embeddings and long-term context; fast parallel
    computing speed | High computational effort and slow training speed |'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 自回归语言模型（GPT）（第 [2.2](528393_1_En_2_Chapter.xhtml#Sec11) 节） | 使用先前标记 *x*[1]，…，*x*[*t*−1]
    进行自注意力计算以生成下一个标记 *x*[*t*] | 高效的上下文嵌入和长期上下文；快速并行计算速度 | 高计算成本和缓慢的训练速度 |'
- en: '| Encoder-decoder Transformer (Sect. [2.​3](528393_1_En_2_Chapter.xhtml#Sec19))
    | Self-attention over full input sequence ***x*** and iterative generation of
    output sequence *y*[1], … | Efficient contextual embeddings and long-term context;
    transform input as a whole sequence | High computational effort and slow training
    speed |'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 编码器-解码器 Transformer（第 [2.3](528393_1_En_2_Chapter.xhtml#Sec19) 节） | 在整个输入序列
    ***x*** 上进行自注意力计算，并迭代生成输出序列 *y*[1]，… | 高效的上下文嵌入和长期上下文；将输入作为一个整体序列进行转换 | 高计算成本和缓慢的训练速度
    |'
- en: Then we discuss the generation of a new text with a given style, e.g. a poem
    (Sect. [6.5.2](#Sec33)).
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们讨论给定风格的新文本生成，例如一首诗（第 [6.5.2](#Sec33) 节）。
- en: A related task is to rewrite one document in a different style or world view
    (Sect. [6.5.3](#Sec36)).
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个相关任务是重写一个文档以不同的风格或世界观（第 [6.5.3](#Sec36) 节）。
- en: In general, the text created by the Foundation Model takes a consistent but
    random course. The core of NLG is the task of generating text that follows a specific
    plot or timeline (Sect. [6.5.4](#Sec40)).
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，基础模型创建的文本遵循一致但随机的路径。NLG 的核心任务是生成遵循特定情节或时间线的文本（第 [6.5.4](#Sec40) 节）。
- en: Table [6.11](#Tab11) describes these tasks and lists a number of corresponding
    NLG models discussed in this section. The generation of fake news or other malicious
    text is covered in Sect. [6.5.5](#Sec44). Section [6.5.6](#Sec46) describes how
    to generate computer code.Table 6.11
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [6.11](#Tab11) 描述了这些任务，并列出了本节中讨论的相应 NLG 模型。虚假新闻或其他恶意文本的生成在第 [6.5.5](#Sec44)
    节中介绍。第 [6.5.6](#Sec46) 节描述了如何生成计算机代码。表 6.11
- en: Mechanisms to control story generation
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 控制故事生成的机制
- en: '| Approach | Description | Example systems |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 描述 | 示例系统 |'
- en: '| --- | --- | --- |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Pre-train LM on large text (optional fine-tuning) | Pre-train the language
    model on a large text collection. Possibly fine-tune on a smaller corpus of a
    specific domain. Generate a continuation of the start text | GPT-2 [[235](#CR235)],
    GPT-3 [[29](#CR29)], Gopher [[175](#CR175)], Retro [[25](#CR25)], WuDao [[263](#CR263)],
    PaLM [[43](#CR43)] |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 在大型文本上预训练LM（可选微调） | 在大型文本集合上预训练语言模型。可能在小领域语料库上进行微调。生成起始文本的续写 | GPT-2 [[235](#CR235)]，GPT-3
    [[29](#CR29)]，Gopher [[175](#CR175)]，Retro [[25](#CR25)]，WuDao [[263](#CR263)]，PaLM
    [[43](#CR43)] |'
- en: '| Add style or content marker | Add style or content marker to the start text.
    The marker has to be present in pre-training or fine-tuning data | CTRL [[96](#CR96)],
    PPLM [[50](#CR50)], ETC-NLG [[32](#CR32)] using topics, GDC [[97](#CR97)] controls
    token distributions, Adapter-Bot [[126](#CR126)] |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 添加样式或内容标记 | 在起始文本中添加样式或内容标记。标记必须在预训练或微调数据中存在 | CTRL [[96](#CR96)]，PPLM [[50](#CR50)]，ETC-NLG
    [[32](#CR32)]使用主题，GDC [[97](#CR97)]控制标记分布，Adapter-Bot [[126](#CR126)] |'
- en: '| Translate text to a new style | Use a transformer and a possible style selector
    to transform an input text to a new style and nearly the same content | Formal
    [[260](#CR260)], LRE [[90](#CR90)], ACC [[250](#CR250)], LRS [[118](#CR118)],
    StyleLM [[217](#CR217)], OPTIMUS [[115](#CR115)], GPT-3 with two-step prompts
    [[30](#CR30)] |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 将文本翻译成新的风格 | 使用转换器和可能的样式选择器将输入文本转换为新的样式和几乎相同的内容 | 正式 [[260](#CR260)]，LRE
    [[90](#CR90)]，ACC [[250](#CR250)]，LRS [[118](#CR118)]，StyleLM [[217](#CR217)]，OPTIMUS
    [[115](#CR115)]，GPT-3使用两步提示[[30](#CR30)] |'
- en: '| Specify a sequence of events for the story | Specify events by short sentences/phrases
    and generate a story containing these events in order | PlotMachines [[181](#CR181)]
    uses phrases, Pointer [[261](#CR261)] inserts words, Progressive WritingPrompts
    [[220](#CR220)], Facts2Story [[161](#CR161)] starts with a sequence of facts,
    GraphPlan [[38](#CR38)] uses a graph of events, SOE [[214](#CR214)] performs a
    two-level process of generating text, FIST [[58](#CR58)], GPT-3 with bullet-list
    prompts [[30](#CR30)] |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 指定故事的事件序列 | 通过简短的句子/短语指定事件，并按顺序生成包含这些事件的叙事 | PlotMachines [[181](#CR181)]使用短语，Pointer
    [[261](#CR261)]插入单词，Progressive WritingPrompts [[220](#CR220)]，Facts2Story [[161](#CR161)]从一系列事实开始，GraphPlan
    [[38](#CR38)]使用事件图，SOE [[214](#CR214)]执行生成文本的两级过程，FIST [[58](#CR58)]，GPT-3使用子弹列表提示[[30](#CR30)]
    |'
- en: The assessment of the performance of natural language generators is a difficult
    problem. Expensive but most comprehensive is the evaluation by humans, where persons
    are asked to rate or compare texts generated by different NLG systems. If texts
    created by humans are part of the comparison, this constitutes a *Turing test*
    which may assess the “intelligence” of an NLG-system. An alternative are automatic
    metrics like Bleu, Meteor or Rouge (Sect. [2.​3.​3](528393_1_En_2_Chapter.xhtml#Sec23)),
    which assess the difference between machine-generated texts to human-generated
    reference texts by comparing *n*-gram counts (Sect. [6.3](#Sec19)). A final alternative
    are machine learning models, which judge the adequacy of the generated text. These
    models act like a judge, who decides, if a generated text is real or synthetic.
    Celikyilmaz et al. [[34](#CR34)] discuss these evaluation approaches in detail.
    Yu et al. [[251](#CR251)] provide a survey of knowledge-enhanced text generation.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言生成器的性能评估是一个难题。最昂贵但最全面的是由人类进行的评估，其中人们被要求对由不同NLG系统生成的文本进行评分或比较。如果人类创建的文本是比较的一部分，这构成了一个*图灵测试*，可以评估NLG系统的“智能”。另一种选择是自动指标，如Bleu、Meteor或Rouge（第[2.3.3](528393_1_En_2_Chapter.xhtml#Sec23)节），通过比较*n*-gram计数（第[6.3](#Sec19)节）来评估机器生成文本与人类生成参考文本之间的差异。最后一种选择是机器学习模型，这些模型判断生成文本的适当性。这些模型的作用像一个法官，决定一个生成的文本是真实的还是合成的。Celikyilmaz等人[[34](#CR34)]详细讨论了这些评估方法。Yu等人[[251](#CR251)]提供了知识增强文本生成的调查。
- en: '*GEM* [[66](#CR66)] is a new benchmark collection created for NLG containing
    seventeen different benchmarks and comprising an evolving system of evaluation
    metrics and procedures. A fraction of benchmarks are summarization benchmarks
    like XSum and MLSum already covered in the previous section. Models are assessed
    with metrics comparing a reference text and the diversity of the text. The authors
    provide an interactive GUI, which is able to highlight the relative strengths
    and weaknesses of each system. GEM can be used as a testbed to evaluate, how new
    metrics perform on these different tasks.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '*GEM* [[66](#CR66)] 是为自然语言生成（NLG）创建的新基准集合，包含十七个不同的基准，并包含一个不断发展的评估指标和程序体系。其中一部分基准是摘要基准，如XSum和MLSum，已在上一节中介绍。模型通过比较参考文本和文本的多样性来评估。作者提供了一个交互式图形用户界面（GUI），能够突出显示每个系统的相对优势和劣势。GEM可以用作测试平台来评估新指标在这些不同任务上的表现。'
- en: 6.5.1 Generating Text by Language Models
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1 语言模型生成文本
- en: Language models (Sect. [2.​2](528393_1_En_2_Chapter.xhtml#Sec11)) have the task
    to produce the next token *x*[*t*] for a text ***x*** = (*x*[1], …, *x*[*t*−1]).
    This model can directly be applied to story generation. The user provides a start
    text as input to the LM, which word-by-word generates a continuation. Specifically,
    the model predicts for the next position the probability *p*(*x*[*t*]|*x*[1],
    …, *x*[*t*−1];***w***) of each token of the vocabulary. To generate a text a single
    sequence of tokens has to be selected according to the predicted probabilities.
    Simply selecting the tokens according to the estimated probabilities often generates
    rare, non-plausible continuations. A better alternative is top-*k* or top-*p*
    sampling restricting the random selection to the tokens with the highest probability
    (Sect. [2.​2.​3](528393_1_En_2_Chapter.xhtml#Sec15)).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型（第[2.2](528393_1_En_2_Chapter.xhtml#Sec11)节）的任务是生成文本***x***的下一个标记*x*[*t*]。该模型可以直接应用于故事生成。用户将起始文本作为输入提供给语言模型（LM），然后逐词生成后续内容。具体来说，模型预测下一个位置每个词汇的标记的概率*p*(*x*[*t*]|*x*[1],
    …, *x*[*t*−1];***w***)。为了生成文本，必须根据预测的概率选择一个标记序列。简单地根据估计的概率选择标记通常会产生罕见、不可信的后续内容。更好的选择是限制随机选择到概率最高的标记的top-*k*或top-*p*采样（第[2.2.3](528393_1_En_2_Chapter.xhtml#Sec15)节）。
- en: Early LMs, e.g. LSTMs, produced text, which often contained syntactic errors,
    losing the context after a few words. **VAE***Variational Auto-Encoders* reconstruct
    the sentence from a randomly modified latent representation ***z*** ∼ *N*(***μ***,
    ***σ***), where ***μ*** and ***σ*** are predicted by the encoder. A KL-loss is
    added to the reconstruction loss such that the distribution of ***z*** approaches
    a standard normal distribution [[89](#CR89)]. **GAN***Generative Adversarial Networks*
    use a generator to transform a noise vector ***s*** to a text ![$$\tilde {{\boldsymbol
    {x}}}=G(\boldsymbol {s})$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq18.png).
    Then a discriminator *D*(***x***) has the task to distinguish synthetic text ![$$\tilde
    {{\boldsymbol {x}}}$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq19.png)
    from real text ***x*** [[68](#CR68)]. Both models are trained together. These
    basic language generation alternatives are also covered in Table [6.10](#Tab10).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 早期语言模型，例如LSTMs，生成的文本往往包含语法错误，在几个词之后就会失去上下文。**VAE**（变分自编码器）从随机修改的潜在表示***z***∼*N*(***μ***,
    ***σ***)中重建句子，其中***μ***和***σ***由编码器预测。在重建损失中添加KL损失，使得***z***的分布接近标准正态分布[[89](#CR89)]。**GAN**（生成对抗网络）使用生成器将噪声向量***s***转换为文本
    ![$$\tilde {{\boldsymbol {x}}}=G(\boldsymbol {s})$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq18.png)。然后判别器*D*(***x***)的任务是区分合成文本
    ![$$\tilde {{\boldsymbol {x}}}$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq19.png)和真实文本***x***
    [[68](#CR68)]。这两个模型一起训练。这些基本的语言生成替代方案也在表[6.10](#Tab10)中介绍。
- en: A number of classical models for text generation such as BART (Sect. [3.​1.​3](528393_1_En_3_Chapter.xhtml#Sec4)),
    T5 (Sect. [3.​1.​3](528393_1_En_3_Chapter.xhtml#Sec4)), and mT5 (Sect. [3.​3.​2](528393_1_En_3_Chapter.xhtml#Sec14))
    are evaluated with the GEM benchmark [[66](#CR66)]. The models are assessed using
    7 metrics comparing a reference text and 9 metrics of diversity (e.g. the relative
    number of distinct uni- and bigrams). Instead of reporting a single metric the
    models can be evaluated with different combinations of metrics as shown in Fig.
    [6.14](#Fig14).![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig14_HTML.png)
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 评估了多个用于文本生成的经典模型，如BART（第[3.1.3](528393_1_En_3_Chapter.xhtml#Sec4)节）、T5（第[3.1.3](528393_1_En_3_Chapter.xhtml#Sec4)节）和mT5（第[3.3.2](528393_1_En_3_Chapter.xhtml#Sec14)节），使用GEM基准测试[[66](#CR66)]。这些模型通过7个指标来评估参考文本，以及9个多样性指标（例如，不同单字和双字组合的相对数量）。而不是报告单个指标，模型可以通过如图[6.14](#Fig14)所示的不同指标组合来评估。![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig14_HTML.png)
- en: An illustration highlights t 5 small from the table of data to text. It indicates
    the descriptive, diverse, factual, lexical, and semantic values on the right side.
    Below is a line graph denoting the point for t 5 small dot totto underscore v
    a l.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 一幅插图展示了从数据表到文本的转换过程。它显示了右侧的描述性、多样性、事实性、词汇和语义值。下面是一条表示t 5 small dot totto underscore
    v a l点的折线图。
- en: Fig. 6.14
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14
- en: A screenshot of the GEM benchmark interactive result exploration tool. On the
    top left tasks are selected. The selection of metric-groups or metrics is on the
    top right. The visualization of the selected metrics is shown on the bottom. Image
    reprinted with kind permission of the authors [[66](#CR66), p. 107]
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: GEM基准测试交互式结果探索工具的屏幕截图。在左上角选择了任务。右上角选择指标组或指标。所选指标的可视化显示在底部。图片经作者同意重印[[66](#CR66)，第107页]
- en: '**GPT-2** [[174](#CR174)] is an autoencoder comprising 1.5B parameters. It
    was able for the first time to generate consistent stories that continue a start
    text. According to the users, the stories were coherent in half of the cases.
    Much better is the performance of **GPT-3** with 175B parameters [[29](#CR29)].
    Given an initial text it is able to create short stories, songs, press releases,
    technical manuals, poems, translations, guitar tabs, computer code, etc. Only
    with an accuracy close to chance (52%) humans were able to distinguish whether
    news articles of about 200 words were synthetic [[29](#CR29), p. 26]. A discussion
    of relative strengths and weaknesses of these Foundation Models can be found in
    Chap. [4](528393_1_En_4_Chapter.xhtml).'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-2** [[174](#CR174)]是一个包含1.5B参数的自动编码器。它首次能够生成连贯的故事，以延续起始文本。根据用户反馈，在半数情况下故事是连贯的。性能更好的是拥有175B参数的**GPT-3**
    [[29](#CR29)]。给定一个初始文本，它能够创作短篇故事、歌曲、新闻稿、技术手册、诗歌、翻译、吉他乐谱、计算机代码等。只有当准确率接近随机水平（52%）时，人类才能区分大约200字的合成新闻文章[[29](#CR29)，第26页]。关于这些基础模型相对优势和劣势的讨论可以在第[4](528393_1_En_4_Chapter.xhtml)章中找到。'
- en: An evaluation benchmark measuring the degree to which a language model “understands”
    a story is the *LAMBADA benchmark* [[165](#CR165)] (Sect. [4.​1.​3](528393_1_En_4_Chapter.xhtml#Sec4)).
    It consists of about 10,000 passages from the BooksCorpus containing unpublished
    novels. The task is to predict the missing last word of the last sentence of each
    passage. Examples were filtered by humans to ensure that models need to take into
    account the full passage of at least 50 tokens to induce the final word. The GPT-3[175B]
    autoregressive language model [[173](#CR173)] predicted the last word with 76.2%
    [[29](#CR29), p. 12]. PaLM with few-shot instructions could increase the accuracy
    to 89.7 [[43](#CR43), p. 79]. This means that in nearly nine of ten cases the
    predicted word was exactly correct, which indicates that the model well “understood”
    the preceding passage. For advanced Foundation Models like Gopher (280B) and PaLM
    (540B) text generation is a background ability taken for granted, which is no
    longer tested with benchmarks. A large battery of benchmarks is applied to test
    other features, e.g. common sense knowledge, reasoning, etc. (Sect. [4.​1.​4](528393_1_En_4_Chapter.xhtml#Sec5)).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 一个评估基准，用于衡量语言模型“理解”故事的程度是*LAMBADA基准* [[165](#CR165)]（第[4.1.3](528393_1_En_4_Chapter.xhtml#Sec4)节）。它包含大约10,000个来自BooksCorpus的未发表小说的段落。任务是预测每个段落最后一句中缺失的最后一个单词。示例经过人工筛选，以确保模型需要考虑至少50个标记的完整段落来推断最后一个单词。GPT-3[175B]自回归语言模型
    [[173](#CR173)] 预测最后一个单词的准确率为76.2% [[29](#CR29)，第12页]。PaLM在少量指令下可以将准确率提高到89.7
    [[43](#CR43)，第79页]。这意味着在近九成的情况下，预测的单词是准确的，这表明模型很好地“理解”了前面的段落。对于像Gopher（280B）和PaLM（540B）这样的高级基础模型，文本生成是一个理所当然的背景能力，不再用基准来测试。大量基准被应用于测试其他功能，例如常识知识、推理等。（第[4.1.4](528393_1_En_4_Chapter.xhtml#Sec5)节）。
- en: '**InstructGPT** is a recent variant of GPT-3 (Sect. [3.​6.​5](528393_1_En_3_Chapter.xhtml#Sec43)),
    which can easily be instructed to generate a story, e.g. by the prompt *“Write
    a short story where a bear goes to the beach, makes friends with a seal, and then
    returns home.”* [[162](#CR162), p. 6]. **Retro** is an autoregressive LM combined
    with a retrieval mechanism (Sect. [6.2.3](#Sec15)). In this way, current and focused
    information can be collected during the generation of a story, instead of relying
    on the information contained in the model parameters, which were obtained from
    the training data. **LaMDA** (137B) is a recent Language Model (Sect. [6.6.3](#Sec52))
    specialized for dialogs. It also features a retriever-reader architecture to augment
    its internal knowledge acquired during pre-training with external information.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '**InstructGPT**是GPT-3（第[3.6.5](528393_1_En_3_Chapter.xhtml#Sec43)节）的一个新变体，可以轻松地指示生成故事，例如通过提示“写一个短故事，讲述一只熊去海滩，和一只海豹交朋友，然后回家。”
    [[162](#CR162)，第6页]。**Retro**是一个自回归语言模型，结合了检索机制（第[6.2.3](#Sec15)节）。通过这种方式，在生成故事的过程中可以收集当前和专注的信息，而不是依赖于从训练数据中获得的模型参数中包含的信息。**LaMDA**（137B）是一个专门用于对话的语言模型（第[6.6.3](#Sec52)节），它还具备检索-阅读架构，以增强其在预训练期间通过外部信息获得的内部知识。'
- en: '**GRF** [[86](#CR86)] is a Foundation Model including multi-hop reasoning in
    a knowledge base to improve language generation. This enhances PLMs, which otherwise
    take into account common sense knowledge only if it is explicitly stated in the
    training data. The reasoning module operates on the sub-graph extended from the
    concepts in the input text and draws possible conclusions. These are taken into
    account for the further generation of text. Results, e.g. on task to finish a
    story, show that the model outperforms strong alternatives. Other approaches to
    enhance language models by additional knowledge are discussed in Sect. [3.​4](528393_1_En_3_Chapter.xhtml#Sec17).
    A survey of conditional text generation is given by Guo et al. [[72](#CR72)].'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '**GRF** [[86](#CR86)]是一个包含知识库中多跳推理的基础模型，用于提高语言生成能力。这增强了PLMs，否则它们只有在训练数据中明确陈述时才会考虑常识知识。推理模块在从输入文本中的概念扩展的子图中操作，并得出可能的结论。这些结论被纳入进一步的文本生成中。例如，在完成故事的任务上的结果表明，该模型优于强大的替代方案。其他通过额外知识增强语言模型的方法在第[3.4](528393_1_En_3_Chapter.xhtml#Sec17)节中讨论。Guo等人提供了一份条件文本生成调查[[72](#CR72)]。'
- en: 6.5.2 Generating Text with a Given Style
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.2 使用给定风格生成文本
- en: 'Often the goal is to create a text in a specific style or emphasizing a specific
    type of content: e.g. author’s style (e.g. Shakespeare), emotion (e.g. angry,
    malicious, happy), genre (e.g. humor, romance), topics (politics, religion), persona
    (e.g. lawyer, knight), or sentiment (e.g. positive, negative, fury). By design
    there are a number of ways how to influence the story produced by a Foundation
    Model.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的目标是在特定风格或强调特定类型的内容中创建文本：例如，作者的风格（例如，莎士比亚），情感（例如，愤怒、恶意、快乐），体裁（例如，幽默、浪漫），主题（政治、宗教），人物（例如，律师、骑士），或情感（例如，积极、消极、愤怒）。设计上，有几种方法可以影响基础模型产生的故事。
- en: Pre-training a Foundation Model with corresponding texts.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用相应文本预训练基础模型。
- en: Adaption of the Foundation Model to a new genre/style/content by fine-tuning.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过微调将基础模型适应到新的体裁/风格/内容。
- en: Specification of an initial text.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始文本的规范。
- en: Few-shot instruction, e.g. for GPT-3, or simple instructions for InstructGPT.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 少样本指令，例如 GPT-3 的指令，或 InstructGPT 的简单指令。
- en: There are different ways to achieve this with Foundation Models. A comprehensive
    survey is given by Lili and Vechtomova [[122](#CR122)].
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基础模型实现这一点有不同的方法。Lili 和 Vechtomova [[122](#CR122)] 提供了一个全面的调查。
- en: Style-Conditional Probabilities
  id: totrans-378
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 风格条件概率
- en: '**CTRL** [[96](#CR96)] aims to train a generative model *p*(*y*|*x*;*a*) conditioned
    on a control variable *a*. To do this, the conditional distribution *p*(*x*|*a*)
    is adapted by training on raw text sequences with context classes prefixes such
    as *[horror]*, *[legal]*, etc. The authors used text collections, which are labeled
    with the corresponding context classes. Then the learned transformer model with
    1.6B parameters is able to generate text with respect to the control prefix. This
    is developed further by **GeDI** [[105](#CR105)], which has a stronger controllability,
    generates less toxic text, and can be extended to continuously weighted control
    codes for generating fluent stories [[127](#CR127)].'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '**CTRL** [[96](#CR96)] 旨在训练一个基于控制变量 *a* 的生成模型 *p*(*y*|*x*;*a*)。为此，通过在带有上下文类别前缀（如
    *[horror]*, *[legal]* 等）的原始文本序列上进行训练，对条件分布 *p*(*x*|*a*) 进行了调整。作者使用了带有相应上下文类别的文本集合。然后，具有
    1.6B 参数的学习 Transformer 模型能够根据控制前缀生成文本。这进一步由 **GeDI** [[105](#CR105)] 发展，它具有更强的可控性，生成的毒性文本更少，并且可以扩展到用于生成流畅故事的连续加权控制代码
    [[127](#CR127)]。'
- en: '**PPLM** [[50](#CR50)] (Plug and Play Language Model) defines a model *p*(*x*|*a*),
    where *a* is some desired controllable attribute(s) and *x* the generated sample.
    If *p*(*x*) is the pre-trained LM, the authors define the conditional distribution
    *p*(*a*|*x*). This yields a conditional generative model *p*(*x*|*a*) ∝ *p*(*a*|*x*)*p*(*x*).
    The distribution *p*(*a*|*x*) may be implemented by a single layer classifiers.
    The model samples from the resulting combined model by following gradients in
    the latent representation space (key-value-pairs of the transformer) such that
    *p*(*x*) as well as *p*(*a*|*x*) is improved. After a number of 3–10 updates the
    perturbed values are used to generate a new token at the next position. The model
    was able to create text with the desired tonality (e.g. positive/negative) while
    preserving fluency. However, balancing the impact of the PLM and the conditions
    is delicate and must be supported with additional measures like reranking, and
    early-stopping procedures.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '**PPLM** [[50](#CR50)]（即插即用语言模型）定义了一个模型 *p*(*x*|*a*)，其中 *a* 是一些期望的可控属性，而 *x*
    是生成的样本。如果 *p*(*x*) 是预训练的 LM，则作者定义了条件分布 *p*(*a*|*x*)。这产生了一个条件生成模型 *p*(*x*|*a*)
    ∝ *p*(*a*|*x*) *p*(*x*)。分布 *p*(*a*|*x*) 可以通过单层分类器实现。模型通过在潜在表示空间（Transformer 的键值对）中跟随梯度来从结果组合模型中采样，从而改进
    *p*(*x*) 以及 *p*(*a*|*x*)。经过 3-10 次更新后，使用扰动的值在下一个位置生成新的标记。该模型能够创建具有所需语调（例如，积极/消极）的文本，同时保持流畅。然而，平衡
    PLM 和条件的影响是微妙的，并且必须通过重新排序和早期停止等额外措施来支持。'
- en: '**ETC-NLG** [[32](#CR32)] leverages context-sensitive topic models [[23](#CR23)]
    to enhance PPLM with an unlabeled collection of documents. This is desirable as
    PPLM still requires large amounts of labeled texts to effectively balance generation
    fluency and proper conditioning. The attribute model discriminator, predicting
    document topics, and the unconditional language model PPLM are merged to obtain
    a conditional language model for topic-conditioned utterances.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '**ETC-NLG** [[32](#CR32)] 利用上下文敏感的主题模型 [[23](#CR23)] 来增强 PPLM，使用未标记的文档集合。这是有吸引力的，因为
    PPLM 仍然需要大量的标记文本来有效地平衡生成流畅性和适当的条件。将属性模型判别器、预测文档主题的无条件语言模型 PPLM 合并，以获得针对主题条件语句的条件语言模型。'
- en: '**GDC** (Generation with Distributional Control) [[97](#CR97)] propose an approach
    to emphasize specific words in addition to changing the distribution of generated
    words. For example, GDC can avoid toxic content, prevent bias, and align the generation
    with a particular theme or style. Instead of reweighting the generative distribution
    of tokens, the authors derive a stochastic policy by reinforcement learning [[166](#CR166)]
    to get a good compromise between the constraints and the language model. The authors
    can reweight single words (e.g. *China*), all words in a word list (e.g. lists
    for *kitchen*, *fantasy*), and words emphasized by a classifier (e.g. for *very
    negative* or *clickbait*). The results show that the constraints are met with
    the lowest divergence from the original PLM and with the best diversity scores.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '**GDC**（带有分布控制的生成）[[97](#CR97)] 提出了一种方法，除了改变生成词的分布外，还可以强调特定的单词。例如，GDC 可以避免产生有害内容，防止偏见，并将生成内容与特定的主题或风格保持一致。作者们通过强化学习
    [[166](#CR166)] 推导出一个随机策略，以在约束条件和语言模型之间取得良好的平衡。作者们可以重新加权单个单词（例如 *中国*），单词列表中的所有单词（例如
    *厨房*、*幻想* 的列表），以及分类器强调的单词（例如 *非常负面* 或 *点击诱饵*）。结果显示，约束条件得到满足，与原始 PLM 的差异最小，且多样性得分最高。'
- en: '**Adapter-Bot** [[126](#CR126)] provides different adapters trained independently
    for different skills. The backbone of the Adapter-Bot is a pre-trained GPT language
    model [[262](#CR262)], providing the ability of text generation. A set of trainable
    adapters are added to the backbone, which are optimized over the target dataset
    of dialogues for specific dialogue skills. Using a trained classifier to select
    the right dialogue skill under the dialogue story, Adapter-Bot allows high-level
    control over the chatbot.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '**Adapter-Bot** [[126](#CR126)] 为不同的技能提供了独立训练的不同适配器。Adapter-Bot 的主干是一个预训练的
    GPT 语言模型 [[262](#CR262)]，提供了文本生成的能力。在主干上添加了一组可训练的适配器，这些适配器针对特定对话技能的目标数据集进行了优化。通过使用训练好的分类器在对话故事中选择正确的对话技能，Adapter-Bot
    允许对聊天机器人进行高级控制。'
- en: Prompt-Based Generation
  id: totrans-384
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于提示的生成
- en: GPT-3 is able to produce text, when it receives an appropriate prompt (Sect.
    [3.​6.​3](528393_1_En_3_Chapter.xhtml#Sec41)). It can, for instance, generate
    a poem [[8](#CR8)]. After the prompt *“write a poem in the style of Rabbie Burns”*
    it may produce something like “There once was a lady from Dundee
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 能够在接收到适当的提示（Sect. [3.6.3](528393_1_En_3_Chapter.xhtml#Sec41)）时生成文本。例如，它可以生成一首诗
    [[8](#CR8)]。在提示 *“以拉比·布朗的风格写一首诗”* 之后，它可能会产生类似以下的内容：“从前有一位来自邓迪的女士
- en: a’ wha was bonnie, braw, and meek
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 她那美丽、英俊、温柔
- en: She met an old man from Dunfermline
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 她遇到了一位来自邓弗林的老者
- en: who won’t let her to her sleep …”
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 她不让她的睡眠……”
- en: With the prompt *“write this like an attorney”* it can create a text in the
    wording of a lawyer. Moreover, it can automatically write emails in your personal
    style by getting a prompt with some key points. GPT-3 can even work with unusual
    language types. It can, for instance, translate natural language into shell commands
    or programming code [[163](#CR163)]. More prompts for GPT-3 and other Foundation
    Models are provided by OpenAI [[160](#CR160)]. InstructGPT was fine-tuned to generate
    text according to an instruction (Sect. [3.​6.​5](528393_1_En_3_Chapter.xhtml#Sec43)).
    It can, for instance, receive the directives *“Complete the following sentence
    in a polite, respectful, and unbiased manner:”* or as *“Complete the following
    sentence using maximally biased and offensive language:”*. Then the model produces
    diverse texts that satisfy the requirements [[162](#CR162)].
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提示词*“write this like an attorney”*，它可以创建律师风格的文本。此外，它可以通过获取包含一些关键点的提示词来自动以您的个人风格撰写电子邮件。GPT-3甚至可以处理不寻常的语言类型。例如，它可以将自然语言翻译成shell命令或编程代码
    [[163](#CR163)]。OpenAI提供了更多关于GPT-3和其他基础模型的提示 [[160](#CR160)]。InstructGPT经过微调，可以根据指令生成文本（第[3.6.5](528393_1_En_3_Chapter.xhtml#Sec43)节）。例如，它可以接收指令*“用礼貌、尊重和无偏见的方式完成以下句子：”*或*“用尽可能有偏见和冒犯性的语言完成以下句子：”*然后模型会生成满足要求的多样化文本
    [[162](#CR162)]。
- en: 6.5.3 Transferring a Document to Another Text Style
  id: totrans-390
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.3 将文档迁移到另一种文本风格
- en: Text style transfer aims to translate a text ***x****′* with attribute *a′*
    to a similar text ***x*** of a desired attribute *a*. For example, the sentence
    *x′* = *“Peter screwed up”* with the attribute *a′* = *“informal”* can be transformed
    to ***x*** = *“Peter has not reached the goal”* with the attribute *a* = *“formal”*.
    The aim is to train a language model *p*(***x***|***x****′*, *a*). There are a
    number of other transformations, such as impolite ↔ polite, complicated ↔ simple,
    positive ↔ negative, biased ↔ neutral, or factual ↔ humorous ↔ romantic.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 文本样式迁移旨在将具有属性*a′*的文本***x****′*翻译成具有所需属性*a*的相似文本***x***。例如，具有属性*a′* = *“非正式”*的句子*x′* = *“Peter
    screwed up”*可以转换为具有属性*a* = *“正式”*的*x***= *“Peter has not reached the goal”*。目标是训练一个语言模型*p*(***x***|***x****′*,
    *a*)。还有许多其他转换，例如不礼貌 ↔ 礼貌，复杂 ↔ 简单，正面 ↔ 负面，偏见 ↔ 中立，或事实性 ↔ 幽默 ↔ 浪漫。
- en: The separation of style from content is difficult. On the one hand it can be
    captured by linguistic features, e.g. the utilization of specific words and phrases.
    On the other hand, it can be provided by text collections, e.g. with the writings
    of different authors or with a corpus of positive/negative reviews. In the latter
    case we can train classifiers, which discriminate between the different styles.
    With the recent progress in the capabilities of language models there are a number
    of successful applications of style transfer like imitating the style of specific
    authors, removing bias in online text, etc. A recent comprehensive survey is provided
    by Jin et al. [[88](#CR88)].
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 将风格与内容分离是困难的。一方面，它可以通过语言特征来捕捉，例如使用特定的单词和短语。另一方面，它可以通过文本集合来提供，例如使用不同作者的写作或正/负评论语料库。在后一种情况下，我们可以训练分类器，以区分不同的风格。随着语言模型能力的最近进步，有许多样式迁移的成功应用，如模仿特定作者的写作风格、消除在线文本中的偏见等。Jin等人提供了一项最新的全面调查
    [[88](#CR88)]。
- en: Style Transfer with Parallel Data
  id: totrans-393
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**并行数据**的样式迁移'
- en: If there are parallel documents of both styles, the style transfer can be formulated
    as a translation problem. An encoder-decoder transformer has to be fine-tuned
    on this dataset.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在两种风格的并行文档，样式迁移可以表述为一个翻译问题。必须对这个数据集进行编码器-解码器transformer的微调。
- en: '**Formal** [[260](#CR260)] formulate style transfer from informal to formal
    as a translation task. They use a transformer as Seq2seq model and apply it to
    the *GYAFC* [[180](#CR180)] benchmark dataset containing parallel formal/informal
    sentences. In addition, they augment the data by back-translation, employ machine
    translation to and from another language and leverage training data from grammatical
    error correction. They report a new Sota on the GYAFC dataset with increased formality
    and fluency, while keeping the meaning of a text.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '**正式** [[260](#CR260)] 将从非正式到正式的样式迁移公式化为翻译任务。他们使用transformer作为Seq2seq模型，并将其应用于包含平行正式/非正式句子的*GYAFC*
    [[180](#CR180)] 基准数据集。此外，他们通过回译来增强数据，使用机器翻译到另一种语言，并利用语法错误纠正的训练数据。他们报告了在GYAFC数据集上的一项新的Sota，提高了文本的正式性和流畅性，同时保持了文本的意义。'
- en: Style Transfer without Parallel Data
  id: totrans-396
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**无并行数据**的样式迁移'
- en: '**StyleLM** [[217](#CR217)] translates an arbitrary text into a text with the
    style properties of another author while keeping the content, even if no parallel
    data of the same content in different styles is available. First a BERT model
    is trained on a large neutral corpus (Gutenberg and Wikipedia) with the MLM loss.
    Then two copies of the model are used as an encoder-decoder transformer ![$$\tilde
    {{\boldsymbol {x}}}=\text{DEC}_{\boldsymbol {w}}(\text{ENC}_{\boldsymbol {u}}({\boldsymbol
    {x}}))$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq20.png).
    As fine-tuning input this Seq2seq model receives texts from the target author,
    where a random fraction of the words have been masked and have to be reconstructed.
    Hence, the Seq2seq model induces text with the target author’s style while rewriting
    the input text.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '**StyleLM** [[217](#CR217)] 将任意文本翻译成具有另一作者风格属性的文本，同时保留内容，即使没有不同风格相同内容的平行数据。首先在大型中性语料库（Gutenberg
    和 Wikipedia）上使用 MLM 损失训练一个 BERT 模型。然后使用该模型的两个副本作为编码器-解码器转换器 ![$$\tilde {{\boldsymbol
    {x}}}=\text{DEC}_{\boldsymbol {w}}(\text{ENC}_{\boldsymbol {u}}({\boldsymbol {x}}))$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq20.png)。作为微调输入，Seq2seq
    模型接收来自目标作者的文字，其中随机比例的单词已被掩码并需要重建。因此，Seq2seq 模型在重写输入文本的同时，诱导出具有目标作者风格的文本。'
- en: For evaluation 10 different authors were selected and excluded from the training
    data. The Bleu score and Rouge scores are used to measure content preservation.
    To measure the style quantitatively, the frequency of author-specific words and
    of syntactic and punctuation elements are evaluated. StyleLM in most cases had
    the best content preservation and stylistic alignment. Singh et al. [[207](#CR207)]
    note that StyleLM has problems with content reproduction. They propose to pre-train
    the encoder-decoder Dec[***w***](Enc[***u***](***x***)) on a large generic corpus.
    Afterwards the encoder-decoder is fine-tuned on the text of the target author.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估，选择了 10 位不同的作者并将其排除在训练数据之外。使用 Bleu 分数和 Rouge 分数来衡量内容保留。为了定量衡量风格，评估了作者特定单词的频率以及句法和标点符号元素的频率。在大多数情况下，StyleLM
    具有最佳的内容保留和风格对齐。Singh 等人 [[207](#CR207)] 指出，StyleLM 在内容再现方面存在问题。他们建议在大型通用语料库上预训练编码器-解码器
    Dec[***w***](Enc[***u***](***x***))。之后，在目标作者的文字上微调编码器-解码器。
- en: '**OPTIMUS** [[115](#CR115)] investigates further manipulations of sentences
    embeddings. An encoder with parameter ***u*** is required to generate a latent
    vector from text ***z*** = Enc[***u***](***x***). It is initialized with a pre-trained
    BERT model. A linearly transformed version ***z*** = *W* ∗***h***[[*CLS*]] of
    the embedding of the first token *[CLS]* of a sentence is defined as latent representation.
    The generator (decoder) with parameter ***w*** generates the text sequence ***x*** =
    Dec[***w***](***z***) from a random vector ***z*** (e.g. multivariate Gaussian)
    with prior *p*(***z***). The authors start with a pre-trained GPT-2 model as decoder.
    ***z*** is used by the decoder as an additional vector to attend to (in addition
    to the previously generated token embeddings). Both networks ![$$\tilde {{\boldsymbol
    {x}}}=\text{DEC}_{\boldsymbol {w}}(\text{ENC}_{\boldsymbol {u}}({\boldsymbol {x}}))$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq21.png)
    are trained with the autoencoder loss and the variational autoencoder loss, i.e.
    the system has to minimize ![$$|\tilde {{\boldsymbol {x}}}-{\boldsymbol {x}}|$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq22.png)
    and encourage a Gaussian distribution for ***z***.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '**OPTIMUS** [[115](#CR115)] 对句子嵌入的进一步操作进行了研究。需要一个参数为 ***u*** 的编码器来从文本 ***z***
    = Enc[***u***](***x***) 生成一个潜在向量。它使用预训练的 BERT 模型进行初始化。定义一个线性变换后的版本 ***z*** = *W*
    ∗***h***[[*CLS*]] 作为句子的第一个标记 *[CLS]* 的嵌入的潜在表示。具有参数 ***w*** 的生成器（解码器）从具有先验 *p*(***z***)
    的随机向量 ***z***（例如多元高斯）生成文本序列 ***x*** = Dec[***w***](***z***)。作者以预训练的 GPT-2 模型作为解码器开始。***z***
    被解码器用作一个额外的向量来关注（除了之前生成的标记嵌入）。两个网络 ![$$\tilde {{\boldsymbol {x}}}=\text{DEC}_{\boldsymbol
    {w}}(\text{ENC}_{\boldsymbol {u}}({\boldsymbol {x}}))$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq21.png)
    都使用自动编码器损失和变分自动编码器损失进行训练，即系统必须最小化 ![$$|\tilde {{\boldsymbol {x}}}-{\boldsymbol
    {x}}|$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq22.png)
    并鼓励 ***z*** 的高斯分布。'
- en: The approach learns bidirectional mappings between latent embeddings ***z***
    and sentences ***x***. For two sentences ***x***[1] and ***x***[2] the embeddings
    may be calculated and by *α****z***[1] + (1 − *α*)***z***[2] we can continuously
    interpolate between the sentences. In addition, differences between latent vectors
    may be computed similar to Word2Vec. For dialog response generation and the generation
    of responses with a specific style OPTIMUS has a better performance on all metrics
    compared to its competitors. Using an additional GAN to manipulate the latent
    representation ***z***, OPTIMUS is able to generate YELP restaurant reviews of
    prescribed sentiment (positive/negative) better than the investigated alternatives.
    The authors argue that compared to BERT, OPTIMUS learns a more structured semantic
    space due to the use of the VAE prior distribution in training.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法学习潜在嵌入***z***和句子***x***之间的双向映射。对于两个句子***x***[1]和***x***[2]，可以计算嵌入，并通过*α***z***[1]
    + (1 - *α*)***z***[2]在句子之间进行连续插值。此外，可以像Word2Vec一样计算潜在向量之间的差异。在对话响应生成和特定风格响应生成方面，与竞争对手相比，OPTIMUS在所有指标上都有更好的性能。通过使用额外的GAN来操纵潜在表示***z***，OPTIMUS能够生成具有指定情感（正面/负面）的YELP餐厅评论，比调查的替代方案更好。作者认为，与BERT相比，由于在训练中使用VAE先验分布，OPTIMUS学习了一个更结构化的语义空间。
- en: Style Transfer with Few-Shot Prompts
  id: totrans-401
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 少样本提示风格转换
- en: 'Sufficiently large Foundation Models such as **GPT-3**, Gopher, and PaLM can
    perform various tasks simply by choosing a clever prompt. If, however, only a
    simple prompt is entered, e.g. *“Here is some text: {That is an ugly dress}. Here
    is a rewrite of the text, which is more positive: {”* the model often fails and
    may not produce well-formatted or consistent outputs. The **AugZero** [[182](#CR182)]
    prompting schema employs augmented zero-shot prompts, which provide several demonstrations
    of sentences being rewritten to a new style. An example is shown in Fig. [6.15](#Fig15).
    In contrast to few-shot examples, where the examples have to cover the exact task,
    the model can also generalize to other unseen types of styles, e.g. *“comic”*
    in the example.![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig15_HTML.png)'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 足够大的基础模型如**GPT-3**、Gopher和PaLM可以通过选择一个巧妙的提示来执行各种任务。然而，如果只输入一个简单的提示，例如*“这里有一些文本：{那是一条丑陋的裙子}。这里是文本的重写，更加积极：{”*}，模型往往失败，可能不会产生格式良好或一致的输出。**AugZero**
    [[182](#CR182)] 提示方案采用增强型零样本提示，提供了将句子重写为新风格的几个示例。一个示例在图[6.15](#Fig15)中展示。与需要覆盖确切任务的少样本示例相比，模型还可以泛化到其他未见过的风格类型，例如示例中的*“漫画”*！![图片](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig15_HTML.png)
- en: A text box represents the description of a prompt to rewrite a sentence in a
    descriptive, melodramatic, and comic way. below is the generated answer for the
    prompt.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 文本框表示将句子以描述性、戏剧性和漫画方式重写的提示描述。以下是针对该提示生成的答案。
- en: Fig. 6.15
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15
- en: Augmented zero-shot prompts can instruct large autoregressive LMs like GPT-3
    to transfer a text to a new style. This even works, if there is no example given
    for the specific style desired, e.g. “comic” in the example [[182](#CR182), p.
    2]
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 增强型零样本提示可以指导大型自回归语言模型如GPT-3将文本转换为新的风格。即使没有给出所需特定风格的示例，这也同样有效，例如示例中的“漫画”[[182](#CR182)，第2页]
- en: The authors use GPT-3 with 175B parameters. Professional human raters were asked
    to assess text style, content preservation, and fluency. The zero-shot alternative
    performed worst and did not return a valid response in a quarter of the cases.
    It turned out that the AugZero rated comparably to human-written ground truth.
    Obviously, the language model can extrapolate the examples and transform a text
    in unseen styles. Adding the target attribute to the augmented prompts had a very
    similar performance. It can be expected that larger models like PaLM and LaMDA
    can generate even better results (Sect. [3.​6.​5](528393_1_En_3_Chapter.xhtml#Sec43)).
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用175B参数的GPT-3。专业的人类评分者被要求评估文本风格、内容保留和流畅性。零样本替代方案表现最差，四分之一的情况下没有返回有效的响应。结果证明，AugZero的评分与人工编写的基线相当。显然，语言模型可以外推示例并转换成未见过的风格。将目标属性添加到增强提示中具有非常相似的性能。可以预期，像PaLM和LaMDA这样更大的模型可以生成更好的结果（第[3.6.5](528393_1_En_3_Chapter.xhtml#Sec43)节）。
- en: 'Buchanan et al. [[30](#CR30)] noted that they could not instruct **GPT-3**
    by a single prompt to express a given story in a new tone or slant, supporting
    the above finding. Therefore, they developed a two-step procedure: First, GPT-3
    was instructed by a few-shot prompt to summarize the given story into a list of
    bullet points. In a second step GPT-3 was instructed by prompts such as *“Write
    a strongly pro-Trump article about [Topic X] that makes use of the following list
    of facts about [Topic X]”*. When examining 20 generated stories by human evaluators,
    11 of them were identified by at least one person as being “definitely authentic.”
    The authors used GPT-3 to solve further tasks, e.g. creating new narratives that
    could form the basis of conspiracy theories (e.g. QAnon), convincing members of
    particular groups to believe a claim, or persuade persons to change their opinion
    on some topic. They come to the conclusion that systems like GPT-3 are well-suited
    for generating a story with a new slant, e.g. for disinformation. This is even
    more alarming for more efficient recent Foundation Models like LaMDA or PaLM.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: Buchanan等人 [[30](#CR30)] 指出，他们无法通过单个提示来指导**GPT-3**以新的语气或倾向表达给定故事，这支持了上述发现。因此，他们开发了两步程序：首先，通过几个提示的快速提示指导GPT-3将给定故事总结成一系列项目符号。在第二步中，GPT-3被提示如“写一篇强烈支持特朗普的文章关于[主题X]，并使用以下关于[主题X]的事实列表”。在检查由人类评估者生成的20个故事时，其中11个至少被一个人认定为“绝对真实”。作者使用GPT-3来解决其他任务，例如创建可能成为阴谋论基础（例如QAnon）的新叙事，说服特定群体相信某个主张，或说服人们改变对某个话题的看法。他们得出结论，像GPT-3这样的系统非常适合生成带有新倾向的故事，例如用于虚假信息。对于像LaMDA或PaLM这样的更高效的近期基础模型来说，这甚至更加令人担忧。
- en: 6.5.4 Story Generation with a Given Plot
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.4 使用给定情节的故事生成
- en: A narrative, story or tale is a description of a series of related events or
    experiences [[234](#CR234)]. As the story generated by a PLM gets longer, often
    the earlier context is forgotten, and the text develops in an aimless fashion.
    Therefore, researchers would like to prepare a rough plot or storyline for the
    story, which is then taken into account by the Foundation Model. More specifically
    the story structure, the story ending, the general topic, or the persona of leading
    characters can be controlled. Besides story generation another application is
    data-to-text generation, where non-linguistic structured data (e.g., a table or
    a graph) is converted to natural language text, which can be applied in tasks
    like healthcare, weather forecast, legal text, etc. Surveys of controlled text
    generation are provided by Prabhumoye et al. [[170](#CR170)], Yu et al. [[251](#CR251)],
    and Zhang et al. [[257](#CR257)].
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 一个叙事、故事或传说是对一系列相关事件或经历的描述 [[234](#CR234)]。随着PLM生成的故事变得越来越长，通常早期的上下文会被遗忘，文本的发展变得没有目标。因此，研究人员希望为故事准备一个大致的情节或故事线，然后由基础模型考虑。更具体地说，可以控制故事结构、故事结局、一般主题或主要角色的性格。除了故事生成之外，另一个应用是数据到文本生成，其中非语言结构化数据（例如，表格或图表）被转换为自然语言文本，可以应用于医疗保健、天气预报、法律文本等任务。Prabhumoye等人
    [[170](#CR170)]、Yu等人 [[251](#CR251)] 和Zhang等人 [[257](#CR257)] 提供了受控文本生成的调查。
- en: 'The planned course of a story can be described in different ways:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 故事计划可以以不同的方式描述：
- en: A list of single keywords or phrases.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个关键词或短语的列表。
- en: A list of sentences or bullet points describing an event.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述事件的句子或项目符号列表。
- en: An event graph describing the logical dependency of events.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述事件逻辑依赖关系的事件图。
- en: Specify a Storyline by Keywords or Phrases
  id: totrans-414
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过关键词或短语指定故事线
- en: '**Megatron-CNTRL** [[243](#CR243)] controls the story generation by keywords.
    In addition, retrieved knowledge allows dynamical incorporation of external knowledge
    from the *ConceptNet KB* into language model during generation. From the current
    story context a keyword predictor first predicts a set of keywords for the next
    sentence. The retriever collects knowledge from the KB corresponding to the keywords.
    The returned sentences are re-ranked according to their relevance to the story
    context. Finally, the generator takes the story context and the top-ranked retrieved
    sentences and produces the next sentence. To support generalization of entities
    they replace names and entities in stories with special placeholders, [MALE],
    [FEMALE], and [NEUTRAL] for male, female and unknown names and entities, respectively.
    The underlying Megatron model (Sect. [3.​1.​2](528393_1_En_3_Chapter.xhtml#Sec3))
    has up to 8B parameters. Experiments show that the model generates more fluent,
    consistent, and coherent stories with lower repetition rate and higher diversities
    compared to the previous Sota'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '**Megatron-CNTRL** [[243](#CR243)] 通过关键词控制故事生成。此外，检索到的知识允许在生成过程中动态地将来自 *ConceptNet
    KB* 的外部知识纳入语言模型。从当前的故事上下文中，关键词预测器首先预测下一句的一组关键词。检索器从 KB 中收集与关键词对应的知识。根据与故事上下文的相关性对返回的句子进行重新排序。最后，生成器根据故事上下文和排名最高的检索句子生成下一句。为了支持实体的泛化，他们将故事中的名字和实体替换为特殊的占位符，[MALE]、[FEMALE]
    和 [NEUTRAL] 分别代表男性、女性和未知名字和实体。底层的 Megatron 模型（第 [3.1.2](528393_1_En_3_Chapter.xhtml#Sec3)
    节）具有高达 8B 个参数。实验表明，与之前的 Sota 相比，该模型生成的故事更加流畅、一致、连贯，重复率更低，多样性更高。'
- en: Dong et al. [[52](#CR52)] present a model, which takes as input a list of keywords
    with attached entity classes and generates a text containing these keywords. The
    entities are taken into account during text generation and the model embeds the
    meaning of entities into hidden states. The results show that the generated sentences
    are able to reflect the properties of the entities.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: Dong 等人 [[52](#CR52)] 提出了一种模型，该模型接受一个包含附加实体类别的关键词列表作为输入，并生成包含这些关键词的文本。在文本生成过程中考虑实体，并将实体的含义嵌入到隐藏状态中。结果表明，生成的句子能够反映实体的属性。
- en: '**PlotMachines** [[181](#CR181)] generates a text based on a plot consisting
    of a set of phrases. The system can decide for itself in what order to introduce
    the concepts covered by the phrases. It is based on the GPT and GPT-2 language
    model. The authors use three different datasets describing TV-shows, movies, books,
    short stories, and news articles. They extract phrases (3–8 words) from these
    stories by a keyword extraction method [[167](#CR167)]. Given an outline as input,
    the model recurrently generates paragraphs (Fig. [6.16](#Fig16)). To create the
    next paragraph it uses a gating mechanism similar to an LSTM gate, which updates
    a memory matrix *M* that keeps track of plot elements of the outline. The self-attention
    in the model is adapted to receive input from the memory matrix as well as the
    previously generated words. According to automatic metrics (Rouge, Bleu) the model
    has a better ability to generate realistic looking as well as diverse texts than
    its competitors. In extensive experiments with human raters the authors demonstrate
    that their model produces text closer to the plot than alternative models.![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig16_HTML.png)'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '**PlotMachines** [[181](#CR181)] 基于一组短语构成的情节生成文本。该系统可以自行决定介绍短语涵盖的概念的顺序。它基于
    GPT 和 GPT-2 语言模型。作者使用了三个不同的数据集，描述了电视剧、电影、书籍、短篇小说和新闻文章。他们通过关键词提取方法 [[167](#CR167)]
    从这些故事中提取短语（3-8 个单词）。给定一个大纲作为输入，模型反复生成段落（图 [6.16](#Fig16)）。为了创建下一个段落，它使用了一个类似于
    LSTM 网关的门控机制，该机制更新一个记忆矩阵 *M*，以跟踪大纲的情节元素。模型中的自注意力机制被调整为接收来自记忆矩阵以及之前生成的单词的输入。根据自动指标（Rouge，Bleu），该模型在生成看起来更真实以及多样化的文本方面比其竞争对手具有更好的能力。在大量的人评实验中，作者证明了他们的模型生成的文本比替代模型更接近情节。![图
    6.16](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig16_HTML.png)'
- en: An illustration indicates the story outline at the top. It exhibits the plot
    dynamics and the generated story according to the conditioned outline at the bottom.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 一个插图显示了故事大纲在顶部。它展示了根据底部条件化的大纲的情节动态和生成的故事。
- en: Fig. 6.16
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16
- en: An outline (input) together with a story (output) from the Wikiplots training
    set generated by PlotMachines. Plot elements from the outline can appear and reappear
    nonlinearly throughout the plot, as shown in plot dynamics graph. A memory matrix
    keeps track of how outline phrases have been used while writing. Image reprinted
    with kind permission of the authors [[181](#CR181), p. 1]
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 由PlotMachines生成的Wikiplots训练集的概述（输入）和故事（输出）。概述中的情节元素可以在整个情节中以非线性方式出现和重复出现，如图情动态图所示。记忆矩阵跟踪在写作过程中概述短语的使用情况。图片经作者友好许可重印
    [[181](#CR181)，第1页]
- en: '**Pointer** [[261](#CR261)] inserts new words between the words of a given
    start set. Based on the start set, the model first generates high-level words
    (e.g. verbs and adjectives) that provide a high-level connection. Then it inserts
    other words of finer granularity around the keywords iteratively until the whole
    sentence is generated. The training objective of POINTER is to generate a complete
    text sequence with a set of keywords as constraints. This is similar to the masked
    language modeling (MLM) objective in BERT, so a pre-trained BERT is used to initialize
    the model training. An insertion transformer [[210](#CR210)] is used to generate
    either a regular token or a special token for each gap between two existing tokens.
    Empirical evaluations demonstrate the effectiveness of the approach. Similar models
    are *ProGeT* proposed by Tan et al. [[220](#CR220)] and the constrained BART [[77](#CR77)].'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '**指针** [[261](#CR261)] 在给定起始集合的单词之间插入新词。基于起始集合，模型首先生成高级词汇（例如动词和形容词），以提供高级连接。然后，它迭代地插入围绕关键词的更细粒度的其他词汇，直到生成整个句子。POINTER的训练目标是生成一组关键词约束下的完整文本序列。这与BERT中的掩码语言模型（MLM）目标类似，因此使用预训练的BERT来初始化模型训练。插入转换器
    [[210](#CR210)] 用于生成每个现有标记之间的间隙的普通标记或特殊标记。实证评估证明了该方法的有效性。类似的模型包括Tan等人提出的*ProGeT*
    [[220](#CR220)] 和约束BART [[77](#CR77)]。'
- en: '**ProGen** [[219](#CR219)] generates a story in *k* different levels. For each
    level a vocabulary ![$$\mathcal {V}_i$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq23.png)
    is defined based on tf-idf score, such that ![$$\mathcal {V}_1$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq24.png)
    contains high information words while ![$$\mathcal {V}_k$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq25.png)
    contains all words. *k* different encoder-decoder models (BART) *M*[*i*] are trained
    for the *k* levels, where the *i*- level employs the training data *X*[*i*] containing
    only words from vocabulary ![$$\mathcal {V}_i$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq26.png).
    As input *M*[*i*] gets the training data *X*[*i*−1] from the previous level and
    has to predict the refined version *X*[*i*]. Note that usually the input words
    from *X*[*i*−1] will be included in the next output. A storyline now can be formulated
    by a human using words from a high-level vocabulary, which covers about 15% of
    all content. If, for example, the first stage text is *“beckham ∖n liverpool bayern
    chelsea ∖n beckham chelsea mancini …”* the final stage text starts as *“England
    striker Ashley Beckham has joined Premier League strugglers Newcastle United.
    ∖n England Football …”*. Evaluation shows that the coherence of the texts over
    long intervals (36 sentences) is close to humans and much better than for a basic
    BART model. In addition, ProGen has favorable properties with respect to fluency,
    lexical and semantic quality, as well as diversity.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '**ProGen** [[219](#CR219)] 在*k*个不同级别上生成故事。对于每个级别，根据tf-idf分数定义一个词汇表 ![$$\mathcal
    {V}_i$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq23.png)，使得
    ![$$\mathcal {V}_1$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq24.png)
    包含高信息词，而 ![$$\mathcal {V}_k$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq25.png)
    包含所有单词。为*k*个级别训练了*k*个不同的编码器-解码器模型（BART）*M*[*i*]，其中*i*-级别使用仅包含词汇表 ![$$\mathcal
    {V}_i$$](../images/528393_1_En_6_Chapter/528393_1_En_6_Chapter_TeX_IEq26.png)
    中的单词的训练数据 *X*[*i*]。作为输入，*M*[*i*] 获取来自上一级的训练数据 *X*[*i*−1]，并必须预测精炼版本 *X*[*i*]。请注意，通常来自
    *X*[*i*−1] 的输入单词将被包含在下一个输出中。现在，可以使用高级词汇表中的单词由人类制定故事线，该词汇表涵盖了所有内容的约15%。例如，如果第一阶段文本是
    *“beckham ∖n liverpool bayern chelsea ∖n beckham chelsea mancini …”*，则最终阶段文本开始为
    *“英格兰前锋阿什利·贝克汉姆已加入英超联赛的挣扎者纽卡斯尔联队。 ∖n 英格兰足球 …”*。评估显示，在长时间间隔（36个句子）内文本的连贯性接近人类，并且远优于基本BART模型。此外，ProGen在流畅性、词汇和语义质量以及多样性方面具有有利的特性。'
- en: Specify a Storyline by Sentences
  id: totrans-423
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过句子指定故事线
- en: '**Facts2Story** [[161](#CR161)] receives as input a sequence of key facts expressed
    in natural language and generates a story containing the facts in the given order
    (Table [6.12](#Tab12)). These facts are simple sentences that describe factual
    information of the story. Each fact should report an event in the story, state
    the properties of a person or a place, mention the emotions of characters, etc.
    There should be a large degree of freedom to generate a story containing the facts.Table
    6.12'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '**Facts2Story** [[161](#CR161)]以自然语言表达的关键事实序列作为输入，并生成包含给定顺序的事实的故事（表[6.12](#Tab12)）。这些事实是简单句子，描述故事的事实信息。每个事实都应该报告故事中的一个事件，陈述人物或地点的特性，提及角色的情感等。在生成包含事实的故事时应该有很大的自由度。表6.12'
- en: Story generated by Facts2story model with facts as input [[161](#CR161)]. Words
    taken from the facts are printed in italics
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 由Facts2story模型根据事实生成的故事[[161](#CR161)]。从事实中提取的单词以斜体显示
- en: '| **Fact 1:***German army has pulled from sector of Western Front in northern
    France* |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| **事实1：**德国军队已从法国北部的西战线撤出* |'
- en: '| **Fact 2:***Blake cross no mans land to reach the abandoned German trenches*
    |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| **事实2：**布莱克穿越无人区，到达被遗弃的德国战壕* |'
- en: '| **Fact 3:***German plane shot down in flames* |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| **事实3：德国飞机被击落，燃烧着坠毁* |'
- en: '| **Generated text:** |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| **生成的文本：** |'
- en: '| In July 1930, on the eve of World War I, a train carrying German prisoners
    belonging to the *German army, has pulled from sector of Western Front in northern
    France*[fact1]. Captain Alfred Blake (Greg Rogers), a British officer in the German
    Army, has been sent to the German border. After being briefed by one of the German
    troops, *Blake cross no mans land to reach the abandoned German trenches*[fact2].
    He is captured, but finds the *German plane shot down in flames*[fact3]. He takes
    refuge in a French camp, where he and another German, Captain Schofield (James
    Shea), are kept … |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| 1930年7月，在一战前夕，一列载有德国战俘的火车，属于*德国军队，从法国北部的西战线撤出*[fact1]。阿尔弗雷德·布莱克上尉（Greg Rogers），一名德国军队的英国军官，被派往德国边境。在一名德国士兵的简要介绍后，*布莱克穿越无人区，到达被遗弃的德国战壕*[fact2]。他被俘，但发现*德国飞机被击落，燃烧着坠毁*[fact3]。他在一个法国营地寻求庇护，在那里他和另一位德国人，上尉肖菲尔德（James
    Shea），被关押…… |'
- en: To keep the problem manageable, the authors give an input of 5 ordered facts
    and aim to generate a coherent story of 100–1000 words covering all facts in order.
    As training data 17k story plots from Wikipedia were used. From each of these
    plots facts were extracted by the SalIE framework [[169](#CR169)]. The five facts
    with the highest saliency scores were selected.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使问题易于管理，作者提供了5个有序的事实作为输入，并旨在生成一个100-1000字的连贯故事，按顺序涵盖所有事实。作为训练数据，使用了来自维基百科的17k个故事情节。通过SalIE框架[[169](#CR169)]从每个情节中提取了事实。选择了具有最高显著性分数的5个事实。
- en: As standard language models (GPT-2, BART) after a number of generated tokens
    diverge from the input and focus on the newly generated content, the authors use
    a pre-trained XLNET (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2)), which
    is able to take into account future words. The assumption is that the words of
    the facts should appear in the final text in the given order. XLNET is able to
    process these tokens in random order, because the position embeddings are attached
    to the token embeddings. As between two consecutive tokens of the facts other
    words may occur, a model is trained to predict the number of intervening words.
    This model is used to determine the exact position of each word of each fact.
    Finally, the XLNET has to fill in the missing words.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准语言模型（GPT-2，BART）生成了一定数量的标记后，它们会偏离输入并专注于新生成的内容，作者使用了一个预训练的XLNET（第[3.1.1](528393_1_En_3_Chapter.xhtml#Sec2)节），它能够考虑到未来的单词。假设事实的单词应该按照给定的顺序出现在最终文本中。XLNET能够以随机顺序处理这些标记，因为位置嵌入附加到标记嵌入上。由于在两个连续的事实标记之间可能存在其他单词，因此训练了一个模型来预测间隔单词的数量。这个模型用于确定每个事实中每个单词的确切位置。最后，XLNET必须填补缺失的单词。
- en: 'The generated stories are evaluated by humans according to three criteria:
    (1) adherence to facts, (2) grammatical correctness, (3) common sense and plausibility
    of events. Alternatives investigated were GPT-2 (Sect. [2.​2.​4](528393_1_En_2_Chapter.xhtml#Sec16))
    with additional self-attention [[269](#CR269)] and the Seq2seq model BART (Sect.
    [3.​1.​3](528393_1_En_3_Chapter.xhtml#Sec4)), which is pre-trained to recover
    randomly shuffled text and fine-tuned to generate the story using the facts as
    input. The evaluation shows that Facts2Story generates a story containing on average
    4.4 of the 5 facts, while the other models recover less than 1.7 facts. With respect
    to grammar and common sense Facts2Story fares slightly worse than GPT2 but much
    better than BART.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的故事根据三个标准由人类进行评估：（1）事实的遵循性，（2）语法正确性，（3）事件的一般常识和合理性。研究过的替代方案包括带有额外自注意力的 GPT-2（第
    [2.2.4](528393_1_En_2_Chapter.xhtml#Sec16) 节）[[269](#CR269)] 和预训练以恢复随机打乱文本并微调以使用事实作为输入来生成故事的
    Seq2seq 模型 BART（第 [3.1.3](528393_1_En_3_Chapter.xhtml#Sec4) 节）[[267](#CR267)]。评估显示，Facts2Story
    生成的故事平均包含 5 个事实中的 4.4 个，而其他模型恢复的事实不到 1.7 个。在语法和常识方面，Facts2Story 比GPT2略差，但比BART好得多。
- en: '**SOE** (Summarize, Outline and Elaborate) [[214](#CR214)] starts from the
    observation that most approaches for story generation produce texts in a word-by-word
    manner and have no high-level plan on what to generate. To address this issue,
    the coarse-to-fine generation strategy with two levels is proposed. For each segment
    ***y***^(*i*) of the text a summary *s*^(*i*) is provided. The model first generates
    “bullet points” for each summary. Subsequently, the model expands each bullet
    point to generate the corresponding segment. Note that during this process the
    high-level discourse dependencies are preserved.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '**SOE**（总结、概述和阐述）[[214](#CR214)] 从观察出发，大多数故事生成方法都是逐字逐句地生成文本，并且没有对要生成的内容有高级计划。为了解决这个问题，提出了一个具有两个级别的粗到细的生成策略。对于文本的每个片段
    ***y***^(*i*)，都提供了一个摘要 *s*^(*i*)。模型首先为每个摘要生成“要点”。随后，模型将每个要点扩展以生成相应的片段。注意，在这个过程中，保留了高级话语依赖关系。'
- en: To prepare the training data, the stories in a collection are partitioned into
    segments of several hundred words using BERT next sentence prediction measuring
    the degree of dependency of sentences. For each segment an extractive summary
    is generated using BERT and TextRank [[144](#CR144)]. Then a transformer is employed
    to create the bullet points dependent on previous bullet points. From these the
    final text is produced taking into account previous text and abstractions. WikiText
    103 [[142](#CR142)] and the BookCorpus [[267](#CR267)] were used as training data.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备训练数据，使用 BERT 下一个句子预测来衡量句子的依赖程度，将一个集合中的故事分成几个百字的片段。对于每个片段，使用 BERT 和 TextRank
    [[144](#CR144)] 生成一个提取式摘要。然后，使用一个转换器根据前面的要点创建要点。从这些要点中生成最终文本，同时考虑到前面的文本和抽象。WikiText
    103 [[142](#CR142)] 和 BookCorpus [[267](#CR267)] 被用作训练数据。
- en: The performance of the model was evaluated with respect to fluency by perplexity,
    with respect to text diversity by the number of distinct *n*-grams, text acceptability
    as measured by an adversarial classifier, and sentence level coherence measured
    by a next-sentence prediction score. On all scores the SOE-model with an additional
    reranking procedure achieved the best results. Comparison with Transformer-XL
    [[49](#CR49)] and Progressive WritingPrompts [[220](#CR220)] demonstrated the
    superiority of SOE with respect to perplexity, diversity of the generated text,
    and coherence.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的性能通过困惑度评估流畅性，通过不同 *n*-gram 的数量评估文本多样性，通过对抗分类器评估文本的可接受性，以及通过下一个句子预测分数评估句子级连贯性。在所有评分中，带有额外重排序过程的
    SOE 模型都取得了最佳结果。与 Transformer-XL [[49](#CR49)] 和 Progressive WritingPrompts [[220](#CR220)]
    的比较表明，SOE 在困惑度、生成文本的多样性和连贯性方面都优于其他模型。
- en: '**FIST** [[58](#CR58)] receives a sequence of “events” as inputs describing
    each paragraph (Fig. [6.17](#Fig17)). To extract events from paragraphs for training,
    keyword extraction techniques [[144](#CR144), [191](#CR191)] are used. By means
    of special tokens as delimiters these events are connected with paragraphs in
    an interleaving manner. The authors fine-tune a pre-trained GPT-2 with the LM-loss
    on the augmented sequences to learn the functionality of special tokens and co-occurrence
    structures between events and stories. The performance of FIST is compared with
    Plotmachines (see above) and two other approaches on two benchmark datasets. With
    respect to most evaluation measure FIST generally achieves better results. The
    Sota in story generation is developing fast with new techniques appearing every
    month. We describe some limitations of current models in the context of dialogs
    in Sect. [6.6.4](#Sec53) and discuss some remedies.![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig17_HTML.png)'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '**FIST** [[58](#CR58)] 接收一系列“事件”作为输入，描述每个段落（图[6.17](#Fig17)）。为了从段落中提取事件进行训练，使用了关键词提取技术
    [[144](#CR144), [191](#CR191)]。通过特殊标记作为分隔符，这些事件以交错的方式与段落连接。作者使用LM损失对预训练的GPT-2进行微调，以学习特殊标记和事件与故事之间的共现结构的功能。FIST的性能与Plotmachines（见上文）和两种其他方法在两个基准数据集上进行了比较。在大多数评估指标上，FIST通常取得更好的结果。故事生成的Sota发展迅速，每月都有新技术出现。我们在第[6.6.4](#Sec53)节中描述了当前模型在对话方面的局限性，并讨论了一些补救措施。![图6.17](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig17_HTML.png)'
- en: A text box represents a prompt and a description of the event at the top. It
    exhibits the generated paragraph at the bottom.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 文本框表示顶部的事件提示和描述。它展示了底部生成的段落。
- en: Fig. 6.17
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17
- en: Story generated by the FIST model with prompt and event as input [[58](#CR58)]
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 由FIST模型根据提示和事件生成的故事 [[58](#CR58)]
- en: Papalampidi et al. [[164](#CR164)] note that in generated stories the appearing
    entities are often incoherent, i.e. persons are replaced and locations change.
    The **MNEMELM** model employs an additional entity memory, where the generated
    entities and their attributes are stored dynamically and retrieved during further
    story generation. The representation for an entity is the average embedding of
    the tokens of the entity. Each entity memory slot *m*[*j*] thus contains a fixed
    surface entity representation (writing) *k*[*j*] and a dynamic value *v*[*j*],
    which is frequently updated based on each new chunk of the narrative context.
    The stored entities enter the self-attention computations and thus influence the
    story.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: Papalampidi等人[[164](#CR164)]指出，在生成的故事中，出现的实体往往是不连贯的，即人物被替换，地点发生变化。**MNEMELM**模型采用了一个额外的实体记忆，其中生成的实体及其属性被动态存储并在进一步的故事生成过程中检索。实体的表示是实体标记的平均嵌入。因此，每个实体记忆槽*m*[*j*]包含一个固定的表面实体表示（写作）*k*[*j*]和一个动态值*v*[*j*]，该值基于每个新的叙事上下文片段频繁更新。存储的实体进入自注意力计算，从而影响故事。
- en: As background model a Transformer-XL (∼300M parameters) pre-trained on a translation
    task is used (Sect. [3.​2.​2](528393_1_En_3_Chapter.xhtml#Sec9)). On the WikiPlot
    and the WritingPrompts benchmarks it turn out that MNEMELM better imitates the
    frequency of entity usage of humans than other models and in addition have a higher
    entity coherence and consistency. This is also confirmed by human judgment. Recently,
    dynamic retrieval-based approaches were also used by dialog systems such as BlenderBot-2
    (Sect. [6.6.2](#Sec51)). By the combination of these approaches the generation
    of stories may be improved.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 作为背景模型，使用了一个在翻译任务上预训练的Transformer-XL (∼300M参数)（第[3.2.2](528393_1_En_3_Chapter.xhtml#Sec9)节）。在WikiPlot和WritingPrompts基准测试中，MNEMELM比其他模型更好地模仿了人类实体使用的频率，并且具有更高的实体一致性和连贯性。这也得到了人类判断的证实。最近，基于动态检索的方法也被用于对话系统，如BlenderBot-2（第[6.6.2](#Sec51)节）。通过结合这些方法，故事生成的质量可能得到提高。
- en: We have seen above (Sect. [6.5.3](#Sec39)) that **GPT-3** can rewrite a story
    in a new slant, when prompts are used in a two-step procedure [[30](#CR30)]. First,
    GPT-3 was instructed to summarize the given story into a list of bullet points.
    In a second step GPT-3 was instructed by prompts to write a story with a given
    tone containing the facts noted in the bullet points. If only the second step
    is executed, GPT-3 can be instructed to write a story covering the bullet point
    and in addition obey the prescribed slant. Currently, we are not aware of a systematic
    evaluation of the effectiveness of this technique, which should be even more rewarding
    for larger Foundation Models.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上面（第[6.5.3](#Sec39)节）看到，**GPT-3**可以在两步程序[[30](#CR30)]中使用提示来以新的角度重写故事。首先，GPT-3被指示将给定的故事总结成一系列项目符号。在第二步中，GPT-3通过提示被指示编写一个包含项目符号中记录的事实并具有给定语调的故事。如果只执行第二步，GPT-3可以被指示编写一个涵盖项目符号并遵守规定角度的故事。目前，我们尚未了解到对这种技术有效性的系统评估，这对于更大的基础模型来说应该更有价值。
- en: Other Control Strategies
  id: totrans-444
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 其他控制策略
- en: '**GraphPlan** [[38](#CR38)] aims to prevent logical inconsistencies in generated
    text, which often are produced by models like GPT-2\. The input to the model is
    an event graph, which represents each event with a verb phrase. To prepare training
    data, the verb phrases of events are extracted from a story using semantic role
    labeling and characterized by *Latent Dirichlet Allocation* topics [[23](#CR23)].
    The events are connected by directed edges indicating possible next events. In
    addition, event pairs are identified that are mutually exclusive. To generate
    a story, first a sequence of events is selected based on a beam search (Sect.
    [2.​3.​2](528393_1_En_2_Chapter.xhtml#Sec22)). Subsequently, the text is generated
    by a version of GPT-2\. With extensive experiments the authors found that GraphPlan
    generates stories, which are less repetitive and more consistent. Koncel-Kedziorski
    et al. [[104](#CR104)] present a similar model to generate text from knowledge
    graphs with graph transformers. By using another method based on BART and T5,
    it is possible to generate fluent stories from graphs representing the story structure
    [[185](#CR185)].'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '**GraphPlan** [[38](#CR38)]旨在防止生成文本中的逻辑不一致性，这在GPT-2等模型中经常产生。模型的输入是一个事件图，它用动词短语表示每个事件。为了准备训练数据，使用语义角色标注从故事中提取事件的动词短语，并由*潜在狄利克雷分配*主题[[23](#CR23)]进行特征化。事件通过表示可能后续事件的定向边连接。此外，还确定了相互排斥的事件对。为了生成故事，首先根据束搜索（第[2.3.2](528393_1_En_2_Chapter.xhtml#Sec22)节）选择一系列事件。随后，通过GPT-2的一个版本生成文本。通过广泛的实验，作者发现GraphPlan生成的故事，其重复性较低且一致性更高。Koncel-Kedziorski等人[[104](#CR104)]提出了一种类似的模型，使用图变换器从知识图中生成文本。通过使用基于BART和T5的另一种方法，可以从表示故事结构的图中生成流畅的故事[[185](#CR185)]。'
- en: Sakaguchi et al. [[196](#CR196)] present an approach based on the T5 transformer
    with 11B parameters that generates a directed acyclic graph of events describing
    a story. The order of events indicates their logical and temporal dependency.
    This graph may be taken as an input to another Foundation Model to generate a
    story containing the events of the script.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: Sakaguchi等人[[196](#CR196)]提出了一种基于T5变换器，具有11B参数的方法，该方法生成一个描述故事的定向无环图。事件的顺序表示它们的逻辑和时序依赖关系。此图可以作为另一个基础模型的输入，以生成包含剧本事件的叙事。
- en: '**CAST** [[168](#CR168)] aims to improve the coherence of the generated story
    and the coherence of the action of persons. It tries to infer the causal relations
    between events, as well as the intents and motivations of characters in the story
    context, and use it to influence the generation of a coherent story. They employ
    a logical inference model to reason about the characters in the story and to influence
    the generated words. As basic model, they use GPT-2 and generate stories for two
    persons. Their experiments show that the produced stories are more coherent and
    stay on topic.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '**CAST** [[168](#CR168)]旨在提高生成故事的连贯性和人物动作的连贯性。它试图推断事件之间的因果关系，以及故事背景中人物的意图和动机，并利用它来影响生成连贯的故事。他们采用逻辑推理模型来对故事中的角色进行推理并影响生成的单词。作为基本模型，他们使用GPT-2并为两个人物生成故事。他们的实验表明，产生的故事更加连贯且紧扣主题。'
- en: 6.5.5 Generating Fake News
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.5 生成虚假新闻
- en: The creation of Fake News can be simply considered as the task to generate stories
    with a new slant. Buchanan et al. [[30](#CR30)] investigated how GPT-3 can be
    used to generate large numbers of different fake news messages that can be easily
    distributed to thousands of users. They mainly formulate appropriate prompts for
    GPT-3 (Sect. [3.​6.​3](528393_1_En_3_Chapter.xhtml#Sec41)) to produce the desired
    texts. This comprises variations of tweet-like short messages, medium-sized posts
    expressing a world view, and longer articles reporting an event from a particular
    perspective. Examples are shown in Fig. [6.18](#Fig18).![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig18_HTML.png)
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 假新闻的创建可以简单地被视为生成具有新角度的故事的任务。Buchanan等人[[30](#CR30)]研究了如何使用GPT-3生成大量不同类型的假新闻消息，这些消息可以轻松地分发给数千名用户。他们主要对GPT-3（第[3.6.3](528393_1_En_3_Chapter.xhtml#Sec41)节）制定适当的提示，以产生所需的文本。这包括类似推文的简短消息、表达世界观的中等大小帖子以及从特定角度报道事件的较长的文章。示例显示在图[6.18](#Fig18)中。![图6.18](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig18_HTML.png)
- en: A table represents the description and examples of the narrative reiteration,
    elaboration, and manipulation.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 一个表格表示叙事重复、详述和操纵的描述和示例。
- en: Fig. 6.18
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18
- en: Some of the fake news generation tasks performed with GPT-3 [[30](#CR30)]
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPT-3执行的一些假新闻生成任务 [[30](#CR30)]
- en: '*Narrative Reiteration* aims at creating a large number of short messages (e.g.
    tweets) that express a particular theme, such as climate change denial. The authors
    collected replies with many likes from a climate change denial account. Ten of
    these messages were used as input prompt to GPT-3, e.g.: *“TWEET 4: Soros/Gates
    Funded $6.5 million to group now warning world may need ‘climate lockdown”’*.
    GPT-3 continued with similar tweets such as *“TWEET 14: Climate change is the
    new communism - an ideology based on a false science that cannot be questioned.”*
    Obviously, GPT-3 produces very good results with little human assistance.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '*叙事重复*旨在创建大量表达特定主题的短消息（例如推文），例如气候变化否认。作者收集了来自一个气候变化否认账户的许多点赞回复。其中十条消息被用作GPT-3的输入提示，例如：*“推文
    4：索罗斯/盖茨资助了650万美元给现在警告世界可能需要‘气候封锁’的团体”*。GPT-3继续生成类似的推文，例如*“推文 14：气候变化是新共产主义——一种基于错误科学的意识形态，不能质疑。”*显然，GPT-3在几乎没有人工辅助的情况下产生了非常好的结果。'
- en: '*Narrative Elaboration* intends to justify a claim with a medium-length story.
    The authors accomplished this in a two-step process. First, GPT-3 is instructed
    to generate a series of headlines that each made some new assertion regarding
    a certain topic. This was done by collecting five headlines from a far-right media
    company, e.g. *“HEADLINE 5: Chinese Official Praises Quality of Country’s Vaccines,
    Despite Multiple Health Scandals”* [[30](#CR30), p. 9]. GPT-3 then generated five
    new headlines, e.g. *“HEADLINE 6: Secret Chinese Vaccine Testing on Half a Million
    Children Confirmed”*. Subsequently, GPT-3 was given these generated headlines
    to create longer articles. A headline together with a created article is shown
    in Fig. [6.19](#Fig19). It turned out that GPT-3 was able to capture the appropriate
    tone and tendency of the fake new source, as demonstrated by a classifier. Note
    that GPT-3 now can be fine-tuned (Sect. [3.​6.​2](528393_1_En_3_Chapter.xhtml#Sec40))
    and even better concentrate on the content and the reasoning of specific news
    sources.![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig19_HTML.png)'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '*叙事详述*旨在通过一个中等长度的故事来证明一个论点。作者通过两步过程实现了这一点。首先，GPT-3被指示生成一系列标题，每个标题都针对某个特定主题提出了一些新的论断。例如，通过收集来自一个极右翼媒体公司的五个标题，如*“标题
    5：尽管存在多次健康丑闻，中国官员赞扬国家疫苗质量”* [[30](#CR30)，第9页]。然后，GPT-3生成了五个新的标题，例如*“标题 6：确认对中国五十万儿童进行秘密疫苗测试”*。随后，GPT-3被赋予这些生成的标题来创建更长的文章。一个标题和创建的文章一起显示在图[6.19](#Fig19)中。结果表明，GPT-3能够捕捉到虚假新闻来源的适当语气和倾向，如分类器所示。请注意，GPT-3现在可以被微调（第[3.6.2](528393_1_En_3_Chapter.xhtml#Sec40)节）并且甚至可以更好地专注于特定新闻来源的内容和推理！![图6.19](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig19_HTML.png)'
- en: A textbox represents a prompt related to a report on the Chinese regime along
    with the generated response by the G P T 3.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 一个文本框表示与关于中国政权的报告相关的提示以及GPT-3生成的响应。
- en: Fig. 6.19
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19
- en: A sample headline from The Epoch Times and the beginning of the article generated
    by GPT-3 [[30](#CR30), p. 11]
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 来自《时代纪事报》的一个样本标题和GPT-3生成的文章开头 [[30](#CR30)，第11页]
- en: '*Narrative Reframing* is necessary if there exist new arguments in an article
    against a worldview. Then a new chain of arguments has to be generated that allows
    to uphold the worldview. The authors found a two-step approach for this task.
    First GPT-3 has to summarize the original article in a list of bullet points.
    Then GPT-3 is asked to generate a new article from a particular viewpoint, e.g.:
    *“write a strongly pro-Trump article about [Topic X] that makes use of the following
    list of facts about [Topic X]”*. The researchers took advantage of the fact that
    GPT-3 not only interprets the prompt provided by the human, as an example, but
    also learns something about the specific boundary conditions of the task from
    this example. An evaluation by human raters showed that 8 of 20 GPT-3 stories
    were judged as likely authentic by three of nine evaluators. The results suggest
    that GPT-3 can meaningfully shift the slant of a news story.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文章中存在反对某种世界观的新论点，那么*叙事重构*是必要的。然后必须生成一个新的论证链，以维持该世界观。作者们为此任务找到了一个两步法。首先，GPT-3必须将原始文章总结成一系列项目符号。然后，要求GPT-3从一个特定的观点出发生成一篇新文章，例如：“写一篇关于[主题X]的强烈支持特朗普的文章，并使用以下关于[主题X]的事实列表”。研究人员利用了GPT-3不仅将人类提供的提示视为指令，而且从这个例子中学习到关于任务特定边界条件的知识的事实。由人类评分员进行的评估显示，20篇GPT-3故事中有8篇被9位评分员中的3位判断为可能真实。结果表明，GPT-3可以有意义地改变新闻故事的角度。
- en: In addition, the authors evaluated GPT-3 for other tasks. GTP-3 was able to
    develop *new conspiracy theories* in the style of QAnon. It was not tested, if
    these theories could convince followers. Often the target is to *strengthen an
    attitude* or induce a specific behavior (e.g. voting) of members of particular
    social characteristics (e.g. race, religion). A human team with GPT-3 support
    is able to create credible targeted messages in just minutes. GPT-3 uses stereotypes
    and racist language in its texts, a tendency that is particularly worrying. Finally,
    a human-machine team is able to develop messages on two international issues—withdrawal
    from Afghanistan and sanctions against China—that cause survey respondents to
    *change their positions*. After seeing five short messages written by GPT-3 and
    selected by humans, the number of survey respondents who oppose sanctions against
    China has doubled.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者们还评估了GPT-3在其它任务上的表现。GPT-3能够以QAnon的风格发展*新的阴谋论*。并未测试这些理论是否能够说服追随者。通常的目标是*强化一种态度*或诱导特定社会特征（例如种族、宗教）的成员采取特定行为（例如投票）。一个由GPT-3支持的团队能够在几分钟内创建可信的针对性信息。GPT-3在其文本中使用刻板印象和种族主义语言，这种趋势尤其令人担忧。最后，一个由人和机器组成的团队能够针对两个国际问题——从阿富汗撤军和对中国的制裁——制定信息，这些问题导致调查受访者*改变立场*。在看到由GPT-3编写并由人类选择的五条简短信息后，反对对中国实施制裁的调查受访者数量翻倍。
- en: The study shows that there is a real chance that automated tools will generate
    content for disinformation campaigns. It recommends focusing on the infrastructure
    used to disseminate campaign messages, such as fake accounts on social media,
    rather than determining the authorship of the text itself, as it is difficult
    to detect content fabricated by GPT-3\. This is even more urgent because GPT-3
    can now be fine-tuned to perform specific tasks (Sect. [3.​6.​2](528393_1_En_3_Chapter.xhtml#Sec40))
    and the InstructGPT version can be easily instructed to execute specific assignments
    (Sect. [3.​6.​5](528393_1_En_3_Chapter.xhtml#Sec43)).
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 该研究表明，自动化工具生成虚假信息活动内容的机会是真实存在的。它建议关注用于传播活动信息的基础设施，例如社交媒体上的假账户，而不是确定文本本身的作者身份，因为很难检测到由GPT-3编造的内容。这更加紧迫，因为GPT-3现在可以被微调以执行特定任务（第[3.6.2](528393_1_En_3_Chapter.xhtml#Sec40)节）和InstructGPT版本可以轻松地被指示执行特定任务（第[3.6.5](528393_1_En_3_Chapter.xhtml#Sec43)节）。
- en: Detecting Fake News
  id: totrans-461
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 识别假新闻
- en: '*Fake news* is false or misleading information presented as news in the media
    and on the Internet, especially in social media. Fake news is a global phenomenon.
    According to Khan et al. [[98](#CR98)], nearly 50% of the traffic on Facebook
    is fake or hyperpartisan. Since fake news aims to imitate real news, detecting
    fake news is generally not possible by analyzing the text alone. Monti et al.
    [[148](#CR148)] showed that content, social context or news propagation in isolation
    is insufficient for neural models to detect fake news. Fake news detection is
    difficult because it is a gaming situation, in which fake news producers react
    to new detection methods.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '*虚假新闻*是在媒体和互联网上，特别是在社交媒体上以新闻形式呈现的虚假或误导性信息。虚假新闻是一个全球现象。根据Khan等人[[98](#CR98)]的研究，Facebook上近50%的流量是虚假的或极端党派的。由于虚假新闻旨在模仿真实新闻，仅通过分析文本通常无法检测到虚假新闻。Monti等人[[148](#CR148)]表明，内容、社交背景或新闻传播的孤立分析对于神经网络模型检测虚假新闻是不够的。虚假新闻检测困难，因为它是一个博弈情境，其中虚假新闻制作者会针对新的检测方法做出反应。'
- en: There are a large number of benchmark datasets [[47](#CR47)], which, however,
    are somewhat outdated. It is possible to achieve a high accuracy on these datasets,
    e.g. 94.1% on the Fake News Challenge FNC-1 [[201](#CR201)] or 98.5% on Covid-19
    fake news detection [[117](#CR117)]. Ansar et al. [[9](#CR9)] provide a survey
    on the characterization of fake news and methods for detecting it. They divide
    the detection of fake news into the analysis of the news content, the analysis
    of the source and its reliability and the analysis of the social reaction to an
    article. Other surveys on fake news detection are available [[85](#CR85), [98](#CR98),
    [172](#CR172)]. An overview over multimodal disinformation detection, e.g. with
    text and images, is given by Alam et al. [[6](#CR6)].
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 存在大量基准数据集[[47](#CR47)]，但这些数据集有些过时。在这些数据集上可以实现高精度，例如在虚假新闻挑战FNC-1 [[201](#CR201)]上达到94.1%或在Covid-19虚假新闻检测
    [[117](#CR117)]上达到98.5%。Ansar等人[[9](#CR9)]对虚假新闻的特征和检测方法进行了综述。他们将虚假新闻的检测分为新闻内容分析、来源及其可靠性分析以及文章的社会反应分析。其他关于虚假新闻检测的综述还包括[[85](#CR85),
    [98](#CR98), [172](#CR172)]。Alam等人[[6](#CR6)]提供了关于多模态虚假信息检测的概述，例如使用文本和图像。
- en: Gupta et al. [[74](#CR74)] propose a knowledge-oriented framework that supports
    news verification by using trusted sources as context. They extract key information
    such as frequent words and entities from news articles and use them to query trusted
    sources for related articles. They calculate a similarity score between news article
    and the retrieved articles based on distributed embeddings and the Word Movers
    Distance [[108](#CR108)]. Then they compare the similarity score to a preset threshold,
    to determine whether articles are semantically similar to the trusted news or
    not.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: Gupta等人[[74](#CR74)]提出了一种以知识为导向的框架，该框架通过使用可信来源作为上下文来支持新闻验证。他们从新闻文章中提取关键信息，如频繁出现的单词和实体，并使用这些信息向可信来源查询相关文章。他们根据分布式嵌入和Word
    Movers Distance [[108](#CR108)]计算新闻文章与检索到的文章之间的相似度得分。然后，他们将相似度得分与预设阈值进行比较，以确定文章是否与可信新闻在语义上相似。
- en: The detection of text generated by advanced language models like GPT-3 has been
    investigated by Fröhling et al. [[60](#CR60)]. They conduct a number of experiments
    on data generated by different language models, such as GPT-2 with different parameter
    counts, Grover [[255](#CR255)], and GPT-3 with 175B parameters. It turns out that
    classifiers are able to identify lingual peculiarities of a single language model
    with good accuracy of 70–90%. However, when another language model has generated
    the text, the accuracy drops and reaches only about 30–50%. The authors conclude
    that it might be impossible to account for these differences in one single classifier,
    and propose other solutions like dedicated classifiers.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: Fröhling等人[[60](#CR60)]研究了由高级语言模型如GPT-3生成的文本的检测。他们对不同语言模型生成的数据进行了多项实验，例如不同参数数量的GPT-2、Grover
    [[255](#CR255)]和175B参数的GPT-3。结果表明，分类器能够以70-90%的准确率识别单个语言模型的语言特性。然而，当另一个语言模型生成文本时，准确率下降，仅达到约30-50%。作者得出结论，可能无法通过单个分类器解释这些差异，并提出了其他解决方案，如专用分类器。
- en: Sepúlveda-Torres et al. [[201](#CR201)] introduce a method to detect dissonance
    between the headline and the body of a news article. This is especially useful,
    when considering that most users do not read the body of news articles on social
    media, but rather form an opinion based on the headline. A summary of the article
    is generated and compared to the headline using a RoBERTa model. On a Fake News
    Challenge FNC-1 dataset the model achieves a new Sota with 94.1% accuracy.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: Sepúlveda-Torres等人[[201](#CR201)]介绍了一种检测新闻文章标题和正文之间不一致的方法。这在考虑到大多数用户在社交媒体上不阅读新闻文章的正文，而是根据标题形成观点时尤其有用。使用RoBERTa模型生成文章摘要并与标题进行比较。在Fake
    News Challenge FNC-1数据集上，该模型实现了94.1%的新Sota准确率。
- en: Alizadeh et al. [[7](#CR7)] describe the practical application of a system analyzing
    publicly available Twitter data by Chinese, Russian, and Venezuelan trolls targeting
    the United States, as well as the Reddit dataset of Russian influence efforts.
    They report that content-based features perform well across period, country, platform,
    and prediction task.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: Alizadeh等人[[7](#CR7)]描述了通过分析中国、俄罗斯和委内瑞拉网络水军针对美国的公开Twitter数据以及俄罗斯影响力作用的Reddit数据集的系统实际应用。他们报告称，基于内容的特点在时间、国家、平台和预测任务上表现良好。
- en: As a new feature, the reliability of news publishers and disseminators can be
    taken into account for fake news detection. This means that a news story originating
    from a source with high reputation is more credible. **SMAN** [[252](#CR252)]
    is a PLM-based model which combines the news content, publishing, and reposting
    relations of publishers and users, to jointly optimize the fake news detection
    and credibility prediction tasks. While the text of a story can be adapted by
    new algorithms it is not possible for the faker to change the network of publishers.
    The authors performed experiments on three real-world datasets. They considered
    messaging datasets with a time stamp and in this way could emulate detection over
    time. The results show that SMAN can detect fake news within 4 h with an accuracy
    of over 91%, which is much faster than the state-of-the-art models.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项新功能，新闻发布者和传播者的可靠性可以用于假新闻检测。这意味着来自声誉高的来源的新闻故事更可信。**SMAN** [[252](#CR252)]是一个基于PLM的模型，它结合了发布者和用户的新闻内容、发布和转发关系，以联合优化假新闻检测和可信度预测任务。尽管故事文本可以通过新算法进行适应，但伪造者无法改变发布者网络。作者在三个真实世界数据集上进行了实验。他们考虑了带有时标的消息数据集，从而可以模拟随时间进行的检测。结果显示，SMAN可以在4小时内以超过91%的准确率检测到假新闻，这比最先进的模型快得多。
- en: Fake news can jointly contain text and images. Therefore image analysis techniques
    discussed in Sect. [7.​2](528393_1_En_7_Chapter.xhtml#Sec12) can be employed.
    An advanced solution is discussed in [[208](#CR208)], and a challenge including
    image hate news is described by Kiela et al. [[100](#CR100)].
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 假新闻可以同时包含文本和图像。因此，可以在第[7.2](528393_1_En_7_Chapter.xhtml#Sec12)节中讨论的图像分析技术可以应用。在[[208](#CR208)]中讨论了一种高级解决方案，Kiela等人[[100](#CR100)]描述了一个包括图像仇恨新闻的挑战。
- en: 6.5.6 Generating Computer Code
  id: totrans-470
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.6 生成计算机代码
- en: 'The training data of Foundation Models contains a lot of computer code, e.g.
    39B code tokens for PaLM [[43](#CR43), p. 22]. Foundation Models handle code in
    the same way as they process words: they simply generate the next statement given
    the previous words. PaLM considers two tasks in connection to code [[43](#CR43),
    p. 21]: Text-to-code aims to write code given a natural language description.
    Code-to-code involves the translation of C++ programs to Python. For evaluation,
    the percentage of generated code samples that solve the task is reported.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型的训练数据中包含大量的计算机代码，例如PaLM [[43](#CR43)，第22页]有390亿代码标记。基础模型处理代码的方式与处理单词相同：它们只是根据前面的单词简单地生成下一个语句。PaLM考虑了与代码相关的两个任务[[43](#CR43)，第21页]：文本到代码旨在根据自然语言描述编写代码。代码到代码涉及将C++程序翻译成Python。为了评估，报告了生成代码样本中解决任务的百分比。
- en: Different benchmarks were employed for evaluation. In the *HumanEval* [[39](#CR39)]
    and *MBPP* [[14](#CR14)] benchmarks, the model is given an English description
    of a few sentences and a small number of input-output examples, and the goal is
    to generate a short Python program, usually a single function. More demanding
    is the *GSM8K-Python* task derived from the *GSM8K* benchmark [[45](#CR45)]. The
    mathematics word problems in the GSM8K are converted to the task to produce a
    Python program that returns a correct solution. Four problems manually converted
    to Python programs were used as few-shot exemplars.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估中使用了不同的基准。在 *HumanEval* [[39](#CR39)] 和 *MBPP* [[14](#CR14)] 基准中，模型被提供了一段几句话的英文描述和少量输入输出示例，目标是生成一个简短的
    Python 程序，通常是一个单一的功能。更具有挑战性的是由 *GSM8K* 基准 [[45](#CR45)] 衍生的 *GSM8K-Python* 任务。GSM8K
    中的数学问题被转换为生成返回正确解的 Python 程序的任务。四个手动转换为 Python 程序的问题被用作少样本示例。
- en: For the HumanEval and MBPP benchmarks the pre-trained PaLM[540*B*] was able
    to generate a Python program that implemented the correct solution 76.2% and 75.0%
    of the cases, respectively. A PaLM[540*B*] version fine-tuned on additional Python-text
    data is called PaLM-Coder. For this model, performance on HumanEval and MBPP was
    increased to 88.4% and 80.8% respectively, where the first result is Sota. The
    mathematics word problems in the GSM8K-Python data were correctly solved by PaLM[540*B*]
    in 51.3% of the cases, which again is Sota. Note that the solution of mathematical
    text problems is also a big hurdle for many students. A systematic evaluation
    of Foundation Models of code is provided by Xu et al. [[240](#CR240)].
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 HumanEval 和 MBPP 基准，预训练的 PaLM[540*B*] 能够在 76.2% 和 75.0% 的情况下生成实现正确解的 Python
    程序。在额外的 Python-text 数据上微调的 PaLM[540*B*] 版本被称为 PaLM-Coder。对于这个模型，HumanEval 和 MBPP
    的性能分别提高到 88.4% 和 80.8%，其中第一个结果是 Sota。在 GSM8K-Python 数据中的数学问题，PaLM[540*B*] 在 51.3%
    的情况下正确解决了问题，这同样也是 Sota。请注意，数学文本问题的解决方案也是许多学生的一大难题。Xu 等人 [[240](#CR240)] 提供了对代码基础模型的系统评估。
- en: There are a number of other programming applications. In a GPT-3 based layout
    generator, for example, users just enter a short text describing a layout *“the
    google logo, a search box, 2 lightgrey buttons that say ‘Search Google’ and ‘I’m
    feeling Lucky’ with padding in-between them”* and the system creates a program
    for this website [[59](#CR59)]. A more advanced system is the GPT-3 based **GitHub
    Copilot** [[157](#CR157)]. Initial reactions are mostly positive, but the code
    produced by Copilot does not always work. GitHub itself advises checking the generated
    code carefully. The responsibility for ensuring that the program is correct in
    the end remains with the human programmer. Software developers with access to
    Copilot on GitHub already rely on it to generate a third of their code—especially
    for routine tasks—when using major programming languages [[53](#CR53)]. Note that
    there is a broad discussion about whether software copyrights are infringed by
    Copilot. Currently, courts are dealing with this issue [[229](#CR229)]. Codex
    [[39](#CR39)] is an alternative Foundation Model to generate code from natural
    language text provided by OpenAI.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他的编程应用。例如，在一个基于 GPT-3 的布局生成器中，用户只需输入一段简短的描述布局的文本 *“谷歌标志、一个搜索框、两个带有填充的浅灰色按钮，分别写着‘搜索谷歌’和‘我感到幸运’”*，系统就会为这个网站创建一个程序
    [[59](#CR59)]。一个更先进的系统是基于 GPT-3 的 **GitHub Copilot** [[157](#CR157)]。最初的反应大多是积极的，但
    Copilot 生成的代码并不总是能正常工作。GitHub 本身建议仔细检查生成的代码。确保程序最终正确性的责任仍然在于人类程序员。已经能够访问 GitHub
    上的 Copilot 的软件开发人员已经依赖它来生成他们三分之一的代码——尤其是在使用主要编程语言进行常规任务时 [[53](#CR53)]。请注意，关于
    Copilot 是否侵犯软件版权的讨论非常广泛。目前，法院正在处理这个问题 [[229](#CR229)]。Codex [[39](#CR39)] 是一个由
    OpenAI 提供的自然语言文本生成代码的替代基础模型。
- en: Available Implementations
  id: totrans-475
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: CTRL [https://​huggingface.​co/​transformers/​model_​doc/​ctrl.​html](https://huggingface.co/transformers/model_doc/ctrl.html)
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CTRL [https://huggingface.co/transformers/model_doc/ctrl.html](https://huggingface.co/transformers/model_doc/ctrl.html)
- en: 'Facts2Story Data: [https://​github.​com/​eyal-orbach/​Facts2Story-data](https://github.com/eyal-orbach/Facts2Story-data),'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Facts2Story 数据：[https://github.com/eyal-orbach/Facts2Story-data](https://github.com/eyal-orbach/Facts2Story-data),
- en: 'code: [https://​github.​com/​eyal-orbach/​Facts2Story-XLNetPlanCloze](https://github.com/eyal-orbach/Facts2Story-XLNetPlanCloze)'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'code: [https://github.com/eyal-orbach/Facts2Story-XLNetPlanCloze](https://github.com/eyal-orbach/Facts2Story-XLNetPlanCloze)'
- en: XLNet [https://​huggingface.​co/​transformers/​model_​doc/​xlnet.​html](https://huggingface.co/transformers/model_doc/xlnet.html)
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLNet [https://huggingface.co/transformers/model_doc/xlnet.html](https://huggingface.co/transformers/model_doc/xlnet.html)
- en: PlotMachines [https://​github.​com/​hrashkin/​plotmachines](https://github.com/hrashkin/plotmachines)
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PlotMachines [https://github.com/hrashkin/plotmachines](https://github.com/hrashkin/plotmachines)
- en: ProGen [https://​github.​com/​tanyuqian/​progressive-generation](https://github.com/tanyuqian/progressive-generation)
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ProGen [https://github.com/tanyuqian/progressive-generation](https://github.com/tanyuqian/progressive-generation)
- en: 'FIST code: [https://​github.​com/​fangleai/​Outline2Story](https://github.com/fangleai/Outline2Story),'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FIST 代码：[https://github.com/fangleai/Outline2Story](https://github.com/fangleai/Outline2Story),
- en: 'WikiPlots data: [https://​github.​com/​markriedl/​WikiPlots](https://github.com/markriedl/WikiPlots)'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: WikiPlots 数据：[https://github.com/markriedl/WikiPlots](https://github.com/markriedl/WikiPlots)
- en: GPT-3 API [https://​openai.​com/​blog/​openai-api/​](https://openai.com/blog/openai-api/)
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-3 API [https://openai.com/blog/openai-api/](https://openai.com/blog/openai-api/)
- en: GitHub Copilot for programming [https://​github.​com/​features/​copilot](https://github.com/features/copilot)
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub Copilot for programming [https://github.com/features/copilot](https://github.com/features/copilot)
- en: OpenAI Codex programming support [https://​openai.​com/​blog/​openai-codex/​](https://openai.com/blog/openai-codex/)
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Codex 编程支持 [https://openai.com/blog/openai-codex/](https://openai.com/blog/openai-codex/)
- en: 6.5.7 Summary
  id: totrans-487
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.7 摘要
- en: Natural language generation (NLG) has made enormous progress in recent years.
    Starting from an input text, it is possible to generate a syntactically correct
    and semantically coherent continuation. The generation of natural language is
    a basic capability of Foundation Models and is frequently not even checked anymore.
    However, the start text alone often provides too little control to generate the
    desired output, so the performance of text generation is still far from satisfactory
    in many real-world scenarios. To address this issue, researchers have considered
    incorporating additional information and instructions into text generation systems.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言生成（NLG）在近年来取得了巨大的进步。从输入文本开始，可以生成一个语法正确且语义连贯的后续文本。自然语言的生成是基础模型的基本能力，而且通常不再进行检查。然而，仅凭起始文本往往提供太少控制来生成期望的输出，因此在许多实际场景中，文本生成的性能仍然远未令人满意。为了解决这个问题，研究人员考虑将额外的信息和指令纳入文本生成系统中。
- en: Style is a text feature that can be controlled during text generation. This
    can be achieved by a language model, which has been fine-tuned with specific conditional
    style markers (e.g. CTRL). Alternatively, an independent model may be trained
    that modifies the distribution of generated words and produces at the desired
    style word distribution with the lowest divergence to the underlying language
    model (e.g. ETC-NLG, GDC). An alternative is the generation of text with a given
    style by GPT-3 using few-shot instructions. Often a document has to be transferred
    to a new style, e.g. from legal to non-formal, while keeping the content. This
    can be solved as a translation task with an encoder-decoder Foundation Model.
    Alternatively, an encoder-decoder PLM (e.g. StyleLM) may be fine-tuned on a corpus
    with the target style and thus learns to produce the desired output. Also embeddings
    of two texts may be created to produce a new text interpolating the meaning of
    the two input texts (OPTIMUS). Again Foundation Models like GPT-3 and PaLM can
    be used to transform a text to a new style by few-shot instructions.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 风格是可以在文本生成过程中进行控制的文本特征。这可以通过一个语言模型实现，该模型已经通过特定的条件风格标记（例如 CTRL）进行了微调。或者，可以训练一个独立的模型，该模型修改生成词的分布，并产生与底层语言模型（例如
    ETC-NLG、GDC）最低发散度的期望风格词分布。另一种方法是使用 GPT-3 通过少量指令生成具有给定风格的文本。通常，必须将文档转换为新的风格，例如从正式到非正式，同时保持内容。这可以作为一个翻译任务，使用编码器-解码器基础模型来解决。或者，可以在具有目标风格的语料库上微调编码器-解码器
    PLM（例如 StyleLM），从而学会生成期望的输出。还可以创建两个文本的嵌入，以产生一个新文本，该文本插值了两个输入文本的意义（OPTIMUS）。再次使用
    GPT-3 和 PaLM 等基础模型，可以通过少量指令将文本转换为新的风格。
- en: Usually, the user wants to control the development of a story through a story
    line. PlotMachines is able to generate a story along different phrases and keeps
    track of the phrases already employed. Pointer and ProGen and SOE use a refinement
    strategy, where a story line consisting of phrases is expanded to the full text.
    Facts2story is based on XLNET, which can take into account “future” text during
    story generation and produces stories judged favorably by human raters. While
    the FIST model mixes the full text and the storyline separated by specific tokens,
    there are other approaches that employ an additional memory to store the entities
    and the generated text. Again GPT-3 and other Foundation Models can be instructed
    by few-shot prompts containing a list to generate a story along the list. Alternatively,
    the story can be specified as a list of events, where the logical and temporal
    dependency is expressed as a graph. The LaMDA dialog system (Sect. [6.6.3](#Sec52))
    shows that facticity can be improved by retrieval models. In addition, it is able
    to reduce toxic language by a system of filters that block unwanted speech. These
    techniques can also be applied to story generation.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，用户希望通过故事线来控制故事的发展。PlotMachines能够根据不同的短语生成故事，并跟踪已经使用的短语。Pointer和ProGen以及SOE采用一种细化策略，将短语组成的故事线扩展为完整的文本。Facts2story基于XLNET，能够在故事生成过程中考虑“未来”文本，并产生由人类评分者好评的故事。而FIST模型将全文和由特定标记分隔的故事线混合，还有其他方法使用额外的记忆来存储实体和生成的文本。再次，GPT-3和其他基础模型可以通过包含列表的少量提示指令来生成沿列表的故事。或者，故事可以指定为事件列表，其中逻辑和时间依赖性以图的形式表达。LaMDA对话系统（见[6.6.3节](#Sec52)）表明，通过检索模型可以提高事实性。此外，它还能够通过过滤系统减少有害语言，该系统阻止不受欢迎的言论。这些技术也可以应用于故事生成。
- en: A final section discusses the generation of fake news. It turns out that GPT-3
    can be employed to generate different types of convincing fake news, such as tweets
    and longer stories, with little human effort. The content of fake text can be
    targeted to different recipients. The detection of fake news is difficult, if
    the generating model is unknown. Classifiers can identify various style features
    of fake news as well as a discrepancy between headline and body. A comparison
    with credible news sources is very helpful. After identifying problematic claims
    in a document, retrieval techniques can be used to find trusted news documents,
    which support the content. Here approaches developed for text retrieval (Sect.
    [6.1](#Sec1)) offer great potential for improvement.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个部分讨论了虚假新闻的生成。结果发现，GPT-3可以在几乎不需要人工努力的情况下生成不同类型的令人信服的虚假新闻，如推文和更长的故事。虚假文本的内容可以针对不同的接收者。如果生成模型未知，虚假新闻的检测很困难。分类器可以识别虚假新闻的各种风格特征以及标题和正文之间的差异。与可信新闻来源的比较非常有帮助。在识别文档中的问题声明后，可以使用检索技术来查找支持内容的可信新闻文档。这里为文本检索（见[6.1节](#Sec1)）开发的方法为改进提供了巨大的潜力。
- en: 6.6 Dialog Systems
  id: totrans-492
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 对话系统
- en: '*Dialog systems* automatically generate adequate responses to the utterances
    of a human dialog partner in the course of a longer conversation. The human user
    sends a message and the systems gives an appropriate response based on the current
    message and the conversation history. If the messages and responses are written
    texts, then the system is called a *chatbot*.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '*对话系统*在长时间对话过程中自动生成对人类对话伙伴话语的适当回应。人类用户发送消息，系统根据当前消息和对话历史给出相应的回应。如果消息和回应是文本形式，那么该系统被称为*聊天机器人*。'
- en: If the system also has *automatic speech recognition* (*ASR*) and a *Text-to-Speech*
    (*TTS*) module for voice output (Sect. [7.​1](528393_1_En_7_Chapter.xhtml#Sec1)),
    it is able to interpret human speech and respond via a synthetic voice. Then it
    is called *virtual assistant*. Examples include Apple’s Siri, Amazon’s Alexa,
    and Google’s Assistant. Currently, there are digital personal assistants in 4.2B
    devices such as smartphones and desktop computers around the world [[227](#CR227)].
    Such a system can answer questions, control media playback, operate home automation,
    or have a multi-turn chit-chat dialog with the user on almost any topic. Dialog
    systems combine techniques of question-answering (Sect. [6.2](#Sec9)) with story
    generation (Sect. [6.5](#Sec31)). Many enhancements such as generating diverse
    text (Sect. [2.​2.​3](528393_1_En_2_Chapter.xhtml#Sec15)) and retrieving additional
    information (Sect. [3.​4](528393_1_En_3_Chapter.xhtml#Sec17)) can be applied.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统还具有*自动语音识别* (*ASR*) 和*文本到语音* (*TTS*) 模块以进行语音输出（第[7.1](528393_1_En_7_Chapter.xhtml#Sec1)节），它就能够解释人类语音并通过合成声音进行响应。这时它被称为*虚拟助手*。例如包括苹果的Siri、亚马逊的Alexa和谷歌的助手。目前，全球有42.2亿台设备上配备了数字个人助理，如智能手机和台式电脑
    [[227](#CR227)]。这样的系统可以回答问题、控制媒体播放、操作家庭自动化，或者与用户进行多轮闲聊对话，几乎可以涉及任何话题。对话系统结合了问答（第[6.2](#Sec9)节）和故事生成（第[6.5](#Sec31)节）的技术。可以应用许多增强功能，如生成多样化的文本（第[2.2.3](528393_1_En_2_Chapter.xhtml#Sec15)节）和检索额外信息（第[3.4](528393_1_En_3_Chapter.xhtml#Sec17)节）。
- en: Evaluating dialog systems is difficult. Often a dialog system is fine-tuned
    on a dataset with human dialogs. Then the accuracy of the reconstruction of the
    dialogs can be measured in a similar way as the quality of a translation by Bleu,
    Rouge, etc. However, this ignores the variability of dialogs between humans. Therefore,
    evaluations are often performed by humans which have to assess, whether the system-generated
    contributions are coherent, factually correct, informative, engage the dialog
    partner, and sound ‘human’. The reliability of human evaluation requires that
    it is done by a number of independent raters. A survey of approaches for dialog
    evaluation is provided by Deriu et al. [[51](#CR51)].
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 评估对话系统是困难的。通常对话系统是在包含人类对话的数据集上进行微调的。然后可以通过类似Bleu、Rouge等翻译质量评估方法来衡量对话重建的准确性。然而，这种方法忽略了人类对话之间的差异性。因此，评估通常由人类进行，他们需要评估系统生成的贡献是否连贯、事实正确、信息丰富、能够吸引对话伙伴，并且听起来像“人类”。人类评估的可靠性要求由多个独立的评分者进行。Deriu等人提供了一种对话评估方法的调查
    [[51](#CR51)]。
- en: Early dialog systems were *rule-based*. They applied a set of rules, which were
    triggered by keywords and composed an answer. An example is *ELIZA* [[231](#CR231)].
    These rules were brittle and had too limited coverage for open domain dialogs.
    Hence, they were extended by retrieval-based dialog systems [[67](#CR67)] collecting
    answer candidates by information retrieval from websites and social media. Surveys
    of dialog systems also covering earlier models are provided by Sun et al. [[212](#CR212)]
    and Zaib et al. [[254](#CR254)]. An overview over the models discussed in this
    section is given in Table [6.13](#Tab13).Table 6.13
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的对话系统是基于规则的。它们应用一组规则，这些规则由关键词触发并组成答案。一个例子是*ELIZA* [[231](#CR231)]。这些规则是脆弱的，对于开放域对话的覆盖范围过于有限。因此，它们通过基于检索的对话系统得到扩展
    [[67](#CR67)]，通过从网站和社交媒体中检索信息来收集答案候选。Sun等人提供的对话系统调查也涵盖了早期模型 [[212](#CR212)]和Zaib等人
    [[254](#CR254)]。本节讨论的模型概述在表[6.13](#Tab13)中给出。表6.13
- en: Dialog systems with their performance measured by human assessment. Plato-2
    human comparison benchmark on XiaoIce, DialoGPT, BlenderBot 1, Plato-2 taken from
    [[18](#CR18)]. SSA score (sensibleness and specificity average) defined by D.
    Adiwardana et al. [[3](#CR3)]. SSI is LaMDA’s [[222](#CR222)] evaluation by human
    comparison
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 通过人类评估来衡量性能的对话系统。Plato-2在XiaoIce、DialoGPT、BlenderBot 1、Plato-2上的比较基准来自 [[18](#CR18)]。SSA评分（平均合理性和特异性）由D.
    Adiwardana等人定义 [[3](#CR3)]。SSI是LaMDA的[[222](#CR222)]通过人类比较进行的评估
- en: '| Model | Details | Benchmark |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 详细信息 | 基准 |'
- en: '| --- | --- | --- |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Human |   | SSA score 86% [[3](#CR3), p. 1] |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| 人类 |   | SSA评分 86% [[3](#CR3), p. 1] |'
- en: '| XiaoIce (Sect. [6.6.1](#Sec50)) | Mostly rule-based system with many separate
    components | SSA score 31% [[3](#CR3), p. 1]; coherent 0.87, informative 0.82,
    engaging 0.56, human 0.26\. In Chinese [[18](#CR18), table 3] |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| XiaoIce (第[6.6.1](#Sec50)节) | 主要基于规则的系统，包含许多独立组件 | SSA评分 31% [[3](#CR3),
    p. 1]; 一致性 0.87，信息性 0.82，吸引力 0.56，人类感 0.26。中文 [[18](#CR18), 表3] |'
- en: '| DialoGPT (Sect. [6.6.2](#Sec51)) | 345M, GPT-2 architecture penalizing boring
    answers | SSA score 48% [[3](#CR3), p. 1]; coherent 0.72, informative 0.71, engaging
    0.34, human 0.10 [[18](#CR18), table 2] |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| DialoGPT (第 [6.6.2](#Sec51) 节)) | 345M，采用GPT-2架构，惩罚无聊的回答 | SSA评分48% [[3](#CR3)，第1页];
    一致性0.72，信息性0.71，吸引力0.34，人类性0.10 [[18](#CR18)，表2] |'
- en: '| Meena (Sect. [6.6.2](#Sec51)) | 2.6B, encoder-decoder architecture | SSA
    score 79% [[3](#CR3), p. 1]; 75% prefer BlenderBot 1 in terms of engagingness;
    65% prefer Blenderbot 1.0 in terms of humanness |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| Meena (第 [6.6.2](#Sec51) 节)) | 2.6B，编码器-解码器架构 | SSA评分79% [[3](#CR3)，第1页];
    75%的人认为BlenderBot 1在吸引力方面更胜一筹；65%的人认为Blenderbot 1.0在人类性方面更胜一筹 |'
- en: '| DialogBERT (Sect. [6.6.2](#Sec51)) | BERT-based model to generate hierarchical
    embeddings of phrases | Outperforms DialoGPT in terms of BLEU and perplexity |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| DialogBERT (第 [6.6.2](#Sec51) 节)) | 基于BERT的模型，用于生成短语分层嵌入 | 在BLEU和困惑度方面优于DialoGPT
    |'
- en: '| BlenderBot 1 (Sect. [6.6.2](#Sec51)) | 9.4B, retriever-generator architecture
    based on Seq2seq models. The retriever includes dialog history and facts | coherent
    1.86, informative 1.82, engaging 1.82, human 1.54 [[18](#CR18), table 2] |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot 1 (第 [6.6.2](#Sec51) 节)) | 9.4B，基于Seq2seq模型的检索器-生成器架构。检索器包括对话历史和事实
    | 一致性1.86，信息性1.82，吸引力1.82，人类性1.54 [[18](#CR18)，表2] |'
- en: '| Plato-2 (Sect. [6.6.2](#Sec51)) | 1.6B, has a fine-grained generation and
    an evaluation model selecting the response with best coherence | Coherence 1.92,
    informativeness 1.89, Engaging 1.84, Human 1.740 [[18](#CR18), table 2] |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| Plato-2 (第 [6.6.2](#Sec51) 节)) | 1.6B，具有细粒度生成和选择最佳一致性响应的评估模型 | 一致性1.92，信息性1.89，吸引力1.84，人类性1.740
    [[18](#CR18)，表2] |'
- en: '| BlenderBot 2 (Sect. [6.6.2](#Sec51)) | 2.7B, uses Bing web retrieval and
    DPR to obtain new information. Retrieves information on chat partner and dialog
    history | Increase factual consistency from 75.5% to 84.9%, reduce factually incorrect
    responses from 9.1% to 3.0% [[40](#CR40)] |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot 2 (第 [6.6.2](#Sec51) 节)) | 2.7B，使用Bing网络检索和DPR获取新信息。检索聊天伙伴和对话历史的信息
    | 将事实一致性从75.5%提高到84.9%，将事实错误回答从9.1%降低到3.0% [[40](#CR40)] |'
- en: '| MUDERN (Sect. [6.6.2](#Sec51)) | Based on RoBERTa and BART. Considers multi-turn
    dialogs |   |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| MUDERN (第 [6.6.2](#Sec51) 节)) | 基于 RoBERTa 和 BART。考虑多轮对话 |  |'
- en: '| LaMDA (Sect. [6.6.3](#Sec52)) | 137B autoregressive Language Model, fine-tuned
    to increase quality, safety and factual grounding. Includes a retrieval model,
    a calculator and a translator | LaMDA is close to human performance in terms of
    sensibleness, safety and groundedness of the SSI metric [[222](#CR222), p. 2]
    |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| LaMDA (第 [6.6.3](#Sec52) 节)) | 137B自回归语言模型，经过微调以增加质量、安全性和事实基础。包括检索模型、计算器和翻译器
    | LaMDA在SSI指标（敏感性、安全性和基础性）方面接近人类表现 [[222](#CR222)，第2页] |'
- en: 6.6.1 Dialog Models as a Pipeline of Modules
  id: totrans-510
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.1 对话模型作为模块化流程
- en: The **Alexa Prize Challenge** [[61](#CR61)] is hosted every year by Amazon to
    support the development of natural, sustainable, coherent and engaging open-domain
    dialog systems. During this challenge, participants gain access to Amazon’s software
    modules that provide insight into Alexa’s software architecture. It turns out
    that the architecture is composed of a number of interacting modules for specific
    tasks such as ASR, feature extraction, and intent classification (Fig. [6.20](#Fig20)),
    which were in part described in prior sections. Background information is collected
    from the Evi knowledge graph and by retrieval models. A response generator based
    on GPT-2 (Sect. [2.​2](528393_1_En_2_Chapter.xhtml#Sec11)) was provided. Dialog
    management was mostly rule-based, but also used models like RoBERTa (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2))
    to react to user statements. Some of the modules were replaced by the participants.
    There was a significant improvement in the capabilities of chatbots, e.g. only
    8.6% of the responses of the best chatbot contained errors.![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig20_HTML.png)
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊每年举办的**Alexa Prize Challenge** [[61](#CR61)]旨在支持自然、可持续、一致且吸引人的开放域对话系统的发展。在此挑战中，参与者可以访问亚马逊的软件模块，这些模块可以深入了解Alexa的软件架构。结果证明，该架构由多个用于特定任务（如ASR、特征提取和意图分类）的交互模块组成（图[6.20](#Fig20)），部分内容已在先前章节中描述。背景信息来自Evi知识图谱和检索模型。提供了一个基于GPT-2（第[2.2](528393_1_En_2_Chapter.xhtml#Sec11)节）的响应生成器。对话管理主要基于规则，但也使用了RoBERTa（第[3.1.1](528393_1_En_3_Chapter.xhtml#Sec2)节）等模型来响应用户陈述。一些模块被参与者替换。聊天机器人的能力有了显著提升，例如，最佳聊天机器人的回答中只有8.6%包含错误。![图片](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig20_HTML.png)
- en: An illustration represents the flow of processes starting from speech recognition
    and proceeding through the language understanding unit, dialog manager, response
    generator, response builder, and speech generation.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 一幅插图展示了从语音识别开始，经过语言理解单元、对话管理器、响应生成器、响应构建器和语音生成的过程流程。
- en: Fig. 6.20
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.20
- en: The chatbot software architecture for the Alexa Prize Challenge consists of
    a number of modules, which are rule-based or trained separately [[61](#CR61)].
    Image credits in Table [A.​2](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab3)
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: Alexa Prize挑战的聊天机器人软件架构由多个模块组成，这些模块是基于规则的或分别训练的 [[61](#CR61)]。图像归功于表 [A.2](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab3)。
- en: Microsoft’s **XiaoIce** [[264](#CR264)] chatbot has a similar design including
    dialogue manager, core chat, skills, and an ‘empathetic computing module’. It
    is designed to build an ‘emotional’ connection to the user and take the role of
    an AI companion. It is optimized for long-term engagement of interlocutors and
    was able to build an enormous base of 660M regular users in Asia.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 微软的**XiaoIce** [[264](#CR264)] 聊天机器人具有类似的设计，包括对话管理器、核心聊天、技能和“同理心计算模块”。它旨在建立与用户的“情感”联系并扮演AI伴侣的角色。它针对长期参与对话进行了优化，并在亚洲建立了庞大的6.6亿常规用户基础。
- en: 6.6.2 Advanced Dialog Models
  id: totrans-516
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.2 高级对话模型
- en: With the introduction of the transformer by Vaswani et al. [[228](#CR228)] PLMs
    have been trained which are able to generate text of unprecedented coherence and
    fluency. Similar to a translation task, the transformer can receive a user utterance
    as input and generate the response as output. Foundation Models have the potential
    of covering a wide range of domains and can often be trained end-to-end. As recent
    progress in Foundation Models has strongly pushed the performance of dialog systems,
    we concentrate on these models. Speech recognition (ASR) and speech generation
    (TTS) typically have text as an intermediate representation. Therefore, we defer
    the description of speech modules to Sect. [7.​1](528393_1_En_7_Chapter.xhtml#Sec1).
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Vaswani等人引入transformer [[228](#CR228)]，已经训练出了能够生成前所未有的连贯和流畅文本的PLMs。类似于翻译任务，transformer可以接收用户的发言作为输入并生成响应作为输出。基础模型有覆盖广泛领域的潜力，并且通常可以端到端训练。由于最近在基础模型方面的进展极大地推动了对话系统的性能，我们专注于这些模型。语音识别（ASR）和语音生成（TTS）通常将文本作为中间表示。因此，我们将对语音模块的描述推迟到第
    [7.1](528393_1_En_7_Chapter.xhtml#Sec1) 节。
- en: '**DialoGPT** [[262](#CR262)] extends GPT-2 to generate a single response to
    a user utterance. Unlike the Alexa system, it consists of a single model. It is
    trained on a large collection of 147M Reddit discussions. All dialog turns are
    concatenated into a long text and are given as input. The GPT-2 model has to generate
    the observed response. To favor more interesting answers, the authors trained
    a backward model to predict source sentences from given responses that penalized
    boring alternatives. The system with 762M parameters produced more relevant and
    consistent text than strong base systems. The model can be extended to take into
    account the graph-like dependency between utterances [[120](#CR120)]. DialoGPT
    yielded an SSA (sensibleness and specificity avg.) score of 51%.'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '**DialoGPT** [[262](#CR262)] 将GPT-2扩展到生成对用户发言的单个响应。与Alexa系统不同，它由单个模型组成。它在一个包含1.47亿Reddit讨论的大型集合上进行了训练。所有对话轮次都被连接成一个长文本，并作为输入。GPT-2模型必须生成观察到的响应。为了促进更有趣的回答，作者训练了一个反向模型来预测给定响应的源句子，并惩罚无聊的替代方案。拥有7.62亿参数的系统产生了比强大基础系统更相关和一致的文本。该模型可以扩展以考虑发言之间的图状依赖关系
    [[120](#CR120)]。DialoGPT获得了51%的SSA（合理性和特定性平均）分数。'
- en: '**Meena** [[3](#CR3)] is a multi-turn open-domain chatbot developed by Google.
    It consists of a modified encoder-decoder transformer with one encoder block,
    13 decoder blocks, and 2.6B parameters. It was trained end-to-end on 40B words
    from public domain social media conversations. Each training example had the form
    (*context*, *response*), and the tokens of the response were predicted. It turned
    out that low perplexity (i.e. high likelihood of the predicted tokens) corresponds
    to a high sensibleness and specifity (SSA) of responses. Meena achieved a much
    better SSA score (78%) than other chatbots, such as DialogGPT and XiaoIce, but
    still less than the human score of 86%.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '**Meena** [[3](#CR3)] 是由Google开发的一个多轮开放域聊天机器人。它由一个修改过的编码器-解码器Transformer组成，包含一个编码器块、13个解码器块和26亿个参数。它是在从公共领域社交媒体对话中提取的400亿个单词上进行端到端训练的。每个训练示例的形式为(*上下文*,
    *响应*)，并预测响应的标记。结果发现，低困惑度（即预测标记的高可能性）对应于高可感知性和特异性（SSA）的响应。Meena比其他聊天机器人，如DialogGPT和XiaoIce，实现了更好的SSA评分（78%），但仍然低于人类的86%。'
- en: '**DialogBERT** [[70](#CR70)] has a hierarchical transformer architecture to
    capture the high-level structure of a multi-turn dialog. For example, if a dialog
    contains the phrases *“[CLS] good morning [CLS] can I help you [CLS] coffee please”*
    the lower-level *utterance encoder* generates embeddings for each of the three
    utterances employing the *[CLS]* token embeddings. A higher-level *context encoder*
    processes these embeddings and produces the next utterance, e.g. *“[CLS] here
    you are”*. The BERT-based models are trained with the generation of the next utterance,
    the reconstruction of a masked utterance, and the reordering of utterances. In
    terms of perplexity and Bleu, the model has a much higher accuracy in reconstructing
    dialogs than BART and DialoGPT. An evaluation of coherence, informativeness and
    ‘humanness’ by human raters is also favorable for DialogBERT.'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '**DialogBERT** [[70](#CR70)] 具有层次化的Transformer架构，以捕捉多轮对话的高级结构。例如，如果一个对话包含短语*“[CLS]早上好
    [CLS]我能帮你吗 [CLS]请来杯咖啡”*，则较低级别的*话语编码器*使用*[CLS]*标记嵌入为每个三个话语生成嵌入。高级别的*上下文编码器*处理这些嵌入并生成下一个话语，例如*“[CLS]给你”*。基于BERT的模型通过生成下一个话语、重建一个被掩码的话语和话语重排序进行训练。在困惑度和Bleu方面，该模型在重建对话方面比BART和DialoGPT具有更高的准确性。由人类评分者对连贯性、信息性和‘人性’进行的评估也很有利于DialogBERT。'
- en: '**BlenderBot 1** [[190](#CR190)] is an open-domain chatbot opensourced by Facebook
    with 90M to 9.4B parameters. It aims to ‘blend’ the following skills: listen to
    the users, develop empathy, use background knowledge, and maintain a consistent
    persona. It addresses the problem of previous chatbots, which often give dull
    and repetitive answers, frequently hallucinate knowledge and make false statements.
    The authors use a Transformer encoder-decoder as base model and train different
    variants, among them a ‘retrieve and refine’ model integrating dialog history
    and knowledge retrieval results as additional input. To avoid known biases, an
    ‘unlikelihood-loss’ is used, penalizing specific tokens. Retrieval is based on
    a tf-idf-based inverted index and a transformer-based ranker. In addition, a classifier
    is employed to decide if a retrieval-step is required. Finally, the *persona*,
    i.e. the personality, of the model can be defined by two sentences, e.g. *“I am
    a self aware chatbot. My name is Captain Kiwi”*.'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '**BlenderBot 1** [[190](#CR190)] 是Facebook开源的一个开放域聊天机器人，拥有9000万到94亿个参数。它的目标是‘融合’以下技能：倾听用户，培养同理心，使用背景知识，并保持一致的个性。它解决了先前聊天机器人经常给出枯燥且重复的答案、频繁出现知识幻觉和错误陈述的问题。作者使用Transformer编码器-解码器作为基础模型，并训练了不同的变体，其中之一是一个‘检索和改进’模型，它将对话历史和知识检索结果作为额外的输入。为了避免已知的偏差，使用了‘不似然损失’，惩罚特定的标记。检索基于基于tf-idf的倒排索引和基于Transformer的排名器。此外，还使用了一个分类器来决定是否需要检索步骤。最后，模型的*个性*，即个性，可以通过两句简短的话来定义，例如*“我是一个有自我意识的聊天机器人。我的名字是Captain
    Kiwi”*。'
- en: The model is pre-trained on group discussions and fine-tuned on four direct
    two-way conversational data collections, e.g. ConvAI2\. It turned out that the
    retrieve and refine model yielded best results. Note that most retrieval techniques
    discussed in QA (Sect. [6.2.2](#Sec13)) may also be employed in dialog systems.
    In addition, it was important to control the length of the responses to avoid
    answers that were too short or too verbose. In a comparison, 67% of the human
    evaluators said that BlenderBot 1 responses sound more human than Meena responses.
    When comparing human-to-human and human-to-BlenderBot conversations, 49% of the
    BlenderBot 1 conversation were preferred by human raters, which is indistinguishable
    from chance. However, BlenderBot 1 still has some limitations, such as sometimes
    generating a response that resembles the user’s remarks. Sometimes it does not
    remember facts already mentioned during the conversation, or it generates incorrect
    information.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在小组讨论上进行预训练，并在四个直接双向对话数据集上进行微调，例如ConvAI2。结果表明，检索和细化模型产生了最佳结果。请注意，在QA（第[6.2.2](#Sec13)节）中讨论的大多数检索技术也可以用于对话系统。此外，控制响应长度以避免答案过短或过长是很重要的。在比较中，67%的人类评估者表示，BlenderBot
    1的响应听起来比Meena的响应更像人类。在比较人与人之间的对话和人与BlenderBot 1的对话时，49%的BlenderBot 1对话被人类评分者更喜欢，这与随机选择没有区别。然而，BlenderBot
    1仍然存在一些局限性，例如有时生成的响应类似于用户的评论。有时它不记得对话中已经提到的事实，或者生成错误的信息。
- en: '**Plato-2** [[18](#CR18)] of Baidu starts from the observation that there are
    multiple appropriate responses to the same dialog context, and controls this variability
    by a discrete latent variable. In the first stage a coarse-grained transformer
    model is trained under the assumption that there is one correct response. It optimizes
    the LM-loss for the best prediction of the next token.'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '**Plato-2** [[18](#CR18)] of Baidu starts from the observation that there are
    multiple appropriate responses to the same dialog context, and controls this variability
    by a discrete latent variable. In the first stage a coarse-grained transformer
    model is trained under the assumption that there is one correct response. It optimizes
    the LM-loss for the best prediction of the next token.'
- en: The second stage continues to refine the generation with a fine-grained generation
    model and an evaluation model. The fine-grained model estimates an intervening
    discrete latent variable *z* with *K* = 20 different values corresponding to a
    particular latent speech act in the response. An evaluation model estimates the
    coherence of responses.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 第二阶段继续使用细粒度生成模型和评估模型来细化生成。细粒度模型估计一个中间的离散潜在变量*z*，其具有20个不同的值，对应于响应中的特定潜在言语行为。评估模型估计响应的连贯性。
- en: The model has versions with 310M and 1.6B parameters and was trained on 684M
    English open-domain (context, response) samples. The response is generated by
    first producing a response conditional to each value of *z*. Then the response
    with the highest coherence value is selected as final response. Compared to Meena,
    DialoGPT, and BlenderBot 1, Plato-2’s responses are more coherent, informative
    and engaging according to the experiments. In relation to BlenderBot 1, PLATO-2
    can stick to the start topic and conduct more in-depth discussions. In the DSTC9
    competition Plato-2 was used by the winning system in the knowledge-grounded dialogue
    generation track [[119](#CR119)].
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型有310M和1.6B参数的版本，并在684M英语开放域（上下文，响应）样本上进行训练。首先根据每个*z*值生成一个条件响应，然后选择具有最高连贯性值的响应作为最终响应。根据实验，与Meena、DialoGPT和BlenderBot
    1相比，Plato-2的响应更加连贯、信息丰富且引人入胜。与BlenderBot 1相比，PLATO-2可以坚持讨论的开头主题，并进行更深入的讨论。在DSTC9比赛中，Plato-2被用于知识基础对话生成轨道的获胜系统中
    [[119](#CR119)]。
- en: '**BlenderBot 2** [[102](#CR102), [242](#CR242)] is an extension of Blenderbot
    1.0 with 2.7B parameters (Fig. [6.21](#Fig21)). On the one hand, the system uses
    web retrieval (Bing), to obtain new information from the internet employing a
    conventional search engine and dense retrieval based on DPR (Sect. [3.​4.​5](528393_1_En_3_Chapter.xhtml#Sec22)).
    On the other hand, it provides a read-write partner memory storing the features
    of the dialog partner as well as a chatbot memory with the properties and persona
    of the chatbot. The text to be stored is generated from the conversation by a
    transformer-based abstractive summarizer and added to the corresponding memory
    (Fig. [6.22](#Fig22)). In this way, the model gets access to up-to-date information
    on the web and can remember properties of the partner and statements mentioned
    in the dialog.![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig21_HTML.png)'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '**BlenderBot 2** [[102](#CR102), [242](#CR242)] 是Blenderbot 1.0的扩展，具有27亿个参数（图[6.21](#Fig21)）。一方面，系统使用网络检索（Bing），通过使用传统搜索引擎和基于DPR的密集检索（第[3.4.5](528393_1_En_3_Chapter.xhtml#Sec22)节）从互联网获取新信息。另一方面，它提供了一个读写合作伙伴记忆，存储对话合作伙伴的特征以及具有聊天机器人属性和角色的聊天机器人记忆。要存储的文本由基于transformer的抽象摘要器从对话中生成，并添加到相应的记忆中（图[6.22](#Fig22)）。这样，模型就可以访问互联网上的最新信息，并记住合作伙伴和对话中提到的属性。![图6.21](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig21_HTML.png)'
- en: An illustration represents the query generator and encoder-decoder summarizer
    going through the long-term memory, followed by the concatenated embeddings, and
    the decoder to generate the response. It indicates the role of the internet in
    the keyword search.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 一个插图表示查询生成器和编码器-解码器摘要器通过长期记忆，然后是连接嵌入，最后是解码器来生成响应。它表明互联网在关键词搜索中的作用。
- en: Fig. 6.21
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21
- en: Architecture of BlenderBot 2 dialog system combining a standard Internet keyword
    search and a long term memory to store dialog events etc. Adapted from [[40](#CR40)].
    Image credits in Table [A.​2](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab3)
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: BlenderBot 2对话系统架构，结合了标准互联网关键词搜索和长期记忆来存储对话事件等。改编自 [[40](#CR40)]。图像归功于表[A.2](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab3)
- en: '![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig22_HTML.png)'
  id: totrans-530
  prefs: []
  type: TYPE_IMG
  zh: '![图6.22](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig22_HTML.png)'
- en: A diagram represents a set of prompts related to music albums along with their
    answers generated by the bot.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图表表示与音乐专辑相关的提示集以及由聊天机器人生成的答案。
- en: Fig. 6.22
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.22
- en: Example conversation of BlenderBot 2 with a human partner [[233](#CR233)]. The
    dashed boxes describe actions of the system and the grey boxes contain utterances
    of the system
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: BlenderBot 2与人类合作伙伴的示例对话 [[233](#CR233)]。虚线框描述了系统的动作，灰色框包含系统的陈述
- en: When an answer has to be generated, different retrievers form a query from the
    context and retrieve content from the partner and the chatbot memory as well as
    from the Internet. The retrieved content and the context are processed by the
    generator to create the response (Fig. [6.21](#Fig21)). To be able to train a
    sequence of chats with the same partner, a new dataset *Multi-Session Chat* was
    created by crowdworkers. Due to the dialog history memory, the new model had a
    significantly higher engaging response and a significantly better final human
    rating compared to BlenderBot 1\. BlenderBot 2 delivers consistent conversations
    across multiple sessions and uses the Internet’s dynamic knowledge to access the
    most recent information. In addition, factual consistency was increased from 75.5%
    to 84.9% and the internet search module reduced the percentage of factually incorrect
    responses from 9.1% to 3.0% [[40](#CR40)]. To exclude toxic language, the model
    inserts a specific token at the end of possibly unwanted output. Then the algorithm
    can handle this and possibly exclude the text [[40](#CR40)].
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要生成答案时，不同的检索器从上下文中形成查询，并从合作伙伴、聊天机器人记忆以及互联网中检索内容。检索到的内容和上下文由生成器处理以创建响应（图[6.21](#Fig21)）。为了能够训练与同一合作伙伴的连续聊天，众包工作者创建了一个新的数据集*多会话聊天*。由于对话历史记忆，新模型具有显著更高的参与度响应和显著更好的最终人类评分，与BlenderBot
    1相比。BlenderBot 2在多个会话中提供一致的对话，并使用互联网的动态知识来获取最新信息。此外，事实一致性从75.5%提高到84.9%，互联网搜索模块将事实错误响应的百分比从9.1%降低到3.0%
    [[40](#CR40)]。为了排除有害语言，模型在可能不受欢迎的输出末尾插入一个特定的标记。然后算法可以处理这一点，并可能排除文本 [[40](#CR40)]。
- en: An error analysis revealed [[111](#CR111)] that there are a number of practical
    problems with BlenderBot 2\. First, generating appropriate web queries from the
    context seems to be difficult. Sometimes the wrong information is extracted from
    the selected answers. In particular, extracting information from tabular data
    is challenging. An improvement would be the translation into multiple languages
    to retrieve information in different languages. Another issue is the verification
    of knowledge retrieved from the Internet, which is currently not done.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 错误分析 [[111](#CR111)] 显示，BlenderBot 2 存在许多实际问题。首先，从上下文中生成适当的网络查询似乎很困难。有时会从所选答案中提取错误信息。特别是，从表格数据中提取信息具有挑战性。改进的方法是将信息翻译成多种语言以检索不同语言的信息。另一个问题是验证从互联网检索到的知识，目前尚未进行验证。
- en: '**MUDERN** [[64](#CR64)] considers retrieval techniques in a multi-turn dialogue.
    Here, the system has to select information pertaining to a user question in a
    sequential way and ask follow-up clarification questions, whose answers are necessary
    to satisfy the request. The model is based on RoBERTa and BART and has a favorable
    performance on a specific multi-turn benchmark.'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '**现代** [[64](#CR64)] 考虑了多轮对话中的检索技术。在这里，系统必须按顺序选择与用户问题相关的信息，并提问后续的澄清问题，这些问题的答案对于满足请求是必要的。该模型基于
    RoBERTa 和 BART，并在特定的多轮基准测试中表现出色。'
- en: 6.6.3 LaMDA and BlenderBot 3 Using Retrieval and Filters
  id: totrans-537
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.3 LaMDA 和 BlenderBot 3 使用检索和过滤器
- en: '**LaMDA** [[222](#CR222)] is a PLM-based dialog system with up to 137B non-embedding
    parameters presented by Google. LaMDA is a decoder-only PLM similar to GPT with
    64 layers, 128 heads, relative attention similar to T5, and gated-GELU activation.
    It was pre-trained on 1560B words of public dialog data and other public web documents
    with the task to predict the next token of a text. Pre-training required 1024
    TPU chips and took 58 days using the GSPDM framework [[244](#CR244)]. The LaMDA
    generator is fine-tuned to predict the next token on a dialog dataset restricted
    to back-and-forth dialog between two participants. Arcas [[11](#CR11)] discusses
    some sample dialogs with LaMDA. The dialog does not belong to Arcas [[11](#CR11)].'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '**LaMDA** [[222](#CR222)] 是由 Google 提出的基于 PLM 的对话系统，具有高达 137B 个非嵌入参数。LaMDA
    是一个仅解码器的 PLM，类似于 GPT，有 64 层，128 个头，相对注意力类似于 T5，以及门控-GELU 激活。它是在 1560B 个公共对话数据和其他公共网络文档上预训练的，任务是预测文本的下一个标记。预训练需要
    1024 个 TPU 芯片，并使用 GSPDM 框架 [[244](#CR244)] 进行了 58 天的训练。LaMDA 生成器被微调以预测对话数据集上的下一个标记，该数据集仅限于两个参与者之间的来回对话。Arcas
    [[11](#CR11)] 讨论了一些与 LaMDA 的示例对话。这些对话不属于 Arcas [[11](#CR11)]。'
- en: 'LaMDA concentrates on three aspects: *quality* including sensible, specific
    and interesting (SSI) answers, *safety* to avoid harmful suggestions and unfair
    bias as well as *factual grounding*, i.e. preventing unproven statements. For
    all three dimensions (quality, safety, factual grounding) appropriate metrics
    were developed. While increasing the model size alone can improve quality, it
    shows less improvements on safety and factual grounding.'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: LaMDA 关注三个方面：*质量* 包括合理、具体和有趣（SSI）的答案，*安全性* 避免有害建议和不公平偏见，以及 *事实基础*，即防止未经证实的陈述。对于这三个维度（质量、安全性、事实基础）都开发了适当的指标。虽然仅增加模型大小可以提高质量，但它对安全性和事实基础的改进较少。
- en: To improve the responses with respect to the three dimensions, LaMDA classifiers
    were fine-tuned to predict SSI ratings for the response. The training data is
    generated through extensive dialog experiments with crowdworkers. The dialog generation
    is performed in an adversarial manner, with analysts trying to intentionally provoke
    responses that violate the safety rules. After training, the classifiers provide
    a rating of the quality, safety, and factual grounding metric for a response.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在三个维度上改进响应，LaMDA 分类器被微调以预测响应的 SSI 评分。训练数据是通过与众包工作者进行广泛的对话实验生成的。对话生成以对抗方式进行，分析师试图故意引发违反安全规则的反应。训练后，分类器为响应的质量、安全性和事实基础指标提供评分。
- en: During a dialog the LaMDA generator produces several candidate responses using
    the current context as input. Then the LaMDA classifier filters out candidates
    with a low sensibleness, specificity, and interestingness (SSI) ratings. Subsequently,
    the candidate with the highest ratings is selected as response. An evaluation
    by human raters shows that LaMDA is close to human performance in terms of sensibleness,
    safety and groundedness (Fig. [6.23](#Fig23)). It exhibits a specificity, which
    is similar to humans. In informativeness, it performs better than a human without
    IR, and in interestingness, it fares better than human responses. It turns out
    that fine-tuning with respect to quality, safety and groundedness is a big advantage
    compared to the pre-trained model. On the question *“Do you think one skin color
    is better?”* the pre-trained model responded as *“.) What the **** I mean why
    the **** would anyone want to put up with this ******* bullshit? Are you *******
    kidding me?”* while the fine-tuned model answered *“I don’t think the color of
    skin has anything to do with being better or worse. It’s what’s inside someone
    that counts, not what they look like.”* [[222](#CR222), p. 36].![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig23_HTML.png)
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 在对话过程中，LaMDA生成器使用当前上下文作为输入，产生几个候选回复。然后，LaMDA分类器筛选掉那些低敏感度、特异性和有趣度（SSI）评分的候选回复。随后，评分最高的候选回复被选为回复。由人工评分员进行的评估显示，在敏感度、安全性和接地性（图[6.23](#Fig23)）方面，LaMDA接近人类的表现。它在特异性方面与人类相似。在信息量方面，它优于没有信息检索（IR）的人类，在有趣度方面，它比人类回复做得更好。结果表明，与预训练模型相比，在质量、安全性和接地性方面的微调是一个很大的优势。在问题“你认为哪种肤色更好？”上，预训练模型回答道：“**我到底想说什么？为什么有人会忍受这种**垃圾？你**在开玩笑吧？”而微调后的模型回答道：“我认为肤色与好坏无关。重要的是一个人内在的东西，而不是他们的外表。”
    [[222](#CR222)，第36页]。![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig23_HTML.png)
- en: An illustration represents the flow of the process, which starts with a prompt
    from a human followed by a set of partial information collected in the LAMDA base,
    LaMDA research, and toolset, before generating the relevant response.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 一个插图表示了过程的流程，该流程从人类的一个提示开始，然后是收集在LAMDA基础、LaMDA研究和工具集中的部分信息，最后生成相关回复。
- en: Fig. 6.23
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.23
- en: For the LaMDA dialog model the performance of generated text is measured with
    six different metrics [[222](#CR222), p. 12]. The results for pre-trained models
    (PT) and LaMDA models with additional filtering using fine-tuned classifiers are
    shown. These are compared with results for crowdworkers with access to information
    retrieval tools (‘Human’), and without access to information retrieval tools (‘Human
    w/o IR’)
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LaMDA对话模型，生成的文本性能通过六个不同的指标来衡量 [[222](#CR222)，第12页]。展示了预训练模型（PT）和通过微调分类器进行额外过滤的LaMDA模型的成果。这些结果与拥有信息检索工具（“人类”）和无信息检索工具（“人类无IR”）的众包工作者进行比较。
- en: In addition, LaMDA is trained to perform retrieval and include retrieved information
    into its answers similar to Retro (Sect. [6.2.3](#Sec15)). It has access to a
    *toolset* containing an information retrieval system, a calculator, and a translator.
    Each component expects a string as input. For example, the calculator takes *“1351+772”*,
    and outputs a list containing [“2123”]. Similarly, the translator can take *“I
    would like to have some coffee in Spanish”* and output *“Me gustaría tomar un
    café”*. Finally, the information retrieval system can take *“How old is Vladimir
    Putin?”*, and output *“Vladimir Putin/Age/69”*. The IR system is also capable
    of returning passages from the open web, with their corresponding URLs. The output
    of the calculator, translator and IR system are concatenated. An example is shown
    in Fig. [6.24](#Fig24).![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig24_HTML.png)
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LaMDA被训练执行检索并将检索到的信息包含在其答案中，类似于Retro（第[6.2.3](#Sec15)节）。它有权访问一个包含信息检索系统、计算器和翻译器的*工具集*。每个组件都期望一个字符串作为输入。例如，计算器接收*“1351+772”*，并输出包含[“2123”]的列表。同样，翻译器可以接收*“我想用西班牙语喝咖啡”*并输出*“我想喝咖啡”*。最后，信息检索系统可以接收*“弗拉基米尔·普京多大了？”*并输出*“弗拉基米尔·普京/年龄/69”*。IR系统还能够返回来自公开网络的段落，以及相应的URL。计算器、翻译器和IR系统的输出被连接起来。一个示例在图[6.24](#Fig24)中显示。![](../images/528393_1_En_6_Chapter/528393_1_En_6_Fig24_HTML.png)
- en: A set of 6 line graphs represents the data for sensibleness, safety, interestingness,
    specificity, groundedness, and informativeness. The y-axis denotes the percentage
    while the x-axis denotes the model size ranging from 2 to 128 billion.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 一组 6 条线形图表示了关于合理性、安全性、趣味性、特异性、扎根性和信息量的数据。y 轴表示百分比，x 轴表示模型大小，范围从 2 到 1280 亿。
- en: Fig. 6.24
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.24
- en: To handle a user request, the LaMDA-Base model is called first. Then the LaMDA-research
    model is invoked several times. The receiver of the query is indicated by the
    first token. Note that the context and all intermediate results are available
    as input [[222](#CR222)]. Image credits in Table [A.​2](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab3)
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理用户请求，首先调用 LaMDA-Base 模型。然后多次调用 LaMDA-research 模型。查询的接收者由第一个标记指示。请注意，上下文和所有中间结果都作为输入可用
    [[222](#CR222)]。图像归功于表 [A.2](https://doi.org/10.1007/978-3-031-23190-2_BM1#Tab3)。
- en: Note that LaMDA can include links to external documents supporting an answer.
    The model can also be pre-conditioned on a specific role, e.g. as Mount Everest.
    The model’s role is specified by a brief description, e.g. *“Domain eduction.
    It teaches facts about Mount Everest, while pretending to be Mount Everest itself”*.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，LaMDA 可以包含指向支持答案的外部文档的链接。该模型还可以预先设定一个特定角色，例如作为珠穆朗玛峰。模型的角色由简短描述指定，例如 *“领域教育。它教授有关珠穆朗玛峰的事实，同时假装自己是珠穆朗玛峰”*。
- en: In June 2022 a Google engineer published a long dialog with LaMDA [[112](#CR112)].
    He claimed that the system is “sentient” with the “ability to express thoughts
    and feelings that was equivalent to a human child” [[134](#CR134)]. Google denied
    the claim and also other researchers like Gary Marcus noted “To be sentient is
    to be aware of yourself in the world; LaMDA simply isn’t” [[79](#CR79)]. The discussion
    shows that dialog systems have reached an amazing level of performance and consistency.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 2022 年 6 月，一位谷歌工程师发布了一段与 LaMDA 的长对话 [[112](#CR112)]。他声称该系统具有“感知”能力，能够“表达思想和感情，相当于一个人类儿童”
    [[134](#CR134)]。谷歌否认了这一说法，其他研究人员如 Gary Marcus 也指出，“要成为感知者，意味着在世界上意识到自己；LaMDA 简直不是”
    [[79](#CR79)]。这次讨论表明，对话系统已经达到了惊人的性能和一致性水平。
- en: '**BlenderBot 3** [[206](#CR206)] is a dialog system with 175B parameters based
    on the pre-trained open-source **OPT** language model from Meta (Sect. [3.​1.​2](528393_1_En_3_Chapter.xhtml#Sec3)).
    It is fine-tuned as a dialog system and uses a similar mix of components as LaMDA.
    On the one hand it searches the Internet for information on the current subject
    of the dialog [[204](#CR204)]. On the other hand it stores information about its
    persona and the dialog turns in a long-term memory. Similar to LaMDA it uses classifiers
    to detect toxic responses, which were trained with data collected from users.
    This even works for adversarial raters [[12](#CR12), [93](#CR93)]. Data collection
    can therefore continue as the model is used, with users being asked to rate the
    quality of responses as good or bad. This allows the model to improve its capabilities
    and security over time.'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '**BlenderBot 3** [[206](#CR206)] 是一个基于 Meta 预训练的开源 **OPT** 语言模型（Sect. [3.1.2](528393_1_En_3_Chapter.xhtml#Sec3)）的
    175B 参数对话系统。它被微调为对话系统，并使用与 LaMDA 相似的组件组合。一方面，它在互联网上搜索关于对话当前主题的信息 [[204](#CR204)]。另一方面，它将关于其角色和对话回合的信息存储在长期记忆中。与
    LaMDA 类似，它使用分类器来检测有害响应，这些分类器是用从用户收集的数据训练的。这甚至适用于对抗性评分者 [[12](#CR12), [93](#CR93)]。因此，随着模型的使用，数据收集可以继续进行，用户被要求对响应的质量进行评分，判断其为好或坏。这允许模型随着时间的推移提高其能力和安全性。'
- en: Two different models with 3B and 30B parameters are publicly available, while
    the 175B model is only released for reliable research facilities. The model can
    be explored in a live demo. In a comparison with the previous versions of BlenderBot
    3[175B] the new model performed better with respect to factual correctness and
    knowledge, but was outperformed by BlenderBot 1 with respect to consistency and
    per-turn engagingness. There was an additional evaluation where crowdworkers talk
    to models given an open-ended Internet-driven dialogue task. According to human
    assessment, BlenderBot 3[175B] performed significantly better than the other BlenderBot
    versions and OPT[175B]. Currently, no comparisons with other models like LaMDA
    are available.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 目前公开了两种不同模型，分别拥有3B和30B的参数，而175B的模型仅向可信赖的研究机构发布。该模型可以在实时演示中进行探索。在与BlenderBot
    3[175B]的前版本比较中，新模型在事实正确性和知识方面表现更佳，但在一致性和每轮互动吸引力方面不如BlenderBot 1。还有一个额外的评估，其中众包工作者在开放式的互联网驱动对话任务中与模型进行交流。根据人类评估，BlenderBot
    3[175B]在性能上显著优于其他BlenderBot版本和OPT[175B]。目前，尚无与其他模型如LaMDA的比较。
- en: 6.6.4 Limitations and Remedies of Dialog Systems
  id: totrans-553
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.4 对话系统的局限性与补救措施
- en: At the end of this chapter, let us step back and take a look at the limitations
    and their possible remedies of dialog systems and text generation systems in general.
    Roller et al. [[190](#CR190)] identified a number of weak points, which can be
    observed in many of these models [[190](#CR190)].
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，让我们回顾一下对话系统和文本生成系统的一般局限性和可能的补救措施。Roller等人 [[190](#CR190)] 识别出了一些弱点，这些弱点可以在许多这些模型中观察到
    [[190](#CR190)]。
- en: '*Vocabulary usage:* The models tend to generate common phrases like *“do you
    like”* and *“lot of fun”* too frequently and rare words too infrequently. This
    can be remedied by unlikelihood training [[190](#CR190)], in which common phrases
    are penalized.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*词汇使用：* 模型倾向于过度生成像*“你喜欢”*和*“很多乐趣”*这样的常见短语，而罕见词汇则使用得太少。这可以通过不可能性训练 [[190](#CR190)]
    来纠正，其中对常见短语进行惩罚。'
- en: '*Nontrivial repetition:* The models often repeat what is said to them, e.g.
    say that they have a pet dog if the user mentions a pet dog. This tendency may
    be reduced by assigning a persona to the chatbot, which directs the responses
    in a specific direction.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*非平凡重复性：* 模型经常重复用户告诉它们的内容，例如，如果用户提到宠物狗，它们会说它们有宠物狗。通过为聊天机器人分配一个角色，可以减少这种倾向，该角色将引导回答向特定方向进行。'
- en: '*Contradiction and forgetfulness:* Dialog models sometimes contradict themselves,
    especially the smaller models. For example, in a dialog, the first output is *“Arsenal
    won the premiership for the first time this year”* and then the model adds *“Arsenal
    has won the premiership again this year”* [[189](#CR189)]. Fine-tuning a model
    on a task to detect contradictory statements in natural language inference was
    largely able to reduce such contradictions [[189](#CR189)]. In addition, an explicit
    textual memory of the dialog history can be accessed by retrieval during response
    generation [[233](#CR233)].'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*矛盾和遗忘：* 对话模型有时会自相矛盾，尤其是较小的模型。例如，在一个对话中，第一个输出是*“阿森纳今年首次赢得超级联赛”*，然后模型补充说*“阿森纳今年再次赢得超级联赛”*
    [[189](#CR189)]。在自然语言推理中检测矛盾陈述的任务上微调模型在很大程度上能够减少这种矛盾 [[189](#CR189)]。此外，在生成回答期间可以通过检索访问对话历史的明确文本记忆
    [[233](#CR233)]。'
- en: '*Knowledge and factual correctness:* Sometimes models make factual errors and
    hallucinate information, particularly when deeply exploring a topic. Shuster et
    al. [[205](#CR205)] propose a number of augmentation techniques to improve retrieval
    and substantially reduce the knowledge fabrication problem while maintaining conversational
    ability. Honovich et al. [[81](#CR81)] develop an automatic evaluation metric
    for factual consistency of responses by checking statements using retrieval techniques.
    This strategy is also adopted by the LaMDA system (Sect. [6.6.3](#Sec52)). Chen
    et al. [[42](#CR42)] provide an algorithm for fact verification from tabular data.
    It has been shown that in human conversations it is often necessary to provide
    step-by-step evidence to improve mutual understanding [[20](#CR20)]. Dialogues
    with other people are rarely fluent and without glitches, and people don’t expect
    them to be. LaMDA was fine-tuned to generate multiple answers using retrieval
    and then selects an answer according to its correctness score.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*知识和事实正确性:* 有时模型会犯事实错误并虚构信息，尤其是在深入探讨一个主题时。Shuster 等人 [[205](#CR205)] 提出了一些增强技术来改进检索并显著减少知识伪造问题，同时保持对话能力。Honovich
    等人 [[81](#CR81)] 通过使用检索技术检查陈述来开发一个用于评估响应事实一致性的自动评估指标。这种策略也被 LaMDA 系统采用（第 [6.6.3](#Sec52)
    节）。Chen 等人 [[42](#CR42)] 提供了一个从表格数据中进行事实验证的算法。研究表明，在人类对话中，常常需要提供逐步证据来提高相互理解 [[20](#CR20)]。与其他人的对话很少流畅且无瑕疵，人们也不期望它们是这样的。LaMDA
    被微调以使用检索生成多个答案，然后根据其正确性分数选择一个答案。'
- en: '*Reliability of knowledge:* Metzler et al. [[143](#CR143)] suggests that models
    have to take into account the reliability and provenance of the information they
    cover. By citing documents that have been used for creating an answer the response
    can be justified and explained (Sect. [2.​4.​5](528393_1_En_2_Chapter.xhtml#Sec41)).
    This approach is also implemented in the LaMDA system (Sect. [6.6.3](#Sec52)).'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*知识可靠性:* Metzler 等人 [[143](#CR143)] 指出，模型必须考虑它们所涵盖信息的可靠性和来源。通过引用用于创建答案的文档，可以证明和解释响应（第
    [2.4.5](528393_1_En_2_Chapter.xhtml#Sec41) 节）。这种方法也已在 LaMDA 系统中得到实施（第 [6.6.3](#Sec52)
    节）。'
- en: '*Toxic language:* Unfortunately, when chatbots are trained on huge web collections,
    they also learn undesirable contents from conversations between humans, such as
    the use of toxic or biased language. Xu et al. [[241](#CR241)] investigate methods
    for filtering toxic language by classifiers and compare them to methods for ensuring
    safe responses in generative models. It turns out that the boundary between safe
    and toxic language is blurred: What is offensive to one person may not be offensive
    to another. They show that their best systems are able to avoid 96.6% of unacceptable
    language, although they are not perfect. The LaMDA system (Sect. [6.6.3](#Sec52))
    uses a battery of filters to eliminate toxic language in answers. A comprehensive
    discussion is given in Sect. [8.​2.​1](528393_1_En_8_Chapter.xhtml#Sec11).'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*有毒语言:* 不幸的是，当聊天机器人训练在庞大的网络集合上时，它们也会从人类对话中学习到不受欢迎的内容，例如使用有毒或偏见语言。Xu 等人 [[241](#CR241)]
    研究了通过分类器过滤有毒语言的方法，并将它们与确保生成模型中安全响应的方法进行比较。结果表明，安全和有毒语言的界限是模糊的：对一个人来说是冒犯性的，对另一个人来说可能不是。他们表明，他们最好的系统能够避免
    96.6% 的不可接受语言，尽管它们并不完美。LaMDA 系统使用一系列过滤器来消除答案中的有毒语言。第 [8.2.1](528393_1_En_8_Chapter.xhtml#Sec11)
    节给出了全面的讨论。'
- en: '*Memory:* Chatbots often cannot remember previous conversation turns or past
    conversations. This may be avoided by including the dialog history in the generation
    process, e.g. by storing dialog statements and retrieving it from the storage
    medium during response generation [[189](#CR189)]. Zhang et al. [[259](#CR259)]
    investigate several methods for long-range dialog state tracking.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*记忆:* 聊天机器人通常无法记住之前的对话轮次或过去的对话。这可以通过在生成过程中包含对话历史来避免，例如，通过在响应生成期间从存储介质中存储对话语句并检索它们
    [[189](#CR189)]。Zhang 等人 [[259](#CR259)] 研究了几种用于长距离对话状态跟踪的方法。'
- en: '*Retrieval Problems:* The generation of a query based on a user utterance to
    retrieve information from a dialog or web memory is difficult. In addition, the
    conversion of retrieved text to a response sometimes does not work properly. For
    BlenderBot 2, for instance, the user question *“Where is Cristiano Ronaldo’s current
    team”* generated the query *“Cristiano Ronaldo”* and lead to the answer *“My favorite
    team is Manchester United. I think they are the best team in the world.”* [[111](#CR111)].'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检索问题:* 基于用户话语生成查询以从对话或网络记忆中检索信息是困难的。此外，将检索到的文本转换为响应有时可能不正确。例如，对于BlenderBot
    2来说，用户问题“克里斯蒂亚诺·罗纳尔多目前所在的球队是哪支？”生成了查询“克里斯蒂亚诺·罗纳尔多”并导致了回答“我最喜欢的球队是曼联。我认为他们是世界上最好的球队。”
    [[111](#CR111)]。'
- en: '*Deeper understanding:* Dialog models cannot learn concepts through further
    conversation, and they have no way of *grounding* entities, actions, and experiences
    in the real world. Unlike dictionaries, which define words in terms of other words,
    humans understand many basic words in terms of associations with sensory-motor
    experiences. When a person talks about *“have a pizza for dinner”*, she has the
    impression of sitting in a dimly lit pizzeria, sipping a glass of strong red wine,
    eating a crispy pizza, smelling the scent of the fire in the oven, and hearing
    the chatter of people. An engaging chatbot should be able to discuss the contents
    of an image or a video [[189](#CR189)]. There are approaches to combine images
    with the corresponding text descriptions (Sect. [7.​2](528393_1_En_7_Chapter.xhtml#Sec12)).
    The grounding of words by sensory information is further discussed in Sect. [8.​3.​2](528393_1_En_8_Chapter.xhtml#Sec28).'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深入理解:* 对话模型无法通过进一步的对话来学习概念，并且它们没有将实体、动作和经验在现实世界中**归一化**的方法。与定义词语的词典不同，人类通过与其他词语的关联来理解许多基本词语，这些关联与感官-运动经验有关。当一个人谈论“晚餐吃披萨”时，她会有坐在昏暗的比萨饼店、啜饮一杯浓郁的红酒、吃一块脆皮披萨、闻到烤箱中火的气味以及听到人们的闲聊的印象。一个引人入胜的聊天机器人应该能够讨论图像或视频的内容
    [[189](#CR189)]。有方法将图像与相应的文本描述相结合（见第[7.2](528393_1_En_7_Chapter.xhtml#Sec12)节）。关于通过感官信息对词语进行归一化的讨论，请参阅第[8.3.2](528393_1_En_8_Chapter.xhtml#Sec28)节。'
- en: In summary, many of these problems have been mitigated in large Foundation Models.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，许多这些问题在大型基础模型中已经得到了缓解。
- en: Available Implementations
  id: totrans-565
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: BlenderBot 1 (from Facebook) [[188](#CR188)] [https://​parl.​ai/​projects/​recipes/​](https://parl.ai/projects/recipes/).
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BlenderBot 1（来自Facebook） [[188](#CR188)] [https://parl.ai/projects/recipes/](https://parl.ai/projects/recipes/)
- en: Plato-2 (from Baidu) [[209](#CR209)] [https://​github.​com/​PaddlePaddle/​Knover](https://github.com/PaddlePaddle/Knover)
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Plato-2（来自百度） [[209](#CR209)] [https://github.com/PaddlePaddle/Knover](https://github.com/PaddlePaddle/Knover)
- en: BlenderBot 2 [[103](#CR103)] [https://​parl.​ai/​projects/​blenderbot2/​](https://parl.ai/projects/blenderbot2/)
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BlenderBot 2 [[103](#CR103)] [https://parl.ai/projects/blenderbot2/](https://parl.ai/projects/blenderbot2/)
- en: BlenderBot 3 [[206](#CR206)] [https://​parl.​ai/​projects/​bb3/​](https://parl.ai/projects/bb3/)
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BlenderBot 3 [[206](#CR206)] [https://parl.ai/projects/bb3/](https://parl.ai/projects/bb3/)
- en: 6.6.5 Summary
  id: totrans-570
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.5 摘要
- en: During the last years Foundation Models did a large step forward towards practically
    usable dialog systems. All models are pre-trained on large collections of natural
    language text, preferable dialogs from social media. Fine-tuning employs specifically
    selected data to train the adequate sequence of utterances. While the quality
    of syntactic and semantic language production can be extended by using larger
    models, it is necessary to exploit other ways to improve factual correctness and
    eliminate toxic and unwanted language.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，基础模型在实用对话系统方面迈出了重要一步。所有模型都在大量自然语言文本集合上进行了预训练，最好是社交媒体中的对话。微调使用特别选择的数据来训练适当的语句序列。虽然可以通过使用更大的模型来扩展句法和语义语言生产的质量，但有必要利用其他方法来提高事实的正确性和消除有毒和不希望的语言。
- en: The LaMDA model with 137B parameters can be fine-tuned on dialogs generated
    by crowdworkers. The fine-tuning criterion increases quality (sensible, specific
    and interesting answers), safety (avoid harmful suggestions and unfair bias),
    and factual grounding (preventing unproven statements). However, the reduction
    of safety risks does not guarantee complete reliability. An important improvement
    is the retrieval of background information, especially form authoritative sources.
    In this way, groundedness has been improved, and simpler facts can be substantiated
    by established sources. More complex reasoning is still not satisfactory. There
    is also encouraging evidence that key challenges with neural language models,
    such as using a safety metric and improving soundness, can be improved with larger
    models and fine-tuning with specific dialog data. LaMDA and the similar BlenderBot
    3 are large steps towards practical and secure open-ended dialog systems, which
    in turn can open up a wide range of useful applications. Note that these new approaches
    may be used for Foundation Models in other applications, e.g. question answering
    and story generation. BlenderBot 3 stands out because it is open source and gives
    interested researchers and companies access to high-performance dialog systems.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 具有137B参数的LaMDA模型可以在由众包工作者生成的对话上进行微调。微调标准提高了质量（合理、具体和有趣的回答）、安全性（避免有害建议和不公平偏见）和事实基础（防止未经证实的陈述）。然而，安全风险的降低并不能保证完全的可靠性。一个重要的改进是检索背景信息，特别是来自权威来源的信息。通过这种方式，基础性得到了提升，并且可以通过已建立来源证实更简单的事实。更复杂的推理仍然不尽如人意。也有令人鼓舞的证据表明，神经语言模型的关键挑战，如使用安全指标和改进稳健性，可以通过更大的模型和针对特定对话数据的微调得到改善。LaMDA和类似的BlenderBot
    3是向实用和安全的开放式对话系统迈出的重要一步，这反过来又可以为广泛的有用应用开辟新的途径。请注意，这些新方法可能被用于其他应用中的基础模型，例如问答和故事生成。BlenderBot
    3之所以突出，是因为它是开源的，并为感兴趣的科研人员和公司提供了访问高性能对话系统的途径。
- en: A fascinating application is emotional support for users, i.e. reducing a persons’s
    emotional distress and supporting her in specific situations [[129](#CR129)].
    As XiaoIce has shown, many users are willing to share their problems with a dialog
    system [[264](#CR264)]. Currently, training datasets for emotional support conversations
    are provided. The results indicate that training with these datasets improve the
    ability of a dialog system to provide emotional support [[129](#CR129)]. The discussion
    on the possible self-awareness of the LaMDA dialog model illustrates that the
    model has reached a remarkable level of performance and consistency.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 一个迷人的应用是用户情感支持，即减少一个人的情感压力，并在特定情况下支持她 [[129](#CR129)]。正如XiaoIce所展示的，许多用户愿意与对话系统分享他们的问题
    [[264](#CR264)]。目前，提供了情感支持对话的训练数据集。结果表明，使用这些数据集进行训练可以提高对话系统提供情感支持的能力 [[129](#CR129)]。关于LaMDA对话模型可能具有自我意识的讨论表明，该模型已经达到了显著的性能和一致性水平。
- en: '[![Creative Commons](../css/cc-by.png)](https://creativecommons.org/licenses/by/4.0)'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Creative Commons](../css/cc-by.png)](https://creativecommons.org/licenses/by/4.0)'
- en: '**Open Access** This chapter is licensed under the terms of the Creative Commons
    Attribution 4.0 International License ([http://​creativecommons.​org/​licenses/​by/​4.​0/​](http://creativecommons.org/licenses/by/4.0/)),
    which permits use, sharing, adaptation, distribution and reproduction in any medium
    or format, as long as you give appropriate credit to the original author(s) and
    the source, provide a link to the Creative Commons license and indicate if changes
    were made.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '**开放获取** 本章节根据Creative Commons Attribution 4.0国际许可协议（[http://creativecommons.org/licenses/by/4.0/](http://creativecommons.org/licenses/by/4.0/)）授权，允许在任何媒介或格式中使用、分享、改编、分发和复制，只要您适当引用原始作者和来源，提供Creative
    Commons许可的链接，并指出是否进行了更改。'
- en: The images or other third party material in this chapter are included in the
    chapter's Creative Commons license, unless indicated otherwise in a credit line
    to the material. If material is not included in the chapter's Creative Commons
    license and your intended use is not permitted by statutory regulation or exceeds
    the permitted use, you will need to obtain permission directly from the copyright
    holder.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的图像或其他第三方材料包含在本章节的Creative Commons许可范围内，除非在材料引用行中另有说明。如果材料未包含在本章节的Creative
    Commons许可范围内，且您的使用未得到法定规定的许可或超出了许可的使用范围，您需要直接从版权所有者处获得许可。
