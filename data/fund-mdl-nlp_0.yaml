- en: '© The Author(s) 2023G. Paaß, S. GiesselbachFoundation Models for Natural Language
    ProcessingArtificial Intelligence: Foundations, Theory, and Algorithms[https://doi.org/10.1007/978-3-031-23190-2_1](https://doi.org/10.1007/978-3-031-23190-2_1)'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: © 作者(们) 2023 G. Paaß, S. Giesselbach
- en: 1. Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 引言
- en: Gerhard Paaß^([1](#Aff5)  ) and Sven Giesselbach^([1](#Aff5))(1)Knowledge Discovery
    Department, Team NLU, Fraunhofer Institute for Intelligent Analysis and Information
    Systems (IAIS), Sankt Augustin, Nordrhein-Westfalen, Germany
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Gerhard Paaß^([1](#Aff5)  ) 和 Sven Giesselbach^([1](#Aff5))(1)知识发现部门，NLU 团队，弗劳恩霍夫智能分析与信息系统研究所
    (IAIS)，圣奥古斯丁，北莱茵-威斯特法伦州，德国
- en: Abstract
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: With the development of efficient Deep Learning models about a decade ago, many
    Deep Neural Networks have been used to solve pattern recognition tasks such as
    natural language processing and image recognition. An advantage of these models
    is that they automatically create features arranged in layers which represent
    the content and do not require manually constructed features. These models rely
    on Machine Learning employing statistical techniques to give machines the capability
    to ‘learn’ from data without being given explicit instructions on what to do.
    Deep Learning models transform the input in layers step by step in such a way
    that complex patterns in the data can be recognized. This chapter first describes
    how a text is pre-processed and partitioned into tokens, which form the basis
    for natural language processing. Then we outline a number of classical Machine
    Learning models, which are often used as modules in advanced models. Examples
    include the logistic classifier model, fully connected layers, recurrent neural
    networks and convolutional neural networks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在大约十年前高效深度学习模型的发展中，许多深度神经网络被用于解决诸如自然语言处理和图像识别等模式识别任务。这些模型的一个优点是它们可以自动创建按层排列的特征，这些特征代表了内容，并且不需要手动构建特征。这些模型依赖于机器学习，使用统计技术来赋予机器从数据中“学习”的能力，而不需要给出明确的操作指令。深度学习模型通过逐步在层中转换输入，使得数据中的复杂模式可以被识别。本章首先描述了文本是如何进行预处理和分割成标记的，这些标记是自然语言处理的基础。然后我们概述了若干经典机器学习模型，这些模型通常被用作高级模型中的模块。例如，包括逻辑分类器模型、全连接层、循环神经网络和卷积神经网络。
- en: KeywordsNatural language processingText preprocessingVector space modelStatic
    embeddingsRecurrent networksConvolutional networks
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词自然语言处理文本预处理向量空间模型静态嵌入循环神经网络卷积神经网络
- en: 1.1 Scope of the Book
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 本书范围
- en: With the development of efficient Deep Learning models about a decade ago, many
    Deep Neural Networks have been used to solve pattern recognition tasks such as
    *natural language processing* (*NLP*) and image processing. Typically, the models
    have to capture the meaning of a text or an image and make an appropriate decision.
    Alternatively they can generate a new text or image according to the task at hand.
    An advantage of these models is that they create intermediate features arranged
    in layers and do not require manually constructed features. *Deep Neural Networks*
    such as Convolutional Neural Networks (CNNs) [[32](#CR32)] and Recurrent Neural
    Networks (RNNs) [[65](#CR65)] use low-dimensional dense vectors as a kind of distributed
    representation to express the syntactic and semantic features of language.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在大约十年前高效深度学习模型的发展中，许多深度神经网络被用于解决诸如自然语言处理 (*NLP*) 和图像处理等模式识别任务。通常，这些模型必须捕捉文本或图像的意义并做出适当的决策。或者，它们可以根据任务生成新的文本或图像。这些模型的一个优点是它们创建中间特征，这些特征按层排列，并且不需要手动构建特征。*深度神经网络*，如卷积神经网络
    (CNNs) [[32](#CR32)] 和循环神经网络 (RNNs) [[65](#CR65)] 使用低维密集向量作为分布式表示，以表达语言的句法和语义特征。
- en: All these models can be considered as *Artificial Intelligence* (*AI*) Systems.
    AI is a broad research field aimed at creating intelligent machines, acting similar
    to humans and animals having natural intelligence. It captures the field’s long-term
    goal of building machines that mimic and then surpass the full spectrum of human
    cognition. *Machine Learning**(ML)* is a subfield of artificial intelligence that
    employs statistical techniques to give machines the capability to ‘learn’ from
    data without being given explicit instructions on what to do. This process is
    also called ‘training’, whereby a ‘learning algorithm’ gradually improves the
    model’s performance on a given task. *Deep Learning* is an area of ML in which
    an input is transformed in layers step by step in such a way that complex patterns
    in the data can be recognized. The adjective ‘deep’ refers to the large number
    of layers in modern ML models that help to learn expressive representations of
    data to achieve better performance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些模型都可以被认为是*人工智能*（AI）系统。人工智能是一个广泛的研究领域，旨在创造能够像人类和动物一样具有自然智能的智能机器。它捕捉了该领域的长期目标，即构建能够模仿并最终超越人类认知全谱的机器。*机器学习*（ML）是人工智能的一个子领域，它采用统计技术使机器能够在没有给出明确指令的情况下从数据中“学习”。这个过程也被称为“训练”，在这个过程中，“学习算法”逐渐提高模型在特定任务上的性能。*深度学习*是机器学习的一个领域，其中输入在层中逐步转换，这样就可以识别数据中的复杂模式。形容词“深度”指的是现代机器学习模型中大量层的数量，这些层有助于学习数据的表达性表示，以实现更好的性能。
- en: In contrast to computer vision, the size of *annotated* training data for NLP
    applications was rather small, comprising only a few thousand sentences (except
    for machine translation). The main reason for this was the high cost of manual
    annotation. To avoid overfitting, i.e. overadapting models to random fluctuations,
    only relatively small models could be trained, which did not yield high performance.
    In the last 5 years, new NLP methods have been developed based on the *Transformer*
    introduced by Vaswani et al. [[67](#CR67)]. They represent the meaning of each
    word by a vector of real numbers called *embedding*. Between these embeddings
    various kinds of “attentions” can be computed, which can be considered as a sort
    of “correlation” between different words. In higher layers of the network, attention
    computations are used to generate new embeddings that can capture subtle nuances
    in the meaning of words. In particular, they can grasp different meanings of the
    same word that arise from context. A key advantage of these models is that they
    can be trained with unannotated text, which is almost infinitely available, and
    overfitting is not a problem.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 与计算机视觉相比，自然语言处理应用的*标注*训练数据量相对较小，仅包含几千个句子（除了机器翻译）。主要原因是因为人工标注的成本很高。为了避免过拟合，即过度适应模型的随机波动，只能训练相对较小的模型，这并没有产生高性能。在过去的5年里，基于Vaswani等人提出的*Transformer*的新自然语言处理方法得到了发展[[67](#CR67)]。它们通过称为*嵌入*的实数向量来表示每个单词的含义。在这些嵌入之间可以计算各种“注意力”，这可以被视为不同单词之间的一种“相关性”。在网络的高层，注意力计算被用来生成新的嵌入，这些嵌入可以捕捉单词含义中的微妙细微差别。特别是，它们可以理解由于上下文而产生的同一单词的不同含义。这些模型的一个关键优势是它们可以用未标注的文本进行训练，这种文本几乎无限可用，并且过拟合不是一个问题。
- en: 'Currently, there is a rapid development of new methods in the research field,
    which makes many approaches from earlier years obsolete. These models are usually
    trained in two steps: In a first *pre-training* step, they are trained on a large
    text corpus containing billions of words without any annotations. A typical pre-training
    task is to predict single words in the text that have been masked in the input.
    In this way, the model learns fine subtleties of natural language syntax and semantics.
    Because enough data is available, the models can be extended to many layers with
    millions or billions of parameters.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，研究领域的新的方法正在快速发展，这使得许多早期的方法变得过时。这些模型通常分为两个步骤进行训练：在第一个*预训练*步骤中，它们在没有任何标注的大型文本语料库上进行训练，该语料库包含数十亿个单词。一个典型的预训练任务是预测输入中被遮蔽的单个单词。通过这种方式，模型学习自然语言语法和语义的细微差别。由于有足够的数据，模型可以扩展到具有数百万或数十亿参数的许多层。
- en: In a second *fine-tuning* step, the model is trained on a small annotated training
    set. In this way, the model can be adapted to new specific tasks. Since the fine-tuning
    data is very small compared to the pre-training data and the model has a high
    capacity with many millions of parameters, it can be adapted to the fine-tuning
    task without losing the stored information about the language structure. It was
    demonstrated that this idea can be applied to most NLP tasks, leading to unprecedented
    performance gains in semantic understanding. This *transfer learning* allows knowledge
    from the pre-training phase to be transferred to the fine-tuned model. These models
    are referred to as *Pre-trained Language Models* (*PLM*).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个**微调**步骤中，模型在小型标注训练集上进行训练。这样，模型可以适应新的特定任务。由于微调数据与预训练数据相比非常小，并且模型具有数百万个参数的高容量，因此它可以适应微调任务而不会丢失关于语言结构的存储信息。已经证明，这种想法可以应用于大多数NLP任务，从而在语义理解方面带来前所未有的性能提升。这种**迁移学习**允许从预训练阶段的知识转移到微调模型。这些模型被称为**预训练语言模型**（PLM）。
- en: In the last years the number of parameters of these PLMs was systematically
    enlarged together with more training data. It turned out that in contrast to conventional
    wisdom the performance of these models got better and better without suffering
    from overfitting. Models with billions of parameters are able to generate syntactically
    correct and semantically consistent fluent text if prompted with some starting
    text. They can answer questions and react meaningful to different types of prompts.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，这些PLM的参数数量系统地增加，同时伴随着更多训练数据。结果证明，与常规观点相反，这些模型的性能越来越好，而没有受到过拟合的影响。具有数十亿参数的模型能够在给出一些起始文本的情况下生成语法正确且语义一致的流畅文本。它们可以回答问题，并对不同类型的提示做出有意义的反应。
- en: Moreover, the same PLM architecture can simultaneously be pre-trained with different
    types of sequences, e.g. tokens in a text, image patches in a picture, sound snippet
    of speech, image patch sequences in video frames, DNA snippets, etc. They are
    able to process these media types simultaneously and establish connections between
    the different modalities. They can be adapted via natural language prompts to
    perform acceptably on a wide variety of tasks, even though they have not been
    explicitly trained on these tasks. Because of this flexibility, these models are
    promising candidates to develop overarching applications. Therefore, large PLMs
    with billions of parameters are often called *Foundation Models* [[9](#CR9)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，相同的PLM架构可以同时用不同类型的序列进行预训练，例如文本中的标记、图片中的图像块、语音的片段、视频帧中的图像块序列、DNA片段等。它们能够同时处理这些媒体类型并建立不同模态之间的联系。它们可以通过自然语言提示进行适应，以在广泛的各种任务上表现良好，即使它们没有明确地针对这些任务进行训练。正因为这种灵活性，这些模型有望成为开发通用应用的理想候选者。因此，具有数十亿参数的大型PLM通常被称为**基础模型**
    [[9](#CR9)]。
- en: 'This book is intended to provide an up-to-date overview of the current Pre-trained
    Language Models and Foundation Models, with a focus on applications in NLP:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书旨在提供当前预训练语言模型和基础模型的最新概述，重点关注自然语言处理（NLP）中的应用：
- en: We describe the necessary background knowledge, model architectures, pre-training
    and fine-tuning tasks, as well as evaluation metrics.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们描述了必要的背景知识、模型架构、预训练和微调任务，以及评估指标。
- en: We discuss the most relevant models for each NLP application group that currently
    have the best accuracy or performance, i.e. are close to the *state of the art*
    (Sota). Our purpose here is not to describe a spectrum of all models developed
    in recent years, but to explain some representative models so that their internal
    workings can be understood.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们讨论了每个自然语言处理（NLP）应用组中最相关的模型，这些模型目前具有最佳精度或性能，即接近**最先进水平**（Sota）。我们的目的不是描述近年来开发的所有模型的范围，而是解释一些代表性模型，以便理解它们的内部工作原理。
- en: Recently PLMs have been applied to a number of speech, image and video processing
    tasks giving rise to the term Foundation Models. We give an overview of most relevant
    models, which often allow the joint processing of different media, e.g. text and
    images
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近，预训练语言模型（PLMs）被应用于许多语音、图像和视频处理任务，从而产生了“基础模型”这一术语。我们概述了其中最相关的模型，这些模型通常允许联合处理不同媒体，例如文本和图像。
- en: We provide links to available model codes and pre-trained model parameters.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们提供了可用的模型代码和预训练模型参数的链接。
- en: We discuss strengths and limitations of the models and give an outlook on possible
    future developments.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们讨论了这些模型的优势和局限性，并对可能的未来发展趋势进行了展望。
- en: There are a number of previous surveys of Deep Learning and NLP [[1](#CR1)–[4](#CR4),
    [10](#CR10), [15](#CR15), [16](#CR16), [27](#CR27), [39](#CR39), [50](#CR50),
    [53](#CR53), [54](#CR54), [59](#CR59), [66](#CR66)]. The surveys of Han et al.
    [[22](#CR22)], Lin et al. [[41](#CR41)], and Kalyan et al. [[31](#CR31)] are the
    most up-to-date and comprehensive. Jurafsky and Martin [[30](#CR30)] prepare an
    up-to-date book on this field. In addition, there are numerous surveys for specific
    model variants or application areas. Where appropriate, we provide references
    to these surveys. New terminology is usually printed in *italics* and models in
    **bold**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 已有关于深度学习和自然语言处理的许多综述[[1](#CR1)–[4](#CR4), [10](#CR10), [15](#CR15), [16](#CR16),
    [27](#CR27), [39](#CR39), [50](#CR50), [53](#CR53), [54](#CR54), [59](#CR59),
    [66](#CR66)]。Han等人[[22](#CR22)]、Lin等人[[41](#CR41)]和Kalyan等人[[31](#CR31)]的综述是最新的和最全面的。Jurafsky和Martin[[30](#CR30)]编写了关于这个领域的最新书籍。此外，还有许多针对特定模型变体或应用领域的综述。在适当的地方，我们提供了对这些综述的引用。新的术语通常用*斜体*表示，模型用**粗体**表示。
- en: The rest of this chapter introduces text preprocessing and *classical NLP models*,
    which in part are reused inside PLMs. The second chapter describes the main architectures
    of *Pre-trained Language Models*, which are currently the workhorses of NLP. The
    third chapter considers a large number of *PLM variants* that extend the capabilities
    of the basic models. The fourth chapter describes the information captured by
    PLMs and Foundation Models and analyses their syntactic skills, world knowledge,
    and reasoning capabilities.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分介绍了文本预处理和*经典自然语言处理模型*，其中一部分在PLMs中被重新使用。第二章描述了*预训练语言模型*的主要架构，这些模型目前是自然语言处理的工作马。第三章考虑了大量的*PLM变体*，它们扩展了基本模型的功能。第四章描述了PLMs和基础模型捕获的信息，并分析了它们的句法技能、世界知识和推理能力。
- en: The remainder of the book considers various application domains and identifies
    PLMs and Foundation Models that currently provide the best results in each domain
    at a reasonable cost. The fifth chapter reviews *information extraction* methods
    that automatically identify structured information and language features in text
    documents, e.g. for relation extraction. The sixth chapter deals with *natural
    language generation* approaches that automatically generate new text in natural
    language, usually in response to a prompt. The seventh chapter is devoted to models
    for analyzing and creating *multimodal content* that typically integrate content
    understanding and production across two or more modalities, such as text, speech,
    image, video, etc. The general trend is that more data, computational power, and
    larger parameter sets lead to better performance. This is explained in the last
    *summary* chapter, which also considers social and ethical aspects of Foundation
    Models and summarizes possible further developments.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本书剩余部分考虑了各种应用领域，并确定了在合理成本下，每个领域目前提供最佳结果的PLMs和基础模型。第五章回顾了自动识别文本文档中结构化信息和语言特征的*信息提取*方法，例如用于关系提取。第六章处理*自然语言生成*方法，这些方法可以自动以自然语言生成新文本，通常是对提示的响应。第七章致力于分析和创建*多模态内容*的模型，这些模型通常在两种或多种模态（如文本、语音、图像、视频等）之间整合内容理解和生成。一般趋势是，更多的数据、计算能力和更大的参数集导致更好的性能。这在上面的*总结*章节中得到了解释，该章节还考虑了基础模型的社会和伦理方面，并总结了可能进一步的发展。
- en: 1.2 Preprocessing of Text
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 文本预处理
- en: The first step in preprocessing is to extract the actual text. For each type
    of text document, e.g. pdf, html, xml, docx, ePUB, there are specific parsers,
    which resolve the text into characters, words, and formatting information. Usually,
    the layout and formatting information is removed.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理的第一步是提取实际文本。对于每种类型的文本文档，例如pdf、html、xml、docx、ePUB，都有特定的解析器，它们将文本解析为字符、单词和格式化信息。通常，布局和格式化信息会被移除。
- en: Then, the extracted text is routinely divided into *tokens*, i.e. words, numbers,
    and punctuation marks. This process is not trivial, as text usually contains special
    units like phone numbers or email addresses that must be handled in a special
    way. Some text mining tasks require the splitting of text into sentences. Tokenizers
    and sentence splitters for different languages have been developed in the past
    decades and can be included from many programming toolboxes, e.g. *Spacy* [[64](#CR64)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，提取的文本通常会被常规地划分为*标记*，即单词、数字和标点符号。这个过程并不简单，因为文本通常包含需要特殊处理的特殊单元，如电话号码或电子邮件地址。一些文本挖掘任务需要将文本分割成句子。在过去几十年中，已经为不同语言开发了标记器和句子分割器，并且可以从许多编程工具箱中获取，例如
    *Spacy* [[64](#CR64)]。
- en: In the past, many preprocessing methods aimed at generating new relevant features
    (part-of-speech tags, syntax parse trees) and removing unnecessary tokens (stemming,
    stop word removal, lemmatization). In most cases, this is no longer necessary
    with modern approaches that internally automatically derive the features relevant
    for the task at hand.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，许多预处理方法旨在生成新的相关特征（词性标注、句法分析树）和删除不必要的标记（词干提取、停用词去除、词形还原）。在大多数情况下，随着现代方法内部自动推导出与任务相关的特征，这已不再是必要的。
- en: In an optional final step, the word-tokens can be further subdivided and rearranged.
    A simple technique creates *character**n-grams* (i.e. all sequences of *n* adjacent
    characters in a word) as additional features. Alternatively, *word**n-grams* can
    be formed consisting of *n* consecutive words.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个可选的最终步骤中，词标记可以被进一步细分和重新排列。一种简单的技术是创建*字符**n-gram*（即单词中所有相邻的 *n* 个字符的序列）作为额外的特征。或者，可以形成由
    *n* 个连续单词组成的 *word**n-gram*。
- en: Currently, the most popular approach tries to limit the number of different
    words in a vocabulary. A common choice is *byte-pair encoding* [[19](#CR19)].
    This method first selects all characters as tokens. Then, successively the most
    frequent token pair is merged into a new token and all instances of the token
    pair are replaced by the new token. This is repeated until a vocabulary of prescribed
    size is obtained. Note that new words can always be represented by a sequence
    of vocabulary tokens and characters. Common words end up being a part of the vocabulary,
    while rarer words are split into components, which often retain some linguistic
    meaning. In this way, out-of-vocabulary words are avoided.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，最流行的方法试图限制词汇表中的不同单词数量。一个常见的选择是 *字节对编码* [[19](#CR19)]。这种方法首先选择所有字符作为标记。然后，依次将最频繁的标记对合并成一个新的标记，并将标记对的所有实例替换为新的标记。这个过程会重复进行，直到获得规定大小的词汇表。请注意，新词总是可以用词汇表标记的序列和字符来表示。常见的单词最终会成为词汇表的一部分，而较少见的单词则被分解成组件，这些组件通常保留一些语言意义。通过这种方式，避免了词汇表外的单词。
- en: The *WordPiece* [[69](#CR69)] algorithm also starts by selecting all characters
    of the collection as tokens. Then it assumes that the text corpus has been generated
    by randomly sampling tokens according to their observed frequencies. It merges
    tokens *a* and *b* (inside words) in such a way that the likelihood of the training
    data is maximally increased [[60](#CR60)]. There is a fast variant whose computational
    complexity is linear in the input length [[63](#CR63)]. *SentencePiece* [[35](#CR35)]
    is a package containing several subword tokenizers and can also be applied to
    all Asian languages. All the approaches effectively interpolate between word level
    inputs for frequent words and character level inputs for infrequent words.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*WordPiece* [[69](#CR69)] 算法也是首先选择集合中的所有字符作为标记。然后，它假设文本语料库是通过根据观察到的频率随机采样标记生成的。它将标记
    *a* 和 *b*（在单词内部）合并，以最大程度地增加训练数据的可能性 [[60](#CR60)]。有一个快速变体，其计算复杂度与输入长度成线性关系 [[63](#CR63)]。*SentencePiece*
    [[35](#CR35)] 是一个包含多个子词标记器的软件包，也可以应用于所有亚洲语言。所有这些方法有效地在频繁单词的词级输入和罕见单词的字符级输入之间进行插值。'
- en: Often the language of the input text has to be determined [[29](#CR29), [57](#CR57)].
    Most *language identification methods* extract character *n*-grams from the input
    text and evaluate their relative frequencies. Some methods can be applied to texts
    containing different languages at the same time [[42](#CR42), [71](#CR71)]. To
    filter out offensive words from a text, one can use lists of such toxic words
    in different languages [[62](#CR62)].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通常需要确定输入文本的语言[[29](#CR29), [57](#CR57)]。大多数*语言识别方法*从输入文本中提取字符*n*-grams并评估它们的相对频率。一些方法可以应用于包含不同语言的文本[[42](#CR42),
    [71](#CR71)]。为了从文本中过滤掉冒犯性词汇，可以使用不同语言的此类有毒词汇列表[[62](#CR62)]。
- en: 1.3 Vector Space Models and Document Classification
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 向量空间模型和文档分类
- en: To apply Machine Learning to documents, their text has to be transformed into
    scalars, vectors, matrices, or higher-dimensional arrangements of numbers, which
    are collectively called *tensors*. In the previous section, text documents in
    a corpus were converted into a sequence of tokens by preprocessing. These tokens
    now have to be translated into tensors.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要将机器学习应用于文档，其文本必须被转换成标量、向量、矩阵或更高维度的数字排列，这些统称为*tensors*。在上一节中，语料库中的文本文档通过预处理被转换成一系列标记。这些标记现在需要被转换成张量。
- en: The *bag-of-words* representation describes a given text document *d* by a vector
    ***x*** of token counts. The *vocabulary* is a list of all different tokens contained
    in the collection of training documents, the *training corpus*. Ignoring the order
    of tokens, this bag-of-words vector records how often each token of the vocabulary
    appears in document *d*. Note that most vector entries will be zero, as each document
    will only contain a small fraction of vocabulary tokens. The vector of counts
    may be modified to emphasize tokens with high information content, e.g. by using
    the *tf-idf* statistic [[43](#CR43)]. Table [1.1](#Tab1) summarizes different
    representations for documents used for NLP.Table 1.1
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*词袋模型*表示通过一个标记计数的向量***x***来描述给定的文本文档*d*。*词汇表*是包含在训练文档集合（*训练语料库*）中的所有不同标记的列表。忽略标记的顺序，这个词袋向量记录了词汇表中的每个标记在文档*d*中出现的频率。请注意，大多数向量条目将是零，因为每个文档只包含词汇表中的一小部分标记。计数向量可以通过使用*tf-idf*统计量来修改，以强调具有高信息内容的标记，例如[[43](#CR43)]。表[1.1](#Tab1)总结了用于NLP的文档的不同表示。'
- en: Representations for documents used in NLP Models.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 用于NLP模型中的文档表示。
- en: '| Type | Generated by ... | Used by ... |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 由...生成 | 由...使用 |'
- en: '| --- | --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Bag-of-words | Tokenization and counting | Logistic classifier, SVM. Section
    [1.3](#Sec3). |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 词袋模型 | 通过标记化和计数生成 | 逻辑分类器、SVM。第[1.3](#Sec3)节。 |'
- en: '| Simple embeddings | Correlation and regression: topic models [[7](#CR7)],
    Word2Vec [[46](#CR46)], GloVe [[51](#CR51)]. | Classifiers, clustering, visualization,
    RNN, etc. Section [1.5](#Sec5) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 简单嵌入 | 相关性和回归：主题模型 [[7](#CR7)], Word2Vec [[46](#CR46)], GloVe [[51](#CR51)].
    | 分类器、聚类、可视化、RNN等。第[1.5](#Sec5)节 |'
- en: '| Contextual embeddings | Attention computation: ElMo [[52](#CR52)], Transformer
    [[67](#CR67)], GPT [[55](#CR55)], BERT [[17](#CR17)] and many others. | Fine-tuning
    with supervised training data. Section [2.​1](528393_1_En_2_Chapter.xhtml#Sec1).
    | *Document classification* methods aim to categorize text documents according
    to their content [[33](#CR33), [61](#CR61)]. An important example is the logistic
    classifier, which uses a bag-of-words vector ***x*** as input and predicts the
    probability of each of the *k* possible output classes *y* ∈{1, …, *k*}. More
    precisely, there is a random variable *Y*  which may take the values 1, …, *k*.
    To predict the output class *y* from the input ***x***, a score vector is first
    generated as![$$\displaystyle \begin{aligned} \boldsymbol{u}=A{\boldsymbol{x}}+\boldsymbol{b}
    {} \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ1.png)(1.1)using
    an *affine transformation* of the input ***x***. Here, the vector ***x*** is transformed
    by a *linear transformation**A****x*** and then a *bias* vector ***b*** is added.
    The resulting *score vector****u*** of length *k* is then transformed to a probability
    distribution over the *k* classes by the *softmax function*![$$\displaystyle \begin{aligned}
    \operatorname{\mathrm{softmax}}(u_1,\ldots, u_k) &amp;= \frac{(\exp(u_1),\ldots,
    \exp(u_k))}{\exp(u_1)+\cdots+ \exp(u_k)} {}, \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ2.png)(1.2)![$$\displaystyle
    \begin{aligned} p(Y\mkern1.5mu{=}\mkern1.5mu m|{\boldsymbol{x}};A,\boldsymbol{b})
    &amp;= \operatorname{\mathrm{softmax}}(A{\boldsymbol{x}}+\boldsymbol{b}) {}. \end{aligned}
    $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ3.png)(1.3)Since
    the softmax function converts any vector into a probability vector, we obtain
    the conditional probability of output class *m* as a function of input ***x***.
    The function![$$\displaystyle \begin{aligned} \text{LRM}({\boldsymbol{x}})=\operatorname{\mathrm{softmax}}(A{\boldsymbol{x}}+\boldsymbol{b})
    {} \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ4.png)(1.4)is
    called a *logistic classifier* model [[48](#CR48)] with parameter vector ***w*** = *vec*(*A*,
    *b*). In general, a function mapping the input ***x*** to the output *y* or a
    probability distribution over the output is called a *model**f*(***x***;***w***).The
    model is trained using *training data**Tr* = {(***x***^([1]), *y*^([1])), …, (***x***^([*N*]),
    *y*^([*N*]))}, whose *examples* (***x***^([*i*]), *y*^([*i*])) have to be independent
    and identically distributed (*i.i.d.*). The task is to adjust the parameters ***w***
    such that the predicted probability *p*(*Y* =*m*|***x***;***w***) is maximized.
    Following the *Maximum Likelihood principle*, this can be achieved by modifying
    the parameter vector ***w*** such that the complete training data has a maximal
    probability [[24](#CR24), p. 31]![$$\displaystyle \begin{aligned} \max_{\boldsymbol{w}}p(y^{[1]}|{\boldsymbol{x}}^{[1]};{\boldsymbol{w}})*\cdots*p(y^{[N]}|{\boldsymbol{x}}^{[N]};{\boldsymbol{w}}).
    \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ5.png)(1.5)Transforming
    the expression by log and multiplying by − 1.0 gives the *classification loss*
    function *L*[MC](***w***), also called *maximum entropy loss*.![$$\displaystyle
    \begin{aligned} L_{\text{MC}}({\boldsymbol{w}})=-\left[\log p(y^{[1]}|{\boldsymbol{x}}^{[1]};{\boldsymbol{w}})+\cdots+\log
    p(y^{[N]}|{\boldsymbol{x}}^{[N]};{\boldsymbol{w}})\right]. {} \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ6.png)(1.6)To
    optimize the loss function, its gradient is computed and minimized by stochastic
    gradient descent or another optimizer (c.f. Sect. [2.​4.​1](528393_1_En_2_Chapter.xhtml#Sec28)).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '| 上下文嵌入 | 注意力计算：ElMo [[52](#CR52)]，Transformer [[67](#CR67)]，GPT [[55](#CR55)]，BERT
    [[17](#CR17)] 以及许多其他方法。 | 使用监督训练数据进行微调。章节 [2.1](528393_1_En_2_Chapter.xhtml#Sec1)。
    | *文档分类* 方法旨在根据其内容对文本文档进行分类 [[33](#CR33)，[61](#CR61)]。一个重要的例子是逻辑分类器，它使用一个词袋向量
    ***x*** 作为输入，并预测每个可能的输出类别 *y* ∈ {1, …, *k*} 的概率。更确切地说，存在一个随机变量 *Y*，它可以取值 1, …,
    *k*。为了从输入 ***x*** 预测输出类别 *y*，首先通过一个 *仿射变换* 生成一个得分向量！[$$\displaystyle \begin{aligned}
    \boldsymbol{u}=A{\boldsymbol{x}}+\boldsymbol{b} {} \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ1.png)(1.1)使用输入
    ***x*** 的一个 *线性变换**A**x*** 和一个 *偏置向量**b***。然后，将得到的长度为 *k* 的 *得分向量**u*** 通过 *softmax
    函数* 转换为一个概率分布，该概率分布覆盖了 *k* 个类别！[$$\displaystyle \begin{aligned} \operatorname{\mathrm{softmax}}(u_1,\ldots,
    u_k) &amp;= \frac{(\exp(u_1),\ldots, \exp(u_k))}{\exp(u_1)+\cdots+ \exp(u_k)}
    {}, \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ2.png)(1.2)![$$\displaystyle
    \begin{aligned} p(Y\mkern1.5mu{=}\mkern1.5mu m|{\boldsymbol{x}};A,\boldsymbol{b})
    &amp;= \operatorname{\mathrm{softmax}}(A{\boldsymbol{x}}+\boldsymbol{b}) {}. \end{aligned}
    $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ3.png)(1.3)由于
    softmax 函数可以将任何向量转换为概率向量，因此我们得到输出类别 *m* 的条件概率作为输入 ***x*** 的函数。函数！[$$\displaystyle
    \begin{aligned} \text{LRM}({\boldsymbol{x}})=\operatorname{\mathrm{softmax}}(A{\boldsymbol{x}}+\boldsymbol{b})
    {} \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ4.png)(1.4)被称为
    *逻辑回归模型* [[48](#CR48)]，其参数向量为 ***w*** = *vec*(*A*, *b*)。一般来说，将输入 ***x*** 映射到输出
    *y* 或输出概率分布的函数称为 *模型**f**(*x*; *w*)。该模型使用 *训练数据**Tr* = {(***x***^([1]), *y*^([1])),
    …, (***x***^([*N*]), *y*^([*N*]))} 进行训练，其中 *示例* (***x***^([*i*]), *y*^([*i*]))
    必须是独立同分布的 (*i.i.d.*)。任务是调整参数 ***w***，使得预测概率 *p*(*Y* = *m*|***x***; ***w***) 最大化。根据
    *最大似然原理*，可以通过修改参数向量 ***w*** 来实现，使得完整训练数据具有最大概率 [[24](#CR24)，第 31 页]![$$\displaystyle
    \begin{aligned} \max_{\boldsymbol{w}}p(y^{[1]}|{\boldsymbol{x}}^{[1]};{\boldsymbol{w}})*\cdots*p(y^{[N]}|{\boldsymbol{x}}^{[N]};{\boldsymbol{w}}).
    \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ5.png)(1.5)通过对表达式取对数并乘以
    -1.0 得到 *分类损失函数* *L*[MC](***w***)，也称为 *最大熵损失函数*。![$$\displaystyle \begin{aligned}
    L_{\text{MC}}({\boldsymbol{w}})=-\left[\log p(y^{[1]}|{\boldsymbol{x}}^{[1]};{\boldsymbol{w}})+\cdots+\log
    p(y^{[N]}|{\boldsymbol{x}}^{[N]};{\boldsymbol{w}})\right]. {} \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ6.png)(1.6)为了优化损失函数，计算其梯度并通过随机梯度下降或其他优化器（参见图
    [2.4.1](528393_1_En_2_Chapter.xhtml#Sec28)）进行最小化。'
- en: The performance of classifiers is measured on separate *test data* by accuracy,
    precision, recall, F1-value, etc. [[21](#CR21), p. 410f]. Because the bag-of-words
    representation ignores important word order information, document classification
    by a logistic classifier is less commonly used today. However, this model is still
    a component in most Deep Learning architectures.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的性能通过准确率、精确度、召回率、F1值等在独立的*测试数据*上衡量。[21](#CR21)，第410页及以下。由于词袋表示法忽略了重要的词序信息，因此使用逻辑回归分类器进行文档分类在当今不太常见。然而，这个模型仍然是大多数深度学习架构的组成部分。
- en: 1.4 Nonlinear Classifiers
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 非线性分类器
- en: It turns out that the logistic classifier partitions the input space by linear
    hyperplanes that are not able to solve more complex classification tasks, e.g.,
    the XOR problem [[47](#CR47)]. An alternative is to generate an internal *hidden
    vector****h*** by an additional *affine transformation**A*[1]***x*** + ***b***[1]
    followed by a monotonically non-decreasing nonlinear *activation function**g*
    and use this hidden vector as input for the logistic classifier to predict the
    random variable *Y* ![$$\displaystyle \begin{aligned} {\boldsymbol{h}} &amp;=
    g(A_1{\boldsymbol{x}}+\boldsymbol{b}_1) , {} \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ7.png)(1.7)![$$\displaystyle
    \begin{aligned} p(Y\mkern1.5mu{=}\mkern1.5mu m|{\boldsymbol{x}}; {\boldsymbol{w}})
    &amp;= \operatorname{\mathrm{softmax}}(A_2{\boldsymbol{h}}+\boldsymbol{b}_2) ,
    {} \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ8.png)(1.8)where
    the parameters of this model can be collected in a parameter vector ***w*** = *vec*(*A*[1],
    *b*[1], *A*[2], *b*[2]). The form of the nonlinear activation function *g* is
    quite arbitrary, often ![$$\tanh (x)$$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_IEq1.png)
    or a *rectified linear unit*![$$\text{ReLU}(x)=\max (0,x)$$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_IEq2.png)
    is used. Fcl(***x***) = *g*(*A*[1]***x*** + ***b***[1]) is called a *fully connected
    layer*.This model (Fig. [1.1](#Fig1)) is able to solve any classification problem
    arbitrarily well, provided the length of ***h*** is large enough [[21](#CR21),
    p. 192]. By prepending more fully connected layers to the network we get a *Deep
    Neural Network*, which needs fewer parameters than a shallow network to approximate
    more complex functions. Historically, it has been called *Multilayer Perceptron*
    (MLP). Liang et al. [[40](#CR40)] show that for a large class of piecewise smooth
    functions, the sizes of hidden vectors needed by a shallow network to approximate
    a function is exponentially larger than the corresponding number of neurons needed
    by a deep network for a given degree of function approximation.![](../images/528393_1_En_1_Chapter/528393_1_En_1_Fig1_HTML.png)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，逻辑回归分类器通过线性超平面将输入空间分割，而这些超平面无法解决更复杂的分类任务，例如XOR问题。[47](#CR47)。一种替代方案是通过额外的*仿射变换**A*[1]***x***
    + ***b***[1]生成一个内部*隐藏向量**h**，然后通过单调不减的非线性*激活函数**g**，并使用这个隐藏向量作为逻辑回归分类器的输入来预测随机变量*Y*！[$$\displaystyle
    \begin{aligned} {\boldsymbol{h}} &amp;= g(A_1{\boldsymbol{x}}+\boldsymbol{b}_1)
    , {} \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ7.png)(1.7)![$$\displaystyle
    \begin{aligned} p(Y\mkern1.5mu{=}\mkern1.5mu m|{\boldsymbol{x}}; {\boldsymbol{w}})
    &amp;= \operatorname{\mathrm{softmax}}(A_2{\boldsymbol{h}}+\boldsymbol{b}_2) ,
    {} \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ8.png)(1.8)其中，该模型的参数可以收集在一个参数向量
    ***w*** = *vec*(*A*[1], *b*[1], *A*[2], *b*[2])中。非线性激活函数 *g* 的形式相当任意，通常使用 ![$$\tanh
    (x)$$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_IEq1.png) 或 *修正线性单元*![$$\text{ReLU}(x)=\max
    (0,x)$$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_IEq2.png)。Fcl(***x***)
    = *g*(*A*[1]***x*** + ***b***[1]) 被称为*全连接层*。这个模型（图[1.1](#Fig1)）能够以任意好的方式解决任何分类问题，只要
    ***h*** 的长度足够大。[21](#CR21)，第192页。通过在网络前添加更多的全连接层，我们得到一个*深度神经网络*，它比浅层网络需要的参数更少，可以逼近更复杂的函数。从历史上看，它被称为*多层感知器*（MLP）。Liang等人[[40](#CR40)]表明，对于一大类分段光滑函数，浅层网络逼近函数所需的隐藏向量的大小比深层网络在给定函数逼近程度下所需的神经元数量呈指数级大。![图片](../images/528393_1_En_1_Chapter/528393_1_En_1_Fig1_HTML.png)
- en: An illustration of a neural network has input nodes of 0.1, 1.3, and negative
    0.4 linked to six blocks of u 1, Relu of u 1, hidden vector, via A 2 h + b 2 to
    4 blocks of u 2, soft max of u 2, and ends with 4 output probabilities of 0.2,
    0.3, 0.1, and 0.4.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经网络的示意图，其输入节点为0.1、1.3和负0.4，连接到六个块u 1、u 1的ReLU、隐藏向量，通过A 2 h + b 2到四个块u 2、u
    2的softmax，并以4个输出概率0.2、0.3、0.1和0.4结束。
- en: Fig. 1.1
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1
- en: A neural network for classification transforms the input by layers with affine
    transformations and nonlinear activation functions, e.g. ReLU. The final layer
    usually is a logistic classifier
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 用于分类的神经网络通过具有仿射变换和非线性激活函数（例如ReLU）的层转换输入。最后一层通常是一个逻辑分类器
- en: The *support vector machine* [[14](#CR14)] follows a different approach and
    tries to create a hyperplane, which is located between the training examples of
    the two classes in the input space. In addition, this hyperplane should have a
    large distance (*margin*) to the examples. This model reduces overfitting and
    usually has a high classification accuracy, even if the number of input variables
    is high, e.g. for document classification [[28](#CR28)]. It was extended to different
    kernel loss criteria, e.g. graph kernels [[56](#CR56)] which include grammatical
    features. Besides SVM, many alternative classifiers are used, such as random forests
    [[24](#CR24), p.588f] and gradient boosted trees [[24](#CR24), p.360], which are
    among the most popular classifiers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*支持向量机* [[14](#CR14)] 采用不同的方法，试图在输入空间中两个类别的训练示例之间创建一个超平面。此外，这个超平面应该与示例保持较大的距离（*间隔*）。该模型减少了过拟合，并且通常具有很高的分类精度，即使输入变量的数量很高，例如在文档分类
    [[28](#CR28)] 中。它被扩展到不同的核损失标准，例如图核 [[56](#CR56)]，这些核包括语法特征。除了SVM之外，还使用了许多其他分类器，例如随机森林
    [[24](#CR24), p.588f] 和梯度提升树 [[24](#CR24), p.360]，它们是最受欢迎的分类器之一。'
- en: 'For these conventional classifiers the analyst usually has to construct input
    features manually. Modern classifiers for text analysis are able to create relevant
    features automatically (Sect. [2.​1](528393_1_En_2_Chapter.xhtml#Sec1)). For the
    training of NLP models there exist three main paradigms:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些传统分类器，分析师通常需要手动构建输入特征。现代文本分析分类器能够自动创建相关特征（见第[2.1](528393_1_En_2_Chapter.xhtml#Sec1)节）。对于NLP模型的训练，存在三种主要范式：
- en: '*Supervised training* is based on training data consisting of pairs (***x***,
    ***y***) of an input ***x***, e.g. a document text, and an output ***y***, where
    ***y*** usually is a manual annotation, e.g. a sentiment. By optimization the
    unknown parameters of the model are adapted to predict the output from the input
    in an optimal way.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监督训练*基于由输入 ***x*** 和输出 ***y*** 组成的训练数据对，其中 ***x*** 例如是一个文档文本，而 ***y*** 通常是一个手动标注，例如情感。通过优化，模型的未知参数被调整以以最佳方式从输入预测输出。'
- en: '*Unsupervised training* just considers some data ***x*** and derives some intrinsic
    knowledge from unlabeled data, such as clusters, densities, or latent representations.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无监督训练*仅考虑一些数据 ***x*** 并从未标记数据中推导出一些内在知识，例如聚类、密度或潜在表示。'
- en: '*Self-supervised training* selects parts of the observed data vector as input
    ***x*** and output ***y***. The key idea is to predict ***y*** from ***x*** in
    a supervised manner. For example, the language model is a self-supervised task
    that attempts to predict the next token *v*[*t*+1] from the previous tokens *v*[1],
    …, *v*[*t*]. For NLP models, this type of training is used very often.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自监督训练*选择观察到的数据向量的一部分作为输入 ***x*** 和输出 ***y***。关键思想是以监督方式从 ***x*** 预测 ***y***。例如，语言模型是一个自监督任务，试图从之前的标记
    *v*[1], …, *v*[*t*] 预测下一个标记 *v*[*t*+1]。对于NLP模型，这种类型的训练被非常频繁地使用。'
- en: 1.5 Generating Static Word Embeddings
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 生成静态词嵌入
- en: One problem with bag-of word representations is that frequency vectors of tokens
    are unable to capture relationships between words, such as synonymy and homonymy,
    and give no indication of their semantic similarity. An alternative are more expressive
    representations of words and documents based on the idea of *distributional semantics*
    [[58](#CR58)], popularized by Zellig Harris [[23](#CR23)] and John Firth [[18](#CR18)].
    According to Firth *“a word is characterized by the company it keeps”*. This states
    that words occurring in the same neighborhood tend to have similar meanings.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋表示法的一个问题是，标记的频率向量无法捕捉词语之间的关系，例如同义和同音，并且没有提供它们语义相似性的任何指示。一种替代方案是基于*分布语义*的思想，这种思想更能够表达词语和文档，由Zellig
    Harris [[58](#CR58)] 和 John Firth [[18](#CR18)] 推广。根据Firth的说法，“一个词的特点是它所伴随的词语”。这表明在相同邻域中出现的词语往往具有相似的意义。
- en: Based on this idea each word can be characterized by a *d*[*emb*]-dimensional
    vector ![](../images/528393_1_En_1_Chapter/528393_1_En_1_IEq3_HTML.gif)italic
    e m b left parenthesis word right parenthesis element of double struck upper R
    Superscript d Super Subscript italic e m b), a *word embedding*. Usually, a value
    between 100 and 1000 is chosen for *d*[*emb*]. These embeddings have to be created
    such that words that occur in similar contexts have embeddings with a small vector
    distance, such as the Euclidean distance. A document then can be represented by
    a sequence of such embeddings. It turns out that words usually have a similar
    meaning, if their embeddings have a low distance. Embeddings can be used as input
    for downstream text mining tasks, e.g. sentiment analysis. Goldberg [[20](#CR20)]
    gives an excellent introduction to static word embeddings. The embeddings are
    called *static embeddings* as each word has a single embedding independent of
    the context.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一想法，每个词都可以用一个*d[*emb*]-维向量 ![](../images/528393_1_En_1_Chapter/528393_1_En_1_IEq3_HTML.gif)italic
    e m b left parenthesis word right parenthesis element of double struck upper R
    Superscript d Super Subscript italic e m b) 来表征，即一个*词嵌入*。通常，*d[*emb*]的值选择在100到1000之间。这些嵌入必须被创建，使得在相似上下文中出现的词具有小的向量距离，例如欧几里得距离。然后，一个文档可以通过这样的嵌入序列来表示。结果发现，如果词的嵌入距离较低，通常这些词具有相似的意义。嵌入可以用作下游文本挖掘任务的输入，例如情感分析。Goldberg
    [[20](#CR20)] 提供了一个关于静态词嵌入的优秀介绍。这些嵌入被称为*静态嵌入*，因为每个词都有一个独立的嵌入，与上下文无关。
- en: There are a number of different approaches to generate word embeddings in an
    unsupervised way. Collobert et al. [[13](#CR13)] show that word embeddings obtained
    by predicting neighbor words can be used to improve the performance of downstream
    tasks such as named entity recognition and semantic role labeling.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的方法可以无监督地生成词嵌入。Collobert 等人 [[13](#CR13)] 表明，通过预测邻近词获得的词嵌入可以用来提高下游任务（如命名实体识别和语义角色标注）的性能。
- en: '**Word2vec** [[45](#CR45)] predicts the words in the neighborhood of a central
    word with an extremely simple model. As shown in Fig. [1.2](#Fig2) it uses the
    embedding vector of the central word as input for a logistic classifier ([1.3](#Equ3))
    to infer the probabilities of words in the neighborhood of about five to seven
    positions. The training target is to forecast all neighboring words in the training
    set with a high probability. For training, Word2Vec repeats this prediction for
    all words of a corpus, and the parameters of the logistic classifier as well as
    the values of the embeddings are optimized by stochastic gradient descent to improve
    the prediction of neighboring words.![](../images/528393_1_En_1_Chapter/528393_1_En_1_Fig2_HTML.png)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**Word2vec** [[45](#CR45)] 使用一个极其简单的模型预测中心词附近的词。如图 [1.2](#Fig2) 所示，它使用中心词的嵌入向量作为逻辑分类器
    ([1.3](#Equ3)) 的输入，以推断大约五到七个位置附近词的概率。训练目标是高概率地预测训练集中的所有邻近词。为了训练，Word2Vec 对语料库中的所有词重复进行这种预测，并通过随机梯度下降优化逻辑分类器的参数以及嵌入的值，以提高邻近词的预测准确性。![](../images/528393_1_En_1_Chapter/528393_1_En_1_Fig2_HTML.png)'
- en: An architecture has input words of Biden, has, been, U S, president, since,
    and 2021\. These are linked to embedding vector. The word U S via logistic classifier
    SoftMax, gives 4 word probabilities, that point to the words, has, been, president,
    and since.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构的输入词为 Biden, has, been, U S, president, since, and 2021\. 这些词与嵌入向量相连接。通过逻辑分类器
    SoftMax，词 U S 给出 4 个词的概率，指向 has, been, president, 和 since。
- en: Fig. 1.2
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2
- en: Word2vec predicts the words in the neighborhood of a central word by logistic
    classifier *L*. The input to *L* is the embedding of the central word. By training
    with a large set of documents, the parameters of *L* as well as the embeddings
    are learned [[54](#CR54), p. 2] The vocabulary of a text collection contains *k*
    different words, e.g. *k* = 100, 000\. To predict the probability of the *i*-th
    word by softmax ([1.2](#Equ2)), *k* exponential terms ![$$\exp (u_i)$$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_IEq4.png)
    have to be computed. To avoid this effort, the fraction is approximated as![$$\displaystyle
    \begin{aligned} \frac{\exp(u_i)}{\exp(u_1)+\cdots+exp(u_k)} \approx \frac{\exp(u_i)}{\exp(u_i)+\sum_{j\in
    S} exp(u_j)}, {} \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ9.png)(1.9)where
    *S* is a small sample of, say, 10 randomly selected indices of words. This technique
    is called *noise contrastive estimation* [[21](#CR21), p. 612]. There are several
    variants available, which are used for almost all classification tasks involving
    softmax computations with many classes. Since stochastic gradient descent works
    with noisy gradients, the additional noise introduced by the approximation of
    the softmax function is not harmful and can even help the model escape local minima.
    The shallow architecture of Word2Vec proved to be far more efficient than previous
    architectures for representation learning.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec通过逻辑分类器*L*预测中心词周围的词语。*L*的输入是中心词的嵌入。通过大量文档的训练，*L*的参数以及嵌入都被学习到 [[54](#CR54),
    p. 2]。一个文本集合的词汇表包含*k*个不同的词语，例如*k* = 100,000。为了通过softmax ([1.2](#Equ2))预测第*i*个词语的概率，需要计算*k*个指数项
    ![$$\exp (u_i)$$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_IEq4.png)。为了避免这种努力，这个分数被近似为![$$\displaystyle
    \begin{aligned} \frac{\exp(u_i)}{\exp(u_1)+\cdots+exp(u_k)} \approx \frac{\exp(u_i)}{\exp(u_i)+\sum_{j\in
    S} exp(u_j)}, {} \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ9.png)(1.9)，其中*S*是随机选择的一些词语索引的小样本，例如10个。这种技术被称为*噪声对比估计*
    [[21](#CR21), p. 612]。有几种变体可用，它们被用于几乎所有的涉及softmax计算和许多类别的分类任务。由于随机梯度下降与噪声梯度一起工作，因此由softmax函数近似引入的额外噪声并不有害，甚至可以帮助模型逃离局部最小值。Word2Vec的浅层架构在表示学习方面证明比以前的架构更有效率。
- en: Word2Vec embeddings have been used for many downstream tasks, e.g. document
    classification. In addition, words with a similar meaning may be detected by simply
    searching for words whose embeddings have a small Euclidean distance to the embedding
    of a target word. The closest neighbors of *“neutron”*, for example, are *“neutrons”*,
    *“protons”*, *“deuterium”*, *“positron”*, and *“decay”*. In this way, synonyms
    can be revealed. Projections of embeddings on two dimensions may be used for the
    exploratory analysis of the content of a corpus. **GloVe** generates similar embedding
    vectors using aggregated global word-word co-occurrence statistics from a corpus
    [[51](#CR51)].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec嵌入已被用于许多下游任务，例如文档分类。此外，通过简单地搜索与目标词嵌入具有较小欧几里得距离的词语，可以检测到具有相似意义的词语。例如，“中子”的最近邻是“中子”，“质子”，“氘”，“正电子”和“衰变”。这样就可以揭示同义词。可以在两个维度上对嵌入进行投影，用于语料库内容的探索性分析。**GloVe**使用来自语料库的聚合全局词-词共现统计来生成类似的嵌入向量
    [[51](#CR51)]。
- en: It turns out that differences between the embeddings often have an interpretation.
    For example, the result of *emb*(Germany) − *emb*(Berlin) + *emb*(Paris) has *emb*(France)
    as its nearest neighbor with respect to Euclidean distance. This property is called
    *analogy* and holds for a majority of examples of many relations such as capital-country,
    currency-country, etc. [[45](#CR45)].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，嵌入之间的差异往往具有可解释性。例如，*emb*(Germany)−*emb*(Berlin)+*emb*(Paris)的结果在欧几里得距离上最近的邻居是*emb*(France)。这种性质被称为*类比*，对于许多关系的大多数例子都成立，如首都-国家、货币-国家等。
    [[45](#CR45)]。
- en: '**FastText** [[8](#CR8)] representations enrich static word embeddings by using
    subword information. Character *n*-grams of a given length range, e.g., 3–6, are
    extracted from each word. Then, embedding vectors are defined for the words as
    well as their character *n*-grams. To train the embeddings all word and character
    *n*-gram embeddings in the neighborhood of a central word are averaged, and the
    probabilities of the central word and its character *n*-grams are predicted by
    a logistic classifier. To improve the probability prediction, the parameters of
    the model are optimized by stochastic gradient descent. This is repeated for all
    words in a training corpus. After training, unseen words can be reconstructed
    using only their *n*-gram embeddings. *Starspace* [[68](#CR68)] was introduced
    as a generalization of FastText. It allows embedding arbitrary entities (such
    as authors, products) by analyzing texts related to them and evaluating graph
    structures. An alternative are *spherical embeddings*, where unsupervised word
    and paragraph embeddings are constrained to a hypersphere [[44](#CR44)].'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**FastText** [[8](#CR8)] 表示通过使用子词信息丰富了静态词嵌入。从每个单词中提取给定长度的字符 *n*-gram，例如，3–6。然后，为单词及其字符
    *n*-gram 定义嵌入向量。为了训练嵌入，将中心词及其周围所有单词和字符 *n*-gram 的嵌入进行平均，并通过逻辑分类器预测中心词及其字符 *n*-gram
    的概率。为了提高概率预测，通过随机梯度下降优化模型参数。这适用于训练语料库中的所有单词。训练后，可以使用单词的 *n*-gram 嵌入来重建未见过的单词。*Starspace*
    [[68](#CR68)] 被引入作为 FastText 的一般化。它通过分析与之相关的文本并评估图结构，允许嵌入任意实体（如作者、产品）。另一种选择是 *球面嵌入*，其中无监督的词和段落嵌入被限制在超球面上
    [[44](#CR44)]。'
- en: 1.6 Recurrent Neural Networks
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 循环神经网络
- en: '*Recurrent Neural Networks* were developed to model sequences *v*[1], …, *v*[*T*]
    of varying length *T*, for example the tokens of a text document. Consider the
    task to predict the next token *v*[*t*+1] given the previous tokens (*v*[1], …,
    *v*[*t*]). As proposed by Bengio et al. [[6](#CR6)] each token *v*[*t*] is represented
    by an embedding vector ***x***[*t*] = *emb*(*v*[*t*]) indicating the meaning of
    *v*[*t*]. The previous tokens are characterized by a hidden vector ***h***[*t*],
    which describes the state of the subsequence (*v*[1], …, *v*[*t*−1]). The RNN
    is a function Rnn(***h***[*t*], ***x***[*t*]) predicting the next hidden vector
    ***h***[*t*+1] by![$$\displaystyle \begin{aligned} {\boldsymbol{h}}_{t+1}=\text{RNN}({\boldsymbol{h}}_t
    , {\boldsymbol{x}}_t). \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ10.png)(1.10)Subsequently,
    a *logistic classifier* ([1.3](#Equ3)) with parameters *H* and ***g*** predicts
    a probability vector for the next token *v*[*t*+1] using the information contained
    in *h*[*t*+1],![$$\displaystyle \begin{aligned} p(V_{t+1}|v_1,\ldots,v_t)=\operatorname{\mathrm{softmax}}(H*{\boldsymbol{h}}_{t+1}+\boldsymbol{g}),
    \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ11.png)(1.11)as
    shown in Fig. [1.3](#Fig3). Here *V*[*t*] is the random variable of possible tokens
    at position *t*. According to the definition of the conditional probability the
    joint probability of the whole sequence can be factorized as![$$\displaystyle
    \begin{aligned} p(v_1,\ldots,v_T) = p(V_T\mkern1.5mu{=}\mkern1.5mu v_T|v_1,\ldots,v_{T-1})*
    \cdots* p(V_2\mkern1.5mu{=}\mkern1.5mu v_2|v_1)*p(V_1\mkern1.5mu{=}\mkern1.5mu
    v_1). {} \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ12.png)(1.12)A
    model that either computes the joint probability or the conditional probability
    of natural language texts is called *language model* as it potentially covers
    all information about the language. A language model sequentially predicting the
    next word by the conditional probability is often referred to *autoregressive
    language model*. According to ([1.12](#Equ12)), the observed tokens (*v*[1], …,
    *v*[*t*]) can be used as input to predict the probability of the next token *V*[*t*+1].
    The product of these probabilities yields the correct joint probability of the
    observed token sequence (*v*[1], …, *v*[*T*]). The same model Rnn(***h***, ***x***)
    is repeatedly applied and generates a sequence of hidden vectors ***h***[*t*].
    A *simple RNN* just consists of a single *fully connected layer*![$$\displaystyle
    \begin{aligned} \text{RNN}({\boldsymbol{h}}_t , {\boldsymbol{x}}_t) = \tanh \left(A*\begin{bmatrix}
    {\boldsymbol{h}}_t\\ {\boldsymbol{x}}_t\end{bmatrix}+{\boldsymbol{b}}\right).
    \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ13.png)(1.13)The
    probabilities of the predicted words *v*[1], …, *v*[*T*] depend on the parameters
    ***w*** = *vec*(*H*, ***g***, *A*, ***b***, *emb*(*v*[1]), …, *emb*(*v*[*T*])).
    To improve these probabilities, we may use the stochastic gradient descent optimizer
    (Sect. [2.​4.​1](528393_1_En_2_Chapter.xhtml#Sec29)) and adapt the unknown parameters
    in ***w***. Note that this also includes the estimation of new token embeddings
    *emb*(*v*[*t*]). A recent overview is given in [[70](#CR70), Ch. 8–9].![](../images/528393_1_En_1_Chapter/528393_1_En_1_Fig3_HTML.png)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*循环神经网络*（Recurrent Neural Networks）是为了模拟长度可变的序列 *v*[1]，…，*v*[*T*]，例如文本文档的标记而开发的。考虑这样一个任务：根据前面的标记（*v*[1]，…，*v*[*t*]）预测下一个标记
    *v*[*t*+1]。正如 Bengio 等人提出的 [[6](#CR6)]，每个标记 *v*[*t*] 都由一个嵌入向量 ***x***[*t*] = *emb*(*v*[*t*])
    表示，它表示 *v*[*t*] 的含义。前面的标记由一个隐藏向量 ***h***[*t*] 描述，它描述了子序列（*v*[1]，…，*v*[*t*−1]）的状态。RNN
    是一个函数 Rnn(***h***[*t*]，***x***[*t*])，通过以下公式预测下一个隐藏向量 ***h***[*t*+1]：![$$\displaystyle
    \begin{aligned} {\boldsymbol{h}}_{t+1}=\text{RNN}({\boldsymbol{h}}_t , {\boldsymbol{x}}_t).
    \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ10.png)(1.10)随后，一个具有参数
    *H* 和 ***g*** 的 *逻辑分类器* ([1.3](#Equ3)) 使用 *h*[*t*+1] 中包含的信息预测下一个标记 *v*[*t*+1]
    的概率向量，如下所示：![$$\displaystyle \begin{aligned} p(V_{t+1}|v_1,\ldots,v_t)=\operatorname{\mathrm{softmax}}(H*{\boldsymbol{h}}_{t+1}+\boldsymbol{g}),
    \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ11.png)(1.11)如图
    [1.3](#Fig3) 所示。在这里，*V*[*t*] 是位置 *t* 处可能标记的随机变量。根据条件概率的定义，整个序列的联合概率可以分解为！[$$\displaystyle
    \begin{aligned} p(v_1,\ldots,v_T) = p(V_T\mkern1.5mu{=}\mkern1.5mu v_T|v_1,\ldots,v_{T-1})*
    \cdots* p(V_2\mkern1.5mu{=}\mkern1.5mu v_2|v_1)*p(V_1\mkern1.5mu{=}\mkern1.5mu
    v_1). {} \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ12.png)(1.12)一个计算自然语言文本的联合概率或条件概率的模型被称为
    *语言模型*，因为它可能涵盖了关于语言的所有信息。通常将按条件概率顺序预测下一个单词的语言模型称为 *自回归语言模型*。根据 ([1.12](#Equ12))，观察到的标记（*v*[1]，…，*v*[*t*]）可以用作输入来预测下一个标记
    *V*[*t*+1] 的概率。这些概率的乘积产生了观察到的标记序列（*v*[1]，…，*v*[*T*]）的正确联合概率。相同的模型 Rnn(***h***，***x***)
    重复应用并生成一系列隐藏向量 ***h***[*t*]。一个 *简单的 RNN* 只包含一个 *全连接层*![$$\displaystyle \begin{aligned}
    \text{RNN}({\boldsymbol{h}}_t , {\boldsymbol{x}}_t) = \tanh \left(A*\begin{bmatrix}
    {\boldsymbol{h}}_t\\ {\boldsymbol{x}}_t\end{bmatrix}+{\boldsymbol{b}}\right).
    \end{aligned} $$](../images/528393_1_En_1_Chapter/528393_1_En_1_Chapter_TeX_Equ13.png)(1.13)预测的单词
    *v*[1]，…，*v*[*T*] 的概率取决于参数 ***w*** = *vec*(*H*，***g***，*A*，***b***，*emb*(*v*[1])，…，*emb*(*v*[*T*]))。为了提高这些概率，我们可能使用随机梯度下降优化器（Sect.
    [2.​4.​1](528393_1_En_2_Chapter.xhtml#Sec29)）并调整 ***w*** 中的未知参数。请注意，这还包括估计新的标记嵌入
    *emb*(*v*[*t*])。最近的一个概述可以在 [[70](#CR70)，Ch. 8–9] 中找到。![](../images/528393_1_En_1_Chapter/528393_1_En_1_Fig3_HTML.png)'
- en: An architecture has input words of, the, cat, sat, on, and the. These are linked
    to embedding vector, hidden vector, logistic classifier, token probabilities,
    and ends with tokens to be predicted which are, the, cat, sat, on, the, mat.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一种架构具有输入单词 the, cat, sat, on 和 the。这些与嵌入向量、隐藏向量、逻辑分类器、标记概率相连接，并以要预测的标记结束，这些标记是
    the, cat, sat, on, the, mat。
- en: Fig. 1.3
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3
- en: The RNN starts on the left side and successively predicts the probability of
    the next token with the previous tokens as conditions using a logistic classifier
    *L*. The hidden vector ***h***[*t*] stores information about the tokens that occur
    before position *t*
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 从左侧开始，并使用逻辑分类器 *L* 依次预测下一个标记的概率，条件是使用前面的标记。隐藏向量 ***h***[*t*] 存储关于位置 *t*
    之前出现的标记的信息
- en: It turns out that this model has difficulties to reconstruct the relation between
    distant sequence elements, since gradients tend to vanish or “explode” as the
    sequences get longer. Therefore, new RNN types have been developed, e.g. the *Long
    Short-Term Memory* (LSTM) [[26](#CR26)] and the *Gated Recurrent Unit* (GRU) [[11](#CR11)],
    which capture long-range dependencies in the sequence much better.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，这个模型在重建远程序列元素之间的关系方面存在困难，因为随着序列变长，梯度往往会消失或“爆炸”。因此，已经开发出了新的 RNN 类型，例如 *长短期记忆*
    (LSTM) [[26](#CR26)] 和 *门控循环单元* (GRU) [[11](#CR11)]，它们在序列中更好地捕捉长距离依赖关系。
- en: Besides predicting the next word in a sequence, RNNs have been successfully
    applied to predict properties of sequence elements, e.g. named entity recognition
    [[36](#CR36)] and relation extraction [[38](#CR38)]. For these applications *bidirectional
    RNNs* have been developed, consisting of a forward and a backward language model.
    The *forward language model* starts at the beginning of a text and predicts the
    next token, while the *backward language model* starts at the end of a text and
    predicts the previous token. Bidirectional LSTMs are also called *biLSTMs*. In
    addition, *multilayer RNNs* were proposed [[72](#CR72)], where the hidden vector
    generated by the RNN-cell in one layer is used as the input to the RNN-cell in
    the next layer, and the last layer provides the prediction of the current task.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 除了预测序列中的下一个单词外，RNN 还成功应用于预测序列元素的属性，例如命名实体识别 [[36](#CR36)] 和关系抽取 [[38](#CR38)]。对于这些应用，已经开发出了
    *双向 RNN*，它由一个正向语言模型和一个反向语言模型组成。*正向语言模型* 从文本的开始处开始，预测下一个标记，而 *反向语言模型* 从文本的末尾开始，预测上一个标记。双向
    LSTM 也被称为 *biLSTMs*。此外，还提出了 *多层 RNN* [[72](#CR72)]，其中一层中 RNN 单元的生成的隐藏向量被用作下一层
    RNN 单元的输入，最后一层提供当前任务的预测。
- en: '*Machine translation* from one language to another is an important application
    of RNNs [[5](#CR5)]. In this process, an input sentence first is encoded by an
    *encoder* RNN as a hidden vector ***h***[*T*]. This hidden vector is in turn used
    by a second *decoder* RNN as an initial hidden vector to generate the words of
    the target language sentence. However, RNNs still have difficulties to capture
    relationships over long distances between sequence elements because RNNs do not
    cover direct relations between distant sequence elements.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*机器翻译* 从一种语言到另一种语言是 RNN 的重要应用 [[5](#CR5)]。在这个过程中，一个输入句子首先被一个 *编码器* RNN 编码为一个隐藏向量
    ***h***[*T*]。这个隐藏向量随后被第二个 *解码器* RNN 作为初始隐藏向量用来生成目标语言句子的单词。然而，RNN 仍然难以捕捉序列元素之间长距离的关系，因为
    RNN 不涵盖远程序列元素之间的直接关系。'
- en: '*Attention* was first used in the context of machine translation to communicate
    information over long distances. It computes the correlation between hidden vectors
    of the decoder RNN and hidden vectors of the encoder RNN at different positions.
    This correlation is used to build a *context vector* as a weighted average of
    relevant encoder hidden vectors. Then, this context vector is exploited to improve
    the final translation result [[5](#CR5)]. The resulting translations were much
    better than those with the original RNN. We will see in later sections that attention
    is a fundamental principle to construct better NLP model.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* 首次在机器翻译的背景下使用，用于在长距离传递信息。它计算解码器 RNN 的隐藏向量和编码器 RNN 在不同位置的隐藏向量之间的相关性。这种相关性被用来构建一个
    *上下文向量*，作为相关编码器隐藏向量的加权平均。然后，这个上下文向量被用来提高最终的翻译结果 [[5](#CR5)]。结果翻译比原始 RNN 的翻译要好得多。我们将在后面的章节中看到，注意是一个构建更好的
    NLP 模型的基本原理。'
- en: '**ELMo** [[52](#CR52)] generates embeddings with bidirectional LSTM language
    models in several layers. The model is pre-trained as forward and backward language
    model with a large non-annotated text corpus. During fine-tuning, averages of
    the hidden vectors are used to predict the properties of words based on an annotated
    training set. These language models take into account the words before and after
    a position, and thus employ contextual representations for the word in the central
    position. For a variety of tasks such as sentiment analysis, question answering,
    and textual entailment, ELMo was able to improve Sota performance.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**ELMo** [[52](#CR52)] 使用多层双向 LSTM 语言模型生成嵌入。该模型使用大量非标注文本语料库进行预训练，作为正向和反向语言模型。在微调期间，使用隐藏向量的平均值来预测基于标注训练集的词性。这些语言模型考虑了位置前后词语，因此为中央位置的词语提供了上下文表示。对于各种任务，如情感分析、问答和文本蕴含，ELMo
    能够提高 Sota 性能。'
- en: 1.7 Convolutional Neural Networks
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7 卷积神经网络
- en: '*Convolutional Neural Networks* (*CNNs*) [[37](#CR37)] are widely known for
    their success in the image domain. They start with a small quadratic arrangement
    of parameters called *filter kernel*, which is moved over the input pixel matrix
    of the image. The values of the filter kernel are multiplied with the underlying
    pixel values and generate an output value. This is repeated for every position
    of the input pixel matrix. During training the parameters of a filter kernel are
    automatically tuned such that they can detect local image patterns such as blobs
    or lines. Each layer of the network, which is also called *convolution layer*,
    consists of many filter kernels and a network contains a number of convolution
    layers. Interspersed *max pooling* layers perform a local aggregation of pixels
    by maximum. The final layer of a Convolutional Neural Network usually is a fully
    connected layer with a softmax classifier.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积神经网络* (*CNNs*) [[37](#CR37)] 因其在图像领域的成功而广为人知。它们从称为 *滤波器核* 的小型二次参数排列开始，该核在图像的输入像素矩阵上移动。滤波器核的值与底层像素值相乘，生成一个输出值。对于输入像素矩阵的每个位置都重复此操作。在训练过程中，滤波器核的参数会自动调整，以便检测局部图像模式，如块或线条。网络的每一层，也称为
    *卷积层*，由许多滤波器核组成，一个网络包含多个卷积层。间隔的 *最大池化* 层通过最大值进行局部像素聚合。卷积神经网络的最后一层通常是一个具有 softmax
    分类器的全连接层。'
- en: Their breakthrough was *AlexNet* [[34](#CR34)], which receives the RGB pixel
    matrix of an image as input and is tasked with assigning a content class to the
    image. This model won the 2012 *ImageNet* competition, where images had to be
    assigned to one of 1000 classes, and demonstrated the superior performance of
    Deep Neural Networks. Even earlier the deep CNN of Cireşan et al. [[12](#CR12)]
    achieved Sota performance on a number of image classification benchmarks. A highly
    successful CNN is *ResNet* [[25](#CR25)] which employs a so-called *residual connection*
    working as a bypass. It can circumvent many layers in the beginning of the training
    and is the key to training neural networks with many hundred layers. It resulted
    in image classifiers which have a higher accuracy than humans.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的突破是 *AlexNet* [[34](#CR34)]，它接收图像的 RGB 像素矩阵作为输入，并负责为图像分配内容类别。该模型赢得了 2012
    年的 *ImageNet* 比赛，其中图像必须被分配到 1000 个类别之一，并展示了深度神经网络的优越性能。更早之前，Cireşan 等人的深度 CNN
    [[12](#CR12)] 在多个图像分类基准测试中实现了 Sota 性能。一个非常成功的 CNN 是 *ResNet* [[25](#CR25)]，它使用一种所谓的
    *残差连接* 作为旁路。它可以绕过训练开始阶段的许多层，是训练具有数百层神经网络的密钥。它导致了具有比人类更高的准确率的图像分类器。
- en: While Recurrent Neural Networks were regarded as the best way to process sequential
    input such as text, some CNN-based architectures were introduced, which achieved
    high performance on some NLP tasks. Kim [[32](#CR32)] proposed a rather shallow
    CNN for sentence classification. It contains an embedding layer, a convolutional
    layer, a max-pooling layer, and a fully connected layer with softmax output. *1-D
    convolutions* were applied to the embeddings of the input words, basically combining
    the information stored in adjacent words, treating them as *n*-grams. The embeddings
    are processed by a moving average with trainable weights. Using this architecture
    for classification proved to be very efficient, having a similar performance as
    recurrent architectures that are more difficult to train.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当循环神经网络被认为是最适合处理如文本这样的顺序输入时，一些基于CNN的架构被引入，它们在某些NLP任务上实现了高性能。Kim [[32](#CR32)]
    提出了一种相当浅的CNN用于句子分类。它包含一个嵌入层、一个卷积层、一个最大池化层和一个具有softmax输出的全连接层。*一维卷积*被应用于输入单词的嵌入，基本上结合了相邻单词中存储的信息，将它们视为
    *n*-grams。嵌入通过具有可训练权重的移动平均进行处理。使用这种架构进行分类已被证明非常高效，其性能与更难训练的循环架构相似。
- en: Another interesting CNN architecture is *wavenet* [[49](#CR49)], a deeper network
    used mainly for text-to-speech synthesis. It consists of multiple convolutional
    layers stacked on top of each other, with its main ingredient being *dilated causal
    convolutions*. Causal means that the convolutions at position *t* can only utilize
    prior information ***x***[1], …, ***x***[*t*−1]. Dilated means that the convolutions
    can skip input values with a certain step size *k*, i.e. that in some layer the
    features at position *t* are predicted using information from positions *t*, *t* − *k*,
    *t* − 2*k*, …. This step size *k* is doubled in each successive layer, yielding
    dilations of size *k*⁰, *k*¹, *k*², …. In this way, very long time spans can be
    included in the prediction. This model architecture has been shown to give very
    good results for text-to-speech synthesis.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的卷积神经网络架构是 *wavenet* [[49](#CR49)]，这是一个主要用于文本到语音合成的更深层次的网络。它由多个堆叠在一起的卷积层组成，其主要成分是
    *扩张因果卷积*。因果意味着位置 *t* 的卷积只能利用先前的信息 ***x***[1]，…，***x***[*t*−1]。扩张意味着卷积可以跳过具有特定步长
    *k* 的输入值，即在某些层中，位置 *t* 的特征是通过位置 *t*、*t* − *k*、*t* − 2*k*，… 的信息预测的。这个步长 *k* 在每一层连续加倍，产生大小为
    *k*⁰、*k*¹、*k*²，… 的扩张。这样，可以包含非常长的时间跨度进行预测。这种模型架构已被证明在文本到语音合成方面取得了非常好的效果。
- en: 1.8 Summary
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.8 摘要
- en: Classical NLP has a long history, and machine learning models have been used
    in the field for several decades. They all require some preprocessing steps to
    generate words or tokens from the input text. Tokens are particularly valuable
    because they form a dictionary of finite size and allow arbitrary words to be
    represented by combination. Therefore, they are used by most PLMs. Early document
    representations like bag-of-words are now obsolete because they ignore sequence
    information. Nevertheless, classifiers based on them like logistic classifiers
    and fully connected layers, are important building blocks of PLMs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的自然语言处理有着悠久的历史，机器学习模型在该领域已经使用了数十年。它们都需要一些预处理步骤来从输入文本生成单词或标记。标记尤其有价值，因为它们形成了一个有限大小的字典，并允许任意单词通过组合来表示。因此，它们被大多数PLM所使用。早期的文档表示，如词袋模型，现在已经过时，因为它们忽略了序列信息。尽管如此，基于它们的分类器，如逻辑分类器和全连接层，是PLM的重要构建块。
- en: The concept of static word embeddings initiated the revolution in NLP, which
    is based on contextual word embeddings. These ideas are elaborated in the next
    chapter. Recurrent neural networks have been used to implement the first successful
    language models, but were completely superseded by attention-based models. Convolutional
    neural networks for image processing are still employed in many applications.
    PLMs today often have a similar performance on image data, and sometimes CNNs
    are combined with PLMs to exploit their respective strengths, as discussed in
    Chap. [7](528393_1_En_7_Chapter.xhtml).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 静态词嵌入的概念引发了自然语言处理领域的革命，该领域基于上下文词嵌入。这些想法将在下一章中详细阐述。循环神经网络被用来实现第一个成功的语言模型，但被基于注意力的模型完全取代。用于图像处理的卷积神经网络在许多应用中仍然被采用。今天的预训练语言模型在图像数据上通常具有相似的性能，有时将CNN与PLM结合以利用它们各自的优势，如第[7](528393_1_En_7_Chapter.xhtml)章所述。
- en: '[![Creative Commons](../css/cc-by.png)](https://creativecommons.org/licenses/by/4.0)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Creative Commons](../css/cc-by.png)](https://creativecommons.org/licenses/by/4.0)'
- en: '**Open Access** This chapter is licensed under the terms of the Creative Commons
    Attribution 4.0 International License ([http://​creativecommons.​org/​licenses/​by/​4.​0/​](http://creativecommons.org/licenses/by/4.0/)),
    which permits use, sharing, adaptation, distribution and reproduction in any medium
    or format, as long as you give appropriate credit to the original author(s) and
    the source, provide a link to the Creative Commons license and indicate if changes
    were made.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**开放获取** 本章节根据Creative Commons Attribution 4.0 International License（[http://creativecommons.org/licenses/by/4.0/](http://creativecommons.org/licenses/by/4.0/)）的条款进行许可，允许在任何媒介或格式中使用、分享、改编、分发和复制，只要您适当引用原始作者和来源，提供Creative
    Commons许可的链接，并指出是否进行了修改。'
- en: The images or other third party material in this chapter are included in the
    chapter's Creative Commons license, unless indicated otherwise in a credit line
    to the material. If material is not included in the chapter's Creative Commons
    license and your intended use is not permitted by statutory regulation or exceeds
    the permitted use, you will need to obtain permission directly from the copyright
    holder.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节中的图像或其他第三方材料包含在本章节的Creative Commons许可中，除非在材料引用行中另有说明。如果材料未包含在本章节的Creative
    Commons许可中，且您的使用意图不受法定法规允许或超出允许的使用范围，您将需要直接从版权持有人处获得许可。
