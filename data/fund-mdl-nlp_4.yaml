- en: '© The Author(s) 2023G. Paaß, S. GiesselbachFoundation Models for Natural Language
    ProcessingArtificial Intelligence: Foundations, Theory, and Algorithms[https://doi.org/10.1007/978-3-031-23190-2_5](https://doi.org/10.1007/978-3-031-23190-2_5)'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: © 作者（们）2023 G. Paaß, S. Giesselbach自然语言处理基础模型[https://doi.org/10.1007/978-3-031-23190-2_5](https://doi.org/10.1007/978-3-031-23190-2_5)
- en: 5. Foundation Models for Information Extraction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5. 信息提取基础模型
- en: Gerhard Paaß^([1](#Aff5)  ) and Sven Giesselbach^([1](#Aff5))(1)Knowledge Discovery
    Department, Team NLU, Fraunhofer Institute for Intelligent Analysis and Information
    Systems (IAIS), Sankt Augustin, Nordrhein-Westfalen, Germany
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Gerhard Paaß^([1](#Aff5)  ) 和 Sven Giesselbach^([1](#Aff5))(1)知识发现部门，NLU 团队，弗劳恩霍夫智能分析和信息系统研究所（IAIS），圣奥古斯丁，北莱茵-威斯特法伦，德国
- en: Abstract
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In the chapter we consider Information Extraction approaches that automatically
    identify structured information in text documents and comprise a set of tasks.
    The Text Classification task assigns a document to one or more pre-defined content
    categories or classes. This includes many subtasks such as language identification,
    sentiment analysis, etc. The Word Sense Disambiguation task attaches a predefined
    meaning to each word in a document. The Named Entity Recognition task identifies
    named entities in a document. An entity is any object or concept mentioned in
    the text and a named entity is an entity that is referred to by a proper name.
    The Relation Extraction task aims to identify the relationship between entities
    extracted from a text. This covers many subtasks such as coreference resolution,
    entity linking, and event extraction. Most demanding is the joint extraction of
    entities and relations from a text. Traditionally, relatively small Pre-trained
    Language Models have been fine-tuned to these task and yield high performance,
    while larger Foundation Models achieve high scores with few-shot prompts, but
    usually have not been benchmarked.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们考虑自动识别文本文档中的结构化信息的信息提取方法，并包含一系列任务。文本分类任务将文档分配到一个或多个预定义的内容类别或类别。这包括许多子任务，如语言识别、情感分析等。词义消歧任务为文档中的每个词分配一个预定义的含义。命名实体识别任务在文档中识别命名实体。实体是文本中提到的任何对象或概念，而命名实体是使用专有名称引用的实体。关系提取任务旨在识别从文本中提取的实体之间的关系。这包括许多子任务，如指代消解、实体链接和事件提取。最具有挑战性的是从文本中联合提取实体和关系。传统上，相对较小的预训练语言模型经过微调以完成这些任务，并取得了高性能，而较大的基础模型在少量提示下取得了高分，但通常没有进行基准测试。
- en: KeywordsText classificationNamed entity recognitionRelation extractionSentiment
    analysisLanguage understandingThere are a large number of NLP applications of
    Pre-trained Language Models (PLMs), which can be roughly divided into three areas
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词文本分类命名实体识别关系提取情感分析语言理解预训练语言模型（PLMs）在自然语言处理（NLP）中有大量应用，大致可以分为三个领域
- en: '*Information Extraction* (*IE*) automatically identifies structured information
    in textual documents and analyzes language features (Chap. [5](528393_1_En_5_Chapter.xhtml)).'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*信息提取*（*IE*）自动识别文本文档中的结构化信息并分析语言特征（第 [5](528393_1_En_5_Chapter.xhtml) 章）。'
- en: '*Natural Language Generation* (*NLG*) automatically generates new natural language
    text, often in response to some prompt (Chap. [6](528393_1_En_6_Chapter.xhtml)).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自然语言生成*（*NLG*）自动生成新的自然语言文本，通常是对某些提示的响应（第 [6](528393_1_En_6_Chapter.xhtml)
    章）。'
- en: '*Multimodal Content Analysis* and generation integrates the understanding and
    production of content across two or more modalities like text, speech, image,
    video, etc (Chap. [7](528393_1_En_7_Chapter.xhtml)).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多模态内容分析* 和生成将理解与生产内容整合到两个或多个模态中，如文本、语音、图像、视频等（第 [7](528393_1_En_7_Chapter.xhtml)
    章）。'
- en: 'These applications are described in the three following chapters.In the present
    chapter we focus on **information extraction** with PLMs. Information extraction
    includes the following tasks:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些应用在接下来的三个章节中进行了描述。在本章中，我们重点关注使用 PLMs 进行 **信息提取**。信息提取包括以下任务：
- en: '*Text classification* assigns a document to one or more pre-defined content
    *categories* or classes (Sect. [5.1](#Sec1)). Note that many subtasks can be formulated
    as classification problems, e.g. language identification, sentiment analysis,
    etc. (Table [5.1](#Tab1)).Table 5.1'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文本分类* 将文档分配到一个或多个预定义的内容 *类别* 或类别（第 [5.1](#Sec1) 节）。请注意，许多子任务可以表述为分类问题，例如语言识别、情感分析等。（表
    [5.1](#Tab1)）表 5.1'
- en: Language analysis tasks based on text classification illustrated by examples
  id: totrans-11
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于文本分类的示例说明语言分析任务
- en: '| Task | Description | Example |'
  id: totrans-12
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 任务 | 描述 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Language identification | Determine the language of a text, Sect. [1.​2](528393_1_En_1_Chapter.xhtml#Sec2).
    | *Shakespeare lived 400 years ago*![$$\rightarrow $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq1.png)*English*
    |'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 语言识别 | 确定文本的语言，Sect. [1.2](528393_1_En_1_Chapter.xhtml#Sec2)。 | *莎士比亚生活在400年前*![$$\rightarrow
    $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq1.png)*英语* |'
- en: '| Document classification | Assign a content category (class), e.g. economy,
    to a document or text, Sect. [5.1](#Sec1) | *The Dow-Jones is up 50 points*![$$\rightarrow
    $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq2.png)*economy*
    |'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 文档分类 | 为文档或文本分配内容类别（类别），例如经济，Sect. [5.1](#Sec1) | *道琼斯指数上涨50点*![$$\rightarrow
    $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq2.png)*经济* |'
- en: '| Sentiment analysis | Classification of a text according to the sentiment
    expressed in it (e.g. positive, negative, neutral), Sect. [5.1](#Sec1) | *Today
    I feel really lousy.*![$$\rightarrow $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq3.png)*negative*
    |'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 情感分析 | 根据文本中表达的情感对文本进行分类（例如积极、消极、中性），Sect. [5.1](#Sec1) | *今天我感觉真的很糟糕*![$$\rightarrow
    $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq3.png)*消极* |'
- en: '| Hate speech detection | Recognize if a text contains hate speech, Sect. [5.1.1](#Sec2)
    | *Immigrants infest our country*![$$\rightarrow $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq4.png)*hate
    speech* |'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 仇恨言论检测 | 识别文本中是否包含仇恨言论，Sect. [5.1.1](#Sec2) | *移民侵占了我们的国家*![$$\rightarrow
    $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq4.png)*仇恨言论*
    |'
- en: '| Fake news detection | Detect a text that contains fake news, Sect. [6.​5.​5](528393_1_En_6_Chapter.xhtml#Sec45)
    | *Measles vaccination causes meningitis.*![$$\rightarrow $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq5.png)*fake
    news* |'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 假新闻检测 | 检测包含假新闻的文本，Sect. [6.5.5](528393_1_En_6_Chapter.xhtml#Sec45) | *麻疹疫苗接种导致脑膜炎*![$$\rightarrow
    $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq5.png)*假新闻* |'
- en: '| Logical relation | Determine whether the second text contains a logical consequence,
    a contradiction, or a neutral statement relative to the first text, Sect. [2.​1.​5](528393_1_En_2_Chapter.xhtml#Sec7)
    | *John has a flat.* ↔[*contradiction*]*John is a homeless person.* |'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 逻辑关系 | 确定第二文本相对于第一文本是否包含逻辑后果、矛盾或中性陈述，Sect. [2.1.5](528393_1_En_2_Chapter.xhtml#Sec7)
    | *约翰的轮胎没气了* ↔[*矛盾*]*约翰是无家可归的人* |'
- en: '| Text entailment | Does the first text imply the truth of the second text?
    Sect. [2.​1.​5](528393_1_En_2_Chapter.xhtml#Sec7) | *Exercising improves health.*
    →[*entails*]*Physical activity has good consequences.* |'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 文本蕴涵 | 第一文本是否意味着第二文本的真实性？Sect. [2.1.5](528393_1_En_2_Chapter.xhtml#Sec7)
    | *锻炼有益于健康。* →[*蕴涵*]*身体活动有良好的后果* |'
- en: '| Paraphrase detection | Determine if two texts are semantically equivalent,
    Sect. [2.​1.​5](528393_1_En_2_Chapter.xhtml#Sec7) | *Fred is tired. /Fred wants
    to sleep.*![$$\rightarrow $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq6.png)*equivalent*
    |'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 意义消歧 | 确定两个文本是否在语义上等价，Sect. [2.1.5](528393_1_En_2_Chapter.xhtml#Sec7) | *弗雷德累了。/弗雷德想睡觉*![$$\rightarrow
    $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq6.png)*等价* |'
- en: '| Dialog act classification | Determine the type of an utterance in a dialog
    (question, statement, request for action, etc.) | *Where is the dog?*![$$\rightarrow
    $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq7.png)*question*
    |'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 对话行为分类 | 确定对话中话语的类型（问题、陈述、行动请求等） | *狗在哪里？*![$$\rightarrow $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq7.png)*问题*
    |'
- en: '*Word Sense Disambiguation* (*WSD*) connects a predefined meaning to each word
    in a document. This is especially important for the interpretation of *homonyms*,
    i.e. words that have several meanings depending on the context (Sect. [5.2](#Sec7)).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*词义消歧* (*WSD*) 将预定义的意义与文档中的每个词相关联。这对于*同音异义词*的解释尤为重要，即根据上下文具有几个不同意义的词（Sect.
    [5.2](#Sec7)）。'
- en: '*Named Entity Recognition* (*NER*) identifies *named entities* in a document.
    An *entity* is an any object or concept mentioned in the text. A *named entity*
    is an entity that is referred to by a proper name. NER also associates a type
    with each entity, e.g. person, location, organization, etc. (Sect. [5.3](#Sec12)).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*命名实体识别* (*NER*) 在文档中识别 *命名实体*。*实体* 是文本中提到的任何对象或概念。*命名实体* 是通过专有名称引用的实体。NER
    还将每个实体关联到一个类型，例如人、地点、组织等。（Sect. [5.3](#Sec12)）。'
- en: '*Relation Extraction* aims to identify the relationship between *entities*
    extracted from a text (Sect. [5.4](#Sec19)). This covers many subtasks such as
    coreference resolution, entity linking, and event extraction (Table [5.3](#Tab3)).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关系抽取*旨在识别从文本中提取的*实体*之间的关系（见第[5.4](#Sec19)节）。这包括许多子任务，如指代消解、实体链接和事件抽取（见表[5.3](#Tab3)）。'
- en: Due to the large number of different approaches, we focus on representative
    models which exhibit a high performance at the time of writing. Traditionally
    relatively small PLMs have been fine-tuned to these task and yield high performance,
    while larger Foundation Models achieve high scores with few-shot prompts, but
    usually have not been benchmarked.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在大量不同的方法，我们专注于在撰写时表现出高性能的代表性模型。传统上，相对较小的PLM对这些任务进行了微调并取得了高性能，而较大的基础模型通过少量提示获得高分，但通常没有进行基准测试。
- en: We outline the inner logic and main features of the methods, taking into account
    necessary resources, e.g. computational and memory requirements. For standard
    models a link to the description in earlier chapters is provided. Under the heading
    “Available Implementations” you will find links to available code and pre-trained
    models for a task. Good general sources for code are the websites [[30](#CR30),
    [35](#CR35), [74](#CR74), [79](#CR79)].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们概述了方法的内部逻辑和主要特征，考虑到必要的资源，例如计算和内存需求。对于标准模型，提供了早期章节中的描述链接。在“可用实现”标题下，您可以找到针对任务的可用代码和预训练模型的链接。代码的好来源是网站[[30](#CR30)，[35](#CR35)，[74](#CR74)，[79](#CR79)]。
- en: 5.1 Text Classification
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 文本分类
- en: Automatic *text classification* is a common task in natural language processing
    where a *class*, (also called *category* or *label*) is assigned to a short text
    or a document. The set of classes is predefined and may contain just two classes
    (*binary classification*), or more classes (*multiclass classification*). Each
    text must be assigned a single class, which means that the classes are exclusive.
    Typical tasks include spam detection in emails, sentiment analysis , categorization
    of news articles , hate speech detection, dialog act classification, and many
    more. Some examples are listed in Table [5.1](#Tab1). Kowsari et al. [[44](#CR44)],
    Li et al. [[49](#CR49)] and Minaee et al. [[64](#CR64)] provide surveys on text
    classification.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 自动*文本分类*是自然语言处理中的一个常见任务，其中将一个*类别*（也称为*类别*或*标签*）分配给一个简短文本或文档。类别集合是预定义的，可能只包含两个类别（*二元分类*），或更多类别（*多类分类*）。每个文本都必须分配一个类别，这意味着类别是互斥的。典型任务包括电子邮件中的垃圾邮件检测、情感分析、新闻文章分类、仇恨言论检测、对话行为分类等。一些例子列在表[5.1](#Tab1)中。Kowsari等人[[44](#CR44)]、Li等人[[49](#CR49)]和Minaee等人[[64](#CR64)]提供了关于文本分类的综述。
- en: Often a document covers several topics simultaneously, e.g. a news article on
    the construction cost of a soccer stadium. In this case it is necessary to assign
    multiple classes to a document, in our example *“soccer”* and *“finance”*. This
    type of classification is called *multilabel classification*. *Extreme multilabel
    classification* is a variant containing a very large label set with thousands
    of labels.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 常常一个文档同时涵盖多个主题，例如关于足球场建设成本的新闻文章。在这种情况下，需要将多个类别分配给一个文档，在我们的例子中是*“足球”*和*“金融”*。这种类型的分类称为*多标签分类*。*极端多标签分类*是一种包含非常大量标签集（数千个标签）的变体。
- en: There are a number of popular benchmarks to assess the performance of document
    classification approaches covering two or more classes. Typically, the benchmarks
    contain many thousand training and test examples. Table [5.2](#Tab2) describes
    the properties of some popular text classification benchmarks. Often documents
    are categorized according to the subjective opinions of users. An example are
    reviews of movies or restaurants, which can be classified as positive, negative,
    or neutral. Then the classification corresponds to a *sentiment analysis* task.Table
    5.2
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多流行的基准用于评估涵盖两个或更多类别的文档分类方法的表现。通常，基准包含成千上万的训练和测试示例。表[5.2](#Tab2)描述了一些流行文本分类基准的特性。通常，文档是根据用户的主观意见进行分类的。一个例子是电影或餐厅的评论，这些可以分类为正面、负面或中性。然后，分类对应于*情感分析*任务。表5.2
- en: Popular text classification benchmarks
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的文本分类基准
- en: '| Task | Description | Classes |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 描述 | 类别 |'
- en: '| --- | --- | --- |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| *IMDB* [[56](#CR56)] | Reviews from the movie rating page IMDB. 25k training,
    25k test and 50k unlabeled reviews | Two classes: positive and negative |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| *IMDB* [[56](#CR56)] | 来自电影评分页面IMDB的评论。25k个训练样本、25k个测试样本和50k个未标记评论 | 两个类别：正面和负面
    |'
- en: '| *Yelp* [[131](#CR131)] | Yelp reviews of stores and restaurants. 560k training
    and 38k text reviews. | Binary: positive/negative multiclass: five star classes
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| *Yelp* [[131](#CR131)] | Yelp商店和餐厅的评价。560k个训练样本和38k个文本评价 | 二元：正面/负面 多类：五星类别
    |'
- en: '| *DBpedia* [[7](#CR7)] | 14 non-overlapping classes from the DBpedia ontology.
    Each class is represented by 40k training samples and 5k test samples, | 14 different
    classes: company, artist, athlete, animal, album, film, etc. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| *DBpedia* [[7](#CR7)] | 来自DBpedia本体学的14个非重叠类别。每个类别由40k个训练样本和5k个测试样本表示 | 14个不同的类别：公司、艺术家、运动员、动物、专辑、电影等
    |'
- en: '| *ArXiv* [[32](#CR32)] | 33k scientific articles from arXiv with documents
    of average length 6300 and length > 5000 | 11 classes: artificial intelligence,
    computer vision, group theory, etc. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| *ArXiv* [[32](#CR32)] | 来自arXiv的33k篇科学文章，文档平均长度为6300字，长度超过5000字 | 11个类别：人工智能、计算机视觉、群论等
    |'
- en: '| *SemEval-20 Task 12* [[128](#CR128)] | 14k Twitter tweets available for five
    languages: English, Arabic, Danish, Greek, Turkish | Two classes: offensive or
    not offensive. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| *SemEval-20任务12* [[128](#CR128)] | 五种语言（英语、阿拉伯语、丹麦语、希腊语、土耳其语）的14k条Twitter推文
    | 两个类别：攻击性或非攻击性 |'
- en: '| *EURLex-4K* [[53](#CR53)] | Benchmark of law documents containing 45, 000
    training examples with an average length of 727 words and an average of five correct
    classes per example | 4271 non-exclusive classes |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| *EURLex-4K* [[53](#CR53)] | 包含45,000个训练示例的法律文档基准，平均长度为727字，每个示例平均有五个正确类别
    | 4271个非独家类别 |'
- en: '| *Amazon670k dataset* [[60](#CR60)] | Descriptions of amazon products. 490k
    training and 153k test samples. About 5.5 classes per document. | 679k non-exclusive
    categories: products in the Amazon catalog, about 4 samples per category |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| *Amazon670k数据集* [[60](#CR60)] | 亚马逊产品描述。490k个训练样本和153k个测试样本。每份文档大约有5.5个类别
    | 679k个非独家类别：亚马逊目录中的产品，每个类别大约有4个样本 |'
- en: Table 5.3
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.3
- en: Language analysis tasks based on relation extraction [[4](#CR4), p. 10]. Underlining
    indicates phrases annotated by the model
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基于关系抽取的语言分析任务[[4](#CR4)，第10页]。下划线表示模型标注的短语
- en: '| Task | Description | Example |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 描述 | 示例 |'
- en: '| Coreference resolution | Group phrases which refer to the same object. |
    *Betty*[(1)]*loves**her*[(1)]*cute dog*[(2)]. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 共指消解 | 将指代同一对象的短语分组。 | *贝蒂*[(1)]*爱**她的*[(1)]*可爱小狗*[(2)]。 |'
- en: '| Aspect-based sentiment analysis | Extract phrases (aspects) from a text and
    determine sentiments for them (positive, negative, neutral). | *The steak*[*aspect*]*was**horrible*[*negative*].
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 基于方面的情感分析 | 从文本中提取短语（方面），并确定它们的情感（正面、负面、中性）。 | *那块牛排*[*方面*]*太糟糕了*[*负面*]。
    |'
- en: '| Entity relation extraction | Extract relations among entities or concepts
    in a text. | *Peter works as a lawyer.*![$$\rightarrow $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq8.png)*profession(Peter,
    lawyer)* |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 实体关系抽取 | 从文本中提取实体或概念之间的关系。 | *彼得是一名律师*。![$$\rightarrow $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq8.png)*职业(Peter,
    lawyer)* |'
- en: '| Event extraction | Extract events, i.e. n-ary relations among entities or
    nouns in a text. | *At**noon*[*time*]*terrorists*[*attacker*]*detonated a**bomb*[*instrument*]*in**Paris*[*place*].
    ![$$\rightarrow $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq9.png)*conflict-attack*
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 事件抽取 | 提取事件，即文本中实体或名词之间的n元关系。 | *在**中午*[*时间*]*恐怖分子*[*攻击者*]*在**巴黎*[*地点*]引爆了**炸弹*[*工具*]。
    ![$$\rightarrow $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq9.png)*冲突-攻击*
    |'
- en: '| Semantic role labeling | For each verb determine the role of phrases w.r.
    to the verb. | *Mary*[*agent*]*sold*[*verb*]*the book*[*theme*]*to**John*[*recipient*].
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 语义角色标注 | 对于每个动词，确定短语相对于动词的角色。 | *玛丽*[*行为者*]*卖掉了*[*动词*]*那本书*[*主题*]*给**约翰*[*接收者*]。
    |'
- en: Early methods for document classification in the 1990s used classical machine
    learning approaches [[44](#CR44)]. In the first preprocessing step, manually created
    features were extracted from the documents. In the second step, a classifier was
    trained with these features to reconstruct the manually assigned class labels
    (Sect. [1.​3](528393_1_En_1_Chapter.xhtml#Sec3)). Finally, this classifier was
    applied to new documents. Usually, *bag-of-words* representations were used to
    represent the input documents. Popular classification methods included *naive
    Bayes*, *logistic classifier*, the *support vector machine*, and tree-based methods
    like *random forests*. However, all these methods were hampered by the shortcomings
    of the bag-of-words representation (Sect. [1.​3](528393_1_En_1_Chapter.xhtml#Sec3)),
    which ignores the sequence of words in a document.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 20世纪90年代早期用于文档分类的方法采用了经典的机器学习方法 [[44](#CR44)]。在第一个预处理步骤中，从文档中提取了手动创建的特征。在第二个步骤中，使用这些特征训练了一个分类器，以重建手动分配的类别标签（见[1.3节](528393_1_En_1_Chapter.xhtml#Sec3)）。最后，这个分类器被应用于新的文档。通常，使用*词袋模型*来表示输入文档。流行的分类方法包括*朴素贝叶斯*、*逻辑分类器*、*支持向量机*以及基于树的如*随机森林*等方法。然而，所有这些方法都受到了词袋模型表示的局限性（见[1.3节](528393_1_En_1_Chapter.xhtml#Sec3)），它忽略了文档中单词的顺序。
- en: In the next sections, we consider current classification models for mutually
    exclusive as well as “overlapping” classes. It turns out that most of the current
    best approaches are based on PLMs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们考虑了当前用于互斥以及“重叠”类别的分类模型。结果发现，大多数当前的最佳方法都是基于PLMs。
- en: 5.1.1 Multiclass Classification with Exclusive Classes
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 具有互斥类别的多类分类
- en: A prominent application of **BERT** is fine-tuning for a classification task
    (Sect. [2.​1.​2](528393_1_En_2_Chapter.xhtml#Sec4)). Here, a pre-trained BERT
    is adapted to this task by supervised fine-tuning, using the contextual embedding
    of the *“[CLS]”* token in the highest layer as input for a logistic classifier.
    This classifier is extremely successful for natural language understanding tasks
    (Sect. [2.​1.​5](528393_1_En_2_Chapter.xhtml#Sec7)).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT**的一个显著应用是用于分类任务的微调（见[2.1.2节](528393_1_En_2_Chapter.xhtml#Sec4)）。在这里，通过监督微调，将预训练的BERT适应到这个任务，使用最高层中*“[CLS]”*标记的上下文嵌入作为逻辑分类器的输入。这个分类器在自然语言理解任务（见[2.1.5节](528393_1_En_2_Chapter.xhtml#Sec7)）中极为成功。'
- en: '**XLNet** [[120](#CR120)] is trained by reconstructing a permuted token sequence
    (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2)), and is therefore able to
    capture a lot of knowledge about the language. It achieves 96.2% accuracy on the
    binary IMDB classification task. This performance is surpassed by **ERNIE-Doc**
    [[26](#CR26)] with 97.1%. ERNIE-Doc is a transformer with an enhanced recurrence
    mechanism capable of considering many previous segments of a text in the same
    layer. The model aims to mitigate problems of other transformer-based models for
    long contexts such as the Longformer, which do not provide the contextual information
    of whole documents to each segment. The Sota is currently held by a simpler model
    [[101](#CR101)], which modifies the well known paragraph vector [[47](#CR47)]
    and Naive Bayes weighted bag of *n*-grams. It achieves an accuracy of 97.4%.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**XLNet** [[120](#CR120)] 通过重建一个排列后的标记序列（见[3.1.1节](528393_1_En_3_Chapter.xhtml#Sec2)）进行训练，因此能够捕捉到大量的语言知识。它在二进制IMDB分类任务上达到了96.2%的准确率。这个性能被**ERNIE-Doc**
    [[26](#CR26)] 超越，达到了97.1%。ERNIE-Doc是一个具有增强递归机制的transformer，能够在同一层中考虑许多文本的前一段。该模型旨在减轻其他基于transformer的模型在长上下文（如Longformer）中的问题，这些模型没有为每个段提供整个文档的上下文信息。当前Sota由一个更简单的模型
    [[101](#CR101)] 保持，该模型修改了众所周知的段落向量 [[47](#CR47)] 和Naive Bayes加权的*n*-gram词袋。它达到了97.4%的准确率。'
- en: The current best model on the IMDB dataset with 10 classes is **ALBERT-SEN**
    [[23](#CR23)]. The authors propose an approach which evaluates the overall importance
    of sentences to the whole document, with the motivation that different sentences
    can contain different polarities but that the overall polarity depends on a few
    important sentences. Their model uses ALBERT (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2))
    to encode sentences via the *[SEP]* and *[CLS]* token representations. They concatenate
    these representations with class-weighted representations. Then they have a document
    encoder that calculates importance weights for every sentence and creates a weighted
    representation of the sentences as document representation. Finally, they calculate
    a sentiment score by utilizing the document representation and the class representations,
    which were also used in the sentence encoder. The model achieves an accuracy of
    54.8%. It should be noted that subtle nuances in language expressions must be
    taken into account in this classification task with 10 classes.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有10个类别的IMDB数据集上，当前最佳模型是**ALBERT-SEN** [[23](#CR23)]。作者提出了一种评估句子对整个文档整体重要性的方法，其动机是不同的句子可能包含不同的极性，但整体极性取决于少数几个重要句子。他们的模型使用ALBERT（见第[3.1.1](528393_1_En_3_Chapter.xhtml#Sec2)节）通过*[SEP]*和*[CLS]*标记表示来编码句子。他们将它们与类权重表示连接起来。然后，他们有一个文档编码器，为每个句子计算重要性权重，并创建句子的加权表示作为文档表示。最后，他们通过利用文档表示和类表示（这些表示也用于句子编码器）来计算情感分数。该模型达到了54.8%的准确率。需要注意的是，在这个有10个类别的分类任务中，必须考虑到语言表达中的细微差别。
- en: For the Yelp benchmark, **XLNet** performs best for the binary version with
    an accuracy of 98.4% and achieves the second-best accuracy of 73.0% for the fine-granular
    version with 5 classes. The leading model for this task is **HAHNN** [[1](#CR1)]
    with an accuracy of 73.3%. HAHNN combines convolutional layers, gated recurrent
    units and attention mechanisms. It builds on non-contextual FastText [[16](#CR16)]
    embeddings as word representations and uses a stack of convolutional layers to
    obtain contextual information. This is followed by a word encoder which applies
    recurrent GRU cells to obtain word representations, and an attention mechanism
    to create weights for the input words. Sentence representations are then formed
    as an attention-weighted average of the words. Another GRU layer is employed to
    create sentence representations, which are then combined via attention to generate
    a document level representation. This establishes the input to a fully connected
    layer with softmax activation for classification.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Yelp基准测试，**XLNet**在二进制版本中表现最佳，准确率为98.4%，在具有5个类别的细粒度版本中达到了第二好的准确率，为73.0%。该任务的领先模型是**HAHNN**
    [[1](#CR1)]，准确率为73.3%。HAHNN结合了卷积层、门控循环单元和注意力机制。它基于非上下文FastText [[16](#CR16)] 嵌入作为词表示，并使用卷积层堆叠来获取上下文信息。随后是一个词编码器，它应用循环GRU单元来获取词表示，并使用注意力机制为输入词创建权重。然后，通过注意力加权平均形成句子表示。另一个GRU层被用来创建句子表示，然后通过注意力结合生成文档级别的表示。这为具有softmax激活的全连接层提供了输入，用于分类。
- en: '**BigBird** [[127](#CR127)] is especially valuable for classification tasks
    with long documents, as it can process input sequences of length 4096 (Sect. [3.​2.​1](528393_1_En_3_Chapter.xhtml#Sec8)).
    Following BERT, the output embedding of the first *[CLS]* is input for the classifier.
    For the IMDB data with shorter documents there is no performance gain compared
    to simpler models. On the *ArXiv benchmark* [[32](#CR32)] with documents of average
    length 6300 and 11 classes BigBird improves Sota by about 5% points.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**BigBird** [[127](#CR127)] 对于处理长文档的分类任务特别有价值，因为它可以处理长度为4096的输入序列（见第[3.2.1](528393_1_En_3_Chapter.xhtml#Sec8)节）。在BERT之后，第一个*[CLS]*的输出嵌入被输入到分类器中。对于较短的IMDB数据，与简单模型相比没有性能提升。在平均长度为6300和11个类别的*ArXiv基准测试*
    [[32](#CR32)] 中，BigBird将Sota提升了大约5个百分点。'
- en: With the advent of Web 2.0 and the ability for users to create and share their
    own content with the world, the proliferation of harmful content such as hate
    speech, has increased. This is now fueled by bots and machine learning models
    that automatically create such content at a scale that humans can barely manage.
    *Hate speech* is often defined as a hostile or disparaging communication by a
    person or group referring to characteristics such as race, color, national origin,
    gender, disability, religion, or sexual orientation [[36](#CR36)]. According to
    European law, hate speech is a punishable criminal offense.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Web 2.0 的出现以及用户能够与世界分享他们自己的内容的能力，有害内容如仇恨言论的泛滥也增加了。现在，这被机器人和学习模型所推动，它们可以自动以人类几乎无法管理的规模创建此类内容。*仇恨言论*通常被定义为个人或团体针对种族、肤色、国籍、性别、残疾、宗教或性取向等特征进行的敌对或贬低性交流
    [[36](#CR36)]。根据欧洲法律，仇恨言论是一种应受惩罚的刑事犯罪。
- en: Hate speech detection can be solved as a text classification task. Recognizing
    such a text is difficult because the line between hate speech, irony, free speech,
    and art is blurred. Jahan et al. [[36](#CR36)] and Yin et al. [[123](#CR123)]
    give a systematic review on automatic hate speech detection. Because of the importance
    of the task, let’s take a closer look at current approaches.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 仇恨言论检测可以作为文本分类任务来解决。识别此类文本很困难，因为仇恨言论、讽刺、言论自由和艺术的界限模糊。Jahan 等人 [[36](#CR36)]
    和 Yin 等人 [[123](#CR123)] 对自动仇恨言论检测进行了系统综述。鉴于这项任务的重要性，让我们更详细地看看当前的方法。
- en: Roy et al. [[88](#CR88)] follow a multilingual approach. They preprocess the
    text from Twitter by using a special tokenization of tweets. The cleaned text,
    emojis and segmented hashtags are encoded by different transformers and concatenated.
    A final multilayer perceptron generates the classification. The results for the
    *HASOC 2019 tweet dataset* [[58](#CR58)] show that the additional signal from
    the emojis and the hashtags yield a performance boost for hate speech detection
    as well as for classifying the type of hate speech. They achieve F1-values of
    90.3%, 81.9% and 75.5% on the English, German, and Hindi test sets.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Roy 等人 [[88](#CR88)] 采用了一种多语言方法。他们通过使用特殊的推文标记化来预处理 Twitter 中的文本。清洗后的文本、表情符号和分段的标签被不同的转换器编码并连接起来。一个最终的多层感知器生成分类。*HASOC
    2019 推文数据集* [[58](#CR58)] 的结果表明，来自表情符号和标签的额外信号提高了仇恨言论检测以及仇恨言论类型分类的性能。他们在英语、德语和印地语测试集上实现了
    90.3%、81.9% 和 75.5% 的 F1 值。
- en: Mathew et al. [[59](#CR59)] argue that the decisions of hate speech classifiers
    should be explained. They present the *HateXplain* dataset with about 20k posts.
    The annotation contains class labels (hateful, offensive, or normal), the target
    group being vilified, and span annotations of words causing the classification.
    Overall a BERT model yields the best results in explaining the hate speech classification
    decisions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Mathew 等人 [[59](#CR59)] 认为，仇恨言论分类器的决策应该得到解释。他们提出了包含约 20k 篇帖子的 *HateXplain* 数据集。注释包含类别标签（仇恨、攻击性或正常）、被诽谤的目标群体以及导致分类的单词的跨度注释。总的来说，BERT
    模型在解释仇恨言论分类决策方面取得了最佳结果。
- en: A recent competition was the SemEval-20 Task 12 [[128](#CR128)], where 14,100
    Twitter tweets were manually labeled as either offensive or not offensive. Using
    a **RoBERTa** classifier (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2)) Wiedemann
    et al. [[110](#CR110)] achieved 92.0% F1-value and won the competition. In a later
    experiment an ensemble of *ALBERT* models (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2))
    increased this score to 92.6%. In summary, the automatic classification of hate
    speech can be solved by PLMs with high quality.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的一项比赛是 SemEval-20 任务 12 [[128](#CR128)]，其中 14,100 条 Twitter 推文被手动标记为攻击性或非攻击性。Wiedemann
    等人 [[110](#CR110)] 使用 **RoBERTa** 分类器（第 [3.1.1](528393_1_En_3_Chapter.xhtml#Sec2)
    节）实现了 92.0% 的 F1 值并赢得了比赛。在随后的实验中，*ALBERT* 模型（第 [3.1.1](528393_1_En_3_Chapter.xhtml#Sec2)
    节）将这一分数提高到 92.6%。总的来说，仇恨言论的自动分类可以通过高质量的 PLM 解决。
- en: 5.1.2 Multilabel Classification
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 多标签分类
- en: Multilabel classification is required whenever a text can belong to multiple
    classes simultaneously. When a very large number of classes is available, this
    is sometimes called *extreme multilabel classification*. An example problem is
    the assignment of tags to Wikipedia articles, where Wikipedia has almost 500k
    tags. In multilabel classification usually a score or probability for each class
    is returned. This can be used to rank the classes. Traditional metrics such as
    accuracy, which assume that only one class is correct, cannot be applied. An alternative
    is to measure the quality of ranking induced by the score (c.f. Sect. [6.​1.​2](528393_1_En_6_Chapter.xhtml#Sec3)).
    A popular measure for a predicted score vector ![$$\hat {y}_i\in [0,1]$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq10.png)
    and a ground truth label vector *y*[*i*] ∈{0, 1} is the *precision at**k*, which
    counts, how many correct classes are among the *k* classes with the highest score:![$$\displaystyle
    \begin{aligned} prec@k = \frac 1k \sum_{l\in \text{rank}_k(\hat{y})} y_l \qquad  DCG@k
    = \frac 1k \sum_{l\in rank_k(\hat{y})} \frac{y_l}{\log(l+1)} , \end{aligned} $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_Equ1.png)(5.1)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个文本可以同时属于多个类别时，就需要进行多标签分类。当可用的类别数量非常多时，这有时被称为 *极端多标签分类*。一个例子问题是将标签分配给维基百科文章，维基百科有近
    500k 个标签。在多标签分类中通常返回每个类别的得分或概率。这可以用来对类别进行排序。假设只有一个类别是正确的传统度量，如准确率，不能应用。一种替代方法是测量由得分诱导的排序质量（参见图
    [6.1.2](528393_1_En_6_Chapter.xhtml#Sec3)）。对于预测得分向量 ![$$\hat {y}_i\in [0,1]$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq10.png)
    和真实标签向量 *y*[*i*] ∈ {0, 1} 的一个流行度量是 *k* 的精度，它计算在得分最高的 *k* 个类别中有多少个是正确的类别：![$$\displaystyle
    \begin{aligned} prec@k = \frac 1k \sum_{l\in \text{rank}_k(\hat{y})} y_l \qquad  DCG@k
    = \frac 1k \sum_{l\in rank_k(\hat{y})} \frac{y_l}{\log(l+1)} , \end{aligned} $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_Equ1.png)(5.1)
- en: where ![$$\text{rank}(\hat {y})=(i_1,\ldots ,i_k)$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq11.png)
    is the vector of the indices of the *k* largest values of ![$$\hat {y}_i$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq12.png)
    sorted in descending order ![$$\hat {y}_{i_1}\ge \cdots \ge \hat {y}_{i_k}$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq13.png)
    . The second measure *DCG@k* is the *discounted cumulative gain*, where the correct
    assignments *y*[*l*] are weighted by their rank *l* transformed with ![$$1/\log
    (l+1)$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq14.png)
    [[14](#CR14)]. This reflects that correct assignments with a lower rank should
    get a lower weight. In addition, there is a normalized version *nDCG@k*, where
    *DCG@k* is divided by its maximal possible value.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![$$\text{rank}(\hat {y})=(i_1,\ldots ,i_k)$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq11.png)
    是按降序排列的 ![$$\hat {y}_i$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq12.png)
    的 *k* 个最大值的索引向量 ![$$\hat {y}_{i_1}\ge \cdots \ge \hat {y}_{i_k}$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq13.png)
    。第二个度量 *DCG@k* 是 *折算累积收益*，其中正确的分配 *y*[*l*] 通过 ![$$1/\log (l+1)$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq14.png)
    进行加权，并转换为排名 *l* [[14](#CR14)]。这反映了排名较低的分配应该获得较低的权重。此外，还有一个归一化版本 *nDCG@k*，其中 *DCG@k*
    被其最大可能值除以。
- en: Separate classifiers for each class often yield a very good accuracy, but suffer
    from very bad training and prediction time. In the worst case these classifiers
    have to be trained per label with all positive instances of a label and all instances
    of the other labels as negative samples. To mitigate this effect **Parabel** [[83](#CR83)]
    is based on a tree ensemble. First Parabel creates label representations by averaging
    all the instances that belong to a label and normalizing this averaged vector
    to 1\. Then balanced 2-means clustering is applied on the label space recursively
    until all leaf nodes in the clustered label tree contain fewer than *M* labels,
    e.g. *M* = 100\. For each internal node of the tree and for the leaf nodes, classifiers
    are trained to decide which path of the tree an instance follows. Thus, a balanced
    label hierarchy is generated efficiently based on a label representation such
    that labels with similar inputs end up together at the leaves. Up to 3 such trees
    are used as an ensemble.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个类别单独训练分类器通常会产生非常高的准确率，但训练和预测时间非常糟糕。在最坏的情况下，这些分类器必须针对每个标签使用所有正实例和所有其他标签的实例作为负样本进行训练。为了减轻这种影响，**Parabel**
    [[83](#CR83)] 基于树集成。首先，Parabel通过平均属于一个标签的所有实例并归一化这个平均向量到1来创建标签表示。然后，递归地应用平衡的2-means聚类到标签空间，直到聚类标签树中的所有叶子节点包含少于*M*个标签，例如*M*
    = 100。对于树的每个内部节点和叶子节点，训练分类器以决定实例遵循的树路径。因此，基于标签表示有效地生成平衡的标签层次结构，使得具有相似输入的标签最终在叶子节点上聚集在一起。最多使用3棵这样的树作为集成。
- en: Finally, for each label, 1-vs-All classifiers are trained as a MAP estimate
    of the joint probability distribution over labels. The negative examples used
    for training these classifiers are drawn from the other labels in the same leaf,
    so the most similar or confusing counterexamples are employed. For prediction
    a beam search is performed in the tree and only for the *k* most probable labels
    a classification is actually performed. Parabel has been applied to problems with
    7M labels and can make predictions in logarithmic time. Parabel is significantly
    faster at training and prediction than state-of-the-art extreme classifiers while
    having almost the same precision. On the EURLex-4K it achieves a prec@1 value
    of 81.5 and on the Amazon-670k a prec@1 value of 43.9, which is worse than the
    45.4 of the best approach, but its time for prediction is only 1/1000.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于每个标签，都训练了1-vs-All分类器，作为标签联合概率分布的MAP估计。用于训练这些分类器的负样本来自同一叶子中的其他标签，因此使用了最相似或最易混淆的反例。在预测时，在树中执行束搜索，并且只为最可能的*k*个标签执行分类。Parabel已应用于具有7M个标签的问题，并且可以在对数时间内进行预测。Parabel在训练和预测方面比最先进的极端分类器快得多，同时几乎具有相同的精度。在EURLex-4K上，它实现了81.5的prec@1值，在Amazon-670k上实现了43.9的prec@1值，这比最佳方法的45.4要差，但它的预测时间仅为1/1000。
- en: '**AttentionXML** [[124](#CR124)] is a tree-based classifier, which uses contextual
    embeddings as input features. With an attention between the many labels and the
    tokens, AttentionXML represents a given text differently for each label. The architecture
    of AttentionXML consists of a word representation layer, a bidirectional LSTM
    layer, an attention layer with attention from all labels to the BiLSTM (Sect.
    [1.​6](528393_1_En_1_Chapter.xhtml#Sec6)) encoded input and lastly a fully connected
    layer and an output layer.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**AttentionXML** [[124](#CR124)] 是一种基于树的分类器，它使用上下文嵌入作为输入特征。通过在许多标签和标记之间建立注意力，AttentionXML为每个标签表示不同的给定文本。AttentionXML的架构包括一个词表示层、一个双向LSTM层、一个注意力层（从所有标签到编码输入的BiLSTM，见[1.6](528393_1_En_1_Chapter.xhtml#Sec6)）、最后是一个全连接层和一个输出层。'
- en: AttentionXML first builds a deep tree similar to Parabel. Then the tree is compressed
    to a shallow and wide tree, which allows to handle millions of categories, especially
    for “tail labels”, i.e. classes with only a few examples in the training set [[37](#CR37)].
    The model uses the binary cross-entropy loss function. For each level of the tree
    this model is trained, being initialized with the model of the prior tree level.
    AttentionXML trains label ranking with negative labels sampled by fine-tuned label
    recalling models. For prediction the tree is used for a beam search, so only tree
    branches where the parent nodes have highest scores are considered.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: AttentionXML 首先构建一个类似于 Parabel 的深度树。然后，将树压缩成一个浅层且宽的树，这允许处理数百万个类别，特别是对于“尾部标签”，即训练集中只有少数示例的类别
    [[37](#CR37)]。该模型使用二元交叉熵损失函数。对于每个训练的树级别，该模型使用先前树级别的模型进行初始化。AttentionXML 使用通过微调的标签召回模型采样的负标签来训练标签排名。对于预测，使用树进行束搜索，因此只考虑父节点得分最高的树分支。
- en: On the *EURLex-4K benchmark* AttentionXML achieves *prec@*1 = 87.1% and *prec@*5 = 61.9%.
    This means that the highest scoring prediction of the model is correct for 87.1%
    of the test predictions and 61.9% of the five highest scoring predictions are
    correct. Note that the choice of *k* should be made according to the average number
    of labels per document in the training set. On the *Amazon670k dataset* [[60](#CR60)]
    with 679k categories AttentionXML achieves *prec@*1 = 47.6% and *prec@*5 = 38.9%.
    This means that about 40% of the alternative products are correctly identified.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *EURLex-4K 基准测试* 中，AttentionXML 实现了 *prec@*1 = 87.1% 和 *prec@*5 = 61.9%。这意味着模型得分最高的预测在测试预测中正确率为
    87.1%，而在五个最高得分预测中，正确率达到了 61.9%。请注意，*k* 的选择应根据训练集中每个文档的平均标签数量来确定。在包含 679k 个类别的
    *Amazon670k 数据集* [[60](#CR60)] 上，AttentionXML 实现了 *prec@*1 = 47.6% 和 *prec@*5
    = 38.9%。这意味着大约 40% 的替代产品被正确识别。
- en: '**LightXML** [[39](#CR39)] employs a transformer encoder to generate contextual
    word features and generates negative examples for each category in a dynamic way.
    First, a set of label clusters is created based on the input features so that
    each label belongs to one cluster. Then a pre-trained model like RoBERTa (Sect.
    [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2)) is employed to encode the input text
    of an instance into contextual embeddings. To represent the input text of a training
    example, the embeddings of the *[CLS]* token in the last five layers are concatenated.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**LightXML** [[39](#CR39)] 使用 Transformer 编码器生成上下文词特征，并为每个类别以动态方式生成负例。首先，基于输入特征创建一组标签簇，以便每个标签属于一个簇。然后，使用类似
    RoBERTa (Sect. [3.1.1](528393_1_En_3_Chapter.xhtml#Sec2)) 的预训练模型将实例的输入文本编码为上下文嵌入。为了表示训练示例的输入文本，将最后五层中
    *[CLS]* 标记的嵌入进行拼接。'
- en: A specific *label recalling* model aims to predict the label clusters using
    the *[CLS]* embeddings as input. In addition, the *label ranking model* receives
    the *[CLS]* embeddings of a training instance as well as the corresponding label
    . Negative examples with other labels are dynamically generated with the label
    recalling model. The loss terms of both the generator and the discriminator are
    combined in a joint loss function allowing end-to-end training. On the EURLex-4K
    benchmark LightXML achieves a *prec@*1 = 87.6% and *prec@*5 = 63.4%. On the Amazon670k
    benchmark it reaches a *prec@*1 = 49.1% and *prec@*5 = 39.6%. Both values are
    slightly better than those of AttentionXML. The approach also demonstrates Sota
    performance compared to 7 alternative model on three other multilabel datasets.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一种特定的 *标签召回* 模型旨在使用 *[CLS]* 嵌入作为输入来预测标签簇。此外，*标签排名模型* 接收训练实例的 *[CLS]* 嵌入以及相应的标签。使用标签召回模型动态生成具有其他标签的负例。生成器和判别器的损失项在联合损失函数中结合，允许端到端训练。在
    EURLex-4K 基准测试中，LightXML 实现了 *prec@*1 = 87.6% 和 *prec@*5 = 63.4%。在 Amazon670k
    基准测试中，它达到了 *prec@*1 = 49.1% 和 *prec@*5 = 39.6%。这两个值略好于 AttentionXML 的值。该方法与 7
    个其他多标签数据集上的 7 个替代模型相比也展示了 Sota 性能。
- en: '**Overlap** [[51](#CR51)] groups labels into overlapping clusters. In product
    categorization, for example, the tag *“belt”* can be related to a vehicle belt
    (in the *“vehicle accessories”* category), or a man’s belt (under *“clothing”*
    category). Each label can now occur at most *λ*-times, where *λ* is a hyperparameter
    of the approach. The authors initialize their partitioning with a balanced *k*-means
    clustering and then proceed with an optimization method to reassign labels in
    a way that maximizes the precision rate. On the Amazon670k benchmark the model
    reaches Sota values of *prec@*1 = 50.7% and *prec@*5 = 41.6%. There are also alternative
    models with a tree-based search, which are able to increase recall rates and reduce
    effort [[22](#CR22)].'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**重叠** [[51](#CR51)] 将标签分组到重叠的簇中。例如，在产品分类中，标签“皮带”可以与车辆皮带（在“车辆配件”类别下）或男士皮带（在“服装”类别下）相关联。每个标签现在最多可以出现
    *λ* 次，其中 *λ* 是该方法的超参数。作者使用平衡的 *k* 均值聚类初始化他们的分区，然后使用优化方法重新分配标签，以最大化精确率。在Amazon670k基准测试中，该模型达到了
    *prec@*1 = 50.7% 和 *prec@*5 = 41.6% 的Sota值。还有基于树搜索的替代模型，它们能够提高召回率并减少工作量 [[22](#CR22)]。'
- en: There is a great similarity of extreme multilabel classification with text retrieval,
    which is covered in Sect. [6.​1](528393_1_En_6_Chapter.xhtml#Sec1). This group
    of text applications has seen a large progress in recent years. For dense retrieval
    the query and the document representations are encoded by a BERT model, and the
    documents with largest cosine similarity are returned. Probably many approaches
    from this field may be used for text classification.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 极端多标签分类与文本检索有很强的相似性，这在第[6.1](528393_1_En_6_Chapter.xhtml#Sec1)节中有详细说明。这一组文本应用在近年来取得了很大的进展。对于密集检索，查询和文档表示由BERT模型编码，并返回余弦相似度最大的文档。可能许多来自这一领域的方案都可以用于文本分类。
- en: 5.1.3 Few- and Zero-Shot Classification
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.3 少样本和零样本分类
- en: Large autoregressive language models like GPT-2, GPT-3, Gopher and PaLM have
    acquired an enormous amount of information about facts and language by pre-training.
    They can be instructed to classify a text by a few examples [[76](#CR76)], as
    described in Sect. [3.​6.​3](528393_1_En_3_Chapter.xhtml#Sec41). Figure [5.1](#Fig1)
    provides an example prompt for the classification of a text by sentiment [[91](#CR91)].
    This means that no additional fine-tuning dataset is required, but only a prompt
    with a few examples. In the same way the pre-trained Gopher model [[85](#CR85)]
    was applied to a comprehensive set of about 150 benchmark tasks, which require
    the generation of answers using few-shot instructions. Similar to other autoregressive
    models it may predict class labels for documents (Sect. [2.​2.​5](528393_1_En_2_Chapter.xhtml#Sec17)).
    As the results show [[85](#CR85), p. 56], Gopher is often able to outperform conventional
    PLMs fine-tuned on the domain. Therefore, classification by instruction seems
    to be a viable alternative, if a large autoregressive PLM such as GPT-3, Gopher
    or GPT-Neo is available.![](../images/528393_1_En_5_Chapter/528393_1_En_5_Fig1_HTML.png)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于GPT-2、GPT-3、Gopher和PaLM这样的大型自回归语言模型，通过预训练已经获得了大量关于事实和语言的信息。它们可以通过几个示例来指导对文本进行分类
    [[76](#CR76)]，如第[3.6.3](528393_1_En_3_Chapter.xhtml#Sec41)节所述。图[5.1](#Fig1)提供了一个通过情感对文本进行分类的示例提示
    [[91](#CR91)]。这意味着不需要额外的微调数据集，只需要一个包含几个示例的提示。同样，预训练的Gopher模型 [[85](#CR85)]被应用于大约150个基准任务的综合集合，这些任务需要使用少量指令生成答案。与其他自回归模型类似，它可能为文档预测类别标签（第[2.2.5](528393_1_En_2_Chapter.xhtml#Sec17)节）。正如结果所示
    [[85](#CR85), p. 56]，Gopher通常能够优于在特定领域微调的传统PLM。因此，如果有一个大型自回归PLM，如GPT-3、Gopher或GPT-Neo，指令分类似乎是一个可行的替代方案！![图片](../images/528393_1_En_5_Chapter/528393_1_En_5_Fig1_HTML.png)
- en: A screenshot contains two sections titled prompt and G P T Neo. It has text
    inputs for sentiment analysis of respective tweets with G P T Neo as positive.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 截图包含两个部分，标题分别为“提示”和“G P T Neo”。它包含用于对相应推文进行情感分析的文本输入，其中G P T Neo表示积极。
- en: Fig. 5.1
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1
- en: A query for few-shot learning for sentiment analysis with GPT-Neo, a free version
    of GPT with 2.7B parameters. The query can be evaluated on the API [[91](#CR91)]
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对GPT-Neo进行少量指令学习的情感分析查询，GPT-Neo是具有27亿参数的GPT免费版本。该查询可以在API [[91](#CR91)] 上进行评估。
- en: 'Recently, the *RAFT* [[3](#CR3)] benchmark was released. RAFT is specifically
    designed for evaluating few-shot performance in text classification tasks. It
    covers 11 real-world datasets, 8 of which are binary classification, two contain
    three classes, and one contains 77 classes. Each task comes with natural language
    instructions and 50 labeled training examples. An example benchmark is *“Label
    the sentence based on whether it is related to an adverse drug effect. Sentence:
    No regional side effects were noted. Label: not related. …”*. A prompt contained
    less than 50 examples. The performance is measured by an average F1 over all 11
    tasks. On these RAFT benchmarks BART yields an F1 average of 38.2%, GPT-Neo (2.7B)
    achieves 48.1%, AdaBoost decision trees 51.4%, and GPT-3 (175B) scores 62.7%.
    Humans achieve an average F1 of 73.5%.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，*RAFT* [[3](#CR3)] 基准测试被发布。RAFT专门设计用于评估文本分类任务中的少样本性能。它涵盖了11个真实世界数据集，其中8个是二分类，两个包含三个类别，一个包含77个类别。每个任务都附带自然语言指令和50个标记的训练示例。一个基准示例是*“根据句子是否与不良药物反应相关进行标注。句子：未发现区域副作用。标注：不相关。…”*。提示包含少于50个示例。性能通过11个任务的平均F1值来衡量。在这些RAFT基准测试中，BART的平均F1值为38.2%，GPT-Neo（2.7B）达到48.1%，AdaBoost决策树51.4%，GPT-3（175B）得分为62.7%。人类平均F1值为73.5%。
- en: '**PET** [[90](#CR90)] asks users to specify one or more patterns that convert
    an input example *x* into a *cloze prompt* (Sect. [2.​1.​2](528393_1_En_2_Chapter.xhtml#Sec4))
    so that it can be processed by a masked language model like BERT. In addition,
    users must describe the meaning of all output classes. This is done with a “verbalizer”
    that assigns a natural language expression to each output *y*. Multiple verbalizers
    may be specified for the same data. An example is *“I really enjoyed this movie.
    It was [MASK].”* and *“I really enjoyed this movie. Question: Is this a positive
    movie review? Answer: [MASK].”* for the text *“I really enjoyed this movie”*.
    The PLM is then trained to maximize *p*(*y*|*x*) for observed pairs. PET achieves
    a new state of the art on RAFT with an average F1 of 82.2% and performs close
    to nonexpert humans for 7 out of 11 tasks.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**PET** [[90](#CR90)] 要求用户指定一个或多个将输入示例 *x* 转换为 *cloze提示*（第[2.1.2](528393_1_En_2_Chapter.xhtml#Sec4)节）的模式，以便它可以由BERT等掩码语言模型处理。此外，用户必须描述所有输出类的含义。这是通过一个“verbalizer”来完成的，它将自然语言表达式分配给每个输出
    *y*。可以为相同的数据指定多个verbalizer。一个例子是*“我真的很喜欢这部电影。它很棒。”*和*“我真的很喜欢这部电影。问题：这是否是一部积极的影评？答案：…”*，针对文本*“我真的很喜欢这部电影”*。然后PLM被训练以最大化观察到的对
    *p*(*y*|*x*)。PET在RAFT上实现了平均F1值为82.2%的新纪录，并且在11个任务中有7个任务的表现接近非专家人类。'
- en: Foundation Models can also be used to generate new data for a text classification
    task. If, for example, input for a restaurant classification task is required,
    the model can be prompted to generate a new restaurant review for a specific label
    Sect. [3.​6.​6](528393_1_En_3_Chapter.xhtml#Sec46). In this way training data
    for fine-tuning a model can be created.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型也可以用于为文本分类任务生成新数据。例如，如果需要餐厅分类任务的输入，模型可以被提示为生成特定标签的新餐厅评论（第[3.6.6](528393_1_En_3_Chapter.xhtml#Sec46)节）。这样就可以创建用于微调模型的训练数据。
- en: Available Implementations
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: The code and trained parameters of many classical models like BigBird, XLNET,
    T5 are available at Hugging Face [https://​huggingface.​co/​transformers/​](https://huggingface.co/transformers/).
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多经典模型如BigBird、XLNET、T5的代码和训练参数可在Hugging Face[https://huggingface.co/transformers/](https://huggingface.co/transformers/)找到。
- en: The LightXML model code is here [https://​github.​com/​kongds/​LightXML](https://github.com/kongds/LightXML).
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LightXML模型代码在此处[https://github.com/kongds/LightXML](https://github.com/kongds/LightXML)。
- en: The code of PET can be found here [https://​github.​com/​timoschick/​pet](https://github.com/timoschick/pet).
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PET的代码可在此处找到[https://github.com/timoschick/pet](https://github.com/timoschick/pet)。
- en: 5.1.4 Summary
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.4 摘要
- en: For document classification, a PLM that has been pre-trained with a large set
    of documents is usually fine-tuned to solve a specific classification task. Typically,
    the embedding of a particular token such as *[CLS]* is used as input to a logistic
    classifier. This setup has outperformed all previous bag-of-word classifiers such
    as the SVM. Specialized PLM variants like XLNET or ALBERT show a higher performance
    because of their more effective pre-training. For longer documents, suitable models
    like BigBird yield good results. Identifying hate speech can be considered as
    a classification task, where good results are achieved with standard models such
    as BERT and RoBERTa.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文档分类，通常使用在大规模文档集上预训练的 PLM 来解决特定的分类任务。通常，特定标记（如*[CLS]*）的嵌入被用作逻辑分类器的输入。这种设置优于所有之前的词袋模型，如
    SVM。像 XLNET 或 ALBERT 这样的专用 PLM 变体由于更有效的预训练而表现出更高的性能。对于较长的文档，像 BigBird 这样的合适模型能取得良好的结果。识别仇恨言论可以被视为一个分类任务，其中使用标准模型如
    BERT 和 RoBERTa 可以取得良好的结果。
- en: The situation is different for multi-label classification, where several categories
    can be correct for one document. Here, tree-like classifiers in combination with
    contextual embeddings show good results. By the tree a small number of candidate
    classes can be selected reducing the training and execution times. Extreme multi-label
    classifications, such as matching product descriptions to related product descriptions,
    are close to a document retrieval tasks and can benefit from techniques developed
    in this area, e.g. dense retrieval by DPR.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多标签分类的情况则不同，一个文档可能有多个正确的类别。在这种情况下，结合上下文嵌入的树形分类器显示出良好的结果。通过树形结构，可以选出少量候选类别，从而减少训练和执行时间。极端的多标签分类，如将产品描述与相关产品描述匹配，接近文档检索任务，并可以从该领域开发的技术中受益，例如
    DPR 的密集检索。
- en: Large pre-trained autoregressive language models like GPT-3, Gopher and PaLM
    may be instructed by few-shot learning to solve classification tasks. Recent approaches
    achieve a performance close to humans. Not long ago an API has been released which
    allows to pre-train GPT-3 and adapt it to specific data and specific classification
    tasks (Sect. [3.​6.​2](528393_1_En_3_Chapter.xhtml#Sec40)). A simpler alternative
    is InstructGPT, which can be easily directed to perform a classification, e.g.
    a sentiment analysis (Sect. [3.​6.​5](528393_1_En_3_Chapter.xhtml#Sec43)). However,
    a formal evaluation of the performance of this approach is not yet available,
    as the model would have to process the training data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 大型预训练的自回归语言模型，如 GPT-3、Gopher 和 PaLM，可以通过少样本学习来指导解决分类任务。最近的方法达到了接近人类的性能。不久前，一个
    API 已被发布，允许预训练 GPT-3 并将其适应特定的数据和特定的分类任务（见[3.6.2](528393_1_En_3_Chapter.xhtml#Sec40)节）。一个更简单的替代方案是
    InstructGPT，它可以很容易地被指导执行分类，例如情感分析（见[3.6.5](528393_1_En_3_Chapter.xhtml#Sec43)节）。然而，由于模型需要处理训练数据，因此这种方法性能的正式评估尚不可用。
- en: While PLMs have achieved promising results on demanding benchmarks, most of
    these models are not interpretable. For example, why does a model arrive at a
    particular classification? Why does a model outperform another model on one dataset,
    but performs worse on other datasets? Although the mechanisms of attention and
    self-attention provide some insight into the associations that lead to a particular
    outcome, detailed investigation of the underlying behavior and dynamics of these
    models is still lacking (Sect. [2.​4.​5](528393_1_En_2_Chapter.xhtml#Sec38)).
    A thorough understanding of the theoretical aspects of these models would lead
    to a better acceptance of the results.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 PLM 在一些要求较高的基准测试中取得了令人鼓舞的结果，但大多数这些模型都是不可解释的。例如，为什么一个模型会得出特定的分类结果？为什么一个模型在一个数据集上优于另一个模型，但在其他数据集上表现较差？尽管注意力和自注意力机制的机制为导致特定结果的相关性提供了一些洞察，但这些模型底层行为和动态的详细研究仍然不足（见[2.4.5](528393_1_En_2_Chapter.xhtml#Sec38)节）。对这些模型理论方面的深入了解将有助于更好地接受这些结果。
- en: 5.2 Word Sense Disambiguation
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 词义消歧
- en: In nearly all languages the same word may express different concepts. An example
    is the word *“set”*, which may be a verb, an adjective, or a noun and can be interpreted
    as ‘a group of things’, a ‘scenery’, a mathematical concept, a sports term, etc.
    The WordNet [[62](#CR62)] lexical database lists 45 different senses for this
    word. *Word sense disambiguation* (*WSD*) aims to distinguish these different
    meanings and annotate each word with its sense. It can be treated as a classification
    task, where each word is assigned to a sense of a sense inventory such as WordNet.
    The contextual embeddings generated by PLMs offer a way to identify these meanings.
    Bevilacqua et al. [[13](#CR13)] provide a recent survey of WSD approaches.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎在所有语言中，同一个词可能表达不同的概念。例如，单词*“set”*可能是一个动词、形容词或名词，可以解释为“一组事物”、“风景”、“数学概念”、“体育术语”等。WordNet
    [[62](#CR62)] 词汇数据库列出了该词的45种不同词义。*词义消歧* (*WSD*) 的目的是区分这些不同的含义并为每个词标注其词义。它可以被视为一个分类任务，其中每个词都被分配到词义库存中的一个词义，例如WordNet。PLMs生成的上下文嵌入提供了一种识别这些含义的方法。Bevilacqua等人
    [[13](#CR13)] 提供了WSD方法的最新综述。
- en: WSD can be used for a number of purposes. A traditional application is search,
    where the different senses of the same word are distinguished in the query. *Lexical
    substitution* [[13](#CR13)] aims to replace a word or phrase in a text with another
    with nearly identical meaning.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: WSD可以用作多种目的。一个传统应用是搜索，在查询中区分同一个词的不同词义。*词汇替换* [[13](#CR13)] 的目的是用一个几乎相同意义的词或短语替换文本中的一个词或短语。
- en: 5.2.1 Sense Inventories
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 词义库存
- en: WSD obviously depends on the definition of senses, which have to be assigned
    to the words. The main sense inventory for WSD in English is *WordNet* [[62](#CR62)].
    It consist of expert-made *synsets*, which are sets of synonymous words that represent
    a unique concept. A word can belong to multiple synsets denoting its different
    meanings. Version 3.0 of WordNet covers 147,306 words (or phrases) and 117,659
    synsets. WordNet is also available for languages other than English through the
    *Open Multilingual WordNet* project [[17](#CR17)]. *Wikipedia* is another sense
    inventory often used for *Entity Linking* (Sect. [5.3.3](#Sec16)), where a person,
    a concept or an entity represented by a Wikipedia page has to be linked to a given
    *mention* of the entity in a text. *BabelNet* [[71](#CR71)] is a mixture of WordNet,
    Wikipedia and several other lexical resources, such as Wiktionary [[111](#CR111)]
    and OmegaWiki [[75](#CR75)]. It is highly multilingual covering more than 500
    languages.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: WSD显然依赖于词义的界定，这些词义必须分配给单词。英语中WSD的主要词义库存是 *WordNet* [[62](#CR62)]。它由专家制作的 *synsets*
    组成，这些是表示独特概念的同义词集合。一个词可以属于多个synsets，表示其不同的含义。WordNet 3.0版本涵盖了147,306个单词（或短语）和117,659个synsets。WordNet还通过
    *Open Multilingual WordNet* 项目 [[17](#CR17)] 可用于英语以外的语言。*维基百科* 是另一个常用于 *实体链接*
    (Sect. [5.3.3](#Sec16)) 的词义库存，其中代表维基百科页面的个人、概念或实体必须与文本中给定 *提及* 的实体链接。*BabelNet*
    [[71](#CR71)] 是WordNet、维基百科和几个其他词汇资源的混合体，如 Wiktionary [[111](#CR111)] 和 OmegaWiki
    [[75](#CR75)]。它是高度多语言的，覆盖了500多种语言。
- en: WordNet’s sense inventory is often too fine-grained. For example, the noun *“star”*
    has eight meanings in WordNet. The two meanings referring to a *“celestial body”*
    distinguish only whether the star is visible from earth or not. Both meanings
    are translated in Spanish as *“estrella”*, so this sense distinction is useless
    for this translation. It has been shown that for many tasks more coarse-grained
    sense inventories are better [[81](#CR81)].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: WordNet的词义库存通常过于细致。例如，名词 *“star”* 在WordNet中有八个含义。指代 *“天体”* 的两个含义仅区分星星是否可见于地球。这两种含义在西班牙语中都被翻译为
    *“estrella”*，因此这种词义区分对这个翻译来说是无用的。已经证明，对于许多任务来说，更粗粒度的词义库存更好 [[81](#CR81)]。
- en: The best WSD algorithms use PLMs pre-trained on large document corpora. Through
    fine-tuning, they are trained to assign senses from the available sense inventory.
    In some cases, nearest neighbor operations are employed to measure the distance
    between embeddings and determine the most appropriate sense.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的WSD算法使用在大规模文档语料库上预训练的PLMs。通过微调，它们被训练来从可用的词义库存中分配词义。在某些情况下，使用最近邻操作来测量嵌入之间的距离并确定最合适的词义。
- en: 5.2.2 Models
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 模型
- en: '**GlossBERT** [[33](#CR33)] employs a pre-trained BERT encoder. Its fine-tuning
    input is both the context sentence (where the word is used in the specific sense)
    and the *gloss* (a sentence defining the meaning of the word). GlossBERT is trained
    to predict whether the gloss correctly describes the use of the target word. The
    *SemCor3.0* [[61](#CR61)] benchmark is annotated with WordNet senses. GlossBERT
    achieves a new Sota of 77.0% F1 on this data.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**GlossBERT** [[33](#CR33)] 采用预训练的 BERT 编码器。其微调输入包括上下文句子（其中单词在特定意义上使用）和 *gloss*（定义单词意义的句子）。GlossBERT
    被训练来预测 gloss 是否正确描述了目标单词的使用。*SemCor3.0* [[61](#CR61)] 基准数据标注了 WordNet 意义。GlossBERT
    在这些数据上实现了 77.0% 的 F1 新 Sota。'
- en: '**EWISER** [[12](#CR12)] expresses WSD as a simple *Word annotation* task (Sect.
    [2.​1.​3](528393_1_En_2_Chapter.xhtml#Sec5)), where a sense label is assigned
    to each word. It starts with an average of BERT embeddings for each word *v*[*t*]
    from different contexts and transforms them with a linear layer and the *Swish*
    [[86](#CR86)] activation function ![$$f(x)=x\cdot  \operatorname {\mathrm {sigmoid}}(\beta
    x)$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq15.png). For
    each combination of a word and a part-of-speech a set *S*(*v*[*t*]) of possible
    word senses and hypernyms is determined similar to [[78](#CR78)]. Then the approach
    computes probabilities that a word belongs to a synset in *S*(*v*[*t*]). By this
    approach the prediction takes into account which WordNet senses are possible for
    a word. It achieves a new Sota of 80.1% on a combination of WSD benchmarks. This
    value is also an estimated upper bound on human inter-annotator agreement [[69](#CR69)],
    showing that WSD is on par with humans. The paper lists the results for a number
    of alternative approaches. The **BEM** model [[15](#CR15)] is a similar system
    yielding comparable accuracy. A detailed analysis of how PLMs (especially BERT)
    capture lexical ambiguity can be found in [[52](#CR52)]. The authors show that
    the embedding space of BERT covers enough detail to distinguish word senses.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**EWISER** [[12](#CR12)] 将 WSD 表达为一个简单的 *Word annotation* 任务（第 [2.1.3](528393_1_En_2_Chapter.xhtml#Sec5)
    节），其中每个单词都分配一个意义标签。它从不同上下文中每个单词 *v*[*t*] 的平均 BERT 嵌入开始，并通过一个线性层和 *Swish* [[86](#CR86)]
    激活函数 ![$$f(x)=x\cdot  \operatorname {\mathrm {sigmoid}}(\beta x)$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq15.png)
    进行转换。对于每个单词和词性的组合，确定一个可能的单词意义和上义词集合 *S*(*v*[*t*])，类似于 [[78](#CR78)]。然后该方法计算一个单词属于
    *S*(*v*[*t*]) 中的某个同义词集的概率。通过这种方法，预测考虑了单词可能具有的 WordNet 意义。它在 WSD 基准数据的组合上实现了 80.1%
    的新 Sota。这个值也是人类标注者之间一致性的估计上限 [[69](#CR69)]，表明 WSD 与人类相当。论文列出了许多替代方法的成果。**BEM**
    模型 [[15](#CR15)] 是一个类似系统，具有可比的准确性。关于 PLMs（尤其是 BERT）如何捕捉词汇歧义的分析可以在 [[52](#CR52)]
    中找到。作者表明，BERT 的嵌入空间包含了足够的信息来区分词义。'
- en: '**MuLaN** [[9](#CR9)] is based on a multilingual list ![$$\mathcal {D}$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq16.png)
    of *synsets* in different languages. For example, ![$${\mathcal {D}}$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq17.png)
    may contain the synset corresponding to the *“fountain”* meaning of *“spring”*,
    which is expressed in different languages as *“Quelle*[*DE*]*”*, *“spring*[*EN*]*”*,
    *“fountain*[*EN*]*”*, *“manantial*[*ES*]*”*, *“brollador*[*CAT*]*”*, *“source*[*FR*]*”*,
    *“fonte*[*IT*]*”*, and *“sorgente*[*IT*]*”*. The semantic repositories *WordNet*
    [[62](#CR62)] and *BabelNet* [[71](#CR71)] are employed to create ![$${\mathcal
    {D}}$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq18.png).
    MuLaN has the task to annotate an unlabeled corpus *U* in the target language
    with senses using a corpus *L*[lab] in the source language (e.g. English) as input,
    which is annotated with senses from ![$${\mathcal {D}}$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq19.png)
    . This is done in the following steps:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**MuLaN** [[9](#CR9)] 基于一个多语言 *synsets* 列表 ![$$\mathcal {D}$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq16.png)，包含不同语言中的同义词集。例如，![$${\mathcal
    {D}}$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq17.png) 可能包含对应于
    *“spring”* 的 *“fountain”* 意义的同义词集，该意义用不同的语言表达为 *“Quelle*[*DE*]*”*、*“spring*[*EN*]*”*、*“fountain*[*EN*]*”*、*“manantial*[*ES*]*”*、*“brollador*[*CAT*]*”*、*“source*[*FR*]*”*、*“fonte*[*IT*]*”*
    和 *“sorgente*[*IT*]*”*。语义库 *WordNet* [[62](#CR62)] 和 *BabelNet* [[71](#CR71)]
    被用来创建 ![$${\mathcal {D}}$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq18.png)。MuLaN
    的任务是使用源语言（例如英语）的标注语料库 *L*[lab] 作为输入，对目标语言的未标注语料库 *U* 进行意义标注，该语料库标注了来自 ![$${\mathcal
    {D}}$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq19.png) 的意义。这是通过以下步骤完成的：'
- en: '*Creating embeddings*: The multilingual mBERT (Sect. [3.​3.​1](528393_1_En_3_Chapter.xhtml#Sec13))
    trained on 104 languages is used to compute the embedding *emb*(*σ*, *w*) of every
    word *w* in context *σ* in *L*[lab]. If *w* is split into multiple tokens, their
    average is used. If *w* is a compound, first the tokens of each word within the
    compound are averaged and then the average over words is taken as representation
    for *w*.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*创建嵌入*：使用在 104 种语言上训练的多语言 mBERT (Sect. [3.​3.​1](528393_1_En_3_Chapter.xhtml#Sec13))
    计算在 *L*[lab] 的上下文 *σ* 中每个单词 *w* 的嵌入 *emb*(*σ*, *w*)。如果 *w* 被分割成多个标记，则使用它们的平均值。如果
    *w* 是复合词，则首先计算复合词中每个单词的标记的平均值，然后取单词的平均值作为 *w* 的表示。'
- en: '*Candidate production*: Then for each word *w* with embedding *emb*(*σ*, *w*)
    in context *σ* from *L*[lab] the nearest 1000 neighbors from the unlabeled corpus
    *U* are determined by *FAISS* [[40](#CR40)]. As an example we select the text
    span *v* = *“correre”* from the context *τ* = *“Mi hanno consigliato di andare
    a correre.”* in *L*[lab] as the closest candidate *emb*(*τ*, *v*) for the instance
    *w* = *“running”* from the sentence *σ* = *“I’ve seen her go running in the park.”*.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*候选生成*：然后，对于来自 *L*[lab] 的上下文 *σ* 中具有嵌入 *emb*(*σ*, *w*) 的每个单词 *w*，使用 *FAISS*
    [[40](#CR40)] 确定未标记语料库 *U* 中最近的 1000 个邻居。例如，我们从 *L*[lab] 中的上下文 *τ* = *“Mi hanno
    consigliato di andare a correre.”* 中选择文本跨度 *v* = *“correre”* 作为实例 *w* = *“running”*
    来自句子 *σ* = *“I’ve seen her go running in the park.”* 的最接近候选 *emb*(*τ*, *v*)。'
- en: '*Synset compatibility*: Subsequently, it is checked if the closest candidate
    word *v* is contained in a synset of *w* in ![$${\mathcal {D}}$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq20.png).
    Otherwise it is discarded.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*同义词集兼容性*：随后，检查最近的候选词 *v* 是否包含在 *w* 的同义词集中。如果不是，则将其丢弃。'
- en: '*Backward compatibility*: Finally, the nearest neighbors of *emb*(*τ*, *v*)
    in context *τ* in *L*[lab] are determined. (*τ*, *v*) is only retained, if its
    nearest neighbor list contains *w*.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*向后兼容性*：最后，确定在 *L*[lab] 的上下文 *τ* 中 *emb*(*τ*, *v*) 的最近邻居。(*τ*, *v*) 只有在其最近邻居列表包含
    *w* 时才被保留。'
- en: '*Dataset generation*: After a number of additional filtering steps the final
    annotation of words in the target corpus *U* is performed.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据集生成*：经过一系列额外的过滤步骤后，对目标语料库 *U* 中的单词进行最终标注。'
- en: As a labeled corpus *L*[lab] a union of *SemCor* [[63](#CR63)] and the *WordNet
    Glos Corpus* (*WNG*) [[46](#CR46)] is used, which are annotated with senses. As
    unlabeled corpus *U* the Wikipedia is used for Italian, French, Spanish and German.
    When tested on *SemEval-13* [[70](#CR70)] and *SemEval-15* [[66](#CR66)], MuLaN
    is the best system to annotate words with senses in the four languages with F1-values
    above 80%. An important advantage of MuLaN is that it is able to transfer sense
    annotations from high-resource to low-resource languages.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 作为带标签的语料库 *L*[lab]，使用 *SemCor* [[63](#CR63)] 和 *WordNet Glos Corpus* (*WNG*)
    [[46](#CR46)] 的并集，这些语料库都标注了词义。作为未标记语料库 *U*，使用维基百科进行意大利语、法语、西班牙语和德语的标注。在 *SemEval-13*
    [[70](#CR70)] 和 *SemEval-15* [[66](#CR66)] 上进行测试时，MuLaN 是标注四语言词义的最好系统，其 F1 值均超过
    80%。MuLaN 的重要优势在于，它能够将高资源语言的词义标注转移到低资源语言。
- en: '**Escher** [[8](#CR8)] reformulates WSD as a span prediction problem. The input
    to the model is a sentence with a target word and all its possible sense definitions.
    The output is a text span identifying the gloss expressing the target words most
    suitable meaning. As an example consider Fig. [5.2](#Fig2) with the input sentence
    *“<s> The bully had to <t> back down </t>. </s>”* where the target word is enclosed
    in *“<t>”* and *“</t>”*. Subsequently, two glosses are appended.![](../images/528393_1_En_5_Chapter/528393_1_En_5_Fig2_HTML.png)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**埃舍尔** [[8](#CR8)] 将 WSD 重新表述为跨度预测问题。模型的输入是一个包含目标词及其所有可能词义的句子。输出是一个文本跨度，标识表达目标词最合适意义的释义。例如，考虑图
    [5.2](#Fig2) 中的输入句子 *“<s> The bully had to <t> back down </t>. </s>'
- en: A flow diagram of various target words, transformer, 2 logistic regression models
    for start and end of span, and span blocks.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 各种目标单词、转换器、用于跨度开始和结束的 2 个逻辑回归模型以及跨度块的流程图。
- en: Fig. 5.2
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2
- en: Escher [[8](#CR8)] takes as input a sentence, where the target word *“back down”*
    is enclosed by *“<t>”* and *“</t>”*. The most probable sense of the target word
    is indicated by the sentence selected by span prediction. A high probability of
    a span start is indicated by *“[”* and a high probability of the span end is indicated
    by *“]”*
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Escher [[8](#CR8)] 将一个句子作为输入，其中目标词 *“back down”* 被括号 *“<t>”* 和 *“</t>”* 包围。通过跨度预测选择的句子指示了目标词最可能的意义。跨度开始的高概率由
    *“[”* 表示，跨度结束的高概率由 *“]”* 表示。
- en: The span is predicted similar to Sect. [2.​1.​3](528393_1_En_2_Chapter.xhtml#Sec5)
    by separately computing the probability for the first and last token of the span
    covering the correct gloss. In the example the sentence *“Move backwards from
    a certain position.”* is selected as span, which describes the correct sense.
    By lowering the prior probability of the most frequent sense for a word the approach
    is able to reduce the most frequent sense bias. Escher uses BART[LARGE] (Sect.
    [3.​1.​3](528393_1_En_3_Chapter.xhtml#Sec4)) as PLM architecture, as it is effective
    for reading comprehension. The output of its last decoder layer is used to represent
    the input tokens and to compute the start and end token distributions. On a number
    of SemEval datasets [[66](#CR66)] Escher has higher F1-scores compared to its
    competitors and this difference is statistically highly significant. Best results
    are achieved for nouns and adjectives with F1-values > 83%, while for verbs the
    F1-value is only 69.3%.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与第 [2.1.3](528393_1_En_2_Chapter.xhtml#Sec5) 节类似，通过分别计算覆盖正确 gloss 的跨度中第一个和最后一个标记的概率来预测跨度。在示例中，句子
    *“从某个位置后退。”* 被选为跨度，它描述了正确的意义。通过降低一个词最频繁意义的前验概率，该方法能够减少最频繁意义偏差。Escher 使用 BART[LARGE]（第
    [3.1.3](528393_1_En_3_Chapter.xhtml#Sec4) 节）作为 PLM 架构，因为它对阅读理解有效。其最后一个解码层的输出用于表示输入标记，并计算开始和结束标记分布。在多个
    SemEval 数据集 [[66](#CR66)] 上，与竞争对手相比，Escher 的 F1 分数更高，这种差异在统计学上非常显著。对于名词和形容词，实现了最佳结果，F1
    值超过 83%，而对于动词，F1 值仅为 69.3%。
- en: '**ConSec** [[10](#CR10)] determines the sense of a token by considering not
    only the context words, but also the senses assigned to the neighboring words.
    It is based on an extension of DeBERTa, a BERT variant with superior performance
    (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2)). ConSec uses WordNet example
    sentences with annotated meanings (glosses) as additional training data. The approach
    yields a Sota of 83.2% F1 when applied to the *SemCor3.0* benchmark [[61](#CR61)].'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**ConSec** [[10](#CR10)] 通过考虑不仅上下文单词，还包括邻近单词分配的意义来确定一个标记的意义。它基于 DeBERTa 的扩展，DeBERTa
    是一个性能优越的 BERT 变体（第 [3.1.1](528393_1_En_3_Chapter.xhtml#Sec2) 节）。ConSec 使用带有标注意义（glosses）的
    WordNet 示例句子作为额外的训练数据。当应用于 *SemCor3.0* 基准测试 [[61](#CR61)] 时，该方法实现了 83.2% 的 F1
    值。'
- en: Available Implementations
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: The codes of GlossBERT and EWISER and trained models are available for a number
    of different languages [https://​github.​com/​HSLCY/​GlossBERT](https://github.com/HSLCY/GlossBERT)[https://​github.​com/​SapienzaNLP/​ewiser](https://github.com/SapienzaNLP/ewiser).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GlossBERT 和 EWISER 的代码以及训练好的模型可用于多种不同的语言 [https://github.com/HSLCY/GlossBERT](https://github.com/HSLCY/GlossBERT)[https://github.com/SapienzaNLP/ewiser](https://github.com/SapienzaNLP/ewiser)。
- en: Escher along with the necessary training data is available at [https://​github.​com/​SapienzaNLP/​esc](https://github.com/SapienzaNLP/esc).
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Escher 以及必要的训练数据可在 [https://github.com/SapienzaNLP/esc](https://github.com/SapienzaNLP/esc)
    获取。
- en: 5.2.3 Summary
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 摘要
- en: WSD can be handled as a classification task, where each word is assigned to
    a number of possible meaning classes. Often WordNet is used as the sense inventory.
    GlossBERT compares the contextual embedding of a word with the embedding of a
    word in an example sentence (gloss) of WordNet. EWISER and MULAN directly work
    on the synsets of WordNet and capture the sets of possible senses and hypernyms.
    They are able to annotate senses in four languages with an F1-value above 80%.
    Escher reformulates WSD as a span prediction problem increasing F1 to 83%. ConSec
    takes into account the senses of nearby tokens and achieves a similar performance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: WSD 可以被处理为一个分类任务，其中每个词被分配到多个可能的意义类别。通常使用 WordNet 作为意义库存。GlossBERT 比较一个词的上下文嵌入与
    WordNet 中示例句子（gloss）中该词的嵌入。EWISER 和 MULAN 直接在 WordNet 的同义词集中工作，并捕获可能的意义和上位的集合。它们能够用
    F1 值超过 80% 的准确率对四种语言的意义进行标注。Escher 将 WSD 重新表述为一个跨度预测问题，将 F1 值提高到 83%。ConSec 考虑到附近标记的意义，实现了相似的性能。
- en: As WSD models get better, there is a need for more demanding benchmark datasets,
    which possibly may be generated by adversarial techniques. Moreover, there is
    a trend to WSD models which are more robust to domain shift and can cope with
    text from social media documents. To advance WSD it is necessary to extend sense-annotated
    data, especially for rare senses. In addition, multilingual WSD systems may be
    constructed which require large-scale multilingual WSD benchmarks. There are tendencies
    in WSD to do away with the fixed sense inventory and to distinguish the senses
    in other ways, e.g., in a lexical substitution task or by generating the definition
    of a word in a particular context.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 随着WSD模型变得更好，需要更多要求的基准数据集，这些数据集可能可以通过对抗技术生成。此外，存在一种趋势，即WSD模型对领域变化的鲁棒性更强，可以处理来自社交媒体文档的文本。为了推进WSD，有必要扩展标注的语义数据，特别是对于罕见语义。此外，可以构建需要大规模多语言WSD基准的多语言WSD系统。在WSD中存在一种趋势，即放弃固定的语义库存，并以其他方式区分语义，例如，在词汇替换任务中或通过在特定上下文中生成一个词的定义。
- en: An opportunity is the integration of WSD with entity linking (Sect. [5.3.3](#Sec16)),
    where the model is required to associate mentions with entries in a knowledge
    base such as Wikipedia. As WSD systems work fairly well now, it would be possible
    to combine them with other applications like question answering or dialog systems.
    It has to be tested, whether an explicit inclusion of WSD is able to generate
    better results. For retrieval tasks, WSD has been superseded by embedding-based
    methods (Sect. [6.​1](528393_1_En_6_Chapter.xhtml#Sec1)), which provide a better
    hit rate.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 机会是将词义消歧（WSD）与实体链接（第[5.3.3](#Sec16)节）集成，其中模型需要将提及与知识库（如维基百科）中的条目关联起来。由于WSD系统现在工作得相当好，因此可以将它们与其他应用程序（如问答或对话系统）结合使用。必须测试是否显式包含WSD能够产生更好的结果。对于检索任务，基于嵌入的方法（第[6.1](528393_1_En_6_Chapter.xhtml#Sec1)节）已经取代了WSD，这些方法提供了更好的命中率。
- en: 5.3 Named Entity Recognition
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 命名实体识别
- en: '*Named entity recognition* (*NER*) refers to the task of tagging *mentions*
    of *named entities*, such as persons, organizations and locations in texts. Labeled
    datasets for NER exist across many domains, e.g. news, science and medicine [[72](#CR72)].
    Typically these datasets are annotated in the *IOB2 format*, which, for instance
    annotates the first token of a person with B-per and all other tokens of that
    entity with I-per. The O-tag is used for all tokens outside of entity mentions.
    An example is *“U.N.*[*B-org*]*official*[*O*]*Peter*[*B-per*]*Ekeus*[*I-per*]*heads*[*O*]*for*[*O*]*Bagdad*[*B-loc*].*”*
    NER involves the prediction of these tags for each token, i.e. the suffixes in
    the prior example. Therefore, it can be considered as a classification task, where
    a tag is assigned to each token. A standard dataset for NER is the CoNLL-2003
    dataset [[89](#CR89)], which contains English resp. German news texts with annotations
    for persons, organizations, locations, and miscellaneous names. Surveys on NER
    are provided by Li et al. [[48](#CR48)], Nasar et al. [[68](#CR68)] and Bose et
    al. [[18](#CR18)].'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*命名实体识别* (*NER*) 指的是标记文本中*提及*的*命名实体*的任务，例如人名、组织机构和地点。NER的标注数据集存在于许多领域，例如新闻、科学和医学
    [[72](#CR72)]。通常这些数据集以 *IOB2格式* 进行标注，例如，将人的第一个标记标注为B-per，并将该实体的所有其他标记标注为I-per。O标签用于实体提及之外的所有标记。例如，“*U.N.*[*B-org*]*official*[*O*]*Peter*[*B-per*]*Ekeus*[*I-per*]*heads*[*O*]*for*[*O*]*Bagdad*[*B-loc*].*”
    NER涉及预测每个标记的这些标签，即先前的例子中的后缀。因此，它可以被认为是一个分类任务，其中每个标记都分配一个标签。NER的标准数据集是CoNLL-2003数据集
    [[89](#CR89)]，它包含带有对人物、组织机构、地点和杂项名称的标注的英语或德语新闻文本。NER的调查由Li等人 [[48](#CR48)]、Nasar等人
    [[68](#CR68)] 和Bose等人 [[18](#CR18)] 提供。'
- en: NER is particularly useful in areas with a highly specialized vocabulary. Examples
    include the fields of healthcare or electromobility, where many thousands of publications
    are released each year. Since few experts understand the terminology, NER systems
    are particularly valuable for identifying publications on specialized topics.
    Of course, the NER types must be adapted to each area.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）在具有高度专业词汇的领域中特别有用。例如，医疗保健或电动汽车领域，每年都会发布成千上万篇出版物。由于很少有专家理解这些术语，因此NER系统在识别专门主题的出版物方面特别有价值。当然，NER类型必须适应每个领域。
- en: In the following section, we present approaches to ordinary NER where each word
    can have a single entity type. Named entities can also be nested, e.g. *“[[UK]*[*gpe*]*Embassy
    in [France]*[*gpe*]*]*[*facility*]*”*. This case is discussed in the second section.
    Even more challenging is the mapping of a named-entity phrase to the underlying
    unique entity in a knowledge base or ontology, e.g., a person. This is called
    entity linking and is discussed in the third section.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们介绍了普通NER的方法，其中每个单词可以有一个单一的实体类型。命名实体也可以嵌套，例如*“[[英国]*[*gpe*]*驻[法国]*[*gpe*]*[*设施*]*”*。这种情况将在第二节中讨论。更具挑战性的是将命名实体短语映射到知识库或本体中基础唯一的实体，例如，一个人。这被称为实体链接，并在第三节中讨论。
- en: 5.3.1 Flat Named Entity Recognition
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 平坦命名实体识别
- en: In *flat named entity recognition* each token corresponds to at most one named
    entity. **BERT** can be fine-tuned to NER by predicting tags for each token using
    a logistic classifier (Fig. [2.​5](528393_1_En_2_Chapter.xhtml#Fig5)) as a final
    layer. For this setup BERT[LARGE] yielded 92.8% F1-value on the CoNLL-2003 test
    data. While the F1-values for persons and locations were higher (≈ 95%), the F1-value
    for miscellaneous names (78%) was much lower, as these entities form a vaguely
    defined class.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在*平坦命名实体识别*中，每个标记最多对应一个命名实体。**BERT**可以通过使用逻辑分类器（如图[2.5](528393_1_En_2_Chapter.xhtml#Fig5)）预测每个标记的标签来微调到NER，作为最后一层。对于这种设置，BERT[LARGE]在CoNLL-2003测试数据上达到了92.8%的F1值。虽然人名和地点的F1值更高（≈95%），但杂项名称的F1值（78%）要低得多，因为这些实体构成一个定义模糊的类别。
- en: '**LUKE** [[117](#CR117)] treats words and entities in a given text as independent
    objects, and outputs contextual embeddings of tokens and entities. The model is
    based on RoBERTa and trained to predict randomly masked words and entities in
    a large entity-annotated corpus derived from Wikipedia. In this way, it obtains
    a lot of information on the relation between entities in the text. It contains
    an entity-aware self-attention mechanism that is an extension of BERT’s self-attention
    mechanism and takes into account embeddings, which indicate if a token represents
    text or an entity. It yields an F1-value of 94.3-F1 for CoNLL-2003, which is near-Sota.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**LUKE** [[117](#CR117)] 将给定文本中的单词和实体视为独立对象，并输出标记和实体的上下文嵌入。该模型基于RoBERTa，并在从维基百科中提取的大量实体标注语料库上训练，以预测随机遮蔽的单词和实体。通过这种方式，它获得了关于文本中实体之间关系的大量信息。它包含一个实体感知的自注意力机制，这是BERT自注意力机制的扩展，并考虑了指示一个标记是否代表文本或实体的嵌入。它在CoNLL-2003上产生了94.3-F1的F1值，接近Sota。'
- en: '**ACE** [[106](#CR106)] builds on the assumption that weighted sums ∑[*i* ∈
    *I*]*A*[*i*] ∗ *emb*(*v*[*i*]) of different embeddings *emb*(*v*[*i*]) of tokens
    *v*[*i*] yield better results than single embeddings. A controller samples a subset
    *I* from a set of eight embeddings (e.g. BERT[BASE], GloVe, fastText, etc.) and
    a NER model is trained and returns an accuracy score. The accuracy is treated
    as a reward signal in a reinforcement setting using the policy gradient algorithm
    ([[112](#CR112)]) to select an optimal subset *I*. As NER model a BiLSTM model
    (Sect. [1.​6](528393_1_En_1_Chapter.xhtml#Sec6)) with a final CRF-layer was chosen.
    A CRF (Conditional Random Field) [[100](#CR100)] is able to model the probabilistic
    relation between the tags in detail. The fine-tuned model reaches a Sota F1-score
    of 94.6% for CoNLL-2003.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**ACE** [[106](#CR106)] 建立在加权求和 ∑[*i* ∈ *I*]*A*[*i*] ∗ *emb*(*v*[*i*]) 的假设之上，不同的标记
    *v*[*i*] 的嵌入 *emb*(*v*[*i*]) 的加权求和比单个嵌入产生更好的结果。控制器从一个包含八个嵌入（例如BERT[BASE]、GloVe、fastText等）的集合中采样一个子集
    *I*，并训练一个NER模型，返回一个准确度分数。在强化设置中，使用策略梯度算法([[112](#CR112)])将准确度作为奖励信号来选择最优子集 *I*。作为NER模型，选择了一个带有最终CRF层的BiLSTM模型（第[1.6](528393_1_En_1_Chapter.xhtml#Sec6)节）。CRF（条件随机场）[[100](#CR100)]
    能够详细地模拟标签之间的概率关系。微调后的模型在CoNLL-2003上达到了94.6%的Sota F1分数。'
- en: '**KeBioLM** [[126](#CR126)] is a biomedical pre-trained language model aiming
    to improve NER by including additional knowledge. The authors extract 660M entities
    from the *PubMed corpus* [[73](#CR73)] with abstracts of biomedical literature
    and link them to the UMLS knowledge base that contains more than 4M entities and
    their synonyms as well as relations. They train a variant of BERT on the PubMed
    data and explicitly generate embeddings for entities. Relation information is
    included by the TransE-mechanism (Sect. [3.​4.​1](528393_1_En_3_Chapter.xhtml#Sec18)).
    The joint loss function is a mixture of loss functions for masked language modeling,
    entity detection, and entity linking. The *JNLPBA benchmark* contains 2000 PubMed
    abstracts with molecular biology-related entities. KeBioLM reaches a Sota of 82.0%
    F1 on JNLPBA. This shows that pre-training on domain texts and the inclusion of
    additional knowledge can improve NER results.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**KeBioLM** [[126](#CR126)]是一个生物医学预训练语言模型，旨在通过包含额外的知识来提高NER。作者从包含生物医学文献摘要的*PubMed语料库*[[73](#CR73)]中提取了6600万个实体，并将它们链接到包含超过400万个实体及其同义词以及关系的UMLS知识库。他们在PubMed数据上训练了一个BERT变体，并明确地为实体生成嵌入。通过TransE机制（第[3.4.1](528393_1_En_3_Chapter.xhtml#Sec18)节）包含关系信息。联合损失函数是掩码语言建模、实体检测和实体链接损失函数的混合。*JNLPBA基准*包含2000篇与分子生物学相关的PubMed摘要。KeBioLM在JNLPBA上达到了82.0%的F1
    Sota。这表明在领域文本上进行预训练和包含额外知识可以提高NER结果。'
- en: '*Retrieval* is a way to enhance the context a PLM may use for NER. Wang et
    al. [[107](#CR107)] query a search engine with the input text that should be tagged.
    They rank (Sect. [3.​4.​5](528393_1_En_3_Chapter.xhtml#Sec22)) the returned results
    by the similarity of RoBERTa embeddings and concatenate the top ranked results
    and the input text. This is fed into a variant of RoBERTa to generate token embeddings.
    As the model can exploit the attention to the retrieved texts, the generated embeddings
    are potentially more expressive. The results on CoNLL 2003 indicate that retrieval
    can increase the F1-value about 0.5% and could be combined with current Sota-models.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*检索*是增强PLM可能用于NER的上下文的一种方法。Wang等人[[107](#CR107)]使用应被标记的输入文本查询搜索引擎。他们根据RoBERTa嵌入的相似性对返回的结果进行排序（第[3.4.5](528393_1_En_3_Chapter.xhtml#Sec22)节），并将排名靠前的结果和输入文本连接起来。这些结果被输入到一个RoBERTa变体中，以生成标记嵌入。由于模型可以利用对检索文本的注意力，生成的嵌入可能更具表达性。在CoNLL
    2003上的结果表明，检索可以将F1值提高约0.5%，并且可以与当前Sota模型结合使用。'
- en: 5.3.2 Nested Named Entity Recognition
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 嵌套命名实体识别
- en: Often named entities have an internal structure. An example for such *nested
    entities* is the sentence *“Last night, the [[Chinese]*[*gpe*]*embassy in [France]*[*gpe*]*]*[*facility*]*was
    closed.”* In this case a single token may have several entity tags and the NER
    task has to be formulated differently.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的命名实体具有内部结构。这类*嵌套实体*的例子是句子*“昨晚，中国[*gpe*]在[*gpe*]法国[*facility*]的大使馆[*gpe*]关闭了。”*在这种情况下，一个单独的标记可能包含多个实体标签，NER任务必须以不同的方式制定。
- en: '**MRC** [[50](#CR50)] treats nested NER as a question-answering task. For example,
    the extraction of entities with a “location” label is formalized as the question:
    *“Which locations are mentioned in the text?”* The questions are formulated using
    templates that reflect the annotation guidelines. When these questions are answered
    for each entity type, overlapping named entities can be detected. MRC uses BERT’s
    span prediction approach (Sect. [2.​1.​3](528393_1_En_2_Chapter.xhtml#Sec5)) to
    mark the beginning and end of spans in the token sequence for an entity type.
    In addition, MRC predicts the start and the end of each entity to allow that there
    are overlapping entities of the same type.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**MRC** [[50](#CR50)]将嵌套NER视为问答任务。例如，提取带有“位置”标签的实体被形式化为问题：“文本中提到了哪些位置？”这些问题使用反映注释指南的模板来制定。当为每种实体类型回答这些问题时，可以检测到重叠的命名实体。MRC使用BERT的跨度预测方法（第[2.1.3](528393_1_En_2_Chapter.xhtml#Sec5)节）在标记序列中标记实体类型的开始和结束跨度。此外，MRC预测每个实体的开始和结束，以允许存在相同类型的重叠实体。'
- en: Nested entities are common in the medical domain. The *Genia Corpus* [[43](#CR43)]
    contains entity annotations for proteins, viruses, DNA, RNA and many more, with
    17% of the entities being nested. MRC achieves a Sota of 83.8% F1 on the Genia
    benchmark. The ACE-2005 benchmark [[104](#CR104)] contain diverse nested entities
    like persons, facilities, or vehicles with an overlap of 22%. MRC reached an F1-value
    of 86.9% for ACE-2005\. A similar approach [[125](#CR125)] also predicts spans
    of different entities and yields 85.4% for ACE-2005\. A two-stage algorithm called
    Locate and Label is proposed by Shen et al. [[93](#CR93)], who first extract candidate
    entities and then categorize them in a second step. They yield 86.7% for the nested
    NER on ACE-2005 using BERT or one of its variants.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学领域，嵌套实体很常见。*Genia语料库* [[43](#CR43)] 包含蛋白质、病毒、DNA、RNA等实体的标注，其中17%的实体是嵌套的。MRC在Genia基准测试上达到了83.8%的F1分数。ACE-2005基准测试
    [[104](#CR104)] 包含了22%重叠的多种嵌套实体，如人物、设施或车辆。MRC在ACE-2005上达到了86.9%的F1分数。一种类似的方法 [[125](#CR125)]
    也预测了不同实体的跨度，并在ACE-2005上获得了85.4%的分数。Shen等人[[93](#CR93)]提出了一种名为“定位和标注”的两阶段算法，他们首先提取候选实体，然后在第二步中对其进行分类。他们使用BERT或其变体在ACE-2005的嵌套NER上获得了86.7%的分数。
- en: Instead of using a BERT model pre-trained on general documents, **PubMedBERT**
    [[102](#CR102)] pre-trains its BERT model with 100M parameters exclusively on
    21 GB medical texts from PubMed. PubMedBERT achieves 86.3% F1 for NER on the *BLURB
    benchmark* [[31](#CR31)]. The model also yields Sota scores for other task like
    classification and relation extraction summarized in an average score of 82.9%.
    This result strongly supports pre-training on domain-specific data. **BioELECTRA**
    [[42](#CR42)] is a biomedical domain-specific language encoder model that adapts
    ELECTRA (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2)) for the Biomedical
    domain. ELECTRA employs a sample-efficient ‘replaced token detection’ technique
    for pre-training, which causes the model to include an enormous amount of information
    from the training data. BioELECTRA is pre-trained on PubMed and PubMed Central
    full-text medical articles. For NER, it arrives at the best score with 86.7% F1-value
    on the BLURB benchmark [[31](#CR31)]. The model also yields a similar score of
    82.6% as PubMedBERT for the other BLURB tasks.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用在通用文档上预训练的BERT模型不同，**PubMedBERT** [[102](#CR102)] 使用PubMed中21GB医学文本的100M参数专门预训练其BERT模型。PubMedBERT在*BLURB基准测试*
    [[31](#CR31)] 上实现了86.3%的F1分数。该模型在分类和关系抽取等其他任务上也取得了Sota分数，平均得分为82.9%。这一结果强烈支持在特定领域数据上预训练。**BioELECTRA**
    [[42](#CR42)] 是一个针对生物医学领域的特定领域语言编码器模型，它将ELECTRA（第[3.1.1](528393_1_En_3_Chapter.xhtml#Sec2)节）应用于生物医学领域。ELECTRA采用高效的“替换标记检测”技术进行预训练，这使得模型能够包含来自训练数据的大量信息。BioELECTRA在PubMed和PubMed
    Central全文医学文章上进行了预训练。对于NER，它在BLURB基准测试上达到了86.7%的F1分数。该模型在其他BLURB任务上也获得了与PubMedBERT相似的82.6%的分数。
- en: Available Implementations
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: BERT[LARGE] for token classification [https://​huggingface.​co/​transformers/​model_​doc/​model_​doc/​bert.​html](https://huggingface.co/transformers/model_doc/model_doc/bert.html),
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT[LARGE]用于标记分类[https://huggingface.co/transformers/model_doc/model_doc/bert.html](https://huggingface.co/transformers/model_doc/model_doc/bert.html),
- en: Luke [https://​huggingface.​co/​transformers/​model_​doc/​model_​doc/​luke.​html](https://huggingface.co/transformers/model_doc/model_doc/luke.html)
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luke [https://huggingface.co/transformers/model_doc/model_doc/luke.html](https://huggingface.co/transformers/model_doc/model_doc/luke.html)
- en: ACE [https://​github.​com/​Alibaba-NLP/​ACE](https://github.com/Alibaba-NLP/ACE),
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ACE [https://github.com/Alibaba-NLP/ACE](https://github.com/Alibaba-NLP/ACE),
- en: MRC [https://​github.​com/​ShannonAI/​mrc-for-flat-nested-ner](https://github.com/ShannonAI/mrc-for-flat-nested-ner)
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MRC [https://github.com/ShannonAI/mrc-for-flat-nested-ner](https://github.com/ShannonAI/mrc-for-flat-nested-ner)
- en: Locate and Label [[93](#CR93)] [https://​github.​com/​tricktreat/​locate-and-label](https://github.com/tricktreat/locate-and-label)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定位和标注 [[93](#CR93)] [https://github.com/tricktreat/locate-and-label](https://github.com/tricktreat/locate-and-label)
- en: Bioelectra for nested NER [https://​github.​com/​kamalkraj/​BioELECTRA](https://github.com/kamalkraj/BioELECTRA)
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bioelectra用于嵌套NER [https://github.com/kamalkraj/BioELECTRA](https://github.com/kamalkraj/BioELECTRA)
- en: 5.3.3 Entity Linking
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.3 实体链接
- en: After identifying a named entity in a text (*entity mention*), one often wants
    to disambiguate it, i.e. assign the mention to a unique entity in a KB or ontology.
    This involves unifying different writings of an entity name. To attach the corresponding
    facts and relation to the same entity, it is important to link the different writings
    of a name, e.g. *“Joe Biden was elected as 46th president of the United States
    of America”* and *“President Biden was born in Scranton Pennsylvania”*. Note that
    there exist about 35 writings for the name *“Muammar Muhammad Abu Minyar al-Gaddafi”*,
    e.g. *“Qadhafi”*, *“Gaddafi”* and *“Gadhafi”* in addition to versions with the
    different first names. *Entity Linking* approaches aim to solve this problem.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在识别文本中的一个命名实体（*实体提及*）之后，人们通常希望对其进行消歧，即把提及分配给知识库或本体中的一个唯一实体。这涉及到统一一个实体名称的不同写法。为了将相应的事实和关系附加到同一实体上，链接名称的不同写法非常重要，例如，“乔·拜登被选为美国第46任总统”和“拜登总统出生于宾夕法尼亚州的斯克兰顿”。请注意，对于“穆阿迈尔·穆罕默德·阿卜杜勒-卡德尔·卡扎菲”这个名字，存在大约35种不同的写法，例如“卡扎菲”、“加达菲”和“加达菲”，以及不同名字版本。*实体链接*方法旨在解决这个问题。
- en: Entity linking is useful for tasks such as knowledge base population, chatbots,
    recommender systems, and question answering to identify the correct object or
    entity referred to. It is also required as a preprocessing step for models that
    need the entity identity, such as KnowBERT [[80](#CR80)] or ERNIE [[99](#CR99)]
    (Sect. [3.​4.​1](528393_1_En_3_Chapter.xhtml#Sec18)). Early approaches rely on
    semantic embeddings to match entity mentions belonging together [[82](#CR82)].
    Modern procedures use contextual embeddings to characterize the entity mentions.
    Sevgili et al. [[92](#CR92)] provide a comprehensive survey of Deep Learning based
    entity linking approaches. They sketch the general solution architecture of entity
    linking approaches as shown in Fig. [5.3](#Fig3) and compare different methods.![](../images/528393_1_En_5_Chapter/528393_1_En_5_Fig3_HTML.png)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 实体链接对于知识库构建、聊天机器人、推荐系统和问答等任务非常有用，用于识别正确的对象或实体。它也是需要实体身份的模型（如KnowBERT [[80](#CR80)]
    或 ERNIE [[99](#CR99)]（第[3.4.1](528393_1_En_3_Chapter.xhtml#Sec18)节））预处理步骤所必需的。早期方法依赖于语义嵌入来匹配属于同一组的实体提及[[82](#CR82)]。现代程序使用上下文嵌入来描述实体提及。Sevgili等人[[92](#CR92)]提供了一篇基于深度学习的实体链接方法的全面综述。他们概述了实体链接方法的通用解决方案架构，如图[5.3](#Fig3)所示，并比较了不同的方法。![图5.3](../images/528393_1_En_5_Chapter/528393_1_En_5_Fig3_HTML.png)
- en: A block diagram of various entities includes the following steps. Input text,
    named entity recognition, entity mention, candidate generation from K B, candidates,
    embeddings, entity ranking model, and entity with the highest score.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 各种实体的框图包括以下步骤：输入文本、命名实体识别、实体提及、从知识库生成候选实体、候选实体、嵌入、实体排名模型和得分最高的实体。
- en: Fig. 5.3
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3
- en: Entity Linking includes the three steps entity recognition, which identifies
    entity mentions in a text, candidate generation generating possible entities for
    the mention using the KB, and entity ranking, computing a similarity score between
    the candidates and the mention. Image adapted from [[92](#CR92)], reprinted with
    kind permission of authors
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 实体链接包括三个步骤：实体识别，用于识别文本中的实体提及；候选生成，使用知识库生成可能的实体；以及实体排名，计算候选实体与提及之间的相似度分数。图像改编自[[92](#CR92)]，经作者同意重印
- en: '**BLINK** [[113](#CR113)] follows the scheme of Fig. [5.3](#Fig3). First entity
    mentions together with their types are extracted from a text by NER. Then it uses
    a BERT model to compute embeddings for mention contexts and the entity descriptions
    in the KB. This also involves the normalization of entity names. Using an efficient
    approximate *k*-nearest-neighbor indexing scheme FAISS [[40](#CR40)] for embeddings
    (Sect. [6.​1.​4](528393_1_En_6_Chapter.xhtml#Sec5)). FAISS is able to retrieve
    the best matching entity candidates from the KB with little computational effort.
    This approach is identical to dense retrieval by DPR (Sect. [3.​4.​5](528393_1_En_3_Chapter.xhtml#Sec22)).
    Each retrieved candidate is then examined more carefully with a cross-encoder
    that concatenates the input context, the mention and entity text and assigns a
    score to each candidate entity. Finally, the candidate with the highest score
    is selected. Although no explicit entity embeddings are computed, the approach
    achieves Sota on the *TACKBP-2010 benchmark* [[29](#CR29)] with an accuracy of
    94.5%. A very similar approach is chosen by **EntQA** [[130](#CR130)], which also
    exploits a retriever-reader architecture and yields competitive results on several
    benchmarks.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**BLINK** [[113](#CR113)] 遵循图 [5.3](#Fig3) 的方案。首先，通过命名实体识别（NER）从文本中提取实体提及及其类型。然后，它使用
    BERT 模型计算提及上下文和 KB 中实体描述的嵌入。这也涉及到实体名称的归一化。使用高效的近似 *k* 近邻索引方案 FAISS [[40](#CR40)]
    对嵌入进行索引（见第 [6.1.4](528393_1_En_6_Chapter.xhtml#Sec5) 节）。FAISS 能够以很小的计算成本从 KB 中检索最佳匹配的实体候选者。这种方法与
    DPR（见第 [3.4.5](528393_1_En_3_Chapter.xhtml#Sec22) 节）的密集检索方法相同。然后，使用一个交叉编码器仔细检查检索到的每个候选者，该编码器将输入上下文、提及和实体文本连接起来，并为每个候选实体分配一个分数。最后，选择得分最高的候选者。尽管没有显式计算实体嵌入，但该方法在
    *TACKBP-2010 基准测试* [[29](#CR29)] 上实现了 94.5% 的准确率，达到了 Sota。**EntQA** [[130](#CR130)]
    也选择了一个非常类似的方法，它也利用了检索器-阅读器架构，并在多个基准测试上取得了有竞争力的结果。'
- en: '**GENRE** [[25](#CR25)] departs from the common solution architecture to most
    entity linking approaches and uses the encoder-decoder model BART (Sect. [3.​1.​3](528393_1_En_3_Chapter.xhtml#Sec4))
    to disambiguate entities. This model has to recover text corrupted by a number
    of different approaches during pre-training and therefore gathers a lot of knowledge
    about language. The model is fine-tuned to generate disambiguated named entities.
    For example, the sentence *“In 1503, Leonardo began painting the Mona Lisa.”*
    is translated to *“In 1503, [Leonardo](Leonardo da Vinci) began painting the [Mona
    Lisa](Mona Lisa).”*, where *“[Leonardo](Leonardo da Vinci)”* and *“[Mona Lisa](Mona
    Lisa)”* are the unique headings of the corresponding articles in Wikipedia. GENRE
    uses a constrained BEAM search for decoding, which either copies the input text
    or generates a unique Wikipedia entity name. In addition, GENRE can perform mention
    detection and end-to-end entity linking by associating a mention with the corresponding
    KB entity (e.g. the Wikipedia article). On six different benchmarks, GENRE achieves
    an average F1-value of 88.8% and outperforming BLINK, which scores 77.0%. In addition,
    GENRE has a smaller memory footprint (2.1 GB) than BLINK (30.1 GB). Finally, the
    model has a tendency to copy the mention exactly, which is helpful for new, unseen
    named entities.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型** [[25](#CR25)] 与大多数实体链接方法的常见解决方案架构不同，GENRE 使用编码器-解码器模型 BART（见第 [3.1.3](528393_1_En_3_Chapter.xhtml#Sec4)
    节）来消歧实体。该模型在预训练期间必须恢复由多种不同方法损坏的文本，因此积累了大量关于语言的知识。该模型经过微调以生成消歧的命名实体。例如，句子 *“在 1503
    年，达芬奇开始绘制蒙娜丽莎。”* 被翻译为 *“在 1503 年，[达芬奇](Leonardo da Vinci) 开始绘制 [蒙娜丽莎](Mona Lisa)。”*，其中
    *“[达芬奇](Leonardo da Vinci)”* 和 *“[蒙娜丽莎](Mona Lisa)”* 是维基百科中相应文章的唯一标题。GENRE 使用约束的
    BEAM 搜索进行解码，要么复制输入文本，要么生成独特的维基百科实体名称。此外，GENRE 可以通过将提及与相应的 KB 实体（例如维基百科文章）关联来执行提及检测和端到端实体链接。在六个不同的基准测试中，GENRE
    实现了平均 F1 值为 88.8%，超过了得分为 77.0% 的 BLINK。此外，GENRE 的内存占用（2.1 GB）比 BLINK（30.1 GB）小。最后，该模型倾向于精确复制提及，这对于新的、未见过的命名实体是有帮助的。'
- en: '**EntMask** [[118](#CR118)] is similar to LUKE (Sect. [3.​4.​4](528393_1_En_3_Chapter.xhtml#Sec21))
    and learns to predict masked entities. To disambiguate new mentions, the authors
    use local contextual information based on words, and global contextual information
    based on already disambiguated entities. Their model is trained to jointly produce
    embeddings of words and entities and is also based on BERT[LARGE]. For fine-tuning
    30% entities corresponding to Wikipedia hyperlinks are masked randomly and have
    to be predicted as shown in Fig. [5.4](#Fig4). During application the model predicts
    an entity for each mention, and from the unresolved mentions actually assigns
    the mention with the highest probability as ‘observed’. In this way, this assignment
    can influence the prediction for the remaining mentions, introducing a global
    perspective. On a number of benchmarks the approach yields roughly similar results
    to GENRE, with a small advantage on a few benchmarks.![](../images/528393_1_En_5_Chapter/528393_1_En_5_Fig4_HTML.png)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**EntMask** [[118](#CR118)] 与LUKE（第[3.4.4](528393_1_En_3_Chapter.xhtml#Sec21)节）类似，学习预测被掩码的实体。为了区分新的提及，作者们使用基于单词的局部上下文信息和基于已区分实体的全局上下文信息。他们的模型被训练以联合生成单词和实体的嵌入，并且也基于BERT[LARGE]。为了微调，对应维基百科超链接的30%实体被随机掩码并需要预测，如图[5.4](#Fig4)所示。在应用过程中，模型为每个提及预测一个实体，并从未解决的提及中实际分配概率最高的提及作为“观察到的”。这样，这种分配可以影响剩余提及的预测，引入全局视角。在多个基准测试中，该方法在几个基准测试上略优于GENRE，其他基准测试结果大致相似！[](../images/528393_1_En_5_Chapter/528393_1_En_5_Fig4_HTML.png)'
- en: A flow diagram has an input of text tokens and unique K B entities with token
    types, linked to the input embeddings, autoencoder with self-attentions, output
    embeddings, 2-layer logistic classifier, and entity probabilities to give the
    correct entity.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 流程图以文本标记和具有标记类型的唯一K B实体为输入，与输入嵌入、具有自注意力的自动编码器、输出嵌入、2层逻辑分类器和实体概率相连接，以给出正确的实体。
- en: Fig. 5.4
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4
- en: BERT[LARGE] can be fine-tuned to predict masked ‘entity tokens’ taking into
    account the corresponding text. During application successively the entities with
    highest probability are assigned. In this way, the joint probability of entities
    can be exploited [[118](#CR118)]
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: BERT[LARGE]可以微调以预测掩码的“实体标记”，同时考虑相应的文本。在应用过程中，依次分配概率最高的实体。这样，可以利用实体的联合概率 [[118](#CR118)]
- en: Available Implementations
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: 'GENRE: model source code and datasets from Facebook [https://​github.​com/​facebookresearch​/​GENRE](https://github.com/facebookresearch/GENRE)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GENRE：来自Facebook的模型源代码和数据集 [https://github.com/facebookresearch/GENRE](https://github.com/facebookresearch/GENRE)
- en: BLINK available at [https://​github.​com/​facebookresearch​/​BLINK](https://github.com/facebookresearch/BLINK)
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BLINK可在[https://github.com/facebookresearch/BLINK](https://github.com/facebookresearch/BLINK)获取
- en: 'EntMask code: [https://​github.​com/​studio-ousia/​luke](https://github.com/studio-ousia/luke).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EntMask代码：[https://github.com/studio-ousia/luke](https://github.com/studio-ousia/luke).
- en: 5.3.4 Summary
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.4 摘要
- en: It is well known that named entities play a crucial role in understanding the
    meaning of a text. Thousands of new named entities appear every day, requiring
    special effort to interpret their sense. Due to the availability of contextual
    embeddings in PLMs Named Entity Recognition (NER) could increase F1-value on the
    CoNLL 2003 benchmark from 85% to 94.6%, dramatically reducing errors. The standard
    approach is token annotation by BERT, which marks each token with its corresponding
    entity type. Higher performance can be achieved by treating named entities as
    special tokens (LUKE), combining different kinds of embeddings (ACE), or using
    retrieval approaches based on embeddings. Empirical evaluations demonstrate that
    it is extremely important to train the underlying PLM on domain texts, e.g. from
    the medical domain. Single tokens or compounds can belong to multiple entity types
    at the same time. For this, nested NER question-answering approaches can be used
    to mark token spans as belonging to an entity type. Again training on domain texts
    is essential.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，命名实体在理解文本的意义中起着至关重要的作用。每天都有成千上万的新的命名实体出现，需要特别努力来解释它们的含义。由于PLM中存在上下文嵌入，命名实体识别（NER）可以将CoNLL
    2003基准测试的F1值从85%提高到94.6%，显著减少错误。标准方法是BERT的标记注释，它将每个标记标记为其对应的实体类型。通过将命名实体视为特殊标记（LUKE）、结合不同类型的嵌入（ACE）或使用基于嵌入的检索方法，可以实现更高的性能。经验评估表明，在领域文本上训练底层PLM（例如来自医学领域的文本）极为重要。单个标记或复合词可以同时属于多个实体类型。为此，可以使用嵌套NER问答方法来标记标记跨度属于实体类型。再次强调，在领域文本上进行训练是至关重要的。
- en: In Sect. [5.4.4](#Sec24) approaches for joint entity and relation extraction
    are presented. The approaches described there can also be used for NER alone and
    promise high performance. An example is REBEL, which uses the BART encoder-decoder
    to translate the input sentence to a unique representation of the covered entities
    and relations.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [5.4.4](#Sec24) 节中介绍了联合实体和关系提取的方法。那里描述的方法也可以单独用于命名实体识别（NER），并承诺有高性能。一个例子是REBEL，它使用BART编码器-解码器将输入句子翻译成覆盖实体和关系的独特表示。
- en: Entity linking aims to map an entity mention to the underlying unique entity
    in a KB. One approach exploits the retriever-reader architecture to find entity
    candidates from a knowledge base (BLINK, EntQA). Subsequently, a reader module
    scrutinizes candidates and the mention to arrive at a final assignment. An alternative
    is GENRE’s encoder-decoder architecture, which translates entity mentions to unique
    entity names. Finally, a BERT model can determine self-attentions between token
    embeddings and entity embeddings and exploit this to predict unique entities contained
    in a text.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 实体链接旨在将实体提及映射到知识库中的底层唯一实体。一种方法利用检索器-阅读器架构从知识库中找到实体候选者（BLINK，EntQA）。随后，阅读器模块仔细审查候选者和提及，以得出最终分配。另一种方法是GENRE的编码器-解码器架构，它将实体提及翻译成唯一实体名称。最后，BERT模型可以确定标记嵌入和实体嵌入之间的自注意力，并利用这一点来预测文本中包含的唯一实体。
- en: The majority of entity linking models still rely on external knowledge like
    Wikipedia for the candidate generation step. However, this is not sufficient when
    identifying a person who is not a celebrity. In this case we have to perform a
    search in the web or social media to find information. As retrieval-reader approaches
    gain popularity, this may be possible in the future. It turns out that NER and
    entity linking should be performed jointly, i.e. assignments should take into
    account each other to increase accuracy.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数实体链接模型仍然依赖于外部知识，如维基百科，用于候选生成步骤。然而，在识别非名人时，这并不足够。在这种情况下，我们必须在网络上或社交媒体上进行搜索以找到信息。随着检索器-阅读器方法越来越受欢迎，这可能在将来成为可能。结果证明，命名实体识别（NER）和实体链接应该联合进行，即分配应考虑彼此以提高准确性。
- en: 5.4 Relation Extraction
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 关系提取
- en: After identifying relevant entities in a sentence, a crucial part of information
    extraction is often the extraction and classification of relations between these
    entities. This is useful, for example, when we automatically want to populate
    databases or knowledge graphs with linked information. Table [5.3](#Tab3) contains
    examples of language analysis tasks based on relation extraction that are discussed
    in this section. Instances include coreference resolution, i.e. finding different
    mentions of an entity in the same text, aspect-based sentiment analysis, which
    links phrases in a text to opinions about them, or semantic role labeling, which
    identifies the function of a phrase for a predicate in a sentence. Because entity
    linking associates mentions of entities with the underlying unique object or person
    in an ontology, it differs from relation extraction. A survey on prior work in
    relation extraction is given by Nasar et al. [[68](#CR68)].
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在识别句子中的相关实体之后，信息提取的一个关键部分通常是提取和分类这些实体之间的关系。这在例如我们想要自动填充数据库或知识图谱时非常有用。表 [5.3](#Tab3)
    包含了本节讨论的基于关系抽取的语言分析任务示例。实例包括核实指代，即在同一文本中找到实体的不同提及，基于方面的情感分析，它将文本中的短语与关于它们的意见联系起来，或者语义角色标注，它识别句子中谓词的短语功能。由于实体链接将实体提及与本体中基础唯一的对象或人关联起来，它不同于关系抽取。Nasar
    等人提供了一篇关于关系抽取先前工作的调查 [[68](#CR68)]。
- en: 5.4.1 Coreference Resolution
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 核实指代
- en: A first type of relation extraction is *coreference resolution*, whose goal
    is to establish a relation between all entity mentions in a text that refer to
    the same real-world entities. As an example, consider the sentence *“I voted for
    Biden because he was most aligned with my values”, she said.* where *“I”*, *“my”*,
    and *“she”* refer to the speaker, and *“Biden”* and *“he”* pertain to Joe Biden.
    Due to the combinatorial number of subsets of related phrases, coreference analysis
    is one of the most challenging tasks of NLP. A survey of coreference resolution
    is provided by Stylianou et al. [[98](#CR98)].
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 关系抽取的第一种类型是 *核实指代*，其目标是建立文本中所有指代同一现实世界实体的实体提及之间的关系。例如，考虑以下句子 *“我投票给了拜登，因为他与我价值观最相符”，她说*，其中
    *“我”*、*“我的”* 和 *“她”* 指的是说话者，而 *“拜登”* 和 *“他”* 指的是乔·拜登。由于相关短语子集的组合数量是组合数，核实指代分析是自然语言处理中最具挑战性的任务之一。Stylianou
    等人提供了一篇关于核实指代调查的论文 [[98](#CR98)]。
- en: '**SpanBERT** [[41](#CR41)] is a version of BERT, which predicts contiguous
    subsequences of masked tokens during pre-training, and therefore accumulates knowledge
    about spans of words (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2)). The
    authors consider all possible spans of text and identify relevant mentions spans.
    In parallel, for each span *x*, the preceding spans *y* are examined, and a scoring
    function estimates whether the spans refer to the same entity.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**SpanBERT** [[41](#CR41)] 是 BERT 的一种版本，它在预训练过程中预测被掩码的连续子序列，因此积累了关于词语片段的知识（见第
    [3.1.1](528393_1_En_3_Chapter.xhtml#Sec2) 节）。作者考虑了所有可能的文本片段，并识别出相关的指代片段。同时，对于每个片段
    *x*，会检查其前面的片段 *y*，并通过评分函数估计这些片段是否指代同一实体。'
- en: This scoring function is defined as *s*(*x*, *y*) = *s*[*m*](*x*) + *s*[*m*](*y*) + *s*[*c*](*x*,
    *y*). Here *s*[*m*](*x*) and *s*[*m*](*y*) measure how likely *x* and *y* are
    entity mentions. *s*[*c*](*x*, *y*) determines how likely *x* and *y* refer to
    the same entity. As input from a span, the scoring function gets the output embeddings
    of the two span endpoints and a summary of the tokens embeddings of the span.
    The probability that *y* is coreferent to *x* is computed as ![$$p(y)=\exp (s(x,y))/\sum
    _{y'\in Y} \exp (s(x,y'))$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq21.png).
    In this way, subsets of spans mentioning the same entity are formed. During the
    iterations of the approach, the span definitions may be refined, and an antecedent
    pruning mechanism is applied to reduce the number of spans to be considered. *OntoNotes*
    [[109](#CR109)] is a corpus of 1.5M words comprising various genres of text with
    structural information, e.g. coreference. After fine-tuning on OntoNotes, Span-BERT
    achieves a Sota result of 79.6% F1-value on the test set. Dobrovolskii [[27](#CR27)]
    propose a variant which performs its analysis on the word level thus reducing
    the complexity of the task. It raises the Sota on OntoNotes to 81.0%.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这个评分函数定义为 *s*(*x*, *y*) = *s*[*m*](*x*) + *s*[*m*](*y*) + *s*[*c*](*x*, *y*)。其中
    *s*[*m*](*x*) 和 *s*[*m*](*y*) 测量 *x* 和 *y* 作为实体提及的可能性。*s*[*c*](*x*, *y*) 确定x和y是否指向同一实体的可能性。作为跨度输入，评分函数获取两个跨度端点的输出嵌入以及跨度标记嵌入的摘要。*y*
    是 *x* 的同指概率计算为 ![$$p(y)=\exp (s(x,y))/\sum _{y'\in Y} \exp (s(x,y'))$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq21.png)。这样，提及同一实体的跨度子集就形成了。在方法的迭代过程中，跨度定义可能会被细化，并应用一个先行词剪枝机制来减少需要考虑的跨度数量。*OntoNotes*
    [[109](#CR109)] 是一个包含 1.5M 个单词的语料库，包括各种文本类型的结构化信息，例如指代。在 OntoNotes 上微调后，Span-BERT
    在测试集上实现了 79.6% 的 Sota F1 值。Dobrovolskii [[27](#CR27)] 提出了一种变体，它在词级别进行其分析，从而降低了任务的复杂性。它将
    OntoNotes 上的 Sota 提高到 81.0%。
- en: '**CorefQA** [[114](#CR114)] solves coreference resolution as a question-answering
    problem. A first stage considers all spans up to a maximum length as potential
    mentions. The authors use a SpanBERT model to compute embeddings for all tokens.
    To reduce the number of mentions, a proposal module combining the start and end
    embeddings of spans is pre-trained to predict relevant mentions. Subsequently,
    each mention is in turn surrounded by special tokens and the network is trained
    to mark all coreferent spans similar to the question-answering fine-tuning of
    BERT (Sect. [2.​1.​3](528393_1_En_2_Chapter.xhtml#Sec5)). To reduce the number
    of computations only a limited number of candidates in one direction is considered.
    The mention proposal and mention clustering can be trained end-to-end. On the
    coreference benchmark CoNLL 2012 [[84](#CR84)] the approach improves Sota significantly
    to 83.1% F1-value. Toshniwal et al. [[103](#CR103)] extend this approach by tracking
    only a small bounded number of entities at a time. This approach can reach a high
    accuracy in coreference resolution even for long documents.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**CorefQA** [[114](#CR114)] 将指代消解问题作为问答问题来解决。第一阶段考虑所有长度不超过最大长度的跨度作为潜在的提及。作者使用
    SpanBERT 模型计算所有标记的嵌入。为了减少提及的数量，一个结合跨度起始和结束嵌入的提案模块被预训练来预测相关提及。随后，每个提及依次被特殊标记包围，网络被训练来标记所有与
    BERT 问答微调类似的核心跨度（Sect. [2.1.3](528393_1_En_2_Chapter.xhtml#Sec5)）。为了减少计算量，只考虑一个方向的一小部分候选者。提及提案和提及聚类可以端到端训练。在核心指代基准
    CoNLL 2012 [[84](#CR84)] 上，这种方法将 Sota 提高到 83.1% 的 F1 值。Toshniwal 等人 [[103](#CR103)]
    通过同时跟踪少量有限数量的实体来扩展这种方法。这种方法即使在长文档中也能在指代消解中达到高精度。'
- en: Available Implementations
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: SpanBERT for relation extraction and coreference resolution at GitHub [https://​github.​com/​facebookresearch​/​SpanBERT](https://github.com/facebookresearch/SpanBERT)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 GitHub 上用于关系抽取和指代消解的 SpanBERT [https://github.com/facebookresearch/SpanBERT](https://github.com/facebookresearch/SpanBERT)
- en: CorefQA at GitHub [https://​github.​com/​ShannonAI/​CorefQA](https://github.com/ShannonAI/CorefQA)
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CorefQA at GitHub [https://github.com/ShannonAI/CorefQA](https://github.com/ShannonAI/CorefQA)
- en: 5.4.2 Sentence-Level Relation Extraction
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 句级关系抽取
- en: There are various types of relations which can be extracted, e.g. in the sentence
    *“Goethe succumbed to his suffering in Weimar”* the *“died-in”* relation relates
    a person (*“Goethe”*) to a location (*“Weimar”*). In this section we assume that
    entities have already been extracted from a sentence by NER (Sect. [5.3](#Sec12)).
    Therefore, NER errors will increase the errors for relation extraction.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 可以提取各种类型的关系，例如，在句子 *“歌德在魏玛忍受了他的痛苦。”* 中，*“died-in”* 关系将一个人 (*“歌德”*) 与一个地点 (*“魏玛”*)
    相关联。在本节中，我们假设实体已经通过 NER（第 [5.3](#Sec12) 节）从句子中提取出来。因此，NER 错误会增加关系提取的错误。
- en: '**SpanBERT** [[41](#CR41)] is particularly suitable for relation extraction,
    since entity mentions often span over multiple tokens, and are masked by SpanBERT
    during pre-training (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2)). For fine-tuning
    the model gets one sentence and two spans with possible relation arguments as
    input, which are replaced by their NER tags. An example is *“[CLS] [SUBJ-PER]
    was born in [OBJ-LOC] , Michigan, . . .”*. The final [CLS] embedding is input
    to a logistic classifier, which predicts one of the 42 predefined relation types,
    including “no relation”. *Re-TACRED* [[97](#CR97)] is a large-scale relation extraction
    dataset with 120k examples covering 41 relation types (e.g., per:schools-attended
    and org:members) and carefully checked relation annotations. SpanBERT showed good
    performance on Re-TACRED with 85.3% F1-value [[95](#CR95)].'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**SpanBERT** [[41](#CR41)] 特别适合于关系提取，因为实体提及通常跨越多个标记，并且在预训练期间被 SpanBERT 掩码（第
    [3.1.1](528393_1_En_3_Chapter.xhtml#Sec2) 节）。在微调模型时，模型接收一个句子和两个可能的关系论元跨度的输入，这些跨度被它们的
    NER 标签替换。一个例子是 *“[CLS] [SUBJ-PER] 出生在 [OBJ-LOC] 密歇根，……”*。最终的 [CLS] 嵌入被输入到一个逻辑分类器，该分类器预测
    42 个预定义的关系类型之一，包括“无关系”。*Re-TACRED* [[97](#CR97)] 是一个包含 120k 个示例的大型关系提取数据集，涵盖了
    41 种关系类型（例如，per:schools-attended 和 org:members）以及仔细检查的关系注释。SpanBERT 在 Re-TACRED
    上表现良好，F1 值为 85.3% [[95](#CR95)]。'
- en: '**RoBERTa** (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2)) can be used
    to generate token embeddings for relation extraction. Zhou et al. [[135](#CR135)]
    evaluate various entity representation techniques. They use RoBERTa[LARGE] to
    encode the input text by embeddings of the last layer. The embeddings of the first
    token in each span of relation argument mentions are used to represent these arguments.
    These are concatenated and adopted as input for a softmax classifier. It turns
    out that enclosing an entity and adding its type with special tokens yields the
    best results on the Re-TACRED dataset with 91.1% F1-value.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**RoBERTa** (第 [3.1.1](528393_1_En_3_Chapter.xhtml#Sec2) 节) 可以用于生成关系提取的标记嵌入。周等人
    [[135](#CR135)] 评估了各种实体表示技术。他们使用 RoBERTa[LARGE] 通过最后一层的嵌入来编码输入文本。每个关系论元跨度的第一个标记的嵌入用于表示这些论元。这些嵌入被连接起来，并作为
    softmax 分类器的输入。结果表明，使用特殊标记包围实体并添加其实体类型在 Re-TACRED 数据集上取得了最佳结果，F1 值为 91.1%。'
- en: '**Relation-QA** [[24](#CR24)] rephrase the relation classification problem
    into a question answering problem. Consider the sentence *s* =  *“Sam Brown was
    born in 1991.”* with the extracted entities *“Sam Brown”* and *“1991”*. Then the
    authors create two queries, such as *“When was Sam Brown born?”* and *“Who was
    born in 1991?”*. They fine-tune ALBERT (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2))
    to answer these queries by marking the spans containing the desired entity. If
    no span is returned the relation does not hold. The approach achieves an F1-value
    of 74.8% for TACRED, an older version of ReTACRED with many annotation problems.
    **RECENT** [[55](#CR55)] extends SpanBERT and trains more than one relation classification
    model, i.e. one classifier for each different pair of entity types. This restricts
    the possible output relation types and helps to increase performance. On TACRED
    the approach yields a Sota F1-value of 75.2%.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**Relation-QA** [[24](#CR24)] 将关系分类问题重新表述为一个问答问题。考虑句子 *s* = *“Sam Brown 出生于
    1991 年。”* 中提取的实体 *“Sam Brown”* 和 *“1991”*。然后作者创建了两个查询，例如 *“Sam Brown 出生于什么时候？”*
    和 *“谁在 1991 年出生？”*。他们微调 ALBERT（第 [3.1.1](528393_1_En_3_Chapter.xhtml#Sec2) 节）来通过标记包含所需实体的跨度来回答这些查询。如果没有返回跨度，则关系不存在。该方法在
    TACRED 上实现了 74.8% 的 F1 值，这是一个带有许多注释问题的 ReTACRED 的旧版本。**最近** [[55](#CR55)] 扩展了
    SpanBERT 并训练了多个关系分类模型，即每个不同实体类型对都有一个分类器。这限制了可能的输出关系类型，有助于提高性能。在 TACRED 上，该方法实现了
    75.2% 的 Sota F1 值。'
- en: 5.4.3 Document-Level Relation Extraction
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.3 文档级关系提取
- en: Especially for larger documents, the assumption that relations occur only inside
    a sentence is too restrictive. Therefore, some models check for relations on the
    document level. When relation arguments are in different sentences the corresponding
    entities are often only referred to via coreferent mentions. Therefore, we assume
    in this section that entities have been extracted and grouped into clusters denoting
    the same entity by coreference resolution (Sect. [5.4.1](#Sec20)). Obviously the
    errors of coreference resolution will increase the final relation extraction errors.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其对于较大的文档，假设关系仅发生在句子内部这一假设过于严格。因此，一些模型在文档级别检查关系。当关系论据位于不同句子时，相应的实体通常仅通过共指提及来引用。因此，在本节中，我们假设实体已经被提取并分组为通过共指消解（Sect.
    [5.4.1](#Sec20)）表示相同实体的簇。显然，共指消解的错误会增加最终的实体关系提取错误。
- en: '**SSAN** [[115](#CR115)] (Structured Self-Attention Network) directly takes
    into account structural information such as coreference and cooccurrence of entity
    mentions for PLMs such as RoBERTa. The authors modify the self-attention computations
    in encoder blocks by adding specific biases, if two mentions refer to the same
    entity and/or are located in the same sentence. These biases are computed from
    the query and key vectors by a “transformation model” trained during fine-tuning.
    Therefore, the scalar products between keys and queries are modified depending
    on whether the corresponding tokens are coreferent, in the same sentence, or not.
    Entity embeddings are obtained via average pooling of token embeddings of the
    entity mention. For each pair *emb*[*i*], *emb*[*j*] of entity embeddings the
    probability of a relation *r* is computed by a bilinear transformation ![$$ \operatorname
    {\mathrm {sigmoid}}({emb}_i^\intercal W_r {emb}_j)$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq22.png)
    with a trainable parameter matrix *W*[*r*].'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**SSAN** [[115](#CR115)]（结构化自注意力网络）直接考虑了结构信息，如共指和实体提及的共现，对于 RoBERTa 等预训练语言模型。作者通过添加特定偏差修改了编码块中的自注意力计算，如果两个提及指向同一实体并且/或者位于同一句子中。这些偏差是通过在微调期间训练的“转换模型”从查询和键向量计算得出的。因此，键和查询之间的标量积根据相应的标记是否共指、位于同一句子中或不是而修改。通过平均池化实体提及的标记嵌入来获得实体嵌入。对于每个实体嵌入对
    *emb*[*i*]，*emb*[*j*]，通过一个具有可训练参数矩阵 *W*[*r*] 的双线性变换 ![$$ \operatorname {\mathrm
    {sigmoid}}({emb}_i^\intercal W_r {emb}_j)$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq22.png)
    计算关系 *r* 的概率。'
- en: '*DocRED* [[121](#CR121)] is a large benchmark of documents annotated with named
    entities, coreferences, and relations whose arguments may be located in different
    sentences. Using RoBERTa[LARGE] as base network, the authors achieve a Sota of
    65.9% F1 on DocRED. Using a special BERT version SciBERT [[11](#CR11)] trained
    on scientific papers from Semantic Scholar, the algorithm also yields Sota results
    for benchmarks with chemical as well as biological texts.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*DocRED* [[121](#CR121)] 是一个大型基准，其中包含带有命名实体、共指和关系的文档，其论据可能位于不同的句子中。使用 RoBERTa[LARGE]
    作为基础网络，作者在 DocRED 上实现了 65.9% 的 Sota F1。使用在 Semantic Scholar 上的科学论文训练的特殊 BERT 版本
    SciBERT [[11](#CR11)]，该算法在化学和生物文本的基准测试中也实现了 Sota 结果。'
- en: '**ATLOP** [[136](#CR136)] marks the start and end of a mentions by a special
    token and encodes a document by BERT resulting in embeddings for each token. The
    embedding of token at the mention start is used as the mention embeddings. An
    entity embedding is computed by pooling coreferent mentions. The first and the
    second argument entity embedding of a relation are transformed by different fully
    connected layers to ***x***[1] and ***x***[2]. Subsequently, the probability of
    a relation *r* for an entity pair is estimated by a sparse bilinear transformation
    ![$$ \operatorname {\mathrm {sigmoid}}({\boldsymbol {x}}_1^\intercal W {\boldsymbol
    {x}}_2)$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq23.png).
    Trainable probability thresholds are used to decide if a relation holds. On the
    DocRED benchmark the model achieves an F1-value of 63.4%.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**ATLOP** [[136](#CR136)] 通过特殊标记标记提及的开始和结束，并使用 BERT 对文档进行编码，从而为每个标记生成嵌入。提及开始的标记嵌入用作提及嵌入。通过池化共指提及来计算实体嵌入。关系的第一和第二个论据实体嵌入通过不同的全连接层转换为
    ***x***[1] 和 ***x***[2]。随后，通过稀疏双线性变换 ![$$ \operatorname {\mathrm {sigmoid}}({\boldsymbol
    {x}}_1^\intercal W {\boldsymbol {x}}_2)$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq23.png)
    估计实体对的一个关系 *r* 的概率。使用可训练的概率阈值来决定关系是否成立。在 DocRED 基准测试中，该模型实现了 63.4% 的 F1 值。'
- en: 5.4.4 Joint Entity and Relation Extraction
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.4 联合实体和关系抽取
- en: Since NER and relation extraction are closely related tasks and relation extraction
    depends on the results of NER, it is a natural choice to model these tasks jointly.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于命名实体识别（NER）和关系抽取是紧密相关的任务，且关系抽取依赖于NER的结果，因此将这些任务联合建模是一个自然的选择。
- en: '**UniRE** [[108](#CR108)] encodes entity and relation properties in a joint
    matrix, which has a row and a column for each text token. While named entities,
    e.g. PER, are marked on the diagonal, relations are matrix entries off-diagonal.
    If, for example, *“David Perkins”* lives in *“California”* the matrix entries
    in the rows of the *“David Perkins”* tokens and the columns of the *“California”*
    tokens are marked with the *PHY* *S* relation. Note that in this way asymmetric
    relations may be specified.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**UniRE** [[108](#CR108)] 在一个联合矩阵中编码实体和关系属性，该矩阵为每个文本标记有一个行和一个列。例如，命名实体（如PER）在对角线上标记，而关系则位于非对角线上的矩阵条目。例如，如果
    *“David Perkins”* 住在 *“California”*，则 *“David Perkins”* 标记的行和 *“California”* 标记的列上的矩阵条目将标记为
    *PHY* *S* 关系。请注意，通过这种方式可以指定非对称关系。'
- en: All words in the input are encoded using a BERT encoder and then a biaffine
    model is used to create a scoring vector for a pair *h*[*i*] and *h*[*j*] of embeddings![$$\displaystyle
    \begin{aligned} p(y_{i,j}|s) = \operatorname{\mathrm{softmax}}\left( ({\boldsymbol{h}}_i^{first})^\intercal
    U_1 {\boldsymbol{h}}_j^{sec} + U_2 \lbrack{\boldsymbol{h}}_i^{first},{\boldsymbol{h}}_j^{sec}\rbrack
    +b \right), \end{aligned} $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_Equ2.png)(5.2)where
    ![$${\boldsymbol {h}}_i^{first}=\text{FCL}_{first}({\boldsymbol {h}}_i)$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq24.png)
    and ![$${\boldsymbol {h}}_i^{sec}=\text{FCL}_{sec}({\boldsymbol {h}}_i)$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq25.png)
    are fully connected layer transformations of the first and second relation argument
    respectively. The softmax function obtains a probability distribution over the
    entity and relation labels for all matrix cells. The model minimizes three losses,
    one based on the actual labels of each cell, one based on the knowledge that diagonal
    of entity labels should be symmetrical and one based on the fact that a relation
    label implies that respective entity labels must be present. *ACE 2005* [[104](#CR104)]
    consists of text of various types annotated for entities, relations and events.
    On ACE 2005 UniRE yields an F1-value of 66.0% for joint entity and relation extraction,
    which is less than the current Sota of 70.5%.**PL-Marker** [[122](#CR122)] investigate
    different types of mention encodings. For a possible relation it surrounds the
    first argument span (subject) by solid marker tokens. The possible second argument
    spans (objects) are marked by *leviated tokens**Oi* and ∕*Oi* outside the text
    (Fig. [5.5](#Fig5)). These get the same position embeddings as the corresponding
    object spans in the text. Their attention connections are restricted, i.e they
    are visible to each other, but not to the text token and other pairs of markers.
    Therefore, depending on the subject span the object token embeddings can capture
    different aspects. For each pair of subject-object arguments, the corresponding
    embeddings are concatenated and used as input to a logistic classifier to estimate
    the probability of the possible relations (or ‘no relation’). Pre-trained variants
    of BERT are fine-tuned with ACE 2005 to predict the relations. With a BERT[BASE]
    model of 105M parameters the approach yields an F1-value of 68.8% on the ACE05
    benchmark. If ALBERT[XXLARGE] [[45](#CR45)] with 235M parameters is used to compute
    the embeddings, the F1-score grows to 72.3%.![](../images/528393_1_En_5_Chapter/528393_1_En_5_Fig5_HTML.png)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 输入中的所有单词都使用BERT编码器进行编码，然后使用双亲和模型为嵌入对 *h*[*i*] 和 *h*[*j*] 创建评分向量！[$$\displaystyle
    \begin{aligned} p(y_{i,j}|s) = \operatorname{\mathrm{softmax}}\left( ({\boldsymbol{h}}_i^{first})^\intercal
    U_1 {\boldsymbol{h}}_j^{sec} + U_2 \lbrack{\boldsymbol{h}}_i^{first},{\boldsymbol{h}}_j^{sec}\rbrack
    +b \right), \end{aligned} $$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_Equ2.png)(5.2)其中
    ![$${\boldsymbol {h}}_i^{first}=\text{FCL}_{first}({\boldsymbol {h}}_i)$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq24.png)
    和 ![$${\boldsymbol {h}}_i^{sec}=\text{FCL}_{sec}({\boldsymbol {h}}_i)$$](../images/528393_1_En_5_Chapter/528393_1_En_5_Chapter_TeX_IEq25.png)
    分别是第一和第二关系参数的完全连接层变换。softmax函数为所有矩阵单元的实体和关系标签获得一个概率分布。该模型最小化三个损失，一个基于每个单元的实际标签，一个基于实体标签对角线应该是对称的知识，一个基于关系标签意味着相应的实体标签必须存在的实际情况。*ACE
    2005* [[104](#CR104)] 包含各种类型文本的实体、关系和事件标注。在ACE 2005 UniRE中，联合实体和关系提取的F1值为66.0%，低于当前Sota的70.5%。**PL-Marker**
    [[122](#CR122)] 研究不同类型的提及编码。对于一个可能的关系，它用实心标记标记包围第一个参数跨度（主语）。可能的第二个参数跨度（宾语）在文本外部用*leviated
    tokens**Oi* 和 ∕*Oi* 标记。这些获得与文本中相应宾语跨度相同的位置嵌入。它们的注意力连接受到限制，即它们彼此可见，但不能看到文本标记和其他标记对。因此，根据主语跨度，宾语标记嵌入可以捕捉不同的方面。对于每个主语-宾语参数对，相应的嵌入被连接起来，并用作逻辑分类器的输入，以估计可能关系的概率（或“无关系”）。使用ACE
    2005对预训练的BERT变体进行微调以预测关系。使用105M参数的BERT[BASE]模型，该方法在ACE05基准测试上实现了68.8%的F1值。如果使用235M参数的ALBERT[XXLARGE]
    [[45](#CR45)] 来计算嵌入，F1分数将增长到72.3%。![](../images/528393_1_En_5_Chapter/528393_1_En_5_Fig5_HTML.png)
- en: A flow diagram of input tokens and their positions, linked to the input embeddings,
    autoencoder with self-attentions, output embeddings, 2-layer logistic classifier,
    and relation probabilities to give the observed relation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 输入标记及其位置的流程图，与输入嵌入、具有自注意力的自动编码器、输出嵌入、2层逻辑分类器以及关系概率相关联，以给出观察到的关系。
- en: Fig. 5.5
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5
- en: For a possible relation the PL-marker model marks the first relation argument
    by special ‘solid’ markers and the possible second arguments by ‘leviated’ markers
    outside the text. The latter get the same positions as the corresponding tokens,
    and do not influence the embeddings of normal tokens during attention computation.
    The marker embeddings are concatenated to compute the probability of the corresponding
    relation [[122](#CR122)]
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可能的关系，PL标记模型通过特殊的“实心”标记标记第一个关系论元，并通过文本外的“轻化”标记标记可能的第二个论元。后者获得与相应标记相同的位臵，并且在注意力计算期间不影响正常标记的嵌入。标记嵌入被连接起来以计算相应关系的概率
    [[122](#CR122)]。
- en: For NER, the PL-Marker model uses a similar approach. For each possible span
    in the input starting at token *v*[*i*] and ending at token *v*[*j*,*j*≥*i*],
    leviated markers are created, which do not affect the embeddings of the normal
    tokens. Again the embeddings of the start and end tokens of a span as well as
    the embeddings of leviated markers are input for a logistic classifier computing
    the probability of the different NE-types. The model uses an efficient ‘packing’
    to reduce computational effort. On the CoNLL03 named entity benchmark, PL-markers
    with a pre-trained RoBERTa[LARGE] achieve an F1-value of 94.0, which is well below
    the current Sota of 96.1% held by DeBERTa [[19](#CR19)]. When the relation extraction
    employs the entity types and spans predicted by the PL-MARKER NER, the F1-value
    of the joint approach drops to 70.5%, which is Sota for the ACE05 benchmark on
    joint NER and relation extraction.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于命名实体识别（NER），PL-Marker 模型采用类似的方法。对于输入中从标记 *v*[*i*] 开始并结束于标记 *v*[*j*]（*j*≥*i*）的每个可能跨度，创建轻化标记，这些标记不影响正常标记的嵌入。同样，跨度的起始和结束标记的嵌入以及轻化标记的嵌入被输入到一个逻辑分类器中，该分类器计算不同命名实体类型的概率。该模型使用高效的“打包”方法来减少计算工作量。在
    CoNLL03 命名实体基准测试中，使用预训练的 RoBERTa[LARGE] 的 PL标记实现了 94.0 的 F1 值，这远低于目前由 DeBERTa
    保持的 96.1% 的当前 Sota。当关系提取使用 PL-MARKER NER 预测的实体类型和跨度时，联合方法的 F1 值下降到 70.5%，这是 ACE05
    基准测试在联合 NER 和关系提取方面的 Sota。
- en: '**REBEL** [[20](#CR20)] uses the encoder-decoder transformer BART[LARGE] (Sect.
    [3.​1.​3](528393_1_En_3_Chapter.xhtml#Sec4)) for joint entity and relation extraction
    that outputs each relation (*h*, *r*, *t*) triplet present in the input text.
    It translates a raw input sentence containing entities, together with implicit
    relations between them, into a set of triplets that explicitly refer to those
    relations. An example is shown in Fig. [5.6](#Fig6). Each relation in the text
    appears in the output according to the position of its first argument. An entity
    may be part of different relations, which are ordered according to the position
    of the second argument. This defines the order of relations in the linearized
    representation.![](../images/528393_1_En_5_Chapter/528393_1_En_5_Fig6_HTML.png)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**REBEL** [[20](#CR20)] 使用编码器-解码器转换器 BART[LARGE]（第 [3.1.3](528393_1_En_3_Chapter.xhtml#Sec4)
    节）进行联合实体和关系提取，该提取器输出输入文本中存在的每个关系 (*h*, *r*, *t*) 三联组。它将包含实体的原始输入句子及其之间的隐含关系翻译成一组三联组，这些三联组明确指出了那些关系。一个例子显示在图
    [5.6](#Fig6) 中。文本中的每个关系都根据其第一个论元的位臵出现在输出中。一个实体可能是不同关系的组成部分，这些关系按照第二个论元的位臵进行排序。这定义了线性表示中关系的顺序。![](../images/528393_1_En_5_Chapter/528393_1_En_5_Fig6_HTML.png)'
- en: A block diagram of the input text and relation triples gives the linearized
    representation for the song titled this must be the place by the new wave band
    talking heads from their album speaking in tongues.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 输入文本和关系三联组的框图给出了新潮乐队 talking heads 从专辑 speaking in tongues 中歌曲 titled this must
    be the place 的线性表示。
- en: Fig. 5.6
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6
- en: For the training set the relation information on the left side is linearized
    to the representation on the right side. The REBEL model thus learns to translate
    the input text to this linearized representation [[20](#CR20)]
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练集，左侧的关系信息被线性化到右侧的表示中。因此，REBEL 模型学习将输入文本翻译成这种线性表示 [[20](#CR20)]。
- en: The pre-trained BART[LARGE] with 400M parameters is first fine-tuned on a Wikipedia
    and WikiData training set with 220 relation types. Then it is fine-tuned a second
    time on varying benchmark datasets. On the *DocRED benchmark* [[121](#CR121)]
    it achieves Sota with an F-value of 47.1%. On the *New York Times dataset* it
    has a Sota performance with 93.4% F1\. On the ReTACRED benchmark it yields 90.4%
    F1 without the inclusion of entity type markers used by other approaches.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的 BART[LARGE] 模型，参数量为 400M，首先在包含 220 种关系的维基百科和 WikiData 训练集上进行微调。然后，它在不同的基准数据集上进行了第二次微调。在
    *DocRED 基准* [[121](#CR121)] 上，它实现了 Sota，F 值为 47.1%。在 *纽约时报数据集* 上，它实现了 93.4% 的
    Sota F1 性能。在 ReTACRED 基准上，它在不包含其他方法使用的实体类型标记的情况下，产生了 90.4% 的 F1。
- en: Aspect-Based Sentiment Analysis
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于方面的情感分析
- en: '*Aspect-based sentiment analysis*, also known as aspect-level sentiment analysis,
    feature-based sentiment analysis, or simply, aspect sentiment analysis, allows
    organizations to perform a detailed analysis of their member or customer feedback
    data. This ranges from analyzing customer reactions for a restaurant to evaluating
    the attitude to political statements made by a politician. An example is *“The**waiter*[*1-aspect*]*was**very
    friendly*[*1-positive*],*but the**steak mignon*[*2-aspect*]*was**extremely burnt*[*2-negative*].*”*
    Note that a sentence may contain different aspects and each sentiment has to be
    assigned to one aspect. A recent survey of aspect-based sentiment analysis is
    given by Zhang et al. [[129](#CR129)].'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于方面的情感分析*，也称为方面级情感分析、基于特征的情感分析，或简称为方面情感分析，允许组织对其成员或客户反馈数据进行详细分析。这包括从分析餐厅顾客的反应到评估政治家对政治声明的态度。例如，“*服务员*[*1-方面*]非常友好*[*1-正面*]，但*牛排*[*2-方面*]烧得非常严重*[*2-负面*]。”*
    注意，一个句子可能包含不同的方面，每个情感必须分配给一个方面。张等人提供了一篇关于基于方面情感分析的最近调查[[129](#CR129)]。'
- en: '**DeBERTa** (Sect. [3.​1.​1](528393_1_En_3_Chapter.xhtml#Sec2)) is a powerful
    BERT-like model, which assumes that the aspects are already known. It employs
    a disentangled attention mechanism for computing separate attention scores between
    words and positions disentangling semantic (content) and syntactic (position)
    representation of the textual data. The objective is to determine the sentiment
    of each aspect of a given entity. The input consist of a text and an aspect, e.g.
    *x* = *“[CLS] …nice video camera and keyboard …[SEP] keyboard [SEP]”*, where *“keyboard”*
    is a possible aspect span from the text [[94](#CR94)]. The output embedding of
    *[CLS]* is used as input to a logistic classifier which generates the probabilities
    of three possible labels positive, negative, neutral. The model is fine-tuned
    on the *SemEval 2014 Task 4.2 benchmark*. It yields a mean accuracy for the Restaurant
    and Laptop data of 86.1%. There are much more complex approaches like **LSA**
    (local sentiment aggregation) [[119](#CR119)] achieving a Sota of 88.6% on this
    benchmark.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**DeBERTa**（第 [3.1.1](528393_1_En_3_Chapter.xhtml#Sec2) 节）是一个强大的类似 BERT 的模型，它假设方面已经已知。它采用解耦注意力机制来计算单词和位置之间的单独注意力分数，解耦文本数据的语义（内容）和句法（位置）表示。目标是确定给定实体的每个方面的情感。输入包括一个文本和一个方面，例如
    *x* = *“[CLS] …好的视频摄像头和键盘 …[SEP] 键盘 [SEP]”*，其中 *“键盘”* 是文本中的一个可能的方面跨度 [[94](#CR94)]。*[CLS]*
    的输出嵌入被用作逻辑分类器的输入，该分类器生成三个可能标签（正面、负面、中性）的概率。该模型在 *SemEval 2014 任务 4.2 基准* 上进行微调。它在餐厅和笔记本电脑数据上实现了
    86.1% 的平均准确率。还有许多更复杂的方法，如 **LSA**（局部情感聚合）[[119](#CR119)]，在这个基准上实现了 88.6% 的 Sota。'
- en: '**GRACE** [[54](#CR54)] aims at extracting aspects and labels simultaneously.
    It consists of a first BERT[BASE] module generating token embeddings of the input
    text, which are fine-tuned to mark aspects by IOB2 tags for each token. The resulting
    information is fed into a Transformer decoder to predict the sentiments (positive,
    negative, neural) for each token. This decoder uses a multi-head cross attention
    to include the information from the first aspect module. Again for each token
    embedding in the last layer a logistic classifier is used to compute the probabilities
    of sentiments. To make the model more robust, small perturbations for input token
    embeddings are used during training. Note that no masked cross-attention is necessary
    as the decoder is not autoregressive. In this way, the model is able to take into
    account the interactions between aspect terms when labeling sentiments. The model
    achieves 87.9% F1 score for aspect extraction for the laptop reviews from SemEval
    2014 and a Sota of 70.7% F1-value for the joint extraction of aspects and sentiments.
    On the restaurant reviews it yields an F1 of 78.1% and on a tweet benchmark 58.3%
    for joint sentiment extraction, again outperforming a number of other models.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**GRACE** [[54](#CR54)]旨在同时提取方面和标签。它由一个BERT[BASE]模块组成，生成输入文本的标记嵌入，这些嵌入通过IOB2标签对每个标记进行微调以标记方面。这些信息被输入到Transformer解码器中，以预测每个标记的情感（正面、负面、中性）。这个解码器使用多头交叉注意力来包含来自第一个方面模块的信息。再次，对于最后一层的每个标记嵌入，使用逻辑分类器来计算情感的概率。为了使模型更鲁棒，在训练期间使用对输入标记嵌入的小扰动。请注意，由于解码器不是自回归的，因此不需要掩码交叉注意力。这样，模型能够在标注情感时考虑到方面术语之间的相互作用。该模型在SemEval
    2014的笔记本电脑评论中实现了87.9%的F1分数，在方面提取方面达到了70.7%的Sota F1值。在餐厅评论中，它实现了78.1%的F1分数，在推文基准测试中实现了58.3%的联合情感提取，再次优于其他一些模型。'
- en: Semantic Role Labeling
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语义角色标注
- en: Semantic role labeling considers a predicate (e.g. verb) of a sentence and word
    phrases are classified according to their syntactic roles, such as agent, goal,
    or result. It can be used to determine the meaning of the sentence. As an example
    consider the sentence *“They want to do more .”* where *“want”* is the predicate,
    *“They”* is the agent and *“to do more”* is the object (thing wanted).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 语义角色标注考虑句子中的谓词（例如动词），将词短语根据其句法角色进行分类，例如施事、目标或结果。它可以用来确定句子的意义。例如，考虑句子*“他们想做得更多。”*其中*“想”*是谓词，*“他们”*是施事，*“做得更多”*是宾语（想要的东西）。
- en: '**Crf2o** [[133](#CR133)] is a tree-structured conditional random field (treecrf)
    [[28](#CR28)] using contextual embeddings of the input tokens computed by RoBERTa
    as input. The sequence ***x*** = (*x*[1], …, *x*[*T*]) of inputs can be arranged
    in a tree ***y*** and gets a score, which is the sum of all scores of its subtrees
    *s*(***x***, ***y***) =∑[*t* ∈***y***]*s*(***x***, *t*). Similar to dependency
    parsing, this can be used to model the dependency of phrases from the predicate
    in semantic role labeling [[87](#CR87)]. To generate all possible subtrees requires
    *T*³ operations, which is very inefficient. The authors were able to reduce this
    effort using structural constraints. In addition, they could take into account
    the dependency between two branches of the tree, which generated a second order
    tree. During training the models maximize the probability of the provided tree
    structure of the training data for an input. *CoNLL05* [[21](#CR21)] and *OntoNotes*
    [[84](#CR84)] are two widely used benchmarks for semantic role labeling. For CoNLL05
    the Crf2o yields an F1-value of 89.6% and for OntoNotes it achieves an F1-value
    of 88.3%, which both constitute a new Sota. Note that this technique may also
    be used for *dependency parsing* [[132](#CR132)], which describes the syntactic
    structure of a sentence by a tree structure.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**Crf2o** [[133](#CR133)]是一个使用RoBERTa计算输入标记上下文嵌入的树结构条件随机场（treecrf）[[28](#CR28)]。输入序列***x***=(*x*[1],
    …, *x*[*T*])可以排列成一个树***y***，并得到一个分数，即其子树的所有分数之和，s(***x***, ***y***)=∑[*t* ∈***y***]*s*(***x***,
    *t*)。类似于依存句法分析，这可以用来在语义角色标注中建模谓词的短语依赖[[87](#CR87)]。要生成所有可能的子树需要*T*³操作，这非常低效。作者能够通过结构约束来减少这种努力。此外，他们还能够考虑树的两个分支之间的依赖关系，这产生了一个二阶树。在训练过程中，模型最大化输入的树结构训练数据的概率。*CoNLL05*
    [[21](#CR21)]和*OntoNotes* [[84](#CR84)]是两个广泛使用的语义角色标注基准。对于CoNLL05，Crf2o实现了89.6%的F1值，对于OntoNotes，它实现了88.3%的F1值，这两者都构成了新的Sota。请注意，这种技术也可以用于*依存句法分析*
    [[132](#CR132)]，它通过树结构描述句子的句法结构。'
- en: Extracting Knowledge Graphs from Pre-trained PLMs
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从预训练的 PLM 中提取知识图谱
- en: A systematic way to extract knowledge from big language models has been demonstrated
    by Wang et al. [[105](#CR105)]. Their **MaMa** approach consist of a match stage
    and a map stage. The match stage generates a set of candidate facts from the text
    collection exploiting the internal knowledge of a language model. Similar to TransE
    (Sect. [3.​4.​1](528393_1_En_3_Chapter.xhtml#Sec18)) each fact is represented
    as a relation triple (head, relation, tail), or (*h*, *r*, *t*). A language model
    is used to generate tokens corresponding to *r* or *t*. As a condition, the *r*
    values should be contiguous text sequences and express frequent relations.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 王等人已经展示了一种从大型语言模型中提取知识的方法 [[105](#CR105)]。他们的 **MaMa** 方法包括一个匹配阶段和一个映射阶段。匹配阶段从文本集合中利用语言模型的内部知识生成一组候选事实。类似于
    TransE (第 [3.4.1](528393_1_En_3_Chapter.xhtml#Sec18) 节)，每个事实表示为一个关系三元组（头，关系，尾），或
    (*h*, *r*, *t*)。语言模型用于生成对应于 *r* 或 *t* 的标记。作为条件，*r* 的值应该是连续的文本序列，并表达频繁的关系。
- en: In the map stage the triples are mapped to related triples with appropriate
    relations. As an example (Dylan, is, songwriter) is mapped to (Bob Dylan.Q392,
    occupation.P106, Songwriter.Q753110) according to the Wikidata schema. This stage
    is related to entity linking discussed in Sect. [5.3.3](#Sec16). The reason for
    mapping to an existing KG schema is to make use of the high-quality schema designed
    by experts.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在映射阶段，三元组被映射到具有适当关系的相关三元组。例如，(Dylan, is, songwriter) 根据 Wikidata 架构映射到 (Bob
    Dylan.Q392, occupation.P106, Songwriter.Q753110)。这一阶段与第 [5.3.3](#Sec16) 节中讨论的实体链接相关。映射到现有的
    KG 架构的原因是为了利用专家设计的优质架构。
- en: A subgraph of the generated relations is shown in Fig. [5.7](#Fig7). Compared
    to the Sota information extraction system Stanford OpenIE [[5](#CR5)] with 27.1%
    F1-value the approach yields 29.7% F1-value. The authors report that performance
    increases with model size because larger models can store more knowledge.![](../images/528393_1_En_5_Chapter/528393_1_En_5_Fig7_HTML.png)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的关系的子图如图 [5.7](#Fig7) 所示。与具有 27.1% F1 值的 Sota 信息提取系统 Stanford OpenIE [[5](#CR5)]
    相比，该方法产生了 29.7% 的 F1 值。作者报告称，性能随着模型大小的增加而提高，因为更大的模型可以存储更多的知识。![图 5.7](../images/528393_1_En_5_Chapter/528393_1_En_5_Fig7_HTML.png)
- en: An interconnected network connects the central node of Bob underscore Dylan
    dot Q 392 with various music genres, places, awards, and other artists.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相互连接的网络将 Bob underscore Dylan dot Q 392 的中心节点与各种音乐流派、地点、奖项和其他艺术家连接起来。
- en: Fig. 5.7
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7
- en: 'A snapshot subgraph of the open KG generated by MAMA [[105](#CR105)] using
    BERT[LARGE] from Wikipedia pages neighboring *“Bob Dylan”*. The blue node and
    arrow represent the mapped facts in the Wikidata schema, while the yellow node
    and arrow denote the unmapped facts in the open schema. The correct facts that
    are new in Wikidata are visualized in yellow. Image source: [[105](#CR105), p.
    6], with kind permission of the authors'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 BERT[LARGE] 从维基百科页面中与 *“Bob Dylan”* 相邻的页面生成的开放知识图谱的快照子图 [[105](#CR105)]。蓝色节点和箭头表示在
    Wikidata 架构中映射的事实，而黄色节点和箭头表示在开放架构中未映射的事实。在 Wikidata 中新的正确事实以黄色可视化。图片来源：[[105](#CR105)，第
    6 页]，作者授权使用
- en: '***Available Implementations***'
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '***可用实现***'
- en: PL-Marker Code and models are publicly available at [https://​github.​com/​thunlp/​PL-Marker](https://github.com/thunlp/PL-Marker).
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PL-Marker 代码和模型在 [https://github.com/thunlp/PL-Marker](https://github.com/thunlp/PL-Marker)
    公开提供。
- en: REBEL on GitHub [https://​github.​com/​babelscape/​rebel](https://github.com/babelscape/rebel)
    and Hugging Face [https://​huggingface.​co/​Babelscape/​rebel-large](https://huggingface.co/Babelscape/rebel-large)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: REBEL 在 GitHub [https://github.com/babelscape/rebel](https://github.com/babelscape/rebel)
    和 Hugging Face [https://huggingface.co/Babelscape/rebel-large](https://huggingface.co/Babelscape/rebel-large)
- en: 'MaMa: Source code and pre-trained models at [https://​github.​com/​theblackcat102/​language-models-are-knowledge-graphs-pytorch](https://github.com/theblackcat102/language-models-are-knowledge-graphs-pytorch)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MaMa：源代码和预训练模型在 [https://github.com/theblackcat102/language-models-are-knowledge-graphs-pytorch](https://github.com/theblackcat102/language-models-are-knowledge-graphs-pytorch)
    提供
- en: 5.4.5 Distant Supervision
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.5 远程监督
- en: Obtaining a large annotated dataset for relation extraction is a tedious task
    and often difficult due to privacy issues. Since much relational knowledge is
    stored in knowledge bases, Mintz et al. [[65](#CR65)] proposed the *distant supervision*
    paradigm. The idea behind it is to collect all text mentions where two entities
    co-occur, which are in a relation in the knowledge base. Then it is assumed that
    for this mention pair the relation holds. Since this is not correct for all such
    mention pairs, many approaches aim to combat this ‘noise’. One approach is *multi-instance
    learning*, which relaxes the original assumption that all text mention pairs represent
    the relation to the assumption that the relation holds for at least one pair [[2](#CR2),
    [137](#CR137)], or a specified fraction like 10% or depending on a score value.
    Take for example the entities *“Barack Obama”* and *“Hawaii”*, which might be
    in a relation *“born_in”* in a KB. Sentences obtained by searching for occurrences
    of these two entities could be *“Obama was born in Hawaii”* as well as *“Obama
    was on family vacation in Hawaii”*, where only the former represents the relation
    and should be used for training.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 获取用于关系抽取的大量标注数据集是一项繁琐的任务，并且由于隐私问题通常很难。由于许多关系知识存储在知识库中，Mintz等人[[65](#CR65)]提出了*远监督*范式。其背后的理念是收集所有两个实体共现的文本提及，这些提及在知识库中存在关系。然后假设对于这个提及对，关系是成立的。由于并非所有这样的提及对都是正确的，许多方法旨在对抗这种“噪声”。一种方法是*多实例学习*，它放宽了原始假设，即所有文本提及对都代表关系，改为假设关系至少对一个对成立[[2](#CR2)，[137](#CR137)]，或者根据分数值指定一个比例，例如10%。以实体“Barack
    Obama”和“Hawaii”为例，它们可能在知识库中存在关系“born_in”。通过搜索这两个实体的出现情况获得的句子可能是“Obama was born
    in Hawaii”以及“Obama was on family vacation in Hawaii”，其中只有前者代表关系，应该用于训练。
- en: '**KGPool** [[67](#CR67)] uses entity pairs obtained from a KB, but also attributes
    associated with them. The idea is to create representations of the entity nodes,
    the sentence in which they occur, and the attributes of the entity nodes in a
    knowledge base, such as their description, instance-of and alias attribute. All
    this information is embedded using word and character embeddings and bidirectional
    LSTMs and connected as a heterogeneous information graph. Next three layers of
    graph convolutional networks are used with readout layers. Only relevant attribute
    nodes are picked by using self-attention on the readout representations, calculating
    a softmax score and then filtering via a hyperparameter according to the scores.
    A dynamic mask is created which pools out the less essential entity attribute
    nodes. Finally, all intermediate representations of both entities, the sentence
    and the readouts are each concatenated to form the final entity, sentence and
    readout representation. These representations together with relation representations
    are then passed through a fully connected layer with softmax activation to calculate
    the scores per relation. The *New York Times dataset* is a standard benchmark
    for relation extraction with distant supervision. KGPool achieves a Sota precision@10
    of 92.3%, which is the fraction of relevant results if the ‘best’ 10 of the matches
    are used.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**KGPool** [[67](#CR67)] 使用从知识库中获得的实体对，以及与之相关的属性。其理念是创建实体节点、它们出现的句子以及知识库中实体节点的属性（如描述、instance-of和alias属性）的表示。所有这些信息都通过词和字符嵌入以及双向LSTM进行嵌入，并作为一个异构信息图连接起来。接下来使用三个图卷积网络层和读出层。通过在读出表示上使用自注意力机制，仅选择相关的属性节点，计算softmax分数，然后根据分数通过超参数进行过滤。创建一个动态掩码，将不那么重要的实体属性节点汇总。最后，将实体、句子和读出表示的所有中间表示分别连接起来，形成最终的实体、句子和读出表示。这些表示与关系表示一起通过一个具有softmax激活的全连接层传递，以计算每个关系的分数。*《纽约时报》数据集*是使用远监督进行关系抽取的标准基准。KGPool实现了Sota精度@10为92.3%，这是如果使用匹配的“最佳”10个结果时相关结果的分数。'
- en: 5.4.6 Relation Extraction Using Layout Information
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.6 使用布局信息进行关系抽取
- en: To understand a formal text, often the document layout has be taken into account
    in addition to its text. Especially in form-like texts, the positions of words
    and filled-in values are important. In Sect. [7.​2](528393_1_En_7_Chapter.xhtml#Sec12)
    we will describe, how text and images can be simultaneously processed by one or
    more transformers to extract meaning from both media. In anticipation, we will
    use this ability of transformers to process multimodal inputs and additionally
    include layout information via 2-dimensional positional features. A comprehensive
    overview of progress in layout analysis is provided by Stanisławek [[96](#CR96)].
    We will focus on methods for key-value extraction in this subchapter. In the task
    of key-value extraction, documents are analyzed to extract printed values to written
    keys of interest. Sample applications are the automatic processing of invoices,
    in which keys are attributes such as invoice date or the total amount to be paid.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解正式文本，通常除了文本本身外，还需要考虑文档布局。特别是在类似表单的文本中，单词和填充值的位非常重要。在第 [7.2](528393_1_En_7_Chapter.xhtml#Sec12)
    节中，我们将描述如何通过一个或多个转换器同时处理文本和图像以从这两种媒体中提取意义。作为预期，我们将利用转换器处理多模态输入的能力，并通过二维位置特征额外包含布局信息。Stanisławek
    [[96](#CR96)] 提供了布局分析进展的全面概述。在本子章节中，我们将重点关注键值提取的方法。在键值提取的任务中，分析文档以提取打印的值到感兴趣的书面键。示例应用包括自动处理发票，其中键是诸如发票日期或应付总额等属性。
- en: '**ReLIE** [[57](#CR57)] is a framework for key-value extraction from form-like
    documents. The candidate generation step has the purpose of finding all possible
    value candidates for a certain key, e.g. the value *“1/16/2018”* for the key *“Date”*.
    Often these value candidates correspond to basic types such as numbers, amounts,
    dates, etc. and can be found via rule based matchers. Then a transformer-based
    scoring model is trained, to identify valid values among the extracted value candidates.
    To this end, embeddings are learned for the keys, the position of the value candidate
    and for neighboring tokens and their positions. Positions of a value candidate
    and each of its neighbors are described using the 2-D Cartesian coordinates of
    the centroids of their respective bounding boxes. Note that the text of the candidate
    value is not encoded to avoid overfitting. All embeddings are related to each
    other by self-attention in an autoencoder. The field embedding and the candidate
    embedding are then compared via cosine similarity and the resulting score is scaled
    into a range of [0, 1]. The model achieves an f1-score of 87.8% on key-value extraction
    for invoices and 83.3% for receipts.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReLIE** [[57](#CR57)] 是从类似表单的文档中提取键值对的框架。候选生成步骤的目的是找到某个特定键的所有可能值候选，例如键 *“Date”*
    对应的值 *“1/16/2018”*。通常这些值候选对应于基本类型，如数字、金额、日期等，可以通过基于规则的匹配器找到。然后训练一个基于转换器的评分模型，以识别从提取的值候选中有效的值。为此，学习键、值候选的位置以及邻近标记及其位置的嵌入。值候选及其每个邻居的位置使用它们各自边界框质心的二维笛卡尔坐标来描述。请注意，候选值的文本未进行编码以避免过拟合。所有嵌入通过自注意力在自动编码器中相互关联。然后通过余弦相似度比较字段嵌入和候选嵌入，并将得到的分数缩放到[0,
    1]的范围内。该模型在发票的键值提取任务上实现了87.8%的f1分数，在收据上为83.3%。'
- en: '**DocFormer** [[6](#CR6)] consists of a CNN visual backbone and an encoder-only
    transformer architecture. Visual embeddings of the document are produced via a
    ResNet50 model and projected to the appropriate embedding size via a linear layer.
    Text tokens are contained in a bounding box and the top-left and lower-right position
    of each token bounding box are transformed to embeddings by two different matrices.
    In addition, the height, width and distances between neighboring bounding boxes
    are encoded. The 2D-positional embeddings are enriched with absolute positions
    via 1D-positional embeddings. Separate spatial embeddings are trained for visual
    and textual features. The attention mechanism of the DocFormer is a modified version
    of the original attention mechanism. Separate attention scores are calculated
    for the visual and the textual representation of tokens. In addition to the key-query
    attention, the relative position embeddings of both query and key tokens are used
    to add relative position attentions as well as a spatial attention for both the
    visual and the textual embeddings. The spatial attention weights are shared between
    the visual and the textual representations.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**DocFormer** [[6](#CR6)] 由一个CNN视觉骨干和一个仅编码器的transformer架构组成。文档的视觉嵌入通过ResNet50模型生成，并通过一个线性层投影到适当的嵌入大小。文本标记包含在一个边界框内，每个标记边界框的左上角和右下角位置通过两个不同的矩阵转换为嵌入。此外，还编码了边界框的高度、宽度和相邻边界框之间的距离。二维位置嵌入通过一维位置嵌入丰富了绝对位置。为视觉和文本特征分别训练了单独的空间嵌入。DocFormer的注意力机制是原始注意力机制的修改版。为视觉和文本表示的标记计算了单独的注意力分数。除了键查询注意力外，查询和键标记的相对位置嵌入还被用来添加相对位置注意力以及视觉和文本嵌入的空间注意力。空间注意力权重在视觉和文本表示之间共享。'
- en: 'DocFormer is pre-trained with three different pre-training tasks: multi-modal
    masked language modeling (MM-MLM), learn to reconstruct (LTR) and text describes
    image (TDI). In the MM-MLM task, tokens are masked and should be reconstructed
    by the model. In LTR, the model is tasked to reconstruct the image of a document,
    given the multi-modal representation. A smooth-L1 loss is used to calculate differences
    between the original and the reconstructed image. TDI requires a text-image matching
    task, in which the model has to predict for random samples whether the image and
    the text are aligned or not. The *FUNSD benchmark* [[38](#CR38)] considers forms
    in 199 scanned documents, where tokens have to be assigned to a semantic key,
    such as ‘question’ or ‘answer’. On FUNSD DocFormer reaches an F1-value of 84.6%,
    which was Sota at publication time.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: DocFormer使用三种不同的预训练任务进行预训练：多模态掩码语言建模（MM-MLM）、学习重建（LTR）和文本描述图像（TDI）。在MM-MLM任务中，标记被掩码，模型应该重建这些标记。在LTR中，模型的任务是根据多模态表示重建文档的图像。使用平滑-L1损失来计算原始图像和重建图像之间的差异。TDI要求进行文本-图像匹配任务，其中模型必须预测对于随机样本，图像和文本是否对齐。*FUNSD基准*
    [[38](#CR38)] 考虑了199份扫描文档中的形式，其中标记必须被分配到语义键，例如“问题”或“答案”。在FUNSD中，DocFormer达到了84.6%的F1值，这在发表时是Sota（当时最先进的）。
- en: '**LayoutLM3** [[34](#CR34)] uses an image embedding method inspired by the
    Vision Transformer (Sect. [7.​2.​2](528393_1_En_7_Chapter.xhtml#Sec14)). Each
    image is partitioned into 16 × 16 image patches similar to the Vision Transformer
    and linearly transformed to embeddings. As shown in Fig. [5.8](#Fig8) words and
    image patches are processed by the same autoregressive Transformer. For pre-training
    the model uses the masked language modeling task, masked image patches and word-patch
    alignment pre-training task. In the masked image patches task, image patches have
    to be reconstructed by the model. The word-patch alignment task has to enable
    the model to learn alignments between textual and visual representations. The
    model should classify whether text and image patch of a token are aligned, i.e.
    both are unmasked, or unaligned, i.e. the image patch is masked. The *PubLayNet
    benchmark* [[134](#CR134)] contains the document layout of more than 1 million
    pdf documents matched against the correct document structure. Here LayoutLM3 achieves
    Sota with 94.5% mean average precision of bounding boxes. It outperforms DocFormer
    on the FUNSD key-value extraction tasks and other benchmarks. *LayoutXLM* is a
    recent multilingual version of LayoutLM2 [[116](#CR116)].![](../images/528393_1_En_5_Chapter/528393_1_En_5_Fig8_HTML.png)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**LayoutLM3** [[34](#CR34)] 使用了一种受视觉Transformer（第 [7.2.2](528393_1_En_7_Chapter.xhtml#Sec14)
    节）启发的图像嵌入方法。每个图像被分割成与视觉Transformer相似的 16×16 图像块，并线性转换为嵌入。如图 [5.8](#Fig8) 所示，单词和图像块由相同的自回归Transformer处理。为了预训练模型，使用掩码语言建模任务、掩码图像块和单词-图像块对齐预训练任务。在掩码图像块任务中，模型必须重建图像块。单词-图像块对齐任务必须使模型能够学习文本和视觉表示之间的对齐。模型应分类文本单词和图像块是否对齐，即两者都是未掩码的，或者未对齐，即图像块被掩码。*PubLayNet
    基准测试* [[134](#CR134)] 包含了超过 100 万份 PDF 文档的文档布局，与正确的文档结构相匹配。LayoutLM3 在边界框的平均平均精度方面达到了
    94.5% 的 Sota。它在 FUNSD 关键值提取任务和其他基准测试中优于 DocFormer。*LayoutXLM* 是 LayoutLM2 的最新多语言版本
    [[116](#CR116)]。![](../images/528393_1_En_5_Chapter/528393_1_En_5_Fig8_HTML.png)'
- en: A flow diagram has an input of word tokens and image patches via word and linear
    embedding, linked to 1 D and 2 D position embeddings, input embeddings, autoencoder
    with self-attentions, output embeddings, and logistic classifiers to give probabilities
    for M L M, W P A, and M I M.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 流程图通过单词和线性嵌入输入单词标记和图像块，链接到 1D 和 2D 位置嵌入、输入嵌入、带有自注意力的自动编码器、输出嵌入和逻辑分类器，以给出 M L
    M、W P A 和 M I M 的概率。
- en: Fig. 5.8
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8
- en: 'LayoutLMv3 takes the linear projection of image patches and word tokens as
    inputs and encodes them into contextualized vector representations. LayoutLMv3
    is pre-trained with discrete token reconstructive objectives of Masked Language
    Modeling (MLM) and Masked Image Modeling (MIM). Additionally, LayoutLMv3 is pre-trained
    with a Word-Patch Alignment (WPA) objective to learn cross-modal alignment by
    predicting whether the corresponding image patch of a text word is masked. “Seg”
    denotes segment-level positions. Image source: [[34](#CR34), p. 3], printed with
    kind permission of the authors'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutLMv3 以图像块和单词标记的线性投影作为输入，并将它们编码成上下文向量表示。LayoutLMv3 使用掩码语言建模（MLM）和掩码图像建模（MIM）的离散标记重建目标进行预训练。此外，LayoutLMv3
    还使用单词-图像块对齐（WPA）目标进行预训练，通过预测文本单词的对应图像块是否被掩码来学习跨模态对齐。“Seg”表示段级位置。图像来源：[[34](#CR34)，第
    3 页]，经作者许可印刷
- en: Available Implementations
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用实现
- en: KGPool at [https://​github.​com/​nadgeri14/​KGPool](https://github.com/nadgeri14/KGPool)
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KGPool 在 [https://github.com/nadgeri14/KGPool](https://github.com/nadgeri14/KGPool)
- en: 5.4.7 Summary
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.7 摘要
- en: Relation extraction has the task to evaluate the expressed relationship in the
    text with respect to specific entities. An example is the assessment of certain
    product characteristics by customers, which can help to improve the product or
    service. Given the massive amount of textual content, it is intractable to manually
    process the opinion information.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 关系抽取的任务是评估文本中特定实体表达的关系。例如，评估客户对某些产品特性的评价，这有助于改进产品或服务。鉴于大量文本内容，手动处理意见信息是不可行的。
- en: For simple cases, the relation arguments are know and relation extraction can
    be solved as a simple classification task using some BERT variant like RoBERTa,
    DeBERTa, or SpanBERT. However, to actually use these models we have to extract
    the relation arguments in a prior step, which leads to an increased total error.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单的情况，关系论元是已知的，关系抽取可以作为一个简单的分类任务来解决，使用一些BERT变体，如RoBERTa、DeBERTa或SpanBERT。然而，为了实际使用这些模型，我们必须在先前步骤中提取关系论元，这会导致总误差的增加。
- en: More challenging is the simultaneous extraction of relation arguments and the
    corresponding relation type, as these task depend on each other. UniRE annotates
    entities and relations in a joint matrix and introduces a corresponding bias into
    the self-attention computations. PL-marker marks the first relation arguments
    with special tokens and the second argument with so-called leviated tokens. These
    tokens have specific attention properties and are able to improve the performance
    on popular benchmarks. GRACE employs a specific encoder-decoder architecture where
    the encoder labels the relation arguments (aspects) and the decoder assigns relation
    tags to each token. REBEL uses the BART encoder-decoder to translate the input
    sentence to a unique representation of the covered relations.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 同时抽取关系论元和相应的关系类型更具挑战性，因为这些任务相互依赖。UniRE在联合矩阵中注释实体和关系，并在自注意力计算中引入相应的偏差。PL-marker使用特殊标记标记第一个关系论元，并使用所谓的升位标记标记第二个论元。这些标记具有特定的注意力属性，能够提高在流行基准上的性能。GRACE采用特定的编码器-解码器架构，其中编码器标记关系论元（方面），解码器为每个标记分配关系标签。REBEL使用BART编码器-解码器将输入句子翻译为覆盖关系的唯一表示。
- en: Relation extraction models have been adapted to specific applications. GRACE
    has been tuned for aspect-based sentiment analysis and Crf2o to semantic role
    labeling. The latter uses contextual embeddings and determines the relation between
    predicate and corresponding phrases by an efficient TreeCRF. Finally, MaMa can
    be used to build a knowledge graph from extracted relations between entities.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 关系抽取模型已被调整以适应特定应用。GRACE已被调整用于基于方面的情感分析，而Crf2o用于语义角色标注。后者使用上下文嵌入，并通过高效的TreeCRF确定谓语与相应短语之间的关系。最后，MaMa可以用来从实体间提取的关系构建知识图谱。
- en: Often the spatial layout of documents and web pages contains relevant information
    for the extraction of relation arguments. In this case, visual information from
    the document image can be exploited to arrive at a valid interpretation. This
    visual information can be included via the position of bounding boxes for keys
    and values, but also in the form of image patches, which are explored later with
    the image transformer.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，文档和网页的空间布局包含对关系论元抽取相关的信息。在这种情况下，可以利用文档图像中的视觉信息来得出有效的解释。这种视觉信息可以通过键和值的边界框位置来包含，也可以以图像补丁的形式包含，这些补丁将在后面的图像转换器中进行探索。
- en: All recent relation extraction approaches are based on PLMs. Most models use
    small BERT variants for their experiments. Therefore, it can be assumed that larger
    models will directly increase performance. In addition, Foundation Models like
    GPT-3 may be fine-tuned (Sect. [3.​6.​2](528393_1_En_3_Chapter.xhtml#Sec40)) and
    probably will result in a higher accuracy. A related alternative is InstructGPT
    (Sect. [3.​6.​5](528393_1_En_3_Chapter.xhtml#Sec43)), which can be easily directed
    to perform a relation extraction via question answering, e.g. *“Who built the
    statue of liberty?”* [[77](#CR77), p. 29]. However, it seems to be difficult to
    evaluate the performance of this approach with respect to some test data.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 所有最近的关系抽取方法都是基于预训练语言模型（PLMs）。大多数模型在实验中使用了小的BERT变体。因此，可以假设更大的模型将直接提高性能。此外，像GPT-3这样的基础模型可能可以进行微调（见第[3.6.2](528393_1_En_3_Chapter.xhtml#Sec40)节）并且可能带来更高的准确率。一个相关的替代方案是InstructGPT（见第[3.6.5](528393_1_En_3_Chapter.xhtml#Sec43)节），它可以通过问答轻松地被引导执行关系抽取，例如*“谁建造了自由女神像？”*
    [[77](#CR77), p. 29]。然而，似乎很难用一些测试数据来评估这种方法的表现。
- en: '[![Creative Commons](../css/cc-by.png)](https://creativecommons.org/licenses/by/4.0)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Creative Commons](../css/cc-by.png)](https://creativecommons.org/licenses/by/4.0)'
- en: '**Open Access** This chapter is licensed under the terms of the Creative Commons
    Attribution 4.0 International License ([http://​creativecommons.​org/​licenses/​by/​4.​0/​](http://creativecommons.org/licenses/by/4.0/)),
    which permits use, sharing, adaptation, distribution and reproduction in any medium
    or format, as long as you give appropriate credit to the original author(s) and
    the source, provide a link to the Creative Commons license and indicate if changes
    were made.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**开放获取** 本章节根据Creative Commons Attribution 4.0 International License（[http://creativecommons.org/licenses/by/4.0/](http://creativecommons.org/licenses/by/4.0/)）的条款进行许可，允许在任何媒介或格式中使用、分享、改编、分发和复制，只要您适当引用原始作者和来源，提供Creative
    Commons许可的链接，并指出是否进行了修改。'
- en: The images or other third party material in this chapter are included in the
    chapter's Creative Commons license, unless indicated otherwise in a credit line
    to the material. If material is not included in the chapter's Creative Commons
    license and your intended use is not permitted by statutory regulation or exceeds
    the permitted use, you will need to obtain permission directly from the copyright
    holder.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节中的图像或其他第三方材料包含在本章节的Creative Commons许可中，除非在材料引用行中另有说明。如果材料未包含在本章节的Creative
    Commons许可中，且您的使用意图不受法定法规允许或超出允许的使用范围，您将需要直接从版权持有人处获得许可。
